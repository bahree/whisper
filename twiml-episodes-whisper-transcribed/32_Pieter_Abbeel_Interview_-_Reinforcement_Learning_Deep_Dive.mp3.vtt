WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.960
people doing interesting things in machine learning and artificial intelligence.

00:20.960 --> 00:23.760
I'm your host Sam Charrington.

00:23.760 --> 00:28.320
This week we continue our exploration into industrial AI.

00:28.320 --> 00:33.600
Is that you ask, well in my forthcoming paper on the topic, I define industrial AI as

00:33.600 --> 00:40.080
any application of AI relating to the physical operations or systems of an enterprise.

00:40.080 --> 00:45.040
I go on to note that the focus of industrial AI is on helping enterprises monitor, optimize

00:45.040 --> 00:51.200
or control the behavior of these operations and systems to improve their efficiency and performance.

00:51.200 --> 00:54.320
When people hear the phrase industrial, they quickly jump to manufacturing, but it's

00:54.320 --> 01:00.320
more than that. Industrial AI includes manufacturing applications like robotics and using computer

01:00.320 --> 01:05.440
vision for quality control, but also applications like supply chain optimization and risk management,

01:05.440 --> 01:11.120
warehouse automation, the monitoring and operation of building HVAC systems and much more.

01:11.120 --> 01:18.000
For more information about industrial AI or the report, visit twimlai.com slash industrial AI.

01:18.000 --> 01:25.840
This week our guest is Peter Rabiel, Assistant Professor at UC Berkeley, Research Scientist at Open AI

01:25.840 --> 01:31.280
and Co-Founder of Great Scoop. Peter has an extensive background in AI research,

01:31.280 --> 01:36.320
going way back to his days as Andrew Ng's first PhD student at Stanford.

01:36.320 --> 01:39.200
His work today is focused on deep learning for robotics.

01:39.840 --> 01:43.920
During this conversation, Peter and I really dig into reinforcement learning,

01:43.920 --> 01:49.280
which is a technique for allowing robots or other AI's to learn through their own trial and error.

01:50.080 --> 01:54.960
Before we jump in, a quick nerd alert. This conversation explores cutting edge research with

01:54.960 --> 02:00.640
one of the leading researchers in the field, and as a result, it gets pretty technical at times.

02:00.640 --> 02:05.600
I try to up level it when I can keep up myself, so hang in there. I promise that you'll learn

02:05.600 --> 02:11.200
a ton if you keep with it. I could also use your feedback here. You want more or fewer of these

02:11.200 --> 02:16.080
kinds of conversations. Let me know in the comments, along with any feedback, comments,

02:16.080 --> 02:22.320
or questions you have about this episode. Before we jump in, a word about our sponsor.

02:22.320 --> 02:26.640
I introduced Bonsai last week, and once again, I'd like to thank the team over there

02:26.640 --> 02:31.360
for sponsoring this podcast series, as well as my forthcoming industrial AI research.

02:32.240 --> 02:38.000
Bonsai offers an AI platform that empowers enterprises to build and deploy intelligent systems.

02:38.000 --> 02:42.720
If you're trying to build AI-powered applications focused on optimizing and controlling the systems

02:42.720 --> 02:47.440
in your enterprise, you should take a look at what they're up to. They've got a unique approach to

02:47.440 --> 02:52.560
building AI models that let you use high-level code to model real-world concepts in your application,

02:53.200 --> 02:57.200
automatically generate, train, and evaluate low-level models for your project,

02:57.200 --> 03:03.120
using reinforcement learning and other technologies, and then easily integrate those models into your

03:03.120 --> 03:10.320
applications and systems using APIs. You should check them out at bonds.ai, bons.ai, bons.ai,

03:10.320 --> 03:15.920
and definitely let them know you appreciate their support of the podcast. And now on to the show.

03:24.560 --> 03:30.800
All right, hello, everyone. I have got Peter Abil on the line. Peter is an associate professor

03:30.800 --> 03:38.240
at UC Berkeley, a research scientist at OpenAI, and a co-founder at GradeScope. Peter, how are you?

03:39.200 --> 03:45.360
Doing pretty well. How about you, Sam? I'm doing very well and excited to jump into this conversation.

03:45.360 --> 03:52.720
In addition to all of those positions you hold, you are also at the cutting edge of a very interesting

03:52.720 --> 03:59.120
field in machine learning called deep reinforcement learning. And I'm really looking forward to

03:59.120 --> 04:05.200
learning a bunch about that, talking with you today. Why don't we get started by

04:05.200 --> 04:09.840
having you tell us a little bit about your background and kind of how you got to where you are now

04:09.840 --> 04:15.920
in your area of research? Sure, yeah. If I go pretty far back, actually, as a high school,

04:15.920 --> 04:21.200
my excitement was mostly about physics and math, and from there engineering was a natural

04:21.200 --> 04:26.640
next pick. And when I was wrapping up my bachelor's in engineering, I just found so many topics so

04:26.640 --> 04:31.360
exciting and it was hard to choose, but ultimately it seemed artificial intelligence was the area

04:31.360 --> 04:37.600
that could drive almost all other areas. It is by making progress in AI, it might be possible to help

04:37.600 --> 04:43.200
know how to do many, many other things. And so that's kind of what drove me into AI and then got

04:43.200 --> 04:48.480
me started on my masters and then PhD on the artificial intelligence with Andrew at Stanford.

04:48.480 --> 04:52.880
And then from there I became professor at Berkeley and research scientist at OpenAI.

04:52.880 --> 04:57.680
Well, working with Andrew, that's an impressive credential. He's done a lot of amazing things in

04:57.680 --> 05:02.880
the field. It couldn't agree more. It's very fortunate that actually his first PhD students,

05:02.880 --> 05:10.320
I saw him start from ground zero, which was amazing. Oh wow. Wow. And so what do you focus on today?

05:11.200 --> 05:16.800
So a lot of what drives my work is trying to get robots out in the real world,

05:16.800 --> 05:21.200
meaning beyond repetitive tasks, as you would see in current robot deployments.

05:21.200 --> 05:27.760
And so what I think is key to get robots out in the real world and doing more flexible things is

05:27.760 --> 05:32.640
to give them more intelligence and key to that will be for them to be able to learn rather than

05:32.640 --> 05:35.760
us having to program them for every possible scenario they could encounter.

05:36.480 --> 05:42.560
And so reinforcement learning obviously comes up in that. Is that just one of a number of techniques

05:42.560 --> 05:50.160
that you're looking into to make robots smarter or what's kind of the landscape of things that you're

05:50.160 --> 05:55.280
pursuing there? All right. So there's there's many ways to learn. If you look at the kind of

05:55.280 --> 06:00.400
machine learning landscape, there is supervised learning, which is recognizing a pattern between

06:00.400 --> 06:05.120
inputs and outputs. There is unsurface learning, which is trying to make sense of just data doesn't

06:05.120 --> 06:10.640
have any labels. There's reinforcement learning, which looks at trying to optimize the reward,

06:10.640 --> 06:16.160
which is a really good fit for robotics. So let's say you have a robot and maybe you want it to

06:16.160 --> 06:22.640
clean your home. You could define a reward function that says the cleaner my home, the higher the

06:22.640 --> 06:28.800
reward. And then a reinforcement algorithm deployed on a robot would try to optimize that reward

06:28.800 --> 06:32.880
than as a consequence, optimize the cleanliness of your home if it's successful.

06:34.240 --> 06:40.800
So we've been hearing a lot about deep reinforcement learning of late, but is reinforcement learning

06:40.800 --> 06:47.600
as a whole? Is it new or has it been in use for a while? And how has it done prior to deep learning?

06:48.240 --> 06:53.600
Yeah. So reinforcement learning has been around for a long time. Ever since people start thinking

06:53.600 --> 06:58.160
about artificial intelligence, they were thinking about things like reinforcement learning. Because

06:58.160 --> 07:05.200
when you think about AI, you think about some kind of intelligent system supposedly required to

07:05.200 --> 07:10.160
make a decision. And then after making that decision, there will be consequences. And then it will

07:10.160 --> 07:15.200
need to deal with the consequences of the action. And this will repeat over and over and over.

07:15.200 --> 07:19.200
And that is exactly the reinforcement learning setting where some systems, some AI systems

07:19.200 --> 07:23.440
are supposed to make decisions that have impact over time and then adjust over it.

07:24.320 --> 07:28.800
And so it's been around for a very long time. Actually, even before deep reinforcement,

07:28.800 --> 07:34.160
there were quite a few interesting success stories. For example, Andrew, his autonomous helicopter,

07:34.160 --> 07:38.720
I was part of the group working on that. But the helicopter at Stanford was largely driven by

07:38.720 --> 07:43.360
a reinforcement learning. And this was just regular reinforcement learning, no deep neural nets

07:43.360 --> 07:51.920
behind it. Russ Tedrick, professor at MIT, but during his PhD at MIT also, he build a biped walker

07:51.920 --> 07:56.800
that learned to walk with regular reinforcement learning, no deep networks involved.

07:57.760 --> 08:03.360
But what characterized those early successes is that it required a combination of a lot of

08:03.360 --> 08:09.200
demand expertise with expertise in reinforcement learning. So you would carefully think about how

08:09.200 --> 08:14.320
does the helicopter work? What are the relevant aspects of controlling helicopter? Talk to experts

08:14.320 --> 08:19.200
in helicopter piloting what they pay attention to, what they think about. And then you would condense

08:19.200 --> 08:24.480
that into some representation. You'd define to decide what it means to be a good helicopter

08:24.480 --> 08:28.240
control policy. You'd say, well, helicopter control policy would look at these and these and these

08:28.240 --> 08:34.400
aspects of helicopter state and then make a decision based on that. And leave a few free parameters

08:34.400 --> 08:38.960
in there. Stanford, a helicopter control kit, there were 12 parameters that were not determined,

08:38.960 --> 08:43.360
there were just real numbers that were hard to come up with by hand. But then the reinforcement

08:43.360 --> 08:48.640
learning algorithm would find those 12 parameters better than a person could find them by hand.

08:48.640 --> 08:54.880
And that would lead to extremely reliable helicopter flight. But the big difference now with

08:54.880 --> 09:01.920
a deeper reinforcement learning is that it largely takes away the need for domain expertise.

09:02.560 --> 09:09.680
So if you look at the results on Atari go or the results in robotics that we, for example,

09:09.680 --> 09:16.320
got on Berkeley with learning assembly, you look at those and what goes into the thing that's

09:16.320 --> 09:20.880
learning is raw sensory percepts. So it'll get raw pixels from the Atari game, it'll get

09:20.880 --> 09:26.640
it just the raw board configuration, not some new encoding of strengths or weaknesses

09:26.640 --> 09:31.440
about the board configuration, just the raw configuration for learning assembly. We'll get raw

09:31.440 --> 09:37.280
pixels of what the robot is seeing and we'll need to make decisions based on that. And so

09:38.240 --> 09:43.680
now it's the deep network that somehow makes sense of this raw sensory information and turns

09:43.680 --> 09:50.320
it then into after a long computation into meaningful control commands, which is a very different

09:50.320 --> 09:54.160
setup from this previous successes where you would have had to analyze ahead of time.

09:54.160 --> 09:58.960
What is it that we need to pay attention to? How can we extract that with a separate piece of code

09:58.960 --> 10:06.640
that then is fed into the reinforcement learner? Are we limited by our ability to incorporate

10:06.640 --> 10:12.160
that domain expertise into the deep neural networks? And the background for that question is I'm

10:12.160 --> 10:18.000
just imagining that if we were able to incorporate that domain expertise in, the models would be

10:18.000 --> 10:22.880
even smarter and more accurate. This is a very interesting question. How can you kind of get the

10:22.880 --> 10:28.160
best of both worlds? Exactly, exactly. I think it's like, how do you get both the domain expertise

10:28.160 --> 10:35.520
and the flexibility of learning things from raw sensory information? Like, for example,

10:35.520 --> 10:41.360
I had a conversation with Stefano Irman who you may know over at Stanford. And one of the things

10:41.360 --> 10:48.160
that we talked about was some of his work on incorporating, for example, physics models into

10:49.280 --> 10:56.400
machine learning models and how they were able to dramatically increase the accuracy of a

10:57.440 --> 11:04.480
projectile trajectory model by telling a little bit about the parabolic trajectory that

11:04.480 --> 11:09.600
things take when they're moving through free space to be a physics. And it strikes me that if we

11:09.600 --> 11:16.640
could, again, get the best of both worlds, that would help us here. Absolutely. So that is a perfect

11:16.640 --> 11:22.880
example. So let me maybe expand the scope of this a little bit. So if you look at prior knowledge,

11:22.880 --> 11:28.000
I can come in in many different formats. So one thing is, for example, you might know that

11:28.000 --> 11:35.200
physics is involved and that the laws of physics could help you in making decisions. It might also be

11:35.200 --> 11:40.000
that certain existing algorithms could be relevant. Maybe you know that a common filter which was

11:40.000 --> 11:46.960
actually used to track the first rocket that went to the moon, that that idea is going to be relevant

11:46.960 --> 11:52.320
because you want to track maybe the position of your self-driving car. Or maybe you know that

11:52.320 --> 11:57.440
a planning competition, a competition that doesn't just reactively look at the current

11:57.440 --> 12:03.440
sensory inputs and the spit tide and action, but actually thinks ahead simulates what would happen

12:03.440 --> 12:08.320
if you were to take a certain sequence of actions. And then based on that makes a decision.

12:08.320 --> 12:14.720
And so there's many of those ideas out there that we know can play a role in decision-making.

12:15.600 --> 12:21.040
And if a deep neural net is supposed to figure it out all from scratch, the data needs might be

12:21.040 --> 12:27.280
prohibitive to get to a practical application anytime soon. And so you can do it there, which is

12:27.280 --> 12:32.640
actually really interesting, which maybe transcends deep networks a little bit, which is actually the

12:32.640 --> 12:39.120
automatic differentiation frameworks, such as TensorFlow, Theano, PyTorch, and so forth. These

12:39.120 --> 12:45.120
frameworks actually can differentiate to anything. They're not specific to neural nets. And so we

12:45.120 --> 12:51.120
can do is you can set up something more complex than a neural net, more generalized the competition

12:51.120 --> 12:55.600
graph. What you can do with that is you can tend to set up a competition graph that encodes

12:56.720 --> 13:00.560
the algorithm or the prior knowledge that you have. So for example, if you thought you wanted to

13:00.560 --> 13:05.840
use a common filter, you could set up the equations of the common filter inside these frameworks.

13:06.960 --> 13:11.040
Now typical would happen is you still need to deal with raw pixels. So you would

13:11.840 --> 13:16.880
take your raw pixels, feed them through a deep network that feeds into these common filter

13:16.880 --> 13:23.520
equations that then leads to some output. And so what you get then if you train this is that

13:23.520 --> 13:28.640
you're training this big competition graph that has the flexibility of a neural net,

13:28.640 --> 13:33.520
namely the ability to adapt to how you should process raw sensory information. But then also the

13:34.080 --> 13:39.120
advantage of knowing that a certain competition will matter and have it built into it. And you can

13:39.120 --> 13:44.640
optimize this all in one optimization. You don't need to separately find a neural net that you then

13:44.640 --> 13:49.680
plug into a common filter. The same is true for the physics equations that you were referring to

13:49.680 --> 13:53.680
that could be, for example, inside of a planner. So you could have your neural net processing

13:53.680 --> 14:00.400
raw sensory information, feed it into another competition that has a planner in it and the planner

14:00.400 --> 14:05.920
can rely on physics equations. And you don't fully wire up the details of how it's going to rely

14:05.920 --> 14:10.960
on that. You let it figure that out, but it's a component that sits there ready for it to use

14:10.960 --> 14:16.000
such that it can be more effective at learning from whatever data it's getting, what the right thing

14:16.000 --> 14:22.000
is to do. It sounds like there's definitely a lot in there to unpack, but it sounds like in a

14:22.000 --> 14:28.160
nutshell, what you're saying is that these, you know, our prior knowledge, whether it's a form of

14:28.160 --> 14:33.360
in the form of governing equations or models or subject matter expertise or what have you,

14:33.360 --> 14:39.120
you can almost think of them as like features that you're eventually going to be

14:39.120 --> 14:45.040
using as inputs to your neural networks. And in fact, you know, these features, we've got a

14:45.040 --> 14:49.600
tremendous amount of depth that we can express through, you know, matrix math and differential

14:49.600 --> 14:54.320
equations, you know, they can go through the same infrastructure that we're using to train our

14:54.320 --> 15:00.480
reinforcement learning models vis-a-vis TensorFlow and the like. Absolutely correct. And one thing

15:00.480 --> 15:06.000
that this also reminds me of is when you think about all these equations, they've been developed

15:06.000 --> 15:11.920
over time. Mathematics is been developed over time. If you think of, you know, there's a lot of

15:11.920 --> 15:19.280
benefits to reinforcement learning in and of itself. And it's really powerful to be able to

15:19.280 --> 15:27.440
learn from scratch. But the way humans often learn is actually by imitation, which is just as

15:27.440 --> 15:33.680
important an area of deep learning for action, for systems that can take action. Because if you

15:33.680 --> 15:38.880
think about it, I mean, imagine, you know, our intelligence now, our reinforcement and capabilities

15:38.880 --> 15:43.040
now as humans are probably not that different from what our reinforcement and capabilities

15:43.040 --> 15:48.240
were 100,000 years ago or even 200,000 years ago. But we live a very different life. And the

15:48.240 --> 15:53.440
reason we live a very different life is because we don't know start from scratch. We actually build

15:53.440 --> 15:58.720
upon what previous generations have developed, have built, and then we learn from that much more

15:58.720 --> 16:05.120
effectively than if we had to start from scratch. And so a lot of applications of reinforcement

16:05.120 --> 16:10.080
learning actually will rely on a combination of imitation learning and reinforcement learning.

16:10.080 --> 16:14.800
So typical setting would be something where you say, well, I want my robot maybe to, I don't know,

16:14.800 --> 16:21.440
one maybe stack some dishes. And then a natural thing to do would be to instead of starting from

16:21.440 --> 16:25.600
scratch with reinforcement learning, which of course, in some sense is beautiful intellectually,

16:25.600 --> 16:30.160
but at the same time is very ineffective given that you know what stacking dishes looks like.

16:30.640 --> 16:36.480
The more natural thing to do would be to show how to stack a few dishes and have the system

16:36.480 --> 16:43.440
watch you do that. And then use that as a guide to then learn its own motor skills that will

16:43.440 --> 16:49.680
allow it to match up with what you just did. That should poses a lot of challenges. In the simplest

16:49.680 --> 16:54.320
version, you would actually just move the robot's arms around and make the robot experience everything.

16:54.320 --> 16:58.400
But that is not how you would want to do it in the longer and longer and you want to just do it

16:58.400 --> 17:04.880
yourself has the robot watch you and understand what is the essence namely the objects that are being

17:04.880 --> 17:10.080
moved around. And what is not essence namely that it's your hands versus robot hands or that maybe

17:10.080 --> 17:14.880
you are moving your head in certain ways that are irrelevant to the task because you just might

17:14.880 --> 17:20.400
be looking because somebody comes by and checking out what they're doing. And so a robot understanding

17:20.400 --> 17:26.000
from watching a human what is the essence and distilling that into then understanding how to do

17:26.000 --> 17:30.320
something themselves in their own body which is different from the human's body.

17:31.440 --> 17:34.400
There's a lot of interesting challenges that actually will go a long way in terms of

17:34.400 --> 17:40.960
seeding the robot's capabilities to then bring in reinforcement learning to really get fine-tuned

17:40.960 --> 17:46.880
skills. When we're doing imitation learning what what's the underlying mechanism or what are some

17:46.880 --> 17:52.160
of the underlying mechanisms that we're using to kind of capture what the you know what we're

17:52.160 --> 17:57.360
training on that what we're imitating is it like we're constraining the the state space of the

17:57.360 --> 18:01.760
ultimate solution. And so like we don't have to train a bunch of things or are we like building

18:01.760 --> 18:06.080
representations of what the robot's seeing or some combination of all that plus other stuff.

18:06.720 --> 18:10.800
Yeah so there's been quite a few interesting things happening over the past year that I think

18:11.600 --> 18:18.320
are changing what's possible with imitation learning. So one piece of work that happened to come

18:18.320 --> 18:26.400
out of OpenAI about a half year ago was a third person imitation learning. And so what that considers

18:26.400 --> 18:33.120
is it it considers the specific problem of how to learn when you are watching from a third

18:33.120 --> 18:37.840
person point of view what it is that you should do but then later you should do it yourself

18:37.840 --> 18:42.400
in which case it'll look very different because it's now you with your own hands from your own

18:42.400 --> 18:48.480
viewpoints, first person viewpoints. And so some of the ideas we we put out play there were

18:49.120 --> 18:55.040
actually quite related to some work Stefano Armand did at Stanford who you just mentioned

18:55.040 --> 19:02.640
on imitation learning through generative adversarial networks. And so the idea there was to

19:02.640 --> 19:08.400
say in the original paper from Stefano's group, this was John Fennel, Stefano's student,

19:08.400 --> 19:15.200
they looked at how can we generate a behavior that an adversarial network cannot distinguish

19:15.200 --> 19:20.400
from the demonstration behavior because if an adversarial neural net cannot distinguish the robot

19:20.400 --> 19:26.960
behavior from the demonstrations, then that means the robot might have captured what matters about

19:26.960 --> 19:33.200
those demonstrations. Now the tricky part is what does behavior mean here is it is it the movements

19:33.200 --> 19:40.400
or the outcomes? So those are things that the organ designer has a little bit of an open

19:40.400 --> 19:44.240
and a choice there. Do you just let it look at the outcome? Do you let it look at the entire

19:44.240 --> 19:48.800
trajectory? Most of the work let's again look at the entire trajectory and based on that make a

19:48.800 --> 19:56.480
decision of whether this is the same or not the same as the expert. Now when when you do first

19:56.480 --> 20:02.000
person demonstrations where you're inside the robot, this will work. But now if you have a third

20:02.000 --> 20:06.640
person view on the demonstration, this will never work. This is always very easy to distinguish

20:06.640 --> 20:11.280
the demonstration from the robot's execution because while the demonstration is going to be this

20:11.280 --> 20:16.320
this human doing something and then the robot's going to do it and it's obviously there's a human

20:16.320 --> 20:21.520
or a robot and so you can't directly apply this in a third person setting. And when you say inside

20:21.520 --> 20:27.200
the robot, you mean controlling a robot remotely using some controller? Yeah, we're using some

20:27.200 --> 20:33.040
controllers, but then the robot experiences it themselves as if they're doing it. And so

20:34.320 --> 20:40.320
what you need then is something extra, something that says, I want the GAN not to be able to distinguish

20:40.320 --> 20:45.840
between the two, but what it's looking at the GAN should not have access to certain pieces of

20:45.840 --> 20:50.960
information. She not have access to essentially what identifies the human versus robot because then

20:50.960 --> 20:54.400
it's obvious and it's not actually paying attention to the essence of what you care about.

20:54.400 --> 20:58.960
Right. So in the third person imitation work, we brought everybody in another actually GAN

20:58.960 --> 21:05.120
related idea, domain confusion. And what you do there is you you process information through

21:05.120 --> 21:11.360
your neural net and then at some layer in the neural net, you decide that it should not be possible

21:11.360 --> 21:18.080
to distinguish between two things. In this case, between whether it was a robot or a human doing it.

21:18.960 --> 21:22.560
And if you're not allowed to distinguish in that layer, that means that layer, the features that

21:22.560 --> 21:28.320
live in that layer are not allowed to contain information anymore about human versus robot,

21:28.320 --> 21:32.800
but only about other things like the objects in the scene, which are shared between the

21:32.800 --> 21:39.360
demonstration and the robot. And so that's a way to make sure that that information is removed

21:39.360 --> 21:44.080
and then you start paying attention to the essence when you're trying to imitate.

21:44.800 --> 21:50.160
That shares other applications too. For example, Richard Semmel's group at the University of Toronto

21:50.160 --> 21:55.520
has looked at this in terms of understanding things like fairness in machine learning.

21:55.520 --> 21:59.040
So you can imagine that you don't want to make a decision based on certain features.

21:59.040 --> 22:02.720
Maybe you don't want to make a decision with your machine learning system based on race.

22:02.720 --> 22:07.440
But if you just remove race from your feature set, that's not enough because the zip code might

22:07.440 --> 22:11.920
curl it with race or anything else might curl it with race. And so as you know in the processes

22:11.920 --> 22:17.360
things, you could decide that at some layer, the information should not be extractible anymore,

22:17.360 --> 22:22.960
what the race was of the person being processed. And then at that point, you know that the

22:22.960 --> 22:27.920
decisions being made by your machine learning system, while not depending on the feature that you

22:27.920 --> 22:33.040
didn't want to depend on, not just not directly depending on it, but also implicitly not depending

22:33.040 --> 22:38.240
on it through how it could have figured it out from other features. So that's the domain

22:38.240 --> 22:47.120
confusion idea. And another application domain. And now how are we identifying the layers that

22:47.120 --> 22:52.000
are encoding this feature that we don't want to be able to use?

22:54.000 --> 22:58.000
So that's a good question and often requires a little bit of trial and error. But essentially,

22:58.000 --> 23:02.720
you take a deep network and somewhere in the middle of that network, you pick a layer and decide

23:02.720 --> 23:08.240
this layer is the one I'm going to use. And at this point, it's not allowed to be distinguishable

23:08.240 --> 23:14.240
anymore, let's say between human and robot, or you know, can't recover race or yet, you know,

23:14.240 --> 23:20.240
sometimes people would do it between simulated environment and real world environment and so forth.

23:20.240 --> 23:24.960
And what does it mean to not allow the network to use that layer? Does it mean you're not propagating

23:25.760 --> 23:29.040
things from that layer forward or you're not, you're changing weights or

23:29.040 --> 23:34.880
so what that means is that at that layer, there is a network that continues that will try to make

23:34.880 --> 23:39.600
a decision of maybe what action to take or whether it's experts versus non expert and so forth.

23:39.600 --> 23:45.600
But then you branch off a second head of the neural network. It'll get a second head and

23:45.600 --> 23:50.640
it could be a pretty deep head that can do a lot of competition. And that second head is on

23:50.640 --> 23:58.080
its output classifies, let's say, between robot and person. And then instead of maximizing the

23:58.080 --> 24:02.960
accuracy of that head, you minimize the accuracy. You make it maximally confused.

24:04.000 --> 24:09.920
So you're basically training your network to forget about or obfuscate that information in

24:09.920 --> 24:16.480
that network and that layer rather. Exactly. So if you wanted to be that that network can do well

24:16.480 --> 24:21.920
and the head is trying to be accurate, but the layers before the split need to do something to

24:21.920 --> 24:27.520
ensure that that head cannot be accurate. So the early set of layers needs to lose the information

24:27.520 --> 24:32.720
so that that head cannot achieve what it's trying to do. That's incredible.

24:34.320 --> 24:41.040
Interesting, interesting. So we've gotten pretty deep here. I want to maybe take a step back

24:41.040 --> 24:50.000
to kind of RL deep reinforcement learning and maybe address, I think when most folks come into

24:50.000 --> 24:56.400
contact with reinforcement learning, it's probably in the context of games like Atari video games

24:56.400 --> 25:05.040
and other games that people are training RL models to try to play. Well, first, I'm wondering if

25:05.040 --> 25:12.080
you can speak to, what's the significance of games to RLY? Are they so popular as training

25:12.080 --> 25:19.840
vehicles? And then maybe we can dig into some of the techniques that folks are using these

25:19.840 --> 25:23.840
are policy gradients and Q learning and what those mean and how they're applied.

25:23.840 --> 25:29.120
Sure. So there are a few things that make games very interesting as the research environment.

25:29.120 --> 25:36.880
So one aspect is that games are designed by humans for other humans. And so they're designed with

25:36.880 --> 25:40.800
some kind of intelligence in mind. And so it's very interesting to see if we can build

25:40.800 --> 25:46.080
an artificial intelligence that can also play those games. Now related to that, they're designed by

25:46.080 --> 25:51.440
humans for other humans, but they were designed not with artificial intelligence in mind. So

25:51.440 --> 25:56.240
it's not something where you get to design a game once you have your argument in mind. The

25:56.240 --> 26:01.120
games already exist. And we need to see if our algorithms can tackle these existing games.

26:02.720 --> 26:08.080
I will say there are also a few downsides to games. So one of their big upsides is of course

26:08.080 --> 26:15.360
that simulation is easier than real world experimentation. Far safer, you can parallelize more easily

26:15.360 --> 26:20.400
and you can often get more self-contained environments to make it easier to run large-scale

26:20.400 --> 26:25.120
experiments through the size of the environment. And so there's a lot of benefits to simulation,

26:25.120 --> 26:30.800
including also simulation of other things such as simulated robots. Where games can get a little

26:30.800 --> 26:37.040
tricky is that it's not always obvious if you start looking at more advanced things like

26:37.040 --> 26:41.520
transfer. If you learn in one game, can you learn more quickly in another game? It's not always

26:41.520 --> 26:46.560
obvious that whether this should be possible or not, whereas if you look at things like, for example,

26:46.560 --> 26:52.160
simulated or real robotic manipulation, it's more natural to expect that if you learn to pick up one

26:52.160 --> 26:58.320
object, it should help you learn to pick up another object. And if your algorithm is not able

26:58.320 --> 27:03.440
to get that kind of transfer, it's probably all of them that's that fault. Or maybe it didn't see

27:03.440 --> 27:08.960
enough data yet. Whereas if you learn to play, let's say, Pong, and then you're supposed to learn

27:08.960 --> 27:13.120
Montezuma as a revenge, it's not immediately obvious that there should be any transfer between the

27:13.120 --> 27:21.200
two games. And so one of the big questions, I think, when you think about RL, are you learning

27:21.200 --> 27:28.720
for mastery or you're learning for generalization? And so, masteries where you stay within one environment,

27:28.720 --> 27:33.840
one game. You say, I want to master Montezuma's revenge, which probably no system has done yet.

27:33.840 --> 27:41.440
It's one of the harder Atari games in that suite that's researched a lot. But it's still a different

27:41.440 --> 27:46.400
question. Can you master one game versus can you learn something that then can be helping you

27:46.400 --> 27:50.800
in the future to learn something else more quickly? Right. And so that's that's where

27:51.840 --> 27:56.880
games can get a little trickier unless you're very careful about maybe which set of games you choose.

27:56.880 --> 28:00.960
And at the risk of kind of going kind of arguing down into another detail.

28:02.880 --> 28:07.600
With regards to transfer learning, is there any work looking at transfer learning?

28:07.600 --> 28:15.520
Is transfer learning only done kind of at the level of an entire deep network? Or can you transfer

28:15.520 --> 28:23.280
learn specific layers or architectural subsets of a network? That's a good question. I think

28:23.280 --> 28:29.040
transfer learning is still very much an open problem to claim anybody's found like a full solution

28:29.040 --> 28:32.960
to it. There's definitely been some progress and people have done very interesting things.

28:32.960 --> 28:39.760
So for example, one type of transfer that's been very successful is training on ImageNet

28:39.760 --> 28:44.720
and then fine tuning on a new data set. So this would be for computer vision. You want to do a

28:44.720 --> 28:49.280
good computer vision on a new task where you have a small amount of data. You first train on ImageNet,

28:49.280 --> 28:55.120
which is a data set with many, many labeled images, a thousand categories. And it turns out if

28:55.120 --> 29:00.800
you train to be good at recognizing those thousand categories, the later layers of this deep network

29:00.800 --> 29:04.320
contain features that are quite good. In fact, the entire network contains information that's

29:04.320 --> 29:10.160
quite good to then reuse to train another data set. Still a vision data set of course, but one

29:10.160 --> 29:15.600
that might not have as many labels, it might have completely different categories. So that's one

29:15.600 --> 29:21.600
example. The idea being that the training on ImageNet teaches your network, things like edges

29:21.600 --> 29:26.320
and textures and things like that that are transferable to other vision related tasks.

29:26.320 --> 29:32.960
Exactly. And so I think some of the most exciting work in terms of transfer has actually been

29:32.960 --> 29:40.160
inspired by those results. And it falls under the category of a few shot learning. And the idea

29:40.160 --> 29:45.120
there is that at training time, you might see a lot of data that you can do all kinds of things with.

29:45.120 --> 29:51.360
But then at test time, you'll get to see new data that has different categories. And you're

29:51.360 --> 29:57.120
supposed to learn very quickly what to do for those new categories. For example, a standard

29:57.120 --> 30:00.960
thing could be maybe have ImageNet, which is a thousand categories that training time you only

30:00.960 --> 30:06.240
get to train on 800 categories. At test time, the new categories get presented to you. I need to

30:06.240 --> 30:11.360
adapt very, very quickly. And so that's the few shot learning setup. People do for other data sets

30:11.360 --> 30:17.200
like Omnigloth, which is a handwritten character data set. And so some of the ideas there essentially,

30:17.200 --> 30:23.200
one example that we worked on recently, this was led by Chelsea Finnipi, as she's doing that

30:23.200 --> 30:29.360
Berkeley was to see if it's possible to also apply this to reinforcement learning. So people had

30:29.360 --> 30:34.400
had some success in supervised learning, but can you in reinforcement learning train in training

30:34.400 --> 30:41.280
environments, but then somehow reuse what you learn there in new test environments that are

30:41.280 --> 30:49.360
related. So maybe you learn to control a and simulated robot to do certain things, like maybe

30:49.360 --> 30:53.920
running at certain speeds. But then at test time, it needs to run at a very different speed.

30:53.920 --> 30:58.160
And the question is how quickly can it learn to run at that different speed? Can it do it with a

30:58.160 --> 31:02.880
very small number of policy-gradient updates? And indeed, the experience found that it is possible

31:02.880 --> 31:09.120
with a very small number of updates to adjust to a new task at test time compared to typical RL

31:09.120 --> 31:15.200
if you were to learn from scratch would need a very large number of iterations.

31:15.200 --> 31:21.600
Which brings us back to policy-gradients and Q learning. Yes, absolutely. So let's start with

31:21.600 --> 31:28.240
Q learning. So what's the idea behind Q learning? What's a Q value? A Q value is the Q value of a

31:28.240 --> 31:34.720
current state and current action is how much reward you expect to get when you start in that

31:34.720 --> 31:42.400
current state, take that action and from then onwards act optimally. So if you have the Q

31:42.400 --> 31:46.880
values, it's very easy to decide what to do. You look at the Q values in your current state and

31:46.880 --> 31:51.440
you just choose the action that maximizes the Q value in that current state and that's the best

31:51.440 --> 31:56.560
action to take. Now of course, the tricky part is how do you find your Q values? You need to

31:56.560 --> 32:01.600
somehow know what they are for every state that might be in the world. And so there's a lot of

32:01.600 --> 32:07.280
states. And so building just a table that tells you whatever Q value is is not practical unless

32:07.280 --> 32:13.200
your environment is really, really tiny and not of practical interest. So for realist

32:13.200 --> 32:19.040
environments or even just for larger, not that realist environment, like just some simple video

32:19.040 --> 32:23.840
games, typically the Q values are represented by deep neural nets this day. And so input to the

32:23.840 --> 32:28.320
neural net would be, let's say pixel values, what's currently on the screen and output would be

32:28.320 --> 32:33.520
for each action that you can take the Q value of that action for the current screen configuration.

32:34.080 --> 32:38.400
And so for the initial Atari results from DeepMind, they trained a Q network.

32:39.840 --> 32:44.800
And from its own tron error, this network was trained to take on the right values or good enough

32:44.800 --> 32:50.560
values such that if you use the trained Q network to choose your actions, you actually perform quite

32:50.560 --> 32:56.480
well in the game. The intuition on how you train this Q network is as follows. You say, okay,

32:56.480 --> 33:02.000
what does it mean to be a Q value? It's the value of current state and action, okay? That is

33:02.000 --> 33:07.600
actually equal to the reward you're going to get in the first transition that you encounter from

33:07.600 --> 33:14.000
current time to next time, plus the Q value at the next state that you landed. Because

33:14.640 --> 33:19.040
how well you do from current state is how well you do in the first step, plus then how well you do

33:19.040 --> 33:24.000
in all future steps. And so that, that then gives you actually a self consistent set of equations.

33:24.000 --> 33:29.040
That says Q value equals reward plus Q value at next state. And so what Q learning algorithms

33:29.040 --> 33:35.120
do is they solve this self consistent set of equations. And the way to solve it is by collecting

33:35.120 --> 33:40.080
a lot of data from running trials in the environment. And on the states and actions that are

33:40.080 --> 33:47.040
experienced, trying to enforce the self consistency of that set of equations. And once you've

33:47.040 --> 33:52.480
enforced the self consistency, you end up with, if it's fully enforcing up with the correct Q

33:52.480 --> 33:57.120
values and that will prescribe your actions. And also tell you how good it is to be in a certain

33:57.120 --> 34:02.960
state and take a certain action. And practice the one before we made self consistent. It's a very

34:02.960 --> 34:07.680
large set of equations. And it's not easy to make that self consistency true. But stochastic

34:07.680 --> 34:13.840
gradient taking updates will get you closer to self consistency. And as you get closer, acting

34:13.840 --> 34:21.200
based on those Q values will will typically lead to pretty good behavior. And so our policy

34:21.200 --> 34:27.280
gradients, an enhancement of that basic technique, or is it a different technique altogether?

34:27.280 --> 34:32.320
So it's very interesting. So policy gradients are a different family of techniques. But I'll get

34:32.320 --> 34:36.560
back to how they might actually be quite similar. Most I've explained it because actually there

34:36.560 --> 34:39.760
is some recent work showing that there might be stronger connections than people might have

34:39.760 --> 34:46.400
initially thought. Okay. But so what policy gradients do in some senses is much simpler to explain.

34:46.400 --> 34:51.520
What's a policy? A policy is in these days, it's a function. And these days it's typically a deep

34:51.520 --> 34:56.160
neural net. So it's a deep neural net that takes in, let's say current pixels and outputs the

34:56.160 --> 35:01.600
action that you're going to take. Or a distribution over actions is what often is used. So input,

35:01.600 --> 35:06.720
current situation, output distribution over actions. So once you have a policy, you can follow that

35:06.720 --> 35:11.600
policy by sampling from that distribution over actions. And then you're executing the policy.

35:11.600 --> 35:16.960
Of course, most policies are not good policies. So you need to do some work to find a good policy.

35:16.960 --> 35:22.720
And so a policy grand approach is a very, use a very simple idea. Let's say you have a current

35:22.720 --> 35:27.920
policy, you execute a few times, you see what happens, you see how much reward you got,

35:27.920 --> 35:32.240
and now you can use, you can perturb your policy. You can say, let me use a slightly different

35:32.240 --> 35:38.480
policy and execute again and see what happens. Now you can compare how well did my original

35:38.480 --> 35:43.920
policy do, how well did my new policy do in terms of how much reward they collected. Which

35:43.920 --> 35:47.920
ever is the better one you retain and you repeat, that would be a simple way to do it.

35:48.960 --> 35:55.040
And so that way you're gradually improving your policy as you iterate in your algorithm.

35:55.040 --> 36:00.960
And so you said a policy is for all intents and purposes a deep neural network. When we

36:00.960 --> 36:05.840
perturb the policy, are we changing the weights or are we randomly changing the weights,

36:05.840 --> 36:09.120
so what does that mean specifically? So there's different ways, that's a really good

36:09.120 --> 36:15.120
question. Different strategies to do policy perturbation. So one way can perturb the policies by

36:15.120 --> 36:21.440
just randomly preserving the weights in the neural net. Another way you can get variation to get

36:21.440 --> 36:26.080
gradient signal from is by ensuring that your distribution, that you have a distribution of

36:26.080 --> 36:30.560
our actions. So a non-deterministic policy. And then what happens is you sample your actions

36:30.560 --> 36:36.400
from that distribution. And so every rollout will lead to different behavior. And it turns out

36:36.400 --> 36:39.920
that you can compute policy gradients from that too using something called the likelihood ratio

36:39.920 --> 36:45.680
of policy gradient. And that effectively pieces apart and then makes actions that led to better

36:45.680 --> 36:51.840
reward more likely and actions that led to less reward less likely and it doesn't object to your

36:51.840 --> 36:57.040
policy that way. You can also do something like finite differences where you go per coordinate.

36:57.040 --> 37:01.440
Of course, in high dimensions, that's a little tricky. You could go per coordinate, increase the

37:01.440 --> 37:05.600
value of that coordinate. Cornet here is a single weight in your neural net. For each thing,

37:05.600 --> 37:10.160
each weight in your neural net, you can increase the weight, decrease it, and just use like a high

37:10.160 --> 37:15.600
school type derivative calculation. With finite difference ways, say I increase the x value,

37:15.600 --> 37:22.000
decrease the x value. Now look at f of x plus minus f of x minus divided by the size of the

37:22.000 --> 37:28.320
perturbation and that gives me my derivative. That's probably a baseline that you could check,

37:28.320 --> 37:33.760
but that would be somewhat sampling efficient if you had a high dimensional policy.

37:34.560 --> 37:39.040
Okay, so the summary on Q learning and policy gradients is

37:40.240 --> 37:47.040
Q learning, you've got a neural network that's representing essentially a sequence of consistent

37:47.040 --> 37:54.320
equations and you solve that. You solve for your model by enforcing that consistency and

37:54.320 --> 38:03.840
coming up with a set of weights that kind of maximizes the score, if you will. Maximize the

38:03.840 --> 38:08.800
consistency. Okay, okay. And then with policy gradients, you've got this policy that is,

38:09.600 --> 38:13.360
well, explain for me actually the relationship between the policy neural network and the

38:13.360 --> 38:18.160
broader neural network is one subset of the other or they just two totally different things.

38:18.160 --> 38:23.600
So in policy gradients, the policy network is the only network that you might use and it just

38:23.600 --> 38:29.280
represents a policy directly. Okay. Now what's interesting is that some recent words both from

38:29.280 --> 38:34.320
DeepMind and from OpenAI. Here, John Schillman was lead author here at OpenAI.

38:34.320 --> 38:39.440
Show that there's a very close connection between policy gradients and Q learning.

38:39.440 --> 38:45.520
Ask Q learning is typically used. So if you look at how Q learning is typically used in practice,

38:45.520 --> 38:49.760
look at the details because there's there's various incarnations you can have of Q and with this

38:49.760 --> 38:55.280
typical way of using it in practice is that you collect data based on what your current

38:55.280 --> 39:01.200
Q function prescribes. Once you do that, it becomes a lot closer to a policy gradement because

39:01.200 --> 39:05.040
the policy gradient method also says I have a current policy at collect data and I improved the

39:05.040 --> 39:09.280
policy based on that. If you have a Q function, a current Q function which of course doesn't

39:09.280 --> 39:14.880
satisfy the self-consisting equations yet, we collect data based on that current Q function

39:14.880 --> 39:19.600
and then try to get the self-consistency to be more satisfied. It turns out that that update

39:20.160 --> 39:25.360
is extremely similar and under some additional specific assumptions that are quite practical and

39:25.360 --> 39:32.880
people often have algorithms that match those, the two become unified and Q learning and policy

39:32.880 --> 39:39.440
gradients end up using the same update equations as each other which is very intriguing and actually

39:39.440 --> 39:44.080
explains in many ways some of the mystique that has been behind Q learning in the sense that

39:44.800 --> 39:49.920
if you look at Q learning the self-consistency set of equations, it turns out that

39:51.520 --> 39:58.160
what ends up being found doesn't really fully satisfy the self-consistency and also the values

39:58.160 --> 40:03.280
that you find running Q learning, which are supposed to be how much reward you'll get going

40:03.280 --> 40:10.160
forward from that state and action. The values are often way way off. They're not precise at all

40:10.160 --> 40:17.120
and nevertheless somehow this Q learning algorithm leads to a good policy and so this connection

40:17.120 --> 40:25.520
between the two that John Shulman figured out essentially shows why it might be that Q learning

40:25.520 --> 40:30.800
leads to good policies, namely that it's secretly running something like a policy-grant algorithm

40:30.800 --> 40:37.760
underneath or something very close to it. Is the idea then when you talked about collecting

40:37.760 --> 40:45.120
additional data is that in both Q learning and policy gradients you're training some agent

40:45.120 --> 40:52.000
to kind of navigate an environment and the agent in either case tends to perform behaviors

40:52.000 --> 40:58.240
in a rough neighborhood of what it has previously seen and done and that's kind of the cause

40:58.240 --> 41:06.400
of the one approximating the other. That's exactly right. The space is so big that the learning tends

41:06.400 --> 41:13.840
to focus on where you currently would be going with what you have learned so far and once you do

41:13.840 --> 41:18.320
that in Q learning because it's natural to restrict attention to that because why try to learn

41:18.320 --> 41:23.280
about everything there's so much that you might be busy for too long. Once you mostly pay attention

41:23.280 --> 41:28.640
to what your current Q function prescribes you start being extremely similar to a policy-grating

41:28.640 --> 41:34.160
method. You've mentioned pixels a few times in your descriptions is reinforcement learning only

41:34.160 --> 41:41.440
applied to applications that have some vision component? So reinforcement learning can take in

41:41.440 --> 41:48.800
any type of sensory input. So raw sensory information could be pixels but it could be something else

41:48.800 --> 41:54.720
in robotics. We also take in joint angles and joint velocities because while the motors that

41:54.720 --> 41:59.680
are part of the robot know those values and they're very informative about what situation the robot

41:59.680 --> 42:07.120
is currently in. So that's being fed in too. Looking ahead things that are very interesting to me are

42:07.120 --> 42:13.280
things like tactile sensing. If you have a robot hand if you can have tactile sensing on that robot hand

42:14.320 --> 42:20.000
that should amplify what this robot hand is capable of doing but now how do you process that

42:20.000 --> 42:26.240
information? How do you turn this raw sensory tactile information into an understanding of how you're

42:26.240 --> 42:29.680
holding the object? Well there are some object properties of the object that you're holding and

42:29.680 --> 42:36.320
so forth. Those are challenging problems but also the kind of problems that I suspect deep learning

42:36.320 --> 42:43.600
could help solve because it is the same flavor of problem as the image processing type problems.

42:43.600 --> 42:48.160
You have high dimensional sensory information. The information is intrinsically in there but it

42:48.160 --> 42:54.080
just somehow needs to be teased apart from these raw sensory inputs and so it should be learnable

42:54.080 --> 42:59.680
if we set up the data collection for that if we have some supervision or if there's some reward

42:59.680 --> 43:07.200
related to if you had a reward such that to be successful on that reward tactile would matter

43:07.200 --> 43:12.560
then presumably that robot hand would learn how to process tactile information because that would be

43:12.560 --> 43:20.960
the way to maximize reward. So we've talked about games we've talked about robots

43:22.000 --> 43:27.920
focusing in on the industrial applications of reinforcement learning in and around robots and

43:27.920 --> 43:35.680
other other use cases that would appear within an enterprise context. What use cases that we

43:35.680 --> 43:41.360
seen success with and where are we kind of getting close? Okay that that's a great question that

43:41.920 --> 43:48.400
actually first I think a lot of reinforcement learning right now is happening in still research

43:48.400 --> 43:54.320
environments. So if you look at a lot of the big success tours of reinforcement learning and

43:54.320 --> 43:59.440
that are very well known learning to play Atari games. Learning to play go which by the way there's

43:59.440 --> 44:04.320
a combination of imitation and reinforcement learning in that case. Learning simulated locomotion

44:04.320 --> 44:11.760
skills that was some of the work John Schumer and Sergey Leven did at at at Berkeley or learning

44:11.760 --> 44:18.160
robot motor control but still for very simple tasks was Sergey Leven and Chelsea Finna Berkeley

44:18.160 --> 44:23.840
how how to assemble let's say toys all of those are are still a little removed from what you

44:23.840 --> 44:30.080
would think of as real world deployment. I think there are a few reasons for that. I think one reason

44:30.080 --> 44:37.120
is that these algorithms are only recently have become like really something that people are

44:37.120 --> 44:42.800
able to get to work five years ago people didn't think those things are possible but now they're

44:42.800 --> 44:48.880
starting to work and it takes some time to transition that into applications especially since for

44:48.880 --> 44:54.720
now it still requires a little bit of well I would say a substantial amount of reinforcement

44:54.720 --> 45:00.960
learning expertise to make sure these things work out and so the number of people who can put this

45:00.960 --> 45:06.320
to use is still a little limited and a lot of those people are actually excited about expanding

45:06.320 --> 45:11.600
the research frontier rather than necessarily putting it into application so there's maybe a slight

45:11.600 --> 45:18.080
shortage of reinforcement learning experts that are taking their expertise then and and try to

45:18.080 --> 45:22.320
deploy them in the real world because the real world has many so man's where it could be applied

45:23.680 --> 45:29.600
anything where you make decisions over time reinforcement learning is going to matter this could be

45:29.600 --> 45:37.280
for your HFAC system this could be for let's say servicing demand in cues where maybe you're

45:37.280 --> 45:41.520
providing support longer run once language understanding is better this could be part of

45:41.520 --> 45:47.920
dialogue because the goal and dialogue is not just to spit out a statistically reasonable sentence

45:47.920 --> 45:52.720
and reply to the previous one the goal and dialogue tends to be figuring out what the other person

45:52.720 --> 45:57.840
wants to achieve and helping them achieve what they're trying to achieve and so in that scenario

45:57.840 --> 46:02.480
there is a reinforcement learning problem in terms of maximizing reward is maximizing happiness

46:02.480 --> 46:07.360
of the other side in terms of what they get out of this conversation and so forth so one of the

46:07.360 --> 46:13.360
things we're actually doing in late august is this need to get it with a few other people so also

46:13.360 --> 46:18.880
Vladimir from from deep mind under her path from open AI serving 11 from Berkeley Chelsea Finn

46:18.880 --> 46:24.480
from Berkeley and then John Schillman rocket one and Peter Chen from open AI is organizing a

46:24.480 --> 46:30.960
deep reinforcement learning boot camp in late august the incentive here is that it seems

46:30.960 --> 46:37.040
reinforcement learning is getting ready to be deployed in various application domains in industry

46:37.040 --> 46:44.080
but it will require more experts and so to educate experts to then start you know they they will

46:44.080 --> 46:48.560
see the applications once their experts they'll see the applications and latch onto them and start

46:48.560 --> 46:54.560
deploying things and so the hope is to kind of accelerate that a little bit by having a weekend

46:54.560 --> 47:01.440
a very intense weekend with lectures but at least for about 50% lab sessions where people really

47:01.440 --> 47:07.680
you know get things working in old environments we talked about simulated robots video games and so

47:07.680 --> 47:12.960
forth get the essence down but then with the hope that they can take it back to their companies

47:12.960 --> 47:19.200
or to their research efforts because it's also useful for researchers to be more productive and

47:19.200 --> 47:26.640
get things done with reinforcement learning that sounds great I will get the URL to that from you

47:26.640 --> 47:31.520
and we'll make sure to include it in the show notes it sounds like the summary is that the technology

47:31.520 --> 47:39.200
is ready but there is still a shortage of expertise to help folks build out these applications and

47:40.000 --> 47:46.160
you know start so that we can start to see you know proliferation of success stories out in

47:46.160 --> 47:54.400
the commercial world yeah am I am I say the technology is at the cost of being ready I wouldn't say

47:54.400 --> 48:00.400
it's like very mature it's like it's it's getting there and the first application should become

48:00.400 --> 48:05.200
possible in the future but the technology shouldn't also still be improved a lot over the next few

48:05.200 --> 48:13.120
years and in your experience is reinforcement learning often a an alternative to some other

48:13.120 --> 48:20.320
technique that you know might work or is reinforcement learning kind of the only way to solve

48:20.320 --> 48:25.120
the problems that reinforcement learning is good at and you know is there a general way to

48:25.120 --> 48:30.960
characterize like the benefits or advantages of our own relative to you know some alternative

48:30.960 --> 48:36.320
approaches so the typical starting point when you need to make decisions over time like

48:36.320 --> 48:43.200
it reinforcement learning would be imitation learning because imitation learning is simpler

48:43.200 --> 48:48.800
it's it's like supervised learning you would demonstrate what needs to happen and then you would

48:48.800 --> 48:54.240
try to learn something that matches what you did during the demonstrations now when it's hard to

48:54.240 --> 49:00.320
demonstrate that could be an issue or it could be that it's not too hard to demonstrate but it's

49:00.320 --> 49:06.240
hard to demonstrate at the scale that you need to learn from and that's where reinforcement

49:06.240 --> 49:10.880
can do autonomous data collection and so it might be able to collect a much larger amount of data

49:10.880 --> 49:15.120
than you can get from demonstrations it could also be that you can demonstrate in the format that

49:15.120 --> 49:21.920
you need so maybe you want a robot to do something that maybe load your dishwasher but it's very hard

49:21.920 --> 49:27.120
to make the robot do it you can do it yes by hand yourself but that's not the exact form factor

49:27.120 --> 49:31.040
that's easiest to learn from and it would be the third person imitation again which is

49:31.040 --> 49:36.400
still a hard problem even though some progress has been made so the first shot typically

49:36.400 --> 49:42.880
I would argue is you try and find a way to get imitation in place see how far you can get with that

49:42.880 --> 49:46.800
and then take it from there and typically you'll you'll build reinforcement on top of that

49:46.800 --> 49:51.440
that will fine tune or in some cases imitation will just not be workable because you can get

49:52.080 --> 49:57.680
the demonstrations that that you need yeah I would imagine even if you can kind of demonstrate

49:57.680 --> 50:04.720
the activity in a perfect environment reinforcement learning has still advantages in being somewhat

50:04.720 --> 50:10.480
more robust to the position of the object that you're picking relative to just a pure imitation

50:10.480 --> 50:16.560
learning is that true in general that capability of adaptation once deployed if you let your

50:16.560 --> 50:21.520
reinforcement learner continue to learn once is deployed is very different from what you would get

50:21.520 --> 50:26.320
from a standard imitation learning setup and so yes that's absolutely a big part that also

50:26.320 --> 50:31.920
reminds me of the I think automation provides big opportunities and one I'm personally particularly

50:31.920 --> 50:37.920
interested in is how to get reinforcement learning and imitation learning to start playing big

50:37.920 --> 50:44.800
roles in how automation works for manufacturing to make that a lot more flexible than the way

50:44.800 --> 50:48.240
things tend to be right now which is a lot more rigid than how you need to set things up

50:48.240 --> 50:55.200
and the idea being there that as opposed to a particular you know an agent like a robot

50:55.200 --> 51:00.160
manipulating something we're talking about now sequences of steps are we talking about kind of

51:00.160 --> 51:04.320
the big picture manufacturing are we talking about you know still individual devices

51:04.320 --> 51:11.040
it could be both so at the individual level it could be that instead of you know setting up a robot

51:11.040 --> 51:17.840
for taking multiple days or weeks to set up a robot for an individual manipulation skill you just

51:17.840 --> 51:22.880
demonstrate a few times it learns from that reinforcement learns on top of that and based on that

51:22.880 --> 51:28.320
maybe can be deployed within hours rather than days or weeks in the bigger picture I think it's

51:28.320 --> 51:33.600
very fascinating I mean big it wouldn't be that easy to execute on a small scale with a bigger

51:33.600 --> 51:40.000
scale if you say I want a factory that can take in any raw materials and output any goods and the

51:40.000 --> 51:45.120
system can just adapt itself to whatever the needs are just send your design files and outcomes

51:45.120 --> 51:50.960
you know in goes raw materials outcomes the product I think I mean that I'm not saying that that's

51:50.960 --> 51:59.760
going to happen tomorrow but that kind of flexibility would be really amazing and it seems it requires

51:59.760 --> 52:04.800
a lot of flexibility a lot of adaptation the robots in such a system need to be able to do a wide

52:04.800 --> 52:08.880
range of things you can just set them up for one thing because then something new needs to

52:08.880 --> 52:14.400
manufactures and they need to reconfigure themselves take up a new skill and work together to get the

52:14.400 --> 52:19.360
product made right right well that's a super compelling vision and maybe a great place to

52:19.360 --> 52:24.400
leave off here anything else that you'd like to leave the audience with we'll definitely share

52:24.400 --> 52:27.840
that link to the deep learning bootcamp but anything else you'd like to share

52:27.840 --> 52:34.320
um thanks for listening uh thanks for having me Sam this is a great and fun chat yeah great well

52:34.320 --> 52:41.840
thanks so much Peter I really appreciate it bye bye all right everyone that's our show for today

52:41.840 --> 52:47.920
thanks so much for listening and for your continued support comments and feedback we're excited to

52:47.920 --> 52:53.520
hear what you guys think about this show and the series I'd also like to thank our sponsor

52:53.520 --> 53:02.320
Banzai once again be sure to check out what they're doing at Banz.ai B-O-M-S.ai another reminder

53:02.320 --> 53:08.160
there are less than two weeks left into the O'Reilly AI conference in New York City and I just saw

53:08.160 --> 53:14.720
an email this morning that prices go up today if you'd like to attend you can save 20% on registration

53:14.720 --> 53:21.600
using our discount code which is PC Twimble PCT W-I-M-L we'll link to the registration page

53:21.600 --> 53:27.200
in the show notes I'd love to meet up with listeners at the conference and as I mentioned last time

53:27.200 --> 53:31.840
I'm planning a meetup during the event I'll share details as soon as they've been ironed up

53:32.880 --> 53:38.160
the notes for this episode can be found at Twimble AI dot com slash talk slash 28

53:38.880 --> 53:45.280
for more information on industrial AI my report or the industrial AI podcast series visit

53:45.280 --> 53:52.080
Twimble AI dot com slash industrial AI as always remember to post your favorite quote or take away

53:52.080 --> 53:57.840
from this episode or any other and we'll send you a laptop sticker you can post them as comments

53:57.840 --> 54:05.120
to the show notes page via Twitter at Twimble AI or via our Facebook page once again thanks so

54:05.120 --> 54:15.120
much for listening and catch you next time

