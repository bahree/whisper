1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,120
I'm your host Sam Charrington.

4
00:00:32,120 --> 00:00:38,000
Today we are joined by Andrew Trask, PhD student at the University of Oxford and leader of

5
00:00:38,000 --> 00:00:40,320
the Open Mind project.

6
00:00:40,320 --> 00:00:45,440
Open Mind is an open source community focused on researching, developing and promoting tools

7
00:00:45,440 --> 00:00:51,840
for secure, privacy preserving, value aligned, artificial intelligence.

8
00:00:51,840 --> 00:00:56,760
Andrew and I caught up back at Noreps to dig into why Open Mind is important and to explore

9
00:00:56,760 --> 00:01:03,160
some of the basic research and technologies supporting private decentralized data science.

10
00:01:03,160 --> 00:01:09,480
We touch on ideas such as differential privacy, secure, multi-party computation, and how

11
00:01:09,480 --> 00:01:14,360
these come into play and for example federated learning.

12
00:01:14,360 --> 00:01:19,240
Before we jump in, I'd like to join Pegasystems this episode's sponsor and inviting you to

13
00:01:19,240 --> 00:01:25,680
meet me at the MGM Grand in Las Vegas from June 2nd to 5th at Pegaworld, the company's

14
00:01:25,680 --> 00:01:29,040
annual digital transformation conference.

15
00:01:29,040 --> 00:01:34,240
Pegasystems puts AI at the center of its customer engagement software so that every customer

16
00:01:34,240 --> 00:01:38,720
touchpoint on every channel is optimized in real time.

17
00:01:38,720 --> 00:01:43,800
So the customers find each interaction relevant and timely, whether a sales call, a marketing

18
00:01:43,800 --> 00:01:46,520
campaign, or a customer service chat.

19
00:01:46,520 --> 00:01:51,760
At Pegaworld, you'll hear great stories of AI applied to the customer experience at

20
00:01:51,760 --> 00:01:54,360
real Pegacustomers.

21
00:01:54,360 --> 00:01:59,600
The event is a great way to learn from a who's who of the Fortune 500, and of course, I'll

22
00:01:59,600 --> 00:02:02,160
be there speaking as well.

23
00:02:02,160 --> 00:02:08,800
To register, visit Pegaworld.com and use the promo code TWIMIL-19 when you sign up for

24
00:02:08,800 --> 00:02:10,120
$200 off.

25
00:02:10,120 --> 00:02:13,600
Again, that's TWIMIL-19, it's as easy as that.

26
00:02:13,600 --> 00:02:17,120
Hope to see you there, and now on to the show.

27
00:02:17,120 --> 00:02:20,840
All right, everyone.

28
00:02:20,840 --> 00:02:25,600
I am here in Montreal at the NURPS conference, and I am with Andrew Trask.

29
00:02:25,600 --> 00:02:32,000
Andrew is a PhD student at Oxford, as well as leader of the Open Source Open Mind project.

30
00:02:32,000 --> 00:02:34,600
Andrew, welcome to this week in Machine Learning and AI.

31
00:02:34,600 --> 00:02:35,600
Awesome.

32
00:02:35,600 --> 00:02:36,600
Thank you.

33
00:02:36,600 --> 00:02:37,600
Great to be here.

34
00:02:37,600 --> 00:02:38,600
Awesome.

35
00:02:38,600 --> 00:02:39,920
Why don't we get started by having you tell us a little bit about your background?

36
00:02:39,920 --> 00:02:46,120
How did you get involved in machine learning and privacy in the intersection of those two?

37
00:02:46,120 --> 00:02:51,680
Yeah, so I went to undergrad at a liberal arts college in Tennessee called Belmont University.

38
00:02:51,680 --> 00:02:58,960
And while I was there, I was originally a music student, and I stumbled into a CS course

39
00:02:58,960 --> 00:03:04,000
dabbled around, and there was an AI course, and that got me really into machine learning.

40
00:03:04,000 --> 00:03:09,080
So as soon as I, there's just really great professor named Dr. Hooper, as many great

41
00:03:09,080 --> 00:03:10,080
stories start.

42
00:03:10,080 --> 00:03:14,720
I really compelling professor, teaching really exciting concepts, and I trained my first

43
00:03:14,720 --> 00:03:19,080
neural nets, and got them to converge, and got a paper into undergraduate conference.

44
00:03:19,080 --> 00:03:23,880
And at that point, I was pretty committed on the machine learning front.

45
00:03:23,880 --> 00:03:31,200
After that, I joined a local company called Digital Reasoning that does deep learning and

46
00:03:31,200 --> 00:03:34,080
AI at scale on private data sets.

47
00:03:34,080 --> 00:03:35,080
Very natural, actually, right?

48
00:03:35,080 --> 00:03:36,080
Yeah, they're natural.

49
00:03:36,080 --> 00:03:38,080
Yeah, so Belmont is also a natural.

50
00:03:38,080 --> 00:03:39,080
Okay.

51
00:03:39,080 --> 00:03:40,080
Got it.

52
00:03:40,080 --> 00:03:41,080
Yeah.

53
00:03:41,080 --> 00:03:45,080
And that's when I really got exposed to the power of doing machine learning on data that

54
00:03:45,080 --> 00:03:46,080
you don't have access to.

55
00:03:46,080 --> 00:03:52,080
So I mean, they work with a lot of government agencies, like the intelligence coming across

56
00:03:52,080 --> 00:03:53,080
them.

57
00:03:53,080 --> 00:03:54,080
Oh, that's awesome.

58
00:03:54,080 --> 00:03:56,080
I'll throw out that story.

59
00:03:56,080 --> 00:04:00,000
I got the dabble in research as well as product management there, and really just came to

60
00:04:00,000 --> 00:04:04,600
appreciate how challenging it is to do machine learning and deep learning on unseen data.

61
00:04:04,600 --> 00:04:10,800
So when I left to do a PhD, I had already a very good appreciation for that, and also

62
00:04:10,800 --> 00:04:15,640
some sort of high-level understanding of the challenges of that environment.

63
00:04:15,640 --> 00:04:22,440
And then, during my first year at Oxford, the first year of PhD is often very exploratory.

64
00:04:22,440 --> 00:04:27,000
And one of the first things that came across was homeomorphic encryption, which just to

65
00:04:27,000 --> 00:04:28,520
me was absolutely magical.

66
00:04:28,520 --> 00:04:31,280
I couldn't believe that you could do computation on encrypted data.

67
00:04:31,280 --> 00:04:36,360
And I wrote a blog post, sort of just trying to jerry rig some homeomorphic encryption

68
00:04:36,360 --> 00:04:39,400
code that I found with training and neural net.

69
00:04:39,400 --> 00:04:43,320
And I got a tremendous amount of positive feedback from it, ended up chatting with some

70
00:04:43,320 --> 00:04:45,520
folks at the Future Humanity Institute in Oxford.

71
00:04:45,520 --> 00:04:50,120
So I don't know if you know Nick Bostrom and kind of the safety work that they do.

72
00:04:50,120 --> 00:04:54,000
And it just became really clear that there was a really exciting research opportunity here

73
00:04:54,000 --> 00:04:59,120
that one was underserved, so there weren't many people working on it, and secondarily,

74
00:04:59,120 --> 00:05:04,000
and perhaps more importantly, that there was good reason to believe we can make a lot

75
00:05:04,000 --> 00:05:08,520
of progress in a relatively short period of time on a topic with pretty significant

76
00:05:08,520 --> 00:05:09,520
social gains.

77
00:05:09,520 --> 00:05:12,080
And so that at the beginning of PhD is a no-brainer.

78
00:05:12,080 --> 00:05:15,360
Like you actually want to jump on those kind of opportunities.

79
00:05:15,360 --> 00:05:18,640
And since then, what I've discovered is that tons of great workers going on in sort

80
00:05:18,640 --> 00:05:23,000
of the field of cryptography, tons of great workers going on in the field of statistics,

81
00:05:23,000 --> 00:05:24,920
and of course in machine learning and deep learning.

82
00:05:24,920 --> 00:05:29,960
But these three communities, they don't necessarily talk as much as they should.

83
00:05:29,960 --> 00:05:34,760
And so by simply trying to become as familiar as fast as possible with sort of the core

84
00:05:34,760 --> 00:05:40,360
primitives of these three different fields, and by just trying to combine them in ways

85
00:05:40,360 --> 00:05:44,760
that solve important use cases, I think we've already been able to make quite a bit of progress

86
00:05:44,760 --> 00:05:47,000
and expect to be so in the future.

87
00:05:47,000 --> 00:05:48,480
So that's kind of where I'm at.

88
00:05:48,480 --> 00:05:49,480
Nice, nice.

89
00:05:49,480 --> 00:05:56,240
Here, some kind of core concepts, maybe from the cryptography side, that it makes sense

90
00:05:56,240 --> 00:06:02,200
to kind of talk about, like mention homomorphic encryption that are fundamental to all the

91
00:06:02,200 --> 00:06:07,000
rest of the stuff you're doing, or it's better to go top down.

92
00:06:07,000 --> 00:06:12,160
Yeah, so I mean, the three core concepts that we deal with, two of which are from cryptography,

93
00:06:12,160 --> 00:06:15,200
are differential privacy and secure multi-party computation.

94
00:06:15,200 --> 00:06:19,520
So and then there's federated learning from the machine learning side.

95
00:06:19,520 --> 00:06:23,640
So those three things kind of make up what I think is the future of privacy-reserving

96
00:06:23,640 --> 00:06:25,400
machine learning.

97
00:06:25,400 --> 00:06:30,680
So differential privacy is concerned with, I suppose your viewers already know this,

98
00:06:30,680 --> 00:06:35,200
in the context of machine learning, differential privacy is focused on allowing models to sort

99
00:06:35,200 --> 00:06:39,720
of learn from a training data set without them accidentally learning information we don't

100
00:06:39,720 --> 00:06:40,720
want them to learn.

101
00:06:40,720 --> 00:06:42,720
Like, you know, private training data itself, for example.

102
00:06:42,720 --> 00:06:43,720
Yeah, yeah, exactly.

103
00:06:43,720 --> 00:06:46,600
Or it's any specific features about a specific person, right?

104
00:06:46,600 --> 00:06:52,000
So it might, a good example might be, you want a model to learn to identify cancer in

105
00:06:52,000 --> 00:06:59,760
radiology scans without it memorizing which individual person has cancer, right?

106
00:06:59,760 --> 00:07:02,120
And so yeah, differential privacy is super important.

107
00:07:02,120 --> 00:07:05,120
That one is already, there's already good community form between that and the machine

108
00:07:05,120 --> 00:07:06,120
learning community.

109
00:07:06,120 --> 00:07:08,840
It's really exciting to see that develop.

110
00:07:08,840 --> 00:07:13,680
Homomorphic encryption and secure multi-party computation are a little less known, in particular

111
00:07:13,680 --> 00:07:19,040
secure multi-party, I'll just say secure NPC from now on, for short, because it's such

112
00:07:19,040 --> 00:07:21,040
a wordy thing.

113
00:07:21,040 --> 00:07:24,400
Homomorphic encryption, I think, is more intuitive for people because it's sort of a, it's

114
00:07:24,400 --> 00:07:27,080
typically laid out in the form of a standard encryption.

115
00:07:27,080 --> 00:07:31,280
So you have a public key and a private key, and you'll use the private key to encrypt

116
00:07:31,280 --> 00:07:32,280
something, right?

117
00:07:32,280 --> 00:07:35,680
So I can encrypt the number five, and then I can give it to you.

118
00:07:35,680 --> 00:07:39,280
And while it's in its encrypted state, you know, while you have it, and I can't see it,

119
00:07:39,280 --> 00:07:41,520
you can do arithmetic with that number, right?

120
00:07:41,520 --> 00:07:43,920
And you could do computation with that number.

121
00:07:43,920 --> 00:07:47,360
And then you can give the result back to me, still encrypted, and I can decrypt the result.

122
00:07:47,360 --> 00:07:51,280
And so that's interesting from machine learning standpoint, because I can take, say, all

123
00:07:51,280 --> 00:07:54,960
the parameters of my model, so a model, you know, a neural net, for example, is just a big

124
00:07:54,960 --> 00:07:56,120
collection of numbers.

125
00:07:56,120 --> 00:07:57,720
I can encrypt all of them.

126
00:07:57,720 --> 00:08:01,600
I can give them to you, and then you can do prediction or training, or anything else

127
00:08:01,600 --> 00:08:04,320
that you'd like to do with a machine learning model.

128
00:08:04,320 --> 00:08:08,360
And then, so if you do training, for example, you can give me back the resulting model.

129
00:08:08,360 --> 00:08:12,760
And you were able to, it's sort of a one-way information flow, where you're able to make

130
00:08:12,760 --> 00:08:14,760
the model smarter, but you're not actually able to use it.

131
00:08:14,760 --> 00:08:19,040
So there's lots of interesting commercial use cases that protect privacy in that context.

132
00:08:19,040 --> 00:08:20,680
The one, and some people have heard about this.

133
00:08:20,680 --> 00:08:24,080
So homework encryption is sort of mildly well-known.

134
00:08:24,080 --> 00:08:26,840
But the problem is, is that it's very, very slow.

135
00:08:26,840 --> 00:08:32,280
So I mean, it's several, meaning more than five orders of magnitude slower than typical,

136
00:08:32,280 --> 00:08:35,280
what we call plain text computation, what we do normally.

137
00:08:35,280 --> 00:08:38,600
And this is where NPC really starts to become interesting.

138
00:08:38,600 --> 00:08:43,680
So there's, security NPC is a slightly different setup.

139
00:08:43,680 --> 00:08:47,960
Instead of trying to encrypt a number, we actually split it into what's called shares.

140
00:08:47,960 --> 00:08:51,600
So let's say we want to encrypt the number five.

141
00:08:51,600 --> 00:08:55,280
I can take the number five, I can split it into two shares, we'll say a two and a three,

142
00:08:55,280 --> 00:08:56,280
right?

143
00:08:56,280 --> 00:08:58,320
And I can give you the three, and I can hang on to the two.

144
00:08:58,320 --> 00:09:01,360
So the three is your share, the two is my share.

145
00:09:01,360 --> 00:09:05,720
Now we could restore this number five by just adding these two shares together, if that's

146
00:09:05,720 --> 00:09:06,720
an instance.

147
00:09:06,720 --> 00:09:11,600
But the interesting thing about security NPC is that while it's in this state, neither of

148
00:09:11,600 --> 00:09:14,920
us know what the number is, it's hidden between us, right?

149
00:09:14,920 --> 00:09:18,520
And you may have no idea that this was a five.

150
00:09:18,520 --> 00:09:20,440
But we can compute over it.

151
00:09:20,440 --> 00:09:25,240
So let's say we want it to multiply at times two, like if you multiply your share at times

152
00:09:25,240 --> 00:09:29,160
two, I multiply my share at times two, then this hidden value that's between us was

153
00:09:29,160 --> 00:09:31,160
also multiplied by two.

154
00:09:31,160 --> 00:09:33,520
And this is better than homeomorphic encryption for two reasons.

155
00:09:33,520 --> 00:09:35,640
One, it's much faster, typically.

156
00:09:35,640 --> 00:09:40,320
So there's a recent paper show and you could train a deep cognet that's only 20 times slower

157
00:09:40,320 --> 00:09:43,760
than the normal, which I know that sounds mildly annoying to anyone who's training deep

158
00:09:43,760 --> 00:09:48,680
known that's, but it's totally different than like 10,000 or 100,000 times slower, right?

159
00:09:48,680 --> 00:09:57,400
And is it faster only in aggregate or for each individual participant?

160
00:09:57,400 --> 00:10:02,880
So it is, in this specific setup, it's two or three party computations.

161
00:10:02,880 --> 00:10:09,400
So we've split this deep neural net, all the parameters in it into shares that three

162
00:10:09,400 --> 00:10:11,240
parties own.

163
00:10:11,240 --> 00:10:13,200
And in that case, it's faster in aggregate.

164
00:10:13,200 --> 00:10:17,280
So we typically try to measure it in the context of a use case because different functions.

165
00:10:17,280 --> 00:10:18,680
So you imagine this is a protocol, right?

166
00:10:18,680 --> 00:10:24,240
And I described how to multiply this hidden value five by two, but addition is different

167
00:10:24,240 --> 00:10:27,680
from multiplication, which is different from comparison operators, which is different

168
00:10:27,680 --> 00:10:31,920
than how you bootstrap sine and ray-lew and like, sigmoid and all these kinds of things.

169
00:10:31,920 --> 00:10:37,000
So we typically try to look at the end use case and just sort of give a high level empirical

170
00:10:37,000 --> 00:10:39,560
results for what the slowdown is.

171
00:10:39,560 --> 00:10:43,480
But the interesting thing is, one more on that, if it's based on homeomorphic encryption,

172
00:10:43,480 --> 00:10:45,720
how is it end up being faster than homeomorphic encryption?

173
00:10:45,720 --> 00:10:50,560
Oh, so it's not always based on homeomorphic encryption.

174
00:10:50,560 --> 00:10:51,960
Sometimes there's a homeomorphic encryption component.

175
00:10:51,960 --> 00:10:54,360
They kind of steal ideas from each other.

176
00:10:54,360 --> 00:10:58,280
But the cool thing is that this is not a form of encryption.

177
00:10:58,280 --> 00:11:02,560
It actually, it's a protocol for sharing.

178
00:11:02,560 --> 00:11:05,640
So encryption is not necessarily involved.

179
00:11:05,640 --> 00:11:06,640
It's more...

180
00:11:06,640 --> 00:11:07,640
Yeah.

181
00:11:07,640 --> 00:11:11,600
So it's funny because it has an API that's very similar to encryption to the extent that

182
00:11:11,600 --> 00:11:16,200
I can take a value and I can make it so that a group of people can't see it, right?

183
00:11:16,200 --> 00:11:17,680
But we can still compute over it.

184
00:11:17,680 --> 00:11:21,600
So to that extent, it can do everything that homeomorphic encryption can do.

185
00:11:21,600 --> 00:11:24,000
But it has two specific properties that are different.

186
00:11:24,000 --> 00:11:25,000
Well, so three.

187
00:11:25,000 --> 00:11:30,360
One, it's a lot faster because you basically are exchanging heavy compute, typically,

188
00:11:30,360 --> 00:11:33,600
which is homeomorphic encryption for a more network overhead.

189
00:11:33,600 --> 00:11:36,680
So whereas with homeomorphic encryption, I could just send you an encrypted asset and

190
00:11:36,680 --> 00:11:38,520
you can do things with it.

191
00:11:38,520 --> 00:11:40,880
With NPC, we have to be online the whole time.

192
00:11:40,880 --> 00:11:45,800
It's a protocol for how we can exchange messages in a way to compute functions on hidden

193
00:11:45,800 --> 00:11:47,680
data.

194
00:11:47,680 --> 00:11:49,920
And there's two properties that we really like.

195
00:11:49,920 --> 00:11:51,720
One, it gets this privacy property.

196
00:11:51,720 --> 00:11:55,160
But secondarily, and this is different from homeomorphic encryption, everyone has a private

197
00:11:55,160 --> 00:11:56,480
key.

198
00:11:56,480 --> 00:11:57,480
And this creates...

199
00:11:57,480 --> 00:12:02,000
It's a lot more like shared ownership of a data structure instead of just encryption.

200
00:12:02,000 --> 00:12:05,600
So in this particular case, if I take a neural net, I take all of its parameters and I

201
00:12:05,600 --> 00:12:09,640
split it, split each parameter into four shares, then I have four governors.

202
00:12:09,640 --> 00:12:11,960
I have four owners for this model.

203
00:12:11,960 --> 00:12:17,240
And any one of those four people could say, oh, I'm not willing to perform computation

204
00:12:17,240 --> 00:12:19,760
with you guys on this data point.

205
00:12:19,760 --> 00:12:24,280
Because I haven't been paid or because I don't believe in it or for whatever reason, right?

206
00:12:24,280 --> 00:12:28,600
And that's fundamentally different from homeomorphic encryption, which kind of defaults to a single

207
00:12:28,600 --> 00:12:30,480
owner who has a single private key.

208
00:12:30,480 --> 00:12:31,480
Okay.

209
00:12:31,480 --> 00:12:35,200
So yeah, we really like that sort of distributed governance apparatus.

210
00:12:35,200 --> 00:12:37,200
Okay.

211
00:12:37,200 --> 00:12:39,200
And so what does OpenMind come in?

212
00:12:39,200 --> 00:12:40,200
Yeah.

213
00:12:40,200 --> 00:12:44,640
So OpenMind is basically about lowering the barrier to entry to being able to use these

214
00:12:44,640 --> 00:12:45,640
tools.

215
00:12:45,640 --> 00:12:51,440
As I described a minute ago, these communities have kind of lived in separate worlds.

216
00:12:51,440 --> 00:12:56,520
So the crypto community has their own toolkits, typically in C++, the machine learning community

217
00:12:56,520 --> 00:13:00,560
has their own toolkits that's like, you know, PyTorch and TensorFlow.

218
00:13:00,560 --> 00:13:03,080
And statistics has their own, it's like R and all these kinds of things.

219
00:13:03,080 --> 00:13:08,920
So the problem is that if I want to do deep learning with security and PC, I either

220
00:13:08,920 --> 00:13:14,840
have to take an NPC framework and write all my autograd and my layer types and all

221
00:13:14,840 --> 00:13:19,400
stuff from scratch, or I have to go to a deep learning framework and know all the cryptography

222
00:13:19,400 --> 00:13:20,400
to be able to add.

223
00:13:20,400 --> 00:13:28,320
And as it turns out, this is prohibitively difficult and nearly most papers that I come across,

224
00:13:28,320 --> 00:13:32,480
people write some big amorphous C++ blog from scratch.

225
00:13:32,480 --> 00:13:35,480
And then it becomes difficult to repurpose, difficult for industry to pick up.

226
00:13:35,480 --> 00:13:40,520
So what OpenMind is all about is about installing these cryptography primitives inside of the

227
00:13:40,520 --> 00:13:41,840
major deep learning toolkits.

228
00:13:41,840 --> 00:13:44,120
So PyTorch and TensorFlow being the first two.

229
00:13:44,120 --> 00:13:48,960
So that people in the machine learning community don't have to really know anything about cryptography

230
00:13:48,960 --> 00:13:49,960
to be able to use them.

231
00:13:49,960 --> 00:13:54,360
Or if you're able to do research on them to be able to study these different protocols.

232
00:13:54,360 --> 00:13:59,880
So in particular, in one example, if that is in PyTorch, we have this new tensor type

233
00:13:59,880 --> 00:14:02,000
that's an NPC shared tensor.

234
00:14:02,000 --> 00:14:06,160
So it feels like a normal tensor has the same API, but every time you add two tensors

235
00:14:06,160 --> 00:14:10,640
together, under the hood it actually sends messages to multiple different machines and

236
00:14:10,640 --> 00:14:11,960
does the protocol correctly.

237
00:14:11,960 --> 00:14:15,960
This is the multi-party aspect of it.

238
00:14:15,960 --> 00:14:18,160
Is it like widely distributed?

239
00:14:18,160 --> 00:14:23,960
Like you might think of the parties representing actual people that are in different corners

240
00:14:23,960 --> 00:14:30,920
of the earth, or is it are the parties typically like computational processes that might be local

241
00:14:30,920 --> 00:14:34,040
to an environment and under the same control?

242
00:14:34,040 --> 00:14:37,000
Like how does it typically, what's the typical topology?

243
00:14:37,000 --> 00:14:38,520
Great question.

244
00:14:38,520 --> 00:14:43,320
So for the purpose of development, we've got nice sort of what we call virtual workers

245
00:14:43,320 --> 00:14:47,280
that will simulate it on one machine so that you can build protocols and that kind of thing.

246
00:14:47,280 --> 00:14:55,280
But in the context of a more real world use case, there's different philosophies on how

247
00:14:55,280 --> 00:14:56,280
it will actually roll out.

248
00:14:56,280 --> 00:15:00,840
So the thing to know is that the more people you have governing a data point, the more

249
00:15:00,840 --> 00:15:04,600
people have to touch it for it to be decrypted or for it to be used in computation.

250
00:15:04,600 --> 00:15:09,000
But meaning the more owners it has, the slower the computation is going to go.

251
00:15:09,000 --> 00:15:14,840
So our vision is that most of privacy-preserving machine learning will operate with two owners

252
00:15:14,840 --> 00:15:18,440
at a time because that's sort of the fastest version of it.

253
00:15:18,440 --> 00:15:23,360
And this is where, as I mentioned, I may have mentioned this before, we really see a

254
00:15:23,360 --> 00:15:28,120
trifecta of like three core technologies coming together, secure multi-party computation

255
00:15:28,120 --> 00:15:31,520
being one of them, and then federated learning and differential privacy being the other.

256
00:15:31,520 --> 00:15:36,840
So the nice thing about federated learning, as it's a protocol that allows me as a model

257
00:15:36,840 --> 00:15:42,120
owner to train a model on a highly distributed data set by performing training with one

258
00:15:42,120 --> 00:15:43,360
person at a time.

259
00:15:43,360 --> 00:15:48,720
So I'll send a model to someone and it will train there for a while.

260
00:15:48,720 --> 00:15:51,960
I think I could do this with 10 people in parallel.

261
00:15:51,960 --> 00:15:56,640
But the nice thing is that I should probably come back to that.

262
00:15:56,640 --> 00:15:59,880
So the short answer is I see it happening two people at a time.

263
00:15:59,880 --> 00:16:04,400
And when you combine it with these other technologies, that actually works out to be quite

264
00:16:04,400 --> 00:16:05,400
performed.

265
00:16:05,400 --> 00:16:07,400
When do you want to go back to the federated?

266
00:16:07,400 --> 00:16:08,400
Oh, yeah.

267
00:16:08,400 --> 00:16:10,640
It's like an interesting topic.

268
00:16:10,640 --> 00:16:15,720
How well developed is that in the literature, the models for federated machine learning.

269
00:16:15,720 --> 00:16:20,200
I've seen a lot of work on distributed training.

270
00:16:20,200 --> 00:16:27,000
And then more recently in the context of like IoT and like edge devices, federated training

271
00:16:27,000 --> 00:16:32,080
in that sense, where you are trying to do some amount of learning kind of at the edge,

272
00:16:32,080 --> 00:16:37,240
but also share the model back to some centralized thing so that it could incorporate learning

273
00:16:37,240 --> 00:16:43,120
from other edge devices, but also so that it becomes like the master model.

274
00:16:43,120 --> 00:16:47,920
And then you ship out updates back out to the other edge devices.

275
00:16:47,920 --> 00:16:52,840
I don't know if the same kinds of techniques apply or what the overlap might be there.

276
00:16:52,840 --> 00:16:53,840
They absolutely do.

277
00:16:53,840 --> 00:16:57,440
If federated learning is out of these three things, like federated learning, multi-party

278
00:16:57,440 --> 00:17:01,200
computation and differential privacy, federated learning is the most mature.

279
00:17:01,200 --> 00:17:05,920
I mean, to that extent, so if you have a smartphone and you open it up and you go to

280
00:17:05,920 --> 00:17:10,760
text someone, you know, try to recommend the next word for your text, that model for

281
00:17:10,760 --> 00:17:15,360
both, to my knowledge, to both Apple and Google is trained using federated learning.

282
00:17:15,360 --> 00:17:19,480
So to that extent, federated learning is already deployed on, you know, billions of devices

283
00:17:19,480 --> 00:17:21,480
around the planet.

284
00:17:21,480 --> 00:17:24,520
So to that extent, it's quite robust.

285
00:17:24,520 --> 00:17:27,520
And the nice thing about federated learning, so I guess for anyone listening that doesn't

286
00:17:27,520 --> 00:17:31,440
know what federated learning is, typically when you're training a machine learning model,

287
00:17:31,440 --> 00:17:37,080
you would aggregate all the data to sort of one central server, then you'd initialize

288
00:17:37,080 --> 00:17:40,280
a random model and then you'd train it on that data, right?

289
00:17:40,280 --> 00:17:43,880
So federated learning kind of flips the script and says instead of bringing all the data

290
00:17:43,880 --> 00:17:47,680
to the model in one location, you're going to bring the model out to the data wherever

291
00:17:47,680 --> 00:17:48,680
it already lives.

292
00:17:48,680 --> 00:17:51,840
And the general goal is that whoever owns the data doesn't want to have to send it to

293
00:17:51,840 --> 00:17:52,840
someone.

294
00:17:52,840 --> 00:17:55,440
They want to own and maintain the only copy.

295
00:17:55,440 --> 00:17:58,240
So in this particular case, this means that, you know, if I've got a billion phones,

296
00:17:58,240 --> 00:18:03,000
I as a central server, we'll then send the model with some instructions on how to train

297
00:18:03,000 --> 00:18:06,400
it down to individual data owners.

298
00:18:06,400 --> 00:18:10,240
And this is where the nice thing about that is it does two things.

299
00:18:10,240 --> 00:18:11,880
One, it protects the privacy of the data.

300
00:18:11,880 --> 00:18:15,480
And two, it typically actually results in less communication than sending the data to

301
00:18:15,480 --> 00:18:16,480
the cloud.

302
00:18:16,480 --> 00:18:22,760
Look at the original paper on federated learning from Brendan McMahon.

303
00:18:22,760 --> 00:18:27,040
They, I think they decided 10 to 100x less network overhead and sending the data sets to

304
00:18:27,040 --> 00:18:28,040
the cloud.

305
00:18:28,040 --> 00:18:31,240
I mean, it depends on how big the data set is, but often it can actually be less network

306
00:18:31,240 --> 00:18:32,760
intensive.

307
00:18:32,760 --> 00:18:36,760
And so as you can imagine, this is where MPC comes in.

308
00:18:36,760 --> 00:18:40,520
So the big, the nice thing here is you protect the privacy of the data, but the problem is

309
00:18:40,520 --> 00:18:44,920
is that you send a plain text copy of the model to potentially thousands or millions of

310
00:18:44,920 --> 00:18:45,920
people, right?

311
00:18:45,920 --> 00:18:49,880
And if this is your $10 million healthcare model, well, then you'll be, you're sort of at

312
00:18:49,880 --> 00:18:55,440
risk of any one of these millions of people stealing it and trying to monetize it for themselves.

313
00:18:55,440 --> 00:18:57,920
This is where secure MPC becomes interesting.

314
00:18:57,920 --> 00:19:02,080
So instead of sending the model, you would MPC share the model, and they would MPC share

315
00:19:02,080 --> 00:19:06,840
their data set, and I would perform sort of encrypted training with each individual person

316
00:19:06,840 --> 00:19:11,840
in parallel, and then only aggregate their gradients sort of at the end.

317
00:19:11,840 --> 00:19:15,840
This is why federated learning allows you to do MPC sort of two people at a time, which

318
00:19:15,840 --> 00:19:17,160
is quite a more optimal.

319
00:19:17,160 --> 00:19:25,920
What's the kind of user experience of OpenMind is that like system level packets that they're

320
00:19:25,920 --> 00:19:28,680
using, or is it more framework level?

321
00:19:28,680 --> 00:19:32,440
Is the typical user a developer or a data scientist?

322
00:19:32,440 --> 00:19:35,120
How do you kind of look at the world from that perspective?

323
00:19:35,120 --> 00:19:36,120
Yeah.

324
00:19:36,120 --> 00:19:39,600
So speaking from today, it's mostly the framework level.

325
00:19:39,600 --> 00:19:45,800
So our target audience today is primarily sort of machine running researchers.

326
00:19:45,800 --> 00:19:51,720
Basically a scientist, perhaps working at enterprise wanting to do a pilot in privacy

327
00:19:51,720 --> 00:19:53,120
preserving machine learning.

328
00:19:53,120 --> 00:19:58,360
So the vast majority of the code we've written is extending PyTorch Intensive Flow.

329
00:19:58,360 --> 00:20:02,600
So if you're not a PyTorch Intensive Flow user, you don't have any, then we're not quite

330
00:20:02,600 --> 00:20:03,880
ready for you yet.

331
00:20:03,880 --> 00:20:08,840
I think in the future, this will likely get packaged more into sort of, here's your federated

332
00:20:08,840 --> 00:20:09,840
learning system.

333
00:20:09,840 --> 00:20:13,120
Here's how you spin up the server, and this is where all your data adapters will attach

334
00:20:13,120 --> 00:20:16,600
to that kind of thing, but we still have quite a bit of code to write until we get there.

335
00:20:16,600 --> 00:20:23,760
So is it extending your PyTorch Intensive Flow in an analogous way to the way different

336
00:20:23,760 --> 00:20:27,240
distributed training methods are working?

337
00:20:27,240 --> 00:20:28,240
Yeah.

338
00:20:28,240 --> 00:20:36,000
So for PyTorch, for example, the first thing we built was a remote execution protocol.

339
00:20:36,000 --> 00:20:42,120
So for anyone who knows PyTorch that's listening, it allows you to take a tensor, which is

340
00:20:42,120 --> 00:20:46,520
a tensor as a simple list of numbers or nested list of numbers.

341
00:20:46,520 --> 00:20:50,840
So if we have a texture called like x equals, you know, some tensor, maybe 5, 0 or something

342
00:20:50,840 --> 00:20:56,760
like that, I can go x.send and send it to some machine and put an address for that machine.

343
00:20:56,760 --> 00:21:00,000
And then what gets returned to me is a pointer to that tensor.

344
00:21:00,000 --> 00:21:04,520
And then whatever that pointer has the exact same API as a tensor normally would.

345
00:21:04,520 --> 00:21:06,480
So it feels like a normal tensor.

346
00:21:06,480 --> 00:21:10,160
But the nice thing is that then we can use this underlying primitive to coordinate remote

347
00:21:10,160 --> 00:21:14,400
executions and coordinate higher level protocols like security PC.

348
00:21:14,400 --> 00:21:18,760
So most of it's built on top of that, kind of a remote execution paradigm.

349
00:21:18,760 --> 00:21:19,760
But then the API.

350
00:21:19,760 --> 00:21:24,240
And that was standard already existing in both PyTorch Intensive Flow, something analogous

351
00:21:24,240 --> 00:21:25,240
to it.

352
00:21:25,240 --> 00:21:28,680
So PyTorch Intensive Flow have not had pointers in the past, so that's a new thing for

353
00:21:28,680 --> 00:21:29,680
us.

354
00:21:29,680 --> 00:21:30,680
Okay.

355
00:21:30,680 --> 00:21:35,880
But the thing that we try to keep the same is we want that pointer to have basically have

356
00:21:35,880 --> 00:21:38,920
the exact same API as if the data was on your local machine, right?

357
00:21:38,920 --> 00:21:39,920
Okay.

358
00:21:39,920 --> 00:21:44,240
So we want to make it so that you don't have to relearn a new framework or really any

359
00:21:44,240 --> 00:21:48,320
other paradigms to be able to use things like federated learning or NPC.

360
00:21:48,320 --> 00:21:52,360
And that's why our NPC tensor feels like it's a normal tensor.

361
00:21:52,360 --> 00:21:55,960
It feels like it's on your device, has the same API and protocol and you can do the same

362
00:21:55,960 --> 00:21:56,960
things.

363
00:21:56,960 --> 00:22:02,200
You can call back propagate on a lot of remote tensors.

364
00:22:02,200 --> 00:22:07,400
But it's under the hood, it's actually doing all the protocol for you, such that you

365
00:22:07,400 --> 00:22:09,320
don't have to learn that protocol yourself.

366
00:22:09,320 --> 00:22:10,320
Yeah.

367
00:22:10,320 --> 00:22:16,800
And you're using this API or to get the pointers, you're specifying the number of parties

368
00:22:16,800 --> 00:22:19,960
and IP addresses or something like that, or?

369
00:22:19,960 --> 00:22:20,960
Yeah.

370
00:22:20,960 --> 00:22:25,440
So we do have the ability to spin up servers, which we call workers, that are connected via

371
00:22:25,440 --> 00:22:28,960
you know, sockets or web sockets.

372
00:22:28,960 --> 00:22:31,960
And what you'll get is actually a pointer to that worker on the client side.

373
00:22:31,960 --> 00:22:36,080
So we typically name them Bob and Alice canonically, just because that's sort of the crypto literature.

374
00:22:36,080 --> 00:22:37,560
Yeah, exactly.

375
00:22:37,560 --> 00:22:41,800
So we have a big set of tutorials, actually, we just listed this shows how the whole protocol

376
00:22:41,800 --> 00:22:42,800
goes.

377
00:22:42,800 --> 00:22:46,840
So if you go right and get hub slash examples slash, so the PICIF library is the library

378
00:22:46,840 --> 00:22:47,840
I'm referring to.

379
00:22:47,840 --> 00:22:50,720
PICIF slash examples slash tutorials will have that.

380
00:22:50,720 --> 00:22:57,800
But yeah, so if I'm going to, if I'm going to send it, I'll go X, my variable, dot send.

381
00:22:57,800 --> 00:23:01,640
And then in parentheses, I can put in one or more workers that I want to send it to and

382
00:23:01,640 --> 00:23:02,640
return to that.

383
00:23:02,640 --> 00:23:03,640
Okay.

384
00:23:03,640 --> 00:23:07,960
And then I pass in a list of people, a list of pointers to their workers that I want

385
00:23:07,960 --> 00:23:09,240
to NPC share it.

386
00:23:09,240 --> 00:23:10,240
Yeah.

387
00:23:10,240 --> 00:23:11,800
And that's the sort of the encrypted version.

388
00:23:11,800 --> 00:23:17,080
Do you also have kind of flexibility over whether you want to use encryption or just NPC

389
00:23:17,080 --> 00:23:23,200
or whether you want to layer in differential privacy or is the framework making those kinds

390
00:23:23,200 --> 00:23:24,360
of decisions for me?

391
00:23:24,360 --> 00:23:25,360
No.

392
00:23:25,360 --> 00:23:27,520
So the framework is totally agnostic to those decisions.

393
00:23:27,520 --> 00:23:33,600
So obviously we have higher level libraries that try to give you nice defaults.

394
00:23:33,600 --> 00:23:39,960
But the bulk of the framework is this low level protocol for allowing you to either design

395
00:23:39,960 --> 00:23:44,480
your own sort of remote execution strategies or to pull one of the off the shelf ones that

396
00:23:44,480 --> 00:23:49,560
you provide, like federated averaging or different kinds of NPC protocols.

397
00:23:49,560 --> 00:23:51,280
I think we have two different NPC protocols.

398
00:23:51,280 --> 00:23:52,280
Okay.

399
00:23:52,280 --> 00:23:55,920
And what are the distinctions between the different protocols?

400
00:23:55,920 --> 00:24:01,800
So they're APIs primarily, so and so the first one we implemented was called speeds.

401
00:24:01,800 --> 00:24:09,920
So speeds is a protocol that allows you to do, in our case, addition and multiplication.

402
00:24:09,920 --> 00:24:12,800
And then we extended it with this new protocol called Secure and In, which allows you to

403
00:24:12,800 --> 00:24:14,680
do comparison operators.

404
00:24:14,680 --> 00:24:19,440
So to compare two encrypted numbers, as it turns out, we can bootstrap sort of the rest

405
00:24:19,440 --> 00:24:22,280
of the deep learning API just from those three core primitives.

406
00:24:22,280 --> 00:24:27,760
How mature would you say is the, you know, the particular, the project and kind of the

407
00:24:27,760 --> 00:24:33,800
ecosystem around it, like are, are you finding folks extra, is it like, you know, your personal

408
00:24:33,800 --> 00:24:36,800
effort mostly that's kind of pushing along, or are there other folks that are kind of

409
00:24:36,800 --> 00:24:38,560
jumping in and contributing?

410
00:24:38,560 --> 00:24:39,560
How's that been going?

411
00:24:39,560 --> 00:24:41,600
So that part's actually been going really well.

412
00:24:41,600 --> 00:24:46,400
So I think open-mind Slack team at this point has around 3,400 people in it.

413
00:24:46,400 --> 00:24:47,400
Wow.

414
00:24:47,400 --> 00:24:48,400
Yeah.

415
00:24:48,400 --> 00:24:49,400
Oh, wow.

416
00:24:49,400 --> 00:24:53,000
And I think we're up to 170 or 180 people who have contributed code.

417
00:24:53,000 --> 00:24:54,000
Oh.

418
00:24:54,000 --> 00:24:57,640
So to that extent, like, I think a lot of people get that privacy is important at these

419
00:24:57,640 --> 00:25:02,160
tools are a really compelling answer, or they could be in a long run and that by building

420
00:25:02,160 --> 00:25:05,400
better research tools and bringing these two kind of crypto and machine learning community

421
00:25:05,400 --> 00:25:09,640
together, that we can make a lot of progress, and then it's worth their nights and weekends

422
00:25:09,640 --> 00:25:11,920
to work on.

423
00:25:11,920 --> 00:25:19,280
That being said, I would describe PICIF today as kind of an alpha, alpha level project.

424
00:25:19,280 --> 00:25:23,400
So PICIF, again, is the extension deep to PyTorch Intensive Flow.

425
00:25:23,400 --> 00:25:26,840
Open Mind is just kind of the general community name.

426
00:25:26,840 --> 00:25:31,080
So yeah, PICIF is kind of an alpha phase where it's got all the features that we would

427
00:25:31,080 --> 00:25:36,920
like for it to have, but they're not necessarily like a product ready state, and they're also

428
00:25:36,920 --> 00:25:37,920
using an older version.

429
00:25:37,920 --> 00:25:42,560
It was in the particular case of PyTorch, they're using 0.3.1.

430
00:25:42,560 --> 00:25:47,760
So the main push at the moment is to upgrade to PyTorch 1.0, which was recently released,

431
00:25:47,760 --> 00:25:55,280
as well as to really make them robust enough that the institutions can do kind of pilots

432
00:25:55,280 --> 00:25:58,600
and practice use cases and these kinds of things, and we're hoping to release that around

433
00:25:58,600 --> 00:25:59,600
February and March.

434
00:25:59,600 --> 00:26:05,680
Are there any examples of use cases that you can talk about?

435
00:26:05,680 --> 00:26:10,600
So we are in the midst of crafting several early pilots, but I don't think they're quite

436
00:26:10,600 --> 00:26:12,440
public yet, but I'm very excited about them.

437
00:26:12,440 --> 00:26:17,280
We also do have several, I think about a half dozen firms that are in the process of

438
00:26:17,280 --> 00:26:22,680
putting together research grants for people to work on PICIF in Open Mind full-time, which

439
00:26:22,680 --> 00:26:27,920
will greatly help both their internal adoption as well as the health of the library, which

440
00:26:27,920 --> 00:26:29,880
I'm excited about.

441
00:26:29,880 --> 00:26:33,200
And so where do you see this going?

442
00:26:33,200 --> 00:26:38,880
What are some of the next steps for the community and your research in general?

443
00:26:38,880 --> 00:26:44,600
For my personal research, I think I want to sort of stay focused on whatever is needed

444
00:26:44,600 --> 00:26:47,560
for the steps to become mainstream and why they adopted.

445
00:26:47,560 --> 00:26:51,680
At the moment, that's raising awareness and lowering the barrier to entry for tools

446
00:26:51,680 --> 00:26:57,560
that already exist and conducting research to solve the holes that still miss in the

447
00:26:57,560 --> 00:26:58,560
technologies.

448
00:26:58,560 --> 00:27:01,760
There are some sub-problems, especially in differential privacy that have not yet been

449
00:27:01,760 --> 00:27:08,640
solved to the extent that the widest use cases could be implemented today.

450
00:27:08,640 --> 00:27:12,600
So in the next three to six months, I think that it's still mostly a research and open-source

451
00:27:12,600 --> 00:27:14,680
tool push.

452
00:27:14,680 --> 00:27:18,000
In about a six-month to 18-month timeline, I think that we're going to start seeing

453
00:27:18,000 --> 00:27:19,720
considerable enterprise adoption.

454
00:27:19,720 --> 00:27:26,840
So there's already some interesting movement happening in Europe as kind of an echo to GDPR.

455
00:27:26,840 --> 00:27:32,640
There's a handful of startups in the US that I've seen growing up in this space.

456
00:27:32,640 --> 00:27:36,560
And so I think probably the six to 18-month timeline is when enterprises which already

457
00:27:36,560 --> 00:27:39,640
have the data will be interested in moving into this space.

458
00:27:39,640 --> 00:27:44,760
You mentioned some kind of point challenges on the differential privacy side that need

459
00:27:44,760 --> 00:27:49,520
to be solved to better facilitate all of this.

460
00:27:49,520 --> 00:27:53,080
Are there some examples of those that you can share?

461
00:27:53,080 --> 00:27:58,440
So from what I've observed, there's two different kinds of differential privacy right?

462
00:27:58,440 --> 00:28:01,600
There's local and there's global.

463
00:28:01,600 --> 00:28:06,360
Local differential privacy focuses on actually adding noise to each individual row of

464
00:28:06,360 --> 00:28:10,520
data to make sure that each individual row of data is protected before you even do anything

465
00:28:10,520 --> 00:28:11,520
to it.

466
00:28:11,520 --> 00:28:15,560
But as you can imagine, if you have a million data points, that means you're adding a ton

467
00:28:15,560 --> 00:28:19,040
of noise to the data set as a whole.

468
00:28:19,040 --> 00:28:22,800
And you need to have like a lot of people in order to be able to get interesting insights

469
00:28:22,800 --> 00:28:26,320
about it because you've had to add so much noise to each individual data point.

470
00:28:26,320 --> 00:28:31,880
Global differential privacy allows you to compute some function on the data and then only

471
00:28:31,880 --> 00:28:35,920
add noise to the output, which can actually be relatively small amounts depending on what

472
00:28:35,920 --> 00:28:38,640
that function is.

473
00:28:38,640 --> 00:28:44,760
So one thing I have yet to see in deep learning literature is a global differential privacy

474
00:28:44,760 --> 00:28:52,520
technique that really fits the business use case that people want to use it for.

475
00:28:52,520 --> 00:28:56,360
The best one that I know of that comes to mind is the Pate algorithm, which I think if

476
00:28:56,360 --> 00:28:58,920
people know different differential privacy and deep learning is probably the one that

477
00:28:58,920 --> 00:29:00,840
they'd be most familiar with.

478
00:29:00,840 --> 00:29:07,000
This is a really innovative approach wherein you split a data set into 10 different buckets

479
00:29:07,000 --> 00:29:08,000
or in different buckets.

480
00:29:08,000 --> 00:29:12,400
I'll just use 10 as an example and you train a model on each different one that's a private

481
00:29:12,400 --> 00:29:13,400
model.

482
00:29:13,400 --> 00:29:17,800
And then there's this clever trick where you can use these models to annotate a second

483
00:29:17,800 --> 00:29:23,280
data set that is a public data set in a way that all of the labels, you can sort of enforce

484
00:29:23,280 --> 00:29:26,640
differential privacy by the way that you label this second data set using these private

485
00:29:26,640 --> 00:29:32,240
models such that you then train another model on this sort of synthetic, synthetically labeled

486
00:29:32,240 --> 00:29:33,240
data set.

487
00:29:33,240 --> 00:29:35,000
I think that one is Nicholas Paprono.

488
00:29:35,000 --> 00:29:36,000
Yeah, that's right.

489
00:29:36,000 --> 00:29:38,520
We did a podcast interview with him as well.

490
00:29:38,520 --> 00:29:39,520
Oh, excellent.

491
00:29:39,520 --> 00:29:40,520
Yeah.

492
00:29:40,520 --> 00:29:45,160
That's probably the most famous and most successful general purpose deep learning for differential

493
00:29:45,160 --> 00:29:47,240
privacy algorithm that we have.

494
00:29:47,240 --> 00:29:52,560
But the problem is that it requires this, the second data set that you already have.

495
00:29:52,560 --> 00:29:57,680
And in practice in industry, this is not usually the case, right?

496
00:29:57,680 --> 00:30:04,080
So if I'm a hospital network and I've got say 50,000 examples that are unlabeled, the

497
00:30:04,080 --> 00:30:09,080
friction for me to go to 10 other hospitals and train models with all of them and synthesize

498
00:30:09,080 --> 00:30:13,600
this new model just to like annotate my data set and train my secondary model is it's

499
00:30:13,600 --> 00:30:14,600
pretty rare use case.

500
00:30:14,600 --> 00:30:18,000
Typically, it's cheaper, easier for them to just pay some of the annotated, right?

501
00:30:18,000 --> 00:30:20,560
It's a little more straightforward and it's not as experimental, not kind of thing.

502
00:30:20,560 --> 00:30:25,680
So if we can figure out a way where you can have a similar level of global differential

503
00:30:25,680 --> 00:30:30,760
privacy like this, actually even it's debatable whether this is global or local, it's actually

504
00:30:30,760 --> 00:30:37,920
it's probably closer to local, which doesn't have this core assumption that would that would

505
00:30:37,920 --> 00:30:43,800
be a pretty big breakthrough and it would be the kind of thing where that's the form

506
00:30:43,800 --> 00:30:49,480
of a DP technique that would be general purpose, I think.

507
00:30:49,480 --> 00:30:54,960
And I would very much like to see sort of someone push through on that front, but obviously

508
00:30:54,960 --> 00:30:59,680
lots of people are trying and we'll hopefully we'll have something in the next little while.

509
00:30:59,680 --> 00:31:05,960
Any pointers or thoughts or folks that want to dig more into this area, where should they

510
00:31:05,960 --> 00:31:12,800
start looking for ways to get up to speed on these three pillars or your project?

511
00:31:12,800 --> 00:31:16,880
Yeah, so I mean the three activities that we do and open mind hopefully are designed

512
00:31:16,880 --> 00:31:18,240
to make that as easy as possible.

513
00:31:18,240 --> 00:31:22,960
So we build open source tools, we create learning resources and we build community through

514
00:31:22,960 --> 00:31:25,400
like hackathons and through the Slack channel.

515
00:31:25,400 --> 00:31:30,600
So the first thing that I'd recommend is join the Slack team and kind of scroll through

516
00:31:30,600 --> 00:31:31,960
the articles people are posting.

517
00:31:31,960 --> 00:31:38,680
So you can go to slack.openmind.org, openmind is spelled OPE in I in ED.

518
00:31:38,680 --> 00:31:43,520
Yeah, I happen to general discussion, you know, say hello and then I just finished writing

519
00:31:43,520 --> 00:31:49,640
a nine length notebook tutorial walking from the very basics of kind of remote execution

520
00:31:49,640 --> 00:31:54,480
through federated learning and secure NPC, it'll be extended with differential privacy

521
00:31:54,480 --> 00:31:55,480
in a bit.

522
00:31:55,480 --> 00:31:58,760
And the real goal of that tutorial series is just to walk someone who you know just knows

523
00:31:58,760 --> 00:32:04,880
in this case PyTorch through in a very, you know, hands-on kind of way how these techniques

524
00:32:04,880 --> 00:32:05,880
work.

525
00:32:05,880 --> 00:32:06,880
Awesome.

526
00:32:06,880 --> 00:32:07,880
Yeah.

527
00:32:07,880 --> 00:32:08,880
Awesome.

528
00:32:08,880 --> 00:32:09,880
Well, Andrew, thanks so much for taking the time to chat with me.

529
00:32:09,880 --> 00:32:10,880
It was great to meet you.

530
00:32:10,880 --> 00:32:11,880
Great to meet you too.

531
00:32:11,880 --> 00:32:17,160
All right, everyone, that's our show for today.

532
00:32:17,160 --> 00:32:23,760
For more information on Andrew or any of the topics covered in this episode, visit twimmelai.com

533
00:32:23,760 --> 00:32:27,240
slash talk slash 241.

534
00:32:27,240 --> 00:32:30,720
As always, thanks so much for listening and catch you next time.

