WEBVTT

00:00.000 --> 00:16.640
All right, everyone. I am here with Andrea Benino. Andrea is a research scientist at DeepMind.

00:17.200 --> 00:23.120
Andrea, welcome to the Twomol AI podcast. Thanks, Sam. Thanks for having me. So you are

00:23.120 --> 00:30.720
working on artificial general intelligence at DeepMinds. Tell us a little bit about where your

00:30.720 --> 00:37.200
interest in AGI comes from. My interest in AGI comes from my background. I'm actually a neuroscientist

00:38.320 --> 00:45.520
who decided to study how the brain works and in particular memory, how we create our memory,

00:45.520 --> 00:55.120
why we create our memory. And in particular, I'm interested in a subset of the memory field,

00:55.120 --> 01:03.600
which is called a episodic memory. So episodic memory, those memory that relates to some

01:04.720 --> 01:12.400
episode that you experience in the past in a specific location, sometimes, or with specific people.

01:12.400 --> 01:18.560
So they are also referred as autobiographical memories.

01:18.560 --> 01:23.280
An episodic is in contrast to what other kinds of memory?

01:23.280 --> 01:33.360
Another example could be semantic memory. So semantic is like, you know, the meaning of something,

01:33.360 --> 01:39.680
for example, you know, what is a bicycle that could be a semantic memory. But if I ask you to remind

01:39.680 --> 01:48.880
you to remember a specific when when you when you cycle with someone, then maybe now you're

01:48.880 --> 01:53.680
thinking about a specific episode in your life that would be an episodic memory. So you relate

01:54.400 --> 02:05.120
something to a specific moment in time. It's also described as the what when and where of a specific

02:05.120 --> 02:14.160
it went. To some degree, the relationship between memory and intelligence is kind of an obvious one

02:14.160 --> 02:24.080
in the sense that, you know, we we use our memory and our prior experiences and interacting with

02:24.080 --> 02:33.200
the world, making decisions and all that. But is there a kind of broader significance to memory

02:33.200 --> 02:42.160
and the development of AGI? Yeah, I think as you say, it's kind of an obvious one, right? So

02:42.160 --> 02:46.960
we live in a world that is consistent. So if we gain some experience in the world, then we want

02:46.960 --> 02:52.800
to reuse that experience if we don't want to relearn every time from scratch. So you can already

02:52.800 --> 02:59.200
see how that's viable. But something that I actually study over my PhD is that episodic memory,

02:59.200 --> 03:06.480
it's also way to so it's something that enables generalization. In particular, we study how

03:07.920 --> 03:14.000
let's say if you have if you experience two events A and B that are related together. And later

03:14.000 --> 03:20.240
in time, you experience other two events like B and C that are again related together. You are

03:20.240 --> 03:27.200
brain without you doing any effort directly relate A to C together, such that you relate,

03:27.200 --> 03:32.800
you do this kind of inference through your episodic memory. So if you want to give like if I can tell

03:32.800 --> 03:42.480
you like a more precise example would be like, if you see someone going out with a dog in the

03:42.480 --> 03:48.720
morning, and then you see the same dog with a different person in the afternoon, immediately your

03:48.720 --> 03:53.680
brain is going to try to connect the two people that we are going out with the dog. And that's the

03:53.680 --> 04:01.120
kind of inference that episodic memory we know support. So that's that's quite important, right?

04:02.160 --> 04:13.680
Right, right. And what is the the history of memory, you know, in this effort to kind of get us

04:13.680 --> 04:23.120
closer to AGI? How have how have we used memory in the field to facilitate intelligence? Okay, so

04:23.760 --> 04:28.320
this is I think it's a nice question because I think in some sense the kind of memory I'm talking

04:28.320 --> 04:37.600
about is still an open question. How we do it properly in in AI? We have different forms of memory

04:37.600 --> 04:43.520
currently, which we know how to to play with in particular recurrent networks both RNN and the

04:43.520 --> 04:50.720
STM are so-called working memory. So it's kind that gives you like imagine a white white board where

04:50.720 --> 04:56.400
you can write something and reason about that and then erase because it won't stay there forever.

04:56.400 --> 05:04.000
It's like kind of a canvas where you can you can make predictions. That's that's what we know.

05:04.000 --> 05:11.680
Then we have also something called memory augmented neural networks. In that case we we basically give

05:11.680 --> 05:20.560
a neural networks an external memory where they can write a previous computation that they performed

05:20.560 --> 05:26.320
and then read back from there and reuse previous computation and this gets a little bit closer to

05:26.320 --> 05:34.560
the kind of memory I hinted before but we are not there yet and then we have retrieval augmented

05:34.560 --> 05:40.320
models which are those models that basically go back in a if you like in a table where we store

05:41.040 --> 05:47.200
almost everything we have seen in the past like a dictionary if you like and then they try to

05:47.200 --> 05:53.280
look up things but most of the time they look up and they they they use that look up to answer

05:53.280 --> 05:59.440
but they don't consolidate back the knowledge into the into the weights if you like of the system

05:59.440 --> 06:05.760
so they need to do the look up all the time which in some sense it's a waste we don't do as

06:05.760 --> 06:11.440
as I said before we don't do that we kind of make sense of what we are to and use it later.

06:12.480 --> 06:18.960
Right so in that lot of case you can think of it as kind of this fixed boundary between the memory

06:18.960 --> 06:29.200
and the computation in a sense and the computation isn't ever updated the way we think about or

06:29.200 --> 06:35.920
the way the model thinks about its inputs is never updated with regard to the things that it

06:35.920 --> 06:40.560
learns in a memory it's just checking the memory constantly with each each input. Yeah sometimes

06:40.560 --> 06:46.480
there are models for I can I that comes now to my mind a model where that computation is updated

06:46.480 --> 06:52.960
but it's very difficult to scale up those models because they require a lot of computation so we

06:52.960 --> 06:58.000
don't know yet how to make that model big to a scale where they're actually useful to target

06:58.000 --> 07:04.720
a very complex problem and also we have finally the last one will be a very recent

07:05.360 --> 07:11.440
advancement in in AI which is just something that everyone knows I'm sure transformer

07:11.440 --> 07:19.680
this model have shown some of the properties of this kind of memory in language there are what

07:19.680 --> 07:26.960
the sometimes they show a few short learning which is a prerogative of like the memory models I'm

07:26.960 --> 07:31.920
talking about but that happens in language only not in other domains so I'm not sure if it's

07:32.800 --> 07:37.120
big that then that raised the question it's because of the model capable of doing that or it's

07:37.120 --> 07:46.000
because of this specific domain where you train the model that allows that so no one has done that

07:46.000 --> 07:52.560
experiment yet but I think it could be interesting and can you drill down on that how should we

07:52.560 --> 07:59.920
think about transformers from a memory perspective and exhibiting these properties that you

07:59.920 --> 08:04.960
mentioned first of all I think most of the transformer that we use now in language

08:04.960 --> 08:10.720
transformer excel and it's a specific instantiation of transformer whereby you add

08:11.760 --> 08:17.760
a memory and a sort of external memory at each layer of the transformer so we already know

08:17.760 --> 08:23.440
that that's necessary in some sense to overcome some of the limits like not having recurrence for instance

08:26.640 --> 08:32.720
so I would say they are limited in terms of memory also because given that we do this

08:32.720 --> 08:39.920
all to all comparison over the sequence we cannot process very long context there are now papers

08:39.920 --> 08:46.800
trying to deal with that and reduce the complexity to to linear size but still I don't think

08:46.800 --> 08:53.600
we are at the point where let's say we can process several books and ask inference questions

08:53.600 --> 09:01.440
about books without like do like this sort of external retrieval all the time you know oftentimes

09:01.440 --> 09:06.720
there are ideas that we might want to apply in the context of neural networks and a big challenge

09:06.720 --> 09:12.160
is yeah are they differentiable you know so that we can apply techniques like gradient descent

09:13.760 --> 09:20.880
is is memory you know typically differentiable is it typically undifferentiable and like how does

09:20.880 --> 09:28.960
how does that plan to you touch the perfect the perfect downside of like these kind of external

09:28.960 --> 09:36.400
memory where we do what is called k-nears neighbor retrieval and that's a non-differentiable operation

09:37.760 --> 09:47.760
so that obviously has some limitations on on then what you can get and again I don't think

09:47.760 --> 09:54.720
with the current system we know how to backprop or to do backpropagation of a very large memory

09:54.720 --> 10:02.480
which again it's an open problem I think it's a very nice problem to tackle and we should probably

10:02.480 --> 10:09.360
stop now I think we might be able to do it where what is this how does the size of the memory play

10:09.360 --> 10:16.080
into whether we can backprop over it or not because most of the time you use a soft max operation

10:16.080 --> 10:27.040
over the dimension of the memory and you know it becomes quickly impractical to send gradients

10:27.040 --> 10:35.280
over a very large large memory and that's because it's computationally infeasible so it doesn't fit

10:35.280 --> 10:43.920
in memory and also when you get with very large soft maxes also you have problem with gradients

10:43.920 --> 10:52.160
there's another aspect of memory that comes to mind you know we've talked about specific

10:52.800 --> 11:04.000
kind of features and you know architectures that are used to emulate memory but oftentimes one

11:04.000 --> 11:09.920
of the critiques of deep learning is that the networks themselves like remember stuff and you know

11:09.920 --> 11:15.360
that becomes a problem it causes problems with generalization are there ways that

11:17.120 --> 11:24.160
that can be harnessed more directly to achieve some of your goals for episodic memory

11:25.920 --> 11:34.960
yeah okay yes memorization it's definitely an issue like overfitting it's it's it's an issue although

11:34.960 --> 11:48.800
yeah most of what we do as well is like kind of memory so we enlarge our our data set of experience

11:48.800 --> 11:57.280
as we grow right so in some sense we tend to overfeed as well in most of what we do so I don't

11:57.280 --> 12:02.800
think that's that's a huge problem that's also there was actually a nice paper last year called

12:02.800 --> 12:11.040
direct feed to nature which I recommend your listen to to look up from Williamson which basically

12:11.760 --> 12:19.040
poses these the same question you are posing to me and it basically answers in this way so we

12:19.040 --> 12:26.160
we as we grow as we have grown as a species through evolution and also doing our life we kept

12:26.160 --> 12:33.120
enlarging the kind of the the amount of experience that we used to grow our our our brain our network

12:33.760 --> 12:38.000
so in some sense we kept memorizing it's just that we don't know where we start from scratch

12:38.640 --> 12:45.120
and also we have this ability of generalizes that I think it's still a little bit missing from

12:45.120 --> 12:49.840
from deep learning although we shouldn't we should recognize this this generalization

12:50.720 --> 12:55.200
is not that we can generalize to everything right so we have in some sense limited as well

12:55.200 --> 13:01.120
in generalization maybe neural networks are slightly more limited than us but we still we already

13:01.120 --> 13:08.080
see some example of generalization which are starting to to emerge in neural networks and I think

13:08.080 --> 13:12.880
that's a good thing and something we cannot neglect so for instance I had a paper couple of

13:13.680 --> 13:18.640
in two thousand eighty so three years ago in nature where we actually use

13:18.640 --> 13:25.680
and we and view the neural network with representation like the one like the one we have in the brain

13:25.680 --> 13:33.920
in the apocampus and that way the agent was able to travel that was an obligation task the agent

13:33.920 --> 13:39.280
was able to take shortcut and traverse part of the environment that was previously blocked

13:40.160 --> 13:44.960
and the agent was able to do that with the right representation so I don't think the problem is

13:44.960 --> 13:51.520
the back propagation of the model themselves it might be a problem how we train that in terms of

13:51.520 --> 13:58.400
both data we we use in the representation that we force to emerge you mentioned this paper

13:59.600 --> 14:07.200
that implemented something akin to the representation that's used in the apocampus what is that

14:07.200 --> 14:15.680
representation how does how does that work and how the paper we had these we studied these

14:15.680 --> 14:21.840
these things called grid cell so in the apocampus grid cells yes so in the apocampus we have

14:22.400 --> 14:27.280
so it's kind it's a memory machine of the brain but it's also the spatial machine right you can

14:27.280 --> 14:34.080
see also spatial as memory but that's not going to that that's how I feel that I don't want to go

14:34.080 --> 14:41.680
into but basically we have the two cells that are probably the three cells that are more known

14:42.240 --> 14:48.880
like head direction cell so those are cell that fires every time you look in a certain direction

14:48.880 --> 14:54.000
but I'm talking about allocentric direction so every time you face north there will be a cell

14:54.000 --> 14:59.840
firing with a certain probability distribution over north and the same for the other the other

14:59.840 --> 15:06.720
allocentric direction and the north is not the cardinal north the north is the let's say is the

15:06.720 --> 15:14.160
one in with which relate to a certain reference point in the environment so let's say okay

15:15.840 --> 15:20.720
then we have play cell those are neurons that fires every time you are in a particular location

15:22.320 --> 15:27.280
independently aware you're looking at and then we have grid cells which are these

15:27.280 --> 15:34.640
visually and mathematically beautiful cells that basically fires following an hexagonal lattice

15:36.240 --> 15:44.000
they have a 60 degree offset and they are very beautiful and no and there have been several theories

15:44.000 --> 15:51.520
that try to motivate the reason why we have that and one of these was because we can basically

15:51.520 --> 15:56.800
we can use that to calculate shortcut to calculate the shortest vector between two points

15:58.240 --> 16:04.560
and we manage to do two things in that paper first of all we manage to make the representation

16:04.560 --> 16:09.840
emerge in our neural network trained to do path integration so to do our navigation task

16:09.840 --> 16:15.120
and secondly we use these those representations in our in our enforcement learning agent

16:15.120 --> 16:21.680
and we prove through through several ablation as well that the world that was the only agent

16:21.680 --> 16:27.040
able to take shortcut and if we lesion let's say some grid cell the ability of the agent to take

16:27.040 --> 16:34.400
shortcut just wet down so it was kind of an empirical paper to prove what the grid cell are for

16:36.080 --> 16:43.680
is it is the idea in terms of the implementation that i'm imagining you're like adding

16:43.680 --> 16:50.880
signing cosine elements to your loss function or something like that no no no no no no no no no no it's

16:50.880 --> 16:57.040
data driven that our goal was really to be super data driven and we achieved we achieved that by

16:57.040 --> 17:04.800
actually a specific arc it was a recurring architecture two things were really important one was

17:04.800 --> 17:11.360
to introduce a dropout such that no or not all yours were able to fire the same time and the

17:11.360 --> 17:18.320
second one was introduced noise in the gradients and that basically helped if those are two things

17:18.320 --> 17:28.160
we always want to do yeah yeah particularly uh specific to this problem that's that's that's

17:28.160 --> 17:32.400
probably why people like so much that kind of paper because it was kind of a general approach

17:32.400 --> 17:38.480
to to make that one of the although i have to say it was kind of difficult to analyze but

17:38.480 --> 17:46.000
you know one of the reason why noise helps because it helps you moving away from certain solution

17:46.000 --> 17:52.960
in the landscape of the loss and our our our our way to work was to help the network

17:53.840 --> 17:59.600
going down the solution we were we liked so grid cell but that wasn't too difficult actually

17:59.600 --> 18:09.920
to achieve okay that's why i think it it was a nice piece of work yeah interesting interesting um

18:10.400 --> 18:17.760
so we were i'm trying to speak a memory i'm trying to know how we got here actually

18:17.760 --> 18:22.560
in a little the ponder stuff i guess you invited me to talk about the ponder the ponder

18:22.560 --> 18:28.400
the ponder anything inspired by memory exactly from the study that i mentioned at the beginning

18:28.400 --> 18:36.320
the this thing about related doing inference associative inference that was part of my PhD and

18:37.440 --> 18:42.800
one of the paper that came out during that time was disability of the hippocampus to basically

18:43.920 --> 18:50.480
do this recursive ponder before actually being able to do this sort of inference so you see people

18:50.480 --> 18:55.520
when you ask people people when you ask i remember doing this experiment practically with with

18:55.520 --> 19:01.040
people and when when you ask them to late a and c to tell your story about a c they think more

19:01.040 --> 19:09.440
they really spend more time in thinking compared to a b and b c and that's and that's that's

19:09.440 --> 19:14.720
how i got inspired because you know our brain works so the same mechanism because when we did

19:14.720 --> 19:20.560
then FMI study right the same mechanism was involved it's just that the the answer was going out

19:20.560 --> 19:27.200
of the apocampus and then back into the apocampus few times and but then was processed by the same

19:28.240 --> 19:34.960
system if you like and i think that's a nice probability to have in an angry then

19:36.240 --> 19:43.280
does that mean i'm probably you know taking this too far but you know when i hear you say that i

19:43.280 --> 19:50.880
i make associations like the you know the brain knows how to do scanning and it doesn't have

19:50.880 --> 20:05.760
something like an inner join okay no the the problem there is that i what does it mean the brain

20:05.760 --> 20:11.120
no i guess from a computational perspective then i could ask you what's the loss that the brain

20:11.120 --> 20:16.320
is minimizing then to do that which i don't know i don't know the answer maybe i don't know maybe

20:16.320 --> 20:21.280
it could be uncertainty reduction because at the end of the day we want to get better prediction we

20:21.280 --> 20:27.440
want to be able to i think better predict the model the world because it's then it will be less

20:27.440 --> 20:32.400
uncertain and so less risky but i think there are people like much better than me that could explain

20:32.400 --> 20:36.800
that yeah yeah no i think you that was going in maybe a slightly different direction i was

20:36.800 --> 20:47.520
inferring from the what i heard you say was that when we ask people to do these kind of associative

20:47.520 --> 20:55.920
types of tasks or inferences you know where they need to get from a to b and b to c you know that

20:55.920 --> 21:07.120
takes longer which kind of suggests that there's not some built-in associative thing in the brain no

21:07.120 --> 21:14.880
because in the in the fmi study that we did we saw that also if we do longer inference we are

21:14.880 --> 21:20.640
still able to do that it just takes more time even more time so i think that the algorithm we

21:20.640 --> 21:27.680
applies the same which is the ability of making associations is just that the longer the jump

21:27.680 --> 21:35.440
that i ask you to do in doing this sort of associative mechanism the more you need to do you do

21:35.440 --> 21:40.560
it a bit hierarchically right you put i don't know the things that are just two steps separated you

21:40.560 --> 21:45.120
calculate that then three steps and then you might be able to put together two and three and do five

21:45.120 --> 21:53.840
so we went down this particular rat hole in trying to provide some context for ponder net which

21:55.040 --> 22:02.240
you know i'll have you explain but i still don't really see the connection with memory when i

22:02.240 --> 22:11.360
when i read the abstract for ponder net i think about things like you know hyper parameter

22:11.360 --> 22:17.840
tuning and like early stopping like the way we train networks and like pulling that into the network

22:17.840 --> 22:23.440
as opposed to anything having to do with memory and the stuff we were just talking about yeah

22:23.440 --> 22:28.800
so tell me more about the connection two way answer on about the connection the first one is the

22:28.800 --> 22:37.600
more high level maybe and wavy if you like it's generalization so we believe that mechanism like

22:37.600 --> 22:45.360
ponder net that can get you a little bit far in terms of generalization compared to not doing

22:45.360 --> 22:52.400
ponder in deep neural networks and that's the kind of things that i we discussed before right so

22:52.400 --> 22:59.120
one of the benefit of memory is you're giving you the ability to to have the right input to generalize

22:59.120 --> 23:08.640
i guess let me put it this way the second is that a previous work to ponder net was another work

23:08.640 --> 23:14.800
i did during my PhD which was called memo and was a memory of mental neural network where we did

23:14.800 --> 23:22.240
exactly we studied this mechanism of recirculation so you you the network speed out an answer

23:22.240 --> 23:27.760
you compare it with previous answer and you only give the final answer to a certain problem when

23:27.760 --> 23:33.680
you are satisfied so you though you were already like already implementing this sort of ponder

23:33.680 --> 23:42.080
mechanism but there it was really really early stage because we use basic reinforcement learning

23:42.080 --> 23:50.400
to train to train a Bernoulli variable that was basically saying go stop and we saw that

23:50.400 --> 23:57.600
as was really hard to train and very noisy in terms of variance so we decided to do something more

23:57.600 --> 24:06.160
principled and that's that's what led us to to to ponder net okay so taking a step back what's the

24:06.160 --> 24:11.600
problem that you're trying to solve with ponder net yeah i think that's the first line of the

24:11.600 --> 24:18.320
abstract which basically says that the normally neural networks so the amount of computation that we

24:18.320 --> 24:24.480
spend in neural network grows with the size of the input but not with the complexity of the problem

24:25.680 --> 24:32.960
but as we just mentioned like few minutes ago that's not how we reason right the more complex

24:32.960 --> 24:39.920
the problem the more we spend time on it and that's that's what we wanted to get a session

24:40.480 --> 24:47.200
with this this work and also we wanted to to make it fairly general such that it could be applied

24:47.200 --> 24:54.080
to any architecture that was there to be architectural and mystic got it and so the size of the input

24:54.800 --> 24:59.200
we know what that means you were talking about like feature dimensionality yeah we know the

24:59.200 --> 25:05.280
problems that come along with that complexity of the problem what exactly does that mean and how do

25:05.280 --> 25:13.920
we measure that okay so i think empirically again we have like the the example i like in the

25:13.920 --> 25:22.080
paper is that it takes more to divide than to sound so that's exactly the same problem

25:22.080 --> 25:27.920
mathematically speaking but for some reason we spend more time dividing than doing some nation

25:27.920 --> 25:39.200
it's fair to say that we're we're talking about the computational complexity of a given problem

25:39.200 --> 25:47.440
um as opposed to some conceptual type of complexity yeah yeah yeah yeah so if you have to apply

25:47.440 --> 25:54.000
the same algorithm to uh so the algorithm is the same it's just that the specific

25:54.000 --> 25:59.280
instantiation where you have to apply it requires more more compute more compute time

26:00.560 --> 26:05.120
so it's like a computer right so it's the same the same you apply the same function but in

26:05.120 --> 26:11.040
some some circumstances it takes more time so the computer thinks more that's not how neural networks

26:11.040 --> 26:18.640
work it's they imply the same amount of thinking for each input i can give you another example in

26:18.640 --> 26:25.120
a sentence for instance when we read we don't spend the same amount of time of gazing each single

26:25.120 --> 26:31.360
world in a sentence so that's been proved in psychology so we tend to focus our attention

26:31.360 --> 26:36.640
few words in the sentence that are more important to process the whole thing so that's another

26:36.640 --> 26:47.200
that's another practical example okay okay um and so you want to create a neural network that

26:48.400 --> 26:55.920
um you know would you describe it as kind of budgets it's you know computational investment

26:55.920 --> 27:01.040
in solving a problem according to the inherent complexity of the problem is that a way to think of it

27:01.040 --> 27:07.760
yeah i would say it's a fair description and so how did you do that in ponder net

27:08.720 --> 27:15.200
okay so important first of all this is based on our previous work called adaptive computation time

27:16.480 --> 27:22.960
and the the problem there is that they are they they directly minimize the number of pondering

27:22.960 --> 27:33.040
step so the number of step the the network took whereas in our case what we did was to make

27:33.040 --> 27:40.960
this probabilistic so to be to be a slightly more practical on this for each time step in the

27:40.960 --> 27:49.680
sequence we calculate the prediction the probability of alting and the next step so the probability

27:49.680 --> 27:57.760
of alting is just a Bernoulli random variable which tells you the probability of alting at this

27:57.760 --> 28:07.360
particular step given that you have not altered the previous step and then from there what we can do

28:07.360 --> 28:15.120
then is to calculate a probability distribution by basically multiplying the probability at each

28:15.120 --> 28:23.760
time step in order to form a proper distribution a proper geometric distribution once we have that

28:23.760 --> 28:32.960
what we can do is to basically wait each single so we calculated the loss for each prediction in

28:32.960 --> 28:38.880
the sequence that we made and then the loss is then weighted by the probability of having

28:38.880 --> 28:45.120
altered that particular step and that's a critical difference between us and act because in act

28:45.760 --> 28:52.320
they instead output a weighted average of the prediction so they don't output a specific

28:52.320 --> 28:59.360
prediction but a weighted average of the whole prediction and that creates a bias integrated

28:59.360 --> 29:05.440
whereas in our case we can basically just really take the exact loss at each time step

29:05.440 --> 29:12.720
given given the decision of alting and then we take the particular training time we take just

29:12.720 --> 29:19.920
the particular step that has at threshold that basically so past the threshold that we decide for

29:19.920 --> 29:25.600
alting whereas at test time we just sample from the probability that from the probability distribution

29:25.600 --> 29:35.760
that we know okay I mentioned earlier that it calls to mind you're talking about holding here

29:35.760 --> 29:43.760
calls to mind early stopping is the idea that early stopping is kind of like you know we're trying

29:43.760 --> 29:49.440
to conserve training time is the idea here that we're trying to conserve inference time like we

29:49.440 --> 29:55.680
have exactly we're trying to make a prediction yeah instead of going through the whole thing let's keep

29:55.680 --> 30:00.560
going until we're sure what the answer is and then stop early and this is an approach to getting

30:00.560 --> 30:04.560
there yeah I think our hypothesis was that you know you can let's say you want to implement an

30:04.560 --> 30:10.320
algorithm on your phone you can train it in a you know in the cloud no problem a training time

30:10.320 --> 30:17.840
like the amount of commitment for a few people no problem but then at in fact it's time you want

30:17.840 --> 30:24.080
to be quick and that's and that's and that's important but now I'm laughing a little bit but

30:24.640 --> 30:30.000
you know if done proper I think this could also reducing the amount of resources that you

30:30.000 --> 30:34.560
spend a training time because we actually have an experiment in the paper where we see that

30:34.560 --> 30:40.880
the total number of gradient updates for pondernet as more than other methods

30:40.880 --> 30:49.040
even the same final performance so these could also help reducing the amount of resources

30:49.040 --> 30:57.840
that for certain people I guess okay and then circling back the the connection to memory here

30:57.840 --> 31:05.360
is it in storing these probabilities and the Bernoulli variables that kind of stuff or is there

31:05.360 --> 31:11.040
a different connection yeah I guess the we left the connection with memory maybe a little bit

31:11.040 --> 31:19.440
behind because okay however you might think I remember one tweet that actually also inspired

31:19.440 --> 31:24.400
these work a little bit from and I can't party that was basically say one of the limitation of

31:24.400 --> 31:28.800
transformer is actually they spend the amount the same amount of compute for each token in the

31:28.800 --> 31:36.960
sequence so if we treat broadly speaking like transformer has a form of preliminary memory right

31:36.960 --> 31:42.320
you can you can think of applying these on top of transformer and see if that could help right

31:42.320 --> 31:49.280
maybe spending different amount of compute per point in the sequence got it got it got it so

31:49.280 --> 32:00.160
I understand it's a bit stretchy but yeah it's it's it's kind of it's an analogy of some sort

32:00.160 --> 32:04.400
it's not necessarily an implementation of a memory system that we're talking about here

32:05.200 --> 32:10.800
is pondernet is a specific network architecture as opposed to a technique that you could apply

32:10.800 --> 32:17.520
to different architectures or is it the latter it's a technique so it's indeed if you see in the

32:17.520 --> 32:21.920
paper the step function what we call S in the paper could be anything could be an RNN

32:21.920 --> 32:29.360
a CNN of transformer okay on a real agent whatever as long as you return so as long as you

32:29.360 --> 32:35.280
add these extra unit that calculates the probability ofality you can apply it to everything that

32:35.280 --> 32:41.200
that was important for us and indeed in the paper we do that so we applied it three different

32:41.200 --> 32:51.680
architectures okay and how did you evaluate the results so we use one task from the ACT paper

32:51.680 --> 32:59.600
called the parity task so you have a string of of 1 and 0 or 1 and minus 1 whatever and you

32:59.600 --> 33:07.440
need to calculate the parity of that string could so could be either positive odd or even right

33:07.440 --> 33:13.200
and the good thing that we could that that's a nice task because you can also train on

33:14.640 --> 33:22.640
parity up let's say in our case up to 48 integers but then test up to twice as long the length

33:22.640 --> 33:28.480
to test these a bit of extrapolate and indeed we see that our network extrapolates much better than

33:28.480 --> 33:38.000
baselines the other methods then we applied it to a reasoning task which is called a Bobby and

33:38.000 --> 33:43.760
basically you have 20 tasks which you train all in parallel it's a language task where you get

33:43.760 --> 33:50.800
ask questions actually can I can I pause you and go back to parity I'm trying yeah yeah I'm

33:50.800 --> 33:56.000
trying to work this through my head like okay so I guess the first thought that occurred to me is

33:56.000 --> 34:02.880
it was some number of bits that you're trying to calculate the parity for it's not like you're

34:02.880 --> 34:08.400
trying to end the bits together and as soon as you hit a zero or something like that you know the

34:08.400 --> 34:14.000
answer right you still you need to look at all of the bits yes and the input for parity yeah but

34:14.880 --> 34:23.600
there's the premises that independent of that particular fact inside the network

34:23.600 --> 34:30.240
you can still stop early relative to going through some number of computations

34:31.680 --> 34:37.440
and still answering the right question the question correctly we have a baseline we pick this

34:37.440 --> 34:42.640
task exactly because we know this is a task that like a normal RNN is having trouble

34:42.640 --> 34:54.240
okay doing got it that's a that's a kind of a well-known issue in this sort of literature okay got

34:54.240 --> 35:04.240
it yeah on the topic of transformers which have come up a couple times you have you did a workshop

35:04.240 --> 35:14.400
at ICML talking about transformers and reinforcement learning can you talk about taking two topics

35:14.400 --> 35:20.000
that people are really excited about and combining them together tell us a little bit about the

35:20.000 --> 35:28.160
goals of that work so the I think I'm really got inspired by the birth work where they have

35:28.160 --> 35:39.280
their these known causal masking and you know in in RL we we keep using LSTM essentially for doing

35:39.280 --> 35:44.880
most of the task but we know that they suffer from what it's called emergency bias so they tend

35:44.880 --> 35:51.680
to pay attention only to the last bit of the sequence that they are trained on which to be honest

35:51.680 --> 35:59.200
for most of the environment that we we use nowadays it's fine but if you grow in size and the

35:59.200 --> 36:05.600
context you want to pay attention to is quite long then they start to suffer so one option

36:06.720 --> 36:12.480
would be to use transformer because we know that they can end a long term the long context longer

36:12.480 --> 36:21.360
context than LSTM however the problem in RL is that the reward are sparser most of the time and the

36:21.360 --> 36:28.400
gradient being shown to be noisy so it's difficult to train so many ways so what we did in that work

36:28.400 --> 36:35.840
was basically to to generalize the birth training to which you know is done on token so those are

36:35.840 --> 36:42.400
like categorical numbers so that you can apply softmax on the other side we generalize

36:42.400 --> 36:51.360
the the birth masking to real value numbers input so to basically features so we send the

36:51.360 --> 36:58.160
features from the CNN into the into the transformer we mask some of them and then on the opposite side

36:58.160 --> 37:06.720
we basically use a contrastive approach to reconstruct the the masking so we give

37:06.720 --> 37:13.200
the like some negative taken from the same sequence and and and positive and the network has to

37:13.200 --> 37:21.520
discriminate and and also what we did in that work was to combine LSTM and transformer in the

37:21.520 --> 37:26.320
same architecture and and the good thing about that is that it helps you reducing the size

37:27.200 --> 37:35.200
of the the transformer to gain in speed so because the and we let the agent actually learn when

37:35.200 --> 37:41.040
to use transformer sorry when to use a LSTM alone or when to combine transformer and the LSTM

37:42.000 --> 37:47.920
such that in some task it can basically avoid the extra complexity of the transformer and just

37:47.920 --> 37:55.520
use the LSTM whereas in other more complex tasks it will focus on using both. Got it and so in a sense

37:55.520 --> 38:02.720
you there are echoes of the pondernet paper in that you're trying to manage the computation

38:02.720 --> 38:09.280
or let the agent manage the computational investment based on an assessment of complexity.

38:09.920 --> 38:14.640
In some sense yes through the RL gradients there so that was really the agent by playing with

38:14.640 --> 38:21.200
the environment that was deciding what to use because yeah it's although I would love to actually

38:21.200 --> 38:28.480
have agents that stop and ponder that I think will be nice. What types of problems did you

38:28.480 --> 38:34.880
experiment with this paper? Three three different domains the whole of Atari suite

38:35.600 --> 38:41.040
deep mind control suite which I'm particularly proud because that's something that normally

38:41.040 --> 38:49.040
with policy methods like the one that we had there we didn't do so much work and then deep

38:49.040 --> 38:56.400
deep mind laboratory which is a suite of 30 task 3d complex task that you play all at the same

38:56.400 --> 39:01.600
time so it's also free of multi task escape that was the task we escape.

39:03.040 --> 39:11.120
And from a performance perspective what kind of results did you see was this you know promising

39:11.120 --> 39:18.640
enough to keep poking around that or was it you know really good performance that you know it's

39:18.640 --> 39:25.520
kind of challenges state of the art. First of all we always improve on the

39:25.520 --> 39:33.280
efficiency massively compared to baseline and in many domains especially in DM lab so in the

39:33.280 --> 39:41.280
deep mind laboratory we actually also got state of the art performance okay which was so then

39:41.280 --> 39:47.040
that's good because that was the more complex domain where we played so I think and I think that

39:47.040 --> 39:53.040
was kind of not an issue but you know something we could have done slightly better to focus more

39:53.040 --> 39:58.800
and more on complex domain because I think that's where this kind of method we shine so

39:59.440 --> 40:04.800
like complex architecture probably will benefit more from like complex method although I still think

40:06.240 --> 40:12.000
it's something I quite passionate about I think there's lots of stuff we can do to improve

40:12.000 --> 40:18.240
transformer and memory in general in reinforcement learning especially in relation to the

40:18.240 --> 40:25.120
length of the context that we can process that's something I think important and kind of a bottleneck

40:25.120 --> 40:36.320
in my opinion. Meaning the approach you took in this work of you know coupling the LSTM

40:36.320 --> 40:40.880
and the transformer and allowing the agent to choose sounds like you're saying you know that

40:40.880 --> 40:47.200
that's kind of a beginning place but there's a lot more a lot more to be done there. I think so

40:47.200 --> 40:52.400
I think so. As for us we have different sort of memory as I said before we have very long term

40:52.400 --> 40:58.160
memory we have shorter memory I think I'm not the first one to say these there are few papers out there

40:59.280 --> 41:08.240
already and they argue and I argue that agents should be equipped with this sort of different

41:08.240 --> 41:15.680
timescale memory. Awesome awesome. Andrea thanks so much for taking the time to share a bit about

41:15.680 --> 41:20.880
what you're working on. It's been a pleasure Sam and again thanks a lot for for

41:20.880 --> 41:50.800
inviting me. Actually I know. Absolutely thank you so much.

