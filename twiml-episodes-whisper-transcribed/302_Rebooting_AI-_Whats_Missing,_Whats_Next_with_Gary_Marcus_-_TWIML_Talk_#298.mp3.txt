Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
With less than three weeks to go, time is running out to register for the event we are all looking
forward to, Twimblecon AI platforms.
At Twimblecon, you'll hear from industry luminaries like Andrew Aing, Ubers Fran Bell
and Cruises Hussein Mahana on what leading organizations are doing to increase the efficiency
of developing machine learning and deep learning models and getting them into production.
You'll learn about the practices and platforms being put into place by companies like Airbnb,
Capital One, Comcast, Liva, SurveyMonkey, Zappos, and many more.
And you'll get to engage with fellow data science, machine learning, platform engineering,
and MLOPS practitioners and leaders to learn, share and connect on how to accelerate, automate
and scale machine learning and AI in the real world.
We are super excited that we'll get to see you all on October 1st and 2nd in San Francisco.
For more details, visit twimblecon.com and while you're there, send me a message on the
chat for information about our team discounts, and now on to the show.
Alright everyone, I am on the line with Gary Marcus.
Gary is the CEO and founder at roboss.ai, he was also the CEO and founder of the machine
learning startup Geometric Intelligence, which was acquired by Uber in 2016.
Gary is the author of five books, including his latest rebooting AI, which will be available
on the day this podcast is published.
Gary, welcome to this week machine learning and AI.
Thanks for having me.
I am really excited to jump in and chat with you about this book.
I had a chance to dig into it.
And awesome, awesome book.
Let's just jump in.
Before we really dive into talking about the book, I'd love to explore a little bit about
your background you spent quite a bit of your career at NYU as a professor of psychology
and neuroscience.
Tell us a little bit about your background and the perspective that this creates for you.
So I'm trained primarily as a cognitive scientist.
My research for many years and my PhD with Steve Pinker was all about how children learn
language and how children start to understand the world.
So I'm a developmental cognitive scientist by training.
And at the same time, I've been interested in AI since I was about eight years old when
I first learned about programming computers.
And in the last seven years or so, I've focused almost exclusively on answering the question,
what can cognitive science bring to AI?
So AI is currently dominated by certain statistical approaches that from my perspective as a cognitive
scientist, as someone who studies how humans work, seem a little weird to me.
So I don't think of children as giant data machines, but the way that AI is kind of rolling
right now, it's all about big data.
And I've been trying to see what I can contribute to AI from the perspective of cognitive science.
So when you were, when you created geometric intelligence, was that a company that really
commercialized a cognitive science based approach or was there a statistical approach involved
in your work there?
Well, geometric intelligence, which was my first company, was inspired in some ways by
a cognitive science.
It wasn't slavish to it.
So there's always this tension of, you know, if you're building airplanes, you don't want
to fly exactly like birds, because that wouldn't make any sense and who wants to flap their
wings so many times a minute.
But you also want to understand something about the dynamics of flight in my last company
and also in this company, we're trying to take some lessons from biology, in particular
from how humans think, and apply those to AI problems.
So we're not in the last company was not like trying to be neuroscientifically, to perfectly
accurate.
We're not trying to be faithful to the brain.
We're trying to take inspiration from the brain.
The last company, the broad problem that it was trying to address is how do you learn
from small amounts of data?
And that question itself in some ways comes from cognitive science.
I think machine learning is catching up to it now in the last couple of years, but
it's always been clear from cognitive science, especially from the field of language acquisition
that learning from small data is the name of the game.
Children can generalize from tiny amounts of examples.
My dissertation was about how children learn the ADD rule for forming the past tense,
which they sometimes use incorrectly.
They'll say gold or went to things like that.
They learn that from a small amount of data.
Sometimes they make mistakes and over-apply it, but they don't have gigabytes of data the
way that say the GPT system does now.
And so the last company was really focused on one particular way of solving this small
data problem.
And our, I think, most impressive results were we were beating deep learning in terms
of data efficiency.
So we could learn things with half as much data without having specific priors about
the nature of the things we were learning.
So we would take M-NIST, which is benchmarked probably a lot of your audience knows,
recognizing characters.
We could do M-NIST with half as much data without having to build in anything about the nature
of letters or anything like that.
So we were working towards a general way of doing supervised learning and maybe some
other things using less data.
And we were inspired there by humans.
We weren't necessarily doing it exactly the way the humans do.
But I think the core intellectual property is something that Zubin Garamani and I developed
and I sort of set a direction that was based on some things that made sense to me from
a cognitive science perspective and Zubin, who's brilliant mathematician, figured out
how to apply it.
And so, you know, I think a lot of our listeners, when they hear the idea of creating AI on
limited data, we'll think about things like, you know, one-shot learning, zero-shot learning.
But it sounds like your approach was very different from these or was it?
I mean, there's some interrelations and I can't say too much because Uber owns the IP
and there's NDAs and all that kind of stuff.
But I would say that zero-shot learning and one-shot learning, first of all, are names
of problems, they're not names of techniques and people use different kinds of techniques
to do them.
And they're often, I think, narrowly construed.
So, you know, there are lots of problems in the world where you have some data.
It's not the zero data, but you just don't have that much.
Something I often like to talk about is what my daughter did when she climbed through
a chair.
So, we were sitting in a Whole Foods about a year and a half ago, she was about four
and a half years old or four years old at the time.
And we sat in a chair that had a back and then a gap between the back and the base of
the chair, if you can kind of visualize that.
And she'd never seen the TV program, The Dukes of Hazard, where they climbed through
the windows.
So, she didn't have any data from like a model of people doing wacky things, sticking
their bodies through, you know, an aperture inside of a chair.
So, this was not a big data problem, or at least there wasn't a lot of directly relevant
big data.
She had data about how her body worked, the size of her body, and she probably explored
other apertures before.
She did what a lot of people might call abstract least unsupervised learning, but she didn't
use any of the techniques that we would call unsupervised learning.
So, it was unsupervised in the sense that she didn't have training examples saying this
is the right, you know, torque to apply to your torso in order to spin through the chair,
right, in the way like a reinforcement learning robot might try it a million times and get
reinforcement.
I got stuck this way.
I didn't get stuck that way and so forth.
She just did it in the space of like a minute.
And then the second time that she did it, I asked her to reenact it and I took pictures
of the second time.
I wish I had taken pictures the first time or taken a video of the second, but anyway,
you look at this sequence of pictures that I took and she actually got stuck at one point
and then she figured out how to get unstuck.
And so there was problem solving process there and there was also kind of leveraging modest
amounts of data.
She had no direct data on this problem except what she got from trying it herself in that
moment.
And then she had a bunch of background data from other kinds of problems that she had
solved.
And she knew enough, maybe not consciously, but unconsciously, about physics and how
her body moved and so forth that she could integrate all of that.
So that doesn't fall into the paradigm of zero shot learning, although you could sort
of call it a zero shot problem, but it's not like the things that people do in literature
and it doesn't fall into the one shot learning and it doesn't really fit with how people
think about unsupervised learning where they like take clusters of things or predict the
next frames in the video.
It's not really like any of those problems.
And yet it's kind of what little kids like my children do all the time.
They say, here is some challenge that I have never confronted before.
I'm going to figure it out.
It's like 80% I feel like I'm probably exaggerating, but it's a large fraction of what my kids
do is they set new challenges.
So right now my son's a little older, he's six and a half, my daughter's five now.
And they like play games all day long and they don't play existing games, they play games
that they invent.
And so they're like, well, let's pretend you can't fly anymore because you broke your
wing or whatever.
They're constantly making up assumptions and then doing problem solving relative to those
reference points.
It's just completely far away from what people are doing in AI now.
And part of the reason that Ernie Davis and I wrote this book rebooting AI is to like
reorient the field and reboot just like start over.
So we're doing great on all this supervised learning stuff where we have a ton of data,
ton of label data.
But you know, the reality is that's not really what the real world is like.
And it's certainly not like what children do as they come to understand the world.
And there's a gap right now between I think memorizing or doing something is a little
bit better than memorization and understanding.
So deep learning is like a better way of doing memorization, you can interpolate between
examples you've seen before, but it's not really about comprehension, it's not really
about like building a model of chairs and apertures and bodies and understanding how
those interrelate.
And so what Ernie and I are trying to do is to get the feel to look in a different direction
that's more about comprehension and understanding and so forth.
Going back to your question for a second, I mean, did my last company do all of that?
No, I mean, we were a small startup, we were, when we were bought, we were 15 people,
we had one very specific way of solving a supervised learning problem with less data.
There's a lot that goes into the human way approach to less data.
Another thing that goes into it that we didn't work on in the last company at all is an
ateness.
So in Chomsky's arguments, which I think are correct, is that we start with something
that constrains how we learn language.
We're not open to any possible language. We're born knowing certain things about language.
I differ from him a little bit about what those things are, but I would say we're probably
born knowing that you can concatenate symbols in order to express things.
Is it not conscious?
Is it not conscious?
Is it not conscious?
Is it not conscious?
I was about to say, it may not be conscious, but I'll tell you about an experiment that
I did, which is probably my best known result in the psychology literature.
I taught seven-month-old kids in artificial language, and I didn't tell them the rules
for the language.
It just gave them examples, two minutes, and that was enough for seven-month-old babies
to figure out the abstract grammar.
So they heard sentences like La Tata, Ganana, so they had an ABB pattern, of course we're
psychologists, we counterbalance, so others saw the AAB pattern and some saw an ABA pattern,
et cetera.
So you hear one of these grammars, and now you have to hear new sentences that are made
up.
Well, they're all made up of the same words, but some of them, or sorry, the new words,
and some of them also have a new grammar, some of the same grammar.
So you hear ABA sentences, now you get tested on ABB sentences, or not.
And what we found was that the seven-month-olds, they're not paid for their participation
and they're getting course credit for intro psych, they're just sitting there listening,
but they want to know what's going on here.
So they hear two minutes of this stuff, and they can already tell when you change to
a new grammar.
They start paying attention more.
So they habituate, they get bored by hearing the same thing over and over again, and then
they dishabituate, they show interest, when we change the grammar.
I was hoping that this was going to be teaching them Klingon, or Dothraki, although it's
probably going to be a little bit after that time.
My favorite line of all of the Star Trek movies was about hearing Shakespeare in the original
Klingon.
So subjects in our experiment heard something that was even proto-Klingon, proto-human.
In any case, they picked this up.
And then what happens at developmental psychology is if you do an experiment, you show that kids
of a certain age can do things, then if it's interesting enough, people try to extend
it in different ways, as many people have done for this.
And they also try to show that even younger kids can do it.
So somebody's actually shown using brain measures.
It's not a perfect experiment, but a pretty good one, showed that even newborns are doing
the same kind of grammatical analysis that our seven-month-olds were doing.
We show that kids were more likely to do this with speech than with musical tones and
stuff like that.
And if they could get the gist of it from speech, then they could transfer it to musical tones,
but they didn't analyze musical tones in the same way.
It's a very interesting set of results with the bottom line for what we're talking about.
It does actually suggest that some of the roots of grammar are there as early as we can
test children.
As an aside, something can be innate and not be there at birth.
So my capacity to grow a beard was innate, I didn't learn how to do it.
But it wasn't expressed until I was, I don't know, 14 or 15 years old.
So people in developmental psychology literature get confused and think if it's not there
at birth, it's not innate.
But the human brain comes out of the oven before it's fully developed, not everything
that happens afterwards is about learning.
In any case, I would say that we have innately that.
We also have innately probably a distinction about subjects and predicates, like these entities
are undergoing this change or something like that, and various other things.
So when we start getting exposed to language, we have some hooks already built in that we
can attach that to.
Now you compare that to the kind of reinforcement learning for language acquisition experiments
that deep mind has played around with over the last few years.
If I can jump in here, I think to contextualize reinforcement learning is one of the things
that many in the field are really excited about because in a lot of ways, it appears as one
of the closest things we have to learning the way children learn, right?
There's an environment, you know, there's an agent that interacts with that environment
and learns things if we can put that learning around air quotes.
It's all true, but it's missing something and that's sort of where it was going.
So I think you characterize correctly what reinforcement and deep reinforcement learnings
to do.
And at the level of abstraction that you described, it has to be right, right?
I mean, what you do is you try things in the environment and you look for feedback.
And one of the forms of feedback is sort of am I right or am I wrong?
And that's what does this lead to a good outcome or outcome?
And that's what reinforcement learning is all about.
And you know, and saying would argue that some of that happens.
Right.
And there's a lot of conversation around and how difficult it is in practice, you know,
for a variety of reasons, sample inefficiencies and how difficult it is to get the reward function
correctly.
But I think you're saying something more fundamental about the difference between
real learning and reinforcement learning.
Yeah.
You know, in the improv school, the improv school, they teach you, yes, and, right.
So yes, and you need a whole lot of other stuff to make it actually work.
So the problem right now is like a stereotypical version of this is the Atari game system that
DeepMind built, which led to their sale or was part of why Google bought them for all
that money.
And what was cool about it was they could play a lot of different games.
They could play breakout, play space invaders and something they could play at superhuman
level, and there was nothing built in about the rules of any of those games, which by
the way is not true of their go demonstrations and so forth.
But that original demo, it was really like interesting intellectual proof of concept.
However, it didn't really work that well.
In the sense that if you change the circumstances at all, it all fell apart.
So Vicarious had a really nice demo, and actually my company had a similar demo.
My last company that we never published, but it makes the same point.
So Vicarious did was they took breakout and they moved the paddle up a few pixels, and
you have like this famous video on the web of the DeepMind thing playing breakout learning
to break through the wall and people kind of talk in very cognitive language about how
the system has learned to go through the wall or whatever.
And then when Vicarious moved the paddle up three pixels suddenly performance went from
superhuman to mediocre.
As a system hadn't really learned what a ball is, what a paddle is, or what a wall is.
What it really learned was the statistical contingencies that worked on the screen in
which it had been trained.
And that's much more superficial than the human way of playing breakout, which is to learn
about the kind of physics within the game of the ball, the paddle, and the bricks.
And so going back to reinforcement learning, DeepMind's right to set it up as a reinforcement
learning problem, but they're actually wrong to build nothing at all in.
And when they want to, because they, if you start from zero, you just don't get that far.
So in fact, when they play Go, which is in some ways a hard to problem, some ways not,
they build in the rules of Go, they build in Monte Carlo tree search, which sets up the
problem is kind of if I go there, then you go there, they don't learn that stuff.
And they've been putting out all of this kind of PR around a very blank slate approach
and where there's nothing built in, but they don't actually do that when they solve
the hard problems.
And then they had this paper that really kind of aggravated me called mastering Go without
human knowledge.
And in fact, they had like a world class Go player on the team, and there's lots of ways
in which human knowledge was embedded.
I have an archive article, ARXIV, maybe you can link and show notes, taking apart all
the human knowledge that actually went in there.
But they're doing it kind of through the back door, like, hey, nobody pay attention to
the fact that we built in the rules and carefully designed how many layers through a lot of
experimentation and haven't tested it on a different size board.
What we really need to do is to think about the principle information that needs to be
built in in conjunction with reinforcement learning, so it will work.
You could say that what my daughter did was like online reinforcement learning, but she
wasn't just like, what happens if I do this small torque on this limb, she was trying
to figure out how to do this relative to a pretty rich model of her own body in the three-dimensional
geometry of the world and a knowledge about the physics of like rigid objects, you probably
would have done it differently if the back was made of string, and she had more space
and it was flexible.
And I think you said a lot of actually reasoning in the context of reinforcement learning.
Yeah, yeah.
I think it's an important word there in model.
One of the recurring themes on this show that I've talked about quite a bit that really
just kind of jumped out for me after a number of interviews was like this pendulum swing
from a world in which the way we understood the environment that we interact with this
through creating these models around physics and engineering, et cetera, now we've kind
of gone to the other end of the pendulum swing and everything is, there's a lot of focus
and excitement around statistical approaches, and there is a lot of interesting work happening
kind of at really more of an equilibrium point where folks are looking at marrying the
model-based approaches and statistical approaches, and it sounds like what you're really proposing
is something similar to the, you know, in support of AGI, artificial general intelligence
or GAI general artificial intelligence, whichever of those acronyms you prefer, do you agree
with that?
I would slightly rephrase what you just said, but basically I agree with it.
The problem right now is we're mostly focusing on narrow intelligence and deep learning is
a very good tool for that or deeper reinforcement learning for certain narrow problems, but those
tools are not good for general intelligence, whether you want to call that AGI or GAI
or what have you, but the kind of intelligence that is inherent in a flexible person for
example, you can solve problems in different ways if the assumptions change a little bit
from what you started with, and to get there, you need to do exactly what you just said,
you need to marry the modeling approach which may use, for example, symbolic techniques
from classical AI with the more statistical techniques that we have now, and if you look
at humans, that's exactly what they do.
We do some perceptual stuff that seems to be kind of driven by statistics and a lot
of experience, and we do some abstract reasoning and language and so forth that don't seem
to use the same mechanisms.
Connemons cut on that is system one versus system two, the kind of reflexive system versus
deliberative systems, the way I like to talk about those.
I think you fast thinking slow.
And I think that the AI techniques that we have right now are good, I wouldn't even
call it a thinking, but at classifying, which is a little bit like his system one, classifying
it's seen this pattern before, it looks like this other thing that I've seen, but we're
not right now as a community focusing that much on the kind of system two stuff where
you deliberate where you recognize your assumptions are wrong and you change, and that's where
the model stuff lives.
The system one stuff, but you don't have time for a model, you're just relying on kind
of memory traces effectively of things you've seen before.
The system two kind of stuff, you have to have a cognitive model.
What is going on here?
What are the causal relationships between these entities?
If I change this thing, what would happen to this other thing?
That kind of reasoning, which humans do all the time, they do it grad school, but we
also just do it in daily life, that part's not being captured and so much money is going
into the deep learning side that it's kind of perverted, I think, the mission of AI,
which was originally to go after general AI, it was much more, I think, originally sympathetic
to learning from humans.
Right now, the kind of mathematicians and cluster builders have the upper hand, like computer
cluster builders have the upper hand, because these techniques are yielding a lot of short-term
fruit, but I think it's perverting the overall direction of the field, and that's what this
book is about, is to try to correct the shape.
Not to throw that stuff overboard, we need it too.
The deep learning or some success or two, it's going to stick around, but we need to have
some focus on other things, like, how do you do causal reasoning about what happens if
I do this thing to this system, or temporal reasoning?
Where is this system going to be 20 minutes from now?
I just saw a paper showing that people are getting pretty good at using deep learning
system to predict the next clock tick, like the next, say, quarter second in a video, or
the next frame in a video, and they're pretty terrible, just bouncing balls, billiard
balls.
They're good at that, but they had no way of predicting what would happen five minutes
later.
You could look and say, well, they're going to come to rest, and they're going to be scattered
in distribution like this, and the deep learning systems couldn't do that.
They could make these very short-term predictions, essentially, by consulting a library of videos
they've seen before, interpolating over that library, but that's not the same thing as
temporal reasoning.
I tell you that I have an airplane ticket to go to California next week, you can predict
it, I'll be at the airport on Tuesday morning, or if I give you enough information, you
can make these long-term predictions that aren't about looking at frames in a video library.
They're about reasoning about abstract entities like, okay, he needs to be in California
on Tuesday, then he's probably going to have to go to the airport to do that, who probably
have to go through a security line.
You make all these inferences about what's typical and what might happen, what if I missed
my flight, and that will be the alternatives I would consider.
You can do all of this higher-level reason that's very different from predicting the
next frame of a video.
I guess one of the questions I have that's maybe a little bit of a pushback on that is,
is it just okay that we're making a lot of progress in a narrow way in AI right now?
Even in the book, you talk about curing or treating cancer and venting new materials,
addressing climate change, big AI, AGI, could have a huge impact on those areas, but we're
also having small but significant impacts in a lot of those areas today with the narrow
techniques that we have.
There's two things there.
There's two things there, okay, go ahead.
One is, I just hate leaving that much potential for human progress on the floor.
I think we could be doing better.
I think with the amount of money that's being invested right now, if it was spent a little
bit more wisely, we could make huge progress, and I think it's worth thinking about that
rather than just sort of taking the next step because it's the obvious next step.
Sometimes the obvious next step is not the efficient one.
That's one reason.
I deal with this.
I'd like to see us make as rapid progress as possible, and I think we could do that by
compensating where the ship is going a little bit.
The other side of it is we're relying a lot on these techniques right now that are
pretty dumb, and the people who made them more smart, but the techniques are dumb in
the sense that they don't have cognitive models of what's going on.
They're just relying on brute force or variations on brute force.
By using those techniques that don't have rich understanding of the world, we get in
trouble.
People are applying those techniques, for example, to driverless cars.
Within the driverless cars, a couple of days ago from when we were recording this, it
looks like a Tesla drove into a tow truck that was parked on the side of the road.
Because you mentioned this example in the book a couple of times about fire trucks and
tow trucks.
I had not heard this.
This is a thing.
I had mentioned the tow truck actually when we finished the book.
Books go to press.
They take a while.
In the book, we mentioned fire trucks and police trucks who were stopped on the side of
the road.
Then a few days ago in Moscow, a Tesla rose into a tow truck.
There's clearly a pattern there.
It's continuing pattern, even as we're recording the interview.
You want a system where at least if you have a couple of fatalities, oh, and there's tractor
trailers too, that they've run over.
We've had multiple fatalities from the running into tractor trailers, and I don't think
any of these were fatal, but now multiple accidents basically running into emergency vehicles
stopped.
Well, we want to have an AI system where you can specify an abstract language, don't
run into stopped emergency vehicles, and have the system that's smart enough to figure
that out.
We actually have our systems that need a lot of label data, and there aren't a lot of
label data of tow trucks stopped on the side of the road.
Nobody thought to get them.
Maybe they'll collect them now, but there'll be some other case.
This is what I mean by edge cases or outliers cases.
There are going to be some other kind of emergency vehicle that looks a little bit different.
A person's going to be able to reason that must be an emergency vehicle, but it's not
going to be in the data set.
The other part of the answer to your question is we're using this technique now.
These are at stake, so we have to do something.
Either we outlaw driverless cars until we figure out something better, or we work actively
right now to think about what a better approach to AI would be.
I thought this was a really well-stated and important point in the book, and that is
around this core issue of trust.
Other words around what you were just describing, and that we, in particular, in the general
public sense of the word, we don't understand the way the things we're calling AI today
are operating and don't understand the failure modes, and as a result of that, we trust
them too much, and there's a potential as you were just describing that we put them
in the situations where lives are at stake and people don't understand that they're likely
to fail in some ways.
That's right.
And the same thing's happening over and over again, so people put a lot of stock and face
recognition, and it's really not that good, so you have a lot of police departments running
around using it, and effectively adds to our profiling problems, and it's not that good.
All these things, they might be like 90% accurate, but we're using some of them in situations
where we need more.
It's fine if an advertisement recommendation system is 90% accurate, but if we use a face
recognition system in crimes and it's 90% accurate, that's actually pretty bad.
You don't want one in 10 missed calls there.
And a driverless car thing, even if it's 99% accurate, that's not nearly good enough.
And so when we're doing mission critical things with AI, that we don't understand, that's
the interpretability problem you were just referring to, and they themselves don't really
understand the world in which they're operating, that's a recipe for problems.
And that's why the whole book is really about trust, is because we are increasingly assigning
autonomy to systems that don't really understand the world, and that we don't really understand,
we are getting ourselves in a bad position.
The worst case here is AI could get shut down.
We could have a winter kind of firm without, you know, multiple AI winters, because funding
gets dried up.
Like if enough people died on one day in a driverless car thing, like Congress could like say,
you know, enough, no more AI research, and we certainly don't want that.
And so we have to make people's expectations realistic, you and I haven't talked about
hype today, but that's another theme in the book.
And we have to branch out and look more broadly in the space of possible architectures,
including a bunch of things that are really out of fashion, like symbol manipulating
classically AI, not that we should be rebuilding that stuff, we should be borrowing from it,
we should be borrowing from the old stuff, you know, something old, something new, the
right kind of marriage, as you were saying before, but we need to do that if we're going
to be counting on the machines, and we are, and we can't really, we can't put them all
in a box, we got any better.
I'd like to come back to the areas that you see as having promise in, you know, taking
us past where we are today, but before we do that, you mentioned hype, and I think that
is going to, you know, addressing the hype and kind of counter balancing it is going
to be, you know, big contribution of this book, a big part of the first several chapters
of the book is really trying to address that hype, like there are a lot of really good
examples in here of, you know, where the hype underlives the, or where the hype doesn't
live up to, or where the reality rather doesn't live up to the hype, and, you know, in particular,
you spend a lot of time talking about reading, you know, walk us through, you know, that
as an example, and, you know, some of the ways that, you know, you gave some examples
about Microsoft and Alibaba, and the squad results that, you know, were published some
years ago.
There are a bunch of examples in there.
Sure.
So I don't have the book in front of me, but basically the case that we made there is,
there were a bunch of, I mean, in that specific example was a bunch of media accounts saying
that Microsoft had just achieved a superhuman reading or a match human reading, excuse me,
in the reality, and there were even news stories that said, like, you know, humans are running
out of jobs, and like, you know, the sky is falling kind of stuff because of this result.
And what actually happened was Microsoft, it's slightly better than it had done, like,
either anyone else had done a couple weeks earlier, and they now happened to match humans
on this one test, which was called squad, but squad is not really that much of a test
of reading.
It's really a test of, like, can you underline the piece of the passage that corresponds
to a question?
And that's easy sometimes, but a lot of times, or, I mean, that's adequate sometimes,
but most of the time when we're reading, we're trying to figure out things that aren't
in the text, we're trying to go beyond the text.
So if I just want to, you know, read a story about what Donald Trump did today and say,
you know, who's the president? You can probably match the words president and Donald Trump
with a system like that.
And you can call that reading if you want, but a lot of reading is about reading between
the lines and you make inferences.
So I can say somebody walked into a store and you can figure out they're probably a human
being, you know, all kinds of things that are just, like, logically obvious or inductively
obvious to human beings all the time that go into the process of reading.
And that test just didn't happen to measure any of them.
And you would never hire that machine to, like, summarize news stories for you, or, or
to tell you, especially like the implications, even the most obvious implications that aren't
written on the page.
And reading isn't, you know, focused throughout the book.
One of the reasons that Arnie Davis and I wrote this book is we had written op-eds and
things like that, trying to make some of these points, but there's a real mismatch between
what current machines do and what you actually need to do in reading in terms of all the inferences
that you draw.
So we had some worked examples a little bit later in the book, not in the opening chapter,
where we look at a children's story and just show all the things that you figure out when
you're reading something by the woman who wrote Little House on the Prairie.
Just like a paragraph and all the inferences and no system around even cries to do that.
And we wanted to go at length in depth to try to help people better understand why the
direction, the thrust of current AI research is just nothing like what you need to actually
read.
Just wrapping up on high for a second.
One of my favorite parts of the book is just early in the first chapter, we give advice
about questions that people at home can use when they read news stories.
News stories are often like the byproduct of some press release that a big company puts
out.
And, you know, there are some very good people in the media, but there are a lot that just
kind of report the press release and so the press release is often wind up more or less
on editing media.
So we give a set of questions that you can ask.
Like did they do this on some toy problem or a bigger problem, what were the data like
to help people to be able to go and read the news and have a healthy sense of skepticism.
And this should be true for all of science.
Of course, anytime you read science, probably anything that you read in the news, you need
to be careful as we try to arm people with a set of questions to sift through the hype
and figure out like, is this the real result, you know, does this mean systems can read
or just that they passed this one test, what else do they need to do?
And for that matter was any, you know, skeptic like me, for example, consulted to give
any kind of, you know, counterview, if not, that's a sign in itself that this is, you
know, press by press release.
Yeah, I've got this particular one dog year in a note in my notes where I call the Gary's
bullshit detector.
But since I do have it in front of me, the six points were stripping away the rhetoric.
So what did the system actually do?
And so trying to get to, you know, is it actually reading or the squad results?
How general are those results, you know, is there a demo, you know, a lot of times we
see these results, either in academic papers or, you know, commercial and there's not
a demo that anyone can go see.
You know, if there's a comparison about the systems performance relative to humans, then,
you know, which humans in particular, are we talking about and how much better?
How far it is succeeding in this particular task actually take us towards building genuine
AI and then the robustness of the system, which I kind of take as like failure modes, like
where does it fall down?
Is that, is that what you meant there?
You will notice that the direct consequence of writing this book is that I founded a
company called Robust at AI, one of the things that, that, I mean, exaggerate a little bit
for comic effect, but it's also true.
Ernie Davis and I wrote this chapter on reading that I was just alluding to and then we wrote
a similar chapter on robots, like talking about the gap between, you know, a demonstration
of a robot doing a backflip and having a robot actually working your home like Rosie
the robot and the gap is just immense and the biggest problem is robustness, you can make
a lab demonstration of any one action, but you really want the robots to decide for themselves
what to do.
I've, I've started thinking about a kind of distinction between automation and autonomy.
So the field has learned of robotics has learned very well how to automate certain things if
the environment doesn't change at all, but they're not very robust.
So, you know, you can pack a million iPhones into boxes and as long as the boxes are exactly
where you expect them to, it all works out.
But if something unusual happens, then the system may have no idea what, what to do about
it and that's basically what, the problem is with the driverless cars, those are robots.
And they're fine under ordinary circumstances and then, you know, it rains or someone has
a hand-lettered sign or there's a stop-toe truck that isn't in your training set and they
don't work very well anymore.
And so the key challenge, I think, in robotics, which is a great test of how good you're
AI is, is to be robust, to get the same thing to work when the lighting is different, when
there's somebody there that you weren't expecting, who is an object that you weren't expecting,
when your map turns out to be out of date or wrong, all of these kinds of things.
This is really a measure indirectly of intelligence.
An intelligence system will recognize that its assumptions were wrong and compensate for
that fact.
A blind system just keeps doing what it's doing.
So I'm driving on the road, I don't see anything in my little test set, seems good and
I just drive right into the tow truck.
Like that's the opposite of robustness.
One of the chapters in the book is called Insights from the Human Mind.
It jumped out at me that you said mind and not brain and I was curious what was that distinction
for you and what.
Sorry, that was very deliberate.
My view is that neuroscience is a prestige field right now but hasn't come up with the
goods and psychology is the study of the mind and more generally cognitive science is
the study of my and actually already has a lot to offer that isn't really being paid
attention to.
So someday we will build better AI by understanding the wiring diagram of human brain and getting
some insights from it, but right now we don't understand that wiring diagram.
We don't understand even basic things like how does the brain do short term memory?
So if I say let's pause I'll call you back in five minutes, you'll be expecting my
call in five minutes.
You don't need a thousand trials to hammer that into your brain, but we don't know how
the brain does that.
We really don't ask all the neuroscientists, you know, how did we do that?
I'll call you back in five minutes.
How does that work?
What are the brain?
People can sort of say, well, you know, something lights up in your prefrontal cortex
and like it's so hopelessly vague.
So neuroscience is not going to be our salvation in the short term.
On the other hand, we know a lot about, for example, the dynamics of human memory from
a cognitive science perspective and we know a lot about things like goals and plans
and intentions and beliefs and desires, stuff like that.
We don't know how they work in the brain, but we obviously have them and we can build
those things into our machines.
And so the point of that chapter is to look at cognitive science and see what clues it
gives us.
I've already alluded to a couple of those like, it's okay to have an eight structure.
The machine learning world right now is so obsessed with learning.
It doesn't want to build anything in except for convolution.
Well, convolution is actually in a neat prior, your technical people will know exactly how
it works that allows you to recognize an object that's in different places in a visual
field in cognitive science.
They call that translation invariance for a very long time.
And that's a clever way of building in, convolution is a clever way of building that into a neural
network.
There's so much more of that.
But there's this like attitude and machine learning that that's cheating, you shouldn't
build anything in.
There's a crazy attitude.
Look at biology.
You know, it's spent a billion years building in the right set of genes to make us have
brains that could react with the environment in a flexible way, like let's not throw all
that away.
So that's one example of the things that are in the cognitive science chapter.
Yeah, one of my recent guests said, and in fact, I think this became the title of the interview
episode that, and he was talking about something very different.
I want to make that clear, but that, you know, AI is a systems engineering problem.
And it sounds like in a lot of ways you might agree with that that, you know, but a lot
of the pieces of the system that we need to engineer don't exist yet.
I agree with that too, although I would say that we are as a community not doing a great
job of leveraging the ones that do exist, that as a community, we're fragmented.
So there's a small kind of classical AI group that's kind of doing what it's always been
doing and not really paying that much attention to deep learning.
There's a huge deep learning community that's not paying much attention to all to classical
AI.
They're kind of smuggler like we have good equations, we don't need that stuff.
And so the divisiveness between the culture which goes back all the way to the 50s has kept
people from doing the right systems engineering.
But I also agree that we probably need to invent a whole bunch of new stuff too.
So we can do much better even with the existing tools.
And I think if we worked harder to integrate the existing tools we get a clearer idea
of where we actually need the new tools and that's part of the thinking behind my new
company with Rodney Brooks, the robust AI company.
And is this Rodney Brooks, Brooks of Mythical Manman fame?
This is Rodney Brooks best known I guess for the Roomba which he co-invented and a fantastic
set of blogs about kind of what's possible and not that he's been writing recently.
He was the chair of MIT's AI CSAIL laboratory for many years.
He's done many great things and it's fantastic that we have him in the company.
From a kind of concrete technical perspective, what are the opportunities?
And it sounds like maybe a context for this is the things that you think that we have
that we could be integrating into these systems.
So I don't want to say too much in a way to protect details of what I'm thinking about
within the company context, but broadly speaking, deep learning is part of the picture clearly.
We need to do perceptual classification.
But we also need that kind of knowledge representation, the classical AI used, in order to figure
out how to represent common sense in machine interpretable form.
There has to be a way of putting together the vast knowledge that humans have already
accumulated with the kind of experiential knowledge that deep reinforcement learning
is good at acquiring.
That has to be the focus.
And you mentioned a couple of areas in the conversation thus far, calls or reasoning,
temporal reasoning.
Are there any independent of what you're doing with the company?
Are there things that folks that are practitioners that are listening to this and
wondering, OK, how can I buy into this idea of kind of swinging the pendulum back to
the middle and doing more kind of model-based approaches?
You mentioned knowledge representation.
What's there for folks?
Where should folks be looking to really start working in this direction?
I would say they should do a couple of things first, which are to read the new book of
mine, repeating AI with Ernie Davis, because we talk about what common sense is and whether
the challenge is in representing it and so forth.
And I think we do that in a more accessible form that is in the technical literature.
I think people should look at that.
I also think they should look at Judea Pearl's very recent book, The Book of Why, on the
question of causal reasoning.
OK.
So I think those are the two best places to start.
They're not the most technical places.
Both books refer to lots of technical literature from there.
Another thing I would say is everybody in the field should know about the CYC psych system
that Doug Lennett created, which still exists, been working on it for 30 years.
Most people think it's a failure, and I think everybody who wants to work in general AI
should have a view about that system, what it can do, why it didn't entirely work.
What strikes me a lot is a lot of young people in machine learning have never even heard
of it.
It's the most sophisticated and comprehensive attempt to have a machine, understand everyday
reasoning, like how objects work, how people interact with each other, kind of intuitive
physics, intuitive economics, intuitive psychology.
And maybe it didn't work, but I think that the mountain that Doug Lennett was trying to
get across is one that we still have to get across.
I don't see how you get a reading system, for example, that really works if you can't
ask part of the system, what happens when somebody loses their wallet?
You want to know that they care, that their money isn't it, they might try to go find
it.
And you're not going to get all of that by just memorizing movies that you watch, which
is kind of what I think the field is trying to do.
So Lennett had an approach, and I think people need to use that as a mental crucible.
So when you're getting started, you should read, rebooting AI, you should read the book
of why, and you should read something about psych, you know, you can find various review
papers that Lennett has written over the years.
And like you got to start there by understanding what the problem is.
None of the three things that I just mentioned have the answers.
They just have a better cut on the questions, you know, Ernie and I don't know exactly
how to build a general purpose AI.
But I think that we are able to articulate some of the problems that need to be solved,
that are being neglected.
Fantastic.
Well Gary, thank you so much for taking the time to share with us what you're working
on.
Thanks very much for having me.
It's a really fun interview.
All right, everyone, that's our show for today.
For more information on Gary or any of our other guests, visit twimmelai.com.
Register now for Twimmelcon, AI platforms.
You definitely don't want to miss out.
As always, thanks so much for listening and catch you next time.
