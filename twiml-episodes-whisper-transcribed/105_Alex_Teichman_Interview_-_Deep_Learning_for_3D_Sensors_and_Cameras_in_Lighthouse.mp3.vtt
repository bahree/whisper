WEBVTT

00:00.000 --> 00:16.120
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:16.120 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.440
I'm your host Sam Charrington.

00:32.440 --> 00:33.840
Contest alert.

00:33.840 --> 00:38.960
This week we have a jam-packed intro, including a new contest we're launching.

00:38.960 --> 00:43.040
So please bear with me, you don't want to miss this one.

00:43.040 --> 00:46.720
First, a bit about this week's shows.

00:46.720 --> 00:51.440
As you may know, I spent a few days at CES earlier this month.

00:51.440 --> 00:56.400
While there, I spoke with a bunch of folks applying AI in the consumer electronics industry,

00:56.400 --> 01:01.040
and I'm including you in those conversations via this series of shows.

01:01.040 --> 01:05.440
Stay tuned as we explore some of the very cool ways that machine learning and AI are being

01:05.440 --> 01:08.600
used to enhance our everyday lives.

01:08.600 --> 01:13.680
This includes work being done at Anki, who built Cosmo, the cutest little computer vision-powered

01:13.680 --> 01:15.280
robot.

01:15.280 --> 01:22.120
Nighthouse, whose smart home security camera combines 3D sensing with deep learning and NLP.

01:22.120 --> 01:28.120
Intel, who's using the single-shot multi-box image detection algorithm to personalize video

01:28.120 --> 01:31.680
fees for the Ferrari Challenge North America.

01:31.680 --> 01:36.480
First beat, a company whose machine learning algorithms analyzed your heartbeat data to

01:36.480 --> 01:42.360
provide personalized insights into stress, exercise, and sleep patterns.

01:42.360 --> 01:48.400
3AI and Koito, who have partnered to bring machine learning-based adaptive driving beams

01:48.400 --> 01:52.280
or automatically adjusting high beams to the U.S.

01:52.280 --> 01:59.480
And last but not least, aerial.ai, who applies sophisticated analytics to Wi-Fi signals to

01:59.480 --> 02:05.640
enable some really interesting home automation and healthcare applications.

02:05.640 --> 02:11.000
Now, as if six amazing interviews wasn't enough, a few of these companies have been so

02:11.000 --> 02:15.520
kind as to provide us with products for you, the Twimmel community.

02:15.520 --> 02:19.360
And keeping with the theme of this series, our contest will be a little different this

02:19.360 --> 02:20.360
time.

02:20.360 --> 02:25.400
To enter, we want to hear from you about the role AI is playing in your home and personal

02:25.400 --> 02:28.640
life, and where you see it going.

02:28.640 --> 02:36.200
Just head on over to twimmelai.com slash myaicontest, fire up your webcam or smartphone camera,

02:36.200 --> 02:39.120
and tell us your story in two minutes or less.

02:39.120 --> 02:43.600
Go post the videos to YouTube, and the video with the most likes wins their choice of

02:43.600 --> 02:50.480
great prizes, including an Anki Cosmo, a lighthouse smart home camera, and more.

02:50.480 --> 02:55.040
Submissions will be taken until February 11th, and voting will remain open until February

02:55.040 --> 02:56.040
18th.

02:56.040 --> 03:02.360
Good luck.

03:02.360 --> 03:06.560
Before we dive into today's show, I'd like to thank our friends at Intel AI for their

03:06.560 --> 03:09.520
continued support of this podcast.

03:09.520 --> 03:14.760
Intel was extremely active at this year's CES, with a bunch of AI autonomous driving

03:14.760 --> 03:17.200
and VR-related announcements.

03:17.200 --> 03:21.440
One of the more interesting partnerships they announced was a collaboration with the Ferrari

03:21.440 --> 03:24.720
Challenge North America race series.

03:24.720 --> 03:29.440
Along with the folks at Ferrari Challenge, Intel AI aspires to make the race viewing experience

03:29.440 --> 03:35.360
more personalized by using deep computer vision to detect and monitor individual race

03:35.360 --> 03:40.680
cars via camera feeds, and allow viewers to choose the specific cars feeds that they'd

03:40.680 --> 03:42.920
like to watch.

03:42.920 --> 03:47.800
Look for my conversation with Intel's Andy Keller and Emil Chindicki later in this series

03:47.800 --> 03:54.480
for an in-depth discussion about this project, and be sure to visit ai.intel.com, where you'll

03:54.480 --> 03:58.080
find Andy's technical blog post on the topic.

03:58.080 --> 04:01.760
And now a bit about today's show.

04:01.760 --> 04:07.280
In this episode, I sit down with Alex Teichmann, CEO and co-founder of Lighthouse, a company

04:07.280 --> 04:11.480
taking a new approach to the in-home smart security camera.

04:11.480 --> 04:16.040
Alex and I dig into what exactly the Lighthouse product is, and all of the interesting stuff

04:16.040 --> 04:22.000
inside, including its combination of 3D sensing, computer vision, and natural language processing.

04:22.000 --> 04:26.320
We also talk about Alex's process for building the Lighthouse network architecture.

04:26.320 --> 04:30.440
The tech stack the product is based on, and some things that surprised him in their efforts

04:30.440 --> 04:33.640
to get AI into a consumer product.

04:33.640 --> 04:35.880
And now on to the show.

04:35.880 --> 04:37.880
All right, everyone.

04:37.880 --> 04:42.240
I am here at CES, and I've got the pleasure of being seated with Alex Teichmann.

04:42.240 --> 04:45.440
Alex is the CEO and co-founder of Lighthouse.

04:45.440 --> 04:47.840
Alex, welcome to this weekend machine learning and AI.

04:47.840 --> 04:48.840
Excellent.

04:48.840 --> 04:49.840
Thank you.

04:49.840 --> 04:50.840
Absolutely.

04:50.840 --> 04:52.160
Great to have you on the show.

04:52.160 --> 04:57.600
So, why don't we get started by having you tell us a little bit about your background.

04:57.600 --> 05:00.200
You've done some interesting things in the AI sphere.

05:00.200 --> 05:01.200
Oh, thanks.

05:01.200 --> 05:02.200
Thanks.

05:02.200 --> 05:03.200
Yeah.

05:03.200 --> 05:07.160
So, my background is in perception systems for self-driving cars.

05:07.160 --> 05:11.840
This is all about getting them to understand what they see in the world.

05:11.840 --> 05:16.320
What is a car, and what is a bicyclist, and what is a pedestrian, and that sort of thing.

05:16.320 --> 05:23.720
So, I joined Sebastian Thruns Lab back in 2007, right when the DARPA challenges were

05:23.720 --> 05:24.720
wrapping up.

05:24.720 --> 05:28.240
What were some of the specific things you were working on there?

05:28.240 --> 05:34.440
So, my focus was on how you use 3D sensing, lidar in particular, in that case, to do a better

05:34.440 --> 05:39.160
job of understanding what you're seeing in the world, you being a self-driving car or

05:39.160 --> 05:40.640
a computer more generally.

05:40.640 --> 05:45.680
So, this is very different from using a regular color camera to understand what you see in

05:45.680 --> 05:47.280
the world.

05:47.280 --> 05:53.480
When you have a 3D sensor, you've got the full structure to work with in real time.

05:53.480 --> 05:57.560
And that opens up a variety of different computer vision techniques, and it makes many of

05:57.560 --> 06:04.200
the very difficult computer vision subproblems quite easy, not all of them, but it makes many

06:04.200 --> 06:05.200
of them easy.

06:05.200 --> 06:06.560
Can you give us an example of that?

06:06.560 --> 06:07.560
Yeah.

06:07.560 --> 06:12.400
So, for example, segmentation and tracking of objects for which you have no computer vision

06:12.400 --> 06:18.720
model is extraordinarily difficult with regular video, but when you have a 3D sensor and when

06:18.720 --> 06:23.040
certain assumptions are met, then you could do a very good job segmenting and tracking

06:23.040 --> 06:27.240
objects, even if you have no idea what they are, if you have no semantic information whatsoever.

06:27.240 --> 06:32.720
And this is something that's made use so very heavily in the self-driving car world, where

06:32.720 --> 06:35.960
you can see that there is a physical thing in the structure of the environment, and it's

06:35.960 --> 06:36.960
moving around.

06:36.960 --> 06:40.040
You don't have to know what it is to drive safely around it.

06:40.040 --> 06:47.760
Yeah, there seems to be a, you know, there's a school of thought in and around the self-driving

06:47.760 --> 06:53.320
cars that is taking advantage of what you're describing using LIDAR and things like that,

06:53.320 --> 06:56.560
but then there's another school of thought where folks are saying LIDAR is too expensive

06:56.560 --> 07:05.360
to be on every production vehicle, and we're going to try and do things with just cameras.

07:05.360 --> 07:06.600
Any thoughts on that?

07:06.600 --> 07:07.600
It's hard.

07:07.600 --> 07:08.600
Which one?

07:08.600 --> 07:09.600
That's all of it.

07:09.600 --> 07:10.600
Yeah, no, no.

07:10.600 --> 07:15.360
Well, getting all this stuff to work with just regular color cameras, it will eventually

07:15.360 --> 07:16.360
happen, right?

07:16.360 --> 07:20.640
The information is all there, and humans do it with what essentially amounts to just a

07:20.640 --> 07:21.640
camera.

07:21.640 --> 07:26.840
It's not very effective at that range, right?

07:26.840 --> 07:31.080
We are just machines in some sense, very complex, very sophisticated machines, but we are

07:31.080 --> 07:32.080
able to do it.

07:32.080 --> 07:35.760
The information is there, and eventually we will get computers to be able to do that sort

07:35.760 --> 07:40.120
of thing, but we seem to be a long way off from that.

07:40.120 --> 07:40.960
It is quite hard.

07:40.960 --> 07:46.960
This is why virtually every self-driving car project is using LIDAR, because it makes

07:46.960 --> 07:50.440
many of those hard problems a lot easier.

07:50.440 --> 07:54.360
So, fast forward to LIDAR, what's LIDAR up to?

07:54.360 --> 08:01.560
Yeah, so the story of LIDAR, so we were talking a lot about self-driving cars here.

08:01.560 --> 08:07.480
What we're doing is basically we're taking that set of computer vision and machine perception

08:07.480 --> 08:12.840
techniques, and we're translating that from the self-driving car world into the home.

08:12.840 --> 08:18.560
That's the technology perspective on what LIDAR is, that's the machine learning perspective.

08:18.560 --> 08:24.760
From the customer perspective, LIDAR is, imagine you had a traditional home camera, but it

08:24.760 --> 08:28.680
had the intelligence of something like Alexa or Google Home.

08:28.680 --> 08:33.520
It's a new kind of interactive assistant that's based on this 3D sensing and computer vision

08:33.520 --> 08:39.600
and cameras that lets you tell it what you care about, and then it tells you when it sees

08:39.600 --> 08:41.200
those things happen.

08:41.200 --> 08:49.760
And is the application, is it security or personal virtual assistant or something beyond?

08:49.760 --> 08:50.760
It's both.

08:50.760 --> 08:51.760
Okay.

08:51.760 --> 08:52.760
Okay.

08:52.760 --> 08:56.000
So, give me an example of how I might use it.

08:56.000 --> 08:57.000
Yeah.

08:57.000 --> 09:01.440
So, one thing you can do with Lighthouse is you can say, tell me if you don't see the kids

09:01.440 --> 09:06.240
by 4 p.m. on weekdays, and you literally just say those words, that's it.

09:06.240 --> 09:08.160
It understands what you're asking for.

09:08.160 --> 09:11.400
It has a very good computer vision model for what children are, it knows what they look

09:11.400 --> 09:15.360
like, and it knows that you're asking for, you know, by 4 p.m. on weekdays, Monday

09:15.360 --> 09:20.200
through Friday, and if it doesn't see children by that time, Monday through Friday, it'll

09:20.200 --> 09:21.200
send you a notification.

09:21.200 --> 09:22.200
Okay.

09:22.200 --> 09:23.960
And if it does, then it won't bother you.

09:23.960 --> 09:24.960
Right.

09:24.960 --> 09:25.960
Interesting.

09:25.960 --> 09:32.320
A few years ago, I was, I had some crazy project that I was going to do around the house

09:32.320 --> 09:37.920
and one of the first things I started trying to figure out was presence, and this was

09:37.920 --> 09:45.840
pre-deep learning, CNNs, all that kind of stuff, and I started looking at NFC and all

09:45.840 --> 09:51.480
these other kinds of things, and it's just so obvious now that the cameras and vision

09:51.480 --> 09:54.240
is the way to do this.

09:54.240 --> 09:58.960
What are some of the challenges associated with deploying a kind of a vision appliance,

09:58.960 --> 10:02.000
I guess, in the home environment?

10:02.000 --> 10:06.560
Well, everything in computer vision is hard to celebrate, because it's new or kind of

10:06.560 --> 10:11.360
just like the dawn of artificial intelligence here, and all of these different techniques

10:11.360 --> 10:13.880
are very cutting edge.

10:13.880 --> 10:18.000
So we're really pushing the boundaries in what's possible with deep learning, and combining

10:18.000 --> 10:24.200
that intelligently with the sorts of techniques you can use with 3D sensing, in particular

10:24.200 --> 10:27.640
around segmentation and tracking.

10:27.640 --> 10:32.120
There is a lot of complexity and a lot of difficulty around building the hardware to do this, too,

10:32.120 --> 10:39.040
because this is the first 3D sensor that has a 95-degree diagonal field of view that

10:39.040 --> 10:45.600
can see how it depends on the details, but 7 to 10 meters is typical.

10:45.600 --> 10:48.040
It's quite challenging to put all that stuff together.

10:48.040 --> 10:51.760
Hardware is hard, is the phrase for a reason.

10:51.760 --> 10:52.760
Right.

10:52.760 --> 10:57.840
So is the device itself a kind of a connect-plus-a-camera?

10:57.840 --> 11:03.520
You can actually kind of think of it that way, so it uses a different underlying depth

11:03.520 --> 11:04.520
sensing technique.

11:04.520 --> 11:09.160
It actually depends on which connect you're referring to.

11:09.160 --> 11:13.240
So the original connect was structured lights, and it's kind of like stereo.

11:13.240 --> 11:17.160
There's like a texture pattern going out, a projector, and you know where that projector

11:17.160 --> 11:20.160
is, and there's an infrared camera, and you can triangulate from that.

11:20.160 --> 11:21.160
That was the original connect.

11:21.160 --> 11:25.560
It was actually what we prototyped Lighthouse on in the very beginning, and what we're

11:25.560 --> 11:29.200
using now is a time-of-flight camera, where that sends out modulated light, and then you

11:29.200 --> 11:34.400
look at the phase shift between that modulated light as it returns, and a reference signal

11:34.400 --> 11:38.760
in that phase shift, tells you how far away things are, essentially.

11:38.760 --> 11:44.600
So at every pixel in the image, not only do you see, you know, oh, it's like this shade

11:44.600 --> 11:50.680
of brown, you also see it's 3.72 meters away, you get that for the whole scene.

11:50.680 --> 11:51.680
Interesting.

11:51.680 --> 11:55.800
Trying to remember the name of this thing, there was a kickstarter that I backed.

11:55.800 --> 11:59.360
I haven't done anything with this thing yet, but it was like a mini-lightar scans, I think,

11:59.360 --> 12:00.360
was the name of it.

12:00.360 --> 12:01.360
Have you ever come across that?

12:01.360 --> 12:02.960
I haven't come across that one.

12:02.960 --> 12:05.400
Was it for scanning your face or your self?

12:05.400 --> 12:06.400
No.

12:06.400 --> 12:11.600
It was kind of, you know, for hobbyists, you could, you know, put it on a mobile robot and

12:11.600 --> 12:13.120
just experiment with it, that kind of thing.

12:13.120 --> 12:17.560
I don't think there was any, like, specific end user use case associated with it.

12:17.560 --> 12:18.560
Okay.

12:18.560 --> 12:23.640
So it was focused on kind of, I think it was just an example of, you know, the, the, you

12:23.640 --> 12:29.080
know, how to scale, light our down to something that fits in upon your hand and is relatively

12:29.080 --> 12:30.080
cheap.

12:30.080 --> 12:31.080
I see.

12:31.080 --> 12:32.080
Yeah.

12:32.080 --> 12:34.880
Yeah, I'm not familiar with that one, but, you know, both generations of the Connector,

12:34.880 --> 12:39.080
a good example, the iPhone X, there's a 3D sensor built into it and that's also a good

12:39.080 --> 12:40.080
example.

12:40.080 --> 12:43.360
They use that to make face ID actually reliable.

12:43.360 --> 12:47.560
And then, you know, self-driving cars, obviously, with all the different varieties of light

12:47.560 --> 12:53.400
that's out there, you know, maybe a little bit more detail around the, the, some more

12:53.400 --> 12:58.640
examples of kind of use cases for the device itself might be helpful.

12:58.640 --> 12:59.640
Yeah.

12:59.640 --> 13:00.640
Yeah.

13:00.640 --> 13:02.880
So I mentioned the one about, like, if the children don't come home by a particular time.

13:02.880 --> 13:07.120
And is it just children or is it like if Bobby doesn't come home or Susie doesn't come

13:07.120 --> 13:08.120
home?

13:08.120 --> 13:13.960
Like, do you, are you able to identify specific faces and associate them with, with kids or

13:13.960 --> 13:15.200
is it just children?

13:15.200 --> 13:16.200
Yeah.

13:16.200 --> 13:19.600
So you can do either, in fact, with lighthouse.

13:19.600 --> 13:23.160
So lighthouse has the ability to understand, oh, that is a child generally, it also has

13:23.160 --> 13:27.800
the ability to understand faces of specific people.

13:27.800 --> 13:32.880
And in particular, what, what that is most useful for is so you can do something like

13:32.880 --> 13:37.880
say to lighthouse, you know, hey, tell me if you see someone you don't recognize while

13:37.880 --> 13:41.280
Cindy and I are away, for example.

13:41.280 --> 13:45.760
And that lets you get at, you know, I don't know if your children bring home a new friend

13:45.760 --> 13:48.720
while you and your wife are out at work or something and you might just want to know,

13:48.720 --> 13:52.200
like, oh, who is this new person and it'll tell you about it, it'll send you a push notification

13:52.200 --> 13:53.960
when it sees that.

13:53.960 --> 13:57.560
Or if, you know, you have a dog walker or a babysitter and one day it's somebody different

13:57.560 --> 13:59.320
or somebody new is there, right?

13:59.320 --> 14:01.280
It'll proactively notify you about this.

14:01.280 --> 14:03.640
You don't have to go back and check every day, right?

14:03.640 --> 14:06.720
Because you have set up this alert with natural language.

14:06.720 --> 14:07.720
Mm-hmm.

14:07.720 --> 14:11.120
Yeah, it was just occurring to me that as you were describing these use cases that, you

14:11.120 --> 14:17.840
know, as complex as the, you know, the computer vision and the 3D sensing is there's also an

14:17.840 --> 14:24.680
NLP challenge like how do you capture, you know, the full breadth of what someone's going

14:24.680 --> 14:27.720
to want to ask this thing?

14:27.720 --> 14:33.360
Are there, you know, we've talked a bit on the podcast about some of the underlying

14:33.360 --> 14:37.200
NLP technologies and spoke with someone on the Alexa team.

14:37.200 --> 14:43.040
Like are there unique challenges associated with the way you're using NLP in the context

14:43.040 --> 14:45.040
of this device?

14:45.040 --> 14:49.760
So I wouldn't say there's necessarily, I don't know, unique research challenges on

14:49.760 --> 14:52.360
the natural language process inside.

14:52.360 --> 14:58.480
There are difficult and important engineering challenges that we need to nail on that side

14:58.480 --> 14:59.480
of things.

14:59.480 --> 15:04.280
It's the computer vision where the really, the really heavy duty, you know, research

15:04.280 --> 15:09.640
grade techniques are being deployed, at least for the current generation of Lighthouse.

15:09.640 --> 15:14.320
I mean, you can imagine, you know, Google Assistant needs to answer virtually any question

15:14.320 --> 15:15.320
you could throw away.

15:15.320 --> 15:16.320
Right.

15:16.320 --> 15:17.320
Right.

15:17.320 --> 15:19.480
With Lighthouse, there's actually a more restricted set of things.

15:19.480 --> 15:22.800
You know, if you ask Lighthouse for, you know, directions from like, you know, here or

15:22.800 --> 15:26.680
two, wherever, like we don't do that, that's not what we do.

15:26.680 --> 15:30.240
But for the set of things that we understand on the perception side, like we're, you know,

15:30.240 --> 15:33.960
it's actually we're very good at being able to answer those questions.

15:33.960 --> 15:34.960
It's a more constrained space.

15:34.960 --> 15:35.960
That makes the problem easier.

15:35.960 --> 15:36.960
There's more structure in it.

15:36.960 --> 15:37.960
Okay.

15:37.960 --> 15:42.600
And so do you, you know, when you're providing kind of the user manual for this thing, like

15:42.600 --> 15:48.160
are you telling someone these are the 10 things you could ask it or are you setting the expectation

15:48.160 --> 15:52.880
that they should just be able to ask it, things related to the kinds of stuff that it can

15:52.880 --> 15:53.880
do.

15:53.880 --> 15:54.880
Yeah.

15:54.880 --> 15:58.880
So we float rotating suggestions in front of people in the app, right?

15:58.880 --> 16:03.240
So like, so when you're in the kind of the natural language interface screen, you'll see

16:03.240 --> 16:07.080
here is, you know, here's the set of things you might consider asking some examples of

16:07.080 --> 16:11.280
these things in this category and it kind of guides you through what categories of things

16:11.280 --> 16:12.280
we understand.

16:12.280 --> 16:18.240
And that includes, you know, for object recognition, it's, you know, people and children and pets

16:18.240 --> 16:22.680
and that kind of thing for action recognition, recognize waving at the device.

16:22.680 --> 16:26.200
So you can say something like, hey, tell me if you see someone waving hello while I'm

16:26.200 --> 16:27.200
out.

16:27.200 --> 16:28.200
Okay.

16:28.200 --> 16:29.200
That kind of thing.

16:29.200 --> 16:36.280
I understand, you know, time ranges, we allow you to set up alerts for things that happen

16:36.280 --> 16:37.280
in the future.

16:37.280 --> 16:40.880
And so I'm going to kind of guide you through those different categories of what we do.

16:40.880 --> 16:42.400
Okay.

16:42.400 --> 16:48.520
And so on the computer vision side, what are kind of the key research level challenges

16:48.520 --> 16:51.040
that you're tackling?

16:51.040 --> 16:57.640
So what it is really coming down to is applying deep learning at a large scale with 3D sensors

16:57.640 --> 16:59.840
combined with color cameras.

16:59.840 --> 17:04.720
And there's, there are particular things that this set up let you do that you just can't

17:04.720 --> 17:06.560
do in any other domain.

17:06.560 --> 17:12.080
So for example, the 3D sensor lets you segment and track objects through the space without

17:12.080 --> 17:14.080
you having to have any sort of semantic understanding.

17:14.080 --> 17:16.240
You don't have to know what that thing is.

17:16.240 --> 17:18.640
You just know it's a thing and it's moving through the space.

17:18.640 --> 17:24.120
Now your unit of classification from a deep learning point of view is that segmented object

17:24.120 --> 17:26.360
track through space and time.

17:26.360 --> 17:28.640
And this enables several things.

17:28.640 --> 17:32.800
One, it's just more accurate because you have more views of an object as it's moving

17:32.800 --> 17:35.440
about and you can integrate all of that information.

17:35.440 --> 17:40.400
And two, it's a very, very natural setup for doing action recognition because you've got

17:40.400 --> 17:44.480
this thing moving through space and time and you can ask questions like is this a dog

17:44.480 --> 17:49.240
but also is this a dog jumping up on my couch or is this a person waving hello or you

17:49.240 --> 17:51.880
know, and so on.

17:51.880 --> 17:58.120
So it's a great setup for working on these kinds of very challenging computer vision problems.

17:58.120 --> 17:59.120
Okay.

17:59.120 --> 18:04.240
You've talked about kind of segmenting these objects and I'm thinking about this primarily

18:04.240 --> 18:06.640
being driven by the 3D sensor.

18:06.640 --> 18:12.720
In what ways does having the camera augment what you're able to do beyond just the 3D point

18:12.720 --> 18:13.720
cloud?

18:13.720 --> 18:20.320
Oh, well, so at deep learning time, it's a specialized architecture that's using both

18:20.320 --> 18:21.800
of those channels.

18:21.800 --> 18:27.880
So the almost the attentional mechanism, if you want to, if you want to call it that,

18:27.880 --> 18:29.960
that's primarily driven by the 3D sensor.

18:29.960 --> 18:33.600
But then once you're kind of analyzing what is this thing, now we use everything we

18:33.600 --> 18:34.600
have.

18:34.600 --> 18:35.600
Okay.

18:35.600 --> 18:39.600
And that is including the 3D sensor data, the point cloud of the object.

18:39.600 --> 18:43.440
As well as the color camera data, we combine these things in a deep learning architecture

18:43.440 --> 18:48.640
that uses both of those and then, you know, merges them and then goes into an LSTM

18:48.640 --> 18:54.600
for doing like, you know, understanding of what is happening over time, right?

18:54.600 --> 18:55.600
Okay.

18:55.600 --> 19:01.720
And so how do you, what was the process for kind of coming up with the network architecture

19:01.720 --> 19:03.480
for this thing?

19:03.480 --> 19:10.400
Did you start with something off the shelf like inception or, you know, name your network

19:10.400 --> 19:12.440
architecture or did you build it up from the ground up?

19:12.440 --> 19:13.440
Yeah.

19:13.440 --> 19:17.640
So, I mean, in this kind of context, it always makes sense to start from a baseline.

19:17.640 --> 19:21.760
It's reasonably easy to just, you know, pull a thing out of the box and deploy it, see

19:21.760 --> 19:22.760
what happens, right?

19:22.760 --> 19:25.840
And so we did that with, you know, Google and that 100 years ago just to see what would

19:25.840 --> 19:27.320
occur.

19:27.320 --> 19:30.480
And yeah, it was, you know, it did something, it was good.

19:30.480 --> 19:34.440
But it was pretty clear that we need to customize this thing to get the level of accuracy

19:34.440 --> 19:35.520
that we really want.

19:35.520 --> 19:36.520
Yeah.

19:36.520 --> 19:42.560
And then the process from there is, well, are you familiar with the phrase graduate student

19:42.560 --> 19:43.560
descent?

19:43.560 --> 19:44.560
Sure.

19:44.560 --> 19:45.560
Okay.

19:45.560 --> 19:46.560
Yes.

19:46.560 --> 19:55.400
But, I mean, it really, it's, you know, it's intuition combined with significant perseverance

19:55.400 --> 19:57.200
combined with lots of compute, right?

19:57.200 --> 20:02.800
Yeah, I think the current way of saying that post-Nips 2017 is alchemy.

20:02.800 --> 20:03.800
Yes.

20:03.800 --> 20:05.800
There's a lot of that.

20:05.800 --> 20:10.080
I mean, it's kind of sad, actually, in that like a lot of my, a lot of my PhD work was

20:10.080 --> 20:13.360
kind of like during the age when, you know, proper machine learning techniques should

20:13.360 --> 20:17.160
be, you know, convex and just like, yeah, it's a descent method, like you're always going

20:17.160 --> 20:20.000
downhill and just like roll to the bottom and you'll find the solution.

20:20.000 --> 20:21.000
It'll be great.

20:21.000 --> 20:22.000
Right.

20:22.000 --> 20:23.000
Right.

20:23.000 --> 20:25.600
And now it's just, it's non-convex and just like, maybe it's working and maybe it's not

20:25.600 --> 20:26.600
working.

20:26.600 --> 20:29.720
And, you know, oh, I don't know, try a, try a different momentum term and like maybe

20:29.720 --> 20:30.720
it'll work this time.

20:30.720 --> 20:31.720
Yeah.

20:31.720 --> 20:32.720
Right.

20:32.720 --> 20:35.560
And then it has, it has challenges and advantages too, right?

20:35.560 --> 20:36.960
Like now things actually work.

20:36.960 --> 20:37.960
That's pretty cool.

20:37.960 --> 20:38.960
Mm-hmm.

20:38.960 --> 20:45.520
So folks that are trying to productize around deep neural networks, like what, I mean,

20:45.520 --> 20:50.560
I just, you know, I guess I struggle with the, the graduate student descent as the answer,

20:50.560 --> 20:51.560
right?

20:51.560 --> 20:54.520
I guess, probably we all do a little bit.

20:54.520 --> 21:01.480
Have you developed, you know, any intuition or rigor around or methodology rather around

21:01.480 --> 21:07.720
kind of the way you, you know, the way you build out network architectures for, for

21:07.720 --> 21:12.840
this problem space or even maybe another question as background is like, was the network

21:12.840 --> 21:17.800
architecture like up front work that you did and it's kind of static or is it, what, how,

21:17.800 --> 21:19.600
how rapidly does that evolve?

21:19.600 --> 21:23.840
So, so that is an ongoing effort in many different ways.

21:23.840 --> 21:31.080
So in one way, we are, you know, collecting new annotated data all the time, you know,

21:31.080 --> 21:35.840
both from our own early access testers who provide us access to their data for us to

21:35.840 --> 21:37.600
use for training purposes.

21:37.600 --> 21:41.600
And also, if there is a mistake in the field, you can, you can annotate it as such and

21:41.600 --> 21:44.560
we'll make use of it and improve the models and we have, we have a stream of annotated

21:44.560 --> 21:45.560
data coming in.

21:45.560 --> 21:50.080
And so we're always taking the same network structure and taking that new training data

21:50.080 --> 21:52.440
and turning the crank and redeploying.

21:52.440 --> 21:57.320
And that cycle, I mean, it depends on the details, but that's on the order of days, right?

21:57.320 --> 22:03.680
The new architecture deployment cycle, that's more like weeks or months as we, you know,

22:03.680 --> 22:06.520
we come up with some new idea of like, oh, what if, you know, maybe we can compress the

22:06.520 --> 22:09.840
network this way or maybe it would make a lot of sense to, you know, build out this piece

22:09.840 --> 22:13.760
of the network and then we'll go work very hard and validate that new network and find

22:13.760 --> 22:17.320
out, oh, indeed, this, you know, reduces compute time on our end and produces a better

22:17.320 --> 22:18.320
experience for the customer.

22:18.320 --> 22:19.320
Great.

22:19.320 --> 22:20.320
Let's go deploy this.

22:20.320 --> 22:23.280
You know, it's all about large scale quantitative testing.

22:23.280 --> 22:26.320
And you mentioned compressing the architecture.

22:26.320 --> 22:32.200
Are you deploying the network on the device or are you doing inference in the cloud or something

22:32.200 --> 22:33.200
like that?

22:33.200 --> 22:34.200
It's largely in the cloud.

22:34.200 --> 22:35.200
Okay.

22:35.200 --> 22:38.880
There's a variety of reasons that make sense, although I should mention it is not entirely

22:38.880 --> 22:40.200
in the cloud.

22:40.200 --> 22:44.280
It really is a distributed computer vision system to squeeze all the last, you know, bits

22:44.280 --> 22:46.240
of performance out of it that we can.

22:46.240 --> 22:48.920
You really do want it to not all run in one place.

22:48.920 --> 22:52.720
It makes sense to have some of it run the device, some run in the backend.

22:52.720 --> 22:56.200
So talk a little bit about that in more detail, like how do you, you know, what is running

22:56.200 --> 23:00.560
on the device, how do you partition, what's running on the device and what's running in

23:00.560 --> 23:01.560
the cloud?

23:01.560 --> 23:02.560
Yeah.

23:02.560 --> 23:06.240
So the device is doing the attentional mechanism.

23:06.240 --> 23:09.400
It's doing the segmentation and tracking of what is interesting and new.

23:09.400 --> 23:10.400
Okay.

23:10.400 --> 23:14.600
And then there's nuance here, but at a high level, it's doing that.

23:14.600 --> 23:20.160
So at kind of a simplistic perspective, you're not sending a bunch of frames out to the

23:20.160 --> 23:23.160
cloud if there's nothing happening.

23:23.160 --> 23:24.160
That's largely correct.

23:24.160 --> 23:25.160
Yeah.

23:25.160 --> 23:30.200
We do have to send some data once in a while, you know, one frame every, you know, a few

23:30.200 --> 23:31.200
seconds, basically.

23:31.200 --> 23:32.200
Okay.

23:32.200 --> 23:33.200
Okay.

23:33.200 --> 23:35.520
This is actually so we can present to you a beautiful summary of the day.

23:35.520 --> 23:36.520
Okay.

23:36.520 --> 23:37.520
Right.

23:37.520 --> 23:41.560
So you, we call it a smart time lapse or a daily recap where you, you know, you press one

23:41.560 --> 23:46.000
button and you get a, you know, a 10 second or one minute kind of summary of what happened

23:46.000 --> 23:49.240
during the day and it goes fast during the boring parts and it goes slow when there's

23:49.240 --> 23:50.240
something of interest to you.

23:50.240 --> 23:51.240
Oh, interesting.

23:51.240 --> 23:52.240
Yeah.

23:52.240 --> 23:53.240
Okay.

23:53.240 --> 23:56.360
But yeah, generally, we actually, we don't have to stream 30 frames per seconds because

23:56.360 --> 23:59.520
it's actually not what customers really care about.

23:59.520 --> 24:07.200
Users don't care about, you know, what were the RGB pixel values at like 3.47 AM, you

24:07.200 --> 24:11.520
know, yesterday, what they care about is, you know, did my kids come home on time and

24:11.520 --> 24:14.560
what has the dog done since I left the house because I just think it makes me feel warm

24:14.560 --> 24:17.360
and fuzzy inside or, you know, was anybody new here?

24:17.360 --> 24:18.760
Who was new here last week?

24:18.760 --> 24:19.760
Just, you know, show me these things.

24:19.760 --> 24:20.760
Yeah.

24:20.760 --> 24:21.760
That's what they care about.

24:21.760 --> 24:26.240
We're, with traditional home cameras, we're kind of a wash in, in data, but we don't

24:26.240 --> 24:31.240
have much, you know, useful information and that's what Lighthouse is all about is taking

24:31.240 --> 24:36.600
that enormous stream of data and compressing it down into just the bits that you actually

24:36.600 --> 24:37.600
care about.

24:37.600 --> 24:38.600
Interesting.

24:38.600 --> 24:44.360
One thing that I'm curious about, you know, being kind of here at CES and seeing, you

24:44.360 --> 24:49.760
know, tons of different consumer-oriented products that are trying to incorporate AI

24:49.760 --> 24:55.760
in one way or another, are there any things that you've kind of learned that were surprising

24:55.760 --> 25:00.520
about, you know, pulling AI into consumer-oriented products?

25:00.520 --> 25:01.520
Yes.

25:01.520 --> 25:08.320
I actually, when we started Lighthouse, myself and my co-founder, I thought it would be

25:08.320 --> 25:12.920
the AI problems that were the hardest across the board.

25:12.920 --> 25:14.320
And they are hard, for sure.

25:14.320 --> 25:15.840
There's no question of that.

25:15.840 --> 25:19.240
It turns out there's other hard problems that you have to solve along the way.

25:19.240 --> 25:25.440
For example, getting the UX right, like getting the UI and the interface and the user experience

25:25.440 --> 25:27.680
really right, that's really quite difficult.

25:27.680 --> 25:32.880
It's something we spend a lot of time on because what we, you know, ultimately the reason

25:32.880 --> 25:37.280
we exist is to deliver a delightful and useful experience to our customers.

25:37.280 --> 25:40.720
And we're able to do that with AI, but like that's not the only thing.

25:40.720 --> 25:41.720
Yeah.

25:41.720 --> 25:44.200
And it's actually, it can be quite hard to get those things right.

25:44.200 --> 25:50.160
Especially in, you know, breaking new ground in a new kind of interactive assistance,

25:50.160 --> 25:53.960
how does one actually, you know, build the best interface to this kind of thing?

25:53.960 --> 25:55.880
It takes a lot of work and iteration.

25:55.880 --> 26:01.720
You know, do you have kind of the lighthouse laws of effective, intelligent user experience

26:01.720 --> 26:02.720
design?

26:02.720 --> 26:08.360
Like, have you, you know, boiled, you know, what you've learned down into key ideas that

26:08.360 --> 26:13.320
you tell a new team member?

26:13.320 --> 26:17.520
You know, I'm not sure we've refined it to that point, where I could concisely communicate

26:17.520 --> 26:18.520
something.

26:18.520 --> 26:19.520
Yeah.

26:19.520 --> 26:20.520
Yeah.

26:20.520 --> 26:29.840
I've asked people this on and off for the last couple of years, I think that it strikes

26:29.840 --> 26:37.720
me that, you know, we've developed a fair amount of, you know, fair amount of methodology

26:37.720 --> 26:41.680
around traditional user experience via mobile, via the web.

26:41.680 --> 26:49.520
And it strikes me that there's, you know, some sort of rules that will evolve around designing

26:49.520 --> 26:56.520
intelligent systems, or not that's kind of too broad, but presenting intelligent experiences

26:56.520 --> 27:01.640
to consumers, but I haven't really found, you know, no one said, oh, yeah, I read this

27:01.640 --> 27:02.640
book about it.

27:02.640 --> 27:04.440
We're still too early for that.

27:04.440 --> 27:05.440
Too early for this.

27:05.440 --> 27:10.880
There is one guiding principle, actually, that is worthy of mention here, that's something

27:10.880 --> 27:14.520
that's always kind of in the back of my mind with this kind of interface.

27:14.520 --> 27:22.400
The reason it exists is to make useful information accessible to you as quickly as possible.

27:22.400 --> 27:24.840
That's the reason natural language interfaces are good.

27:24.840 --> 27:30.720
So, you know, stepping outside of lighthouse, looking at something like Alexa or Google Home,

27:30.720 --> 27:34.880
one of the reasons they're so good is because, you know, you don't have to go find your phone

27:34.880 --> 27:37.720
or pull your phone out of your pocket and like unlock it and go to this, you know, go

27:37.720 --> 27:41.440
to the right app and then play your music and say, no, like, play it on this interface

27:41.440 --> 27:44.480
and then finally, it comes out where, you know, no, you just, you know, you just yell

27:44.480 --> 27:47.600
across the room, hey, play this thing and it just works, right?

27:47.600 --> 27:50.760
And the reason that's amazing is because it saves you 10 seconds.

27:50.760 --> 27:51.760
Right.

27:51.760 --> 27:52.760
And it seems so trivial, right?

27:52.760 --> 27:53.760
But it's not.

27:53.760 --> 27:56.240
It really, really, really matters.

27:56.240 --> 28:00.920
And when you look at this from the, I don't know if you want to call it the nerd point

28:00.920 --> 28:02.560
of view, certainly says me, right?

28:02.560 --> 28:11.600
But it's all about reducing latency and increasing bandwidth in the human machine interface.

28:11.600 --> 28:16.640
That's the point of natural language is that you have a thought in your mind.

28:16.640 --> 28:18.800
There's a thing you want to do.

28:18.800 --> 28:23.560
And right now, generally, you have to translate that into, okay, I'm going to pull up my phone.

28:23.560 --> 28:26.880
I'm going to tap on these buttons to get to the right app and then I'm going to tap on

28:26.880 --> 28:30.040
some more buttons to do the thing I'm trying to do and I have to go to this menu and adjust

28:30.040 --> 28:31.040
the slider buttons.

28:31.040 --> 28:32.040
It's like, right?

28:32.040 --> 28:33.040
It's just like, it's terrible.

28:33.040 --> 28:34.320
What you should do is there's that thought in your mind.

28:34.320 --> 28:35.320
Just say the thought.

28:35.320 --> 28:36.320
Right.

28:36.320 --> 28:37.320
And it just happens.

28:37.320 --> 28:38.320
Right.

28:38.320 --> 28:39.320
That's what that is.

28:39.320 --> 28:40.320
That is just so much better.

28:40.320 --> 28:44.840
But I don't know, it's an order of magnitude improvements in latency in that interface

28:44.840 --> 28:49.080
between this intelligence and my head and this intelligence in my phone.

28:49.080 --> 28:57.520
On the NLP side of things, did you start out with any of the kind of popular cloud-based

28:57.520 --> 28:59.600
platforms for doing that kind of stuff?

28:59.600 --> 29:04.880
Like the, I forget what it's called, not x.ai, what that is, but all the cloud vendors

29:04.880 --> 29:05.880
have their own.

29:05.880 --> 29:07.640
Or did you kind of roll your own?

29:07.640 --> 29:11.240
You know, they are useful prototyping platforms.

29:11.240 --> 29:14.960
And there may even be some applications where they get you all the way.

29:14.960 --> 29:17.920
But that is not the case for Lighthouse.

29:17.920 --> 29:23.440
I mean, I can tell you that for sure because I used one of them over a weekend to produce

29:23.440 --> 29:25.680
a little demo of like, hey, this is what I have in mind.

29:25.680 --> 29:30.880
I think this might be a way to really nail the user interface for this thing.

29:30.880 --> 29:34.920
By the way, actually, I mean, when we started Lighthouse, we knew the direction to go

29:34.920 --> 29:39.520
into to solve the perception problems, but we didn't know how to solve the UX problems.

29:39.520 --> 29:43.760
And it was only along the way that we discovered that like, oh, my God, natural language interfaces

29:43.760 --> 29:49.160
are the way to do this, like it is actually not possible as far as we are aware to produce

29:49.160 --> 29:53.960
an interface with buttons and sliders and whatever else it might be to get you to be able

29:53.960 --> 29:57.600
to say, hey, tell me if you're seeing anyone new at the doorstep while Cindy and I are

29:57.600 --> 29:58.600
away next week.

29:58.600 --> 29:59.600
Oh, yeah.

29:59.600 --> 30:00.600
Right?

30:00.600 --> 30:01.600
Like, how would you do that?

30:01.600 --> 30:02.600
Like, you just can't, right?

30:02.600 --> 30:03.600
But with natural language, it just works.

30:03.600 --> 30:04.600
It just works.

30:04.600 --> 30:05.600
Yep.

30:05.600 --> 30:06.600
Right.

30:06.600 --> 30:10.360
I've gotten that feedback quite a lot from folks that are trying to productize NLP, like

30:10.360 --> 30:18.800
the platforms are an interesting way to start, but you run out of runway in terms of their

30:18.800 --> 30:21.640
flexibility and ability to get you all the way.

30:21.640 --> 30:22.640
Yeah.

30:22.640 --> 30:23.640
So we built all around.

30:23.640 --> 30:24.640
Okay.

30:24.640 --> 30:25.960
It's the only thing to do in this area.

30:25.960 --> 30:30.640
Can you tell me a little bit about your tech stack generally?

30:30.640 --> 30:31.640
Yeah.

30:31.640 --> 30:32.640
Yeah.

30:32.640 --> 30:33.640
Happy to.

30:33.640 --> 30:39.840
We use a lot of C++, because this is build on device and in cloud.

30:39.840 --> 30:40.840
Both.

30:40.840 --> 30:41.840
Okay.

30:41.840 --> 30:46.280
It's a real-time, performance, memory-intensive, computer vision, right?

30:46.280 --> 30:53.080
Running at scale, well, either at scale on the back end or on a limited compute device

30:53.080 --> 30:55.720
out on the front end that is touching hardware, right?

30:55.720 --> 31:00.680
And so in both of these places, C++ is the right thing to use at that level.

31:00.680 --> 31:07.080
Now when we're prototyping a new architecture for our deep learning system, it's totally

31:07.080 --> 31:13.880
reasonable to twitle around in Python to have faster iterations on that.

31:13.880 --> 31:21.120
But ultimately when it's building and deploying real systems, it ends up being C++.

31:21.120 --> 31:27.560
So did you build out the NLP platform on C++ as well, like the whole system for intense

31:27.560 --> 31:28.560
and all that kind of stuff?

31:28.560 --> 31:29.920
I'm simplifying a bit, of course.

31:29.920 --> 31:34.560
So the core computer vision system is in C++, there's a Java layer around that because

31:34.560 --> 31:38.040
that's easier to interface with your phones, for example.

31:38.040 --> 31:42.080
And it turns out that's also a good place to build your natural language processing.

31:42.080 --> 31:48.960
For whatever reason, in academia, at least my circles of academia back in my Stanford days,

31:48.960 --> 31:52.040
natural language processing was generally done in Java.

31:52.040 --> 31:56.720
And computer vision on self-driving cars, for example, was like all C++.

31:56.720 --> 32:01.400
And it probably is the case that on self-driving cars, C++ is a more natural fit because

32:01.400 --> 32:04.200
you have to interface with sensors and you have real time requirements.

32:04.200 --> 32:08.880
And it's like very heavy data, whereas natural language processing is often less.

32:08.880 --> 32:14.280
So in any case, that is a natural fit, our natural language system is lives out there.

32:14.280 --> 32:21.480
And is the Java ecosystem for the natural language stuff as mature as the Python ecosystem?

32:21.480 --> 32:22.480
Or more maybe?

32:22.480 --> 32:24.480
That's a good question.

32:24.480 --> 32:26.280
I don't think I actually know.

32:26.280 --> 32:31.760
They were from a company perspective, where are you in kind of the life cycle of bringing

32:31.760 --> 32:33.280
this product to market?

32:33.280 --> 32:36.240
We are very close to general availability.

32:36.240 --> 32:42.160
So in fact, you can go to our website right now, www.light.house and enter your email.

32:42.160 --> 32:48.400
And we will add you to our special offer lists and if you're lucky, you might get one.

32:48.400 --> 32:54.800
And if not, we will be available for anybody to buy in the not too distant future.

32:54.800 --> 32:56.440
We are quite close now.

32:56.440 --> 32:57.440
Nice.

32:57.440 --> 33:01.120
As seen pictures of the device, it's like it looks like a, it's not a mobile device.

33:01.120 --> 33:02.120
It's stationary.

33:02.120 --> 33:05.320
You put it on a countertop or something like that.

33:05.320 --> 33:09.520
You know, you either have to be very, very strategic about where you put this thing or

33:09.520 --> 33:13.400
you have to envision a world where you've got ten of these all over the place.

33:13.400 --> 33:14.840
Kind of like a Lexus becoming, right?

33:14.840 --> 33:16.960
You have one in every room or something like that.

33:16.960 --> 33:18.520
Is that the way you're thinking about the world?

33:18.520 --> 33:26.400
Like you've got, you eventually have a full 3D and, you know, three color map of everyone's

33:26.400 --> 33:28.880
home or is it something different?

33:28.880 --> 33:29.880
Yeah, actually not.

33:29.880 --> 33:33.080
I mean, maybe I'm not doing my job as like, you know, CEO of this company and like, oh,

33:33.080 --> 33:37.560
you should have one in every room or something, but I actually don't see it that way.

33:37.560 --> 33:43.600
I think several in a normal sized, you know, middle class American house, two to three is

33:43.600 --> 33:44.600
probably the right number.

33:44.600 --> 33:46.400
And you get a ton of value out of one.

33:46.400 --> 33:48.840
And so you can kind of, you know, you get one, you play with it and you're like, oh my

33:48.840 --> 33:49.840
god, this is amazing.

33:49.840 --> 33:51.480
You get two and two and three.

33:51.480 --> 33:57.760
I actually don't think it makes that much sense to have every single room covered.

33:57.760 --> 34:02.160
It's usually particular areas of interest.

34:02.160 --> 34:06.040
And so, you know, we often see the first one goes in an area that is kind of near the front

34:06.040 --> 34:07.040
door.

34:07.040 --> 34:10.040
So you see like what traffic is coming and going, but you also see a reasonable amount

34:10.040 --> 34:12.000
of the kind of the floor plan of the home.

34:12.000 --> 34:14.800
So you get a sense of what's going on there.

34:14.800 --> 34:19.880
What other common places for Lighthouse to end up in or in the garage, because often

34:19.880 --> 34:25.400
the door might get left open and you want to know if somebody's in there or you might

34:25.400 --> 34:29.840
have tools out there and children and you want to know like, you know, are the kids going

34:29.840 --> 34:32.800
out there when I'm at home, things like that.

34:32.800 --> 34:36.760
Or, you know, upstairs in the kids room or just outside of the kids room to see if they're

34:36.760 --> 34:38.240
getting up out of bed in the middle of the night.

34:38.240 --> 34:41.160
I mean, you would literally just say, you know, hey, Lighthouse, you know, tell me if you

34:41.160 --> 34:46.600
see the kids out in the hallway between, you know, 10 p.m. and 6 a.m. and then it works.

34:46.600 --> 34:51.840
We need to implement the call me if you see this features coming down the road.

34:51.840 --> 34:56.160
But it's those kind of areas of, you know, particular interests and it depends on the

34:56.160 --> 34:57.720
particular homeowner.

34:57.720 --> 35:02.360
Another common place is kind of in the living room, looking into the area where the dog

35:02.360 --> 35:06.520
hangs out so that you can, you know, just get the warm and fuzzy feelings of like, hey,

35:06.520 --> 35:08.320
what's my dog been up to since I left home?

35:08.320 --> 35:09.320
Right.

35:09.320 --> 35:16.400
So if you had this and you pointed it at the front door, can it, you know, can it effectively,

35:16.400 --> 35:24.480
you know, track the state of the home and kind of be a general purpose presence detector

35:24.480 --> 35:27.360
like, you know, keep track of someone walked in.

35:27.360 --> 35:31.120
That person walked out so they're no longer inside and at any given time, like query it

35:31.120 --> 35:33.400
and determine who's in the house.

35:33.400 --> 35:37.120
So we can make that query and that does work.

35:37.120 --> 35:39.800
But we don't do it with computer vision, actually.

35:39.800 --> 35:40.800
Okay.

35:40.800 --> 35:44.640
And the, you know, the reason is we, there's often many entrances and exits to a home

35:44.640 --> 35:49.280
and we don't expect that you buy one lighthouse for every entrance and exit, necessarily.

35:49.280 --> 35:50.280
Right.

35:50.280 --> 35:54.920
So the way we do presence, absence detection is just, you know, is with phone presence

35:54.920 --> 35:55.920
and absence.

35:55.920 --> 35:59.520
GPS is part of that, but also looking at blue to the signals coming out of the devices.

35:59.520 --> 36:00.520
All right.

36:00.520 --> 36:04.520
Just going to say I just started playing with the Samsung smart things and it does it the

36:04.520 --> 36:08.360
same way and it kind of sucks like it's, it's very coarse.

36:08.360 --> 36:11.560
You have to work hard at it to get to work well, but I mean, there is a big advantage

36:11.560 --> 36:13.880
in that we have a blue to the signal coming out of the device.

36:13.880 --> 36:14.880
Ah, okay.

36:14.880 --> 36:19.200
That's going to work a lot better than, you know, the GPS you're within a mile of your

36:19.200 --> 36:20.200
house.

36:20.200 --> 36:21.200
So therefore you're in your house.

36:21.200 --> 36:26.280
If you just use location services, like as provided by the standard phone API's on

36:26.280 --> 36:29.200
its own, it would be hard to make it really good.

36:29.200 --> 36:30.200
Yeah.

36:30.200 --> 36:33.440
Now to be fair, also like we will not cover the case where you walk out of your house

36:33.440 --> 36:36.680
and you go to your neighbor's house, it's going to be hard for us to tell.

36:36.680 --> 36:37.680
Right.

36:37.680 --> 36:38.680
And like it will still think you're home.

36:38.680 --> 36:43.040
But when you get far enough away, then it, you know, this gets you almost all of the

36:43.040 --> 36:44.040
way.

36:44.040 --> 36:45.040
Right.

36:45.040 --> 36:46.560
Assuming that you've got your phone with you.

36:46.560 --> 36:47.560
That is correct.

36:47.560 --> 36:48.560
Yep.

36:48.560 --> 36:49.560
Interesting.

36:49.560 --> 36:52.600
I mean, that's one of the reasons that the children in classifier is a big deal because

36:52.600 --> 36:55.400
they often don't have phones at all.

36:55.400 --> 36:58.840
And you want to know what are they up to and did they get home on this, you know, on their

36:58.840 --> 37:01.240
schedule and so on.

37:01.240 --> 37:04.640
And so does this, does the lighthouse have an API?

37:04.640 --> 37:11.640
Is it something that you envision people kind of getting and hacking on or is it more,

37:11.640 --> 37:15.080
you know, just kind of the stated use cases as far?

37:15.080 --> 37:21.920
You know, we have seen a tremendous enthusiasm for adding lighthouse capabilities to other

37:21.920 --> 37:25.720
parts of the, the IoT world of the smart home.

37:25.720 --> 37:26.720
Right.

37:26.720 --> 37:30.440
You know, actually to this smart home, you know, device when you see something or other

37:30.440 --> 37:32.640
kind of thing.

37:32.640 --> 37:37.240
And I'm really excited to get to the point where we can actually start to tap into that.

37:37.240 --> 37:40.920
We're not there just yet, but it's certainly on the roadmap.

37:40.920 --> 37:46.280
We will be deploying something like that, some integration with other smart home capabilities

37:46.280 --> 37:50.200
that, you know, that early adopters can plug together.

37:50.200 --> 37:53.440
We will be providing that sometime this year.

37:53.440 --> 37:54.440
It will not be immediate.

37:54.440 --> 37:55.440
Yeah.

37:55.440 --> 37:56.440
Yeah.

37:56.440 --> 37:58.320
What's the long term, you know, view further company?

37:58.320 --> 37:59.600
What are you trying to accomplish?

37:59.600 --> 38:06.000
So when I take a step back and look at why lighthouse exists, the home is a piece of it,

38:06.000 --> 38:07.000
for sure.

38:07.000 --> 38:10.360
And it's a very exciting piece, but it's not the only thing.

38:10.360 --> 38:16.120
The reason lighthouse exists is to improve human life by augmenting our physical spaces

38:16.120 --> 38:19.360
with useful and accessible intelligence.

38:19.360 --> 38:22.320
And that stated very broadly, quite deliberately.

38:22.320 --> 38:28.800
Like there's sensors beyond computer, beyond cameras, beyond time of flight cameras, and

38:28.800 --> 38:33.800
you know, beyond vision generally that are very interesting and that we absolutely should

38:33.800 --> 38:37.120
integrate into this kind of thing.

38:37.120 --> 38:39.400
And it also goes beyond the home.

38:39.400 --> 38:45.800
There's many different AI service domains that are quite interesting to us.

38:45.800 --> 38:48.440
We're not spending a lot of time there right now because, you know, it's hard enough to do

38:48.440 --> 38:49.440
one of these things.

38:49.440 --> 38:53.000
So we're very, very focused on delivering the home product into the world and having

38:53.000 --> 38:57.160
that be a big success and make people's lives better.

38:57.160 --> 39:02.160
But once that is established and growing more or less on its own, then it'll be time

39:02.160 --> 39:05.280
to take our attention to another AI service domain.

39:05.280 --> 39:08.880
What's an example of another one beyond the home that's interesting?

39:08.880 --> 39:10.160
Elderly care is a big deal.

39:10.160 --> 39:11.160
Okay.

39:11.160 --> 39:16.760
It is a particularly big deal and we are particularly well suited to solve problems in that area.

39:16.760 --> 39:21.160
And we're actually, we're starting to see hints of this already, even in the home,

39:21.160 --> 39:29.160
we're aging in place in particular where you have an elderly loved one who may be there,

39:29.160 --> 39:34.440
maybe they might need to go into a facility, like a nursing care facility, but you kind

39:34.440 --> 39:38.400
of want to extend their time in their own home as long as you possibly can.

39:38.400 --> 39:45.680
And a system like Lighthouse is actually really good for this use case because they get

39:45.680 --> 39:50.080
a great security camera out of it or a camera they can see what their dog was up to or

39:50.080 --> 39:51.560
whatever it might be.

39:51.560 --> 39:59.320
And then the adult child gets the early warning system where you don't have to be looking

39:59.320 --> 40:00.320
at it every day.

40:00.320 --> 40:03.360
You just say, hey, Lighthouse, if you don't see anyone in the kitchen by 8 a.m. every day,

40:03.360 --> 40:05.080
just let me know.

40:05.080 --> 40:09.640
And then it might be that they just let in, but maybe today is a good day for you to call

40:09.640 --> 40:11.280
and just see how are you doing?

40:11.280 --> 40:12.280
Yeah.

40:12.280 --> 40:13.280
Yeah.

40:13.280 --> 40:18.880
Now, it seems like there are tons of folks kind of nibbling away at pieces of this space,

40:18.880 --> 40:22.240
like how many devices does Amazon have alone?

40:22.240 --> 40:28.600
Like they've got the key thing, which has a camera, they've got the look thing, which

40:28.600 --> 40:34.800
is your kind of fashion visual system, but you know, they seem to be very gung ho of

40:34.800 --> 40:36.720
getting cameras in your home, right?

40:36.720 --> 40:41.800
You know, how does a consumer react to all these people trying to push cameras into

40:41.800 --> 40:44.360
their houses and point clouds and all of this stuff?

40:44.360 --> 40:45.360
Yeah.

40:45.360 --> 40:52.840
So, there is a very fundamentally different perspective on the space when you're at a

40:52.840 --> 40:54.920
place like Amazon.

40:54.920 --> 41:01.440
Their goal is to magnify their marketplace, right?

41:01.440 --> 41:03.080
Like they're trying to sell things.

41:03.080 --> 41:06.840
That's why they're trying to put a camera into your house so that, oh, we can deliver more

41:06.840 --> 41:08.400
things to you, right?

41:08.400 --> 41:12.240
Or we can, you know, understand that like, oh, this scarf would look really good on you,

41:12.240 --> 41:14.360
I'll try to sell this to you, or whatever it might be, right?

41:14.360 --> 41:18.640
I mean, that's legitimately the stated purpose of that device.

41:18.640 --> 41:21.960
With Lighthouse, it's very different.

41:21.960 --> 41:26.560
We exist to provide this delightful AI service to you in return for money, and that's

41:26.560 --> 41:28.560
the end of the transaction.

41:28.560 --> 41:33.720
We're not looking to, you know, sell you a better hat or something, but, you know, taking

41:33.720 --> 41:40.000
a step back from all that, it is super interesting what's happening in the home generally,

41:40.000 --> 41:44.280
you know, at this CES in particular, there's this, you know, I, I, I, I, I, I, I, I, I, I,

41:44.280 --> 41:48.720
I almost want to describe it as an epic slug fast between, you know, Alexa and Google

41:48.720 --> 41:53.400
assistance to, to like, you know, with, like, oh, this is the AI is coming to the home in

41:53.400 --> 41:54.400
this particular form.

41:54.400 --> 41:55.400
Right.

41:55.400 --> 41:56.400
And it's really interesting.

41:56.400 --> 41:59.680
And, you know, who knows where it's going to be in a year from now, but what is very

41:59.680 --> 42:04.560
clear is that adding perception capabilities and having, then that same kind of conversational

42:04.560 --> 42:07.680
capability is super exciting, and that's going to write where Lighthouse is.

42:07.680 --> 42:08.680
Mm-hmm.

42:08.680 --> 42:12.480
Yeah, there's often this question about, like, is, you know, thing X, is it like a product

42:12.480 --> 42:17.080
or a feature, and, you know, what you're doing in a lot of ways is, like, bringing together

42:17.080 --> 42:23.600
the, you know, the vision piece, which is, you know, I guess I'm wondering, like, long

42:23.600 --> 42:28.840
term, like, does, you know, does something like Lighthouse and Alexa, do they converge?

42:28.840 --> 42:34.520
Like, do you, do you want to, if Alexa was more open, like, do you want to have to deal

42:34.520 --> 42:40.880
with the NLP, you know, or do you, you know, want the, the vision to, to kind of tack

42:40.880 --> 42:46.040
on to that or take advantage of the broader ecosystem, and I guess I'm mostly thinking

42:46.040 --> 42:50.720
about this from the perspective of a consumer, like, how many of these devices do I want

42:50.720 --> 42:56.120
in my house listening to, you know, listening to everything, and, you know, already I've got

42:56.120 --> 43:01.200
like, you know, the, the Google Home, and you have to, it has its wake word, and Alexa,

43:01.200 --> 43:04.560
I've got two different wake words in the house, it's like, it's already getting a bit

43:04.560 --> 43:05.560
maddening.

43:05.560 --> 43:06.560
Mm-hmm.

43:06.560 --> 43:07.560
Yeah.

43:07.560 --> 43:11.600
You know, with, with Lighthouse, you actually don't talk to the device itself.

43:11.600 --> 43:12.600
Oh, really?

43:12.600 --> 43:13.600
Yeah.

43:13.600 --> 43:18.360
But because usually the responses we're providing are video, and there's, there's no, there's

43:18.360 --> 43:19.360
no screen.

43:19.360 --> 43:20.360
Like talking to your device.

43:20.360 --> 43:21.360
Yeah.

43:21.360 --> 43:23.200
And, and usually you're out and about, right?

43:23.200 --> 43:24.200
That makes a lot of sense.

43:24.200 --> 43:27.040
It's usually, you're, you're at work or, you know, you're, you're on a train or something,

43:27.040 --> 43:29.960
and you're just like, you know, hey, what did the dog do since I left, or like, you

43:29.960 --> 43:32.640
know, hey, it had, you know, what did the kids do while it was out yesterday?

43:32.640 --> 43:33.640
I got it.

43:33.640 --> 43:34.640
And then you, you see the results there.

43:34.640 --> 43:35.640
Right.

43:35.640 --> 43:38.040
And it's all about delivering video, answers in video form.

43:38.040 --> 43:39.040
Mm-hmm.

43:39.040 --> 43:40.040
Right.

43:40.040 --> 43:43.720
So, so we won't be adding to that confusion about like so many different things that, that

43:43.720 --> 43:45.440
like can respond to you in the home.

43:45.440 --> 43:46.440
Mm-hmm.

43:46.440 --> 43:49.200
I don't have an answer to that problem, but I don't know, a good chat with the Alexa folks,

43:49.200 --> 43:50.200
I guess.

43:50.200 --> 43:51.200
Mm-hmm.

43:51.200 --> 43:52.200
Interesting.

43:52.200 --> 43:53.200
Interesting.

43:53.200 --> 43:54.200
All right.

43:54.200 --> 43:55.520
Well, Alex, thank you so much for taking the time to chat with me.

43:55.520 --> 44:00.040
I enjoyed learning about you, your background, Lighthouse, sounds like an interesting space,

44:00.040 --> 44:01.040
and good luck here at CES.

44:01.040 --> 44:02.040
Cool.

44:02.040 --> 44:03.040
Well, thank you so much.

44:03.040 --> 44:04.040
It's been fun.

44:04.040 --> 44:05.040
All right.

44:05.040 --> 44:08.200
All right, everyone.

44:08.200 --> 44:10.360
That's our show for today.

44:10.360 --> 44:14.760
Thanks so much for listening and for your continued feedback and support.

44:14.760 --> 44:21.360
Remember, for your chance to win in our AI at home giveaway, head on over to twimolei.com

44:21.360 --> 44:25.920
slash my AI contest for complete details.

44:25.920 --> 44:30.560
For more information on Alex, Lighthouse, or any of the topics covered in this episode,

44:30.560 --> 44:35.000
head on over to twimolei.com slash talk slash 103.

44:35.000 --> 44:39.200
Thanks once again to Intel AI for their sponsorship of this series.

44:39.200 --> 44:43.200
To learn more about their partnership with Ferrari North America Challenge and the other

44:43.200 --> 44:47.800
things they've been up to, visit ai.intel.com.

44:47.800 --> 44:52.880
Of course, we'd be delighted to hear from you, either via a comment on the show notes page

44:52.880 --> 44:59.320
or via Twitter directly to me at at Sam Sharrington or to the show at at twimolei.

44:59.320 --> 45:06.320
Thanks once again for listening and catch you next time.

