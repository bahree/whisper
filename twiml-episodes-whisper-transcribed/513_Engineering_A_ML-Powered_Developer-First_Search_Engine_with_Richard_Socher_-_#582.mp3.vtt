WEBVTT

00:00.000 --> 00:10.560
All right, everyone. Welcome to another episode of the Twimble AI podcast. I am, of course,

00:10.560 --> 00:15.840
your host, Sam Charrington. And today I'm joined by Richard Socher. Richard is co-founder

00:15.840 --> 00:22.040
and CEO at U.com. Before we get into our conversation today, please be sure to take a moment

00:22.040 --> 00:27.440
to head over to Apple Podcasts or your listening platform of choice. And if you enjoy the show,

00:27.440 --> 00:34.560
please leave us a five-star rating and review. Richard, it's been almost exactly two years since

00:34.560 --> 00:39.680
we spoke you were at Salesforce. Then welcome back to the podcast. It's great to be back, man.

00:39.680 --> 00:46.080
A lot has changed in those two years. A lot has changed. I think the last time we spoke to you were

00:46.080 --> 00:55.760
like hold up in some exotic bunker, you know, early in the pandemic. But we made that conversation

00:55.760 --> 01:00.400
work. And I'm super excited to get caught up with you. You've been a busy guy for the past couple

01:00.400 --> 01:08.480
of years. It's been, yeah, busy bays. But exciting can complain. Awesome. Well, for those who

01:09.360 --> 01:14.560
don't know you, I'd love to have us start with you sharing a little bit about your background

01:15.280 --> 01:20.320
before we dive into what you've been up to recently. Sounds great. Yeah. Hello, everyone. I'm Richard.

01:20.320 --> 01:27.440
Where do I start originally from Germany that my PhD at Stanford in deep learning for

01:27.440 --> 01:34.480
natural language processing and computer vision. And thought it would be great to have more people

01:34.480 --> 01:41.600
use deep learning for natural language processing, which was still quite received with quite a

01:41.600 --> 01:48.240
lot of skepticism back then. And so I started teaching at Stanford deep learning for natural

01:48.240 --> 01:57.360
language processing in 2014. And well, needless to say, nowadays is kind of obvious that like that

01:57.360 --> 02:03.040
is the right technology after four years for a variety of reasons. But one being that everyone

02:03.040 --> 02:08.480
was then teaching deep learning as the main approach for natural language processing. I stopped

02:08.480 --> 02:16.080
teaching. My main job was doing a startup meta-mind enterprise AI platform. We got acquired by

02:16.080 --> 02:22.800
Salesforce where I became a chief scientist. And then build out the research group there. And

02:22.800 --> 02:32.160
eventually also a lot of the AI product engineering teams that I let. And then couldn't quite

02:32.160 --> 02:36.960
shake this idea off. I actually during the last couple of days of my PhD, I'd implemented a first

02:36.960 --> 02:42.160
little prototype of a new search engine that was going to summarize the web for people and be

02:42.160 --> 02:48.160
just much more useful and quicker in Google. At the time I thought, oh man, all my smart friends

02:48.160 --> 02:53.680
are going to Google. No one's ever complaining about it, where as far as I can hear, it's just

02:53.680 --> 02:58.400
maybe two or three days of an idea. But I couldn't quite shake the idea off over the last eight,

02:58.400 --> 03:05.760
nine years. And after many amazing wonderful years with the great teams at Salesforce, I decided I

03:05.760 --> 03:12.640
think I need to do this. I think the world needs a better search engine for a lot of macro reasons

03:12.640 --> 03:22.400
as well as sort of user reasons. And so I decided in the summer of 2020 to start this with Brian

03:22.400 --> 03:31.200
McCann, one of my amazing collaborators and co-workers, both at Stanford and at Salesforce. And yeah,

03:31.200 --> 03:37.520
couldn't be happier with Brian and the team we've built at U.com. When you talk about the meta

03:37.520 --> 03:43.600
and user reasons for a new search engine, what exactly does that mean? What's the motivation for

03:43.600 --> 03:49.840
U.com? Yeah, so, and the high level reason, it's kind of crazy that the entire economy is moving

03:49.840 --> 03:55.840
online. And you have the single gatekeeper in the beginning of most people's online journey that

03:55.840 --> 04:01.520
mostly wants to sell you to the highest bidding advertiser and you and your queries. At the same time,

04:02.320 --> 04:07.520
we are in an information age and there's information overload. 20 years ago, it's hard to get access

04:07.520 --> 04:13.520
to information. But nowadays, it's actually almost too easy to get access to a lot of not that

04:13.520 --> 04:18.400
useful information and you need AI to help you deal with this flux of information, help you

04:18.400 --> 04:23.920
summarize all the things that are going on and get quickly to what you want to actually achieve

04:23.920 --> 04:28.720
and do. And you give that intent usually to a search engine, but now 60 percent of all Google

04:28.720 --> 04:33.680
queries are zero clicks, meaning they don't leave the Google ecosystem anymore. They stay within

04:33.680 --> 04:40.160
YouTube, the maps. And they try to suck you into these engagement loops rather than trying to be

04:40.160 --> 04:45.760
as useful as they can, summarize and then get you on your way, either somewhere else onto the internet

04:45.760 --> 04:51.280
or just that you execute on the intent that you have. So if I search for

04:51.280 --> 04:59.040
how to do a sort array by value or something in Python, I just want the code snippet. And that's

04:59.040 --> 05:02.240
what, you know, in your dot com, one of the many, many features that we just give you. There's a

05:02.240 --> 05:06.080
code snippet and a copy and paste button because we know that that's probably what you want. Instead

05:06.080 --> 05:10.000
of a list of 10 links and you go there and you don't have good string matching and whatnot.

05:10.000 --> 05:16.080
So those are just just one of many examples that's helpful for the user, connect to the macro. Then

05:16.080 --> 05:23.200
there's sort of outside of AI and machine learning reasons. And just that when every company has

05:23.200 --> 05:29.520
to pay this tax to exist on that first page, which is, you know, by paying for ads, it creates

05:29.520 --> 05:35.280
some really odd incentives that we've, that I've observed now, multiple people kind of tell me

05:35.280 --> 05:42.160
a story where they got organically up in the Google ranker. And then, you know, they make,

05:42.160 --> 05:48.640
start to make millions of dollars because the content was just good. And so Google ads team comes

05:48.640 --> 05:52.880
over to sales teams, say, hey, do you want to, you know, buy some ads to get even more traffic?

05:52.880 --> 05:55.760
And you're like, no, we're good. We're getting so much traffic. They basically

05:57.040 --> 06:03.680
disappeared and went to page nine. And then I'm like, sorry, yeah, we'll buy the ads. And then

06:03.680 --> 06:10.160
magically they come back to page one with the ads too. That's kind of one of the, one of the many

06:10.160 --> 06:16.240
reasons to do it. And then in some crazy way, also now, people are actually complaining more and

06:16.240 --> 06:23.200
more about just the ranking and the relevance. You have all these SEOed microsites that kind of look

06:23.200 --> 06:28.800
at try to reverse engineer the algorithm that Google decides for everyone what to see and read and

06:28.800 --> 06:35.040
consume and buy. And, and they're trying to reverse engineer it. And so you get all these like

06:35.040 --> 06:39.520
really odd microsites that have a bunch of sort of language model samples on there that are

06:39.520 --> 06:47.120
known to resonate well. You look for like good machining tools for like building a roof or something.

06:47.120 --> 06:51.040
And at the bottom, there's some weird Wikipedia article about California on the page that comes

06:51.040 --> 06:55.200
up highest on Google. It's because they know that stuff ranks well in the algorithm. So there's

06:55.200 --> 06:59.920
all this reverse engineering. And I think part of the problem there is that, and it's partially

06:59.920 --> 07:06.880
an AI problem, but it's also a systemic problem and how you approach AI is that Google decides

07:06.880 --> 07:11.760
and wants to be able to decide for everyone what they read, consume and buy, because they have

07:11.760 --> 07:17.520
so much power by then showing you mostly ads, which is also just becoming 90% of any page of the

07:17.520 --> 07:21.440
results that, you know, when you actually want to buy something and have them monetize. It's got

07:21.440 --> 07:30.160
very bad experience. It's degraded surprisingly over the years. Yeah. And so I think it's important

07:30.160 --> 07:39.440
to help people still get stuff done, but not just try to do that through ads. And by giving them

07:39.440 --> 07:45.280
also some control. So we have, of course, a large neural network too to rank what we actually

07:45.280 --> 07:50.480
think about the more is the apps that you're looking for, like big content islands, like Reddit,

07:50.480 --> 07:54.400
things like that. Stuff where, you know, social signals that people actually care about rather than

07:54.400 --> 08:00.960
SEOs, microsites. And, but we actually give people control and say, Oh, I like this source.

08:00.960 --> 08:06.480
I don't like the source. And that way, if you try to manipulate this too much by playing sort of,

08:06.480 --> 08:12.000
you know, SEO games, people will just download you and you'll disappear. And so I think giving

08:12.000 --> 08:17.760
people control over the AI that influences so much of their information diet is yet another

08:17.760 --> 08:21.600
reason I could go on forever. There's so many reasons why, but yeah, I mean, it's also one of the

08:21.600 --> 08:27.360
most exciting AI applications that's so important in an information age. It tackled summarization,

08:27.360 --> 08:32.480
which is one of the big, hard, unsolved problems in AI and so on. It's just the most exciting thing.

08:32.480 --> 08:38.640
I couldn't not do it. Awesome. Awesome. But when you, you know, starting to build a search engine

08:38.640 --> 08:44.320
in 2020, do you start with something that's fundamentally similar to a page rank type of an

08:44.320 --> 08:51.600
approach or are you approaching it very differently? Yeah, it's a good question. We are actually approaching

08:51.600 --> 08:56.880
it quite differently. We have not sort of replicated the list of blue links. We're getting that

08:56.880 --> 09:02.320
from an API. But what we actually are doing is using large neural networks to understand what

09:02.320 --> 09:07.360
the intent actually is and then try to give you the most useful application. And we want to be a

09:07.360 --> 09:12.560
much more open platform for, you know, building these applications out so that different people can

09:12.560 --> 09:18.480
actually implement and contribute to that first page of the internet and make it very, very seamless.

09:18.480 --> 09:26.320
And so we have essentially relied much more on the content and the semantics. And then, of course,

09:26.320 --> 09:31.280
we can nowadays already extract a lot of popularity signals that you use to need page rank four

09:31.280 --> 09:36.560
because a lot of like recipe sites and codes and a bit sites like stack overflow. They actually have

09:36.560 --> 09:41.600
votes on how popular something is. So you can extract that. That's part of the signal together with

09:41.600 --> 09:48.160
the natural language processing signals. So on top of the kind of core search engine, you also

09:48.160 --> 09:53.040
recently announced a couple of extensions. Well, at least the code is new is right new also.

09:53.600 --> 09:58.080
Yeah, yeah, it's a lot of new stuff's coming out. You know, we're thinking about how can we be

09:58.080 --> 10:03.440
more useful? And of course, if you build a search engine nowadays, the really tricky balance is

10:03.440 --> 10:09.440
how many of your resources do you spend on just catching up to Google versus how many resources

10:09.440 --> 10:13.520
do you spend on doing something that's different that Google doesn't yet do, right? And I'm sure

10:13.520 --> 10:18.080
in the beginning, when people saw Google flights, they're like, oh, why does a search engine

10:18.080 --> 10:21.920
help you book flights? And we get some of the things now when we write an essay for you, like you

10:21.920 --> 10:27.360
search how to write an essay about World War Two or the American Revolution and this and that,

10:28.160 --> 10:34.000
it'll just write you an essay and you can modify it and then generate new ones and it helps you

10:34.000 --> 10:38.800
with blank page problems. I'm like, okay, how do I start? You can run and write a blog post about,

10:38.800 --> 10:45.920
you know, your Airbnb project or whatever it is. And so we think like that is something unique

10:45.920 --> 10:52.160
that AI can also bring. We have these exciting large language model applications now twice

10:52.160 --> 11:00.320
in the search engine. One is in code completion. So if you look for, you know, how to do something in

11:00.320 --> 11:07.520
Python or other languages, the code complete app comes up and it's just a full on like GitHub

11:07.520 --> 11:13.360
co-pilot-like model that essentially just auto-generates the code that you're looking for. And then,

11:13.360 --> 11:16.560
of course, you have, you know, stack overflow apps with copy and paste buttons. We have simple

11:16.560 --> 11:21.680
tutorials and, you know, to sort of get you started with things and you have hugging face and

11:21.680 --> 11:28.880
pytorge kinds of official documentation to and it's all just right within their first page with

11:28.880 --> 11:34.560
code snippets and copy and paste buttons to be very easy. And then on the writing natural language,

11:34.560 --> 11:40.240
probably like natural language side, we have this you write app under you.com search right where

11:40.240 --> 11:46.800
you basically can have an API just write an essay or blog post for you. Got it. Got it. So kind

11:46.800 --> 11:52.880
of stepping back, you've got AI throughout this platform. You've described a couple of applications

11:52.880 --> 11:58.720
that are, you know, I don't know if you think of them as part of the search engine or kind of a

11:58.720 --> 12:05.040
Jason or on top of the search engine, but then you're using AI as part of surfacing relevant

12:05.040 --> 12:11.600
results to folks. You know, thinking about that that core, maybe dig a little bit deeper into some

12:11.600 --> 12:18.960
of the ways that you're using machine learning to deliver relevant results to folks. The large

12:18.960 --> 12:23.440
language models come into play there or is it more on the summarization side? Yeah, it's a great

12:23.440 --> 12:30.560
question. There are a bunch of summary features too. And AI is sort of both in terms of our product,

12:31.520 --> 12:36.320
sort of very deeply. And we can get into that like in 10 classifications, slot filling,

12:37.200 --> 12:46.080
ranking of different applications and so on. It's also a user persona that we know a lot about

12:46.080 --> 12:51.120
and care a lot about. And just people who program AI applications. And you know,

12:51.120 --> 12:57.600
hence we have all these specific tutorials and documentation sets for hugging face for instance

12:58.720 --> 13:06.000
for PyTorch and a bunch of other things. We have GitHub issues. We crawled all of GitHub so you

13:06.000 --> 13:11.360
can find all the different issues about your code that you're working on directly all in the

13:11.360 --> 13:16.400
search engine. When you when you're basically trying to understand a query, you want to understand

13:16.400 --> 13:21.440
like what programming language someone is using or we like if you say I want directions from

13:21.440 --> 13:26.160
San Francisco to LA. It's kind of a standard example. That's a good one for slot filling where you

13:26.160 --> 13:32.720
want to extract sort of certain things from the query directly and input them into an app. And then

13:32.720 --> 13:40.000
basically let people immediately kind of have that filled out the slots of you know the from

13:40.000 --> 13:46.240
direction to direction. So you get the time that it takes you to drive somewhere in the directions

13:46.240 --> 13:53.280
for that. So that that is an example of slot filling that we have to do for 150 or so apps that

13:53.280 --> 13:59.440
that we have. If you look for Dow jobs or Web 3 jobs for instance, then we kind of extract that's

13:59.440 --> 14:04.320
the kind of job type you're looking for. And we have a bunch of apps that basically show you

14:04.320 --> 14:11.040
job listings for these kinds of categories. So slot filling is a is a pretty big part of the

14:11.040 --> 14:14.480
search engine. And then of course you have the intent kind of knowing this is just the weather

14:14.480 --> 14:21.200
intent. And that leads then to influencing and kind of into and training a large a large

14:21.200 --> 14:26.240
ranking model that basically ranks all these different apps in terms of their priority for

14:26.240 --> 14:35.040
helping you get something done. And so when you're are the the intense or the apps are those kind

14:35.040 --> 14:40.640
of top down you know created by you or project manager, hey we're going to need our travel app,

14:40.640 --> 14:44.880
we're going to need our directions app, we're going to need you know this list of you know hundreds

14:44.880 --> 14:51.520
of things or are they kind of bubbled up from the queries that people are making themselves.

14:51.520 --> 14:59.040
Yeah great question. You bring up two good points. One is namely privacy. We care a lot about

14:59.040 --> 15:04.480
privacy. I think it's a really important important right. And so we have a private mode in which we

15:04.480 --> 15:08.560
don't track anything so we don't know really what people are doing. We don't have the queries.

15:08.560 --> 15:13.520
But we also have a personalized mode. And so some people actually want to give us feedback

15:14.080 --> 15:19.600
and tell us what they want. So we've got we have a very active community of thousands of people

15:19.600 --> 15:24.080
that give a lot of like feature requests and you know it's kind of tricky sometimes because

15:24.080 --> 15:29.040
research touches really everything right. Everything we do often online like starts with search.

15:29.840 --> 15:36.560
And then we also yeah can look at sort of churn queries like things that you know people

15:36.560 --> 15:42.640
tried to do that they couldn't do and then they leave forever. And so sports was a good example

15:42.640 --> 15:48.480
of that that just kept bubbling up people wanting to see sports results. And so we now have

15:48.480 --> 15:54.000
actually releasing this week a bunch of you know results for sports that are live coming in from

15:54.720 --> 16:01.360
different APIs. So you can kind of see what's going on. So yeah it's a mix of direct feedback and

16:01.360 --> 16:05.680
then something direct feedback. Can you talk a little bit about the way that the ways that you're

16:05.680 --> 16:11.040
using summarization? You've referenced that a couple of times. Yeah yeah I think summarization

16:11.040 --> 16:16.880
in the limit is actually one of the hardest and most interesting and impactful and LP applications

16:16.880 --> 16:23.040
of our time right now because we're in the an attention economy and in the information age we need

16:23.040 --> 16:29.280
better summarization. But really if you think about it if you're deeply engrained in a space and

16:29.280 --> 16:34.000
you only got a summary for a new paper it's very different to when you've never read anything in

16:34.000 --> 16:40.800
that in that space right. I sort of because of progenous protein generation model I've been working

16:40.800 --> 16:48.640
on at Salesforce. Now I've been reading a lot more bio papers and like there's just so much

16:48.640 --> 16:54.560
lingo that you don't know that if you were to try to summarize and make it even shorter you

16:54.560 --> 16:59.680
would not understand anything. And so the summary might need to simplify things but also add some

16:59.680 --> 17:06.480
explanations for some things. And then you think about things where you're an expert like

17:06.480 --> 17:12.320
oh the Elmo paper was basically like the contextual vector code paper but instead of translation

17:12.320 --> 17:17.280
they use language modeling. And then Bert was essentially like Elmo but instead of you know

17:17.920 --> 17:23.120
an LSTM model they use a transformer model. And so that's like a one cent in summary if you know

17:23.120 --> 17:27.520
exactly what all of these things mean already you're like boom I get it and you know their models

17:27.520 --> 17:33.120
are larger is probably also a good add-on to that summary. But if you don't know what a language

17:33.120 --> 17:39.760
model is what a word vector is what you know what a neural network is on an LSTM and Bert and

17:39.760 --> 17:44.880
all of these things are then these are non-useful summaries for you. So long story short summary

17:44.880 --> 17:51.600
super interesting super hard AI problem but for us you know we're to say okay well we don't know

17:51.600 --> 17:57.760
that much about what what a user knows. How can we start with something that's useful for pretty

17:57.760 --> 18:04.080
much everyone where we started is basically with coding like if you look for this here's like the

18:04.080 --> 18:09.840
most relevant code snippet that's in some ways a you know sort of multimodal if you think of

18:09.840 --> 18:15.200
programming as a different modality to natural language multimodal kind of summary. And then

18:15.200 --> 18:20.000
another one is just pros and cons. So if you look for best headphones for instance you want to

18:20.000 --> 18:26.640
just extract whether the main pros and cons of this particular headset. So we kind of extract those

18:26.640 --> 18:32.960
from professional reviews review sites you can very quickly skim a bunch of results and know

18:32.960 --> 18:37.520
what are the pros and cons of the different headphones. Another one is recipes we heard a lot of

18:37.520 --> 18:42.560
people complain about having to read the whole life story of someone if they just want to get like

18:42.560 --> 18:48.080
a lasagna or chocolate chip cookie recipes. So we extracted like here the ingredients here the 10

18:48.080 --> 18:53.280
steps to actually make the cookie and then you're done. So those are examples that you already see

18:53.280 --> 18:58.720
in the product on u.com that you can just kind of see a useful summary that pretty much for everyone

18:58.720 --> 19:05.440
is going to be universally useful. And are summaries the way that you're using them are they

19:06.000 --> 19:14.000
tending towards more generative summaries versus extractive summaries or do you use both in

19:14.000 --> 19:18.800
different places? You have to you have to kind of use both our values or trust facts and kindness

19:18.800 --> 19:24.560
and if you think about it as much as I love these language models you can't quite trust their facts

19:24.560 --> 19:29.200
yet, right? They they make stuff up, right? You can just say write me an article about how Hillary

19:29.200 --> 19:34.160
can one the election and it will write you a perfectly reasonable sounding article how that

19:34.160 --> 19:42.000
that happened, you know? And so the veracity and just like factual correctness of large language

19:42.000 --> 19:48.480
models is still you know iffy sometimes. So you can't just let them generate you might get some

19:48.480 --> 19:55.040
some pretty not so great tasting cookies if you just do that. So you have to you have to start

19:55.040 --> 20:01.200
with with some clear extractions but then you can kind of simplify things and get rid of redundancies

20:01.200 --> 20:09.520
for multi-document summarization and things like that. Your example reminded me of one of the

20:09.520 --> 20:20.320
issues that came up with Google sometime this past year where I think the you know just one of

20:20.320 --> 20:25.440
these examples I'd be typing a query what should you do if you know someone's having a seizure

20:25.440 --> 20:34.000
and like it extracted the things the list of things not to do. That's right. And you know I'm curious

20:34.000 --> 20:41.440
if you can talk about you know in the in the context of using similar technology and trying to

20:41.440 --> 20:47.600
to present information you know in a similar kind of summarized way like how do you you know

20:47.600 --> 20:52.720
when you're starting from the ground up build guard rails you know so that you you can achieve

20:52.720 --> 20:57.680
this level of trust that you're aspiring to. Yeah it's a good question. I actually think you

20:57.680 --> 21:04.640
you have to keep your users and yourself and the use cases in mind and the more important

21:04.640 --> 21:10.320
the use case is the more important it is to really have some human oversight you know I love AI

21:10.320 --> 21:18.400
I think I can change anything from cow diets to reduce methane to you know agriculture to medicine

21:18.400 --> 21:23.280
and creating new protein structures and all of these different things but when it comes to like

21:23.280 --> 21:28.400
life-threatening information you still need to have some human oversight so for instance when

21:28.400 --> 21:34.160
you when you ask how to commit suicide we just have a handcrafted message that your life matters

21:34.160 --> 21:38.720
and you know don't do it here's a suicide hotline you don't want some AI to kind of do that

21:39.520 --> 21:44.400
of course there are some interesting sort of chatbot applications to have longer conversations

21:44.400 --> 21:49.760
with someone who is thinking about this and I think like companies like replica where full

21:49.760 --> 21:54.080
disclosure I'm an investor too you know I have done a phenomenal job kind of being essentially

21:54.080 --> 21:59.680
a journal that talks back to you and seems to care about you and kind of helps you work through

21:59.680 --> 22:04.160
issues so there are some exciting and interesting applications in that space as well but yeah

22:04.160 --> 22:10.480
we are basically thinking about the more impactful the application is the more careful we have to

22:10.480 --> 22:18.720
be in just letting AI run run off and do its thing. You know when you were when you were working on

22:18.720 --> 22:26.960
metamine I'm imagining that you know you had to be pretty far out in the you know it's called

22:26.960 --> 22:35.680
the research domain to you know get these raw tools to do the kind of things that you wanted them

22:35.680 --> 22:43.360
to do you know how how how much is that change or not now like are you able to kind of are

22:43.360 --> 22:52.720
using mostly off the shelf stuff or you you know thinking are you required to do a lot of you

22:52.720 --> 23:01.760
know research oriented things you know novel architectures or novel training methods or that kind

23:01.760 --> 23:07.520
of thing. Yeah it's a great question I think it's actually gotten so much easier to build AI

23:07.520 --> 23:13.120
companies and just functionally like useful algorithms it's it's been really incredible to see

23:13.120 --> 23:20.640
like back in metamine we still built those CNNs and C++. And so it sounds like the your general

23:20.640 --> 23:25.600
take is that there you have a lot more ability to kind of pull things off the shelf and

23:28.000 --> 23:34.640
and you get the you have raw horsepower there that you're not kind of needing to push the

23:34.640 --> 23:42.800
innovation frontier in that way. Yeah so yeah that I think you can build a lot more you can have

23:42.800 --> 23:47.360
a lot of impact with the applications in fact we've done so much research that you can just

23:48.160 --> 23:53.120
have I think currently more impact in just applying that research to all these different domains

23:53.120 --> 23:57.920
and problems that you see in the world and workflows that are not yet automated as much as they

23:57.920 --> 24:05.200
can. Then you could I think in pure research right now it's kind of been in terms of pure research

24:05.200 --> 24:10.080
and sort of major ideas it's been mostly executing on make the models larger

24:11.120 --> 24:17.200
efficient to optimize and you know paralyze on current hardware and get more training data

24:17.200 --> 24:22.000
and collect that training data in reasonable ways and think about the bias and so on.

24:22.000 --> 24:28.080
And the models haven't really changed and honestly even if like there's a different kind of model

24:28.080 --> 24:32.000
we just need a very large general function approximated that's efficient and has enough

24:32.000 --> 24:38.400
nobs to tune and then you can kind of optimize that whole that whole setup. My hunch is if it wasn't

24:38.400 --> 24:45.280
for the particular hardware that we're using we would probably be we could use any other large

24:45.280 --> 24:50.320
model too. I think LSTMs aren't inherently worse they're just like less easy to optimize and

24:50.320 --> 24:57.520
paralyze and then train henceforth on our current hardware. So that's one aspect of the answer.

24:57.520 --> 25:03.680
The second one is we definitely have to still innovate because there is no hugging phase model

25:03.680 --> 25:09.920
to build a neural network ranking mechanism that takes into consideration your intent your slots

25:09.920 --> 25:14.320
and then 150 of the apps that you know we build for the first time ever and no search engine ever

25:14.320 --> 25:18.960
had before and then ranking them properly. So you do still have to innovate and build brand new

25:18.960 --> 25:25.120
models. Now for now we can't really publish those. I actually hope that at some point we are

25:26.160 --> 25:31.120
safe enough as like in terms of the existence of the company that we can be much more open in

25:31.120 --> 25:36.080
the future but yeah so the answers of course we still have to massively innovate because of

25:36.080 --> 25:41.120
these hard and interesting new problems that we're tackling where we also don't have that much

25:41.120 --> 25:46.160
data right. So large language models enable us to know that if you look for you know a Chinese

25:46.160 --> 25:51.600
restaurant near me or East Asian restaurants in my area or close to where I am and all of these

25:51.600 --> 25:56.960
things they all mean the same thing. That was you know something that in the past you couldn't

25:57.600 --> 26:03.520
know and you would have to get a ton of training data and then you know be able to actually map

26:03.520 --> 26:09.920
them all to a similar kind of you know natural language sorry API language to then triggers.

26:09.920 --> 26:15.760
You essentially have to translate human language into the language of computers and APIs.

26:15.760 --> 26:21.280
So so there has to be a lot of innovation on our side for that and then maybe a last note since

26:21.280 --> 26:28.240
we brought up Meta Mind is that back then Meta Mind tried to do I think so many different things

26:28.240 --> 26:33.520
like help you kind of label your data and drag and drop it into the browser. That's you know

26:33.520 --> 26:39.040
scale and crowd flower now and scale is like an eight billion dollar company or something.

26:39.040 --> 26:43.280
That that piece of itself that was just like one of the many features of Meta Mind and then

26:43.280 --> 26:48.800
you know deploying it and scaling that deployment and helping you do error analysis and then

26:48.800 --> 26:56.240
just making it a simple Python interface to actually run your AI classifier or model in production.

26:56.240 --> 27:02.240
All of these things now have companies that are valued in the hundreds or millions or billions

27:02.240 --> 27:07.040
of dollars each of these one like separate features that we had implemented Meta Mind from scratch

27:07.040 --> 27:13.680
pre you know having anything like PyTorch or TensorFlow and so it's just fascinating how a good

27:13.680 --> 27:20.560
tooling is gotten for AI and that was sort of my tangent on like we're investing in both vertically

27:20.560 --> 27:25.680
integrated but also sort of tooling companies that AI expenditures and and because of that it's

27:25.680 --> 27:30.880
gotten a lot easier for people to to have impact in those applications. You reference the

27:30.880 --> 27:38.000
particulars of the hardware that you're using is that say that you're using kind of non-traditional

27:38.000 --> 27:45.520
exotic things or just that you know there's a affinity between GPUs and and transformers that

27:45.520 --> 27:52.400
allows you to scale. Yeah we definitely are using very standard hardware because we need to scale

27:52.400 --> 28:00.160
we need to be able to pull up a data center in in Europe or in Asia and in different different

28:00.160 --> 28:06.800
places so we don't really want to rely on any non-standard hardware right now and even GPUs are

28:06.800 --> 28:12.480
often there's a shortage so we have to sometimes map some of the GPU models and see if we can

28:12.480 --> 28:18.800
make them fast enough on the CPU just so that we can have more data centers and have you know less

28:18.800 --> 28:26.400
lag time when they're not enough GPUs available in certain Geos so yeah it's it's an interesting

28:26.400 --> 28:35.200
interesting challenge for sure. And how how are you finding the level of maturity from an engineering

28:35.200 --> 28:40.960
perspective to allow you to achieve that that kind of scale? I guess it's just about the people

28:40.960 --> 28:48.320
like we've gotten really lucky and I'm having hired like an incredible AI and engineering team

28:48.320 --> 28:53.360
and also DevOps you know just like like spawning out all these machines with a click of a button

28:53.360 --> 28:58.720
you have a whole new data center and reduce latency for people in a different geo it's it's

28:58.720 --> 29:06.320
yeah mostly about the people and I'm imagining you've invested significantly in kind of building

29:06.320 --> 29:12.560
out a enabling platform that allows you to kind of develop and train models quickly and get them

29:12.560 --> 29:18.160
in a production quickly was that a big focus? Yeah yeah it's definitely definitely something we're

29:18.160 --> 29:22.320
also relying on you know things like weights and biases full disclosure I'm also an investor and

29:22.320 --> 29:29.280
them to help with with experiments and running those and you know there are a lot of a lot of good

29:29.280 --> 29:34.400
tools that you can use now but ultimately to actually spawn out the whole system and like have

29:34.400 --> 29:39.840
a new end-to-end you know search engine that runs with a bunch of different machines and so

29:39.840 --> 29:45.520
on they all communicating that is still something that we had to build ourselves from scratch and then

29:45.520 --> 29:51.040
we also want to make it easy to create a new application so now we just have it so that you have

29:51.040 --> 29:56.880
a new JSON-like data dump and then boom of like a config file and you have a new application within

29:56.880 --> 30:03.360
u.com and so I'm excited to in the future essentially let anyone kind of build that and have

30:03.360 --> 30:10.960
search capabilities over all that data so I think automating search over new kinds of data sets

30:10.960 --> 30:18.560
that has been you know an interesting and tough challenge that we tackled. We spoke a little

30:18.560 --> 30:26.640
bit earlier about the code module code application and a lot of ways the kind of the use case that

30:26.640 --> 30:31.200
you described of hey you know I'm searching Stack Overflow really I just want this code snippet

30:32.480 --> 30:38.480
I think I mentioned that in a conversation with Greg Brockman about codex you know how that's

30:38.480 --> 30:44.400
you know ultimately what we want you know what we need you know in terms of the process of

30:44.400 --> 30:52.560
building that module can you maybe compare contrast with what you've seen others do or I'm you know

30:52.560 --> 31:01.040
codex copilot that kind of thing. Yeah I think you know I think there's basically I think about

31:01.040 --> 31:06.320
this in kind of two levels either you're trying to solve a problem that people have solved before

31:06.320 --> 31:12.240
and at that point you just want the direct code snippet as is or you're trying to combine it

31:12.240 --> 31:17.280
and and have you know sort of a new combination of problems that no one has yet quite solved

31:17.280 --> 31:23.200
like this before and at that point you need like codex or like our code complete and you

31:23.200 --> 31:30.160
.com to just give you the answer and generate something novel and and it's kind of incredible how

31:30.160 --> 31:34.720
these models aren't just kind of able to deal with things inside the convex hull but really in

31:34.720 --> 31:40.560
the hypercube of like you know different combinations and combinatorial that I mean combinatorial

31:40.560 --> 31:46.000
combinations of things they have seen in the training data to generate and then combine them so

31:47.360 --> 31:54.320
that's kind of how I think about these two levels of generation. It's a ground-up model that you

31:54.320 --> 32:00.240
built as opposed to an API that you're using. No yeah we're also using API for the code generation.

32:00.240 --> 32:08.560
Oh for the code generation you are. For the models that you're building what are some of the

32:08.560 --> 32:13.520
training data sources that you rely most heavily on? I guess there's you know sort of large-scale

32:14.640 --> 32:21.760
internet available data that is there and then we have to crawl a ton too. So that is probably

32:21.760 --> 32:28.080
the biggest one and it's been something that I think a lot of machine learning leaders can relate to

32:28.080 --> 32:32.880
which is everyone wants to come in and build cool models and you know but then they realize

32:32.880 --> 32:36.160
man that's really hard so they download hugging face models and that's just kind of work out

32:36.160 --> 32:43.840
of the box and then the biggest thing is that you know everyone wants to not very few ML engineers

32:43.840 --> 32:49.600
want to spend a lot of time on data which you know let Andrew to say oh let's just have data

32:50.320 --> 32:54.800
competitions like we're ever can get the most interesting data set for for this problem and that's

32:54.800 --> 32:59.680
something for us that that often meant we had to spend a lot of time crawling and eventually we

32:59.680 --> 33:04.480
just hired some people who are actually excited about crawling data and getting us that data that

33:04.480 --> 33:09.200
we need to then be able to actually train summarizing models summarization models and so on

33:10.640 --> 33:16.560
later on and so yeah it's been a continuous challenge to crawl and it's one of the many places

33:16.560 --> 33:21.360
that the monopoly of Google comes into because there's some sites that say only Google's allowed

33:21.360 --> 33:25.440
to crawl us no one else's and they're like well you know however we're gonna be Google if we

33:25.440 --> 33:31.200
can't do that and so there yeah all kinds of interesting challenges both on the technical side but

33:31.200 --> 33:39.280
also the sort of systemic side okay you mentioned earlier when I asked about page rank I thought

33:40.000 --> 33:47.760
your response was saying that you weren't crawling for kind of the the index but rather you

33:47.760 --> 33:54.880
are consuming that via an API so that's the index but you are crawling for some of these other

33:54.880 --> 34:00.400
applications your building is that the idea yeah sorry I was yeah there's some ambiguity there so

34:00.400 --> 34:06.880
we actually think that the list of like a blue link of lists a blue list of links isn't going

34:06.880 --> 34:14.560
to be as important anymore as you know the actual larger content islands like Reddit like medium

34:14.560 --> 34:20.960
like Twitter and so on and then in order to be able to summarize things you also can't really

34:20.960 --> 34:25.520
do that on the fly these large models are not fast enough people want things in hundreds of milliseconds

34:25.520 --> 34:32.800
and took us a long time to for 90% of queries now be faster than you know Dr. Go and other competitors

34:32.800 --> 34:38.080
in the search engine space and almost as fast as Google at least when you're close to our

34:38.080 --> 34:43.200
data centers we don't have as many all over the world of course but a long story short

34:43.920 --> 34:49.280
we are actually crawling a ton of data in order to build these apps and make them fast enough

34:50.080 --> 34:55.040
there are also some several times where we thought we could rely on an API from someone else

34:55.040 --> 35:01.280
but then just the scale and burstiness of search and when you get tens of thousands of queries

35:02.240 --> 35:06.480
in a few hours like you just know API was able to deal with that we have to build

35:07.280 --> 35:12.960
and have that content ourselves index it be able to do interesting vector search operations

35:12.960 --> 35:17.360
and things like that with the data all in a few hundred milliseconds to then be able to

35:17.360 --> 35:23.280
surface the right kind of content very quickly so yeah we're kind of slowly crawling the web

35:23.280 --> 35:29.680
through the most important content islands like Stack Overflow like GitHub like you know

35:29.680 --> 35:36.080
PyTorch or HuggingFace documentation or all of Medium which is also pretty large so

35:36.080 --> 35:40.720
there are a bunch of interesting things that we are you know sort of we have to crawl ourselves

35:40.720 --> 35:44.960
just to be able to have the speed and the AI capabilities that you have to run offline

35:44.960 --> 35:53.360
the the goal is to produce a better general search engine but you've also specialized in some

35:53.360 --> 35:58.400
ways that makes it a super interesting search engine today for more technical folks

36:00.000 --> 36:05.200
yeah how do you think about like one you hit the knee of the curve that it's like better for

36:05.200 --> 36:11.040
for everyone yeah well we've learned so far is that we're better for developers already

36:11.040 --> 36:17.760
like a lot of people I posted with a couple of features on a Twitter thread for our you know you

36:17.760 --> 36:24.880
code kind of special search and it blew up like crazy hundreds of thousands like 300 400

36:24.880 --> 36:30.000
thousand impressions thousands of likes and so it resonates a lot with that crowd now what we've

36:30.000 --> 36:34.800
learned is that I sort of jokingly say it turns out developers are people too and they want to know

36:34.800 --> 36:39.600
what the weather outside is and what the sports results are and how to travel and like all of these

36:39.600 --> 36:46.080
things and so you know where to buy food and maybe order food and so we have to basically if you

36:46.080 --> 36:50.160
want to be the best search engine for developers and be your default and be there every day and in

36:50.160 --> 36:57.840
your nav bar through you know Chrome extensions and things like that we have to be able to do

36:57.840 --> 37:03.360
everything else in search too which is tough for a small startup but we've now gotten to a point

37:03.360 --> 37:09.520
where once we launch sports results there's maybe only the travel category where we're not as good

37:09.520 --> 37:16.400
for everyone else and then most other things we actually are you know we have answers for

37:16.400 --> 37:22.000
movies and and things like that and you know there are still some APIs like the movie API that

37:22.000 --> 37:27.120
is a little bit slow so it takes like two or three seconds to load rather than you know less than

37:27.120 --> 37:34.800
one second and we've gotten complaints about that also but yeah there's some proprietary data that

37:34.800 --> 37:40.720
we could probably just crawl I guess the law just kind of changed a little bit because LinkedIn

37:40.720 --> 37:46.960
lost a big lawsuit that you know they tried to prevent folks from from crawling data but I'm

37:46.960 --> 37:53.760
sorry short a lot of pralings happening and the speed is speed is always super important and we

37:53.760 --> 38:01.040
are we are having to build a lot of that in house of course congrats on on the on the launch of

38:01.040 --> 38:09.520
v.com and and you.com code and write before we part ways I did want to circle back on a couple of

38:09.520 --> 38:16.320
the things that we spoke about that you started at Salesforce it sounds like you're you're still

38:16.320 --> 38:22.560
working on those the protein generation one we spent a fair amount of time talking about that

38:22.560 --> 38:28.800
the last time we spoke and we'll include the link to that in the the show notes we didn't I don't

38:28.800 --> 38:34.320
think spend much time talking about the AI economist I think the timing didn't quite work out to

38:34.320 --> 38:42.720
dig into that so would love to have you share a bit about that project and I think you have some

38:42.720 --> 38:49.680
recent news there new recent updates that's right this week in machine learning we we actually

38:49.680 --> 38:55.520
got the science advances paper out about the AI economist and maybe just at a very high level

38:55.520 --> 39:04.160
what is the AI economist was word by Stefan Stefan Chang and Alex Trot and and a few others

39:04.160 --> 39:11.120
at Salesforce and myself and basically the idea is using reinforcement learning for some of the

39:11.120 --> 39:18.000
most important applications that we can think about it for humanity period instead of having our

39:18.000 --> 39:24.880
L play games that are kind of interesting but ultimately themselves not very useful why don't we

39:24.880 --> 39:31.200
see if we can build a very realistic simulation and we're far from that in terms of realism right

39:31.200 --> 39:40.320
now this will have to scale up over time too but I think it's a brand new area of AI that can have

39:40.320 --> 39:45.760
a huge amount of impact and so the high level ideas you have a two level reinforcement learning

39:45.760 --> 39:53.920
problem where you have an RL agent that sets that sets taxes and subsidies for a bunch of other

39:53.920 --> 39:59.200
RL agents that which themselves are just trying to optimize their own utility as in they try to

39:59.200 --> 40:05.040
maximize resources they can collect money they can make houses they can build blocking of other

40:05.040 --> 40:11.040
people from resources by you know building houses around them things like that and are basically

40:11.040 --> 40:17.840
to some degree more selfish and you know but may also eventually identify patterns to collaborate

40:17.840 --> 40:24.240
towards their own selfish goals and maximizing their own utility functions and so the interesting

40:24.240 --> 40:30.720
thing is now you can give that top level RL agent that sort of the AI economist the ability

40:30.720 --> 40:37.680
to subsidize or tax different income groups differently in order to maximize a specific objective

40:37.680 --> 40:44.000
that you've given that AI so the idea here is that you can now say oh I want to help the middle class

40:44.000 --> 40:50.480
or I want to maximize productivity of this economy or I want to maximize equality in this economy

40:50.480 --> 40:55.760
or a combination of all of these things that you wait and you say okay I care about the one we

40:55.760 --> 41:00.560
chose in the end was productivity multiplied with equality which is one minus the genie index

41:00.560 --> 41:04.880
it's essentially thinking about how how equal you want to be you don't like you know in the

41:04.880 --> 41:09.680
limit you don't want to be like everyone is extremely equal but extremely poor right that's

41:09.680 --> 41:14.960
that's not helpful too so you have this like overall productivity in there as well and so that

41:14.960 --> 41:19.840
that is kind of the high level and so what that means is that you know if you if you take that

41:19.840 --> 41:27.040
idea and you really scale it out and you make the simulation more realistic you increase the size

41:27.040 --> 41:34.880
of the number of agents to hundreds of thousands and you actually put in sort of historic data

41:34.880 --> 41:40.880
into this that you know to start the model that in the future of a politician says oh I'm doing

41:40.880 --> 41:46.800
like these following five things to help the middle class or to help this particular group of people

41:46.800 --> 41:55.040
whoever it is like worldwide right then you can run that suggestion across and compare it

41:55.040 --> 42:04.960
and contrast it with millions and millions of years of simulated taxation where you basically

42:04.960 --> 42:10.960
try to identify what the fairest or best or most sustainable or most equal or most productive

42:10.960 --> 42:17.840
way is to tax that entire system that touches on highly philosophical things like you know

42:17.840 --> 42:24.560
communism capitalism socialism market market economy and so on and combinations of these

42:24.560 --> 42:30.080
systems on the one side but it's very concrete like it'll you know could change and be another

42:30.080 --> 42:36.720
input into economists models to be more realistic it's kind of crazy but you know there's there's

42:36.720 --> 42:42.560
models that are being used right now like the ps formula very famous Berkeley professor in economics

42:43.600 --> 42:50.480
who has this provably correct way to tax different income brackets but it's provably correct in a

42:50.480 --> 42:58.400
one-step economy turns out people iterate right like turns out time moves on and and and so this

42:58.400 --> 43:04.320
AI economist model can actually recover the provably correct solution for a one-step economy

43:04.320 --> 43:10.560
but then as the models learn as the agents adapt as the time continues like that model just

43:10.560 --> 43:16.960
is so much more powerful and realistic than any of the linear models and one-step models that

43:16.960 --> 43:24.000
we're currently using that it's just hard for me to not see how that won't change all of economics

43:24.000 --> 43:29.600
which in a grand scheme of things has it been an area that hasn't been impacted by I nearly as

43:29.600 --> 43:34.800
much as I think it could or should and if you think about how much bloodshed there has been human

43:34.800 --> 43:41.120
history to identify what the right model is of taxation and representation and things like that

43:41.120 --> 43:47.360
like it's just so powerful to be able to try to offload that into a simulation get millions of

43:47.360 --> 43:53.360
years of taxation going and then you know learn from that and see if we can use some of these things

43:53.360 --> 43:58.240
and of course you know like we don't want like an iDictator either like we need it as like another

43:58.240 --> 44:07.200
data point as a model that helps us make more you know more accurate like decisions but ultimately

44:07.200 --> 44:13.040
people still want to decide what the objective is so that's still like a very important one

44:13.040 --> 44:16.880
and we have to sanity check it of course before you implement these things so I'm not like

44:16.880 --> 44:21.520
absolutist like this has to be like a new religion or anything but like it I think it's just

44:21.520 --> 44:26.880
such a powerful tool and and I have high hopes that just like what we've seen in the

44:26.880 --> 44:31.760
linguistics and natural language processing or we've seen in computer vision or we've seen in

44:31.760 --> 44:36.960
robotics or we've seen self-driving all of these different application areas of AI

44:36.960 --> 44:43.440
that economics could be another such application area sounds like a model that would be really good

44:43.440 --> 44:50.640
at the Sims yeah it's it's not actually crazy to think that that is like a pretty good simulation

44:50.640 --> 44:55.600
now of course the problem is that it doesn't capture like sort of as realistic utility functions

44:55.600 --> 45:00.560
that people have like in the Sims like like people might not get as tired and then like just don't

45:00.560 --> 45:04.240
want to work anymore because they want to sleep and things like that so you want to you know

45:04.240 --> 45:08.800
adjust and for most people like you know they're sort of logarithmic happiness curves to like

45:09.360 --> 45:13.440
making like an order of magnitude you need to make an order of magnitude more money often in order

45:13.440 --> 45:18.400
to be like a little bit happier and then it sort of levels out logarithmically they're all kinds

45:18.400 --> 45:23.440
of interesting things that we have found in in psychology but what's fascinating too is that you

45:23.440 --> 45:28.240
can actually say well I think people are this and that like I think people are going to want to

45:28.240 --> 45:33.520
work more I want to work less you can actually make that very explicit in the beginning of the

45:33.520 --> 45:39.840
simulation and then see how those assumptions about how people you know define their own

45:39.840 --> 45:46.880
utilities will actually influence the optimal political model or not political some degree you know

45:46.880 --> 45:54.480
the few kind of group all of these different policies into one cluster but you know just generally

45:54.480 --> 46:03.040
sort of taxation and financial is the work on behavior economics you know things like predictably

46:03.040 --> 46:10.720
irrational all of that you know does it does it say that you kind of akin to what you're saying

46:10.720 --> 46:15.760
that everyone has their own utility functions and they're not as uniform as traditional economics

46:15.760 --> 46:23.920
might like or you know is there is it more that you know there's just an emotional irrational

46:23.920 --> 46:29.200
component and if that is the case like how do you even model something like that yeah it's a

46:29.200 --> 46:35.600
great question so you can you can have a prior distribution and then you sample different like

46:35.600 --> 46:40.160
you know you sample from that prior distribution that you have for instance for utility functions

46:41.120 --> 46:45.520
and then you know based on that sample and based on how you define your prior distribution

46:46.080 --> 46:52.720
you can get different sets of agents that that come out of it and so so that's that's one one

46:52.720 --> 47:00.640
aspect and then and then there's some things where the irrationality has not been captured yet

47:00.640 --> 47:05.360
as as realistically in that simulation just the idea that you know sometimes people do

47:06.000 --> 47:10.160
something that they know is actually sub-optimal for them but they think because of fairness they

47:10.160 --> 47:16.720
want to still do it and and so those are are not yet modeled in that simulation that we ran

47:16.720 --> 47:24.160
but at the same time those are L agents that have neural networks and can try to adjust their behavior

47:24.160 --> 47:29.920
to others and so on still much more realistic than anything that economists use nowadays which is

47:29.920 --> 47:37.280
like often linear models and one step kind of like probably correct formulas and if you have

47:37.280 --> 47:43.920
you published the models themselves or the simulation environment yes it's actually extremely important

47:43.920 --> 47:48.720
you know imagine someone is like I know what this is right for everyone and I had a I said it

47:48.720 --> 47:53.520
let's all trust it that's of course a terrible idea so you have to open source these models you

47:53.520 --> 47:59.200
have to open source all the assumptions you made that went into the simulation and the simulation

47:59.200 --> 48:03.360
itself that could be some pretty insidious bugs right if you said oh this is how everyone's

48:03.360 --> 48:08.480
gonna tax get taxed and then there was a bug and you're like oops so you know a lot of people need

48:08.480 --> 48:13.760
to do this and and that's you know one thing I loved about Salesforce Research 2 can still love

48:13.760 --> 48:20.960
that you know for these kinds of important human kinds of applications we we did open source

48:20.960 --> 48:26.320
all that model and there there's some really exciting ongoing projects now that you know you

48:26.320 --> 48:32.320
can use this also to avoid things like tragedy of the comments where it's like old example of

48:32.320 --> 48:36.480
if all the sheep farmers put all their sheep into one field and the field just gets completely

48:36.480 --> 48:40.800
destroyed and no one has a field anymore if any sheep so you have to kind of partner up and

48:40.800 --> 48:44.320
make sure you don't use your resources too much it's something that I think we're going to hit

48:44.320 --> 48:50.000
worldwide in terms of sustainability and deforestation and things like that and water

48:50.720 --> 48:56.480
so we all have to kind of avoid tragedy of the comments in like sort of worldwide yeah

48:57.200 --> 49:04.800
given the meta factors that we've talked about you know with regard to the way revenue models

49:04.800 --> 49:11.760
impact you know search engine behavior like how does u.com become a viable company

49:12.720 --> 49:18.080
if it's not going to be ad-based and fall into the same traps that you know we saw with Google

49:18.080 --> 49:24.320
yeah great question so the main goal is to have these applications that we're building actually

49:24.320 --> 49:29.680
provide enough value that people would want to pay for them you write as one example you know

49:29.680 --> 49:36.240
costs a lot of money to run a large language model as it writes a blog post for you or an essay

49:36.240 --> 49:42.640
like you can pay for that and that's one thing I also think that private ads can be used

49:42.640 --> 49:46.480
especially in our private mode where we don't log anything we don't know what's going on at all

49:47.280 --> 49:50.880
and we can't really monetize it in any way other than through private ads and what I mean by

49:50.880 --> 49:56.400
private ads is just ads that are dependent on the query and that's a luxury that you can have

49:56.400 --> 50:01.440
as a search engine because people give you an intent of what they want to do if your social

50:01.440 --> 50:06.240
network they don't really tell you like I want to buy an air purifier right now they just talk

50:06.240 --> 50:10.880
to their friends and be like oh I'm peep sneezing or coughing and maybe I have dust mites in my home

50:10.880 --> 50:15.040
and and then you kind of have to like spy on them if you want to sell the ads to like

50:15.040 --> 50:21.840
know what they might want to buy in the future but as a search engine even if you don't know anything

50:21.840 --> 50:29.760
about the user if you just look at a query and then you based on that query give an ad I think that

50:29.760 --> 50:34.400
is is better and it's kind of what we've seen ducked up go doing too you know they care about

50:34.400 --> 50:40.400
privacy as well and they have these private ads that basically are not user dependent they're

50:40.400 --> 50:48.960
only query dependent and the advertisers don't really know which user is seeing the ad and you know

50:48.960 --> 50:53.520
just it's just basically really only based on the query and so I think that that can be kind of

50:53.520 --> 50:57.600
a backup but I really hope that we can build something that's useful enough that people are

50:57.600 --> 51:03.040
going to want to pay money for certain things well Richard it has been wonderful catching up

51:04.160 --> 51:08.480
again congrats on the recent launches and all the amazing work that's gone into

51:09.280 --> 51:15.040
building what you built over the couple years the past couple years and looking forward to

51:15.040 --> 51:19.840
catching up again soon thanks so much great questions and yeah been a pleasure chatting with you

