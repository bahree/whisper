Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.
Alright everyone, I am on the line with Sammy Koyejo. Sammy is an assistant professor at
the Department of Computer Science in the University of Illinois, just a couple of hours away from
where we are here in St. Louis, Sammy. Welcome to the Tumel AI Podcast. Thank you. It's
a pleasure to be here. It is great to have you on the show and I'm looking forward to learning
a bit more about your research. Let's start up. Absolutely. Let's start up by having you share a bit
about how you came to work in ML and AI. Sure. So a reasonable place to start would be
Brown grad school where I was interested in working on physical air communication systems.
As time went on, I got more and more interested in intelligence,
bit onto those systems and I started working on an area known as cognitive radios. Over time,
I spent more and more time thinking about the intelligence piece and maybe less on the communication
piece and eventually I started working on machine learning as my main area of research. So by the
time I finished my PhD, I switched advisors and worked in a different area and specifically,
I was looking at probabilistic models, Bayesian inference and related topics. My thesis focused on
what is known as constrained or regularized Bayesian inference. It's a style of probabilistic
inference where you add part of your prior structure is built into the inference algorithm and not
just as part of the prior specification. So it's an interesting thread. From there, close to
my PhD, I started to work with cognitive neuroscientists and so I got interested in cognitive
neuroscience and I ended up spending a couple of years at Stanford building machine learning
methods and machine learning models for cognitive science applications, neuroimaging applications
and related ideas. About four years ago I started here at Illinois and I think of my research
broadly as adaptive and robust machine learning. My research is quite broad, so I work in a bunch
of different areas. I still work in cognitive neuroscience and neuroimaging a bit and building
machine learning tools for those applications. But in addition, in my core machine learning work,
I think about scalable machine learning, fault tolerance and machine learning and a variety of
topics related to how to construct machine learning systems that make good predictions for
various kinds of evaluation metrics and how that interacts with human decision making.
So sort of a variety of research threads but hopefully coming together in a coherent set of ideas.
Nice. Now I'm so kind of intrigued by this idea of a cognitive radio. How far did you get into
that topic? Is that an actual thing? Yeah, yeah, it's an actual thing. I haven't followed
the development over the past few years but it came about at some point where essentially
ran out of spectrum in the US and in many developed countries. And so in order to get more
bandwidth, those idea to make use of spectrum that ends up being sort of dead spots. So in various
cities, different channels say broadcast on certain radio frequencies but leave some others.
And so one way to construct a radio that works in these systems is to actually sort of hop between
different frequencies by detecting when a frequency is in use and when is not in use and
you sort of hop in between and sort of communicate for a short time. And this way you can make
better use of the spectrum and actually make use of spectrum that otherwise would you could not use
for any other purpose. So it's an interesting idea. I believe that there are systems built out.
I don't know the extent to which there are popular implementations of these things because
again, I haven't followed this idea. But it was there in response to I think quite difficult
problem in the communication space of running out of wireless spectrum. Okay, interesting. Interesting.
So you mentioned a couple of key areas in your research. One focused on metrics and metric
elicitation, the other focus on robust distributed learning. And let's start with the first of those.
When you're thinking about metric elicitation, tell us a little bit more about the problem that you
are trying to solve there. Yeah, yeah, happy to. So it's a bit of a history. So it's probably worth
taking us half a step back and giving you some context. Sure thing. So the way I like to think about
the problem is roughly when you build a machine learning system and you're sitting using the
machine learning system for decision making, many real-world decision making tasks are actually
quite complex and they involve trade-offs between different factors. And the issue is that many of
our default machine learning metrics don't account for these trade-offs in decision making.
And so at some point around, it's not been a few years, but I think around 2014 or so,
I got interested with, at the time, my postdoc advisor, Pradeep Ravi Kumar,
in thinking about how to construct machine learning models that could optimize complex metrics.
So think of, in fact, there are lots of great sort of common examples. So
and information retrieval, popularly, people use what is known as the F measure. So this is some
ratio of precision and recall. It's commonly used for prediction in these settings because
this thought to be a good measure of performance. But up until some of our early work,
there wasn't a good understanding of how one built a machine learning system that was specifically
good at optimizing F measure. So before that, you build your system to be good at optimizing accuracy,
and which again, we're fairly good at doing or maybe some weighted accuracy. But there wasn't
a good sense of what to do if I changed the measure into something more complex like F measure.
So we started this series of papers over a few years where we got better and better understanding
how to construct good learning models for, again, what we call complex metrics. What is sometimes
called non-decomposable metrics. And they're called non-decomposable metrics because I can't write
down the metric as an average. And once you break this averaging possibility, lots of standard tools
that we like to use, like say, graded descent in a sort of straightforward way, or tools that
we like to use in terms of analysis start to break down. And so it becomes somewhat of a more
challenging problem to solve. And so like I said, we had a series of papers where we tried to
understand these metrics a lot more and come up with good methods for optimizing them. And I'd give
these talks and really excited about early work on this showing, hey, give me your really complex
evaluation measure. I can tell you how to optimize it. And I should say that we had a characterization
that was quite general. And so it could adapt to different notions of what good measures are.
How complex are you able to get, often when I think about metrics in the way you're describing them,
you've got your metrics that the data scientist or machine learning engineer is trying to build
a system to, and then maybe there are metrics that aren't the one they're using to train their
models, but they're still kind of in their domain like your F-score. But then on the other end
of the spectrum, there are the actual metrics that business people care about which don't look like
either of those, or often don't look like either of those. Do your research get to that other end
of the spectrum? Yes and no. So I can give you some sense of the scope of where we can say some
useful things. Okay. So the first few papers, I should take a step back and mention that for
classification problems, the primary statistic that one uses to measure whether you have a good
model or not is something related to what is known as the confusion matrix. So the confusion matrix
is essentially measuring for every kind of prediction and for every ground truth label. How often
in a setting, but a certain label do you make a certain prediction? So I look at the average times
I say, predict one when the ground truth is one, and this gives me some number. It's a confusion
matrix for one one, or I can do this for one two. How often do I predict two when the ground truth
is one, or how often do I predict three when the ground truth is one, and all combinations of this
for say a multi-class classification problem. So with K classes, end up with a K squared confusion
matrix for every pair of ground truth and prediction. So initially we worked on linear combinations
of the entries as a confusion matrix. So you could imagine, for instance, that predicting three
when the ground truth is one is more expensive than predicting two when the ground truth is one.
And ideally you want to predict one when the ground truth is one. We're kind of familiar with
this kind of scenario when we, you know, in its simplest form, like false positive and false
negatives and some applications. Exactly those. Yes. I went to the more general multi-class case,
but I think the binary case is enough for illustrative purposes. So in the binary case, it's true
positives, true negatives, false positives, false negatives. Thanks for pushing me that direction.
So initially we worked through, say, linear weighted combinations of this. Eventually we got
two ratios of linear things, which captures things like f measure. They can write metrics like
that as ratios of linear things. Now we're at the point where we can pretty much do sort of any
function of the confusion matrix and we can come up with what is known as a consistent estimator.
So some estimator that we know will have good large sample properties. So some learning algorithm
where I can come up with a learner or optimization process that I know will have good behavior
in terms of optimizing for some arbitrary function of confusion matrix. So how far does this go to
reward metrics? We think it goes reasonably far, but it's clearly once you get to real world
settings, many of the things you care about don't always, I'm not always captured by the confusion
matrix and or cannot be reduced in that sort of simple way. Though many are, so many things
are just weighted forms of some weight attacks different kinds of mistakes. So as long as you
sort of roughly in a setting where the thing you care about is the ticks of mistakes and waiting
on those or even in survivatory functions of those, they're pretty good algorithms now that
based on work that I and others have worked on to build to build good algorithms for optimizing
these. So like I said, or as I was going to say, I would give these talks being excited by
this line of algorithms and saying, hey, we can do in any metric you like and you get the feedback,
well, now you've made it clear that there are lots of different ways to measure what performance is.
Well, which of these should I use? So it's quite unclear. Now, once it's clear that there are many ways
of measuring performance, it becomes trickier to think of or to pick which one is best suited to
a certain setting. And so the idea of metricalistation, which is where we ended up with, is trying to
turn the problem on its head. I'm trying to ask, can I elicit good metrics by interaction with
experts or with users or with panels of experts? So are there strategies that can come up with,
that can interact with an expert to figure out what measure is closest is a close approximation
to how they're determining trade-offs or value of different kinds of predictions. And so the idea
is that if we can do this well, then you can optimize for that metric. And more importantly,
this metric is transportable. So I could change the class of models I'm optimizing. I could change the
data distribution. I could change the setting in some ways. But as long as it's capturing the expert
trade-offs, then this is a good way of measuring good performance. And so this is something that I can
use as I change the settings. So the example I like to give is a simplest example I think that
maybe illustrates the idea of elistation is in a healthcare setting where say a doctor would
try to is interested in constructing an automated health decision-making system. And so the doctor
is an expert. They have some notion of how expensive say for a certain set of measurements,
how expensive it is to say misdiagnosis, so misdiagnosis when there's actually sort of some disease
there or overdiagnose somehow. So like predict the say that someone has some disorder when they
actually don't. And so this the actual trade-off depends on the costs of treatment and maybe
sort of potential side effects, sort of all these other considerations. But if you think about
this as a decision-making problem, you can imagine that you could sort of compress everything down to
there's some cost to making a prediction in one direction, some other cost making a prediction
in the other end. Often going from this intuition idea to a concrete trade-off function is hard.
If it was easy then one could sort of then construct models to directly try to optimize the doctor's
trade-off function. But it's hard to do in real settings. And so what the idea of metricalistation
is is to come up with a strategy to interact with say the doctor is an expert. And based on this
interaction, actually pinpoint the right trade-offs that should correspond with their preferences.
And then you can optimize those preferences directly and if you construct things at a downstream
models. Other examples we have considered say things like ranking models. We haven't worked on
this. We don't have results for this yet, but in the pipeline are things like say ranking models
if you're building recommender system. Imagine that your users have different preferences
and the order in which they want to see things. So you could imagine then constructing a
listation procedure that did a series of say AB tests with your users and tried to pinpoint
for the user population what the best approximation to the best sort of ranking cost function should
be for their setting. Is the process always akin to AB testing? You mentioned that in the
scenario of users. It is the same thing applied to doctors. You know, what do you prefer choice,
say choice B? So there are many ways you could imagine developing a problem like this.
We chose to go with the pairwise preference approach. The motivation after talking to experts
in the area was that with pairwise preferences, it's sort of much more likely that we can get
users to easily give us answers to the comparisons than if we tried other ways of
interacting with experts or with users. Because somehow pairwise preferences are easier
for experts to give feedback on than other kinds of ways of querying.
Okay. So I should mention this is work led by my student, Gauru Shirendiani,
here at Illinois and in collaboration with colleagues, Rutameta and sort of other folks at Illinois.
So this is work that again a few years, at least the listation piece has been a couple years
old. And again, it's trying to answer this question of what metric should I optimize in order to
get my machine learning system to do sort of the thing in the real world that I wanted to do.
So again, the thing in the real world is rarely optimize accuracy. It's typically something
much more subtle, something much more complex. And we're starting to get initial answers for
sort of reasonable algorithms for trying to answer this question. I also should mention,
so the pain point in trying to construct this listation procedures is how many queries
are going to ask the expert because in principle, in theory, if you had had infinite queries,
you could elicit anything. But yeah. So the idea is in as few queries as possible get to
formulation of your metric that is as accurate as possible or matches as closely to what the expert
would do you define it as kind of produces a classifier that most closely matches what the expert
would predict. Like, how do you tie it back into the metric of the classifier?
I think you're talking about evaluation, which is tricky in these problems.
But the target, the conceptual target is to like get the best approximation to the
trade-off function that the expert is using. The practical evaluation is if I sort of change the
classification setting or change distribution in some way, I should be able to get the same sort
of outcomes as ideal outcomes from the model as what the expert would pick as ideal outcomes
from the model. So it should replicate the expert's predictions, but again, this idea of
transportability. So we should be able to do this in a variety of settings. I should mention it's
quite close. Metricalistation is not that far from ideas like inverse reinforcement learning.
So again, there's a whole literature primarily in the reinforcement learning world,
whereas a lot of focus on learning good reward functions, and sometimes by learning this
good reward functions by interaction with humans. In some sense, we're solving a easier version of
the problem than what the RL folks are trying to solve. We take advantage of a lot of additional
structure that comes from the classification version of the problem, which is most of what we've
focused on so far. So we can get sort of much stronger results, much better algorithms than often,
what could get and does it much more general reinforcement learning setting, because you don't
have to think about sequentiality of sort of in the same way that an RL setting would have to
reason about. One issue also I should mention is an investor reinforcement learning in particular,
there's not always a focus on actually getting the reward function right. So to your point earlier,
often the focus is on replicating behavior, not necessarily getting your reward function right.
So in this say driving setting, I want to be able to drive the same way that the human drove
where the human is the expert. So get me the reward function that does this the best in the setting.
Not necessarily in this in these settings, sometimes there is not a focus on say what happens if I
change the environment a little bit. And so now the reward function has sort of so tuned to the
original setting that it doesn't work as well in the new setting. So one difference is that we're
very focused on again, this idea of transportability. So we're the focus is still on learning or
eliciting metrics such that they are agnostic to things like data distribution and the specific
learner they're using and sort of other kinds of important, but things that want to abstract
the way because we want it to have these trade-off functions that you can then apply in sort of
general settings. So learning a simple setting potentially apply in a more complex setting is
maybe one way to think about it. And so how do you get to that level of generalizability? Is it
in the you know your selection of data that you're training on or does it have to do with the
questions or sequence of questions that you're asking the pairs that you present or they're like
you know black art techniques like prop out or things like that. Now it's actually in fact for
the binary classification setting with linear trade-offs in the confusion matrix. It essentially
balls down to a very simple binary search. So actually many settings that it balls down to
almost trivial sort of textbook algorithms. And all the work is in characterizing sort of how
do I want to define the feasible metric space and how does one reason about how to search efficiently
into space. And so often once you do that work the final step of the algorithm to elicit in many
settings is actually much more straightforward than you might imagine. So the thing that enables
transportability is so far we've mostly focused on settings where the metric of interest
is some function of the confusion matrix elements. And what's interesting to note is that sort of
trade-offs in confusion matrix elements don't depend on sort of how good you are in classifying.
They just trade-offs between different kinds of errors. So those kinds of functions are
agnostic really to sort of if you're able to estimate them well enough they're agnostic to
things like data distribution function class things like that. So for instance specifically again
in the doctor example if you're interested in the cost of misdiagnosis versus sort of overdiagnosis
of missing versus not missing a diagnosis. If you can think about this as a binary classification
problem but just weights between false positives and false negatives. The what matters is getting
those weights right and the actual value of the false positive and false negative doesn't matter
as much so you can have a learner that's much better at getting low false positive false negatives
and not to learn that's much worse at getting low false positive false negatives. So this
differences would be say differences in using a linear model versus say maybe using a deep learning
model in these two settings. So they would have different confusion matrix trade-offs but as
long as you get the trade-offs right the actual values are not that important. So again we've
focused mostly on settings where that property is mostly true. In classification this is most
often the case if you're focusing on functions of the confusion matrix so it comes up sort of
naturally based on the problem definition that we're interested in. And are there any
properties that arise that relate the you know for example the number of pairs that you have
to present to the dimensionality of your confusion matrix or something like that? Absolutely yeah so
it roughly grows about linearly with the size of the confusion matrix. So to get roughly the
so the conceptual with theoretical claim is to get a certain error accuracy the number of queries
that you need scales roughly linearly with the sort of size of the confusion matrix. At least for
I should say that this is true for linear and ratio of linear things if you're doing more complicated
function classes other terms that to show up. So for linear things it mostly scales linearly
with the size of the confusion matrix which is again sort of number of classes squared.
For ratio you sort of have an extra factor of two there but again order wise it's mostly linear.
If you go to say polynomial functions or something more complicated then it scales roughly that
earlier of size of confusion matrix plus or times some term that depends on order to polynomial.
So roughly that order. So you pay some cost for more and more complex types of score functions.
There's some other discussion which we've been trying to reason through about how how complex is
your sort of score function space need to be to capture human preferences appropriately. I think
that's an important question that we have an answer then and I think maybe not that many folks in
the field have maybe thought about very carefully. In fact we've been working a bit on actually
reducing the complexity from even linear because some of the say psychology literature suggests that
we mostly focus on sort of a few features as opposed to say arbitrary trade-offs between things.
And so potentially the space of metrics is even lower dimensional than say linear in some large
confusion matrix space. Again there's some interplay between human computer interactions sort of
psychology, a bit of algorithms, a bit of machine learning. So it's an interesting set of problems
and an interesting space for us to work in because it's sort of quite unique within machine learning
to have all of these problems come together. But we think it's an important set of problems
because we think it addresses core problems particularly in practice when folks are trying
design systems and they have either a specific rough notion of what good systems should look like
but accuracy is not cutting it or they're interested in some downstream measure that might
involve say interaction with users and again potentially accuracy isn't getting them the results
that they want. One area that we started to look at that is quite exciting as an application area.
Again it's early days but I thought I should mention this. It's thinking about elicitation
in the fairness space. So in machine learning fairness it's very clear that different measures
of fairness end up with different notions of trade-offs between how you treat different
say subgroups. I'm thinking primarily about say statistical group fairness in this case but
similar ideas hold for other notions of fairness as well. And so one could imagine and Sarah's
sort of first steps on this and there are also a couple of papers on this idea of coming up with
elicitation procedures that can build context-specific notions of what metrics or
statistics you should be trying to normalize across groups in order to achieve a fairness goal
in a certain setting. So that's an application area where thinking very carefully about exactly
what you're measuring is interesting and potentially quite important to get the results that one
would want. I think it's still not absolutely clear to me and either the medical or the fairness
scenario what these pairs concretely look like. In the case of I'm even having trouble like
formulating the question concretely in the medical case. But I can imagine that there's
a degenerate case where you're asking the physician would you rather spend like you're taking
pokes at the function? Would you rather do this a thousand times or this one time or something
like that but I'm getting a sense that that's not exactly it. You can show that if your metric is
a function of certain quantities there are only things that matter our differences in those
quantities. So in terms of pair-right comparisons for the confusion matrix setting you might imagine
comparing confusion matrices which is not something that's easy to do by the way. And so part of the
work is coming up with ways to translate those comparisons and two comparisons that say a medical
expert could do. So the variety of techniques that we've started to work with to try to solve this
last mile task. So you can imagine for instance showing where two different classifiers that have
different confusion matrices is sort of the outputs where they differ in terms of their predictions
or a variety of ways of working on sort of interpreting train models. So once you have a way to
so we have good ways by the way of translating confusion matrices back to classifiers this
ties very closely to earlier work I mentioned an optimizing arbitrary metric. So we have very good
understanding now of how confusion matrices relate to models. So we can sort of go back and forth
very easily. So once you have this then you can convert comparisons of confusion matrices which
is what matters in terms of the trade-off into comparisons of models. So sort of model that
achieves confusion A versus model achieves confusion B. And what the expert needs to be able to do
is tell us their preference between model A versus model B some sequence of times. And we choose
the sequence of comparisons in such a way that after sort of after a few queries we can pinpoint
the trade-offs that best capture how they're weighing different kinds of errors in the confusion
matrix for instance. So is is this model comparison formulation A way of looking at this or is
is kind of fundamental to what you've described around metric elicitation always based on this
model comparison. The way that we have built up the approach the fundamental piece is to summarize
is roughly being able to compare confusion matrices which will bolt down to comparing models.
To take us that back though again it's sort of whatever you're using as the sort of the parameters
of your cost function. So the quantities in your cost function only differences in those
quantities will show up as differences in the measure. So for instance if in addition to
confusion matrix entities you really care about smoothness of the function that becomes a third
thing that you add sort of a new parameter in the set of things that you would be comparing.
And so you'd get say two classifiers that differed in confusion matrix or confusion matrices
that they achieved and also maybe had different smoothness. And you would then tell sort of you
would be you would be asked to give a preference between the two. So it's comparing the fundamental
thing is being able to compare whatever quantities determine the metric. So however you define
a metric whatever quantities determine a metric you need some procedure that allows the expert to
compare those two things. In the classification space which is a space that we've studied by
for the most. The natural entities are confusion matrices. And so you need a way to compare
confusion matrices which we do by sort of providing back to models. I should say we have
started a new line of work thinking about how to maybe do this how to select samples intelligently
so you can imagine instead of comparing models using say whether predictions differ the most
you could imagine the algorithm also selects a specific sample it says if you pick this model
make this kind of prediction if you get picked this other model make decide a kind of prediction
and using that as a way to get feedback. It's still early days on that line so it's hard to say
sort of very clearly what is doable and what works well. Right now I'd say the work that is
most mature is focusing on comparing confusion matrices translating this into comparing models.
And then using that as a way to pinpoint preferences for the expert decision maker.
And then we're going to talk about a totally different experience. Yeah. Yeah.
A bit of extra time on this but so another line of work which we've been making I think
quite interesting progress on is a question of robust distributed learning. So this is work
led by my student Song Sier and collaborator here to annoy Indy Gupta who is a professor in the
system side at Illinois. So the setup is that we're interested I'm laughing because sort of like
you said how different it is but so the setup of the problem is that for various reasons
particularly so the scalability in privacy there's a lot more interest in training machine learning
models in a distributed way. So scalability is being able to use sort of lots of machines at the
same time and potentially just getting more throughput running through much more data per second.
So you can imagine this in data centers where sort of each machine has some amount of computing
power. The idea is if I run lots of these machines at the same time on a stack of data I can
and I do things appropriately I can I can sort of train my model much faster. You could also imagine
in fact one of the I think interesting use cases of this is in sometimes called sort of
internet of things or edge networks where say you're interested in training machine learning
models partially on your edge device. So good example is something like a cell phone. You want
to do somewhat processing on your cell phone and the idea is that if I do this appropriately I
can avoid sharing data directly with the centralized server so I don't have to transfer data.
This might win in terms of communication and if I do some extra work and I also get a win in
terms of privacy so I can actually protect the user's data from some easy snooping but still get
the benefits of training a big machine learning model across lots of devices. So that's the set
of the general setup of again distributed machine learning in general. So one unique problem that
shows up in distributed machine learning is that once you distribute your machine learning process
you've made the system much more vulnerable to failures of various kinds and potentially to
explicit adversarial attacks. So failures if you have 10 computers and so any one of them could
potentially fail at some point. You could have communication issues so just now some network
thing fails and in between within an optimization loop or between a training loop and so because of
that if you're modeling and your optimization process is not robust you could imagine
potentially breaking the whole training process. The worst case version of this and this comes
from the system literature is known as Byzantine attacks. This is the idea that you want to protect
your overall system against the worst case setting where an attacker takes over some sub-setting
machines and does whatever they want in those sub-set machines. So they could for instance
try to poison data on those machines or try to send wrong information back to the rest of the
system or whatever else. And in Byzantine machine learning or Byzantine robustness the focus
is on typically the idea is the attacker is doing this as a way to break the system. So if they
can send the right wrong information if you like they can get the model to converge to whatever
they like and sort of get arbitrarily bad behavior in your system. And so what you what we're
interested in is our strategies one for just better distributed machine learning as normally as
our initial target and just coming up with better optimization strategies both for standard
distributed learning and also this idea of what is generally called federated machine learning.
This idea of again training machine learning systems distributed way without sharing
say gradients or sort of sharing information at every setting. You said without sharing gradients or
the idea in federated machine learning very close to what is sometimes called local SGD is that each
device runs several steps of gradient descent on their local data. And instead of sharing
gradients at every step as you would do in a standard distributed setting they would share model
parameters after a certain amount of training on the device. So again if you do this plus a few
extra steps you can get privacy you can get much lower communication overhead. If you allow for
machines to come in and out then you get something close to say what the Google system a federated
learning system does where they can train say next word prediction models on your cell phone without
actually sort of transporting your text data all the way to Google server so you can get privacy
you can get some robustness but you can still get sort of reasonable performance hopefully close to
what you would get if all the data was in the same place. There are a variety of strategies but the
rough ideas is again targeting this distributed optimization in a way that hopefully replicates
something close to centralized optimization. Most of our work has focused on the setting where there
is a server somewhere and the optimization or the learning setup is that the workers communicate
with the server every few rounds so either again using gradients or using models if it's either
federated as standard distributed settings. And so there's a simple strategy actually that was quite
popular when people started getting interested in robustness in distributed learning systems.
So the idea was well mostly federated averaging which is the standard method for
sort of federated machine learning or even standard distributed learning. Most of the methods work
by averaging the gradients at the centralized server so the workers do whatever they do for a few
steps one or a case steps they send some information back to the server server averages it and that
becomes the information it gets sent across. And so the idea initially was well we know how to do
robust great robust averaging. So if the potential failure point is this average of lots of
different to the model parameters across devices and there's the potential for some of these
model parameters to be incorrect or explicit attacks then we could do robust averaging and if you
do robust averaging then you avoid the possibility that one of these devices can lead your model in
sort of the wrong direction so that over steps you know have this what is again known as business
behavior so get you to arbitrarily have a bad estimate or a bad model parameter by sort of
optimization failure. So a lot of the early work in this area focused on trying to come up with or
use robust ways of computing averages. So they're placed say the mean with the median which is
known to be robust to sort of lots of outliers and other more sophisticated schemes that is a
trimmed mean approach way throw away the sort of largest and smallest elements in your average.
A few other more sophisticated this crumb which is quite popular as a way to do this sort of
robust average. What we showed last year is an interesting behavior which I think was not obvious
when we're I think folks would first think about this problem. So it turns out that
you can construct a sequence of sort of bad model updates such that the mean remains close
but the model parameter diverges over optimization steps. And the issue is that sort of the mean being
close is not the same as sort of the optimization direction for lack of better term going in the right
direction. So I'll try to explain this in the sort of standard distributed learning case. I think
this is where it's maybe clear to see. So in the standard distributed learning case all the workers
compute gradients under local data they send the gradients to the centralized server centralized server
computes the average of the gradients and sends this back out to the workers and this average of
the gradients is what is used for sort of the next step of gradient descent. So again the original
papers try to just compute this average gradient in a robust way to avoid failures. So again if some
steps of the workers were sending wearing information as long as use the robust average the mean
would be close to the sort of original mean even if there were a few failures. But it turns out
that if I'm running gradient descent I construct gradient updates such that the means are close
but the direction of the sense is actually if you like even opposite from the direction that
it should be going. So I can get the model to do really anything I want while keeping the
means close at every step. Is the idea that you're accumulating small distances in the same
in a deliberate direction over time and thus you're throwing your mean off or is it more nuanced
than that? It's close. It boils down to the difference between sort of distance and angle.
So what really matters for good gradient descent self-management is to be going in the right direction.
So the way you construct the attack is you keep the distance close where you get the direction to
just be a little bit off and you do this and accumulate this sort of a little bit off direction
over steps. And so again you can get the model to do really whatever you want in this distributed
setting. So does paper I believe it's in UAI last year where we show this? Yes, UAI 2019 called
fall of empires breaking Byzantine tolerant SGD by inner product manipulation where we essentially
break all of the existing methods for try to do robust distributed learning by computing robust
averaging. Does the paper demonstrate that in a scenario that is real worldish to some degree
that the attacker has enough information to actually execute the attack? That's a great question.
So the setup in a lot of security work and very definitely into Byzantine world is that you try
to protect against the worst case with the hope that if you get the worst case then you sort of
you get easier cases for free including for instance benign cases so things just fail and turn off.
So the focus intentionally is not on what is easily rep upcubal in real world settings. It's on
if the attacker had full knowledge of everything and could do whatever they want, what could they do?
And can I come up with a procedure that's robust to the thing that they could do? And it is that
if you're robust in a setting then you get easier settings for free and it's typically the way a
lot of security folks think about so security design is can I be secure against the worst case
behavior? You could argue I think reasonably that sometimes it's a bit of overkill but again
the idea is if you get this you get easier cases for free. And luckily in machine learning
there are distributed machine learning there's lots of interesting things we can say and actually
importantly you don't lose that much in terms of sort of overall training performance. So we have
both theory and lots of experiments showing that if you do reasonable things you still get reasonable
training performance not that far from what you'd get in standard settings. In fact I'd say more
specifically if there are no failures you get something very close to training in a standard way.
If there are failures you can show that many of the failures would break the standard training system.
You can still train and get good results though you converge slower than if you were in a sort
of completely benign setting with no failures. So you pay some cost but you pay a cost that sort of
allows you to actually get results compared to settings where again if there was an attack you
would just be completely vulnerable. So what's the nature of the approach? So one of the approaches
that we found very effective is a bit of a twist on the problem but I like it because I think it's
I think it's clever. So the idea is to use a validation set. So what we do is we actually
we assume that the centralized server has access to an additional data set that's separate from
the data sets trained on that the workers are training on. And so what we do is in every step
use your centralized data set to check whether the gradients that you're getting are helping you
optimize better or not. So I think pushing the model in a direction that sort of minimizing
validation error essentially. So it feels like cheating but maybe I can argue that it's not
it's ever ways. So one so you're paying some cost you're doing a bit of extra work on the centralized
server. So before the centralized server all it had to do was compute an average or maybe a sort of
smart average. Now the centralized server is doing this extra work of checking whether you're
making some progress or not based on the gradient that you got. We can show that you can do a
sort of highly stochastic check. So in particular you can construct a checking procedure
that roughly boils down to taking one sample and checking whether this one sample
slightly improves and and loss. And if that happens that's good enough to be able to check
whether things are making progress and this is good enough to give you the protection that you
need. So somehow the claim is that in terms of computational cost the overhead you're going to
pay is low. In practice the win is quite large so this kind of approach is robust to some of the
attacks I talked about where none of the robust averaging methods are. So all the robust averaging
methods are susceptible to again getting the distance right so making the distance close but
getting the angle completely wrong so going in the wrong direction. Whereas this method that checks
whether you're actually making progress stochastically. So again just using a few samples can do
this very efficiently and is able to give you protection against this sort of kind of worst case
attack. So this is a paper we presented at ICML last year which we call Zeno Rebus synchronous
SGD with an arbitrary number of Byzantine workers. Another nice property of the method
is that previous work gave you protection to up to half of the machines potentially being
corrupted. In this setup you can show that we have protection for much much higher
potential fraction of corrupted workers. So really you need roughly maybe one good worker in
your system to actually make progress. Again of course if there's only one good worker you pay
some cost so everything will be slower. But another nice thing is if there's corruption but it's
low magnitude you can actually be okay so you can actually use corrupted gradients to still make progress.
Again as long as you can imagine an incorrect gradient being computed that's sort of a little bit
off from the true gradient but not completely off sort of benign failure setting. We can still make
progress using that where sometimes the standard method can have trouble with it. So again this
line of work I think is quite exciting because it's sort of coming up with good ways for training
large scale distributed systems with robustness built in. Along the lines of kind of the assuming
the worst case security scenario is there are you or anyone else working on some kind of model where
the workers and the centralized server kind of are in cahoots to determine if either of them is
is corrupted because the centralized server is kind of a weak point in your previous example.
I think that's a very important point. So this is assuming sort of centralized server that can be
trusted essentially. There is a bit of work and not fully trusting the centralized server. I'm
familiar to have to look up to get the exact references on paper is that where there's sort of
two layers of checks or checks in both directions. I think that that entire direction is extremely
interesting. So there's a bit of work thinking about that. Though again it's a bit sort of somewhat
early days in that line of research. Okay. Cool. Yeah. So we've been thinking about this like I said
for standard distributed learning settings where you're pushing gradients around. We're thinking
about this for federate learning settings where you're pushing model parameters around. Combining
this with just trying to scale up distributed learning to be faster doing adaptive learning rates
and sort of other methods to get distributed learning to convert faster. So again this whole
echo system of scaling up distributed learning and combining this with robustness because of
special failure modes that show up when you train distributed learning systems.
Very interesting stuff. I'm starting to think about how we're going to combine all this into a single
title more. Yeah. Yeah. Well usually just sticking in and I also have to do it when I write up
documents for some reason. And I didn't get to talk about any of my sort of cognitive near imaging
work. So if I made your job a little bit easier. Yeah. So some of the threads. I mean of course
clearly those sort of basic tools. Go back and forth. I should say and I don't think I emphasize
this enough. I've been really lucky to have excellent colleagues and truly amazing students
to work with here. And a lot of those sort of great ideas come from these years and from the students.
So I mean sometimes I say that my job is to just get out of their way and sort of get them to
do great things. So yeah. Sometimes that leads to a bit of sort of breath and ideas. Things that
are somewhat constant. There's some I think the core mathematical tools are roughly. There's
some new things that come on but a lot of the basics are shared. I think there's some nice
cross-talk between for instance I didn't mention this but because we're thinking about robustness now
as in general we've been thinking about robustness and other settings. So in the standard learning
settings we think about robustness and loss functions, robustness and listation. So there's again
some cross-talk between ideas that come from this. And the other way we're thinking about distributed
learning when you have again complex settings, complex losses, interacting, complex prediction
problems. So how does one change a distributed learning problem to account for the complex setting?
So I think there is some cross-talk that comes between both these. And again share tools but
I think it's fair to say that they're quite distributed, not pun intended.
Well Semy, thanks so much for taking some time to walk us through what you're up to.
Yeah, it was a pleasure. Thank you for time and I was glad to have some time to finally chat
with you. I think we tried to set this up for you sometimes. Yeah, absolutely.
Absolutely. We had a chance to go through this. It was a pleasure. Absolutely. Same. Thank you.
All right everyone that's our show for today. For more information on today's show
visit twomolai.com slash shows. As always, thanks so much for listening and catch you next time.
