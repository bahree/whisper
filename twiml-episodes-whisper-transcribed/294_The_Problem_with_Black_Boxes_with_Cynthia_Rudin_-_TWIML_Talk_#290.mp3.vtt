WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.400
I'm your host, Sam Charrington, hey what's up everyone, I'm on the road so I hope

00:34.400 --> 00:39.640
you'll forgive that this intro may not be up to our usual audio standards.

00:39.640 --> 00:44.640
Last week we dropped a huge Twimble Con content update announcing that we double the number

00:44.640 --> 00:48.960
of sessions and sharing a bunch of the amazing speakers and sessions that we've got in store

00:48.960 --> 00:50.640
for you.

00:50.640 --> 00:56.720
One content, another important focus for us in creating this event is community.

00:56.720 --> 01:01.320
We see this conference as an opportunity to help create and support a community of professionals

01:01.320 --> 01:06.600
dedicated to helping their organizations be more successful with machine learning and AI

01:06.600 --> 01:09.320
by sharing and learning from one another.

01:09.320 --> 01:13.360
As a part of this commitment to community we want to make sure that you're aware of our

01:13.360 --> 01:17.080
volunteer program and diversity scholarships.

01:17.080 --> 01:22.200
To learn more about both of these great initiatives visit twimblecon.com slash volunteer and if

01:22.200 --> 01:25.920
you're interested fill out the appropriate application.

01:25.920 --> 01:30.840
Submissions will be accepted through August 30th so get them in as soon as possible.

01:30.840 --> 01:33.120
And now on to the show.

01:33.120 --> 01:41.000
Alright everyone, I am on the line with Cynthia Rudin, Cynthia is a professor of computer

01:41.000 --> 01:47.040
science, electrical and computer engineering and statistical science at Duke University.

01:47.040 --> 01:49.840
Cynthia, welcome to this week in machine learning and AI.

01:49.840 --> 01:51.520
Thanks for having me.

01:51.520 --> 01:56.600
Let's jump right in and get started with a little bit of your background and how you came

01:56.600 --> 01:58.840
to work in machine learning.

01:58.840 --> 02:04.080
Yeah, I think I ended up working in machine learning because I like the name support vector

02:04.080 --> 02:05.080
machines.

02:05.080 --> 02:07.480
No, I'm just kidding.

02:07.480 --> 02:11.680
Well, I'm actually sort of only partially kidding.

02:11.680 --> 02:18.360
I happen to be walking around in Princeton at any sea labs and at the time there are

02:18.360 --> 02:23.800
a lot of researchers there who are really, really good at support vector machines and I walked

02:23.800 --> 02:29.040
into Gary Flakes office and he gave me Vapnik's book and I read it and it was, it was very

02:29.040 --> 02:34.640
difficult to read for me at that time, but after that I was sold, you know, it was, it

02:34.640 --> 02:38.120
was going to be machine learning all the way so that's it.

02:38.120 --> 02:47.040
Yeah, nice and were you, this was during grad school or, okay, awesome, I had, I ended up

02:47.040 --> 02:52.480
having two PhD advisors, so I'm Anchored Dubsheet was one of them, the first one and she

02:52.480 --> 02:57.960
said to me, you know, I haven't, you know, I don't work in machine learning is it okay,

02:57.960 --> 03:02.160
you know, after I asked her to be my advisor, you know, I said, Anchored, will you be my

03:02.160 --> 03:08.440
advisor after Gary left and she said, is it okay that there's no, that I, I don't work

03:08.440 --> 03:11.280
in machine learning and I've never advised a student in machine learning and I said,

03:11.280 --> 03:13.920
yeah, don't worry about it, everything will be fine.

03:13.920 --> 03:14.920
Nice, nice.

03:14.920 --> 03:15.920
Yeah.

03:15.920 --> 03:22.080
I also noticed that you're a three-time winner of the Informed Innovative Applications

03:22.080 --> 03:24.600
in Analytics Award.

03:24.600 --> 03:29.840
I, Informed was one of the conferences that I went to in grad school, but I've always

03:29.840 --> 03:40.520
associated with like industrial engineering more so than computer science or machine learning.

03:40.520 --> 03:45.240
You're a chair of some element of that conference as well, is that right?

03:45.240 --> 03:53.360
Well, okay, so Informed is, is the, the organization that, like the main organization in the

03:53.360 --> 04:00.280
United States that is, represents operations research and management science and the

04:00.280 --> 04:07.880
work that I do is very closely related to aspects of decision making and management.

04:07.880 --> 04:12.080
And so that's why I became involved with this organization.

04:12.080 --> 04:18.080
Informed does have a very active data mining section and, and I was a past, I'm a past

04:18.080 --> 04:23.840
chair of that data mining section and I'm also an editor for one of the journals, one

04:23.840 --> 04:29.600
of the top journals in that, sponsored by the organization called Management Science.

04:29.600 --> 04:34.000
Tell us a little bit about your, your research into, into that area.

04:34.000 --> 04:38.040
It sounds like it's fairly interdisciplinary.

04:38.040 --> 04:45.400
Yeah, I started off as a theorist working on kind of theoretical machine learning problems,

04:45.400 --> 04:48.040
convergence of adabuse specifically.

04:48.040 --> 04:54.680
But then after I, you know, graduated and after I finished my postdoc, I went to work on

04:54.680 --> 04:59.360
a project, an interdisciplinary project with the power company in New York City.

04:59.360 --> 05:05.120
The power company is called Con Edison and they, their job is to maintain the, the oldest

05:05.120 --> 05:10.600
and largest underground power network in the world, which is the New York City power

05:10.600 --> 05:11.600
grid.

05:11.600 --> 05:15.920
And at the time we were doing something that was, you know, totally crazy, which is, can

05:15.920 --> 05:18.320
you maintain the power grid with machine learning?

05:18.320 --> 05:23.600
Like can you use machine learning to help prioritize inspections and repairs on the grid?

05:23.600 --> 05:25.160
One was this?

05:25.160 --> 05:33.440
This was between 2007 and I think I worked on it all the way up through 2012 and beyond.

05:33.440 --> 05:34.440
Okay.

05:34.440 --> 05:35.440
Yeah.

05:35.440 --> 05:42.680
And, you know, no one, no one had done this before and we were using data that was from

05:42.680 --> 05:43.680
the 1890s.

05:43.680 --> 05:47.680
I mean, all these, all these huge, right?

05:47.680 --> 05:53.840
This was a huge amount of very complicated data that involved free text documents written

05:53.840 --> 05:57.240
by people who were trying to maintain the power network.

05:57.240 --> 06:02.440
There were engineers going into the, the manholes and doing repairs and they were giving

06:02.440 --> 06:06.360
that information to the dispatchers who were typing it in.

06:06.360 --> 06:11.520
And we also had accounting records dating back from all the way since the power grid essentially

06:11.520 --> 06:15.760
started, you know, since the days of, of Edison, Thomas Edison.

06:15.760 --> 06:20.480
So it was a really, really interesting, very, very challenging data set.

06:20.480 --> 06:22.480
A lot of the data we didn't know what it was.

06:22.480 --> 06:26.760
It was just sort of a pile of text and a pile of, just like garbage to us.

06:26.760 --> 06:31.040
We had no idea what it was, but it, you know, and data science as a field hadn't really

06:31.040 --> 06:34.520
been invented yet when we were working on this project.

06:34.520 --> 06:40.400
So the job of my team was to try to put all of these data together and figure out which

06:40.400 --> 06:44.240
manholes were the, were the ones that were the most likely to explode.

06:44.240 --> 06:51.200
So that, and do the repair work before, you know, that bad event actually happened.

06:51.200 --> 06:52.200
Wow.

06:52.200 --> 06:58.040
So this was a kind of big change from doing the theoretical work that I had done in the

06:58.040 --> 06:59.040
past.

06:59.040 --> 07:05.600
And while I, while I was working on this power grid reliability project, we made some errors

07:05.600 --> 07:10.760
that, you know, people make when you're working, when you're doing data science, we made

07:10.760 --> 07:15.200
some of the classic errors that, that you'd make.

07:15.200 --> 07:19.000
If you, if you didn't, you know, if you didn't have that experience, and I learned from

07:19.000 --> 07:26.560
that very quickly that black box models were not the way to go, because, you know, because

07:26.560 --> 07:31.280
the mistakes we made were things like not really understanding what the important variables

07:31.280 --> 07:34.440
were that, that we were using to make predictions.

07:34.440 --> 07:40.120
And if you don't really understand that stuff, you can't really make very good decisions.

07:40.120 --> 07:46.520
So I switched to kind of working on projects that were more along the lines of interpretable

07:46.520 --> 07:47.520
machine learning.

07:47.520 --> 07:52.200
So still keep everything machine learning, keep it data driven, but be able to understand

07:52.200 --> 07:57.760
exactly what the important variables were, how the variables were being combined, and

07:57.760 --> 08:00.320
then be able to make a decision that was informed.

08:00.320 --> 08:04.440
And not just a blind, oh, the model said this kind of decision.

08:04.440 --> 08:11.320
When I talk to folks about interpretability, I'll often ask them if they make a distinction

08:11.320 --> 08:15.080
between interpretability and explainability.

08:15.080 --> 08:20.760
And that's something that you, is, is very significant distinction to you.

08:20.760 --> 08:25.440
In fact, one of your papers is stop explaining black box machine learning models for high

08:25.440 --> 08:30.200
stakes decision and use interpretable models instead.

08:30.200 --> 08:38.040
Can you talk a little bit about that distinction and, and kind of lead us into a discussion

08:38.040 --> 08:40.880
of the main problem you're taking on with that paper?

08:40.880 --> 08:43.160
Well, let me give you a little bit of history.

08:43.160 --> 08:47.800
The field of data science sort of is evolving so quickly that things go off in different

08:47.800 --> 08:52.240
directions without really carefully thinking about them.

08:52.240 --> 08:54.800
And explainability was one of those things.

08:54.800 --> 08:57.080
So, yeah.

08:57.080 --> 09:03.080
So interpretability is a really old concept that people have been working on since the,

09:03.080 --> 09:10.120
you know, the 80s and 90s, you know, Leo Breiman and Cart and Quinlan, you know, these people

09:10.120 --> 09:14.560
were working on this in the, in literally the early 90s because they realized how important

09:14.560 --> 09:18.480
it was to have models that were interpretable.

09:18.480 --> 09:24.320
And that field has existed since that time, but then recently in the last few years,

09:24.320 --> 09:28.640
people have been working kind of only with black box models.

09:28.640 --> 09:35.160
And I think this is partly an illusion due to the fact that neural networks were performing

09:35.160 --> 09:39.520
really well for computer vision, starting in around 2012.

09:39.520 --> 09:43.440
And so people decided that, you know, we should only be teaching neural networks and that

09:43.440 --> 09:47.160
was the only type of machine learning that should be going forward.

09:47.160 --> 09:51.560
And so there was this whole group of people that could really, that really only knew how

09:51.560 --> 09:56.120
to do neural networks and black box models.

09:56.120 --> 10:03.560
There's also kind of the industry is propelled by these complex models because if these very

10:03.560 --> 10:07.120
simple models, then they can't license them and they can't make a profit off of them.

10:07.120 --> 10:11.480
So they really like to keep their models complicated.

10:11.480 --> 10:16.400
So because of that, people then tried to explain the black box models because they either

10:16.400 --> 10:20.560
couldn't produce interpretable models or they wanted to use black box models, but produced

10:20.560 --> 10:23.720
some sort of explanation for what they were doing.

10:23.720 --> 10:31.480
And you know, that, that evolved sort of very quickly kind of around 2015, 2016.

10:31.480 --> 10:35.360
And then sort of people forgot that, wait, you know, we might be able to produce interpretable

10:35.360 --> 10:38.800
models that are just as accurate as these black box models that, that wasn't sort of

10:38.800 --> 10:44.120
a thought that, that, that people were thinking in the last few years.

10:44.120 --> 10:49.040
And I wrote this paper to kind of remind people that, hey, you know, you don't always need

10:49.040 --> 10:50.040
a black box.

10:50.040 --> 10:54.360
You, and in fact, if you try to use black box models for high stakes decisions, really bad

10:54.360 --> 10:55.680
things can happen.

10:55.680 --> 10:59.480
And I gave a bunch of examples in that paper where bad things have happened because people

10:59.480 --> 11:04.200
tried to use overly complicated models when they didn't need them.

11:04.200 --> 11:07.120
I can go through some of the examples if you think it would help.

11:07.120 --> 11:09.000
But please, okay.

11:09.000 --> 11:13.760
So well, one of the examples is in criminal recidivism prediction.

11:13.760 --> 11:19.080
So there's a model that's used sort of throughout the US justice system, which is a very, it's

11:19.080 --> 11:23.160
a very unusual model for the justice system in that it's a very complicated model.

11:23.160 --> 11:25.800
It involves 137 factors.

11:25.800 --> 11:28.840
And nobody knows exactly how those factors are combined.

11:28.840 --> 11:30.320
Is this the compass model?

11:30.320 --> 11:31.800
Yes, this is compass.

11:31.800 --> 11:32.800
Okay.

11:32.800 --> 11:37.720
It's used by a private company that licenses access to software.

11:37.720 --> 11:42.960
And compass is used in, it's used regularly in parole decisions.

11:42.960 --> 11:48.720
And there was at least one parole decision that was very famous where it led to like an

11:48.720 --> 11:52.120
article in the New York Times, where the person was denied parole.

11:52.120 --> 11:57.760
And they figured out afterward that it was because of a typographical error on their

11:57.760 --> 11:59.360
compass score sheet.

11:59.360 --> 12:03.880
So in other words, the factors that went into that black box model, one of them had a typographical

12:03.880 --> 12:04.960
error in it.

12:04.960 --> 12:07.760
And nobody spotted it until after the parole decision was made.

12:07.760 --> 12:10.400
And by that point, you couldn't reverse the parole decision.

12:10.400 --> 12:14.720
So here you have a typographical error making a high stakes decision about someone's, you

12:14.720 --> 12:16.720
know, future.

12:16.720 --> 12:22.360
And I, you know, I don't, I think that's a kind of procedural unfairness that really kind

12:22.360 --> 12:23.960
of shouldn't exist, right?

12:23.960 --> 12:28.800
We shouldn't be making high stakes decisions that deeply affect people's lives based on

12:28.800 --> 12:33.120
typographical errors and those types of kind of clerical issues.

12:33.120 --> 12:34.120
Right.

12:34.120 --> 12:35.120
Right.

12:35.120 --> 12:40.120
And the underlying implication is that if this wasn't a black box model and we understood

12:40.120 --> 12:45.680
the basis upon which it was making this decision, we would have easily seen that in the process.

12:45.680 --> 12:49.280
Or the people that were responsible for this process.

12:49.280 --> 12:51.120
Yeah, that's the implication.

12:51.120 --> 12:54.920
I mean, there have also been typographical errors in very simple models.

12:54.920 --> 13:00.320
But if you think about, you know, the number of typographical errors in the simple models

13:00.320 --> 13:05.880
versus the more complicated models, you know, even if you do a very simple calculation,

13:05.880 --> 13:13.040
you know, if you have kind of a 1% error rate in your typographical errors, then it means

13:13.040 --> 13:17.920
that on almost every compass score sheet, there should be one typographical error.

13:17.920 --> 13:22.560
I'm not sure if that if that calculation kind of holds water, but the fact is that the

13:22.560 --> 13:28.040
more complicated the model, the much, it's much, much, much more easier to make an error

13:28.040 --> 13:30.000
than if it's a much simpler model.

13:30.000 --> 13:34.600
And also the simpler models can be double checked and triple checked, whereas the more complicated

13:34.600 --> 13:37.240
models is very, very difficult to check.

13:37.240 --> 13:43.600
There were other errors too that kind of propagated throughout society because of black box

13:43.600 --> 13:44.600
models.

13:44.600 --> 13:50.120
So for instance, another example was given to me by someone at the EPA at the Environmental

13:50.120 --> 13:51.600
Protection Agency.

13:51.600 --> 13:57.960
They had a very understandable model, they have a very understandable model for air quality

13:57.960 --> 14:02.840
that's used to assess whether it's safe to go outside.

14:02.840 --> 14:09.760
And at some point during the California wildfires, Google replaced the EPA's air quality

14:09.760 --> 14:15.000
measure with one from a, that was a proprietary model from a company.

14:15.000 --> 14:20.160
And that model told everyone that it was safe to go outside on a day when people saw

14:20.160 --> 14:23.600
a layer of ash on their cars.

14:23.600 --> 14:27.360
So yeah, so what went wrong with that model?

14:27.360 --> 14:29.080
I'm not sure.

14:29.080 --> 14:35.400
And it's pretty clear that the company who released that model wasn't, you know, there's

14:35.400 --> 14:36.720
something that went wrong.

14:36.720 --> 14:38.280
And nobody will know what it is.

14:38.280 --> 14:42.840
But clearly they didn't troubleshoot as carefully as they could have.

14:42.840 --> 14:46.720
And I'm guessing if they had used a model that they actually understood, then this kind

14:46.720 --> 14:50.080
of blatant mistake wouldn't have happened.

14:50.080 --> 14:57.960
Is there a framework or taxonomy for these kinds of errors or thinking about the different

14:57.960 --> 15:05.200
failure modes of black box systems or is it just, you know, black box systems, we don't

15:05.200 --> 15:08.480
understand them and that's a problem.

15:08.480 --> 15:12.680
Well, black box models can come in a couple of different varieties.

15:12.680 --> 15:16.640
Usually when I say a black box model, I mean, either a model that's too complicated

15:16.640 --> 15:21.480
for any human to understand or it's, you know, it's a function that's too complicated

15:21.480 --> 15:25.520
for humans to understand, like it has a million logical conditions in it or a bunch of things

15:25.520 --> 15:30.400
added together and then transformed and then added together again and then transformed.

15:30.400 --> 15:35.440
Or another type of way a model can be black boxes if it's proprietary, like if it's owned

15:35.440 --> 15:39.560
by a company that doesn't want to release the secret formula.

15:39.560 --> 15:43.760
And very often black boxes are both.

15:43.760 --> 15:51.840
So I think what you're asking me is to kind of talk a little bit more about kind of a way

15:51.840 --> 15:56.840
to go from black box to interpretable on sort of a gray scale, if you will, is that what

15:56.840 --> 15:57.840
you're asking about?

15:57.840 --> 16:05.600
I guess the step before that in my mind is, so you have, you know, this kind of class

16:05.600 --> 16:15.520
of models that are black box or, you know, otherwise opaque and you gave some examples of kind

16:15.520 --> 16:17.240
of the ways in which they fail.

16:17.240 --> 16:23.680
And I'm curious whether, you know, there are categories of, you know, failure modes,

16:23.680 --> 16:31.400
like, you know, you know, these fail because, and I don't even know what that might be,

16:31.400 --> 16:32.400
right?

16:32.400 --> 16:34.040
Like, you know, problems with the inputs.

16:34.040 --> 16:39.440
Like, you mentioned the, the, the, the typographic errors, like you, you know, they fit, you

16:39.440 --> 16:43.920
know, they could fail because, you know, you give them inputs and inputs are malformed

16:43.920 --> 16:50.720
and you kind of don't understand that, you know, it could be, you know, the underlying

16:50.720 --> 16:57.880
function doesn't well represent, you know, there's some kind of data drift and the, the

16:57.880 --> 17:03.440
model is trained on, you know, oh, yeah, there's, there's all kinds of stuff like that.

17:03.440 --> 17:07.520
I mean, it can go wrong anywhere, you know, when you trust a model, you, if you're going

17:07.520 --> 17:11.520
to trust a black box model, you don't just trust the formula, you actually have to trust

17:11.520 --> 17:14.200
the whole database that the model was trained from.

17:14.200 --> 17:19.120
And there's a lot of issues with trusting data, you know, data, I, I don't know if I've

17:19.120 --> 17:26.280
ever seen a clean database in my life, you know, if data can be very complicated and

17:26.280 --> 17:32.080
have all kinds of things wrong with it, it can be, you know, a lot of data can be missing

17:32.080 --> 17:37.000
or data can not represent the cases that you actually care about, or it can simply

17:37.000 --> 17:43.720
not represent the full range of cases that exist.

17:43.720 --> 17:48.040
Models can go wrong because they can overfit the data, so they can sort of memorize the

17:48.040 --> 17:56.680
data without actually generalizing to new cases, yeah, you can, you can have just about

17:56.680 --> 18:04.840
anything go wrong with a model or a data set, also, you know, a lot of these decisions,

18:04.840 --> 18:07.840
there's a cost involved to making a wrong decision.

18:07.840 --> 18:14.440
And machine learning is a field developed in kind of a low cost environment, you know,

18:14.440 --> 18:22.320
like predicting which advertisements some online user is going to click on.

18:22.320 --> 18:25.560
Now if you give the user the wrong advertisement, it's not a big deal.

18:25.560 --> 18:30.200
But if you deny someone's parole, that's a high cost decision.

18:30.200 --> 18:33.840
And then you also have the problem that the people who create the models are not the

18:33.840 --> 18:36.520
people who suffer those costs.

18:36.520 --> 18:40.920
So the people who created the compass model, they're not subject to it.

18:40.920 --> 18:47.840
So if they make a, you know, a bad prediction on someone because of a typographical error,

18:47.840 --> 18:50.680
the person who suffers is not the person who designs the model.

18:50.680 --> 18:55.760
And there's that, that sort of misaligned incentives that's a serious problem for machine

18:55.760 --> 18:58.960
learning deployment right now.

18:58.960 --> 19:03.240
One more question before we jump into kind of the path to addressing these issues.

19:03.240 --> 19:10.640
Do you, I think early I use opaque and black box kind of interchangeably are those interchangeable

19:10.640 --> 19:16.600
to you or do you, are there nuances there that are important to you?

19:16.600 --> 19:21.680
I think yeah, opaque and black box to me mean the same thing, but interpretable and explainable

19:21.680 --> 19:26.760
mean different things to me, because, you know, interpretable means that you can fully understand

19:26.760 --> 19:32.200
or you can understand the path of computations leading to the prediction, whereas explainable

19:32.200 --> 19:41.480
to me is like a post hoc, can you, can you somehow justify what the model did?

19:41.480 --> 19:46.920
And there are a wide variety of, well, there's a ton of research happening now to your earlier

19:46.920 --> 19:58.120
point on explaining these black box, black box models, but in the end, they're not interpreting

19:58.120 --> 19:59.760
what the model is doing.

19:59.760 --> 20:07.520
They are coming up with an explanation that hopefully correlates to that in some way.

20:07.520 --> 20:14.280
Yeah, but some of those explanations are such poor explanations that it's, you know,

20:14.280 --> 20:18.240
the explanations have almost nothing to do with the original model other than the fact

20:18.240 --> 20:22.840
that they produce predictions that are fairly similar.

20:22.840 --> 20:25.240
So they may not use the same important variables.

20:25.240 --> 20:32.320
They may claim the black box is depend on specific variables that they don't depend on.

20:32.320 --> 20:37.200
And yeah, they're sometimes their explanations are so incomplete that you actually don't

20:37.200 --> 20:41.440
really understand what the black box did at all, like, you know, they might give the same

20:41.440 --> 20:44.480
explanation for all of the different classes.

20:44.480 --> 20:48.800
So you really don't, you really don't actually know what the black box is doing.

20:48.800 --> 20:49.800
What's an example of that?

20:49.800 --> 20:56.120
It sounds like you're describing a system in which the explainability algorithm is like

20:56.120 --> 21:00.000
literally just given explanation that has nothing to do with anything.

21:00.000 --> 21:04.160
Well, you know, it's, it's funny.

21:04.160 --> 21:08.680
It's funny because people think that the explanations are meaningful, but they're not.

21:08.680 --> 21:13.320
So we give an example in the paper of saliency maps.

21:13.320 --> 21:15.560
And this is from my interpretable neural networks group.

21:15.560 --> 21:22.160
This is Oscar Lee and Chef Anchen and several other students where we're, you know, we're

21:22.160 --> 21:25.200
trying to understand what the saliency maps do.

21:25.200 --> 21:27.200
And what are saliency maps?

21:27.200 --> 21:28.200
Okay.

21:28.200 --> 21:31.480
Yeah, it's supposed to highlight the part of an image that the neural network is using

21:31.480 --> 21:32.640
to make its prediction.

21:32.640 --> 21:33.640
Okay.

21:33.640 --> 21:41.040
I like this attention heat maps that we've many of us have seen exactly, exactly.

21:41.040 --> 21:48.000
And so you get this neural network that says, okay, this is a picture of a golden retriever.

21:48.000 --> 21:50.720
And then it highlights the golden retriever's face.

21:50.720 --> 21:54.040
And you say, oh, yes, the network must be right because it's highlighting the golden retriever

21:54.040 --> 21:55.040
space.

21:55.040 --> 21:56.720
That's why it thinks it's a golden retriever.

21:56.720 --> 22:00.880
But then you ask the network, why do you think this image and you give the same image,

22:00.880 --> 22:01.880
okay?

22:01.880 --> 22:04.880
The same image of the golden retriever and you say, why do you think this image is a tennis

22:04.880 --> 22:05.880
ball?

22:05.880 --> 22:09.080
And it highlights exactly the same thing, the golden retriever's face.

22:09.080 --> 22:12.640
And it says, this is why I think it's a tennis ball.

22:12.640 --> 22:18.480
So it's giving you, it's just giving you this meaningless explanation for, you know, you

22:18.480 --> 22:22.680
think it's useful because it's telling you the correct pixels for the correct class.

22:22.680 --> 22:29.720
But it's just edges, you know, it's just telling you, I'm kind of looking over here.

22:29.720 --> 22:35.000
I guess my, my reaction to that is that there's, there's some, maybe there's some nuance

22:35.000 --> 22:36.000
missing there.

22:36.000 --> 22:43.280
Like it's more like conditioned on if this were a tennis ball, this would be why I think

22:43.280 --> 22:45.200
it's a tennis ball.

22:45.200 --> 22:48.080
But that's not what it told you, it told you it thought it was a golden retriever.

22:48.080 --> 22:50.560
So does that make that explanation wrong?

22:50.560 --> 22:55.880
Well, I mean, the, what happens when the neural network predicts, I mean, if it predicts

22:55.880 --> 22:59.480
correctly, fine, it's looking at the pixels that you think it should look at.

22:59.480 --> 23:04.680
But what if it predicts incorrectly, then it's just highlighting the edges as it did before.

23:04.680 --> 23:09.320
You know, there's no way to troubleshoot it because you, you're saying, well, it's looking

23:09.320 --> 23:11.840
at this, you know, there's an image of a golden retriever here.

23:11.840 --> 23:15.560
It says it's a conquer spaniel, but it's highlighting the face of the golden retriever

23:15.560 --> 23:19.080
and saying, this is why I think it's a conquer spaniel, like, right, right.

23:19.080 --> 23:21.440
You know, there's just no way to troubleshoot it.

23:21.440 --> 23:22.440
Yeah, yeah.

23:22.440 --> 23:27.920
Like it's the same thing maps are certainly helpful if you can find out that the network

23:27.920 --> 23:32.960
is looking in the wrong part of an image to make its prediction, but if it's looking

23:32.960 --> 23:37.240
at the correct pixels, you still have no idea what it's doing with those pixels.

23:37.240 --> 23:42.880
Going back to the, your title for this paper, stop explaining black box models.

23:42.880 --> 23:50.520
It sounds like your general contention is that none of these explainability approaches

23:50.520 --> 23:58.160
are adequate, and therefore we should just not be using black box models for high-stakes

23:58.160 --> 23:59.160
decisions.

23:59.160 --> 24:03.360
Well, if I want, if my credit is going to get denied when I go to the bank, I want to

24:03.360 --> 24:04.360
know why it got denied.

24:04.360 --> 24:09.840
I don't want somebody to say, oh, there were a million factors that went into it, but,

24:09.840 --> 24:13.800
you know, the primary one is, is that your credit history is not long enough.

24:13.800 --> 24:15.200
No, that's not good enough for me.

24:15.200 --> 24:20.480
I want to know exactly what part of my credit history made me get this loan, you know,

24:20.480 --> 24:23.080
made my loan be denied.

24:23.080 --> 24:28.400
And if I'm going to be denied parole, I don't want some 137 factors making that decision

24:28.400 --> 24:33.040
for me when the truth is that only three factors are important and I could get an equally

24:33.040 --> 24:35.920
accurate decision with only three factors, right?

24:35.920 --> 24:38.280
What if it was a typo that made the decision for me?

24:38.280 --> 24:41.920
What if my credit history wasn't entered in the computer correctly?

24:41.920 --> 24:47.480
There's a lot of reasons, really good reasons why for high-stakes decisions we should be

24:47.480 --> 24:53.000
using interpretable models rather than black boxes with post-doc explanations.

24:53.000 --> 24:58.040
Given those kinds of examples, one would think you wouldn't get a lot of resistance to

24:58.040 --> 25:05.680
that, but, you know, yet models like Compass and others exist, suggesting that, you know,

25:05.680 --> 25:08.280
maybe those folks aren't reading your paper.

25:08.280 --> 25:14.880
What needs to happen and, you know, what do you, do you propose something in the paper

25:14.880 --> 25:21.240
for, you know, getting us from, you know, where we are today to where we need to be?

25:21.240 --> 25:28.600
Well, so, yeah, I have gotten some resistance and I think part of the problem is that people

25:28.600 --> 25:30.600
really love their black boxes.

25:30.600 --> 25:36.560
They love the idea that a black box can uncover secret hidden patterns that they don't think

25:36.560 --> 25:40.360
an interpretable model could, you know, they say it's too simple, it couldn't possibly

25:40.360 --> 25:45.760
be that accurate, which for many problems is not true.

25:45.760 --> 25:49.800
In particular, for a criminal recidivism prediction, we've been able to produce interpretable

25:49.800 --> 25:54.720
models that are just as accurate as the best black box machine learning models.

25:54.720 --> 26:00.400
For credit risk assessment, we've worked on some data from FICO for an interpretable

26:00.400 --> 26:04.320
or an explainable machine learning challenge, but even in that challenge, you didn't need

26:04.320 --> 26:07.840
a black box model, you could, you could do it with an interpretable model.

26:07.840 --> 26:11.960
So, I think, you know, a lot of people, first of all, they love their black box models

26:11.960 --> 26:15.800
because they don't believe interpretable models exist, but also they want to make money

26:15.800 --> 26:17.880
from their black boxes.

26:17.880 --> 26:23.960
And they think the black boxes, you know, they uncover secret hidden patterns.

26:23.960 --> 26:28.000
And also black boxes are much easier to train than interpretable models, so that's a major

26:28.000 --> 26:29.000
issue.

26:29.000 --> 26:30.000
Easier to train?

26:30.000 --> 26:34.320
Yeah, it's much easier to train a black box than to train something that's interpretable,

26:34.320 --> 26:38.480
because if you're training the black box, you don't have to have constraints, you know,

26:38.480 --> 26:41.440
technical constraints on the model that force it to be interpretable.

26:41.440 --> 26:46.320
Whereas if you're creating interpretable models, you have to, you have to minimize loss,

26:46.320 --> 26:50.680
you know, make it accurate, subject to some kind of interpretability constraint.

26:50.680 --> 26:54.600
And those constraints make the problems, the optimization problems harder.

26:54.600 --> 26:59.640
So it's actually much harder to design an interpretable model than it is to design a

26:59.640 --> 27:01.160
black box model.

27:01.160 --> 27:05.280
But the software for designing interpretable models has advanced considerably over the

27:05.280 --> 27:07.000
last however many years.

27:07.000 --> 27:13.240
And so, you know, it's kind of sad that people are not even trying to construct interpretable

27:13.240 --> 27:21.200
models that they want to instead just run their black box algorithms and then try to explain

27:21.200 --> 27:24.880
what they're doing afterward rather than actually go to the trouble of correcting the underlying

27:24.880 --> 27:28.440
problem and designing the model to be interpretable in the first place.

27:28.440 --> 27:35.760
Let's dig into this a little bit more. When I hear interpretable models, I think the,

27:35.760 --> 27:41.440
I tend to interpret that, or let's not overuse interpret.

27:41.440 --> 27:50.560
The picture that forms for me is one of using kind of simpler models that have some

27:50.560 --> 27:56.560
inherent trait of interpretability, like a decision tree.

27:56.560 --> 28:06.400
For example, as opposed to what I thought I heard you suggesting was a model that may

28:06.400 --> 28:12.360
be more complicated than that, but has, you know, it's trained against some interpretability

28:12.360 --> 28:13.360
constraint.

28:13.360 --> 28:18.040
Well, decision trees you could write down as an optimization problem.

28:18.040 --> 28:19.040
No.

28:19.040 --> 28:26.920
So, you know, maximize accuracy, subject to a constraint on the size of the decision tree.

28:26.920 --> 28:32.760
Now, you know, the most popular algorithms for decision trees are Carton C4.5 and those

28:32.760 --> 28:36.320
algorithms stay back from the 90s and so they're not that good.

28:36.320 --> 28:43.680
I mean, they work, you know, they work surprisingly well for an algorithm being from the 90s, but

28:43.680 --> 28:49.640
they aren't as good as, you know, the modern machine learning methods because Carton C4.5

28:49.640 --> 28:53.200
don't actually optimize anything, you know.

28:53.200 --> 28:55.240
They're not optimization techniques.

28:55.240 --> 29:01.480
So, you know, over the last several years, there have been several teams working on optimal

29:01.480 --> 29:03.600
decision trees.

29:03.600 --> 29:08.520
And I gave an example in the please stop explaining paper of the work that we've done in the

29:08.520 --> 29:13.120
corals project, which is an optimal decision list paper.

29:13.120 --> 29:20.200
So the goal is to globally optimize over all possible decision lists, the, you know, minimize

29:20.200 --> 29:26.680
the loss, subject to the model being sparse.

29:26.680 --> 29:30.720
So, you know, that code is available and people could download and use it.

29:30.720 --> 29:32.640
Can you elaborate on that a little bit more?

29:32.640 --> 29:38.440
I have a decision lists relative to decision trees is something that I'm not too familiar

29:38.440 --> 29:39.440
with.

29:39.440 --> 29:42.320
Oh, a decision list is a one-sided decision tree.

29:42.320 --> 29:43.920
It's a series of if-then rules.

29:43.920 --> 29:44.920
Got it.

29:44.920 --> 29:45.920
Okay.

29:45.920 --> 29:46.920
Yeah.

29:46.920 --> 29:51.200
Decision lists are exponentially easier to create than decision than full blown decision

29:51.200 --> 29:52.560
trees.

29:52.560 --> 29:59.400
And both decision trees and decision lists to create them optimally is computationally hard.

29:59.400 --> 30:02.600
It's NP hard with no polynomial time approximation.

30:02.600 --> 30:08.280
So these are actually very, very hard optimization problems, but, you know, computers have increased

30:08.280 --> 30:14.320
their speed, you know, a million times in the last, oh, 20 years or something like that.

30:14.320 --> 30:20.680
So we actually can solve optimal decision list problems in reasonable amounts of time

30:20.680 --> 30:23.760
for reasonably large data sets.

30:23.760 --> 30:27.240
And that's what the corals project is about.

30:27.240 --> 30:33.840
With corals, have you applied it to some of the same types of problems, you know, for

30:33.840 --> 30:39.200
which you've kind of given these examples of, you know, the failures of black box models?

30:39.200 --> 30:40.200
Yeah.

30:40.200 --> 30:46.360
So we actually applied corals to the criminal recidivism problem.

30:46.360 --> 30:47.360
Okay.

30:47.360 --> 30:55.520
And what we found was that we found a very, very tiny model that I can probably tell you

30:55.520 --> 30:59.640
what it is if I can just grab it real quick, but we found a really small model that was

30:59.640 --> 31:04.600
just as accurate as, as compass or any of the other machine learning methods.

31:04.600 --> 31:09.200
And the model is so simple, it says, like, if you're between 18 to 20 years old in your

31:09.200 --> 31:14.640
mail, predict, you'll be arrested within two years, Elsa, if you're between 21 and 23

31:14.640 --> 31:18.720
and you have two to three prior offenses, predict, arrest, Elsa, if you have more than three

31:18.720 --> 31:21.440
priors, predict, arrest, otherwise predict, no arrest.

31:21.440 --> 31:27.240
So it's basically if you're younger and you have more priors, more prior crimes, then

31:27.240 --> 31:30.360
the algorithm predicts that you're more likely to be arrested.

31:30.360 --> 31:37.640
And this is, this is the algorithm making a prediction kind of independently based on

31:37.640 --> 31:38.640
the data.

31:38.640 --> 31:43.320
This is not, you're not trying to fit against what compass might do.

31:43.320 --> 31:49.040
No, this is just, this is just trying to predict the outcome, which is whether the person

31:49.040 --> 31:54.080
will be arrested within two years, this is data from Broward County, Florida.

31:54.080 --> 31:57.960
And we compared it directly against the compass scores.

31:57.960 --> 32:02.760
And what we found was that the corals model and the compass model were equally accurate

32:02.760 --> 32:08.960
in predicting whether someone will be arrested within two years of their release.

32:08.960 --> 32:13.440
So that tiny little model that I just told you, compared to a complicated black box

32:13.440 --> 32:20.080
model with over 130 factors, both are about equally accurate to predicting arrest.

32:20.080 --> 32:25.360
And also, if you try any other machine learning method, boosted decision trees right

32:25.360 --> 32:29.960
and for us, anything you want, you try it on the same data from Florida and it'll be

32:29.960 --> 32:33.200
almost equally accurate to both compass and corals.

32:33.200 --> 32:38.000
So as far as we can tell, for predicting arrest, there doesn't seem to be any benefit

32:38.000 --> 32:43.720
of using a very complicated black box model, you can use a very small decision tree to get

32:43.720 --> 32:44.720
it.

32:44.720 --> 32:47.520
It's a lot of computational work to produce that decision tree.

32:47.520 --> 32:55.040
But once you have the decision tree, there's no clear reason to use anything more complicated.

32:55.040 --> 33:00.280
When I think of black box models and you referenced this earlier, I think a lot of neural networks

33:00.280 --> 33:10.520
and deep learning and they are also notoriously computationally expensive and also data,

33:10.520 --> 33:19.880
you know, sample intensive, do the models that you're the interpretable models like corals,

33:19.880 --> 33:22.760
which you're also describing as computationally expensive?

33:22.760 --> 33:25.680
Do they, are they applicable for similar problems?

33:25.680 --> 33:29.920
And if so, do you have a sense for their relative computational expensiveness?

33:29.920 --> 33:34.320
Well, okay, so that gets us into computer vision.

33:34.320 --> 33:37.680
So there are certain problems that are just different from the other problems.

33:37.680 --> 33:44.400
So computer vision, speech, there are certain natural language processing problems.

33:44.400 --> 33:48.120
Some of these particular problems are the ones that machine learning algorithms have

33:48.120 --> 33:53.280
been super successful at starting from around 2012.

33:53.280 --> 33:59.400
And also for computer vision, the notion of interpretability changes, right?

33:59.400 --> 34:09.280
Even before we get to computer vision, there is work that's starting to happen and people

34:09.280 --> 34:16.800
starting to look at applying deep learning to tabular data.

34:16.800 --> 34:23.960
This has been done, you know, for a couple of, well, a couple of the examples that come

34:23.960 --> 34:32.240
to mind for me are some of the Kaggle competitions, and this is discussed in the Fast.ai course,

34:32.240 --> 34:33.240
quite a bit.

34:33.240 --> 34:40.080
The idea being that, hey, we can throw a deep learning network against this, and we

34:40.080 --> 34:44.880
don't have to have data scientists with lots of domain experience, people that know how

34:44.880 --> 34:51.520
to create deep learning models, you know, but don't know anything about the domain can

34:51.520 --> 34:57.560
perform all on these products, you know, just by throwing a deep learning model against

34:57.560 --> 35:05.240
these kind of tabular problems, the one that, one example is the Rothman stores, I think

35:05.240 --> 35:11.960
was the name Kaggle competition, where you're trying to predict, I forget store sales

35:11.960 --> 35:17.520
or something like that based on, you know, some set of tabular data, meaning not image

35:17.520 --> 35:24.600
or audio or any kind of media, you know, just regular data, like you might apply to, you

35:24.600 --> 35:27.160
know, a more simple machine learning model.

35:27.160 --> 35:32.000
Yeah, I mean, there's definitely a lot of domains in which, well, not a lot, but there's

35:32.000 --> 35:37.120
at least several domains in which neural networks are just the best, and there's no question

35:37.120 --> 35:38.120
about that.

35:38.120 --> 35:42.840
But there's no reason you can't also have neural networks that are interpretable.

35:42.840 --> 35:46.960
So we've been trying to design these neural networks for computer vision that are, that

35:46.960 --> 35:51.920
they do a form of case-based reasoning, so that they don't just produce a prediction,

35:51.920 --> 35:56.680
like, yes, this is a Siberian husky.

35:56.680 --> 36:02.200
The networks that we're trying to produce give you the reasoning process behind why it

36:02.200 --> 36:04.320
thinks this is a Siberian husky.

36:04.320 --> 36:11.520
It says, the network says, I think that this part of the image looks like this prototypical

36:11.520 --> 36:17.280
ear of a Siberian husky that I've seen before, and this prototypical, and this part of

36:17.280 --> 36:21.880
the paw of the Siberian husky looks like this prototypical paw of a Siberian husky that

36:21.880 --> 36:23.040
I've seen before.

36:23.040 --> 36:28.920
And so, you know, even with neural networks, you can still add interpretability constraints

36:28.920 --> 36:35.040
and, you know, not lose accuracy when you make predictions.

36:35.040 --> 36:38.800
So even a deep neural network doesn't have to be a complete black box.

36:38.800 --> 36:41.640
And one of the things that I've come across is like this, you know, turtles all the way

36:41.640 --> 36:42.640
down problem.

36:42.640 --> 36:52.160
Like, is it, is the, the interpretable model some kind of hybrid of, you know, explainable

36:52.160 --> 36:58.640
and, you know, the model, or is it like, what makes it inherently interpretable versus

36:58.640 --> 37:04.240
some explainable feature kind of bolted deep inside the architecture of the, the black

37:04.240 --> 37:05.240
box model?

37:05.240 --> 37:06.240
Does that make sense?

37:06.240 --> 37:11.760
Yeah, so, you know, what we're trying to do is design a deep neural network that will

37:11.760 --> 37:17.960
explain its predictions in a way that's similar to how a human would describe its reasoning

37:17.960 --> 37:20.680
process behind the predictions.

37:20.680 --> 37:26.400
We've been doing a lot of work on the CUB data sets, the bird identification data sets

37:26.400 --> 37:33.280
because bird identification is kind of difficult for humans and we can get reasonable explanations

37:33.280 --> 37:34.280
out of them.

37:34.280 --> 37:40.320
But the goal is to actually embed the network with the explanations, you know, with constraints

37:40.320 --> 37:45.320
as part of the explanation so that when the network says, I'm using, I'm comparing this

37:45.320 --> 37:49.600
image to these other images and that's how I'm making my decision.

37:49.600 --> 37:52.200
The network is, it's, this is not a post-hawk thing.

37:52.200 --> 37:53.760
This is actually part of the network.

37:53.760 --> 37:56.640
It's saying, it's saying, here is my decision process.

37:56.640 --> 37:57.640
I'm taking the image.

37:57.640 --> 38:01.320
I'm comparing it to this, this, this and that image.

38:01.320 --> 38:04.840
And because I think this looks like that, this looks like that, this looks like that,

38:04.840 --> 38:10.680
and this looks like that, and that's why I think this is a clay-colored sparrow as opposed

38:10.680 --> 38:13.120
to a robin or something.

38:13.120 --> 38:14.120
Okay.

38:14.120 --> 38:15.920
So it's not a post-hawk.

38:15.920 --> 38:19.280
It's, it's not, I trained the network and then afterward I tried to figure out what

38:19.280 --> 38:20.760
the network was doing.

38:20.760 --> 38:26.360
This is instead the network saying, this is the computation I'm doing and the, the, the

38:26.360 --> 38:31.600
ex, you know, this is the reasoning process behind why I made that prediction.

38:31.600 --> 38:33.720
Do you, do you understand the difference?

38:33.720 --> 38:41.400
I do understand the, I do understand the difference between those two, trying to think if there's

38:41.400 --> 38:47.160
a better way to describe the distinction that I was trying to make.

38:47.160 --> 38:50.720
The way I like to think about it is kind of like a real estate agent pricing houses.

38:50.720 --> 38:51.720
Mm-hmm.

38:51.720 --> 38:55.280
You know, the, the way real estate agents price houses that they look for comps, right?

38:55.280 --> 39:00.840
They look for comparable houses in the same, you know, maybe in the same neighborhood.

39:00.840 --> 39:05.320
And they say, well, that house has a, it's about the same square footage as yours.

39:05.320 --> 39:09.640
And it has a big backyard and yours has a big backyard.

39:09.640 --> 39:11.800
And this other house is right down the block.

39:11.800 --> 39:17.240
So the, you know, I'm getting a price from the location.

39:17.240 --> 39:20.840
And so the real estate agent combines all of that information from the comparable houses

39:20.840 --> 39:26.240
to try to create a price that is the real estate agent's prediction.

39:26.240 --> 39:28.720
And they're explaining to you how they came up with that price.

39:28.720 --> 39:33.920
It's like, I'm comparing, oh, they're backyard to your backyard and so on.

39:33.920 --> 39:42.560
And so the models that these interpretable computer vision models are they, at least

39:42.560 --> 39:48.960
the ones that you're working with, are they explicitly referencing some database of,

39:48.960 --> 39:54.800
you know, they, when they're making their decisions, are they referencing, is it like an

39:54.800 --> 40:00.160
information retrieval thing where they're referencing some database or some set of images

40:00.160 --> 40:05.800
from the training data, or is this, yes, the referencing images from the training data.

40:05.800 --> 40:09.200
They're saying, you know, I've seen a prototypical Robin before.

40:09.200 --> 40:16.760
I have a whole bunch of them in my database, you know, they, they, the throat of a Robin

40:16.760 --> 40:21.320
looks like this prototypical throat of a Robin that I've seen before.

40:21.320 --> 40:24.920
And it doesn't know what a throat of a Robin is, it's just highlighting part of the image

40:24.920 --> 40:29.180
and saying, I think this part of your image is similar to this part of this prototypical

40:29.180 --> 40:30.600
Robin image.

40:30.600 --> 40:34.440
And that similarity is what I'm using to help make my prediction.

40:34.440 --> 40:42.600
And so you, you've alluded to the difficulty that humans have in explaining some of their

40:42.600 --> 40:51.360
decisions in bird watching and in other areas, is the, you know, does that mean in a sense

40:51.360 --> 41:00.120
that the goal is to, to have kind of superhuman performance, not, not in a sensationalistic

41:00.120 --> 41:07.920
way, but to create the models that are better than humans at explaining what they're doing.

41:07.920 --> 41:12.480
I mean, there's a lot of critique of kind of the whole explainability thing that there's

41:12.480 --> 41:17.240
research that says that we make up things when we're asked to explain things kind of like

41:17.240 --> 41:19.240
how you're saying models do.

41:19.240 --> 41:23.480
Yeah, that, that, that's part of the reason why we're working on the recidivism project

41:23.480 --> 41:28.880
is because, you know, judges are humans and humans are biased black boxes, right?

41:28.880 --> 41:29.880
Humans, right.

41:29.880 --> 41:35.240
They, they claim they're making a decision because of acts, but the real reason that they're

41:35.240 --> 41:41.080
making the decision is unknown to them and to everyone else.

41:41.080 --> 41:46.400
And so you have different judges making different decisions for different reasons.

41:46.400 --> 41:51.080
And so the whole point was to have these very simple models that would kind of get everybody

41:51.080 --> 41:57.040
in the same page and, you know, be more consistent and be more accurate.

41:57.040 --> 42:01.600
Because also, you know, humans, humans can't process whole databases in their head and

42:01.600 --> 42:05.440
come up with accurate predictions, we're just not good at that.

42:05.440 --> 42:08.560
And that's why we rely on machine learning algorithms.

42:08.560 --> 42:15.520
So you know, we're hoping that by, by sort of leveraging large databases to make accurate

42:15.520 --> 42:21.440
predictions and having explanations that are, you know, understandable to human experts

42:21.440 --> 42:26.480
that will hopefully be able to help get everyone on the same page.

42:26.480 --> 42:30.920
And then, you know, if the human decision maker has extra factors that are not in the database,

42:30.920 --> 42:37.800
they can calibrate those extra factors into the model that they started with, right.

42:37.800 --> 42:42.920
So if the recidivism prediction model says there's a 76% chance you'll recidivate, but

42:42.920 --> 42:48.120
let's say there's some additional factor that the judge knows about that's not in the database.

42:48.120 --> 42:52.280
The judge can use that as a mitigating factor to change their decision.

42:52.280 --> 42:59.120
So can you tell a little bit about your future directions in this area?

42:59.120 --> 43:02.240
Well, we've been working on optimal decision trees for a really long time.

43:02.240 --> 43:07.840
I didn't even know how many years I've been working on optimal decision trees.

43:07.840 --> 43:13.480
I've also been working on optimal sparse linear models with integer coefficients for a long

43:13.480 --> 43:14.480
time.

43:14.480 --> 43:16.720
So these are like medical scoring systems.

43:16.720 --> 43:22.840
These are models that give you like one point if your age is above 50 and then two points

43:22.840 --> 43:26.040
if you've had a prior heart condition and so on.

43:26.040 --> 43:32.960
So those point scores, we're trying to fully optimize them based on data which substitutes

43:32.960 --> 43:38.200
for having a team of doctors in a room trying to decide the point scores.

43:38.200 --> 43:42.040
We're using, we're instead using large databases and machine learning algorithms to design

43:42.040 --> 43:47.680
the point scores, but the final models are very similar to what the doctors in the room

43:47.680 --> 43:52.880
might have constructed themselves.

43:52.880 --> 43:56.000
So we've been working on that for quite a while.

43:56.000 --> 44:01.960
So these are, this is optimal scoring systems and then the optimal neural networks project

44:01.960 --> 44:07.840
we've had going for a few years, sorry, not optimal neural networks, interpretable neural

44:07.840 --> 44:08.840
networks.

44:08.840 --> 44:12.520
We've had the interpretable neural networks project going for a few years.

44:12.520 --> 44:19.240
And then I also have several projects on causal inference where we're trying to do interpretable

44:19.240 --> 44:21.080
matching for causal inference.

44:21.080 --> 44:25.080
So if you have a treatment group and a control group, so people who've taken the drug and

44:25.080 --> 44:30.440
who haven't taken the drug and you want to determine what the effect of the drug is, then

44:30.440 --> 44:36.080
you would normally try to match, you normally try to match people with their identical twin.

44:36.080 --> 44:39.840
People who've had the drug match them with an identical twin who didn't have the drug

44:39.840 --> 44:44.120
and then you could figure out what the effect of the drug is.

44:44.120 --> 44:50.480
So we have a project on doing that in a more accurate way.

44:50.480 --> 44:52.840
I'm also still working on criminal recidivism.

44:52.840 --> 44:56.960
We've been working on that for several years now.

44:56.960 --> 45:02.520
We had a paper published in 2015 called interpretable classification models for recidivism prediction

45:02.520 --> 45:08.240
where we showed that the interpretable machine learning models were just as accurate for

45:08.240 --> 45:14.760
predicting all different kinds of recidivism than complicated machine learning models.

45:14.760 --> 45:19.360
So they're just as accurate as the complicated machine learning models.

45:19.360 --> 45:23.160
So we were still working on recidivism and then I have a lot of projects on health care,

45:23.160 --> 45:28.720
which is another high stakes domain where you care about interpretability.

45:28.720 --> 45:32.240
You want to make very careful decisions in health care.

45:32.240 --> 45:40.360
If you had to call out an area that you think needs more work or attention but is not

45:40.360 --> 45:45.800
one that you're working on, is there any particular thing that comes to mind?

45:45.800 --> 45:50.640
I mean, there's a lot of societal good applications that are almost impossible to work on because

45:50.640 --> 45:53.240
gathering the data is very, very difficult.

45:53.240 --> 45:57.680
I wish I could work on more problems like that but the data is just so hard to get that.

45:57.680 --> 46:00.000
Any particular example comes to mind?

46:00.000 --> 46:05.800
Well, I think income inequality in the world is a huge problem for instance.

46:05.800 --> 46:10.640
Like any major world problem, we should be throwing data and algorithms at it because

46:10.640 --> 46:11.640
we can.

46:11.640 --> 46:13.920
We have people, we have expertise.

46:13.920 --> 46:15.320
People want to work on those problems.

46:15.320 --> 46:16.960
It's just that it's hard to do it.

46:16.960 --> 46:17.960
Yeah.

46:17.960 --> 46:21.600
Those are more data problems than algorithmic problems.

46:21.600 --> 46:26.680
I think health care is a huge frontier.

46:26.680 --> 46:28.440
There's a lot of people working on health care.

46:28.440 --> 46:35.560
I think a lot of people realize how important it is but the data is really messy, is confounded

46:35.560 --> 46:39.400
in every possible way you could imagine.

46:39.400 --> 46:46.520
But it's also really important because if we understood what the right cancer treatments

46:46.520 --> 46:53.960
are and what to do about the opioid epidemic, that would be amazing if we could solve some

46:53.960 --> 46:58.080
of these problems with algorithms and so on.

46:58.080 --> 47:05.520
In terms of algorithmic challenges, one problem that we're just starting to try to tackle

47:05.520 --> 47:14.080
is whether you can know if an interpretable model exists before going to the trouble of

47:14.080 --> 47:19.480
finding one because finding them is computationally very demanding.

47:19.480 --> 47:20.480
Right.

47:20.480 --> 47:27.480
Before you go and do all that extra work, it'd be nice to know in advance whether you're

47:27.480 --> 47:31.280
likely to find an interpretable model.

47:31.280 --> 47:34.400
So we're just starting to think about that question now.

47:34.400 --> 47:39.520
Well Cynthia, thanks so much for taking the time to chat with me really, definitely really

47:39.520 --> 47:46.560
interesting work and a lot of thought-provoking issues that are involved in it.

47:46.560 --> 47:48.960
Okay, my pleasure.

47:48.960 --> 47:50.440
Thank you.

47:50.440 --> 47:56.280
Alright everyone, that's our show for today.

47:56.280 --> 48:02.320
For more information on today's show, visit twomolai.com slash shows.

48:02.320 --> 48:08.160
Make sure you head over to twomolcan.com to learn more about the Twomolcan AI Platforms

48:08.160 --> 48:09.640
Conference.

48:09.640 --> 48:38.160
As always, thanks so much for listening and catch you next time.

