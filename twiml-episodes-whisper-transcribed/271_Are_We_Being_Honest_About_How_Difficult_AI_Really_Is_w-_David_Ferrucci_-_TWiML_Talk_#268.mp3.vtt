WEBVTT

00:00.000 --> 00:15.000
Hello and welcome to another episode of Twimble Talk, the podcast.

00:15.000 --> 00:25.000
We interrupt our program to bring you this important message.

00:25.000 --> 00:28.000
Happy birthday, Twimble.

00:28.000 --> 00:36.000
Today is a very special day and this is a very special show because we are celebrating the third.

00:36.000 --> 00:41.000
That's right, the third birthday of your favorite machine learning and AI podcast.

00:41.000 --> 00:48.000
I'll be honest, if you told me three years ago when I was struggling to get that very first episode of this week in machine learning and AI,

00:48.000 --> 00:57.000
recorded and posted, that three years later, I'd still be at it, that we'd be publishing two, three, four or sometimes five shows a week,

00:57.000 --> 01:03.000
that I'd have interviewed well over 300 incredible ML and AI researchers and practitioners.

01:03.000 --> 01:09.000
That we'd have an amazing community of fans that engage and encourage us daily.

01:09.000 --> 01:15.000
That we'd help hundreds of people learn machine learning and deep learning through our volunteer led study groups.

01:15.000 --> 01:19.000
That we'd hit five million downloads on our third birthday?

01:19.000 --> 01:22.000
Yes, you heard that right, that just happened?

01:22.000 --> 01:29.000
Well, let's just say that if you told me any of that, I'd have definitely thought you were crazy.

01:29.000 --> 01:35.000
It's been a truly amazing and humbling experience bringing you the podcast every week

01:35.000 --> 01:43.000
and I am more appreciative than ever of you, our listeners and the amazing community that we've built around this show.

01:43.000 --> 01:49.000
From the entire Twimble team, thank you so much and Happy Birthday.

01:53.000 --> 01:57.000
With that said, we'd love to hear your Twimble story.

01:57.000 --> 02:03.000
In our conversations with listeners over time, we've heard quite a few stories detailing what you've learned from the podcast

02:03.000 --> 02:08.000
and how you've applied it to your own work, research, philosophy and lives.

02:08.000 --> 02:16.000
And now we want to hear from all of you. Please take a moment to hop over to TwimbleAI.com slash 3B day

02:16.000 --> 02:27.000
or leave a voicemail at 1636-735-3658, letting us know your favorite tidbit that you've been able to take from the show

02:27.000 --> 02:30.000
and how you've applied it to what you do.

02:30.000 --> 02:35.000
Everyone who joins in will be sent a limited edition third birthday Twimble sticker

02:35.000 --> 02:40.000
and the best submissions have a chance to be featured in an upcoming episode of the show.

02:40.000 --> 02:51.000
Again, that's TwimbleAI.com slash 3B day for your written comment or 1636-735-3658 for your voicemail message.

02:51.000 --> 02:55.000
And whatever you do, stay tuned.

02:55.000 --> 02:59.000
We've got tons of exciting things in the works that we can't wait to bring you.

02:59.000 --> 03:05.000
For starters, next week we'll be releasing volume 2 of our AI platform series,

03:05.000 --> 03:10.000
a follow-up to one of your all-time favorites based on the feedback we've received.

03:10.000 --> 03:17.000
We've got much, much more in the pipeline for you including a huge update that we are super excited about

03:17.000 --> 03:20.000
but those are still top secret for now.

03:20.000 --> 03:24.000
Just be sure whatever you do to listen in next week.

03:24.000 --> 03:31.000
And now on to the show.

03:31.000 --> 03:34.000
Alright everyone, I am on the line with Dave Farouche.

03:34.000 --> 03:40.000
Dave is the founder, CEO and Chief Scientist at Elemental Cognition.

03:40.000 --> 03:43.000
Dave, welcome to this week of Machine Learning and AI.

03:43.000 --> 03:45.000
Hi, how are you? It's a pleasure to be here.

03:45.000 --> 03:48.000
Absolutely, absolutely great to have you on the show.

03:48.000 --> 03:56.000
So in some circles, you are perhaps best known as having built and led the IBM Watson team.

03:56.000 --> 04:01.000
And I'm curious, how did you arrive at that point in your career?

04:01.000 --> 04:05.000
Well, I was always interested in artificial intelligence.

04:05.000 --> 04:10.000
I mean, I have been since, actually since high school early college.

04:10.000 --> 04:15.000
It's been my fascination. I started programming, I guess, in high school.

04:15.000 --> 04:24.000
And right from the very beginning, I mean, after I think I wrote my first program was like in basic.

04:24.000 --> 04:29.000
I think it was on a PDP 11, I can't remember exactly, but it was really cheaper.

04:29.000 --> 04:39.000
And I think what blew me out of the water right away was, wow, if I can just describe my thought process,

04:39.000 --> 04:41.000
I can get a computer to do work for me.

04:41.000 --> 04:45.000
And that was just very exhilarating.

04:45.000 --> 04:52.000
And my mind directly went to this notion, I don't even think I personally had a word for it at the time.

04:52.000 --> 05:01.000
I learned later, it was called artificial intelligence, but this notion that if I can describe how I solve problems,

05:01.000 --> 05:08.000
if I can describe how I think, and I could put it in this language that the computer executes for me,

05:08.000 --> 05:14.000
I can get the computer to do all the hard work and allow me to kind of be creative.

05:14.000 --> 05:20.000
And I hated doing repetitive tasks, so this was just mind blowing to me.

05:20.000 --> 05:23.000
And I was on the path to become a medical doctor.

05:23.000 --> 05:25.000
My parents wanted me to be a medical doctor.

05:25.000 --> 05:27.000
That was like the thing to do in my neighborhood.

05:27.000 --> 05:30.000
If you were smart enough, I guess.

05:30.000 --> 05:32.000
But I just got this bug.

05:32.000 --> 05:39.000
And so by the time I was in college, I was doing more and more and more programming, programming everything I could possibly get my hands on,

05:39.000 --> 05:44.000
and then eventually switched to wanting to go to grad and set up going to medical school,

05:44.000 --> 05:47.000
wanting to go to graduate school and computer science.

05:47.000 --> 05:57.000
I eventually got my PhD and got a chance to work at IBM Research on an AI project.

05:57.000 --> 06:04.000
And interestingly enough, this was back in the, I think it was the late 80s.

06:04.000 --> 06:08.000
And artificial intelligence at the time became a bad word.

06:08.000 --> 06:10.000
And there was an AI winter.

06:10.000 --> 06:15.000
And it was all about the customers worrying that AI was going to take jobs.

06:15.000 --> 06:16.000
Can you imagine?

06:16.000 --> 06:19.000
And here we are in the same spot when we used later.

06:19.000 --> 06:20.000
It's fascinating.

06:20.000 --> 06:25.000
And they went around canceling AI projects.

06:25.000 --> 06:30.000
And my manager at the time said, you know, Dave, you've got to learn how to do.

06:30.000 --> 06:33.000
You could become a specialist in like a domain area.

06:33.000 --> 06:35.000
And you don't want to be a technologist, right?

06:35.000 --> 06:37.000
And we're not going to be doing AI anymore.

06:37.000 --> 06:38.000
And I quit.

06:38.000 --> 06:42.000
It's like because this is not that's clearly what I want to do.

06:42.000 --> 06:44.000
This is actually between my masters, my PhD.

06:44.000 --> 06:46.000
And so I went back and I finished my PhD.

06:46.000 --> 06:49.000
I actually ended up returning to IBM.

06:49.000 --> 06:56.000
And, you know, working on a number of different projects related to AI,

06:56.000 --> 07:02.000
to tech, analytics, speech, image analytics, building, software and infrastructure.

07:02.000 --> 07:07.000
I eventually got into open domain question answering had built a team.

07:07.000 --> 07:14.000
We competed in a lot of government-sponsored competitions in the open domain question answering track.

07:14.000 --> 07:19.000
So I got to a point where I was doing all the things I love to do.

07:19.000 --> 07:27.000
Software, software, architecture, software engineering, all around solving AI-type problems.

07:27.000 --> 07:29.000
And so I had built a team.

07:29.000 --> 07:36.000
So by about 2006, I had built a team of about 25 people,

07:36.000 --> 07:41.000
all, you know, specialists and a combination of software engineering and different aspects of AI,

07:41.000 --> 07:48.000
including natural language processing, machine learning, knowledge representation and reasoning.

07:48.000 --> 07:55.000
And this idea of doing this Jeopardy challenge came up.

07:55.000 --> 07:58.000
And actually, it came up two years before that.

07:58.000 --> 08:10.000
But the executive who wanted to do it kept on being turned down by the vast majority of scientists and researchers he would go to

08:10.000 --> 08:12.000
and say, hey, can we get this done?

08:12.000 --> 08:15.000
And they said, no, it's impossible. You can't do it. It's too hard.

08:15.000 --> 08:22.000
And I was interested in it when it came around in 2004 and 2005,

08:22.000 --> 08:24.000
but very busy with another project.

08:24.000 --> 08:29.000
And then at the end of 2006, I was coming off of the project.

08:29.000 --> 08:33.000
And I said, you know, I think this is possible.

08:33.000 --> 08:38.000
And I think we not only should we do it, we sort of have an obligation to do it.

08:38.000 --> 08:43.000
We've been working in open domain factoid question answering for some time.

08:43.000 --> 08:50.000
My team had participated in the track question answering track for a number of years.

08:50.000 --> 08:53.000
It was, I was doing a number of different things.

08:53.000 --> 08:57.000
It was maybe the team was about seven or so people dedicated to that.

08:57.000 --> 09:01.000
And I think we have to take this on.

09:01.000 --> 09:06.000
And if we had a chance of making the appropriate investment, we should do it.

09:06.000 --> 09:10.000
And even if it looks hard, we have to understand why it's hard.

09:10.000 --> 09:14.000
I mean, even if I fail, I'll be able to kind of tell the community, hey, look,

09:14.000 --> 09:17.000
we made a concerted effort at this. Here's how we did it.

09:17.000 --> 09:20.000
Here's what was hard about it. Here's what worked. Here's what didn't work.

09:20.000 --> 09:23.000
So it was a great opportunity to do real research.

09:23.000 --> 09:27.000
And to get that, you know, funded, you know, by IBM at the right levels,

09:27.000 --> 09:31.000
because they were excited about, you know, getting the the jeopardy challenge

09:31.000 --> 09:38.000
to think on television. And so I did, I did a feasibility study.

09:38.000 --> 09:41.000
We did a little bit. We played around some ideas.

09:41.000 --> 09:44.000
And I proposed it that we can do it.

09:44.000 --> 09:48.000
And they bought in and said, okay, you know, you're the guy.

09:48.000 --> 09:52.000
We're going to invest in this. And you're going to take on this challenge.

09:52.000 --> 09:56.000
And so I built up the team over the, that was the end of 2006.

09:56.000 --> 10:04.000
So 2007, 2008, 2009, 2010, completely focused on that building up,

10:04.000 --> 10:07.000
building up and rounding out the team.

10:07.000 --> 10:11.000
And basically going from scratch and building, building Watson,

10:11.000 --> 10:17.000
that ultimately won on jeopardy. And I guess we played it in 2011.

10:17.000 --> 10:23.000
So that's kind of the story from, from, from my passion all the way through,

10:23.000 --> 10:26.000
right through, right through the Watson stuff.

10:26.000 --> 10:33.000
And after Watson, did you jump right into elemental cognition or?

10:33.000 --> 10:35.000
So what's stopping points along the way?

10:35.000 --> 10:39.000
Yeah, so Watson was interesting because it was a huge,

10:39.000 --> 10:41.000
it was a tremendous success for us.

10:41.000 --> 10:46.000
It was a tremendous, the technical team was just so,

10:46.000 --> 10:49.000
had worked so hard and was so proud.

10:49.000 --> 10:54.000
We went into the final contest with about a 70, 75,

10:54.000 --> 10:59.000
between 70 and 75, there was 73% chance of winning based on all the stats that we did,

10:59.000 --> 11:02.000
all the simulations that we did.

11:02.000 --> 11:04.000
We hadn't worked our way up from when we started.

11:04.000 --> 11:07.000
There was about, you know, we were getting,

11:07.000 --> 11:09.000
we had zero percent chance of winning.

11:09.000 --> 11:13.000
You know, the, the system out of the box when we started was getting like 13%.

11:13.000 --> 11:14.000
All right.

11:14.000 --> 11:17.000
So it was a huge accomplishment.

11:17.000 --> 11:20.000
And it was in the middle of both sort of a combination of science,

11:20.000 --> 11:24.000
obviously, and we had the scientific results,

11:24.000 --> 11:27.000
business, you know, the business and the marketing at IBM.

11:27.000 --> 11:29.000
It was a big project for them and entertainment,

11:29.000 --> 11:33.000
because we had to work with the jeopardy of production folks and the television folks

11:33.000 --> 11:35.000
and pull the salt together.

11:35.000 --> 11:40.000
So having accomplished that, no one knew where it was going.

11:40.000 --> 11:42.000
It was such a focus on those things,

11:42.000 --> 11:44.000
and then no one knew where it was going.

11:44.000 --> 11:48.000
And interestingly, and I think this relates to kind of the perceptions

11:48.000 --> 11:50.000
of artificial intelligence.

11:50.000 --> 11:54.000
I think interestingly, and if you've saw, if you watched the games,

11:54.000 --> 11:57.000
but we presented this thing we called the answer panel,

11:57.000 --> 12:00.000
where it showed the top three answers and the chances we thought,

12:00.000 --> 12:03.000
the probability of that, those answers being right.

12:03.000 --> 12:07.000
And I think that the, putting that up on the television screen,

12:07.000 --> 12:13.000
got people to imagine that the machine was doing more than looking this stuff

12:13.000 --> 12:16.000
at a table, which of course it was.

12:16.000 --> 12:20.000
But I think when people just watch a computer give an answer,

12:20.000 --> 12:23.000
you know, the general thing is, well, computers just know everything.

12:23.000 --> 12:25.000
They just look it up and they know everything.

12:25.000 --> 12:27.000
But when they see this notion of wait a second,

12:27.000 --> 12:31.000
I must be doing more than that, because it's coming with a probability.

12:31.000 --> 12:34.000
So it must be kind of calculating why an answer might be right

12:34.000 --> 12:36.000
or why an answer might be wrong.

12:36.000 --> 12:40.000
And in fact, we were doing something certainly along those lines,

12:40.000 --> 12:45.000
including a confidence model for that based on hundreds of features

12:45.000 --> 12:49.000
and our underlying machine learning algorithms.

12:49.000 --> 12:53.000
And so this really captured the imagination, I think,

12:53.000 --> 12:55.000
of the consumer, of the customer.

12:55.000 --> 12:59.000
And before you knew it, IBM had a lot of people coming and saying,

12:59.000 --> 13:01.000
hey, we want this thing.

13:01.000 --> 13:03.000
Like, how do we get this thing?

13:03.000 --> 13:08.000
And at that point, the project took on a very different,

13:08.000 --> 13:12.000
you know, tone, very different nature was all about,

13:12.000 --> 13:14.000
how do we commercialize this?

13:14.000 --> 13:18.000
How do we bring AI and this technology to customers?

13:18.000 --> 13:22.000
And I personally wanted to kind of take a step back.

13:22.000 --> 13:27.000
And looking at how Watson worked and what my dreams were

13:27.000 --> 13:29.000
for artificial intelligence in general.

13:29.000 --> 13:31.000
So, you know, we're not really there yet.

13:31.000 --> 13:34.000
Well, there's a lot of technology here that could be exploited

13:34.000 --> 13:37.000
and leveraged in a variety of different applications.

13:37.000 --> 13:41.000
We're not at this point where the machine really understands language.

13:41.000 --> 13:44.000
And that's where I wanted to be.

13:44.000 --> 13:48.000
I wanted to be where I can fluently converse with the computer

13:48.000 --> 13:50.000
just like you can on Star Trek.

13:50.000 --> 13:53.000
And I can get it to understand what I'm saying.

13:53.000 --> 13:57.000
I can get it to deliver and summarize and read stuff,

13:57.000 --> 13:59.000
summarize information for me.

13:59.000 --> 14:02.000
I can get it to help me problem solve, you know,

14:02.000 --> 14:05.000
become like a thought partner through the fluency of language.

14:05.000 --> 14:07.000
Think the way I do only better.

14:07.000 --> 14:09.000
But in a way that I can understand it,

14:09.000 --> 14:11.000
it can be explained to me.

14:11.000 --> 14:13.000
I can tell it what I know.

14:13.000 --> 14:15.000
It can tell me what it knows.

14:15.000 --> 14:17.000
And this was the dream.

14:17.000 --> 14:19.000
And we were far from that.

14:19.000 --> 14:23.000
And I really wanted to step back and do that kind of thing.

14:23.000 --> 14:27.000
And IBM was on a very different road at that point

14:27.000 --> 14:31.000
because of the consumer interest, the customer interest.

14:31.000 --> 14:35.000
To be precise, not necessarily the consumer, but business interest

14:35.000 --> 14:37.000
and stuff like that.

14:37.000 --> 14:39.000
So I really wanted to take that step back.

14:39.000 --> 14:41.000
I got a chance.

14:41.000 --> 14:45.000
I was, you know, for a variety of reasons won't go into it.

14:45.000 --> 14:49.000
I, you know, some of my choices at the time were for limited.

14:49.000 --> 14:53.000
And I got really interested in this company not far from where I led,

14:53.000 --> 14:55.000
called Bridgewater.

14:55.000 --> 14:57.000
And I started working for Bridgewater.

14:57.000 --> 14:59.000
And part of the reason I was interested in Bridgewater

14:59.000 --> 15:01.000
also relates to my philosophy around AI,

15:01.000 --> 15:05.000
which is that Bridgewater is a hedge fund that was approaching markets

15:05.000 --> 15:07.000
in what they call the fundamental and systematic way.

15:07.000 --> 15:11.000
Meaning that any kind of prediction they were going to make,

15:11.000 --> 15:15.000
they were going to make sure they had an explicable model for that prediction.

15:15.000 --> 15:17.000
So this was the kind of AI I was interested in.

15:17.000 --> 15:19.000
Of course, you know, if you're in the market,

15:19.000 --> 15:21.000
you're a washing data.

15:21.000 --> 15:23.000
So it is about data and it's about data science.

15:23.000 --> 15:27.000
But it's not about sort of blind past predicts the future.

15:27.000 --> 15:30.000
It's really about building theoretical models and being able to explain yourself.

15:30.000 --> 15:33.000
So that was very aligned with my interest of completely different domain.

15:33.000 --> 15:37.000
But nonetheless, not expressly about language,

15:37.000 --> 15:39.000
but the same sort of approach toward AI.

15:39.000 --> 15:44.000
And I got involved with them and did some work for them,

15:44.000 --> 15:46.000
still doing some work for them.

15:46.000 --> 15:47.000
But during the course of that,

15:47.000 --> 15:52.000
got them really interested in sort of my vision for language understanding.

15:52.000 --> 15:56.000
And that ultimately the future of AI has to land in a place.

15:56.000 --> 16:00.000
And this is where we both really agreed that the future of AI has to land in a place

16:00.000 --> 16:03.000
that we build machines that are understandable, they're explicable.

16:03.000 --> 16:09.000
Their logic can be probed and can be challenged and can be explained.

16:09.000 --> 16:17.000
So I got, they gave me the opportunity to start my own company called Elemental Cognition.

16:17.000 --> 16:23.000
And that was in 2015 that I started that I started that.

16:23.000 --> 16:28.000
And that's been our mission is to focus on language understanding

16:28.000 --> 16:37.000
with the, to build a learning machine that can learn in a way that ultimately interprets language,

16:37.000 --> 16:42.000
builds on causal models based on its interpretation,

16:42.000 --> 16:47.000
can speak to people, acquire knowledge, reason about that knowledge,

16:47.000 --> 16:49.000
answer questions and provide explanations.

16:49.000 --> 16:53.000
I think this is the Holy Grail for language AI.

16:53.000 --> 16:57.000
And that's the mission for Elemental Cognition.

16:57.000 --> 17:02.000
Awesome. I hadn't realized the Bridgewater connection,

17:02.000 --> 17:10.000
Bridgewater for being perhaps popularized more recently with Ray Dalio's book.

17:10.000 --> 17:15.000
Yes, Ray wrote the book on principles

17:15.000 --> 17:20.000
and is popularizing that.

17:20.000 --> 17:24.000
Of course, it's the biggest hedge fund in the world.

17:24.000 --> 17:29.000
And it's sort of known for this approach toward markets,

17:29.000 --> 17:32.000
which is again fundamental and systematic where they build,

17:32.000 --> 17:36.000
they of course leverage the computer, but they build explicable systems.

17:36.000 --> 17:39.000
And that's kind of the link between Bridgewater and me

17:39.000 --> 17:42.000
is we have a very similar philosophy about AI.

17:42.000 --> 17:47.000
And one of the things that occurred to me as you were talking about the Watson experience

17:47.000 --> 17:52.000
and the way that you presented the results via this panel

17:52.000 --> 17:58.000
was that it was in some ways kind of an early view at AI explainability, right?

17:58.000 --> 18:01.000
We're not just going to show this one result and say it's the answer.

18:01.000 --> 18:05.000
We're going to show these results and explicitly acknowledge that, you know,

18:05.000 --> 18:07.000
there are probabilities involved.

18:07.000 --> 18:12.000
That's right, and there's a great story about that.

18:12.000 --> 18:15.000
I thought that was so important to do,

18:15.000 --> 18:18.000
because we had, you know, we had taped many games,

18:18.000 --> 18:25.000
we made practice games, and we would tape them and show them to different, you know, audiences.

18:25.000 --> 18:29.000
And at the time, I think my younger story was seven or eight.

18:29.000 --> 18:36.000
And we showed her one of these tapes of Watson playing against former Jeffrey players.

18:36.000 --> 18:41.000
And Watson, you know, didn't know an answer and decided not to answer.

18:41.000 --> 18:46.000
We didn't have that answer panel up that showed its top three choices.

18:46.000 --> 18:52.000
And when the computer didn't answer, she turned around and said, you know, did a crash.

18:52.000 --> 18:56.000
And I thought that was fascinating that, you know, at that young age, you know,

18:56.000 --> 19:02.000
she already had this notion that a computer should answer if it didn't answer, it's down.

19:02.000 --> 19:11.000
And I thought, no, I said, you know, Watson decided not to answer because it wasn't sure.

19:11.000 --> 19:13.000
And that's so completely different.

19:13.000 --> 19:16.000
And so, so then I just became convinced.

19:16.000 --> 19:18.000
And I, of course, was watching the game too.

19:18.000 --> 19:23.000
And I thought, you know, this is just not nearly as interesting and as exciting.

19:23.000 --> 19:27.000
Because, you know, when you're watching a human, you think the humans like you,

19:27.000 --> 19:30.000
when you're projecting what you would be doing answering a question,

19:30.000 --> 19:33.000
you'd be like wrestling with whether you do the answer or not,

19:33.000 --> 19:35.000
and whether or not you wanted to buzz in.

19:35.000 --> 19:40.000
But, you know, you don't have that same model for what a computer does, interestingly.

19:40.000 --> 19:45.000
And so, it was so obvious just not as interesting.

19:45.000 --> 19:49.000
So, I said, we really have to get that answer panel up on the screen.

19:49.000 --> 19:53.000
And at first, Jeffrey didn't want to do it.

19:53.000 --> 19:59.000
And eventually, long story short, we convinced Alex Trebek by showing him

19:59.000 --> 20:02.000
a couple of games with their answer panel up.

20:02.000 --> 20:04.000
And then, and he got so fascinated.

20:04.000 --> 20:09.000
But he said, you know, this is really jeopardy because your, your attention is distracted.

20:09.000 --> 20:11.000
That answer panel is so fascinating.

20:11.000 --> 20:16.000
It completely takes you away from the normal experience of what a jeopardy game is about.

20:16.000 --> 20:20.000
And so, we, um, so he then showed him a few, uh,

20:20.000 --> 20:23.000
a show him a few games with without the answer panel.

20:23.000 --> 20:25.000
And he was like, wait a second.

20:25.000 --> 20:31.000
Like this is more. Right. So he sort of, and it was so clearly the worst.

20:31.000 --> 20:34.000
And he was like, we got to get that up there.

20:34.000 --> 20:37.000
And I think that really changed the perception of AI.

20:37.000 --> 20:39.000
And you're absolutely right.

20:39.000 --> 20:41.000
It is the beginnings of why.

20:41.000 --> 20:45.000
And it's not enough because I think when machines are, are, are not doing

20:45.000 --> 20:50.000
factoid question answering, but they're making predictions, um, whether they're in,

20:50.000 --> 20:57.000
uh, law, um, you know, um, policymaking or law or, or, or, um, you know,

20:57.000 --> 21:02.000
health care or, or even finance anywhere where, you know, wow,

21:02.000 --> 21:05.000
this is going to affect your life in a serious way.

21:05.000 --> 21:11.000
Um, you, it's really a, a, um, it's a collaboration.

21:11.000 --> 21:15.000
I mean, I think the best way, it's not just an answer.

21:15.000 --> 21:17.000
It's really a back and forth collaboration.

21:17.000 --> 21:20.000
It's a dialogue that you have to go through.

21:20.000 --> 21:25.000
Because as you hear the answers, you want to know, why does that answer make sense?

21:25.000 --> 21:29.000
Am I missing something? Um, do I, now that I hear you tell me that answer?

21:29.000 --> 21:35.000
Maybe there are risks that I just didn't imagine or values that I weren't

21:35.000 --> 21:37.000
considering or weighing properly.

21:37.000 --> 21:42.000
Now that I see the answer and the reasons why that answer may be right or wrong.

21:42.000 --> 21:47.000
Um, so it, and, and, and you, and you can experience that just through the thought

21:47.000 --> 21:48.000
experiment.

21:48.000 --> 21:53.000
Just think of how you, um, interact with any human expert, whether you're consulting

21:53.000 --> 21:56.000
with the, um, with you're consulting with the lawyer or you're working with the

21:56.000 --> 21:58.000
teachers trying to help you understand something.

21:58.000 --> 21:59.000
Yeah.

21:59.000 --> 22:03.000
Um, I was going to say that sounds like my conversations with my wife about what we're

22:03.000 --> 22:04.000
going to have for dinner.

22:04.000 --> 22:06.000
There's that much back and forth.

22:06.000 --> 22:09.000
There you go. Right.

22:09.000 --> 22:12.000
Even when it's not that important, right?

22:12.000 --> 22:17.000
You know, you think you have the answer, but, um, or, you know, our doctor or a

22:17.000 --> 22:20.000
lawyer, you know, it really is, you know, you think it's simple, but it's not so

22:20.000 --> 22:23.000
simple when it's, when it's important enough.

22:23.000 --> 22:28.000
And I think at that point, you're really expecting the expert to start to bring

22:28.000 --> 22:33.000
you into the decision making process to give you the explanations that you need

22:33.000 --> 22:36.000
to kind of have that back and forth.

22:36.000 --> 22:40.000
Can you imagine, you know, making an important decision and, and asking an

22:40.000 --> 22:45.000
advisor and the advisor says, well, I think you should take this treatment or you,

22:45.000 --> 22:47.000
or you should invest your money here, whatever it is.

22:47.000 --> 22:49.000
And then you said, well, why do you think that?

22:49.000 --> 22:51.000
And the person just says, well, trust me.

22:51.000 --> 22:52.000
Right.

22:52.000 --> 22:53.000
It's my intuition.

22:53.000 --> 23:00.000
Well, this idea of it, uh, being not so simple is, is one that, uh,

23:00.000 --> 23:04.000
reminds me of something you said when we were speaking before, uh, the

23:04.000 --> 23:06.000
interview started.

23:06.000 --> 23:10.000
And that is that there's this big question for you around AI.

23:10.000 --> 23:15.000
And that is, are we really being honest about how difficult the problem is?

23:15.000 --> 23:18.000
Can you elaborate a little bit on that and what that, you know, what that statement,

23:18.000 --> 23:20.000
what that question means for you?

23:20.000 --> 23:21.000
Yeah.

23:21.000 --> 23:26.000
So, um, I think there's, I think what's going on in, you know, in the industry

23:26.000 --> 23:32.000
right now is we have a set of techniques that are good at a particular approach.

23:32.000 --> 23:37.000
Um, so we have this deep learning stuff, which by the way, I, I love,

23:37.000 --> 23:43.000
and I use any chance I get, but I'm also honest about what it's capable of and

23:43.000 --> 23:45.000
not capable of so far.

23:45.000 --> 23:50.000
And we have this technique and we build data sets and challenge problems that

23:50.000 --> 23:55.000
are, that are, you know, subject to that tech subject in one form or another

23:55.000 --> 23:59.000
are susceptible, vulnerable to that technique working.

23:59.000 --> 24:02.000
And, and I don't think they're ambitious enough.

24:02.000 --> 24:06.000
I think that understanding is actually a very hard problem.

24:06.000 --> 24:11.000
And I think if we step back and really think about how hard it is,

24:11.000 --> 24:14.000
independently of how a computer would work, and we say,

24:14.000 --> 24:17.000
what does it mean to understand stuff?

24:17.000 --> 24:21.000
To understand language, we just have to think about how much,

24:21.000 --> 24:26.000
how much energy and thought humans put into understanding each other.

24:26.000 --> 24:29.000
This is not a simple thing.

24:29.000 --> 24:31.000
We, we're, we're sort of language of machines.

24:31.000 --> 24:36.000
We're great at it and terrible at it at the same time.

24:36.000 --> 24:37.000
Right? We're great.

24:37.000 --> 24:40.000
We're, we're great at generating all kinds of language.

24:40.000 --> 24:41.000
We can write prolifically.

24:41.000 --> 24:42.000
We can talk prolifically.

24:42.000 --> 24:46.000
We can fill up, you know, news 24 or seven new shows.

24:46.000 --> 24:49.000
Um, do we reach and understand it?

24:49.000 --> 24:51.000
Like if you sit down, everybody's sort of nods,

24:51.000 --> 24:53.000
but what's your level of understanding?

24:53.000 --> 24:59.000
Even reading, um, whether, you know, whether it's an article or a book or whatever,

24:59.000 --> 25:03.000
people can debate endlessly for what's really meant,

25:03.000 --> 25:05.000
what you're really getting out of it.

25:05.000 --> 25:08.000
What, what, what is, what, what, what information is actually communicating

25:08.000 --> 25:10.000
and how precisely and how confidently,

25:10.000 --> 25:14.000
and why are we having different opinions about what's going on?

25:14.000 --> 25:18.000
In science, of course, we invented formal languages for this kind of stuff.

25:18.000 --> 25:20.000
Um, so in engineering, we don't rely on that.

25:20.000 --> 25:26.000
We rely on engineering specification diagrams and formal semantics and things like that.

25:26.000 --> 25:29.000
Um, and even there, uh, there are challenges in mathematics.

25:29.000 --> 25:31.000
We do mathematical proofs.

25:31.000 --> 25:35.000
And yet humans don't want to communicate that way clearly,

25:35.000 --> 25:37.000
but there has to be some in between.

25:37.000 --> 25:41.000
There has to be a recognition that understanding is actually pretty hard.

25:41.000 --> 25:44.000
Uh, humans invest an enormous amount of energy.

25:44.000 --> 25:50.000
Whether it be in teaching or journalism or, uh, writing or film,

25:50.000 --> 25:54.000
or that's an enormous amount of energy communicating and trying to understand each other.

25:54.000 --> 25:56.000
We struggle mildly with it.

25:56.000 --> 26:00.000
So, so do we expect, you know, computers to take a large corpora,

26:00.000 --> 26:03.000
digest them statistically and come out and do this?

26:03.000 --> 26:04.000
Right.

26:04.000 --> 26:06.000
There's just a lot more going on here.

26:06.000 --> 26:11.000
And I think we have to, we have to kind of open our eyes to what else is going on.

26:11.000 --> 26:14.000
Um, for you and I to get comfortable understanding each other,

26:14.000 --> 26:20.000
we probably have to spend a lot of time sort of synchronizing what our background knowledge is.

26:20.000 --> 26:24.000
Um, what are, what, how we, how we communicate about different things,

26:24.000 --> 26:27.000
how we use particular words, phrases, metaphors.

26:27.000 --> 26:30.000
If I know you're an expert at something, um,

26:30.000 --> 26:34.000
and if I know that too, I can use that as a foundation for doing metaphors.

26:34.000 --> 26:37.000
Um, so there's just a lot to do.

26:37.000 --> 26:41.000
And I think when we imagine elemental cognition,

26:41.000 --> 26:45.000
we imagine the computer kind of engaging with the human continuously.

26:45.000 --> 26:50.000
You know, it was becoming this thought partner that evolves within a community of humans,

26:50.000 --> 26:56.000
talking about a thing and reading about a thing and learning how to align its internal models

26:56.000 --> 27:01.000
and, and how to acquire the right background knowledge to speak and build understanding.

27:01.000 --> 27:07.000
And we start like with the kids, you know, the kid in first grade,

27:07.000 --> 27:11.000
reading to try to understand stuff and imagine collaborating with the teacher.

27:11.000 --> 27:14.000
Look, I don't know what this means. I don't know what that means.

27:14.000 --> 27:18.000
What, um, you know, why would, you know, why would you do that?

27:18.000 --> 27:22.000
And if you're talking about learning about plants and how they grow with light and water

27:22.000 --> 27:27.000
as the simplest sort of stuff, um, what do you have to know to understand that

27:27.000 --> 27:33.000
and put that in the right framework in your head so you can make me a useful predictions about what you've learned?

27:33.000 --> 27:36.000
It's a complex process that involves learning the language,

27:36.000 --> 27:40.000
learning how to map it on to models, learning how to reason over those models,

27:40.000 --> 27:43.000
learning what background knowledge applies and what doesn't apply,

27:43.000 --> 27:47.000
learning what the metaphors, the analogies, the phrases, the words,

27:47.000 --> 27:50.000
the word sense is mean in that context.

27:50.000 --> 27:54.000
And it always involves the kind of the going, the back and forth,

27:54.000 --> 27:58.000
doing the back and forth to kind of get your bearings in your context.

27:58.000 --> 28:01.000
So it's just a challenging, it's just a challenging problem.

28:01.000 --> 28:08.000
So one of the points you made is that we're not setting our aspirations high enough.

28:08.000 --> 28:12.000
And earlier in this conversation, you talked about the AI winter.

28:12.000 --> 28:17.000
Like, is there a relationship between, you know, where we set our aspirations

28:17.000 --> 28:22.000
and the kind of expectations that were set last time with the AI winter

28:22.000 --> 28:25.000
and what we need to do to manage them?

28:25.000 --> 28:30.000
Like, or rather, is there a risk in setting our aspirations too broadly

28:30.000 --> 28:35.000
that we're painting a picture beyond what the technology is capable of

28:35.000 --> 28:41.000
and we kind of set ourselves up for another deflation of those expectations?

28:41.000 --> 28:43.000
Yeah, so it's a good question.

28:43.000 --> 28:47.000
I mean, my perspective on that is a little bit too fold.

28:47.000 --> 28:50.000
I mean, I think we're not in the danger that we once were

28:50.000 --> 28:55.000
in terms of suffering another sort of full-blown AI winter.

28:55.000 --> 29:01.000
And the reason is because I think that there's so many,

29:01.000 --> 29:10.000
there's so much low-hanging fruit for deep learning to continue to provide value.

29:10.000 --> 29:16.000
And I think what deep learning has done for voice recognition,

29:16.000 --> 29:21.000
image recognition, how is it actually helped advance

29:21.000 --> 29:26.000
at least superficial and at least superficial NLP?

29:26.000 --> 29:32.000
I think these are for control systems as well.

29:32.000 --> 29:38.000
I think that this is enormous and will continue to deliver for some time.

29:38.000 --> 29:42.000
And I think we'll continue to deliver value and transformation

29:42.000 --> 29:47.000
actually for some time. The other prong of my answer though is I think

29:47.000 --> 29:52.000
that in some circles there are these expectations for this understanding thing.

29:52.000 --> 29:54.000
I think that's a harder problem.

29:54.000 --> 29:57.000
And I don't think we're yet making the right advancement.

29:57.000 --> 30:00.000
I think that problem is, I think that problem is tractable.

30:00.000 --> 30:04.000
But I don't think yet we're making the right investments.

30:04.000 --> 30:09.000
I mean, the elemental cognition, one of my goals is to demonstrate what I think

30:09.000 --> 30:13.000
the right type of investment is to tackle that problem.

30:13.000 --> 30:15.000
But I think we need more of that type of investment.

30:15.000 --> 30:20.000
And hopefully I can convince people that that's the case as we progress to get to that.

30:20.000 --> 30:23.000
But I think that's a longer road.

30:23.000 --> 30:30.000
I think that the other promising thing where these two paths come together

30:30.000 --> 30:36.000
is that as the AI continues to be impactful and transformative,

30:36.000 --> 30:40.000
it's engaging humans more and more.

30:40.000 --> 30:45.000
So what that means is that you're using, you know, you have your phone,

30:45.000 --> 30:49.000
your computer, you have all these different ways in which.

30:49.000 --> 30:56.000
You're interacting with applications today that engage humans in various forms of interactions.

30:56.000 --> 31:01.000
I think that's essential for the vision that if you ultimately want machines to understand

31:01.000 --> 31:05.000
you're going to need a deep levels of engagement from humans.

31:05.000 --> 31:07.000
And I think that's happening.

31:07.000 --> 31:09.000
That's transforming that's happening.

31:09.000 --> 31:10.000
We're seeing that.

31:10.000 --> 31:12.000
So you have huge opportunity to engage them.

31:12.000 --> 31:14.000
The chicken and the egg problem is that you kind of have to,

31:14.000 --> 31:20.000
you have to engage them well enough that they want to have the back and forth with you.

31:20.000 --> 31:25.000
I'm thinking of IVR systems and immediately pressing the zero button.

31:25.000 --> 31:27.000
Exactly.

31:27.000 --> 31:32.000
Or, you know, giving up on your favorite chatbot, you know,

31:32.000 --> 31:36.000
for anything, anything that's actually complicated or involved,

31:36.000 --> 31:39.000
and you just kind of give up.

31:39.000 --> 31:44.000
Or, let me talk to a human right away and you keep hitting the pound sign until you get a human, whatever it is.

31:44.000 --> 31:49.000
I mean, I think that, you know, so there's a little bit of a chicken and egg where they have to be good enough

31:49.000 --> 31:51.000
to engage you so they can learn from you.

31:51.000 --> 31:55.000
And so that's kind of like what you need that initial investment.

31:55.000 --> 31:59.000
So I'm optimistic that we're not going to suffer an AI winter.

31:59.000 --> 32:03.000
I worry a little bit of people go out there and expect these things to be extraordinary,

32:03.000 --> 32:06.000
at least from the understanding of the site right away.

32:06.000 --> 32:11.000
We have to kind of have a little bit of a cooperation, a collaboration,

32:11.000 --> 32:13.000
and you have to inspire that.

32:13.000 --> 32:20.000
So it sounds like you, you know, while you're respectful of deep learning and what it offers,

32:20.000 --> 32:24.000
you don't think it's the entire solution.

32:24.000 --> 32:28.000
How do you articulate what you think that solution looks like?

32:28.000 --> 32:34.000
So that's also an interesting question because I think the answer is a little bit more nuanced

32:34.000 --> 32:39.000
and my philosophy about that is a little bit more nuanced.

32:39.000 --> 32:50.000
So I'm sort of, with regard to the power of deep learning as a general approach to intelligence,

32:50.000 --> 32:58.000
I'm a little bit, I'm probably lean toward, you know, agnostic may be positive,

32:58.000 --> 33:03.000
a little bit passive as a believer that maybe it's enough to achieve general intelligence

33:03.000 --> 33:05.000
in some theoretical way.

33:05.000 --> 33:10.000
Meaning there exists a neural network of some arbitrary depth and breadth

33:10.000 --> 33:12.000
that can get us to AGI?

33:12.000 --> 33:13.000
Yes.

33:13.000 --> 33:16.000
Given some unspecified data and training method.

33:16.000 --> 33:21.000
Given some unspecified features, your features, right?

33:21.000 --> 33:22.000
Exactly.

33:22.000 --> 33:27.000
So, probably right, exactly, you got it, you got it.

33:27.000 --> 33:32.000
And just to put a fine point on that, you know, the brain is generating features.

33:32.000 --> 33:37.000
It's generating an enormous amount of internal features, particularly when it comes to

33:37.000 --> 33:42.000
socio-economic stuff, emotional stuff, things that relate a lot to how we understand

33:42.000 --> 33:46.000
and build internal models that we could then apply language to.

33:46.000 --> 33:48.000
It's generating a lot of its own stuff.

33:48.000 --> 33:52.000
It's not external data and some effect it's internal data.

33:52.000 --> 33:57.000
You can argue that with exposure to enough of exactly the same stimulus

33:57.000 --> 34:02.000
that, you know, some specified, you know, yet to be specified in the neural net

34:02.000 --> 34:04.000
would generate a lot of those internal features.

34:04.000 --> 34:06.000
I don't know, not necessarily.

34:06.000 --> 34:11.000
And that's the interesting thing about the technology is not necessarily.

34:11.000 --> 34:15.000
It may build a completely different conceptualization of the world around it

34:15.000 --> 34:20.000
in order to survive or do similar things that would be completely incompatible with yours.

34:20.000 --> 34:25.000
So, but anyway, like I said, I mean, sure, maybe, right?

34:25.000 --> 34:32.000
And now the other approach is, if I said my goal was to take all this content,

34:32.000 --> 34:38.000
all these symbols that is our language, and map it to representations

34:38.000 --> 34:42.000
that represent our understanding of that language,

34:42.000 --> 34:48.000
which is now something that more directly models the full internal representation

34:48.000 --> 34:51.000
that we may have or something that's isomorphic to it.

34:51.000 --> 34:54.000
Not necessarily the way we represented it in our brain,

34:54.000 --> 34:58.000
but represented in a way that we would end up with the same language for it.

34:58.000 --> 35:00.000
And I had enough of that data.

35:00.000 --> 35:05.000
And I trained in a deep learning system.

35:05.000 --> 35:12.000
Would it be able to now read and produce an understanding of a blind language?

35:12.000 --> 35:18.000
Maybe, maybe, but you need a hell of a lot of that kind of data.

35:18.000 --> 35:22.000
And we don't generate that type of data readily.

35:22.000 --> 35:29.000
We can't even agree often on what that common understanding of that thing is.

35:29.000 --> 35:32.000
As I said, this goes back to the back and forth.

35:32.000 --> 35:41.000
We sort of assemble and refine and align that understanding through our interaction and collaborations.

35:41.000 --> 35:44.000
So that's an interesting question.

35:44.000 --> 35:50.000
So where I end up with is, we need a hybrid system.

35:50.000 --> 35:53.000
And that hybrid system puts some stakes in the ground.

35:53.000 --> 35:57.000
It says, this is what I think an understanding is.

35:57.000 --> 36:02.000
And I demonstrate that that understanding is ambitious, but not in the lab.

36:02.000 --> 36:08.000
It's ambitious, but good and not so ambitious that it becomes impossible.

36:08.000 --> 36:16.000
For example, you can read a text and you can go on and on about all the depth of understanding

36:16.000 --> 36:21.000
and the layers and layers of meaning and the metaphorical implications and so forth and so on.

36:21.000 --> 36:27.000
Or you can get that text and you could say, I know all the agents.

36:27.000 --> 36:29.000
I know what they did. I know when they did it.

36:29.000 --> 36:33.000
I know the relative geospatial relationships.

36:33.000 --> 36:36.000
And I know how it lays out in time.

36:36.000 --> 36:40.000
And I can tell you what all the individual motivations were.

36:40.000 --> 36:43.000
And their incentives were to take the actions they took.

36:43.000 --> 36:46.000
And I can tell you what events cause what other events.

36:46.000 --> 36:48.000
That would be impressive.

36:48.000 --> 36:51.000
I would be very happy with that.

36:51.000 --> 36:55.000
If that was my fifth grader, I would say, good job.

36:55.000 --> 36:57.000
That's an impressive level of understanding.

36:57.000 --> 37:00.000
If I give you an arbitrary text and you can do that.

37:00.000 --> 37:06.000
But that's also, that's not everything, but it's damn impressive.

37:06.000 --> 37:10.000
So now to do that, what would I need to do?

37:10.000 --> 37:13.000
And the system would need to dialogue effectively.

37:13.000 --> 37:17.000
It would need to systematically be able to acquire knowledge.

37:17.000 --> 37:23.000
To do the full NLP stack and then some that we're also familiar with.

37:23.000 --> 37:28.000
It would need to be able to, if it's going to converse with you at all,

37:28.000 --> 37:33.000
it can't be completely absent of any background knowledge at all.

37:33.000 --> 37:39.000
So it needs to do kind of corpus analysis and knowledge graph building.

37:39.000 --> 37:42.000
It needs to be able to build an internal representation.

37:42.000 --> 37:47.000
And then reason over so it can make logical predictions and entailments.

37:47.000 --> 37:50.000
So you can imagine as I'm going through all this stuff,

37:50.000 --> 37:54.000
the system that we're building at EC has dialogue components,

37:54.000 --> 37:59.000
has deductive and inductive reasoning mechanisms.

37:59.000 --> 38:05.000
It does uses deep learning to do corpus analysis and knowledge graph building.

38:05.000 --> 38:08.000
It uses deep learning to do basic NLP.

38:08.000 --> 38:13.000
And of course, it has an architecture through which all these components are integrated.

38:13.000 --> 38:16.000
It's been, it's enormously challenging.

38:16.000 --> 38:21.000
We have a ways to go, but those are the ingredients that we're playing with in this system.

38:21.000 --> 38:23.000
How do you characterize?

38:23.000 --> 38:27.000
Can I wear you are with it relative to?

38:27.000 --> 38:31.000
Well, relative, relative to benchmarks that we need to define.

38:31.000 --> 38:35.000
The one one is like you, which is just outlined with regard to understanding.

38:35.000 --> 38:38.000
Well, that's exactly right. I mean, I think that look,

38:38.000 --> 38:45.000
we're about six months to a year away from I think defining a good,

38:45.000 --> 38:50.000
the way I like to phrase is a good, ambitious challenge problem.

38:50.000 --> 38:56.000
So in other words, one that has a data set, a clear metric.

38:56.000 --> 39:01.000
As I said, a very ambitious metric, a button on the less a clear metric.

39:01.000 --> 39:08.000
And in an evaluation process, that just sets the bar a lot higher than what we're seeing today.

39:08.000 --> 39:13.000
It's not limited by what deep learning stuff can do today.

39:13.000 --> 39:18.000
It sort of takes that barrier and says, hey, let's forget about what's possible and isn't possible today.

39:18.000 --> 39:22.000
What do we think on what we should be able to do?

39:22.000 --> 39:28.000
Would be a good definition of this understanding problem that is ambitious enough and challenging enough.

39:28.000 --> 39:33.000
But not impossible, or at least our perspective is not impossible.

39:33.000 --> 39:42.000
And also to demonstrate an approach that is viable against that challenge problem.

39:42.000 --> 39:45.000
And I'm super excited about that because I think this is what AI needs.

39:45.000 --> 39:49.000
I mean, if we really want to tackle understanding, we need that.

39:49.000 --> 39:54.000
When you think about this definition of understanding and the challenge problem,

39:54.000 --> 40:00.000
to what extent does it incorporate elements they get at nuance,

40:00.000 --> 40:04.000
like I'm thinking of things like Winagrad Schema challenge and things like that.

40:04.000 --> 40:08.000
Yeah, I mean, I think the Winagrad Schema challenge is interesting.

40:08.000 --> 40:13.000
And we should be able to tackle that type of stuff.

40:13.000 --> 40:18.000
So I think it absolutely has to get at the nuance that you suggest there.

40:18.000 --> 40:23.000
And again, because it is building an internal representation,

40:23.000 --> 40:27.000
it will get confused. It should know it's confused.

40:27.000 --> 40:31.000
It should be able to say, I don't understand this.

40:31.000 --> 40:35.000
I can't fit this into my prior models of how the world works.

40:35.000 --> 40:38.000
And here's how you can help me.

40:38.000 --> 40:41.000
I mean, that's how it should behave.

40:41.000 --> 40:47.000
And so you characterize this as a hybrid system.

40:47.000 --> 40:51.000
And I'm envisioning hybrid not just being a connection of two different things,

40:51.000 --> 40:54.000
but lots of different things.

40:54.000 --> 40:59.000
And it strikes me that a big part of how you glue all this stuff together

40:59.000 --> 41:06.000
is kind of formalizing the formal knowledge piece and representation.

41:06.000 --> 41:12.000
To what extent is that an important element of the overall functioning of a system like this?

41:12.000 --> 41:16.000
Yeah, so the formal representation is an important part.

41:16.000 --> 41:22.000
And so you could imagine, I mean, it's not, I don't think it's hard to imagine if you thought at all about,

41:22.000 --> 41:25.000
you know, architect these kinds of systems.

41:25.000 --> 41:27.000
You know, you have to, you know, language comes in.

41:27.000 --> 41:29.000
You have to go a little parsed.

41:29.000 --> 41:33.000
You have to be able to do an initial syntactic and semantic,

41:33.000 --> 41:36.000
at least shallow semantic interpretation.

41:36.000 --> 41:39.000
You're going to get all kinds of possibilities.

41:39.000 --> 41:41.000
So you're going to get different parses.

41:41.000 --> 41:44.000
You're going to get different word senses.

41:44.000 --> 41:50.000
You know, you're going to, you're going to get different semantic interpretations at different levels.

41:50.000 --> 41:52.000
And you have to start to make sense of it.

41:52.000 --> 41:56.000
And so when it gets to the making sense part,

41:56.000 --> 41:59.000
you have to be able to reason about it.

41:59.000 --> 42:05.000
So, you know, there's a formal representation that ultimately we map to.

42:05.000 --> 42:09.000
There are multiple reasoning engines that pour over this

42:09.000 --> 42:15.000
and start to evaluate different interpretations for the level of sense that they make.

42:15.000 --> 42:18.000
It's as smart as it's prior knowledge.

42:18.000 --> 42:24.000
So there's a Bayesian aspect to this as well as it uses prior knowledge to actually try to determine confidence

42:24.000 --> 42:27.000
in what is the right interpretation and how do I move forward?

42:27.000 --> 42:33.000
It has to be open ended as more information lands and gets acquired.

42:33.000 --> 42:38.000
It has to be able to kind of go back and say, okay, which direction do I go to continue to make sense out of this?

42:38.000 --> 42:43.000
It has to know what it's doing so that it can dialogue and ask questions

42:43.000 --> 42:45.000
and ask for help about its interpretations.

42:45.000 --> 42:50.000
And it has to do that in a way that isn't incredibly stupid

42:50.000 --> 42:53.000
because otherwise humans aren't going to engage in it.

42:53.000 --> 43:01.000
So it has to do everything it possibly can do in extracting back our knowledge from large corpora.

43:01.000 --> 43:04.000
Even if it doesn't completely understand the large corporate yet

43:04.000 --> 43:07.000
because it doesn't have the foundations to do that.

43:07.000 --> 43:12.000
It at least has to be able to use that to prune its search and shape its interaction with humans.

43:12.000 --> 43:16.000
So it doesn't get so stupid that nobody wants to talk to it.

43:16.000 --> 43:23.000
So these are all enormous challenges that bring in sort of every aspect of AI,

43:23.000 --> 43:27.000
save robotics type stuff, hardware type stuff,

43:27.000 --> 43:33.000
but in terms of knowledge representation, reasoning,

43:33.000 --> 43:38.000
natural language processing, learning, deep learning, reinforcement learning,

43:38.000 --> 43:41.000
dialogue management, you name it.

43:41.000 --> 43:48.000
You certainly make a good argument for the level of difficulty of the problem.

43:48.000 --> 43:53.000
The other part of the argument you're making is the type of investment

43:53.000 --> 43:57.000
and need to invest in solving it.

43:57.000 --> 44:00.000
How do you characterize that for folks?

44:00.000 --> 44:03.000
Yeah, so I mean, you know, the team, so first of it's interesting

44:03.000 --> 44:10.000
because I'm all, you know, EC while we're doing or approaching a hard research problem,

44:10.000 --> 44:14.000
you know, we're not strictly an academic research institution.

44:14.000 --> 44:19.000
We are engineering a system and we're continually refining it, building it, and evaluating it.

44:19.000 --> 44:23.000
So we have a mix of engineers and researchers,

44:23.000 --> 44:28.000
and you can imagine that there's quite a bit of diversity.

44:28.000 --> 44:33.000
So if people who specialize in machine learning, deep learning for folks,

44:33.000 --> 44:37.000
we have NLP folks, dialogue folks, we have knowledge representation,

44:37.000 --> 44:42.000
we have reasoning folks, we have linguists, so, you know,

44:42.000 --> 44:49.000
and having a underlying architecture that is laid out well enough

44:49.000 --> 44:52.000
that these individuals can work together.

44:52.000 --> 44:55.000
You know, and I did this on, even though this is,

44:55.000 --> 44:59.000
and towards a magnitude more complicated than the Watson architecture,

44:59.000 --> 45:06.000
the basic approach toward managing a team is like this is very similar,

45:06.000 --> 45:10.000
and how to conduct and assemble that team is very similar.

45:10.000 --> 45:15.000
So the first set of interests, very committed to the mission,

45:15.000 --> 45:18.000
you know, is incredibly important.

45:18.000 --> 45:22.000
And at the same time, coming at it from very different perspectives

45:22.000 --> 45:26.000
they all see themselves as contributing to a larger, more complex architecture

45:26.000 --> 45:31.000
that isn't just an architecture in theory.

45:31.000 --> 45:35.000
It's an engineered system and sort of you know where to plug in

45:35.000 --> 45:39.000
and how to contribute and sort of move the ball forward.

45:39.000 --> 45:44.000
So anyway, these are how I think about managing this type of project.

45:44.000 --> 45:48.000
In terms of the scale of the investment, also good question,

45:48.000 --> 45:54.000
always thinking about that myself, I'm always careful to invest incrementally.

45:54.000 --> 46:00.000
So as I see, as I see the pieces coming together and, you know,

46:00.000 --> 46:05.000
you start with a few people, you grow out, as you see pieces coming together,

46:05.000 --> 46:08.000
you look for where the bottlenecks are, you look where for the opportunities are

46:08.000 --> 46:11.000
and where the bottlenecks are, and you grow accordingly.

46:11.000 --> 46:15.000
And so I tend to do that very carefully and pick the right people to fit into

46:15.000 --> 46:19.000
the right places as we go. And so we've been doing that.

46:19.000 --> 46:25.000
We're up to about 24 people now or, and is that enough?

46:25.000 --> 46:28.000
No, I think that ultimately investment requires more than that.

46:28.000 --> 46:31.000
And it'll probably continue to grow. But as I said, we'll grow incrementally

46:31.000 --> 46:36.000
as we, as it's clear that that's where we're going to get the,

46:36.000 --> 46:39.000
you know, the incremental value.

46:39.000 --> 46:43.000
Earlier you mentioned that there's plenty of low hanging fruit

46:43.000 --> 46:48.000
in deep learning. What's the, the argument for investing in clearly a difficult

46:48.000 --> 46:52.000
problem relative to picking off some of that low hanging fruit?

46:52.000 --> 46:55.000
It's tough. It's a tough call.

46:55.000 --> 47:00.000
You know, it's difficult and it's difficult in terms of,

47:00.000 --> 47:05.000
I'm lucky, you know, that I have the investor that I have who's interested

47:05.000 --> 47:09.000
in the bigger picture and the longer term role that AI has to play.

47:09.000 --> 47:14.000
And how it, how it, how the human machine interaction gets shaped over time.

47:14.000 --> 47:19.000
And also has an appetite for a longer,

47:19.000 --> 47:25.000
longer term type investment and sort of bigger bang for the buck in terms of the impact.

47:25.000 --> 47:32.000
So that's good, hard to find than the 18 months or 24 month lower hanging fruit stuff.

47:32.000 --> 47:36.000
So that's hard to find, but I've got that.

47:36.000 --> 47:39.000
But it also hits you in the recruiting side because you get a lot of people

47:39.000 --> 47:41.000
who are like, you know, I want the quick win.

47:41.000 --> 47:45.000
And there are like quite a number of exciting applications.

47:45.000 --> 47:48.000
I think you could approach with, with just, you know,

47:48.000 --> 47:50.000
straight applications of deep learning.

47:50.000 --> 47:52.000
And they're cool and they're fun.

47:52.000 --> 47:59.000
So you end up really looking and finding people who are just really want,

47:59.000 --> 48:02.000
they want that machine that fluently talks to them.

48:02.000 --> 48:07.000
They want that, they want that, you know, that Star Trek computer that becomes their,

48:07.000 --> 48:08.000
you know, thought partner.

48:08.000 --> 48:12.000
And, you know, just some people really have that dream.

48:12.000 --> 48:16.000
And they understand that, that, that this is,

48:16.000 --> 48:18.000
this is one of the best shots for getting there.

48:18.000 --> 48:21.000
Maybe to kind of start to wrap things up.

48:21.000 --> 48:28.000
Can you talk a little bit about how you deliver something like that in phases?

48:28.000 --> 48:33.000
Or, or, is it, you know, definitionally kind of a big bang thing?

48:33.000 --> 48:37.000
Like we just get there because we have to have all these pieces just right

48:37.000 --> 48:43.000
in order to execute that ultimate vision.

48:43.000 --> 48:46.000
Yeah, so another, I, another really good question.

48:46.000 --> 48:51.000
And, and, and we, we work, we, you know, we struggle through that question.

48:51.000 --> 48:53.000
I mean, I think that there are phases.

48:53.000 --> 48:57.000
And I think that are both phases from a business perspective,

48:57.000 --> 49:01.000
from an investment perspective, and there are also phases from a,

49:01.000 --> 49:05.000
more of a scientific or research perspective.

49:05.000 --> 49:12.000
And we do spend time thinking about, you know, incremental applications

49:12.000 --> 49:15.000
that we can build with the technology that we're creating.

49:15.000 --> 49:17.000
It's part of the investment.

49:17.000 --> 49:20.000
It's not the main thrust right now.

49:20.000 --> 49:24.000
The main thrust is really staging the, the scientific work.

49:24.000 --> 49:28.000
And as I mentioned, you know, defining the problem really well,

49:28.000 --> 49:32.000
making sure that we can scientifically evaluate progress against it.

49:32.000 --> 49:38.000
We continually actually apply deep learning systems and see how they fail

49:38.000 --> 49:40.000
and do the air analysis.

49:40.000 --> 49:45.000
We continually apply our evolving system, look at where it's failing,

49:45.000 --> 49:47.000
and continue to try to improve it.

49:47.000 --> 49:50.000
And we, we create incremental milestones for ourselves.

49:50.000 --> 49:54.000
And that's important because to manage the project,

49:54.000 --> 49:57.000
you can't have this thing that's like five years away.

49:57.000 --> 50:00.000
You have to create those incremental milestones

50:00.000 --> 50:02.000
and even creating those themselves are challenged,

50:02.000 --> 50:04.000
particularly in the very beginning stages.

50:04.000 --> 50:07.000
We're getting to a point where, where we can,

50:07.000 --> 50:09.000
we can create those intermediate scientific milestones,

50:09.000 --> 50:12.000
I think more right, really, and more effectively,

50:12.000 --> 50:15.000
because we have enough of the architecture built out that we can do that.

50:15.000 --> 50:17.000
We can run this on content.

50:17.000 --> 50:21.000
We can do the air analysis. We can identify where the problems are.

50:21.000 --> 50:23.000
We can start to iterate on making the system smarter.

50:23.000 --> 50:26.000
So we're getting, we're at that point now, which is great.

50:26.000 --> 50:28.000
But that's a very important thing.

50:28.000 --> 50:32.000
And learning how to evolve those milestones well is important.

50:32.000 --> 50:36.000
On the, on the kind of incremental application side,

50:36.000 --> 50:40.000
that's also kind of getting some attention for us.

50:40.000 --> 50:44.000
And we're thinking about ways that we can use the,

50:44.000 --> 50:48.000
even though we don't have that deep general understanding yet,

50:48.000 --> 50:52.000
we've built a lot of impressive kind of NLP and,

50:52.000 --> 50:56.000
and representation and reasoning stuff that we think can help out

50:56.000 --> 50:59.000
in a number of different areas and replaying with that.

50:59.000 --> 51:02.000
We don't, we don't have to spin off though,

51:02.000 --> 51:04.000
because of our investment structure,

51:04.000 --> 51:06.000
we don't have to spin off those applications.

51:06.000 --> 51:10.000
But we want to keep ourselves honest and make sure that we can demonstrate

51:10.000 --> 51:15.000
to ourselves, our investors, that this technology will have impact

51:15.000 --> 51:18.000
and that it can, and it can do that incrementally.

51:18.000 --> 51:22.000
So that certainly is a part of what we, we think about and do.

51:22.000 --> 51:24.000
But it's, it's, it's good.

51:24.000 --> 51:27.000
You're asking like, you're, you're asking key questions

51:27.000 --> 51:29.000
to figuring out how to, how to manage a business like this.

51:29.000 --> 51:30.000
You're absolutely right.

51:30.000 --> 51:34.000
Well, Dave, thanks so much for taking the time to chat with me

51:34.000 --> 51:35.000
about what you're working on.

51:35.000 --> 51:37.000
It sounds like really interesting stuff.

51:37.000 --> 51:39.000
You bet. It's been a pleasure.

51:39.000 --> 51:41.000
Awesome, my pleasure. Thanks so much.

51:41.000 --> 51:42.000
Thank you.

51:46.000 --> 51:47.000
All right, everyone.

51:47.000 --> 51:49.000
That's our show for today.

51:49.000 --> 51:51.000
Make sure you leave us your birthday message

51:51.000 --> 51:54.000
over at twimmelai.com slash 3Bday

51:54.000 --> 52:00.000
or via voicemail at 1636-735-3658.

52:00.000 --> 52:02.000
We cannot wait to hear from you.

52:02.000 --> 52:05.000
We want to know your favorite gem from the podcast,

52:05.000 --> 52:07.000
what you've learned and how you've applied it

52:07.000 --> 52:09.000
to what you do.

52:09.000 --> 52:13.000
For information about today's guest, visit twimmelai.com.

52:13.000 --> 52:16.000
As always, and especially today,

52:16.000 --> 52:43.000
thanks so much for listening and catch you next time.

