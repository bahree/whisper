1
00:00:00,000 --> 00:00:11,200
All right, everyone. Welcome to another episode of the Twomel AI podcast. I am your host,

2
00:00:11,200 --> 00:00:18,080
Sam Charington. And today I'm joined by Arash Bebudi. Arash is a machine learning researcher

3
00:00:18,080 --> 00:00:23,840
at Qualcomm Technologies. Before we get going, be sure to take a moment to hit that subscribe

4
00:00:23,840 --> 00:00:28,640
button wherever you're listening to today's show. Arash, welcome to the podcast.

5
00:00:28,640 --> 00:00:33,440
Yeah, hi, Sam. Thanks for having me. It's great to have you on the show and I'm looking forward

6
00:00:33,440 --> 00:00:38,640
to the chat. I'd love to have you start out by sharing a little bit about your background and

7
00:00:38,640 --> 00:00:46,320
what led you into the field of machine learning. Well, my background is information theory,

8
00:00:46,320 --> 00:00:51,840
a mathematical signal processing on the one hand, that my PhD was about basically deriving

9
00:00:51,840 --> 00:01:00,160
Shannon theoretic bounds on information capacity of certain cooperative networks on their

10
00:01:00,160 --> 00:01:07,760
channel on certain T. So kind of mathematical PhD work. And after that, kind of transition into

11
00:01:08,480 --> 00:01:15,520
in the field of compress sensing, it's quite fascinating to me because the core problem

12
00:01:15,520 --> 00:01:22,480
in compress sensing was you have set an observations of an unknown signal, but you don't have enough

13
00:01:22,480 --> 00:01:27,680
observation to recover it fully. So you have to rely on some prior assumption about the signal.

14
00:01:27,680 --> 00:01:33,520
And the question is, if you know that prior, what is a good, you know, how you can recover your

15
00:01:33,520 --> 00:01:40,080
signal and what are theoretical bounds on, basically, sample complexity, how many observations you

16
00:01:40,080 --> 00:01:45,120
need and all that. So kind of dealing with that notion of a structure, signal structure,

17
00:01:45,120 --> 00:01:51,280
and how this can be used in different kind of inverse problems. That was very, very interesting

18
00:01:51,280 --> 00:01:57,600
for me and kind of I was working on that afterward. But then of course deep learning comes with a lot

19
00:01:57,600 --> 00:02:07,040
of a lot of interesting promises and challenges. So immediately I said, you know, maybe you know,

20
00:02:07,040 --> 00:02:13,120
with deep learning can even do better with, you know, to do a better job of working with structures

21
00:02:13,120 --> 00:02:21,520
and learning better recovery algorithms. And even from theoretical side, I was very

22
00:02:21,520 --> 00:02:26,400
interested in understanding like mysteries of deep learning. So I think kind of that dragged

23
00:02:26,400 --> 00:02:35,680
me into deep learning. But funny enough, you know, on the side note, when I was undergrad,

24
00:02:35,680 --> 00:02:42,240
I could one of my main interests was philosophy. And kind of I did that in parallel doing whole,

25
00:02:42,240 --> 00:02:49,040
in my whole life. And when I was doing my PhD in parallel, I also did master degree on that. And

26
00:02:49,040 --> 00:02:54,160
kind of a lot of topics that kind of studied there, philosophy of mind, language and all that.

27
00:02:54,800 --> 00:02:59,920
Question of meaning, intelligence, consciousness, all that's kind of re-emerged in this field

28
00:03:00,800 --> 00:03:06,400
of machine learning again. And kind of it's it's it became a very exciting area to work on.

29
00:03:06,400 --> 00:03:12,240
And kind of I'm happy that all my interests are coming together within within this field.

30
00:03:12,240 --> 00:03:21,120
Oh, that's fantastic, fantastic. What, you're at obviously Qualcomm AI research now. What are your

31
00:03:21,120 --> 00:03:28,240
primary research interests on that team? Yeah. So, you know, information, I talked about

32
00:03:28,240 --> 00:03:32,080
information theory and compress and seeing a mathematical signal processing. And

33
00:03:32,080 --> 00:03:37,520
wireless communication, of course, you know, Shannon's theory is mathematical theory of communication.

34
00:03:37,520 --> 00:03:42,560
So kind of communication and wireless communication in particular has always been very also

35
00:03:42,560 --> 00:03:47,600
interesting to me. And if I worked on that also as kind of a motivation for a lot of problems,

36
00:03:47,600 --> 00:03:56,560
I was I was formulating. And so what we're trying to do here, we're basically looking at wireless

37
00:03:56,560 --> 00:04:03,120
communication from machine learning perspective, put it that way. We're trying to understand how

38
00:04:04,000 --> 00:04:12,000
we can design new machine learning architectures useful for different wireless tasks and how we

39
00:04:12,000 --> 00:04:17,760
can improve different parts of wireless system design using machine learning. And this kind of

40
00:04:18,800 --> 00:04:23,040
relates to the current technologies that we have and the next technologies that are going to

41
00:04:23,040 --> 00:04:28,880
come. And so the research that we have, I like to call it wireless AI research. So it's a kind of

42
00:04:30,480 --> 00:04:36,720
doing machine learning research for wireless communication. And one of the papers that we're

43
00:04:36,720 --> 00:04:42,560
going to talk about that you have had accepted at ICML this year is in the same field of

44
00:04:42,560 --> 00:04:48,080
compress sensing that has come up a few times. It sounds like it's an area that you're continuing

45
00:04:48,080 --> 00:04:54,080
to explore pretty deeply. Yeah, absolutely. I think, you know, the inverse problems there

46
00:04:54,080 --> 00:05:01,600
everywhere. We kind of from medical imaging to all sorts of kind of you can talk about drug discovery

47
00:05:01,600 --> 00:05:08,480
problems by, you know, you know, protein unfolding is another example. So we can find a lot of

48
00:05:08,480 --> 00:05:15,680
a lot of areas where inverse problems are important. And wireless communication is actually

49
00:05:15,680 --> 00:05:21,920
full of those problems like from child estimation to other topics. So you can find a lot of inverse

50
00:05:21,920 --> 00:05:29,920
problems. And yeah, I mean, the compress sensing is definitely like a core core interest for me there.

51
00:05:29,920 --> 00:05:35,840
Now, I've talked to some of your colleagues about some of the work that other teams are doing

52
00:05:35,840 --> 00:05:42,480
in compression. How does compression and the setting there compare with compress sensing?

53
00:05:42,480 --> 00:05:49,600
Is it the same? Is it different? It is different. So in compress sensing, the idea is that you want

54
00:05:49,600 --> 00:05:57,760
to efficiently sense the environment in order to infer something. In compression, the idea is you

55
00:05:57,760 --> 00:06:04,720
have a source, you have information source, and you want to efficiently compress it. So there is no

56
00:06:04,720 --> 00:06:11,600
sensing medium there. The only thing that's important is how, how will you compress and reconstruct

57
00:06:11,600 --> 00:06:17,600
your information source? In compress sensing, the idea is how you sense the medium given the

58
00:06:17,600 --> 00:06:22,960
constraint that is given to you. You do not choose that. And then how well you can reconstruct

59
00:06:22,960 --> 00:06:28,560
based on based on the observation you have. So they they're conceptually they might be connected

60
00:06:28,560 --> 00:06:35,760
but if they're kind of a parallel field, I would say. And so with that said, what is the motivation

61
00:06:35,760 --> 00:06:43,920
for your ICML paper, which is called equivalent priors for compress sensing with unknown orientation?

62
00:06:43,920 --> 00:06:50,240
Great. So let me let me start with the first talking about generative priors and its relation

63
00:06:50,240 --> 00:06:57,840
to compress sensing. This is probably a good start. So when I talked about the structure,

64
00:06:57,840 --> 00:07:03,840
so assuming some some prior about the structure of the signal, for example, the typical example

65
00:07:03,840 --> 00:07:09,920
of a structure in classical compress sensing was sparsity. You assume that your signal is sparse

66
00:07:09,920 --> 00:07:15,600
in a given basis, for example. And then you use that to some sort of a one minimization to basically

67
00:07:15,600 --> 00:07:24,400
find your signal. Now with deep learning, with generative models specifically, we notice that

68
00:07:24,400 --> 00:07:34,880
actually generative models provide a way of parametrizing space of signals of interest using

69
00:07:34,880 --> 00:07:40,800
basically the latent space of the generative model. So and specifically, if you cannot

70
00:07:40,800 --> 00:07:46,560
tractably represent a signal, in this case, let's say images, if you cannot tractably represent

71
00:07:46,560 --> 00:07:53,920
them, represent the prior generative models provide a way of parametrizing. And there was a work by

72
00:07:53,920 --> 00:08:01,280
Bora and co-authors around 2015 that showed actually you can use generative priors,

73
00:08:01,280 --> 00:08:10,000
generative models, and do gradient descent on the latent space of that generative model

74
00:08:10,000 --> 00:08:17,040
in order to estimate the signal. So generative models in that sense provide a structural

75
00:08:17,040 --> 00:08:24,720
structure prior on your signal. And the paper showed quite interesting results and it was quite

76
00:08:25,440 --> 00:08:32,000
quite an encouraging work with theoretical guarantees on top, which is always good to have.

77
00:08:32,640 --> 00:08:38,480
So that was the original work. Now a lot of work came afterward basically using flows,

78
00:08:38,480 --> 00:08:42,880
using different techniques for doing a better job in this thing. We're extending it into

79
00:08:42,880 --> 00:08:51,520
different type of non-linear problems and values extensions. Some challenges that these models have

80
00:08:51,520 --> 00:08:58,240
are, let's say, convergence and latency, meaning that sometimes when you do gradient descent

81
00:08:58,240 --> 00:09:03,040
on the latent space, you might need to restart the whole process because it's not converging.

82
00:09:04,080 --> 00:09:11,920
So the convergence and the way you optimize your problem can become an issue,

83
00:09:11,920 --> 00:09:17,040
especially if you're interested in low latency, let's say solutions, that can become a bottleneck.

84
00:09:17,040 --> 00:09:25,040
So there were actually some works about how you can effectively invert a generative model.

85
00:09:25,040 --> 00:09:31,200
And actually Alex Demarkis from the TU Austin had some follow-up works. Actually it was a quarter,

86
00:09:31,200 --> 00:09:36,480
a quarter of the original papers, some follow-up works on kind of a layer by layer inversion and

87
00:09:36,480 --> 00:09:43,600
some other tricks following that as well. So that's one part, like how why generative models are

88
00:09:43,600 --> 00:09:52,400
relevant for compressency. The other part was, okay, in many applications, we might not have,

89
00:09:52,960 --> 00:10:00,320
when we want to measure the signal, the signal might go under some transformation. Let's say

90
00:10:00,320 --> 00:10:05,840
some rotation before the measurement. This can happen. A typical example is a cryo-electron

91
00:10:05,840 --> 00:10:12,560
microscopy. So cryo-electron microscopy is a way of basically taking a picture of a biomolecule.

92
00:10:12,560 --> 00:10:17,520
You can look at it like that. And the problem is that when you take this picture, of course,

93
00:10:17,520 --> 00:10:22,080
the picture is very noisy and all that, but the molecule that you have is in an unknown orientation.

94
00:10:23,680 --> 00:10:28,160
And if you have multiple pictures, these orientations are not the same, so are not aligned.

95
00:10:28,160 --> 00:10:35,600
Also, you can find in different, so you can, in general, you can think of a signal that you

96
00:10:35,600 --> 00:10:40,240
actually can turn a generative model on, but the orientation of that signal might change

97
00:10:41,520 --> 00:10:46,960
before the measurement. So you might say, okay, let's, you know, what I can do probably is I try

98
00:10:46,960 --> 00:10:52,560
to first figure out the orientation and then reconstruct the signal and or doing it iteratively

99
00:10:52,560 --> 00:10:59,040
basically finding solving the problem like that. What we thought was, oh, why do we need to

100
00:10:59,840 --> 00:11:06,640
kind of separate orientation discovery and the signal recovery? So we can do this jointly.

101
00:11:07,360 --> 00:11:15,280
And the way we can do it is we can train a generative model that has already disinformation,

102
00:11:15,280 --> 00:11:20,560
has already disinformation embedded in it. Of course, you can say, oh, you know, what we can do,

103
00:11:20,560 --> 00:11:27,760
we can get a generative model, use data augmentation and basically, now we have it already there somehow

104
00:11:27,760 --> 00:11:32,640
because we have seen all these different orientations. But when we know that we want to have this

105
00:11:34,240 --> 00:11:41,520
this steerable basically transformable latent space, equivalence models are excellent candidates.

106
00:11:42,320 --> 00:11:49,280
So what are equivalent equivalence models? So the equivalence models are models that when you

107
00:11:49,280 --> 00:11:55,760
transform the inputs of the network according to certain group transformations, let's say rotation.

108
00:11:55,760 --> 00:12:01,600
When you rotate the input, then the features that are outputs of that network also rotate.

109
00:12:02,880 --> 00:12:08,560
So therefore, a rotation, a change of orientation at the output correspond to the change of orientation

110
00:12:08,560 --> 00:12:15,120
as the input. This also imposes some sort of a structure on the latent space of your generative model.

111
00:12:15,120 --> 00:12:22,080
So the main idea was, let's try to use equivalence priors so that we can just solve this whole problem

112
00:12:22,080 --> 00:12:29,440
altogether. Awesome. Now I've talked with colleagues in the past of yours about Equavareans as well.

113
00:12:30,400 --> 00:12:36,400
I think in the context of those previous conversations, we're primarily talking about supervised

114
00:12:36,400 --> 00:12:44,320
types of problems, has Equavareans been applied to generative types of problems previously?

115
00:12:44,320 --> 00:12:51,760
Yeah, this is a good question. Actually, the answer is yes. The answer is in context of

116
00:12:52,480 --> 00:12:58,400
generative model, there have been some works that try to incorporate this equivalence into the

117
00:12:58,400 --> 00:13:04,000
model architecture. The way we kind of did that in our work, they're kind of equivalent VAE

118
00:13:04,000 --> 00:13:09,520
that we built, as far as Vino, as far as I know, this is the first time that we have such an

119
00:13:09,520 --> 00:13:16,960
equivalent VAE constructed. But at the end of the day, I think another example that comes to my

120
00:13:16,960 --> 00:13:20,800
mind is the kind of equivalence normalizing flows, that's something that has been

121
00:13:22,320 --> 00:13:29,280
published I think last year. So one challenge of those works are very nice and elegant,

122
00:13:29,280 --> 00:13:37,280
but kind of scaling them up to kind of a problem that wants to have low complexity, low latency,

123
00:13:37,280 --> 00:13:42,480
which is actually the main motivation at the end of the day. Scaling them or bringing them to that

124
00:13:45,680 --> 00:13:51,200
to that type of problem, it becomes a bit challenging. For example, for normalizing,

125
00:13:51,200 --> 00:13:56,800
Equavare normalizing flows, you know, you had to work with this type of a kind of continuous

126
00:13:57,760 --> 00:14:03,120
time and neural networks, like ODE type models. And training those models are difficult,

127
00:14:03,120 --> 00:14:09,360
scaling them up are difficult. And what we're targeting here is just having a nice small

128
00:14:09,360 --> 00:14:15,360
equivalent models that can already, can perform already quite well compared to the counter-counter parts.

129
00:14:16,000 --> 00:14:23,200
How did you, assuming you started with a traditional VAE type of an architecture, how did you

130
00:14:24,240 --> 00:14:30,240
evolve it to be equivariant based or to understand equivariants?

131
00:14:30,240 --> 00:14:35,840
Equivariants, you know, the variational auto encoders, you have an encoder network and decoder

132
00:14:35,840 --> 00:14:45,360
network. The decoder network is the network that is going to be used for our recovery task,

133
00:14:45,360 --> 00:14:51,520
because that is the generator part, and we're going to use the latent space. So we want the

134
00:14:52,320 --> 00:14:59,200
output orientation change. So basically transformation of our signal translates into the transformation

135
00:14:59,200 --> 00:15:03,920
of our input. So what we want is that the generator of the network should be equivariant,

136
00:15:03,920 --> 00:15:10,480
so that the decoder network will pick an equivariant network off the shelf from the existing models.

137
00:15:10,480 --> 00:15:16,880
Now, what happens to the encoder network? The encoder network basically in VAE gives

138
00:15:17,760 --> 00:15:23,840
parameters of your approximate posterior. So if you're assuming that we have a Gaussian type

139
00:15:23,840 --> 00:15:30,320
of posterior, you have a mean value, and you have a Koreans matrix basically. Now, when you rotate

140
00:15:30,320 --> 00:15:37,840
the image, you, since this, you're going to, your encoder network is going to give you a

141
00:15:37,840 --> 00:15:43,280
distribution basically approximate posterior, you will get a new distribution when the input

142
00:15:43,280 --> 00:15:50,880
is rotated. What should be this distribution? So the random, consider random variable corresponding

143
00:15:50,880 --> 00:15:57,680
to the untransformed image, right? Then you transform that image, you want to get a new random

144
00:15:57,680 --> 00:16:04,960
variable, a random variable, that is the transformed version of that. So basically, if the

145
00:16:04,960 --> 00:16:09,120
distribution that you want, you want it to correspond to the transformed random variable.

146
00:16:10,160 --> 00:16:16,320
So, okay, so that is, that would, let me know. Now, what happens to the parameters of this

147
00:16:16,320 --> 00:16:23,520
distribution? Let's start with mean value. The mean value, as we wanted, is when we transform

148
00:16:24,240 --> 00:16:29,600
a random vector with the rotation matrix, for example, then the mean value of that random

149
00:16:29,600 --> 00:16:35,280
vector is also transformed according to a, to rotation matrix. So this basically means that

150
00:16:36,560 --> 00:16:43,440
the, the part of the network giving us mean value should be equivalent. So that is also

151
00:16:43,440 --> 00:16:50,000
a figure out. The challenge is for the covariance matrix. Then you transform your random vector

152
00:16:50,000 --> 00:16:55,440
according to a matrix, let's say, A, you have A times your random variable, random vector.

153
00:16:55,440 --> 00:17:01,120
The covariance matrix changes according to A times the previous covariance matrix times

154
00:17:01,120 --> 00:17:08,480
a hermitian. So we do not have like the classical equivalence anymore because if you transform

155
00:17:08,480 --> 00:17:13,920
your input according to A, the covariance part should be transformed according to A, A hermitian.

156
00:17:13,920 --> 00:17:20,880
First conclusion out of that is that we cannot use the typical diagonal covariance matrix assumption

157
00:17:20,880 --> 00:17:26,480
used in VAs. So we need to consider like a full covariance matrix. Another thing is how we're going

158
00:17:26,480 --> 00:17:32,000
to build this kind of, this transformation, this, this type of network. So the way we did that,

159
00:17:32,000 --> 00:17:37,760
we said, okay, with covariance matrix is a positive definite matrix. So we can already

160
00:17:37,760 --> 00:17:44,000
transform, you write it down as a V, V transpose. We considered other ways of parameterizing that

161
00:17:44,000 --> 00:17:49,120
as well. So we write the, the covariance matrix, the positive definite covariance matrix as VV

162
00:17:49,120 --> 00:17:57,920
transpose. And then if that V matrix is equivalent. So it's just transformed. Then the whole

163
00:17:57,920 --> 00:18:02,960
covariance matrix satisfies the condition that we want. So we built the, the, the, the, the

164
00:18:02,960 --> 00:18:08,400
decoder network giving us the encoder network, giving us the covariance matrix in a way that,

165
00:18:09,360 --> 00:18:15,360
that part of the network is, is equivalent and it's just parametrizes basically the V presentation.

166
00:18:15,360 --> 00:18:19,680
There are other ways of doing that, but we tried that also in the paper more or less they give

167
00:18:19,680 --> 00:18:28,880
same, same performance, all of them. And does the way that you approach that, does that apply

168
00:18:28,880 --> 00:18:34,800
constraints to your inputs? There's no, there's no constraints necessary on the input.

169
00:18:35,520 --> 00:18:41,120
So the only thing you need to know is how the transformation that you're interested in,

170
00:18:41,120 --> 00:18:47,840
that's a rotation, is acts on the input space. That's what you need to do in order to build your

171
00:18:47,840 --> 00:18:53,120
equivalent networks. You need to know how the input is transformed so that the equivalence is

172
00:18:53,120 --> 00:18:56,800
built into the architecture. That's the only thing you need to know. Otherwise, you don't need to

173
00:18:56,800 --> 00:19:02,720
put any constraint on it. And so how did you go about assessing the performance of the

174
00:19:02,720 --> 00:19:08,240
architecture? So, yeah. So what we did is said, okay, let's, of course, we are interested in

175
00:19:08,240 --> 00:19:14,960
recovering signals with unknown orientation, right? So we said, let's try to pick like a powerful

176
00:19:14,960 --> 00:19:20,320
flow networks with, you know, the most powerful flow network that, the normalizing flow network

177
00:19:20,320 --> 00:19:28,240
that we can have, like a typical real MVP setup. And then let's try to iteratively find

178
00:19:29,040 --> 00:19:35,760
the orientation and kind of a latent space latent code by just the gradient descent on the

179
00:19:35,760 --> 00:19:39,840
latent. This is not, this is the so-called vanilla or like a baseline that we had.

180
00:19:41,680 --> 00:19:49,200
And then we compared that with our VAE, equivalent VAE method. We had other baselines as well,

181
00:19:49,200 --> 00:19:52,640
but I think the normalizing flow is the most interesting part because it usually gives the best

182
00:19:52,640 --> 00:19:58,480
result in terms of reconstruction. So we noticed that our equivalent VAE is much simpler,

183
00:19:58,480 --> 00:20:08,240
it's much smaller, and it already gives same or better result than flows in many cases, which

184
00:20:08,240 --> 00:20:12,080
is more complicated and then you have to do all those iterative assumptions and all that.

185
00:20:12,800 --> 00:20:18,720
And then something interesting happened as well that even if the orientation is known,

186
00:20:18,720 --> 00:20:24,400
so or you don't have any random orientation, actually equivalent VAE turned out to perform

187
00:20:24,400 --> 00:20:30,080
quite well in this task. And our conjecture is that actually the equivalence

188
00:20:31,280 --> 00:20:38,720
adds certain additional structure on the latent space that makes the life of optimization

189
00:20:38,720 --> 00:20:44,720
algorithm much easier. And therefore, you know, we can do, we can do a lot, a lot with it.

190
00:20:44,720 --> 00:20:50,080
So that was an interesting thing. So the gain was not only in terms of reconstruction quality,

191
00:20:50,640 --> 00:20:57,920
not only in terms of finding unknown orientation, but also in terms of latency, convergence,

192
00:20:57,920 --> 00:21:02,720
which are kind of important metrics for these generative priors. I really think like

193
00:21:03,280 --> 00:21:09,440
lattice in convergence or do metrics that are quite important for applicability of these

194
00:21:09,440 --> 00:21:16,960
methods in practice, especially if we could target something that requires, you know,

195
00:21:16,960 --> 00:21:22,000
implementation on edge device, low latency constraint, that's quite crucial.

196
00:21:22,560 --> 00:21:30,640
You talked about wireless and cryo electron microscopy as the use cases in the microscopy case,

197
00:21:30,640 --> 00:21:37,600
there's kind of this obvious image that you're trying to work with. And wireless are you

198
00:21:37,600 --> 00:21:41,920
applying, are we talking about applying this to images that happen to be transmitted wirelessly,

199
00:21:41,920 --> 00:21:48,640
or is there some application of the same idea to broader wireless problems?

200
00:21:48,640 --> 00:21:58,960
That's an excellent question. Actually, so let's start with the new kind of frequency band that is

201
00:21:58,960 --> 00:22:09,360
being introduced in 5G. It's kind of a millimeter wave frequency bands. So when we go higher in

202
00:22:09,360 --> 00:22:17,680
frequency for wireless communication, what happens is that the beams become very, very, basically,

203
00:22:17,680 --> 00:22:24,080
you need to use directional beams to get the performance that you want. These are some artifacts

204
00:22:24,080 --> 00:22:29,520
of like having a smaller antenna aperture and all that. So to to summarize the whole thing is that

205
00:22:29,520 --> 00:22:34,880
you you just cannot rely on kind of a rich scattering environment that you have in order to get

206
00:22:34,880 --> 00:22:40,560
signals that you want. You need to really be able to steer your beams, your antenna beams,

207
00:22:40,560 --> 00:22:46,480
in a direction that kind of gets the most energy and also direct your beams and design your beams

208
00:22:46,480 --> 00:22:53,120
accordingly. Now, if you want to do that on your device and kind of I leave it at a high level

209
00:22:53,120 --> 00:23:01,040
like that, if you want to do that on device, you're usually kind of keeping your device and you're

210
00:23:01,040 --> 00:23:06,880
talking and then you put it on the table or walk around, your device is permanently rotating.

211
00:23:08,320 --> 00:23:12,880
But the environment is the same environment. It's just the way you view it is changing. So it's

212
00:23:12,880 --> 00:23:19,040
kind of undergoes some transformation. So that is the main main idea in wireless communication.

213
00:23:19,040 --> 00:23:27,040
So that's why kind of having incorporating those geometric priors in our design, wireless design

214
00:23:27,040 --> 00:23:30,640
becomes really, really crucial, especially when we talk about these high frequencies.

215
00:23:31,440 --> 00:23:37,040
Speaking of wireless, what are some other research areas that you're pursuing in that domain?

216
00:23:37,040 --> 00:23:48,480
I think, you know, look at wireless communication. At the end of the day, we are working with

217
00:23:48,480 --> 00:23:56,080
with lots of physics there. You have Maxwell equations and at the end of the day, there's an

218
00:23:56,080 --> 00:24:02,960
electromagnetic wave moving from one point to another undergoing a lot of effects in an environment,

219
00:24:02,960 --> 00:24:09,680
reflected from different objects scattered, diffracted and all that. That's the underlying

220
00:24:09,680 --> 00:24:17,600
nature of wireless communication is kind of physics, right? So modeling these complicated

221
00:24:19,440 --> 00:24:26,240
complicated propagation effects precisely, real precisely, it's a challenging task.

222
00:24:27,200 --> 00:24:35,200
And the way people traditionally build models, at least so far, so you have two class of models

223
00:24:35,200 --> 00:24:41,120
for wireless. One is kind of a ray tracing type models, which is based on just, you know,

224
00:24:41,920 --> 00:24:47,440
kind of deterministically following paths of the ray. And you have what I can call statistical

225
00:24:47,440 --> 00:24:53,840
channel models, that kind of build an average case statistical model that you put a distribution

226
00:24:53,840 --> 00:24:59,280
on the channel gains that you have on the delays that you get and you work with those hard-coded

227
00:24:59,280 --> 00:25:06,880
assumptions. Now, with machine learning, we know that using data, we can build much better

228
00:25:07,840 --> 00:25:13,680
models if we can just use those models to build either better statistical channel models or

229
00:25:13,680 --> 00:25:18,720
better kind of a more especially consistent channel models. So one of the research areas that are,

230
00:25:18,720 --> 00:25:26,560
you know, or is quite important for us is exactly this channel modeling, which basically is about

231
00:25:26,560 --> 00:25:32,000
kind of learning physics. This is kind of a topic. It's not limited to wireless. You know that

232
00:25:32,000 --> 00:25:38,000
there are other folks working on neural networks, a physics-inspired neural network or

233
00:25:38,000 --> 00:25:42,240
neural networks that can learn physics. So that's a topic that is quite interesting. And one

234
00:25:42,240 --> 00:25:49,680
thing that like we did was, you know, let's try to gather very simple field data, not requiring

235
00:25:49,680 --> 00:25:57,360
really kind of hard really like special devices. And let's build a generative model on that that

236
00:25:57,360 --> 00:26:02,400
generates the underlying channel. So implicitly learns the underlying channel. So that's kind of one

237
00:26:02,400 --> 00:26:07,360
of the one of the works we had and we continue to work on on those topics as well.

238
00:26:08,720 --> 00:26:17,840
The other thing that is quite interesting is wireless perception. So again, our visual perception

239
00:26:17,840 --> 00:26:22,400
is it's still based on electromagnetic waves. It's just a different frequency. Of course,

240
00:26:22,400 --> 00:26:28,240
it's much higher frequency than what we're working right now in wireless. But on the other hand,

241
00:26:28,240 --> 00:26:35,760
in wireless we might have access to much more details, much more refined concepts than what we

242
00:26:35,760 --> 00:26:44,240
probably have visually. So the question of wireless perception and sensing is quite important as well.

243
00:26:44,240 --> 00:26:49,840
Like how we can build right now, let's say, machine learning models that can perceive the

244
00:26:49,840 --> 00:26:57,040
environment through the learns of wireless signal. And also that was one of the topics that we're

245
00:26:57,040 --> 00:27:03,520
working on basically trying to solve a simple slam problem by incorporating physics of

246
00:27:03,520 --> 00:27:10,480
propagation into the machine learning model and try to learn that from a wireless signal.

247
00:27:10,480 --> 00:27:16,160
Slam being the same kind of slam that comes up in robotics, simultaneous location and mapping,

248
00:27:16,160 --> 00:27:21,200
I think. Precises, yeah, some sense of localization mapping. So that's exactly the same thing.

249
00:27:21,200 --> 00:27:26,560
But this time you want to do it with wireless signal. Awesome, awesome. So kind of returning to

250
00:27:26,560 --> 00:27:34,480
ICML, what are some other papers that you and your team or other teams that Qualcomm are presenting

251
00:27:34,480 --> 00:27:40,800
there? Yeah, I mean, there were a couple of very interesting works from my colleagues at Qualcomm.

252
00:27:41,520 --> 00:27:49,120
And the first paper that I want to mention is a paper that has an oral presentation paper this

253
00:27:49,120 --> 00:27:59,280
year at Qualcomm. And it is about basically oscillation of, you know, a quantization of a training.

254
00:27:59,280 --> 00:28:05,440
So basically, and dealing with that, those problems. So to give a little bit of background on that,

255
00:28:05,440 --> 00:28:11,680
you know, again, you you probably talked with some other colleagues before that they work on

256
00:28:11,680 --> 00:28:15,360
quantization and compression of neural networks. So if you know that,

257
00:28:17,360 --> 00:28:23,200
yeah. So if you know that kind of with quantization, let's say with post training,

258
00:28:23,200 --> 00:28:28,400
a quantization techniques, you train a network, you go and quantize it, you can actually get

259
00:28:28,400 --> 00:28:33,920
very good performance decent performance with 8-bit quantization out of these networks, right?

260
00:28:34,640 --> 00:28:38,960
So this is quite quite quite interesting. However, if you want to push it down, push this

261
00:28:38,960 --> 00:28:46,000
quantization down to four bits, then just post quantization post training quantization techniques

262
00:28:46,000 --> 00:28:51,680
are not sufficient for pushing the performance. So we need to basically do what people call

263
00:28:51,680 --> 00:28:57,360
quantization of their training. This basically means that the quantization noise that kind of

264
00:28:57,360 --> 00:29:01,920
is added because of quantization operation, you include that in the training process. So what you

265
00:29:01,920 --> 00:29:08,000
are doing is that you quantize and then you backprop through the quantization operation.

266
00:29:08,480 --> 00:29:15,040
Now, back and learn to minimize those errors as it's training. Exactly. Exactly. Now,

267
00:29:15,040 --> 00:29:20,800
then you backprop through the through the quantization operation, basically the technique that's

268
00:29:20,800 --> 00:29:27,760
used is straight to estimator, one of the techniques. So the straight to estimator, what it does is that

269
00:29:27,760 --> 00:29:35,440
you basically have your shadow weights, basically weights that they live in real valued numbers.

270
00:29:36,240 --> 00:29:41,680
And they get quantized by quantization operation and gives you your quantized value.

271
00:29:42,240 --> 00:29:48,240
And now, when you backprop at inference, you work with only quantization quantized value.

272
00:29:48,240 --> 00:29:52,640
So that's the ultimate network. However, when you're training it and you backprop through that,

273
00:29:53,360 --> 00:29:59,680
you straight through estimator is roughly speaking means, okay, forget about the quantization

274
00:29:59,680 --> 00:30:05,920
operation that you have here, just directly backprop through the shadow weight. That was the real number.

275
00:30:05,920 --> 00:30:13,040
So that is the rough idea. Okay, that already helps kind of overcoming some of the some of the

276
00:30:13,040 --> 00:30:19,040
difficulties. However, it's not it is not sufficient. And one interesting phenomena that can happen

277
00:30:19,040 --> 00:30:26,080
is that then shadow weights are close to quantization threshold. So basically threshold between two

278
00:30:26,080 --> 00:30:34,720
different quantization beans, they start to actually oscillate around that. And if you look at the

279
00:30:34,720 --> 00:30:42,640
quantized value now, they oscillate between different quantization. And this oscillator behavior has

280
00:30:42,640 --> 00:30:47,600
been reported before and it's kind of a peculiar phenomena. The interesting thing about this

281
00:30:47,600 --> 00:30:52,080
phenomena is that, first of all, the learning grade, but by choosing different learning grade,

282
00:30:52,080 --> 00:30:57,360
you cannot control this oscillation. The oscillation, the amplitude might change, but the frequency

283
00:30:57,360 --> 00:31:05,520
of oscillation remains. And what my colleagues noticed that is actually this frequency is dependent

284
00:31:05,520 --> 00:31:14,880
on the gap between the best, the ground truce value of the optimal value and the quantized value.

285
00:31:14,880 --> 00:31:18,880
If the gap is larger, that oscillation is going to be bigger. If the gap is smaller,

286
00:31:18,880 --> 00:31:23,360
the solution can be smaller. So something something like that, the more details are in the paper.

287
00:31:24,800 --> 00:31:33,120
But they observed such such a behavior. And then what are some drawbacks of this oscillatory behavior?

288
00:31:33,120 --> 00:31:41,280
The first drawback is the discrepancy in bachnormous statistics, meaning that the bachnormous statistics

289
00:31:41,280 --> 00:31:49,040
that you compute during training is going to be different from what you will get at test because

290
00:31:49,040 --> 00:31:57,600
of this oscillation. So what you need to do, you need to recompute the bachnormous statistics,

291
00:31:57,600 --> 00:32:05,280
which manages to solve this discrepancy. However, the problem is not only that. Actually, it has

292
00:32:06,800 --> 00:32:11,600
negative impact on the optimization process as well. This basically means that

293
00:32:14,480 --> 00:32:18,800
if you look at the optimization process, this oscillation actually prevents network to

294
00:32:18,800 --> 00:32:24,880
converge to a better local minimum and gets better performance. So it is important to kind of

295
00:32:24,880 --> 00:32:33,200
overcome this oscillatory behavior. And on that last point, the idea is that beyond just

296
00:32:34,960 --> 00:32:38,640
resulting in a larger error, it will prevent converges altogether.

297
00:32:40,000 --> 00:32:45,120
Or converges to a good good point. So you might converge to something that is not a good place.

298
00:32:45,120 --> 00:32:54,080
So that is the main issue. So the other thing is that, okay, what are some solutions? How

299
00:32:54,080 --> 00:33:00,800
you can actually deal with that issue? So my colleagues actually proposed two methods. The first one

300
00:33:00,800 --> 00:33:07,680
is a dampening regularizer. The idea is, when you're close to quantization threshold, you're going

301
00:33:07,680 --> 00:33:13,280
to see this oscillatory behavior. So what if you put a regularizer on training that pushes the

302
00:33:13,280 --> 00:33:19,440
weights to be close to the center of beings instead of quantization thresholds? And that already

303
00:33:19,440 --> 00:33:30,560
is quite helpful. The other approach that they have is about freezing the weights. What does that

304
00:33:30,560 --> 00:33:38,160
mean? This means if you see that the weights are oscillating, then you freeze them. You just

305
00:33:38,160 --> 00:33:44,160
don't update them because you know that this is going to have a negative impact. But there are

306
00:33:44,160 --> 00:33:51,680
subtleties in the way you do that. First of all, how do you decide if you freeze a weight or not?

307
00:33:51,680 --> 00:33:58,800
You have to monitor the oscillatory behavior and oscillation frequency. And you basically have to

308
00:33:58,800 --> 00:34:03,360
compute that oscillation frequency, which they do by using some kind of exponential moving average.

309
00:34:04,160 --> 00:34:12,000
And then put a threshold on that. So this is, are you tracking this and managing this on a weight

310
00:34:12,000 --> 00:34:18,880
by weight basis? Or is it, are you looking at some notion of oscillation that's kind of vector

311
00:34:18,880 --> 00:34:27,440
based? So as far as I know, this is weight by weight basis. Again, I encourage everyone reading

312
00:34:27,440 --> 00:34:35,680
the papers in all details to make sure I'm not destroying my colleagues' work. But I think it's

313
00:34:35,680 --> 00:34:42,960
a weight by weight because you compute it in a weight by weight basis. So at the end of the day,

314
00:34:42,960 --> 00:34:50,560
what happens there is that, yeah, you monitor that after a certain threshold, you just freeze it,

315
00:34:50,560 --> 00:34:54,640
and then you freeze it to a value that kind of occurred most. So there's actually like a mechanism

316
00:34:54,640 --> 00:34:59,680
for selecting that. As a matter of fact, it manages with this technique, you can actually bring the

317
00:34:59,680 --> 00:35:08,160
quantization with down to, let's say, four bits, three bits, for, let's say, mobile net efficient

318
00:35:08,160 --> 00:35:14,400
net type architectures on image net data, which is quite remarkable result. Wow, wow.

319
00:35:15,200 --> 00:35:20,880
And is oscillation, you know, to your knowledge, is oscillation been the primary impediment to

320
00:35:20,880 --> 00:35:28,400
reducing the level that we're able to quantize to? Or are there other key challenges there?

321
00:35:28,400 --> 00:35:37,920
Yeah, I mean, again, I have to give a caveat that my colleagues are definitely more knowledgeable

322
00:35:37,920 --> 00:35:46,560
in that area. But that definitely was one of the main factors, for sure, because after fixing

323
00:35:46,560 --> 00:35:55,600
that oscillation problem, you could do a great job that you couldn't do before. So this is

324
00:35:55,600 --> 00:36:00,320
definitely one important factor. And there are another couple of papers that your colleagues

325
00:36:00,320 --> 00:36:07,360
are presenting. Exactly. We can rapidly cover some of those as well. So on, on personalization,

326
00:36:07,360 --> 00:36:14,880
there's this paper of my colleagues on variational, on the fly personalization paper. So

327
00:36:15,920 --> 00:36:22,160
when you deploy a model on edge device, sometimes you would need kind of personalization of that

328
00:36:22,160 --> 00:36:29,680
model specifically to to the user that is using it. Let's say, for example, let's talk about

329
00:36:29,680 --> 00:36:34,000
the speech verification, right? You want to be able to personalize the machine learning model

330
00:36:34,000 --> 00:36:40,480
that you have for a specific user that is using that service, right? Now, there are some challenges

331
00:36:40,480 --> 00:36:47,680
for personalization. You really don't want to do on device training because of the kind of the

332
00:36:47,680 --> 00:36:52,880
you don't know what's happening. You have to monitor everything and it's kind of challenging. You

333
00:36:52,880 --> 00:36:58,960
want to do it in an unsupervised way, right? You want to do it few shots. So you really don't want to

334
00:36:58,960 --> 00:37:06,000
get that much label from the user and you want to do it with few samples. And yeah, I mean,

335
00:37:06,000 --> 00:37:10,000
you don't want to basically send data back to a source. So you really want to be able to do it

336
00:37:10,000 --> 00:37:17,120
on device and kind of without requiring great training and all that. So then this personalization

337
00:37:17,120 --> 00:37:25,680
problem was formulated using a variational principle. The idea is that you actually train so-called

338
00:37:28,160 --> 00:37:36,960
variational hyper-personalizer, so we can call it that way. So there's this hyper-personalization

339
00:37:36,960 --> 00:37:45,520
basically network that gets those few samples and basically based on that updates

340
00:37:45,520 --> 00:37:53,280
weights of the model according to those samples. So it's just a way of adjusting the distribution of

341
00:37:53,280 --> 00:37:58,000
those weights to the model that you're getting. So of course, there are more details in the paper,

342
00:37:58,000 --> 00:38:04,080
but the core idea that we have is we have there is that, you know, this is the first time that you

343
00:38:04,080 --> 00:38:11,520
can do on the fly personalization. And yeah, this is I think it's very, very important for

344
00:38:11,520 --> 00:38:21,040
age device machine learning model deployment. Awesome. Awesome. And the last one that we wanted to

345
00:38:21,040 --> 00:38:30,000
cover was on kind of causal identifiability for or from temporal intervened sequences or citrus.

346
00:38:30,000 --> 00:38:36,880
What's that one about? Sounds juicy. Indeed, indeed. That's a that's a that's a 40 page paper. So

347
00:38:36,880 --> 00:38:42,320
also encourage everyone to to go and read all the details. It's a very exciting topic. I mean,

348
00:38:42,320 --> 00:38:49,920
we all know that causality and identifying causal factors are quite quite important. So

349
00:38:51,840 --> 00:39:00,880
so the core idea of the paper is that you so first you want you represent your causal factors

350
00:39:00,880 --> 00:39:07,200
as a multinational vector. So that's the that's the first thing. This is representation of causal factors.

351
00:39:07,200 --> 00:39:12,400
The second thing is that the data that you have and based on which you try to solve this problem,

352
00:39:12,400 --> 00:39:20,560
this causal identification is basically what it works based on access to a temporal sequence.

353
00:39:21,280 --> 00:39:26,800
In which there are some intervention is happening. So basically there are some interaction with

354
00:39:26,800 --> 00:39:34,240
with the scene. So through that you can actually figure out some causal factors out of those

355
00:39:34,240 --> 00:39:42,640
those interactions and out of the temporality that you have. So then there is a V.A. model

356
00:39:42,640 --> 00:39:51,120
is proposed where the the causal fact that the latent space of that are basically in the latent

357
00:39:51,120 --> 00:39:57,600
space of that you can disentangle causal factors represented as this multidimensional factors.

358
00:39:57,600 --> 00:40:03,360
So basically you can in the latent space of that V.A. you can actually disentangle causal factors

359
00:40:04,480 --> 00:40:11,120
using having access to those things. And the other one interesting thing about the paper is that

360
00:40:11,120 --> 00:40:17,440
this is just not like a you can use that V.A.E. but as a matter of fact if I give you an arbitrary

361
00:40:17,440 --> 00:40:25,040
autoencoder, pre-trained autoencoder without knowledge of this, then it is possible to train

362
00:40:25,040 --> 00:40:32,640
a normalizing flow that manages to disentangle causal factors from the latent space of that autoencoder,

363
00:40:33,200 --> 00:40:39,760
which is actually quite an interesting thing. So definitely encourage everyone to go and read

364
00:40:39,760 --> 00:40:47,280
the details of the of the paper. Awesome. And before we finish up we also wanted to touch

365
00:40:47,280 --> 00:40:54,800
on a workshop that you're participating in your colleagues. This one on Bayesian optimization.

366
00:40:55,600 --> 00:41:03,040
Yes exactly. So that's also an interesting interesting paper by my colleagues on micro based

367
00:41:03,040 --> 00:41:09,600
on optimization for macro placement. So what is macro placement? So if you look at kind of a

368
00:41:09,600 --> 00:41:15,840
cheap placement, cheap placement problem, you you basically have a macro unit. Still though those

369
00:41:15,840 --> 00:41:21,360
are memory blocks that you have and then you have standard cells that you can put on on the chip.

370
00:41:22,080 --> 00:41:31,840
So usually you design the you know you place all those objects in chip under some constraints.

371
00:41:31,840 --> 00:41:39,760
These constraints basically are basically you know you can say power performance area. Those are

372
00:41:39,760 --> 00:41:47,200
very important constraints, but to evaluate these for each placement that you choose to evaluate

373
00:41:47,200 --> 00:41:52,320
those that the objecting function that you have is very costly because it has to go like a lengthy

374
00:41:52,320 --> 00:41:57,600
simulation and kind of compute everything. So you really cannot integrate that in the optimization.

375
00:41:57,600 --> 00:42:03,040
So you have to use some sort of a black box optimization for that. And people kind of

376
00:42:03,040 --> 00:42:07,520
traditionally use simulated annealing type methods, but Bayesian optimization actually

377
00:42:07,520 --> 00:42:13,280
provides an elegant way and alternative way of solving this problem. It's a combinatorial

378
00:42:13,280 --> 00:42:19,040
optimization problem. So it's a very tough problem. And Bayesian optimization roughly like you build

379
00:42:19,040 --> 00:42:24,880
a surrogate model and you have access to use a Gaussian process to build a surrogate model and

380
00:42:24,880 --> 00:42:31,360
basically integrate that into use that for your optimization. So the contribution of the paper

381
00:42:31,360 --> 00:42:38,160
is basically solving this macro placement problem using Bayesian optimization. Also very,

382
00:42:38,160 --> 00:42:44,080
very interesting work and yeah of course quite important for what we took. Got it.

383
00:42:44,080 --> 00:42:50,320
And that one's at the real ML workshop. Do you know what the real ML acronym is?

384
00:42:51,840 --> 00:42:59,360
I cannot remember. But it was I think it has to something to do with active learning for real

385
00:42:59,360 --> 00:43:04,080
world systems. Real world active learning and machine learning or something like that.

386
00:43:04,080 --> 00:43:08,560
Exactly, exactly. But it cannot remember. Those are complicated names sometimes.

387
00:43:11,600 --> 00:43:19,200
Awesome. Well, Arash, in terms of your research on kind of machine learning for wireless, what are

388
00:43:19,200 --> 00:43:25,600
some of the future directions that you're excited about? Yeah, I would say I think I touched a

389
00:43:25,600 --> 00:43:32,400
little bit on this modeling part. The fact that the wireless channel modeling at the end of

390
00:43:32,400 --> 00:43:40,080
today is about learning physics. Neural networks that are integrated with this bias coming from

391
00:43:40,080 --> 00:43:45,360
physics or they can learn part of physics. That's a very, very interesting and exciting

392
00:43:45,360 --> 00:43:51,440
the research direction for me and kind of investing a lot of time on that. I hope that in

393
00:43:51,440 --> 00:43:59,040
a couple of months I can be back and talk about that. But that's a very interesting research

394
00:43:59,040 --> 00:44:03,760
topic and I think it can be quite game changer. Well, thanks so much for joining and sharing

395
00:44:03,760 --> 00:44:10,880
a bit about what you're up to and your work, your presentations at ICMA. Yeah, absolutely.

396
00:44:10,880 --> 00:44:22,080
Thanks for having me. It's a lot of fun.

