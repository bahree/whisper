Alright everyone, welcome to another episode of the Twimmel AI Podcast.
I am of course your host Sam Charrington, and today I'm joined by Mike DelBalso, co-founder
and CEO of Techton.
Before we get going, be sure to take a moment to hit that subscribe button wherever you're
listening to today's show.
Mike, welcome back to the podcast.
Thank you for having me.
It's great to be here.
I'm excited for the conversation today.
I am as well.
It's been a while since we spoke.
I think the most recent time was about a year and a half ago.
We were talking about feature stores for MLOPS.
I suspect that topic will come up again.
But let's dig in.
I'm looking forward to talking about what you've been up to, the idea of feature platforms
and of course the intersection with all that and data-centric AI.
For those who haven't heard any of our interviews or don't know your background, why don't you
share a little bit about how you came to work in ML?
I first got involved in machine learning.
I was a product manager at Google and dating myself now.
But back in 2013, I joined the ads team at Google and as you know, Google uses a lot
of machine learning to determine which ads to show people.
This is before machine learning was super cool and stuff like that.
I don't even think we use the word machine learning at the time, but we had a lot of models
in production that were doing some real time decision making around who is this person,
what kind of interests do they have, which had his most relevant to show to them.
The team was really excellent at what we did not at a term we didn't have at the time,
which was MLOPS.
There's a lot of MLOPS that we were doing at that time, production machine learning.
That was kind of like phase one of how I got involved in machine learning.
After that, I actually joined Uber in the early data ML days of Uber at the time when there
wasn't a lot of, there was very little machine learning happening in production at Uber.
It was kind of the task for me to start the ML team and help us figure out what to do
with all of this data, we had all this data at Uber.
How do we help Uber make a lot of excellent automated and smart decisions and experiences
in the product?
We built the ML platform team at Uber and we made a platform called Michelangelo, which
was really exciting and we can talk about that, of course.
In the process of doing that, we developed a lot of cool patterns, really spent a lot
of time developing a lot of MLOPS workflows and different pieces of infrastructure, one
of which is the feature store and that is a really big inspiration and a really big
kind of like theme of tech content that I'm working on today.
You know, when I think back to Michelangelo and that blog post that got so many of us
excited about what you were doing there at Uber and the state of play at the time, like
compared to now, you know, we've built out a lot of the stack for MLOPS now or at least
we've got contenders for various pieces.
But when you left Uber to start tech time, the field was pretty wide open.
Why'd you start with a feature store?
I mean, you had all the pieces, right?
That's a really good question. And I think we all have to think back to that time and think
about a very different time where, you know, if you look today at the like, what are all
the MLOPS tools in the industry, the map of the industry, there's 4,000 MLOPS companies
and like, what plugs into what and how do I choose what it, you know, it's crazy.
At that time, it felt like there was a lot of stuff, but there actually wasn't a lot
of stuff relative to today, right?
But it always feels complicated and, you know, a lot of the terms, there wasn't the same
level of specialization and stuff like that.
So back in the time, when we started Michelangelo, we started it in 2015.
So, you know, it's a very different kind of generation of ML tooling and ML techniques
and stuff like that.
Back at that time, the thinking was an end to end ML platform.
Let's just build a solid experience for, but it was all about democratizing ML.
How do we make it possible for a data scientist to do machine learning?
You know, and it was kind of like enabling them to do it for the first time.
And so we built an end-to-end platform that enabled them to, you know, choose their label,
choose their features in the Web UI and get their model built in a Web UI and, you know,
manage it in a Web UI all the way to deploy into production.
It was revolutionary at the time.
It was something that really enabled people to go from zero to one.
And it handled the full workflow.
The industry was moving from this general idea, the dream of having the one size fits
all platform that will enable all of the people within the organization really democratize
everything to the kind of point where people are disillusioned with that dream and they
really realize, hey, you know what we need is, you know, a bunch of reusable components
because there's not a one size fits all one system.
There's a bunch of reusable components that we can piece together to form the ML application
that I'm trying to build, you know, so it would have the right kind of serving system
for me and it will, and I can plug into the training system that's relevant for me.
And so we actually did that that we brought Michelangelo through that journey as well
from being a monolithic system to more of like a collection of best best practice components
that are compatible and fit together quite well.
And so in that going through that journey and building Michelangelo in the first place,
you know, we were really focused on how do we help people get, not a model built, not
like showing some results in AUC curve to their team, but how do we, you know, help the
fraud team, help the ETA team at Uber, all these different teams, help them actually get
into production and get value, you know, they were use case teams, they were trying to
get something done.
They didn't really care about like, cool, I've trained some models and they, they're
not plugged into the product.
And so we thought about it as they're building an ML application.
And so this is super related to the concept of data centric AI, you know, what's part
of an ML application?
Well, of course you have, you got to build your models and manage your models.
And then secondly, there's a variety of data pipelines that are also part of that ML application,
the things that generate the data that your model consumes, your training system consumes
to generate a model, or the data pipelines that your model in production uses to generate
inference in real time, all kinds of data pipelines happening there.
And so we realized, hey, in the Michelangelo system, we really built a lot of model management
stuff, but what we spent most of our time building is a bunch of data management stuff.
And that's actually what we called the feature store.
And so it was a lot of kind of centralized and automated data engineering that we found
that we were doing again and again across all these different use cases.
And we brought that together into this one layer in the Michelangelo system and became
our kind of its own component.
We called it the feature store.
We found that it was one of the most impactful systems to help someone go from zero to one
to get into production quickly, and to be able to reuse machine learning across these
components across different use cases to share and to help reduce the incremental cost
of creating the next machine learning model.
And so reflecting on that at the time we thought we glimpsed to the future.
We realized it's really about the data, the feature store, the concept of the feature store,
and later the feature platform is really the kind of the data layer for machine learning.
And that's what got us so excited after publishing a blog post and everybody had like all kinds
of extremely positive feedback around that.
And I'm trying to build something like that, you know, can you guys come work for us kind
of thing?
There's a lot of attention there.
And that made it clear to us that this is a design that's here to stay.
And this is a design where this is a pattern that there should be a proper enterprise solution
built around.
And so that motivated us to create tech on, tech on is an enterprise feature platform.
And you know, we help teams who are putting machine learning and production manage their
data flows for their ML application through all stages of the ML lifecycle.
You referenced the complexity of data infrastructure when you were building Michael Angelo.
Speak a little bit to how you've seen that evolve over the past five years.
Has it been relatively static?
Has it changed significantly a little above?
I think we have seen some things change quite a bit.
Some things change a lot less than I would have liked.
I think one of the biggest changes in our journey and the journey for many companies is
frankly going from having a lot of data on-prem in a Hadoop cluster, you know, five years
ago to now I'm using a cloud data platform and all my data is on a hyperscaler cloud provider.
So what does that mean for a data team and an ML team?
What does it mean for a data team?
It means you're not managing a bunch of Hadoop clusters.
You don't need a 30 person data infrastructure team to manage basic.
How do I store my data and keep it up and running?
Like for example, when I joined Uber, there was just outages like the data wasn't available.
Like the whole Uber app would go down all the time because there was just issues with
maintaining all of the data systems.
Now a lot of those basic data capabilities are largely resolved.
You know, you put your data on the cloud, you adopt a data lake house or a data warehouse,
put your data in them.
For example, snowflake or Databricks, they're great solutions and you're going to get
really good performance, great reliability and it's going to be likely at a price point
that works for your team.
It's going to be cheaper than having maintaining your own team to build all of this stuff.
And then it's interesting to reflect on what does that mean for a machine learning team?
Because machine learning teams, previously in that previous world, were building custom
connectors to the custom data systems that existed in their business is, how do I connect
to our weird file format and the lack of reliability?
Yeah, I mean, there's all kinds of, the ML teams would be constrained by the data choices
of the underlying data platform team that they dependent on.
And that would come with a lot of constraints at Uber, we were super fortunate because
we had a super legit data platform team that did a lot of stuff.
We had real time data, really high quality streaming data and an excellent kind of like
a batch data system of gigantic Hadoop cluster.
But that's not something that everyone was so fortunate to have.
And moving to the cloud, you kind of get all of this stuff now very easily accessible,
often serverless.
And it makes for, and then I think that has a really big impact on the machine learning
space or any consumer of data of underlying data systems because now they're standardization.
I'm a machine learning team in company A, I build a connector to Snowflake and some
cool thing that plugs into Snowflake and I can build models with it.
Well I can share it with company B and now they can use my machine learning tool that
was built for that underlying data system because we share the same underlying data system.
And so there's a lot of commonality at this foundation layer that allows people to
share at the ML layer, share things that they've built and have them be reusable across
teams.
They increase the pace of innovation there.
And thinking about data lake house data warehouse, those are more about static data so to speak
relative to streaming as streaming matured as much or to the same degree.
Streaming has matured a bunch, but it is, there's still a long way to go, honestly, and we
find that a lot of our customers are still struggling with it.
And frankly part of the value proposition of the feature platform that we build that
tech ton is, it makes a lot of this stuff just a lot easier.
So how streaming used to be, you needed to have a whole team spin up a Kafka deployment
for you and maintain a Kafka cluster.
Now there's really good cloud solutions, there's confluent has confluent cloud.
So it's much simpler with confluent cloud nowadays.
There's a variety of other online streaming solutions that are, that make things much simpler.
But we're finding that ML teams are still struggling to use them.
So for example, a machine learning team is predicting fraud at a big bank.
They want to find a way to say, hey, if this user has sent more than 100 transactions
in the past five minutes, let's ban, let's not allow any other transactions, because it's
likely that it's going to be fraud, right?
Well, what do they need to do to get there, and they want to do this at scale for a lot
of different users, they need to run a lot of this streaming infrastructure, and they need
to be running stream processing to aggregate over all of those transaction events, turn
them into feature aggregations, and then plug that into their model.
And then, not just set this up once, but staff up a team to ensure that these systems don't
go down, they don't go out of memory, if it goes down, that they're retried, that they're
available, and high availability, and debuggable, and someone's on call for it.
That's the productionization side of streaming.
That's still pretty tough, and we find that a lot of teams that are trying to build interactive
machine learning driven experiences, you want your recommendations on your website to
react to what the customer was just clicking on.
You want your fraud detection system to take into account like what the person's actions
were, just were on this account.
That kind of stuff productionizing it relies very frequently on streaming, it's pretty
hard to do today, and it's one of the things that actually we've built a lot of IP and
making a lot simpler for ML teams.
All these real-time ML capabilities are an area where there's a lot of innovation happening
right now.
And so I think it is clear, based on that, that what you're seeing with regards to feature
platforms isn't about replacing all of this infrastructure that we've only recently
standardized, but rather it hooks into them.
Well, let's just talk about what a feature platform is, and then we'll talk about how
it can connect to your systems, and what does it do, is it replace your existing systems
or not?
When we work with teams who are trying to put machine learning into production, what
they have to do is not just build a model and put a model into production, but they need
to stand up, they need to develop, productionize, and operate data flows, feature pipelines
that are constantly computing the up-to-date and very fresh feature signals that are going
to be available, are going to be used for real-time inference.
So we just gave the example of real-time fraud where I'm doing some aggregations over
some recent web events, let's say.
There may be a search use case, and so a very different type of data path, the data might
come from a search box that someone typed a query into.
That's more like a real-time signal that comes in from the end user application.
And some signals, they're just pre-computed.
Is this person in a banned country?
Well, we can pre-comput that, and we can just make that ready to serve right when we
want to make that prediction.
There's a lot of data engineering that goes into each type of these features.
People trip up with a lot of different types of data challenges that comes up with them.
For example, consistency between that data at inference time compared to consistency
with that data at training times, you want that data to be the same.
When you generate a training data set, you need to go back in history for all of that
data.
You need to have all of that data log so you can generate historical training examples.
You need to monitor this data to make sure it is serving at the right, operationally
serving at the right speed, it's staying real-time, it's fresh, it's available.
But also, you want to make sure that it's accurate, the data quality is high.
And then there's a variety of data infrastructure that you end up having to spin up and support
in production, for example, systems to serve this data at scale, systems to operate, to
do the stream processing to generate your features.
We think about it as transforming your raw data into features, storing that data both
for training and for inference, serving that data for inference in real-time, monitoring
the data, and building an excellent developer workflow for MLOps and for the engineers
to fit it into their DevOps processes.
These are all within the scope of the feature platform.
And the reason why there's so many things here is because there's a lot of challenges
that you need to solve before you can actually credibly claim that your model is running
in production in a reliable enough way that you're comfortable depending your revenue
on it, you want to actually depend your product on it, type of thing.
And so we've built a platform to make all of those data challenges much simpler.
And so how do we do this, though?
Well, we are not delivering a completely new data and ML stack to the ML user, to our customer.
We are plugging into the data sets, the data infrastructure that they have.
So people come to us and they say, hey, I have Snowflake and I have Databricks and I have
a Redis cluster.
And we'll say, that's great.
We will actually orchestrate those systems, so we'll orchestrate transformations on both
of those.
We'll plug into those, take data out of them, we'll load it up into Redis, we'll maintain
an online serving layer for your model.
So the interfaces here are, we plug into raw data and we serve that data to the model
that's in production or the training system that's building the model.
But under the hood, we're not implementing all of those different data processes from
scratch and running all of that infrastructure in our domain.
We're actually orchestrating the best in class cloud data infrastructure that your team
is already running within your own stack.
So we're a lot more of an orchestration layer for like a data orchestration layer for
machine learning than anything else.
To relate to us a bit back to this idea of data-centric AI, you mentioned before we started
recording the interview as we were chatting this idea about a ML flywheel that kind of reiterated
the importance of data for ML.
Can you talk a little bit about that idea?
For sure.
So one thing that I think is really nice about data-centric ML is it's a theme that comes
along with it is declarative interfaces and maybe some of the other folks that you've
been speaking to in recent episodes have been talking about this as well.
But this is one of the key things that we have really focused on.
We don't have our users say, hey, plug this infrastructure into this infrastructure, run
this operation, retry it if it doesn't work, and then plug it in here and then run it
at this frequency.
We just have the person say, this is my feature, this is the feature transformation, you take
care of everything.
To be in production and the tech-ton system handles a lot of things behind the scenes to make
that happen.
Earlier on in the conversation, I mentioned like at Michelangelo, one of the things that
made us really successful was we were focused on the end-to-end machine learning application.
It wasn't, we weren't just focused on helping someone train a model or manage models.
It was, hey, what's the whole series of workflows that you have to go through to actually
have an application that's both reliable and accurate and making repeated predictions.
It's like a live production application that you're operating.
There's a couple of steps to that that we see that the best teams are doing when they're
building their production ML applications.
We can walk through these steps, right?
First, you got to build the training data set.
What is that training data set come from?
It comes from your company's underlying data sets that are model data sets that are
shared across the business, wherever your company's data is.
That data is typically a bunch of logs of user events that come from your product.
There's just this data loop that we found our customer is building, right?
You make some prediction and so maybe you're predicting like, is someone going to click
on this product or not?
Then based on if they click on that product or not, you log that data.
That's going to become a future ground truth, a future label.
You may join that data together with other labels that you've logged.
You have a, like, now they're joined label data set and maybe you log some features.
You have a feature data set as well, feature logs.
Then you may assemble these data sets into a training data set and so now you want to
take all that data and you want to build a model.
Now you build a model and then you want to use that model.
There's a variety of other data sets that you're building on that time and real-time inference
type of workflows where you're maybe generating some candidates and then generating a feature,
some features for each of those candidates and then you want to score those candidates
and you ultimately are making a prediction and you're delivering that back to the
product, to the user, to the customer.
This is kind of data loop of, like, collect some data, organize it, learn from it and decide
and then based on, you know, make a prediction and then based on that, collecting again,
organizing, you know, and we want to, we want, this data goes around in this loop and
what we've seen is that the best ML teams, the teams across the industry that are the
most successful at machine learning, are really, really good at managing that loop and
building that loop.
They're really, really good at getting that flywheel to really be running.
The more, every iteration of that flywheel, you're collecting more data, you're training
your model, your model's getting better, you're making better predictions.
And so there's a variety of things that help people become, there's a variety of, like,
good things about that come out of being excellent at that flywheel.
You have really clear ownership of who, you know, runs each part of that flywheel and
it's easier to make changes, everything's more debuggable and more easily monitored and
just small changes feel like small changes and ML just feels natural and easy.
If you're ML flywheel, you don't really have it working well, maybe you don't know,
hey, who's actually the person that logs data from the application into the data warehouse?
Because now I want to add a new feature to my model, but I've got to go find that person
so they can log some data for me.
In that world, every small change you want to make, it ends up becoming like a big thing.
Like, you don't know who runs this system and then it's broken, you've got to go debug
it with and you've got to go find the person kind of thing.
And so ML becomes really hard, you know, models don't update as often, you tend to see
those teams, a symptom of those teams as they're still stuck in version one as the model
that's in production.
And so the machine learning, the teams that are really good at ML, they've really focused
on building and managing this ML flywheel and it's my claim that great ML applications
require a great ML flywheel.
And so I was talking about the declarative definitions there, that fits into this whole
thing because it's really tough to, there's a lot of parts, a lot of different technology
all throughout the ML flywheel.
And so I think it's going to be really important, you know, we're talking about data-centric
AI now, the declarative definitions, patterns that are central to data-centric AI, that's
going to be applied.
I think, I think applying those to the whole ML flywheel is going to be a big unlock.
It's going to make things a lot easier for teams to really build and maintain their whole
ML flywheel because it manages and it hides away a lot of the complexity of all of the
different technologies that span from the analytic world to the production world, from the
forward pass of the data, you know, learning and making predictions to the bringing the
data back into the analytic side of logging and organizing the data.
It's a lot of stuff and teams need some much simpler interfaces to define these things.
And I think these declarative interfaces are really the key to unlocking the ability to
manage the ML flywheel for the average team.
And a lot of ways the promise of ML ops was giving teams a platform technology for managing
this flywheel or, you know, even more strongly creating a flywheel where before there was
kind of bespoke one-off transitions and handoffs from one team to the next.
And part of that idea was like applying some of popular ideas like DevOps and continuous
delivery from software engineering to ML, how have you seen that evolve?
Do you think that that has played out the way, you know, folks have wanted to or do you
where do you still see gaps?
So one kind of symptom that things are not as they should be is that there's, we talked
about the ML ops landscape, there's like 4,000 different things there.
And it's just in here, even if you know the space really well, like you do, it's still
like, whoa, there's a lot of things here.
And an average ML team, firstly, realistically, the average ML team is not expert at all
of those different things.
But secondly, they have to piece together a lot of these different items and kind of ductate
them all together into like a coherent application that runs smoothly.
And I think there's been a variety of ML ops efforts to make that process smoother.
What we've seen is people have gotten it wrong in two different ways.
One way is their scope was too small.
So there's a variety of systems that focus only on a subset of that whole flywheel, right?
So they may just focus on, let me make the process of training a model really good.
Like I'm going to do some, you know, experiment management just in the learning phase, for example.
Or let me just, you know, this is, I'm a tool to help, you know, do some prediction,
inference stuff better, but it doesn't really address the whole flywheel part of it.
And then the other way in which a variety of tools just have not been successful is I think
they just bit off too much more than they can chew.
And so they got a little bit too ambitious and they said, well, you know, we're the system
that will do everything for you.
And so they really set their scope on the whole ML flywheel, even if they weren't, you
know, actively, consciously talking about like that bottom half of that flywheel that
brings the data back into your, from production back into your training data sets.
But, you know, it's just the dream of like, we handle everything, just just use our tool
and everything will be so much easier for you.
Yeah.
I love that you bring up both those points because I've written about this previously in
the, okay, definitive guide to ML platforms ebook and called it this wide versus deep paradox.
Like, you know, you have just as you said, a bunch of tools that are trying to, you know,
solve the end and platform, but don't have sufficient depth in any particular area.
And then you have others that are, you know, they only do one thing, but then, you know,
they don't necessarily integrate the pieces together.
And it's a tough spot.
And I think that's a part of that just state of play is, you know, what's led to the rise
of platform teams that are, you know, forming to kind of have been forming to pull all the
pieces together and try to ensure an end-to-end experience.
If not based on end-to-end tools, that's a really good point that that does speak to the
need for platform teams and that helps and platform teams justify their existence with
that to finish up on like in which ways the very broad folks get it wrong.
I think we're still evolving as an industry.
The best patterns are still emerging here, but I think what we're seeing is those folks
that promise everything, you know, it's just a really hard problem.
And there's a lot of things to get right and, you know, realistically, if you're depending,
if your product is, we're going to get every single thing, right?
That's unrealistic and customers, you know, average ML teams, they don't want to get stuck
building the, they don't want to get stuck with an inflexible ML platform that only lets
them do certain things.
So, you know, what we're, our approach with the ML flywheel is not to say we do everything.
We, our scope is the whole ML flywheel, but we're really focused on managing the data
sets and enabling the data flows through that ML flywheel.
So we're focused on helping you create your feature logs, create your training data sets,
create your, you know, calculate your features in real time, have a unified data model across
this whole ML flywheel and generate compatible schemas and all of the different pieces of infrastructure
and manage as much of the data engineering through that as possible.
So we're taking a really big chunk of all of the work that needs to get done, but there's
a lot of stuff that we're not doing.
We don't touch models, we don't build models or not the model management system.
And so we think that that's a much more manageable domain that's very valuable that we can
really focus on the workflows, we can really focus on improving those workflows and making
an amazing user experience there.
So that's our approach and how we really see us doing something different than the whole
end-to-end ML platforms that, you know, take on too much and the very specialized tools
that only solve one part of the workflow.
And so you brought us back to this flywheel part of the discussion, but I know you've
got an interesting take on platforms teams as well.
Yeah, I mean, what are you saying there?
Well, on the platform teams, we were just talking about like how a lot of these challenges
allow ML platform teams to, you know, justify their existence.
And so if you can kind of think of both of those domains, right?
If they're either piecing together a bunch of small things together and they're really
running a bunch of glue code and building kind of like a brittle infrastructure within
the company, or they may not want to take on that type of challenge.
And then the platform team is all about evaluating different end-to-end platforms and just choosing
the right one and operating that within the business.
And there's different failure modes for both of those technically.
So there's, you know, it's really hard to maintain your glue code and your duct tape
and, you know, keep everything running in a reliable way on the first one.
On the second one, you're fundamentally signing up.
You're putting all your eggs in one basket and you're investing in a system that inherently
is going to be, have the flexibility of just a single system.
And as soon as your business needs, I'll grow that then it's going to be on you to solve
that.
And that's even a much harder position to be in because then you have to expand from
a single platform that you have within your business to building a whole stack from scratch.
So that's a little bit on the technical side, but I have to say that I've seen, I haven't
seen ML platform teams be very successful in industry generally.
And this is a little bit of, you know, it's not something that like ML platform teams
necessarily want to hear, but, you know, we work with a lot of ML platform teams and a
lot of ML use case teams.
And so the use case teams are the folks who are building the recommender system to power,
you know, the website or they're building the fraud detection system to block certain transactions,
stuff like that.
They have a very specific business impact that they're measuring their success on and they're
under the gut.
They're trying to get this stuff going and launched as soon as possible and they're working
with business people and it's a team of data scientists, engineers, I'll mix together
and the product engineers as well.
The platform teams are really focused on, hey, let me make some, let me kind of like
centralize a lot of our engineering investments that the use case teams are making and bring
a lot of that engineering into one place and build some reusable components to better enable
those use case teams.
And so that's great and that's actually is a really good model.
The challenge that we have and that we see a lot of like mistakes that we see a lot
of companies making is it's not uncommon to see a platform team exist before the use case
teams exist and that's a problem because what is the platform team trying to do if there's
no use cases, you got to start with the use case, you have to have concrete business outcomes
that you're trying to drive with machine learning and then only then once you understand
those requirements, does it make sense to build a platform team and when you have multiple
of those use case teams so you can support them.
One thing that we, you know, a really good sign that a platform team is struggling and
is not set up for success is when they don't have requirements.
When you talk to them and you say, so what exactly are you trying to build, like what kind
of, what kind of scale do you need, what kind of latencies do you need, like what do you
need and they don't know.
And when they don't know, it's because they don't have use cases yet and then they also
don't have a clear decision making process as well because when there's not concrete
business use case tied to it, it's kind of like building for the future, it's all speculative,
who should make this decision and we see those teams struggle and they spend their wheels
a lot because they don't really have like a concrete direction that they're going towards
and so I would recommend that any ML, if you're on an ML platform team, think really carefully
about who your internal customers are, what those use cases use case teams are because
the ML platform is a product just like any other company is building a product and the
most, like the best advice for anybody building a product is to focus on your customers.
And if you don't know who your customers are as an ML platform team, it's really hard
to find success.
Now, that's awesome. It prompts me to plug our TwilmoCon event because we've spent a
lot of time trying to help platform teams understand what the best teams in the industry
are doing and you know, we've got this event coming up in the fall, but also we just revamped
our website and all of the content from the past due conferences is up there and I'm
thinking of a couple in particular that have talked a lot about this kind of thing.
We've been doing these team tear downs when we talk about how the platform teams engage
with the user teams and why those interaction modes are so important as well as thinking
about an interview that I did with Fran Bell who ran a platform team at Uber that was
focused on forecasting and kind of higher level platform functionality and really interesting.
Their approach was to, you know, these teams were off doing what they were doing, these
use case teams that you were describing and they would kind of talk to all these teams
and identify the things that the wheel that everyone was reinventing and kind of build
the platform around that kind of functionality.
So lots of great insights there from folks that have been doing it.
And you mentioned Fran Bell. I worked with Fran at Uber and that platform was very successful
that she built and the approach that they had was not, hey, you know what would be cool?
Let's build a time series for passing platform. It was definitely not that.
It was, it was, we have a bunch of time series forecasting problems that were solving and
they went and built them and then they had the three, four and then it became 10 different
teams that were doing the same thing. They were like, you know, why don't we just build,
you know, a thin layer that these different teams can reuse and they kind of just added
more automation into this layer and centralize more, more into it over time.
And as they were able to bring in the different use cases, that requirements from the different
use cases, they were, they had a purview that individual use cases didn't have and they
were able to think more creatively and more innovatively about how to solve this stuff.
And that's a, a layer where they made some really amazing inventions. And, and that works
both at the, at the kind of like platform team looking at the use case team direction.
And it also works at the platform team looking upstream at other infrastructure dimension
because they were able to build on top of the Michelangelo system. And so we actually
integrated those systems over time and so all the time series, the time series stuff that
the time series platform was building was actually, you know, compiled down to Michelangelo
jobs and Michelangelo model training and model serving types of things. And, and that
was like an excellent example of like a great value added by a platform team, but none
of that would have been possible if the, if friends original attitude was not to just
solve the problem. Like just get, you know, get your first time series thing solved and
then you can figure out how to do some cool platform thing later on. Yeah. Yeah. And
I'll link to that particular interview in the show notes for folks that want to, for
folks who this problem of how to build a successful platform team resonates. He said something
else in our kind of pre live chat that I thought was interesting around kind of what really
matters when trying to, you know, when trying to do machine learning, you know, what matters
ultimately is creating business value. And so it's either helping your company have more
revenue, reducing the costs adding or reducing risks. And, and, and so, you know, we talked
about ways in which platform teams things go wrong for them. But there's also individual
ML products projects rather that, that trip up in a variety of different ways. And I think
the biggest, the biggest theme that I see is focus focusing on problems that are just
not that important to the business. And I saw this a bunch of times at Uber, a data science
team, you know, in some corner, there's three people spending a lot of time solving this
problem that's, you know, it's an interesting machine learning problem. It's really cool
technology. It's fun to work on. But it takes them, of course, it's just like any project.
It takes some engineering investment. It takes any type of different like cross team collaboration
to actually get this thing across the finish line and into production when you want to
deploy it into production. And, and, you know, that requires someone up to change some
executive to say, yeah, we should spend engineers on this. This is important enough that we
should, you know, staff the team against this. And often that doesn't happen. And why?
Because the problem is just, you know, it's a small thing. Who cares? We didn't really
need to, it wasn't that important for us that, that random problem that you chose. And
so I think this is like a really important thing for like ML practitioners to think about.
And I've seen it again and again have big impact on people's careers, the ability to choose
the important problems. You know, you have a company, there's so many different cool
things you can apply machine learning to. But how do you choose the ones that are, that
are most important to the business that are going to have the biggest impact at the end
of the day? I think that really requires understanding what is the, what matters to the business?
What are the company's goals? What metrics are they trying to move? And then which levers,
levers do I have at my, from my position to help me have some impact there? And not think
about it like as a technology first, what's a cool problem? I heard about this cool technique.
Let me apply it to this problem type of thing. That's, that's where a lot of teams on the
use case side of things, not the platform teams get things wrong.
As you're, as you're talking to, you know, use case teams and platform teams, you've identified
a bunch of the pitfalls that you see them falling into, you know, how do you, you know,
characterize the, the different teams that you speak in in terms of their level of maturity
and how effective they are at side stepping these, these traps.
I think a lot of, there are a variety of different ways to kind of plot out a like steps of,
of maturity. And one of the things that's that we think about a lot is as a, as a, a
sign of being at a minimal level of maturity for us to really care about working with those
people or, or to feel like they're mature enough that we should work with them is do they
have one model in production? That's a really good sign of the health of your ML team.
Because if you can't get one model in production, what, what is it? It may not be, the problem
is it may not be a machine learning thing. It may be you, I mean, you may not have executive
buy-in. You may be working on the wrong problem. Maybe nobody in your company cares about
that problem. And so no one's helping you get to production. Maybe you don't have the
right underlying data infrastructure. Maybe the engineering team just sucks. And so you're
just not, you know, there's all these possible things. And so you could be super skilled
at machine learning. But then there's all of these other things that are holding your
team back. And so we de-risk that by only working with people who have one model in production.
And that's a sense of mature, that gives us a sense of not your ML maturity, actually,
but all of the rest of the maturity. Do you have your data in the right place and stuff
like that? You were at the right competence to get past the finish line once at least.
And then what we can help you with is help it help you make that process a positive ROI.
Make that process, you know, valuable for you, make it repeatable, make it reliable, all
of that kind of stuff. But there's some kind of like core foundational things that teams
need to get right first. But I think in terms of different types of ML use cases, some teams
are focused on operational machine learning and the care about real-time ML. You can imagine
a different maturity curve and kind of milestones for a team to get there than a similar maturity
curve that would apply to a team that is doing offline, like document analysis or something
like that.
So I don't think there's a general maturity curve that applies to every team. But it's
more like on a per-use case basis of like, what is your role in the interaction there?
And then how can you find the milestones that matter to you? And then that example of
getting the past to finish line once is one important one. And it strikes me that one
model in production, yeah, that seems like a really low bar for 2022.
Yeah, and you'd be surprised how many teams are still working on that. And it's not that
wow, like machine learning is really hard there. But the types of problems are, hey, our
data is still on prem. And our data science team started on AWS. And so they can kind
of like pull in data into the cloud. But like, is the data productionized into the cloud?
Are we going to run the ML application in the cloud? Are we going to bring the ML application
back to our data center? We're still figuring out these details and there's a lot of like
political things. And then those kind of things just kind of keep going. And that's not
where if you're trying to help someone out with machine learning, you want to, you can't
really help with the internal political stuff. You want to help them, you know, do their
workflows and their processes properly?
Yeah, I was going to ask if part of it is a semantic thing where, you know, folks will
say my model is in production, but it's in production in the sense that it's making
predictions that end up in a exosperate sheet that someone's manually analyzing versus
kind of closing the loop so that a system is reacting to the decisions made by the model.
That's a really good distinction. And so people say we have a bunch of models in production
and we talk about operational machine learning. So I kind of break the machine learning into
two buckets. Analytical machine learning, which is where it can be offline. It's mainly
its main purpose is to influence a human driven decision. So let me, let me, let me score
some leads and then I'll look at the scores and then maybe I'll send some emails or something
like that based on them. And we'll do that on a weekly basis. That's an analytic machine
learning type of use case. Operational machine learning is online. It needs to be monitored.
It affects the customers directly drives automated decisions and it's often real time. And
those are the types of use cases where ML is powering your product. Those are the types
of use cases where being in production means something very different. Yeah. Yeah. Mike,
as always, this has been a wonderful chat. You know, maybe a place to wrap up is for,
you know, folks that are interested in what you've been talking at your approach to data
centric AI kind of where this should they be looking. Yeah. Thanks, Sam. Two places.
So and maybe one update since we last spoke. TechCon is now the main maintainer of the
popular open source feature store called Feast. So that's a solution that anybody can
try very lightweight. It's the kind of build your own feature store framework. And it's
the most certainly the most popular feature store. You can find that at feast.dev. And
if if there's any listeners from companies who are really trying to figure out like, hey,
how do we build real production applications? ML applications and we need some help on
the data side of things to help this stuff get into production either in real time, super
reliably. That's what the TechCon feature platform is for. And so folks can check us
out. So that TechCon dot AI. Awesome. Awesome. Well, thanks so much, Mike. Thank you.
