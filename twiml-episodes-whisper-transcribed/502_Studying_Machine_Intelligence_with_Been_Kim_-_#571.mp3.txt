All right, everyone. Welcome to another episode of the Twomo AI podcast. I'm your host, Sam
Charrington, and today I'm joined by Bean Kim, a staff research scientist at Google Brain,
and an ICLR 2022 inviting speaker. Before we get into today's conversation, though, I encourage
you to head over to Apple Podcasts or your listening platform of choice and leave us a five-star
rating and review if you enjoy the show. Bean, welcome to the podcast. Thank you, Sam. Thanks for
having me. It's honor to be here. I'm really looking forward to our conversation, and I would love to
have you start by introducing yourself to our audience, sharing a bit about your background,
and letting us know how you came into the field of machine learning. Yeah, sure. I'm a staff research
scientist at Brain. I grew up in South Korea. I actually did, I majored in mechanical engineering
back in the day, until I came to MIT for grad school. I made some robots initially, then until I
realized robots are so cool, however, limited by physical, like just the time that you have to
spend to make the robot. And I thought, you know what, if I just remove the physical aspect of
their research and just go with the brain, I can go faster. I can do many things at the same time,
computers can do things while I'm sleeping. So I switched to machine learning and did a PhD
from MIT, then I went through some other jobs and landed in brain. And so tell us a little bit
about your current research focus. What are the things that you enjoy thinking about? Yeah, so for
many years, I've been singularly focused on interpretability, which is kind of an umbrella term
for things starting from how do you understand the data distribution that you're about to train
your model on to building inherently interpretable models. You design model from scratch, embed rules
or logic to make sure that you can understand when the model is trained. And although it to
postdoc interpretability methods, like you have a led your model, somebody gave it to you and you
can't really change the model, then what do you do? You can use gradients and other things to extract
some explanations. So that's what I've been really poorly focused on for like last 10 plus years.
And recently, I've made a little bit of transition thinking about, okay, we've come this far
developing lots of engineering tools to provide explanations. What are we missing? What do we have
to do? What is the gap to fill in order for us to move to the next level? And that's what I talk
about in I clear keynote this year about, well, what we really need to do is to paralyze what we do
with engineering effort and science. So engineering effort is making tools, perhaps in absence of
principles, you just kind of try things out and see what works out, build a lot of tests around
that. Whereas science is approaching this complex mechanism called neural network or machine learning
models or AI as an organism that we want to study. So go ahead and probe them, analyze their behaviors
to see what comes out. Like pretend that this is something that we build the hypothesis and build
tests and data analysis structure around it in order to gain more understanding about this unknown
object. So I talk a lot about that in my keynote. Your keynote, which is titled Beyond Interpretability
as you're describing, developing a language to shape our relationships with AI. Before we do go
beyond interpretability, though, I wonder if it makes sense to start by having you characterize
where we are as an industry and community with interpretability. What's the best way to characterize
the current state of the art or the state of the field? There's a lot of needs and passion
and requirements. We're not fully there yet, right? We're definitely not fully there yet. In fact,
we're quite far away from it. Some of my work, I kind of took this course, if you call it a journey
together with the community. I feel like I exactly was at the position where this is so exciting,
we can do so much things and did a lot of things. And then I was at one point in 2018 in Eurip's
paper. We shared a little shocking discovery that showed that a lot of interpretability methods
that we've been using and deployed are not actually showing what you think it's showing.
To TLDR of that paper is that when you show explanation from a trained model and you randomize
that model, so go in there, just mess it up. And so now the model is garbage and show explanations
from that model. You can't tell them apart. As a human, you can't tell them apart. As a computer,
oftentimes you also can't tell them apart. That was a big paper. Right. And that was, you know,
it received both very enthusiastic and people, some people who didn't like to, didn't like what
they're reading, right? Because we had this trust and passion around these methods. And including
myself, I test the own method that I developed and to my surprise and disappointment that the
methods that I worked on also don't really pass that sanity check test to what we call.
And since then, I think we've been, as a community, many smart people, people smarter me,
joined to think really carefully about what does it mean to make progress? It's not just about
making a reasonable and convincing picture of a bird and show an explanation of why that was a bird.
Much more beyond that, we have to rigorously validate what we're seeing. We have to conduct human
experiments to make sure that what we hope to happen actually happens with the human. So if,
for example, this might be a good segue to also talk about one of my people or this conference
that I clear where we argue that if your goal is to detect spurious correlation, then existing
methods won't work for you unless you know what that spurious correlations are or let it.
So just the community has been thinking a lot about, that's just a example of many other work
that start to think about what is a test that we can use this explanation for and what are the
tasks we simply cannot and should not. In terms of, do you have a sense for
in practice and in industry practice, kind of what are the techniques that folks are
using and the kind of state of the field from that perspective and I ask that mostly because
you know, some of the first methods that I remember hearing about are like lime and
chapp and those are still the ones that I hear about. You know, is that I'm not hearing and there's
this rich field of folks of things that folks are doing or are those still the ones that are
most used in practice? Do you have a sense for that? Yeah, I've seen many methods used across
Google and other industry just, you know, people picked their favorites and I should mention that
you know, the conclusion of our study that I just mentioned in 2018, I need to check paper,
isn't to conclude, okay, all these methods don't use it, it's not that. The conclusion is that
we don't yet understand what these explanations are showing, what information these explanations
have and so if your goal is to, for example, just look at candidates of features that might have
been used in model prediction, then perhaps these methods do work for you. In fact, we have
plenty of other papers that showed they present these explanations in from doctors, like actual
doctors and show that yes, they do help. So it's not to say, you know, all these family of methods,
we should throw them away, it's not, it's just to say we should be very careful, we should build a
lot of tests around it to make sure that it fits your bill, what you're trying to do. So to your
question, I think they're, and exactly to follow to what I just said, it really depends on what
you're trying to do. So sometimes Lyme is a very simple method, elegant, elegant simple,
but that simplicity also implies that if your decision boundary is very curvy, for example,
very complex, fitting a linear function in any of the local area is not going to be a good
approximation of how your model works. So in that cases, it will not, it will not be the best
friend for you. Other cases, you know, there are computationally really expensive methods,
and if you are a task has to be something that decision has to make very quickly,
that's not going to work for you, because it's just simply so slow, it doesn't work for what
you're trying to do. So I've seen many family of methods like those, you know, Greg came,
in fact, SmoothGrad also used in many domains, like TKW used many. There isn't, I haven't seen a
single method that works for everybody, and I suspect there will be, there will never be
a single method that's going to work for everybody. Awesome, awesome. And so your talk
beyond interpretability, you kind of organize that around these core ideas of studying machines
and isolation, and you spoke a little bit to that, studying machines in conjunction with humans,
and then this kind of broader idea of alignment. You know, let's maybe go through those when you
talk about studying machines with in isolation, you're talking about applying the scientific method
to studying the machine learning methods we're building. You know, speak more about some of the
methods that you've explored and the way you think about that category. Yeah, this is the
category that I'm super excited about, because there's so much work we can do in this, and just
so little has been explored. And the things that I talk about in this talk is just one of many,
many, many centuries old studies after studies about humans. This is just one tiny bit of it,
not even point zero zero one percent. And that should just kind of give, hopefully give you a
scope of what I'm talking about when I say studying machines as a scientific object and leverage
studies done on humans. We've done many, many studies on humans, biases, perceptual biases,
cognitive biases, or just capability, what we can see and what we can cannot see.
So there's a Gestalt study that was a historical Gestalt study from psychology.
Was it psychology and you applied similar ideas to studying machine learning models?
Yeah, that's right. So I can tell you a little more about what I exactly did. So Gestalt
phenomenon for those who are familiar with it is something that is very strong phenomenon.
That for example, I show you this triangle. Actually, just think about Amazon logo, right?
The Amazon logo has this arrow hidden inside of it. It's called A to Z and it's point at it.
FedEx logo actually has also this arrow embedded inside. And those design decisions are
essentially based on a group of strong perceptual phenomenon called the Gestalt phenomenon.
And designers to this day leverage those very strong phenomenon to make visually appealing
things. The one specific rule of Gestalt, Gestalt rule, if you make your Gestalt principles
or law, that we study is a closure effect, which is if I show you a pack, my three pack
pens that align nicely, humanized can now help yourself, help yourself, but seeing a triangle,
even though there's no triangle. And we are, our core question of that paper was,
well, does neural network have the same thing? Because if it does, that says something about how,
how our brain works versus how neural network brain works. And you can imagine that's just
the beginning of so many other studies that we've done on humans. If I give you a high number,
for example, a really famous example, Danny Cannonman spoke, talks about, if I give you a high
number of any kind, like I just say, how many things, how many trees do you think we have in Seattle,
and you give me a number? Then I ask you completely on related question, let's say, how many people
cross borders between United States and Canada or something? My answer is biased higher.
Exactly. Yeah. And it's such a robust statistical phenomenon that it's so embedded in us.
So then the question is, what are those embedded inherent biases for phenomenon that neural network
has? We have no idea. We just have no idea. And there's so much studies we can do to just literally
leverage what we did in humans and just test to ask machines, craft a design experiment really
well and do that experiment to see what happens in machines. Now, when you're talking about the
Gestalt phenomenon in particular, and this example of the Pac-Man and the triangle, are you
trying to conduct like literally that specific experiment, meaning a visual experiment and things
like Pac-Man and triangles? Because it raises for me the question, what does it even mean for the
machine to be doing what the human is doing? I thought you were going to ask a slightly different
question also to both our interesting questions. So let me ask you a question first.
What does it even mean the machine is to be doing what humans are doing?
I just elaborate, you know, I'm thinking of, you know, as we first started to build deep networks,
you know, some of the early studies like deep dream and others like looked at the early layers
of the network and found, you know, textures here and shapes here. Is it like trying to
introspect networks and see if we've got to triangle somewhere or or something else?
No, it's a lot more global understanding of of the object. So let's say let's we didn't do this
in the paper, but let's say we did the following and I can tell you what that might imply. So
let's say I showed in the paper, yeah, the network has guessed all a fact and which means that we
can we don't even have to occlude things in an image in order for it to learn how to fill in the
gap because you know what, if I put a triangle and put something on top of it and now network can
only see parts of the triangle, we already know that network will fill in that gap and what that
means that might have an implication for the way that we augment the data, the data augmentation,
we intentionally rotate and shift or blow up the image or put occlusion on top of it to make
sure that network can be robust to these changes. Perhaps you don't have to do that if you know
enough about what the network is already capable of, maybe that could inform data augmentation.
And that, that's just a small example of many, many things that will guide our developing,
our decisions when they we develop these neural networks. What are the gaps that we should really
teach them to? Because they don't have it a LED in inherently. And what are the things we should
rather, we don't have to worry about because it already has some architecture, something about the
way we built it, already builds in those capabilities. And so what are some examples of experiments that
you ran to try to demonstrate this? And maybe before that, what was the question that you thought I
was going to ask? Yes, so I thought you were going to ask, you know, in traditional Gestalt
phenomenon, the way that they test humans is response time. So they put some distracting objects
in the screen and they put a Gestalt triangle and say where's the Gestalt triangle or where's
this anger thing that how fast they can find it. And so I thought you're going to ask, how did
you test that in neural network? Because that's, I think, the core challenge of doing what I'm,
what I'm incurred, what I am excited about doing to in order to bring test experiments that's
done on humans to be done for neural networks. You would have to think very carefully about,
okay, we can't really measure response time because there's no such thing in neural network.
How would you design an experiment such that you can still measure the effect robustly,
but convincingly? So things that we did, or we did a lot of testing into, okay, the way that we
measure is the simple metric of closure measure, what we call closure measure. Now, what could go wrong?
Would it go wrong? If I have, these are some, some of the details, but there are some confounding
factors that we should really rule out and test that metric with those potential confoundings to
make sure that we're not fooling ourselves, that this metric actually fakes fully measure what we
want to measure. Can you explain the metric before talking through some of the confounding factors?
Yeah, it's actually pretty simple. So metric itself is a cosine similarity between two set
two pairs. So first pair is a gestal triangle, which invokes that closure phenomenon to full
triangle. So this measures how similar is it to the neural networks embedding space, how similar.
And we subtract that cosine similarity with a, what we call disorder triangle. So the same
Pac-Man, but Pac-Man are now slightly rotated so that you no longer see the triangle.
And similarly between that disorder triangle to the full triangle. And we just subtract two.
Then that's all there is really. And we do this for every layer and every, every bottom like
layers. For the rest of that, if there's speed connection, you make sure that every information
is coming to that layer. For the confoundings, now many things can go wrong. So for example,
if I make, if I don't, if I don't control for the location of this triangle, for example,
when I compare the gestal triangle to full triangle, if they're, they're literal pixel overlap,
is a lot more than this order triangle, that's a problem, right? Because it's literally the same,
it's just this pure pixel overlap is going to be a confounding factor.
So the, because in the journal or same thing in the same place is going to think of, I think that
these two things are similar. And it's not going to abstract out to read the read our intention,
which is to express, well, here are incomplete triangle versus complete triangles.
So what we do is we change the background color. We rotate them around. We, in this way,
and both in, in local data, which we call, we control for all that and measure the average effect.
I'm not sure that we cover this, but the, the presumption being the result is you identified the,
that this effect is present in neural networks. That's right. But how do you characterize it to,
to a certain degree, or what, what's the interesting, you know, what's the degree, I guess?
Yeah. So the interesting nugget in this is that it only exists when the network is able to
generalize. So if let's say I have a network that was overfitted to some random labels,
which we can totally do, we can achieve up to 90 something present accuracy. If I give you random
label, it's happily overfit. But test time, it doesn't know how to generalize. It never learn
what is the meaning of say frog or cat. Those networks do not show the closure effect.
It's only the regularly trained network. We also try to keep the label correct, but instead
shuffle the pixels. So that means I am destroying the local features and train the network.
Again, it is happy to overfit the data because labels are correct, but pixels are shuffled.
If there is some statistical patterns across the classes, for example, all the cats,
the average value of pixels or 0.5, whereas all the dogs are 0.3. So our question was,
can, would neural network leverage that neural network train those destroyed local structure?
Will it still have closure effect? Because if it does, it's not the spatial
meaningfulness that causes closure effect. It's something else. It's something else that we don't
know. And that's something we should, we must investigate. It turns out that if you destroy
the local feature, it does not have closure effect. So you might ask, okay, so what does that mean?
What does that mean? Ultimately, why are we doing this? So ultimately, my studies like this,
I think will lead to insights such as, for example, does that mean that if we build in more
human-like perception in neural network, when we design this network, LLM's, vision language model,
whatever, maybe that means we will help generalization. Now, there's a huge logical jump that I made
there because error this way, causal analysis, this way, doesn't mean the other way. However,
a lot of studies that could perhaps show that relationship or absence of that relationship
could really guide us designing these models because at the end of the day, we're the ones
making decisions what to make next. Within this broad category of studying machines and isolation,
that's just one of the studies that you've conducted. What are some others?
Yeah, so in another study that I share, it's preliminary results, but this is in that work,
we think about the saliency methods, which is the target of the topic of discussion in
the sanity check paper that where we show the training or on training or you can tell the
difference in this saliency map based family of explanation. So we kind of take a deeper dive
into that. So then our question in that paper was, okay, so humans can tell the difference,
but maybe the information is there. It's just that we can tell. And perhaps the other way around
is true too. What if there's information that human can tell the machines can't? So in that
paper, we show both of the information. So it turns out that if I train a model with the same
data, same architecture, but just different seeds. So now I have two models that roughly
achieve the same accuracy, but just literally different seeds and different weights.
When I get explanations, humans can tell the difference. They look the same. Like I looked at
many, many of those same, but for neural network, it can tell which which explanation came from
which model close to perfection, close to 99% 96, the more than 96% accuracy. So that's like
concrete evidence of some information that humans cannot read off of these explanations,
but it's there. This paper characterizes that effect as a fingerprinting of the model.
Does that have implications in kind of adversarial data leakage and questions like that,
or are those tangential to the kinds of things you're thinking about?
I think it has implications to that. But in this work, we focus on explanations alone
that what sort of fingerprints of the model we show that one can read off of the explanation.
So for example, you can imagine actually now you said it. Now I think actually there's
better more link to that. So for example, there has been work that shows, if you just give me
explanations, I can actually completely reconstruct a model. I think this has worked from many years
ago, Barry Moritz-Hardt's group in when he was in Berkeley. And what does that mean? Well,
that means that if somebody requires me to provide older explanations for the input data that
customers are using, then that could be used to reverse engineer preparatory information
and intellectual property gets really a big question. I think that's what you're referring to.
And further, there's other results that suggest that because if we're talking about large models
that can memorize data, there's actually training data that can leak into the model. And if that
leaks out through explanations, then you're also exposing the potentially private information.
That's right. And Catalina Uler's work from MIT showed that this also happens in VAE models
where you can completely reconstruct every single training data because they're attracting points
of the network. You can 100%. And apparently, this phenomenon is widely common. Nicholas
Papernod's paper, Rikus Karlin's paper, all show that this liquid not about explanation,
but just the liquid of the training data completely is possible. So that all should really hint us
that there's just so much we don't know about neural network. And all these studies are super
useful. But somebody should also think about, okay, well, let's just go back to the drawing board,
zero, beginning. And consider this as an alien. And let's think about how we can study this alien
because there's just simply so much to study, we need a lot of different approaches to study
this important alien. And so this first category that we've talked about is, hey, we've got the alien
in a room and we're just kind of, you know, probing and studying the alien. Is the studying machines
with humans? Is that more looking at the alien and the context of collaborating or working with
humans? That's right. To extend the analogy. Yes, exactly. That's right. And I think we should go
one step forward. So a lot of people talk about collaborating with machines and how we can leverage
each other's intelligence and augmented intelligence and so on. But in fact, I think what's perhaps
more important to recognize is that as we interact with these machines and work with AI and live
in this society where every single thing that we do or somehow some AI models are behind them
or most of it anyway, we should know that that is changing our behavior. That's changing us.
It's changing the society. Take Twitter, take YouTube, whatever. People spend a normal
amount of time on social media. Who decides what you see? Who decides what you see from which you
decide you make a decision about your life, about your career? How what you think? How angry you
are or how happy you are? That ecosystem between machines and humans, it's not just collaborations
and leveraging each other. It's a whole world that we're now living with AI. And the language
aspect of my talk, how I think that this language must be developed that we should put
force together to develop this language. And I even think that once we develop this language,
this might be something we are taught in school. For kids as they learn math in high school and
middle school, maybe this is another language that they will learn in schools because it's so
important so that we have effective ways to manage and communicate with these machines.
So pulling back again. I was curious why you characterize it as a language as opposed to
a set of principles or foundational understanding about using machines. What makes you think of it as
as a language or communication vehicle? It representational differences. So if we have same
representation as a human, we can agree on our principle because the underlying
vocabulary that exercises those principles, we agree. We know that you and I know we live a life
and we live and die. We live under the constraints of gravity. We all know that. We have common
understanding. With the machines, we don't have that. So that means even if we have a principle
defined in our own language, we may not be able to communicate what that really means to machines.
We even have hard time communicating with each other what, for example, fairness, fair justice means.
And the challenge is so high, I believe that in order to just really feel that gaps in our
representational spaces, it would have to be back and forth. It would have to be a conversation
and to have a conversation, we need rich language that both of us can express what we really mean.
And that's why language is important. Tell us a bit about your research program that explores
some of these ideas. Yeah, so I must say that this is weird at the beginning of a huge effort
that we, as a community, I hope that we will push down, push on. And lots of folks already have
thought about this. So to build this language, we have to study airlines as we're AI models,
as an object of scientific study. And not just that in isolation, but also in this whole ecosystem
between machines and humans. We live in an ecosystem. We influence each other. We should study
that interaction very carefully. But on top of, but in addition to that, we need to humans have
amazing capacity to learn new things. We should expand what we know. This is a great opportunity
to meet a coworker who is sometimes smarter than me, thinking about your network,
who could really show us how to view a problem that we've been trying to solve from a different
angle. And that could really inspire us to see the solution that we didn't perhaps previously
solve before. So expanding what we know is another branch of this research program that I am
envisioning. And some of those work that I talked about in keynote is Alpha Zero, thinking about Alpha
Zero, the superhuman network that plays chess better than any other humans. How do we go about,
can we do in-depth study in this amazing network to see how do we interpret them? What does it know
and what does it not know? How much of our representational space overlap?
What's your sense for how far we are there? How far we are with that in the case of Alpha Zero?
We are far, we're far away. I think that answers to a lot of those questions. All of the far
questions were far. Yeah, it's why it's a research in which why I'm excited to pursue
distraction for, you know, probably first. I hope that before I, before end of my day,
I would see some a lot different than than where we are now. But we are far away, but the things
that we discovered in that work is already really interesting and frankly encouraging. So when
we started that project, we have this Alpha Zero Network. Only thing that we knew about that is
that it plays chess really, really well. It can be the stock fish engine, which is arguably the
best chess engine in the world. And note that the chess engine, human player, it's very hard for
human player, any human players to beat the chess engine. So this is arguably a very, very good
machine. So when we started that project, we weren't sure what we're going to find. We had ideas
that, okay, well, let's first try to detect human chess concepts in this network and see if it's
there at all. And we honestly thought we don't know what will happen. How much like we will,
will we find 30% of human concepts, 20%, 80%, and we ended up finding quite identifying these
human concepts a lot in Alpha Zero, which is encouraging news, because that means that at least
some concept that at least correlates with how we understand chess, how we play chess,
exists in Alpha Zero. That's a great news as a common ground to start to expand what we know.
So in the follow-up work that we're going to do this summer and next year kind of a long term
project is with this amazing performer professional chess player also PhD student in machine learning.
She's going to join, her name is Lisa Shutt. She's going to join us to work on this problem of,
okay, now we have an expert chess player who also know machine learning. How can we discover new
concepts that we didn't know before? What are some other work that you're seeing that in the
machines with humans category? You mentioned that you're early on in that part of the research.
Are there other examples of things that you're looking at there? Yeah, so I would love to talk
about artist art project that we've done in as a part of expanding what we know. This is human and
machine collaborating together to inspire each other. So I have a long passion for art related
projects because my aunt is an artist in Paris, my mom is an artist. I was just naturally dragged
to something that could, that are just beautiful, beautiful to see, beautiful to feel and think
about. So this project came out of the art and culture team in at Google and with a nor project
who are amazing three designers and artists in London and they, we had this idea of, okay,
what if we use the bizzalignment between how machines view the world when I show you this flower,
would machine also think it's pretty? Would machine think it's very small? Would machine think
it's purple and is it inspiring? Does it inspire it spring time of the year? Or would it think about
something else? And what if I excommunicate something to the machine and say, hey, machine,
I think this is pretty. I think this has, Phil makes me think about babies because it's a small plant.
What do you think machine? And machine comes back with other images from different pool that say,
hey, I think these are images that inspires me when you talk about that flower. And there's this
communication back and forth from which I get inspired by say, oh, interesting. So you are thinking
ocean for some reason from this flower. I can kind of say that inspires me this one time I was
in the lotion blah, blah, blah. So that communication, what which we call moodboard, well, it's a standard
word, but we call moodboard search and concept camera is some tools that we built to enable this
communication, which we are all open sourcing in a couple of weeks. And I think directions like
that, like deep dream, you talked about deep dream, there has been our projects around deep dream
to inspire humans. Directions like that, our project, music, we are hoping to also do some
music projects, or exciting because it's also about not just thinking about task tasks,
task prediction, prediction prediction, it's also about how can we enrich
humans, us and artists and other people behind a machine learning community to leverage this tool.
I think that's extremely exciting. In the case of that heart project concept camera, I'm envisioning
you've got some embedding space of images and you've got your little plant, you find where it is
in that embedding space or what's closest to it. And then you kind of walk the neighborhood to see
what's around it. And that's what the machine is thinking of is, is that along the lines?
Very similar. Okay. Yeah, it's very similar. So I can give you an example of what artists,
one artist, we hired artists to build artistic concept. It's not just a one flower. And one of the
concepts that she built was, this was actually one of our engineers, she built a concept of father.
It reminded me of all the images she collected, reminded her of her father who passed. And it
consists of images like ocean, maybe some blurry leaves, it's very diverse set of images. And from
that such an abstract concept, what machines, we take all those abstract concepts and build the
concept vector from that those images in the embedding space as you described. And then find
other images from a different pool, maybe by cropping or blowing up some part of the image or
removing some part of an image to identify where does this concept present in other set of images.
And that's how we create the conversation. Talk a little bit about the, is there an interactive
element of the conversation? Is that just sequentially kind of choosing another image or is there
another framing for that? Yeah, yeah. So we do, there's a lot of interactions. So after
machines came back in with set of images, you can up vote or down vote to indicate, no, no, no,
that's not what I meant. Yes, yes, more of these. You can crop and scale the images to show better
showcase this is really what I meant. And you can go back and force infinitely. Images are such a powerful
means for visual art in terms of communication, because how quickly you can digest number of images
at the same time. So we leverage that in this work. And is there also an explanations element to
this work? Yeah, good question. So we tried a few things in the tools that we broadcast. One of
which we do is focus mode, which represent we simply do very simple things because again,
we weren't sure if the saliency maps would hold up to this task. So instead, we decided to do
something very simple. So whenever there's a concept present, we, if you click a box, then it will
show a box in the whole image to show what machine, which part of the image that was most activated
by this concept from machines perspective. So that's the only explanation part of this project.
Okay. There's also heap met mode, but that's that's rather
stealth mode for now. That's what? It's a stealth part of the project. Not in an exciting way.
I should. Yeah. It's some some more work has to be done there. Got it. Got it. Other other aspects of
this machines, studying machines in conjunction with with humans that we can talk about.
Let's see. Some of the aspects that I am excited about and I can tell you a little bit about
few projects that I'm currently working on. And I think that's that's really exciting next step. So
one, we are looking at what sort of behaviors emerge in multi-agent reinforcement learning setting.
So if you, if you just let them wild and learn and give them
pretty high level rewards and they are team working at trying to work as team,
what are what's the division of labor? How do they learn what to do? So if I build say an agent that
has four legs, intuitively if we give two people a task of let's try to run as fast as we can and
you you have this this thing, then we would naturally the first person would grab the first leg
and then the second person will grab the letter to two back legs, right? Will that what does that
happen in in multi-agent RL setting? You don't know. What if we have 10 legs? What if we also have
legs and a wheel and a wing? What happens? Those are just really out there curiosity-based questions
that we're thinking about. It reminds me a little bit of a conversation I had a couple of years ago
with Blaze, Agroira Yarkas, also at Google and some of his thoughts on kind of this concept of
social intelligence emerging between agents. Is that a collaborator on this project or is that
that? Oh Blaze, Blaze very busy. I would love to collaborate with you Blaze if you were watching
the fun, but he's very busy. It is similar. I think in many years ahead of time that's completely
feasible idea. I wouldn't be surprised that there are some efficient behaviors that are maybe
similar, maybe this similar with how humans work together, they would emerge in this important
thing is though that we have to look out for it. We have to have our tool right tools to see when
that emerges we notices and we confirm or just confirm that it's the desirable thing that we want
them there and that there's something to be done or maybe if you should do something about it.
I was going to also tell you about this second project that briefly might be interest people.
There has been a lot of, as everyone knows, that large language model or vision language model has
been just impressive. Frankly, it might be fair to say there's some change of paradigm to some
extent in the way that we work with these different modalities or one modality in different tasks.
So we've been thinking about in the case of, say, palm model from Google and work with Jeff
Dean and others, how does it do what it does in that big giant yet sparse model, where does it come
from? And if you, for example, fine tune the model to some other tasks, what changes, what really
changes, what information resides and what right information gets lost. That's been something that's
been on my mind too. And do you have early results from that work that you can talk about?
No. Clearly an exciting and important area given the, as you describe the paradigm shift that
LLMs are bringing about. Yeah. And I think, I think, I know you also, you also talk to Emily Bender
about about whole thing, think about LLM. I think we need to be careful. And which is why I don't
want to, you know, make claims about, oh, we can't have this preliminary results until I really
confirm as a scientist, you know, really confirm, rigorously confirm, and triple confirm to see,
okay, this is, this is really it. And then I share it. Otherwise, there's just a lot of information
that may be lacking or to, to only fuel the hype around AI when sometimes it's fair to have
the hype sometimes isn't fair. One of the things on that note that LLMs and transformers
more broadly are unlocking is this whole opportunity to do multimodal models.
What's the, the state of play around applying some of the work around explainability and
understanding to multimodal? Yeah, it is in the, it is infancy. I think the difficulty, it's
difficult to answer that question because while I see a lot of tools that we developed are on
embedding space. For example, this concept activation vector or concept vector, we do everything
on embedding space because we know that that's an efficient, tightly learned space that we can do
stuff about intervene, do causal inference, it's a nice space. And that still exists in this
model model. So it should extend. Ideally, it should, but then in a way, it kind of shouldn't
because do the things that what they can do is so different, it must be that some different
way of encoding information has happened in this large language model in one way or another.
So in a way, I'm optimistic to where baby we can extend these techniques, but in other,
on the other hand, we have to be very careful, especially careful because our human biases
are amazing, something that we have and we cannot get over, can escape. And that means because
that's excited about these advances, because I, the developer, already biased, when we work on these
models, we have to be very careful, probably double blind myself when I do research to make sure
that I'm not fooling myself. So that introduces additional challenge.
It seems like appropriate caution coming from the author of paper that said,
hey, all these things that we learned about explainably don't explain what we thought they explained.
I learned my lesson in a hard way.
You mentioned under the final category that you described that you were talking through in your
talk around alignment. One of those core elements there is this idea of concept vectors and some
of the work there. Can you speak a little bit to that? Yeah, yeah. It's a line of work that I'm
hugely excited about and have been working on for years. So the overarching idea is the following.
So prior to concept-based explanation, what we've been looking at are things like saliency map,
where I put a number in the pixel, and that's what you're looking at. So if you want an explanation
for a picture of a bird, then you look at a picture of a bird and maybe 10,000 other pictures of birds
to see, okay, what are the common patterns among the class of birds. What concept-based explanation
suggests is that let's not do that because that's not even how humans communicate to each other.
I talked to you in words that in abstractist words and you understand you're nodding, right?
I can tell you your glasses, you sit on a chair. I don't have to explain everything about each
pixel in you that I'm looking at you to communicate what I'm saying. So we have a more efficient
high band with communication. So why don't we build that high band communication with machines
and humans? And this concept-based explanation is a first step towards that. So instead of pixels,
let's abstractize and use the language that we used with the between humans. So fluffy bird,
fluffy dog, or a striped, or a circular shaped biopsy and then cell that looks a little bit
oval shape. That's how doctors can talk to each other. So let's use those extra-type concepts,
extra abstract, more abstract concepts to have machines communicate to us. And the way we do it
is to by building this concept activation vectors. So you say doctors, give me, I have this medical
concept of irregular boundaries in the biopsy samples. I will give you some examples and we will
map those examples in an embedding space, which gives us a vector. There are lots of different ways
to do it, but we do it many, many times so that we can get some robust measure of this direction
and the vector. And once you have this vector, you can do a lot of things. You can ask the model,
okay, here's a direction in the embedding space. Do you care about this direction? If I move things
a little bit, would your prediction change a lot? Because that means that then the direction
influence system model in terms of first order derivative, in terms of sensitivity test.
If I remove that direction altogether, what do you do? Do you do you explode and give up and
go home? Or you're just fine, like it's as if it didn't have to exist. So by the core concept is
simply the building in this complex concept around medical or human ideas into this one vector
and then so that we can build that this vector can be a bridge between machines and humans'
communication. In thinking about the concept we discussed earlier around language that
linguistic constructs to enable machines and humans to communicate and these abstract ideas
around concepts, does it support at all the call that some industry are making for us to kind
of marry symbolic methods for AI and statistical methods for AI? Yes, absolutely. I know Rao
is a proponent of this and I agree with Rao on this that at some level I think
when our principles are just the only way and perhaps safest way to do a task,
then I think symbolic based representations would be inevitable because that's just we decided
as a human, we're the human, we're making AI machines. As a human we decided that that's just how
it's going to go, especially might be important in high-stake decisions where we know that this
is the way that we've been doing and we don't yet have better way to do it or we need more
empirical evidence until we switch the way that we do things. Then I think that those representation
would have to be part of the equation. We can't just do everything in freely distributed
representation unless there's some breakthrough that could convince me otherwise.
We'll stay tuned for that. Being, it's been wonderful chatting with you. Thanks so much for joining
us and sharing a bit about what you're working on and what you're excited about and what you
share it at. I clear. Thank you, Sam. Thanks for having me. Thank you.
