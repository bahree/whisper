1
00:00:00,000 --> 00:00:15,280
Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.

2
00:00:23,280 --> 00:00:28,720
All right everyone, I am super excited to be on the line with Yashua Benjiyo. Yashua is a

3
00:00:28,720 --> 00:00:34,240
professor in the Department of Computer Science and Operations Research at the University of Montreal

4
00:00:34,240 --> 00:00:40,000
and the founder and scientific director of Miele. Yashua, welcome to the Twimal AI Podcast.

5
00:00:40,000 --> 00:00:47,600
Hi, it's good to be here. It is super exciting for me to have you on the line. I've already said

6
00:00:47,600 --> 00:00:53,680
super exciting twice, so maybe that's an indication of my enthusiasm for the opportunity to chat with

7
00:00:53,680 --> 00:01:00,800
you a little bit about your work. Your name is certainly well-known among our audience and your

8
00:01:00,800 --> 00:01:07,520
contributions to this field. I read somewhere that you are ranked as the most cited computer

9
00:01:07,520 --> 00:01:14,560
scientist worldwide or one of. In terms of recent contributions, yes. There are people who have been

10
00:01:14,560 --> 00:01:24,320
in the field longer than me and I'm among the young old people. Nice and just last year, you along

11
00:01:24,320 --> 00:01:32,240
with Jeff Hinton and Jan Lecune received the ACM Touring Award for your contributions with

12
00:01:32,240 --> 00:01:36,720
deep learning, deep neural networks. Why don't we start with having you tell us a little bit about

13
00:01:36,720 --> 00:01:43,680
that journey and how you came to work in AI and on deep learning and of course we'll get to what

14
00:01:43,680 --> 00:01:48,560
you're working on now, but let's start with some of that background. Yeah, deep learning,

15
00:01:48,560 --> 00:01:55,120
neural nets, it's been my life, my professional life at least. I had started as an adolescent reading

16
00:01:55,120 --> 00:01:59,200
a lot of science fiction and getting acquainted with the notion of AI and robots and so on

17
00:02:00,320 --> 00:02:07,760
with the three laws of robotics and all that and movies like 2001 Space Odyssey. And then at

18
00:02:07,760 --> 00:02:14,480
university, I studied first computer and electrical engineering and then computer science

19
00:02:15,680 --> 00:02:20,880
in my masters and I had to choose a topic and by chance I got to read some of the early neural

20
00:02:20,880 --> 00:02:26,000
net papers from people like Jeff Hinton and I realized that this was the field I wanted to work on

21
00:02:26,000 --> 00:02:34,720
because it was about trying to understand the principles that would explain our own intelligence,

22
00:02:34,720 --> 00:02:41,040
humans and intelligence is very central to who we are and then building machines thanks to

23
00:02:41,040 --> 00:02:46,080
understanding these principles like the way that we understand how burst fly but we're not

24
00:02:46,080 --> 00:02:50,160
necessarily copying the birds, we are trying to understand those principles so we can build airplanes

25
00:02:50,160 --> 00:02:57,360
that also fly. A lot of your recent work is focused on this idea of consciousness. What's the

26
00:02:57,360 --> 00:03:04,480
relationship between consciousness and intelligence? Very good question. So first of all, let me put

27
00:03:04,480 --> 00:03:09,840
things in perspective about deep learning. So a lot of the progress we've made in deep learning

28
00:03:09,840 --> 00:03:18,000
and neural nets in the last few decades has helped us to build machines that can do pretty well

29
00:03:18,000 --> 00:03:22,800
at some of the things that brains are good at including perception, the ability to understand

30
00:03:22,800 --> 00:03:30,000
images, sounds and so on. But there are also things that humans do with their brain, the things

31
00:03:30,000 --> 00:03:34,480
that we are conscious of. So when you decide to do something and you're able to report what you're

32
00:03:34,480 --> 00:03:41,040
thinking about or why you do something, that kind of cognitive ability which is associated with

33
00:03:41,040 --> 00:03:47,680
being conscious of it is not something that we're good at in AI right now and some people might

34
00:03:47,680 --> 00:03:52,080
even think that it's incompatible with all of the ideas that have been put forward with the

35
00:03:52,080 --> 00:03:58,320
neural nets and deep learning and it's more related to some of the older ideas in AI based on

36
00:03:58,320 --> 00:04:04,080
symbols and logic and expert systems. But actually your brain is a huge neural net that we need to

37
00:04:04,080 --> 00:04:11,200
understand better. And the good news is that in the last two or three decades, neuroscientists,

38
00:04:11,200 --> 00:04:16,320
cognitive neuroscientists have made a lot of progress in understanding what is going on in your

39
00:04:16,320 --> 00:04:22,000
brain when you're doing something consciously which parts of your brain get activated in what

40
00:04:22,000 --> 00:04:27,920
order and so on. And so whereas the research on consciousness was almost something taboo in the 20th

41
00:04:27,920 --> 00:04:33,600
century, in this century it is something that has become an important topic of serious science.

42
00:04:35,520 --> 00:04:41,440
And so my research now is at the cusp of this transformation where computer science and AI

43
00:04:41,440 --> 00:04:49,040
research are starting to take stock of what has been discovered in neuroscience and try to take

44
00:04:49,040 --> 00:04:56,240
some of these ideas and import them into new types of deep learning systems that would come with

45
00:04:56,240 --> 00:05:00,800
the advantages of being conscious, which I can also explain. One of the ways that you describe the

46
00:05:00,800 --> 00:05:06,400
relationship between the type of deep learning systems that we have today and this model that you

47
00:05:06,400 --> 00:05:12,160
see as a possibility as we start to incorporate these ideas of consciousness is system one and

48
00:05:12,160 --> 00:05:18,240
system two coming from Daniel, condiments book thinking fast and slow. Maybe you can elaborate a

49
00:05:18,240 --> 00:05:26,240
little bit on these two different systems as he puts it and from my curiosity was that an

50
00:05:26,240 --> 00:05:32,240
inspiration for the way you're thinking about this now or did it become a handy explanation

51
00:05:32,240 --> 00:05:39,280
for some of what you're doing? Oh, it's definitely an inspiration. His work and the work of

52
00:05:39,280 --> 00:05:47,840
psychologists and neuroscientists who have helped clarify the different types of functions that are

53
00:05:47,840 --> 00:05:55,920
being that are happening in your brain has really helped me and others think about how modern AI

54
00:05:55,920 --> 00:06:01,120
systems could could incorporate these things. So first of all realizing that you have these two

55
00:06:01,120 --> 00:06:07,360
very different types of cognitive abilities. A system one is the kinds of things that deep learning

56
00:06:07,360 --> 00:06:12,880
is good at right now. The kinds of things you can do in half a second. So you see an image,

57
00:06:12,880 --> 00:06:17,200
you know that it's a cat and you don't need to think about it. It happens automatically.

58
00:06:17,920 --> 00:06:23,200
In fact, you don't even need to be conscious of it. It's something that we can see in your brain

59
00:06:23,680 --> 00:06:30,480
that if it's maybe happening on the side or you have not enough time, your brain will record

60
00:06:30,480 --> 00:06:35,840
that it's a cat but it doesn't even get to your consciousness. So these things are intuitive

61
00:06:35,840 --> 00:06:43,200
and they're hard to verbalize. Like you can't explain why you recognize that this was a cat.

62
00:06:43,200 --> 00:06:50,000
Or you could try to explain it but it's not a good explanation. In fact, a lot of the earlier failures

63
00:06:50,000 --> 00:06:54,880
of computer vision research was that we were trying to take our own internal explanation. So,

64
00:06:54,880 --> 00:06:59,760
you know, what is it that makes a cat and take that in trying to go through all this. But it was

65
00:06:59,760 --> 00:07:04,320
never, you know, very good because there's a lot of the way that our brain does it which we don't

66
00:07:04,320 --> 00:07:11,120
have access to. So, everything that is intuitive that is at a, you know, subverbal's unconscious

67
00:07:11,120 --> 00:07:16,560
level is roughly system one. There are some differences between conscious and conscious and system

68
00:07:16,560 --> 00:07:24,400
one system two but it's a good way to first order to understand things. And system two is instead

69
00:07:24,400 --> 00:07:29,360
the things you typically do consciously. The things that you can report, the things that you,

70
00:07:29,360 --> 00:07:33,360
you know, you can verbalize, not everything can be verbalized but a lot of the things that are

71
00:07:33,360 --> 00:07:39,680
your conscious of can be verbalized. And that you do in sequence. It's like at each step you,

72
00:07:39,680 --> 00:07:45,520
you might be involving intuitive computation in your brain but you're going to sequence these things

73
00:07:45,520 --> 00:07:53,760
in your mind in a way that you control and that, you know, gives you an extra power, an extra

74
00:07:53,760 --> 00:08:02,400
flexibility which allows us to do things like learn to drive or learn to drive in a new city

75
00:08:02,400 --> 00:08:08,320
or figure out what to do in some unusual circumstances where we have to be creative and find solutions

76
00:08:08,320 --> 00:08:14,640
on the spot and it doesn't look like anything we've seen before. This kind of very powerful,

77
00:08:14,640 --> 00:08:23,440
dynamically adaptive behavior that humans employ to solve new problems is a very typical of system two.

78
00:08:23,440 --> 00:08:31,120
And current AI isn't very good at these things. Current AI, if you train the systems on some data

79
00:08:31,120 --> 00:08:39,600
and then you deploy them in situations that are not exactly the same kind, you get a big hit

80
00:08:39,600 --> 00:08:44,160
in performance. It doesn't work as well whereas humans are able to transfer their knowledge to

81
00:08:45,040 --> 00:08:53,120
new domains to, you know, new environments much more easily. And very often this is when you use

82
00:08:53,120 --> 00:08:59,840
your conscious strengths. So when you're doing something habitual, you don't need to think about

83
00:08:59,840 --> 00:09:05,280
it. So the example I give is when you're driving on the usual path, you can talk to the person

84
00:09:05,280 --> 00:09:09,680
besides you. You're conscious of the conversation but you don't need to be conscious of the details

85
00:09:09,680 --> 00:09:14,160
of the road. I mean, of course you're taking chances when you're doing that but the point is a lot

86
00:09:14,160 --> 00:09:20,640
is going on unconsciously because it's something familiar that you have seen a lot and you don't

87
00:09:20,640 --> 00:09:27,840
need to be mindful of it. Whereas when you're doing something that's more unusual and you need to

88
00:09:27,840 --> 00:09:35,040
practice a new skill or to have a new coherent way of putting together pieces of knowledge,

89
00:09:35,040 --> 00:09:39,600
you're using system two and you're using conscious processing. It was interesting to me that in

90
00:09:39,600 --> 00:09:46,160
your description of these two systems, you talked about unvocalize or subverbal and, you know,

91
00:09:46,160 --> 00:09:55,440
things that we can vocalize and put into words. And in your descriptions of consciousness, you

92
00:09:55,440 --> 00:10:02,000
use the examples of language a lot to represent these ideas. I'm curious to relationship between

93
00:10:02,000 --> 00:10:09,680
language as a construct and consciousness. Yeah. This is obviously an open question. So you have

94
00:10:09,680 --> 00:10:14,640
to realize that the science of consciousness is something fairly new, as I said, like a couple of

95
00:10:14,640 --> 00:10:20,080
decades a bit more, and there are a lot that we need to understand. So we have to be very humble

96
00:10:20,080 --> 00:10:25,200
about how much we don't know. But yeah, it seems very obvious that there's a very strong connection

97
00:10:25,200 --> 00:10:32,880
between language and consciousness, but they're not equivalent. For example, you might have

98
00:10:34,240 --> 00:10:39,120
things that are happening in the area of your brain that deals with language that are kind of

99
00:10:39,120 --> 00:10:43,760
different from the things that have to do with being conscious of something. So, but there are

100
00:10:43,760 --> 00:10:51,680
strong connections. And in fact, one of my current research tracks is to exploit hypothesis

101
00:10:52,400 --> 00:11:00,640
that that connection is very tight in the sense that the high level concepts that you manipulate

102
00:11:00,640 --> 00:11:07,680
consciously also correspond to things like words, things that you can verbalize. And so the

103
00:11:07,680 --> 00:11:11,440
representation level, the two things are very close to each other. Yeah, there's been

104
00:11:11,440 --> 00:11:17,760
a past work, nothing that I could cite or that I know intimately, but that we kind of talk about

105
00:11:19,360 --> 00:11:25,520
generally that, for example, in some cultures, they don't have words for certain types of things.

106
00:11:25,520 --> 00:11:30,960
And the implication being that that kind of conditions their thought in certain directions

107
00:11:30,960 --> 00:11:36,640
that differs from other cultures. And it seems very much in line with this idea of

108
00:11:36,640 --> 00:11:43,520
consciousness as you're defining it. Yes, absolutely. And in fact, the way that I want to use this

109
00:11:45,040 --> 00:11:53,680
hypothetical connection is to help the learning of these high level concepts. So, what deep learning

110
00:11:53,680 --> 00:11:58,240
is about, which I didn't have time to say earlier, is learning good representations.

111
00:11:59,520 --> 00:12:04,960
And we've been very good at learning, let's say, low level and mid-level representations with deep

112
00:12:04,960 --> 00:12:12,480
learning, especially on the visual input. But we don't yet have good algorithms to discover

113
00:12:12,480 --> 00:12:18,720
the right variables or high level concepts that humans manipulate consciously. Of course, we can,

114
00:12:18,720 --> 00:12:23,680
you can kind of cheat by telling machines, this is a cap, this is a dog, and then they kind of know

115
00:12:23,680 --> 00:12:28,800
what is a cat and a dog by example. But what we don't have is an ability to discover these

116
00:12:28,800 --> 00:12:36,480
high level things. And so, by connecting, say, video input with a corresponding language, in the

117
00:12:36,480 --> 00:12:43,200
right way, we can force a deep learning system to learn representations, which at the top level

118
00:12:43,760 --> 00:12:50,400
would have features, if you want, that correspond to words or phrases or linguistic constructs.

119
00:12:50,400 --> 00:12:57,040
And that would help the representation learning system discover those high level concepts,

120
00:12:57,040 --> 00:13:03,360
in the same way that when we talk to children, using words in particular context helps them

121
00:13:03,360 --> 00:13:08,160
build the corresponding meaning of those words in their mind. But of course, they don't need the

122
00:13:08,160 --> 00:13:13,600
words. They're building meaning absent of the words. Initially, they learn a lot of things that

123
00:13:13,600 --> 00:13:18,640
they don't have words for. But it's actually difficult to disentangle these things, because even though

124
00:13:18,640 --> 00:13:24,160
it might take a couple of years before they start talking, during those couple of years, they also

125
00:13:24,160 --> 00:13:30,320
hear a lot of language. Now, the interesting clue is that in some cultures, parents don't talk to

126
00:13:30,320 --> 00:13:35,200
their children. They talk with each other, and the children just listen. Whereas in our western

127
00:13:35,200 --> 00:13:39,600
cultures, we tend to talk a lot to our babies. So you might think that they're learning from that

128
00:13:39,600 --> 00:13:46,000
interaction. But it looks like even without a direct sort of naming, oh, this is a cat, this is a dog,

129
00:13:46,000 --> 00:13:54,560
the babies can catch the connection between the labels, the words, and the things in the world.

130
00:13:54,560 --> 00:14:01,120
So we jumped in pretty quickly and started talking about some of the implications of how you're

131
00:14:01,120 --> 00:14:09,360
defining consciousness. But you really launched into this field with a paper in 2017, the consciousness

132
00:14:09,360 --> 00:14:14,480
prior. That's right. What exactly is the consciousness prior? And maybe tell us the, you know,

133
00:14:14,480 --> 00:14:19,680
the main points that you're trying to convey in that paper. Okay. I'm going to try to do that

134
00:14:19,680 --> 00:14:28,240
in an accessible way. Yeah. So first of all, what is the word prior means? It's a term we use

135
00:14:28,240 --> 00:14:37,840
in machine learning research to say that the learning system is exploiting some kind of assumption

136
00:14:37,840 --> 00:14:43,120
about the world. And there's even theorems that say that you have to have assumptions, at least

137
00:14:43,120 --> 00:14:50,640
minimal ones in order to be able to learn successfully. And so the brain has these kinds of assumptions

138
00:14:50,640 --> 00:14:56,080
about the world that, you know, we inherit from our ancestors through that have, you know,

139
00:14:56,080 --> 00:15:01,440
these assumptions have encoded somehow in our genes. And we are born with those things.

140
00:15:02,240 --> 00:15:08,240
And they help us to learn faster and better in the world around us. So when I said at the

141
00:15:08,240 --> 00:15:12,880
beginning that one of my research goals is to understand the principles that give rise to

142
00:15:12,880 --> 00:15:18,080
intelligence, well, an important part of this is what are the kinds of assumptions that humans

143
00:15:18,080 --> 00:15:24,320
exploit about the world that allow us to learn efficiently about it, to build, to understand how it

144
00:15:24,320 --> 00:15:31,200
works, to learn language, to learn to model the world, to act in the world and so on. So notions,

145
00:15:31,200 --> 00:15:36,880
you can think of things like, well, notions of time and space of agency that, you know, I do things

146
00:15:36,880 --> 00:15:44,480
and their effects and so on, are probably things that we got from our genes and that our brain

147
00:15:45,360 --> 00:15:51,040
is exploiting as some kind of assumptions about the world. And so the consciousness prior is

148
00:15:51,040 --> 00:15:56,400
one such assumption which would be connected with a notion of consciousness, but they actually,

149
00:15:56,400 --> 00:16:02,720
I actually have a whole list of related assumptions, but this one is very central. So what it says is

150
00:16:02,720 --> 00:16:11,200
there are two kinds of knowledge about the world, which is somewhere in our brain. And that's

151
00:16:11,200 --> 00:16:15,360
basically the system one knowledge and the system two knowledge, right? So the system one knowledge

152
00:16:15,920 --> 00:16:21,520
is knowledge that is difficult to put in words, as we were defining system one this way essentially,

153
00:16:22,080 --> 00:16:27,440
whereas system two knowledge is knowledge that is easy to put in words. Now what kind of knowledge

154
00:16:27,440 --> 00:16:35,440
has the property that it can easily put in words? Well, there is a nice structural property

155
00:16:35,440 --> 00:16:42,160
of the kind of knowledge we can communicate with words. And the property is that we are able to

156
00:16:42,160 --> 00:16:49,200
make predictions about things that can happen, about words, given other things that we know.

157
00:16:49,200 --> 00:16:53,920
So for example, if I say, if I drop my phone, it will fall on the ground,

158
00:16:53,920 --> 00:17:00,560
that sentence only involves a few concepts. It involves a phone, it involves the ground,

159
00:17:00,560 --> 00:17:07,840
it involves the act of dropping and the result, right? That's very few elements if you think about

160
00:17:07,840 --> 00:17:14,880
it. Normally when you try to make a prediction about something in the world, you need a whole lot

161
00:17:14,880 --> 00:17:21,520
of other things to make that prediction accurate. So if I'm trying to predict the next pixel that

162
00:17:21,520 --> 00:17:27,840
will show up at some position in the video that I'm seeing right now, I need to know about all the

163
00:17:27,840 --> 00:17:33,120
other pixels and all the pixels that I've seen in the last few seconds or something or minutes.

164
00:17:34,880 --> 00:17:41,440
And that's like millions of numbers that come into that prediction. So pixels are difficult to

165
00:17:41,440 --> 00:17:48,400
manipulate to explain with language. They don't have this property. But instead, if I explain the

166
00:17:48,400 --> 00:17:55,120
world in terms of objects that I can name, like I did with my phone that could drop on the ground,

167
00:17:55,120 --> 00:18:00,000
this way of representing information, of representing knowledge is one where we can make

168
00:18:00,880 --> 00:18:05,680
statements about things that should be true and things that should not be true.

169
00:18:06,880 --> 00:18:13,120
Where each of those statements only involves very few concepts. So with natural language,

170
00:18:13,120 --> 00:18:17,600
we can communicate a lot of knowledge about the world, but it's decomposing to little pieces

171
00:18:17,600 --> 00:18:25,280
like sentences. In each of these sentences only involves very few concepts. So in machine learning

172
00:18:25,280 --> 00:18:32,240
jargon, we say we can summarize this in a single phrase. We could say that the joint distribution

173
00:18:32,240 --> 00:18:39,680
of the high level concepts is sparse. Sparse here means that if you draw the connections between

174
00:18:39,680 --> 00:18:47,200
all the concepts as a graph, the graph has very few edges coming out of each node. So each

175
00:18:47,200 --> 00:18:56,240
node corresponds to a concept. And each concept is attached to other concepts through very few edges.

176
00:18:56,240 --> 00:19:03,360
So the graph between the connections between the concepts in that sentence or the concepts in

177
00:19:03,360 --> 00:19:07,360
all the things that you could say, if you think about all the things that you could say,

178
00:19:07,920 --> 00:19:12,880
you decompose all the things that you could say into sentences just to make it keep it simple.

179
00:19:12,880 --> 00:19:18,480
Now each sentence is like a special kind of node in the graph. And each sentence connects to

180
00:19:18,480 --> 00:19:24,640
very few words. Now the words are connected to many, all of the sentences that they could appear in.

181
00:19:25,840 --> 00:19:31,360
But each of these sentences only involves very few words. And so overall, the structure of that

182
00:19:31,360 --> 00:19:37,760
graph is extremely sparse. Whereas if I were to draw a graph of the statistical dependencies

183
00:19:37,760 --> 00:19:42,560
between pixels, it would all most need to be like fully connected. That every pixel needs to talk

184
00:19:42,560 --> 00:19:49,600
to every pixel. So what happens is your brain has created these high-level variables, high-level

185
00:19:49,600 --> 00:19:55,520
concepts, which allow to decouple a lot of the complicated dependencies that would otherwise

186
00:19:55,520 --> 00:20:00,240
seem to exist. And in addition, so that's the consciousness probably, but there are like

187
00:20:00,240 --> 00:20:05,200
side statements to this. So one of them is that those high-level variables have to do with

188
00:20:05,200 --> 00:20:10,240
causality. So those dependencies, when I said that if I dropped the phone, it's going to fall on

189
00:20:10,240 --> 00:20:16,240
the ground. It's also like a causal statement. It says something about if I do a particular action,

190
00:20:16,960 --> 00:20:22,480
this is going to be the consequence. And there's an object which is going to be affected by the

191
00:20:22,480 --> 00:20:32,000
action of dropping the phone. And so this is important because causality allows us to understand

192
00:20:32,000 --> 00:20:37,200
the world in a strong sense. That goes beyond making predictions. It allows us to

193
00:20:37,200 --> 00:20:44,400
imagine what could happen, for example, or what could have happened. These are called counterfactuals.

194
00:20:45,520 --> 00:20:54,080
You give an example in some of your talks on this about putting on sunglasses and that simple

195
00:20:54,080 --> 00:21:02,160
act which can be reduced to one bit. Change has a significant implications on all the pixels

196
00:21:02,160 --> 00:21:09,760
that are firing in your retina, but it's a single bit. Yeah. So exactly. And so I was talking about

197
00:21:09,760 --> 00:21:14,560
causality earlier. And one of the particularly interesting aspects of causality is that it tells us

198
00:21:14,560 --> 00:21:22,640
about how the world typically changes. So it's not just about how the concepts are related to

199
00:21:22,640 --> 00:21:30,480
each other, but how these relations changes over time due to agents like people or robots,

200
00:21:30,480 --> 00:21:37,760
eventually, and animals doing things. And what happens is that those changes are very localized

201
00:21:38,400 --> 00:21:44,320
in the sense that they involve only one or a few concepts. So typically when you come up with an

202
00:21:44,320 --> 00:21:48,800
explanation for something that has happened and our brain constantly tries to come up with explaining

203
00:21:48,800 --> 00:21:54,960
what is going on verbally, we end up being able to provide a very short explanation like one sentence.

204
00:21:54,960 --> 00:22:00,800
Oh, he dropped the phone. But this is amazing, right? Because the world has changed like I've put

205
00:22:00,800 --> 00:22:07,280
on these dark glasses. And I'm able to explain it by referring to very few things. I have put on

206
00:22:07,280 --> 00:22:13,520
my dark glasses. Whereas if you didn't have that assumption, you might imagine that everything

207
00:22:13,520 --> 00:22:18,560
has changed and you can't do any prediction anymore. But now the fact that is very few things

208
00:22:18,560 --> 00:22:22,480
that have changed allow you to recover from those changes. And this is what humans are good at.

209
00:22:22,480 --> 00:22:28,480
They're good at recovering from changes that are happening in the world. So that's this adaptive

210
00:22:28,480 --> 00:22:34,400
strength skill that humans have that we would like to put in machines. So this whole research

211
00:22:34,400 --> 00:22:39,680
on consciousness is not just about understanding an important part of who we are, who we are,

212
00:22:39,680 --> 00:22:45,040
which is clearly consciousness is a big part of it. But it's also building those abilities in

213
00:22:45,040 --> 00:22:50,400
machines to be more robust to the changes that can happen in the environment. The idea of

214
00:22:50,400 --> 00:22:56,240
consciousness, then you kind of express it as this low dimensional representation of the broader

215
00:22:56,240 --> 00:23:02,160
connections that are in the brain or that are in some system that we don't have yet. That's related

216
00:23:02,160 --> 00:23:08,000
to the idea of attention that we've been experimenting with in neural networks. What's the connection

217
00:23:08,000 --> 00:23:12,880
between those? Yes, yes. Very good question. And actually, it's a tough one because a lot of people

218
00:23:12,880 --> 00:23:20,000
still don't see the connection. So let me try to explain it. So remember I said that we're exploiting

219
00:23:20,000 --> 00:23:27,440
this assumption that the dependencies between concepts have this sparsity, like pieces of knowledge

220
00:23:27,440 --> 00:23:36,400
each would be like a sentence. Now, if you want a machine or brain to compute over that knowledge

221
00:23:36,400 --> 00:23:42,080
base that we have in our brain or in a computer, you want to take advantage of that sparsity.

222
00:23:42,080 --> 00:23:49,120
And a good way to do it is to focus the computation on just the right pieces at each step,

223
00:23:49,120 --> 00:23:53,040
because then you only need to consider the interactions between a few variables.

224
00:23:53,600 --> 00:23:59,200
And that's much easier both in terms of computations and in terms of what we call the statistical

225
00:23:59,200 --> 00:24:03,840
advantage. So if something changes then we can learn it quickly if it involves only a few variables.

226
00:24:03,840 --> 00:24:13,680
So attention is a way to exploit this inherent sparsity, the assumed sparsity that I'm talking about,

227
00:24:14,400 --> 00:24:18,960
so that we can do this very special kind of computation that involves very few variables at a time.

228
00:24:18,960 --> 00:24:23,280
So attention selects just these few variables that come into our working memory.

229
00:24:24,480 --> 00:24:32,000
And then we can utter a sentence to share what we have in our mind. But there's more to this,

230
00:24:32,880 --> 00:24:38,640
the fact that we have this attention bottleneck, which is also this bottleneck is a central piece of

231
00:24:39,280 --> 00:24:45,280
current theories of consciousness, forces the part of the knowledge about the world which

232
00:24:45,280 --> 00:24:53,120
should go in system two to go there. So only the things that can go through this bottleneck

233
00:24:53,120 --> 00:24:58,640
that involves little pieces of knowledge involving a few variables at a time are being

234
00:24:58,640 --> 00:25:02,800
represented at that level. The stuff that you can't handle with consciously goes, you know,

235
00:25:02,800 --> 00:25:08,400
system one, and it takes more time to learn because it about the interaction of many things together,

236
00:25:08,400 --> 00:25:15,840
like extracting information from images, but the stuff that somehow has this property,

237
00:25:15,840 --> 00:25:23,680
the consciousness prior property, that will be processed by system two, and now you have

238
00:25:23,680 --> 00:25:29,200
advantages because you can do things at that level because you exploit that sparsity,

239
00:25:29,200 --> 00:25:37,120
you can quickly reason and do all kinds of things plan that are harder to do otherwise.

240
00:25:37,120 --> 00:25:44,720
Is the idea there that this system two, if we think of it as like a memory,

241
00:25:44,720 --> 00:25:50,960
you know, the bottleneck says that it can't just store anything, it can only store these higher

242
00:25:50,960 --> 00:25:56,320
level concepts or representations, is that? You're right. We only store the stuff that goes

243
00:25:56,320 --> 00:26:00,880
through our consciousness. The things that you're unconscious of are going to stay in your mind,

244
00:26:00,880 --> 00:26:05,520
in your brain, you know, very short time, and then you're going to forget them. Whereas the things

245
00:26:05,520 --> 00:26:10,320
that you've been conscious of are more likely to be stored in long-term memory. But in addition,

246
00:26:10,320 --> 00:26:16,320
when they go through this short-term memory where you can operate on them, you can do pretty

247
00:26:16,320 --> 00:26:24,960
fancy things that is what we call thinking or reasoning or planning or discovering something in

248
00:26:24,960 --> 00:26:34,160
coherent and that we do consciously. So is the idea of attention as applied to consciousness

249
00:26:34,160 --> 00:26:41,520
that we might use attention as a way to train this consciousness prior? Yes. Yes. Yes. So attention

250
00:26:41,520 --> 00:26:47,280
is part of the architecture of the neural net in order to enforce the prior. What's interesting

251
00:26:47,280 --> 00:26:53,920
is it has already brought a kind of revolution within deep learning. So there's been amazing

252
00:26:53,920 --> 00:26:57,600
progress in natural language processing. It started with the work we did in which

253
00:26:57,600 --> 00:27:05,360
translation around 2014. And then since 2016, this has been put in Google Translate and then,

254
00:27:05,360 --> 00:27:11,600
you know, it's become the dominant kind of technology for machine translation. But since then,

255
00:27:12,320 --> 00:27:18,480
with new architectures that exploit consciousness like what's called the Transformers,

256
00:27:19,680 --> 00:27:25,920
the progress in all kinds of natural language tasks has really, really progressed a lot.

257
00:27:25,920 --> 00:27:34,320
And it's changing the very nature of the way we think about neural nets. So in the traditional

258
00:27:34,320 --> 00:27:42,000
neural net, we think of the computation as operating one step at a time on these vectors. So like

259
00:27:42,000 --> 00:27:48,000
a fixed set of numbers, which correspond to a bunch of neurons having some activity. But when

260
00:27:48,000 --> 00:27:55,280
you have attention mechanisms, what it makes it possible is to operate on sets of objects rather

261
00:27:55,280 --> 00:28:04,160
than on these fixed size vectors. So already, this is having a big impact in natural language

262
00:28:04,160 --> 00:28:10,000
processing because language has this property that, you know, you want to just take some elements

263
00:28:10,000 --> 00:28:15,760
of what I've been talking about in the last five minutes. And then reuse it in order to say

264
00:28:15,760 --> 00:28:21,920
something, you know, that makes sense with respect to what I said. So it tends to select a few

265
00:28:21,920 --> 00:28:27,360
elements, combine them in new ways, add some new things. But at each step, you only consider a

266
00:28:27,360 --> 00:28:35,920
few things at a time. And current transformer architectures have this inherent structure.

267
00:28:35,920 --> 00:28:41,280
Another thing that's going on with these attention-based systems is that in a way we are introducing

268
00:28:41,280 --> 00:28:48,880
some of the old ideas from AI of indirection and naming things and having things that have a

269
00:28:48,880 --> 00:28:54,960
type. So these are concepts that have been there since the beginning of programming,

270
00:28:54,960 --> 00:29:00,640
but haven't really, it wasn't clear how to incorporate these ideas in your nets. And so

271
00:29:01,600 --> 00:29:06,640
with attention, there's something really interesting going on, which is now you select

272
00:29:07,280 --> 00:29:11,040
which neuron, for example, is going to talk to which neuron. So it's kind of like dynamically

273
00:29:11,040 --> 00:29:18,560
changing the connection pattern between neurons or groups of neurons. And so when you have

274
00:29:18,560 --> 00:29:24,960
this dynamic connectivity going on, you need to carry information about where is the signal coming

275
00:29:24,960 --> 00:29:30,800
from. So it's not just the information I send you, but it's coming from me and you need to keep

276
00:29:30,800 --> 00:29:39,600
track of that. In a lot of ways, the idea of a consciousness prior makes me think of the work

277
00:29:39,600 --> 00:29:46,000
that's happening around model-based machine learning. So in reinforcement learning, for example,

278
00:29:46,000 --> 00:29:53,120
we're starting to see a lot of work or we've been seeing quite a bit of work around models.

279
00:29:53,120 --> 00:29:59,760
It is the idea that the consciousness prior is like the specific type of model that's analogous

280
00:29:59,760 --> 00:30:05,280
to what you would call our consciousness. So the connection with model-based reinforcement

281
00:30:05,280 --> 00:30:14,480
learning is the following. When you do model-free reinforcement learning, you train a policy,

282
00:30:14,480 --> 00:30:19,120
in other words, you train a neural net which is going to be called whenever you have to take a

283
00:30:19,120 --> 00:30:25,360
decision and it takes a decision and it's sort of automatic. So it's like when you're driving

284
00:30:25,360 --> 00:30:29,920
and a habitual route and you don't need to think about it, it knows what to do and there's no need

285
00:30:29,920 --> 00:30:36,640
to involve consciousness. But when you plan in your route on the fly, let's say there's some funny

286
00:30:36,640 --> 00:30:41,520
construction going on on your road and suddenly you realize that you're going to have to use a

287
00:30:41,520 --> 00:30:47,680
different path and you think about it. That is planning on the fly and that kind of planning on

288
00:30:47,680 --> 00:30:54,160
the fly is a form of reasoning and it's something that's conscious and that allows you to deal with

289
00:30:54,160 --> 00:31:00,960
as unexpected occurrences and that in principle is exactly what model-based reinforcement learning

290
00:31:00,960 --> 00:31:07,600
is about. So once you have a model of how the world works, you can create a new policy on the fly,

291
00:31:07,600 --> 00:31:12,240
something you've never done before. By combining the pieces of knowledge you already know, say about

292
00:31:12,240 --> 00:31:21,440
pieces of roads and so on, in order to come up with a new plan and that dynamic decision-making

293
00:31:21,440 --> 00:31:28,000
about the future and imagining the future in order to take decision in a very flexible way

294
00:31:28,000 --> 00:31:33,600
is very, very characteristic of conscious behavior and system two. So I think that at the end of the

295
00:31:33,600 --> 00:31:39,840
day we're going to have reinforcement learning that has both model-free elements and model-based

296
00:31:39,840 --> 00:31:45,120
elements because they both have their strengths. So earlier I referred to this idea of conscious

297
00:31:45,120 --> 00:31:54,560
as far as a memory. Is it strictly speaking memory in nature or is it more active? Does that question

298
00:31:54,560 --> 00:32:00,960
make sense? Yes, it's more active. So it's more about how you process information but the connection

299
00:32:00,960 --> 00:32:08,320
to memory is it's also connected to how we represent a lot of conscious knowledge. So the things

300
00:32:08,320 --> 00:32:14,640
that you can access in memory are conscious pieces of information. I mean they're not conscious

301
00:32:14,640 --> 00:32:20,480
until you retrieve them from memory but they can become conscious. And so there's a sense in which

302
00:32:20,480 --> 00:32:24,480
a lot of your conscious knowledge is stored in your long-term memory and then it could be retrieved

303
00:32:24,480 --> 00:32:32,640
as needed in order to solve problems that you're facing today. And conscious processing is dealing

304
00:32:32,640 --> 00:32:38,320
with the computations that your brain is doing to do these things, to retrieve things from memory,

305
00:32:38,320 --> 00:32:43,520
to interpret what you're seeing now and potentially also to visualize things that could happen

306
00:32:43,520 --> 00:32:50,240
in the future. I think that the memory analogy was that it was kind of the store of the relationships

307
00:32:50,240 --> 00:33:00,240
between the things. But the planning based on that information is not necessarily the same thing.

308
00:33:00,240 --> 00:33:07,440
That's right. In a sense there is declarative knowledge. So all the pieces of knowledge

309
00:33:07,440 --> 00:33:12,800
that are in memory and then there's the computation that you do on the fly in order to combine these

310
00:33:12,800 --> 00:33:20,240
pieces of knowledge with what is going on now or what you're imagining in order to come up with

311
00:33:20,240 --> 00:33:23,920
sometimes better explanations about the past or about what you'd like to do in the future.

312
00:33:24,800 --> 00:33:31,360
What's kind of the current state of this line of research and how do you see it evolving?

313
00:33:32,000 --> 00:33:40,160
Oh it's still in its infancy. I think how many decades it took for like Neonets and Deep Learning as

314
00:33:40,160 --> 00:33:47,440
it stands now to really reach the maturity that it has. So I think this is a long-term project

315
00:33:48,160 --> 00:33:56,880
and I also think that there's a huge importance in the collaboration between the brain sciences.

316
00:33:57,840 --> 00:34:03,920
That includes cognitive science, neuroscience, but also philosophers of mind which have been

317
00:34:03,920 --> 00:34:09,360
thinking about consciousness and mind for a long time. So all of these people who have been thinking

318
00:34:09,360 --> 00:34:16,800
about the human side of the equation should be collaborating with the people in AI and Deep Learning

319
00:34:18,000 --> 00:34:28,160
who are interested in understanding those principles and trying different ways of capturing

320
00:34:28,160 --> 00:34:34,560
these things in computers. And that can go also back in the other direction because one of the

321
00:34:34,560 --> 00:34:43,280
problems with say neuroscience or philosophy is that it's difficult to come up with good theories

322
00:34:43,280 --> 00:34:50,880
that explain the many observations that we have. But machine learning can come up with interesting

323
00:34:50,880 --> 00:34:55,360
theories because they are motivated from the learning theory point of view. In other words,

324
00:34:55,360 --> 00:35:00,960
so my conscious prior idea is something that makes sense from a machine learning perspective

325
00:35:00,960 --> 00:35:06,880
because as soon as you start making assumptions about the world, that means you could learn faster,

326
00:35:06,880 --> 00:35:13,440
you could adapt faster. And so these kinds of justification can help

327
00:35:14,560 --> 00:35:20,560
constrain theories that neuroscience or philosophers might be considering. So at the end of the day,

328
00:35:20,560 --> 00:35:26,800
be theories that both are consistent with what we know about humans. And that makes sense from

329
00:35:28,080 --> 00:35:34,480
a computational perspective, from a learning perspective. Before we got started talking about

330
00:35:34,480 --> 00:35:39,840
consciousness, we were actually before we started recording. We were talking a little bit about

331
00:35:40,480 --> 00:35:47,760
kind of what's going on in the world now as we record this mid-March and COVID-19. And one of the

332
00:35:47,760 --> 00:35:54,320
other topics that you've been spending some time working on is how machine learning and AI

333
00:35:54,320 --> 00:36:00,240
could make a difference in that setting. Can you share with us a little bit of what you're doing there?

334
00:36:00,240 --> 00:36:04,960
Yes, just put a bit of context. So I've been involved, of course, in basic research in machine

335
00:36:04,960 --> 00:36:10,400
learning and deep learning for many decades. But also in the last few years, I've realized the

336
00:36:10,400 --> 00:36:20,160
importance of thinking about how AI is deployed, will be deployed in society. And how we can steer

337
00:36:21,200 --> 00:36:28,000
our collective boat in directions that will make AI a useful force for the world and for humanity.

338
00:36:28,720 --> 00:36:35,600
So this is why I got, I'm barked, for example, in the project of writing the Montreal Declaration

339
00:36:35,600 --> 00:36:41,840
for Responsible Development of AI. This is why I'm involved in the project called AI Commons

340
00:36:41,840 --> 00:36:52,480
to try to help developers and NGOs and philanthropy work together on applying AI to areas that

341
00:36:53,600 --> 00:36:58,560
really AI for social good that may not be necessarily profitable but are important to do for

342
00:36:58,560 --> 00:37:05,200
humanity. This is also why I've been involved in the last couple of years in working on how AI could

343
00:37:05,200 --> 00:37:13,600
be used to fight climate change. So we wrote a very long paper that does a survey of many different

344
00:37:13,600 --> 00:37:22,480
areas in which machine learning can be used to help reducing greenhouse emissions, to design new

345
00:37:22,480 --> 00:37:28,880
materials, to better use the renewable energy sources that we have or to even help people understand

346
00:37:28,880 --> 00:37:33,200
better what is going on with climate change. And then, of course, in the last few weeks,

347
00:37:33,200 --> 00:37:41,680
like everyone else, I've been, you know, into this tornado to try to think with many of my

348
00:37:41,680 --> 00:37:48,720
colleagues, not just in AI, but also in other, especially in healthcare, thinking about what AI

349
00:37:48,720 --> 00:37:54,400
can do among many other disciplines that are, you know, putting their brains together. What can we

350
00:37:54,400 --> 00:38:01,920
do to help fight this COVID-19 pandemics? So I'm currently involved in a number of projects.

351
00:38:01,920 --> 00:38:10,160
One that is taking a lot of my time these days is the design of a tracing app. So one of the things

352
00:38:10,160 --> 00:38:18,000
that we can do to really find a good balance between saving lives and allowing people to go out of

353
00:38:18,000 --> 00:38:26,640
their homes is keeping track of where people go and who they meet so as to estimate their risks of

354
00:38:26,640 --> 00:38:32,960
being infected and reduce those risks. And we want to do it in a way that's very mindful of privacy,

355
00:38:33,600 --> 00:38:38,560
maybe unlike some of the apps that have already been put out there. This has come up quite a bit

356
00:38:38,560 --> 00:38:43,120
recently. Yeah, it's very important because you just use Bluetooth and make the information available

357
00:38:43,120 --> 00:38:49,840
to, you know, the benevolent government agencies. That's right. So I don't think it would pass

358
00:38:50,560 --> 00:38:55,280
in North America. And I think there are good reasons for this. But the good news is there are

359
00:38:55,280 --> 00:39:00,080
technical solutions to this. And we're working on that. And we're working on machine learning to

360
00:39:00,080 --> 00:39:05,520
help predict your risk level based on the encounters you had before. So, you know, maybe you didn't

361
00:39:05,520 --> 00:39:11,280
meet somebody, you didn't meet somebody who we knew was infected. But maybe you met somebody,

362
00:39:11,280 --> 00:39:15,120
who met somebody, who met somebody who was infected. So then what the probability that you are

363
00:39:15,120 --> 00:39:20,320
infected, right? And you've made 20 of these encounters. So maybe the risks accumulate. So how do you

364
00:39:20,320 --> 00:39:27,520
aggregate all that information and help people know what is their risk? Another thing we're working

365
00:39:27,520 --> 00:39:32,800
on is the design of new drugs. So machine learning and especially deep learning has been used

366
00:39:32,800 --> 00:39:38,560
in the last couple of years. There's been a flurry of papers using these systems to propose new

367
00:39:38,560 --> 00:39:44,000
candidate drugs. In particular, we're working on antivirals. In other words, drugs that you would

368
00:39:44,000 --> 00:39:48,320
give to somebody who's already sick. And maybe who's very sick and we're, you know, concerned that

369
00:39:48,320 --> 00:39:55,280
they might die. And so we could give them these experimental drugs because because these are

370
00:39:55,280 --> 00:40:01,600
new things. And so there's a problem where machine learning can come handy is that the normal,

371
00:40:01,600 --> 00:40:07,760
usual development of new drugs could take many years, sometimes a decade. But it turns out that

372
00:40:07,760 --> 00:40:13,520
machine learning can greatly accelerate the search for good molecules. When you're talking to clinicians

373
00:40:13,520 --> 00:40:19,440
and practitioners, folks from the healthcare, side of things, as well as, um,

374
00:40:20,160 --> 00:40:27,600
virologists and epidemiologists and, uh, kind of the end users of systems like these.

375
00:40:28,320 --> 00:40:34,000
What are the things that they say they need from, you know, us as a community of, um,

376
00:40:34,800 --> 00:40:39,280
you know, data scientists, machine learning, uh, researchers and the like.

377
00:40:39,280 --> 00:40:47,120
Well, it's not always easy to get that answer. I can tell you, I've been learning a lot,

378
00:40:47,120 --> 00:40:52,560
uh, in, in the last few weeks and, uh, I'm sure a lot of people have been learning a lot because

379
00:40:52,560 --> 00:40:59,120
we all need to understand what is going on. So, uh, there are many questions. On the healthcare

380
00:40:59,120 --> 00:41:04,720
and clinical side, they, they would like to be able to predict just monitor what is going on.

381
00:41:04,720 --> 00:41:11,920
Is, is already difficult. And, uh, predict, uh, where there might be greater need for healthcare

382
00:41:11,920 --> 00:41:18,720
resources to allocate resources properly, um, predict what patient, given their history,

383
00:41:18,720 --> 00:41:26,320
would be most likely to need like an ICU, uh, or a respirator, um, predict the risk level of people

384
00:41:26,320 --> 00:41:32,400
as, as I was mentioning earlier, uh, help, uh, epidemiologists model what, you know, what's

385
00:41:32,400 --> 00:41:38,240
going to be the likely, uh, scenarios that we're, we're going to be facing, even, even things like

386
00:41:38,240 --> 00:41:42,880
logistics. So a lot, there are a lot of problems right now where we're just not organized properly to

387
00:41:42,880 --> 00:41:49,440
deal with the massive number of people calling for help, right? So, uh, there's just lots and

388
00:41:49,440 --> 00:41:56,080
lots of areas where machine learning can be useful. And, uh, it takes a lot of time to understand

389
00:41:56,080 --> 00:42:03,120
those issues to talk to those people, uh, to get access to data as a big thing. But now the good

390
00:42:03,120 --> 00:42:10,080
news is, in our societies, access to healthcare data has been a huge problem, um, because we've put

391
00:42:10,080 --> 00:42:17,600
all of our, uh, weight on, uh, privacy. And, uh, it's, it has meant that it's been difficult to,

392
00:42:18,400 --> 00:42:22,080
for researchers, machine learning researchers to have access to this kind of data. But, but now what

393
00:42:22,080 --> 00:42:29,120
is happening with COVID is that the health authorities are seeing that, that we're missing a boat,

394
00:42:29,120 --> 00:42:35,040
like, or they're, you know, quickly changing the ways that we're doing things to allow researchers

395
00:42:35,040 --> 00:42:41,040
to get their hands on, on the proper data sets, or we're going to lose lives where, you know,

396
00:42:41,040 --> 00:42:47,600
we could have saved those lives. So I think it's an important moment to demonstrate the need for

397
00:42:47,600 --> 00:42:54,720
a more agile, uh, data infrastructure, uh, for healthcare. With regards to the app that you mentioned,

398
00:42:54,720 --> 00:42:59,520
we've talked quite a bit on the podcast about differential privacy. Does that come into play,

399
00:43:00,320 --> 00:43:05,200
uh, in allowing you to use this location data in a, in a private way?

400
00:43:06,160 --> 00:43:11,520
We're looking at different options. Right now, I don't think we need differential privacy. So we,

401
00:43:11,520 --> 00:43:15,920
we may need to blur some of some pieces of evidence that would make it too easy to reach race

402
00:43:15,920 --> 00:43:21,200
people. But if you want to predict the risk level of a person, otherwise predict the probability

403
00:43:21,200 --> 00:43:28,800
that you are currently infected, um, you don't need to know where I was. Um, you all need to know that,

404
00:43:29,840 --> 00:43:36,320
uh, you know, maybe yesterday I, I was close to somebody who had risk level six and the day before

405
00:43:36,320 --> 00:43:43,520
I was close to somebody who had risk level seven and, and so on. And you don't need to know, uh,

406
00:43:43,520 --> 00:43:48,720
the kind of trace of where everyone was, that can be computed in each person's phone.

407
00:43:50,320 --> 00:43:57,840
The only thing you need to share as data for training the, the risk predictor is what were the

408
00:43:57,840 --> 00:44:02,320
encounters, like in the sense of what were the risk level of people you're encountered and when,

409
00:44:03,600 --> 00:44:08,400
and that, from that information is very difficult to trace who met with whom because you don't,

410
00:44:08,400 --> 00:44:15,760
you don't have a handle on who was where when. So, so I think that the, you can, you, we could

411
00:44:15,760 --> 00:44:22,000
globally share the level of the planet, that kind of data and deal very good models of your, um,

412
00:44:22,000 --> 00:44:28,800
risk level. You know, when you look across all of the things that are happening, you know,

413
00:44:28,800 --> 00:44:37,440
your research and, uh, elsewhere, what are the things that, um, you see as most exciting in terms

414
00:44:37,440 --> 00:44:48,240
of AI's contributions of this fight? Wow, I guess I'm very biased. So, uh, I'm, I'm most invested

415
00:44:48,240 --> 00:44:56,160
right now in this tracing thing because, um, I think that, uh, medical treatments are going to take

416
00:44:56,160 --> 00:45:03,600
months, probably, you know, your ish two years, for some cases to converge to something everyone

417
00:45:03,600 --> 00:45:09,440
can have, especially vaccines. You have to understand that before we release a vaccine, uh,

418
00:45:09,440 --> 00:45:13,040
we need to make sure this drug is really, really harmless because we're going to give it to

419
00:45:13,040 --> 00:45:18,960
everybody, right? So, right, right. And that process takes a while, even if it's, even if it's

420
00:45:18,960 --> 00:45:23,200
fast track, it takes a while. No, but like, if one percent of the people we give a vaccine to die

421
00:45:23,200 --> 00:45:31,120
because of the vaccine, that is not good, okay? Right. But a antiviral is a drug you could give

422
00:45:31,120 --> 00:45:36,160
to somebody who's already close to dying anyways. And so, we can take a bigger chance. And so,

423
00:45:36,160 --> 00:45:42,160
we don't need to wait a year or two, uh, to know that it's okay. And so, I'm, I'm invested in

424
00:45:42,160 --> 00:45:49,840
the, uh, project around, uh, the development of new antivirals. Um, I mean, uh, before that,

425
00:45:49,840 --> 00:45:54,960
there are already been work in using machine learning to test existing drugs. This is like the

426
00:45:54,960 --> 00:45:59,840
first line is there a drug that has already been approved. So, we don't need clinical trials.

427
00:45:59,840 --> 00:46:04,720
Uh, that we can just use tomorrow morning. Okay. And that's what we're going on. There's a bunch

428
00:46:04,720 --> 00:46:08,960
of clinical trials going on with promising leads and machine learning has already been used to

429
00:46:08,960 --> 00:46:14,480
suggest candidates. But the next step, if, you know, if none of these things work is to develop, uh,

430
00:46:14,480 --> 00:46:21,840
uh, a new molecule that didn't exist. Right. So, so the tracing thing is very important because it,

431
00:46:21,840 --> 00:46:27,440
it's, uh, it's something we can do even before all this, right? Even before we find which,

432
00:46:27,440 --> 00:46:34,000
uh, those, those drugs and, and, and do clinical trials for them, we should be able, in a matter of

433
00:46:34,800 --> 00:46:44,000
days and weeks, to all have on our phone, an app that will help trace our contacts, uh, in a,

434
00:46:44,000 --> 00:46:50,400
in a private sea respecting way, um, and make it possible for us to meet, for example, other people

435
00:46:50,400 --> 00:46:54,560
and know that they're unlikely to be infected. And so it's okay. We can be close to each other.

436
00:46:54,560 --> 00:46:59,040
We can work together. Uh, we can be in the same bus together. This is very important. A lot of

437
00:46:59,040 --> 00:47:03,840
people right now don't have any transportation because if you don't have a car or if you don't drive

438
00:47:04,560 --> 00:47:09,280
and you're infected, for example, you, you can't have transportation. So like, we need to know

439
00:47:10,320 --> 00:47:16,720
who is likely to be infected and not or at one degree in order to organize society around this

440
00:47:16,720 --> 00:47:23,840
for the next probably one or two years. Well, super important work. Thank you so much for

441
00:47:23,840 --> 00:47:30,160
taking a time to chat with us about what you're up to, both the work that you've been focusing on

442
00:47:30,160 --> 00:47:36,000
broadly, the consciousness work, as well as the more recent work you've been doing, uh, in this

443
00:47:36,000 --> 00:47:41,200
fight against COVID. My pleasure. It's great to speak with you. Thank you. Bye.

444
00:47:45,600 --> 00:47:50,640
All right, everyone. That's our show for today. For more information on today's show,

445
00:47:50,640 --> 00:47:58,400
visit twomolai.com slash shows. As always, thanks so much for listening and catch you next time.

