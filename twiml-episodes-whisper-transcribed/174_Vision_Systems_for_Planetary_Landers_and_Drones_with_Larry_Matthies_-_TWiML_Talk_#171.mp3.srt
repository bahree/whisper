1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,480
I'm your host Sam Charrington. Today we're joined by Larry Mathees, senior research scientist

4
00:00:34,480 --> 00:00:40,280
and head of computer vision in the mobility and robotics division at JPL.

5
00:00:40,280 --> 00:00:45,600
Larry joins us on the heels of two presentations at this year's CVPR conference, the first

6
00:00:45,600 --> 00:00:52,080
on onboard stereo vision for drone pursuit or sense and avoid and another on vision systems

7
00:00:52,080 --> 00:00:57,960
for planetary landers. In our conversation we touch on both of these talks, his work on

8
00:00:57,960 --> 00:01:04,120
vision systems for the first iteration of Mars Rovers in 2004 and the future of planetary

9
00:01:04,120 --> 00:01:07,160
landing projects. Enjoy.

10
00:01:07,160 --> 00:01:12,560
Alright everyone, I am on the line with Larry Mathees. Larry is a senior research scientist

11
00:01:12,560 --> 00:01:17,920
at JPL and head of the computer vision group there within the mobility and robotic system

12
00:01:17,920 --> 00:01:23,080
section, as well as an adjunct professor in computer science at the University of Southern

13
00:01:23,080 --> 00:01:26,760
California. Larry, welcome to this weekend machine learning in AI.

14
00:01:26,760 --> 00:01:28,800
Thank you. It's a pleasure to be here.

15
00:01:28,800 --> 00:01:31,680
Let's get started by having you tell us a little bit about your background and how you

16
00:01:31,680 --> 00:01:36,120
got started working on vision for intelligent systems.

17
00:01:36,120 --> 00:01:43,320
Sure, I actually grew up in Canada. I studied computer graphics for a master's degree and

18
00:01:43,320 --> 00:01:50,520
then I went to Carnegie Mellon for a PhD. I was interested in artificial intelligence.

19
00:01:50,520 --> 00:01:59,480
Back in 1981 I spent a year so as a student of some of the AI faculty there and I felt

20
00:01:59,480 --> 00:02:07,080
that in those days the theoretical underpinnings weren't quite what I was satisfied by so I

21
00:02:07,080 --> 00:02:16,040
ended up working on computer vision for robotics which to me had a nice combination of solid

22
00:02:16,040 --> 00:02:24,840
underpinnings in math and physics and some of the appeal of AI and the kind of the

23
00:02:24,840 --> 00:02:30,440
visual gratification of computer graphics. It was a nice hybrid.

24
00:02:30,440 --> 00:02:36,680
How long have you been at JPL? Since 1989 so I think that's almost 29 years.

25
00:02:36,680 --> 00:02:41,360
Wow. What's your research focus there at the lab?

26
00:02:41,360 --> 00:02:46,480
I've been working on computer vision for autonomous navigation of unmanned vehicles. That's

27
00:02:46,480 --> 00:02:52,760
been our main focus the whole time. Specific activities have shifted. I started working

28
00:02:52,760 --> 00:02:58,040
on autonomous navigation of ground vehicles and pretty much the whole time I've been here.

29
00:02:58,040 --> 00:03:04,920
I've spent roughly half time working on NASA related projects and half time working

30
00:03:04,920 --> 00:03:11,200
on non-NASA projects. Most of our funding here comes from government sources so the non-NASA

31
00:03:11,200 --> 00:03:17,440
work has been mostly places like DARPA and the Army Research Lab. I worked on ground

32
00:03:17,440 --> 00:03:25,720
vehicles. Then I started trying to address landers and orbiters on the NASA side. I always

33
00:03:25,720 --> 00:03:31,400
tried to compliment what we do on the NASA side with related things on the non-NASA side.

34
00:03:31,400 --> 00:03:37,880
In the early 2000s I started working on vision for drones along the way we've done work

35
00:03:37,880 --> 00:03:42,400
on vision for autonomous navigation. Those have been my main activities.

36
00:03:42,400 --> 00:03:50,280
You recently returned from the CVPR conference in Salt Lake City where you presented as part

37
00:03:50,280 --> 00:03:57,480
of a workshop on visual odometry and computer vision based on location clues. That was the

38
00:03:57,480 --> 00:04:03,560
title of the workshop. Can you parse that out for us and what is visual odometry and

39
00:04:03,560 --> 00:04:07,080
what's the role of the location queues and all that?

40
00:04:07,080 --> 00:04:14,960
Visual odometry is a technique used to estimate the motion of a camera. Usually that camera

41
00:04:14,960 --> 00:04:20,040
is attached to something else you care about. In my business that's usually a camera on

42
00:04:20,040 --> 00:04:26,560
the robot. You're using the camera to track nearby features in the environment to estimate

43
00:04:26,560 --> 00:04:31,960
the incremental motion of the vehicle. It's called visual odometry as an analogy to

44
00:04:31,960 --> 00:04:37,800
more traditional forms of odometry based on integrating fuel encoders. It's a visual

45
00:04:37,800 --> 00:04:46,040
dead reckoning. The part about location clues. Visual odometry integrates forward from

46
00:04:46,040 --> 00:04:53,000
some starting point and it drifts in robotics and related applications you often want to

47
00:04:53,000 --> 00:04:58,200
have an estimate of absolute position and where the system is. Visual odometry doesn't

48
00:04:58,200 --> 00:05:07,640
do that. You need to have some other absolute reference and location queues could come from

49
00:05:07,640 --> 00:05:15,160
basically maps to give you estimates of absolute position. In this field there's a buzzword

50
00:05:15,160 --> 00:05:20,440
simultaneous localization and mapping which is all about maintaining the history of where

51
00:05:20,440 --> 00:05:27,680
you've been and updating that whole history. If you should double back and see places

52
00:05:27,680 --> 00:05:33,600
you've been before, that's called loop closure and then you can improve your estimate of

53
00:05:33,600 --> 00:05:38,680
the whole history. Although that loop closure doesn't necessarily tell you where you are

54
00:05:38,680 --> 00:05:44,040
absolutely in the world. It tells you that you're back to some place you've been before

55
00:05:44,040 --> 00:05:53,680
which is often just about as important. How do visual odometry and slam relate to or

56
00:05:53,680 --> 00:05:59,840
visual odometry in particular I guess? How does that relate to pose estimation?

57
00:05:59,840 --> 00:06:07,760
Pose estimation refers to the six degree of freedom, position and orientation of a system

58
00:06:07,760 --> 00:06:16,480
so the camera. Visual odometry is estimating the pose of the camera relative to some previous

59
00:06:16,480 --> 00:06:23,480
reference frame every time you take a picture. Pose estimation is basically part of visual

60
00:06:23,480 --> 00:06:33,880
odometry and so your talk was on vision systems for planetary landers. Can you walk us through

61
00:06:33,880 --> 00:06:42,640
the focus of your talk? Sure. In planetary landers so by planetary we're referring to a lot

62
00:06:42,640 --> 00:06:48,640
of our work at JPL has been focused on landing on Mars. We did some work in the last decade

63
00:06:48,640 --> 00:06:57,920
on techniques for precision landing and landing hazard avoidance on the moon and now we're

64
00:06:57,920 --> 00:07:05,240
working to extend these techniques to be applicable to other places like Jupiter's Moon, Europa,

65
00:07:05,240 --> 00:07:14,800
Saturn's Moon, Titan, Comments and Asteroids. In landing you want to, kind of a progression

66
00:07:14,800 --> 00:07:19,960
of capabilities that we worked on. So you want to have a good estimate of the terrain

67
00:07:19,960 --> 00:07:25,160
relative velocity. So in particular you know the vertical velocity and then the horizontal

68
00:07:25,160 --> 00:07:31,120
velocity those have to be within strict limits that touch down or you have a failed landing.

69
00:07:31,120 --> 00:07:36,000
That doesn't tell you where you are. It tells you what your velocity is. Next step is

70
00:07:36,000 --> 00:07:42,560
you'd like to know where you are. So that's the precision landing part. For that you

71
00:07:42,560 --> 00:07:46,520
need, you know, we're saying earlier visual geometry doesn't tell you in absolute terms

72
00:07:46,520 --> 00:07:51,640
where you are. For precision landing on planetary bodies we want to know in absolute terms

73
00:07:51,640 --> 00:08:00,200
where we are. So we have to have some external reference what has been the most approachable.

74
00:08:00,200 --> 00:08:06,400
Often we have orbiters that take high resolution imagery from orbit of places that we want

75
00:08:06,400 --> 00:08:14,160
to land. And so we've been developing algorithms that we use a downlooking camera on the spacecraft

76
00:08:14,160 --> 00:08:18,320
as it gets close to the surface to take pictures and then we register those pictures with

77
00:08:18,320 --> 00:08:23,400
the orbital imagery to get position updates. And we can use visual geometry as part of

78
00:08:23,400 --> 00:08:30,800
that to track features with this camera during descent to improve the velocity knowledge.

79
00:08:30,800 --> 00:08:37,520
So that complements the map registration to give us position knowledge. And then for places

80
00:08:37,520 --> 00:08:43,440
like Mars where we've got really, really good orbital imagery. So resolution of pixels

81
00:08:43,440 --> 00:08:48,440
on the ground of 25 to 30 centimeters per pixel which is good in this business. We can

82
00:08:48,440 --> 00:08:54,320
see almost all of the landing hazards from orbit before the lander ever gets there.

83
00:08:54,320 --> 00:08:59,320
So we don't really have to do onboard landing hazard detection for Mars. At least not

84
00:08:59,320 --> 00:09:03,800
anything we need you to do so far because we can map the hazards before we get there.

85
00:09:03,800 --> 00:09:09,000
And then we just as long as we know where we are during descent, we can plan a trajectory

86
00:09:09,000 --> 00:09:13,800
to a landing site that we already know is fairly safe. For other places that we want to

87
00:09:13,800 --> 00:09:19,360
go that aren't as well mapped, we will need onboard landing hazard detection. So this

88
00:09:19,360 --> 00:09:26,920
was germane to the focus of the workshop because localization visual geometry are important

89
00:09:26,920 --> 00:09:30,640
in planetary landers and that was the theme of the workshop.

90
00:09:30,640 --> 00:09:37,600
How long have we been using vision-based systems in as part of our approach to trying to

91
00:09:37,600 --> 00:09:40,400
land things on other planets?

92
00:09:40,400 --> 00:09:48,080
So the first use of vision in real time in landing was in the Mars exploration rover mission

93
00:09:48,080 --> 00:09:55,080
so that put the rover two rovers on Mars in basically January of 2004. Those rovers

94
00:09:55,080 --> 00:10:01,680
were called Spirit and Opportunity. So we had a system on that. So those rovers landed

95
00:10:01,680 --> 00:10:08,160
with airbags which were deployed after a parachute phase. So we put a system on that mission

96
00:10:08,160 --> 00:10:14,120
that estimated horizontal velocity in the last two kilometers of descent to the surface

97
00:10:14,120 --> 00:10:19,320
so that that velocity knowledge could be used as part of Redfer Rocket firing logic to

98
00:10:19,320 --> 00:10:24,000
reduce the horizontal velocity to make sure that we get inverse the airbags.

99
00:10:24,000 --> 00:10:28,960
We used airbags for the Mars Pathfinder mission in 1997, very successfully without these

100
00:10:28,960 --> 00:10:36,040
techniques. But the rovers by 2003, 2004 were quite a bit heavier and we learned some

101
00:10:36,040 --> 00:10:41,280
things about the winds on Mars that we didn't know before that gave us some concerns.

102
00:10:41,280 --> 00:10:47,840
So we added this technique to add, given added measure of safety. So that was the first

103
00:10:47,840 --> 00:10:53,120
time vision was used in a planetary landing system in real time onboard.

104
00:10:53,120 --> 00:10:59,040
Tell us a little bit about the general approach that you take to solving these types of problems.

105
00:10:59,040 --> 00:11:04,440
From our conversation before getting started, it sounds like you tend to focus on model

106
00:11:04,440 --> 00:11:11,160
based approaches as opposed to deep learning or data driven approaches.

107
00:11:11,160 --> 00:11:18,880
Well, so in doing computer vision for robotics, which is what this is, and especially what

108
00:11:18,880 --> 00:11:28,680
we do at JPL is enable missions. And so it's applied research. And you've always, so it's

109
00:11:28,680 --> 00:11:34,640
applied research in contrast with basic research at universities. So we've always got a mission

110
00:11:34,640 --> 00:11:41,160
in mind. We've all with those missions always have schedule and budget constraints and

111
00:11:41,160 --> 00:11:46,600
the people who build those missions tend to be a little risk averse.

112
00:11:46,600 --> 00:11:53,320
So what we always need to do is take advantage of anything that can give us advantage.

113
00:11:53,320 --> 00:11:59,880
And so that inherently leads us to multi-sensor approaches. So there's been a tendency, at

114
00:11:59,880 --> 00:12:05,800
least there was for a number of years in in academic computer vision research to study

115
00:12:05,800 --> 00:12:11,960
how much can you do with only a camera. But in our business, you know, there's always

116
00:12:11,960 --> 00:12:18,200
an inertial measurement unit that gives you estimates of velocities and accelerations

117
00:12:18,200 --> 00:12:24,080
in angular rates. So you want to use that. And if you have prior knowledge, you want

118
00:12:24,080 --> 00:12:30,600
to use that. So we take advantage of all of those things. And then in the space business,

119
00:12:30,600 --> 00:12:37,560
space-based computing is much, much less powerful than what we can do on earth. So we're talking

120
00:12:37,560 --> 00:12:43,040
three to four orders of magnitude. So you have to squeeze a lot of capability in a very

121
00:12:43,040 --> 00:12:48,000
small amount of compute. And we're going to places that we've either never been before

122
00:12:48,000 --> 00:12:52,800
or we don't have much, we don't have much, we have some, but not much prior data on.

123
00:12:52,800 --> 00:12:57,280
So it can be hard to do learning-based approaches because you just don't have data to learn from

124
00:12:57,280 --> 00:13:03,560
unless you get into transfer learning and things that are probably a little bit too cutting

125
00:13:03,560 --> 00:13:11,680
edge to put in a space vision yet. So we've tended to rely on a lot of geometry because

126
00:13:11,680 --> 00:13:16,880
that, you know, we've got the solid modeling foundation to do that. You know, what we need

127
00:13:16,880 --> 00:13:23,640
for precision landing and landing hazard avoidance is geometry anyway. We need the geometry

128
00:13:23,640 --> 00:13:28,400
of the terrain, we need the position of the spacecraft, the velocity of the spacecraft.

129
00:13:28,400 --> 00:13:34,160
So that's not necessarily strictly geometry, but it's related. It's all with sensors that

130
00:13:34,160 --> 00:13:41,680
are noisy. So this is fundamentally navigation. You know, the whole history of navigation

131
00:13:41,680 --> 00:13:47,320
research and development has always paid attention to modeling the noise and the sensors

132
00:13:47,320 --> 00:13:54,880
and propagating that into uncertainty in your state estimates using that uncertainty

133
00:13:54,880 --> 00:14:02,320
when the state estimates in planning and control. So we levered all of those techniques.

134
00:14:02,320 --> 00:14:06,640
And you know, where we can and where it makes sense, we're trying to use state-of-the-art

135
00:14:06,640 --> 00:14:12,200
learning methods. So those in particular applied to rovers. Once we're on the ground, we want

136
00:14:12,200 --> 00:14:17,640
to do terrain classification. And when we've had rovers driving in areas that we've seen

137
00:14:17,640 --> 00:14:24,040
from orbit and the rovers, you know, by now on Mars, we've accumulated, you know, several

138
00:14:24,040 --> 00:14:28,640
tens of kilometers of traverse. So we've got a lot of images. So we can now start to

139
00:14:28,640 --> 00:14:35,040
train classifiers. So if the rovers understand whether they're on bedrock or on sand, how

140
00:14:35,040 --> 00:14:41,880
much they might slip, how much they might sink. So we've kind of progressed from, you know,

141
00:14:41,880 --> 00:14:48,480
model based techniques that exploit multi-sensor fusion with very limited computation and very

142
00:14:48,480 --> 00:14:53,840
limited prior data to scenarios where we do have more prior data. And we can see in the

143
00:14:53,840 --> 00:14:58,680
future there are things in the works that we hope would give us much more computing power

144
00:14:58,680 --> 00:15:03,920
in the next decade and be able to use more sophisticated techniques on board.

145
00:15:03,920 --> 00:15:09,600
Can you talk a little bit about the more traditional approach where you're incorporating this

146
00:15:09,600 --> 00:15:15,440
multi-sensor fusion? What are the key research challenges there? What has been the, how would

147
00:15:15,440 --> 00:15:24,360
you characterize the progression of research in that field over the past few years? Ah,

148
00:15:24,360 --> 00:15:32,360
well, you know, in all of computer vision and robotics, we've been helped tremendously

149
00:15:32,360 --> 00:15:38,720
by things that basically came into the field from outside. So, you know, it's cliche to

150
00:15:38,720 --> 00:15:45,080
say that advances in computing power and miniaturization and, you know, more compute for less mass

151
00:15:45,080 --> 00:15:50,440
power volume and cost has been hugely enabling, but it's true. Same thing goes for inertial

152
00:15:50,440 --> 00:15:57,240
sensors. Same thing goes for cameras. So the whole, you know, the invention of CMOS

153
00:15:57,240 --> 00:16:07,760
imagers, which was a big step in miniaturizing cameras and power for cameras. But the,

154
00:16:07,760 --> 00:16:13,800
you know, revolutionary progress in mobile electronics for consumer applications has spun

155
00:16:13,800 --> 00:16:21,360
off effectively, tremendously valuable sensors, processors, and communication hardware that

156
00:16:21,360 --> 00:16:28,320
we can use in robotics. So that's one thing. Within our field, you know, the field

157
00:16:28,320 --> 00:16:34,000
ensured a lot in the time that I've been in it. So, last 30 years, 40 years actually

158
00:16:34,000 --> 00:16:42,880
including grad school, in using, you know, initially common filters and incremental batch

159
00:16:42,880 --> 00:16:51,280
estimation techniques, incremental bundle adjustments. So the whole simultaneous localization

160
00:16:51,280 --> 00:17:00,880
at mapping or SLAM, a literature has developed over the last 35 years or so. 3D perception

161
00:17:00,880 --> 00:17:12,640
has gone from, you know, being hardly possible in the early 80s to being, you know, routine

162
00:17:12,640 --> 00:17:19,320
now. So progress and algorithms for stereo vision that actually work and that are affordable

163
00:17:19,320 --> 00:17:27,680
progress in other 3D sensors, LIDAR, in particular, being small and compact and affordable.

164
00:17:27,680 --> 00:17:33,440
That's made a huge difference. So in what I do for planetary exploration, there's not

165
00:17:33,440 --> 00:17:40,960
a lot of object recognition per se, but that's really where one of the most important research

166
00:17:40,960 --> 00:17:47,760
thrusts these days is in applications on Earth. And so that's an area where people tried

167
00:17:47,760 --> 00:17:55,080
more model-based methods for years and that's all becoming very learning-based. So the deep

168
00:17:55,080 --> 00:17:59,560
learning revolution is having a big impact and how people approach those problems.

169
00:18:00,360 --> 00:18:09,720
You mentioned stereo perception and the use of, I guess, image-based approaches to stereo

170
00:18:09,720 --> 00:18:15,720
perception as opposed to point clouds like LIDAR. Can you talk a little bit about that process

171
00:18:15,720 --> 00:18:20,200
and how that works? I haven't thought too much about that or come across and kind of curious

172
00:18:20,200 --> 00:18:27,480
about that. Okay, so I was referring to, let me call it 3D perception. So 3D perception gives

173
00:18:27,480 --> 00:18:34,920
you a point cloud. LIDARs do that. There's multiple LIDAR technology, but think of it as

174
00:18:34,920 --> 00:18:41,960
kind of light-ranging. So you emit a short pulse of laser light and it reflects back to a detector,

175
00:18:41,960 --> 00:18:47,240
you measure that time of light that gives you range. stereo vision gives you a point cloud

176
00:18:47,240 --> 00:18:52,200
by using two cameras. And you know the relative positions of those two cameras. And if you

177
00:18:52,200 --> 00:18:57,960
can find the same, the image of the same object in both of those cameras, then you can triangulate

178
00:18:57,960 --> 00:19:03,560
where that object is in 3D dimension. So it's just like surveyors. Where surveyors, you know,

179
00:19:03,560 --> 00:19:08,200
look at the same point in the world from different locations and then they can triangulate where

180
00:19:08,200 --> 00:19:15,080
that point is. You do that in stereo vision by having two cameras and software that for every pixel

181
00:19:15,080 --> 00:19:20,760
in, let's say, the left image you find in the right image, the pixel that has the projection

182
00:19:20,760 --> 00:19:24,760
of the same object in the world. And so if you do that at every pixel, then you build up what we

183
00:19:24,760 --> 00:19:31,880
call a depth map and you triangulate. And that gives you a point cloud that is essentially

184
00:19:31,880 --> 00:19:39,240
the same kind of data you get from LIDAR. It's got different noise characteristics. There are

185
00:19:39,880 --> 00:19:44,520
strengths and weaknesses for each approach, but they're both methods to give you point clouds.

186
00:19:44,520 --> 00:19:52,200
Okay, okay. LIDAR is much more expensive than cameras are and yet it's being used on a lot

187
00:19:52,200 --> 00:19:59,800
of autonomous vehicles. Can you maybe talk about some of the relative weaknesses of the stereo

188
00:19:59,800 --> 00:20:08,520
vision based approach? Well, so stereo vision, to do this image match, you need to have some

189
00:20:08,520 --> 00:20:15,320
visual texture because that's what helps you find the same point in both cameras. So if you're in

190
00:20:15,320 --> 00:20:22,600
an environment where you've got a lot of surfaces with very little texture, it's hard to do stereo

191
00:20:22,600 --> 00:20:30,920
vision based range measurement. So think of indoor walls that are painted with very little texture

192
00:20:30,920 --> 00:20:39,800
or outdoors if you're driving on a road that has nice, clean smooth asphalt or concrete that's

193
00:20:39,800 --> 00:20:45,080
not very textured. The only way you can get a range measurement to the middle of those surfaces

194
00:20:45,080 --> 00:20:50,920
with stereo vision is by assuming there's a continuous surface and you basically interpolate

195
00:20:50,920 --> 00:20:58,920
into there. Whereas with the LIDAR, you're admitting LIDAR directly, you need the surface to have

196
00:20:58,920 --> 00:21:06,280
enough reflectivity to bounce that light back to the sensor. What you don't need any texture

197
00:21:06,280 --> 00:21:12,120
per se on the surface and because the LIDAR is admitting its own energy, it actually works better

198
00:21:12,120 --> 00:21:18,920
in the dark than it does during daylight. A stereo vision with typical low cost visible

199
00:21:18,920 --> 00:21:25,560
spectrum cameras, it needs an illuminator to work at night, that illuminator has limited range,

200
00:21:25,560 --> 00:21:32,360
so the new stereo vision system has limited range. The error as a function of distance,

201
00:21:33,000 --> 00:21:39,080
the error in stereo vision based range estimates grows as the square of the distance, whereas in LIDAR,

202
00:21:40,920 --> 00:21:46,600
there's tends to be an analogous error characteristic, but you can design a system to have

203
00:21:46,600 --> 00:21:51,160
typically greater range than a stereo system. On the other hand, as you noted,

204
00:21:51,160 --> 00:21:58,920
the stereo cameras can be quite small and cheap, and what they do for you is they give you,

205
00:21:58,920 --> 00:22:05,160
you get all of your pixels at the same point in time, and you can have a field of view that's

206
00:22:05,160 --> 00:22:10,680
quite large in a horizontal and vertical axis. So you can have a large field of regard,

207
00:22:10,680 --> 00:22:14,920
you can get all of your data simultaneously, which matters if you're moving, because if you're

208
00:22:14,920 --> 00:22:21,000
moving and each pixel comes in at a different point in time, that adds complexity of how do

209
00:22:21,000 --> 00:22:25,800
you relate that all to, if you're trying to build a map of the world and your data is coming in,

210
00:22:25,800 --> 00:22:30,520
it's like in a different point in time, that's a harder problem. Most LIDARs have been scanning

211
00:22:30,520 --> 00:22:37,320
LIDARs, and so they have that issue of motion registration of the pixels. Their LIDARs

212
00:22:37,320 --> 00:22:42,840
called flash LIDARs that give you all of your pixels in the point cloud at the same point in time,

213
00:22:42,840 --> 00:22:49,240
but they typically have narrower fields of regard. So those are some of the trade-offs between the two

214
00:22:49,240 --> 00:22:58,040
sensors. And you also mentioned SLAM a couple of times. How does that tend to work? Well, if you mean

215
00:22:58,040 --> 00:23:03,080
does it work well or poorly, these days it works well, if you need to have to talk with the work

216
00:23:04,120 --> 00:23:14,520
of the latter. Okay, these are all fundamentally least squares problems. So you're setting up

217
00:23:14,520 --> 00:23:22,120
an optimization where you've got measurements, so you've got a set of unknowns. Your unknowns

218
00:23:22,120 --> 00:23:29,800
are the poses for the camera from each picture and the 3D coordinates of all of your landmarks.

219
00:23:30,520 --> 00:23:38,360
So this whole technique has some roots in the field of photogrammetry, which is older than

220
00:23:38,360 --> 00:23:45,960
computer vision. And that was used for aerial surveying for long before computer vision had become

221
00:23:45,960 --> 00:23:53,080
big. And in that community, they called this technique of setting up a large least squares

222
00:23:53,080 --> 00:23:58,680
optimization for all the camera poses and all of the landmarks. They called that bundle adjustment

223
00:23:58,680 --> 00:24:05,560
because they thought of it as a bundle of rays from the camera out to all of the landmarks. And so

224
00:24:05,560 --> 00:24:11,160
your optimization is defined, you know, the positions of all these things that adjust this

225
00:24:11,160 --> 00:24:21,400
bundle of rays to minimize some error measure. So SLAM is basically another term for that kind of

226
00:24:21,400 --> 00:24:28,040
technique. And there's been a lot of progress by, you know, a number of people around the world

227
00:24:28,040 --> 00:24:36,280
in finding ways to do this very computationally, efficiently, incrementally. So historically,

228
00:24:36,280 --> 00:24:42,040
in photogrammetry, people would basically do this offline. They would take all of their measurements,

229
00:24:42,280 --> 00:24:47,880
process them offline, all at once, and give you your map. But a lot of what we want to do. So

230
00:24:47,880 --> 00:24:53,800
in photogrammetry, they came up with this term that I think they called real-time bundle adjustment

231
00:24:53,800 --> 00:24:59,800
where, you know, suppose you take one new picture. Can you add that to your optimization and update

232
00:24:59,800 --> 00:25:05,640
it efficiently? So you get a new map without having to redo all of that computation. So the progress

233
00:25:05,640 --> 00:25:13,480
in SLAM has been finding basically linear algebra tricks to do that more and more efficiently over time.

234
00:25:13,480 --> 00:25:21,160
So that we can, you know, in real-time, take new data and update this network of camera poses and

235
00:25:21,160 --> 00:25:27,320
landmark positions. And it's, you know, it's used more and more sophisticated basically linear

236
00:25:27,320 --> 00:25:33,960
algebra tricks to make it efficient to do this incrementally. So maybe going back to your talk

237
00:25:33,960 --> 00:25:42,120
on the vision systems, your goal was to provide a review of the progress that's been made there,

238
00:25:42,120 --> 00:25:50,920
and some of the challenges that the field has run into and perhaps remain. How far did we

239
00:25:50,920 --> 00:25:56,200
get in this conversation in terms of what you presented there? Are there other pieces that you

240
00:25:56,200 --> 00:26:02,040
can share with us or did we cover the bulk of your talk there? I can add a few tidbits.

241
00:26:03,080 --> 00:26:09,400
So, you know, we talked about how the first vision system, real-time vision system in a planetary

242
00:26:09,400 --> 00:26:17,880
lander was in the Mars exploration rover mission that landed in 2004. We're working on another

243
00:26:17,880 --> 00:26:25,800
rover mission to Mars now that we expect will launch in 2020. And the plan is to have a

244
00:26:25,800 --> 00:26:31,480
precision landing capability in that mission where we do what I was describing, where we have a

245
00:26:31,480 --> 00:26:36,920
downward-looking camera that takes pictures during the sand and onboard and real-time and registers

246
00:26:37,560 --> 00:26:43,480
templates from those pictures to orbital maps. So we're planning to use that in the 2020

247
00:26:43,480 --> 00:26:49,640
rover mission to Mars, so that'll be the first time that's been done. And then looking beyond that,

248
00:26:51,640 --> 00:26:59,800
we are about NASA's interested, very interested in two-pairs moving Europa these days. So we're

249
00:26:59,800 --> 00:27:05,960
studying possibilities for how we land on Europa. So I need to be careful to say there isn't

250
00:27:05,960 --> 00:27:12,600
a fund of mission to do this, but we're studying it and the reason to go there is, you know,

251
00:27:12,600 --> 00:27:19,640
profound scientific questions involved. One of them is, you know, is their life elsewhere

252
00:27:19,640 --> 00:27:26,200
than on Earth? And if there isn't life, are there chemical precursors, you know, kind of on a path

253
00:27:26,200 --> 00:27:31,960
toward the chemistry of life? And we know that Jupiter has a liquid water ocean underneath

254
00:27:31,960 --> 00:27:44,600
a water ice crust, and that kept liquid by gravitational flexing in the graph of the Europa

255
00:27:44,600 --> 00:27:50,280
between the gravity field of Jupiter and some of the other larger moons further away from Jupiter

256
00:27:50,280 --> 00:27:57,080
than Europa. And we can see the effects of that in the surface of Europa, which is all broken up,

257
00:27:57,080 --> 00:28:04,360
basically like ice flows. So the question is, you know, in that liquid water ocean inside

258
00:28:04,360 --> 00:28:12,280
of Europa, could there be chemical processes that are related to the chemical processes of life,

259
00:28:12,280 --> 00:28:20,280
or maybe even microbial life? And that's all very speculative, but we know that one of Saturn's

260
00:28:20,280 --> 00:28:27,480
moons and celadus, it's a fairly small moon, I think it's about 500 kilometers in diameter.

261
00:28:29,560 --> 00:28:36,200
We directly detected water vapor coming out of cracks in the surface of celadus,

262
00:28:36,200 --> 00:28:43,640
and we have pictures from the Hubble Space Telescope of Europa where it looks like there are

263
00:28:43,640 --> 00:28:49,400
water vapor queens coming out of Europa. So that's why we want to go there. The surface of Europa

264
00:28:49,400 --> 00:28:55,400
is extremely rough, much rougher than any place we've tried to land on Mars. The orbital

265
00:28:55,400 --> 00:29:01,560
reconnaissance imagery that we're going to have of Europa is lower resolution than we have for Mars.

266
00:29:01,560 --> 00:29:06,280
So, you know, if we're ever going to land there, we need an intelligent landing system, and so we're

267
00:29:06,280 --> 00:29:12,360
working to extend the techniques that were developed for Mars so that someday they could be applicable

268
00:29:12,360 --> 00:29:19,480
to Europa. You know, elsewhere in the outer solar system, we know there are other places that

269
00:29:19,480 --> 00:29:28,360
have liquid water oceans inside moons, Saturn's moon, Titan is one of those, Titan is unique

270
00:29:28,360 --> 00:29:36,040
in the solar system outside of Earth, in that there's a lot of organic molecules on the surface

271
00:29:36,040 --> 00:29:42,360
of Titan. The surface of Titan is 94 Kelvin, so it's really, really cold, and there are lakes and

272
00:29:42,360 --> 00:29:49,160
seas of liquid methane. So, between those organic molecules on the surface and the liquid water

273
00:29:49,160 --> 00:29:56,200
ocean inside, if that liquid water ever came in contact with all the organics is a lot of

274
00:29:56,200 --> 00:30:01,160
potential interesting chemistry that could happen, that could build up more and more complex

275
00:30:01,160 --> 00:30:08,680
organic molecules, and that might teach us some things ultimately about biology. So, we're

276
00:30:08,680 --> 00:30:13,080
interested in landing there, and that's in in in some respects that's even more challenging.

277
00:30:13,800 --> 00:30:18,520
The terrain is not that bad, but the remote sensing data is even lower resolution than we would

278
00:30:18,520 --> 00:30:24,600
have at Europa, so we have to develop techniques that can cope with that. And then someday, you know,

279
00:30:24,600 --> 00:30:32,360
we had also interested in Venus, you know, there have been landers on Venus, the Soviets put landers

280
00:30:32,360 --> 00:30:38,840
on Venus, but they went to some of the most benign terrain there, so to go to more challenging areas

281
00:30:38,840 --> 00:30:43,960
on Venus. It's going to be hard, you know, Venus has the densest atmosphere in the solar system

282
00:30:43,960 --> 00:30:49,640
by far, and the hottest temperature is on the surface by far, and it's got a very opaque cloud layer,

283
00:30:49,640 --> 00:30:55,640
so a very hard navigation problem on Venus, and we don't know how to solve that yet.

284
00:30:55,640 --> 00:31:01,160
From a computer vision perspective, clearly, you know, when you're looking at these different

285
00:31:01,160 --> 00:31:11,000
missions, there are huge system engineering types of challenges adapting to the constraints of

286
00:31:11,000 --> 00:31:18,280
a given mission. How do the algorithmic requirements change the way you approach each individual

287
00:31:18,280 --> 00:31:22,760
mission, or rather, how does each individual mission change the way you approach the algorithms,

288
00:31:22,760 --> 00:31:30,920
I guess? Yeah, so like I said at the start, we're mission oriented, and the way NASA works,

289
00:31:30,920 --> 00:31:37,400
at least the planetary science, really I think all of all of NASA's science, is we started with

290
00:31:37,400 --> 00:31:42,120
what are the scientific questions you want to answer, and then you work backwards from that,

291
00:31:42,120 --> 00:31:49,880
so what technology do you need, and that's always guided by what can we afford, and what has an

292
00:31:49,880 --> 00:31:57,000
acceptable level of risk. So it's kind of case by case, we look at the science we want to do at

293
00:31:57,000 --> 00:32:04,040
a particular planet or moon, and then how do we do that science, and to, you know, minimize the

294
00:32:04,040 --> 00:32:09,880
cost and the risk, we ask, what can we leverage that we've built before, so it minimizes the

295
00:32:09,880 --> 00:32:15,560
cost of new development and minimizes the risk that it's going to work, so you have to find a

296
00:32:15,560 --> 00:32:22,600
happy medium where you find an engineering solution that gives us the science within affordable cost

297
00:32:22,600 --> 00:32:29,960
and acceptable risk, so this is all motherhood, but that is the process. You also did another

298
00:32:29,960 --> 00:32:38,040
presentation at CVPR on onboard stereo vision for drone pursuit. Can you talk a little bit about

299
00:32:38,040 --> 00:32:45,640
that one? Yeah, so, you know, as I was saying, we try to have complimentary work going on where we

300
00:32:45,640 --> 00:32:54,280
have some long-term objectives for capabilities to develop for NASA, and our charter includes

301
00:32:54,280 --> 00:33:01,640
applying unique expertise to problems in other domains that are complimentary, so, you know,

302
00:33:01,640 --> 00:33:06,440
autonomous navigation of drones is one of those areas that's complimentary and can put an

303
00:33:06,440 --> 00:33:14,280
plug where we're developing. The first ever rotograph for Mars was recently approved to be a

304
00:33:14,280 --> 00:33:21,880
technology demonstration on that 2020 Mars rover mission, so, you know, the work that we were

305
00:33:21,880 --> 00:33:28,840
doing in drone pursuit or sense in the void is complimentary to those things, so, and that was

306
00:33:28,840 --> 00:33:36,840
funded by the Army Research Lab. So, we were inspired by, you know, if you want to have a team

307
00:33:36,840 --> 00:33:41,800
of cooperating drones doing anything, you don't want them to collide with each other,

308
00:33:43,000 --> 00:33:49,720
and in some applications, you might want to minimize communication, so you don't necessarily want

309
00:33:49,720 --> 00:33:56,920
the drones exchanging a lot of information. You may not have access to good external localization

310
00:33:56,920 --> 00:34:05,000
sensors like GPS, so you needed to all be on board, and so we thought, you know, long-term,

311
00:34:05,000 --> 00:34:11,320
you'd really like the drones to be able to detect each other with their own onboard sensors,

312
00:34:11,320 --> 00:34:17,560
so they know that they're present, they can estimate their position and velocity of other nearby

313
00:34:17,560 --> 00:34:24,440
drones, and then, you know, do their own path planning and control with awareness of other

314
00:34:24,440 --> 00:34:32,200
nearby aircraft. So, our initial motivation was cooperative teams, but these techniques are

315
00:34:32,200 --> 00:34:37,720
equally applicable to situations where it's not cooperative, where you might want to be pursuing

316
00:34:37,720 --> 00:34:46,120
another drone, and that's relevant to, you know, what people call counter UAS, or counter

317
00:34:46,120 --> 00:34:53,000
unmanned air systems, so there's a lot of concern about small drones being used for hostile purposes.

318
00:34:53,000 --> 00:35:00,520
So, drone pursuit, when we're doing our research just logistically, it turned out to be easier

319
00:35:00,520 --> 00:35:06,840
to do experiments in the pursuit scenario than in the cooperative scenario, so the paper ended up

320
00:35:08,200 --> 00:35:12,840
describing techniques that were relevant to both the cooperative team scenario and the pursuit

321
00:35:12,840 --> 00:35:18,520
scenario, but all of our testing was in the pursuit scenario, and then the stereo vision aspect

322
00:35:18,520 --> 00:35:24,360
comes back to our earlier discussion about the relative pros and cons of stereo vision versus

323
00:35:24,360 --> 00:35:32,440
LiDAR, so here you've got very limited payload, weight, and power capacity on small drones,

324
00:35:32,440 --> 00:35:38,600
so if we can do something with cameras, especially if those cameras can give us a very wide,

325
00:35:38,600 --> 00:35:44,360
instantaneous field of regard in horizontal and vertical axes, you just can't match that with

326
00:35:44,360 --> 00:35:51,800
the LiDAR these days, and these are both techniques that direct with that measure 3D range,

327
00:35:51,800 --> 00:35:57,080
and through that you can estimate velocity of the other aircraft, so it's quite handy if you actually

328
00:35:57,080 --> 00:36:02,920
know the 3D position and velocity of the other aircraft, so we started with techniques,

329
00:36:02,920 --> 00:36:08,360
in particular stereo vision, that would give us that, those do have range limits, so we can only see

330
00:36:08,360 --> 00:36:14,840
other aircraft, you know, some limited distance away, 10, 15, maybe 20 meters at the outside with

331
00:36:14,840 --> 00:36:23,880
our current cameras, so this was the first paper to show that this was possible and a fully

332
00:36:23,880 --> 00:36:31,960
integrated fashion outdoors without the aid of external position sensing sensors, you know,

333
00:36:31,960 --> 00:36:38,920
there needs to be extensions to be able to see other drones further away, you know, stereo

334
00:36:38,920 --> 00:36:43,240
vision is probably not going to cut it for that, so that probably requires techniques that can

335
00:36:43,240 --> 00:36:50,600
use a single camera, those require more work to develop and test than was within the scope of our

336
00:36:50,600 --> 00:37:00,760
paper. And from the algorithmic perspective is the system used here, primarily a composition of

337
00:37:00,760 --> 00:37:07,400
some of the things that we've already talked about, or are there other techniques that are specific

338
00:37:07,400 --> 00:37:14,520
to this system that might be interesting to explore? Well, the work in this paper was most of

339
00:37:14,520 --> 00:37:20,520
the competition of techniques we've already talked about, so, you know, in some respects,

340
00:37:20,520 --> 00:37:26,040
it's analogous through the constraints we have in space, because onboard computation is very

341
00:37:26,040 --> 00:37:31,320
limited, you know, it's a lot more than we have in space, but it's still pretty limited, so we

342
00:37:31,320 --> 00:37:39,480
were using simple techniques to do reliable 3D perception as shore rings that let us do a first

343
00:37:39,480 --> 00:37:47,320
demonstration of this capability. One of the questions I was asked was, could you use deep learning

344
00:37:47,320 --> 00:37:53,720
to detect the other aircraft? It's a very good question. We had not yet tried that for lack of

345
00:37:53,720 --> 00:38:03,160
training data, but that's certainly conceivable. Another thing that could be used is basically

346
00:38:03,160 --> 00:38:09,800
difference imaging, so, you know, a standard technique in surveillance with stationary cameras

347
00:38:09,800 --> 00:38:15,160
is just to take, you know, pictures over time and subtract them and look at what's changed,

348
00:38:15,160 --> 00:38:21,480
and that lets you pick up things that are moving around the camera, but that's harder to do when

349
00:38:21,480 --> 00:38:27,240
the camera itself is moving, and so that was one of the complexities that was out of scope of

350
00:38:27,240 --> 00:38:36,040
this work, but another approach to detects moving drones from a drone that itself is moving is

351
00:38:36,040 --> 00:38:42,120
background subtraction, but then you have to do motion compensation for the motion of your own drone.

352
00:38:42,120 --> 00:38:48,360
That's something to look at in the future. In the paper itself, you showed standard black and

353
00:38:48,360 --> 00:38:55,080
white images, but then also these multicolored images look like some kind of spectral type of

354
00:38:55,080 --> 00:39:01,480
image or something. What are those? I'm thinking you're probably referring to what we call depth maps,

355
00:39:02,120 --> 00:39:11,000
so yeah, that's a false color illustration of the range at each pixel. So when we do stereo

356
00:39:11,000 --> 00:39:15,960
vision, we get an estimate of range at each pixel. You know, a simple way to visualize that is

357
00:39:15,960 --> 00:39:22,680
just a color code, the range, and then show that as a pixel image. Got it. And so this work that you

358
00:39:22,680 --> 00:39:28,600
did, this was a full end-to-end system, not just the, it wasn't a simulation, you actually put

359
00:39:28,600 --> 00:39:32,920
this on a drone and put it out in the wild, is that right? That is right. How did it do?

360
00:39:32,920 --> 00:39:44,360
Yeah. Well, it did quite well. We tested it on the campus of JPL and on the campus of one of our sponsors.

361
00:39:47,240 --> 00:39:52,280
This problem is a lot easier if the other drone you're looking at is silhouette against the sky.

362
00:39:52,280 --> 00:39:57,160
Then it's actually a very easy problem. So it's much harder if the other drone you're looking at

363
00:39:57,160 --> 00:40:03,960
is seeing against what we call background clutter. So, you know, if it's at, if it's below the horizon

364
00:40:03,960 --> 00:40:09,720
essentially. So if you're seeing it against trees or buildings or ground and that changes over time,

365
00:40:10,520 --> 00:40:15,720
it becomes much harder if you look toward the sun because those, you know, sun causes all sorts

366
00:40:15,720 --> 00:40:22,280
of glare artifacts in the image. And we found that a key advantage of our technique is that it's

367
00:40:22,280 --> 00:40:28,840
pretty robust to those issues because the fact that we're using 3D perception means as long as

368
00:40:28,840 --> 00:40:34,200
the drone is within range, we're not fooled by background clutter because we can segment that 3D

369
00:40:34,200 --> 00:40:39,560
object from the background using the point cloud there. And when we look toward the sun and there

370
00:40:39,560 --> 00:40:46,280
are artifacts in the image, you know, unless things get really, really bad, we can often still

371
00:40:46,280 --> 00:40:52,600
do 3D perception that is adequate to track the drone. And since we've, you know,

372
00:40:52,600 --> 00:40:58,280
let's say before you turn directly at the sun, you had a 3D model, you had a model of the position

373
00:40:58,280 --> 00:41:03,800
and velocity of the other drone from previous images. So you can do prediction for a short amount

374
00:41:03,800 --> 00:41:09,880
of time, you know, if you're blinded by the sun and then probably recover track because you've

375
00:41:09,880 --> 00:41:16,440
got that knowledge, model based knowledge that you can use for prediction. So it didn't work pretty

376
00:41:16,440 --> 00:41:23,640
well. And how do you measure the performance of the system? Do you kind of go through after the

377
00:41:23,640 --> 00:41:28,760
fact and measure maybe distance from where the drone thought the thing it was supposed to be

378
00:41:28,760 --> 00:41:35,240
following was and, you know, compare that to some error function or is there some automated way

379
00:41:35,240 --> 00:41:41,720
that you're able to measure performance? So, you know, that's an excellent question. You always

380
00:41:41,720 --> 00:41:49,880
need some kind of ground truth to measure performance. In our case, we manually label the images.

381
00:41:49,880 --> 00:41:55,880
So we, you know, we marked what bounding boxes are on the drone. Another way to do it would be if

382
00:41:55,880 --> 00:42:02,680
we had, you know, an external sensor that told us where everything is. So if we had GPS, for example,

383
00:42:02,680 --> 00:42:10,680
on both aircraft, you could use that to give position knowledge. And in this case, I think we relied

384
00:42:10,680 --> 00:42:17,000
on manual labeling. We could have the sensor infrastructure finished to do that all with GPS.

385
00:42:18,680 --> 00:42:23,720
But that would help automate things. Well, great. Larry, this was a really interesting,

386
00:42:24,920 --> 00:42:29,240
really interesting conversation. I appreciate you taking the time to share with us what you're

387
00:42:29,240 --> 00:42:33,640
working on. Well, thank you for the time to talk about it. I appreciate that. Thanks.

388
00:42:36,680 --> 00:42:42,520
All right, everyone. That's our show for today. For more information on Larry or any of the topics

389
00:42:42,520 --> 00:42:50,200
covered in this episode, head over to twimmelai.com slash talk slash 170. If you're a fan of the pod,

390
00:42:50,200 --> 00:42:55,480
we'd like to encourage you to pop open your Apple or Google podcast app and leave us a five-star

391
00:42:55,480 --> 00:43:00,280
rating and review. Your reviews go a long way in helping new listeners find the show.

392
00:43:00,280 --> 00:43:30,120
As always, thanks so much for listening and catch you next time.

