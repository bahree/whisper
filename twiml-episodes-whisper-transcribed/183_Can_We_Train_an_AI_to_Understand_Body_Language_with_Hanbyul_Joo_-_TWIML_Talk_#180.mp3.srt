1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:30,620
I'm your host Sam Charrington.

4
00:00:30,620 --> 00:00:35,640
In this episode we're joined by Hambiel Zhu, a PhD student in the Robotics Institute

5
00:00:35,640 --> 00:00:38,040
at Carnegie Mellon University.

6
00:00:38,040 --> 00:00:42,560
Han, who's on track to complete his thesis at the end of the year, is working on what is

7
00:00:42,560 --> 00:00:48,680
called the Panoptic Studio, a multi-dimension motion capture studio with over 500 camera

8
00:00:48,680 --> 00:00:53,720
sensors that are used to capture human body behavior and body language.

9
00:00:53,720 --> 00:00:58,240
While robotic and other artificially intelligent systems can interact with humans, Han's

10
00:00:58,240 --> 00:01:03,160
work focuses on understanding how humans interact and behave with one another so that we can

11
00:01:03,160 --> 00:01:07,320
teach AI-based systems to react to humans more naturally.

12
00:01:07,320 --> 00:01:13,400
In our conversation, we discuss his CVPR best student paper award winner, Total Capture,

13
00:01:13,400 --> 00:01:19,080
3D deformation model for tracking faces, hands, and bodies.

14
00:01:19,080 --> 00:01:23,720
Han also shares a complete overview of the Panoptic Studio and we dig into the creation and

15
00:01:23,720 --> 00:01:26,200
performance of the models and much more.

16
00:01:26,200 --> 00:01:28,240
Okay, enjoy the show.

17
00:01:28,240 --> 00:01:31,480
Alright everyone, I am on the line with Hambiel Zhu.

18
00:01:31,480 --> 00:01:36,960
Hambiel is a PhD student in Robotics at Carnegie Mellon University.

19
00:01:36,960 --> 00:01:40,120
Han, welcome to this week in machine learning and AI.

20
00:01:40,120 --> 00:01:42,280
Thank you for having me today.

21
00:01:42,280 --> 00:01:51,440
So you've been a PhD student at CMU since 2012 and a major focus of your work is in and

22
00:01:51,440 --> 00:01:56,120
around an environment that's been built there called the Panoptic Studio.

23
00:01:56,120 --> 00:01:57,720
What is the Panoptic Studio?

24
00:01:57,720 --> 00:02:04,800
Yeah, the Panoptic Studio is basically a multi-b system with more than 500 camera sensors

25
00:02:04,800 --> 00:02:08,520
and the system is also controlled by more than 50 machines.

26
00:02:08,520 --> 00:02:14,920
So this is indeed a giant system using computer vision technique to measure or sense humans

27
00:02:14,920 --> 00:02:16,840
by the behaviors.

28
00:02:16,840 --> 00:02:22,880
So basically we are very interested in the way we are using our bodies for social communication.

29
00:02:22,880 --> 00:02:28,160
For example, all these subtle facial expressions and body gestures and so on.

30
00:02:28,160 --> 00:02:33,160
And we really wanted to make a machine who can understand the body languages we are using

31
00:02:33,160 --> 00:02:38,240
and in the end we want to make a machine which can use these body languages to communicate

32
00:02:38,240 --> 00:02:39,480
with us.

33
00:02:39,480 --> 00:02:44,400
And for the purpose, the first thing we wanted to do is we wanted to collect this kind

34
00:02:44,400 --> 00:02:50,000
of motion capture data of naturally interacting people so that we can use some machine learning

35
00:02:50,000 --> 00:02:53,640
technique on top of that so that we can in the end machine can understand these kind of

36
00:02:53,640 --> 00:02:55,440
signals.

37
00:02:55,440 --> 00:02:59,760
And that was the main motivation of this Panoptic Studio and since we are interested

38
00:02:59,760 --> 00:03:06,160
in multiple people's interaction to avoid all this complicated occlusion kind of problem,

39
00:03:06,160 --> 00:03:11,680
we wanted to make a system with many, many sensors with many viewpoint so that we can

40
00:03:11,680 --> 00:03:16,200
really release these particular occlusion problems.

41
00:03:16,200 --> 00:03:20,840
And this Panoptic Studio goes back quite a ways it sounds like.

42
00:03:20,840 --> 00:03:27,680
Yeah, so CNU has really amazing history on this multi-b system because Professor Dakeo

43
00:03:27,680 --> 00:03:33,880
already started this multi-view system kind of research about, I think, 30 years ago.

44
00:03:33,880 --> 00:03:40,680
And at the moment, the major goal is to reconstruct the 3D world, especially 3D object and

45
00:03:40,680 --> 00:03:42,680
3D human body behavior.

46
00:03:42,680 --> 00:03:47,720
And to do that, obviously, this multi-view system is a really good kind of key to solve all

47
00:03:47,720 --> 00:03:49,880
this depth kind of ambiguity.

48
00:03:49,880 --> 00:03:55,680
At the moment, he already started this project with 15 machines and it was extremely challenging

49
00:03:55,680 --> 00:03:56,880
problem at the moment.

50
00:03:56,880 --> 00:04:03,160
And our Panoptic Studio is kind of third generation and our basic idea is maybe 40 or 50 cameras

51
00:04:03,160 --> 00:04:04,160
I will let it boring.

52
00:04:04,160 --> 00:04:11,520
Let's try to make a system with 1000 cameras, for example, and see what we can solve.

53
00:04:11,520 --> 00:04:16,040
And that was kind of the beginning of our studio and we particularly chose this social

54
00:04:16,040 --> 00:04:21,120
interaction analysis problem because that is indeed a challenging problem and very important

55
00:04:21,120 --> 00:04:26,160
future problems so that the machine can actually have a way to interact with us.

56
00:04:26,160 --> 00:04:35,400
And so how do you frame out this social interaction problem and what are the different research

57
00:04:35,400 --> 00:04:39,800
problems that come out of this broader problem?

58
00:04:39,800 --> 00:04:45,440
So basically, you can easily consider natural language or verbal language problems.

59
00:04:45,440 --> 00:04:53,360
So for example, nowadays, there is a very popular commercial AI system which can understand

60
00:04:53,360 --> 00:04:58,880
human's voice and human's language and use their languages to communicate with humans.

61
00:04:58,880 --> 00:05:04,800
But in that case, the main channel is verbal language, which has sentence and grammar

62
00:05:04,800 --> 00:05:06,280
or just rules.

63
00:05:06,280 --> 00:05:10,960
Basically, we wanted to do the similar thing with non-verbal languages.

64
00:05:10,960 --> 00:05:15,200
And for example, when humans are communicating each other, we use obviously our verbal

65
00:05:15,200 --> 00:05:19,960
languages, but we also use our facial expressions and our body languages.

66
00:05:19,960 --> 00:05:25,240
And all this subtle movement have some specific meaning, which is actually really hard to

67
00:05:25,240 --> 00:05:29,960
define, but humans are really easily understanding.

68
00:05:29,960 --> 00:05:37,400
And so basically, so maybe social signal understanding is something like expanding the dimension

69
00:05:37,400 --> 00:05:40,400
from this verbal channel to non-verbal channel as well.

70
00:05:40,400 --> 00:05:47,080
For example, the input can be, in this case, a verbal voice and also images or motion-catcher

71
00:05:47,080 --> 00:05:51,960
data and machine understand the meaning of each sudden movement and maybe machine can

72
00:05:51,960 --> 00:05:58,000
react to the input signals by using the similar outputs, for example, verbal language or non-verbal

73
00:05:58,000 --> 00:05:59,000
languages.

74
00:05:59,000 --> 00:06:05,360
And so one of the papers that we wanted to talk about is a paper that recently won the

75
00:06:05,360 --> 00:06:08,200
CVPR Best Student Paper Award.

76
00:06:08,200 --> 00:06:10,200
And that was your total capture paper.

77
00:06:10,200 --> 00:06:15,880
And the full headline or the full title is Total Capture, a 3D deformation model for

78
00:06:15,880 --> 00:06:21,960
tracking faces, hands, and bodies, which is right in line with what you've been describing.

79
00:06:21,960 --> 00:06:26,120
Do you want to start kind of jump into that paper or do you want to tell us a little bit

80
00:06:26,120 --> 00:06:32,880
about where that paper fits into kind of the broader scope of research that you're doing

81
00:06:32,880 --> 00:06:34,920
for your PhD in at the studio?

82
00:06:34,920 --> 00:06:41,000
Well, I think by explaining the paper, maybe the concept would be a little bit more clear.

83
00:06:41,000 --> 00:06:48,480
So, yeah, so basically, so let's say we are given a large scale data set and now we have

84
00:06:48,480 --> 00:06:53,160
some consensus that we can solve some problem using machine learning or this fancy deep learning

85
00:06:53,160 --> 00:06:54,440
techniques, right?

86
00:06:54,440 --> 00:07:00,160
So we have seen such kind of success in language part and computer vision or image processing

87
00:07:00,160 --> 00:07:03,280
part or object recognition part.

88
00:07:03,280 --> 00:07:07,040
And let's say we wanted to do the similar thing for body languages.

89
00:07:07,040 --> 00:07:11,800
The problem is the amount of data is extremely rare in this body language.

90
00:07:11,800 --> 00:07:16,160
And maybe the first problem is what kind of data we can use for this purpose, for example.

91
00:07:16,160 --> 00:07:23,080
So machine sees the world maybe using cameras and how can understand the human's behavior.

92
00:07:23,080 --> 00:07:28,000
Maybe machine can use the images and videos directly for the purpose similar to human because

93
00:07:28,000 --> 00:07:33,840
actually we are using our two eyes which captured the scene and we can do some all these visual

94
00:07:33,840 --> 00:07:38,560
kind of understanding part and indeed actually we know the meaning of each other movement

95
00:07:38,560 --> 00:07:39,960
of humans.

96
00:07:39,960 --> 00:07:46,880
But using directly images or videos are extremely challenging because it actually they are actually

97
00:07:46,880 --> 00:07:50,720
nothing to do with humans, but actually they are some pixels and machine need to understand

98
00:07:50,720 --> 00:07:55,160
the meaning of all these pixels so that they can finally infer oh, there is this human

99
00:07:55,160 --> 00:08:00,200
here, there is this human's arm here, arm is moving in such a way, I mean solving all

100
00:08:00,200 --> 00:08:04,040
these problems from the image or video is extremely challenging problem.

101
00:08:04,040 --> 00:08:08,840
So although this is still very popular area in computer vision, using such a data set

102
00:08:08,840 --> 00:08:14,600
for finer output meaning understanding the social interaction is extremely challenging.

103
00:08:14,600 --> 00:08:19,640
So maybe another way to do that is just by collecting a lot of motion capture data because

104
00:08:19,640 --> 00:08:25,200
motion capture is in a 3D space and maybe there can be camera independent, illumination

105
00:08:25,200 --> 00:08:30,480
independent and that each motion capture data like for example 3D skeleton, this already

106
00:08:30,480 --> 00:08:34,880
has some semantic meaning, we know where is the arm, where is the hand, where is, I mean

107
00:08:34,880 --> 00:08:37,120
how they are moving in 3D space.

108
00:08:37,120 --> 00:08:43,680
And this is usually done with the humans are wearing specialized suits with like I've seen

109
00:08:43,680 --> 00:08:48,280
pictures of these things, I don't know if they're balls or lights or sensors or something

110
00:08:48,280 --> 00:08:49,280
like that.

111
00:08:49,280 --> 00:08:50,280
Yeah, exactly.

112
00:08:50,280 --> 00:08:55,840
The area is extremely popular in movie industry and game industry and these are main way to

113
00:08:55,840 --> 00:09:01,120
capture the real world human body signal so that we can put that to the virtual world.

114
00:09:01,120 --> 00:09:07,280
But our idea is to use such data for understanding human behavior using machine learning techniques.

115
00:09:07,280 --> 00:09:13,880
So to do that, we wanted to collect such kind of motion capture data a lot from natural

116
00:09:13,880 --> 00:09:15,240
interacting people.

117
00:09:15,240 --> 00:09:20,480
So the key here is as you mentioned, we really don't want to use any artificial markers

118
00:09:20,480 --> 00:09:25,520
or suit because they may affect the natural motion of humans.

119
00:09:25,520 --> 00:09:30,160
For example, if you are wearing this kind of black suit and say you have all this sensor

120
00:09:30,160 --> 00:09:35,880
or all this kind of markers on your fingers, maybe your motion will be very kind of non-natural.

121
00:09:35,880 --> 00:09:40,320
We really wanted to avoid such kind of cases and we just wanted to capture that in natural

122
00:09:40,320 --> 00:09:43,160
human behavior when they are very naturally interacting.

123
00:09:43,160 --> 00:09:46,560
That means that the method itself should be in necklace.

124
00:09:46,560 --> 00:09:50,360
And this necklace motion capture area is also very popular in computer vision and computer

125
00:09:50,360 --> 00:09:55,640
graphics because if you can do the similar motion capture with a single camera or just multiple

126
00:09:55,640 --> 00:10:00,640
cameras, there can be really great to provide a really reconstruction output or it can

127
00:10:00,640 --> 00:10:07,160
be also used for many main purpose like some action recognition, for example, or all this

128
00:10:07,160 --> 00:10:10,240
movie industry or game industry.

129
00:10:10,240 --> 00:10:15,120
And this itself is a challenging problem and this is especially challenging if there

130
00:10:15,120 --> 00:10:19,120
are multiple people because some part is extremely occluded.

131
00:10:19,120 --> 00:10:23,160
And our idea is, all right, we wanted to collect such data set first so that we can somehow

132
00:10:23,160 --> 00:10:26,320
tackle the final problem we are very interested in.

133
00:10:26,320 --> 00:10:32,480
But solving this problem itself is challenging then why not using a very kind of a good system

134
00:10:32,480 --> 00:10:35,880
which can reduce the challenge of this problem.

135
00:10:35,880 --> 00:10:39,960
So that we can start this project and then if this is really meaningful, then actually

136
00:10:39,960 --> 00:10:44,600
we can expand the project to more challenging environments such as in the wild or maybe

137
00:10:44,600 --> 00:10:47,600
you can try to use some YouTube video for the similar purpose.

138
00:10:47,600 --> 00:10:52,840
So that was kind of the major motivation of our kind of the studio, all right.

139
00:10:52,840 --> 00:10:57,560
This problem itself is too challenging and even obtaining the data is challenging.

140
00:10:57,560 --> 00:11:02,480
So let's solve this problem, the initial data generation problem or human body measurement

141
00:11:02,480 --> 00:11:06,480
problem first so that we can actually tackle the later problem.

142
00:11:06,480 --> 00:11:12,080
And our total capture paper is one of the output of our measurement step.

143
00:11:12,080 --> 00:11:16,840
So here we are interested in interacting multiple people and we are also interested

144
00:11:16,840 --> 00:11:21,480
in measuring all these body signals at the same time, for example, facial expressions,

145
00:11:21,480 --> 00:11:26,560
finger motions and body gestures because when we are using our bodies, these parts are

146
00:11:26,560 --> 00:11:28,240
extremely correlated.

147
00:11:28,240 --> 00:11:32,160
Some specific facial expressions should be very, very correlated to hand signals and we

148
00:11:32,160 --> 00:11:36,800
wanted to make find, we want to find some rules humans are using when they are interacting.

149
00:11:36,800 --> 00:11:41,880
So catching subtle details of entire body part is extremely important, but doing that

150
00:11:41,880 --> 00:11:46,920
is very hard although we are using markers because of all these occlusion, self occlusion

151
00:11:46,920 --> 00:11:53,240
for example, human hands, human have when they're moving their hands, for example.

152
00:11:53,240 --> 00:11:58,080
So yeah, so our total capture paper is person in that direction and it actually showed

153
00:11:58,080 --> 00:12:04,920
some meaningful kind of output in measuring all these parts at the same time.

154
00:12:04,920 --> 00:12:11,200
One question I've got for you about this is are the humans that are in this environment?

155
00:12:11,200 --> 00:12:17,120
Are they just kind of naturally interacting, doing what they're doing and you're capturing

156
00:12:17,120 --> 00:12:23,200
that and then kind of going back and labeling it maybe from video or audio capture or is

157
00:12:23,200 --> 00:12:29,560
there a script that they're kind of working against and well, I'll let you answer then

158
00:12:29,560 --> 00:12:31,760
I may have a follow up question.

159
00:12:31,760 --> 00:12:38,240
Yeah, that's actually a very, very good question because although we have, we collect people

160
00:12:38,240 --> 00:12:44,240
who have no idea about our project, it's very hard to ask them to do some natural motion

161
00:12:44,240 --> 00:12:45,240
right because.

162
00:12:45,240 --> 00:12:46,240
Right.

163
00:12:46,240 --> 00:12:47,240
Because then it's not natural.

164
00:12:47,240 --> 00:12:48,240
Exactly.

165
00:12:48,240 --> 00:12:54,480
So for the purpose, actually we are very closely collaborating with psychologists and they

166
00:12:54,480 --> 00:12:57,680
are actually very careful about that.

167
00:12:57,680 --> 00:13:06,000
What we are doing currently is we build some specific social situation and we actually define

168
00:13:06,000 --> 00:13:13,000
some social game, which is a kind of negotiation game and this negotiation scenario where three

169
00:13:13,000 --> 00:13:19,440
people are negotiating each other and in this game, which we call haggling, there are two

170
00:13:19,440 --> 00:13:26,000
sellers and there is one buyer and the goal of this game is two sellers need to sell some

171
00:13:26,000 --> 00:13:32,440
competitive items to this buyer and if the seller actually successfully sells the product

172
00:13:32,440 --> 00:13:38,800
within a minute of time, we actually gave some bonus for them and because of this bonus,

173
00:13:38,800 --> 00:13:44,080
this monetary bonus, actually they can be really involved in this project because the game

174
00:13:44,080 --> 00:13:48,240
itself is pretty fun and actually they can additionally get this money because of the

175
00:13:48,240 --> 00:13:53,600
region actually we found that the motion became very natural for sure because of the

176
00:13:53,600 --> 00:13:58,800
special system at the very beginning when they first entered the studio, they actually

177
00:13:58,800 --> 00:14:03,960
see the cameras and they can, they usually see the round but we actually spend some time

178
00:14:03,960 --> 00:14:09,040
inside the studio so that they can be fully familiar with the system and because we don't

179
00:14:09,040 --> 00:14:13,240
put any kind of camera in front of their face because cameras are all this cameras are

180
00:14:13,240 --> 00:14:15,160
attached on the surface of the dome.

181
00:14:15,160 --> 00:14:19,560
So it just seems like some kind of special room and after some time, actually the room

182
00:14:19,560 --> 00:14:24,680
is just somehow similar to other kind of room with some special wallpaper.

183
00:14:24,680 --> 00:14:28,320
So we found that their motions are pretty natural and in the end, we actually did some

184
00:14:28,320 --> 00:14:29,320
questionnaire.

185
00:14:29,320 --> 00:14:33,400
It turns out the majority people completely forgot that they are inside the dome and they

186
00:14:33,400 --> 00:14:37,920
can fully kind of involved in this kind of social game.

187
00:14:37,920 --> 00:14:45,520
And so for labeling, have you perhaps in conjunction with your partners on the psychology

188
00:14:45,520 --> 00:14:51,360
side, like have you developed a taxonomy of gestures or some kind of labeling system

189
00:14:51,360 --> 00:14:59,600
or do you have a free, free description of the gestures, which I would imagine would

190
00:14:59,600 --> 00:15:01,440
be pretty difficult to deal with?

191
00:15:01,440 --> 00:15:06,720
Exactly, so that's actually a really big question about this project because let's say we

192
00:15:06,720 --> 00:15:13,360
can obtain all this body motion capture data from this, let's say three people's interaction.

193
00:15:13,360 --> 00:15:17,960
Let's say we have all this facial expression movement and this finger movement and body

194
00:15:17,960 --> 00:15:18,960
movement.

195
00:15:18,960 --> 00:15:25,840
Then what type of problem can we actually tackle given this somehow restrict social situation?

196
00:15:25,840 --> 00:15:31,400
Maybe we can do some type of annotation but actually annotation itself is some big question

197
00:15:31,400 --> 00:15:37,520
in this problem because what kind of label can you annotate in this behavior?

198
00:15:37,520 --> 00:15:42,800
In computer vision, for example, there is just area named action recognition, but different

199
00:15:42,800 --> 00:15:48,200
from object recognition, action is really hard to define because there's starting time

200
00:15:48,200 --> 00:15:51,520
and the time is usually really hard to define, right?

201
00:15:51,520 --> 00:15:56,400
And making the label, the name of each motion is also very challenging because making the

202
00:15:56,400 --> 00:16:00,400
name means we wanted to describe the motion using some language.

203
00:16:00,400 --> 00:16:08,000
The language is in a very discreet space and it's in a very low dimensional space while

204
00:16:08,000 --> 00:16:11,680
the expression, the signals we are using in our social communication is actually really

205
00:16:11,680 --> 00:16:13,200
high dimensional space.

206
00:16:13,200 --> 00:16:21,240
So in that sense, actually, we don't try to do any so-called kind of annotation label.

207
00:16:21,240 --> 00:16:25,280
What we are labeling is kind of more objective thing, for example, who is the winner of this

208
00:16:25,280 --> 00:16:31,600
game, who is the loser of this game, and for example, who is speaking at this moment

209
00:16:31,600 --> 00:16:35,840
so that we can actually be very objective about the label.

210
00:16:35,840 --> 00:16:37,680
And our scenario is something like this.

211
00:16:37,680 --> 00:16:41,840
So how can you maybe make sure that robot detection interesting is a social behavior?

212
00:16:41,840 --> 00:16:44,520
That is another good question we should ask.

213
00:16:44,520 --> 00:16:49,600
And maybe our solution for that problem is something like the following.

214
00:16:49,600 --> 00:16:57,320
Well, if a machine sees the world, sees the human's behavior, and if machine can predict

215
00:16:57,320 --> 00:17:04,760
the future motion of these people, then that can be maybe a way to define that robot has

216
00:17:04,760 --> 00:17:06,560
some understanding about our social behavior.

217
00:17:06,560 --> 00:17:12,160
More statistically, let's say we have all this data and we somehow delete some human's

218
00:17:12,160 --> 00:17:13,320
motion data.

219
00:17:13,320 --> 00:17:19,720
And can we actually predict, can robot predict the person's motion, I mean, this hidden

220
00:17:19,720 --> 00:17:23,800
person's motion by observing the other people's motion?

221
00:17:23,800 --> 00:17:26,120
This is exactly the way we are interacting, for example.

222
00:17:26,120 --> 00:17:30,560
When we are interacting, we observe other people's signal, and we're making some signal

223
00:17:30,560 --> 00:17:31,560
from our body.

224
00:17:31,560 --> 00:17:36,160
And the signal is sent to other people, and the other people are reacting to our signal.

225
00:17:36,160 --> 00:17:39,040
And actually, this is somehow the way we are doing communication.

226
00:17:39,040 --> 00:17:44,600
We just generate some signals using our body, send that, the signal is understood by people,

227
00:17:44,600 --> 00:17:49,080
and that is also, I mean, the reaction is also sent to us, and we are just exchanging

228
00:17:49,080 --> 00:17:51,160
some type of signals.

229
00:17:51,160 --> 00:17:54,360
And we wanted to do the similar thing for the machine, so machine is in the loop and

230
00:17:54,360 --> 00:17:59,040
machine see the world, machine decoded the social behavior with other people, understand

231
00:17:59,040 --> 00:18:01,720
the meaning, and somehow predict the future motion.

232
00:18:01,720 --> 00:18:05,800
So this is a way we define the problem at this moment, and this is exactly what we are

233
00:18:05,800 --> 00:18:07,280
currently working on.

234
00:18:07,280 --> 00:18:15,800
I'm trying to reconcile the idea that you're not doing any kind of annotation with the idea

235
00:18:15,800 --> 00:18:21,160
that you're able to predict motion, but I'm imagining you're not predicting the label

236
00:18:21,160 --> 00:18:22,160
of some motion.

237
00:18:22,160 --> 00:18:29,440
You're trying to predict motion itself, so where is the hand going to be at some future

238
00:18:29,440 --> 00:18:35,560
time based on some set of interactions as opposed to what's the name of the gesture

239
00:18:35,560 --> 00:18:38,400
that the person is going to do?

240
00:18:38,400 --> 00:18:39,400
That's exactly true.

241
00:18:39,400 --> 00:18:44,600
So we just use the motion capture data as the input signal for this machine learning

242
00:18:44,600 --> 00:18:48,920
tool, and output is also the similar types of signals.

243
00:18:48,920 --> 00:18:54,720
But based on the definition of the problem, we can consider different scenarios, but one

244
00:18:54,720 --> 00:19:01,560
kind of scenario we are considering is the input is the other people's body motion,

245
00:19:01,560 --> 00:19:07,520
which is the motion capture itself, the signal itself, and the output is the target person's

246
00:19:07,520 --> 00:19:08,920
future motion.

247
00:19:08,920 --> 00:19:15,200
For example, if somebody is speaking something, maybe our target person can be nodding, which

248
00:19:15,200 --> 00:19:21,360
is synchronized to other people's behavior, or our target person would be maybe laughing,

249
00:19:21,360 --> 00:19:26,400
or moving in a specific way, and actually can we predict such kind of future motion of

250
00:19:26,400 --> 00:19:32,040
the target person given other people's body behavior?

251
00:19:32,040 --> 00:19:33,040
And so does it work?

252
00:19:33,040 --> 00:19:34,040
How?

253
00:19:34,040 --> 00:19:40,160
It sounds like a really interesting problem formulation, and tell us about the system and

254
00:19:40,160 --> 00:19:44,640
how well it performs, and then walk us through kind of how you build it.

255
00:19:44,640 --> 00:19:51,760
Well, so that is something we are currently working on, and for sure to this problem,

256
00:19:51,760 --> 00:19:56,020
we really need a lot of data, because human's behavior would be very different, given the

257
00:19:56,020 --> 00:19:57,020
same situation, right?

258
00:19:57,020 --> 00:20:02,680
That would be maybe very related to our culture or personality, so given the same signal

259
00:20:02,680 --> 00:20:06,640
actually human's behavior would be very different, so basically it's multi-model, very good

260
00:20:06,640 --> 00:20:11,880
is this multi-model issue, and our data set is never sufficient for the purpose.

261
00:20:11,880 --> 00:20:15,800
So we really need to narrow down the scope at this moment, so that we can tackle the initial

262
00:20:15,800 --> 00:20:20,040
problem, and you can consider this simple problem first, for example, unless that we are

263
00:20:20,040 --> 00:20:25,000
always considering three people's behavior interactions, and the input is two sellers'

264
00:20:25,000 --> 00:20:26,000
body behavior.

265
00:20:26,000 --> 00:20:31,640
So we have this skeleton, 3D skeleton kind of motion capture data of two people, two sellers,

266
00:20:31,640 --> 00:20:35,440
and those are the input of our, for example, neural network architecture.

267
00:20:35,440 --> 00:20:39,960
And can we actually guess the gase direction of our buyer?

268
00:20:39,960 --> 00:20:44,520
So you can simply imagine that if somebody is speaking, if all that we don't have any

269
00:20:44,520 --> 00:20:49,960
verbal kind of signals, if somebody is speaking there, if it is some speaking specific kind

270
00:20:49,960 --> 00:20:56,400
of body behavior, and probably the buyer's gaze will be on this speaker, right?

271
00:20:56,400 --> 00:21:02,080
This is some very simple kind of scenario we can imagine.

272
00:21:02,080 --> 00:21:07,000
So this is just one channel, just estimating the gase direction of the target person.

273
00:21:07,000 --> 00:21:11,640
Can we increase the dimension, for example, what about the facial expression?

274
00:21:11,640 --> 00:21:16,400
Facial expression is indeed some 3D face key point, or we can just simply imagine the

275
00:21:16,400 --> 00:21:20,800
mesh itself, so we have all these vertexes moving 3D meshes.

276
00:21:20,800 --> 00:21:24,520
And this itself is the output of the network.

277
00:21:24,520 --> 00:21:29,600
And can you actually predict the target person who is the seller, a buyer in this case,

278
00:21:29,600 --> 00:21:33,640
of the buyer's facial expression based on the other people's kind of behavior?

279
00:21:33,640 --> 00:21:40,200
Or another case is maybe location, where is the location of our target person?

280
00:21:40,200 --> 00:21:44,640
Because basically humans try to have some distance among each other.

281
00:21:44,640 --> 00:21:48,120
And this distance is also somehow trained throughout our life.

282
00:21:48,120 --> 00:21:51,480
And we actually maintain some specific distance when you are communicating.

283
00:21:51,480 --> 00:21:56,440
So actually, machine can predict the distance of the target person and orientation of the

284
00:21:56,440 --> 00:21:57,440
person.

285
00:21:57,440 --> 00:22:01,320
So this is something we can imagine as a very low dimension signal.

286
00:22:01,320 --> 00:22:04,080
And we can actually consider higher, higher dimension signal, for example, can you actually

287
00:22:04,080 --> 00:22:10,000
predict the target person's skeletal movement, or long-term movement, or hand gestures.

288
00:22:10,000 --> 00:22:14,840
And at this moment, all this part is still remains in a very challenging problem, because

289
00:22:14,840 --> 00:22:17,800
we don't have any specific output in this case.

290
00:22:17,800 --> 00:22:21,800
But this is magic focused on our purpose, our motivation.

291
00:22:21,800 --> 00:22:22,800
Got it.

292
00:22:22,800 --> 00:22:34,320
So the big picture is understanding how to predict from these 3D motion capture models,

293
00:22:34,320 --> 00:22:39,680
human behavior in interactions, so that ultimately machines could better understand

294
00:22:39,680 --> 00:22:44,400
human behavior and make predictions based on it.

295
00:22:44,400 --> 00:22:52,040
The longer-term goal to get you there is being able to predict on a, say, pixel-by-pixel

296
00:22:52,040 --> 00:22:59,680
basis what the future state of one of these participants' body position is.

297
00:22:59,680 --> 00:23:07,440
But the intermediate steps or the near-term challenges, and that's really, I think you

298
00:23:07,440 --> 00:23:11,680
sense that that was my, you know, what prompted me to ask, is it working yet?

299
00:23:11,680 --> 00:23:14,000
Yeah, that sounds like a huge problem, and it is.

300
00:23:14,000 --> 00:23:19,480
So the way you're tackling that is these intermediate problems of, can we track the gaze

301
00:23:19,480 --> 00:23:20,480
direction?

302
00:23:20,480 --> 00:23:23,520
Can we track the facial expression?

303
00:23:23,520 --> 00:23:25,880
Can we track hand position, things like that?

304
00:23:25,880 --> 00:23:26,880
Exactly.

305
00:23:26,880 --> 00:23:33,160
So basically, our current kind of problem definition is, we want to make a machine to understand

306
00:23:33,160 --> 00:23:38,040
the human's behavior, and the way to do that is making some system which can predict

307
00:23:38,040 --> 00:23:40,480
the future motion of the target person.

308
00:23:40,480 --> 00:23:45,440
And there, we can imagine some simple good examples and application for this, for example,

309
00:23:45,440 --> 00:23:55,360
let's say we have this AI, which kind of Amazon Alexa or Google Home kind of AI system,

310
00:23:55,360 --> 00:24:02,080
which is nowadays just using verbal language, for example, audio or speaker.

311
00:24:02,080 --> 00:24:05,440
But let's say this small machine has video as well.

312
00:24:05,440 --> 00:24:10,760
So it can capture the scene, the human's behavior using camera, and actually it can display

313
00:24:10,760 --> 00:24:13,840
some behavior using their own display.

314
00:24:13,840 --> 00:24:18,120
I think this can be really interesting because now somehow they can understand the meaning

315
00:24:18,120 --> 00:24:21,120
of our BID behavior, and they can react with that.

316
00:24:21,120 --> 00:24:27,800
You can also consider many interesting AI systems or robot exist, and for example, if there

317
00:24:27,800 --> 00:24:33,960
is a toy robot, which is actually interacting with a child, and if the machine can understand

318
00:24:33,960 --> 00:24:40,120
the meaning of the child's body movement, basically let's say the baby cannot speak at

319
00:24:40,120 --> 00:24:43,400
all, but it can actually make some facial expression and so on and so forth.

320
00:24:43,400 --> 00:24:47,920
But still, the machine can understand this because human can do that, mother and father,

321
00:24:47,920 --> 00:24:51,600
they actually knows the meaning of some specific movement of this child.

322
00:24:51,600 --> 00:24:54,280
So this can be also interesting application.

323
00:24:54,280 --> 00:25:02,240
Also you can imagine some application in medical area, for example, we have all these surveillance

324
00:25:02,240 --> 00:25:08,440
systems, and we need to maybe monitor the elderly people's body behavior so that we can somehow

325
00:25:08,440 --> 00:25:13,960
identify some unusual kind of behavior.

326
00:25:13,960 --> 00:25:19,760
For example, in this case, all this measurement kind of skills and techniques and this understanding

327
00:25:19,760 --> 00:25:26,440
that their behavior would be extremely important kind of techniques for these applications.

328
00:25:26,440 --> 00:25:32,520
And so in the total capture paper you're presenting is a deformation model for tracking

329
00:25:32,520 --> 00:25:34,520
these face-hand and body positions.

330
00:25:34,520 --> 00:25:37,480
What do you mean by deformation model?

331
00:25:37,480 --> 00:25:45,800
So basically we can do some 3D reconstruction without multiple cameras because connect

332
00:25:45,800 --> 00:25:50,520
can do some type of thing or that camera can do some type of 3D reconstruction.

333
00:25:50,520 --> 00:25:57,120
Here deformation model basically means the human's behavior is parameterized by some

334
00:25:57,120 --> 00:26:02,960
limited amount of parameters and this parameter represent body motions.

335
00:26:02,960 --> 00:26:07,800
So body motion means each joint angle is one parameter for each joint.

336
00:26:07,800 --> 00:26:13,320
So we can consider some number of joint and each joint let's say have three-dimensional

337
00:26:13,320 --> 00:26:14,320
rotation vector.

338
00:26:14,320 --> 00:26:17,840
So that is some parameterization for body model.

339
00:26:17,840 --> 00:26:22,200
And also we can do some parameterization for shape deformation.

340
00:26:22,200 --> 00:26:28,520
So we have let's say 10 parameters and by just changing these parameters we can represent

341
00:26:28,520 --> 00:26:31,760
the different size and shape of the people.

342
00:26:31,760 --> 00:26:38,680
So without this parameterization actually reconstructing 3D humans are human is more challenging

343
00:26:38,680 --> 00:26:44,560
because maybe we need to reconstruct all these vertex, all these 3D point of all this

344
00:26:44,560 --> 00:26:49,040
entire body and that dimension is extremely large.

345
00:26:49,040 --> 00:26:54,320
For example let's say we want to reconstruct some mesh model for each individual and

346
00:26:54,320 --> 00:26:58,840
let's say this mesh model has let's say 10,000 vertexes.

347
00:26:58,840 --> 00:27:04,040
The basically we need to estimate the location of these 10,000 vertexes for each time instance.

348
00:27:04,040 --> 00:27:09,360
But the interesting thing is our surface is extremely correlated each other.

349
00:27:09,360 --> 00:27:12,480
So our vertex is not moving arbitrarily.

350
00:27:12,480 --> 00:27:17,440
If we know some location of one part we can easily guess at the location because they

351
00:27:17,440 --> 00:27:24,480
are somehow very, you know, in a low dimensional kind of space in the end.

352
00:27:24,480 --> 00:27:31,080
So this parameterization is basically to parameterize all these variations of human body motions

353
00:27:31,080 --> 00:27:35,600
and body shape in a low dimensional space but still we wanted to have some sufficient

354
00:27:35,600 --> 00:27:41,920
expressive power to express all these body behavior and the shape deformation.

355
00:27:41,920 --> 00:27:47,920
And our total model capture is basically so this deformation model is actually really

356
00:27:47,920 --> 00:27:55,000
popular in computer vision and computer graphics and there it is really popular as deformation

357
00:27:55,000 --> 00:27:57,320
model for body and faces.

358
00:27:57,320 --> 00:28:02,360
Our major contribution of our paper is we build a model which can actually include all

359
00:28:02,360 --> 00:28:07,880
this part together including body deformation, body motion, facial deformation, facial expression

360
00:28:07,880 --> 00:28:11,080
and finger motion and finger deformation and so on.

361
00:28:11,080 --> 00:28:18,680
And when you say you have built a unified model in what way is it unified?

362
00:28:18,680 --> 00:28:24,000
Is it, you know, have you built some ensemble that does each of the things well or have

363
00:28:24,000 --> 00:28:30,720
you kind of abstracted away from the individual parts of the body that you're modeling and

364
00:28:30,720 --> 00:28:38,040
really trained a model that can given any of those inputs, producer and output?

365
00:28:38,040 --> 00:28:41,960
So in this paper actually we defined two different models.

366
00:28:41,960 --> 00:28:47,400
The first model is more related to some independent model.

367
00:28:47,400 --> 00:28:53,200
For example, we have independent facial model and body model and hand model and we somehow

368
00:28:53,200 --> 00:28:56,960
consolidate together by attaching each other explicitly.

369
00:28:56,960 --> 00:29:02,200
So this model is called Frankenstein or Frank because the way we built this system is

370
00:29:02,200 --> 00:29:04,920
similar to this Frankenstein's model.

371
00:29:04,920 --> 00:29:12,120
But yeah, the problem is all part have individual parameterization so sometimes they are not

372
00:29:12,120 --> 00:29:13,880
consistent each other.

373
00:29:13,880 --> 00:29:18,520
The final model we built which we call Adam is actually has single parameter space for

374
00:29:18,520 --> 00:29:21,360
entire shape space.

375
00:29:21,360 --> 00:29:27,480
So if we change one parameter of this space, a phase by the, a phase by the fingers are

376
00:29:27,480 --> 00:29:32,680
changing together and basically defining this low dimensional deformation space means

377
00:29:32,680 --> 00:29:37,920
we want to find out some correlation of different part so that we can reduce the dimension

378
00:29:37,920 --> 00:29:38,920
of this space.

379
00:29:38,920 --> 00:29:40,440
That's the key.

380
00:29:40,440 --> 00:29:46,640
And since we built this model by collecting all this 3D reconstruction of body phase and

381
00:29:46,640 --> 00:29:51,400
hand together, actually when we built this model in a low dimensional space, the model

382
00:29:51,400 --> 00:29:54,800
somehow knows the correlation across bodies.

383
00:29:54,800 --> 00:29:59,600
For example, if somebody has really big maybe face, maybe their hands would be also big

384
00:29:59,600 --> 00:30:00,600
as well.

385
00:30:00,600 --> 00:30:06,280
And by somehow merging this deformation space together, someone we can reduce the space.

386
00:30:06,280 --> 00:30:09,160
So the final model actually has such kind of ability.

387
00:30:09,160 --> 00:30:14,360
So it has some amount of deformation space which can control all this body face hands

388
00:30:14,360 --> 00:30:15,360
together.

389
00:30:15,360 --> 00:30:21,360
And also all this motion on body motion space is in a single skeleton space.

390
00:30:21,360 --> 00:30:29,040
And that means so hand cannot be apart from the body and we know they are somehow linked

391
00:30:29,040 --> 00:30:34,160
together by a skeleton, which is quite important for a computer graphics area, for example,

392
00:30:34,160 --> 00:30:35,800
retargeting zone.

393
00:30:35,800 --> 00:30:40,040
So yeah, that's the meaning of all this total body motion catcher and deformation model

394
00:30:40,040 --> 00:30:41,200
space.

395
00:30:41,200 --> 00:30:48,960
And so is the idea that you were expressing it with the dimensionality reduction with

396
00:30:48,960 --> 00:30:57,800
the atom model that with the Frankenstein model, these parts are, the body parts are more

397
00:30:57,800 --> 00:30:59,280
closely related.

398
00:30:59,280 --> 00:31:07,280
We understand that if as you were describing earlier, if we create a mesh model of the

399
00:31:07,280 --> 00:31:14,560
face, if the vertex and the cheek moves, then the vertex and the eyes are probably going

400
00:31:14,560 --> 00:31:15,560
to move.

401
00:31:15,560 --> 00:31:19,720
Like with these relationships are relatively easy to understand.

402
00:31:19,720 --> 00:31:22,600
But on this atom model, you've got the single parameter space.

403
00:31:22,600 --> 00:31:27,440
And so moving a parameter may have implications in multiple places.

404
00:31:27,440 --> 00:31:35,120
And so it's more challenging and requires a different approach than you might take in

405
00:31:35,120 --> 00:31:40,960
reducing the parameter space on the Frankenstein side or on the ensemble side.

406
00:31:40,960 --> 00:31:43,480
Did I capture that correctly?

407
00:31:43,480 --> 00:31:45,440
Yeah, I am mostly.

408
00:31:45,440 --> 00:31:51,080
So basically the vision why we built this Frankenstein model is actually to build this

409
00:31:51,080 --> 00:31:52,080
atom model.

410
00:31:52,080 --> 00:31:57,800
So for example, for dimension reduction, we simply use, we can simply use PCA kind of method.

411
00:31:57,800 --> 00:32:02,640
But the input of this PCA model is basically the 3D mesh model of many people.

412
00:32:02,640 --> 00:32:05,640
So different shape and different body parts.

413
00:32:05,640 --> 00:32:11,880
So they actually, this PCA can learn the variation space of these humans of deformation

414
00:32:11,880 --> 00:32:13,560
and motion changes.

415
00:32:13,560 --> 00:32:19,600
But generating such input that the mesh data, which include all these details of face by

416
00:32:19,600 --> 00:32:25,120
the enhance, I mean, making such reconstruction is surface challenging, although we use many

417
00:32:25,120 --> 00:32:32,000
many cameras because we are interested in subtle movement of facial expression.

418
00:32:32,000 --> 00:32:35,200
And we are also interested in this subtle movement of fingers.

419
00:32:35,200 --> 00:32:38,560
And at the same time, we are interested in this large movement of bodies.

420
00:32:38,560 --> 00:32:43,400
So how to place the camera is actually a big problem because if we want to capture the

421
00:32:43,400 --> 00:32:45,880
face, the camera should be very close to the face.

422
00:32:45,880 --> 00:32:50,440
If we want to capture the detailed hand, maybe people wanted to use the camera, which is

423
00:32:50,440 --> 00:32:52,360
very close to the finger.

424
00:32:52,360 --> 00:32:57,720
So doing this reconstruction to build the AIDA model, the reconstruction is surface challenging.

425
00:32:57,720 --> 00:33:02,440
So we were only to make some intermediate status by using some already available 3D mesh

426
00:33:02,440 --> 00:33:08,120
model, a deformation model, and that's the main kind of motivation of this Frank model,

427
00:33:08,120 --> 00:33:11,840
which is easier to build without doing any PCA kind of learning.

428
00:33:11,840 --> 00:33:16,880
And after we have this model, actually, we use this model to reconstruct all these details

429
00:33:16,880 --> 00:33:18,040
together.

430
00:33:18,040 --> 00:33:23,840
And then finally, we reconstruct like 100 people's shape deformations, shapes and body motions.

431
00:33:23,840 --> 00:33:28,400
And then we just put all this to this PCA kind of tool so that we can actually learn the

432
00:33:28,400 --> 00:33:31,400
deformation space.

433
00:33:31,400 --> 00:33:34,520
And there are just actually two big regions of doing that.

434
00:33:34,520 --> 00:33:41,000
For example, if we build this deformation model for entire body part, actually we can use

435
00:33:41,000 --> 00:33:46,080
that in the more challenging situation, for example, now let's say we don't have this

436
00:33:46,080 --> 00:33:47,520
motif system.

437
00:33:47,520 --> 00:33:53,240
And now we have this single image or videos from maybe YouTube or Internet.

438
00:33:53,240 --> 00:33:59,320
And let's say our goal is similarly to reconstruct this 3D body behavior by the motion of the

439
00:33:59,320 --> 00:34:00,320
person.

440
00:34:00,320 --> 00:34:03,880
This is obviously extremely challenging problem because we now have single view and we

441
00:34:03,880 --> 00:34:07,480
wanted to get this 3D motion capture data.

442
00:34:07,480 --> 00:34:13,720
But since we now we have some model, which restricts the surface to this low dimensional

443
00:34:13,720 --> 00:34:18,240
space than the original high dimensional space, actually, the problem can be much easier

444
00:34:18,240 --> 00:34:22,960
because now what we need to do is just estimating the parameter of this model, which can express

445
00:34:22,960 --> 00:34:25,360
our measurements, which is the image.

446
00:34:25,360 --> 00:34:31,000
So the major kind of, I mean, the main motivation of research at this total capture is after

447
00:34:31,000 --> 00:34:36,600
we build this model, now we can use this model to do this motion capture face by the enhanced

448
00:34:36,600 --> 00:34:41,480
motion capture in this in the wild situation, so that we can collect more and more data

449
00:34:41,480 --> 00:34:43,160
in this challenge situation.

450
00:34:43,160 --> 00:34:48,040
So that is the major kind of motivation of building this a parametry model.

451
00:34:48,040 --> 00:34:52,800
And another big reason is once we have this parametry model, actually, the data

452
00:34:52,800 --> 00:34:54,280
structure can be very simple.

453
00:34:54,280 --> 00:35:00,160
So instead of having the same mesh structure, which has some arbitrary number of vertices,

454
00:35:00,160 --> 00:35:06,000
now we have some fixed amount of parameter to express the particular motion of the person.

455
00:35:06,000 --> 00:35:11,000
So now we can consider this parameters as the input of our machine learning tools, those

456
00:35:11,000 --> 00:35:12,600
that can be the input.

457
00:35:12,600 --> 00:35:16,560
So other people's motion is abstracted as a parameter.

458
00:35:16,560 --> 00:35:21,240
And then the machine learning tool produces some parameters as the output, which is actually

459
00:35:21,240 --> 00:35:24,280
presenting some motion of the target person.

460
00:35:24,280 --> 00:35:31,680
Can you comment at all about the size of the parameter space relative to the size of

461
00:35:31,680 --> 00:35:37,920
your typical 3D vectorized or mesh type of model?

462
00:35:37,920 --> 00:35:48,280
Well, I don't remember the exact number, but the rough number is we have 21 joint for

463
00:35:48,280 --> 00:35:54,880
the body, and each hand has 21 joint, and each joint has three dimension space.

464
00:35:54,880 --> 00:36:01,040
So 21, 21, and 21, and that is 63, and each joint has three dimensions.

465
00:36:01,040 --> 00:36:07,080
So that is for the motion to represent the body motion.

466
00:36:07,080 --> 00:36:10,520
And we can also have facial expression.

467
00:36:10,520 --> 00:36:17,400
And facial expression is not explained by the sculptor movement that is explained by another

468
00:36:17,400 --> 00:36:22,000
parameterization, which is expressed in the movement of this mesh structure.

469
00:36:22,000 --> 00:36:23,760
And that has 100 dimensions.

470
00:36:23,760 --> 00:36:32,480
So 100 dimension for facial expression, and about 200 dimension for body motion.

471
00:36:32,480 --> 00:36:35,280
And we have about 40 dimension to represent this shape.

472
00:36:35,280 --> 00:36:40,640
So by changing this 40 dimension, the shape, the size, and the shape of the body, bodies

473
00:36:40,640 --> 00:36:41,640
are changing.

474
00:36:41,640 --> 00:36:49,040
But it's more or less I cannot sum them up now, but about 400 to 500 dimension to represent

475
00:36:49,040 --> 00:36:52,440
the human's behavior, including face and body and hands.

476
00:36:52,440 --> 00:36:57,040
But the original dimension can be probably, if we don't have such kind of model, let's

477
00:36:57,040 --> 00:37:00,560
say we wanted to do the similar thing by a mesh structure.

478
00:37:00,560 --> 00:37:06,000
And then the mesh we are using is around maybe 10k of vertexes.

479
00:37:06,000 --> 00:37:11,200
So each vertex has three dimension, and that is more than about 30k dimension.

480
00:37:11,200 --> 00:37:16,920
So we reduced the 30k to this 500 dimension to represent a similar thing.

481
00:37:16,920 --> 00:37:29,360
When you look at the skeletal model, plus the face, you have several 100 or so, 120,

482
00:37:29,360 --> 00:37:35,800
130, then you said you can reduce that down to 40, and then you said, then you started

483
00:37:35,800 --> 00:37:41,640
saying 400, 500, and I didn't catch where the 400, 500 came from.

484
00:37:41,640 --> 00:37:44,040
Okay, let me try to explain this again.

485
00:37:44,040 --> 00:37:51,080
So if we just consider skeletons, which is kind of a stick figure, then we forget about

486
00:37:51,080 --> 00:37:53,400
all this surface movement.

487
00:37:53,400 --> 00:37:58,680
So we can consider the joint location for the body, joint location for the finger, and

488
00:37:58,680 --> 00:38:01,960
some surface key point movement of the face.

489
00:38:01,960 --> 00:38:06,680
So each point is a three dimension space, and we can maybe consider all this number of

490
00:38:06,680 --> 00:38:08,480
3D key point.

491
00:38:08,480 --> 00:38:12,960
So that is our way to represent human body, but let's say now we are more interested

492
00:38:12,960 --> 00:38:16,200
in the subtle details of the surface.

493
00:38:16,200 --> 00:38:22,400
That means we wanted to get the mesh as the input and output of our machine line tool.

494
00:38:22,400 --> 00:38:28,960
And in this case, now instead of just a handful number of stick figure joint location,

495
00:38:28,960 --> 00:38:32,760
now we consider all these vertex locations of the surface.

496
00:38:32,760 --> 00:38:41,280
So that is around, let's say, 40K dimension, because to represent some details, we need

497
00:38:41,280 --> 00:38:43,160
many many vertices.

498
00:38:43,160 --> 00:38:50,920
So the way to reduce this is instead of having independent vertex separately, now we already

499
00:38:50,920 --> 00:38:57,080
learned the correlation of all this vertex movement, and we can reduce the space by using

500
00:38:57,080 --> 00:39:01,280
the motion space and shape deformation space.

501
00:39:01,280 --> 00:39:07,440
So motion space means we just keep the similar motion parameter for each joint, which is

502
00:39:07,440 --> 00:39:13,920
similar to stick figure, and shape deformation space is actually catching the variation of

503
00:39:13,920 --> 00:39:18,680
the surface of the given fixed body motion.

504
00:39:18,680 --> 00:39:24,920
So by changing this deformation space, actually a surface shape can be changing, and by changing

505
00:39:24,920 --> 00:39:29,440
this motion space, human can behave some body motion.

506
00:39:29,440 --> 00:39:37,280
So this deformation, shape deformation space is about 40K, sorry, about 40 dimension,

507
00:39:37,280 --> 00:39:41,600
which is really extremely low dimension, because humans are these are very, very correlated

508
00:39:41,600 --> 00:39:42,920
in this case.

509
00:39:42,920 --> 00:39:46,520
And for motion space, we have three degree of freedom for each joint.

510
00:39:46,520 --> 00:39:54,640
And we have about 60 or 70 joint, so that is about 200 dimension to represent body motion.

511
00:39:54,640 --> 00:39:59,880
And also we have about about 100 kind of dimension to expect facial expressions.

512
00:39:59,880 --> 00:40:04,200
What you're able to represent with this lowest dimension, the lowest dimension of vector

513
00:40:04,200 --> 00:40:12,640
is 40, is not just the stick figure, the body positioning, but also the surface as well.

514
00:40:12,640 --> 00:40:13,640
Is that correct?

515
00:40:13,640 --> 00:40:14,640
Exactly.

516
00:40:14,640 --> 00:40:19,520
So we have a function to convert this parameters to the original mesh space.

517
00:40:19,520 --> 00:40:22,320
The function is somehow fixed function.

518
00:40:22,320 --> 00:40:27,280
So given this parameter space, parameters we obtain, we can convert that to this mesh

519
00:40:27,280 --> 00:40:28,280
space.

520
00:40:28,280 --> 00:40:31,720
So instead of keeping the mesh space, we can just keep the parameters.

521
00:40:31,720 --> 00:40:38,600
The problem obviously here is the expressive power of this model is limited.

522
00:40:38,600 --> 00:40:47,040
So for example, usually in nowadays, this model can express some kind of some body shape

523
00:40:47,040 --> 00:40:52,200
with minimum clothing, because clothing modeling is clothing is extremely challenging,

524
00:40:52,200 --> 00:40:54,480
because of all these variations we have.

525
00:40:54,480 --> 00:41:01,160
So usually when we build this model, the input data is limited to somehow subject with minimum

526
00:41:01,160 --> 00:41:02,480
clothing.

527
00:41:02,480 --> 00:41:07,680
Because of the vision, this model can express only the mesh with minimum clothing.

528
00:41:07,680 --> 00:41:12,360
For example, if you generate any arbitrary body shape and motion from this parameter,

529
00:41:12,360 --> 00:41:18,320
the output is almost always this kind of naked body with minimum clothing.

530
00:41:18,320 --> 00:41:26,080
You've reduced the dimension down to this 40 and then you're able to use this 40 to produce.

531
00:41:26,080 --> 00:41:28,480
You said you refer to it as a fixed function.

532
00:41:28,480 --> 00:41:33,640
Is this a function that's learned through machine learning model or what does that function

533
00:41:33,640 --> 00:41:36,360
come from?

534
00:41:36,360 --> 00:41:43,560
This function is somehow actually a very common function in graphics area.

535
00:41:43,560 --> 00:41:49,280
So actually from the motion capture, if in graphics area, from the motion capture data,

536
00:41:49,280 --> 00:41:55,200
which is basically the three degree of freedom for each joint, we can manipulate, we can

537
00:41:55,200 --> 00:41:59,840
animate the 3D character and actually the 3D character output is mesh, although the

538
00:41:59,840 --> 00:42:03,520
parameter input is this motion parameter.

539
00:42:03,520 --> 00:42:08,280
Similarly, we just use the similar function, which is called linear blending skinning.

540
00:42:08,280 --> 00:42:13,800
So we have some mapping between the vertex movement and the skeleton movement and we are

541
00:42:13,800 --> 00:42:15,920
just manipulating the skeleton.

542
00:42:15,920 --> 00:42:20,160
And then as an output, we can get the location of each vertex of the mesh.

543
00:42:20,160 --> 00:42:26,840
And so you've developed this representation of the body.

544
00:42:26,840 --> 00:42:29,840
How does this tie into the larger goal?

545
00:42:29,840 --> 00:42:31,120
Yeah, exactly.

546
00:42:31,120 --> 00:42:39,160
So first, we wanted to parameterize all this face and body and hands movement in a common

547
00:42:39,160 --> 00:42:42,200
format, a common motion capture format.

548
00:42:42,200 --> 00:42:46,760
That was one of the original building this model.

549
00:42:46,760 --> 00:42:52,840
And as I mentioned, the final goal is actually to use this model to get a similar motion capture

550
00:42:52,840 --> 00:42:54,320
output in the YouTube video.

551
00:42:54,320 --> 00:42:59,280
So actually, we are working on this project and now the goal is instead of having multiple

552
00:42:59,280 --> 00:43:04,200
view, we will just have a single view, single YouTube video, and we wanted to get the motion

553
00:43:04,200 --> 00:43:05,960
capture of the target person.

554
00:43:05,960 --> 00:43:11,400
And motion capture output has all this component facing body and fingers together.

555
00:43:11,400 --> 00:43:16,720
This is ongoing research and we actually get some meaningful result at this moment and

556
00:43:16,720 --> 00:43:21,200
which we plan to submit that to a conference in the end.

557
00:43:21,200 --> 00:43:26,600
So yeah, so basically, we wanted to convert all of our collected data to these types so

558
00:43:26,600 --> 00:43:30,440
that we can actually put this to our understanding part.

559
00:43:30,440 --> 00:43:32,680
That is the current one at this moment.

560
00:43:32,680 --> 00:43:34,080
We are working on.

561
00:43:34,080 --> 00:43:43,000
And is PCA the primary place that machine learning is used in this project or are there

562
00:43:43,000 --> 00:43:46,720
other places where you're building models as well?

563
00:43:46,720 --> 00:43:52,600
Well, actually, this project is a little bit far from the popular machine learning area.

564
00:43:52,600 --> 00:43:57,640
This is more like a three construction.

565
00:43:57,640 --> 00:44:05,720
In this paper, what we show is usually this finger reconstruction and facial of face reconstruction

566
00:44:05,720 --> 00:44:09,720
battery constructions are considered as separate problems.

567
00:44:09,720 --> 00:44:16,080
But we actually somehow demonstrate that we can capture all these components together

568
00:44:16,080 --> 00:44:18,520
without using any markers.

569
00:44:18,520 --> 00:44:26,360
And to do that, maybe one of the key kind of contribution is we use this to the key point

570
00:44:26,360 --> 00:44:29,360
estimation method for each individual camera.

571
00:44:29,360 --> 00:44:33,600
And indeed, we combine them together in this 3D space using a multi-view system.

572
00:44:33,600 --> 00:44:40,080
And in the end, this 3D key point we constructed by using this 2D key point detector.

573
00:44:40,080 --> 00:44:44,640
It turns out they are working extremely well in this challenging problem and we just showed

574
00:44:44,640 --> 00:44:48,840
the result as a total battery reconstruction result.

575
00:44:48,840 --> 00:44:53,920
So in that sense, probably, it would be good to share the 2D detection, post detection

576
00:44:53,920 --> 00:44:56,800
problem our lab is working on.

577
00:44:56,800 --> 00:44:59,360
And that is the open pose work?

578
00:44:59,360 --> 00:45:00,360
Exactly.

579
00:45:00,360 --> 00:45:01,360
Yeah, exactly.

580
00:45:01,360 --> 00:45:05,120
So this 2D body pose detection area is a very popular area.

581
00:45:05,120 --> 00:45:11,080
And one of the most successful areas in computer vision in recent years.

582
00:45:11,080 --> 00:45:18,000
So now we have really great algorithms which can detect this human's body key point, multiple

583
00:45:18,000 --> 00:45:22,680
people's body key point in YouTube video and single image.

584
00:45:22,680 --> 00:45:24,800
And open pose is a version of that.

585
00:45:24,800 --> 00:45:31,520
And one good thing about open pose is this is providing all these facial key point and

586
00:45:31,520 --> 00:45:37,480
body key point and finger to the key point in a single framework in real time.

587
00:45:37,480 --> 00:45:45,160
And multiple papers our lab has been presented is included in this work.

588
00:45:45,160 --> 00:45:51,200
And actually, this is closely related to our panel text studio project because we are

589
00:45:51,200 --> 00:45:56,520
using this detector to reconstruct all the total motion in the end.

590
00:45:56,520 --> 00:46:04,320
And at the same time, we use our system to generate new annotations set to train this

591
00:46:04,320 --> 00:46:06,080
2D detector.

592
00:46:06,080 --> 00:46:15,160
So this is a very interesting idea because usually this multivit system has been used to reconstruct

593
00:46:15,160 --> 00:46:16,160
3D.

594
00:46:16,160 --> 00:46:24,600
But now we believe this multivit system can be a way to generate some 2D annotations for

595
00:46:24,600 --> 00:46:29,320
2D detector because sometimes this generating 2D detector itself can be challenging.

596
00:46:29,320 --> 00:46:35,520
For example, let's say this is one paper we published in last CVPR and the goal is to

597
00:46:35,520 --> 00:46:40,360
make this 2D hand key point detector which is now the part of the open pose.

598
00:46:40,360 --> 00:46:41,560
And the main idea is very simple.

599
00:46:41,560 --> 00:46:46,720
We just wanted to make some data set for 2D hand key point detector.

600
00:46:46,720 --> 00:46:51,440
But just annotating this 2D hand key point is a challenging problem because of all these

601
00:46:51,440 --> 00:46:53,040
side focal illusions.

602
00:46:53,040 --> 00:46:59,400
So at the beginning, we hired some annotators and we asked them to annotate this 2D key point

603
00:46:59,400 --> 00:47:01,560
given some to the image.

604
00:47:01,560 --> 00:47:07,480
But it turns out usually these fingers are glued to each other and just clicking this

605
00:47:07,480 --> 00:47:09,560
finger key point is a challenging problem.

606
00:47:09,560 --> 00:47:15,520
And our idea is if we can capture some data in this multivit system, since we have 500

607
00:47:15,520 --> 00:47:21,160
views, let's say we wanted to capture some finger movement, we wanted to annotate some

608
00:47:21,160 --> 00:47:25,920
key point of some finger movement, then since we have this many views, we can easily

609
00:47:25,920 --> 00:47:31,880
find some good view point to easily annotate the target finger part.

610
00:47:31,880 --> 00:47:38,880
More importantly, if we can have some 3D reconstruction of the target hand, if we can

611
00:47:38,880 --> 00:47:45,880
project this 3D hand to 500 views, each individual view point can have a new annotation by

612
00:47:45,880 --> 00:47:48,640
projecting this 3D hand to this single view.

613
00:47:48,640 --> 00:47:56,680
And we just used this annotation for a neural network architecture to train the 2D detector.

614
00:47:56,680 --> 00:48:01,600
In terms of this, this is extremely successful in making this 2D key point detector and

615
00:48:01,600 --> 00:48:03,600
the key point detector.

616
00:48:03,600 --> 00:48:10,360
So in a way that is, it's kind of a specialized mechanism of data augmentation.

617
00:48:10,360 --> 00:48:12,360
Exactly, exactly.

618
00:48:12,360 --> 00:48:20,560
But in this case, we can use this multi-view supervision so that we can filter out some

619
00:48:20,560 --> 00:48:21,560
noise outputs.

620
00:48:21,560 --> 00:48:27,800
For example, since the original target hand should be in the 3D space, and our image

621
00:48:27,800 --> 00:48:35,120
is just capture of the 3D space in this 2D space, so ideally, if we can generate some

622
00:48:35,120 --> 00:48:39,520
rays from each individual camera, they should intersect in this 3D space, which is the

623
00:48:39,520 --> 00:48:42,000
original 3D joint location.

624
00:48:42,000 --> 00:48:46,040
If they are not, then we can simply say that maybe the detection or 2D measurement is

625
00:48:46,040 --> 00:48:47,040
wrong.

626
00:48:47,040 --> 00:48:51,640
So we can easily filter out all these noisy cases using this multi-view supervision.

627
00:48:51,640 --> 00:48:55,240
And that is a key to make this data augmentation.

628
00:48:55,240 --> 00:48:56,240
Yeah.

629
00:48:56,240 --> 00:48:57,240
Awesome.

630
00:48:57,240 --> 00:49:01,280
Well, Han, thanks so much for taking the time to share this with us.

631
00:49:01,280 --> 00:49:07,560
Are there any final thoughts that you'd like to share to kind of close us out?

632
00:49:07,560 --> 00:49:16,320
Yeah, so currently, we are trying to release entire of our dataset, regenerated from our

633
00:49:16,320 --> 00:49:17,480
panoptic studio.

634
00:49:17,480 --> 00:49:24,120
So people can easily download 500 videos for the same input.

635
00:49:24,120 --> 00:49:31,440
And also, they can get all these 3D annotations regenerated, for example, 3D key point and also

636
00:49:31,440 --> 00:49:38,360
depth information or 3D point cloud for the same target scene, so that we believe people

637
00:49:38,360 --> 00:49:42,600
can use this kind of data to many interesting computer vision and machine learning problem

638
00:49:42,600 --> 00:49:46,520
because people now have some 2D image input.

639
00:49:46,520 --> 00:49:49,400
And they can also have corresponding 3D annotations.

640
00:49:49,400 --> 00:49:53,920
And that can be maybe interestingly trained in many interesting machine learning techniques.

641
00:49:53,920 --> 00:49:58,360
For example, given the single image, can we generate 3D skeleton, given the single

642
00:49:58,360 --> 00:50:03,840
image, can we generate depth data or 3D point cloud because we all captured this all different

643
00:50:03,840 --> 00:50:09,720
sensors output using hardware synchronization and the calibration.

644
00:50:09,720 --> 00:50:14,720
So this can be easily kind of maybe not easy, but this can be in the end used for many interesting

645
00:50:14,720 --> 00:50:15,720
machine learning problems.

646
00:50:15,720 --> 00:50:16,720
Oh, wow.

647
00:50:16,720 --> 00:50:19,920
How many images or videos are you including?

648
00:50:19,920 --> 00:50:20,920
Wow.

649
00:50:20,920 --> 00:50:26,800
Not how many unique kind of scenarios or captures are you including?

650
00:50:26,800 --> 00:50:31,920
So the original data is about 10 hours of many different situations.

651
00:50:31,920 --> 00:50:39,400
So we have captured some musical instruments like playing piano, cello, and so on.

652
00:50:39,400 --> 00:50:46,080
And we also have many social motion capture results, for example, 3D point tracking.

653
00:50:46,080 --> 00:50:51,880
Also we have captured some very simple, example range of motion, which can be easily maybe

654
00:50:51,880 --> 00:50:55,400
useful, much easier kind of problems.

655
00:50:55,400 --> 00:50:59,520
So yeah, and we should collect more and more data and we try to release all of them in

656
00:50:59,520 --> 00:51:00,520
the end.

657
00:51:00,520 --> 00:51:01,520
Awesome.

658
00:51:01,520 --> 00:51:02,520
Awesome.

659
00:51:02,520 --> 00:51:06,040
Well, once again, thank you so much, Tom, for joining us.

660
00:51:06,040 --> 00:51:07,120
Thank you for having me.

661
00:51:07,120 --> 00:51:08,120
Thank you.

662
00:51:08,120 --> 00:51:14,800
All right, everyone, that's our show for today.

663
00:51:14,800 --> 00:51:21,120
For more information on Hambio or any of the topics covered in this episode, visit twimolei.com

664
00:51:21,120 --> 00:51:23,040
slash talk slash 180.

665
00:51:23,040 --> 00:51:27,280
If you're a fan of the podcast, we'd like to encourage you to head over to your Apple

666
00:51:27,280 --> 00:51:31,840
or Google podcast app and leave us a five-star rating and review.

667
00:51:31,840 --> 00:51:36,320
Your reviews help inspire us to create more and better content and they help new listeners

668
00:51:36,320 --> 00:51:38,120
find the show.

669
00:51:38,120 --> 00:51:55,280
As always, thanks so much for listening and catch you next time.

