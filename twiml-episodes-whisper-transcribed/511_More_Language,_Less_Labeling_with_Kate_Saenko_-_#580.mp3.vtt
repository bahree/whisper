WEBVTT

00:00.000 --> 00:11.760
All right, everyone. Welcome to another episode of the Twimal AI podcast. I am of course your host,

00:11.760 --> 00:17.440
Sam Charrington. And today I'm super excited to be joined by Kate Sianco, associate professor at

00:17.440 --> 00:24.560
Boston University, and a consulting professor for the MIT IBM Watson AI lab. Before we get going,

00:24.560 --> 00:29.280
be sure to take a moment to hit that subscribe button wherever you're listening to today's show.

00:29.280 --> 00:36.400
Kate, welcome to the podcast. Thanks so much for having me. I'm looking forward to digging into

00:36.400 --> 00:45.840
our conversation. You are a very, very busy woman at CVPR. You've got a ton of workshops that you're

00:45.840 --> 00:54.080
speaking at, as well as numerous papers. We will scratch the surface of all that activity

00:54.080 --> 00:58.960
in our conversation today. But before we do that, I'd love to have you introduce yourself to

00:58.960 --> 01:04.000
our guests and share a little bit about your background and how you came to work in the field.

01:04.000 --> 01:11.760
Yeah, definitely. I am a professor of computer science at Boston University, as you mentioned.

01:11.760 --> 01:18.320
I lead a research group focused on deep learning, especially applied to visual recognition.

01:19.440 --> 01:27.600
And I'm specifically interested in many topics, some of which are things like vision and language

01:27.600 --> 01:36.560
models, and also data set bias and adaptation to out of distribution data, also efficient models,

01:37.280 --> 01:44.400
and AI forensics. And I would say that more broadly, one of my goals is to create AI that

01:44.400 --> 01:53.040
can see and can understand language and interact with humans in a natural way. And also help us

01:53.040 --> 01:58.880
solve problems in society like helping people who are visually impaired or helping the environment.

02:00.400 --> 02:10.320
So that's briefly about my research. How I came to the field, I got interested in

02:11.920 --> 02:22.400
AI pretty early on. I was always a fan of science fiction. So when I decided to go to graduate

02:22.400 --> 02:31.440
school, I started as a graduate student at MIT, actually studying speech recognition.

02:32.240 --> 02:43.840
And later on, I switched to computer vision, but I've always been really fascinated about robots

02:43.840 --> 02:51.680
and artificial intelligence and kind of related areas. So I got my PhD from MIT,

02:51.680 --> 02:59.360
and then I did a postdoc for a couple of years, and then I became a faculty member. So I've

02:59.360 --> 03:06.560
basically been working in the AI field for a long time. And it's been really amazing to see,

03:07.760 --> 03:12.320
especially in computer vision, the transformation that our field has gone through.

03:13.440 --> 03:20.160
Because when I started as a graduate student, you know, computer vision, I think I can say that

03:20.160 --> 03:30.400
it didn't work for most things. You know, certain things worked, face detection was working pretty

03:30.400 --> 03:35.600
well, but it's nothing like what we're seeing here. And it's just been so exciting to see this

03:35.600 --> 03:43.360
revolution. It hasn't really, it hasn't really been boring for me. So I'm very kind of, I think I'm

03:43.360 --> 03:48.560
lucky that I get to do this kind of work and this kind of research. Absolutely.

03:48.560 --> 03:58.800
You mentioned your interest in a vision and language together and how those, you know,

03:58.800 --> 04:05.840
you shifted from one to the next and now they're kind of coming together. And one of the things

04:05.840 --> 04:11.840
that you're doing at CVPR is speaking at the multimodal learning and applications workshop.

04:11.840 --> 04:16.160
And I guess the first thing that jumps out at me is, you know, your reference to kind of an

04:16.160 --> 04:22.320
exciting time for computer vision. It's also a super exciting time for this idea of multimodal

04:22.320 --> 04:29.040
machine learning. We've been kind of, you know, doing that for a bit, but the, you know, visual

04:29.040 --> 04:36.480
transformers and kind of the convergence of these two architectures over the past a little bit has

04:36.480 --> 04:44.640
really opened up a big opportunity there. And I'd love to hear you kind of riff on your view of

04:44.640 --> 04:50.640
multimodal as a field and the opportunity before us and kind of the research frontier.

04:50.640 --> 04:56.960
Multimodal learning has been around for a long time. Actually, I mentioned that when I was

04:56.960 --> 05:02.640
starting as a graduate student, I was actually looking into audio visual speech recognition,

05:02.640 --> 05:10.640
which is essentially lip reading. So using both audio and the movement of someone's lips to

05:10.640 --> 05:18.080
understand speech. So that's one example of multimodal learning. And then for my PhD, I actually

05:18.080 --> 05:26.880
worked on trying to learn about visual categories of different objects using text that we can

05:26.880 --> 05:37.520
scrape from the web. And this was before deep learning really took off, but kind of the ideas,

05:37.520 --> 05:43.120
ideas have been around for a long time, right? Can we learn something from both audio and video? Can

05:43.120 --> 05:51.120
we get free data, essentially free labeled data from the web? So people have been trying this

05:51.120 --> 05:59.920
for a while. And I think one of the really breakthroughs that we're seeing now is the qualitative

05:59.920 --> 06:07.920
scale in the amount of data that we can get from the web and train on, right? There was a very

06:07.920 --> 06:17.280
recent paper that I just saw talking about emergent properties of large models. And I do agree

06:17.280 --> 06:24.960
that when we reach a certain scale, you know, both in terms of training data and size. So,

06:24.960 --> 06:30.320
of course, those are correlated. You typically need a lot more data to train a very large model

06:30.320 --> 06:36.320
with lots and lots of parameters. I think that you start seeing properties that are emerging that

06:37.200 --> 06:41.840
you're not seeing with smaller models and smaller data sets. So I think that's what we're seeing now

06:41.840 --> 06:51.760
with, you know, things like Dali too. And kind of very cool results that combined vision and

06:51.760 --> 06:58.880
language. And so one one thing that I'm talking about at the workshop is how we can use this

07:00.320 --> 07:07.200
training data that is essentially free, right? So probably you've heard of Clip, which is an

07:07.200 --> 07:14.160
OpenAI model that really I would say transformed our field over the last little while because

07:14.160 --> 07:22.480
they were able to collect lots of training data by scraping photos and their captions from the web.

07:22.480 --> 07:31.040
So if we have access to this kind of data and we can learn not just from images with tags that have

07:31.040 --> 07:37.680
to be sort of manually labeled like ImageNet data. Well, we have an image and we have a category

07:37.680 --> 07:44.720
label for each image, right? So I mean, this data is good and we have been training on this data

07:44.720 --> 07:49.840
for all kinds of computer vision applications. Usually you pre-train on this data and then you

07:49.840 --> 07:55.040
fine tune on your own data set. Turns out that if you can pre-train on the captioned images from

07:55.040 --> 08:02.320
the web, you actually can do much better, especially on zero-shot learning where you have a new

08:02.320 --> 08:08.160
category that you haven't seen, you don't have any training data for this category. It turns out

08:08.160 --> 08:13.840
that these models, these vision language pre-train models actually generalize better than just

08:14.400 --> 08:21.840
traditional image classification data sets for pre-training. So you were talking about the

08:21.840 --> 08:27.520
opportunities presented by all of the information that's out on the internet that is visual

08:27.520 --> 08:33.040
information with associated text. You know, one of the obvious questions that that begs or,

08:33.040 --> 08:38.880
yeah, I guess it's obvious in light of the recent history of large language models is the

08:38.880 --> 08:48.640
inherent bias in the internet information. And I recently saw a tweet that was, I think someone

08:48.640 --> 08:54.480
with access to Dolly II or something like Dolly II typed in, you know, engineers and it was

08:54.480 --> 09:00.400
kind of these generated images of all male engineers standing around or, you know, doctors

09:00.400 --> 09:06.640
same kind of thing. Now, curious your thoughts on that. And, you know, really it's a question of

09:06.640 --> 09:13.840
like having access to, you know, that kind of volume of information is, you know, clearly beneficial

09:13.840 --> 09:18.800
in some ways, but it also has, you know, we also need to be conscious of those biases. How are you

09:18.800 --> 09:29.600
thinking about that? Yeah, I think that the bias in data sets is in some sense inevitable.

09:30.320 --> 09:35.680
And this is something that I've been working on also for a long time. Kind of how do we deal

09:35.680 --> 09:43.680
with the fact that our data sets are biased simply because they're finite, right? It's very hard

09:43.680 --> 09:51.920
to sample the entire visual world and devoid bias. And especially since, as I mentioned,

09:52.640 --> 09:59.360
we have been really trying to go to the internet for our data source in the last, you know,

09:59.360 --> 10:07.920
say decade or so, or a couple of decades. Because it's free, it's easy. Yeah, it's there. I mean,

10:07.920 --> 10:13.200
it's a lot harder to, you know, we used to actually have graduate students go to the lab and take

10:13.200 --> 10:17.280
pictures of objects. And that's, that was the training data or take pictures of people.

10:18.240 --> 10:24.080
I remember having to collect a data set in the lab and get a signed release form from every

10:24.080 --> 10:29.040
person that walked in that we recruited, you know, making sure that they're okay with us using

10:29.040 --> 10:35.520
their data. But we're in a different world now, you know, you go to the internet and you have

10:35.520 --> 10:43.840
billions of images and videos at your fingertips. And I think we're almost, it feels to me like

10:43.840 --> 10:52.480
we're in a gold rush kind of era with respect to, you know, how data is really king right now.

10:52.480 --> 10:58.240
And whoever can get their hands on the most data in some sense is winning the AI race, right?

10:58.240 --> 11:08.080
So I feel like these concerns are very much important. And we should definitely be worrying about them.

11:08.080 --> 11:17.200
That's not something that I particularly work on in, in my research. But there's great people

11:17.200 --> 11:24.400
out there who work on these kinds of more ethical questions. Although, you know, I had a paper a

11:24.400 --> 11:32.720
while back. The first time that I noticed really this kind of bias in image caption data was

11:34.080 --> 11:41.200
some of the very first times that we were able to get deep learning to generate captions for

11:41.200 --> 11:47.600
images, right? And we were working with a fairly small by today's standards data set.

11:47.600 --> 11:55.280
And, you know, at the time was a large scale data set. And, you know, when the model started

11:55.280 --> 12:01.760
working, it was amazing, you know, literally the year before that, the best captions that you

12:01.760 --> 12:10.240
could generate sounded very awkward, very robotic, you know, they would say things like there is one

12:10.240 --> 12:20.160
building and grass, you know, they were templated. And once we got the image captioning to work

12:20.160 --> 12:27.440
with language models using neural networks, you know, because one of the major advancements,

12:27.440 --> 12:33.840
of course, in image captioning is being able to generate fluent natural sounding language.

12:33.840 --> 12:44.080
And once we develop these models that no longer had two separate stages where in the first stage,

12:44.080 --> 12:48.880
you detect all the objects and the second stage of your template and you plug in the objects

12:48.880 --> 12:53.040
you detected in the sentence template, right? So that's how you get the robotic sounding captions.

12:54.400 --> 13:01.440
But so now, instead of that, we had an end-to-end model that you just feed it images and captions,

13:01.440 --> 13:08.080
and it just learns a single neural network that takes the raw image and spits out the entire

13:08.080 --> 13:14.000
sentence, the entire caption, right? And, you know, it was amazing to see, I remember this,

13:14.720 --> 13:19.920
you know, we had a paper on this and a few other labs had a paper on this, it made the New York Times,

13:19.920 --> 13:25.600
it was really amazing, you know, how fluent these captions sounded like. But then we started

13:25.600 --> 13:33.680
looking at this more and digging into results more and we didn't notice that these models definitely

13:33.680 --> 13:40.400
learn shortcuts. They learn to exploit the biases in the data. And so one of the examples

13:40.400 --> 13:49.600
that we wrote about in our paper is when you have a data set where most of the, for example,

13:49.600 --> 14:00.480
images of snowboarders have captions that say, you know, a man is snowboarding. Then the model

14:00.480 --> 14:05.840
sort of learns the shortcut that if you see something that looks like a snowboarder or perhaps

14:05.840 --> 14:15.440
even snow, you should start the sentence by a man is, you know, and then complete it according to

14:15.440 --> 14:20.800
maybe what else you detect in the image. But it's a pretty good shortcut because it will

14:21.520 --> 14:25.920
minimize your loss, your training loss, because you'll get it right, almost to the training

14:25.920 --> 14:32.560
examples. And that's all really that these models care about. So we try to address this kind of bias

14:32.560 --> 14:40.640
in the paper. But, you know, I think we still don't have a very good solution and we we definitely

14:40.640 --> 14:48.880
need more work to improve this kind of, you know, to improve our models so that they don't have

14:48.880 --> 14:55.440
these egregious biases. And it's only getting worse, right? Of course, now we have giant data sets

14:56.720 --> 15:04.400
hundreds of millions or even a billion images much, much harder to both audit them and to ensure

15:04.400 --> 15:12.080
that they don't take shortcuts like this. And the models are a lot more complex and difficult to

15:12.960 --> 15:21.120
intuit about and understand. That's right. They're very block box, right? Yeah. So your talk at

15:21.920 --> 15:26.880
this multimodal learning and applications workshop is more language less labeling,

15:26.880 --> 15:34.800
vision and language pre-training for visual tasks. It immediately calls to mind something

15:34.800 --> 15:40.400
that I've been spending a lot of time exploring recently, this whole idea of data-centric AI.

15:41.600 --> 15:46.800
And in particular, like you start out talking about the cost of labeling and the burden that that,

15:46.800 --> 15:52.480
you know, creates for folks that want to build applications in this base. How is that played out

15:52.480 --> 16:00.320
for you? This is something that I've been interested in the last few years. How do we train models,

16:00.320 --> 16:08.000
especially for new tasks or new domains without having to collect and label a lot of data? And I can

16:08.000 --> 16:15.440
talk about sort of applications where this comes up, but I think it's a lot of applications,

16:15.440 --> 16:22.960
actually. And so this idea of, okay, we're going to get a fixed data set and train on it and then

16:22.960 --> 16:28.720
that's it. That's all we need. That's really not how things work in real life. In real life, you want to

16:30.000 --> 16:36.000
build an application, you have certain things that you want your model to recognize and images.

16:36.000 --> 16:40.240
And there are often things that you don't have a lot of training data for. You have to

16:40.240 --> 16:50.480
pay someone to label it. It costs a lot of money. Depending on what the labels look like, you

16:50.480 --> 16:57.040
might actually need experts. You might need to find someone who is actually good at labeling and

16:58.720 --> 17:06.400
and the scale and it just doesn't scale very well. So I think with one of the really exciting things

17:06.400 --> 17:14.240
that people have been doing recently is figuring out, first of all, that these very large-scale

17:14.240 --> 17:20.640
models that are trained on this web-scale data, like Clip, for example, you can prompt them

17:21.280 --> 17:28.480
to get them to learn very quickly. Sometimes there's not even any learning involved. You just

17:30.480 --> 17:36.160
give them some textual input telling them what you want recognized. And then you don't need any

17:36.160 --> 17:44.320
additional training data to get a very good improvement already on your task. Of course,

17:45.360 --> 17:51.600
you can improve further if you had training data. But if you don't have any training data or

17:51.600 --> 17:57.600
only have a few labels, these kinds of prompting methods seem to be working quite well.

17:59.360 --> 18:04.400
So that's one of the things that I was going to mention in the workshop is some very recent work

18:04.400 --> 18:17.040
where we try to essentially prompt a model like Clip, pre-trained on a large web-scale data set

18:17.040 --> 18:26.240
of images and captions, and then prompt it a test time to classify categories in a zero-shot way

18:26.240 --> 18:32.880
so that where you don't have any labels for those categories, you just want the model to

18:32.880 --> 18:41.600
kind of generalize to any label that you can throw at it essentially. And it does do much better

18:41.600 --> 18:47.360
than previous work on this task. Can you talk about the methodology that you

18:48.480 --> 18:55.040
pursued in that work? Was a lot of the challenge figuring out the right way to prompt the model?

18:55.040 --> 19:04.320
So we actually have two lines of work in this direction. One is exactly this challenge of how

19:04.320 --> 19:15.440
to prompt the model correctly. For example, again, we start with a pre-trained model that already

19:15.440 --> 19:20.880
exists, very hard to retrain these models when you're experimenting with them. So oftentimes,

19:20.880 --> 19:28.880
you just download one that OpenAI has been available. So we start with a pre-trained model,

19:29.600 --> 19:36.880
especially Clip is the one that we often work with. And then we want to

19:39.280 --> 19:47.520
prompt it with essentially some textual tokens. It's already learned to take text tokens

19:47.520 --> 19:54.560
as input and the image as input, compare them, and then predict a score that basically says yes,

19:54.560 --> 20:00.080
these, this textual string and this image are highly related or no, they're not related.

20:01.600 --> 20:08.960
But during pre-training, of course, these models are learning to compare captions,

20:08.960 --> 20:14.960
right? So a caption is a more general kind of label for an image. It could say, you know,

20:14.960 --> 20:22.240
there's a group of people playing Frisbee in the park. But what we're trying to do is now take

20:22.240 --> 20:27.840
that model and just we just want to know, is there a Frisbee in this image? So we want to classify

20:27.840 --> 20:40.000
the label Frisbee or we want to classify the label car or person. So it presents a problem because

20:40.000 --> 20:44.960
the model wasn't trained for that specific task and we need to tweak it. We need to do something

20:45.840 --> 20:56.160
to get it to do this task. And so one of the contributions in this work is to figure out a way

20:56.160 --> 21:05.200
to prompt a model with both a positive and a negative prompt. So you think of the prompt as saying

21:05.200 --> 21:12.800
is there a blank in this image, for example, where the blank is the word for the class you're trying

21:12.800 --> 21:18.880
to detect like Frisbee. And what that does is it makes the task a lot more similar to what the

21:18.880 --> 21:26.000
models learned about because it seems to the model that you're giving it a caption essentially.

21:26.000 --> 21:31.760
But it's a fake caption, of course. You just created a fake caption and just inserted the word

21:31.760 --> 21:39.600
for the class that you want to recognize. And so what we did is in addition to this positive

21:39.600 --> 21:46.320
prompt where we say is there a blank in the image and we replace the blank with the with the label.

21:46.320 --> 21:53.760
So is there Frisbee? We also give it a negative prompt which is saying essentially think of it as

21:53.760 --> 22:03.680
the negative question. Is there no Frisbee in the image? And then we compare the score that the

22:03.680 --> 22:09.520
model gives for the positive and the negative. And if the positive score is higher, then we predict

22:09.520 --> 22:16.880
that yes, there is a Frisbee in the image. In a sense, it's a bit analogous to the kind of games

22:16.880 --> 22:22.880
that you might play with a human label or with human inhuman labeling where you ask multiple

22:22.880 --> 22:29.440
labelers to label an image and compare their responses. This is just kind of a clever way to do that.

22:29.440 --> 22:37.840
Not to minimize it, but yeah, yeah, exactly. And then we had to do some other tricks to extract

22:37.840 --> 22:49.520
the spatial information in a more fine grain matter. I don't want to. Yeah, it's actually not

22:49.520 --> 23:00.320
nothing, nothing fancy. But what the image and caption model does is it takes the whole caption

23:00.320 --> 23:07.680
and compares it to the whole image. And it makes sense, right? Because when you're training the

23:07.680 --> 23:11.680
model, you don't know which part of the caption corresponds to which part of the image, right? So

23:11.680 --> 23:16.960
a group of people playing Frisbee in the park, you don't know where the Frisbee is located in the

23:16.960 --> 23:24.640
image, right? But for our task, we are only looking for the Frisbee, not the entire scene.

23:25.280 --> 23:32.080
So we actually wanted to focus in a more fine grain way on the region where the object might be.

23:32.640 --> 23:41.760
So what we do is we do some surgery on this network to to instead compare our prompt at each location

23:41.760 --> 23:50.480
in the image. Okay. And then decide if the object is there instead of first kind of aggregating

23:50.480 --> 23:54.240
over the whole image and then comparing. So that turns out that works a little bit better.

23:55.440 --> 24:02.800
Kind of akin to a convolutional window, we're just taking slices and passing them into the model.

24:02.800 --> 24:13.120
Yeah, actually, the model already has features that are that it's computing at each sub window in

24:13.120 --> 24:18.880
the image. It's just that then that later on, it is pooling them and aggregating them into a

24:18.880 --> 24:24.960
single vector. And we just kind of go in and do some surgery and compare before it does. So actually,

24:24.960 --> 24:30.400
we don't learn, I think the cool thing about this work is that we don't learn any new parameters

24:30.400 --> 24:36.960
except for these prompt tokens. So all the surgery we do, we don't introduce any learnable parameters,

24:36.960 --> 24:44.080
which means there's very little overhead and learning. So we need very little data to actually

24:44.080 --> 24:49.680
tune the model to do this task. I think in our paper, we report something like

24:51.920 --> 24:57.280
you know, something like maybe 20,000 parameters, the extra that we're learning.

24:57.280 --> 25:05.360
And it's it's just these token embeddings that are fed in as the prompt. And the these token

25:05.360 --> 25:14.240
embeddings, is there a fixed set of labels that you like you're starting with a fixed set of labels,

25:14.240 --> 25:18.560
then you create these prompts and you know, that's where the token embeddings come from.

25:18.560 --> 25:26.160
Yeah, so the tokens are the prompt. So you can think of them as some words, but actually,

25:26.160 --> 25:32.640
we're just learning some arbitrary words. And each word is embedded into a continuous vector.

25:32.640 --> 25:37.840
So that's what I mean by token embeddings. So you can think of them as some words like is there a

25:38.720 --> 25:44.720
something in the image? We're not actually specifying what words the model should use. We're just

25:44.720 --> 25:53.840
letting it learn some words. Yeah, I think I wasn't following that part. I thought I heard you

25:53.840 --> 26:00.160
describing something more like you identified some relatively standard patterns or something

26:00.160 --> 26:05.360
like that, like a template for these prompts. But rather these prompts are learned and they're

26:05.920 --> 26:13.120
they're learned for each of the classes that you're trying to be able to predict like it's you

26:13.120 --> 26:19.280
know, if it's tree a tree, the prompt is specific to tree. Like it's the prompt that produces the

26:19.280 --> 26:25.040
best result for tree as opposed to what might work for car. Is that am I thinking about that

26:25.040 --> 26:34.720
correctly? Yes, you are correct. Although we also did it the other way where the prompt is generic

26:34.720 --> 26:40.640
and you just plug in the word that you want to recognize. So this is really zero shot learning

26:40.640 --> 26:48.720
because you're not even tuning the prompts for any specific class you you you're tuning the

26:48.720 --> 26:54.320
prompts on one set of classes and then the user gives you some new class. You just plug it into

26:54.320 --> 27:01.440
the prompts that you've learned. It turns out that also works and our approach also improved

27:01.440 --> 27:06.240
performance on this kind of zero shot recognition even though the prompts are learned in a generic

27:06.240 --> 27:15.920
way and not specific to these categories. The evaluation on the topic of evaluation. Are you

27:16.560 --> 27:27.760
what are you comparing against? We are comparing against existing models that try to do this kind

27:27.760 --> 27:39.280
of label classification and images. Some of which are probably not working as well because they're

27:39.280 --> 27:45.440
not utilizing these very large vision language models that we're utilizing like Clip.

27:47.200 --> 27:54.720
And this is something that you know often comes up now. Is it fair to compare a model that isn't

27:54.720 --> 28:03.600
using something that was pre-trained on 400 million images with captions? And you know I

28:05.840 --> 28:13.360
I really go back and forth. I think in some sense yes it's not fair and in fact we don't even know

28:13.360 --> 28:19.360
these huge datasets. We don't know what's in them. OpenAI hasn't released the dataset that they

28:19.360 --> 28:24.080
trained on. It could in fact include some of our test data because it's straight from the web.

28:25.760 --> 28:30.320
So it doesn't seem like the best experimental protocol to be using. But on the other hand

28:31.680 --> 28:37.040
they work so well you know it just you can't deny that you get much better performance

28:37.360 --> 28:40.320
and people are using them. So the cat's out of the bag so to speak.

28:41.520 --> 28:46.560
On the other hand models do tend to work well if they've seen the training day. Exactly.

28:46.560 --> 28:54.240
And we don't know if the model has seen the training day or not. I mean we also do compare

28:54.240 --> 29:03.200
a model to kind of an original clip that hasn't been tuned in the way that I described and we do

29:03.200 --> 29:09.280
perform better than that. So there is an advantage to what we're doing when we compare the more

29:09.280 --> 29:17.440
apples to apples way. But I think what's happening now is I think that researchers really are going

29:17.440 --> 29:27.200
to have to keep up with the latest and greatest pre-trained backbones and you know you just the

29:28.560 --> 29:37.920
the method you develop can improve performance on your dataset with respect to a backbone like

29:37.920 --> 29:43.040
ResNet for example. But someone could come along with a backbone like visual transformer

29:43.040 --> 29:46.640
and upperform your whole algorithm just because they have a much better backbone.

29:49.680 --> 29:53.760
It's like I said you know the gold rush or the space race or whatever you want to call it you know

29:53.760 --> 29:59.760
the bigger the bigger the better. And yeah it's a little bit hard I think for researchers to

29:59.760 --> 30:05.600
keep up with this. Yeah I was just going to ask how does that how does that land for you as a

30:05.600 --> 30:13.200
researcher like there's been a lot of talk around you know just the amount of resources that are

30:13.200 --> 30:20.080
going into these you know quote unquote foundation foundational models you know changes what avenues

30:20.080 --> 30:25.440
are available to what researchers just based on the resources of their organizations and

30:26.560 --> 30:31.120
changes the research directions that are accessible to them. What's your take on that?

30:31.120 --> 30:41.200
Yes I think that's a very real issue for many people especially in academic research labs

30:41.200 --> 30:48.480
but also in many industrial research labs there are really only a few labs or companies

30:49.360 --> 30:57.840
that can afford to even train these very large scale models anymore right. And it's great

30:57.840 --> 31:04.880
when they make them publicly available because then the rest of us can kind of experiment with them

31:04.880 --> 31:10.240
we can use them as the feature backbone and or new new algorithms that we're developing. I think

31:10.240 --> 31:20.000
that there are some things that probably are really capabilities that as I mentioned earlier

31:20.000 --> 31:26.880
could be emerging only when you have a very large training data set or a very large model that

31:26.880 --> 31:34.320
you're training. And so that means that certain kinds of research is now in the realm of only

31:34.320 --> 31:41.840
those lucky few that that have access to both the data and the compute. So the other paper is called

31:41.840 --> 31:49.760
prefix conditioning unifies language and label supervision and it's a collaboration with google

31:49.760 --> 32:01.360
and so here we're also looking at prefix conditioning or prompting rather but we have a slightly

32:01.360 --> 32:08.560
different goal where as I mentioned the large vision language models are pre-trained on images

32:08.560 --> 32:16.080
with captions right and that that gives us a lot of data that we get for free essentially from the

32:16.080 --> 32:23.280
web but we also do have some label data sets the traditional data sets like image net which

32:23.280 --> 32:32.480
have an image with a category label. And these two kinds of data are complementary right the

32:34.320 --> 32:40.240
captioned images have a long tail distribution over object so they may not

32:40.240 --> 32:48.080
cover all the categories that you want to cover or they may cover them unequally or like we mentioned

32:48.080 --> 32:54.800
before they might have some bias like all the snowboarders or mail in the caption at least.

32:54.800 --> 33:03.200
And so we might want to combine that data with just traditional labeled image data that is human

33:03.200 --> 33:09.920
annotated and clean or less bias free and I'm not saying image net is completely unbiased either

33:09.920 --> 33:16.960
so if we wanted to do that for pre-training it turns out that's what we look at in this paper

33:17.840 --> 33:24.320
it turns out that you can do better than just combining the two kinds of data you can

33:24.320 --> 33:35.280
so there's work from Microsoft that actually try to transform the labels in image net into

33:35.280 --> 33:41.280
fake captions right so if you have an image an image net labeled with Persian cat you can

33:42.320 --> 33:48.480
make a caption out of it by just plugging it into some template like a close-up of a Persian cat

33:49.760 --> 33:57.200
so so they did this and then when you train on both the fake captions and the real captions

33:58.080 --> 34:05.040
you do get a stronger model you get a more powerful model that combines the knowledge from

34:05.040 --> 34:10.240
these two kinds of data it's kind of like a data augmentation type of approach yes exactly

34:11.120 --> 34:17.600
and it generalizes even better to zero shot tasks so new data sets with novel categories

34:18.560 --> 34:28.000
it generalizes even better to them so so one issue though is that now you have two kinds of

34:28.000 --> 34:36.320
input data sets one with real captions and one with fake captions and these you know deep models they're

34:37.760 --> 34:42.320
they're powerful which means they take shortcuts as we've been saying and so what

34:43.280 --> 34:50.000
what we found is happening is the model actually knows more or less that it's learning from these

34:50.000 --> 34:56.080
are fake captions and these are real captions and it sort of tries to learn from both kinds of captions

34:56.080 --> 35:02.480
but then it ends up you know not really knowing at test time when you give it a new image

35:02.480 --> 35:09.840
what to do because it basically one way to think of it is it's a little confusing for the model to

35:09.840 --> 35:16.240
have you know real captions and fake captions and so what we ended up doing is just something super

35:16.240 --> 35:26.240
simple which is add a token a training time to the real captions that just the token is just saying

35:26.240 --> 35:32.560
this is a real caption and the fake captions we add a token that says this is a fake caption

35:33.840 --> 35:42.080
and when you do that the model actually learns much better it generalizes better it

35:42.080 --> 35:50.720
um essentially helps the model overcome this these two kinds of domains by telling it look

35:50.720 --> 35:55.040
these are two kinds of domains so you can process them a little bit differently right so they're

35:55.040 --> 36:00.400
there's some specialized processing in the language model that we see emerge from that where

36:00.400 --> 36:06.560
if you give it the the caption and then you prefix it with the real caption token it will do one

36:06.560 --> 36:11.520
thing but if you prefix that same caption with the fake caption token it will do another thing and

36:11.520 --> 36:20.320
so for example you know if you prefix with the fake caption token the model will mostly focus on

36:20.960 --> 36:28.240
kind of the noun because it's learned that these fake captions they're just fake so the only

36:28.240 --> 36:34.320
information there is the noun right so and everything else is just kind of a template to

36:34.320 --> 36:41.920
fool it into yeah it's for exactly but if you prefix it with the caption real caption token then

36:41.920 --> 36:48.640
it will start looking at the whole caption because it knows that in real captions there is useful

36:48.640 --> 36:55.520
semantic information in multiple words across the whole caption so so it's kind of cool

36:55.520 --> 37:01.120
it is an example come to mind of you know the illustrates kind of the richness of a real of a

37:01.120 --> 37:10.960
real caption relative to one of these fake captions yeah so this is an example from the paper it's

37:10.960 --> 37:20.000
not a particularly rich caption but if you have a caption that says a sculpture of an airplane

37:21.760 --> 37:29.120
and you prefix it with the you know this is a fake caption then the model ignores the word

37:29.120 --> 37:36.960
sculpture and just focuses on the word airplane right and then if you prefix it with the real

37:36.960 --> 37:44.000
caption token then the model will actually look at the sculpture as well as the airplane

37:45.440 --> 37:53.760
in some sense this seems counterintuitive in the sense that you know that I guess the the

37:53.760 --> 37:59.520
usual thought is hey more data different types of data that will kind of force the model to

37:59.520 --> 38:06.640
generalize and that's going to lead to better performance and you're kind of saying that you know

38:06.640 --> 38:12.880
more data different types of data the model is not generalizing it's just learning that the

38:12.880 --> 38:21.760
classes and cheating right I think it is a little counterintuitive but I think we've seen this

38:21.760 --> 38:31.200
in other work too that when you're training with different kinds of data heterogeneous data

38:32.080 --> 38:39.280
sometimes it's better to specialize the model you know make it aware that it's being trained with

38:39.280 --> 38:47.680
heterogeneous data and because you know I think if you throw everything together you're giving it

38:47.680 --> 38:52.800
also less guidance right adding this prefix token is sort of just giving the model more

38:52.800 --> 38:59.120
information look you know these two sets of images are coming from two different kinds of domains

38:59.760 --> 39:04.480
and then the model can use that or not use that and you know in this case it seems to use it to

39:04.480 --> 39:11.280
to its advantage kind of on the topic of domain generalization but maybe switching modalities a

39:11.280 --> 39:18.320
little bit one of the other papers that you're speaking about at CBPR is focused on domain

39:18.320 --> 39:25.920
generalization an unsupervised manner and this idea of kind of bridging across domains

39:25.920 --> 39:31.200
and visual domains in particular can you talk a little bit about that paper and the motivation

39:31.200 --> 39:37.600
there yeah definitely so this paper it's called unsupervised domain generalization by learning a

39:37.600 --> 39:47.920
bridge across domains and we are looking at this problem of generalization to new kinds of visual

39:47.920 --> 39:59.360
domains so an example is let's say your model was trained on driving data collected from a car

39:59.360 --> 40:08.480
in California and then at test time you're giving it data from let's say Boston or New York in the

40:08.480 --> 40:14.160
winter people look different because they're wearing heavy coats or hats or maybe it's raining or

40:14.160 --> 40:23.040
maybe the trees are look different right so the visual domain has changed even though the

40:23.040 --> 40:28.480
categories you're looking for are the same it's still people and cars and another example of this

40:28.480 --> 40:38.480
is when you train the model on real photos but then you get a clip art or a painting or some other

40:38.480 --> 40:45.040
type of drawing perhaps so this is a more extreme even domain shift in your input data

40:47.120 --> 40:55.760
and you know there's been a lot of interest in robustness where we want models to be robust to

40:55.760 --> 41:01.920
to kind of changes in the image so if you change the top left pixel you know your model shouldn't

41:01.920 --> 41:09.120
all of a sudden flip its answer right and and there's a lot of work on adversarial robustness where

41:09.120 --> 41:15.280
if an adversary changes the top left pixel in a certain way to make the model flip the answer

41:15.280 --> 41:24.800
we don't want that either but I also think that a very very practical model is not some adversary

41:24.800 --> 41:34.240
even or even degradation in the image quality but just just kind of a different viewing angle or

41:34.240 --> 41:41.600
slightly different lighting or you know a little bit of a difference in how that object looks

41:41.600 --> 41:47.200
like like a person with a hat on as opposed to no hat or a winter hat as opposed to summer hats

41:47.200 --> 41:53.200
okay so all of these kinds of variations we want models to generalize to them right we don't

41:53.200 --> 41:59.520
want them to break and they do break they they they currently you know models do not generalize

41:59.520 --> 42:09.760
well to out of domain data data that is distributed differently from the training set so in this

42:09.760 --> 42:18.480
paper we are trying to fix that by doing unsupervised learning where we have a bunch of images

42:18.480 --> 42:25.360
so one example again I'll go back to paintings real images and clip art and sketches right so these

42:25.360 --> 42:31.680
are images of the same classes but from different domains and what we want the model to do is we

42:31.680 --> 42:39.760
wanted to learn that for example a giraffe is the same as a painting as it is in a sketch or in

42:39.760 --> 42:46.400
a real photo right but if you train the model with self supervised losses you know this kind of

42:46.400 --> 42:55.200
contrastive learning losses or sim clear moco all of these popular unsupervised training objectives

42:56.720 --> 43:03.360
what the model does is it tries to learn how images are similar and it ends up actually learning

43:03.920 --> 43:09.120
the domain similarity before category similarity in this case so we'll learn that a sketch of a

43:09.120 --> 43:18.080
giraffe is closer to a sketch of a guitar than it is to a photo of a giraffe right because it's

43:18.080 --> 43:27.040
picking up on those more superficial features are are you making the general statement that unsupervised

43:27.040 --> 43:40.320
approaches have fared worse in multi domain or you know non-constant non-single domain scenarios then

43:43.360 --> 43:51.200
then supervised approaches because of this general tendency to favor domain similarity versus

43:51.200 --> 44:02.480
object similarity yes I think I think I will risk making that claim you know we have a couple papers

44:02.480 --> 44:08.880
that supported within certain parameters but yes I think what happens when you have multi domain

44:08.880 --> 44:15.760
data with class labels you know the class labels are telling the model that this is a giraffe even

44:15.760 --> 44:22.320
though it's a sketch and this is a giraffe even though it's a photo and so focus on the

44:22.320 --> 44:30.640
giraffeness of the thing as opposed to other you know other types of correlations exactly right

44:31.440 --> 44:38.640
so so just that supervision of you care about the category not the domain that helps you learn a

44:38.640 --> 44:47.040
more generalizable representation but when you have unlabeled data the only training single signal

44:47.040 --> 44:53.520
you're giving to your model is okay if you take this sketch of a giraffe and you augment it

44:53.520 --> 44:59.440
with some you know you add some noise or crop it or rotate it or whatever people do or even just

44:59.440 --> 45:04.480
find its nearest neighbor it should be closer to that nearest neighbor than to some other image in

45:04.480 --> 45:12.640
your data that's farther away and and there's no category information there so the model doesn't

45:12.640 --> 45:18.880
know that the giraffe should be close to the painting giraffe should be close to the sketch

45:18.880 --> 45:23.680
giraffe because in pixel space there's a huge difference between those two images

45:23.680 --> 45:38.240
yeah so just very briefly to describe the main idea in the paper we essentially learn this bridge

45:38.240 --> 45:45.840
domain so think of it as creating a version of each training image that tries to remove all the

45:45.840 --> 45:52.800
domain specific information and only keep the general kind of outline and the general features of

45:52.800 --> 46:00.480
the object right so for the giraffe example it ends up looking kind of like an edge image

46:01.520 --> 46:08.320
but that was my impression yeah yeah but unlike an actual you know cany edge detector image or

46:08.320 --> 46:16.080
traditional edge detector which also picks up on a lot of edges on the background seeing you know

46:16.080 --> 46:24.080
a lot of irrelevant edges or it may on the other hand remove edges that are important like the spots

46:24.080 --> 46:30.160
on the giraffe because you know that's just edge detection it doesn't know what's important

46:30.160 --> 46:37.360
and what isn't in our approach this bridge domain is edge-like but it keeps the semantically

46:37.360 --> 46:43.440
important features and edges like the outline of the giraffe and the spots in the giraffe but

46:43.440 --> 46:52.480
removes all the irrelevant edges and the edge domain is is learned as well kind of end-to-end

46:53.840 --> 47:00.160
as part of as part of training the whole model yes so and the important thing is that we don't just

47:00.160 --> 47:07.360
we don't actually use that edge domain to do the final classification it's actually

47:07.360 --> 47:17.840
use to compute which images are similar to each other and so I think one criticism of this

47:17.840 --> 47:22.320
approach initially might be wait a minute but if you're using an edge-like domain aren't you

47:22.320 --> 47:28.480
throwing away color information well in fact we're not the model is still learning color features

47:28.480 --> 47:34.880
but to learn which objects are similar and which ones are not we're using this bridge domain which

47:34.880 --> 47:44.880
is edge-like so does that mean like in the final in the loss function there's you know some factor

47:44.880 --> 47:52.320
relating to the you know distance in the you know the edge domain space and also another factor

47:52.320 --> 47:58.640
relating to distance in the the actual image space yes and in fact the distance in the edge domain

47:58.640 --> 48:08.240
space guides the the model in learning it tells it which which are the similar pairs and which are

48:08.240 --> 48:16.400
the dissimilar pairs so that it can train in the unsupervised way from the original images

48:16.960 --> 48:24.960
right so think of it as kind of this we're faking some supervision that is a little bit stronger

48:24.960 --> 48:30.640
hopefully than just the model by itself trying to figure out which images are similar and which

48:30.640 --> 48:36.240
ones are not and being confused by all of these extraneous kind of textures and domain specific

48:36.240 --> 48:49.440
features and so in creating this edge domain space like how I get how handcrafted is that in a

48:49.440 --> 49:00.480
sense like I'm imagining if you you know found a way to create some embedding of the you know similar

49:00.480 --> 49:08.400
classes that it's not particularly likely that you're going to come out with these cool looking

49:08.400 --> 49:16.320
edge images like that that's more more handcrafted is what I guess the feel that I'm having here

49:16.320 --> 49:26.720
so I think that it seems maybe handcrafted because we initialize the generator with an edge

49:26.720 --> 49:38.320
edge image so the part of the model that learns to map the original images to the bridge domain is

49:38.320 --> 49:47.680
initialized by computing edges of those images and so originally it learns just edge maps but

49:48.640 --> 49:55.280
then as we keep training it in the sense supervised way to kind of learn about objects essentially

49:56.560 --> 50:04.400
it starts to get away from the original just edge detection kind of images and starts to for

50:04.400 --> 50:12.000
example remove some of the relevant background edges and keep the relevant ones that are useful

50:12.000 --> 50:21.280
for computing kind of objects similarity well we'll be linking to your workshop presentations

50:21.280 --> 50:28.640
and slides and folks can kind of dig into those for the papers that you've worked on and that you

50:28.640 --> 50:38.320
recognize cited but I want to thank you for a great discussion once again sounds like you're

50:38.320 --> 50:45.600
going to be super busy as CVPR yeah but it'll be fun the first in-person conference in three years

50:45.600 --> 50:51.440
so I'm excited well Kate thanks so much for taking the time to share with us a bit about what you've

50:51.440 --> 51:00.640
been up to thank you for having me

