1
00:00:00,000 --> 00:00:22,000
All right, everyone. I am here with Luke Stark. Luke is an assistant professor at Western University in London, Ontario. Luke, welcome to the Twimble AI podcast.

2
00:00:22,000 --> 00:00:24,000
Thank you. It's great to be here.

3
00:00:24,000 --> 00:00:42,000
It's great to have you on the show. You are a ethics and AI researcher among other things and you've been a vocal critic of computer vision and facial recognition in particular, and I'm looking forward to digging into those topics with you.

4
00:00:42,000 --> 00:00:57,000
But before we do, I'd love to have you share a little bit about your background and how you came to work in the field.

5
00:00:57,000 --> 00:01:13,000
I was trained by training, by academic training, and I was not a big computer kid. I liked science fiction a lot, but I was mostly reading books. It wasn't too much coding or too much electronical engineering.

6
00:01:13,000 --> 00:01:25,000
I ended up doing my PhD in media studies because I kind of got sick of the fact that historians sort of by convention often kind of stopped their stop their work about 20 to 30 years prior to the present.

7
00:01:25,000 --> 00:01:28,000
That's changing a little bit, but that was that's kind of the tradition.

8
00:01:28,000 --> 00:01:53,000
And so I did a PhD in media studies at NYU and ended up working with a philosopher of technology there named Helen this and bomb, who was at the time thinking about digital privacy until you just come up with a book about privacy and context, the idea that privacy is one of these things that isn't just about control of your data, but it's about kind of socially appropriate or socially contextual data transfers and data flows.

9
00:01:53,000 --> 00:02:17,000
And this was about 2010. And as it turned out, Helen had been thinking about not just privacy, but other human values and computing systems since the mid 90s, which she was kind of part of a, you know, a fairly small niche bunch of folks who were in law schools in information schools in media studies departments who had been had been thinking about these questions.

10
00:02:17,000 --> 00:02:23,000
And, and that's why I got along, I work with her, did a bunch of work with her.

11
00:02:23,000 --> 00:02:39,000
And, and about 2016, 2017, all of a sudden, right, there was this huge upsurge in interest in machine learning, right, and also a huge interest in the social impacts of machine learning ethical AI.

12
00:02:39,000 --> 00:02:53,000
And, and all of a sudden, it felt like all the things we have been talking about in this fairly kind of constrained corners of academia were things that, you know, government's interested in companies are interested in the general public is interested in.

13
00:02:53,000 --> 00:03:03,000
And, and so that's that's been sort of where my work has been ever since I was sort of just lucky lucky to be in in on the ground floor, say, say.

14
00:03:03,000 --> 00:03:19,000
And, that you have been critical of facial recognition, that seems like a good place to start one of your articles from a couple of years ago, describe facial recognition as the plutonium of AI.

15
00:03:19,000 --> 00:03:33,000
And, in the, in the introduction to that piece, you kind of acknowledge that it comes off to some as alarmists, but then you proceed to back it up the analogy.

16
00:03:33,000 --> 00:03:43,000
You know, tell us about that what, why that analogy and more importantly, why do you think that facial recognition is so dangerous.

17
00:03:43,000 --> 00:03:57,000
That piece was inspired by some earlier work I had been doing with a wonderful friend and colleague and Lauren Hoffman at Washington on thinking about metaphors in first big data and then eventually AI.

18
00:03:57,000 --> 00:04:07,000
And how often these natural metaphors get used right to talk with, you know, data mining data lakes, you know, people called data, the new bacon.

19
00:04:07,000 --> 00:04:13,000
I mean, it was kind of a bit of a new oil, the new oil, exactly, I'm from bacon.

20
00:04:13,000 --> 00:04:25,000
And, and so I was thinking a lot about data metaphors and I was thinking, okay, so, so you can use metaphors in a way, you know, to make a product seem or make data seem appealing.

21
00:04:25,000 --> 00:04:38,000
But can thinking about these metaphors do something else, can it, can it kind of orient people towards, you know, some of some of the challenges and maybe problems with, with different kinds of AI technologies.

22
00:04:38,000 --> 00:04:59,000
And the reason I came to plutonium was because I was reading a lot of it, I was really inspired by work in in race and technology studies right work by amazing scholars who have been thinking about the impacts of digital technologies on race and racism on racial categorization.

23
00:04:59,000 --> 00:05:10,000
And how the internet has shaped, you know, has reshaped those conversations over the last 20 years, people like Lisa Nakamura, Simone Brown, Wendy Chun, you know, really, really amazing scholars.

24
00:05:10,000 --> 00:05:21,000
And I want to say, I mean, I think my work is absolutely indebted to and builds on this really path breaking work of these other other people, many of whom are women of color, right, especially black women.

25
00:05:21,000 --> 00:05:43,000
So, so right and what they've been arguing for a long time for decades, right, is that the, you know, the way that digital technologies kind of encourage categorization, right, either new categorizations of people or, or they kind of build in old suspect problematic, you know, racist categorizations.

26
00:05:43,000 --> 00:05:58,000
That that has a kind of toxicity because these, these technologies are now so ubiquitous right there in our pockets, they're, they're making decisions about hiring and, you know, bail and, and all these important things in our lives.

27
00:05:58,000 --> 00:06:13,000
So, and so, I take that, that, that evidence, you know, historical evidence, empirical evidence from sociologists really seriously, and I think the analogy of plutonium is being physically toxic is to facial recognition is socially toxic, right.

28
00:06:13,000 --> 00:06:24,000
And, and then the, you know, at the time that I wrote the piece, I guess that was 20, early 2019, like 2018, the conversation around facial recognition was very different than it is today.

29
00:06:24,000 --> 00:06:34,000
I just shifted a lot. In 2018, there were a few people in media studies sort of saying, this is a really challenging problematic technology.

30
00:06:34,000 --> 00:06:44,000
What he heard so I'm going to have an challenger to legal scholars wrote a piece about the kind of the kind of the kind of democratic danger, the danger to democracy of facial recognition.

31
00:06:44,000 --> 00:07:02,000
I wrote this piece and other people had also written other pieces, but, but I think in the last two to three years, you know, more and more jurisdictions, right, especially municipalities in the states and elsewhere have really come to say we, you know, we don't want this kind of this kind of surveillance being used by government.

32
00:07:02,000 --> 00:07:17,000
There's often mandates to that the police don't use it for obvious reasons, right, because it is so tied up in this history of, of, you know, of unjust policing of policing of minorities, especially black Americans.

33
00:07:17,000 --> 00:07:38,000
And, and so I think I think that social toxicity of facial recognition has become more people become more aware of it, right, they see it in more parts of their of their everyday lives, their, you know, there are stories about how how it screws up, right, how it doesn't it doesn't do the job of capturing certain kinds of faces.

34
00:07:38,000 --> 00:07:47,000
And how it can be used to, you know, repress dissent. And so I think all of those things have meant have meant that social toxicity is increasingly clear.

35
00:07:47,000 --> 00:08:11,000
The, the distinctions that you make in the piece in your work generally is that the, your critique is not so much that, you know, it's used improperly like we've seen examples of local policing organizations use facial recognition improperly and they could have used it better perhaps.

36
00:08:11,000 --> 00:08:37,000
And, you know, it's not necessarily that it's used in the wrong domains, but rather your, your take is that it's kind of fundamentally, it's fundamentally dangerous and so dangerous that it shouldn't be used or should only be used in very, very specific highly regulated situations analogous to plutonium.

37
00:08:37,000 --> 00:08:42,000
You elaborate on on where that, where that contention comes from.

38
00:08:42,000 --> 00:08:55,000
Yeah, sure. And, and I, I have this, I have this, this kind of kind of folksy folksy analogy, I'm becoming analogous, I guess, of, of thinking about a rotten onion. So take a take a rotten onion, right.

39
00:08:55,000 --> 00:09:09,000
You got the onion, you know, it's got these layers and let's say, you know, you take the skin off the onion and the first, the first layer of the skin is of the onion is rotten. That's, that's kind of analogous to the kind of concerns about use and deployment official recognition, right.

40
00:09:09,000 --> 00:09:16,000
So, okay, so, right, we have all these problems with it being being used by the wrong people in the wrong hands for the wrong reasons.

41
00:09:16,000 --> 00:09:25,000
So let's say you say, okay, I accept those, those problems, I accept those critiques, right. I'll peel that first rotten layer of the onion off, right. Hopefully, maybe the next layer down will be better.

42
00:09:25,000 --> 00:09:35,000
Then the second layer of the onion are the, the kind of concerns about technical bias that that have been now lively are articulated about about facial recognition.

43
00:09:35,000 --> 00:09:47,000
So, right, so, so Joe Boilwini and Tindy Jebrew's work on gender shades, right. You know, makes the point that a lot of computer vision data sets and facial recognition data sets are not trained on a diverse set of faces.

44
00:09:47,000 --> 00:09:55,000
So, they're, they're picking up, especially dark skin women, much less precisely than they are picking up white men, right.

45
00:09:55,000 --> 00:10:15,000
And there have been lots of studies, and this is a big critique, not just recognition, but things like analysis, right. One of a great scholar Lauren Rue at Institute of Maryland has talked about this and the kind of emotion recognition that emotion recognition recognition recognition systems recognize white smiles better than they recognize black smiles, right.

46
00:10:15,000 --> 00:10:30,000
So similar, similar thing. That's a much harder problem to solve, right. Because of the, you know, the challenges of getting contextually appropriate data, the challenges of having having diverse data sets, the challenges of our privacy, right.

47
00:10:30,000 --> 00:10:43,000
You know, so there's a whole bunch of technical problems. But let's hypothetically say, and I don't actually think you could fix this, but let's hypothetically say you could fix it. Let's have hypothetically say you could peel that second layer of the onion off that rotten onion.

48
00:10:43,000 --> 00:10:57,000
So now you got like the third, like the core of the onion, right. And to me, this layer of the onion is also rotten in the context of facial recognition because conceptually facial recognition systems, right.

49
00:10:57,000 --> 00:11:09,000
Even though they're not, you know, they're not making any, you know, the people are used to making the judgements about what the technology is doing. But the system itself is designed to put, put numbers to faces, right.

50
00:11:09,000 --> 00:11:22,000
Put, quantify the face and put that face that's quantified in conversation with other faces, right. So, so even if you are doing simple straightforward identification, right.

51
00:11:22,000 --> 00:11:43,000
You're still, you know, you're still identifying the fact that there's a face pattern there in the first place, right. And when you start putting that into like databases, and you start looking at how much that kind of comparison of facial vectors, I guess you could say

52
00:11:43,000 --> 00:12:08,000
what happens, that kind of quantification of the face, even if it's not intended to be, is really, you know, a big chunk of the textbook definition of racism, right. Racism is about, you know, using a kind of external physical quantification, physical, physical features, and making judgements about an individual based on those features, right.

53
00:12:08,000 --> 00:12:18,000
But in my view, almost every kind of kind of facial recognition technology in some way participates in that kind of quantifiable judgment.

54
00:12:18,000 --> 00:12:28,000
And that's why I think that's the third part of the onion is rotten. And I think right if you, and then if you get rid of that, if you get rid of that third part of the onion, you have no onion left.

55
00:12:28,000 --> 00:12:38,000
And so, you know, there's, there's no point and there's no point in investing in a rotten onion. If every layer at every level, there are these instrumental problems.

56
00:12:38,000 --> 00:13:02,000
So that third part of the onion, the argument there is that kind of digitizing faces and just any kind of judgment or analysis or decision making based on the digital digital model of faces is.

57
00:13:02,000 --> 00:13:14,000
Yeah, the argument is that any kind of digital model of, really, it's not just the face, it's the human body.

58
00:13:14,000 --> 00:13:31,000
More generally, right, is open to exploitability as a kind of around racism, right. So it's, it's, it's, so the textbook definition of racism, right, is using exterior physiological traits, physical traits to make judgements about.

59
00:13:31,000 --> 00:13:34,000
You know, the inner, the inner self, right.

60
00:13:34,000 --> 00:13:49,000
And so all of these systems are doing that first part of that definition, they're all, they're all mapping, they're dependent on the kind of quantified mapping of extra physiological traits of the face of the body of whatever.

61
00:13:49,000 --> 00:14:06,000
And unless you're not doing anything with that mapping, unless you, unless unless it's purely a descriptive matching mapping, right, which is not, I don't think really how, you know, that's not very useful in a lot of the ways these, these systems that deployed.

62
00:14:06,000 --> 00:14:19,000
And then the system, you know, is being asked by its designers, is being designed by its designers to make judgements about the faces, the bodies, the digital profiles that it, that it's, it's taken in.

63
00:14:19,000 --> 00:14:33,000
And potentially to, to make put them into a hierarchy, right, potentially to classify them, potentially to, you know, to add, in the case of facial analysis, right, to say, you know, to make a prediction about, about age or gender or whatever.

64
00:14:33,000 --> 00:14:53,000
And I just think, I just think right that, that, that so right, you have, you have there, both the kind of, the kind of textbook definitions of racism, it's quantified external features, and, you know, being, being being judged in some way, and put into kind of hierarchies, whether deliberately or not, right.

65
00:14:53,000 --> 00:15:05,000
I think one thing important to say here is that I don't, I don't think many, you know, technologists and designers think they're being racist, right, they're not, they don't necessarily have racial animus.

66
00:15:05,000 --> 00:15:34,000
But, but one of the, one of the things I take from, from the work of somebody like Wendy Chun, who I mentioned, right, this amazing theorist of race and technology is, is that racism itself, race as a concept is a kind of technology, right, it's kind of a technique, right, it's a technique that the powerful have always used in lots of different context, of course, in the American context, very specifically often around black Americans, but, you know, racism more broadly, right, between to non white people.

67
00:15:34,000 --> 00:16:03,000
It's a technique, a tech, a classificatory technique, right, it's a race, race as a concept, you know, puts people into particular orders, right, it, it, it, it works a bit like, you know, conceptually, what facial recognition systems or any, any kind of recognition system that is, that is mapping the body and mapping many bodies and putting them into kind of into these databases is doing, doing in a dumb way.

68
00:16:03,000 --> 00:16:31,000
Right, doing in a way that, that is, that is just, that's what it's meant to do. It's, and I think it's important to think about that, that, that kind of systemic definition of race and racism, right, that idea of race as a technique, because, you know, you can, you can be using that technique and not be realizing it, right, you can not maybe not realize that technique is threaded through, you know, the, the kind of classification systems that you're, that you're working with.

69
00:16:31,000 --> 00:16:48,000
So you, you seem to be suggesting that that, you know, third core layer of the onion is problematic because it's, you know, or maybe, maybe put another way that facial right, all facial recognition is fundamentally racist.

70
00:16:48,000 --> 00:17:01,000
Yeah, yeah, if you, if you, yeah, basically, if you want to, if you want to boil it down, yeah, just trying to get to, like, so does that extend to, like, face unlock for your phone, is that fundamentally racist.

71
00:17:01,000 --> 00:17:25,000
I think it is, I think it is because just by the act of deciding to put numbers to your face, right, Apple is, or whoever's using face unlock, not to pick on Apple face, you know, these facial unlock systems are,

72
00:17:25,000 --> 00:17:33,000
doing a kind of judgment about what a face is, right, they're saying, okay, I'm going to, I'm looking for a particular pattern of light and dark of, whatever.

73
00:17:33,000 --> 00:17:36,000
And I'm going to define that pattern as a face.

74
00:17:36,000 --> 00:17:43,000
And that's, that's the edge case, right, the edge case that that kind of simple one to one recognition.

75
00:17:43,000 --> 00:17:52,360
um is is like the probably the the kind of right that the the the argument that that that's that

76
00:17:52,360 --> 00:17:59,800
that's racist is intrinsic to the the quantification aspect right it's it's not because it's it's one

77
00:17:59,800 --> 00:18:04,680
to one you're not necessarily putting it in conversation with other faces right and we talk about

78
00:18:04,680 --> 00:18:10,360
that you know we talk about this in in this paper physiognomy AI that I recently um written

79
00:18:10,360 --> 00:18:15,240
with um a great legal scholar Joanne Hudson we talked about that as the edge case we say there are

80
00:18:15,240 --> 00:18:20,600
some I think some good kind of philosophical reasons why even just the identification of the

81
00:18:20,600 --> 00:18:28,280
face with numbers you know is it kind of kind of kind of suggest right it kind of it kind of pushes

82
00:18:28,280 --> 00:18:33,720
you towards this kind of racializing classification scheme and certainly when you put multiple faces

83
00:18:33,720 --> 00:18:41,320
together then that that kind of produces and what is it about the numbers that inherently gives rise

84
00:18:41,320 --> 00:18:47,880
to racism what about other types of judgment based on faces like we go through life all day

85
00:18:48,840 --> 00:18:55,640
making judgment based on faces does that mean that human existence is fundamentally racist

86
00:18:56,840 --> 00:19:02,360
no but I think human existence is fundamentally fundamentally based on on these kind of these

87
00:19:02,360 --> 00:19:06,360
kind of inferences about people these conjectures about people this is a great question right this

88
00:19:06,360 --> 00:19:11,880
is actually often often a response that you get when you talk about these these problems okay you say

89
00:19:11,880 --> 00:19:16,440
okay um you know yeah people do this all the time people are always making judgments about people

90
00:19:16,440 --> 00:19:21,720
based on on what they look like and their face and there are two a couple things to say about that

91
00:19:21,720 --> 00:19:27,320
first right there they're sort of right these these two battling battling uh folks saying right

92
00:19:27,320 --> 00:19:31,800
you know you can't judge a book by his cover versus you know you you never get a second chance to

93
00:19:31,800 --> 00:19:37,400
make a first impression right so there's this kind of there's this kind of um opinion is divided

94
00:19:37,400 --> 00:19:43,800
right about just how useful these individual inferences about people are right and there's been

95
00:19:43,800 --> 00:19:49,720
lots of work in psychology that talks about how much stereotype and other kind of other kind of

96
00:19:49,720 --> 00:19:55,080
incorrect inference permeates what we're doing a lot at the time in in in our daily lives

97
00:19:56,040 --> 00:20:01,240
we're much better at making inferences about people if we know them well right so I uh you know

98
00:20:01,240 --> 00:20:05,800
I know I can I can kind of infer things about my my mother what she's thinking or what how she's

99
00:20:05,800 --> 00:20:13,240
feeling that I wouldn't be able to infer from a total stranger and the the real problem right then be

100
00:20:13,240 --> 00:20:16,120
kind so there are so there are all these problems with this kind of individual personal

101
00:20:16,120 --> 00:20:20,920
inference um lots of books have been written about it right lots of songs were written about

102
00:20:20,920 --> 00:20:26,280
missing etc okay but when you try to scale that right when you try to say okay we're we're going to

103
00:20:26,280 --> 00:20:33,160
make a science out of this we're going to find kind of regular and repeatable patterns in these

104
00:20:33,160 --> 00:20:39,400
in in in faces we're going to infer from one particular person's you know whatever you know some

105
00:20:39,400 --> 00:20:43,720
some set of things about them with some degree of accuracy right that's that's what that's when

106
00:20:43,720 --> 00:20:48,120
it all breaks down right because it's too brittle right it's it's not you you're never going to be

107
00:20:48,120 --> 00:20:58,200
able to um capture the you know capture the the kind of the potentially the reason why that that

108
00:20:58,200 --> 00:21:04,440
inference works and that inference even if it's 70 percent you know accurate at a population level

109
00:21:05,480 --> 00:21:09,240
is also never going to be really applicable down to the individual level right this is what

110
00:21:09,240 --> 00:21:13,960
called it's called the ecological fallacy and statistics just to to be clear so you're you're

111
00:21:13,960 --> 00:21:23,640
shifting focus in the conversation to the paper you did which is the physiognomic AI and that is

112
00:21:23,640 --> 00:21:33,400
broadly a critique of um you know these kinds of studies that will try to look at faces and determine

113
00:21:34,040 --> 00:21:38,280
you know any number of things what are some examples of like the most egregious things you've seen

114
00:21:38,280 --> 00:21:45,000
sure well so so race is one of them right I think I think so in terms of my own trajectory right so

115
00:21:45,000 --> 00:21:51,080
thinking about race and racial race and facial recognition right well well it's not just race or

116
00:21:51,080 --> 00:21:55,800
ethnicity that these systems report to predict claim to be able to predict they claim to do be able

117
00:21:55,800 --> 00:22:02,040
to do things like predict your gender your age your emotion uh your sexual orientation your

118
00:22:02,040 --> 00:22:07,800
political leanings right there's a hole there's been a whole raft of of kind of dubious

119
00:22:07,800 --> 00:22:13,800
machine learning you know kind of kind of prediction studies um and I think I think you know I think

120
00:22:13,800 --> 00:22:19,640
race is a really critical example here because it's you know it's exactly what we're talking about

121
00:22:19,640 --> 00:22:23,640
but but at a bigger you know that physiognomic AI and the problem is that at a scale that's not

122
00:22:23,640 --> 00:22:31,880
just about race right it's looking at these um you know these these these various identity categories

123
00:22:31,880 --> 00:22:39,720
and trying to make assumptions about that identity category um in ways that at that macro level at

124
00:22:39,720 --> 00:22:45,160
that level of populations that then gets translated back down to the individual um can often be

125
00:22:45,160 --> 00:22:51,080
wrong right it can it can be it can be an accurate um and if it and whether or not it is accurate or

126
00:22:51,080 --> 00:22:57,480
not it can you know if it's used in various ways it's it's it's hugely damaging my general sense is

127
00:22:57,480 --> 00:23:04,840
that most of the technologists I interact with look at these studies you know very critically

128
00:23:04,840 --> 00:23:11,480
to say the least um and wonder how they get published uh but of course people write them and

129
00:23:11,480 --> 00:23:15,800
publish them yeah you know and and I think there's a there's a whole conversation to have about

130
00:23:16,520 --> 00:23:21,720
an interesting conversation to have about you know basic machine learning research and applied

131
00:23:21,720 --> 00:23:26,600
machine learning research and how how even when researchers don't quite realize it the line

132
00:23:26,600 --> 00:23:31,640
between them is very very narrow right um I so I was a I was a postdoctoral researcher at

133
00:23:31,640 --> 00:23:36,200
Microsoft research in their fairness group for two years and I mean I really saw this first hand

134
00:23:36,200 --> 00:23:41,640
right a lot of researchers you know I have good intentions they um they actually really want

135
00:23:43,560 --> 00:23:49,560
tools and scaffolding to do a good job and to not not you know trace been to these problematic

136
00:23:50,200 --> 00:23:55,560
deployments um and and so there is a kind of a kind of a set of structural questions around

137
00:23:55,560 --> 00:24:01,080
research but it's just right it's so easy to to productize this stuff right um I'm thinking of

138
00:24:01,080 --> 00:24:07,000
I'm thinking especially of of these various smartphone filters and apps right where you know

139
00:24:07,000 --> 00:24:12,120
they they say oh we'll we'll turn your face into an 80 year old man's face or we'll we'll more

140
00:24:12,120 --> 00:24:19,080
fit so that you look you're you look uh you know stereotypically female or something right um and

141
00:24:19,080 --> 00:24:26,120
that's that's um you know that's really easy to do but I think it's also another example of how how

142
00:24:26,120 --> 00:24:33,320
ubiquitous like this this this this kind of trying to kind of this digitizing of the features and

143
00:24:33,320 --> 00:24:38,440
and kind of and kind of this kind of um this categorizing of features is becoming because of facial

144
00:24:38,440 --> 00:24:42,200
recognition and I and I think you know that those those filters okay maybe they're doing good fun

145
00:24:42,200 --> 00:24:47,640
except for the you know when uh you have a filter that says make you know the beautifying filter

146
00:24:47,640 --> 00:24:53,960
and that beautifying filter um the actual um visual effect is to lighten your skin right or it's

147
00:24:53,960 --> 00:25:00,280
to change the shape of your face uh so that's that's a huge problem um yeah so that's that's that's

148
00:25:00,280 --> 00:25:06,520
that's I mean I and I and I agree I mean there are there is a lot of a lot of good internally critical

149
00:25:06,520 --> 00:25:10,600
work in machine learning and in the tech sector and I think increasingly large amounts of that I

150
00:25:10,600 --> 00:25:18,600
think um I I was asked to give a give a talk about this stuff at CVPR um really this year and

151
00:25:19,320 --> 00:25:23,720
yeah and there was and there was a lot of you know a lot of interest and a lot of a lot of thoughtful

152
00:25:23,720 --> 00:25:32,120
conversation about it um I think that the you know I think I think that there there is a real anxiety

153
00:25:32,120 --> 00:25:36,280
on the part of a lot of researchers about censorship written about sort of being told what to

154
00:25:36,280 --> 00:25:43,320
research and what not to research um and I think that anxiety you know I think that anxiety

155
00:25:43,320 --> 00:25:49,400
is understandable but I think if we all stop to think for a little bit there are lots of boundaries

156
00:25:49,400 --> 00:25:54,120
on on what kinds of research we do right and there are lots of kind of ethical rules and guidelines

157
00:25:54,120 --> 00:26:01,160
and in every academic discipline nobody's just kind of just you know out there kind of plowing over

158
00:26:01,160 --> 00:26:08,200
uh you know every every every social constraint and so I think um you know because there's been

159
00:26:08,200 --> 00:26:16,280
this body of work um that thinks things seriously about about things like classification um I think

160
00:26:16,280 --> 00:26:19,640
it's time and I think it is I think it's good that it's it's now being thought about in the theater.

161
00:26:20,280 --> 00:26:25,560
Yeah we we've talked quite a bit about kind of the I don't know if you call it changing

162
00:26:25,560 --> 00:26:32,680
perceptions but kind of the broader context in which um we're evaluating facial recognition

163
00:26:32,680 --> 00:26:39,480
we've alluded to some of the shifts that have happened in the acceptability of using facial

164
00:26:39,480 --> 00:26:47,240
recognition and policing how would you characterize the I guess the regulatory landscape or the

165
00:26:47,240 --> 00:26:53,320
the legal landscape I know that's a topic that you're interested in and of course it's you know

166
00:26:53,320 --> 00:26:58,440
there's many many municipalities it's hard to do broadly but what do you think are the kind of

167
00:26:58,440 --> 00:27:08,040
the major landmarks um recently yeah so yeah municipal bands and moratorium on facial

168
00:27:08,040 --> 00:27:12,520
recognition have been a big a big kind of part of the regulatory landscape of the last couple of

169
00:27:12,520 --> 00:27:19,720
years in the US um uh cities like I believe like Oakland like Cambridge, Massachusetts um and then

170
00:27:19,720 --> 00:27:27,560
and then also uh you know new focus on uh laws that protect broadly things like biometric privacy

171
00:27:27,560 --> 00:27:33,000
right um the Illinois biometric privacy law um is just one of the strongest in the states

172
00:27:33,000 --> 00:27:39,880
and and it has been like very very heavily attacked by various um big tech firms because it stops

173
00:27:40,520 --> 00:27:48,520
you know various kinds of um of filters various kinds you know various kinds of of facial facial

174
00:27:48,520 --> 00:27:55,640
tech from from being deployed or used in Illinois so I think that's been notable I think that

175
00:27:56,600 --> 00:28:02,200
um there has of course been in the the American context a lot of conversation in Congress about

176
00:28:02,200 --> 00:28:09,960
tech regulation broadly um you know unclear if we'll actually see any legislation but um what

177
00:28:09,960 --> 00:28:15,320
has been notable is the kind of recent set of high rings at the FTC right the Federal Trade Commission

178
00:28:15,320 --> 00:28:20,760
which has sort of become the kind of de facto tech regulator um it has been for a long time around

179
00:28:20,760 --> 00:28:28,120
privacy because of of its mandate to um you know to deal with unfair or deceptive business practices

180
00:28:29,160 --> 00:28:33,880
of which there have been sort of many many in the context of of kind of privacy breaches and

181
00:28:33,880 --> 00:28:39,240
dark patterns and that kind of thing and so I think that that the FTC is really gearing up to think

182
00:28:39,240 --> 00:28:45,320
about all these questions kind of holistically not just privacy but also um and this is something we

183
00:28:45,320 --> 00:28:52,440
argue in we argue for in in this in this recent paper but for um you know understand understanding

184
00:28:52,440 --> 00:28:57,080
things like biometric classification and physiognomy classification as potentially being

185
00:28:57,800 --> 00:29:03,240
per se unfair and deceptive um precisely because right you're doing kind of a classification

186
00:29:03,240 --> 00:29:11,160
on on an individual right that is is is is making assumptions about the individual right that

187
00:29:11,160 --> 00:29:17,240
that may not be accurate right no no classified classifier is 100% accurate um and so that may be

188
00:29:17,240 --> 00:29:24,440
both unfair and deceptive right depending on uh on the context yeah yeah when you looked at the

189
00:29:24,440 --> 00:29:36,680
physiognomic AI landscape what did you find was the kind of the motivator behind that work was it

190
00:29:38,040 --> 00:29:45,480
you know people with the data set and just playing around was it um yeah I don't know you know

191
00:29:45,480 --> 00:29:50,680
I look at a lot again you know we look at these and like you know you see them pop up on Twitter

192
00:29:50,680 --> 00:29:56,120
every few months and it's like okay you know once again correlation not equal causation blah blah blah

193
00:29:56,760 --> 00:30:03,160
um and I I'm curious you know where they come from in terms of thinking yeah and I I think

194
00:30:03,160 --> 00:30:08,440
and I think there's a there's a kind of complex like overlapping set of motivation I think that

195
00:30:08,440 --> 00:30:18,120
the kind of the world of academic prestige and reputation and controversy and and PR

196
00:30:18,120 --> 00:30:24,840
uh does a lot in in feeling some of this work right I think you know if you right if you publish

197
00:30:24,840 --> 00:30:30,360
this study that makes this kind of big claim about being able to tell you know tell somebody's

198
00:30:30,360 --> 00:30:36,680
uh sexual orientation via via a data set or by by facial recognition or political orientation

199
00:30:36,680 --> 00:30:43,400
right you're gonna get written up in all the tech press right um you know it's part of the kind

200
00:30:43,400 --> 00:30:49,320
of the kind of ecosystem of content and clickable headlines and everything in it you know I think

201
00:30:49,320 --> 00:30:54,920
I think that I think right I think they're so so sometimes there's there's there's more more

202
00:30:54,920 --> 00:31:00,360
serious work or more interesting work um that gets written up in a sensational way and then

203
00:31:00,360 --> 00:31:05,000
sometimes there's work that kind of that kind of I think is is aimed at kind of getting

204
00:31:05,000 --> 00:31:12,120
sensational headlines um so I think that's part of it I think um and I you know I also I'm really

205
00:31:12,120 --> 00:31:18,680
aware of of the of the kind of very the very heated nature of of ML research right I mean these are

206
00:31:18,680 --> 00:31:24,440
there's a lot of money involved um you know there's there's there's all this back and forth between

207
00:31:24,440 --> 00:31:32,280
the private sector and universities um everybody's incentivized to make big claims right and and

208
00:31:32,280 --> 00:31:37,480
and and especially the press is sort of incentivized to not be careful about either the claims that

209
00:31:37,480 --> 00:31:43,480
they make or or checking the claims that people make um and then you know and and then governments

210
00:31:43,480 --> 00:31:47,640
pick up on it and they sometimes they sort of believe it right or or you know people people

211
00:31:47,640 --> 00:31:51,480
you know companies believe it customers believe it you want customers to believe it because you

212
00:31:51,480 --> 00:31:57,640
want to sell them the technology at a certain level and I think you know I think that's that's a

213
00:31:57,640 --> 00:32:02,520
lot of a lot of where this is all coming from and this kind of this kind of kind of swirling hype

214
00:32:02,520 --> 00:32:10,200
world um around machine learning um and and this desire right this desire there's a real desire to

215
00:32:10,200 --> 00:32:17,080
want to um peel back the skin and like get to the truth of of things about people right that's

216
00:32:17,080 --> 00:32:20,200
that's not a that's not a desire that started with machine learning machine learning is just the

217
00:32:20,200 --> 00:32:27,240
latest tool that that kind of keeps that that kind of desire going right um in the paper that

218
00:32:27,240 --> 00:32:33,400
you mentioned the dynamic AI you know we talk a lot about the 19th century um you know

219
00:32:33,400 --> 00:32:38,920
computers then but um a similar set of desires to kind of try to like identify people from their

220
00:32:38,920 --> 00:32:47,000
exterior traits right a similar set of desires um to uh to you know to to categorize people to

221
00:32:47,000 --> 00:32:54,120
kind of understand who people are and what they're doing um and I think I think that um I think that

222
00:32:54,120 --> 00:32:58,360
that longstanding that long sun and kind of desire for that certainty I think that actually that's

223
00:32:58,360 --> 00:33:04,600
actually the core problem right it's that these technologies are the symptom but this huge huge

224
00:33:04,600 --> 00:33:09,480
unease about about that you might have to like actually you know engage with somebody and find out

225
00:33:09,480 --> 00:33:14,840
their perspective as opposed to running some sort of sensor over them and getting the information

226
00:33:14,840 --> 00:33:19,400
about them I think that's that's a much longer history um and that's and that's why I think

227
00:33:19,400 --> 00:33:24,520
actually being a historian in this context is really helpful because right the new the tech is new that

228
00:33:24,520 --> 00:33:30,040
that kind of ideology or that kind of that kind of you know um wish is not

229
00:33:32,040 --> 00:33:38,360
in the in that paper did you find examples or in your research broadly in the area did you

230
00:33:38,360 --> 00:33:47,000
find examples of commercial products that are based on uh this research ideas there have been

231
00:33:47,000 --> 00:33:53,160
lots of applications of of of the the kind of simple emotional analysis for instance right

232
00:33:53,160 --> 00:33:57,320
and things like hiring right so higher view it was a famous case right they they they have claimed

233
00:33:57,320 --> 00:34:03,560
that they've stopped doing um a lot of the the kind of facial recognition um or facial analysis

234
00:34:03,560 --> 00:34:08,120
work on emotion but they um you know there there's still a lot of concerns about hiring in general

235
00:34:09,000 --> 00:34:16,280
um there are um a lot of really alarming um examples in the in the kind of sort of law enforcement

236
00:34:16,280 --> 00:34:21,000
national security space both both in the end states and overseas right about around kind of doing

237
00:34:21,000 --> 00:34:27,560
these analyses making judgments and assumptions about suspects um and and there's an increasing

238
00:34:27,560 --> 00:34:32,600
interest in in using these technologies in education right um the pandemic has really meant

239
00:34:32,600 --> 00:34:37,080
right a lot more screen time and and a whole bunch of companies have have sort of moved into the

240
00:34:37,080 --> 00:34:42,760
space promising analytics to help children learn right which which involves analyzing their

241
00:34:42,760 --> 00:34:49,160
attentiveness analyzing there you know their demeanor uh stuff like that so you know it's it's a

242
00:34:49,160 --> 00:34:53,640
kind of it is a quite a heterogeneous set of technologies and you know not not every technology

243
00:34:54,360 --> 00:35:01,240
um is seeking to classify or identify the same thing uh and so in the paper we we tried to kind of

244
00:35:01,800 --> 00:35:07,560
get a as high level a kind of definition as we could to kind of capture the broad range of um

245
00:35:07,560 --> 00:35:14,280
um of where we see the tech going so I wanted to kind of introduce into the conversation some

246
00:35:15,160 --> 00:35:20,920
late breaking news this is not characteristic for the this podcast but since we started our

247
00:35:20,920 --> 00:35:26,040
recording Facebook announced that they're going to be shutting down their facial recognition system

248
00:35:26,600 --> 00:35:31,800
obviously you haven't been reading that but you've been following what the company's been doing um

249
00:35:31,800 --> 00:35:37,240
and any thoughts on on that yeah that's interesting um I think that's I think that's first of all

250
00:35:38,120 --> 00:35:44,040
indicative of how toxic the term facial recognition has gotten um also apparently how like

251
00:35:44,040 --> 00:35:46,920
how ubiquitous somebody was somebody the other day was mentioning that they

252
00:35:48,120 --> 00:35:53,240
that that people sort of sometimes he kind of use facial recognition as synonymous for any kind

253
00:35:53,240 --> 00:35:58,360
of computer vision right oh we use facial recognition to identify this guitar that's interesting

254
00:35:58,360 --> 00:36:03,720
but any who facial recognition you know as a as a as a as a term is clearly really really toxic and

255
00:36:03,720 --> 00:36:09,640
clearly something that the tech companies don't want to be involved with um Facebook uh is of

256
00:36:09,640 --> 00:36:17,640
course um rebranding itself right it's it's it's it's it's it's made had a lot of um a lot of press this

257
00:36:17,640 --> 00:36:23,800
week uh going to to meta um talking about about the the metaverse what's the metaverse the metaverse

258
00:36:23,800 --> 00:36:29,880
is virtual reality right the metaverse is about about um it which includes creating digital

259
00:36:29,880 --> 00:36:38,600
avatars of of users right Facebook um has been researching how to do physiological scans of bodies

260
00:36:38,600 --> 00:36:47,080
to produce more more precise for accurate avatars for some time and so uh Facebook may be shutting

261
00:36:47,080 --> 00:36:51,320
down some aspect of its photo facial recognition right but Facebook is still very interested in

262
00:36:51,320 --> 00:36:58,360
collecting data about the body and using that data to collect information about its users to

263
00:36:58,360 --> 00:37:04,680
to to model it um in fact VR technologies collect enormous amounts of of of data not just about

264
00:37:04,680 --> 00:37:09,560
the face right but about heart rate movement uh right you name it right there they're they're

265
00:37:09,560 --> 00:37:17,800
doing a very granular um uh scan of of of uh of what what you know how you're moving how you're

266
00:37:17,800 --> 00:37:24,120
acting um it's interesting that this comes up actually because there is a whole connection with

267
00:37:24,120 --> 00:37:30,440
these conversations around facial recognition and physiognomy and stereotypes with animation

268
00:37:30,440 --> 00:37:37,880
actually right animation in terms of you know cartoons in terms of CGI right um and one of the big

269
00:37:37,880 --> 00:37:43,560
challenges with animation is that the easiest way to convey emotion and animation is through

270
00:37:43,560 --> 00:37:49,640
a stereotype you know it's through some kind of physical you know big eyes or or it's tiny mouth

271
00:37:49,640 --> 00:37:55,080
right and so you get all of these conventions all these stereotypes um from animation that

272
00:37:55,080 --> 00:37:59,160
that begin to inflect how we think about people you know how we think about people's emotional

273
00:37:59,160 --> 00:38:03,080
expression and the other kinds of expression um and now we're getting to the point right where

274
00:38:03,080 --> 00:38:09,400
where social media companies are trying to uh collect you know to try to animate

275
00:38:09,400 --> 00:38:16,040
users right for their own for their own ends um so um an interesting move by Facebook but but

276
00:38:17,240 --> 00:38:22,360
not not maybe totally shocking or unexpected you also raised an interesting point that

277
00:38:23,000 --> 00:38:30,120
while they are shutting down facial recognition in many ways their stated direction requires

278
00:38:30,120 --> 00:38:40,120
a lot requires them to do what you find most uh most frustrating about facial recognition which

279
00:38:40,120 --> 00:38:47,080
is digitize the body and uh make representations of that and maybe in some other universe or some

280
00:38:47,080 --> 00:38:51,320
digital world right some some digital representation exactly and and you know when a bunch of companies

281
00:38:51,320 --> 00:38:55,960
have done this over the last couple of years right so uh what did Amazon said they were going to

282
00:38:55,960 --> 00:39:00,120
do a moratorium on using their facial recognition systems and systems for police departments okay

283
00:39:00,120 --> 00:39:05,160
let's if that's very narrow you know all these all these announcements from various companies sounded

284
00:39:05,160 --> 00:39:10,440
good they got good headlines right if you dug into it they were you know either they weren't really

285
00:39:10,440 --> 00:39:14,520
in that business anyway and and so there was no skin off their back to say they weren't going to

286
00:39:14,520 --> 00:39:19,560
be in any more right or they or it was a very limited part of of what they were what they were

287
00:39:19,560 --> 00:39:26,680
inviting so um yeah yeah I um I think that's I think that's deflection right on Facebook's part

288
00:39:26,680 --> 00:39:32,680
and I think again that's actually partially why um why we we really wanted to go beyond facial

289
00:39:32,680 --> 00:39:39,240
recognition right in this paper and say um there's this whole class of of classifying

290
00:39:39,240 --> 00:39:43,560
technologies and classificatory technologies and um and because you know you know because we

291
00:39:43,560 --> 00:39:47,720
we don't you know we don't want to play black and roll all the time right people in tech off

292
00:39:47,720 --> 00:39:52,920
and complain that that regulators can't keep up with the technology but part of that is because

293
00:39:52,920 --> 00:39:56,520
of the argument that every technology is new even if some of these technologies have a lot of

294
00:39:56,520 --> 00:40:03,080
common denominators in terms of conceptual basis or the technical basis um and so that's that's

295
00:40:03,080 --> 00:40:10,120
kind of I think you know I think I think a ban on on physiognomic AI analysis yeah would you

296
00:40:10,120 --> 00:40:15,240
know you could apply that to thinking about virtual reality avatars especially if those are

297
00:40:15,240 --> 00:40:21,080
grounded in digital data um and if they're being you know if if if if machine learning is being

298
00:40:21,080 --> 00:40:27,720
used to extrapolate or you know that animation do you have a goal in terms of kind of a regulatory

299
00:40:27,720 --> 00:40:35,160
framework or or thought in terms of what that needs to look like to to be effective yeah so again

300
00:40:35,160 --> 00:40:40,520
this is where the historian hat comes in kind of handy uh I think that right as I sort of have been

301
00:40:40,520 --> 00:40:47,480
saying this problem this problem of of trying to infer things about people at scale is a pretty old

302
00:40:47,480 --> 00:40:56,600
problem right it it it's at least a couple hundred years old and it it you know it for a long time

303
00:40:56,600 --> 00:41:03,160
was was sort of felt by two groups of people broad groups of people experts in the lab right

304
00:41:03,160 --> 00:41:10,360
scientists of various kinds and the kind of the kind of often marginalized populations who got

305
00:41:10,360 --> 00:41:15,000
experimented on right whether that's very broadly right whether that's people you know people

306
00:41:15,000 --> 00:41:19,560
people in colonial you know in in colonized societies who were who were getting you know getting

307
00:41:20,200 --> 00:41:24,040
you know 19th century scientists administrators you know going in and doing stuff to them

308
00:41:24,040 --> 00:41:29,640
whether it's poor people in you know in in Europe and North America right so right those marginalized

309
00:41:29,640 --> 00:41:35,560
people you know understood the in the kind of injustices of these sorts of classifications very well

310
00:41:36,280 --> 00:41:41,320
but you know the kind of the kind of the kind of privilege you know and I count myself as part of

311
00:41:41,320 --> 00:41:45,640
this part of this group right kind of privileged in North America you know white white middle class

312
00:41:45,640 --> 00:41:50,760
people they didn't really see you'll always see the kind of the kind of negative side of classification

313
00:41:50,760 --> 00:41:58,360
and modification so now with AI being so ubiquitous in being being deployed right you know in white

314
00:41:58,360 --> 00:42:05,480
color professions right in in all in all sorts of different spaces um I think it it just means

315
00:42:05,480 --> 00:42:12,600
that that the injustices and and power asymmetries of of classification of the of the body are just

316
00:42:12,600 --> 00:42:18,280
are now more apparent to people for whom they weren't always apparent before um and so right what

317
00:42:18,280 --> 00:42:22,200
so that means it's an old problem that that people are suddenly waking up to not some sort of new

318
00:42:22,200 --> 00:42:29,800
problem and so I think the the end game in terms of regulation is to think really carefully about

319
00:42:30,760 --> 00:42:35,800
when you do inference about humans at all right what you know what how what kind of

320
00:42:35,800 --> 00:42:42,440
if you do is that inference being done solely for research right is is our certain kind of

321
00:42:42,440 --> 00:42:46,440
inferences that are done for research you know is that does that put that research off limits for

322
00:42:46,440 --> 00:42:53,560
commercialization um how does you know how to how to governments how to states you know handle the

323
00:42:53,560 --> 00:42:58,040
kind of data that they have under their control and the kind of the kind of you know inferences they

324
00:42:58,040 --> 00:43:03,400
make based on that on that data about humans um I think that's a conversation that that is

325
00:43:03,400 --> 00:43:09,160
happening has happened happening sort of in patches but I think as an overarching theme needs

326
00:43:09,160 --> 00:43:14,360
to happen more um which is something I'm thinking about um right because not not all

327
00:43:14,360 --> 00:43:20,920
um inferences physiognomic but all physiognomy is inference right so there's a so there's

328
00:43:20,920 --> 00:43:24,280
physiognomy AI that's a class of category we we know there's a problem that's what we're talking

329
00:43:24,280 --> 00:43:28,520
about in the paper there's a higher level class of course category about about doing any kind of

330
00:43:28,520 --> 00:43:33,560
inference about data about humans that I think there's worth the beginning of a bigger conversation

331
00:43:33,560 --> 00:43:38,440
about maybe going all the way back to the onion to kind of wrap things up

332
00:43:38,440 --> 00:43:47,640
uh uh what would you point you know me or or anyone else who's not quite bought into that third layer

333
00:43:47,640 --> 00:43:57,160
of the onion as being you know fundamentally fundamentally wrong like I can buy that you know

334
00:43:57,160 --> 00:44:03,720
MLAI facial recognition is misused all the time I can buy that people you know that the technology

335
00:44:03,720 --> 00:44:11,080
is not quite there yet and is problematic in its current state um you know of applicability

336
00:44:11,080 --> 00:44:20,760
particularly in some use cases I'm not sure that I'm fully uh fully bought into you know digitizing

337
00:44:20,760 --> 00:44:27,400
faces digitizing bodies is fundamentally wrong or fundamentally racist like for for that matter

338
00:44:27,400 --> 00:44:34,120
what what what the best book or books or articles for us to take a look at yeah so you could start

339
00:44:34,840 --> 00:44:38,920
there's a famous book by Stephen J. Gould called the mismeasure of man that's a good introduction

340
00:44:38,920 --> 00:44:43,560
to thinking about chronology and physiognomy in 19th century and and kind of thinking about that

341
00:44:43,560 --> 00:44:50,680
this history of of why people's bodies get quantified in that way um the work of yeah so the work

342
00:44:50,680 --> 00:44:55,560
of some of the people I've already mentioned um of Wendy Chun who's uh at Simon Fraser University

343
00:44:55,560 --> 00:45:01,560
up in Vancouver talking about race and racism as a as a technique or as a technology um work of

344
00:45:01,560 --> 00:45:07,400
Simon Brown um at UT Austin who talks about the sort of history of classification and racial

345
00:45:07,400 --> 00:45:12,280
surveillance of black Americans going back to the slave um you know the slave people and to

346
00:45:12,280 --> 00:45:18,040
and to kind of anti anti anti escaped escape slave laws uh it's been a long time since I've come

347
00:45:18,040 --> 00:45:25,160
across the the gold book but my my immediate reaction to those is that we're kind of circling back to

348
00:45:25,160 --> 00:45:31,160
layer one of the onion which is the use like I get racist use to facial recognition as racist

349
00:45:31,160 --> 00:45:39,560
but like fundamentally any digitization digitization of a face as being racist like

350
00:45:41,480 --> 00:45:47,560
right so well so if you again right if you if if you take the definition of racism as being

351
00:45:47,560 --> 00:45:57,080
assigning you know assigning some some metric to an external feature

352
00:45:57,880 --> 00:46:06,920
and using it to um using it to make a judgment about the person right I mean that's that's in

353
00:46:06,920 --> 00:46:11,640
some ways maybe yeah maybe maybe we've done a kind of like a full circle loop right that's that

354
00:46:11,640 --> 00:46:16,360
is both a use case but it's also fundamental to what these systems do right so it's it's

355
00:46:16,360 --> 00:46:20,520
conceptual it's the concept that they can the point it's almost technological right the point

356
00:46:20,520 --> 00:46:27,240
of facial recognition is to assign a set of numbers to a phase and make a judgment about that phase

357
00:46:28,120 --> 00:46:35,480
right that's that's it and if that and that's like that is the textbook definition of racism racism

358
00:46:35,480 --> 00:46:41,800
being you know making a judgment about about you know you're you know your you know your skin

359
00:46:41,800 --> 00:46:46,840
your your your external your external self so yeah so I think it in some ways it does loop around

360
00:46:46,840 --> 00:46:52,760
but I think that's part of the problem is that is that you know people people don't always get

361
00:46:52,760 --> 00:46:58,440
that that's what that really at bottom that's what racism is right it's it's not it's not being

362
00:46:59,640 --> 00:47:04,040
you know you know being a rationally you know it's not being you know being a rationally

363
00:47:04,040 --> 00:47:09,000
bigoted or something right racism is is a technique right it has this like tech you know this kind

364
00:47:09,000 --> 00:47:14,680
of materiality to it and so you can you know yes you go west why you can be you can be racist

365
00:47:14,680 --> 00:47:19,400
without realizing it right you can be racist you know without with the best of intentions which

366
00:47:19,400 --> 00:47:24,840
kind of ties into bigger conversations for having these days in society and so I think you're

367
00:47:24,840 --> 00:47:30,440
you know your point is right but I think I think it does come around to that to that that

368
00:47:30,440 --> 00:47:39,880
fundamental question of what right what conceptually is putting numbers to humans about

369
00:47:41,400 --> 00:47:44,760
and I think I think and I think I think look I think there's a there's a question here interesting

370
00:47:44,760 --> 00:47:52,920
question here about um you know I mean okay so you could say let's say like in some medical

371
00:47:52,920 --> 00:47:57,880
cases right you want it you maybe you want it you want to you know quantify quantify the

372
00:47:57,880 --> 00:48:04,600
spleen or something or along right right is that is that racist um well there's certainly a lot

373
00:48:04,600 --> 00:48:09,560
of potential for race racism or racial disparity to come into those medical data and there's

374
00:48:09,560 --> 00:48:13,800
a lot of studies of that is that is that quite the same

375
00:48:16,120 --> 00:48:19,320
jury's still out right so I think right so I think I think that's a fair point right that

376
00:48:19,320 --> 00:48:26,440
that that you know quantifying it a lot of a particular organ on you know we that's something to

377
00:48:26,440 --> 00:48:32,360
talk about um but the face right the face is so central to the way we identify each other and

378
00:48:32,360 --> 00:48:37,320
the way we understand each other socially that I don't think you can I don't think you can quantify

379
00:48:37,320 --> 00:48:43,080
the face without without stumbling into that problem. Well there's no question that uh it's

380
00:48:43,080 --> 00:48:50,840
dangerous no question that it is uh you know lends itself or is available for misuse no question

381
00:48:50,840 --> 00:48:57,480
that it's easy to do wrong uh and that we've seen a lot of examples where you know data sets and

382
00:48:57,480 --> 00:49:05,880
now rhythms and all these things contribute to to misuse um so uh I'm glad that food food for thought

383
00:49:05,880 --> 00:49:11,160
food for thought for the audience folks like you're out there you know keeping an eye on it uh and

384
00:49:12,040 --> 00:49:16,200
I'd love to get your tape once you get a chance to actually dig into this Facebook news

385
00:49:16,200 --> 00:49:23,080
um interesting timing. Go kidding well it's never never a dull moment in this business I have to say

386
00:49:23,080 --> 00:49:28,040
boy kind of wish kind of wish there'd be a down week but hasn't happened yet.

387
00:49:28,040 --> 00:49:57,880
Awesome thanks so much Luke. Thanks so much.

