Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
While it nerfs this past December, I attended the second annual Black and AI workshop, which
gathered participants from all over the world to showcase their research, share experiences
and support one another.
This week I'm excited to continue our Black and AI series with this interview with Alvin
Grissom II, Assistant Professor of Computer Science at Ersinus College.
Alvin's research is focused on computational linguistics, and we begin with a brief chat
about some of his prior work on verb prediction using reinforcement learning.
We then dive into the paper he presented at the workshop, pathologies of neuro models
make interpretations difficult.
We talk through some of the quote unquote pathological behaviors he identified in the
paper, how we can better understand the overconfidence of trained deep learning models in certain
settings, and how we can improve model training with entropy regularization.
We also touch on the parallel between his work and the work being done on adversarial examples
by Ian Goodfellow and others.
Enjoy.
Alright everyone, I am on the line with Alvin Grissom II.
Alvin is an Assistant Professor of Computer Science at Ersinus College.
Welcome to this week in machine learning and AI.
Absolutely.
It was really great meeting you at the recent NERPS conference and in particular the Black
and AI workshop where you presented the paper that we'll be talking about today pathologies
of neuro models makes interpretations difficult.
But before we jump into that, I would love to hear a little bit about your background
and how you ended up doing research at the intersection of computational linguistics
and machine learning.
Sure.
So I have a kind of winding path, I mean, so as an undergraduate I was actually very interested
in AI and this was before it sort of blew up into what it has recently become.
And I was always interested in language as well, just in general even before I was in
college.
And actually not a lot of people know this about me, but I considered becoming an English
teacher when I was in high school.
Oh, wow.
Yeah.
So that didn't happen.
But yeah.
So when I was a master student, I was getting a master's in computer science and I was doing
language processing stuff.
And my thesis was on actually some sentiment analysis, like qualitative and quantitative
analysis of sentiment in Japanese.
So I really enjoyed doing that, although I'm not sure I'd want to go back and read my
master's thesis at this point.
And after that, I was actually a PhD student for a year in linguistics.
Decided that that wasn't really for me, at least at that time.
And eventually I worked for a year and then I went back into sort of the computational
linguistics area and that's where I am now.
So several many years later.
Awesome.
And I'm curious about the decision, was that a decision that linguistics wasn't for you,
but computational linguistics was or was it just the timing around the PhD?
Well, it wasn't the timing.
I think there were a lot of things going on, I was, I'm not sure how deeply I want to
go down this rabbit hole, but there are some contentious issues in linguistics and I didn't
realize this at the time.
I mean, I had some sense of it, but I mean, there are a lot of things going on, but I still
am very interested in linguistics, but I wanted to get kind of a broader view.
And I was, it was more satisfying or at least, maybe satisfying isn't the word, but more,
it was easier.
I felt better about the intellectual claims I was making and I could make and computational
linguistics than I did and some of the linguistic stuff I was interested in.
And I'm, I say this with some trepidation because I don't want it to be misinterpreted
as saying linguistics isn't like, isn't real or I'm not saying that, I'm saying that
for me personally, and maybe it's just because of my background, but there are some other
epistemological reasons I found the computational approach to be more to my liking as a F for
a PhD.
I'm going to study something for five plus years, I want to be comfortable with it and
I found that I personally wasn't.
Yeah, linguistics is one of those fields that I've done some kind of cursory, cursory reading
of and I just, I find it fascinating and so when you said that, I kind of got the sense
that they was something there that was kind of, kind of interesting.
So thank you for elaborating on that.
Sure.
I mean, actually, it's funny because a lot of the things that bothered me at the time
don't bother me so much anymore, but I had to kind of get some distance from it, I think,
because I was kind of in fight or flight mode for a whole year when dealing with a lot
of that and then I got away from it and was able to more leisurely sort of think about
the issues and now I'm not so perturbed anymore, but at the time I was not a happy person.
So then your research more recently is focused on the computational side as we've discussed.
Can you elaborate on the kinds of things you've been looking at?
My PhD work which, so I finished my PhD in 2017, I've continued to extend that a little
bit and so that's on simultaneous interpretation.
So trying to make computers do simultaneous interpretation.
So like at the UN or something, you know, you're someone speaking in one language and you
want to translate that in real time without necessarily waiting for the whole sentence
to be uttered and just because that problem is so easy, of course, and things are passing.
You know, we wanted to do this for what's what are called SOV or subject object-verb
languages, to subject-verb-object languages.
So a language like Japanese, for example, the verb comes at the end, but in English it
comes after the subject.
So if you want to translate, you know, I to the store went, you have to wait until you
see when at the end of the sentence before you can say I went to the store.
So that's a problem and so we-
It's a problem for humans too, right?
It is a problem for humans.
So one of the things we had to ask ourselves was how do humans do this and they do a lot
of things.
So they'll reorder sentences, they'll use more vague words sometimes, but the first approach
we sort of looked at and which I'm still working on with some other people is predicting
those verbs.
So humans are, when they do this, it's actually very cognitively taxing and they can only
do it for so long at one time.
I've noticed that at events that the transcriptionists kind of work in teams and they'll swap off
every half an hour or hour or something like that.
And I've had people tell me it's because it gets really taxing for them.
Yeah, it's really, I mean, if you think just think about what they're doing, right?
They're processing two languages simultaneously, translating between them and predicting the
future essentially.
And you know, your brain can only take so much for that long.
And so what I worked on, one of the things was trying to see how computers could do this.
And so it's like, well, let's see if we can predict these verbs, which in terms, it can
Japanese.
Let's see if we can predict the final verb and then just put it in the sentence earlier.
It turns out that's very difficult, but we've probably worked on that and we use what's
called reinforcement learning to try to teach a system.
We were actually the first people to do this.
Some of the people have done it before since then with neural networks and other things,
but let's see if we can learn under which situations we can trust these predictions.
So you have like a prediction of a verb, you have like a translation and let's see when
should we trust each and what should we do with that information.
And so this sort of spawn this other project we're just taking on the life of its own,
which is just verb prediction and predicting that kind of stuff incrementally, which
just it turns out is a very interesting problem, both for cycle linguistics and for machine
learning computer science.
And so since then, I've worked on some other stuff, but the paper you brought up was the
pathologies of neural models, make interpretations difficult.
And so that looks at...
Actually, if you don't mind me hitting Paul, I know it's going pretty fast.
No, you touched on something that is potentially interesting, this use of reinforcement learning
as part of this verb prediction problem.
So it sounds like you made a distinction between reinforcement learning and folks that came
along after and did deep reinforcement learning.
You are using more traditional RL methods.
Right, we were using imitation learning.
Yeah.
Okay.
And so can you walk us through kind of how you formulated the problem there and how you
applied RL and imitation learning to that particular problem?
Sure.
Yeah.
So you can think of sentence.
So we don't speak sentences all at once.
We speak them, you could say a word at a time, although defining word in some languages
can be difficult, but we speak them incrementally.
And you can think of that as like a time series of steps.
So if you kind of discretize the sentence by words, say, then every time, let's all imagine
you are a translator interpreter.
So you can imagine that every time you hear a word, you have a decision to make.
Okay.
And so what should that decision be?
So you might want to do nothing because you might not be certain and not you might not
know enough like to make a good to say anything or to make a translation and incremental translation.
So you might just wait or you might make the best translation you can with what you have.
You might make the best translation you can with what you have while making some kind of
prediction.
So maybe you have a good idea of what the next word is or what the verb is or something
like that.
And so in a reinforcement learning framework, you typically have some kind of time series
of steps and every time you take a step, you choose an action.
So when in this in our case, the actions were things like wait, which means don't try
to translate anything at that point, commit, which just means that you make the best translation
of that fragment you can at that point, verb, which does the same with a verb prediction.
And we had another one for the next word that does the same thing.
And so that in that formulation, every time you get a word, you just choose an action.
And if you have some examples of good actions, this is where the imitation learning comes
in, then you just try to imitate the good examples and try to generalize from those good
examples when you can take what action.
How are you training the model, were you training it on kind of existing translation, you
know, some corpus of translated phrases or documents or something like that?
Right.
We were using a standard web corpus of parallel data.
So it's not ideal because it's not actual simultaneously translated data, which there's
another paper that shows that these look very different.
So that's a weakness in it, but it's also just because that data doesn't really exist
in large measure.
There's some, but it's not enough to really train a model in the traditional way.
So yeah, we were using basic parallel data.
Okay.
It strikes me that actual simultaneously translated data would be, you know, there's a noise
aspect to it because the translators are making these mistakes that, you know, they might
not make if they have full access to full information.
Is that exact?
Yeah.
That's a problem.
I don't know if it's a problem or not, but it's definitely an aspect of the data.
So there's actually some interesting data where they have three translators.
I think it's Japanese, English, three translators of different, I guess you could say experience
levels or ranks, and so, and they have them all translating the same stuff, it's simultaneously
and you can actually see like the different kinds of things that they do and strategies
they use and mistakes they make and stuff like that.
It's pretty interesting.
Huh.
Wow.
Interesting.
You were starting to introduce the paper that I saw you present the pathologies paper.
Can you tell us a little bit about the background and motivation of that paper?
Yeah, so there's been an uptick in interest, well obviously in deep learning in general.
I mean, everybody knows that and who's going to be listening to this podcast, but one
of the criticisms of deep models is that they're black boxes.
So if you have like a logistic regression or something of some kind of linear model,
you can look at the weights and even though that's not a perfect method of analyzing a model,
you can get a sense of why the model is making the decisions it's making, right?
But with a deep model, it's very difficult, right?
And the deeper and more complex the architecture, the harder it gets to interpret what these
models are doing and why.
And so because we want to interpret these models, we thought, well, let's look at what
they're doing.
And so what we found is that they exhibit what we call these pathological behaviors.
So they don't do what any reasonable person would expect.
They would do when they're given something, but they're not used to seeing.
So I think the example I used in the presentation was you can reduce, so for question answering,
like you can reduce the question to just the word did, right?
And not only, so it still gives the right answer, but the confidence in the right answer
goes up to like 91% if you just ask did, right, which is a ridiculous, yeah.
So, so I found that the example really interesting.
Can you, let's walk through that in a little bit more detail.
It was, well, let's actually, let's go back to the data set.
The data set is question answering.
Was it squad?
It was squad.
Yeah, for that one.
Okay.
And so tell us about what you were trying to do with a given example from the squad data
set.
Okay, so you're given some context, like some kind of short sequence of sentences with
some information and you give it a question and you want the model to give you an answer
to the question based on the context.
And intuitively, you want the confidence, the final confidence of the model to reflect
the value of the information and the question, right?
So like if I ask you, so if I give you a paragraph to read and I ask you, you know, what
or something, right?
So you have no idea what I'm asking.
You wouldn't say, yeah, I'm 91% confident that, you know, the answer is x, but we found out
that these models actually do this a lot.
And so we wanted to, you know, systematically show that this is, this is like a systemic
problem and some of these models.
Okay.
And so how did you go about showing that?
So we would incrementally remove words from, so for squad, for example, we would incrementally
remove words and just observe, so without changing the answer, actually, and observe how
the confidence shifted and how, and what we found is that, you know, it doesn't really
comport with what humans, so we also did some human experiments to verify this, but it
doesn't really comport with what a human would expect.
And nor does the human performance reflect necessarily what the models do one of these circumstances.
And so the, the words that you're removing are from the, the context passage or from
the answer from the, from the question, rather, yes, right, right, right.
So you've got, you've got these three pieces, then you've got the kind of the sequence of
sentences that's the context, you've got the, the question and the answer.
And so you're removing words from the question and you're trying to see the way that that
changes the prediction of the, or the confidence level in predicting the answer, is that the
right way to think about it?
That's right.
Yes.
If I remember in this correctly, you, you would kind of sequence the words that you removed
by, were you sequencing on the one that was most likely or had like the least information
or the most information or something like that?
Right.
So we, we remove what we call the unimportant words.
So right.
So we want to remove the words without changing the answer.
You were removing the words that minimize the changing confidence of the, the answer.
And then I'm trying to remember the, like you had, you showed a sequence of the words
that you removed.
And I guess what was strange about it is that you, oftentimes you were removing words
that, as a human, you would think had the most meaning in the question and you ended
up with this, you know, just like a question word.
Right.
I'm trying to capture like the visual on the podcast, which is sometimes hard to do.
But it was really interesting to see the way that that evolved on that slide.
Yeah.
So, yeah.
So that, that was one of the sort of, sort of, have a lot of behaviors, right?
So what, it doesn't, so nothing that we, I don't want to say nothing we did, but a lot
of what we did just doesn't, you know, doesn't go with what we would expect, right?
So what the model thinks is an important term might not necessarily be what a human would
consider an important term.
Did you end up in this paper taxonomizing these pathological behaviors?
Are there, you know, do they, are there, how many of them are there?
Are there names?
Or is it more exploring kind of this one particular one that we've been discussing?
It was a more exploratory, I mean, so we did have a section, so we talked about model
over confidence, which is one problem, right?
And we, I'm trying to think, do we give these names?
I don't think, I don't think we named the pathologies.
I think we just sort of described them.
And so one of them is this overconfidence?
What are some of the other ones that you tended to see?
Right.
So I think rather than, I think a different way of thinking about it is that, so we tried
this on a number of different tasks.
So the way we, so the method that we used was sort of this word removal, right?
And we did this for squad, we did this for visual question answering, and we did this
for SNLI, which looks for things like contradictions and things like that.
And we noticed, so when we showed the same, sort of the same kinds of pathologies on all
of these different tasks, when, by, by using NPV reduction.
Okay.
So you, kind of the same idea of successfully removing words and you ended up in the same
kind of weird part where you're left with words that wouldn't be the most useful ones
that you would expect.
Right.
Right.
And so what, what did all this tell you about neural network models and trying to interpret
them?
Uh, that, it's, it's difficult.
So I mean, you know, I mean, it's not necessarily surprising.
So there's been some previous work that has shown similar things for, especially sort
of images that there, there's this notion of like rubbish examples where you show what
looks like random noise and, you know, the, the classifier, what gives you 90% confidence
that it's, you know, a flower or something.
And so this is, kind of, this is definitely related to that.
And so we also, you know, used a, proposed method of trying to mitigate this using a kind
of regularization, uh, on the, uh, by training on the reduced inputs, uh, but, um, yeah, I
mean, I think we're not just trying to say that, you know, deep learning models are garbage
or whatever.
We're just, uh, trying to identify sort of the shortcomings and understanding them so
that we can try to address that.
Right.
And did, uh, did the regularization approach that you tried, uh, I'm presuming that that
worked to some extent?
It did.
Uh, so we were, so the nice thing about it is that we were able to, uh, use this to, uh,
with almost no change in accuracy.
So we are able to get more, are more reasonable distribution of confidences, uh, that looks
more like, you know, what a human would, what, what expect, uh, without really hurting
accuracy.
Okay.
Um, so, so even, um, you know, even though the model itself is still difficult to interpret,
there is a way of, of training it, uh, that gives you, I guess you could say more interpretable
confidences that, at the end of the prediction.
Is it this specific approach to regularization?
The one that you tried that you found to have the advantage of, you know, let's say correcting
these models in this way, or is it regularization in general, you know, in trying to, to force
the networks to generalize more that, you know, had some kind of general improvement.
Right.
So I don't, uh, know that we try and drop out or something like that.
Uh, I guess I could, uh, ask the other people who are working on this, uh, but, uh, we just
showed that this particular, uh, form of regularization, uh, helps.
Um, so, um, it's possible.
I mean, I, I doubt it would hurt.
I'm, I don't know, um, but, uh, you know, to use, uh, some other kind of regularization,
uh, if you want to call it, drive-out regularization, um, yeah, but, uh, yeah, in our paper, we just
showed this particular method works.
What have you done kind of in this direction since then, or, or where do you, you know,
how would you see extending this, this kind of work?
Um, yeah.
So I personally haven't, uh, extended this, uh, since last month or whenever we post
this.
Maybe, maybe it was two months ago, um, but, uh, there is another, uh, paper, uh, uh, a workshop
paper by an undergraduate student at the University of Maryland, uh, named Eric Wallace, um, that
is, uh, attempting to do something similar.
I think he's using K nearest neighbors, um, to, so he was also the second author on this
paper, uh, to try to, uh, so I haven't read his paper, I have to confess, but I think he's
trying to do that to, uh, do something similar to try to mitigate, uh, make these models
more interpretable.
Maybe tell me a little bit about how this particular paper fits into kind of your broader
research agenda nowadays.
Sure.
Um, so going back to Verr projection a little bit.
So in 2016, I published this paper on, uh, Verr prediction in Japanese and German, mostly
Japanese, uh, and, uh, so, and that was using just the near models and, uh, so I also interested
in psycho linguistics.
So I was interested in sort of comparing the features that, uh, that, uh, that, uh, model
was using to make its Verr predictions to what humans seem to be using to make the
same predictions, right?
So we always try to, I use the word we loosely, but, uh, some people try to make, you know,
these kinds of analogies between machine learning models and humans, but, uh, without actually
looking at what the model is doing, it's not clear that that is justified.
So as we're moving to these, as we have moved to, you know, more deep learning methods,
uh, I think if you want to make claims about, you know, a model, for example, informing,
you know, something about what a human is doing, I think it's helpful to have some sense
of what the model is actually doing, uh, internally, um, so on that Verr prediction products,
for example, um, uh, we will, I looked, I just looked at the weights of the model and, uh,
you know, and, and tried to, uh, look at, uh, how, you know, how adding certain, uh, features
that were linguistically interesting affected the predictions, uh, and you can also do that
with a deep model, um, you can also look at something like attention weights, those kinds
of things. Um, so I, I guess what I'm pushing here is, you know, uh, accuracy on a data
set is an important, uh, metric to look at, but it's not the only metric. It's not all
we care about, uh, you know, for, and for doing scientific inquiry in parade, and we want
to look at the model itself, we want to understand why the model is doing what it's doing to the
best that we can, uh, which is not, again, not to say that accuracy is an important,
but, uh, as someone who's interested also in linguistic questions, um, it's, uh, much
more satisfying scientifically if we can peek inside the models and see what they're
doing. And if we can't, to at least be able to have some sense of why we can't, if that
makes sense.
What's your sense of the kind of relationship between cycle linguistics and the kind
of work that's happening on the machine learning and deep learning side? Are there, have
there been any interesting kind of cross pollinations or cross results there?
Uh, there have. And for quite some time, I think, uh, so I think there's so much potential
that's only beginning to be, uh, the surface of which is only beginning to be scratched.
Um, so for example, there's a whole, you know, area of, uh, neuroscience of language
where we have, uh, these really interesting experiments where, you know, you give a person
a stimulus of some sentences or words or something and you, you see, you put you, uh, you
give them an EEG helmet or you take an FMRI and you see how their brain is, is, uh, responding
to these stimuli. Um, so I think that's an, it's an incredible treasure trove of potential
machine learning research. Um, and it's starting to be, uh, done. Uh, it's been done for a few
years by some people. Um, so these, so for example, uh, if you wear an EEG helmet, uh, you
know, your brain produces, uh, certain, I'll just, for simplicity, I'll call them brain
waves, but it produces a graph that's called an ERP, uh, event-related potential. And, uh,
you know, I think it's really a really interesting question. So what can you predict by looking
at an ERP? Uh, can you, uh, predict, uh, other aspects of, uh, that person's, uh, linguistic
performance based on looking at these, uh, these kinds of things? But even outside of, sort
of neuroscience, which, um, requires a more specialized background, arguably, um, you
know, so, uh, for my verbal prediction work, I looked at, uh, what are called case markers
in Japanese? And it turns out that both, uh, classifier, the, uh, uh, the model that we built,
and, uh, the humans, there's previous resources. It shows that humans also do this. Uh, we're
using these case markers in Japanese tube, uh, predict verbs. And so that was in, in instance,
where it looked like the model and the humans were using the same information in an interesting
way. Um, and so I think there's a lot of potential there for, for, so for example, you might look
at a model and, and think to test whether, uh, what the humans are doing is related to what the
model is doing or vice versa. Um, so I think there's a lot of potential there. Awesome. Uh,
besides the, the paper that we've been discussing here or that we discussed previously here,
are there any interesting references that you would point someone to who's interested in
exploring this area more deeply? Uh, sure. Uh, so the one that, I mean, there, there's a lot, um,
I, I can't, I'm not going to be able to name them off the top of my head. But one that comes to
mind is, uh, the, the paper on, uh, rubbish examples. Um, so if you just Google rubbish examples,
I think it'll, it'll, uh, deep learning what rubbish examples I think it'll come up. Um, so I
think that's, uh, probably, at least that's coming to mind my, my mind right now, the most
related paper. And that was published before quite a bit before hours. And that's more related to
visual, uh, task, I think than, uh, then linguistic task, but it's a similar idea and was, uh,
certainly, uh, part of the inspiration for our work. Okay. Awesome. Awesome. Yeah. Well,
Evan, thank you so much for taking the time to chat with us. Sure. Thank you very much for having me.
All right, everyone, that's our show for today. For more information on Alvin or any of the topics
covered in this show, visit twimmelai.com slash talk slash 229. For more information on a black
and AI series, visit twimmelai.com slash black and AI 19. As always, thanks so much for listening
and catch you next time.
