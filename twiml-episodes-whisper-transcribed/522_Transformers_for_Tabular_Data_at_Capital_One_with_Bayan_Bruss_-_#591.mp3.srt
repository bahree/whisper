1
00:00:00,000 --> 00:00:10,960
All right, everyone. Welcome to another episode of the Twomble AI podcast. I'm your host,

2
00:00:10,960 --> 00:00:17,600
Sam Charrington. And today I'm joined by Bayon Bruce, a senior director of applied ML research

3
00:00:17,600 --> 00:00:22,720
at Capital One. Bayon, welcome to the podcast. Thanks, Sam. It's great to be here.

4
00:00:23,760 --> 00:00:28,240
It's great to have you on the show. I'm looking forward to digging into our conversation. We'll be

5
00:00:28,240 --> 00:00:35,440
talking about some of your work on deep learning for tabular data. Before we get to that subject,

6
00:00:35,440 --> 00:00:40,400
or to get us to that subject, I'd love to have you share a little bit about your background

7
00:00:40,400 --> 00:00:45,840
and how you came to work in the field. Sure, happy to. I'm really excited to be here. I love this

8
00:00:45,840 --> 00:00:52,320
podcast, by the way. I've learned a tremendous amount over the years. So in my current role,

9
00:00:52,320 --> 00:00:57,760
I lead applied machine learning research at Capital One. Capital One, of course, is a major

10
00:00:57,760 --> 00:01:05,040
financial institution in the United States and Canada and the UK. And we provide a broad range

11
00:01:05,040 --> 00:01:11,040
of financial services products to consumers. And within that, we use machine learning for a number

12
00:01:11,040 --> 00:01:17,280
of our applications in how we provide the services to our customers. And it's really embedded

13
00:01:17,280 --> 00:01:23,520
across the ecosystem, across the business. And when we stood up several years ago,

14
00:01:23,520 --> 00:01:29,120
a team of applied machine learning researchers, which I lead, with the goal of looking at the field

15
00:01:29,120 --> 00:01:34,320
of machine learning. And as we know, and as you've brought on many guests, the field itself is

16
00:01:34,320 --> 00:01:40,400
advancing quite rapidly. There's a major breakthrough. It feels like every six months.

17
00:01:40,400 --> 00:01:46,000
And so the idea behind applied research was, okay, what of that realm of all possible advancements?

18
00:01:46,000 --> 00:01:51,680
Is it all relevant to us as a financial services institution? How do we distill that down into

19
00:01:51,680 --> 00:01:57,920
something that is useful for the kinds of problems that we try to solve as a company? And then,

20
00:01:57,920 --> 00:02:02,080
you know, once you've kind of narrowed the focus a little bit, narrowed even further by saying

21
00:02:03,360 --> 00:02:07,120
which of those techniques, which of those breakthroughs actually work on our data,

22
00:02:07,120 --> 00:02:14,080
which of them actually work for specific use cases and problems within the within the company.

23
00:02:14,080 --> 00:02:19,760
And that's a, you know, that's a hard problem because a lot of the data that gets used in

24
00:02:19,760 --> 00:02:23,600
many of the publications is out there. You know, a lot of the benchmark data is very pristine.

25
00:02:24,400 --> 00:02:30,880
It's very static. It's used for, you know, time and time again in a variety of different experiments.

26
00:02:30,880 --> 00:02:35,120
And so it's been picked over by a number of different researchers. When you compare that to the

27
00:02:35,120 --> 00:02:40,720
kind of data that we use inside the company, it's messy, it's noisy, it's complicated.

28
00:02:40,720 --> 00:02:47,120
You may only have a few people that have really rolled up their sleeves and worked with it for a

29
00:02:47,120 --> 00:02:52,960
number of years. And so there's a big gap then between the breakthrough and then what actually

30
00:02:52,960 --> 00:02:58,720
works when you try it out on the noisy messy data. So our team's goal then is to narrow the

31
00:02:58,720 --> 00:03:03,760
scope down, figure out what actually works, test it on our use cases, and then take it a step

32
00:03:03,760 --> 00:03:09,760
further. Can we actually generalize that, build it into some of our production systems and tools

33
00:03:09,760 --> 00:03:15,520
and platforms and make it available for data scientists across the company to then use it in a

34
00:03:15,520 --> 00:03:21,200
variety of our use cases. And so the ultimate objective is to shrink the time from when something's

35
00:03:21,200 --> 00:03:26,880
discovered or some innovation or some breakthroughs made to when it can be used in servicing our

36
00:03:26,880 --> 00:03:32,640
customers in a unique way. And so that's that's kind of the mandate of my team. I've been with

37
00:03:32,640 --> 00:03:37,600
Capital One for five years now doing that. Before Capital One, I had a mixture of background in

38
00:03:37,600 --> 00:03:48,800
academia and startups and consulting. And when we look at the areas that we could be focused on,

39
00:03:48,800 --> 00:03:56,000
I mean, there's obviously quite a lot of the space of machine learning is fairly massive.

40
00:03:57,360 --> 00:04:04,080
Just fairly. It's just fairly. I mean, I can hardly read everything. If you spent all your time

41
00:04:04,080 --> 00:04:08,960
just reading all the papers that were out there, but we try to like organize ourselves thematically

42
00:04:08,960 --> 00:04:13,440
around topic areas within machine learning. And then that changes over time depending on,

43
00:04:13,440 --> 00:04:17,120
you know, whether there's something, you know, some new advancement that's really exciting and

44
00:04:17,120 --> 00:04:23,280
we're like, okay, we need to really focus on this. But they at the moment, what they are for us is

45
00:04:25,120 --> 00:04:31,120
we have a very strong interest in graph machine learning as a company. As a financial services

46
00:04:31,120 --> 00:04:38,560
company, we basically work with data that is derived from financial networks. When you swipe a

47
00:04:38,560 --> 00:04:45,440
credit card, you are establishing an edge between yourself and emergent. And so every time our

48
00:04:45,440 --> 00:04:51,680
customers process a payment, they are doing so on a network, a financial network at a, you know,

49
00:04:51,680 --> 00:04:57,920
national global scale. And so that network then becomes quite useful in a lot of our applications

50
00:04:57,920 --> 00:05:03,120
if you can have machine learning that can handle the kind of cardinality and sparsity that comes

51
00:05:03,120 --> 00:05:08,720
with that kind of network. And so we've been working for a number of years on taking a lot of the

52
00:05:08,720 --> 00:05:13,680
advancements that we've seen in graph convolutional networks and even other more traditional graph

53
00:05:13,680 --> 00:05:19,120
mining algorithms and applying them to some of our financial services applications. So that's one

54
00:05:19,120 --> 00:05:24,560
of the topic areas that's kind of been a long time for us and continues to be of high interest.

55
00:05:24,560 --> 00:05:31,200
Another one is explainability and interpretability. As a, you know,

56
00:05:31,760 --> 00:05:38,800
highly regulated financial institution, we have a very high bar for understanding the soundness of

57
00:05:38,800 --> 00:05:46,240
our models, understanding the way that our models are making decisions. And so as we use more

58
00:05:46,240 --> 00:05:52,800
machine learning across the company, we've thought it very important to invest in figuring out

59
00:05:52,800 --> 00:05:58,240
of all the advancements in model explainability and interpretability, which ones can be most useful

60
00:05:58,240 --> 00:06:04,080
in helping us manage our risks and manage the way we deploy models as a company better.

61
00:06:06,240 --> 00:06:15,440
A third one is anomaly detection. So, you know, financial services, we are kind of under attack

62
00:06:15,440 --> 00:06:22,400
from fraudsters on a daily basis. Those attacks are extremely creative. We have a lot of

63
00:06:22,400 --> 00:06:26,720
different ways of defending against those attacks. Some of those include supervised machine learning

64
00:06:26,720 --> 00:06:31,600
where we've built these massive models that look at all of our credit card transaction data and

65
00:06:31,600 --> 00:06:36,560
and can predict with fairly high accuracy whether a given transaction is fraudulent or not.

66
00:06:37,200 --> 00:06:42,080
Sometimes they're, they're rule-based systems, a combination of, of both of these and in

67
00:06:42,080 --> 00:06:48,160
heuristics. But in addition to that, you know, we realize that there's a broad range of

68
00:06:48,160 --> 00:06:53,440
anomaly detection algorithms and advancements, particularly as you start to look at how do you

69
00:06:53,440 --> 00:06:58,960
scale anomaly detection to the kinds of scale that we're working with that would allow us to

70
00:06:58,960 --> 00:07:05,840
capture emerging trends of fraudulent behavior or other kinds of nefarious behavior that might be

71
00:07:05,840 --> 00:07:10,880
escaping what a supervised model has seen in the past, right? Your supervised models can only

72
00:07:10,880 --> 00:07:15,600
generalize based on what they know. And if it's a new attack pattern, you have to have something that

73
00:07:15,600 --> 00:07:21,040
is not conditioned on that distribution in order to to capture it and respond to it effectively.

74
00:07:21,040 --> 00:07:26,880
And so that's where we see the promise in anomaly detection. And then the final area that we're

75
00:07:26,880 --> 00:07:35,760
actively focused on kind of internally focused is around privacy. We've worked for a number of years

76
00:07:35,760 --> 00:07:43,200
in how do you generate synthetic data so that you can provide people with an understanding of the

77
00:07:43,200 --> 00:07:46,960
the data that might be in production without actually giving them access to that data.

78
00:07:48,960 --> 00:07:54,320
We have more recently started to explore federated learning as a domain where we could potentially

79
00:07:54,320 --> 00:08:02,640
train models at the edge. Now, beyond those four areas, which kind of are very like pressing

80
00:08:02,640 --> 00:08:08,160
and we're focused on at the moment, there's also some areas that we're looking out into the future

81
00:08:08,160 --> 00:08:13,600
and saying, okay, this could be a potential game changer. And oftentimes when we're looking at

82
00:08:13,600 --> 00:08:21,520
those areas, we partner with major academic institutions to help us flesh out the ideas,

83
00:08:21,520 --> 00:08:29,120
help do some of the more experimental research, publish papers, engage with the community.

84
00:08:29,120 --> 00:08:34,480
It has a couple of really nice benefits. One is that if it's an area that's under-explored

85
00:08:34,480 --> 00:08:39,360
in the research community by funding it and getting a community of researchers, it kind of has

86
00:08:39,360 --> 00:08:44,000
these like cascading effects where more researchers say, oh, that's actually an interesting topic.

87
00:08:44,000 --> 00:08:48,640
Let's do some more research. And then you fund one thing and three more papers come out of

88
00:08:48,640 --> 00:08:53,360
from others on the topic because it's become something of interest to the community. And so

89
00:08:54,320 --> 00:09:00,800
within that realm, and what we were going to talk about today is this domain of deep learning

90
00:09:00,800 --> 00:09:05,680
for tabular data. Tabular data, you know, as we look to our use cases as a financial services

91
00:09:05,680 --> 00:09:11,840
company, you know, many of our data problems are formulated within this realm of tabular data.

92
00:09:11,840 --> 00:09:17,920
So, you know, and for those who don't know, tabular data is essentially structured data as

93
00:09:17,920 --> 00:09:23,840
another term for it. It's a data that comes in a table as opposed to an image which, you know,

94
00:09:23,840 --> 00:09:29,680
you have a grid or text where you have a sequence. Tabular data is, you can think of it as this

95
00:09:29,680 --> 00:09:36,080
mixed type data set where you have some numerical features, some categorical features,

96
00:09:36,960 --> 00:09:46,080
some discrete integers. And usually they're like compiled from a variety of source systems into a

97
00:09:46,080 --> 00:09:50,720
single snapshot of the population you're trying to build a model on top of. And then you,

98
00:09:51,360 --> 00:09:55,520
and then you build and train a machine learning model to make some kind of prediction.

99
00:09:55,520 --> 00:10:00,880
You mentioned earlier kind of this flood of innovation that's been happening in the field.

100
00:10:02,080 --> 00:10:06,080
A lot of the flashiest innovations in machine learning have been focused on

101
00:10:07,120 --> 00:10:12,160
images and NLP to name a couple of examples of graphs as well.

102
00:10:14,080 --> 00:10:20,720
There's been a bit of work on tabular data, but it doesn't seem like nearly as much,

103
00:10:20,720 --> 00:10:28,480
especially considering its prevalence in the broader industry, right? Banks run on tabular data,

104
00:10:28,480 --> 00:10:33,680
most businesses run on tabular data. Any takes on why that is?

105
00:10:33,680 --> 00:10:38,080
I think there's a couple of reasons. And it really boils down to the quality of the baselines.

106
00:10:38,080 --> 00:10:42,960
I think if you look back two decades ago, the quality of the baselines in language,

107
00:10:43,600 --> 00:10:47,520
and then not to disparage the language researchers from 20 years ago, like they just weren't

108
00:10:47,520 --> 00:10:58,080
that great or computer vision. The room for growth was humongous. And so then when you're

109
00:10:58,080 --> 00:11:03,760
starting at a very low performing model, and you can see every year exponential improvement,

110
00:11:03,760 --> 00:11:07,920
well, that gets everybody excited and more research and funding goes towards that.

111
00:11:08,720 --> 00:11:15,840
Whereas if we look at the evolution of machine learning for tabular data, you start with

112
00:11:15,840 --> 00:11:23,200
very simple linear and logistic models, and then you kind of advance into your support vector

113
00:11:23,200 --> 00:11:29,280
machines. And then you see these non-parametric tree-based models and ensembles of tree-based

114
00:11:29,280 --> 00:11:38,240
models like random forests and gradient booster machines. Those have, they do really well.

115
00:11:38,240 --> 00:11:42,640
And it's hard to beat them. And there's another piece to it, which is that not only do they do well,

116
00:11:42,640 --> 00:11:48,240
they do well on a wide variety of problems. And the tooling ecosystem that's been built around them,

117
00:11:48,240 --> 00:11:55,040
tools like XG Boost, and the whole ecosystem of Python for data science has really exploded

118
00:11:55,040 --> 00:12:01,600
in the last decade. Make it really, really easy to use those techniques. And so there's not a whole

119
00:12:01,600 --> 00:12:11,280
lot of incentive to ask, well, what's outside of that paradigm? And I think there's a third factor

120
00:12:11,280 --> 00:12:16,880
in it. And that's primarily that like a lot of the big public benchmark data sets are in computer

121
00:12:16,880 --> 00:12:25,040
vision and in NLP. There hasn't historically existed kind of these big challenge type data sets

122
00:12:25,040 --> 00:12:32,400
for tabular data, where you can see that benchmark improvement year over year at, you know,

123
00:12:32,400 --> 00:12:37,200
some of the big conferences like CVPR. It's the other piece that's missing, I think, from the field.

124
00:12:37,200 --> 00:12:42,240
I think the biggest one is the fact that they just, they're really good models. But what you've seen

125
00:12:42,240 --> 00:12:48,000
as a result of that, as you said, like all of this research has focused on computer vision NLP

126
00:12:48,000 --> 00:12:55,360
and more recently graphs, leaving a huge application space of tabular models outside of the main line

127
00:12:55,360 --> 00:13:00,240
of machine learning research. Still a ton of research that happens in the statistical literature,

128
00:13:00,240 --> 00:13:07,280
but the kind of the stuff you see at ICLR and NUREPs and ICML hasn't primarily focused on tabular

129
00:13:07,280 --> 00:13:11,920
data for the last few years with some research here and there. But a couple of things that are

130
00:13:11,920 --> 00:13:18,240
interesting. One is that we haven't fully explored how the advances that we see in computer vision

131
00:13:18,240 --> 00:13:24,160
and NLP apply to tabular data. I think historically we would have said, oh, there's no, there's no

132
00:13:24,160 --> 00:13:28,480
relationship, right? It's a completely different domain. There's no way to reuse those components.

133
00:13:28,480 --> 00:13:34,080
That was actually originally said between computer vision and NLP, right? Like, there was this

134
00:13:34,080 --> 00:13:39,840
page. You couldn't use language models for vision and vice versa. And then more recently,

135
00:13:39,840 --> 00:13:45,200
we've seen, yeah, exactly. Transformers can be used for everything. You can also, you know,

136
00:13:45,200 --> 00:13:53,200
certain instances of language can be modeled quite well with, you know, image, image models.

137
00:13:53,200 --> 00:13:59,280
And so I think the same thing could be said for tabular data. It's just a matter of, you know,

138
00:13:59,280 --> 00:14:04,400
asking the questions and doing the research and doing the exploration. The other piece that

139
00:14:04,400 --> 00:14:11,280
is really, really critical is that, you know, all of the research that surrounds

140
00:14:13,680 --> 00:14:19,360
computer vision and NLP models, primarily around answering questions of how do we make these

141
00:14:19,360 --> 00:14:25,120
models robust? How do we make these models interpretable and explainable? All of those are often

142
00:14:25,120 --> 00:14:31,600
predicated on the model itself being a deep learning model, right? Many of these techniques require

143
00:14:31,600 --> 00:14:38,000
a differentiable model. And so in a lot of cases, we can't take those techniques and then apply them

144
00:14:38,000 --> 00:14:45,680
to your XGBoost model. You've created this bifurcation in what is possible if you use a neural network

145
00:14:45,680 --> 00:14:53,280
and what is possible if you don't. And by maybe closing that gap and asking, can we do the

146
00:14:53,280 --> 00:14:58,480
tabular data in the same paradigm as we're using for computer vision and NLP? Can we then take all

147
00:14:58,480 --> 00:15:04,240
of that, you know, support that's been built around those deep learning models in computer vision

148
00:15:04,240 --> 00:15:10,640
and NLP and use them for our tabular data? And so that's one of the exciting things about bridging

149
00:15:10,640 --> 00:15:19,440
the gap between the two fields. I think, like, candidly, because the baselines are so strong in

150
00:15:21,120 --> 00:15:28,800
for methods like graded boosted trees and random forests, it's that ecosystem of functionality

151
00:15:29,520 --> 00:15:35,280
that makes it compelling more than, you know, incremental improvements, marginal gains in the

152
00:15:35,280 --> 00:15:41,120
in the overall performance. I mean, everybody gets excited when you have a table where you're

153
00:15:41,120 --> 00:15:45,840
you're performing the best on all the data sets you're testing on. But you know, from a practitioner's

154
00:15:45,840 --> 00:15:52,800
perspective, it's much more exciting to be able to use, you know, the broad suite of capabilities

155
00:15:52,800 --> 00:15:57,520
that come with deep learning. I'm curious, are there specific things that come to mind there?

156
00:15:57,520 --> 00:16:08,800
I generally get the idea of, hey, we've got, you know, this broad set of tooling that, you know,

157
00:16:08,800 --> 00:16:17,360
has been built up around deep learning. And some of that, some of that I can see, for example,

158
00:16:17,920 --> 00:16:23,600
you know, hey, you use TensorFlow or PyTorch, you want to use the same tool chain for everything,

159
00:16:23,600 --> 00:16:28,800
just for efficiencies and, you know, learning curves, all that kind of stuff. But some of the other

160
00:16:28,800 --> 00:16:33,440
things that I'm not sure if you specifically mentioned, like some of the explainability methods

161
00:16:33,440 --> 00:16:41,760
that are based around deep learning do, kind of strike a little bit of, you know, hey, we've got

162
00:16:41,760 --> 00:16:46,240
these tools to solve these problems that were created by the methods that we're using to solve

163
00:16:46,240 --> 00:16:52,160
problems that, you know, we can solve otherwise with better performance that don't have the same

164
00:16:52,160 --> 00:16:59,440
opacity. Yeah, that's a good point. Many of these tools were developed to, as you say,

165
00:17:00,640 --> 00:17:08,400
solve some of the problems that come with deep learning. However, many of the models that you will

166
00:17:08,400 --> 00:17:18,240
find used in large industrial systems will be equally opaque. You know, there's, it's just as hard

167
00:17:18,240 --> 00:17:25,920
to interpret or understand a tree-based model that has, you know, 10 splits per tree and that

168
00:17:25,920 --> 00:17:34,160
has several thousand trees and that has several thousand features that's being used. It's

169
00:17:34,160 --> 00:17:40,320
equally hard to explain a single decision of that type of model as it is to explain a neural network.

170
00:17:40,320 --> 00:17:47,840
It's no less complex a model. It's not capturing any fewer interactions. It's just modeling the

171
00:17:47,840 --> 00:17:52,640
data in a different way. And so I think we're seeing a growth and complexity of machine learning

172
00:17:52,640 --> 00:17:59,680
models, regardless of what you're using, whether it's a deep learning model or a tree-based model,

173
00:17:59,680 --> 00:18:06,080
but I think what we're seeing is that for deep learning, there's just been this huge investment

174
00:18:06,080 --> 00:18:12,240
in trying to understand how they work because I think maybe partially because they've been so

175
00:18:12,240 --> 00:18:15,760
effective and people want to know why. They want to know what they're learning. You know, as we

176
00:18:15,760 --> 00:18:23,760
start to make claims about intelligence, people want to pinpoint factors in the decisioning of

177
00:18:23,760 --> 00:18:29,600
these systems. And so that spurred all of this research. It's available. It just doesn't work for

178
00:18:29,600 --> 00:18:37,360
this other tools, for these other tools that we use. But I think that it's really, really,

179
00:18:39,840 --> 00:18:43,600
I mean, it's important not to just say like, oh, because it's good for deep learning, it'll work

180
00:18:43,600 --> 00:18:50,480
for these other systems, but there are certain problems that we've been able to see the methods

181
00:18:50,480 --> 00:18:58,000
that were developed for deep learning to be very, very powerful for tabular data. If, particularly

182
00:18:58,000 --> 00:19:03,120
in the realm of explainability, and I'll give you an example, there's a subdomain of model

183
00:19:03,120 --> 00:19:07,600
explanations, local explanations called counterfactual explanations. I don't know if you're familiar

184
00:19:07,600 --> 00:19:14,160
with counterfactual explanations. Sure, counterfactual explanation essentially is asking the question for

185
00:19:14,160 --> 00:19:19,600
a given input to a model. What would have had to have been different about this input in order for

186
00:19:19,600 --> 00:19:25,360
the model to have made a different prediction? And so you think about a binary classification task,

187
00:19:26,800 --> 00:19:32,640
like whether or not a credit card transaction is fraudulent. Well, we can say, okay,

188
00:19:32,640 --> 00:19:36,560
given that this transaction was classified fraudulent by this model, what would have had been

189
00:19:36,560 --> 00:19:41,040
different about that transaction for this model to not have thought that would have fraudulent.

190
00:19:41,040 --> 00:19:46,880
So what feature changes would you need? And those feature changes, the difference between

191
00:19:47,520 --> 00:19:52,160
the actual features and what would have had to have been different becomes the explanation

192
00:19:52,160 --> 00:19:58,000
of why that model made a decision. And there's a lot of different ways you can do this, and there's been,

193
00:19:58,000 --> 00:20:05,360
you know, many, many papers on the different techniques to do this. A very, very simple way to do

194
00:20:05,360 --> 00:20:11,920
this is to take the inputs to your model and project them into a lower dimensional space.

195
00:20:12,960 --> 00:20:18,080
And then search within that lower dimensional space, which is now a continuous space, right?

196
00:20:18,080 --> 00:20:26,800
This is the standard way that a neural network works. The shortest path to another data point

197
00:20:27,440 --> 00:20:33,760
where the prediction is different. And then come back out from that lower dimensional space

198
00:20:33,760 --> 00:20:45,040
using like a decoder to original feature space. And now you have input to a model in the original

199
00:20:45,040 --> 00:20:50,480
feature space that if you had used the original model to predict it would have resulted in a

200
00:20:50,480 --> 00:20:54,720
different classification. Now, in order to do that, you need a, you need a differentiable model.

201
00:20:54,720 --> 00:20:58,480
You need to have the ability to train a model such that it can project into that lower dimensional

202
00:20:58,480 --> 00:21:07,280
space and use the, use the gradients with respect to the model prediction in order to do that.

203
00:21:08,000 --> 00:21:12,080
There are other ways to solve that problem. There are ways that don't require you to

204
00:21:13,200 --> 00:21:19,280
use the differentiable model in order to come up with the counterfactual. But that's a very simple

205
00:21:19,280 --> 00:21:23,520
and very efficient way to do it. And if you had a deep learning model, you'd be able to do that.

206
00:21:23,520 --> 00:21:28,000
So that's an example where there are solutions out there. You don't have to use deep learning,

207
00:21:28,000 --> 00:21:38,800
but a lot of the paradigm makes it a lot simpler once you adopt it.

208
00:21:58,960 --> 00:22:10,160
I do. I do. I think that's one of the really exciting things is this

209
00:22:12,080 --> 00:22:18,400
deep learning is compositional, right? You can take pieces of deep learning systems and build

210
00:22:18,400 --> 00:22:24,400
them together and then train them in an end to end fashion. Multimodality allows us to do that

211
00:22:24,400 --> 00:22:30,800
in ways that we would never be able to do or we would have to do in very, very complex ways

212
00:22:30,800 --> 00:22:37,840
historically. A good example of that would be within the domain of graph machine learning.

213
00:22:37,840 --> 00:22:42,720
So we have these complex financial networks. We want to build models that help us predict

214
00:22:42,720 --> 00:22:52,560
individual entities, status within that network. Those individual nodes will often have a lot

215
00:22:52,560 --> 00:22:57,520
of tabular data associated with them in addition to the edges that connect them on the graph.

216
00:22:57,520 --> 00:23:03,520
And so knowing what the right way to encode that tabular data is and how to build that into

217
00:23:04,560 --> 00:23:09,200
broader, differentiable model, a deep learning model on the entirety of the graph,

218
00:23:09,200 --> 00:23:15,360
it becomes really, really powerful. And I think the things that are most exciting that we're all

219
00:23:15,360 --> 00:23:22,800
getting very excited about things like Dolly and stable diffusion are situations where people

220
00:23:22,800 --> 00:23:30,720
have figured out how do we fuse together different domains of data in ways that allow us to

221
00:23:30,720 --> 00:23:35,680
interact with that data in entirely new ways. And quite frankly, I'm not entirely certain.

222
00:23:35,680 --> 00:23:40,320
All the different ways we're going to be able to use multi-modality within financial services,

223
00:23:40,320 --> 00:23:45,600
but I think once we've proven how you can do it, we're going to see a lot of really exciting

224
00:24:10,320 --> 00:24:15,600
ways to do it.

225
00:24:24,640 --> 00:24:28,000
Yeah.

226
00:24:28,000 --> 00:24:34,480
I think fundamentally the data is different. The data tends to come

227
00:24:34,480 --> 00:24:43,360
from the process that generates the data is usually not actually a single process.

228
00:24:43,360 --> 00:24:48,560
Oftentimes, in tabular domains, it's multiple processes. You might be looking at

229
00:24:49,920 --> 00:25:00,720
some combination of customers' payment history along with information about where they spend

230
00:25:00,720 --> 00:25:05,120
their money. And so those are two completely different systems. They're not actually fundamentally

231
00:25:05,120 --> 00:25:11,200
related in any other way other than their with regards to a specific customer. You take those

232
00:25:11,200 --> 00:25:16,080
data sets, you engineer the features, you combine them together, and now you have a tabular data set.

233
00:25:16,080 --> 00:25:23,840
Unlike an image where you have this continuous distribution, at least within a specific domain,

234
00:25:23,840 --> 00:25:31,600
all the images come from the same distribution. They are fairly well structured in the fact that

235
00:25:31,600 --> 00:25:39,280
there's strong local correlations within an image. Nearby pixels are very highly likely to be

236
00:25:39,280 --> 00:25:44,880
similar to the one next to them. Columns in a data set have no inherent structure to them.

237
00:25:45,680 --> 00:25:52,400
There's nothing other than the peculiarities of the data scientists who put that column

238
00:25:52,400 --> 00:25:55,440
next to the other column that determine their proximity to one another.

239
00:25:57,600 --> 00:26:01,440
A lot of that inherent structure is missing, which is one of the reasons why

240
00:26:02,560 --> 00:26:07,920
transformers are starting to be the thing that's bridging that gap, because transformers can

241
00:26:08,640 --> 00:26:13,360
look across the entirety of the data set and determine what context is important. They don't have

242
00:26:13,360 --> 00:26:18,240
to rely on individual proximities that are hard-coded into the architecture to figure that out.

243
00:26:18,240 --> 00:26:27,680
That's one is the mixed type and the lack of inherent structure. I think there's been some

244
00:26:27,680 --> 00:26:34,160
really interesting work. It's primarily been focused on not necessarily

245
00:26:35,440 --> 00:26:41,600
architecturally how do we handle tabular data, but it's focused on even a level before that,

246
00:26:41,600 --> 00:26:48,720
which is how do we encode the data in a way that a deep learning model can utilize that

247
00:26:48,720 --> 00:26:56,320
information more effectively? A number of researchers have started to point out that varying

248
00:26:56,320 --> 00:27:04,480
encoding schemes have a tremendous impact on the quality of a deep learning model for tabular

249
00:27:04,480 --> 00:27:13,920
data. These encoding schemes can be anything from simple linear projections. Yeah, exactly,

250
00:27:13,920 --> 00:27:23,360
exactly, piecewise, linear projections, binning, etc. All of those have a very strong effect.

251
00:27:28,720 --> 00:27:33,200
I think there's going to be more research just into how do we encode data better.

252
00:27:33,200 --> 00:27:37,520
I would also be very curious, and I don't know if anybody's done this yet. If those encoding

253
00:27:37,520 --> 00:27:44,800
strategies also benefit models like XGBoost and random forest models, it might be that rather than

254
00:27:45,440 --> 00:27:50,640
doing simple one-hot encoding, like some of these more complex encoding, both for numerical

255
00:27:50,640 --> 00:27:57,520
and categorical features, benefits, all models. That would be great. Another thing that's been

256
00:27:57,520 --> 00:28:03,520
really profound and some of the researchers that have pointed this out, I think doing great work,

257
00:28:04,160 --> 00:28:10,480
is that the way that you regularize the model has a profound impact, and that's not surprising.

258
00:28:10,480 --> 00:28:17,040
Regularization kind of rules everything in machine learning, and that goes back to computer vision

259
00:28:17,040 --> 00:28:23,840
and LP as well. Interestingly enough, if you go back and look at the original research online,

260
00:28:23,840 --> 00:28:29,200
XGBoost, one of the key things they introduced in that model was novel ways of regularizing

261
00:28:30,400 --> 00:28:35,520
the decision trees. So regularization is kind of one of those foundation steps that if you're

262
00:28:35,520 --> 00:28:40,400
ever going to walk into a new domain of machine learning, you have to figure out what regularization

263
00:28:40,400 --> 00:28:50,160
works for this particular domain. It was a paper a while ago that just found that

264
00:28:50,160 --> 00:28:58,560
if you did hyperparameter search over a group of possible regularization techniques

265
00:28:59,040 --> 00:29:04,640
for any given model, and you selected the optimal subset of regularization techniques,

266
00:29:05,440 --> 00:29:11,120
very simple models like MLPs could perform outstanding on tabular data. It was just the right,

267
00:29:11,120 --> 00:29:17,760
they called it a cocktail of regularization techniques. I think that's a really interesting

268
00:29:17,760 --> 00:29:25,040
approach. Interestingly, they also included in what they're calling regularizing a variety of

269
00:29:25,040 --> 00:29:34,080
data augmentation methodologies. This is an area where NLP and more recently computer vision

270
00:29:34,080 --> 00:29:41,360
have seen really interesting research, which is how do we augment the data and use it for self-supervised

271
00:29:41,360 --> 00:29:47,120
pre-training in a way that makes our downstream models more robust. And that is, this data augmentation

272
00:29:47,920 --> 00:29:54,480
is an area that is almost completely lacking in the tabular data domain. We don't know what works

273
00:29:56,320 --> 00:30:05,680
exactly synthetic data. Yeah, but we don't know what the right techniques are. I think

274
00:30:05,680 --> 00:30:12,400
something like cut mix might work well on an image, but do you just apply cut mix to a tabular

275
00:30:12,400 --> 00:30:18,400
data set? Now you have to define a scheme of data augmentation that actually makes sense for

276
00:30:18,400 --> 00:30:25,920
this mixed type domain in tabular data. And so I think that's an area that's really interesting.

277
00:30:25,920 --> 00:30:32,240
There's a final area, so we have encoding, we have regularization, and then there's finally

278
00:30:32,240 --> 00:30:38,080
architecture. What architecture really impacts? And we've been looking at this for a while now,

279
00:30:38,080 --> 00:30:42,240
and we've been partnering with Tom Goldstein at University of Maryland, his grad student,

280
00:30:42,240 --> 00:30:49,120
Gautamie Somapeli, wrote a paper a few years ago called Saint, that looked to take a lot of what

281
00:30:49,120 --> 00:30:56,480
we've learned in transformer architectures and apply them to tabular data. And this is one of the

282
00:30:56,480 --> 00:31:05,120
first papers in this area, and kicked off a lot of the subsequent research. And kind of the novelty

283
00:31:05,120 --> 00:31:11,840
of that research was twofold. One was one of the first papers to look at transformer architecture

284
00:31:12,560 --> 00:31:20,320
across a row within a given data set. So the goal of a model like that is to ask for all of the

285
00:31:20,320 --> 00:31:27,760
features that I'm using to predict this particular outcome, attend to the ones that matter the most

286
00:31:27,760 --> 00:31:33,600
for this particular goal. And that seems fairly straightforward. Transformer architectures are

287
00:31:33,600 --> 00:31:41,360
designed to do that. Interestingly, Saint also uses this notion of intersample attention. And so it

288
00:31:41,360 --> 00:31:48,160
takes subsamples of the training data, and it asks not just to attend to the individual row that

289
00:31:48,160 --> 00:31:54,400
is for a specific data point, but of all the other data points in that sample, which one is most

290
00:31:54,400 --> 00:32:01,600
useful to this prediction task. And so it almost brings together a transformer architecture with

291
00:32:01,600 --> 00:32:06,400
like a canierous neighbor classifier. So you're not just attending to data points that I'm

292
00:32:06,400 --> 00:32:11,920
interested in. You're also attending to similar data points, or maybe even dissimilar data points

293
00:32:11,920 --> 00:32:18,880
depending on what's most useful to the task at hand. And Saint was a very foundational paper in

294
00:32:18,880 --> 00:32:24,320
this space. Since then, there's been a number of different transformer papers that have looked at

295
00:32:24,320 --> 00:32:30,400
how do we apply these architectures. As I mentioned, some people have pointed out that, you know,

296
00:32:30,400 --> 00:32:35,600
given good encodings and good regularization, maybe you don't need a transformer. I think the

297
00:32:35,600 --> 00:32:40,960
question of architecture is still an open question as much as the question of regularization and

298
00:32:40,960 --> 00:32:47,200
encoding is an open question. But I ultimately think that the combination of these three

299
00:32:48,400 --> 00:32:54,880
in whatever the final state will be will be a powerful new new tool system for deep learning

300
00:32:54,880 --> 00:33:04,400
on tabular data. And so how where are we like this? How far does Saint get us or how close does

301
00:33:04,400 --> 00:33:11,600
Saint get us to solving the problem? Is it just kind of demonstrating a particular, you know, a direction?

302
00:33:11,600 --> 00:33:16,640
That's a good question. It far depends on the journey that you're on.

303
00:33:18,880 --> 00:33:25,440
I think if, right, like it all depends on the end destination. If you look at some of the research

304
00:33:25,440 --> 00:33:33,040
recent survey papers on the field, what they found is that on small data sets anywhere from zero to

305
00:33:33,040 --> 00:33:42,560
50,000 training samples, it's hard to beat, you know, a well-trained extibus model to NIPER parameters.

306
00:33:42,560 --> 00:33:48,960
Like that still is the dominant paradigm. You start to see some of these deep learning

307
00:33:48,960 --> 00:33:56,880
methods exceed that when you get above 50,000. And so I think for a while, we're going to see a gap

308
00:33:56,880 --> 00:34:03,760
between small data sets and large data sets in much in the way that for many years we saw within

309
00:34:03,760 --> 00:34:12,080
the field of NLP. If you're working with a small data set, for instance, for sentiment analysis,

310
00:34:12,960 --> 00:34:18,720
it was much more effective to do a TF IDF encoding and a logistic regression model

311
00:34:18,720 --> 00:34:22,480
than it was to use a big language model if you only had 10,000 training samples.

312
00:34:22,480 --> 00:34:29,920
I think we're in that paradigm where NLP eventually went was if you had a large language model

313
00:34:29,920 --> 00:34:36,320
that was trained in an unsupervised way, you could use it on small data sets and fine-tune it

314
00:34:36,320 --> 00:34:40,960
quite effectively. So maybe we'll get there with tabular data. Maybe there will be this notion of

315
00:34:41,680 --> 00:34:48,080
you know, large pre-trained tabular data models that can then be used on small data sets and

316
00:34:48,080 --> 00:34:55,680
effectively transfer there encoding. What does that actually mean? What is the underlying

317
00:34:56,800 --> 00:35:01,680
the underlying relationship between all tabular data that such a thing would exploit?

318
00:35:02,240 --> 00:35:09,120
Yeah, that's a great question. We're starting to make progress on that. I don't have an answer

319
00:35:09,120 --> 00:35:15,600
to your specific question, but we're starting to make progress very simply. We recently submitted a

320
00:35:15,600 --> 00:35:23,440
paper with Roman Levin and also Tom Goldstein, Michael Goldblum, Andrew Gordon Wilson at NYU

321
00:35:24,160 --> 00:35:28,640
and a number of other grad students where we're looking at transfer learning within the realm of

322
00:35:28,640 --> 00:35:41,840
tabular data and the idea there is if you have a high level overlap between the feature space

323
00:35:41,840 --> 00:35:50,320
of any given task. So in that case, we're looking at medical predictions. So we have a variety

324
00:35:50,320 --> 00:35:54,720
of tabular data sets. A lot of them contain a lot of the similar features and some of them contain

325
00:35:54,720 --> 00:36:04,480
distinct features. They're all trying to predict different diseases. And so the idea was can you

326
00:36:04,480 --> 00:36:13,200
pre-train a tabular data set on maybe one of the larger data sets and the predictions for that

327
00:36:13,840 --> 00:36:19,280
for that given disease and then transfer that. Maybe the feature set is slightly different

328
00:36:19,280 --> 00:36:26,160
and we had to come up with a novel way of how do you adjust the feature set within this

329
00:36:26,160 --> 00:36:31,600
pre-training scheme. But then can you use the learning from that much larger data set into that

330
00:36:31,600 --> 00:36:38,400
smaller one. Now again, that that is very different than a foundation model which basically

331
00:36:38,400 --> 00:36:43,760
gobbles up all of the language on the internet pre-trains and then can be used for anything

332
00:36:44,560 --> 00:36:49,600
that is transfer learning in a much smaller scope. But I think it's a starting point to say that

333
00:36:49,600 --> 00:36:59,760
yes, you can use a generic feature encoder from a tabular data set and extend it to other tasks

334
00:36:59,760 --> 00:37:06,160
in a way that retains the original structures that you've learned. I do think that we would have

335
00:37:06,160 --> 00:37:11,920
to answer the fundamental question which you asked, which I think is still not clear in my head,

336
00:37:11,920 --> 00:37:19,520
which is what does it mean if you were to go and build a model that included every tabular data

337
00:37:19,520 --> 00:37:23,920
set in the world? Like what would it mean for a combination of a healthcare data set with a

338
00:37:23,920 --> 00:37:29,120
financial services data set with like a wine classification data set. And when we've seen all

339
00:37:29,120 --> 00:37:32,800
of the data sets that are out there, they're fundamentally different. So what is it learning?

340
00:37:33,600 --> 00:37:37,200
On the other hand, maybe kind of rolling back a little bit of my

341
00:37:38,720 --> 00:37:48,000
disbelief. I think we're interested in tabular data because tabular data is kind of this fundamental

342
00:37:48,000 --> 00:37:57,600
currency of business like it's all over the place. But I suspect that there's a lot of the same

343
00:37:57,600 --> 00:38:02,640
thing happening in lots of places. Like there are a lot of churn models, like there are a lot of

344
00:38:02,640 --> 00:38:09,680
fraud models, like there's, you know, you can, I don't know how many kind of super classes, you know,

345
00:38:09,680 --> 00:38:14,880
if we were to try to taxonomize, you know, all of the tabular data sets, what that looks like. But,

346
00:38:15,680 --> 00:38:23,440
you know, if you could get access to a whole bunch of, you know, churn,

347
00:38:23,440 --> 00:38:32,800
you have data sets that related to churn models, I could envision like some kind of foundational

348
00:38:32,800 --> 00:38:40,960
churn model that, you know, understands propensities to do something based on other things.

349
00:38:40,960 --> 00:38:47,040
I don't know. I think that's right. I think that's right. Even if the, even if the inputs are

350
00:38:47,040 --> 00:38:54,160
fairly heterogeneous, I think that is, and luckily within industry, you know, there's not, they're

351
00:38:54,160 --> 00:39:00,560
not, there are not very many companies that like span multiple distinct domains where, you know,

352
00:39:00,560 --> 00:39:06,000
they're trying to predict credit card fraud and disease outcomes at the same time. Like you don't

353
00:39:06,000 --> 00:39:12,160
see that kind of conglomeration. So yes, within an individual company is this notion of transfer

354
00:39:12,160 --> 00:39:17,680
learning or even kind of industry-specific foundation models actually do potentially make sense,

355
00:39:18,400 --> 00:39:26,720
globally across all data sets. That's maybe a little bit extreme. But yeah, I think within

356
00:39:26,720 --> 00:39:32,640
specific domain, specific applications, we could, we could certainly think about how you bring

357
00:39:32,640 --> 00:39:37,440
together all those different types of data sets and tasks into a single modeling framework.

358
00:39:37,440 --> 00:39:44,960
Of course, the task of pulling those data sets together is a very different one from collecting

359
00:39:44,960 --> 00:39:50,480
images or text off of the internet. Yeah, it's not as easy as just like many just the political,

360
00:39:51,200 --> 00:39:57,440
you know, the, you know, the access to the data itself. That's right. And many tabular data sets

361
00:39:59,040 --> 00:40:05,200
because they are collected within company walls are actually quite sensitive. They contain private

362
00:40:05,200 --> 00:40:11,200
customer information that rightly so those companies don't want to share publicly and make

363
00:40:11,200 --> 00:40:19,280
available. Very little exists the way that text and images just exist free, free to grab on the

364
00:40:19,280 --> 00:40:25,120
internet within the domain of tabular data sets. But within specific industries, there might be

365
00:40:25,120 --> 00:40:34,400
some consortiums that emerge to, you know, start to bring together groupings of those types of

366
00:40:34,400 --> 00:40:40,720
data sets if this, if this paradigm seems to be the one that people find promising. Certainly,

367
00:40:40,720 --> 00:40:48,800
I think, you know, when we look internally at some of these, as you said, you know, you're,

368
00:40:49,680 --> 00:40:54,240
whether it's churn or it's marketing or fraud detection a lot of times, you're using,

369
00:40:54,240 --> 00:40:59,680
you know, similar overlapping data sets and variations on a theme when you're talking about your

370
00:40:59,680 --> 00:41:06,160
task. And, you know, traditional industry would build a separate model for each one of those

371
00:41:06,160 --> 00:41:11,760
use cases. And when you're first getting started as a company in machine learning, having a

372
00:41:11,760 --> 00:41:17,040
separate model for every single thing is probably not that big of a deal. You've got 10, 15, 20

373
00:41:17,040 --> 00:41:23,040
models that you're maintaining. When you become a full-fledged adopter of machine learning and

374
00:41:23,040 --> 00:41:27,760
you have hundreds of models that you're running in production and interacting with each other in

375
00:41:27,760 --> 00:41:33,520
ways that you didn't anticipate and they're relying on each other in stacked ways, managing

376
00:41:33,520 --> 00:41:39,280
that overall system complexity becomes really, really critical and a very big challenge. And so

377
00:41:40,320 --> 00:41:48,400
rethinking it as a, you know, a single pre-trained model with a lot of smaller fine tuning, you know,

378
00:41:48,400 --> 00:41:52,960
that actually changes how you do business, that changes the tech stack that you work with,

379
00:41:52,960 --> 00:41:58,000
that changes how you think about the overall machine learning ecosystem. And so I do think like

380
00:41:58,000 --> 00:42:02,160
long-term it could potentially help a lot of companies reduce their overall machine learning

381
00:42:02,160 --> 00:42:08,000
complexity if they think about it that way. What do we know about solving tabular data problems?

382
00:42:08,000 --> 00:42:18,800
How do you, what's most important there? And how do you think about approaching them today for,

383
00:42:18,800 --> 00:42:23,120
you know, for the real problems that you're trying to solve today? Yeah, I think that the, the

384
00:42:24,160 --> 00:42:29,920
biggest gap right now if it's standing there between some of this research that we've been

385
00:42:29,920 --> 00:42:35,200
talking about and deep learning for tabular data and what we've actually used internally is not

386
00:42:35,200 --> 00:42:41,680
necessarily kind of at this point a novel architecture, a novel encoding scheme or novel

387
00:42:41,680 --> 00:42:46,560
regularization technique. I think each of those has more research and there's going to be some more

388
00:42:46,560 --> 00:42:50,720
work that figures out like which is the best or which are the sets of best. I think the biggest

389
00:42:50,720 --> 00:42:59,040
gap is tooling is do we have, you know, tools that are as easy to use as scikit-learn or XG boost

390
00:42:59,680 --> 00:43:06,400
that you can fit and deploy a tabular data model using, you know, using deep learning as you can

391
00:43:06,400 --> 00:43:14,000
for a tree-based methodologies. And when I say easy to use it's, you know, everything from

392
00:43:14,000 --> 00:43:20,240
not having to configure a thousand hyper parameters just to figure out what which one's going to work

393
00:43:20,240 --> 00:43:30,240
best to is it hardened and, you know, well documented does it have good logging like the whole

394
00:43:30,240 --> 00:43:34,560
software engineering side of it is actually I think what's missing at the moment. There's been a

395
00:43:34,560 --> 00:43:38,720
lot of great research but now there just needs to be some really good quality engineering that

396
00:43:38,720 --> 00:43:44,800
takes that and figures out, okay, can we build libraries or, you know, packages that make this

397
00:43:46,320 --> 00:43:50,800
so that it's not a data scientist figuring out how to apply research. It's a data scientist

398
00:43:50,800 --> 00:43:56,960
using a tool, right? Like that's that's the gap. That sounds like a fairly well-stated problem and

399
00:43:56,960 --> 00:44:03,200
I know Capital One loves open source. Like, is that, are you working on that? Yeah, that is that's

400
00:44:03,200 --> 00:44:08,720
most data problem because it's something that we're thinking about and working on, yes, but I do

401
00:44:08,720 --> 00:44:15,680
think that's the biggest gap at the moment. Maybe this is a tangent but you mentioned graph

402
00:44:16,720 --> 00:44:25,760
learning, deep learning on graphs that whole direction. Do you think that that plays

403
00:44:25,760 --> 00:44:34,480
into working with tabular data in a big way? Like, is each of the rows in a, you know, tabular

404
00:44:34,480 --> 00:44:38,640
data set kind of a node in a graph and, you know, that's going to help us figure all this out?

405
00:44:39,600 --> 00:44:48,080
Yeah, that's a really interesting question and I think one that's not well studied enough. I

406
00:44:48,080 --> 00:44:55,440
mentioned in saint how the saint architecture takes a transformer and applies it to each of the rows

407
00:44:56,080 --> 00:45:05,200
and then it takes that same transformer and looks across, across rows in a sub sample of the data.

408
00:45:06,880 --> 00:45:11,280
If you think about what that's doing is it's essentially treating the data set

409
00:45:11,280 --> 00:45:18,080
like a similarity graph. It's saying, okay, I've got all these data points. Each one of them is a

410
00:45:18,080 --> 00:45:23,760
node in this data set or at least within the sample of it. Attend to the neighborhood around me

411
00:45:23,760 --> 00:45:30,000
that is most useful for this task and so it transforms it into a graph but I don't think even when

412
00:45:30,000 --> 00:45:36,000
we worked on the paper, we didn't really dive into that element of it and looked at the connection

413
00:45:36,000 --> 00:45:40,640
between, okay, now that you've structured it as a graph or you're even thinking about as a graph,

414
00:45:40,640 --> 00:45:47,360
how does that change how you approach the learning task? I definitely think this question of

415
00:45:47,840 --> 00:45:53,680
how do we create graphs from tabular data as an important one? As we look to

416
00:45:55,680 --> 00:46:01,520
leverage more graph machine learning, there is a big question of how do we

417
00:46:02,960 --> 00:46:07,440
take and it's not always tabular in the sense of like features in a model. Sometimes it's just

418
00:46:07,440 --> 00:46:13,680
tabular data of records about a customer that we want to build into an actual network but there's

419
00:46:13,680 --> 00:46:18,320
a fundamental challenge and how do you go from that structure of a data table into something that

420
00:46:18,320 --> 00:46:23,440
you can use a graph convolutional network or even something simpler like belief propagation.

421
00:46:23,440 --> 00:46:28,000
It's a really hard problem oftentimes to go from data tables into graphs and I think that's one

422
00:46:28,000 --> 00:46:33,840
that we're actively looking at at the moment. Where would you say the kind of research frontier is and

423
00:46:33,840 --> 00:46:41,360
is kind of heading around this problem deep learning for our tabular data? I think

424
00:46:42,400 --> 00:46:48,400
when we mentioned, which is self-supervised pre-training, what are the transformers and so yeah.

425
00:46:49,360 --> 00:46:59,040
And particularly, what is the right way to augment data going into that? What's the right way to

426
00:46:59,040 --> 00:47:10,640
build a transformer backbones for tabular data? I think there's going to be continued study on

427
00:47:10,640 --> 00:47:16,480
encoders and regularizers and how you do the optimal combination of those. I don't think that's

428
00:47:16,480 --> 00:47:26,720
going away. And then I think from a theory perspective, we have to understand a little bit more

429
00:47:26,720 --> 00:47:31,680
about particularly as you start to combine data sets and as those data sets get more and more

430
00:47:31,680 --> 00:47:39,920
heterogeneous, what exactly the architecture is doing in that domain that it makes it at all useful

431
00:47:40,560 --> 00:47:46,080
for any of the downstream tasks. So as we start to push into larger pre-training,

432
00:47:47,760 --> 00:47:50,800
maybe this idea of foundation models for tabular data,

433
00:47:50,800 --> 00:47:58,000
there's open questions not just on how do you do it, but what does it even mean? And why would

434
00:47:58,000 --> 00:48:01,840
you do it? Because I think most people, if you mention that to them, we'll have a very similar

435
00:48:01,840 --> 00:48:06,480
to response to the one you did, which is why would you even do that? That doesn't make any sense

436
00:48:06,480 --> 00:48:12,880
for tabular data. And I think that's a very reasonable response. It doesn't make any sense.

437
00:48:12,880 --> 00:48:17,760
And so if we're going to do that research, which I do believe we will, and we have to, because

438
00:48:17,760 --> 00:48:21,600
that's the direction of, that we're seeing many of the domains of machine learning research go,

439
00:48:21,600 --> 00:48:26,640
we need to answer that question. And what is doing that research mean? Is it just collecting more

440
00:48:26,640 --> 00:48:30,160
and more and more data sets and seeing what happens when we try to train models on them?

441
00:48:30,880 --> 00:48:34,480
Yeah, collecting more and more data sets, seeing what happens when we try to chain models on them,

442
00:48:34,480 --> 00:48:43,840
understanding how to combine those data sets, how to train across multiple domains, whether,

443
00:48:43,840 --> 00:48:48,320
I mean, we talked about multi-modality, but even how do you build a

444
00:48:49,920 --> 00:48:54,960
transformer architecture that can then take in a variety of different tabular data sets,

445
00:48:54,960 --> 00:49:00,560
some might combine a bunch of categorical features, some might be very, very small data sets,

446
00:49:00,560 --> 00:49:06,960
other might be very large data sets, the scale and complexity of the diversity within the

447
00:49:06,960 --> 00:49:13,600
tabular domain is much greater than it is in computer vision and LP. And so those are the

448
00:49:13,600 --> 00:49:18,640
kinds of questions we have to answer is, is how do we build things that can generalize using all

449
00:49:18,640 --> 00:49:25,040
of that diverse data? Yeah, so we kind of set this up in talking about there are all these benefits

450
00:49:25,040 --> 00:49:34,960
that you get from the deep learning ecosystem and tooling. The disadvantage is that you,

451
00:49:36,800 --> 00:49:42,480
you know, we're not kind of at performance parity with the, you know, established methods,

452
00:49:42,480 --> 00:49:49,360
XG boost and the like, but you've also kind of highlighted that there are key gaps in terms

453
00:49:49,360 --> 00:49:58,480
of the tooling. Do you need to get to performance parity in order for the benefits of the deep learning

454
00:49:58,480 --> 00:50:05,680
ecosystem to actually think that we are already at performance parity. If you look at a lot of the

455
00:50:05,680 --> 00:50:10,080
research, many of these models are performing as well. Sometimes we're sometimes better,

456
00:50:10,080 --> 00:50:15,920
but on average as well as XG boost. And so that's why I was suggesting that it's the tooling

457
00:50:15,920 --> 00:50:25,360
ecosystem that needs to improve. I think, you know, many data scientists love the benefit of

458
00:50:27,200 --> 00:50:32,080
well-defined APIs, like I could learn the next to boost and that they don't have to spend a lot

459
00:50:32,080 --> 00:50:39,760
of time thinking about how do I get this piece of software to work for me? They just know that the

460
00:50:39,760 --> 00:50:45,440
piece of software has a fit function and a predict function and maybe six or seven hyperparameters

461
00:50:45,440 --> 00:50:48,800
that they have to know about and they have to know what those six or seven hyperparameters do

462
00:50:49,520 --> 00:50:56,640
with respect to how well that model operates. Many data scientists aren't used to, particularly

463
00:50:56,640 --> 00:51:02,480
ones who aren't working on computer vision and NLP, having to answer 400 different questions around

464
00:51:02,480 --> 00:51:07,600
whether I use this particular type of encoder, whether I use these, these sets of regularizers,

465
00:51:07,600 --> 00:51:13,600
how deep does it need to be? What are the dimensions of each of my, you know, different encoder,

466
00:51:14,400 --> 00:51:19,040
transformer encoders, et cetera? You know, there's so many choices in deep learning.

467
00:51:20,480 --> 00:51:26,080
It makes it really, really hard to, you know, plug and play the way that you can plug and play with like

468
00:51:28,720 --> 00:51:34,720
an XG boost model or a random forest. And so I think as we start to learn what things work the

469
00:51:34,720 --> 00:51:40,800
best, we'll get to a place within the deep learning, uh, tabular data ecosystem in which

470
00:51:40,800 --> 00:51:46,960
there are some tools. They have very simple APIs and a lot of that decision-making has already

471
00:51:46,960 --> 00:51:53,600
been made in the way that the, the tool has been built. And it reduces the complexity and the

472
00:51:53,600 --> 00:51:57,920
decisions that the data scientist has to make when they're, when they're choosing to use that model.

473
00:51:57,920 --> 00:52:03,360
And I think that's when we'll finally see widespread adoption. And once that's in place,

474
00:52:03,360 --> 00:52:08,160
then all of the other stuff, like the explainability, the ability to quantify uncertainty,

475
00:52:08,160 --> 00:52:13,200
all the things that have been developed around deep learning will start to be added on top of that.

476
00:52:13,200 --> 00:52:17,920
But that, that very simple interface, that very simple API I think is the, is the major hurdle

477
00:52:17,920 --> 00:52:22,320
that a lot of people face when they try to start using deep learning for tabular data. There's

478
00:52:22,320 --> 00:52:27,760
just too much for them to figure out. Well, Bion, this is, this has been a wonderful chat.

479
00:52:27,760 --> 00:52:33,680
Um, it's been a while since I've, uh, had a guest on the show talking about the, the topic,

480
00:52:33,680 --> 00:52:39,440
surprising as important as it is. Um, but this has been a great chat to get caught up on it.

481
00:52:39,440 --> 00:52:43,440
Thank you so much. It was an absolute pleasure. I really enjoyed talking to you,

482
00:52:43,440 --> 00:52:59,600
Dave Sam. Uh, my pleasure. Thank you.

