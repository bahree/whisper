1
00:00:00,000 --> 00:00:25,760
Hey everyone, hope you all had a wonderful holiday.

2
00:00:25,760 --> 00:00:30,520
For the next few weeks we'll be running back the clock with our second annual AI Rewind

3
00:00:30,520 --> 00:00:32,120
series.

4
00:00:32,120 --> 00:00:36,960
Join by a few friends of the show, we'll be reviewing the papers, tools, use cases,

5
00:00:36,960 --> 00:00:43,160
and other developments that made us splash in 2019 in key fields like machine learning,

6
00:00:43,160 --> 00:00:49,480
deep learning, NLP, computer vision, reinforcement learning, and ethical AI.

7
00:00:49,480 --> 00:00:55,680
Be sure to follow along with the series at twomolai.com slash rewind 19.

8
00:00:55,680 --> 00:01:00,080
As always, we'd love to hear your thoughts on this series, including anything we might

9
00:01:00,080 --> 00:01:01,080
have missed.

10
00:01:01,080 --> 00:01:06,600
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via

11
00:01:06,600 --> 00:01:11,480
a comment on the show notes page you can find at twomolai.com.

12
00:01:11,480 --> 00:01:13,720
Happy New Year, let's get into the show.

13
00:01:13,720 --> 00:01:19,240
Alright everyone, I am here with Timnett Geberu, Timnett is a research scientist at Google

14
00:01:19,240 --> 00:01:22,400
Brain, she co-leads a team in ethical AI.

15
00:01:22,400 --> 00:01:24,760
Timnett, welcome back to the Twomolai AI podcast.

16
00:01:24,760 --> 00:01:26,320
Thanks for having me.

17
00:01:26,320 --> 00:01:33,120
It is so great to see you once again, I think it's been a year since I last saw you in person.

18
00:01:33,120 --> 00:01:34,120
Oh yes, yes.

19
00:01:34,120 --> 00:01:40,200
Well, last year's Neurops and Black and AI, but two years since we spoke on the podcast

20
00:01:40,200 --> 00:01:43,880
about your research.

21
00:01:43,880 --> 00:01:52,680
This time around, we are going to be talking about advances in the fairness and ethics conversation

22
00:01:52,680 --> 00:01:58,040
around AI over the past year, but before we do that, maybe a little bit of a kind of

23
00:01:58,040 --> 00:02:03,480
update background from you for folks who want to hear what you've been up to in the past

24
00:02:03,480 --> 00:02:04,480
couple of years.

25
00:02:04,480 --> 00:02:08,920
Wow, it's like, it feels like so much time has passed.

26
00:02:08,920 --> 00:02:14,240
Yeah, so I think when we last spoke, I was a postdoc at Microsoft Research in their

27
00:02:14,240 --> 00:02:23,360
fate group, and I had just finished my PhD in computer vision, and I had just been at

28
00:02:23,360 --> 00:02:25,800
fate for like a few months, actually.

29
00:02:25,800 --> 00:02:31,920
And so I had, you know, it started to do work in fairness ethics my last year of my PhD,

30
00:02:31,920 --> 00:02:35,240
and then I wanted to do this postdoc to be more embedded in that community and to have

31
00:02:35,240 --> 00:02:37,160
more time to explore.

32
00:02:37,160 --> 00:02:42,320
And I had, you know, we had just done the first Black and AI workshop.

33
00:02:42,320 --> 00:02:46,840
And now it has, you know, this is our third time that we're doing Black and AI, and it

34
00:02:46,840 --> 00:02:51,800
has grown so much over the last two years.

35
00:02:51,800 --> 00:02:58,400
And so then I, I, then, oh, I was in New York when we spoke, and then I, now I'm, since

36
00:02:58,400 --> 00:03:03,560
about a year and three months, I've been at Google Brain, and so, co-leading the Ethical

37
00:03:03,560 --> 00:03:05,720
AI team with Meg Mitchell.

38
00:03:05,720 --> 00:03:11,280
And yeah, there's many, many challenges doing fairness and ethics in industry, but so

39
00:03:11,280 --> 00:03:15,840
far I love, you know, my team, and I love what I'm doing.

40
00:03:15,840 --> 00:03:21,120
So, you know, there's many struggles, but like it's been a journey in the last two years.

41
00:03:21,120 --> 00:03:22,480
Absolutely, absolutely.

42
00:03:22,480 --> 00:03:30,280
So we can probably start with Black and AI, since that plays such a big role in your work

43
00:03:30,280 --> 00:03:38,400
and life, and just so folks understand it, I think one of the things that really jumped

44
00:03:38,400 --> 00:03:43,680
out at me from the workshop this year, I think it's easy to think of it, and I think

45
00:03:43,680 --> 00:03:51,400
even NURPS calls these affinity organizations, and sure, you know, it represents an affinity

46
00:03:51,400 --> 00:03:58,680
for me and identity in a sense, but you know, there's also kind of real work that's happening

47
00:03:58,680 --> 00:04:04,520
not just to allow folks that have an affinity to come together, but to kind of expand the

48
00:04:04,520 --> 00:04:12,440
presence of Blacks and AIU, or someone mentioned that at the, a couple of years ago, or three

49
00:04:12,440 --> 00:04:13,440
years ago now, what?

50
00:04:13,440 --> 00:04:14,440
In 2016, yeah.

51
00:04:14,440 --> 00:04:15,440
In Barcelona.

52
00:04:15,440 --> 00:04:17,680
In Barcelona, there were five.

53
00:04:17,680 --> 00:04:19,760
Yeah, I counted five, yeah.

54
00:04:19,760 --> 00:04:20,760
You counted five.

55
00:04:20,760 --> 00:04:23,760
Yeah, I couldn't see anybody, yeah.

56
00:04:23,760 --> 00:04:27,560
But, you know, for those that don't have this experience in Blacks, sometimes you just

57
00:04:27,560 --> 00:04:29,040
know how many people are in.

58
00:04:29,040 --> 00:04:30,040
Yeah.

59
00:04:30,040 --> 00:04:31,840
And there was no formal count, too, right?

60
00:04:31,840 --> 00:04:35,160
That's one thing that has changed, like they're doing diversity statistics.

61
00:04:35,160 --> 00:04:36,160
They're counting statistics.

62
00:04:36,160 --> 00:04:42,160
Now, they had, Helen had this amazing survey that they spent so much time on that they

63
00:04:42,160 --> 00:04:47,280
put out last year, and now we have, you know, people advocating for accessibility, so we

64
00:04:47,280 --> 00:04:52,520
have disability in AI, we have Latinx and AI do not exist, you know.

65
00:04:52,520 --> 00:04:53,520
We're querying it.

66
00:04:53,520 --> 00:04:54,520
Did not exist at the time.

67
00:04:54,520 --> 00:04:55,520
Yeah.

68
00:04:55,520 --> 00:04:56,520
Yeah.

69
00:04:56,520 --> 00:05:01,440
New and ML, you know, I think there's much more of an understanding now that NURPS has

70
00:05:01,440 --> 00:05:07,080
been transformed, in my opinion, you know, because I, my first time being here was 2015,

71
00:05:07,080 --> 00:05:11,560
and I really, this was a conference I did not want to come back to, and I had written

72
00:05:11,560 --> 00:05:12,560
about it actually.

73
00:05:12,560 --> 00:05:17,160
I was going to send in an anonymous as an anonymous article, and like everybody, my friends

74
00:05:17,160 --> 00:05:20,760
were like, yeah, no, everybody's going to know it was you, so it's not going to be anonymous,

75
00:05:20,760 --> 00:05:24,480
but I was complaining about, you know, I was harassed at a party.

76
00:05:24,480 --> 00:05:28,120
If there was just so much, so much stuff that happened, I did not feel welcome, I didn't

77
00:05:28,120 --> 00:05:30,520
feel like the community was trying to pull me in.

78
00:05:30,520 --> 00:05:35,920
I felt like the community was basically trying to pull me out, push me out, you know.

79
00:05:35,920 --> 00:05:36,920
Why do I belong here?

80
00:05:36,920 --> 00:05:39,120
Why am I here, you know?

81
00:05:39,120 --> 00:05:48,280
And now when I come here, it's really big and overwhelming, that not only has, I feel

82
00:05:48,280 --> 00:05:52,800
like it's changed in terms of diversity and inclusion, but also the scope of, you know,

83
00:05:52,800 --> 00:05:58,880
workshops, they have all sorts of meridethwitticurs here, for example, and this is not just about

84
00:05:58,880 --> 00:06:03,360
ethics, but also organizing labor and why that's important.

85
00:06:03,360 --> 00:06:08,680
So I mean, it's, I cannot believe I'm saying this, but I find it so much more inclusive

86
00:06:08,680 --> 00:06:12,960
than my whole, my vision community, for example, and now it's like when I go to the vision

87
00:06:12,960 --> 00:06:17,680
community, I'm like, whoa, it's like three black people there, and the size is basically

88
00:06:17,680 --> 00:06:22,320
about the time, CVPR I think was like 6,500 people or something like that.

89
00:06:22,320 --> 00:06:26,880
And now when I see all the work that has been done at New Europe's to make it so much

90
00:06:26,880 --> 00:06:34,360
more inclusive, I just, I go to like my vision conferences and I'm just like, wow, there,

91
00:06:34,360 --> 00:06:36,240
it like hasn't even started yet, you know?

92
00:06:36,240 --> 00:06:40,320
Well, the thing that I specifically wanted to acknowledge you for and the team that works

93
00:06:40,320 --> 00:06:45,480
on black and AI is that I think, you know, the affinity aspect of it is great and making

94
00:06:45,480 --> 00:06:51,440
those of us that are in the community, you know, feel more included is wonderful, but this

95
00:06:51,440 --> 00:06:58,280
community is actually going out and finding other people and making it so that they can

96
00:06:58,280 --> 00:06:59,280
be here.

97
00:06:59,280 --> 00:07:00,280
Yeah.

98
00:07:00,280 --> 00:07:08,800
That was not possible before and it became really clear to me, you know, at the workshop,

99
00:07:08,800 --> 00:07:15,040
like after the workshop, there was some administrative thing that was happening that caused the

100
00:07:15,040 --> 00:07:19,360
people that were to line up to want, you know, that we're brought here by black and

101
00:07:19,360 --> 00:07:24,720
black and AI was the only reason they could be here to be lined up, and it was a lot of

102
00:07:24,720 --> 00:07:25,720
people.

103
00:07:25,720 --> 00:07:30,120
We were handing daily, you know, I don't think people understand how much work we do because

104
00:07:30,120 --> 00:07:32,120
you know, there is a lot of affinity groups, right?

105
00:07:32,120 --> 00:07:38,680
But many of them, they, what they do is they organize the workshop and then they raise

106
00:07:38,680 --> 00:07:42,480
some money and they determine how much they can give in funds to, you know, if you're

107
00:07:42,480 --> 00:07:46,840
coming from the African continent, if you're coming from, et cetera, et cetera.

108
00:07:46,840 --> 00:07:51,560
And then they basically give you a certain, they refund you a certain amount of money

109
00:07:51,560 --> 00:07:53,320
after the workshop, right?

110
00:07:53,320 --> 00:07:57,680
And we know that we can't do that for our participants because many of them wouldn't

111
00:07:57,680 --> 00:08:03,560
be able to just pay in advance and then, you know, get refunded, right?

112
00:08:03,560 --> 00:08:10,880
And so we give out a per-dem cash for people to use for food purposes.

113
00:08:10,880 --> 00:08:13,600
So that's what was happening.

114
00:08:13,600 --> 00:08:18,080
For the workshop, we were handing these Visa cards, prepay Visa cards, so that they

115
00:08:18,080 --> 00:08:23,160
can just use it for the rest of the week for their food expenses, yeah.

116
00:08:23,160 --> 00:08:26,800
And you know, there's a lot of advocacy that happens in Black NIA too, you know, we once

117
00:08:26,800 --> 00:08:34,040
again had our Visa issues, and then within Black NIA, you know, we also have to, of course,

118
00:08:34,040 --> 00:08:35,040
there's intersectionality, right?

119
00:08:35,040 --> 00:08:38,520
Just because, you know, somebody's Black doesn't mean we're addressing all of the issues.

120
00:08:38,520 --> 00:08:45,520
There are, you know, US-based, Brazil-based, the huge African continent with 1.1 billion

121
00:08:45,520 --> 00:08:46,520
people.

122
00:08:46,520 --> 00:08:52,080
And, you know, HBCUs, which community are we neglecting?

123
00:08:52,080 --> 00:08:54,760
Which community is underrepresented?

124
00:08:54,760 --> 00:08:56,080
Which communities are overrepresented?

125
00:08:56,080 --> 00:08:59,800
So those are things we had to, we constantly have to think about.

126
00:08:59,800 --> 00:09:04,760
So this year, I was really excited by our partnership with OHAB.

127
00:09:04,760 --> 00:09:12,000
They're great, and like we had some 25-26 people come through our partnership with OHAB,

128
00:09:12,000 --> 00:09:19,680
and they were primarily students from HBCUs, others were founders, and it was like one

129
00:09:19,680 --> 00:09:25,600
amazing moment for me was when this, and we also had many more Brazilians that this

130
00:09:25,600 --> 00:09:29,880
year than we did before, because, you know, just because we're in the US, it doesn't,

131
00:09:29,880 --> 00:09:36,840
we might have a different concept of, we don't always understand the severity of the problems

132
00:09:36,840 --> 00:09:37,840
in other places, right?

133
00:09:37,840 --> 00:09:41,400
So because Brazil was like the last place to abolish slavery, right?

134
00:09:41,400 --> 00:09:45,440
And we have like almost 60% of their population is Black.

135
00:09:45,440 --> 00:09:47,280
And so imagine, right?

136
00:09:47,280 --> 00:09:52,680
And then we, I don't ever see a Black Brazilian in any of my professional settings.

137
00:09:52,680 --> 00:09:59,840
And so this Ramon who came to the workshop was telling them some of the students that

138
00:09:59,840 --> 00:10:05,600
for two years in his undergrad, he never saw a single Black person in his entire undergrad,

139
00:10:05,600 --> 00:10:06,600
right?

140
00:10:06,600 --> 00:10:10,080
In a country where more than 50% of the population, so you understand the severity of

141
00:10:10,080 --> 00:10:12,080
the issues that exist.

142
00:10:12,080 --> 00:10:17,520
And what was interesting is like one of them was saying, so the first time he came to

143
00:10:17,520 --> 00:10:22,240
me, she was like, you're a Brazilian, and he was like, yeah, I was like, I never, you know,

144
00:10:22,240 --> 00:10:27,080
the image of a Brazilian that I see here does not look like you at all, so I didn't even

145
00:10:27,080 --> 00:10:30,920
know that country was majority Black, you know?

146
00:10:30,920 --> 00:10:35,800
And so that, I love that interaction, like it's not just, you know, having this space

147
00:10:35,800 --> 00:10:41,200
for the Black community to be here, but it's also like educating each other about the

148
00:10:41,200 --> 00:10:47,000
different communities and understanding global, like how diverse this community is.

149
00:10:47,000 --> 00:10:48,000
Yeah.

150
00:10:48,000 --> 00:10:50,400
Yeah, so that was a great moment for me when I sat up.

151
00:10:50,400 --> 00:10:51,400
Awesome.

152
00:10:51,400 --> 00:10:52,400
Awesome.

153
00:10:52,400 --> 00:10:54,880
Well, thanks for all your work on that front.

154
00:10:54,880 --> 00:11:00,160
We're a great person to talk to about what's going on and, you know, what is a, a very

155
00:11:00,160 --> 00:11:04,480
broad field, AI ethics, fairness, accountability, et cetera.

156
00:11:04,480 --> 00:11:11,000
So maybe let's start by having you talk about, you know, things that come to mind as, you

157
00:11:11,000 --> 00:11:14,560
know, milestones in 2019 in the field.

158
00:11:14,560 --> 00:11:19,480
Yeah, I think that one of, I was thinking about this, and I think one of the things I'm

159
00:11:19,480 --> 00:11:27,480
seeing right now is the fairness and ethics community starting a little bit to be, to

160
00:11:27,480 --> 00:11:28,960
look and work a little bit.

161
00:11:28,960 --> 00:11:35,000
So it's kind of wondering whether some of the technical approaches, whether they're

162
00:11:35,000 --> 00:11:40,400
causing more harm or whether they're, like, putting bandaid on, you know, just a bandaid

163
00:11:40,400 --> 00:11:47,240
or whether they're financially affecting things, how, you know, our approach needs to

164
00:11:47,240 --> 00:11:49,640
be sociotechnical.

165
00:11:49,640 --> 00:11:54,840
So I think for me, that's the milestone I'm seeing is that that conversation is starting

166
00:11:54,840 --> 00:11:57,760
to happen about the approaches themselves.

167
00:11:57,760 --> 00:12:03,760
So I actually, I'm not sure if this paper came out this year or was it last year, but

168
00:12:03,760 --> 00:12:09,560
I do believe it's like within the last year, it was this paper on delayed impacts of fairness.

169
00:12:09,560 --> 00:12:14,680
So examples of these kinds of papers, I think another paper, the title was, I believe

170
00:12:14,680 --> 00:12:19,440
it was lipstick on a pig, and it was talking about how, you know, these debiasing word

171
00:12:19,440 --> 00:12:27,200
embeddings might not actually be fixing the underlying problem.

172
00:12:27,200 --> 00:12:35,440
And what I'm personally, like, actually most excited about is the understanding that this

173
00:12:35,440 --> 00:12:37,520
can't just be a technical fix.

174
00:12:37,520 --> 00:12:44,960
And that, so for example, and now they had this report discriminating systems, right?

175
00:12:44,960 --> 00:12:46,840
And for them, it's a system, right?

176
00:12:46,840 --> 00:12:51,160
So they're not looking at discrimination in the images, discrimination, they're looking

177
00:12:51,160 --> 00:12:52,160
at the system.

178
00:12:52,160 --> 00:12:55,120
And the system includes the people who are building it.

179
00:12:55,120 --> 00:13:00,400
And so in that report, they were talking about gender discrimination and, you know, they

180
00:13:00,400 --> 00:13:08,800
were giving all sorts of references for people and how basically if you have a group of

181
00:13:08,800 --> 00:13:15,440
people who are homogeneous building these systems, you're never going to be able to achieve

182
00:13:15,440 --> 00:13:17,240
any sort of parity.

183
00:13:17,240 --> 00:13:21,720
And I think it's also the unders, I think the starting to discuss the fact that fairness

184
00:13:21,720 --> 00:13:26,240
is not just about equalizing performance across certain subgroups.

185
00:13:26,240 --> 00:13:30,040
So this is the first year where I'm starting to see more discussion, a lot more discussion

186
00:13:30,040 --> 00:13:34,840
about the fact that fairness is not just equalizing, you know, some error rates across

187
00:13:34,840 --> 00:13:35,840
different subgroups.

188
00:13:35,840 --> 00:13:36,840
Right?

189
00:13:36,840 --> 00:13:45,200
So, like in our team, Alex Hanna, Emily Denton, who just, so I came, I had to attend

190
00:13:45,200 --> 00:13:49,000
her talk right before here, gave a talk.

191
00:13:49,000 --> 00:13:57,280
And Jimi Lesmith, Lawden, Andy Smart just wrote a paper about critical, a critical

192
00:13:57,280 --> 00:14:00,680
race theory approach for fairness, right?

193
00:14:00,680 --> 00:14:06,960
So, so for example, the fact that, so Joanne and I were talking about how in our paper

194
00:14:06,960 --> 00:14:12,680
in gender shades how race is a social construct, right?

195
00:14:12,680 --> 00:14:16,080
It's unstable cross time and space, et cetera, et cetera.

196
00:14:16,080 --> 00:14:21,120
In this paper, they were, they were talking about how you have to really engage with critical

197
00:14:21,120 --> 00:14:29,160
race theory methods and how you have to, you know, you have, races again, an, a social

198
00:14:29,160 --> 00:14:35,280
construct that sometimes maybe that's not what we want to use for annotating data sets,

199
00:14:35,280 --> 00:14:37,160
same with the gender, right?

200
00:14:37,160 --> 00:14:44,480
And, but then there's a tension between, you know, if you're annotating for, let's say,

201
00:14:44,480 --> 00:14:45,480
gender, right?

202
00:14:45,480 --> 00:14:49,320
You, you need to make sure that you're not further harming communities by, you know,

203
00:14:49,320 --> 00:14:57,400
making a binary by further adding like data, additional data that needs to be added from

204
00:14:57,400 --> 00:14:58,400
groups of people.

205
00:14:58,400 --> 00:15:04,080
So that's like additional privacy risk, making sure I think I forgot this paper that talks

206
00:15:04,080 --> 00:15:10,000
about like the burden on the, on minority groups when you're, you're when you're trying

207
00:15:10,000 --> 00:15:16,200
to get more data from them, but at the same time in order to equalize parity across subgroups

208
00:15:16,200 --> 00:15:19,240
or even not even equalize parity, but like just to have parity.

209
00:15:19,240 --> 00:15:23,360
But just to test out how well something is doing across subgroups, you have to define

210
00:15:23,360 --> 00:15:30,520
these subgroups, what they are, what those boundaries are, and you have to then go and gather

211
00:15:30,520 --> 00:15:32,640
additional data sets for that.

212
00:15:32,640 --> 00:15:37,600
So what problems occur when you're thinking about that process itself?

213
00:15:37,600 --> 00:15:38,600
Now is that.

214
00:15:38,600 --> 00:15:41,520
I'm seeing a lot more discussion on that, on that process itself.

215
00:15:41,520 --> 00:15:42,520
You see what I mean?

216
00:15:42,520 --> 00:15:47,640
When I first started working on it, it was like even, you know, the notion that you have

217
00:15:47,640 --> 00:15:54,240
to do intersectional testing, so you don't just test for, you know, if my model is doing

218
00:15:54,240 --> 00:15:57,640
well on everybody, what does well mean?

219
00:15:57,640 --> 00:16:01,360
So there's a lot of, you know, paper is own defining notions of fairness.

220
00:16:01,360 --> 00:16:05,760
There was, you know, and then we had to introduce the concept of how you have to do disaggregated

221
00:16:05,760 --> 00:16:10,640
testing, which means, you know, don't just say, is my model doing well for like women?

222
00:16:10,640 --> 00:16:15,440
Is it just doing well for like, you know, is it doing well for like this group of people,

223
00:16:15,440 --> 00:16:19,760
this race, that race, you have to say, is it doing well for darker skin women, lighter

224
00:16:19,760 --> 00:16:24,760
skinned women, darker skinned men, you know, that concept was a concept that was like

225
00:16:24,760 --> 00:16:25,760
kind of introduced, right?

226
00:16:25,760 --> 00:16:29,560
I mean, we're just saying like, how important it is to do that, to break it down like

227
00:16:29,560 --> 00:16:30,560
that.

228
00:16:30,560 --> 00:16:36,880
Now the question is to make sure people understand, hey, that's really not the only thing.

229
00:16:36,880 --> 00:16:45,600
We don't assume that every single question about fairness is whether the performance is

230
00:16:45,600 --> 00:16:47,520
equal across different separate groups, right?

231
00:16:47,520 --> 00:16:51,720
It's about who has the data, who doesn't, how it's being used, whether a task should

232
00:16:51,720 --> 00:16:54,320
exist or not, et cetera, et cetera.

233
00:16:54,320 --> 00:17:02,160
And then even in cases where you have to define subgroups to see the models performance

234
00:17:02,160 --> 00:17:07,800
on those subgroups, you have to like creating those subgroups is not a piece of cake, right?

235
00:17:07,800 --> 00:17:12,720
Like in the complexities that arise when you define subgroups, who's doing the defining,

236
00:17:12,720 --> 00:17:14,240
the taxonomy that you use?

237
00:17:14,240 --> 00:17:18,800
So like, I'm seeing much more nuanced combo, like discussions around this.

238
00:17:18,800 --> 00:17:25,320
So this paper that I was discussing, critical race theory message for fairness, so I'm

239
00:17:25,320 --> 00:17:27,560
so bad at like these titles is.

240
00:17:27,560 --> 00:17:29,400
We'll find it and put it in the show notes.

241
00:17:29,400 --> 00:17:31,000
Yeah, it's those kinds of things.

242
00:17:31,000 --> 00:17:35,760
And that's what I'm excited about, and especially just like the understanding, the complexity

243
00:17:35,760 --> 00:17:37,440
of what we're talking about, right?

244
00:17:37,440 --> 00:17:41,040
It's not just about defining what is fair, and so you have a mathematical definition

245
00:17:41,040 --> 00:17:42,040
for it.

246
00:17:42,040 --> 00:17:46,440
It's also like how your model interacts with the society that you're in, what kind of

247
00:17:46,440 --> 00:17:50,680
documentation you have available, how you take feedback from that, and I'm starting

248
00:17:50,680 --> 00:17:52,600
to see that conversation right now, right?

249
00:17:52,600 --> 00:17:56,440
So what do you think is driving kind of the increased appreciation of the nuances here?

250
00:17:56,440 --> 00:18:00,560
Is it kind of broadening of the folks that are in the field, or just...

251
00:18:00,560 --> 00:18:02,320
I think they're longer.

252
00:18:02,320 --> 00:18:08,640
I think broadening of the folks that are in the field, I think that for example, more

253
00:18:08,640 --> 00:18:14,240
of us, more people finding each other's voices and kind of amplifying each other's voices.

254
00:18:14,240 --> 00:18:19,480
So for example, Emily Denton is very well known for generative models.

255
00:18:19,480 --> 00:18:21,640
That's what this community knows her for, right?

256
00:18:21,640 --> 00:18:23,320
Like the Neurops community and everything.

257
00:18:23,320 --> 00:18:28,000
So she was invited to give a talk on this workshop, Retrospectives.

258
00:18:28,000 --> 00:18:30,960
So it's basically, do you know about this workshop?

259
00:18:30,960 --> 00:18:35,760
You're supposed to talk about a paper or something that you wrote and like what would you

260
00:18:35,760 --> 00:18:37,240
say now kind of thing?

261
00:18:37,240 --> 00:18:41,240
And she did a retrospective on computer vision as a field.

262
00:18:41,240 --> 00:18:44,640
And what she was discussing is all of these things I'm telling you about, the view from

263
00:18:44,640 --> 00:18:45,640
nowhere.

264
00:18:45,640 --> 00:18:51,560
She was talking about this concept of the view from nowhere and how this has been critiqued

265
00:18:51,560 --> 00:18:54,320
by feminist studies, for example.

266
00:18:54,320 --> 00:19:01,640
And so the view from nowhere means it's basically saying that scientists assume that the view,

267
00:19:01,640 --> 00:19:04,160
it's subject, you know, science is objective.

268
00:19:04,160 --> 00:19:07,200
You're finding these, trying to find this objective truth.

269
00:19:07,200 --> 00:19:10,240
So it's not from anyone's point of view.

270
00:19:10,240 --> 00:19:14,680
It's that like there's objective truth and it's a view from nowhere.

271
00:19:14,680 --> 00:19:17,360
So in fact, it's called the view from nowhere.

272
00:19:17,360 --> 00:19:23,360
So she was describing how this, you know, underlying foundation of this feeling that our work

273
00:19:23,360 --> 00:19:29,400
is the view from nowhere and how that's kind of driving a lot of these, and power dynamics

274
00:19:29,400 --> 00:19:32,080
and how that's driving a lot of these issues we're discussing.

275
00:19:32,080 --> 00:19:36,360
So for example, she was naming certain data sets even that we don't question, right?

276
00:19:36,360 --> 00:19:42,320
And like one example of Celeb A data set, which is Celeb A, which is this data set of

277
00:19:42,320 --> 00:19:44,960
faces that is very widely used in computer vision.

278
00:19:44,960 --> 00:19:50,560
And so one example I want to give you is even when people are thinking about fairness,

279
00:19:50,560 --> 00:19:52,280
they would use Celeb A.

280
00:19:52,280 --> 00:19:56,920
So Celeb A is this data set that has 40 facial attributes that are kind of annotated.

281
00:19:56,920 --> 00:19:57,920
Okay.

282
00:19:57,920 --> 00:20:03,320
And so let's say it's like smiling versus not, young versus not attractive versus not,

283
00:20:03,320 --> 00:20:04,320
you know?

284
00:20:04,320 --> 00:20:09,160
And so when people are using this data set, people have used this data set to train all

285
00:20:09,160 --> 00:20:12,680
sorts of models and especially in the GAN literature.

286
00:20:12,680 --> 00:20:18,960
And when they're even writing about fairness, many people would use this data set and they

287
00:20:18,960 --> 00:20:23,280
would just say, oh, like we're trying to make sure that attractiveness, you know, label

288
00:20:23,280 --> 00:20:25,320
is not dependent on gender or something.

289
00:20:25,320 --> 00:20:26,320
Like, I don't know.

290
00:20:26,320 --> 00:20:27,320
So it's this kind of thing.

291
00:20:27,320 --> 00:20:32,640
And then what we, but also you have to question how, whether we should have an attract

292
00:20:32,640 --> 00:20:37,200
a data set within attractiveness label in the first place, who annotated this data set,

293
00:20:37,200 --> 00:20:38,200
you know?

294
00:20:38,200 --> 00:20:41,520
And where is this kind of model going to be used by whom?

295
00:20:41,520 --> 00:20:44,080
And so that was the kind of stuff that Emily was talking about.

296
00:20:44,080 --> 00:20:46,960
And data annotation practices, data collection practices.

297
00:20:46,960 --> 00:20:53,360
So we, at least there, you know, there's a long, long way to go to even have people listen

298
00:20:53,360 --> 00:20:54,360
to this conversation FYI.

299
00:20:54,360 --> 00:20:57,160
But what I'm saying is, oh, yeah, for sure.

300
00:20:57,160 --> 00:21:01,400
I mean, I was called an actor like I was given all sorts of feedback for having this kind

301
00:21:01,400 --> 00:21:02,400
of talk.

302
00:21:02,400 --> 00:21:08,000
But I'm saying that like, at first, we had to just be like, hey, you have to pay attention

303
00:21:08,000 --> 00:21:13,200
to this thing, you know, and we couldn't be too super nuanced about it.

304
00:21:13,200 --> 00:21:18,680
And now we're saying, hey, like when you're now thinking fairness, just don't think just

305
00:21:18,680 --> 00:21:23,640
equalizing metrics of some metric across the groups and just publishing a paper.

306
00:21:23,640 --> 00:21:24,880
Here are all the nuances.

307
00:21:24,880 --> 00:21:25,880
It's a system.

308
00:21:25,880 --> 00:21:28,520
And here is how we should think about it as a system.

309
00:21:28,520 --> 00:21:31,800
And so that's really the biggest thing I've seen.

310
00:21:31,800 --> 00:21:37,000
And how, when I say a system and like how it's really, you know, labor organizing that

311
00:21:37,000 --> 00:21:39,000
Meredith especially does, right?

312
00:21:39,000 --> 00:21:44,200
And how that feeds into, for example, labor organizing for contract workers, who are

313
00:21:44,200 --> 00:21:47,480
the ones who annotated these data sets that we're talking about, right?

314
00:21:47,480 --> 00:21:50,600
What their incentives are, how they're treated, et cetera, et cetera.

315
00:21:50,600 --> 00:21:55,800
What the economic incentives are, and how that drives some of our decisions in AI.

316
00:21:55,800 --> 00:22:00,560
And then how that feeds in, so like we're starting to look at it as a, or more and more

317
00:22:00,560 --> 00:22:03,200
people are starting to look at it as a system.

318
00:22:03,200 --> 00:22:08,480
But at the same time, because fairness is also now like a much more, I would say it's become

319
00:22:08,480 --> 00:22:13,800
like more of a mainstream thing like for CBPR for the computer vision call for papers.

320
00:22:13,800 --> 00:22:19,520
I saw like an actual explicit like bullet point for fairness, accountability, transparency

321
00:22:19,520 --> 00:22:21,560
and ethics and computer vision.

322
00:22:21,560 --> 00:22:25,240
Whereas last year I co-organized the first workshop of fairness, accountability, transparency

323
00:22:25,240 --> 00:22:26,680
and ethics and computer vision, right?

324
00:22:26,680 --> 00:22:33,000
And so it's starting to be like this mainstream thing in all of these different conferences.

325
00:22:33,000 --> 00:22:41,400
One shift that I'm seeing, and I'm wondering if it's new or you're seeing it also or

326
00:22:41,400 --> 00:22:50,960
is it interesting, it feels like prior conversations about fairness and ethics, or at least

327
00:22:50,960 --> 00:22:54,680
a lot of them that I've been exposed to have been around like, you know, with just what

328
00:22:54,680 --> 00:22:59,080
you've been describing tools to analyze the predictions you're making or your data sets

329
00:22:59,080 --> 00:23:05,840
for biased things like that, and I'm hearing more conversations that are asking more fundamental

330
00:23:05,840 --> 00:23:09,120
questions like, should we be doing thing X at all?

331
00:23:09,120 --> 00:23:10,120
Exactly.

332
00:23:10,120 --> 00:23:13,360
So I think that is that conversation I'm seeing more of now.

333
00:23:13,360 --> 00:23:17,960
In the beginning it was more like, hey, there's this thing called fairness, or, you know,

334
00:23:17,960 --> 00:23:21,520
oh, that's a paper, this thing called fairness, every step of the paper, yeah, that was

335
00:23:21,520 --> 00:23:23,280
people in my team really, really love.

336
00:23:23,280 --> 00:23:24,280
This year?

337
00:23:24,280 --> 00:23:25,280
Yeah.

338
00:23:25,280 --> 00:23:26,280
Okay.

339
00:23:26,280 --> 00:23:27,280
And yeah.

340
00:23:27,280 --> 00:23:28,280
So, so.

341
00:23:28,280 --> 00:23:29,280
Yeah.

342
00:23:29,280 --> 00:23:32,160
So there's, you know, in the beginning, I think, you know, many, many people, especially

343
00:23:32,160 --> 00:23:36,840
in the theoretical, more people in theory, I would say, in the theoretical computer science

344
00:23:36,840 --> 00:23:42,240
we're working on fairness as a, just a set of field, and I would say, a lot of people

345
00:23:42,240 --> 00:23:47,720
were not engaging with like critical waste theory or feminist, you know, theory or anything

346
00:23:47,720 --> 00:23:53,560
like that, from some feminist works or anything like that, and now there are, you know, still

347
00:23:53,560 --> 00:23:59,720
a few people, but who are engaged in both, like, bridges between those two communities,

348
00:23:59,720 --> 00:24:03,400
and then there are some people who are still in one community or the other community,

349
00:24:03,400 --> 00:24:10,240
but I believe that the conversation right now is almost trying to address the fairness

350
00:24:10,240 --> 00:24:11,640
community itself.

351
00:24:11,640 --> 00:24:17,240
So initially, we were, I think people were just trying to have a community at all, and

352
00:24:17,240 --> 00:24:22,280
now, like, I don't think the justification needs to be made that people in, at sea,

353
00:24:22,280 --> 00:24:27,080
in Europe, or at CBPR, or these other conferences should work on fairness, but another conversation

354
00:24:27,080 --> 00:24:31,520
is more, hey, like, when you think of fairness, like, don't just go, you know, take a data

355
00:24:31,520 --> 00:24:35,760
set that's already problematic, think about how it's problematic, and have, you know,

356
00:24:35,760 --> 00:24:39,320
what kinds of tools can you have to analyze why something is a problem in the first place,

357
00:24:39,320 --> 00:24:40,320
right?

358
00:24:40,320 --> 00:24:44,280
The view from nowhere, critical waste theory, like, you know, another conversation we've

359
00:24:44,280 --> 00:24:51,280
been having a lot is about global notions of fairness, a lot of fairness work, because

360
00:24:51,280 --> 00:24:57,600
is, I guess, is being driven by North American people in general, people who might not be

361
00:24:57,600 --> 00:24:59,480
from here, even, but, like, from the U.S.

362
00:24:59,480 --> 00:25:00,760
Have a certain perspective, though.

363
00:25:00,760 --> 00:25:05,200
Yeah, from the U.S. or from Canada, I guess, Mexico, but I wouldn't say Mexico is very

364
00:25:05,200 --> 00:25:10,720
highly represented in this work, have a certain perspective, and a lot of it is grounded

365
00:25:10,720 --> 00:25:14,200
in, like, let's say, civil rights, or our U.S. law, or something, right?

366
00:25:14,200 --> 00:25:19,920
And so, what about other perspectives, what, what transfer is, and what doesn't transfer,

367
00:25:19,920 --> 00:25:23,320
you know, these are some of the conversations that people are starting to have.

368
00:25:23,320 --> 00:25:25,400
So, that's sort of what I'm excited about.

369
00:25:25,400 --> 00:25:31,920
I'm excited about how, you know, how, like, more awareness of how you can't just sit in

370
00:25:31,920 --> 00:25:35,800
a corner and write equations and do fairness.

371
00:25:35,800 --> 00:25:43,240
I'm really, I just, I honestly believe, for me, that it really always comes back to what

372
00:25:43,240 --> 00:25:46,920
people do in international development as well, right?

373
00:25:46,920 --> 00:25:53,840
There's a term called the, I believe, the seductive reductionism of other people's problems,

374
00:25:53,840 --> 00:25:54,840
right?

375
00:25:54,840 --> 00:26:00,040
It's like, okay, so when you're walking down the street, and in your own neighborhood,

376
00:26:00,040 --> 00:26:05,720
you see homelessness, you realize how complex it is to work on homelessness, you know?

377
00:26:05,720 --> 00:26:10,600
But when you're, you know, watching TV and, like, some person is holding an African

378
00:26:10,600 --> 00:26:12,160
baby somewhere, you know what I mean?

379
00:26:12,160 --> 00:26:14,080
There's always these shows, right?

380
00:26:14,080 --> 00:26:17,600
You might be like, oh, let me go solve that, because you don't understand the complexity,

381
00:26:17,600 --> 00:26:18,600
right?

382
00:26:18,600 --> 00:26:21,600
You're not thinking that maybe there are a lot of people who have ideas on how to fix

383
00:26:21,600 --> 00:26:24,960
their own problems, and they don't have, you know, visibility.

384
00:26:24,960 --> 00:26:26,720
They don't have toolkits, right?

385
00:26:26,720 --> 00:26:29,640
And so, how can you uplift them?

386
00:26:29,640 --> 00:26:33,760
And one thing I worry about in the theoretical fairness community is, like, for example, there

387
00:26:33,760 --> 00:26:39,400
was just a fairness workshop that was held at, you know, the Simon's Institute, I believe,

388
00:26:39,400 --> 00:26:42,120
it was all, like, not a single black person.

389
00:26:42,120 --> 00:26:45,600
You know what I'm saying? Like, you have 15, 16 people.

390
00:26:45,600 --> 00:26:51,720
It's like a closed thing, and they're doing research, mathematical research from fairness,

391
00:26:51,720 --> 00:26:56,160
their papers are getting published, they're getting tenure, talking about black people,

392
00:26:56,160 --> 00:26:59,400
and there's not a single black scholar there.

393
00:26:59,400 --> 00:27:01,400
I think that's exploitative.

394
00:27:01,400 --> 00:27:05,960
And then they would probably say, well, like, we're looking for just this specific expertise

395
00:27:05,960 --> 00:27:09,480
and, well, there's no black people with this specific experience, but then part of your

396
00:27:09,480 --> 00:27:14,160
job, if you do really, really care about fairness, whether it's theory or whether something

397
00:27:14,160 --> 00:27:21,520
else, is to make sure there are people in that community who can also get the scientific

398
00:27:21,520 --> 00:27:26,640
credit for this work, right, like, get, and so that's really, really, for me, what's missing

399
00:27:26,640 --> 00:27:30,760
in this community and the thing I worry about the most.

400
00:27:30,760 --> 00:27:39,320
Do you see any salient changes over the past year in the way fairness is approached or

401
00:27:39,320 --> 00:27:43,960
advanced from a kind of practical industry perspective?

402
00:27:43,960 --> 00:27:45,760
Yes, definitely.

403
00:27:45,760 --> 00:27:49,560
So I guess I'll do a plug here.

404
00:27:49,560 --> 00:27:54,560
One example, oh man, yeah, I mean, I didn't mean to do it like this, but one example is

405
00:27:54,560 --> 00:28:01,880
model cards for model reporting, which, Meg Mitchell and so many people in our team and

406
00:28:01,880 --> 00:28:06,520
across collaborations, worked on, right?

407
00:28:06,520 --> 00:28:08,720
And so a bunch of us had been there.

408
00:28:08,720 --> 00:28:13,040
Like this related to the data sets for data sets, for data sets, yeah, yeah, exactly.

409
00:28:13,040 --> 00:28:14,040
Exactly.

410
00:28:14,040 --> 00:28:15,040
Basically, yeah.

411
00:28:15,040 --> 00:28:21,160
And so a bunch, like, you know, I realized, when I was thinking about data sets and people

412
00:28:21,160 --> 00:28:27,280
at MSR, like Hannah, Jen, and Hal, and Jamie, and Brianna, and all these people, and Hal

413
00:28:27,280 --> 00:28:32,840
and Kate Crawford were thinking about it, then Emily Bender, Beth, and Friedman also had

414
00:28:32,840 --> 00:28:35,160
to have this paper data statements for NLP.

415
00:28:35,160 --> 00:28:36,360
So that was like two years.

416
00:28:36,360 --> 00:28:37,360
It's really interesting, right?

417
00:28:37,360 --> 00:28:40,400
And we're independently sort of like thinking about this around the same time.

418
00:28:40,400 --> 00:28:45,840
Of course, we're talking to each other, we're inspired by what we, the other person says.

419
00:28:45,840 --> 00:28:50,600
And a couple of years ago, it was like, hey, we think we think we should do this.

420
00:28:50,600 --> 00:28:52,880
We think, you know, shavdishies for data sets.

421
00:28:52,880 --> 00:28:56,840
And here's why you have to justify it, like here's why you think we, and then around

422
00:28:56,840 --> 00:29:01,600
the same time, like before I arrived at Google, there were also working on, you know, disaggregated

423
00:29:01,600 --> 00:29:06,920
testing and thinking about how they could apply this to models and stuff way back, right?

424
00:29:06,920 --> 00:29:13,640
And now, fast forward, just down, like just like right before Thanksgiving, Google announced

425
00:29:13,640 --> 00:29:19,720
model cards for model reporting as part of their Cloud AI, like explainability toolkit.

426
00:29:19,720 --> 00:29:23,040
And like, you know, you could go see example models.

427
00:29:23,040 --> 00:29:27,520
I mean, this is just V1 kind of, and it's also to help like gather feedback from people

428
00:29:27,520 --> 00:29:30,360
to see like what works well, what doesn't work well.

429
00:29:30,360 --> 00:29:33,360
But this is actually a thing for a real Google product.

430
00:29:33,360 --> 00:29:39,280
You and folks on an ethics team have worked on that is now part of a Google product that

431
00:29:39,280 --> 00:29:41,080
people can go swipe with.

432
00:29:41,080 --> 00:29:42,080
Yeah.

433
00:29:42,080 --> 00:29:43,080
So that's what I mean.

434
00:29:43,080 --> 00:29:45,880
So it's a, you know, before we were trying to convince everybody, this is something

435
00:29:45,880 --> 00:29:47,200
that should happen.

436
00:29:47,200 --> 00:29:48,720
Now it's a start, right?

437
00:29:48,720 --> 00:29:54,600
Like so for a real model, a real Google model is not a toy one, right?

438
00:29:54,600 --> 00:29:56,520
That is being sold to people.

439
00:29:56,520 --> 00:29:59,440
You can see you have a model card for it, right?

440
00:29:59,440 --> 00:30:03,760
And it was, it's not a piece of cake to get an institution to do this, right?

441
00:30:03,760 --> 00:30:04,920
Like it's not.

442
00:30:04,920 --> 00:30:09,360
And it like requires so much hard work and like a mega and Parker and Andrew, I mean,

443
00:30:09,360 --> 00:30:14,280
so many people have like, it requires so many people from different, with different expertise

444
00:30:14,280 --> 00:30:16,760
and different organizations.

445
00:30:16,760 --> 00:30:18,440
And so that's one thing I'm seeing, right?

446
00:30:18,440 --> 00:30:24,580
And I'm seeing actually conversations around data consortiums, just like higher level

447
00:30:24,580 --> 00:30:29,880
concept, concept, conversations, I believe they use having all these conversations.

448
00:30:29,880 --> 00:30:36,760
One thing I see, I think, is that so many organizations are talking about having ethics

449
00:30:36,760 --> 00:30:41,160
principles or guidelines and things like this, but then the question is like, how do they

450
00:30:41,160 --> 00:30:42,160
get enforced, right?

451
00:30:42,160 --> 00:30:44,720
And like, that's one shift I've seen in the last year, right?

452
00:30:44,720 --> 00:30:47,840
Like there's a lot of AI principles, guidelines and all that stuff.

453
00:30:47,840 --> 00:30:51,920
And then now I think the next kind of year or two, whatever is like, how do they get

454
00:30:51,920 --> 00:30:55,760
enforced, you know, how do we beat, how do people, how do people believe that they will

455
00:30:55,760 --> 00:30:57,400
be enforced?

456
00:30:57,400 --> 00:31:04,760
We wrote recently, like with the Debra Rajin, Andrew Smart and many other people, we wrote

457
00:31:04,760 --> 00:31:09,400
a paper trying to kind of think about this, but like, but I think that's the next.

458
00:31:09,400 --> 00:31:13,440
So I guess I was doing like more retrospective of what I want to see in the future kind

459
00:31:13,440 --> 00:31:15,440
of thing, right?

460
00:31:15,440 --> 00:31:21,720
Like, so yeah, so I think that like model cards being real for me is a huge thing, right?

461
00:31:21,720 --> 00:31:28,040
And I don't know, I think, you know, there's many, the partnership in AI has just about

462
00:31:28,040 --> 00:31:33,520
a Mel project now and it's about a Mel is like the acronym.

463
00:31:33,520 --> 00:31:34,520
Oh, okay.

464
00:31:34,520 --> 00:31:39,600
And so they have this like, it's based on, you know, data sheets and model cards and fact

465
00:31:39,600 --> 00:31:44,240
sheets from IBM and they're trying to see how this can actually be a standard, right?

466
00:31:44,240 --> 00:31:47,560
And it's, you know, it's funny, like when we write papers and stuff, we like, this should

467
00:31:47,560 --> 00:31:52,000
happen, but then when you're implemented, it's also like, how many stakeholders are there?

468
00:31:52,000 --> 00:31:56,720
Like, what's the right granularity of documentation for whom and what format?

469
00:31:56,720 --> 00:31:59,960
I mean, it's so much stuff, right?

470
00:31:59,960 --> 00:32:04,680
So, but actually, but the fact that it's actually starting to happen is, is a, is a pretty

471
00:32:04,680 --> 00:32:06,000
big deal for me.

472
00:32:06,000 --> 00:32:08,640
There was a, I don't know if it was a workshop.

473
00:32:08,640 --> 00:32:13,760
I imagine it was a workshop that I saw, I only saw the title of this on one of the electronic

474
00:32:13,760 --> 00:32:18,520
signs in the conference center here at Neurops, that was intriguing.

475
00:32:18,520 --> 00:32:23,840
It was my name, the gap ethics and fairness, do you know what that was about?

476
00:32:23,840 --> 00:32:28,480
I see a lot of people, actually, from my team, Alex Hannah and Vinod from my team are speaking

477
00:32:28,480 --> 00:32:33,560
at the panel there, Meredith was there, I think, I know, I know about this workshop.

478
00:32:33,560 --> 00:32:38,560
I wasn't involved, like, so basically maybe next year, this will change, but when I come

479
00:32:38,560 --> 00:32:42,680
to Neurif's, I will, I don't get involved in any workshops, I don't, I don't give

480
00:32:42,680 --> 00:32:48,960
any talks, I don't, because basically Black and I is then, so it's really hard to do anything

481
00:32:48,960 --> 00:32:49,960
else.

482
00:32:49,960 --> 00:32:50,960
Yeah.

483
00:32:50,960 --> 00:32:55,440
So, I, I chair a session, whatever session, I have to chair, I go there, but like, it's

484
00:32:55,440 --> 00:33:00,320
hard to take on anything additional, but yeah, but this workshop I know, I've, I've heard

485
00:33:00,320 --> 00:33:05,920
about it because of this, but it's actually good that, you know, they have people like,

486
00:33:05,920 --> 00:33:12,680
Meredith, they have another person, I believe who was one of the people who just got fired

487
00:33:12,680 --> 00:33:18,720
before Thanksgiving for, I, I don't know, I mean, one of, he was one of the people who

488
00:33:18,720 --> 00:33:25,400
organized, who were organizing against like Google's relationship with, I guess, ICE, right?

489
00:33:25,400 --> 00:33:27,160
And so it's hard, right?

490
00:33:27,160 --> 00:33:30,760
Like when you're working at an institution, like, there's their policy and then there's

491
00:33:30,760 --> 00:33:35,440
you as an individual working and being like, well, like, I'm an immigrant, you know what

492
00:33:35,440 --> 00:33:36,440
I mean?

493
00:33:36,440 --> 00:33:42,520
I was a refugee and so how can I talk, how can I work in ethics and whatever and I don't

494
00:33:42,520 --> 00:33:45,800
care if you have a model card for the model that you're giving them, right?

495
00:33:45,800 --> 00:33:47,600
These are the layers, right?

496
00:33:47,600 --> 00:33:48,600
Right, right.

497
00:33:48,600 --> 00:33:52,120
And, and you have a, you know, like a relationship with them.

498
00:33:52,120 --> 00:33:56,200
So like, he was, you know, one of the people who was working on this, organizing against

499
00:33:56,200 --> 00:34:02,320
this, who was fired right before Thanksgiving, and as you know, Meredith could organize

500
00:34:02,320 --> 00:34:07,500
the walk out all, all of this anti-maving activism and she's, you know, huge obviously

501
00:34:07,500 --> 00:34:10,040
in the ethics community and she's no longer there, right?

502
00:34:10,040 --> 00:34:17,120
So it, it, it makes me wonder like if my days are numbered or not, you know, let's hope

503
00:34:17,120 --> 00:34:18,120
not.

504
00:34:18,120 --> 00:34:19,120
I hope not.

505
00:34:19,120 --> 00:34:20,120
You know, I mean.

506
00:34:20,120 --> 00:34:21,120
I've got model cards.

507
00:34:21,120 --> 00:34:22,120
Yeah.

508
00:34:22,120 --> 00:34:27,600
What are you seeing happening on the commercial side of things like I'm hearing more and

509
00:34:27,600 --> 00:34:34,680
more like IBM, for example, you know, got fairness 360 and I forget what the other, they've

510
00:34:34,680 --> 00:34:39,720
got several of these kind of ethics, fairness, focused things.

511
00:34:39,720 --> 00:34:45,080
There are startups that are, you know, focusing on explainability, which kind of plays into

512
00:34:45,080 --> 00:34:46,080
this.

513
00:34:46,080 --> 00:34:52,640
You know, how do you think about the, you know, the commercial space or even open source

514
00:34:52,640 --> 00:34:59,160
like the set of activities going towards making this more tangible and accessible to practitioners?

515
00:34:59,160 --> 00:35:00,160
Yeah.

516
00:35:00,160 --> 00:35:05,440
Like I think, you know, again, like a lot of companies that are coming out with toolkits and like

517
00:35:05,440 --> 00:35:08,080
so the IBM one is a good one.

518
00:35:08,080 --> 00:35:12,440
Google like is integrating a lot of things into TensorFlow, like there's fairness indicators.

519
00:35:12,440 --> 00:35:18,520
For example, there's, there's also a lot of educational material that actually like

520
00:35:18,520 --> 00:35:24,640
Andrew Salvador on our team really like worked so hard on various colabs, which you know,

521
00:35:24,640 --> 00:35:27,960
like on other educational material that's out.

522
00:35:27,960 --> 00:35:32,760
So like for anybody who wants to kind of learn a little bit more about, you know, what

523
00:35:32,760 --> 00:35:36,080
things to watch out for, et cetera, like there's this stuff happening.

524
00:35:36,080 --> 00:35:37,080
Let's see.

525
00:35:37,080 --> 00:35:42,000
There's also, you know, when you're on the topic of IBM, so again, there's this brings up

526
00:35:42,000 --> 00:35:43,600
the complexity, right?

527
00:35:43,600 --> 00:35:45,920
Like so they also came out with a data set, right?

528
00:35:45,920 --> 00:35:50,440
So diversity and faces data set, but then there's also other things that show like the complexity

529
00:35:50,440 --> 00:35:52,040
having this kind of data set, right?

530
00:35:52,040 --> 00:35:57,840
So people are talking about like the fact that that data set was scraped through Flickr,

531
00:35:57,840 --> 00:36:03,920
but then, you know, even though you have creative comments licensing, like when people put out

532
00:36:03,920 --> 00:36:07,640
their pictures on the Flickr, they didn't know like 10 years later and that it was going

533
00:36:07,640 --> 00:36:09,400
to be used for this kind of thing.

534
00:36:09,400 --> 00:36:12,480
And so I know there's like some cases of people suing.

535
00:36:12,480 --> 00:36:16,040
I don't know if it's specifically IBM, but also other academic institutions.

536
00:36:16,040 --> 00:36:20,600
I mean, this is the kind of stuff everyone in the community has down, right?

537
00:36:20,600 --> 00:36:23,680
And so that's also other stuff that's happening.

538
00:36:23,680 --> 00:36:27,960
And then, you know, at Google, there's like facets and other kind of tools to visualize

539
00:36:27,960 --> 00:36:28,960
your facets.

540
00:36:28,960 --> 00:36:29,960
Fascists.

541
00:36:29,960 --> 00:36:30,960
You should look it up.

542
00:36:30,960 --> 00:36:31,960
It's great.

543
00:36:31,960 --> 00:36:32,960
It's a toolkit that came from pair.

544
00:36:32,960 --> 00:36:33,960
We actually use it.

545
00:36:33,960 --> 00:36:35,640
Joy and I used it for gender shades.

546
00:36:35,640 --> 00:36:41,040
I mean, by Joy and I, I mean Joy and Deb, like they worked on and the website and it's

547
00:36:41,040 --> 00:36:46,840
a visualization tool helps you visualize your data sets and all sorts of other things.

548
00:36:46,840 --> 00:36:52,560
So it's through like a lot of companies and are coming out with these kinds of toolkits

549
00:36:52,560 --> 00:36:55,280
to help you analyze things.

550
00:36:55,280 --> 00:36:59,240
And then I don't remember, but I do believe there was a whole bunch of people working

551
00:36:59,240 --> 00:37:04,200
on like Python libraries, you know, for some of these things.

552
00:37:04,200 --> 00:37:09,800
But another side of the story is that some people, again, are cautioning against, you

553
00:37:09,800 --> 00:37:16,120
know, having these toolkits is just should be seen as a way to explore what's happening

554
00:37:16,120 --> 00:37:17,720
in your model and data, right?

555
00:37:17,720 --> 00:37:21,240
It shouldn't be seen as a check mark, right?

556
00:37:21,240 --> 00:37:26,560
And so one thing that has happened with the proliferation of a lot of fairness related

557
00:37:26,560 --> 00:37:32,960
to works, I believe, is this were, you know, debiasing X or fixing X or, you know, and

558
00:37:32,960 --> 00:37:36,840
so that gives the illusion that you do X, Y and Z and then it's fixed, right?

559
00:37:36,840 --> 00:37:41,000
And so inherently, this is a very complex problem.

560
00:37:41,000 --> 00:37:44,720
It's context dependent, you know, it's domain knowledge dependent.

561
00:37:44,720 --> 00:37:51,680
And so I think like sometimes I worry about how to get that across even when all of these

562
00:37:51,680 --> 00:37:55,560
toolkits and documentations and things are around, right?

563
00:37:55,560 --> 00:37:56,560
It shouldn't be seen.

564
00:37:56,560 --> 00:38:01,800
It's just inherent tension between kind of democratization and raising the level of abstraction

565
00:38:01,800 --> 00:38:08,280
and providing tools and, you know, what we call leaky abstractions, like the fundamental

566
00:38:08,280 --> 00:38:11,000
complexity of the thing that you're trying to make more accessible.

567
00:38:11,000 --> 00:38:12,000
Yeah.

568
00:38:12,000 --> 00:38:17,160
And I would even say like, you know, I and Howard and many other people have done work

569
00:38:17,160 --> 00:38:19,320
on like automation bias, right?

570
00:38:19,320 --> 00:38:26,240
Like people trusting automated tools more and like more than more than, you know, people

571
00:38:26,240 --> 00:38:27,240
are sometimes, right?

572
00:38:27,240 --> 00:38:29,160
And like people do work in trust.

573
00:38:29,160 --> 00:38:35,160
So, so if I give you a bunch of tools and you do a bunch of tests, like it shouldn't

574
00:38:35,160 --> 00:38:37,760
be like, okay, I did test Y and Z and it's passed, right?

575
00:38:37,760 --> 00:38:42,760
There's also like a framework that needs to be developed around how to use those tools

576
00:38:42,760 --> 00:38:47,160
and at which point in the product development process they're appropriate, right?

577
00:38:47,160 --> 00:38:51,520
It just shouldn't be seen as like this is like a smog check, X, Y and Z done past, you

578
00:38:51,520 --> 00:38:52,520
know?

579
00:38:52,520 --> 00:38:53,520
Right, right, right.

580
00:38:53,520 --> 00:38:54,520
Yeah.

581
00:38:54,520 --> 00:38:58,360
I'm curious what you, you know, if you had to kind of make some predictions about the

582
00:38:58,360 --> 00:39:05,440
field kind of shifting our attention forward, what do you think we accomplished in 2020?

583
00:39:05,440 --> 00:39:08,800
Or, you know, since we're talking about round numbers the next decade for that matter.

584
00:39:08,800 --> 00:39:09,800
Oh, wow.

585
00:39:09,800 --> 00:39:12,280
Oh, oh, I definitely can't do about the next decade.

586
00:39:12,280 --> 00:39:19,680
I do, I do think we're probably moving towards a lot of discussions around standards such

587
00:39:19,680 --> 00:39:23,560
as like the ones that, you know, model car status sheets and stuff.

588
00:39:23,560 --> 00:39:30,200
And like how do those fit into, you know, governance of these kinds of like we've said like many

589
00:39:30,200 --> 00:39:34,120
organizations have AI principles and things but like how are they enforced, right?

590
00:39:34,120 --> 00:39:39,720
So, and also, but then again, like how do we ease the burden on people who don't have

591
00:39:39,720 --> 00:39:41,440
such resources?

592
00:39:41,440 --> 00:39:47,280
So I wrote a paper recently with Ansoon who's a history student and so we were talking

593
00:39:47,280 --> 00:39:53,360
about like lessons that we can learn from from archival history and their data collection

594
00:39:53,360 --> 00:39:58,160
processes and they have data consortiums where they pull resources together and if one

595
00:39:58,160 --> 00:40:03,960
library doesn't have this kind of collection, another library can use it can, you know,

596
00:40:03,960 --> 00:40:08,200
they can use another library's collection.

597
00:40:08,200 --> 00:40:14,040
And so with this increased, you know, burden of documentation which in the end we believe

598
00:40:14,040 --> 00:40:22,240
is necessary and checks and, you know, like GDPR and all this stuff, like how do smaller

599
00:40:22,240 --> 00:40:26,360
institutions and nonprofits and those without much resources?

600
00:40:26,360 --> 00:40:29,400
How can they also, you know, not be left behind, right?

601
00:40:29,400 --> 00:40:32,280
Because I believe that's also a fairness question, right?

602
00:40:32,280 --> 00:40:37,000
If you, if you are doing things such that more, you know, larger institutions than the ones

603
00:40:37,000 --> 00:40:41,440
that benefit, that's, that's a fairness question because that constraints who gets access

604
00:40:41,440 --> 00:40:43,440
to what resources?

605
00:40:43,440 --> 00:40:49,520
And so I think I believe there were like conversations at the governance level around data consortium,

606
00:40:49,520 --> 00:40:51,560
conversations around standards.

607
00:40:51,560 --> 00:40:57,480
So I, I'm wondering if maybe within the next year we might start to see some organize it,

608
00:40:57,480 --> 00:41:03,800
like maybe the EU or maybe, maybe some other organizations starting to think more about

609
00:41:03,800 --> 00:41:09,760
governance and whether some of these things that we've worked on as researchers, like,

610
00:41:09,760 --> 00:41:14,440
you know, data sheets and model cars and things like that will be part of that governance

611
00:41:14,440 --> 00:41:15,440
structure.

612
00:41:15,440 --> 00:41:22,080
You know, for example, when, when I, there's, oh, I forgot a big thing in the last year,

613
00:41:22,080 --> 00:41:27,760
you know, there's been all sorts of legislature pass about around face recognition, right?

614
00:41:27,760 --> 00:41:33,040
And that for me, it's pretty fast from like writing, gender shades to, like, this kind

615
00:41:33,040 --> 00:41:38,480
of, and, and of course, many people like the, at the center for a security, what is it?

616
00:41:38,480 --> 00:41:43,280
Privacy of security, a Georgetown law, you know, Laura Moig, Claire Garvey, Alverbertoia,

617
00:41:43,280 --> 00:41:50,280
all these people, they, they do the most amazing work on like tracking the use of face recognition

618
00:41:50,280 --> 00:41:55,640
or really automated facial analysis in the US by law enforcement, whether it's ICE, whether

619
00:41:55,640 --> 00:41:59,840
it's, you know, other types of law enforcement, and they had the perpetual lineup report

620
00:41:59,840 --> 00:42:02,680
with the first one, America Underwatch was the second one.

621
00:42:02,680 --> 00:42:07,280
So in partnership, you know, in the conjunction with like a lot of people's works, I feel

622
00:42:07,280 --> 00:42:12,680
like that has resulted in some amount of change in legislation within the last year,

623
00:42:12,680 --> 00:42:13,680
right?

624
00:42:13,680 --> 00:42:16,480
And I, I think we're going to start to see more of that.

625
00:42:16,480 --> 00:42:22,240
Another thing I've seen actually recently is that so much more, so many more people in

626
00:42:22,240 --> 00:42:28,120
civil, in civil society are part of this conversation now like the ACLU and many of, many other people

627
00:42:28,120 --> 00:42:33,200
are part of this conversation about whether some sort of technology should exist or not

628
00:42:33,200 --> 00:42:35,920
and like AI governance and things like that.

629
00:42:35,920 --> 00:42:38,280
And I think that'll continue to grow.

630
00:42:38,280 --> 00:42:42,040
One thing I'm really worried about and I, I am predicting this will happen because

631
00:42:42,040 --> 00:42:47,960
it started to happen already is once again, the taking over of marginalized voices, right?

632
00:42:47,960 --> 00:42:48,960
In this space.

633
00:42:48,960 --> 00:42:53,160
So like I said, like people work really hard to make something a thing and then once

634
00:42:53,160 --> 00:42:58,720
it's a thing, the people who didn't suffer the consequences of trying to make it a thing

635
00:42:58,720 --> 00:43:03,400
kind of become the faces and the heads and the, you know, and the people who steer the

636
00:43:03,400 --> 00:43:04,400
ship.

637
00:43:04,400 --> 00:43:09,280
And at that point, it starts going in the wrong direction because it's not really trying

638
00:43:09,280 --> 00:43:13,160
to address the issues of the people who took the risks to make it a thing because they

639
00:43:13,160 --> 00:43:16,800
took that risk because what they really care about is addressing the issues themselves,

640
00:43:16,800 --> 00:43:17,800
right?

641
00:43:17,800 --> 00:43:20,840
And so that's one thing I think will happen increasingly.

642
00:43:20,840 --> 00:43:23,120
And you said you're seeing that already, is there?

643
00:43:23,120 --> 00:43:24,120
Oh yeah, absolutely.

644
00:43:24,120 --> 00:43:29,680
I mean, it's hard to give specific, you know, but no, I, I, I will say like a lot of institutions

645
00:43:29,680 --> 00:43:34,560
like, you know, for example, MIT, Stanford, Stanford, actually Stanford HAI was started

646
00:43:34,560 --> 00:43:42,160
and went partly by my advisor right by PHA and the MIT, it's, there was a, a big announcement

647
00:43:42,160 --> 00:43:46,840
saying like, there's going to be a school of computing and there's going to be a focus

648
00:43:46,840 --> 00:43:49,040
on ethics and things like that, right?

649
00:43:49,040 --> 00:43:53,720
But like, literally, if you can't even have like two black professors, like, and you know,

650
00:43:53,720 --> 00:43:55,160
what I'm, that's what I'm talking about.

651
00:43:55,160 --> 00:43:58,640
So I, I think that kind of, that's going to be very hard to tackle.

652
00:43:58,640 --> 00:44:01,160
It's a surprising absence on the part of ATI.

653
00:44:01,160 --> 00:44:06,320
Oh yeah, so, so H, I was actually also talking about MIT, but, but true, exit, exit.

654
00:44:06,320 --> 00:44:08,240
So with HAI, it's true, right?

655
00:44:08,240 --> 00:44:11,720
And I, I believe they're like, I, I did talk to my advisor about it too.

656
00:44:11,720 --> 00:44:15,960
And like, I think now they're starting to be a little bit more cognizant about it.

657
00:44:15,960 --> 00:44:19,600
But I mean, as an institution, you're going to have money from like different groups of

658
00:44:19,600 --> 00:44:23,720
people and those groups of people are going to be, if you have the name and if you're not

659
00:44:23,720 --> 00:44:27,200
really from that much of a marginalized community, you're going to be more likely to, like,

660
00:44:27,200 --> 00:44:31,360
raise this money and stuff like that, right? Whereas like a lot of other groups of people

661
00:44:31,360 --> 00:44:35,880
that are from marginalized communities and smaller institutions are going to struggle

662
00:44:35,880 --> 00:44:39,680
to, to get credibility and, and, and raise money.

663
00:44:39,680 --> 00:44:44,240
But like, yeah, what I was seeing was, you know, for example, at MIT, like, they're

664
00:44:44,240 --> 00:44:49,240
now seeing this huge endowment and this, you know, ethics and all of, all of this stuff.

665
00:44:49,240 --> 00:44:53,800
But when I look at their faculty, just in general, right, like in computer science or an

666
00:44:53,800 --> 00:44:58,240
engineering, you know, like, they, they, I, I, they might have one black person.

667
00:44:58,240 --> 00:44:59,640
I don't know, but you know what I mean?

668
00:44:59,640 --> 00:45:05,240
So it's, it's really hard to, to think, I mean, at Stanford, I, I believe, I used to

669
00:45:05,240 --> 00:45:11,760
think that there were zero black, they graduated zero black people with a computer science,

670
00:45:11,760 --> 00:45:16,360
um, PhD, they've never graduated black people with a computer science, PhD.

671
00:45:16,360 --> 00:45:22,760
I believe there's, there was maybe one person ever in San Francisco, that's graduated with

672
00:45:22,760 --> 00:45:25,000
a PhD in computer science, right?

673
00:45:25,000 --> 00:45:32,360
And like, you know, and so it's just impossible to have, you know, ethics things without addressing

674
00:45:32,360 --> 00:45:34,360
these kinds of issues, right?

675
00:45:34,360 --> 00:45:38,520
Like if you don't interact, if you're not, you know, there is something wrong in that

676
00:45:38,520 --> 00:45:46,000
system that is not allowing these groups of people to thrive, right, um, and, and to even

677
00:45:46,000 --> 00:45:47,000
be present.

678
00:45:47,000 --> 00:45:52,960
So, so this distinction between quote unquote, ethics and like, um, in, in, in theory and

679
00:45:52,960 --> 00:45:59,360
then in practice and the real world, um, um, I've seen more of it, um, in the last year,

680
00:45:59,360 --> 00:46:03,400
but I've seen more of it being discussed in the last year, but I don't anticipate more

681
00:46:03,400 --> 00:46:05,240
of it happening like in the future.

682
00:46:05,240 --> 00:46:06,240
In the next year?

683
00:46:06,240 --> 00:46:07,240
Yeah.

684
00:46:07,240 --> 00:46:11,040
Yeah, there's, I don't know that I would necessarily say it's a shift in the conversation,

685
00:46:11,040 --> 00:46:16,960
but there's definitely something happening in the conversation.

686
00:46:16,960 --> 00:46:24,280
There's maybe a tension in the conversation about the extent to which AI ethics and fairness

687
00:46:24,280 --> 00:46:31,600
and these conversations should be grounded in self-interest, grounded in protecting the

688
00:46:31,600 --> 00:46:38,080
most, uh, you know, marginalized at risk, uh, you know, you know, who and why should this

689
00:46:38,080 --> 00:46:39,080
be about?

690
00:46:39,080 --> 00:46:41,080
Is, uh, is, are you seeing anything there?

691
00:46:41,080 --> 00:46:42,080
Yeah.

692
00:46:42,080 --> 00:46:45,880
And I'm seeing, I think, a bifurcation in the community and, uh, and that could happen in

693
00:46:45,880 --> 00:46:46,880
the future, right?

694
00:46:46,880 --> 00:46:51,120
So, there's the camp that says, and I'm, I'm probably in that camp that says, you know,

695
00:46:51,120 --> 00:46:55,360
I just don't like this separation between, oh, yeah, like there's this theory that we're

696
00:46:55,360 --> 00:46:59,000
doing that theoretical work, which is the math and the proofs and stuff.

697
00:46:59,000 --> 00:47:03,080
And then there's like all this activism that's happening, you know, and like the diversity

698
00:47:03,080 --> 00:47:07,880
and inclusion and labor organizing and, um, you know, that kind of stuff and like those

699
00:47:07,880 --> 00:47:09,080
should be separate, right?

700
00:47:09,080 --> 00:47:13,480
Like, and so if we are having a theory thing about fairness for, with 30 people and they're

701
00:47:13,480 --> 00:47:18,480
none of them are black, that's okay because we're doing the theory people and I would just

702
00:47:18,480 --> 00:47:20,080
completely disagree with that, right?

703
00:47:20,080 --> 00:47:24,880
And I would say that's expletative because then that's exploiting a particular community

704
00:47:24,880 --> 00:47:30,280
that you're talking about, that you're writing about in your papers, um, to, to kind of advance

705
00:47:30,280 --> 00:47:36,640
your career and this is not helping the community at all, right?

706
00:47:36,640 --> 00:47:39,960
And then there's a camp that says we have to be much more interdisciplinary.

707
00:47:39,960 --> 00:47:45,880
We have to have less boundaries between disciplines and it's like, you know, diversity and inclusion

708
00:47:45,880 --> 00:47:49,560
work and all of this labor organizing and stuff is just a part of this.

709
00:47:49,560 --> 00:47:55,600
I'm very much in that camp and many people in my, in our team are in that camp and, and

710
00:47:55,600 --> 00:48:00,200
I think that's why they gravitate to our team because our team is one of the few homes

711
00:48:00,200 --> 00:48:05,320
where people who believe that and so like, you know, and so going back to where I started

712
00:48:05,320 --> 00:48:10,040
because I came from Emily's talk, it's that idea of the view from nowhere.

713
00:48:10,040 --> 00:48:14,480
So if you say that, you know, it's okay, we're only doing fairness related, um, theory

714
00:48:14,480 --> 00:48:18,320
work and there's 30 white people here and that's fine.

715
00:48:18,320 --> 00:48:21,600
And that's adhering, that is the view from nowhere.

716
00:48:21,600 --> 00:48:26,280
That's assuming that the, that view of those 30 people is not affecting your three at,

717
00:48:26,280 --> 00:48:30,720
a theory that you think is going to help this particular, correctness of the view that

718
00:48:30,720 --> 00:48:35,520
these 30 people are seeking that is independent of the 30 people themselves.

719
00:48:35,520 --> 00:48:36,520
Yeah.

720
00:48:36,520 --> 00:48:37,520
So that's the assumption, right?

721
00:48:37,520 --> 00:48:40,960
This view from nowhere assumption, which feminists have critiqued for a long time.

722
00:48:40,960 --> 00:48:42,320
And so I'm in that camp.

723
00:48:42,320 --> 00:48:48,920
Well, Tim, thanks so much for taking some time to catch us up on kind of your view of

724
00:48:48,920 --> 00:48:50,920
the ethics and fairness landscape.

725
00:48:50,920 --> 00:48:57,160
Certainly we could not do justice to this conversation and, you know, just under an hour, um, but

726
00:48:57,160 --> 00:49:01,720
it's something that will continue to explore on the podcast and looking forward to our

727
00:49:01,720 --> 00:49:03,320
next conversation with you.

728
00:49:03,320 --> 00:49:04,520
Thank you for having me.

729
00:49:04,520 --> 00:49:05,520
Thank you.

730
00:49:05,520 --> 00:49:13,400
All right, everyone, that's our show for today for more information on today's guest

731
00:49:13,400 --> 00:49:21,200
or for links to any of the materials mentioned, check out twimmelai.com slash rewind 19.

732
00:49:21,200 --> 00:49:25,280
Be sure to leave us a five star rating and a glowing review after you hit that subscribe

733
00:49:25,280 --> 00:49:28,240
button on your favorite podcast catcher.

734
00:49:28,240 --> 00:49:58,200
Thanks so much for listening and catch you next time.

