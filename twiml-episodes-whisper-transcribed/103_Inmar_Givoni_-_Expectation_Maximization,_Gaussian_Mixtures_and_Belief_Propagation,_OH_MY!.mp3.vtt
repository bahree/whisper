WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.480
I'm your host, Sam Charrington.

00:23.480 --> 00:28.040
A quick thanks to everyone who participated in last week's Twimble Online Meetup.

00:28.040 --> 00:29.960
It was another great one.

00:29.960 --> 00:35.480
If you missed it, the recording will be posted to the Meetup page at twimbleai.com slash

00:35.480 --> 00:36.480
meetup.

00:36.480 --> 00:37.880
Definitely check it out.

00:37.880 --> 00:43.680
I never cease to be amazed by the generosity and creativity of the Twimble community.

00:43.680 --> 00:48.480
And I'd like to send a special shout out to listener Sharon Glander for her exceptional

00:48.480 --> 00:50.280
sketch notes.

00:50.280 --> 00:55.120
Sharon has been creating beautiful hand sketch notes of her favorite Twimble episodes

00:55.120 --> 00:57.160
and sharing them with the community.

00:57.160 --> 01:00.400
Sharon, we truly love and appreciate what you're doing with those.

01:00.400 --> 01:02.920
So please keep up the great work.

01:02.920 --> 01:06.000
We'll link to her sketch notes in the show notes for this episode.

01:06.000 --> 01:11.440
And you should definitely follow her on Twitter at sharingglander for more.

01:11.440 --> 01:15.840
This is your last chance to register for the rework, deep learning and AI assistant

01:15.840 --> 01:22.200
summits in San Francisco, which are this Thursday and Friday, January 25th and 26th.

01:22.200 --> 01:26.560
These events feature leading researchers and technologists like the ones you heard in

01:26.560 --> 01:29.680
our Deep Learning Summit series last week.

01:29.680 --> 01:34.640
The San Francisco event is headlined by Ian Goodfellow of Google Brain, Daphne Kohler

01:34.640 --> 01:37.360
of Calico Labs, and more.

01:37.360 --> 01:43.000
Definitely check it out and use the code Twimbleai for 20% off of registration.

01:43.000 --> 01:48.800
In this episode, I'm joined by Inmar Gavoni, Autonomy Engineering Manager at Uber ATG to

01:48.800 --> 01:53.800
discuss her work on the paper MinMax Propagation, which was presented at Nip's last month

01:53.800 --> 01:55.280
in Long Beach.

01:55.280 --> 01:59.680
And more and I get into a really media discussion about graphical models, including what they

01:59.680 --> 02:04.800
are, how they're used, some of the challenges they present for both training and inference,

02:04.800 --> 02:07.520
and how and where they can be best applied.

02:07.520 --> 02:12.080
Then we jump into an in-depth look at the key ideas behind the MinMax Propagation paper

02:12.080 --> 02:16.600
itself, including the relationship to the broader domain of belief propagation and ideas

02:16.600 --> 02:21.400
like affinity propagation, and how all these can be applied to a use case example like

02:21.400 --> 02:23.240
the Makesband problem.

02:23.240 --> 02:27.960
This was a really fun conversation, and now on to the show.

02:27.960 --> 02:37.800
All right, everyone, I am on the line with Inmar Gavoni.

02:37.800 --> 02:44.080
Inmar is the Autonomy Engineering Manager at Uber ATG, that's Uber's Advanced Technology

02:44.080 --> 02:46.400
Group in Toronto, Canada.

02:46.400 --> 02:48.960
Inmar, welcome to this week in Machine Learning and AI.

02:48.960 --> 02:49.960
Thank you, Sam.

02:49.960 --> 02:50.960
It's great to be here.

02:50.960 --> 02:53.040
It's awesome to have you on the show.

02:53.040 --> 02:57.600
You've been trying to get this coordinated for quite a while, so I'm glad we're able

02:57.600 --> 03:00.400
to make it happen early in the new year.

03:00.400 --> 03:01.400
Absolutely.

03:01.400 --> 03:03.000
I'm excited about this.

03:03.000 --> 03:07.480
As is our tradition here, why don't we get started by having you tell the audience a

03:07.480 --> 03:11.280
little bit about your background and how you got interested in machine learning?

03:11.280 --> 03:12.280
Sure.

03:12.280 --> 03:16.960
So, for me, it actually started pretty early on, in the sense that back in high school,

03:16.960 --> 03:22.320
I thought I wanted to be a neuroscientist, and I was pretty sure that understanding and

03:22.320 --> 03:28.360
researching the brain is the most interesting thing that I could apply myself to.

03:28.360 --> 03:35.160
So going into university, I chose the areas of computer science and biology, because

03:35.160 --> 03:39.680
I thought of brains as machines and thinking that the computer science and engineering

03:39.680 --> 03:43.760
approaches would be useful for understanding them and biology because it is, after all,

03:43.760 --> 03:46.040
a biological substance.

03:46.040 --> 03:50.960
And I spend a lot of my time taking neuroscience courses and talking to your neuroscientists

03:50.960 --> 03:54.680
and trying to understand how they approach solving the problem.

03:54.680 --> 04:00.320
At the same time, towards the end of my undergraduate degrees of the last year, I also took a machine

04:00.320 --> 04:02.040
learning course.

04:02.040 --> 04:05.840
And first of all, the machine learning course was really interesting.

04:05.840 --> 04:12.440
It was such a different way of thinking about how to solve problems that really appeal

04:12.440 --> 04:13.440
to me.

04:13.440 --> 04:15.840
The mathematics of it were beautiful.

04:15.840 --> 04:22.120
It combines a lot of things that I previously learned, and they never really clicked into

04:22.120 --> 04:23.120
one place.

04:23.120 --> 04:29.600
So anything from linear algebra, calculus, probability theory, statistics, graph theory,

04:29.600 --> 04:34.560
computer torques, all of it is used in one way or the other in machine learning.

04:34.560 --> 04:39.640
And also, it allows us to solve problems that you can't really solve with traditional

04:39.640 --> 04:42.240
programming or engineering approaches.

04:42.240 --> 04:46.760
So it's a whole new mathematical way of looking at these problems and then coming up with

04:46.760 --> 04:48.760
really beautiful solutions.

04:48.760 --> 04:53.840
And as for the neuroscience, I felt like maybe instead of trying to understand the brain,

04:53.840 --> 04:59.440
a different way of doing it would be to try and build software that exhibits intelligent

04:59.440 --> 05:00.600
behavior.

05:00.600 --> 05:04.760
So that drove my decision to do a PhD in machine learning.

05:04.760 --> 05:10.520
And so I moved to Toronto, which is one of the best places in the world to do machine learning

05:10.520 --> 05:14.000
research and did that for a few years.

05:14.000 --> 05:20.640
And while I was in school, I did a few internships and I also realized that I really like working

05:20.640 --> 05:25.680
towards a specific product and towards something that is tangible and is out there in the world

05:25.680 --> 05:29.520
and people use and is also up for that type of scrutiny.

05:29.520 --> 05:36.840
So after graduation, I worked in various companies on applications of machine learning to

05:36.840 --> 05:41.960
real world products and I like physical things, so in all of them, there was some sort of

05:41.960 --> 05:45.960
a physical product that you can actually touch.

05:45.960 --> 05:51.640
And of course, self-driving cars is one of the most exciting new technologies.

05:51.640 --> 05:55.720
I mean, maybe the technology itself is not that new or the idea of doing it is not

05:55.720 --> 05:56.720
that new.

05:56.720 --> 06:02.320
It's been around for about a decade at least, but it's now really coming into full attention

06:02.320 --> 06:07.920
from everyone in the world and I thought joining this office would be an incredible opportunity.

06:07.920 --> 06:10.080
So I've been here for a few months now.

06:10.080 --> 06:11.080
Oh, wow.

06:11.080 --> 06:12.880
And what specifically do you do there?

06:12.880 --> 06:17.280
So the office in Toronto is a research and development office.

06:17.280 --> 06:24.280
It's led by Raquel Ortison who is one of the world experts in the intersection of self-driving,

06:24.280 --> 06:26.720
deep learning and computer vision.

06:26.720 --> 06:33.680
And there is a research group of researchers who are working on creating new innovative

06:33.680 --> 06:38.200
cutting edge algorithms for using deep learning for self-driving.

06:38.200 --> 06:46.160
My role is helping take this product, this research prototypes and first phase algorithms

06:46.160 --> 06:48.720
and actually get them into the car.

06:48.720 --> 06:50.720
So get them into production.

06:50.720 --> 06:51.720
Oh, wow.

06:51.720 --> 06:58.240
I manage a team of applied researchers and software engineers who help with everything

06:58.240 --> 07:03.240
that has to happen in order to take a prototype, something that resembles what you would get

07:03.240 --> 07:09.400
out of a publication and actually make it into a production feature.

07:09.400 --> 07:10.400
Hmm.

07:10.400 --> 07:17.360
And we've talked about, we did a series on autonomous driving last year, I think in the fall

07:17.360 --> 07:23.720
towards the end of last year, and one of the things that jumped out of that was the different

07:23.720 --> 07:30.720
approaches and philosophies to doing machine learning for autonomous driving, kind of,

07:30.720 --> 07:37.200
you know, N10 versus integrating different systems, you know, versus, you know, and camera,

07:37.200 --> 07:41.600
camera first versus, you know, sensor fusion approaches.

07:41.600 --> 07:47.280
Does Uber have a kind of a, what's Uber's approach to?

07:47.280 --> 07:51.240
Autonomous vehicles and in that domain.

07:51.240 --> 07:55.960
So we're definitely as a place that specializes in deep learning.

07:55.960 --> 08:02.200
We believe that deep learning approaches for the various aspects of technologies you

08:02.200 --> 08:08.600
need to introduce, you know, around perception, understanding where the car is understanding

08:08.600 --> 08:16.640
who the actors are, can all benefit from deep learning in terms of what kind of architecture

08:16.640 --> 08:20.200
specifically or, you know, the sensors and so on.

08:20.200 --> 08:26.360
I think this is still something that is an ongoing research and understanding what are the

08:26.360 --> 08:32.240
benefits of the different sensors and when is there a time to use them and in what configuration?

08:32.240 --> 08:35.040
So I would say that's still an open question.

08:35.040 --> 08:42.720
And so one of the things that we wanted to dig into in this conversation was a paper that

08:42.720 --> 08:53.720
you co-author that was released that nips or was in the nips proceedings on minmax propagation.

08:53.720 --> 09:00.400
One of the things that I observed that nips was a lot of conversations around graphical

09:00.400 --> 09:04.360
models and graphical approaches in general for machine learning.

09:04.360 --> 09:10.160
And as maybe a segue into that conversation, I wanted to get a sense from you, is that,

09:10.160 --> 09:16.720
would you say that there was a kind of heightened interest in graphical models this year or

09:16.720 --> 09:24.480
is it a conversation that has been, you know, continuing on, you know, without, I guess

09:24.480 --> 09:30.000
is there a heightened level of interest in this particular type of modeling approach?

09:30.000 --> 09:31.000
Right.

09:31.000 --> 09:33.240
So here is my take on it.

09:33.240 --> 09:41.440
Visual models are very powerful mathematical tools to represent our understanding of the

09:41.440 --> 09:46.120
world and our understanding of what we don't understand and the fact that there is uncertainty

09:46.120 --> 09:47.120
and noise.

09:47.120 --> 09:49.880
So as modeling tools, they're very powerful.

09:49.880 --> 09:57.760
The problem has been that they're very difficult to train and to do learning and inferencing.

09:57.760 --> 10:07.720
So they are computationally expensive and they do not scale up to full blown industrial

10:07.720 --> 10:08.720
purposes.

10:08.720 --> 10:14.120
So it was a, there has always been, not always, but you know, there's been an ongoing

10:14.120 --> 10:17.560
flourishing and research in graphical models for a long time.

10:17.560 --> 10:19.920
I entered the field doing research.

10:19.920 --> 10:26.000
I would say around 2006 and these were very popular and like a big area of focus and

10:26.000 --> 10:31.680
in fact my thesis focuses on application of graphical models to clustering.

10:31.680 --> 10:32.680
Okay.

10:32.680 --> 10:38.960
But it remained within, I would say primarily within academia with the exception of specific

10:38.960 --> 10:44.760
models that were simpler to use than others and were used in industry.

10:44.760 --> 10:49.000
And then we had, you know, over the last few years, obviously, we had this intense focus

10:49.000 --> 10:55.280
on deep learning and neural networks and a lot of people worked on them.

10:55.280 --> 11:01.600
And people shifted from working sound graphical models to focusing on innovation in deep learning.

11:01.600 --> 11:07.240
And I think now that we are kind of getting to a place where a lot of the groundwork has

11:07.240 --> 11:12.920
been done and a lot of the, let's call it the low hanging fruits have been picked.

11:12.920 --> 11:17.560
Now there is going interest in looking again at graphical models and then asking, given

11:17.560 --> 11:24.960
all we've learned about how to do deep learning at scale and how to use it and how to be

11:24.960 --> 11:32.920
able to solve difficult problems with it, can we now merge better the two strategies and

11:32.920 --> 11:39.760
can we create new types of models that leverage the leverage both so that maybe graphical models

11:39.760 --> 11:45.160
can become as useful as deep learning models.

11:45.160 --> 11:51.200
And what are the types of applications that lend themselves most readily to use of graphical

11:51.200 --> 11:52.200
models?

11:52.200 --> 12:00.520
There are many different applications, one that is commonly I would say looked at is in,

12:00.520 --> 12:01.920
let's say, healthcare.

12:01.920 --> 12:09.640
So understanding or predicting whether a particular patient has that particular type of cancer,

12:09.640 --> 12:15.280
you can describe all that you know about the different measurements you've taken and

12:15.280 --> 12:22.280
so on using a graphical model and then you can try to infer the actual underlying state

12:22.280 --> 12:27.240
of their illness if they have on the type, the stage and so on.

12:27.240 --> 12:35.880
And the main idea as I understand it behind the application of graphical models is as

12:35.880 --> 12:42.800
opposed to traditional, I guess traditional, we'll start with calling it traditional,

12:42.800 --> 12:48.640
with a typical data set, let's say where you've got data points and the data set doesn't

12:48.640 --> 12:55.280
really express any inherent relationship between the data points as opposed to a relationship

12:55.280 --> 13:02.640
between you know features within those data points within, well graphical models are

13:02.640 --> 13:10.760
really trying to do is identify relationships between the data points and the data set

13:10.760 --> 13:13.480
is that an accurate assessment?

13:13.480 --> 13:20.360
Yes, yes, they deal with representing all sorts of quantities in the world as variables

13:20.360 --> 13:27.080
and either explicitly saying that there are known complex relationships between these

13:27.080 --> 13:32.920
variables or trying to learn the complex relationships between these variables.

13:32.920 --> 13:40.160
And when you say explicitly saying that there are known complex relationships is, are

13:40.160 --> 13:45.320
you there describing kind of pulling in prior knowledge about the relationships between

13:45.320 --> 13:48.520
these variables into the models or?

13:48.520 --> 13:54.840
Yeah, that would be one way of doing it where you represent all sorts of prior knowledge

13:54.840 --> 14:01.720
given with distributions that you believe are well suited to represent your prior knowledge

14:01.720 --> 14:07.400
and also representing the actual relationships between the variables.

14:07.400 --> 14:13.080
So if we're trying to think of a relatively simple example, let's say that I'm trying

14:13.080 --> 14:16.120
to represent an image in terms of a graph.

14:16.120 --> 14:23.440
So maybe every pixel in the image will be a node in the graph and every pixel will be connected

14:23.440 --> 14:28.000
to the pixel that are around it and these connections, these edges on the graph represent

14:28.000 --> 14:33.640
the fact that we think that the value of the pixel is highly correlated to the value of

14:33.640 --> 14:37.840
the pixel or is around it, right? So for example, if we know about the pixels around it,

14:37.840 --> 14:43.120
we can make guesses as to the value of the pixel or we can represent the possible values

14:43.120 --> 14:47.240
of it as a distribution that is not uniform, it learned, it knows something from it's

14:47.240 --> 14:48.240
surrounding.

14:48.240 --> 14:53.400
So that would be one way of representing relationships between variables.

14:53.400 --> 14:59.240
And our images is that a common application area for graphical models?

14:59.240 --> 15:07.280
So images used to be before we were able to successfully work with neural networks.

15:07.280 --> 15:11.320
But in terms of performance right now, I would say that convolutional neural networks

15:11.320 --> 15:19.080
are much better at tasks around predictions and prediction and detection and various tasks

15:19.080 --> 15:21.000
that have to do with images.

15:21.000 --> 15:30.360
So what have been some of the biggest achievements to date of this renewed interest in graphical

15:30.360 --> 15:38.080
models with the background of the progress and deep learning over the recent years?

15:38.080 --> 15:48.600
I think one interesting idea is to allow for neural networks to operate over graphs.

15:48.600 --> 15:57.600
So there is a new researcher in graph neural networks and graph convolutional neural networks.

15:57.600 --> 16:08.160
And other ideas are that within the network queue represents some of the new model, some

16:08.160 --> 16:15.120
of the network explicitly using some formulation of a graphical model.

16:15.120 --> 16:19.880
And so what does it mean to have the network operate over a graph?

16:19.880 --> 16:23.680
So again, going back to the example of images.

16:23.680 --> 16:26.440
If you think about it as a graph, it's a grid, right?

16:26.440 --> 16:33.680
It's a simple graph and so we can very easily do operations on it like convolutional operations

16:33.680 --> 16:39.280
because we just shift them, shift the filter over and it's the same operation.

16:39.280 --> 16:43.840
But you can also obstruct this notion of a convolution to something where it's not

16:43.840 --> 16:47.160
a nice grid, but it's still a graph.

16:47.160 --> 16:54.080
So but that requires the mathematics to be developed for it and to be made efficient.

16:54.080 --> 17:02.080
So then so is the idea then to kind of replicate what a convolutional network is doing, but

17:02.080 --> 17:04.800
with graphical models?

17:04.800 --> 17:05.800
Not necessarily.

17:05.800 --> 17:13.320
It's just to use the, I guess the body of knowledge and work from graphical models and maybe

17:13.320 --> 17:17.440
we can get into inference a little bit in a moment.

17:17.440 --> 17:23.960
So for doing inference in graphical models, that is not really captured within neural networks.

17:23.960 --> 17:30.480
So so neural networks, people talk about doing learning and inference in neural networks,

17:30.480 --> 17:37.320
but really I think that it's come to mean that learning is when you train the algorithm.

17:37.320 --> 17:38.320
Right.

17:38.320 --> 17:42.360
And inference is when at test time you basically run forward propagations, your network

17:42.360 --> 17:45.480
can come up with a prediction.

17:45.480 --> 17:51.120
You know, it's it's a correct use of the term, but it's in the context of neural networks

17:51.120 --> 17:57.320
that the inference part is easy because it's just a straightforward calculation.

17:57.320 --> 18:01.480
Sometimes computing this prediction, so computing in the case of neural networks, like what

18:01.480 --> 18:04.960
is the probability that there is a particular object in the image, let's say, right?

18:04.960 --> 18:10.080
And then it speeds out a probability of whether it's there or not in this particular place,

18:10.080 --> 18:11.080
let's say.

18:11.080 --> 18:16.640
So in some cases, let's say when when the underlying model is a graphical model, computing this

18:16.640 --> 18:21.920
probability is in and on itself a difficult sub problem.

18:21.920 --> 18:29.080
And then there is a large body of work on computing, running this algorithm.

18:29.080 --> 18:35.520
And in some cases, in particular, in graphical models, the within training itself, so not

18:35.520 --> 18:40.640
just when you're a test time, you're interliving the learning and inference part.

18:40.640 --> 18:47.440
So the learning is often referred to coming up with estimates over the hidden variables.

18:47.440 --> 18:50.920
And we didn't really talk about what hidden variables are, but sorry, coming with estimates

18:50.920 --> 18:56.720
of the parameters governing the behavior of the hidden variables, the parameters in

18:56.720 --> 18:59.800
a neural network would be the weights.

18:59.800 --> 19:03.400
And so we're learning, we're basically optimizing the weights.

19:03.400 --> 19:08.320
And in graphical models, the parameters can be, for example, parameters that govern

19:08.320 --> 19:09.320
distribution.

19:09.320 --> 19:13.920
Like, I assume that there is a Gaussian and I don't know what is the mean and the variance

19:13.920 --> 19:14.920
of the Gaussian.

19:14.920 --> 19:17.760
So that would be the parameters that I'm trying to learn.

19:17.760 --> 19:22.640
But then there is also an inference part, which is computing all sorts of typically predictions

19:22.640 --> 19:29.200
or probability distributions on the variables of interest and that requires running some

19:29.200 --> 19:32.480
sort of a sometimes complex computation.

19:32.480 --> 19:36.280
And when I'm done with that, I sometimes iterate back to the learning part.

19:36.280 --> 19:40.800
So given that I've refined my estimate of the probabilities, now I refine my estimate

19:40.800 --> 19:46.160
of the learning part of the parameters and go back and forth between the two.

19:46.160 --> 19:52.600
So what I've just described is a very high level and algorithm called the expectation

19:52.600 --> 19:58.240
maximization algorithm or EM algorithm, which is used for doing learning and inference in

19:58.240 --> 20:03.120
graphical models like a Gaussian mixture model, for instance.

20:03.120 --> 20:06.800
And if you want, we can talk a little bit about what is a Gaussian mixture model.

20:06.800 --> 20:12.240
Yeah, before we do that, so the expectation maximization that you were just just describing,

20:12.240 --> 20:15.120
this is trying to contextualize this.

20:15.120 --> 20:20.320
We were talking about training and inference and you started to talk about inference.

20:20.320 --> 20:26.720
But this is, this is used as part of training, but it includes inference calculations.

20:26.720 --> 20:27.720
Is that the?

20:27.720 --> 20:34.560
Yeah, so again, in non neural network world, where you're talking about the training procedure

20:34.560 --> 20:39.120
itself may include both learning and inference.

20:39.120 --> 20:40.120
Got it.

20:40.120 --> 20:46.720
And so you need to maybe the Gaussian mixture model is a good example to run through for

20:46.720 --> 20:47.720
that.

20:47.720 --> 20:52.240
So in Gaussian mixture models, we basically have a whole bunch of data.

20:52.240 --> 20:54.720
It's a way of clustering the data, let's say.

20:54.720 --> 21:00.480
So in clustering, we assume that the data can naturally be divided into groups, right?

21:00.480 --> 21:01.480
Or clusters.

21:01.480 --> 21:05.920
And what makes a good cluster is that the points in the cluster are quite similar to each

21:05.920 --> 21:06.920
other.

21:06.920 --> 21:10.200
And they're quite different from other points in other clusters.

21:10.200 --> 21:14.680
But now that I'm given a whole bunch of data, how do I go about actually finding these

21:14.680 --> 21:16.280
different clusters?

21:16.280 --> 21:21.320
And in a Gaussian mixture model, I basically make an assumption that there was some process

21:21.320 --> 21:23.200
that generated the data.

21:23.200 --> 21:27.040
Which is, let's say that the data happens to have five clusters.

21:27.040 --> 21:32.320
So so someone, you know, flipped the coin or all the dice and sampled the centers of

21:32.320 --> 21:33.320
the clusters.

21:33.320 --> 21:34.320
Right.

21:34.320 --> 21:35.320
Right.

21:35.320 --> 21:40.160
And then from each center, a bunch of data points were sampled, but there are some noise

21:40.160 --> 21:42.360
involved, right?

21:42.360 --> 21:48.000
And so now if all I get to see is the end result is I get to see a lot of points and I need

21:48.000 --> 21:49.400
to infer.

21:49.400 --> 21:54.040
I need to learn the parameters, so I need to learn the means of the clusters, the centers

21:54.040 --> 21:59.000
of the clusters and their variances, how noisy the cluster are.

21:59.000 --> 22:06.600
And then I need to infer the probability that each point came from anyone particular cluster.

22:06.600 --> 22:10.680
So that would be a distribution over the clusters.

22:10.680 --> 22:18.320
You're typically, are you also inferring the number of latent variables or processes that

22:18.320 --> 22:23.760
are producing your data, or is that always an assumption that you make in your modeling?

22:23.760 --> 22:25.720
Oh, so that's a good question.

22:25.720 --> 22:31.520
So the traditional algorithm assumes that this is actually given as an input somehow,

22:31.520 --> 22:33.760
I figured out that there are five clusters.

22:33.760 --> 22:34.760
Right.

22:34.760 --> 22:39.960
But of course, that's not really a good assumption because I typically don't know.

22:39.960 --> 22:40.960
Right.

22:40.960 --> 22:41.960
Right.

22:41.960 --> 22:45.960
And so the next obvious step is to, you know, run it with a whole bunch of possible number

22:45.960 --> 22:51.840
of clusters and figure out how to measure which one is best, but that's not naturally,

22:51.840 --> 22:54.920
that's not necessarily easy to do.

22:54.920 --> 23:00.360
And then there is a very interesting literature on what's called non-parametric approaches

23:00.360 --> 23:05.920
or non-parametric Bayesian approaches, which basically say we don't want to make the

23:05.920 --> 23:09.680
assumption that we know the number of clusters ahead of time.

23:09.680 --> 23:14.040
And so we're going to also inject that and because it's Bayesian, they're not actually

23:14.040 --> 23:15.760
inferring the number of clusters.

23:15.760 --> 23:18.560
They are integrating over the number of clusters.

23:18.560 --> 23:25.080
So it gets to fairly complicated math, but there's definitely the attempt to try to accommodate

23:25.080 --> 23:26.080
for that.

23:26.080 --> 23:29.800
And going back to what I was saying before, that's an example of some really interesting

23:29.800 --> 23:38.960
and fairly, you know, beautiful mathematics round that area, but it's not yet practical

23:38.960 --> 23:41.880
to run these algorithms at scale.

23:41.880 --> 23:49.800
Is that when you're saying these algorithms in this case, so you're referring to non-parametric

23:49.800 --> 23:54.800
in particular, the Gaussian mixture models are fairly widely used.

23:54.800 --> 23:55.800
Is that right?

23:55.800 --> 24:00.320
Yeah, if you know the number of clusters, or if you're going to assume that you give

24:00.320 --> 24:05.600
it as an input to the algorithm, then that is an algorithm that can be applied quite

24:05.600 --> 24:08.240
successfully to data.

24:08.240 --> 24:14.840
And so going back to this thing, the EM algorithm, what you typically end up doing is

24:14.840 --> 24:18.440
iterating between two types of computations.

24:18.440 --> 24:25.040
One is given some estimate of different means and variances of each cluster.

24:25.040 --> 24:29.880
I will compute, I will do the inference, so I will compute what is the probability for

24:29.880 --> 24:33.080
each point to be associated with each one of the clusters.

24:33.080 --> 24:37.960
And then after I'm done computing that, I go back to re-estimating my parameters and

24:37.960 --> 24:45.200
refining the estimates of the parameters, and I do that until convergence.

24:45.200 --> 24:53.280
So does that mean you're iterating on the cluster centers and the parameters of your distributions

24:53.280 --> 24:59.160
for each of the clusters as you're incorporating the point into the clusters?

24:59.160 --> 25:01.520
So this is for training time.

25:01.520 --> 25:05.280
So in training time, in this case, I assume I have access to all of my data, and I'm just

25:05.280 --> 25:11.640
trying to figure out where are these cluster centers and their variances, and which point

25:11.640 --> 25:14.240
belongs where?

25:14.240 --> 25:18.440
So I will train by running the EM algorithm, I will end up with estimates of the cluster

25:18.440 --> 25:22.560
centers and their variances, and then if I'm getting a new data point, then I can do

25:22.560 --> 25:30.400
the prediction of what is the probability that it came from cluster 1, 2, 3, 4?

25:30.400 --> 25:33.320
So where does minmax propagation fit in?

25:33.320 --> 25:37.600
Right, so now we kind of have to step quite a bit back.

25:37.600 --> 25:42.400
So at a very, because we kind of dove into one particular example.

25:42.400 --> 25:49.080
And even with Gaussian mixture model, the inference itself is relatively straightforward,

25:49.080 --> 25:50.080
let's say.

25:50.080 --> 25:55.640
So at a very high level, this is a very, very high level novel algorithm to approximately

25:55.640 --> 26:02.560
solve a particular set of NP-hard problems, and the algorithm is a new variant of belief

26:02.560 --> 26:07.920
propagation, and the problems that we're looking at are minmax problems, and in particular,

26:07.920 --> 26:10.400
we've demonstrated on makespan.

26:10.400 --> 26:14.120
Now in order to parse through all of them, we have to talk about what each of these things

26:14.120 --> 26:15.120
mean, right?

26:15.120 --> 26:16.120
Right, right.

26:16.120 --> 26:21.600
So I can start talking about it, and you can interrupt me with questions, but I'll try

26:21.600 --> 26:23.760
to take us through all of it.

26:23.760 --> 26:28.160
So NP-hard problems are, you know, roughly speaking problems for which we don't know

26:28.160 --> 26:32.360
if there exists an efficient solution that will run in a reasonable amount of time.

26:32.360 --> 26:36.560
We typically know a brute force solution, so the problem with brute force is that as

26:36.560 --> 26:41.240
the problem scales, it takes longer and longer in a typically exponential manner, so it's

26:41.240 --> 26:42.840
just not practical.

26:42.840 --> 26:47.640
And so finding solutions that are approximations to NP-hard problems that run in a reasonable

26:47.640 --> 26:51.800
amount of time is a pretty big research area in theoretical computer science and other

26:51.800 --> 26:54.120
disciplines like operational research.

26:54.120 --> 26:55.960
Okay, so that's the hard.

26:55.960 --> 27:01.000
Now if you look about minmax problems, they appear a lot in game theory, decision theory,

27:01.000 --> 27:03.760
and other math and science domain.

27:03.760 --> 27:09.360
And the technical definition is that you have a function of two sets of variables.

27:09.360 --> 27:14.760
Let's call them x and y, and you want to find the min over x max over y of the function

27:14.760 --> 27:15.760
xy.

27:15.760 --> 27:21.400
But that doesn't really give a lot of intuition, so let's try and look at an example.

27:21.400 --> 27:25.560
So in the paper, we'll look at a problem called makespan, which should be fairly easy

27:25.560 --> 27:26.560
to grasp.

27:26.560 --> 27:31.640
So let's say you have a bunch of incoming jobs, and I'm talking about jobs in the computer

27:31.640 --> 27:38.040
science sense, so some automated tasks that need to be executed on a computer, and each

27:38.040 --> 27:40.920
will require some amount of resources.

27:40.920 --> 27:45.920
So let's think about CPU or memory to run.

27:45.920 --> 27:51.160
And we have a bunch of machines, sort of like a computing cluster to run it.

27:51.160 --> 27:55.040
And maybe each machine has slightly different capabilities, so each will take slightly

27:55.040 --> 27:59.040
different time to run each one of these jobs, okay?

27:59.040 --> 28:04.480
So we want to run all of them, so we obviously want to divide up the jobs in a way that everything

28:04.480 --> 28:06.760
finishes as soon as possible.

28:06.760 --> 28:11.720
And if we think about it, that time will be determined by the bottleneck, the machine

28:11.720 --> 28:17.320
that will take the longest to process all the jobs that were assigned to that machine.

28:17.320 --> 28:22.640
So what we want to do is we want to minimize the maximum time it could possibly take.

28:22.640 --> 28:26.320
We want to find a way to split up the jobs across the machine so that we minimize the

28:26.320 --> 28:30.000
maximum time it would possibly take to execute the jobs.

28:30.000 --> 28:33.920
And you call this problem, what makes span?

28:33.920 --> 28:34.920
Makes span.

28:34.920 --> 28:38.280
Makes span, okay, it sounds like a job stop problem.

28:38.280 --> 28:41.480
Yeah, yeah, it's related to that, yeah.

28:41.480 --> 28:44.840
And notice that there is a simple brute force solution, which is, you know, try out all

28:44.840 --> 28:47.200
the different ways you can divide up the load, right?

28:47.200 --> 28:51.080
But a problem of course is that this will be exponential into a number of jobs.

28:51.080 --> 28:53.520
And so it becomes very intractable very quickly.

28:53.520 --> 28:54.520
Okay.

28:54.520 --> 28:59.720
So, okay, so now we understand at least one example of a minmax problem.

28:59.720 --> 29:05.840
And so the algorithm we presented is again in the context of it's to solve the minmax

29:05.840 --> 29:11.640
problem and the paper demonstrated on the makes span problem, but it's a little bit more,

29:11.640 --> 29:12.640
it's more relevant than that.

29:12.640 --> 29:18.680
It's relevant for a set of minmax problems and it goes into the technical properties of

29:18.680 --> 29:21.960
which kind of minmax problems can be addressed.

29:21.960 --> 29:23.960
So I think we can skip that.

29:23.960 --> 29:25.560
So now we can talk about the algorithm.

29:25.560 --> 29:31.280
What can you give us a high level characterization of those types?

29:31.280 --> 29:32.280
Minmax problems.

29:32.280 --> 29:37.360
The types of minmax problems just to get a sense for, you know, even what are the, you

29:37.360 --> 29:41.360
know, what are the dimensions, you know, around what you're thinking about characterizing

29:41.360 --> 29:42.880
these problems?

29:42.880 --> 29:47.760
So they are often notion of, I want to minimize my worst case scenario.

29:47.760 --> 29:48.760
Right.

29:48.760 --> 29:55.680
So, another, another example of minmax problems is when you have some sort of a two-player

29:55.680 --> 30:01.760
game and you're trying to come up with a strategy so that whatever the other player can do,

30:01.760 --> 30:04.760
which is some sort of, you know, I will incur some loss.

30:04.760 --> 30:05.760
Right.

30:05.760 --> 30:08.320
So the maximum loss, I want to minimize that.

30:08.320 --> 30:09.320
Okay.

30:09.320 --> 30:12.560
So that's kind of the most abstract formulation for that problem.

30:12.560 --> 30:18.280
So then it sounds like what you've done in the paper in terms of characterizing the types

30:18.280 --> 30:25.520
of minmax problems to which this particular method applies, it's not necessarily an intuitive

30:25.520 --> 30:26.520
characterization.

30:26.520 --> 30:30.600
It's more mathematical detail, I guess.

30:30.600 --> 30:31.600
Yeah.

30:31.600 --> 30:32.600
Yeah, exactly.

30:32.600 --> 30:33.600
It's a settle.

30:33.600 --> 30:34.600
Okay.

30:34.600 --> 30:35.600
Yeah.

30:35.600 --> 30:36.600
There's restrictions, let's say.

30:36.600 --> 30:37.600
Okay.

30:37.600 --> 30:38.600
And you mentioned belief propagation.

30:38.600 --> 30:39.600
Right.

30:39.600 --> 30:40.600
Let's talk a little bit about that.

30:40.600 --> 30:43.560
So now we can talk about the algorithm we actually propose.

30:43.560 --> 30:45.760
So it is a variant of belief propagation.

30:45.760 --> 30:46.760
Okay.

30:46.760 --> 30:51.960
And so in order to understand belief propagation, we need to know a little bit about inference

30:51.960 --> 30:56.200
in graphical models and message passing algorithms.

30:56.200 --> 31:00.960
And we talked already a little bit about inference in graphical models.

31:00.960 --> 31:06.760
In the context of describing, going back again to the notion of, you know, we have data

31:06.760 --> 31:13.400
or observation and we assume that there is some noise in these observations in these measurements

31:13.400 --> 31:17.240
and then there is some uncertainty and we assume that there are some quantities that

31:17.240 --> 31:24.600
we don't have direct way to measure, but are depend, they have dependencies with relationships

31:24.600 --> 31:26.960
with the quantities we're able to measure.

31:26.960 --> 31:32.400
We often talk about these in terms of observed and hidden variables and they are in graphical

31:32.400 --> 31:37.760
models, we try to capture the way in which they are related using a graph.

31:37.760 --> 31:41.960
So basically every variable, whether it's something that we've observed or something

31:41.960 --> 31:45.640
that we can't observe is represented as a node in the graph.

31:45.640 --> 31:51.880
And then the interactions between these variables are represented as edges in the graph.

31:51.880 --> 31:58.800
And once we describe the problem in that manner, we can start borrowing from, you know, graph

31:58.800 --> 32:05.880
theory and graph algorithms to answer different questions about the graph that we have described,

32:05.880 --> 32:12.080
which is basically a representation of the probability distribution over all of these variables.

32:12.080 --> 32:15.560
So belief propagation is a general algorithm.

32:15.560 --> 32:19.040
It has several existing variants that are quite well known.

32:19.040 --> 32:23.960
One of them is the max product belief propagation and another one is some product belief propagation.

32:23.960 --> 32:29.760
And they are algorithms or recipes for computing these quantities of interest.

32:29.760 --> 32:36.400
So let's say that we have a bunch of these hidden variables and one question we can ask

32:36.400 --> 32:40.120
is, you know, let's say that they are binary.

32:40.120 --> 32:43.120
So they can either take the value zero or one.

32:43.120 --> 32:49.360
We can ask what is the particular setting of all of these variables that yields the highest

32:49.360 --> 32:54.640
probability, this is kind of like the mode of the distribution.

32:54.640 --> 32:59.080
And in order to compute that, we can use something like the max product algorithm.

32:59.080 --> 33:04.000
And at an intuitive level, the way these algorithms work is, again, going back to this

33:04.000 --> 33:07.640
graph that we have in mind, they send messages between the nodes.

33:07.640 --> 33:13.440
So the nodes exchange numerical quantities sometimes in an iterative fashion.

33:13.440 --> 33:17.520
And then eventually they reach some kind of a decision.

33:17.520 --> 33:24.680
So the nodes are variables observed in hidden variables.

33:24.680 --> 33:31.640
And can you give an example of a scenario that includes both these observed and hidden

33:31.640 --> 33:32.640
variables?

33:32.640 --> 33:37.480
Are these hidden variables in a sense of latent variables and a different sense?

33:37.480 --> 33:39.680
Yeah, latent variables.

33:39.680 --> 33:41.680
Okay.

33:41.680 --> 33:47.120
And the observed variables are, what's an example where you'd have both observed and

33:47.120 --> 33:50.040
hidden variables.

33:50.040 --> 33:56.600
So I can actually go back to my graphic, a Gaussian mixture model and I can represent that

33:56.600 --> 34:03.280
as a graphical model where my observed variables is the data that I'm trying to cluster.

34:03.280 --> 34:12.080
And the hidden variables are the clusters that I, that are represented by, by, you know,

34:12.080 --> 34:13.360
these parameters.

34:13.360 --> 34:20.480
And, and maybe I can talk in order to kind of segue into, into this particular, the,

34:20.480 --> 34:27.360
the minmax inference, I'd like to talk about another clustering algorithm, which I think

34:27.360 --> 34:34.280
would really focus us on the, the inference side of the, the hidden variables.

34:34.280 --> 34:35.280
Okay.

34:35.280 --> 34:37.160
So, so that, that's a different clustering algorithm.

34:37.160 --> 34:42.640
And this was actually the focus of my thesis work and it's called affinity propagation.

34:42.640 --> 34:43.640
Okay.

34:43.640 --> 34:50.200
And this is an algorithm that was first presented by my supervisor, Brendan Frye, and his

34:50.200 --> 34:52.120
graduate student, Delbert Duke.

34:52.120 --> 34:57.160
And I focused on various extensions of the algorithm and reformulating of them mathematics.

34:57.160 --> 35:02.440
So anyways, the, the way we represent clustering in that algorithm is we basically say, okay,

35:02.440 --> 35:06.640
we have a bunch of, a bunch of data points who want to split it into groups that make sense.

35:06.640 --> 35:14.200
And instead of talking about means and variances, we're going to say that each cluster is basically

35:14.200 --> 35:17.000
best represented by an exemplar.

35:17.000 --> 35:21.320
So this is like the representative of the cluster.

35:21.320 --> 35:28.040
And so really the problem becomes, can we find the right set of representatives?

35:28.040 --> 35:33.480
And then every point that is not a representative just needs to be associated with the representative

35:33.480 --> 35:37.000
that is closest to it, that is most similar to it.

35:37.000 --> 35:38.000
Okay.

35:38.000 --> 35:39.000
Okay.

35:39.000 --> 35:43.560
So the problem, of course, is you don't know which subset of points is best to represent

35:43.560 --> 35:45.200
these clusters, right?

35:45.200 --> 35:53.120
So in, when you do this max product, belief propagation in the context of this graph, your

35:53.120 --> 35:57.040
variables are basically the data points.

35:57.040 --> 36:03.800
The hidden variables are for each point which cluster should it be assigned to?

36:03.800 --> 36:04.800
Okay.

36:04.800 --> 36:05.800
Okay.

36:05.800 --> 36:06.800
So that's something you don't know.

36:06.800 --> 36:08.920
And for each point, which, right.

36:08.920 --> 36:15.320
And so that specifically, as opposed to the parameters of the cluster itself, right?

36:15.320 --> 36:16.320
Okay.

36:16.320 --> 36:19.640
So we're not using parameters anymore for this algorithm.

36:19.640 --> 36:22.280
So you can see that here, here we'll be doing inference.

36:22.280 --> 36:28.600
We're going to come up with estimates of, for each point, what is the cluster it's going

36:28.600 --> 36:33.360
to be assigned to without talking about a notion of means and variances.

36:33.360 --> 36:34.440
So there is no learning here.

36:34.440 --> 36:35.440
It's only inference.

36:35.440 --> 36:36.440
Right.

36:36.440 --> 36:37.440
Okay.

36:37.440 --> 36:41.280
And so the way the algorithm ends up working is basically, again, sending messages.

36:41.280 --> 36:48.400
So the nodes send messages to each other and the messages come out of a mathematical

36:48.400 --> 36:49.400
derivation.

36:49.400 --> 36:53.760
You look at them, you can kind of give them, give an intuitive explanation of what they're

36:53.760 --> 36:59.720
really trying to say, which is they, it's an iterative process where first, all the nodes

36:59.720 --> 37:03.760
send a message to all of their neighbors, to each one of their neighbors saying, to what

37:03.760 --> 37:07.920
extent do I want you to be my representative, right?

37:07.920 --> 37:12.240
And after collecting all the information from my neighbors, so all my neighbors have told

37:12.240 --> 37:14.800
me to what extent they want me to be the representative.

37:14.800 --> 37:21.120
Now I send all my neighbors a message back saying to what extent do I want to be a representative.

37:21.120 --> 37:22.120
Right.

37:22.120 --> 37:27.520
And so you iterate back and forth on these messages until you converge.

37:27.520 --> 37:33.640
So something back and then in belief propagation, we end up having these messages that are exchanged

37:33.640 --> 37:39.400
between the nodes in the graphical model for the purpose of doing inference for the purpose

37:39.400 --> 37:43.200
of computing these quantities that we care about.

37:43.200 --> 37:47.960
So I think now we might be ready to talk about the mean max propagation.

37:47.960 --> 37:48.960
Yeah.

37:48.960 --> 37:51.720
So just to make sure I'm on the same page.

37:51.720 --> 37:56.920
So we're talking about messages and message passing and things like that.

37:56.920 --> 38:05.120
This is basically a computational tool or an accounting tool that we're using to just

38:05.120 --> 38:12.120
keep track of quantities in this algorithm, really, as we're trying to do the inference.

38:12.120 --> 38:17.840
And the side of my brain that thinks about distributed computing is thinking about real

38:17.840 --> 38:22.480
things, passing messages, and it's like, that's not really what we're doing here.

38:22.480 --> 38:23.480
Right.

38:23.480 --> 38:25.760
These are not text messages or JSON messages.

38:25.760 --> 38:31.560
These are numerical quantities that are computed and then used.

38:31.560 --> 38:36.880
But it's convenient to think about them as, you know, there is a numerical quantity computed

38:36.880 --> 38:39.320
for each one of these nodes.

38:39.320 --> 38:44.000
And then there is another one computed which relies on the previous computation.

38:44.000 --> 38:49.040
So it's almost as if the node sent that numerical representation may be a vector of numbers

38:49.040 --> 38:52.040
to all of its neighbors and said, you know, this is my message.

38:52.040 --> 38:56.480
So now you're ready to do your computation in the next iteration of the algorithm.

38:56.480 --> 38:57.480
Okay.

38:57.480 --> 38:58.480
Great.

38:58.480 --> 39:01.240
So now I think we have all the building blocks.

39:01.240 --> 39:06.400
So we have, you know, these graphical models and we know a little bit about message passing

39:06.400 --> 39:13.760
algorithms and the question we ask in this paper is, can we take this belief propagation

39:13.760 --> 39:20.040
type algorithms and we understand that they can work for the four types of questions

39:20.040 --> 39:24.800
that were interested in the context of, you know, finding the assignment of the variable

39:24.800 --> 39:30.040
that gives us the maximum probability or computing marginals and other tasks.

39:30.040 --> 39:36.000
Can we somehow formulate it to solve the mean max problem?

39:36.000 --> 39:41.840
And so it turns out that we can, it's not by running the same algorithm.

39:41.840 --> 39:49.480
It's by borrowing the general idea and kind of the sum of the mathematics around it and

39:49.480 --> 39:54.480
writing down the mean max problem as a graphical model.

39:54.480 --> 40:02.280
And then deriving the form of the messages that need to be sent in order to compute what

40:02.280 --> 40:04.880
should be the solution to the mean max problem.

40:04.880 --> 40:12.840
So they are similar in principle to the messages you send when you're doing max product or

40:12.840 --> 40:13.840
some product.

40:13.840 --> 40:19.000
But of course, it's its own flavor that required figuring out the right derivation.

40:19.000 --> 40:25.120
And another interesting thing is what we do when we have interaction that are between

40:25.120 --> 40:27.040
quite a few variables, right?

40:27.040 --> 40:34.120
So if we have in our graph an interaction between a subset of variables that is more than

40:34.120 --> 40:42.480
just two, some of the computations can on the surface seem like they are exponential in

40:42.480 --> 40:44.480
the number of the variables.

40:44.480 --> 40:51.440
So in order to be able to actually apply this framework to mean max, we had to recognize

40:51.440 --> 40:57.400
that what looks like it's going to be an exponentially expensive computation can be actually done

40:57.400 --> 41:03.320
by some, you know, a little bit of cleverness and bookkeeping in non-exponential time.

41:03.320 --> 41:06.600
So in something that is reasonable to compute.

41:06.600 --> 41:07.920
Okay.

41:07.920 --> 41:16.840
Just kind of summarizing basically what you did with mean max propagation was you borrowed

41:16.840 --> 41:26.520
from this message passing approach that comes out of belief propagation and applied it to

41:26.520 --> 41:34.360
this makespan problem and in addition, you've kind of mathematically characterized the

41:34.360 --> 41:40.760
more broader set of min max problems to which this model would also apply.

41:40.760 --> 41:41.760
Yes.

41:41.760 --> 41:43.680
And importantly, derive the form.

41:43.680 --> 41:49.480
The messages need to take if you're doing a min max computation as opposed to a sound

41:49.480 --> 41:53.200
product computation or a max product computation.

41:53.200 --> 41:55.040
And what is that form?

41:55.040 --> 41:59.680
So that's in the technical details, so it's basically an equation that describes, you

41:59.680 --> 42:05.600
know, some form of a, you know, operations as minimums and maximums that can be computed

42:05.600 --> 42:12.840
in, let's call it linear time for the purposes of this stuff, but it's an equation, right?

42:12.840 --> 42:16.920
It's an equation that when you go about implementing the algorithm, you will just write in

42:16.920 --> 42:17.920
some code.

42:17.920 --> 42:18.920
Okay.

42:18.920 --> 42:26.240
So given the computation requirements of this and the space of potential application,

42:26.240 --> 42:31.040
where do you think this paper and algorithm will have the broadest impact?

42:31.040 --> 42:38.400
So I think it will be interesting to see what kind of things people use it for because

42:38.400 --> 42:43.240
it's relatively, it's a new formulation.

42:43.240 --> 42:52.320
We wanted to look at the case of, you know, max, max span as a particular application,

42:52.320 --> 42:56.480
which is actually, it is useful for things like scheduling tasks.

42:56.480 --> 43:03.960
It's useful for workload in terms of power consumption on turbines and power plants.

43:03.960 --> 43:11.880
So in the operational research world, I believe, you know, they look at these types of applications.

43:11.880 --> 43:19.040
But from, I think one of the interesting things you get to do as a researcher is sometimes

43:19.040 --> 43:27.040
focus more on, you know, the core algorithm and the mathematics of it and, you know, post

43:27.040 --> 43:31.120
it out there for the world to see so that various people who are looking at, you know,

43:31.120 --> 43:36.360
different problem domains can recognize that this actually matches to their problem of

43:36.360 --> 43:39.360
interest and then take it into interesting places.

43:39.360 --> 43:40.360
Right.

43:40.360 --> 43:41.360
Right.

43:41.360 --> 43:46.440
Maybe worth mentioning that this is all work that you did in the U of T context as opposed

43:46.440 --> 43:48.880
to the Uber context, is that right?

43:48.880 --> 43:49.880
Right.

43:49.880 --> 43:50.880
Right.

43:50.880 --> 43:56.320
And it's also worth mentioning my co-author, Christopher, Siamuk and Brendan.

43:56.320 --> 43:57.320
Okay.

43:57.320 --> 43:58.320
Awesome.

43:58.320 --> 43:59.320
Very cool.

43:59.320 --> 44:05.560
You know, it strikes me that this is a little bit of an aside, but I'm not sure that you

44:05.560 --> 44:11.920
know, but we do a monthly meetup where we kind of dig into research papers and we've

44:11.920 --> 44:17.160
done a bunch of kind of deep learning focus ones, but it strikes me that this would be an

44:17.160 --> 44:24.320
interesting one to maybe have you or one of your co-authors present to the group to really

44:24.320 --> 44:27.400
dig into some of the details here.

44:27.400 --> 44:30.480
A lot of it, it seems like a lot of it is in the details.

44:30.480 --> 44:36.720
Yeah, and I would be happy to talk about that and to look into this opportunity.

44:36.720 --> 44:42.040
I think one of the, again, as a side note, one of the exciting things about deep learning

44:42.040 --> 44:49.760
is that they're so successful in terms of, you know, how applicable they are to problems

44:49.760 --> 44:51.120
that we care about.

44:51.120 --> 44:59.000
The one thing that is that they don't have as much as other algorithms is kind of, you

44:59.000 --> 45:07.480
know, deep mathematics and kind of challenging representations that you really need a lot

45:07.480 --> 45:14.080
of time to wrap your head around and so it's really nice to dig into these papers and

45:14.080 --> 45:20.280
there's typically quite a bit of math in them as well, so I think it'll probably be interesting

45:20.280 --> 45:26.160
and we'll challenge people a little bit in a way that's interesting to look into these

45:26.160 --> 45:27.160
types of papers.

45:27.160 --> 45:32.400
Awesome, where do you go from here with this particular work and your research?

45:32.400 --> 45:38.720
Is this something that you're kind of finishing up that relates back to your time at Toronto

45:38.720 --> 45:41.080
or is this ongoing work that you're involved in?

45:41.080 --> 45:46.120
I would say this is more of a, you know, what I call recreational research.

45:46.120 --> 45:56.040
It's something that I worked with on with Chris in the past was a graduate student as

45:56.040 --> 46:01.040
well in the lab and he kind of took it off from there.

46:01.040 --> 46:06.760
And from me right now, the main focus is definitely ear at work and working on self-driving

46:06.760 --> 46:07.760
cars.

46:07.760 --> 46:08.760
Awesome.

46:08.760 --> 46:09.760
Awesome.

46:09.760 --> 46:14.880
Well, I really appreciate you taking the time to walk through this with me.

46:14.880 --> 46:19.880
I know I've had a lot of questions and I still have a lot of questions there, so this

46:19.880 --> 46:24.920
is not a topic that we've gone into in a lot of detail here on the podcast, but I think

46:24.920 --> 46:26.960
I did.

46:26.960 --> 46:33.520
I had several conversations on graphical models at NipSive, forget how many of those came

46:33.520 --> 46:40.320
out in our Nip series off the top of my head, but we've got a few more to release over

46:40.320 --> 46:43.920
the next few weeks or months.

46:43.920 --> 46:48.960
You know, it seems to be, I don't know, I was surprised to, I guess I was going back

46:48.960 --> 46:56.440
to our earlier conversation, I was surprised by this kind of undercurrent of work happening

46:56.440 --> 47:02.260
in graphical models at NipSive and how often I heard it, although I don't know that that

47:02.260 --> 47:07.560
means anything is the first, it was the first time I was at NipSive, so my baseline is

47:07.560 --> 47:15.000
not exactly, you know, it was not exactly very, you know, dialed in, but they're definitely

47:15.000 --> 47:16.000
seats.

47:16.000 --> 47:18.000
There's a lot of people interested in this area.

47:18.000 --> 47:19.000
Yeah.

47:19.000 --> 47:20.000
Yeah, absolutely.

47:20.000 --> 47:26.360
I mean, again, neural networks have kind of taken from stage for the last few years,

47:26.360 --> 47:34.960
but there's always ongoing research on graphical models and on an array of other problems in machine

47:34.960 --> 47:40.160
learning, and so I think it's kind of nice to see things like the focus going back to

47:40.160 --> 47:44.040
some of these models and seeing what can we do with them now.

47:44.040 --> 47:45.040
Absolutely.

47:45.040 --> 47:49.360
Well, on that note, Emar, thank you so much for joining us.

47:49.360 --> 47:50.360
Absolutely.

47:50.360 --> 47:55.960
Thank you for having me, and I hope it makes for an interesting listening, or at least if

47:55.960 --> 48:02.160
not everything was super clear as an introduction to go and explore some more.

48:02.160 --> 48:03.160
Absolutely.

48:03.160 --> 48:04.160
Absolutely.

48:04.160 --> 48:05.160
Thank you.

48:05.160 --> 48:07.160
Thank you, bye-bye.

48:07.160 --> 48:13.160
All right, everyone, that's our show for today.

48:13.160 --> 48:18.400
Thanks so much for listening, and for your continued feedback and support.

48:18.400 --> 48:23.600
For more information on Emar, or any of the topics covered in this episode, head on over

48:23.600 --> 48:27.760
to twimlai.com slash talk slash 101.

48:27.760 --> 48:33.000
Of course, we'd be delighted to hear from you, either via comment on the show notes page,

48:33.000 --> 48:39.680
or via Twitter, directly to me at At Sam Charrington, or to the show at At Twimlai.

48:39.680 --> 48:42.840
Thanks once again for listening, and catch you next time.

