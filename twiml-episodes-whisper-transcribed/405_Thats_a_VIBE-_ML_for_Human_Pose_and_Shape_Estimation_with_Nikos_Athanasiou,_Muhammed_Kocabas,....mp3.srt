1
00:00:00,000 --> 00:00:15,920
Welcome to the Tumel AI Podcast, I'm your host Sam Charrington.

2
00:00:15,920 --> 00:00:23,560
Hey, what's up everyone?

3
00:00:23,560 --> 00:00:26,560
I hope you all had a wonderful weekend.

4
00:00:26,560 --> 00:00:31,920
As I mentioned last week, the popular causal modeling and machine learning course, hosted

5
00:00:31,920 --> 00:00:37,720
by Tumel, featuring instructor Robert O'Sezua Ness, is back.

6
00:00:37,720 --> 00:00:42,680
We've now held two sessions of this course and student feedback has been amazing.

7
00:00:42,680 --> 00:00:48,440
Students love the in-depth course content, assignments and capstone projects, of course,

8
00:00:48,440 --> 00:00:53,760
but the number one observation has steadily been about Robert's weekly study group sessions

9
00:00:53,760 --> 00:00:57,560
and the one-on-one office hours that students have with him.

10
00:00:57,560 --> 00:01:01,800
You can use this course as an intro to causal modeling by just spending a few hours a week

11
00:01:01,800 --> 00:01:06,120
watching the course videos and attending the study group session, or really dig into

12
00:01:06,120 --> 00:01:10,400
the materials and assignments and develop cutting-edge skills in the field.

13
00:01:10,400 --> 00:01:15,600
Now, no promises, but a couple of former students have gone on to write academic research

14
00:01:15,600 --> 00:01:20,880
papers with Robert based on the materials they learned in this course.

15
00:01:20,880 --> 00:01:26,120
This morning, Robert and I held a great session about causal modeling and the course.

16
00:01:26,120 --> 00:01:30,960
If you're interested in the topic of causal modeling or considering the course, I really

17
00:01:30,960 --> 00:01:38,440
encourage you to check it out by visiting our course page at twimmelai.com slash causal.

18
00:01:38,440 --> 00:01:44,000
As you all know, we are super excited about Twimmelfest, our upcoming AI festival that

19
00:01:44,000 --> 00:01:50,720
is jam-packed with engaging community-oriented and community-driven content.

20
00:01:50,720 --> 00:01:56,000
We are on the lookout for Twimmelfest hosts who will help us create the event.

21
00:01:56,000 --> 00:02:00,920
As a host, you'll get to share your ideas, experiences, expertise, and passions with

22
00:02:00,920 --> 00:02:05,840
the Twimmel community through a session that you propose and organize.

23
00:02:05,840 --> 00:02:11,240
So far, we've had a bunch of wonderful submissions, including an exploration into the impact

24
00:02:11,240 --> 00:02:17,280
that AI will have on the medical field, a quote-unquote rookie playbook for ML beginners,

25
00:02:17,280 --> 00:02:22,880
a code names challenge where AI-powered bots compete, and many more.

26
00:02:22,880 --> 00:02:27,600
There's still time to submit your session ideas by visiting twimmelfest.com, but time

27
00:02:27,600 --> 00:02:32,640
is running out, so put together those great ideas and hit submit.

28
00:02:32,640 --> 00:02:37,760
And even if you're not ready to host, be sure to register for Twimmelfest today.

29
00:02:37,760 --> 00:02:43,640
The event takes place October 13th through 30th is totally free and your registration ensures

30
00:02:43,640 --> 00:02:49,760
that you'll be among the very first to hear about important updates, giveaways, and announcements.

31
00:02:49,760 --> 00:02:53,920
And now on to the show.

32
00:02:53,920 --> 00:03:00,400
All right, everyone.

33
00:03:00,400 --> 00:03:07,280
I am on the line with Mohammed Kojabash, Nikos Athanasu, and Michael Black.

34
00:03:07,280 --> 00:03:14,840
They are with the Max Planck Institute for Intelligent Systems, and we're here to talk about their

35
00:03:14,840 --> 00:03:21,600
most recent paper, a paper called Vib that we will get into in more detail.

36
00:03:21,600 --> 00:03:25,560
But Mohammed, Nikos, and Michael, welcome to the Twimmel AI podcast.

37
00:03:25,560 --> 00:03:26,560
Thanks.

38
00:03:26,560 --> 00:03:28,600
Thanks for having us.

39
00:03:28,600 --> 00:03:29,600
Awesome.

40
00:03:29,600 --> 00:03:35,400
Let's start by having each of you share a little bit about your background and your role

41
00:03:35,400 --> 00:03:41,480
at the Max Planck Institute and what got you, what in your background led you to this work

42
00:03:41,480 --> 00:03:44,720
and what your general research interests are.

43
00:03:44,720 --> 00:03:46,720
Mohammed, why don't we start with you?

44
00:03:46,720 --> 00:03:47,720
Sure.

45
00:03:47,720 --> 00:03:49,040
So I am Mohammed Kojabash.

46
00:03:49,040 --> 00:03:55,160
I'm currently a PhD student in Max Planck Institute for Intelligent Systems in Perseving

47
00:03:55,160 --> 00:04:00,600
Systems Department, doing computer vision and graphics research.

48
00:04:00,600 --> 00:04:06,920
So I did my bachelor's in computer engineering in Istanbul Technical University in Turkey,

49
00:04:06,920 --> 00:04:11,600
and my master's in Middle East Technical University in Turkey again.

50
00:04:11,600 --> 00:04:19,360
So during my bachelor's, so I tried to explore different areas that is interesting for me,

51
00:04:19,360 --> 00:04:24,320
ranging from robotics and LP to computer vision and machine learning.

52
00:04:24,320 --> 00:04:30,120
I continued for a master's degree in computer vision and machine learning, and I started working

53
00:04:30,120 --> 00:04:38,360
on human process estimation, which has broader implications and application areas in the community.

54
00:04:38,360 --> 00:04:42,800
And yeah, and then I started doing a PhD again in the same topic.

55
00:04:42,800 --> 00:04:43,800
Cool.

56
00:04:43,800 --> 00:04:44,800
Cool.

57
00:04:44,800 --> 00:04:45,800
Thank you.

58
00:04:45,800 --> 00:04:46,800
Nikos.

59
00:04:46,800 --> 00:04:47,800
So hi.

60
00:04:47,800 --> 00:04:49,000
I'm Nikos Athanasio.

61
00:04:49,000 --> 00:04:57,480
So I'm also currently a PhD student at Max Planck Institute, I mean my first year.

62
00:04:57,480 --> 00:05:06,040
So a little bit about my background, I completed my bachelor's and master's degree in computer

63
00:05:06,040 --> 00:05:11,000
engineering in Greece in National Technical University.

64
00:05:11,000 --> 00:05:16,520
During the first year of my research, I started doing research basically on natural language

65
00:05:16,520 --> 00:05:24,280
processing, but then I wanted to combine natural language processing, machine learning with

66
00:05:24,280 --> 00:05:29,400
computer vision, so I started my PhD at Max Planck Institute.

67
00:05:29,400 --> 00:05:37,040
I'm currently working on motion, understanding the coding and how we can combine motion

68
00:05:37,040 --> 00:05:40,520
and actions with language semantics.

69
00:05:40,520 --> 00:05:47,200
I would like to combine machine learning and mathematical modeling with human interactions.

70
00:05:47,200 --> 00:05:53,480
I think that's the most interesting part of the machine learning research.

71
00:05:53,480 --> 00:05:58,440
And that's why I followed the PhD degree and started last September.

72
00:05:58,440 --> 00:05:59,440
Awesome.

73
00:05:59,440 --> 00:06:00,440
Awesome.

74
00:06:00,440 --> 00:06:01,440
Thank you.

75
00:06:01,440 --> 00:06:02,440
Michael.

76
00:06:02,440 --> 00:06:03,440
Well, I'm Michael Black.

77
00:06:03,440 --> 00:06:07,760
I'm a director at the Max Planck Institute for Intelligent Systems.

78
00:06:07,760 --> 00:06:14,520
And I've been working on problems in human motion estimation since the early 1990s.

79
00:06:14,520 --> 00:06:20,120
And I became interested in human motion because computers can't really understand us and

80
00:06:20,120 --> 00:06:25,880
be full partners with us until they understand our facial expressions, how we move, how we

81
00:06:25,880 --> 00:06:27,760
interact with the world.

82
00:06:27,760 --> 00:06:32,400
And I'm motivated by Aristotle who said to be ignorant of motion is to be ignorant of

83
00:06:32,400 --> 00:06:33,400
nature.

84
00:06:33,400 --> 00:06:37,880
And I think almost all my research focuses on how things move and trying to get computers

85
00:06:37,880 --> 00:06:40,080
to understand that motion.

86
00:06:40,080 --> 00:06:41,360
A little bit about my background.

87
00:06:41,360 --> 00:06:46,080
I've lived all over the place, educated in the U.S. and Canada.

88
00:06:46,080 --> 00:06:52,360
And before I came to Germany about nine years ago, I was a professor at Brown University.

89
00:06:52,360 --> 00:07:01,480
And I've been in Germany since 2011 and came here to co-found this new institute for intelligent

90
00:07:01,480 --> 00:07:07,320
systems that brings together scientists studying artificial intelligence and robotics and

91
00:07:07,320 --> 00:07:13,680
using techniques all the way from machine learning, like we're talking about here, to physical

92
00:07:13,680 --> 00:07:17,760
systems where they make new materials for soft robotics and so on.

93
00:07:17,760 --> 00:07:21,600
It's a very interdisciplinary and exciting field.

94
00:07:21,600 --> 00:07:28,400
So Mohammed, you mentioned that your area of interest is in pose estimation and affect

95
00:07:28,400 --> 00:07:35,320
the paper that we'll be talking about vibe, which is short for video inference for human

96
00:07:35,320 --> 00:07:43,440
body pose and shape estimation is kind of in the direction of advancing pose estimation.

97
00:07:43,440 --> 00:07:50,560
You share a little bit about kind of the broader landscape of pose estimation and where your

98
00:07:50,560 --> 00:07:52,800
paper is hoping to contribute.

99
00:07:52,800 --> 00:07:58,520
I think we've, you know, folks in the audience have probably seen various images that come

100
00:07:58,520 --> 00:08:06,840
from pose estimation papers and tools that show kind of like a stick figure superimposed

101
00:08:06,840 --> 00:08:09,280
on a picture of a body.

102
00:08:09,280 --> 00:08:14,440
What you're doing is a little different than that and that you're doing kind of 3D pose

103
00:08:14,440 --> 00:08:15,440
estimation.

104
00:08:15,440 --> 00:08:19,400
Talk a little bit about the broad landscape that your paper fits into.

105
00:08:19,400 --> 00:08:20,400
Sure.

106
00:08:20,400 --> 00:08:28,320
So basically the pose estimation is the task to estimate human pose from images, videos

107
00:08:28,320 --> 00:08:31,080
or any kind of sensory data.

108
00:08:31,080 --> 00:08:35,880
But in computer vision specifically, we tackle with the problem of estimating human pose

109
00:08:35,880 --> 00:08:39,440
from images and videos specifically.

110
00:08:39,440 --> 00:08:44,040
And we can define human pose in different kind of ways.

111
00:08:44,040 --> 00:08:49,040
Like as you mentioned, one of them would be to estimate some key points like these are

112
00:08:49,040 --> 00:08:56,400
some parts set of key points only that can be the joint locations or some lean locations

113
00:08:56,400 --> 00:09:04,160
of the human bodies or we can also try to estimate the whole body like in a more structured

114
00:09:04,160 --> 00:09:08,400
and misstructured manner like a human body mesh.

115
00:09:08,400 --> 00:09:13,520
So this kind of representations have some kind of a hierarchy.

116
00:09:13,520 --> 00:09:19,920
So a sparse set of key points are really lacking to explain the real human body.

117
00:09:19,920 --> 00:09:27,040
So and also we can estimate the 2D or 3D sparse set of key points.

118
00:09:27,040 --> 00:09:32,360
But when we proceed to more higher richer representations like human body meshes which

119
00:09:32,360 --> 00:09:39,560
can define the whole body like the whole pose and shape, which has some bleacher information

120
00:09:39,560 --> 00:09:41,920
that explains the human body.

121
00:09:41,920 --> 00:09:49,680
So in this paper specifically, we try to estimate the whole body mesh, which is more explanatory

122
00:09:49,680 --> 00:09:53,280
than on the sparse set of key points.

123
00:09:53,280 --> 00:10:00,760
And also we tackle the problem in 3D space, which makes the problem more difficult to estimate

124
00:10:00,760 --> 00:10:03,760
from the images or videos only.

125
00:10:03,760 --> 00:10:04,760
Right.

126
00:10:04,760 --> 00:10:11,760
And Nika, you mentioned that one of your interests is in applying vision to motion.

127
00:10:11,760 --> 00:10:17,680
Can you talk a little bit about the motion aspect of this and where that comes in?

128
00:10:17,680 --> 00:10:22,000
So we are aiming directly to capture motion.

129
00:10:22,000 --> 00:10:29,880
So we are starting from a plane video, which are multiple images, the sequence of images.

130
00:10:29,880 --> 00:10:37,320
And from those images that we have, we detect the person in the video and we estimate his

131
00:10:37,320 --> 00:10:40,040
or her 3D pose and shape.

132
00:10:40,040 --> 00:10:47,480
So many methods have looked at this problem, but the vast majority focus on single images.

133
00:10:47,480 --> 00:10:53,680
So estimating the human pose in one image and a video is just a sequence of single images.

134
00:10:53,680 --> 00:10:57,560
So you could just apply those techniques one frame at a time.

135
00:10:57,560 --> 00:11:03,520
If you do that, you tend to get a jerky reconstruction that's not moving smoothly and naturally

136
00:11:03,520 --> 00:11:05,920
like a human would.

137
00:11:05,920 --> 00:11:11,320
And so one of the observations here is there's more information in the video sequence in

138
00:11:11,320 --> 00:11:13,120
the continuity of the motion.

139
00:11:13,120 --> 00:11:19,760
And if we can train the computer to exploit that by teaching it what it is to move like

140
00:11:19,760 --> 00:11:25,160
a human, then we can exploit information across a long video sequence and get better results

141
00:11:25,160 --> 00:11:28,680
and you would get by doing it frame by frame.

142
00:11:28,680 --> 00:11:34,640
And as an example of the better nature of the results, you've got a great picture at

143
00:11:34,640 --> 00:11:42,080
the beginning of the paper that shows some areas where the current state of the art method

144
00:11:42,080 --> 00:11:48,160
in a challenging video doesn't seem to track very well, whereas your method performs

145
00:11:48,160 --> 00:11:49,400
better.

146
00:11:49,400 --> 00:11:56,080
What is that kind of reference method that you looked at and what's the data set that

147
00:11:56,080 --> 00:11:59,720
you are building your model with?

148
00:11:59,720 --> 00:12:05,720
Well, the competing method is from one of our very good friends and colleagues, Anju Kanazawa,

149
00:12:05,720 --> 00:12:09,240
who actually did an internship in our institute.

150
00:12:09,240 --> 00:12:13,680
So it's a friendly competition for who's going to have the state of the art.

151
00:12:13,680 --> 00:12:18,400
Anju's amazing and she's a professor at Berkeley.

152
00:12:18,400 --> 00:12:25,600
She also extended one of these single frame methods over time, but in a different way.

153
00:12:25,600 --> 00:12:32,040
And one of the things that distinguishes our approach was actually inspired by something

154
00:12:32,040 --> 00:12:39,440
she did earlier, which is to use a discriminator that knows something about, it's trained

155
00:12:39,440 --> 00:12:44,600
to distinguish between fake human motions and real human motions.

156
00:12:44,600 --> 00:12:50,680
And to enable that, we exploited a data set that we made public back in the fall called

157
00:12:50,680 --> 00:12:51,680
a mass.

158
00:12:51,680 --> 00:12:59,280
And a mass is a data set of about 45 hours worth of human motion capture data that captures

159
00:12:59,280 --> 00:13:03,680
a wide range of how people move.

160
00:13:03,680 --> 00:13:09,000
And then we used that to teach the computer basically what is a good human motion and

161
00:13:09,000 --> 00:13:10,400
what isn't.

162
00:13:10,400 --> 00:13:15,200
And when you say human motion capture, is this kind of regular video or is this the kind

163
00:13:15,200 --> 00:13:20,760
of thing that we will often see with humans in like white suits with like balls around

164
00:13:20,760 --> 00:13:21,760
them?

165
00:13:21,760 --> 00:13:22,760
Yeah, the latter.

166
00:13:22,760 --> 00:13:29,440
There are many, this is commercial motion capture data with markers and infrared reflective

167
00:13:29,440 --> 00:13:31,520
lights and so on.

168
00:13:31,520 --> 00:13:34,320
So very much a lab setup.

169
00:13:34,320 --> 00:13:39,400
And many groups around the world have made small amounts of motion capture available.

170
00:13:39,400 --> 00:13:46,400
Every data set though is in a different format and researchers were struggling to get enough

171
00:13:46,400 --> 00:13:48,920
data to train deep neural networks.

172
00:13:48,920 --> 00:13:53,680
As we all know, deep networks are great and they could do a wonderful job, but they need

173
00:13:53,680 --> 00:13:56,360
a lot of data, they're data hungry.

174
00:13:56,360 --> 00:14:02,280
And so what we did was pool together a dozen different data sets and put them all into

175
00:14:02,280 --> 00:14:09,320
a single format, this representation of the human body we use called simple SMPL.

176
00:14:09,320 --> 00:14:17,120
And by unifying them all, it provides enough training data to really do something bigger.

177
00:14:17,120 --> 00:14:22,760
So Mohammed, you tell us a little bit about your kind of first steps in tackling this

178
00:14:22,760 --> 00:14:23,760
problem.

179
00:14:23,760 --> 00:14:24,760
Sure.

180
00:14:24,760 --> 00:14:29,880
So the first step in general is to create a baseline for your method.

181
00:14:29,880 --> 00:14:36,720
So what we did in the first step is to create a model that can only predict some pause

182
00:14:36,720 --> 00:14:43,520
and shape from a video without using any intricate or complicated architecture.

183
00:14:43,520 --> 00:14:49,880
So at that time, the first thing we tried, so there is a method called human mesh recovery,

184
00:14:49,880 --> 00:14:54,080
which estimates a pause and shape from only a single image.

185
00:14:54,080 --> 00:15:01,000
And we tried to extend this human mesh recovery to estimate a sequence of pauses given a

186
00:15:01,000 --> 00:15:04,800
video using a recurrent neural network architecture.

187
00:15:04,800 --> 00:15:11,720
So a recurrent neural network is basically a type of neural network that lets you integrate

188
00:15:11,720 --> 00:15:17,520
some information from past frames or past data points.

189
00:15:17,520 --> 00:15:24,280
So we take each output of CNNs from human mesh recovery method.

190
00:15:24,280 --> 00:15:31,000
And we try to estimate the sequence of pauses from this recurrent neural network architecture.

191
00:15:31,000 --> 00:15:36,440
This was the first thing that we tried to see how the model performs with the available

192
00:15:36,440 --> 00:15:37,960
data.

193
00:15:37,960 --> 00:15:45,440
And in the next step, we tried to integrate Amaz using a genitive error cell network approach.

194
00:15:45,440 --> 00:15:52,320
So imagine we have this baseline method that produces some noisy or inaccurate estimations

195
00:15:52,320 --> 00:15:54,640
of the motion happening.

196
00:15:54,640 --> 00:16:01,240
And if we integrate a discriminator to the training stage, which tells if the predicted

197
00:16:01,240 --> 00:16:09,640
motion is plausible or similar to real life motion, then we can supervise this baseline

198
00:16:09,640 --> 00:16:14,000
method to produce better looking motions.

199
00:16:14,000 --> 00:16:18,960
And this is what we did in the final stages, where we introduced Amaz data to set and

200
00:16:18,960 --> 00:16:23,800
the discriminator, which supervises this baseline generator we had.

201
00:16:23,800 --> 00:16:28,000
So let's go back to the initial stage.

202
00:16:28,000 --> 00:16:34,160
You've got a CNN whose output you're feeding into an RNN.

203
00:16:34,160 --> 00:16:37,200
What part of the CNN are you feeding into the RNN?

204
00:16:37,200 --> 00:16:43,760
Is it kind of the output of a final classifier stage or is it some earlier layer in the

205
00:16:43,760 --> 00:16:48,280
CNN that you're using as input to your RNN?

206
00:16:48,280 --> 00:16:52,760
So let me first summarize the human mesh recovery architecture.

207
00:16:52,760 --> 00:16:59,160
So we have a CNN that extracts the image features and then a regressor, which is a couple

208
00:16:59,160 --> 00:17:03,440
of NLP layers that estimates simple body pose and shape parameters.

209
00:17:03,440 --> 00:17:07,960
So the CNN part is fully convolution, a fully convolutional model.

210
00:17:07,960 --> 00:17:13,840
And so we get rid of the regressor part to train the wipe and we only use the CNN part

211
00:17:13,840 --> 00:17:17,280
to extract the image features of the paper frame.

212
00:17:17,280 --> 00:17:22,000
And we feed these image features to recurrent neural network we have.

213
00:17:22,000 --> 00:17:29,120
And you talked about the discriminator is the discriminator trained separately or kind

214
00:17:29,120 --> 00:17:33,240
of end to end as part of training the CNN.

215
00:17:33,240 --> 00:17:38,480
Yeah, so actually the generator generates some predictions and we train the whole thing

216
00:17:38,480 --> 00:17:39,800
end to end.

217
00:17:39,800 --> 00:17:47,840
So the discriminator takes as inputs the generators predictions of the pose and shape and it takes

218
00:17:47,840 --> 00:17:50,000
also samples from a mass.

219
00:17:50,000 --> 00:17:57,160
So a mass is exactly what Michael previously explained as the unified motion dataset.

220
00:17:57,160 --> 00:18:03,840
So we use the discriminator to validate if the sequence of poses that we have is plausible.

221
00:18:03,840 --> 00:18:08,680
It can actually be a real human motion.

222
00:18:08,680 --> 00:18:15,640
So the discriminator's job is by taking those two different pose sequences to compare

223
00:18:15,640 --> 00:18:20,640
them and refine the generators output sequence.

224
00:18:20,640 --> 00:18:25,960
So we train this whole thing end to end and the discriminators takes the real fake samples

225
00:18:25,960 --> 00:18:28,600
from a mass and that was generator.

226
00:18:28,600 --> 00:18:34,000
When I think about the in this picture that you created for me Michael with the folks

227
00:18:34,000 --> 00:18:41,880
and suits with the balls that process and those images, I think of those as kind of fairly

228
00:18:41,880 --> 00:18:48,000
sparse points on the body that are being captured and yet these models are, you know, they

229
00:18:48,000 --> 00:18:53,400
look to be like fairly high, you know, resolution, fairly fine grained.

230
00:18:53,400 --> 00:19:02,240
How does the model go from kind of this sparse motion capture to more robust looking mesh?

231
00:19:02,240 --> 00:19:05,440
Yeah, great question.

232
00:19:05,440 --> 00:19:06,640
That's a whole nother paper.

233
00:19:06,640 --> 00:19:13,880
So indeed, when you capture a motion capture lab, you've got a bunch of sparse markers.

234
00:19:13,880 --> 00:19:21,040
And then the traditional method solves for a skeleton that explains those markers.

235
00:19:21,040 --> 00:19:25,960
Now we replace that traditional solve with a different kind of solve.

236
00:19:25,960 --> 00:19:29,160
We actually take our 3D body model.

237
00:19:29,160 --> 00:19:34,280
This is the simple body model and fit it to the marker data in a using a process called

238
00:19:34,280 --> 00:19:37,600
Mosh for motion and shape capture.

239
00:19:37,600 --> 00:19:43,720
And this was a paper that appeared at Cigraf or Cigraf Asia a few years ago and does a really

240
00:19:43,720 --> 00:19:49,880
good job of taking sparse motion capture markers and fitting the most likely body surface

241
00:19:49,880 --> 00:19:51,880
that could explain them.

242
00:19:51,880 --> 00:19:59,960
It turns out that from 20 to 60 sparse markers on the body, you can really figure out what

243
00:19:59,960 --> 00:20:01,080
a person looks like.

244
00:20:01,080 --> 00:20:05,840
You could think about it as a very, very sparse 3D scan of the person.

245
00:20:05,840 --> 00:20:11,680
And with a powerful statistical model of the body, it's about how body shape varies and

246
00:20:11,680 --> 00:20:17,080
how pose varies, you can fit a sequence of these very sparse markers and get out this

247
00:20:17,080 --> 00:20:21,800
kind of realistic detail that we can then use to train other methods.

248
00:20:21,800 --> 00:20:28,720
Maybe, Mohammed, you can help me get to the like the core element of this paper that

249
00:20:28,720 --> 00:20:33,120
helps it perform beyond the previous method.

250
00:20:33,120 --> 00:20:39,400
What's kind of that kernel of contribution here, innovation here that helps get to that

251
00:20:39,400 --> 00:20:43,680
incremental performance and how did you arrive at that?

252
00:20:43,680 --> 00:20:47,960
Was that obvious setting out at this project and you just kind of put the pieces in place

253
00:20:47,960 --> 00:20:53,800
and it all worked or was there a evolutionary process here that helped you arrive at the

254
00:20:53,800 --> 00:20:55,680
final architecture?

255
00:20:55,680 --> 00:21:02,360
So the major contribution in this paper is this motion discriminator, which tells if the

256
00:21:02,360 --> 00:21:05,480
predicted sequences are fake or real.

257
00:21:05,480 --> 00:21:12,480
So this is the major contribution we have and this is the one that we get the most improvement

258
00:21:12,480 --> 00:21:15,600
in the results in the results.

259
00:21:15,600 --> 00:21:23,680
So actually, it is quite, so from the human mesh recovery paper from Anuju Kanazawa, we

260
00:21:23,680 --> 00:21:30,760
already know that using this kind of a discriminator for single image pose estimation helps a lot

261
00:21:30,760 --> 00:21:33,680
to have plausible bodies.

262
00:21:33,680 --> 00:21:40,080
So and we thought that intuitively this should help for a sequence of poses.

263
00:21:40,080 --> 00:21:45,280
And we already know that Amaz is a very large scale data set, which has different kind

264
00:21:45,280 --> 00:21:51,360
of motions, which happens in real time and performed by the real human actors.

265
00:21:51,360 --> 00:21:57,960
And this intuitive reasoning led us to create this architecture.

266
00:21:57,960 --> 00:22:04,800
So yeah, during the development phase, we had a lot of technical difficulties and we just

267
00:22:04,800 --> 00:22:11,280
get through them by doing like fine-grained experiments on which part improves, which

268
00:22:11,280 --> 00:22:14,040
is led us to architecture we have.

269
00:22:14,040 --> 00:22:19,400
What were some of those technical difficulties and experiments that helped you overcome them?

270
00:22:19,400 --> 00:22:27,080
Yeah, for example, tuning the, we have this discriminator and generator and we have this

271
00:22:27,080 --> 00:22:34,960
discriminator losses that tells if this sequence is real or fake and also some other losses

272
00:22:34,960 --> 00:22:42,480
that tells the 2D joint locations and 3D joint locations of a given input video.

273
00:22:42,480 --> 00:22:51,760
So we had to tune these 2 kind of losses to every on an optimal solution.

274
00:22:51,760 --> 00:22:56,600
This was one of the things that we really take.

275
00:22:56,600 --> 00:23:01,800
Maybe it's worth telling the audience also that one of the problems here is we want to

276
00:23:01,800 --> 00:23:08,480
get 3D human poses in detail and yet there's not much ground truth training data where

277
00:23:08,480 --> 00:23:16,800
you have video sequences with the true 3D human pose in correspondence.

278
00:23:16,800 --> 00:23:23,600
And there are large data sets of labeled 2D human poses, but not 3D.

279
00:23:23,600 --> 00:23:30,600
And so this is one way to combine sort of weak 2D information with very strong 3D information.

280
00:23:30,600 --> 00:23:31,800
They're not paired.

281
00:23:31,800 --> 00:23:34,080
So a mass doesn't have any images.

282
00:23:34,080 --> 00:23:36,560
It's not associated with any real videos.

283
00:23:36,560 --> 00:23:41,320
But the discriminator learns what it is to be a motion and then the generator learns

284
00:23:41,320 --> 00:23:44,880
how to go from images to 3D.

285
00:23:44,880 --> 00:23:46,360
Yeah, exactly.

286
00:23:46,360 --> 00:23:52,560
Actually, that's what the high overview of the paper is we get the 2D data.

287
00:23:52,560 --> 00:23:59,320
We output some pose sequences and then we use that huge data set a mass that has various

288
00:23:59,320 --> 00:24:05,560
motions to refine those predictions to be able to not, between the two sequences, one

289
00:24:05,560 --> 00:24:13,640
from a mass and the hour sequence, our generated sequence, the discriminator ideally should

290
00:24:13,640 --> 00:24:15,960
not distinguish between those two.

291
00:24:15,960 --> 00:24:22,920
So the discriminator should be 50% sure about which one is the real and which one is the

292
00:24:22,920 --> 00:24:23,920
fake.

293
00:24:23,920 --> 00:24:24,920
And that's a great point.

294
00:24:24,920 --> 00:24:30,560
So the discriminator is operating purely in the motion domain.

295
00:24:30,560 --> 00:24:36,800
It doesn't know anything about images or the frames of the video.

296
00:24:36,800 --> 00:24:42,120
It's just answering the question, does this seem like the kind of motion that the human

297
00:24:42,120 --> 00:24:49,160
body does based on the mass data set and the CNN and the other parts of the architecture

298
00:24:49,160 --> 00:24:54,360
are more focused on extracting motion from the video frames?

299
00:24:54,360 --> 00:24:57,000
Yeah, exactly.

300
00:24:57,000 --> 00:24:58,000
Cool.

301
00:24:58,000 --> 00:25:07,400
And so I think one of the points that you make early on in the paper is that the motion

302
00:25:07,400 --> 00:25:12,440
that, and even the examples that you give, it's not like normal motion.

303
00:25:12,440 --> 00:25:21,200
It's kind of complex motion that is not something that you see every day is that was the

304
00:25:21,200 --> 00:25:30,840
mass data set kind of curated to identify and include lots of examples of complex motion

305
00:25:30,840 --> 00:25:36,160
in it or is there something in the process that is kind of extrapolating to that kind

306
00:25:36,160 --> 00:25:37,360
of complex motion?

307
00:25:37,360 --> 00:25:38,360
Okay.

308
00:25:38,360 --> 00:25:40,160
It's a good question.

309
00:25:40,160 --> 00:25:41,680
There's a mixture in there.

310
00:25:41,680 --> 00:25:44,240
There's a lot of mundane things like walking.

311
00:25:44,240 --> 00:25:48,760
It seems like in motion capture labs, people capture a lot of walking.

312
00:25:48,760 --> 00:25:54,200
So, but there are some data sets that are a little bit more extreme, have more interesting

313
00:25:54,200 --> 00:25:55,200
poses.

314
00:25:55,200 --> 00:26:01,440
And in fact, we captured something we call extreme poses, which we hired some gymnasts

315
00:26:01,440 --> 00:26:07,640
to come in and do basically the wildest things that the human body can do, just to really

316
00:26:07,640 --> 00:26:10,800
flesh out the space of what's possible.

317
00:26:10,800 --> 00:26:14,920
And there's one other thing that happens in motion capture a lot for video games, people

318
00:26:14,920 --> 00:26:22,520
often capture something like kicking a ball and turning left or turning right in very discreet

319
00:26:22,520 --> 00:26:24,320
movements.

320
00:26:24,320 --> 00:26:28,440
And so we have something called a transition's data set in there as well that captures

321
00:26:28,440 --> 00:26:32,280
people doing one movement and then transitioning to another movement.

322
00:26:32,280 --> 00:26:38,480
So this is again trying to expand these transitions are things that you might not think to capture

323
00:26:38,480 --> 00:26:42,080
on your own, but are really part of the natural human behavior.

324
00:26:42,080 --> 00:26:49,480
So there's a mixture in there and it's rich enough for this task.

325
00:26:49,480 --> 00:26:54,720
Are there specific things that Mohamed Arniko having worked with this data set for this

326
00:26:54,720 --> 00:27:02,600
particular task jump out at you as, you know, I wish the data set had more of, you know,

327
00:27:02,600 --> 00:27:07,840
kind of category X because it would help you build more robust models?

328
00:27:07,840 --> 00:27:15,440
Yeah, basically that a mass data set has almost all of the available mocap data by different

329
00:27:15,440 --> 00:27:21,280
labs captured and in different conditions, but it doesn't have a lot of in the wild data.

330
00:27:21,280 --> 00:27:27,640
That's the thing that it's missing the extreme poses and the in the wild kind of data.

331
00:27:27,640 --> 00:27:37,280
So someone doing extreme gymnastic exercises or having like motion without a constant pace

332
00:27:37,280 --> 00:27:43,920
and having different poses and like opening the hands or the feet extremely quickly or

333
00:27:43,920 --> 00:27:52,000
slowly or I mean, we could always benefit from more variation of extreme and quick or fast

334
00:27:52,000 --> 00:27:53,000
motions.

335
00:27:53,000 --> 00:27:54,000
Yeah.

336
00:27:54,000 --> 00:27:59,160
Now that we are starting to have, you know, I'm thinking like sporting events that are

337
00:27:59,160 --> 00:28:05,720
captured with many cameras from lots of different angles, does that ever able to take the

338
00:28:05,720 --> 00:28:11,920
place of motion capture types of information or are we not kind of sophisticated enough

339
00:28:11,920 --> 00:28:19,320
to correlate all of that and and produce the kind of data sets that kind of traditional

340
00:28:19,320 --> 00:28:21,320
motion capture is able to provide.

341
00:28:21,320 --> 00:28:23,760
Have you looked at that Michael at all?

342
00:28:23,760 --> 00:28:29,040
Yeah, so it's a let's come back to this idea that, you know, traditional motion captures

343
00:28:29,040 --> 00:28:34,040
a bunch of sparse points on the body, not very detailed.

344
00:28:34,040 --> 00:28:39,320
With the images, we've got thousands, hundreds of thousands of points on the body.

345
00:28:39,320 --> 00:28:43,640
It just happened to be in 2D, but they are much richer in some sense.

346
00:28:43,640 --> 00:28:49,200
We have all this facial expression information, details of the hands, subtle, you know,

347
00:28:49,200 --> 00:28:51,320
subtle motions of the body breathing and so on.

348
00:28:51,320 --> 00:28:53,520
You can see all that in a video.

349
00:28:53,520 --> 00:29:00,880
So our hope is, you know, one day that motion capture from video will be more accurate

350
00:29:00,880 --> 00:29:06,000
and more detailed than motion capture from a traditional mocap system today.

351
00:29:06,000 --> 00:29:07,000
We're not there yet.

352
00:29:07,000 --> 00:29:09,240
This is a step in that direction.

353
00:29:09,240 --> 00:29:13,280
I think your intuition about combining multiple views is a good one and it's something

354
00:29:13,280 --> 00:29:18,080
that we're we've looked at in the past and continue to look at.

355
00:29:18,080 --> 00:29:25,680
Can we combine multiple 2D views of a person and get, you know, really high quality output?

356
00:29:25,680 --> 00:29:26,920
I think the answer is yes.

357
00:29:26,920 --> 00:29:33,480
I haven't seen a system yet that competes with the most accurate mocap systems.

358
00:29:33,480 --> 00:29:37,840
But then the most accurate mocap systems, you know, cost hundreds of thousands of dollars

359
00:29:37,840 --> 00:29:40,440
and have dozens of cameras.

360
00:29:40,440 --> 00:29:48,000
I think if you built a video based mocap system of sort of that sophistication, I think

361
00:29:48,000 --> 00:29:50,120
we might be getting close.

362
00:29:50,120 --> 00:29:54,760
And does light are types of systems or like what we've seen with the connect, does that

363
00:29:54,760 --> 00:29:56,360
play into this as well?

364
00:29:56,360 --> 00:29:57,360
Yeah.

365
00:29:57,360 --> 00:29:58,360
Absolutely.

366
00:29:58,360 --> 00:29:59,360
We also work with connect.

367
00:29:59,360 --> 00:30:06,200
The new Microsoft connect for Azure is allows you to put multiple connects together.

368
00:30:06,200 --> 00:30:09,120
So you can actually have several of them looking at a person.

369
00:30:09,120 --> 00:30:13,760
That's a really promising direction for a lightweight, easy to set up system.

370
00:30:13,760 --> 00:30:19,720
We don't have anything to show yet, but yeah, cool, cool.

371
00:30:19,720 --> 00:30:26,280
So Muhammad again, kind of going back to this opening picture in the paper, you know, it's

372
00:30:26,280 --> 00:30:33,720
easy to see where your model is outperforming the traditional model and kind of eyeball the

373
00:30:33,720 --> 00:30:34,720
delta.

374
00:30:34,720 --> 00:30:42,160
How do you evaluate that performance more concretely and mathematically in the paper?

375
00:30:42,160 --> 00:30:48,400
So we have several datasets that has ground through 3D joint positions.

376
00:30:48,400 --> 00:30:54,880
And one of them is a dataset called 3D pauses in the wild again, that is developed in our

377
00:30:54,880 --> 00:30:57,240
group in person systems group.

378
00:30:57,240 --> 00:31:02,440
So this dataset is captured in the wild.

379
00:31:02,440 --> 00:31:09,120
And there are some subjects varying some I am I am you sensors on their body.

380
00:31:09,120 --> 00:31:12,960
And so this I am you sensors can be placed under the cloth.

381
00:31:12,960 --> 00:31:17,160
So it doesn't affect the image quality.

382
00:31:17,160 --> 00:31:22,640
And so you can get the 3D joint positions from those I am you sensors.

383
00:31:22,640 --> 00:31:29,920
And we the major that the most prominent dataset we try is this 3D PW 3D people in the wild

384
00:31:29,920 --> 00:31:31,400
dataset.

385
00:31:31,400 --> 00:31:39,040
And so basically what we do is we take the sequences from the dataset, produces our results,

386
00:31:39,040 --> 00:31:44,560
and also the other methods results and compares what is the distance between the ground through

387
00:31:44,560 --> 00:31:49,080
the joint positions and the predicted ground to the joint positions.

388
00:31:49,080 --> 00:31:54,840
And also there are some other indoor 3D datasets we also use.

389
00:31:54,840 --> 00:32:01,240
One of them is called human 3.6M and the other one is called MPI informatics 3D human

390
00:32:01,240 --> 00:32:03,320
position data set.

391
00:32:03,320 --> 00:32:09,520
The 3D poses in the wild dataset is that you mentioned it uses a particular kind of

392
00:32:09,520 --> 00:32:15,680
sensor is that sensor and the kind of sparsity of that sensor similar to what you see in

393
00:32:15,680 --> 00:32:23,760
a mocap system or is it you know somehow more more dense I'm curious about the relationship

394
00:32:23,760 --> 00:32:31,720
between the primary way your comparing performance is based on kind of these sparse representations

395
00:32:31,720 --> 00:32:39,320
I wonder if that leaves something to be desired or an opportunity in terms of the

396
00:32:39,320 --> 00:32:45,240
ability to kind of capture what's happening in complex motions with these sparse data

397
00:32:45,240 --> 00:32:46,560
points on the body.

398
00:32:46,560 --> 00:32:48,240
It's a good question.

399
00:32:48,240 --> 00:32:54,400
So we use their commercial mocap suit called it from accents uses these inertial measurement

400
00:32:54,400 --> 00:32:55,400
units.

401
00:32:55,400 --> 00:33:01,680
I think there's 10 or 12 on the body, but we don't just use that.

402
00:33:01,680 --> 00:33:09,040
We also use a camera tracking the people around and we use 2D measurements about the joint

403
00:33:09,040 --> 00:33:17,480
locations in those images to precisely align the estimated 3D body pose with the images.

404
00:33:17,480 --> 00:33:26,320
And so we have a technique that fits again our simple SMPL body model to these multiple

405
00:33:26,320 --> 00:33:30,240
kinds of measurements the IMU measurements and the 2D measurements.

406
00:33:30,240 --> 00:33:35,160
We also have to solve for the camera translation and rotation and things like that.

407
00:33:35,160 --> 00:33:39,920
But putting this all together gives reasonable we don't call it ground truth we call it reference

408
00:33:39,920 --> 00:33:40,920
data.

409
00:33:40,920 --> 00:33:45,720
We're a little careful about that you know it's not the same level as a motion capture

410
00:33:45,720 --> 00:33:47,440
lab.

411
00:33:47,440 --> 00:33:52,440
But you get the benefit of being outside in in the wild with all the complex lighting

412
00:33:52,440 --> 00:33:58,160
and occlusion and things that go on in you know in natural scenarios.

413
00:33:58,160 --> 00:33:59,160
Okay.

414
00:33:59,160 --> 00:34:03,520
It's a bit of a trade off it's the holy grail is to have perfect ground truth where you

415
00:34:03,520 --> 00:34:09,880
know every single bit of the motion with no crazy sensors to get in the way outdoors in

416
00:34:09,880 --> 00:34:14,200
the wild but it you know it doesn't exist.

417
00:34:14,200 --> 00:34:19,680
So mom and mentioned earlier you know this process of kind of experimenting and identifying

418
00:34:19,680 --> 00:34:24,760
you know things that didn't work with particular emphasis on the discriminator Nika someone

419
00:34:24,760 --> 00:34:30,000
ring you know from your perspective you know if you can share another example of something

420
00:34:30,000 --> 00:34:36,120
that maybe thought was gonna work and didn't and how you had to adjust the approach to accommodate

421
00:34:36,120 --> 00:34:37,120
it.

422
00:34:37,120 --> 00:34:44,440
Yeah the experimentation procedure was was tough but yeah there are some a few points

423
00:34:44,440 --> 00:34:52,440
so we tried one minor point was that we tried the difficult CNN feature extractors and

424
00:34:52,440 --> 00:34:58,520
that was a difficult part to experiment with but we quickly converts to an optimal one

425
00:34:58,520 --> 00:35:07,320
and the other one is that we decided to use a self attention mechanism which is like

426
00:35:07,320 --> 00:35:13,600
a major kind of a major contribution because actually the self attention mechanism is not

427
00:35:13,600 --> 00:35:19,840
so commonly used in 3D modeling or modeling of human motion so we use the self attention

428
00:35:19,840 --> 00:35:28,480
mechanism to get even better results on the discriminator part which is basically imagine

429
00:35:28,480 --> 00:35:34,840
that we are processing the human motion sequence and we are extracting some features using

430
00:35:34,840 --> 00:35:40,520
2G or U layers which is a type of very current neural network as Mohamed said before and

431
00:35:40,520 --> 00:35:46,040
instead of hard pulling or combining these features statically we are using a weighted

432
00:35:46,040 --> 00:35:53,880
schema to combine those two those features so the features that we have for every pose

433
00:35:53,880 --> 00:36:00,240
in the sequence we combine them as a weighted sum in order to amplify the contribution

434
00:36:00,240 --> 00:36:07,040
of most important frames of the sequence more for the discriminator's performance to improve

435
00:36:07,040 --> 00:36:14,680
so that's what was too key technical difficulties that we have.

436
00:36:14,680 --> 00:36:22,240
Were you able to develop any kind of intuition over why that worked and what the attention

437
00:36:22,240 --> 00:36:26,080
mechanism ended up attending over?

438
00:36:26,080 --> 00:36:33,320
We have made a population experiments that we tested the self attention mechanism so

439
00:36:33,320 --> 00:36:39,200
we tried two different things so instead of using self attention we trying pulling the

440
00:36:39,200 --> 00:36:46,680
features in a static way so concatenating the average and the max of the features and

441
00:36:46,680 --> 00:36:53,120
using that for the discriminator to decide whether the motion is fake or real and we

442
00:36:53,120 --> 00:37:00,280
compared with that case with our case and the performance seemed to improve.

443
00:37:00,280 --> 00:37:06,400
It didn't improve in such a margin that the results were extremely visible but for sure

444
00:37:06,400 --> 00:37:12,440
attention pointed out the frames that they should be corrected in the pose sequence and

445
00:37:12,440 --> 00:37:15,240
that they were the more plausible ones.

446
00:37:15,240 --> 00:37:23,520
I think about the various pieces of the model that you've built here, you've got CNN,

447
00:37:23,520 --> 00:37:31,160
you've got Narin and you've got this attention mechanism, you mentioned VAE in the paper.

448
00:37:31,160 --> 00:37:37,000
It seems like a lot of moving pieces and I'm wondering how that impacted the process

449
00:37:37,000 --> 00:37:44,640
and also from a computational perspective if the training runs were very expensive.

450
00:37:44,640 --> 00:37:48,280
You're also working with video which is a lot of data.

451
00:37:48,280 --> 00:37:55,480
How was the process of working on this from a just computational perspective?

452
00:37:55,480 --> 00:37:59,680
Did that have a lot of impact on the way you approach things, Mohammed?

453
00:37:59,680 --> 00:38:06,080
So surprisingly the model we have both in training time and testing time is quite lightweight

454
00:38:06,080 --> 00:38:09,200
even though we have lots of components.

455
00:38:09,200 --> 00:38:14,320
So the heaviest part in our model is the CNN future extraction part.

456
00:38:14,320 --> 00:38:20,400
And what we do during training, we pre-compure the CNN features and we don't update this

457
00:38:20,400 --> 00:38:26,520
CNN module and we only train the recurrent neural network and since the number of parameters

458
00:38:26,520 --> 00:38:36,200
is quite low with the simple body representation, our neural network is quite lightweight.

459
00:38:36,200 --> 00:38:46,440
And we don't use very deep layers, only a couple of a stack of GRU layers is enough for us

460
00:38:46,440 --> 00:38:50,960
and it wasn't that difficult for us to train the model.

461
00:38:50,960 --> 00:38:57,520
So actually during training time for example, like in half an hour or in one hour we can

462
00:38:57,520 --> 00:39:04,600
converge to a good solution since we use this already pre-computed CNN features.

463
00:39:04,600 --> 00:39:10,760
And during training time we also get rid of the discriminator and we only use the generator

464
00:39:10,760 --> 00:39:15,120
part and it makes the testing much faster.

465
00:39:15,120 --> 00:39:20,160
Okay and then what about from an inference perspective, how expensive is that?

466
00:39:20,160 --> 00:39:27,720
Again, it is not quite expensive since we only have the CNN which is resident 50 in our

467
00:39:27,720 --> 00:39:34,080
case, which is again lightweight and it can run in real time in GPUs.

468
00:39:34,080 --> 00:39:38,800
And the GRU layers we add, it is not again that expensive.

469
00:39:38,800 --> 00:39:47,480
So in a commercial GPU, we can get almost real time speeds during the experience.

470
00:39:47,480 --> 00:39:48,480
Very cool.

471
00:39:48,480 --> 00:39:56,320
Maybe start to wrap things up, I love for each of you to kind of put this paper and

472
00:39:56,320 --> 00:40:00,920
maybe talk about the things that you're either your top lessons learned or the thing

473
00:40:00,920 --> 00:40:04,640
that you're most excited about with this paper and its contributions and kind of how

474
00:40:04,640 --> 00:40:09,040
you see it moving forward in your own research.

475
00:40:09,040 --> 00:40:11,040
Nikos, do you want to start us off with that?

476
00:40:11,040 --> 00:40:12,480
Yeah, I can start.

477
00:40:12,480 --> 00:40:19,880
So I joined the MBI in September and I didn't know a lot of things for 3D human modeling.

478
00:40:19,880 --> 00:40:30,880
So and this paper is a very good first experience and I think one of the most important

479
00:40:30,880 --> 00:40:34,040
features of the paper is that it is clear.

480
00:40:34,040 --> 00:40:43,080
I mean the contributions are clear, we use a lot of unpaired 3D data to refine 2D predictions

481
00:40:43,080 --> 00:40:50,880
actually to the predictions, actually 3D meshes that were predicted mostly from 2D key

482
00:40:50,880 --> 00:40:52,680
points.

483
00:40:52,680 --> 00:41:03,040
And our implementation is converges fast, it can create high quality and state of the

484
00:41:03,040 --> 00:41:12,080
art, the predictions are high quality and state of the art.

485
00:41:12,080 --> 00:41:21,960
And also one important thing is that we've made all the code and data publicly available

486
00:41:21,960 --> 00:41:29,480
so we have detailed instructions for everyone so it's pretty easy for anyone if you want

487
00:41:29,480 --> 00:41:37,080
or see wants to run and use our model and methods, we have clear instructions to do

488
00:41:37,080 --> 00:41:38,080
it.

489
00:41:38,080 --> 00:41:39,080
Great.

490
00:41:39,080 --> 00:41:40,080
Mohammed?

491
00:41:40,080 --> 00:41:46,200
So it seems that why we're working quite well on the restricted data sets for evaluation

492
00:41:46,200 --> 00:41:53,840
we have, but in real life or in the wild videos there are a lot of difficult situations

493
00:41:53,840 --> 00:42:00,960
and a human motion is quite difficult to model because humans are capable of doing lots

494
00:42:00,960 --> 00:42:08,320
of different kind of movements and also in real life situations we have like objects

495
00:42:08,320 --> 00:42:14,920
and scenes around us, we have occlusions caused by again objects or other people.

496
00:42:14,920 --> 00:42:21,680
These kind of problems are still waiting to be tackled with and also we show with

497
00:42:21,680 --> 00:42:33,680
wipe, we cannot perform well in these kind of settings and in the future there are lots

498
00:42:33,680 --> 00:42:40,200
of things that we have to do to model human motion and also track and capture human motion

499
00:42:40,200 --> 00:42:45,560
from videos and this is what I learned in the process even though we have the state of

500
00:42:45,560 --> 00:42:50,680
the art model we cannot solve even some basic scenarios still.

501
00:42:50,680 --> 00:42:51,680
Great.

502
00:42:51,680 --> 00:42:52,680
Thank you.

503
00:42:52,680 --> 00:42:53,680
Michael?

504
00:42:53,680 --> 00:42:59,680
Well, I'm really excited about this paper and we're taking this but I'm always thinking

505
00:42:59,680 --> 00:43:07,800
about the next thing and humans don't move for no reason, they don't move in a vacuum,

506
00:43:07,800 --> 00:43:13,480
they move in the 3D world to solve tasks and to change that world to have an influence

507
00:43:13,480 --> 00:43:14,480
on it.

508
00:43:14,480 --> 00:43:17,880
You cross the room to open the door, you open the door to move through it, you move

509
00:43:17,880 --> 00:43:22,800
through it to get to the next room, you're solving tasks all the time and in solving those

510
00:43:22,800 --> 00:43:26,520
tasks you're interacting with the environment.

511
00:43:26,520 --> 00:43:32,680
Even if you're interacting with another person you're moving your face and you're gesturing

512
00:43:32,680 --> 00:43:37,720
in ways that try to have an influence on the other person, maybe without any contact

513
00:43:37,720 --> 00:43:44,640
but you're still using your body to influence people or the world and this work at the

514
00:43:44,640 --> 00:43:47,360
moment just treats the body in isolation.

515
00:43:47,360 --> 00:43:52,720
The body isn't embedded in the environment, the CNN doesn't really know anything about

516
00:43:52,720 --> 00:43:56,960
the environment, it's a single person not interacting with other people.

517
00:43:56,960 --> 00:44:01,600
I think there's a tremendous amount we still have to do to get computers to really understand

518
00:44:01,600 --> 00:44:09,400
us and those next steps will be about human-human interaction and about human-object-human-seeing

519
00:44:09,400 --> 00:44:10,400
interaction.

520
00:44:10,400 --> 00:44:11,400
Great.

521
00:44:11,400 --> 00:44:18,440
Well, Mohamed Niko's, Michael, thanks so much for taking the time to share with us your

522
00:44:18,440 --> 00:44:20,840
paper and what you're up to.

523
00:44:20,840 --> 00:44:21,840
Thanks a lot.

524
00:44:21,840 --> 00:44:22,840
Thanks.

525
00:44:22,840 --> 00:44:28,520
All right, everyone, that's our show for today.

526
00:44:28,520 --> 00:44:34,480
To learn more about today's guest or the topics mentioned in this interview, visit twimmolai.com.

527
00:44:34,480 --> 00:44:39,480
Of course, if you like what you hear on the podcast, please subscribe, rate, and review

528
00:44:39,480 --> 00:44:42,200
the show on your favorite pod catcher.

529
00:44:42,200 --> 00:45:08,960
Thanks so much for listening and catch you next time.

