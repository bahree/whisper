WEBVTT

00:00.000 --> 00:11.720
All right, everyone. I am here with Pavan Taraga. Pavan is jointly appointed in the schools

00:11.720 --> 00:18.240
of electrical engineering and arts, media, and engineering at Arizona State University.

00:18.240 --> 00:22.640
Pavan, welcome to the Twomo AI podcast. Thank you, Sam. It's a pleasure and an honor

00:22.640 --> 00:27.680
to be here. Thank you. It is my pleasure to host you here. I'm looking forward to digging

00:27.680 --> 00:34.320
into your paper that will be presented at CVPR, revisiting invariance with geometry

00:34.320 --> 00:38.880
and deep learning. But before we do that, I'd love for you to share a little bit about your

00:38.880 --> 00:46.720
background and how you came to work in computer vision and ML. Sounds great. So my beginnings

00:46.720 --> 00:54.480
in this area started as a senior in my undergraduate years. I was looking at problems like

00:54.480 --> 01:00.160
face recognition, face tracking from video, track to do a senior design project really. And

01:00.160 --> 01:07.760
as I started digging more, this was in the early or yeah, early 2000s. And it was a very exciting

01:07.760 --> 01:14.400
time to be in the field of computer vision because the problem statement in those days would

01:14.400 --> 01:20.560
be presented as giving a computer the ability to see and that felt like wow, that's a frontier

01:20.560 --> 01:27.200
topic, right? Yeah. And I said, you know, looks like I'm sufficiently invested in this, looks

01:27.200 --> 01:32.240
like it intersects with things like neuroscience, perception that is interesting mathematics,

01:32.240 --> 01:38.080
that is interesting computing happening, very highly interdisciplinary and it felt like there

01:38.080 --> 01:44.880
was much work to be done. So I decided to go to school in 2000, I mean grad school in 2004

01:44.880 --> 01:52.400
to the University of Maryland studying with Professor Ramachalappa who is a pioneer in this field

01:52.400 --> 01:58.080
and very well known for face recognition techniques and a lot of interesting things in vision at the

01:58.080 --> 02:06.640
time. And as I started my work in the lab, I broadened from just face recognition to other things

02:06.640 --> 02:13.280
like understanding video, understanding time series, understanding the role of light, you know,

02:13.280 --> 02:21.280
geometry, elimination, reflectance, all these physics-based concepts and how they interface with

02:21.280 --> 02:27.120
pattern recognition methods or machine learning methods. And as I went deeper and deeper into it,

02:27.120 --> 02:34.160
I felt like there was a big disconnect between the methods of physics of image formation

02:35.040 --> 02:41.120
and the methods that are used in machine learning where it's just purely a driven and statistical

02:41.120 --> 02:47.280
techniques. And I was trying to find some middle ground where I could inject physical knowledge

02:47.280 --> 02:51.360
into certain structures that could blend well with machine learning techniques. And one thing

02:51.360 --> 02:57.120
led to another, I started getting interested in this area of mathematics called Rimanian geometry

02:57.120 --> 03:03.360
and then topology as a means to express these intuitions and these constraints from physics and

03:03.360 --> 03:09.520
interface them with deep learning or machine learning in before deep learning. So that's the theme of

03:09.520 --> 03:16.720
my work over the past decade, which is try to understand basic phenomena, whether it's, you know,

03:16.720 --> 03:23.920
images or video or human activity. And in recent years, we've also broadened our investigation

03:23.920 --> 03:29.520
beyond computer vision to include things like wearable devices and physiological monitoring where

03:29.520 --> 03:35.440
the phenomena under study is basic human movement and other things. Try to understand it from

03:35.440 --> 03:42.960
first principles and try to express that knowledge in a way that constrains conditions

03:43.760 --> 03:46.960
machine learning. So that's the general intersection of topics I've been looking at.

03:47.920 --> 03:53.200
Okay, and I mentioned in introducing you that one of your appointments is with a school that

03:53.200 --> 03:59.440
has arts in the title. How does that come up in your work and research?

03:59.440 --> 04:06.080
So you're particularly connected to that particular piece of that school.

04:06.080 --> 04:13.120
So that's a whole story by itself. And I can go very organically about how it all began or I can go

04:13.120 --> 04:23.920
in hindsight, this is how it went organically. This is how it began. Our school was founded in 2009

04:23.920 --> 04:30.080
and you know, I graduated from grad school in 2009. But then that was 2009, very similar to 2020,

04:30.080 --> 04:35.680
Wall Street collapsing and you can lose. So I stayed back when I posed or for a couple of years.

04:35.680 --> 04:42.160
And when I went interviewing in 2011, this position opened up in this school under the title

04:42.160 --> 04:49.520
of Assistant Professor in Human Activity Analysis. It was very intriguing. And I had been doing

04:49.520 --> 04:56.480
human activity analysis as far as, you know, understanding a video-based gate analysis.

04:56.480 --> 05:02.320
Okay, so I'm like fitness trackers and quantified self and all that kind of stuff.

05:02.320 --> 05:05.440
Right. So this was a little before all that stuff happened in particular.

05:06.640 --> 05:10.720
And I came in and I visited and there was there was a mind-boggling sense of

05:11.280 --> 05:16.240
interdisciplinary that I saw in the school people in music looking at stuff like that, right?

05:16.240 --> 05:22.000
accelerometers, variables, driving, interactive performance with it. There was a group doing

05:22.000 --> 05:26.960
stroke rehabilitation where you attract human movements through motion capture and

05:27.680 --> 05:33.760
sonify the qualities of your movements. So smoothness or jerkiness would be converted to sound

05:33.760 --> 05:39.440
and you can hear yourself. And that provided additional feedback for you to correct your movements.

05:39.440 --> 05:46.080
Oh wow. It was, yeah, it was a very mind-boggling experience. And even now,

05:46.800 --> 05:52.560
people find it very interviewing when I mentioned these things. So the human activity analysis component

05:53.920 --> 05:59.600
was a way for the school to address, you know, slightly more formal ways to think about human

05:59.600 --> 06:03.600
movement, whether it's sense from a camera, whether it's sense from a variable device, whether

06:03.600 --> 06:08.640
it's motion capture, try to come up with techniques to represent human movement. And then

06:08.640 --> 06:14.080
do machine learning on it or feed into these other applications. And it seemed like I was

06:14.080 --> 06:19.360
like Fritt at the time and I got a job. And I've been here for eight years and it has led me into

06:19.360 --> 06:25.680
very interesting collaborations with, yeah, media artists and health scientists who are interested

06:25.680 --> 06:32.560
in the intersection of computing art and things like health promotion. So that's the other side.

06:32.560 --> 06:41.520
Got it. Got it. You mentioned that one of the big themes of your work is been

06:43.520 --> 06:49.360
integrating physical, physics-based principles and computer vision.

06:51.360 --> 06:57.840
Can you maybe talk a little bit about that in the, you know, broader context of the

06:57.840 --> 07:01.840
computer vision landscape? We've talked about this quite a bit on the podcast and I kind of

07:01.840 --> 07:07.200
described it as a pendulum that's kind of swung from physics-based models to statistical and

07:07.200 --> 07:10.560
kind of is settling somewhere in the middle now. But it sounds like you've been working at this

07:10.560 --> 07:16.720
for a while. I'm curious how, how you think of it? Yes, I mean, you're absolutely right. It's

07:16.720 --> 07:20.960
been a pendulum that keeps swinging and I don't think it settles anywhere. I mean, that's

07:20.960 --> 07:32.000
interesting problem about computer vision. Every time someone thinks it settles, right? Everybody thinks it's

07:32.000 --> 07:36.240
settled now in the soonest chips. So, I mean, yes, it's been going on and off. This idea about,

07:36.240 --> 07:41.040
I mean, I like to think of it as model-based vision versus purely data-driven, you know,

07:41.040 --> 07:48.640
methods of thinking of vision. Yeah. The way the pendulum swings in my opinion is not that

07:48.640 --> 07:55.280
the problems are changing. It's just the language that goes into talking about it and the tools that

07:55.280 --> 08:02.400
go into addressing it keeps shifting, but the problems have remained mostly the same and the

08:02.400 --> 08:09.040
problems are the following in my opinion. So vision is a very unique, you know, some people think

08:09.040 --> 08:14.480
of it as an application of machine learning, which is a reductionistic way to think about it.

08:14.480 --> 08:19.600
Sure, everything is data and everything is fed into a model and outcomes of decision, but vision

08:19.600 --> 08:25.920
and any perceptual, you know, field of inquiry, that can include visions, sound,

08:25.920 --> 08:30.960
haptics, any of these things which have to do with perception, I feel are fundamentally different

08:31.680 --> 08:39.040
than any other application of data analysis. The way we perceive the world is not the way,

08:39.040 --> 08:44.160
let's say, back transactions are processed by a machine learning computer, you know, a machine

08:44.160 --> 08:51.520
learning technique. There is a huge amount of variability is the way I'd call it that exhibits

08:51.520 --> 09:00.560
in the natural world, which is somehow either discarded or properly parsed out by whatever's

09:00.560 --> 09:07.200
happening in our brains and the sources of variability are physics-based to a large extent, you know,

09:07.200 --> 09:11.120
the same picture under a different lighting condition looks different, the same picture under,

09:12.000 --> 09:17.280
you know, a different slightly changed viewpoint looks different. So that looking different part

09:17.280 --> 09:23.280
is what gives rise to statistical variability. And the statistical ways of thinking are, well,

09:23.280 --> 09:28.800
let's just fill up the observation space with more data points and we'll figure out what the

09:28.800 --> 09:35.440
shape of the distribution is from data, which is okay in as intense to infinity, I guess that's fine,

09:35.440 --> 09:40.640
but when data augmentation approaches and domain adaptation and that kind of thing,

09:40.640 --> 09:46.640
yeah, yeah, but under and not tend to infinity, if you only have a few data samples,

09:47.280 --> 09:54.000
you are better off trying to understand how the physics around you affects the observed

09:54.000 --> 10:01.040
imagery. And that's where I think the methods have shifted. So, you know, in the keynote that I have

10:01.040 --> 10:05.520
had CBPR, you know, at the workshop called differential geometry and computer vision, I go through

10:05.520 --> 10:11.280
some of these historical trends. And one of the core themes that brings it all together, the word

10:11.280 --> 10:17.280
that I use is called invariance, which is when you look around and try to classify objects,

10:17.280 --> 10:24.080
we are able to do this in a way that is invariant to a lot of nuisance variables. That is light,

10:24.080 --> 10:29.040
you know, shading viewpoint and also it's of interesting effects that are hard to describe.

10:29.040 --> 10:37.440
When you say invariance, there's two ways to think about it. The physics-based ways to think about

10:37.440 --> 10:43.200
it are, let's say I am looking at this scene, I know everything about this scene, including its

10:43.200 --> 10:48.640
3D geometry, including how the paint reflects off light, including the wavelengths in the incoming

10:48.640 --> 10:55.920
radiation. If I have full knowledge of all of this, then I can re-render a scene, let's say,

10:55.920 --> 11:03.040
I can, just like how it happens in graphics, I can create so many different versions of the same

11:03.040 --> 11:09.280
picture, if I had full knowledge of everything, simply through a forward rendering process and construct

11:09.280 --> 11:15.120
variability. The data driven ways of thinking about it say, if you don't have access to everything

11:15.120 --> 11:20.000
that you need to understand the phenomena, what is the minimal set, what is the minimal piece of

11:20.000 --> 11:26.880
information that is needed to get a job done. That's the dichotomy in the physics-based ways of

11:26.880 --> 11:35.440
thinking and the statistical ways of thinking. When you use this term invariant is the invariant

11:35.440 --> 11:41.440
referring to, say we're talking about a scene with an object in it that might render differently

11:41.440 --> 11:48.560
in different lighting conditions, etc. is the invariant that object that is definitively in the

11:48.560 --> 11:55.360
scene and then we've got all these other effects, or does the invariant refer to something else,

11:55.360 --> 11:58.800
maybe something more in the mathematical from a mathematical perspective.

12:00.480 --> 12:05.600
I mean, the word invariant, of course, will depend upon what the end task is. If it is object

12:05.600 --> 12:12.320
recognition, yes, something intrinsic to the object is the invariant. It is not always as simple

12:12.320 --> 12:17.120
as saying the invariant is the color of the object because that changes. It is often not

12:17.120 --> 12:22.160
same as saying the invariant is the edge map of the object or certain corners of the object

12:22.160 --> 12:28.800
because they go in and out of view. So there's not easy ways of describing what that invariant

12:28.800 --> 12:32.800
actually physically means. So it becomes mathematical at some level. There is no linguistic equivalent

12:32.800 --> 12:40.240
that I can come up with. But if you look at this rendering ways of thinking, if you can render

12:40.240 --> 12:44.240
this object that you're interested to recognize, in all possible wing conditions, all possible

12:44.240 --> 12:49.360
lighting conditions, and you have this huge set of pictures, that huge set of pictures

12:50.240 --> 12:56.640
could be called, you know, the word sometimes that gets used is equivalent class,

12:56.640 --> 13:02.080
or sometimes they call it an orbit. So this object that you're trying to recognize manifests

13:02.080 --> 13:08.080
itself in all these different ways. If you have a handle on that set, you are in good shape.

13:08.720 --> 13:13.200
If you have a different object and you place it in the same scene and you render it in all these

13:13.200 --> 13:19.280
different variations, and you have its own different set, then the invariant that separates these

13:19.280 --> 13:28.640
two is some measure of difference between these two sets of pictures. And sometimes that set of

13:28.640 --> 13:34.720
pictures can have a nice structure which can allow you to compute it in closed form and sometimes not.

13:35.600 --> 13:42.240
So the way we have been trying to express, you know, sometimes delumination is complicated. And

13:42.240 --> 13:47.920
if, in the most general case, we don't know how to describe this full set of pictures,

13:48.640 --> 13:54.880
under some simplifying assumptions, which is rooted in some old work from the 90s,

13:54.880 --> 13:59.600
Bell Humor and Krigman wrote a very famous paper, and I said, what is the set of all faces

13:59.600 --> 14:04.800
under any given illumination condition? And they made some simplifying assumptions of what

14:04.800 --> 14:10.240
of faces, I mean, if I ask you define a face, it's part of how to define a face, right? So how do you

14:10.240 --> 14:16.000
even think? And this is why we've tended towards deep learning and statistical methods over the

14:16.000 --> 14:21.840
past few years because we don't know how to define these things. So exactly. But here is what they

14:21.840 --> 14:27.600
found. There's a, if you define it in some sub-linguistic ways, let's say it's a comics object,

14:27.600 --> 14:32.480
and let's say it's an object which has reflectance defined by some lambershield properties,

14:32.480 --> 14:37.920
then you can actually write down what the set looks like. Now comes deep learning. It says,

14:37.920 --> 14:43.040
I can't define these things, give me data. And the more data you have, the better it is. But

14:43.040 --> 14:46.720
no one knows how much data is enough, right? I mean, the more is better is the answer,

14:46.720 --> 14:52.320
but how much is enough is never known? And we have been positioning ourselves at that intersection,

14:52.320 --> 14:58.000
where we say, look, if I know that I'm looking at faces, I'm going to weaken the structure a little

14:58.000 --> 15:04.320
bit. I'm going to say that, yeah, these objects that we're looking at have some characterization

15:04.320 --> 15:10.000
under these assumptions of simplicity. But then comes deep learning, which allows me to fit

15:10.000 --> 15:15.200
those other degrees, which I'm not able to specify analytically. So we're trying to reduce the

15:15.200 --> 15:20.560
need for larger and larger training sets by restricting the deep net layers somehow that are

15:20.560 --> 15:28.400
motivated by the knowledge of that physics of image formation. Sure, we don't know how much

15:28.400 --> 15:35.040
data we need to get the full specification, but we're saying this will reduce the need for more

15:35.040 --> 15:40.720
and more data. And all things being the same with the same amount of training sets, the same

15:40.720 --> 15:45.600
complexity of the deep architecture, adding these constraints known from the physics of image

15:45.600 --> 15:53.440
formation improves performance. And it also stabilizes performance against degradation of inputs.

15:53.440 --> 15:57.840
You know, typically if you blur a picture, if you surrender a picture in slightly different ways,

15:57.840 --> 16:03.120
performance drops pretty dramatically, we are able to avoid that. It's a middle ground, I'd say.

16:03.120 --> 16:10.400
We are not being super specific about defining objects, nor are we saying more data is good.

16:10.400 --> 16:15.600
We are saying something in the middle, that is, we are trying to come up with some description

16:15.600 --> 16:21.840
of that equivalence class under simplifying assumptions and then let data fill in the rest.

16:21.840 --> 16:27.840
So that's the way we're trying to marry the two things. And is the result

16:29.200 --> 16:36.800
a mathematical analysis in closed form, or is it experimental results on data sets?

16:39.280 --> 16:43.840
I mean, most of the things we do is we end up having constraints which are closed for

16:43.840 --> 16:49.600
mathematical equations. Maybe one layer in the deep net is expected to be orthonormal because

16:49.600 --> 16:54.960
the physics of image formation says that certain variables under certain lighting additions

16:54.960 --> 16:59.120
will have an orthonormal structure. Okay, that's the way we impose the constraint that certain

16:59.120 --> 17:04.560
layers might have an orthonormal constraint put in. But then the network itself has to be learned

17:04.560 --> 17:11.360
end to end with data sets. And the performance has to be validated empirically on data sets.

17:11.360 --> 17:17.200
Okay. One of the interesting things we found is this concept of orthonormality which seems to

17:17.200 --> 17:25.040
have some very special power. What we're finding is whether, you know, think of it. So we played

17:25.040 --> 17:32.800
with this idea in a paper for BMVC last year where we took some classic, you know, disentangling

17:32.800 --> 17:41.280
autoencoder kind of networks. And we had good reason to impose an orthonormality constraint on

17:41.280 --> 17:48.080
the latent blocks of these disentangling autoencoders. By orthonormality, we had to write up a whole,

17:48.960 --> 17:56.400
you know, theory around we expect these factors to represent either movement or lighting conditions

17:56.400 --> 18:03.280
or deformations. And under appropriate relaxation, they all become orthogonal to each other and

18:03.280 --> 18:08.800
they all have this spherical structures. So looking at all of this, it looks like orthonormality

18:08.800 --> 18:14.960
is a trade off, which is coming close to what the physics is telling us to do. And but also being

18:14.960 --> 18:20.160
sensitive to the idea that it has to be implemented easily. We don't want to over complicate things.

18:20.160 --> 18:27.120
And we want our constraints to be differentiable. So it's a design process. So with through in these

18:27.120 --> 18:32.320
orthonormality constraints on disentangling autoencoders and boom, the numbers of disentangling

18:32.320 --> 18:39.200
quality just went up quite significantly. And it's. And so in the case of an autoencoder or in

18:39.200 --> 18:48.320
the layer of a deep network, what does it mean to impose that kind of constraint? Is it, you know,

18:48.320 --> 18:54.480
architectural and does it mean that you're diverging from kind of your CNN, resonant kind of

18:54.480 --> 18:59.680
tried and true architectures or is it, you know, loss function based or something totally different?

18:59.680 --> 19:05.440
How do you impose those constraints? There's a there's two or three ways in which it's happened.

19:05.440 --> 19:09.680
One is we stick with architectures as is don't mess with the architectures, but add-and-loss

19:09.680 --> 19:15.200
functions that works when the constraints are actually expressable as a closed form equation

19:16.000 --> 19:22.160
like spherical losses or, you know, orthonormality. Those can be written on as a closed form equation.

19:22.800 --> 19:27.840
Sometimes the constraints we arrive at do not have an equation, but they have what is called

19:27.840 --> 19:34.480
a manifold structure. And why manifolds arise is very closely related to invariance.

19:35.520 --> 19:41.120
Here is an example, you know, if I say, so the idea of invariance is this, right? I mean,

19:41.120 --> 19:45.280
you have a feature space. Let's say the feature space is something. You have this feature space

19:45.280 --> 19:52.960
in RN and reports specific than something, so it's easier to follow. So let's say it's the, you know,

19:52.960 --> 19:58.080
latent space of AlexNet, okay? Some features come from the latent space of AlexNet,

19:58.080 --> 20:05.520
which is embedded in RN, right? So it's a vector in RN. Yep. And then we come in and we say,

20:05.520 --> 20:10.160
look, I want to impose a slight equivalence here, which is if I rotate my picture,

20:10.160 --> 20:14.240
looks like the features are also changing somehow. I mean, the features are not always

20:14.240 --> 20:19.200
invariant to physical variables like this, right? But if you're able to say that this feature,

20:19.200 --> 20:24.800
this feature, this feature in RN actually represent the same picture, just that they're rotated.

20:24.800 --> 20:32.880
We are trying to basically paste the features and thereby the underlying space into something else

20:32.880 --> 20:39.280
to express that concept of equivalence. And sometimes when that ways of expressing these equivalences

20:39.280 --> 20:46.160
is well defined, what happens is the space gets crumpled. You hope that the neural net learns

20:46.160 --> 20:51.600
to crumple the space all of its own. That is one of the overarching hopes in deep learning

20:51.600 --> 20:56.720
that as you go through the layers of deep learning, the deep learning is learning to squish and crumple

20:56.720 --> 21:04.400
the original feature space into interesting ways to get a job done. But what it's doing is it's not

21:04.400 --> 21:08.720
always getting the job done the right way because you'd never have enough data. But if I explicitly

21:08.720 --> 21:13.760
tell it that here is how rotation affects the features and here is how you paste them together

21:13.760 --> 21:19.040
through whatever mathematics that's needed, then we get some manifold structures. You know,

21:19.040 --> 21:25.760
this crumpling can sometimes be expressed as a manifold. If you want to say in the concept of,

21:26.320 --> 21:30.640
you know, in the paradigm of loss functions, how do you express a manifold as a loss function?

21:30.640 --> 21:40.320
Sometimes you cannot. What you can do instead is, you know, manifolds are basically crumpled spaces

21:40.320 --> 21:47.360
and they have ideas associated with them which are analogous to how we think of maps and,

21:47.360 --> 21:53.520
you know, the earth itself is composed of the earth is a manifold, but then it's also a sphere

21:53.520 --> 21:58.560
approximately. So if you forget the idea that it's a sphere, but if it were a general weirdly shaped

21:58.560 --> 22:05.280
blob, you would represent it by a series of charts and you would explain how the charts connect

22:05.280 --> 22:11.360
and that's the way you specify a manifold through things called charts. And charts are also sometimes,

22:11.360 --> 22:17.040
you know, they have a thing which is similar called tangent spaces. So you sort of flatten the

22:17.040 --> 22:23.600
manifold in local coordinate charts and you can express that tangent space as a vector space.

22:24.640 --> 22:29.680
So once in a while, we have run into these conditions where we have a constraint which was

22:29.680 --> 22:34.720
expressed as a manifold which could not be written down as an equation, but whose tangent space

22:34.720 --> 22:41.360
could be written down. So we were able to enforce conditions of that tangent Z and said,

22:41.360 --> 22:47.840
I want this layer in my deep net to represent coordinates of a manifold on a specific tangent space

22:47.840 --> 22:51.840
and the mapping from that back to the manifold could be written down in closed form. So it depends.

22:54.160 --> 23:02.480
Let me see if I can upload this anywhere close to what you just said. What the way I'm kind of

23:02.480 --> 23:10.960
hearing this is that you've got some problem. Say you've got some object and you apply some simple

23:10.960 --> 23:17.280
transformation to that object, maybe you rotate it. If you've got a deep neural network that is

23:17.280 --> 23:26.160
trying to detect that object, for example, and you've trained it, there may be some feature space

23:26.160 --> 23:36.880
or some representation of that object in the different layers of the neural net. In a oversimplified

23:36.880 --> 23:44.000
world, you'd kind of want there to be a relationship between the rotation of the object itself and

23:44.000 --> 23:48.080
the rotation of the features. Maybe you could apply some simple transformation of the features,

23:48.080 --> 23:54.160
but the world isn't networks aren't that simple. But it turns out there is a relationship

23:54.160 --> 24:00.800
between the features. It's just more like this crumbly manifold thing and you found a way to

24:02.800 --> 24:07.840
express that using the mathematical language of these manifolds that allow you to

24:08.960 --> 24:14.560
detect the actual invariance of the object. That is very correct.

24:17.120 --> 24:18.560
Thank you for giving me those lines.

24:18.560 --> 24:28.640
The only disclaimer is we have been able to do this for a few common sources of physical

24:28.640 --> 24:36.000
variability and that includes things like rotations of objects and deformations of moving parts

24:36.000 --> 24:43.280
in certain cases, lighting conditions, just to be very clear. We haven't been able to do this

24:43.280 --> 24:49.440
across the board for every possible thing. So simple, maybe not three-point studio lighting,

24:49.440 --> 24:57.760
but a simple radio rotation or something like that, but clearly there's lots of things you can

24:57.760 --> 25:04.880
do in the physical world that aren't amenable to that representation.

25:04.880 --> 25:06.000
Absolutely. Yes.

25:06.000 --> 25:15.600
Okay. Okay. Cool. One of the things that comes to mind in thinking about this and in your work,

25:15.600 --> 25:26.000
you fall back on or maybe ground yourself in what you call pragmatic choices of deep architectures,

25:26.000 --> 25:33.360
meaning the popular stuff, the way we're doing things today. I think of Jeff Hinton's

25:33.360 --> 25:40.160
capsule networks is trying to come at some of the same ideas or same problems. Are you familiar

25:40.160 --> 25:47.440
with that work and compare contrast? I mean, we've tracked that body of work also. Again,

25:47.440 --> 25:57.600
he's with all due respect, ACM Turing, I can't do that easily, but it's an over-complication.

25:57.600 --> 26:07.840
I mean, it's ignoring so much. The basic laws of rotations are not that hard if you understand

26:07.840 --> 26:15.200
how to express rotations and we're taking out that invariance is doable without that level of

26:15.200 --> 26:20.000
combination. So I'm correct that you're trying to come at some of the same problems at least.

26:20.000 --> 26:27.920
Right. Okay. In it may be the case that if that enterprise succeeds, if that capsule network

26:27.920 --> 26:37.600
enterprise succeeds, it may be a more general solution to everything, maybe, but if you want to

26:37.600 --> 26:43.440
be a bit specific about understood factors of variation, I feel that's an over-complication.

26:43.440 --> 26:48.560
And there are nicer ways to do that. And I think we're able to do that in a better way.

26:48.560 --> 27:00.960
Yeah. Okay. Yeah. Cool. So you've developed this approach and you mentioned that you've

27:02.400 --> 27:08.160
got some experimental results as well. Can you talk a little bit about how you frame the question

27:08.160 --> 27:18.240
experimentally and what you've seen? Sure. I think the way we frame it is we want to keep

27:18.240 --> 27:24.320
a few things fixed. And the way we keep a few things fixed is we say pick an architecture first

27:24.320 --> 27:29.280
and that can be allocated. We're looking at things like dense net, point net, all those

27:29.280 --> 27:34.480
your architectures which are known to work well for certain databases. Keep the architecture

27:34.480 --> 27:41.840
more or less fixed. Keep the training set more or less fixed. The only thing that varies is,

27:41.840 --> 27:47.440
you know, don't play too much with new fancier data augmentation methods. The only thing we are

27:47.440 --> 27:53.840
doing is adding in constraints, either in some latent variables or we're adding in certain

27:53.840 --> 28:00.960
augmented loss functions. So most of the additional thing that we're doing is a mathematical expression

28:00.960 --> 28:07.840
of some kind. And keeping in, otherwise, it's hard to compare. I mean, if you say, let me train it

28:07.840 --> 28:15.120
for more iterations, but not add a constraint, can you compare it? So keeping mostly the computational

28:15.120 --> 28:20.960
resources fixed. We're asking if this additional mathematical knowledge pushes on the low.

28:20.960 --> 28:25.600
And we've been finding that it does. We have done that for image classification. We've done that for

28:26.400 --> 28:31.680
disentangling networks. We've done that for time series problems recently. And some of our

28:31.680 --> 28:37.600
compelling results are indeed from time series modeling where, you know, we've applied this to

28:37.600 --> 28:42.880
human activity like ignition data sets with either stick figures or wearable devices.

28:42.880 --> 28:49.600
And the kind of factor that we're trying to factor out in human movement is not light and shape

28:49.600 --> 28:55.840
and geometry, but it's time series variability issues, which is, you know, the same action when

28:55.840 --> 29:01.200
performed by the same person, but at a slightly different time will give rise to slightly different

29:01.200 --> 29:06.560
traces because people have intrinsic variability and how they move. And oftentimes that variability

29:06.560 --> 29:12.000
gets expressed through some time warping kinds of relationships. What we've done is express the

29:12.000 --> 29:18.560
time warping property as a constraint, which can be forced by the network to be factored out

29:18.560 --> 29:23.920
in a latent variable if we just throw in that constrictive the loss function. And we've found that

29:23.920 --> 29:29.440
it improves numbers significantly just like that without any additional training, without any

29:29.440 --> 29:37.920
additional data requirements. So the pictures or what I think are the pictures of this if I'm

29:37.920 --> 29:47.040
understanding the problem is along the lines of, you know, start from your seat in the living room,

29:47.040 --> 29:53.760
go to the refrigerator, grab a drink, you know, take off the cover and drop the cover in the trash can,

29:53.760 --> 30:01.920
and you've got this kind of two-dimensional plot of the path that the person might take in doing

30:01.920 --> 30:10.480
all that. And your argument is that the path is an invariant because the task is the same.

30:10.480 --> 30:21.600
It's, you know, do X and Y. And what you're trying to do is identify, well, what is the fundamental

30:21.600 --> 30:30.800
that they said, identifying the path? Is it somehow in a data set with lots of these, you know,

30:30.800 --> 30:37.920
traces or paths, identify which ones correspond to the same actions? It's close. I mean, we haven't

30:37.920 --> 30:43.760
looked at paths in that way, but we've looked at traces of stick figures, you know, so you have

30:43.760 --> 30:49.360
like 50 joints being tracked and you have the full-time series of 50 joints evolving in space

30:49.360 --> 30:55.280
and in three dimensions that comes from motion capture, say. Okay. Yes, the actions not really

30:55.280 --> 30:59.840
unlike what you're seeing, actions in a kitchen, actions in a room, actions in an office, picking

30:59.840 --> 31:06.080
up objects, placing them here and there. And the feature that is invariant, of course, is hard

31:06.080 --> 31:11.520
to linguistically describe, but one of the variables that gives rise to confusion is that

31:12.320 --> 31:17.120
people sometimes take longer to do the same thing. People sometimes are fast in certain phases of

31:17.120 --> 31:22.320
the movement, slow in certain phases of the movement, right? Or there is asymmetry in the body,

31:22.320 --> 31:27.600
you know, the left, the left arm swings more than the right arm. You know, there's all these

31:27.600 --> 31:32.880
interesting sources of variability which are hard to and the only way deep learning will be robust

31:32.880 --> 31:37.440
to that is if you augment it with all these variables, all these sources of variation.

31:38.320 --> 31:43.680
The way we think of it is that the variability here is expressible as a warping of the time

31:43.680 --> 31:50.000
axis, whether it's short versus long or speeding up versus slowing down or if it's one side faster

31:50.000 --> 31:56.160
than the other side or the swings are smaller than the other. It's all a time warp. Sometimes it

31:56.160 --> 32:02.560
can be constant. Sometimes it can be non-constant. So that brings up an interesting question. Do you

32:03.440 --> 32:10.400
assume in your work throughout a single source of invariance or do you also

32:11.920 --> 32:17.120
conceive of multiple sources of invariance? Like, you know, there's a time invariance, but there's

32:17.120 --> 32:23.840
also the left arm swing invariance factor. I mean, that is the, I mean, we are headed in that

32:23.840 --> 32:27.520
direction. I mean, right now our investigations have been affected. That would be the answer.

32:27.520 --> 32:33.680
The goal is to be able to have almost like a linear combination of known invariances that,

32:33.680 --> 32:39.680
you know, you can account for. Right. I mean, at this time, we have been playing it very carefully

32:39.680 --> 32:43.360
that let's take this one source of variable. Let's see if that can be factored out. Let's see if

32:43.360 --> 32:48.640
we can get invaders to that. And we have had success in many different applications.

32:48.640 --> 32:57.840
It sounds like you're further saying, though, that in the case of at least this motion capture

32:57.840 --> 33:11.360
type of a data set that maybe time becomes kind of a meta-invariance that can account for multiple

33:11.360 --> 33:19.440
physical characteristics. Am I hearing that correctly in there? It can. It's hard to write that out

33:19.440 --> 33:27.040
clearly, but it does. Like, for instance, if you had like load bearing, you know, if you were

33:27.040 --> 33:34.480
carrying a heavy bag on your bag, it will have an interesting effect on the time series of your

33:34.480 --> 33:41.040
joins, which is not that easy to explain, but it will sort of stretch out certain phases of a movement

33:41.040 --> 33:47.200
and shrink certain phases of your movement. It does. So, yeah, the stretchings and shrinkings of the

33:47.200 --> 33:54.160
time axis are the key to finding what that invariant is for the lack. Yeah.

33:56.480 --> 34:03.120
And so are there well-established benchmark data sets for these types of tasks? Or are you

34:03.120 --> 34:10.080
rolling your own to explore these methods? No, for motion capture, there are benchmark data sets.

34:10.080 --> 34:15.840
There are, you know, Microsoft has a, it used to have a RGB data set. I mean, the go-by-the-RGBD

34:16.720 --> 34:20.800
activity sort of, you know, keywords. And there's a few out there. There's a few benchmark

34:20.800 --> 34:27.920
data sets out there. NTU has one. MSR is one. And sometimes even, you know, the video data sets

34:27.920 --> 34:34.720
like HMDB have stick figures available through other methods like PostNet, for instance. So, yeah,

34:34.720 --> 34:40.640
there are well-established data sets that we experiment with. And is the task that's posed by

34:40.640 --> 34:50.720
these data sets one of predicting the action that the, it's activity classification and prediction

34:50.720 --> 34:59.440
by and large. Yeah. Okay. Okay. And so what's the kind of state of the art for that kind of

34:59.440 --> 35:06.400
activity detection and how does your method compare to it? So most of the time series in the deep

35:06.400 --> 35:13.120
learning world, most time series things are either a combination of 1D, CNNs or, you know, LSTM models.

35:13.120 --> 35:18.640
So depending upon the data set, the way our process goes is we say, let's find the latest, you know,

35:18.640 --> 35:24.640
benchmarks and we'll improve on those through these mathematical techniques. So, a recent paper

35:24.640 --> 35:32.320
we did in CVPR 2019 used LSTMs as the benchmark data, you know, the technique. And the data sets were

35:32.320 --> 35:40.400
NTU 3D data set and a few others like that motion capture. The tunable parameter in LSTMs is oftentimes

35:40.400 --> 35:46.320
the hidden layers, how many hidden units do you have? And of course, if you scroll through it,

35:46.320 --> 35:51.360
the numbers keep going better and better. The way we've done it is we kept things the same. We say,

35:51.360 --> 35:57.840
let's say 16 hidden units or 32 hidden units. Keep that the same. The only thing will change is add

35:57.840 --> 36:03.520
in this additional module that either disentangles the time or function or adds in as a constraint

36:03.520 --> 36:09.360
and numbers always go out. So in the way we thought about it, if my number, if memory is right,

36:09.360 --> 36:19.840
the NTU RGB data set had like, you know, 80% roughly accuracy with a very fancy LSTM with 200 hidden

36:19.840 --> 36:24.960
units and stuff like that. And we were able to improve it by four five percentage points easy without

36:24.960 --> 36:31.360
any changes to anything, but just this additional constraint added in. So if you find unit more,

36:31.360 --> 36:36.400
sure, there's more things to be squeezed out, but we were able to consistently improve the

36:36.400 --> 36:41.520
performance of LSTM by easy five percentage points and times six eight percentage points with no

36:41.520 --> 36:48.240
change, but a simple constraint on time warping. Yeah. So those are the kinds of results that we've

36:48.240 --> 36:54.160
been finding, which is if you rethink what the constraints should be through understanding the

36:54.160 --> 37:00.080
phenomena first, the payoffs are actually quite significant without any additional requirements on

37:00.080 --> 37:06.080
data or network architecture complexity or training strategies. They can all be very basic.

37:06.080 --> 37:12.080
Mm-hmm. And so now we've talked about a couple of, you know, very different types of problems. One,

37:12.080 --> 37:22.640
kind of a, you know, computer, a very visual type of task in one of this more time series to

37:22.640 --> 37:28.960
apply this to different settings. How much hand crafting needs to go into the loss functions and

37:28.960 --> 37:36.000
the, you know, the different constraints that you're applying to the network? That is where the big

37:36.000 --> 37:43.760
work is. So I think the pendulum is swinging to that level of hand crafting, you know,

37:43.760 --> 37:47.840
moving away from features to architectures and loss functions, right? That's where the pendulum is.

37:47.840 --> 38:00.240
Yep. And the amount of work that goes into hand crafting is a lot of, I would say, studying basically

38:01.040 --> 38:09.120
understanding how these variables actually affect the observed data and try to express it in a way

38:09.120 --> 38:15.440
that is amenable to fusion with the deep net. The beauty is physics is not one way, you know, light

38:15.440 --> 38:19.520
is, there is no single model for expressing how light and surfaces interact. There's layers and

38:19.520 --> 38:25.680
layers and layers to it. Yeah. And you have to know all of that or at least as much as, you know,

38:25.680 --> 38:32.080
as much as you can learn. And then the hand crafting is where in this spectrum of sophistication,

38:32.080 --> 38:41.200
do I stop in a way that I actually have a pragmatic effect on performance without changing anything

38:41.200 --> 38:45.840
else? And that's where a lot of intuition is, you know, you cannot get away from this intuitive

38:45.840 --> 38:52.720
exercise. Despite all the progress of machine learning and deep learning, the networks are arguably

38:52.720 --> 38:57.120
both intuitive and highly unintuitive. I mean, some people have an insight about why a network

38:57.120 --> 39:03.280
works, but presented to someone else, it's mysterious. And the same thing is true of the loss

39:03.280 --> 39:08.960
functions business. Sometimes we can motivate it very easily through simple things like, well,

39:08.960 --> 39:17.280
yeah, cross entropy means where to make sense. Physics is where some of the unintuitive stuff lies.

39:18.560 --> 39:23.920
It's, that's where a lot of design thinking exists and we are doing that. So yes, that's

39:23.920 --> 39:31.440
where much of the work is understanding that when you approach the n plus 1th problem that's

39:31.440 --> 39:38.000
different from the ones that you've looked at previously that you're starting from scratch,

39:38.000 --> 39:47.120
or are there some principles that give you a foothold when trying to apply this method to the

39:47.120 --> 39:52.320
new area? And if so, what are those principles? The principles, I mean, the details, of course,

39:52.320 --> 39:56.240
have to be looked at from scratch, but the principles that we bring to the table are

39:58.160 --> 40:03.280
ideas of geometry and, you know, this idea that look, whatever it is that you're observing,

40:03.280 --> 40:09.760
whatever is the raw space, that is not the space on which you want your analysis to occur. You want

40:09.760 --> 40:17.040
the analysis to occur in a space that is crumpled. And the generalizable knowledge that we bring to

40:17.040 --> 40:22.560
the table is how do we represent these crumpled spaces? And that's the mathematics of humanian

40:22.560 --> 40:28.000
geometry, entropology, group theory. Those are all the new mathematics. It's not new mathematics at

40:28.000 --> 40:34.160
all. It's mathematics of the past two centuries, but in the realm of machine learning, that mathematics

40:34.160 --> 40:41.760
has not made its way in a systematic way. So that's the generalizable knowledge. We bring in group

40:41.760 --> 40:46.560
theory, geometry, differential geometry, topology. That's the way we think about it. But then the

40:46.560 --> 40:53.360
specifics, the problem specifics have to be studied from scratch, but then that knowledge can often

40:53.360 --> 40:58.800
be expressed in the constraints of geometry, entropology, and group theory. And that's where we

40:58.800 --> 41:03.440
specialize. How do we take this domain specific knowledge and look at it through the lens of

41:03.440 --> 41:10.400
groups and invariance? That's a different kind of generalizable knowledge. It's really a way of

41:10.400 --> 41:19.440
thinking about phenomena rather than thinking about data. And going back to your keynote,

41:19.440 --> 41:27.920
are there, do you kind of take a step back and kind of apply this broadly to computer vision,

41:27.920 --> 41:36.000
machine learning? Do you kind of offer any thoughts for where this is all going?

41:38.960 --> 41:40.160
Not. Let's make some up.

41:40.160 --> 41:51.840
So data constraints scenarios, that's where this is all going. Machine learning with unconstrained

41:51.840 --> 41:59.760
amounts of training data is what the last 10 years were about. And we are finding that it's a nice

41:59.760 --> 42:06.720
goal, but there are no guarantees to be ever had, even if you train it forever with as much amount

42:06.720 --> 42:12.960
of data that you've got. If any mission critical deployment requires a guaranteed robustness of

42:12.960 --> 42:19.680
some kind, there is nothing to be given other than, yeah, this is what my numbers are on some data

42:19.680 --> 42:24.960
set. That's all you have. And now if I can just hit pause there, you throughout our conversation,

42:24.960 --> 42:29.520
you've talked about constraints, you've talked about constraints on the network, and

42:29.520 --> 42:37.760
then you've talked about constraints on loss functions, you've talked about constraints on

42:38.320 --> 42:43.680
architectures and not changing architectures. And those have implications on compute constraints.

42:43.680 --> 42:49.280
And you haven't really explicitly talked about constraints on data. How does that fit into

42:49.280 --> 42:56.720
all this other stuff we've talked about? I mean, the way I think about it is if you don't have

42:56.720 --> 43:02.560
access to additional data, you get more bang for your buck by adding these additional

43:02.560 --> 43:07.920
constraints that we were talking about. If you have access to more data and you can collect as much

43:07.920 --> 43:16.080
as you want, you always should. I mean, that's undeniable, but it's becoming more and more clear

43:16.080 --> 43:23.200
that that's not where the future is headed. We are not able to keep training bigger and bigger,

43:23.200 --> 43:27.760
you know, it's an unsustainable path. I mean, there is enough energy going in that direction

43:27.760 --> 43:32.720
anyway, whether or not we like it or I like it, but it's not a sustainable path of progress.

43:32.720 --> 43:38.000
It's smaller and smaller, you know, diminishing returns. I'm with increasing resources.

43:38.800 --> 43:45.920
So that's clear on the margins, but is part of your work trying to get at, you know,

43:45.920 --> 43:54.800
one shot, few shot types of problems or no? We are, I mean, that would be an extreme case.

43:55.840 --> 44:01.520
Yes. I mean, we are thinking more along the lines of if I had to collect more data,

44:02.800 --> 44:11.600
can I first pause before collecting any more data and robustify what I've got with domain

44:11.600 --> 44:17.440
knowledge? That's the way I think about it. One shot and few shot, it's a whole different ball

44:17.440 --> 44:24.000
game. I mean, it's like the wide west of, you know, machine learning and I wouldn't go that far yet,

44:24.000 --> 44:30.800
but will it be applicable? I mean, sure, I don't see why not, but I wouldn't make big claims

44:30.800 --> 44:37.920
of getting one shot performance, but it should definitely help. If not, you know, make it more

44:37.920 --> 44:43.680
amenable to, you know, less training sets. Yeah. Okay. So I interrupted you were talking about

44:44.320 --> 44:50.560
essentially that the data, you know, data collection is always going to be expensive and,

44:51.920 --> 44:58.640
you know, thinking about the problem space, you know, can provide, provide these benefits.

44:58.640 --> 45:05.360
Yeah. Yeah. I mean, unfortunately, human in the loop can't go away. I mean, there is

45:05.360 --> 45:10.480
neural architecture search. Yes. I mean, again, will this succeed? They will succeed at developing

45:10.480 --> 45:15.360
some representations that get a job done. And when you layer in questions of interpretation,

45:15.360 --> 45:20.000
explanation, which everybody's talking about, I have a much simpler take on it, which is,

45:20.800 --> 45:25.920
if you don't have robustness to even simple physical variables, how will you even explain it in

45:25.920 --> 45:31.120
your hand? I mean, if your classification shifts simply because I rotate a picture and you're

45:31.120 --> 45:37.840
asking me to explain it, I think you're asking the wrong question. If you're at least asking me,

45:37.840 --> 45:44.000
can you be first be robust slash invariant to simple things? And then explain it to me,

45:44.000 --> 45:49.840
that's a more well-posed question, but these are premature questions to ask. And some

45:49.840 --> 45:55.360
colleagues of mine have gone so far to say, repeatability, if your machine learning technique is not

45:55.360 --> 46:00.160
repeatable. And by that, things like this, yeah, if I click this picture at a slightly different

46:00.160 --> 46:03.920
time of day, and I think it's really changed, it takes up the time of day, and the decision has

46:03.920 --> 46:10.160
flipped. It's not, the process is not even repeatable. So don't even go to the extent of

46:10.800 --> 46:16.160
explaining a non-repeatable process or trying to interpret a non-repeatable process. Those are all

46:17.040 --> 46:23.280
questions that should come later. So if you think of repeatability, you do an experiment,

46:23.280 --> 46:31.360
you get the same result over and over again. As far as the big things are controlled, machine learning

46:31.360 --> 46:40.080
hasn't yet delivered that even. So we're trying to bring in that level of robustness, I call it

46:40.080 --> 46:44.800
robustness slash invariance. Some people have called it repeatability, simple and repeatability

46:44.800 --> 46:50.560
sounds shinier, and it sounds like the stakes are much higher, but I'm happy to just call it

46:50.560 --> 46:58.000
invariance. Nice, nice. Well, Paven, thanks so much for taking the time to share what you're up to,

46:58.000 --> 47:05.840
and provide us some context for your CVPR keynote. Very cool stuff. Thank you so much Sam. This has

47:05.840 --> 47:21.280
been a pleasure and you've been great. Thank you so much.

