Welcome to the Twimal AI Podcast.
I'm your host, Sam Charrington.
Hey, what's up everyone?
Before we move on to today's show, I want to give you a heads up that we are back with
another interactive podcast viewing party on Monday, May 18th.
This time, I'll be joined by Emily Bender, Professor of Linguistics at the University
of Washington.
Our recent interview explored the question of whether linguistics has been missing from
NLP research.
We dig into questions like, would we be making more progress on more solid foundations if
more linguists were involved in modern NLP or is the progress we're making, such as
with deep learning models like Transformers, just fine.
We also explore issues like fairness bias and ethics in NLP and more.
Emily and I will be in the chat answering all of your questions about the interview.
We'll begin the viewing party at 3 o'clock Pacific on Monday.
You can head over to twimalai.com slash 376 viewing for more details or to add it
to your calendar.
And now on to the show.
Alright, everyone.
I am here with Nathaniel Ruiz.
Nathaniel is a PhD student in image and video computing group at Boston University.
Nathaniel, welcome to the Twimal AI podcast.
Thank you very much.
It is great to have a chance to chat with you.
I'm looking forward to digging into our topic, which is some work you've recently done
on disrupting deepfakes.
But before we dive into that, tell us a little bit about your background and how you got
started working in ML and AI.
Yeah, so the first kind of project that I did in AI was a computer vision project when
I was doing an internship here at MIT.
And it was about basically detecting diseases in cassava plants and we were going to deploy
the application in Uganda.
So that was kind of like my first introduction to deep neural networks and image processing
type of stuff.
And I thought that was incredibly interesting, just like the potential of these types of
applications and this new technology, right, or this technology that had been advancing
like very recently.
So I got really interested in it.
Then I did my master's at Georgia Tech, so that was right after that internship.
And then so at Georgia Tech, I met a mentor and my professor there who was my advisor
for my master's, Jim Ray.
And I joined this group and did a lot of stuff.
So I got interested, I guess, and they do a lot of work on autism and behavioral imaging
for autism, trying to like diagnose or model behavior and attention in toddlers or kids.
So I thought that was an example of a model that they built.
So we have several works on.
So they were already doing several works on gaze estimation or attention estimation.
And when I joined that group, I worked with Angie Chong, one of my collaborators, and
she just graduated actually from that group.
And we did behavior modeling and attention modeling and scenes.
So now just more in general, like if you're looking at a person from a third person point
of view, like an image or a video, someone, you can actually detect where they're attending
in the scene.
And this work kind of generalizes and you can basically detect where the person is attending
two in the scene, and it could even be like if they turned their head, this could pick
up that type of attention towards the back of the room, for example, or if they're looking
at the camera, they're not looking at any object inside of the scene, they're looking
at something, you know, beyond the frame, and it could also detect that type of attention.
So I got really basically interested in faces, mostly, just in human beings and in faces
in an image and video.
So I think a lot of my work kind of like goes from that and then, yeah, that's how I got,
you know, to recent kind of projects that I've been doing.
And how long have you been at BU?
This is my second year, so about a year and a half ago, so I had my PhD with my advisor
Stan Schlerof.
What got you started working on or looking at deepfakes?
So actually, in the past year, I was always very interested in deepfakes, especially like
there's a video where there's like deepfakes of Obama using a lot of computer graphics,
right?
They used deep neural networks and computer graphics, and this was like about like two to three
years ago, there were like some big advancements, and you can see very realistic basically
reanimations of Obama's face.
That already, you know, got me very interested when I was at Georgia Tech, but I think it
didn't have like a technical expertise like really tackled at that point.
And more recently, I think the things that have been most impressive are the deepfake
applications where you only grab using only one image of a person, you can basically create
more images or video of that person with different expressions and moving their head.
And that's been like really amazing.
I think there's a work by Samsung, a neural talking heads or something.
I don't actually remember the full title, but it's just amazing how just with one image,
they can create these types of things.
So I had been working on generative models for faces recently, and while I was working
on that, I had a conversation with Stan, my advisor.
There's always a privacy issue, right?
Like when you're going to try to release a paper, you have all these issues that whose
faces can I actually use for this work, and also kind of the reaction of the public.
And I think there's a lot of good that could come out of these applications, but definitely
a lot of bad that can come out of them, right?
The good part is you could have actors and movies, you could basically edit their faces
in real time and not ask them to shoot a scene again, maybe if they failed, in their
expression wasn't perfectly what you wanted at that point.
And there's a lot of different cool applications that you can do with UI-UX and sending videos
of yourself through an iPhone or something, but there's also really bad stuff that we've
seen, actually.
Like the first thing that started happening is they've been using deepfakes in pornography,
right?
Just switching faces into pornographic scenes, and it's just completely immoral and the
potential for damage is so big.
So basically, there's always that effect if you're publishing something on this topic
that you could be helping people that want to do that, you know?
So that whole thing was in my head all that time, and during a conversation, we're talking
about the privacy issue.
So these networks are called image translation networks.
So this work by Samsung is an image translation network that goes like from one image to, you
know, a new image with a different expression imposed.
They also can fine tune it with like several images.
But in general, that's kind of the framework that a lot of these new works are applying.
So StarGAN, also StyleGAN, all of these organimation also was a, I think, 2017 ECCV paper.
I think got best paper, basically putting new expressions on a person's face, so changing
my expression from an image to be like smiling.
And you've seen like applications of this in apps like FaceApp, you know, that's become
very famous.
So this, you know, image translation network is kind of simple as like neural network that
goes from an image to another image, and you kind of specify what the output you want
it to be, basically.
So, and there's this, all this other work on adversarial attacks, right?
That everyone has been hearing because, you know, you don't want to have an adversarial
attack is basically an imperceptible perturbation on an image that a human being doesn't notice,
but that can completely fool a neural network.
So you know, this thing has been explored since 2013 for classifiers.
So if you had a neural network that tells you, hey, there's a pedestrian in the scene,
you know, or not, then you can fool this classifier.
So everyone is obviously freaked out about the possibility of this, you know, becoming
a reality to attack, you know, neural networks in the wild with this type of thing.
So both of these like kind of fashionable ideas in my head, I think, you know, it just popped
out of nowhere.
So maybe during that conversation, maybe if we attack, you know, an image translation
network, you'll be able to protect your images from being converted into deepfix.
The premise of this work is, you know, it sounds like just like in the adversarial attacks,
you're injecting noise to an image or you're injecting noise against an image and then disrupting
the classifier here, you're trying to inject noise on an image and disrupt the ability
of some generative model to do whatever it's trying to do to manipulate that image.
Is that the general idea?
Yeah, exactly.
So in the classifier scenario, you have, you know, an image that goes into the class,
into the deep neural network, and then you want a class that comes out of it.
So if it's a dog picture, you want it to classify it as a dog, right?
And an attack in that situation would be to make it classify it as, you know, a cat,
for example, so a wrong class or you could be a targeted class like you wanted to actually
always classify it as a cat.
You could do a targeted attack or an untargeted attack to like drive it away from the class
to dog class.
So it could be any other class like, you know, lizard, the one that's closest.
So the closest boundary, basically.
In this case, it's kind of like a more general, it's a harder thing to quantify, right?
If you have like an image translation network that goes from an image of a person, you know,
my face, you know, like with a serious face, and then someone wants me, imagine just like
basically this is one of the types of applications for this is, imagine there's a picture of
me in like a serious situation, right?
And I'm like a political figure or something, right?
And then someone grabs this image, uses animation, which is, you know, it's working right
now.
You could actually use this or or start again or anything or cycle again.
And you and changes my expression into like a smiling expression, right?
That's already directly an image to image deep fake that can have a lot of impact, basically.
And the idea is to, you know, make this type of transformation impossible or basically
the idea of artwork is to make it either obvious or to completely disrupt the output such
that it's too corrupted, it's so easy to notice that it's been corrupted, basically.
So the human observer can be like, can either doubt the source of the image or, you know,
that the image has been manipulated or it can, you know, or it's unusable, basically.
It's like gibberish or black, you know, that's basically the idea.
And so you make it sound so simple, but I'm sure, you know, flipping through the paper,
there's a lot of work that went into this where, you know, what were the challenging parts
and how did you take this from kind of idea to a working model?
Yeah, so I think actually the idea, so what I loved about the idea is that it was so simple
and it was so obviously useful.
That's what really got me excited at first, right?
And it's actually, you know, the first couple of weeks of implementing all of this was actually
not very hard because the main idea of trying to destroy an output, using an adversarial
attack in an image translation network, it's actually a lot of these image translation
networks are very susceptible to attack.
And some of them are a little bit more protected and maybe some of our future work is going
to be on that type of thing, like which architectures have more protection than others?
Why are some difficult to attack and why are some easier to attack, right?
But you know, the paper, so I think a lot of it went on, you know, a lot of the work on
the paper went on showing that this is possible with a lot of different types of architectures,
showing a lot of very good examples that it's like a solid technique basically.
And you know, there are some, so basically this paper is kind of like the first or one
of the first steps into this kind of domain because in general, so for this type of, you
know, attack, it's called a white box attack, where you need to know all of the parameters
of the neural network and you need to know the neural network that they're using, right?
So that's a big, you know, kind of if in the real world this, but definitely this could
work at this moment, you know, because a lot of attacks or a lot of deep fakes, I'm
sorry, are very low effort.
So a lot of things happen where someone puts an architecture online on GitHub, right?
And then promotes it on Reddit, for example, and then a lot of people go and use this architecture
with this like pre-trained model.
And then, you know, create defects with this architecture, right?
So that's kind of low effort that can already be preempted by using this technology, because
you know, the way that script treaties, I don't know if anyone uses that term anymore.
I think we should make a new one because it's the idea that like, you know, with hacking
folks would just, you know, download some Pearl script or whatever and run it against
some site to find, you know, vulnerabilities.
Yeah, exactly.
And you still had, it's funny because, you know, I guess SQL injections were, you know,
were so easy to exploit in the early days, but even until like, you know, like 10, 10
years ago or something, like people were still finding SQL injection vulnerabilities.
And so script kitties kind of like were still able to like work in the real world.
So I think this kind of, you know, we should make a new term maybe like DL kitties or something,
right?
It's just people.
You're here first.
People trying to, trying to use like out of the box kind of stuff on, transfer learning
kitties.
Yeah.
That's a good thing.
Pre-trained model kitties.
There's something in there somewhere.
But at the same time, you know, I always, like, I respect anyone that tries to use these
things out of the box.
That's the first step, right?
Out of the box from, you know, wherever you can find, put your hands on any of this technology
and then start using it and try to learn from it.
If it's not for doing something bad, right?
If it's just to learn, I encourage everyone, right?
To just go on on GitHub, try a bunch of stuff and then see if they can modify it.
Like that's the way forward, right?
But yeah, if you are doing deep fakes for an immoral purpose, that's definitely not good.
Yeah, but this technology, so just for this specific thing, is already usable because
we're able to, you know, find you're probably going to have the access to the, to the weights
and to the architecture.
But this is a very like constrained kind of attack in some sense.
The more advanced types of attacks are called black box attacks.
You could be like gray box where you know the architecture, but you don't know the weights
of the architecture or like different types, there's different types of settings for
that.
But the black box, which is the most powerful, is just you just have a black box where
you send an image and then you get an image back where you get, you send an image and
you get like a class back dog or something.
And then you're trying to kind of attack it without knowing almost anything about it.
I think that's really interesting and that's like where the promise lies in this because
if someone can find very good black box attacks against a lot of these methods, then you'll
definitely have something that that can work in the real world.
Yeah, I mean, you almost envision a scenario where, you know, a Facebook or Twitter, Google,
like when you upload a photo, it's applying this method to all of your photos so that there
are not susceptible to, you know, being used for in a various purposes.
100%.
Yeah, yeah.
And that's like the first kind of application that I had in mind.
The first application that I had in mind was actually for celebrities where, you know,
their likeness is, you know, so, you know, it actually has a big value, right?
And a lot of people are targeting their pictures to try to modify them.
And that type of scenario where you can, you know, maybe, you know, they can buy some
type of service that protects their images.
And then, yeah, if you can generalize it, then you can have any type of platform online
that does this.
Yeah, there's just so many interesting things, right?
You could give permission to someone to use your picture and then to, like, modify your
picture, right?
And then there are some, you know, technologies that we've been thinking of in that direction
as well.
Lots of blockchain opportunities in there.
I feel like, yeah, I think a lot of people kind of jump to blockchain when they're thinking
about protecting images of people or protecting media, but that's one way.
This is another way.
And blockchain, when I want to make a joke about overhyped technologies, yeah, I mean,
we all fall into overhyped sometimes.
That's definitely true.
But I also think, I don't know, sometimes it brings attention that maybe will fade, but
it's still good that these kinds of domains are having a lot of attention because they
have a lot of promise.
And the people that stick with it are the ones that are going to make things happen.
And then the people that leave, right?
Okay, well, they left, and if they're not here to, like, make something happen, then
that's, you know, their issue, basically.
So is the noise that you're injecting?
Is it parameterized with a single, like a single epsilon value or their characteristics
to the noise multiple, you know, characteristics that you're manipulating to make it work with
a particular model?
Yeah.
So the main attacks that we propose, and the paper is not just attacking image translation
systems for deepfakes.
Okay.
It's, it has a lot of other stuff.
For example, we have, you know, you have the image translation case, but you also have
the conditional case where I don't want to distract of your question, but I just want
to say that we have basically several contributions that make it a little bit more than just, oh,
this is, you know, a neat, neat little idea, right?
Because I think the first kind of idea is, is cool, but also the, all the contributions
that we do make this, like, a, kind of, like, bigger kind of type of work.
Because the main idea of attacking these image translation systems, it's actually not
that complicated to adapt some of the techniques that we already had, like FGSM, FastGrading
Sign Method, iterative FGSM, basically FGSM is just kind of taking a step in the direction
of the, of the gradient for, for your image and then modifying your image so that you
go away from the class that you want to classify this thing as.
That's for the classifier case, right?
In our case, you have the, this loss, which is, you can have, like, your main metric is,
what happens if you translate the image without any attack, you have kind of, like, a ground
truth output, right?
Like, this is what the picture is going to look without any attack.
And then you have a translation of your image, so this is the thing that we can modify,
because the translation of your image within attack, right?
That's the thing that we can modify.
And now you want the difference between this ground truth output and this attacked output
to be as big as possible, and with some kind of metric, right?
And we use L2 in the formulation of our attacks, but you could use, you know, a lot of different
type of metrics.
And we use, like, an image, image level metric, which goes, like, pixel pixel to pixel differences
in the, using L2, right?
And then you want to maximize this, right?
You want your output, the attacked output to be as different as possible as the ground
truth output would have been.
So you can actually just do this.
It's just an optimization, you know, problem.
Yeah, I guess one of the things that jumps to mind is as opposed to, like, in L2, distance,
maximizing the ability to fool some kind of discriminator network.
So we don't actually have to do that at all, actually, because, so to train these types
of architectures, yeah, you have a generator that does the image translation, right?
And then you have a discriminator and this type of game between the discriminator that's
creating the translation and the, the generators already and the discriminator that's trying
to detect whether it's a real or fake image is actually what makes the output so good
and makes it, you know, approximate these distributions so well without, without too
much blurriness, et cetera.
You know, that's the power of Gans, but we don't even need, you know, we kind of like
throw away the discriminator in this process, just use the generator and it just becomes
like an optimization problem.
Maybe the layer behind my question was, is, you know, the, your L2 distance is really
a proxy for perceived difference from the actual face and I'm wondering like how good
L2 is for, you know, really, you know, making the generated image far away from the face
or, you know, perceived, the level of perceived distortion.
And so that's where I thought maybe like some kind of discriminator train discriminator
thing could be better than L2, but that's a really good question, actually.
I think, you know, you could think of, of, like an L2 distance, you could, you could
think of, you know, the attack making just the image just a little bit brighter and then
the L2 distance would go up, right?
But, but, so you would have a higher L2 distance between these two, but a human being would
be like, you know, it's a little bit brighter, but it's kind of still the same picture.
So yeah, definitely, it's not the perfect thing in the paper.
We explain why, and on average, you know, it's, it's a good metric to use and we show examples.
So we didn't really just go through the L1, L2 metric and be like, yeah, it's high.
So it's, it's working, right?
We looked at a lot of qualitative examples, but the second, you know, the second step
that you can take after this is definitely, and I, you know, encourage anyone that wants
to try this out, actually, to try it out because I'm trying other things.
But if anyone wants to use like a perceptual metric, so, you know, we have like VGG, for
example, VGG trained on faces is a pretty good kind of proxy, I mean, it's kind of weird
to say, but it's kind of a proxy of perceptual, of a perceptual metric.
So two faces that are similar in this like VGG 16 feature space are, you know, you
can actually kind of cluster then better in this, in this type of sense, instead of using
L2 metrics.
So, yeah, having a higher distance with a VGG, which is kind of like a discriminator
in some sense, it's a neural network, right?
Or you could do what you're saying is you could say, is this image, you know, you could
have like discriminator, I just had this idea right now, actually, you could have a discriminator
that tells you, oh, this image is more or less distorted, right?
And you want, you train your discriminator to detect distorted images and you train your
generator to distort the images.
And then you could have like a game in that way, maybe with a third or second discriminator
as well.
But definitely like that's one of the big questions of systematizing this is it's easier
to systematize with classifiers because you know when the class is wrong, there's just
a number, right?
The class is four, but we wanted five, so it's wrong, right?
In this case, you have something and that's the problem that I've been like bumping into
all over this kind of area is you're talking about human perception and trying to kind
of like model a reaction of a human being, right, or what the human being perceives in
an image.
And that's, you know, much more complex for generating faces and for disrupting defects,
right?
So it started a couple of questions ago asking about like the how the noise is parameterized
and I think the question behind that question was, you know, is there, you know, some way
to just crank the noise up as much as you can before the image starts looking distorted
and use that as a way, you know, does that get you closer to a gray box or a black box
type of scenario or do you really have to, are there, you know, how nuanced is the, does
the noise have to be to defeat a particular system?
Yeah, that's actually a great question.
One thing that I really wanted to say that I didn't get a chance to say before we started
is this is not just my work, right?
Like this is work with really amazing collaborators at BU.
So a recent, a research assistant professor, Sarah Del Bargel, who helped me so much with
this project and then my advisor Stan Sclyroff, right, and all of these kind of ideas have
been like discussed with them and, you know, we've all like put so much, you know, work
into this, right?
So, okay, moving on from that, basically, yeah, this is kind of like, that question is really
good because one of the things that we did was try it on, so first of all, just try to
see how sensitive any of these architectures is to just random noise, right?
Like I'll say no to something and some of them are super sensitive.
If you, I don't really want to say which ones, right, because I don't want to just like
single out any any, and it's not their fault, right?
But some of some architectures, if you inject just a little bit of random noise, then you
can have very big perturbations in the output image.
That was the first step and some of them are very resistant to noise.
And then this finding holds to the adversarial attack case.
So an adversarial attack is just a sort of structured noise that is structured using,
you know, basically you use the gradient of the network to get the biggest type of disruption
in the output possible, right?
So that doing an adversarial attack of same magnitude would be way more effective than
just being random noise of that same magnitude.
But some architectures are just really sensitive to noise.
That's another lesson of this, I think.
Your initial response to that question was talking about the broader contributions to this
paper beyond the kind of the simple, deep fake disruption mechanism.
If that's the right way to characterize it, walk us through the, you know, what you think
are the biggest contributions here.
This idea of attacking image translation systems is pretty natural.
But then there's some specific, you know, specificity of deep fake kind of image translation
networks.
And one of them is that you have, you're always, you almost always have a class or their,
you know, conditional image translation networks.
And your condition could be or your class could be, for example, in animation, you have
the action units, which are, you know, small movements of the phase that could like correspond
to like smiling or moving your lips upward, et cetera, right?
And then by combining them, you create expressions.
So these, these networks are conditional networks.
And you want to kind of attack them irrespective of the class.
So my attack doesn't have to, if, you know, let's just say that you don't want it to
only work on the smiling faces or something.
Exactly.
Like, yeah, maybe this person is going to make everyone smile and pictures, but, you
know, you don't even know what the other person's going to do, maybe close your eyes, right?
So yeah, for example, if you target an attack towards, and in some of these architectures
you're tired to attack towards one class, then it doesn't transfer to other classes as
well.
And in some of them, it's just an attack for one class, just transfers completely to the
others.
And I think actually these two kind of properties, like the fragility of an architecture
to noise and this type of transfer are actually related.
That sounds kind of interesting and that is it kind of saying that the fragility isn't
necessarily an architectural trait, but more specific, like a trait of the weights of an
architecture.
Yeah.
I think so.
I think the fragility of an architecture to attack is actually, that's what I'm kind
of discovering now, is tied not only to the type of architecture, but to the weights.
And if you think about it, the weights is just a function of the training data and the
training kind of process, right?
So the optimization process and the training data.
So these two things are pretty important, I would say.
And it depends how important they are for each kind of architecture.
And it's still like, this is, you know, complete.
This is kind of like an intuition a little bit from what I've seen in my experiments, but
I think there's a lot of like super interesting work to be done here, because I think adversarial
attacks actually are very exciting to me, and I've just gotten into them a little bit later.
But you know, I've always like kind of wondered like, how exactly do deep models work in some
sense?
Like, why do they fail in certain cases and they don't fail in others?
You know, this type of kind of explanatory process of the failures of a network or how
to make it better, you know, or, you know, intuitions and how to make it better.
So I think adversarial attacks are actually like a great window into fragility of an
explanation of these neural networks.
So that's one cool thing about this is by attacking image translation systems, I can
actually see in their output, like when I attack them, what they are doing.
A lot of them.
So, for example, you know, this is something good for a pretty specific, I think, but for
StarGAN, if you attack it, then you have the whole image that changes all at once, and
you have other architecture such as animation that are very targeted towards certain parts
of the image that they're modifying.
So one architecture has learned how to kind of like change the whole frame at once, and
then one architecture has learned to do more like fine-grained types of changes inside
of an image.
And one is, you know, animations more robust than StarGAN.
So it's almost along the lines of work like lime and other things where you're perturbing
inputs or features and seeing how the network responds to the aim of understanding explainability
or producing an explanation.
You know, this is, you're kind of almost at, you know, trying to explain or understand
these networks through the disruptions that you're injecting.
Yeah, I think so.
I don't know.
Actually, this work, you know, I actually don't know about that work that you just mentioned,
you know, going on from your explanation.
I don't know if that has been done, maybe it's been done for like classifiers, but I don't
know if it's been done for image translation networks.
And I think that's like a huge frontier that this kind of opens is to try to understand
what's going on.
And then if you know, so for example, just, you know, example of the bad, if you know
that your image translation network is changing the whole frame when you actually just need
to change the hair color of the person, then you could think of this as a weakness,
right?
And then you could think of how do I change the architecture of the training procedure
to correct this, right?
And there are techniques to do it, which are, which are pretty cool, I think.
So yeah, definitely it's delving into this type of explainability.
I guess that's also fashionable right now, but I think it's a huge thing.
My lab does a lot of, of, of, of explainability and deep networks.
Okay.
Okay.
And so this kind of broader understanding of these architectures is another contribution
of the paper.
What else?
Yeah.
So yeah, yeah, we were on that question kind of easy to get lost.
So that was, that was one of one of our contributions is conditional image translation
networks and, and doing, building attacks that generalize to all of the different types
of, of, of classes.
So yeah, it doesn't just work if someone's trying to put a smile on your face.
It also works when you're, when someone's trying to close your eyes in an image, right?
Or it doesn't just work if someone's trying to make you, you know, blonde or something.
It works also when someone tries to make your hair darker or something.
So that's, that's one thing that the other thing is, so kind of like, okay, now we have
an attack typical in this kind of area is you have an attack.
What are the defenses, right?
Like what is someone that has like an actual like beneficial image translation network?
What can they do to defend against this type of attack?
Because you could also think about the scenario where, you know, in this scenario, you're
trying to, you know, obstruct deepfake.
So you're trying to obstruct something that is done without permission of the users,
and that can be malicious.
But you can imagine a scenario where someone attacks, let's say, I don't know, just off
the top of my head, like, let's say you have like an X-ray and, and you have like an image
translation network that makes some zones more visible to a surgeon or whatever, right?
Like it's hard to come up with an example right now.
But going with that example, you can imagine a malicious actor introducing one of our attacks
or something like this to this X-ray to make the output not work, right?
So what can a person do to defend against this?
And one of the defenses that holds up to all the scrutiny is adversarial training.
I think it done by Madrid all and 2017 that I think that's the paper.
So basically the idea is just it's a very, it's a very simple, very powerful idea.
You have PGD, which is a very strong attack projected gradient descent, and you just
augment your data set with a lot of images that have been trained, that have been attacked
using PGD.
And then you train your neural network with those images, and it gets, it becomes a, you
know, more robust to these types of adversarial attacks in this type of sense that I've been
explaining.
And this also is, so a lot of these defense mechanisms or, you know, defenses against
adversarial attacks are so hard to make because in security, your defense has to be valid
even when an attacker knows the defense you're going to use.
So a lot of defenses fall apart in this, in the scenario, and this is one that that doesn't
or hasn't at this point.
So one of the things is it's not foolproof.
You can still attack the network.
So we're able to do this adversarial training for GANs, and we have like these formulations
because you can, you can, you can train the, the generator with adversarial noise.
But you can also train the generator and the discriminator.
So you attack both the real image, but you attack the real and the fake image.
So, so there's like different ways of doing it.
The most powerful way of doing it is, is doing the generator plus the discriminator adversarial
training, and it does defend against some certain types of attacks.
But so it makes it more robust, but in the end, if we have a very strong attack, it's,
it's still pretty successful.
That's one thing that we learned.
And in the future, more investigation in this is kind of needed to see exactly, you know,
how much robustness it does bring, actually.
Oh, yeah, and the last one.
Yes.
Oh my god, I forgot.
I think all of these are pretty interesting and, you know, I would like to work on all of
them.
It's impossible, right?
So that this one is, the last one is in a certain scenario where you can blur the,
so my advisor just told me, like, oh, okay, you can attack these images.
Great.
But what happens in the, in the real scenario, if you're like a malicious actor, you run
into one of these images and you're like, I suspect that this one's attacked, right?
So I'll just blur it a little bit with, you know, a Gaussian blur or an average blur
or something.
And then maybe since the attack is high frequency structure noise on top of the image,
then maybe this will destroy the attack.
And so that's one thing in this, in our scenario, it's, it's different than the classifier
scenario.
In our scenario, we have to kind of also be careful of this kind of like gray box scenario
where we don't, we know maybe the architecture that they're using and the, and the weights
that they're using, but we don't know what pre-processing they're using.
So there are some ideas in this domain like expectation over transformation where you
just grab kind of like the expectation of all of the losses through, with all of the
transformations that you think of.
And in that paper, they did a cropping and rotating, but they didn't do blurring.
So I think our paper is one of the, you know, to the best of my knowledge, it's one that
is, one that addresses blurring in this type of scenario of transferability across blurring.
But also, yeah, because we noticed that blurring is actually really effective.
You blur and naive attack just a little bit, then you can definitely translate the image.
And there's almost no downside because the output image looks as good.
And when we, when you say attack here, are you speaking in the sense of kind of traditional
adversarial attack or, and or, you know, your work where you're trying to prevent manipulation,
which the manipulation is kind of an attack in this sense.
Yeah.
The terminology is actually what we tried to do is, is deep fake and deep faker, right?
That's, or manipulate, you know, and that's the person that's trying to create the deep
fake and then attacker, an attack and disruption, right, is kind of the, the person that's trying
to, yeah, do an adversarial attack on the image to prevent the manipulation of it.
So in some sense, the attacker is defending against something done onto them in this scenario,
right?
That's kind of hard to keep the terms, yeah.
And then the funny thing is like defense is actually the deep faker that's trying to
defend against, you know, the attacker, right?
Right.
And in this case, actually, so in the, in the paper for the blurring thing, we propose
kind of like a different kind of iterative heuristic method, which is faster than expectation
over transformation, but as effective, for at least for our scenario and the, and that
experiment that we did, expectation over transformation is great work and all of these,
you know, honestly, all of the works that I saw in that in, in this paper are just, are
just really great and just like big steps in, in this kind of field.
And I respect all those people like immensely.
So it's just hard to list all of the work that has kind of influenced what we do.
So the paper kind of explores these areas and what didn't we cover yet?
I mean, I think, I think we covered everything.
One of the things is that maybe neglected to say is that it's, this is another really
interesting kind of thing about the papers.
So if you have these blurring things, so if you have different types of blur, you can
have like different magnitudes of the blur, right?
And every time you're attacking a different kind of blur type, you're attacking at a different
type of kind of like scale, or if you think about it, you're adding like higher, higher
frequency noise and then you're adding lower and lower frequency noise.
So that's why we call, so our attack is called spread spectrum attack.
It kind of is inspired in a spread spectrum water marking where you could put a water
mark in a lot of different in the frequency domain, a lot of different kind of in the frequency
band, basically, not just not just in one, but just in in a spread, you know, spread spectrum
manner.
This is kind of the idea of of that defense that we, though we present in the paper.
Yeah.
And just, I don't know, just for future work, there's so much interesting stuff there.
Are you continuing work on this or are you working on other projects now?
Yeah, definitely. So I tried to like double my efforts in this because I feel like this
is one of the project that has really given me, you know, a lot of ideas and different,
and so many different directions.
So it's actually a little bit stressful because there's maybe a little, a lot of ground
to cover.
And if anyone is listening to this and wants to collaborate, yeah, just send me an email.
You can find it on the, on the paper and then and we can set up a collaboration because
I think, you know, there's at least like three to four directions that are very different,
but also super interesting.
Well, in a 10 year, thanks so much for taking the time to share a bit about what you're
working on.
Yeah.
Thanks so much.
This is a great opportunity and very, you know, best of luck with all of your next shows
and with all of this craziness that's happening right now, right?
Yeah.
Yeah.
Yeah.
Yeah.
How are things for you?
You're staying, staying inside, staying safe.
All that.
Absolutely, like everyone should stay inside as much as possible and just go outside to
shop or, or, you know, as much as you can, right?
As much as, like, allows again, yeah.
But definitely here in Boston, there's nothing going on, everything's closed and BU has
been closed, you know, I don't know.
Maybe we'll be like this for, uh, for around, you know, four months, three months.
Yeah, who knows.
Awesome.
Well, thanks so much and, uh, again, take care.
All right.
Have a good one.
Yeah.
Yeah.
Yeah.
All right, everyone.
That's our show for today.
For more information on today's show, visit twomolai.com slash shows.
As always, thanks so much for listening and catch you next time.
