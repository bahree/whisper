1
00:00:00,000 --> 00:00:13,400
Welcome to the Tumel AI Podcast.

2
00:00:13,400 --> 00:00:15,920
I'm your host, Sam Charrington.

3
00:00:15,920 --> 00:00:24,240
Hey, what's up everyone?

4
00:00:24,240 --> 00:00:28,440
Last time I mentioned the study group, I'm hosting around IBM's AI Enterprise Workflow

5
00:00:28,440 --> 00:00:33,880
courses and the webinar I'll be doing this Saturday with Ray Lopez, the courses instructor.

6
00:00:33,880 --> 00:00:38,760
It's been awesome to see the strong community response to this new program.

7
00:00:38,760 --> 00:00:43,600
These courses complement the modeling focus courses you may have taken and drill in on the

8
00:00:43,600 --> 00:00:49,000
end-to-end process of building, deploying, and managing real-world enterprise AI and machine

9
00:00:49,000 --> 00:00:52,000
learning projects.

10
00:00:52,000 --> 00:00:55,840
Unique to this program is the full immersion experience it offers.

11
00:00:55,840 --> 00:00:56,840
Consider this.

12
00:00:56,840 --> 00:01:01,520
You've just been hired as a data scientist working for a streaming media company.

13
00:01:01,520 --> 00:01:05,880
Your mission, should you choose to accept it, is to apply the tools of data science and

14
00:01:05,880 --> 00:01:10,280
machine learning to drive engagement and growth at the company.

15
00:01:10,280 --> 00:01:15,120
The examples, business problems, data sets, and tools you'll work with throughout the

16
00:01:15,120 --> 00:01:18,640
course are all woven around this scenario.

17
00:01:18,640 --> 00:01:25,120
The webinar will be this Saturday, February 15th, at 9.30 a.m. Pacific time.

18
00:01:25,120 --> 00:01:31,360
To register or learn more, visit twimlai.com slash AI Workflow.

19
00:01:31,360 --> 00:01:35,760
And now on to the show.

20
00:01:35,760 --> 00:01:38,640
All right, everyone.

21
00:01:38,640 --> 00:01:41,880
I am on the line with Ababa Barhani.

22
00:01:41,880 --> 00:01:46,800
Ababa is a PhD student at University College Dublin.

23
00:01:46,800 --> 00:01:50,360
Ababa, welcome to the Twimlai podcast.

24
00:01:50,360 --> 00:01:52,840
Thank you so much for having me, Sam.

25
00:01:52,840 --> 00:01:55,120
I'm really excited about this conversation.

26
00:01:55,120 --> 00:02:03,080
We had an opportunity to meet in person after a long while interacting on Twitter at the

27
00:02:03,080 --> 00:02:09,480
most recent Nureps conference, in particular, the Black and AI workshop, where you not

28
00:02:09,480 --> 00:02:16,120
only presented your paper algorithmic injustices toward a relational ethics, but you won

29
00:02:16,120 --> 00:02:17,560
best paper there.

30
00:02:17,560 --> 00:02:22,640
And so I'm looking forward to digging into that and some other topics.

31
00:02:22,640 --> 00:02:29,560
But before we do that, I would love to hear you kind of share a little bit about your

32
00:02:29,560 --> 00:02:30,560
background.

33
00:02:30,560 --> 00:02:35,040
And I will mention for folks that are hearing the sirens in the background, while I mentioned

34
00:02:35,040 --> 00:02:43,600
that you are from University College Dublin, you happen to be in New York now at the AIES

35
00:02:43,600 --> 00:02:50,940
conference in association with Triple AI, and as folks might know, it's hard to avoid

36
00:02:50,940 --> 00:02:53,920
sirens and construction in New York City.

37
00:02:53,920 --> 00:03:00,560
So just consider that background or mood ambiance background sounds.

38
00:03:00,560 --> 00:03:02,560
So you're background.

39
00:03:02,560 --> 00:03:03,560
Yes.

40
00:03:03,560 --> 00:03:04,560
Yes.

41
00:03:04,560 --> 00:03:06,160
How did you get started working in AI ethics?

42
00:03:06,160 --> 00:03:15,120
So my background is in cognitive science and particularly a part of cognitive science

43
00:03:15,120 --> 00:03:23,840
called embodied cognitive science, which has roots in cybernetics and systems thinking.

44
00:03:23,840 --> 00:03:32,360
The idea is to focus on the social, on the cultural, on the historical, and kind of to

45
00:03:32,360 --> 00:03:39,680
view cognition in continuity with the world, with historical backgrounds and all that's

46
00:03:39,680 --> 00:03:46,800
as opposed to your traditional approach to cognition, which just treats cognition as

47
00:03:46,800 --> 00:03:54,400
something located in the brain or something formalizable, something that can be computed.

48
00:03:54,400 --> 00:03:55,400
So yeah.

49
00:03:55,400 --> 00:03:57,400
So that's my background.

50
00:03:57,400 --> 00:04:07,520
And during my masters, I lean towards the AI side of cognitive science the more I delve

51
00:04:07,520 --> 00:04:16,560
into it, the more I, much more attracted to the ethics side, to injustices, to the social

52
00:04:16,560 --> 00:04:26,400
issues, and so the more the PhD goes on, the more I find myself in the ethics side.

53
00:04:26,400 --> 00:04:33,000
Was there a particular point that you realized that you were really excited about the ethics

54
00:04:33,000 --> 00:04:36,800
part in particular, or did it just evolve for you?

55
00:04:36,800 --> 00:04:39,040
I think it just evolved.

56
00:04:39,040 --> 00:04:46,080
So when I started out, I attained of my masters and at the start of the PhD, my idea is

57
00:04:46,080 --> 00:04:53,360
that, you know, we have this new, relatively new school thing, way of thinking, which is

58
00:04:53,360 --> 00:05:00,080
embodied cogsai, which I quite like very much, because it emphasizes, you know, ambiguities

59
00:05:00,080 --> 00:05:08,160
and messiness and contingencies as opposed to, you know, drawing green boundaries.

60
00:05:08,160 --> 00:05:17,000
And so the idea is yes, I like the idea of redefining cognition as something relational,

61
00:05:17,000 --> 00:05:23,880
something inherently social, and something that is continually impacted and influenced

62
00:05:23,880 --> 00:05:28,280
by other people and the technologies we use.

63
00:05:28,280 --> 00:05:33,320
So the technology aspect, the technology end was my interest.

64
00:05:33,320 --> 00:05:42,640
So initially, the idea is yes, technology is constitutes aspect of our cognition.

65
00:05:42,640 --> 00:05:51,160
You have the famous 1998 thesis by Andy Clarke and David Chalmers, the extended mind,

66
00:05:51,160 --> 00:05:56,080
where they claimed, you know, the iPhone is an extension of your mind.

67
00:05:56,080 --> 00:06:04,520
So you can think of it that way, and I was kind of advancing the same line of thought.

68
00:06:04,520 --> 00:06:12,440
But the more I dealt into it, the more ISO, yes, digital technology, whether it's, you

69
00:06:12,440 --> 00:06:19,280
know, ubiquitous computing, such as face recognition systems on the street, or your phone,

70
00:06:19,280 --> 00:06:25,440
whatever, yes, it does impact, and it does continually shape and reshape our cognition

71
00:06:25,440 --> 00:06:28,400
and what it means to exist in the world.

72
00:06:28,400 --> 00:06:35,680
But what became more and more clear to me is that not everybody is impacted equally.

73
00:06:35,680 --> 00:06:45,760
The more privileged you are, the more in control of you are asked to, you know, what can

74
00:06:45,760 --> 00:06:49,840
influence you and what you can avoid.

75
00:06:49,840 --> 00:06:59,600
So that's where I become more and more involved with the ethics of computation and its impact

76
00:06:59,600 --> 00:07:01,120
on cognition.

77
00:07:01,120 --> 00:07:10,160
The notion of privilege is something that flows throughout the work that you presented

78
00:07:10,160 --> 00:07:18,360
at Black and AI, the algorithmic injustices paper, and this idea, this construct of relational

79
00:07:18,360 --> 00:07:19,440
ethics.

80
00:07:19,440 --> 00:07:22,760
What is relational ethics, and what are you getting at with it?

81
00:07:22,760 --> 00:07:23,760
Yeah.

82
00:07:23,760 --> 00:07:29,920
So relational ethics is actually not a new thing, a lot of people have theorized about it

83
00:07:29,920 --> 00:07:32,200
and have written about it.

84
00:07:32,200 --> 00:07:40,240
But the way I'm approaching it, the way I'm using it is, it's, I guess it kind of springs

85
00:07:40,240 --> 00:07:51,120
from this frustration that for many folk who talk about AI ethics or fairness or justice,

86
00:07:51,120 --> 00:07:59,880
most of it comes down to, you know, constructing this neat formulation of fairness or mathematical

87
00:07:59,880 --> 00:08:06,680
calculation of who you should be included and who should be excluded.

88
00:08:06,680 --> 00:08:09,120
What kind of data do we need, that sort of stuff.

89
00:08:09,120 --> 00:08:15,800
So for me, relational ethics is kind of, let's leave that for a little bit and let's zoom

90
00:08:15,800 --> 00:08:19,920
out and see the bigger picture.

91
00:08:19,920 --> 00:08:27,000
And instead of using technology to solve the problems that emerge from technology itself,

92
00:08:27,000 --> 00:08:35,400
so which means centering technology, let's instead center the people that are, especially

93
00:08:35,400 --> 00:08:42,320
people that are disproportionately impacted by, you know, the limitations or the problems

94
00:08:42,320 --> 00:08:47,000
that arise with the development and implementation of technology.

95
00:08:47,000 --> 00:08:55,880
So there is a robust research in, you can call it AI fairness or algorithmic injustice.

96
00:08:55,880 --> 00:09:02,680
And the pattern is that the more you are at the, at the bottom of the intersectional level,

97
00:09:02,680 --> 00:09:08,280
that means the further away from you are from, you know, your stereotypical white cisgender

98
00:09:08,280 --> 00:09:18,000
domain, the more, the bigger the negative impacts are on you, whether it's classification

99
00:09:18,000 --> 00:09:28,000
or categorization or whether it's being scaled and scored by hiring algorithms or looking

100
00:09:28,000 --> 00:09:36,040
for housing or anything like that, the more you move away from that stereotypical category,

101
00:09:36,040 --> 00:09:40,040
you know, the status quo, the more the heavy the impact is on you.

102
00:09:40,040 --> 00:09:49,120
So the idea of relational ethics is kind of to think from that perspective to take that

103
00:09:49,120 --> 00:09:50,960
as a starting point.

104
00:09:50,960 --> 00:09:57,960
So these are the groups or these are the individuals that are much more likely to be impacted.

105
00:09:57,960 --> 00:10:05,160
So in order to put them at advantage or in order to protect their welfare, what do we need

106
00:10:05,160 --> 00:10:06,160
to do?

107
00:10:06,160 --> 00:10:10,960
So the idea is to start from there and then ask for their question.

108
00:10:10,960 --> 00:10:19,960
Instead of, you know, saying here, we have this technology or we have this set of algorithms

109
00:10:19,960 --> 00:10:27,920
or calculations, how do we apply them or how do we then use them to, you know, for a

110
00:10:27,920 --> 00:10:30,520
better or a fairer outcome?

111
00:10:30,520 --> 00:10:37,720
And sometimes the answer you arrive at is that a particular technology shouldn't exist

112
00:10:37,720 --> 00:10:39,520
in a given form.

113
00:10:39,520 --> 00:10:42,160
Yeah, exactly, exactly.

114
00:10:42,160 --> 00:10:53,200
So I think one of the downsides of obsessively working on some matrices or some equations

115
00:10:53,200 --> 00:11:00,280
on fairness is that you forgot to ask in the first place, do we, should we even do

116
00:11:00,280 --> 00:11:02,880
this in the first place?

117
00:11:02,880 --> 00:11:07,160
And I think some people have articulated this really well.

118
00:11:07,160 --> 00:11:13,200
You can think of this in terms of the, you know, face recognition systems that are becoming

119
00:11:13,200 --> 00:11:17,520
very normalized and common, especially in the states.

120
00:11:17,520 --> 00:11:26,440
Do you feed your face recognition algorithm with diverse data in order so that it recognizes

121
00:11:26,440 --> 00:11:33,000
everybody equally or do you stop and think, do we actually need face recognition systems

122
00:11:33,000 --> 00:11:35,680
in the first place, you know what I mean?

123
00:11:35,680 --> 00:11:36,680
Yeah.

124
00:11:36,680 --> 00:11:43,120
And that's a question that, you know, honestly, I have trouble with in a lot of ways because

125
00:11:43,120 --> 00:11:52,160
I think there are certainly problematic uses of facial recognition, but often the question

126
00:11:52,160 --> 00:11:57,600
is posed or the assertion is made that, you know, we shouldn't use the technology or we

127
00:11:57,600 --> 00:12:02,640
should, you know, I guess, you know, it's not uncommon to hear people kind of take this

128
00:12:02,640 --> 00:12:05,880
position of, hey, we can't put the genie back in the bottle.

129
00:12:05,880 --> 00:12:11,120
And, you know, I think on some levels, I get that that, you know, maybe that's a copout,

130
00:12:11,120 --> 00:12:14,040
but in other ways, it's like pragmatic.

131
00:12:14,040 --> 00:12:22,400
How do you balance the idealism that I think is probably core to the approach you're trying

132
00:12:22,400 --> 00:12:31,280
to take with pragmatism that recognizes what is already happening and the way technology

133
00:12:31,280 --> 00:12:34,280
tends to develop and evolve?

134
00:12:34,280 --> 00:12:43,400
I guess my approach, at least in the paper I presented at Neurips is that, and if you're

135
00:12:43,400 --> 00:12:52,000
starting point really is the welfare of, you know, the most disadvantaged, then I don't

136
00:12:52,000 --> 00:13:01,920
know how that cash is out with pragmatism or even with the whole idea of fairness because

137
00:13:01,920 --> 00:13:09,360
for most approaches to fairness, whether it's explicitly laid out or whether it's implicitly

138
00:13:09,360 --> 00:13:19,240
implied, the idea is, it's very utilitarian, in a sense, you have, you aspire to arrive

139
00:13:19,240 --> 00:13:24,320
at, you know, the greatest happiness for the greatest number of people.

140
00:13:24,320 --> 00:13:31,840
So, which really doesn't work for, if you are from a disadvantaged group or if you

141
00:13:31,840 --> 00:13:38,920
are a minority because you will never, you are not a minority, you are not a majority

142
00:13:38,920 --> 00:13:39,920
in the first place.

143
00:13:39,920 --> 00:13:47,920
So, any solutions that aspire to please the majority will always have negative consequences

144
00:13:47,920 --> 00:13:49,840
and it just doesn't work.

145
00:13:49,840 --> 00:13:59,880
So, that's the struggle with when you want to prioritize the needs and the welfare of

146
00:13:59,880 --> 00:14:08,880
the least privileged and on the other hand, some form of pragmatism or what's based for

147
00:14:08,880 --> 00:14:15,240
the majority or, you know, the greater the whole society, that's the tension that's

148
00:14:15,240 --> 00:14:18,400
probably, that will always exist probably.

149
00:14:18,400 --> 00:14:25,440
And so, that's at the heart of this idea of relational in a sense, it's, you know, pragmatic

150
00:14:25,440 --> 00:14:27,440
relative to who.

151
00:14:27,440 --> 00:14:33,760
Exactly, but also, you have other, other face recognition might still be, still arrived,

152
00:14:33,760 --> 00:14:37,040
still open to so much controversy.

153
00:14:37,040 --> 00:14:45,800
But you have other examples such as, you know, Facebook recently got a patent for socio-economic

154
00:14:45,800 --> 00:14:54,080
group classification of their users and they haven't said much about how, where they

155
00:14:54,080 --> 00:14:57,600
are going to apply it or how they are going to use it.

156
00:14:57,600 --> 00:15:04,880
But, you know, tools like that, you can see it's insidious and anything, it's very, very

157
00:15:04,880 --> 00:15:10,760
unlikely, anything positive or anything good will come out of it, especially for users,

158
00:15:10,760 --> 00:15:20,760
for people who's socio-economic status is, you know, from a really poor background.

159
00:15:20,760 --> 00:15:29,480
So, the idea of, I guess, relational ethics as well as questioning, do we need these tools

160
00:15:29,480 --> 00:15:31,560
in the first place?

161
00:15:31,560 --> 00:15:39,800
It's also thinking about, you know, the bigger picture of what automation, whether it's

162
00:15:39,800 --> 00:15:49,960
job applications or whether it's housing or whether it's insurance, it's what it's doing

163
00:15:49,960 --> 00:15:52,680
to society.

164
00:15:52,680 --> 00:15:59,480
And what kind of values are we prioritizing and embracing in the process?

165
00:15:59,480 --> 00:16:09,000
So, it's kind of thinking of ethics more of, more as a habit, as kind of constantly thinking

166
00:16:09,000 --> 00:16:15,360
of what kind of society we want to live in as opposed to thinking of, I have this piece

167
00:16:15,360 --> 00:16:24,360
of tool or this piece of equipment and how do I make it fair or how do I twitch with

168
00:16:24,360 --> 00:16:29,520
it or work it to find that balance?

169
00:16:29,520 --> 00:16:39,520
You mentioned earlier the, you know, that a lot of fairness is thinking about data bias

170
00:16:39,520 --> 00:16:47,040
and accommodating for data bias, you know, setting aside the issue of whether the thing that

171
00:16:47,040 --> 00:16:53,800
you're trying to address, you know, something that should be done at all, that, you know,

172
00:16:53,800 --> 00:17:00,880
the issue of data bias is kind of just one small piece of the overall fairness puzzle.

173
00:17:00,880 --> 00:17:08,080
How do you think broadly about kind of the different aspects of AI fairness and AI ethics?

174
00:17:08,080 --> 00:17:14,160
Do you have a categorization or framework or, you know, way of thinking about it that you

175
00:17:14,160 --> 00:17:15,160
found helpful?

176
00:17:15,160 --> 00:17:16,160
Yeah.

177
00:17:16,160 --> 00:17:23,320
So, this is actually at the heart of the whole relational ethics trying to reframe the

178
00:17:23,320 --> 00:17:25,960
whole idea of what ethics is.

179
00:17:25,960 --> 00:17:34,960
So, because as you said, a lot of people working on AI ethics really are about, you know, whether

180
00:17:34,960 --> 00:17:42,640
it's explainability or calculating fairness or justice, it really is usually lost in

181
00:17:42,640 --> 00:17:44,200
the fine-grained details.

182
00:17:44,200 --> 00:17:51,280
So, it's not something implementable that I provide, but it's about kind of really zooming

183
00:17:51,280 --> 00:17:59,680
out and thinking, you know, what are we doing, what are we prioritizing, how are we defining

184
00:17:59,680 --> 00:18:07,800
bias, how are we defining ethics, how are we approaching these concepts in general.

185
00:18:07,800 --> 00:18:17,480
And one of the aspects that I've read that as part of the paper that I emphasized is,

186
00:18:17,480 --> 00:18:23,520
I guess it's the nature, the inherent nature of machine learning, which is that we are

187
00:18:23,520 --> 00:18:31,400
continually predicting whether it's health issues, whether it's socio-economic issues, whether

188
00:18:31,400 --> 00:18:37,720
it's who is going to be, you know, the best employee, it's all about making predictions

189
00:18:37,720 --> 00:18:41,000
based on whatever data we get our hands on.

190
00:18:41,000 --> 00:18:48,840
So, the idea of relational thinking is kind of rethinking the whole idea of predicting

191
00:18:48,840 --> 00:18:54,160
as something we need to stop and pause and think.

192
00:18:54,160 --> 00:19:01,920
Instead of continually predicting how about we kind of take it easy and first analyze

193
00:19:01,920 --> 00:19:09,160
and think about the patterns that we are getting, trying to understand the reality and things

194
00:19:09,160 --> 00:19:17,760
as they are, as opposed to using whatever data we have as evidence for our prediction

195
00:19:17,760 --> 00:19:23,080
or as input into our predicting tool.

196
00:19:23,080 --> 00:19:30,000
So the one of the examples I give is Cati O'Neill in Weapons of Destruction also mentions

197
00:19:30,000 --> 00:19:41,160
this is if you take algorithms used in policing in the legal system instead of, say, recidivism

198
00:19:41,160 --> 00:19:47,720
algorithms instead of striving to predict who is likely to reoffend or who is likely

199
00:19:47,720 --> 00:19:56,360
to commit crime or what area should be policed more, we use our tools or we develop our tools

200
00:19:56,360 --> 00:20:05,120
in a way that lend themselves for us to understand why are this group of, this demographic

201
00:20:05,120 --> 00:20:10,280
or this group of people coming up as higher risks?

202
00:20:10,280 --> 00:20:11,280
What can we do?

203
00:20:11,280 --> 00:20:17,440
How do we rehabilitate, say, prisoners instead of how do we catch them when they reoffend?

204
00:20:17,440 --> 00:20:19,960
So it's really switching mentality.

205
00:20:19,960 --> 00:20:26,200
It's thinking about how do we make the society or the better, the world a better place?

206
00:20:26,200 --> 00:20:32,800
How do we help people get back on their foot rather than how do we, you know, rather

207
00:20:32,800 --> 00:20:36,760
than playing a gacha rather than how do we catch them again?

208
00:20:36,760 --> 00:20:43,480
But so thinking of kind of prioritizing understanding, you know, questioning why do we find

209
00:20:43,480 --> 00:20:50,200
these patterns that we are finding and how do we improve that really kind of aligns with

210
00:20:50,200 --> 00:20:56,080
this relational thinking I've been, I've been talking about as opposed to, you know, creating

211
00:20:56,080 --> 00:20:59,480
and building these predictive tools.

212
00:20:59,480 --> 00:21:05,920
Yeah, and how it was interesting that there's multiple levels to this idea of prioritizing

213
00:21:05,920 --> 00:21:06,920
understanding.

214
00:21:06,920 --> 00:21:13,680
There's, you know, as, you know, individuals working in these areas, we should prioritize

215
00:21:13,680 --> 00:21:22,760
our understanding of the people involved in the scenarios that were involved in and how

216
00:21:22,760 --> 00:21:26,760
the people are interacting and affected in these various scenarios.

217
00:21:26,760 --> 00:21:33,160
But also, you're also suggesting that we should prioritize, you know, the tools that we

218
00:21:33,160 --> 00:21:38,600
build to enhance our understanding as opposed to, you know, just spitting out more and more

219
00:21:38,600 --> 00:21:39,600
predictions.

220
00:21:39,600 --> 00:21:40,600
Yeah, yeah.

221
00:21:40,600 --> 00:21:41,600
Exactly.

222
00:21:41,600 --> 00:21:50,360
I haven't yet seen many tools that aim to understand so much of what I come across is

223
00:21:50,360 --> 00:21:52,920
always predictive tools.

224
00:21:52,920 --> 00:22:00,360
And I think prioritizing understanding really will contribute to, you know, the larger,

225
00:22:00,360 --> 00:22:03,480
greater, greaterness of society.

226
00:22:03,480 --> 00:22:10,800
Again, this is not something you can formulate or you can come up with a set of steps that

227
00:22:10,800 --> 00:22:11,800
you can implement.

228
00:22:11,800 --> 00:22:19,240
It's, it's more of kind of changing your habit, developing a different set of habits.

229
00:22:19,240 --> 00:22:27,240
It's something you continually keep in the back of your mind, whether you are an ecosystem

230
00:22:27,240 --> 00:22:31,680
or an engineer or a data scientist.

231
00:22:31,680 --> 00:22:38,560
So it's really zooming out and looking at the larger picture.

232
00:22:38,560 --> 00:22:49,080
But also, it's not to oppose that we should throw out all implementable tools we have on

233
00:22:49,080 --> 00:22:57,720
whether it's fairness or accountability or explaining, explainability, and it's just

234
00:22:57,720 --> 00:23:01,800
that we have to also look at the larger picture.

235
00:23:01,800 --> 00:23:09,960
And I guess another aspect of relational ethics is you might have these implementable tools,

236
00:23:09,960 --> 00:23:18,280
you might have the set of tools to make your system better, but the idea is if you think

237
00:23:18,280 --> 00:23:28,000
of these concepts such as fairness or ethics or even your own set of solutions as something

238
00:23:28,000 --> 00:23:30,200
that are continually changing.

239
00:23:30,200 --> 00:23:35,800
So this is at the, I guess this goes back to at the start I was talking about how the

240
00:23:35,800 --> 00:23:45,760
idea of embodied cognitive science at its core comes from systems thinking and cybernetics

241
00:23:45,760 --> 00:23:54,360
and the social sciences, and at the heart of it is that not only can you define cognition

242
00:23:54,360 --> 00:24:01,160
in isolation from others or in isolation from the tools you use or in isolation from

243
00:24:01,160 --> 00:24:07,320
the environment, it's also that whatever your definition of cognition or whatever your

244
00:24:07,320 --> 00:24:15,440
understanding of the person has to account for the nature of reality, which is that its

245
00:24:15,440 --> 00:24:22,640
never stable, it's never fixable, it's constantly changing, so and it's very contextual.

246
00:24:22,640 --> 00:24:31,240
So you are some a certain type of personality at the moment with certain expected norms

247
00:24:31,240 --> 00:24:37,760
talking to me on the on here, but some other time in some different contexts, in some

248
00:24:37,760 --> 00:24:42,080
different environments, you are also slightly different person.

249
00:24:42,080 --> 00:24:49,080
So the underlying idea is whatever concepts we are dealing with, whatever solutions we

250
00:24:49,080 --> 00:24:57,640
have, they cannot claim to to finalize things, they cannot stabilize this continually moving

251
00:24:57,640 --> 00:25:04,840
nature of being and whatever is ethical in this context might not be ethical in other contexts.

252
00:25:04,840 --> 00:25:11,840
So I think relational ethics helps you leave whatever solution you have somewhat partially

253
00:25:11,840 --> 00:25:19,760
open, so that you can reiterate, so you can revise and change with whatever new evidence

254
00:25:19,760 --> 00:25:25,600
or new data comes up your way the next day or the next year.

255
00:25:25,600 --> 00:25:33,880
So this treating of things as moving and changing really is fundamental, it helps us realize

256
00:25:33,880 --> 00:25:43,600
our solution now is only for now with an unlimited context, with an unlimited environment.

257
00:25:43,600 --> 00:25:49,800
And I think that's a really important thing we can all pay attention to.

258
00:25:49,800 --> 00:25:55,440
The example you gave of who is Sam in these different contexts makes me think a little

259
00:25:55,440 --> 00:26:04,280
bit about and linguistics, the idea of code switching and I may speak in a particular

260
00:26:04,280 --> 00:26:08,920
way when I'm on the podcast and then when I'm at home, I may speak in a slightly different

261
00:26:08,920 --> 00:26:14,680
way and when I'm out in the neighborhood, I might speak in a slightly different way.

262
00:26:14,680 --> 00:26:19,920
And I haven't seen much in machine learning or NLP that tries to capture that or take

263
00:26:19,920 --> 00:26:28,400
a account of that. Do you have some examples of examples of how you might envision machine

264
00:26:28,400 --> 00:26:34,160
learning systems, you know, if they were to follow this aspect of relational.

265
00:26:34,160 --> 00:26:46,160
Yeah, so this is really difficult and I guess at the heart of a lot of issues and so

266
00:26:46,160 --> 00:26:56,440
when you can assume things are stable and somewhat, you know, you can grasp them with whatever

267
00:26:56,440 --> 00:27:04,880
tools or language you have, it's much easier to construct theories or to construct some

268
00:27:04,880 --> 00:27:13,280
sort of tool which is why I mean that this stability, you know, translates to the,

269
00:27:13,280 --> 00:27:17,920
you know, everything's coming from a identical distribution, which is at the foundation

270
00:27:17,920 --> 00:27:21,120
of most of what we do in machine learning.

271
00:27:21,120 --> 00:27:27,040
Yeah, again, I'm not really a computer scientist, as I say at the beginning, I'm a cognitive

272
00:27:27,040 --> 00:27:35,800
scientist and I think about cognition in persons and I don't know any NLP tools or machine

273
00:27:35,800 --> 00:27:45,720
learning approaches that account for this continual change and contexts.

274
00:27:45,720 --> 00:27:51,920
But also even within the cognitive science movement, especially embodied cognitive science

275
00:27:51,920 --> 00:28:02,520
which is trying to push the importance of these change, you know, language and context

276
00:28:02,520 --> 00:28:11,000
is one of the things it struggles with is because it's difficult to formalize and make

277
00:28:11,000 --> 00:28:20,520
up, provide something conclusive, but when you are underlying change, it ends up dealing

278
00:28:20,520 --> 00:28:27,000
with a lot of theorizing as opposed to producing something, something you can model or something

279
00:28:27,000 --> 00:28:34,120
you can formalize. So I guess it's an existing tension.

280
00:28:34,120 --> 00:28:42,840
Again, it's much better to think of it as a habit and to acknowledge this continual

281
00:28:42,840 --> 00:28:51,720
changing nature of things in a sense that acknowledgement makes you aware that your

282
00:28:51,720 --> 00:28:59,720
tool or your solution or your theory is only as good as the, it's a specification and

283
00:28:59,720 --> 00:29:10,440
the context and that acknowledgement further encourages you to live not to conclude your

284
00:29:10,440 --> 00:29:17,320
solution or your tool as something finalizable, something that will be good all the times

285
00:29:17,320 --> 00:29:24,560
for all contexts, but something that you have to leave a little open partially open, something

286
00:29:24,560 --> 00:29:34,120
that needs revision continually. So that's again, for me, for me at the moment, the

287
00:29:34,120 --> 00:29:43,560
best one can do is acknowledge this change and context and leave this partial openness

288
00:29:43,560 --> 00:29:55,320
and embrace reiteration and revision. One of the other ideas in the paper is that, or at

289
00:29:55,320 --> 00:30:01,880
least I, you know, I interpret it as that along the lines of the idea of prioritizing understanding

290
00:30:01,880 --> 00:30:07,160
over prediction, one of the ideas in the paper is that, you know, when we predict, it's

291
00:30:07,160 --> 00:30:13,480
often based on these very reductive labels that we're applying to things. The examples

292
00:30:13,480 --> 00:30:20,120
you gave are successful versus not criminal versus not, and you kind of point out that that

293
00:30:20,120 --> 00:30:29,640
is inherently problematic in many cases. Exactly. You can also look at a lot of algorithms

294
00:30:29,640 --> 00:30:38,440
with them trying to categorize or identify gender identities and I think that's one of the

295
00:30:38,440 --> 00:30:49,400
most obvious cases where the harm of doing so, the harm of categorization becomes very starkly clear

296
00:30:50,360 --> 00:30:56,920
because usually stereotypically and in most societies, you would categorize

297
00:30:56,920 --> 00:31:09,160
gender as, you know, a male or female. Sometimes you might have bisexuals, but as we know, gender

298
00:31:09,160 --> 00:31:18,360
identities are much more, more than just those categories and not only they are larger in number

299
00:31:18,360 --> 00:31:28,840
those categories, but we all know that they are fluid. Someone that was bisexual or take a

300
00:31:28,840 --> 00:31:36,600
trans person, for example, people change their sexual and gender identities. How do you then?

301
00:31:36,600 --> 00:31:43,800
Then it becomes easy to see how difficult it is for whatever algorithmic tools we are developing

302
00:31:43,800 --> 00:31:52,760
to account for that change. But also, as we developed that tool in category, kind of come up with

303
00:31:52,760 --> 00:31:58,840
these categories, in a sense where this advantage and excluding anybody that doesn't

304
00:31:59,560 --> 00:32:06,440
belong in those categories that we have created, this is where, again, the most vulnerable are,

305
00:32:06,440 --> 00:32:13,800
you know, impacted the most. Yeah, so it's problematic in that regard.

306
00:32:15,080 --> 00:32:17,960
I'm curious what kind of reaction you've seen to the paper.

307
00:32:18,920 --> 00:32:29,000
So, at Nureps, it was overwhelmingly positive. It was my first time in Nureps and I went in thinking,

308
00:32:29,000 --> 00:32:33,160
oh, this is, you know, a machine learning AI conference. I'm just, you know, the

309
00:32:33,160 --> 00:32:42,280
outlier cognitive scientist, slash atheists. So I went in feeling I'm not going to fit very well.

310
00:32:43,080 --> 00:32:50,600
But it was really, really positive. And I was, even when I was, when the announcement that

311
00:32:50,600 --> 00:32:58,680
my paper had won the base paper came, I just could not believe it. You know, as a grad student,

312
00:32:58,680 --> 00:33:05,000
you go to conferences, you present a poster or whatever. And some parts of you sometimes,

313
00:33:05,000 --> 00:33:09,640
you know, deep down, you think, oh, I might win, you know, I might have a chance for a

314
00:33:09,640 --> 00:33:15,400
base poster or something like that. But I went into Nureps, like, there is no chance, I'm just

315
00:33:15,400 --> 00:33:23,800
going to relax. Enjoy the dinner. It was dinner party. And I was really shocked. And so it's,

316
00:33:23,800 --> 00:33:32,200
it's been really positive. I have presented similar ideas previously to very exclusively,

317
00:33:33,080 --> 00:33:40,840
kind of very software engineer, machine learning, deep learning, researchers. And

318
00:33:41,960 --> 00:33:47,320
people really are not interested in my ideas because people want something implementable,

319
00:33:47,320 --> 00:33:55,000
something they can code into, you know, something formal, something they can use. So what I'm asking

320
00:33:55,000 --> 00:34:04,360
is a reframing, a rethinking and in a sense, a changing of habits. And it's almost like an

321
00:34:04,360 --> 00:34:09,960
activism. It's like asking, what kind of society do you want to live in? So for some people,

322
00:34:09,960 --> 00:34:17,880
that's really difficult and something they would rather not get involved in. But the more I

323
00:34:17,880 --> 00:34:25,560
interact with people, but and also on Twitter, it's, it's really, really encouraging. People seem

324
00:34:26,600 --> 00:34:35,240
to like what I have to say. So I'm happy. Really quickly before we wind down, you are in New York

325
00:34:35,240 --> 00:34:41,640
to present a more recent paper. I believe it's more recent paper that you have worked on robot

326
00:34:41,640 --> 00:34:48,680
rights. Can you talk a little bit about that paper? Yes, it's unfortunate that the title has

327
00:34:48,680 --> 00:34:56,120
robot rights because it really is not about robot rights. It's robot rights question mark.

328
00:34:56,120 --> 00:35:03,640
Let's talk about human welfare instead. So this paper, I worked on it with my colleague,

329
00:35:03,640 --> 00:35:10,760
Yeli Van Dyke from University of Twenth. He also, he also comes from a distributed

330
00:35:10,760 --> 00:35:16,920
cognition in body cognitive science background. And we talk about this a lot on Twitter.

331
00:35:17,800 --> 00:35:25,640
And we are constantly getting caught up in this Twitter debates whether, you know,

332
00:35:25,640 --> 00:35:32,920
machines can be sentient or whether robots should be given rights, blah, blah, it goes on.

333
00:35:32,920 --> 00:35:40,600
And it's the same kind of pattern of interaction. So very and over and over again. And I think about

334
00:35:40,600 --> 00:35:46,040
five, four, four, five months ago, I asked him on Twitter, how about we write a paper on this?

335
00:35:46,600 --> 00:35:53,320
And writing the paper came really, really easy because we have the same background. We think I like

336
00:35:53,320 --> 00:36:01,880
the idea of the paper is it has in a sense, it's twofold. The first one is kind of philosophical.

337
00:36:03,080 --> 00:36:12,360
So we lay out how robots are not the type of beings that can either be granted or denied rights.

338
00:36:12,920 --> 00:36:21,640
We lean on a lot of, you know, embodied coxay as I was saying earlier, this notion of cognition

339
00:36:21,640 --> 00:36:28,600
as inherently social, inherently relational, people inherently, you know, value-laden,

340
00:36:28,600 --> 00:36:36,840
constantly striving to make meaning of the world. So we use that post-cartesian approach to

341
00:36:37,800 --> 00:36:44,120
to get at the heart of how philosophically speaking robots or animation learning tools

342
00:36:44,120 --> 00:36:53,160
or any missions at all are not the same beings as humans or even animals. And then the second part

343
00:36:53,160 --> 00:37:02,040
we get at the urgent questions that AI ethics really need to focus. Because sometimes, not sometimes,

344
00:37:02,040 --> 00:37:10,120
most times, it's really frustrating to hear robot ethics classified as part of AI ethics. And

345
00:37:10,120 --> 00:37:18,440
for me, personally, it comes across as worrying about future, may happen, may not happen, may become

346
00:37:18,440 --> 00:37:27,720
sentient. A lot of it is really contemplation and thinking ahead about the future. And it's

347
00:37:28,440 --> 00:37:35,560
all that contemplation and philosophical musing, taking so much of the AI ethics space is just unfair.

348
00:37:35,560 --> 00:37:44,760
So a lot of the second part of our paper deals with how the very idea of AI itself, whether it's,

349
00:37:44,760 --> 00:37:51,480
you know, computer vision, whether it's autonomous systems, it's never autonomous. There is

350
00:37:51,480 --> 00:37:58,760
always humans in the loop. And not only that, it's not possible without the exploitive human

351
00:37:58,760 --> 00:38:06,440
labor, whether it's tagging your old data for that's going to be part of an autonomous system,

352
00:38:06,440 --> 00:38:13,000
or some sort of image recognition. Even when you do recaptures, you are in a sense

353
00:38:15,080 --> 00:38:22,600
kind of being, you are putting in your own unpaid labor into making mission space.

354
00:38:22,600 --> 00:38:30,440
So the argument we made, we make there is that AI systems are never autonomous and they never

355
00:38:30,440 --> 00:38:37,800
will be. But in the argument of whether they are autonomous or not, we lose sight of the people

356
00:38:37,800 --> 00:38:46,520
who are underpaid, such as the mechanical Turk or micro workers, they never enter into the

357
00:38:46,520 --> 00:38:55,000
debates. We also teach upon, you know, how the robot systems, such as, you know, Roomba or whatever

358
00:38:55,000 --> 00:39:02,200
are invading private spaces as they roam around our houses and how that should be more

359
00:39:03,000 --> 00:39:11,640
urgent and crucial as opposed to some stereotypical humanoid, such as Rob, what's your name, Sophia.

360
00:39:11,640 --> 00:39:20,360
And yeah, and we get a little bit on algorithmic injustice as well, how the least privileged,

361
00:39:21,240 --> 00:39:28,520
the most disenfranchised are the most impacted and how that should be the focus of AI ethics

362
00:39:28,520 --> 00:39:34,040
as opposed to, you know, hypothetical sindian things. So that's the core of the paper.

363
00:39:36,600 --> 00:39:41,080
And there's been quite a bit of discussion about this one on Twitter. In fact, we're not going to

364
00:39:41,080 --> 00:39:49,640
be able to get into it very deeply, but I would encourage folks to take their reactions to Twitter.

365
00:39:49,640 --> 00:39:53,160
Is it fair to say that this one has been more controversial than the previous one?

366
00:39:53,160 --> 00:40:00,760
It appears to be so. And to be honest, when we wrote it, we wrote it as, when we have those

367
00:40:01,960 --> 00:40:07,640
never ending conversations about rights again on Twitter, instead of repeating the conversation,

368
00:40:07,640 --> 00:40:16,360
we'll just have a paper to point to. But it has provoked a lot of very strong reaction from people

369
00:40:16,360 --> 00:40:24,440
both defending rights for robots and both thinking it's really idiotic to even discuss rights for

370
00:40:24,440 --> 00:40:31,400
robots. So apparently it's controversial. Interesting. People love their robots, I guess.

371
00:40:31,400 --> 00:40:41,800
Yeah, I guess. Yeah. Yeah. Well, Ababa, it has been so great to have a chance to chat with you

372
00:40:42,520 --> 00:40:47,560
in more detail about what you're up to. Thanks so much for taking the time to share with us.

373
00:40:47,560 --> 00:40:50,200
Thank you so much. It's been great. Thank you.

374
00:40:52,200 --> 00:40:57,080
All right, everyone. That's our show for today. For more information about today's guest,

375
00:40:57,080 --> 00:41:04,120
visit twommalai.com slash shows. To learn more about the AI enterprise workflow study group

376
00:41:04,120 --> 00:41:10,680
I'll be leading, visit twommalai.com slash AI workflow. Of course, if you like what you hear on

377
00:41:10,680 --> 00:41:16,600
this podcast, please take a moment to subscribe, rate, and review the show on your favorite

378
00:41:16,600 --> 00:41:30,120
pod catcher. Thanks so much for listening and catch you next time.

