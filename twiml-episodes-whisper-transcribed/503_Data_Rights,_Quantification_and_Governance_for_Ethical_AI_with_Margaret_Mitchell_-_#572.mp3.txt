All right everyone welcome to another episode of the Twemble AI podcast. I'm your host Sam
Charrington and today I'm joined by Meg Mitchell, a chief ethics scientist and researcher at Hugging
Face. Before we get into today's conversation be sure to take a moment to head over to Apple
podcasts or your listening platform of choice and if you enjoy the show please leave us a five-star
rating and review. Find us just over a year ago with a friend of the show, another friend of the
show I should say Emily Bender. That was episode 467 for those keeping track where we discussed
your paper on the dangers of stochastic parrots and of course a ton has changed for you since then.
Welcome back to the podcast Meg. How's everything going? You thank you. Yeah it's great to be here.
I should say that the paper wasn't on the dangers of stochastic parrots. It was on the dangers
of large language models comparing the two stochastic. We don't have a fear of parrots.
So the title on the ACM library is on the dangers of stochastic parrots can language models be too big?
I feel like for this one we should just leave Amari as the voice of God coming in and
make us what the title is paper is. In any case Meg welcome back and let's have you reintroduce
yourself to the audience. Yeah so I am a computer scientist with a background in natural language
processing as well as a bit in computer vision and then for the past six or so years I've been
working on ethical AI which brings in a lot of things about natural language processing and
computer vision in terms of issues like foreseeable harms and misuse and things like that.
And I've worked at Microsoft Research, Google, Google Brain and then recently joined hugging
face full-time which is a startup that is an open source community making machine learning
models and data sets generally available. Awesome awesome and are you the first chief ethics
scientist at the company? Well we're a startup so we there aren't exactly
very set in stone titles. You can kind of start coming up with your own titles for things.
I was definitely the first AI ethics computer scientist. I'm excited to share that we're going
to hire a for real ethicist which will be amazing. It's pretty rare to be able to hire an ethicist
in an AI company. I think that that is something that only came into people's attention very recently.
But yeah I mean I came to work at hugging face because I really wanted to have some time to
just code and code open source. At the larger tech companies you find as you sort of rise higher
and higher in the hierarchy you have less and less time to code and it's replaced more and more
by meetings which is sort of the opposite of what you want as you succeed right. Like if you love
coding and you succeed more why would that be taken away in favor of meetings which I absolutely
hate. So after having you know a couple years where basically it was really hard for me to get
coding time in. I really wanted to join hugging face and just sort of not worry about tons of
meetings not worry about tons of sinking across a large organization because it was already small.
I could really just you know get my hands dirty with code and starting to put out some tools
that I really really wanted to prioritize. And then once I got that out of my system a little bit
I was able to start popping up a little more and working more on things like defining the culture
hiring practices, different kinds of processes that we want to put in place for consideration of
what models get shared, what data get shared and things like that. I've been doing a little less
coding but it's still a coding company essentially. So getting to do a mix of defining top down
structures and processes in the company focusing on like diversity and inclusion and ethical
considerations as well as more bottom up just you know pure coding to share with the world.
So it's a nice mix. Got it got it. And the the coding that you're doing is that
been largely focused or tied into the area of ethics or is are they two separate kind of things
that you find ways to connect. Yeah so I would say that all the coding that I'm prioritizing right
now is intimately tied to current discussions around AI ethics. So you'll see that over the past
year so there's been more and more scholarship in the AI ethics world about data and data sets and
things like benchmarking and the fact that the culture of machine learning has generally had
a really laissez-faire approach to data collection and data sharing. But if we're trying to think about
the long-term effects of technology and the long-term effects of things like sharing data
then we need to think very critically about things like data curation instead and what does
data curation mean? It means things like what kind of values do we want to encode in the data
as well as in the data set development process in order to create data sets that foreseeably
can help more than hurt. And so that means things like coming up with measurements,
quantifications of the data that can be inspired by human values. So there's been some work on this
over the past year as well, but I'm trying to really step that up a bit. And so what does that
mean? That means coming up with metrics for the demographic diversity within a data set,
coming up with metrics for stereotyping. How much a data set might run the risk of propagating
harmful stereotypes? Doing things like measuring the naturalness of the language, how contrived it is
versus how much it actually reflects the kind of language that we'd expect to see when a model
trained on it is actually used, that kind of thing. So data development has been a large focus of
mine and particularly through the lens of what can we measure aligned to human values and what we
wanted to develop. Awesome, awesome. I also want to bring up your participation in the Wiki M3L workshop
at iClear. Can you tell us a little bit about that workshop that goes with the workshop and
what you're speaking about there? Yeah, so it's an interdisciplinary workshop that's focusing
on a lot of different topics. I think part of the idea was essentially to bring together people
who are working on different aspects of AI and how those aspects can be really relevant to the
community. So this is joint with Wiki Media and Wiki Media is really passionate about having
everything be open and transparent and so what that means for computer vision and LP,
machine learning, what we should make available, what we shouldn't, what that means in terms of
the uses of the models that we're developing and then how they might actually be used.
All of that is sort of under the umbrella of this workshop. So touching on a lot of different
issues relevant to the socio-technical context of machine learning. And your particular session
at the workshop? So I get to be involved into one is a panel. The panel is looking at
multi-modality which is an area I've worked on to do things like image descriptions for people
who are blind. And then also there's a keynote I'm giving on biases in AI. The session is called
biases in AI and indigenous data sovereignty. So there's another speaker. I believe his name is
Michael Running Wolf. Awesome. And I should mention I mentioned the workshop by its short name Wiki
M3L but that is Wikipedia and multimodal and multilingual research. And so your conversation
at the and presentation at the workshop focus on data governance and biases in AI that sounds
not very far field from the data curation work that we talked about about earlier. What's the
specific angle that you're taking at the workshop? So my plan so far subject to change
still still chatting is to talk about the kinds of assumptions and values that get encoded
when we are creating and sharing data sets. So there's a lot of things you can talk about when
you say biases in AI that means tons of different things. But my particular interest I think
relevant to this workshop is around how we have inclusive sharing of data sets and creation of
models that don't disproportionately underperform on some sub-populations or sort of exploit some
populations in the data collection practices. So this has a little bit more to do with biases
from the perspective of how we're approaching data set collection, what are sort of internal
cognitive biases and cultural biases are doing in our approach to data and the kind of models we
develop as opposed to something that's I think more traditional bias than AI where people think
of things like fairness. I won't be focusing on fairness. I'll be talking more about the context
of of data and how they relate to models and advocating for things like data sovereignty
which is the other part of that session where individuals and communities who are creating the
data have rights for that data as opposed to the current state of the art which is a set
essentially being exploited without consent to have their data used in models that are then
productionized for profit and things like that. So focusing more on the on the data sovereignty side
of things. On the biases side of things it sounds like what you're alluding to is this idea that
as we as practitioners, researchers, a community got and collect these data sets, there are inherent
ways that we think about the world that are influencing the ultimate results of these data sets.
Can you talk a little bit more about the specific examples and how that comes to be?
This is also really relevant to work I've been doing in big science which is this massive effort
international effort with volunteers from a bunch of different countries working on training
this large language model. And I'll just jump in to suggest the folks that want to learn more about
that can check out my recent interview with your colleague Thomas Wolff that's episode 564.
And so in that I'm unsurprisingly working on data as well but this is
data is more and more my thing I think in part because it's generally been less appreciated than
ML work. I mean data collection practices have morphed over the years. You'll see in some of the
first larger data sets that that were developed for use in what was at that time called corpus
linguistics were very carefully balanced made sure to pay attention to the licenses and rights
of the various people who created the data. So we see things like like the brown corpus
coming around in the early 60s actually taking a look at things like fiction and sports and travel
and all these sort of different topics and trying to be really balanced and making sure that they
have agreement from the holders of the data that this is okay to use and distribute things like that.
You know fast forward into the 90s where we're starting to see a switch from corpus linguistics
to computational linguistics. I mean there's still separate fields but there's more and more
changing at that time in the kinds of work that corpus linguists are doing more towards computational
linguistics and what that means is that you start trying to get bigger and bigger data sets
where the size matters a lot more than the quality. So quantity over quality sort of thing and then
then moving into oh I should say but at that time there was still a lot of respect for the rights
and licenses and so a lot of the data collected in the 90s was from newswire because those were
the sources that would allow this public use into the 2000s as you start to have web 2.0 basically
you start to see a lot of content produced online from individuals reflecting more natural language
right so when we work on natural language processing newswire is just some some sort of contrived
kind of way of talking you know it has like a lot of norms you have to abide by whereas regular
conversation or what we're writing online is a lot more reflective of sort of everyday language use
so with kind of the birth of web 2.0 you see people collecting data from the web
sort of mining it to create larger data sets to build models on with less and less attention paid
to what those data sets actually were representing and more attention paid to what can I get that's
relevant to the topic or task that I care about and so with that you see a little bit more of a
loss of rights this also relates to the fact that when we communicate online our data can be
technically owned by the platform right so when we tweet that data is owned by twitter
you know things like that and so even people who are creating really relevant data
and this can be called data labor you know essentially creating the content themselves they don't
end up having as many rights to that actual data where other companies can use that data
it could be further shared to help to help build other sort of models or to help other companies
without the people actually creating the data's without the knowledge of the people actually
creating that data and then even when you are putting in place some things that clearly has
the intention of rights so even when you are putting on things like creative comments licenses
that kind of thing it doesn't actually map to how your data can legally be used by
by anyone at least in practice right so let me jump in and say what I'm hearing is you know this
kind of field of play around creating these large language models and other kind of large models
has evolved quickly and you know there's a lot of rules that haven't been established or conversely
norms that aren't necessarily you know universally beneficial have established I'd love to have you
talk about the specific harms that you see at play like clearly there's this idea of economic
harm in a sense of people are contributing this data other companies are monetizing it in ways
that weren't necessarily disclosed to the people that contributed the data and the benefit isn't
being shared and you could you know argue whether that's theoretical harm or actual harm but
there's this idea of an economic harm there's idea this idea of I should have the right to give
you permission or not I don't know how to phrase what the right phrasing would be for the harm that
is involved in that but those are a couple of examples of harms and I just wanted to to have you
elaborate on that and further develop you know is there a taxonomy of harms that has arisen from
the way that we're collecting these datasets yeah so there there has been work on on taxonomy
of harms I think people in in different groups have come up with their own there isn't as far as I
know a generally agreed upon kind of taxonomy but one of them one of the key issues is around privacy
so when we share content online we have a basic understanding or trust that what we're producing
is not going to not going to be suddenly on some graduate students laptop or something like that
so if you have information about your personal life that can be used to track you if I mean there's
also things like personally identifying information so with just a few characteristics of an
individual you can often identify who they are which can give rise to things like stocking to
things like identity theft as well as just sort of the very the very personal issue of having things
you say be used by other people without you really knowing and potentially even use for technology
that you don't want to support I think Flickr is a really great example where it's been mined for
years for computer vision applications and so some of that some of that data is used for things
like facial recognition but theoretically and I think you know in reality a lot of people whose
faces were mined don't actually want to help build facial recognition and so there's a disconnect
there between what people's intent and expectations are when they're creating the data and how that
data is actually being used in a way where it could potentially even come back and harm them so
for example facial recognition does not work as well for black people that as white people there's
an argument to be made that maybe white people don't want to have their their faces shared as much
if it's going to create that kind of issue there's a concern that facial recognition is used as
a form of discrimination so you know black people tend to be more targeted and so black people who
have their data as part of what's using what is being used to train a facial recognition system
is really not a situation that they ever hoped to be in where it can come back and harm them if
they're targeted so it ends up creating these these problematic kinds of cycles where the beliefs
and intentions of people who are creating the data are not actually met by how different
companies and organizations end up using that data potentially to disrupt their privacy
as well as as well as things like creating technology that could come back to hurt them
time back to your earlier comment about connecting the inherent beliefs of the data set
collectors to the data sets and these these problems is it you know by the inherent beliefs of
the the folks collecting data sets you know are you meeting things like disregard for
you know the the privacy considerations or are there specific more specific things that
you're trying to get out there yeah disregard for but also private information that that can be
used to identify people so you know if you have bad actors malicious actors your data can potentially
be mined to steal your social security number credit card number these are the sorts of things
that end up being stolen all the time as well as you know figuring out personal information about
you which is a kind of fundamental harm when it comes to psychological safety and so when you
think about where we are from a data rights perspective what's the kind of frontier the research
or frontier of the practice is it primarily what we're seeing around you know legislative and
regulatory action or you know how do you think about the state of the world yeah so there has been
a lot more work in the regulatory space on data protection data protection laws I think most
people are familiar with GDPR but a lot of countries very recently have put forward some other
sorts of data protection legislation and and one of the ideas underlying this is that if people
create the data they own the data and that really takes away the profit models of companies like
you know Google that uses this information in order to essentially get people to look at ads more
so you know if they don't get to own that data then you know that opens up
tons of other companies that that can use it or also or it also takes away their ability to use
that data at all and so right and so you see that there is there are ideas around people needing
to consent to have their data being used and so particularly in China's new legislation around
data protection there's this idea that if you have an instance from an individual and that
individual is an anyway identifiable you have to go and find that person and ask for their
consent to use that data so you see a little bit of individual data rights starting to pop up
that said you know geopolitical entities like the EU are also trying to put together things
like data storehouses where that data is open to everyone so it's a little bit of the reverse
where maybe you don't have have rights to it and instead the the ability to use that data can
be distributed amongst everyone in the country in order to you know maximize the overall profit
in the geopolitical entity and so there is a little bit of a push and pull I think right now where
you see some legislation moving in the direction of individuals having some rights over their data
at the same time as you see some legislation coming out that that sort of proposes that an
individual's data should be open for everyone to use rather than an individual's company so I
think there's there's a lot of discussions right now in which way these things should be going
you know I'm obviously personally a proponent of the idea that everybody should own the data
that they create and then they should be able to consent to whether or not it's it's shared more
broadly but you know I might be in the minority there but all the more reason to sort of talk on
these kinds of programs and present at that places like the the wiki m3l workshop I hadn't heard
about the data privacy regulation in China that you referenced and some ways it flies a little bit
in the face of the broader narrative about privacy in China the social credit score on all these
things yeah do you have any additional perspective there I'm really confused by that actually
so I read through I read through their legislation you know not not being a lawyer and you know
sort of ask people who are more legal scholars if my interpretation of what it was saying was
right and it seems like it is right and so I'm really not sure what's going on there I feel like
it might be someone it might be the job of someone who's more knowledgeable about about China and
you know China's government and and what they're trying to sort of get out there I'm really yeah
I'm I'm very confused by it personally it'd be sort of ironic I suppose if China leads the way
an individual data rights but we might be moving in that direction right now well if anyone knows
anyone who's listening to anyone who's listening to this knows anyone let me get I know yes please
I want to understand you know returning to data curation you can talk a little bit about some
of the ways that your work around measurement and quantification helps you kind of think through
and address the kinds of issues that we've been talking about so far yeah so we want to understand
the quality of data and part of what I'm pushing for is prioritizing quality over quantity a little
bit more arguably if you have good quality data you don't need as much as much of the quantity of
data which means that you can be a lot more selective about whether or not you are including
personal information or personally identifying information whether or not you're including
things like stereotypes and biases which can then be laundered through a model and then pushed
back out at the community affecting all kinds of decisions that happen under the hood and so if
you can start to measure for example the strength of association between a term like smile and
woman versus smile and man in some slice of data that you're thinking of then if it's a very
high association and you want to be working towards data that can reflect a more equitable universe
then perhaps you don't want to sample that data directly perhaps you want to do something a
little bit more clever in order to make sure that there isn't such a strong skill so by being able to
to measure these kinds of skews come up with actual measurements that can do this now we can
start quantifying serious issues in the data and using that to inform what we what we do collect
one of the ways that one of the early ways that as a community we've sought to
do a better job around understanding the biases and data sets are kind of like data sheets for
data sets and that workshop that body of work by Timnett and others Timney and the model cards and
it sounds like what you're doing connects to those efforts increasing to sophisticated you know
those things are are dashboards you're increasing the sophistication of the metrics exactly exactly
and actually I was the first author on model cards so I would say you know the so I'm working on
model cards still and it goes to one big part of that is what you evaluate and so that again
goes to the data so I've really been focusing on that aspect of model cards and the kind of things
you want to quantify in the data and I've also been working on data cards which is heavily inspired
by data sheets but also different data cards are shorter they're a card not a sheet so
just make sure I understand that yeah well so data sheets are pretty comprehensive in terms of
all the different decisions that go into creating a data set so they're really wonderful artifacts
to have in terms of auditing the development of machine learning systems they're a little bit
more difficult to do in terms of getting developers to actually implement them so if you're you
know asking someone who who is trying to upload their data sets all the details of you know the
compensation protocols from crowd workers and things like that they're going to be a lot less likely
to to fill it out at all and so you know data cards are trying to get a bit at how can we
incentivize good practices or how can we encourage good practices while still you know while
still being somewhat palatable to people who are who are uploading and and sharing the data set
so it's not as ideal as data sheets it's it's sort of speaking to the cynical reality that a lot
of people don't care and how do we sort of get them to start caring and so so part of that is
creating measurements that can be automatically run on the data set and then be used to fill out
the data card so that goes back again to how to quantify things in the data set like the biases
and skews like the naturalness which side note this is this is related to the zip fee and distribution
of language it's super fun to implement and measure but then you know so but these measurements
are things that can just be run automatically and the more things that you can run automatically
the more buy and there will be to have things like data cards released alongside your data
so we're trying to sort of lower the barrier to having some sort of you know responsibility informed
development of data and sharing of data by providing some automatic quantifications
that speak to values of discrimination that speak to or I should say speak to values of
non-discrimination that speak to ideas around having naturalistic data or data that's balanced
a bunch of variety of topics all these kinds of things that we ideally can have in a perfect data
curated setting but but coming up with those measurements automatically so that people can see
what kind of characteristics their data actually have and then people can actually decide whether
not to use it for the various you know use cases that they that they foresee and so so this data
measurement area of work that I'm digging into more and more is really to help lower the barrier
to create just sort of more responsible AI development life cycles and really being clear about
what the data is encoding on the topic of buy-in the cycle of adoption isn't surprising but it's my
sense that after kind of a while you know a couple of years of not hearing a lot about data sheets
data cards after their proposal and introduction now I'm starting to hear about their use quite
frequently you know at least relatively is that your sense as well yeah well so personally I would
say that my research has generally been around three years ahead of the general pickup but it's
true I mean I've just noticed it again and again which is great for your citations because then it
ends up being that your work is foundational so I think that's generally true for for me with
the model cards work and it's also true for for Tim neat with the data sheets work because you know
when you're working in AI ethics you're thinking very very deeply about issues at a level of nuance
that people who who aren't working on AI ethics day to day don't really see so it's not until you
start seeing the issues that you start paying attention to the AI ethics things and then that's when
you start to see a little bit more of the need to address some sort of nuances and that's when
you get the pickup from work that people who had already been deeply thinking about these things
have put in place for you so ideally we are moving ideally we are defining things before they're
needed so that once they are needed they're already there and available and I would say that's
part of the goal in ethical AI work is to look forward you look towards the future you you try
and use foresight as much as possible and then create the various tools and processes around that
so they're they're ready to go once people are noticing the issues are actually coming up
and I love to hear how the the these you know papers data sheets cards etc which in some sense
you first hear those yeah that makes sense we should do that I I love to hear how those
have become platforms for you know maybe more you know crunchier work around specific measurements
and being able to generate them efficiently and and that kind of thing I am finding interesting
that in your case the this research about the or this paper about model cards has led to
what I perceive as more I said crunchier but like more technical or more concrete research
into the metrics that then populate those cards yeah yeah and you're getting into you mentioned
Zipfian distributions and and other stuff that kind of give kind of fill out this idea of hey
you should know what your model is about and you should publish data about it right yeah yeah so it's
because I would say it's because people are starting to be affected by the problems that we were
for seeing you know a few years ago right so there's the now now unfortunately classic um
guerrilla thing uh the the Google guerrillas issue um where it had tagged people who are black
as guerrillas um and that was massively offensive uh hitting on a lot of historical harm right um
and so one of the one of the solutions to handling something like that is looking at the
association between different skin tones and different labels right so obvious but if you're
not thinking that way if you're not sort of thinking through how these things can happen
then you don't implement those things right and so that's a kind of measurement this sort of a
problematic associations um that we were trying to put in place as we were for seeing like oh
given what we know about the data sets that being that are being collected for language models
language models are going to hate Muslims right and then that was found like years later
but by doing an analysis of the data you know previously it was very clear that that was something
that language models were going to learn to propagate um so as as the public has started to see
the issues that we were for seeing they're sort of turning to see well what can be done what are we
thinking um and it turns out that that one of the key things to be able to do is to document because
as you document you can trace where issues are coming from and address them and this was this was
one of the main takeaways hopefully uh from the stochastic parrots paper although I think people
were maybe a little bit distracted by the the story the massive firing from Google yeah the back
but the back story but you know we actually thought that the the main exciting thing from that
that paper was uh where that was the difficulty in in doing the parrot emoji uh but one of the takeaways
from that paper was that um uh language models are too big um when the data they're using is not
documentable and the reason that that's an issue is because you'll start having really harmful
behavior from language models that you that you don't know where it came from um and so yeah
I wanted to raise that issue yeah I think that was a theme uh I don't recall how explicit it
it was uh but it was a big takeaway from that conversation I referenced earlier with Thomas like
you know when we're talking about these large language models and the training data set essentially
becomes the internet like how do you manage these kinds of issues and that's that's what you're
speaking to now yeah yeah yeah so if you can start measuring things then you can start actually
curating the data um and so I mean this also goes a lot to what sources you should be using so I
think it's now been generally read upon that you shouldn't use reddit data because there's a lot
of toxic and hateful abusive language there and disproportionately directed it at women um and so
but that that took years to establish right so reddit data used to be cool I think it was even used
in the the Delphi uh ethics system that was that was launched through AI2 but uh but yeah so I mean
this is one of the reasons why coming up with quantifications of data is so um so important as well
as paying attention to the demographics of the people creating that data because now you can really
understand what the data is starting to encode which means what the language model will encode so
it turns out that uh the c4 dataset which is um a colossal uh dataset used for training language
models c4 yeah i have to be confused with c4 not to be yeah c4 is nlp c4 is computer vision
so uh not that that really helps I suppose uh but but the c4 dataset um has a lot of data
sampled from Wikipedia and it turns out that Wikipedia um has a lot of writing disproportionately
more writing from uh white north american men um in their in their 20s uh and so in practice what
does this mean it means that black history is basically not represented or represented very
poorly uh and for a while before I started telling people about this black history would redirect
to african-american history uh which hopefully says so much and if it doesn't sense say so much to
you then perhaps you should check your biases yeah you mentioned naturalness earlier as one of
these metrics that you found interesting elaborate on that yeah so as we collect datasets um
and we're trying to measure or rather as we're trying to collect data that reflects natural
language um there becomes a question of how do i know it's natural and so one approach is to say
well i'll be careful with my sampling in order to make sure that the websites amusing or whatever
captures naturalistic language um but there isn't really a useful quantification of that or they're
there generally hasn't been um other than this more qualitative sort of like that looks natural to me
um and it's important and it's important because if you're trying to build models that can understand
natural language so that means uh you know pulling out the the key entities figuring out the content
of a question things like that these tools that really rely on having a reasonable natural
language understanding uh then it's useful to be able to actually measure naturalness
so from here we can look at things like corpus linguistics uh you know going back to brown
corpus in 1964 uh where there were some really cool ideas around what it means to have natural
language um and one of them was noticing that uh that the zippian distribution of so many things
in nature uh also applies to natural language um and so uh the zippian distribution um it's very
hard to explain in one sentence since it's a mathematical concept uh but basically here's a very
like simple oversimplified way of saying it um the most frequent word uh in someone's
document or whatever um will be roughly twice as frequent as the second most frequent word
which will be roughly twice as frequent as the third most frequent word so there's this inverse
relationship between frequency and essentially rank um and so the zippian distribution so what
what zips law states uh is that they'll it'll it'll follow this this sort of general tendency
um and it turns out that there's this parameter we'll call it alpha that um uh that can kind of
um shape what that curve looks like and it turns out that for English that's already been calculated
by by nice corpus linguists and linguists and stuff and we know that alpha is actually like one
um and so different languages have different different kinds of shapes that follow this uh general
tendency um and because they've already been calculated for a ton of languages we can see how well
some collection fits to the ideal zippian distribution um and then the farther out it is for that
language which we can already know uh then we can say like well according to this sort of theory
it is actually moving farther away from what we would expect in natural language um so you know
it's it's a measure it's not the best measure but it's something that uh that corpus linguists
have put forward and is super useful and and personally in in my work on data measurements uh
I found I found that it's really helped to identify uh data sets that are appropriately capturing
you know um uh general conversations and stuff and data sets that for example have a lot of
artifacts or weird mixing of different domains that wasn't well controlled for and things
like that um so it can be really as well you mentioned as uh I think it was kind of a motivation
for this particular measure natural text on the internet versus newswire press releases which
have a different uh a different way of using language but this also kind of rings of thinking about
you know a world where we have these large language models and they're quote unquote polluting
the internet with spam and kind of uh almost that adversarial and anti adversarial uh cat and
mouse game and and are is that part of what you're thinking about here or is that just tangential
yeah I mean I think one thing that might be relevant here is that if we're trying to figure out
whether or not something is synthetically generated um so you know generated from a language model
versus um created by people we have to be thinking about these sort of second order characteristics um
so what can we maybe not tell as we look at things sentenced by sentence um but if we calculate
statistics over patterns uh in a bunch of sentences are there things that we see um that's different
than what we would expect from when when humans are generating it um and so these kinds of
measurements such as naturalness based measurements would help to get at that um and so as we're trying
to figure out you know uh trolls that are spewing hateful and abusive content that are actually
from from systems that are just using large language models and misinformation and all this
other kind of stuff as we can start developing the second order uh kinds of measurements then we
can start being able to a lot more easily make distinctions so what I'm hearing there is that your
focus is on bias and human impact but the cyber security folks may find this interesting too
or the adversarial hopefully hopefully yeah yeah awesome awesome well Meg it has been wonderful
catching up with you and chatting about some of the things that you've been working on and thinking
about thanks so much for joining us yeah thank you for asking me and letting me babble about things
I'm fascinated by I really appreciate it thank you
