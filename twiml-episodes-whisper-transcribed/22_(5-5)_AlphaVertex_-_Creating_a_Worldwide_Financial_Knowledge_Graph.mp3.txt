All right. So once again, I am at NYU Future Labs with the AI Nexus companies. And this time,
I'm with Muticia Nunda, the co-founder and CEO of Alpha Vertex. Say hi. Hi, thanks. Thanks for having
me. So why don't we get started by having you tell us a little text and what you guys are up to?
Sure. So Alpha Vertex is an innovative financial technology company that is using the next generation of
AI tools to help support investing around the world. Practically what that means is that we develop
predictive models that try to forecast the returns of stocks over multiple time horizons.
And at present, we cover all major markets around the world. So we cover 30,000 stocks globally,
which represent about 95% of the investable universe. Okay. And so presumably you came out of
financial services prior to this? Yeah. So my background is in finance, I've spent over 17 years,
I started out in investment banking, did private equity and ended up as the chief of staff of one
of the largest proprietary market-making businesses in the world at a company called Sescohana International
Group. And then after leaving that, I spent five years at Bloomberg as the head of strategy and
business development for Bloomberg enterprise solutions. Okay. So I've kind of done my tour of duty
on Wall Street and around Wall Street. You've got quite a few punches on your ticket.
You know, it strikes me that of all the companies that I've talked to
here at ANX this today, the trying to help people make better trades with financial data is
there's nothing new about it. Right? That is a classical idea and lots of people have tried to apply,
you know, we've been trying to apply analytics to this for many, many years. What is new and different
about your approach to it? So I'll first just start by sort of saying you're absolutely right that
it is not a new idea because it's not a new problem, but the present moment there's about 98% of
all funds underperform their benchmark over 10 years. And the simple reason is that they rely on
kind of traditional portfolio managers to act as stockpickers. And what happens there is that you
have maybe 1% of all fund managers are kind of really good at making money, but then the 99%
of the population ends up underperforming. And so you're subject to kind of the talent that you
bring in. As financial markets get more and more complicated, you know, there's about 2.5 billion
bytes of data nowadays that people kind of have to keep track of. It's increasingly difficult
for a human being to sort of incorporate all of this information in a useful way into the investment
strategy. So with the onset of, you know, innovations in machine learning and AI in general,
you know, we're able to build models that can capture all of these pieces of information, much
similar to the butterfly effect. So if you think about, we have AI models that sort of will see
things that are happening in Asia and then model how they ripple through global markets to affect
a stock like Apple or Google in the US. It's really, really challenging to be able to do that as a
human being or to build a model that can capture that complexity. So that's sort of what we're doing
that's completely different is we have these very sophisticated models that can kind of look out
to 10 or, you know, 20 degrees of separation away and analyze how things that are going on very
in very distant places can kind of ultimately come back to impact stocks.
So that the, I mean, certainly the hedge funds have been trying to apply this type of stuff for
a very long time as part of what you're doing, bringing the, you know, the quant type of technology
that hedge funds have been doing to the traditional investors, the institutional side.
Yeah, so we, you know, interestingly enough, the earliest adopters of our technology are obviously
the quantitative of systematic hedge funds. They kind of understand how powerful this technology is
and, you know, they employ data driven technology based investment strategies. And so this to them
is a high value signal that is additive to maybe other things that they do on a proprietary basis.
Ultimately, over the long run, what we want to do is make these tools kind of more broadly
accessible to the 99% of the investment managers out there that don't have, you know,
this technology driven investment platforms. So we're working on things like having, you know,
kind of machine learning driven institutional class research and analytics facilities that,
you know, can be accessed via natural language. So you don't have to have the technical capability
of a data scientist or a statistician or a PhD person to be able to do the types of analysis
that we're going to be developing. So I think, you know, in the near term, the earlier adopters
are the more quantitative and hedge funds, but over the long run, we want to go after everybody
in the space. Okay, so you guys are, it's not so much arming the folks that can't keep up with the
quants is that you guys have developed some really cool stuff that the quants need to. Okay, and so, um, thinking about how one might
try to build a machine learning AI, fly effect, that sounds super complex. Like how do you, what's the,
what's the approach there? So it is actually a pretty huge problem. The first thing is you have to try
to represent all the world's financial information in a way that makes sense. Oh, well, that's easy.
So the approach that we've taken is we've modeled everything as a computational graph. And for people
who aren't familiar with that is literally this multi-dimensional relationship map of how everything
is interconnected to everything else. So you can describe things like Apple competes with Samsung,
Apple is a customer of Samsung's and Apple is suing Samsung and all of these are relationships
between these two nodes or these two entities. In a traditional model, you can't represent these
multi-dimensional relationships across companies. And in our case, like in the case of where Apple
is suing Samsung, Apple, the relationship has a direction. So Apple is instigating the legal
lawsuit on Samsung and we can encode the value of the lawsuit, how long it's been going on.
And then you can actually do computations off of all of these connections between companies.
But to do this, you have to monitor all the world's information, not just financial information.
So to be able to just do that, we monitor over a million pieces of information in a given day.
And these are across all types of sources. So we look at structured data sets like financial
market data, we look at unstructured data sets such as regulatory filings, would say the SEC,
the FDA, whatever. What companies are saying in press releases, news articles, we read about 300,000
sources of news in a given day. And one of the things we had to build early on was this ability for
the machine to understand the news on its own, extract relationships that it thought were
meaningful, and then also weigh the quality of that source of information. So not all things
are really created equal. If I read something on a blog in Asia, it might be actually a really
high quality source or it may not be. And so you have to make these autonomous machines
have the capability to sort of weigh the quality of the source of that information.
And once it's extracted and you're putting it into this computational graph,
you know, it has to say that I think this is actually a really good fact. Or if it's not,
then it says it's not a high quality fact. And I need to wait on confirming or additional sources
of, you know, to confirm this relationship that I just found. And that's just the starting point,
right? So we cover, I guess 80,000 publicly traded companies around the world. We monitor 15
million private companies and we track about 200 million people that work at these companies as
a starting point. And then from there, we try to figure out all their products, who they do
business with, whether they're contracting with other companies or with the government,
any kind of litigation that may be going on, who their investors are and what their investors
are doing. So, you know, it's just spider web of knowledge that we have to build
before we can actually do a computation. Right, right. To your, there's so much in there.
Like, just the issue of trying to identify the newsworthiness, the trust level associated
with a given site. I mean, that's Google, right? It's PageRank, right? Are you using something
similar to PageRank in? Yes, but no. So, I mean, Google's an amazing
piece of technology. But, you know, again, with machine learning and a lot of things that we do,
the domain knowledge is extremely important. So, Google doesn't have PageRank for finance.
Right. So, we kind of have to build that ourselves. The good thing is, I guess Google tries to
index the world. We just need to index the financial world. So, it's a subset of what Google does.
But, we can take a lot of the same concepts from what Google does and incorporate that into
our own algorithms. One of the things that we've done is we apply like this machine learning
approach to teaching things whether or not this is a high quality source. So, we can give
the AI good examples of what high quality, you know, like this is the Wall Street Journal,
this is a high quality source. These are the types of relationships of pieces of information
that come from this source versus here as a, you know, like a known-in blog. And then the machine
could kind of extrapolate from that. So, it's a supervised learning problem, as opposed to
go out and index all of these financial sites and figure out which ones are high quality
the way Google. Yeah, because Google basically has boiled the ocean ahead of time. And we kind of,
when we come across new stuff, we make a determination at that point in time. Right, right.
That's super interesting. Tell us about some of the other interesting kind of machine learning
problems that you... Yeah. So, yeah. So, the other thing that is actually really, really hard to do
is kind of twofold. The first one is entity resolution. So, you're reading all of this information
about people, places, companies, whatever. The way things are written is not necessarily always
clear. And so, you might be reading something about Apple, but it's the fruit and not the company,
and you have to be able to do some biguate that. So, we do a lot of really cool stuff in the
entity resolution space. And then we've started to do some really exciting things that were in the
domain of intelligent services, but I think kind of the technology is leaking out now in the sense
of something called record linkage. So, if you have these giant data sets that you can onboard or
acquire or, you know, create yourself, what typically happens is that something may call Apple, Apple
in one dataset. It may call it Apple Incina, a different one, or it might just be a reference to
the CEO of Apple. So, you need to be able to link all of these data sets together. And basically,
you don't have a clear key on which to join everything. So, you have to make these calculated sort
of, you know, similar, you have to basically say, this thing is likely this other thing.
This ambiguity. Correct. And connecting things based off of, you know, you say, is this in the
same location? Or, you know, is it referencing a product of this company? So, you know, and then
basically be able to say, well, actually it is talking about Apple, but it was actually talking
about the iPhone. So, basically merging, like, say, the iPhone to the Apple company without ever
having a necessary mention of Apple in both datasets. Right. So, the record linkage is actually
really, really, really hard. We started to do a lot of that. And it's a really fascinating
kind of area in the big data world. How do you attack that? What technology approaches are you
applying to? So, obviously, the first one is we have to be able to within the data extract
kind of attributes or features that we think are important for the links that we're trying to
establish. So, what are we trying to like in the first place? Yes, company data. So, if we go
and grab every patent in the US for companies, and that's just one giant data dump, linking that
back to, well, all these patents are actually owned by these companies. You know, the US government
patent office might not have the same names as, you know, the publicly traded companies themselves.
Right. So, you have to look at things like, is the address the same? The people mentioned as
the patent holders, employees of these companies. So, you're looking for bodies of evidence that
suggests that these two things are kind of mapped together. And you have different algorithms that
can kind of give you a score and a confidence measures to how closely two records are to each
other. So, that's one. And then another one is a, I think, called relationship extraction. So,
if you read a piece of news as a human being, it's very easy for you to sort of boil it down to
what it means, right? So, you know, this company is buying another company. But in the real world,
that might be written out over three paragraphs. And so, for a computer to sort of establish that
NTTA is acquiring NTTB after reading a paragraph or two is actually a pretty challenging problem.
Right. And so, yeah, there's a lot of really interesting stuff that we use there, but most of it is
in the deep learning, you know, kind of kind of space. So, we're using a lot of TensorFlow models
that, you know, to try to understand the representation of language, extract kind of entities,
and then the relationships that we're looking for. So, today's end up looking like, you know,
those single or individual, like super, you know, very deep neural networks or they many neural
networks that are each serving their own purposes. Yeah. So, we would, we've realized is that
we've had better results with building specialized machine learning models and neural networks
as opposed to one kind of generalized AI or generalized network. And so, even with our
relationship extraction, what we do is we have thousands of models that kind of work in Unison,
and each one is highly specialized in one thing. And so, if you think about having a very wide
funnel at the beginning where you say, this is a draw document, the first sort of set of models
will basically say, this document is actually talking about this concept. And then that gets filtered
into, okay, so within this concept, these are the types of relationships I should be looking for.
And then have more sophisticated models kind of take that information and then say, okay,
fine. So, this is about a corporation, and corporations can have M&A as a relationship. And so,
I kind of, I'm seeing a very strong signal that this document in general is talking about M&A,
let me give it to the most sophisticated guy to go and extract that very specific relationship.
And by sophisticated guy, are we talking about a person or another model?
Another model, sorry, yeah. And so, we have this kind of pipeline of AI models that kind of work
together that start really broad and then end up kind of very high-professionalized at the end.
And are you doing any human and a loop anywhere, or is it all?
We do, but there's a bit of reinforcement learning that goes on, especially with the record
linkage stuff. And then we obviously retrain our models fairly frequently. And so, we'll have
the human and the loop on the retraining of the models. But on a given day, everything's kind
of running autonomous. Yeah, super interesting. So, and what you talked about the identifying the
relationships and how you have the, you know, you can have the three paragraphs, you get to,
yeah, you know, that there was an acquisition that makes me think of like, you know, CFO speak,
right? Do you have a model that can train that is trained like decipher when a CFO is saying
that they're going to miss? Yeah, yeah. Actually, it's an area of interest to us. We haven't yet
done any kind of hard core development there, but there is actually a huge community of users in
the kind of, you know, hedge fund space that you understand tone, not even just the text, but like
the tone. Yeah. If there was a video of the guy making the statement or he was darting around.
There is a lot of kind of interest in that. We just haven't yet had a chance to spend the time in
that space, but it's clearly something, you know, okay. And so, where are you guys in the process?
Yes. So, we've launched a product about, I guess, less than two months now called Precog,
which is our, like, it's a predictive service that basically tries to measure the butterfly effect
and then produce short on forecasts for the returns of these 30,000 stocks around the world.
That is currently in production and we have a number of clients that we service.
So, meaning you give it, you know, a company and you probably have some unique company thing.
Well, just going to database and tell you, you know, you give it maybe say 30 days and it'll tell
you a projected stock price in 30 days, is it? Yes, a lot. That, yeah, it's along those lines. So,
what we do today is we only produce predictions on specific time horizons. So, it would be like a
one week forecast, two week forecast or one month forecast. Okay. But it would be kind of a
rolling day forecast for those horizons. Okay. Today being Monday, we'll give you a forecast for
the return of the stock, not the price, but the return of the stock by next month. 714 and 30 days.
Okay. And that, you know, again, if you do this systematically over a broad universe of stocks
and you sort of, you know, you're never going to get 100% accuracy, which is impossible,
which is, you have this winning prediction, you know, like right now, we think we get anywhere
from 60 to almost 70% accurate in depending on the horizon and the stocks themselves. But 60 to
70 is kind of the average, but across 30,000 names, that is like Vegas odds, right? So, if you're
the casino, you may lose a hand to one player, you know, one table, but across all players, you're
systematically making money. And so, the customers that are using our predictions want them
on a very large universe of stocks, and then they are obviously not betting the firm on one prediction
on one day. They're making sure. This is one signal that they're using out of a portfolio
of signals. So, you're obviously gaining, you're able to make a prediction at day zero, and then, you
know, at days 17, 14 and 30, you're getting additional signals as to, you know, how accurately
your model is performing. How do you then use that signal to retrain? And then how do you deal with,
like attribution issues? You know, you were off by, you know, 50% or even plus minus, like,
how, where in your model did you go wrong? Have you started to figure, look at that?
Yeah, we do. One of the things that we've tried to stay away, shy away from just kind of with
our models is that we've tried to avoid using deep neural nets and all of these types of things
when we're doing our forecasting. And the reason being with these deep neural networks, you actually
don't know kind of what is driving the model's decision to produce the outcome that it produces.
So, the models that we generally rely on tend to have a way to sort of reverse engineer
why it made the decision it made. So, more like decision trees or something else?
It's never one thing. So, we use kind of a hybrid approach where we will use an ensemble of
methods to come up with a single prediction. But most of them have an ability where we can actually
query the model and ask it, what was the most important thing in the decision that you made?
Okay. And then those, those, those weights or those factors are things that we can kind of just,
you know, sanity check in the markets to see if it is what it was. So, you know, we make, say,
for financial stocks, we think, you know, yield spreads and things of that nature should be important
to financials. But there may be a group of financials that are aren't moving with respect to
interest rates. So, the model might rely on that. But then, you know, in a month's time, we could
go back and say, well, why were you way off? And they'll say, well, I over waited, you know,
this moving interest rates. Okay. We will try to retrain the model to make sure it doesn't do that.
The thing we try to do is we don't want to superfluously or just over engineer the models,
because you can end up with overfitting. So, we're really careful about when we train the models
and then what we give them to retrain, you know, and there's a lot of art and science that goes
into kind of being like, you know, 60 percent is good enough or not given everything that we know.
So, it sounds like going back to your funnel analogy at the top of the funnel, you're using a
lot of deep learning to extract signal from various sources and then closer to the end of the funnel,
you're using more using models that have greater explainability.
Absolutely. That's great. Okay. Interesting. Anything else that you'd like to share about what
you guys are up to? I mean, just kind of long term. One of the things, obviously, we're trying to
build, like I said in the beginning, was the capability for then, you know, non-technical users to be
able to sort of surface some of these insights or even, you know, ask the discover new facts that
are kind of not obvious, right, within this knowledge base that we've built and then also be able
to take advantage of some of these predictive models that we're building. So, what we're really
trying to do here is develop, you know, like, an ability for the machine to understand human intent
and then also the context in which the intent is being asked. So, for example, you sound fairly
kind of familiar with the financial domain, but like my grandmother may have a similar question to you,
but the response that the machine should give her versus you needs to be kind of adjusted for that.
So, we're really looking to build on intelligent agent that would be able to work under different
contexts, right? So, if you're dealing with another professional investor, kind of, you know,
having a conversation at that level, if you're dealing with somebody looking for financial advice
who's not that sophisticated, then you kind of want to boil down some of these things and just have,
you know, them explained back to the user in a format that they can understand. Right. So,
that ultimately sort of where we're driving towards. Okay. Now, working folks, learn more about
what you guys are doing. So, on our website is a great place to start. They can also check out,
at least for individual investors or people interested in sort of these signals that we're
producing. They're available on Quantopian. So, they're free to use until you want to put them
into a production strategy. Okay. And they are available for 500 of the most liquid names in the
United States. Okay. And so, the company website is alphavertex.ai. Alphavertex.ai and Quantopian is
the site that is available. Okay. And if folks want to get in touch with you, can they do it
through one of those sites? Yeah, they can email me at infoatalphavertex.ai and we will be glad to,
you know, reach out to them and just understand what their needs are. Awesome. Well, thanks so
much for being on the show. I really learned a lot about, really learned a lot from what you guys
are doing and it sounds really exciting. Yeah, it was a pleasure. Thank you so much. Great. Thank you.
