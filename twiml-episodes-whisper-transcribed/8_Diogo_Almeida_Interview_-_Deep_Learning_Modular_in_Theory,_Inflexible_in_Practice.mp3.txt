Hello and welcome to another episode of Twimmel Talk, the podcast where I interview
interesting people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
The recording you're about to hear is part of a series of interviews I recorded live
from the O'Reilly AI and Stratoconferences in New York City last month.
I'll be sharing these interviews on the podcast over the next several weeks and I'm sure
you'll enjoy them.
This time I interview Diogo Almeida, senior data scientist at healthcare startup and
LITIC.
Diogo and I met at the AI conference where we delivered a great presentation on in the
trenches deep learning titled deep learning modular in theory inflexible in practice.
Diogo and I discussed the ideas he presented which are centered on the data, software,
optimization and understanding issues surrounding deep learning.
Diogo is also a past first place Kaggle competition winner and we spend some time discussing
the competition he competed in and the approach he took to win it.
Before we jump in, a bit of a listener warning.
Our conversation gets pretty technical pretty quickly.
I do try to make sure to summarize key points from time to time and I really think that
if you hang in there, I'm sure you'll learn a ton.
Of course, let me know how you like this level of detail.
I'll be including links to Diogo and a bunch of the data sets and other things that we
discuss in the show notes, which you can find at twimmolai.com slash talk slash eight.
Also as is the case with my other field recordings, there's unfortunately a bit of unavoidable
background noise, sorry for that.
And now on to the show.
Alright, hey everyone, I'm here at the O'Reilly AI conference and I'm sitting with Diogo
Almeida, who just did a really interesting talk on deep learning and he was kind enough
to sit down with us and talk a little bit about what he talked about.
Diogo, why don't you introduce yourself?
Cool.
I'm Diogo Almeida.
I work at this super cool medical deep learning startup where we work on giving like really
accurate, really fast, really safe medical diagnoses and this is something we hope will completely
change the world.
Before that, in past life, I was a math lead.
So I broke a 13 year losing streak for the Philippines in the international math Olympiad
was in the top team in the world at the interdisciplinary competition in modeling and there's a website
for machine learning competitions called Kaggle that I won first place on in one competition
as well.
What was that?
This was in 2013 because the cause effect bears challenge.
Tell us about that.
Oh, it's just a very weird challenge where in most machine learning, you have like tabular
data.
So you know, like you have columns of features, rows of observations.
And in this problem, your data was pairs of sequences.
So you have something like altitude and like one observation is like altitude and height
and you have like a pair of, sorry, a sequence of pairs of like which altitudes correspond to
which heist in some unordered manner.
The idea was given this, you're supposed to predict whether altitude is causes height
or height, sorry, the altitude and height were the same thing.
I meant altitude and temperature.
Right.
So you're supposed to predict if altitude causes temperature, it causes altitude and obviously
that altitude causes temperature right for us.
But there's a lot of like very complicated tasks that we don't know the answer to and
it's kind of like the basic task is to, if you know the saying correlation doesn't apply
causation, it's supposed to do the opposite of that.
You're supposed to figure out how the correlation implies causation, which is, it's extremely
useful because you have like lots and lots of observational data.
Right.
It's very hard to have like a controlled study.
So the more accurate we can get a view of the world from purely observational data,
the more we can either have informed priors before running the control study or figure out
how to order the controlled study in an appropriate way.
Okay.
And this is also the kind of analysis you would use for like a root cause analysis or something
in like an IOT use case where you've got all these observations and you're trying to
figure out what the underlying condition is or.
I'm not as familiar with that.
There are, there was traditional statistical work and there actually was a background for
this topic, but I kind of didn't pay much attention to that because I kind of went my
own way and it was much more for fun than for winning.
And winning was a very nice side effect.
And I went through a much more like software oriented way of just like build a really
complicated powerful model and have it solve this based on like rather than like hand
engineering stuff.
Why not just like automatically engineer a lot of informative variables and then solve
it with that.
Okay.
So can you walk us through the process like how do you, how did you formulate a methodology
for attacking them?
Was this your first Kaggle competition or had you been doing that for a while?
My first serious one.
I've done like one or two before that I didn't really like really spend much time on.
But like you know you quit like after two days because it turns out your teammates were
in useful or something like that.
So I have like played with it before but I've never really gone all out until this one.
So my methodology was, well some background is that there are like statistical tests that
people use that did very well in this task and or sorry that people used to use in this
task and put it roughly in perspective these got like 0.6ish AUC.
So if you see a paper in nature science about a new test for causality it probably gets
around 0.6ish AUC.
Okay.
AUC for those that don't know is the area under the curve and that's a performance metric.
Yeah.
So we were solving a ranking problem or we were trying to rank the outputs given that we
know which ones were, which ones caused each other to a little bit of complicated metric
because we actually had three output classes.
So we did like a bidirectional AUC but that doesn't really matter much.
And so these tests we did like 0.6ish they're roughly a single feature because it's just
the prediction you extracted directly from the data.
The most of the other competitors in like the top 10 had you know tens of features or
something like that and the second place where I think had like a whooping like 100 something
features.
Okay.
I had 50,000 so what I did was I found like a very simple way of determining causality
which would be the rationale would be if x causes y then y is a function of x you know
there's noise in there somewhere.
So roughly you can tell how good one is a function of the other based on how well they
can be approximated by functions.
And this is kind of like a very vague like recipe for how to create these features but the
idea is rather than you know hard coding statistical tests like you know like add a Gaussian
integrate this thing out whatever I just figure that we have an entire field of curve fitting
which is called machine learning right and these are often like built after natural like
very natural priors.
So the idea would be try like a ton of machine learning algorithms all of the ones that were
computationally feasible.
Try a different metrics for what fit means because fit is it's it's kind of like a not like
a very exact term and like throw like these are all the features now throw them all into
like big boost decision tree train this thing for a week on like a 50 core machine and
then you know take a nap the entire time so that was roughly my solution.
Wow and so the solution was was primarily based around the boosted decision tree as opposed
to some super complex ensemble or something like that actually it's a weird story that
for this competition I was so far ahead for almost all the competition I didn't even
try.
So the what was it like for basically everything beyond the last week like yep like maybe
a month or a month and a half before I even started the competition late I was like
so far ahead that the gap between like me and second place was like the equivalent of
like you know second and like 50 and there's something like that.
So I was like feeling really confident and I actually stopped paying attention to this
because I felt that like oh this is going to be easy right.
But then during the last week you know someone you know people started sharing their solutions
like I only got 10 or something here the features I used in all of a sudden like everyone
started rising and this is definitely basically by creating ensembles of everyone's
everyone's a solution like people like Lee kind of hinted at what I think it was only
one person but like they had like a lot of good stuff in there that other people started
using.
And once people were getting performance they like make more of it or something like that.
So people are starting to rise right and like I didn't have even ensemble this far and
I unfortunately had a model that took out like a week to train like I said so and I only
had a one week left for the competition.
So I decided that I tried like a few last minutes attempts at ensembling but nothing beat
my like my super big one week long model.
And so I just stuck with that thing and that ended up actually winning and it actually
was very scary because people ended up passing me on the training on the validation leader
board.
Yeah.
But in test leader board it was like it was completely flipped because by they overfit
yeah they like they had like hundreds of submissions while like my best submission was
like my sub 10th because like it was a very like hands off competition for me.
I cared about it a lot and I like I wrote like lots of software that was I thought nice
but like I was really I really really thought that would have been like an absolute slam
down.
Okay.
So it's exciting though.
Okay.
So where did the 50,000 features come from?
So you can imagine like exponential growth when you're just trying like every combination
of this with every combination of this.
Yep.
There was like every combination of metric that I can think of every combination of machine
learning algorithm that was like computationally tractable.
There was like symmetric features so you could like augment your thing with like difference
features because like it doesn't matter which extra bias right.
There was a a nuanced thing that I don't normally explain when I talk about the competition
which is not all of the input was numerical some of it was categorical.
And like it you just can't like throw categorical data into a numerical algorithm right.
Right.
So it becomes actually a complicated problem.
How do you compare numerical different ways of calibrating your bands or something like
that?
Well, I mean you can it's very easy to convert numerical to categorical but you lose a lot
of information from that drive.
So what I did was I did different ways of converting from like like this is like a categorical
numerical pair metric.
So this stuff like compare you know compute sorry convert numerical to categorical via
like clustering or binning or something right.
And then you know when you want to convert categorical to numerical you do something like
the PCA you know like get the first principle components or something like that or projection
to the first principle components.
And I basically are just looping through all of these things.
So you can imagine like a lot of less before loops and the end I had a bunch of them.
So like that ended up with like 50,000 ish.
And I also skipped a detail there which is I also used a feature selection algorithm in
Earth like make it a little bit smaller, which help performance a bit but it ended up not
being important.
So I usually am it but for the sake of clarity that was also done.
Okay.
Okay.
Wow.
That sounds pretty cool.
And now that was a little bit of a digression.
Yeah.
Complete digression.
Yeah.
Interesting story though.
Yeah.
Absolutely.
Absolutely.
It's actually generalized to new problems as well.
I believe the competition organizer was applying it to some sort of biology problems and
they were showing that they'd actually predict causality in that as well.
Oh really?
So yeah, hopefully that kind of thing could be really useful.
Oh nice.
Nice.
But what you were talking about here was deep learning.
Yeah.
And it was not deep at all.
And I didn't catch all of your talk.
I caught the last bit of it.
But it seemed like what you were going through was kind of a bunch of war stories lessons
learned like, you know, you hear a lot about deep learning, you know, but there are a lot
of things that people broadly believe about deep learning that actually are false.
And why don't you explain kind of what your intent was for the talk and kind of walk
us through, you know, an overview of what you're presenting.
Cool.
So the way I see it is like there's these two competing these views on deep learning,
like extreme views, which is deep learning will solve all our problems and deep learning
is complete garbage.
Sorry, it's all hype, kind of exaggeration, but maybe for exaggerating views, you can
say that.
And there's evidence for each of these views, you know, like there's some amazing results
of deep learning.
There's some made like extremely poor results on deep learning.
Right.
And the idea is that like these are not as informative of the stuff in the middle.
So the idea is like you draw all of this evidence in like this one dimensional plane.
Yeah.
And you like try to like draw like a max margin hyper plane.
You might get like you this interesting decision boundary because like this is where the
interesting stuff lies.
Like this is stuff that's going to be moving slowly over time if deep learning is doing
well, right?
Or the other way if people are starting to like find all sorts of failure cases.
And the idea would be if we talk about like these examples and like the edges of our understanding
or the edges of our everything or like edges of you know, like all the things that are limiting
deep learning nowadays and like keeping us from solving all of our dreams, that can hopefully
give people an impression of like what everything else is like because it's like just very extreme
on the other end to this spectrum.
And I feel like that's just not very much talked about because like you said, like a lot
of people are on the deep learning hype drain or kind of being sad at home and like being
grumpy because now all of the all of the questioners are silent stride.
So if we kind of map out what the corner cases are and the failure mose and things like
that to help us push forward our understanding of this thing is the basic premise.
Yeah.
And kind of like acknowledging it also helps.
I don't think what I did was the greatest acknowledgement of it, but I think it was a
more thorough one than I've seen before and realistic especially in that I think that
sometimes just understanding your problem really well really helps you to solve that problem.
So I know now that I mean like I do research as well and the stuff's very important to
me.
And by looking at it from like a kind of a higher level, I can kind of see better like
this seems like something that looks really promising to me or this doesn't seem promising
at all.
Right.
Like for example, one of the problems with deep learning nowadays is everything's very
local.
Right.
Like you get local and what's that use the gradient, right?
Or maybe higher order driven things, but they for practical purposes use the gradient
and this can be insufficient for some applications, right?
Going to a higher level, maybe it can start with a lower level, right?
Like S3D doesn't work for my spatial transformer network.
This is unfortunate.
Like, let me try Adam, let me try RMS prop, but if you go to a higher level, you realize
that the problem is the local learning, the spatial transformer network, not necessarily
the gradient descent.
So to tell us about spatial transformer networks, yeah, so this is just one example I
use of a kind of network that it's very easy to see the issues of local learning with.
It's very nice because it's a, it's a differentiable network.
It's very easy to see exploration problems in reinforcement learning domains, but this
is one that you have a derivative of and it should be easier to optimize and it is, but
you sometimes don't get what exactly you, it doesn't like fulfill its full potential.
So are you kind of seeing that there are a lot of people coming into the space that,
you know, that, you know, try to throw deep learning at a given problem.
The common way of solving it is using stochastic gradient descent and they don't really think
about, you know, how that's working and that it's, you know, finding a local optimization
and there are some problems that, you know, for which they get kind of stuck in that local
and...
That is unfortunately the case.
Like, I have seen many people introduce to deep learning who think that let's stitch
together an architecture that's differentiable, you know, bingo, bingo, call it a day.
They've like solved problem X, right?
Like, they realize the limitations of requiring large data sets, but they think that that's
what it amounts to.
And I think often, very often times, it doesn't.
So back to spatial transformer networks, what they are is basically, instead of like a single
network that learns how to classify an image, you have two networks.
One of them learns which part of the image to look at and the other part takes what
that network looked at and does the classification on it.
And this is a huge advantage because a lot of the times your input image might be really
large and you don't want to run the network overall, all of it.
It might have like unnecessary information.
It might be really useful to like, co-localize, so like, have the where as well as the
what.
So there's really good reasons to use this and in fact, for medical problems, if it worked
well, I would use it for everything.
Number one, the number two is if it worked well, I would use it for every computer vision
problem.
Because what these spatial transformer networks can do is not only find the region, but it
can also transform the region into a canonical location.
So rather than having to learn filters of like cats at every orientation, you might have
to learn filters of cats at only one orientation, which like would reduce and result in like
much better data and parameter efficiency.
But back to the issue here is that you have these two networks that are, they're not competing,
but they're working together, but they're only using the current network, the current
other network as its source of signal basically.
So if your classification network gets really good early on in training, your localization
network gets stuck in this optimal, right?
Because like if it changes anything at least a little bit, your classification network
will do worse.
So like the gradient tells it like, hey, hey, just stay where you are, you're pretty good
or move you all around the small region, right?
Which might be very far from the intended purpose, right?
Like correctly like zooming all the way into the thing you care about and like rotating
it a lot.
And on the other hand, if the spatial transformer network converges early, so imagine the classification
network is garbage, it might zoom into like regions of the image that are just independent
of the class, but makes the classification network tends to perform a little bit better
on.
So it might like, for example, if you're trying to classify kinds of dogs or like image
net, and it turns out like your classifier starts out like just being good at telling
grass means dog, and the localizer notices and like just zooms into the grass, right?
Like those zooms in, zoom grass.
And basically you've cut the dog out of the image.
And the moment you've cut the dog out of the image, you get no gradient signal.
And when you have no gradient signal, you're stuck there forever.
And this is a problem that people just don't really like to acknowledge in networks, right?
That's actually a very complicated relationship, because now you need to like maintain a balance
and all of that.
And I don't think people even know how to do that.
Like people don't know how to do it with a generative adversarial network either, which
is another example I gave of this.
Yeah.
Yeah.
Huh.
So what was the overall structure of your talk?
So the title of the talk was deep learning, modular, and theory, and flexible in practice.
So I first wanted to talk about the successes of deep learning, rather to show that deep learning
is very modular and it can do a lot of things.
And you know, get them into the mode like, wow, we can solve everything.
And I actually think that I had a somewhat bold claim to end that first part, which is
that deep learning, today's deep learning components can solve any problem, any like
a computable problem, if you ignore the practical aspects, which would be, I mean, I think
it's interesting to point out, right?
Because then now that you isolate that, you know that the practical aspects are the issue,
right?
Right.
And those practical aspects are data software optimization, in probably order of difficulty
of how to understand them.
And the latter part of the talk I talked about, these issues with deep learning like specifically
data software optimization and a final section of understanding, just because I wanted to
point out that while understanding is not necessary for like getting things to work,
which maybe is what we care about, understanding is very necessary to make progress, right?
And we just, it's amazing how little we understand about anything.
Well, let's come back to that and maybe walk through the different sections.
So data, walk us through the points that you were driving home around that.
Okay, so from a super high level, it's that neural networks are extremely data-efficient
and they don't have to be that way.
And data efficiencies, the root cause of all problems, because if we were data-efficient,
the size of data sets wouldn't matter, right?
The data sets we use are kind of flawed in that, like they have known issues that, you
know researchers know about, that they're noisy or like what kinds of known issues.
Like, Pantry Bank is a very small data set, therefore making bigger networks is not
very helpful, because it overfits, therefore you should generally only publish regularization
research on it or something like that.
So you're referring primarily to kind of the known data sets, that kind of thing.
That's the kind of things that, you know, like the mainstream deep learning researchers
publish on to keep into them, hey, I have something cool, use my thing.
And that is, I mean, it's important, right?
Like the alternative is publishing and they said no one knows about, which is also very
hard to get any information from.
But one has kind of, it's almost like a reproducibility kind of issue where there are elements that
are inherent to the data set that, you know, drive towards or require a certain class
of solution.
Yeah, it's a horrible state of affairs where, like you need to, like, if you, you know,
you read a paper, the paper usually has the high level, it doesn't have all the low
little details, that's what the code is for, and you implement the paper exactly as it
says.
And it gets not anywhere near close to what they had, right?
And you're like, yo, what the F. And then, you know, you, maybe you email the authors,
maybe they eventually reach the source code, and you run the source code, because you
don't believe them.
Wow, this reproducibility is exactly what the author said, and it turns out, like, it just
has like a bunch of magic hyper parameters.
Like you said, you know, L2 regularization to this, you need this learning rate schedule
for sure.
Use this optimizer, and also preprocess your data set in this way and sample it in this
way.
And like, these are all things that you really want to be robust to, right?
And you just, you just aren't, right?
Like that is, it's a very unfortunate, like, aspect of the world, right?
Like, you're put into this position where, um, if you don't do, you know, if you don't
play the game, you never get to the art results, and people don't listen to you.
If you do play the game, um, I mean, some people listen to you, but some don't, because
they know the game, but then, like, it's the only way to get people to see your thing.
And then by the game, you mean in terms of the researchers, like they're driven to publish,
you know, you know, win in the competitions for whichever data set that they're looking
at.
And then you're in the competition, but it's usually like, you want to get people interested
in your papers.
Yeah.
And it's very different if you just didn't care and you wanted to publish interesting things,
right?
But if you want to get eyeballs, sometimes, like, unless you're already a respected person,
it's kind of what you have to do, right?
So, um, like, I did, like, uh, it's sometimes that kind of thing is important.
I think that it's kind of very qualitative thing, um, which is unfortunate in the data world
that they get to get a feel of a data set, like when this data set's starting to get, like,
really overfit, um, that, um, perhaps it's not useful anymore.
And I feel like some researchers like qualitatively feel that about, like, CIFAR 10 and CIFAR
100.
Especially CIFAR 10.
I'm not 100% sure about CIFAR 100 as much as that data set.
Um, this is a data set of 32 by 32 RGB images.
Okay.
It's a popular use baseline because, um, it's a very small baseline and images of anything
in particular.
CIFAR 10 has 10 classes.
Okay.
And common classes, um, and they are, um, it's a popular data set because it's a really
small data set, 32 by 32 images, you barely see anything.
And it's not MNIST because people have, like, basically decided, like, MNIST research is
not enough.
So, like, they just don't listen to MNIST research at all, right?
And it's starting to be that way for CIFAR 10, just because we're getting to be so good
on it now.
Okay.
Um, and, yeah, there's just known limitations that makes it, it makes it hard if you
have a genuinely good result to tell people that you have a genuinely good result, especially
because, like, as you scale up, like, it's also very computationally demanding, right?
So, um, you describe the data sets as being overfitted, which, um, explain, elaborate
on that because I tend to think of data as being inherently, well, the community is
overfitted.
They said, not even the algorithm itself.
There's actually this cool test that someone did, I can't remember who, where they showed
us, like, four pictures of images, and they asked, like, these are, these are the four
data sets, or, sorry, maybe not, they said, like, do you know what, they set these pictures
from, these pictures from, these pictures from, these pictures from, right?
Like many people did, like CIFAR is a very canonical data set, um, uh, there's a place
of data set, there's a large team understanding one, right?
And there's ImageNet, which is, like, more general.
So, but, and so you're basically saying that if someone can recognize these data sets so
well, we're designing solutions to them, they are not generalizable or not adequately
generalized.
And, like, people have actually reported, like, native results are generally not reported
as much, because it's just so much of it, right?
It's a very empirical field.
So maybe this is uninteresting now, but, um, this just happens so much, like, people have
noted that, um, the inception architecture seems to work much better in ImageNet than
it does in other tasks, um, and it is a pretty complicated thing, right?
So maybe, maybe that makes sense, or, um, I've had friends that I talk to, I'd hate that
I, a lot of my references are friends, but there's, like, the field moves so fast, right?
That, like, sometimes even archive can't keep up, which is, I think, super awesome for
being in it, where, and, anyway, they chat about sometimes how resnets, um, oftentimes
don't work for their computer vision architectures, right?
Or one of the best, um, practitioners of using Contnets, um, a friend of mine, Sander,
Dielamann, he works at DeepMind, he has not been able to find BatchNorm to work for him,
and I find that to be really interesting, like, is it because all of his other parameters
are tuned to BatchNorm?
Is there something that he solves, that BatchNorm solves also, that is not necessary?
Is, is, is he just wrong?
Um, honestly, I don't know, but I think that there's a bunch of cool stuff there that,
um, maybe we can figure out, right?
And is this inherent issue inherent to deep learning, or is it just the approach we've
taken?
Ooh, um, I mean, I would argue that it's not even an issue in deep learning, it's actually,
like, maybe we can look at the bright side of this, I was like, it's a miracle it even
works.
Um, so, um, going to the understanding topic, right?
There's, as far as I know, no practical theory in deep learning, like, there's nothing
that can actually, like, guide us to understandings, like, there's, what I call stories, like,
every paper has, like, a high level story of, this is why I think it works, and if you,
like, really try to vet the story really well, you can, like, very easily, like, disprove
it, and I know of no story that's, like, 100% bulletproof, um, so I'm willing to make
that claim, and so we have these stories, and, like, they, they guide people, but they,
they rarely work out as useful tools, unfortunately, so what we have instead is empirical results.
What we do is, we want generalization, generalization is kind of like a lofty concept, and we,
we don't really know, like, it's not, like, you can, in, like, traditional statistics,
you can kind of do that, um, but, like, deep learning is as much harder because you have
so many parameters, like, you can't really measure, well, you can measure the VC dimension,
but it's really, it's so big that it doesn't matter, um, there's a lot of things that,
what's the VC dimension? It's, I probably would screw this up, but I'll give you, like,
my best, like, first of all, your approximation of what it is. It's roughly how, um, powerful
your model is, so it shows, it kind of corresponds to, like, how much data you need in order to
get generalization. So, like, very curvy, powerful models have, like, a very high VC dimension,
which means that you need a lot of data. VC doesn't send for very curvy, does it?
No, it stands for, I know the VC stands for Vapnik, um, and the C stands for another person's
name. Okay. Um, sorry. Um, so generalization, like, in a, you know, like, in the very old school
machine learning sense, the sense that I don't think we'll come back to personally. Um, like,
you could have bounds on, like, how much data you need in order to get, like, this epsilon
difference between training tests and stuff like that. And that's just not something that's
going to happen in deep learning, as long as we keep using deep learning, we're probably
not going to get that. Right. So what we have is empirical results. And with these empirical
results, we just have a bunch of experiments and a bunch of data sets. And we show, like,
it seems to work on the data sets we've tried, um, hopefully it works in everything. And
so, like, this is where you might see it as a pro, but I, sorry, as a con, but I see this as a
huge positive of deep learning, right? Like, it's actually super cool that it generalizes,
right? Like, you can get a new computer vision to ask. Um, I use computer vision,
because like, that's one of the easier, um, domains and you kind of a ton of data. And you can just
generalize, you know, you can use it to generalize. You can use image net features to generalize
in that that that's just not something that makes sense, right? Um, I mean, like, if you look at it
from like a really strict perspective of like, there's no guarantee that this should work, but it tends
to work. And that's really interesting. All right. And I think that there's something, uh,
about deep learning that allows it to generalize so well, you know, you can even generalize to
domains that you've not even trained on. I think that there's been some work on
generalizing image net models to cartoons. And like, even like cartoon drawings of the things
that they were classifying, sometimes activate or there's something related to that. Yeah. So, yeah,
it's a wonder of deep learning. I actually, there are some experimental results that try to
explain after the fact why things work, but without being falsifiable, it's questionable how useful
it is. Um, so perhaps maybe deep learning is exploiting some of these kinds of explanations. There
was a recent one on physics. Okay. That, um, that deep learning is the, like, deep learning, the
kind, the class of things that deep learning is very good at fitting are a very like a very natural
class of functions. Therefore, since deep, deep learning models only can fit like a,
efficiently fit a small subset of the function space, but that happens to be like very common,
um, like based on physics, um, kinds of functions that would occur. Okay.
So you started out talking about data and that overfitting problem and then, uh, tools,
was that the network software software software? I, there's two more things in data, though,
which is that data we have, which is problematic. There's a data that we, so data we have when we use,
like data sets, this data we have that we don't use, and there's like tons and tons of data that
we have that we don't use, that I think that we just don't know how to use well, um,
unsupervised learning, multitask learning, transfer learning, we kind of use, but we don't
do very smart things, I think, um, and even like, there's implicit stuff like the trajectories
of the networks that you've passed through. Maybe there's some interesting information there.
And the last kind was the data that we don't have that we need. Like for example, measuring
these things that we really care about, that we are just missing right now. Like we have,
we have no way of measuring long-term dependency, like how well networks capture longer
dependencies. We don't have like a general RNN benchmark. We don't have a good benchmark for
visual attention. Um, we don't have a good benchmark for hierarchical learning. Like how do we
even know we're learning hierarchical stuff, right? Do we want to learn hierarchical stuff? Um,
I don't know, but like if I would think that if we want to learn something, having a benchmark
for it would be really good, right? So that was roughly it for data. Um, from a software perspective,
it was more about like how the tools we use nowadays really limit what we can do in like every
tools flawed in some ways, because it hits home for me personally because I'm a software engineer.
Okay. Um, and I want to use really good tools. You mean TensorFlow doesn't solve every problem in
the universe? Uh, no, not yet. I think they introduced some really good ideas. Um, they definitely
brought something to the table. Um, but it, it alone isn't enough. Um, it might like the,
the like I think better things could be built on top of it. I don't think that it's the low-level
components that are a problem. And I actually don't think like hardware is that big of an issue.
Like it's big of an issue that people, um, make it out to be. Um, in, in theory, in practice,
if you really want to do the art results and things sometimes that's needed, but there's like
higher level problems that you can solve without hardware. So the idea with behind software is that
you can like very like easily see situations where, um, like the software we have actually
prevents us from doing what we want to do. So I, I think I have like two examples that really
resonated with me where that, um, an example of bad software is when, um, it's easier to explain
in words the technique than it is with code because ideally you want to like express idea,
you want like the flow from ideas to code to be really easy and the flow from ideas to words
is generally pretty good. And that just means to give a bottleneck and like words to code. And
maybe it's a reality of life that it'll never be that simple. Did you provide a specific example?
Um, yes. I had like a list of like many examples of like different kinds of, um,
tricks that are hard to do in various frameworks. So depending on the framework you do some things
can be kind of difficult. So like for, what is it? For, um, so when you say tricks and frameworks,
the basic idea being, you know, kind of the, um, at, you know, the research, I did see that you
put a lot of paper, you were just showing a lot of papers, which is great documenting kind of
where the ideas came from. Uh, so in the research, you know, we're introducing all these various
tricks to improve solvability of the, of the deep learning networks. And it's not what I'm hearing is
the tools are, you know, on the one hand, you know, great. They're, they're raising the level
of abstraction and making this stuff, you know, more easily adoptable. But, you know, that also
prevents us from implementing some of these tricks, which have to be plugged in at lower levels.
Yeah, exactly. So, um, when I mentioned trick, I used that as a general term of like this,
like one unit of thing that you do to a neural network. Like, um, you can think of layers as
these tricks, but tricks being more than just layers. Like, for example, an additional regularizer
might be a trick, um, or doing, like, they could be pretty complicated, I think, like doing
unsupervised pre-training might be a trick. And the argument that I would have is that no framework
makes everything really easy. And easy in this sense is that I would, I would ideally like it such
that, um, everything just gets solved for me. Like, I would be able to like, like, this is probably
not going to happen, but we can get closer, right? Like, I would like to express, like, very
declaratively, like, what I want this neural network to be. Like, literally, like, take this
neural network in this database, apply this transformation, um, run this transformation, um,
do it on a, like, train on this training set. Like, I want it to be that simple. And I,
like, I don't think it can be, but like, striving towards that, I think, is good. Sure. And, like,
a lot of the frameworks, like, TensorFlow, um, doesn't support a bunch of the thing, like, it makes
it a large number of lines of code in order to do something rather than few. So what should be an
example, like, batch normalization is like a pretty simple thing, right? So, or sorry, it's a,
it's actually not a very simple thing in terms of implementation. But like, many frameworks can do
batch normalization very, very well. Like, torch can do batch normalization amazingly because like,
they can just implicitly keep it state. And in torch, like, each of the nodes applies its
updates on its own, like, when flowing through the grad and like, applying the updates.
Um, so that's very good. Um, but, um, TensorFlow, for example, like, in order to apply a batch normalization
after, it has to do quite a few things, right? Like, you need to create, like, some state for,
if you're doing the rolling mean approximation, you need to create some state for the mean,
some state for the variance. You need to make sure to, like, apply the updates to this thing. You
need to only apply the updates at training time. And then it becomes, like, much more complicated
than just, like, calling a layer on something, right? Um, depending on how you wrap it, of course.
But it, like, this, this kind of thing is just a layer in torch, right? And like, every framework has
its trade-offs, but I just don't think that we are at, like, the efficient frontier yet of, like,
this is like, like, I think we can get benefits for free, basically. And I actually have written
a few libraries that, um, that try to get these benefits for free. And I think they've been
pretty successful. Um, I'm still experimenting with them because I think there's so much to do there.
But it's, uh, it's an open problem. And are these libraries, uh, these stand-alone frameworks,
or libraries that plug into other existing frameworks? Um, mostly they go on top of
fiannoir tensorflow. Okay. Because I think that they're actually, or both. Um, I think that they
are both, like, very good baseline. So I'm a big fan of the computational graph. Um, I think the
design of theanos actually, like, quite excellent. I'm a huge fan of theano and its developer is,
it has the downside of distributed computing. Um, but I think that its abstraction level is actually
quite good. Like, it can capture that abstraction level very well. Its optimizations are like things
that I probably wouldn't do by hand anyway. So you get them for free. Um, it's, it's a, it's a,
it's a very, I am more focusing on theano tensorflow similar, but kind of as a mix of abstraction
levels. So, um, I'm focusing on the low level aspect. I think those low level aspects are actually,
like, quite good. Like, they might actually be on an efficient frontier of trade-offs, you know,
like trading off like usability versus, um, usability versus like, um,
flexibility, yeah, yeah, flexibility or performance. And I think that that's like, there's,
that's just one view, right? Like, use a, you know, have computational graph, have like,
all of the basic operations and they are, um, optionally use an optimizer in order to do that.
Like, another view would be like the torch it or cafe-ish view where you bundle up the pieces
of functionality that have a lot of, like, the, the highly optimized pieces, right? And like,
that's the view you go for next performance. I think it's also very different philosophically,
but there's nothing wrong with either of these views. So I'm, I'm fine building on top of that.
This is not what you're using. It's more of, yeah, it's more of the level and how you construct
the computational graph, which I think should be independent of theano or tensorflow. Like,
these are just different levels, right? Like, you could have like a really nice low level thing,
but change the high level thing on top of it and it should be fine, which is why I'm not the biggest
fan of tensorflow's like many different abstraction levels. And I think most of, well, all of the best
people I've talked to who use TensorFlow, um, they kind of only use a little bit of it. And they think
that a bunch of it is like, um, it's not the greatest, but I, I don't care, I'm not using it.
Okay. And like, it's, it's at those high levels that I think is very interesting. And like, that's
also where the user interacts with it, right? Like, if you're having code interact with code, it
doesn't matter. You can have like the ugliest interface in the world, like your compiler can just,
you know, switch things around and all of that stuff. Okay. So data, software, what was the third
piece optimization? So I touched a little bit into it with local learning. Yeah. And Andre Carpathi
had a great quote, which I can't remember off the top of my head, but it roughly goes along the lines
of that neural networks only do memorization. They don't do thinking. And this is problematic,
because this is already not as good, but this is problematic because we'd ideally like them to
think. We want them to do like cool, complicated things that like blow our minds in their coolness,
right? And they do blow our minds already. But perhaps those things were simpler than we thought.
Yeah. And what's going to happen when you want to do something pretty darn complicated, right?
Like we'll see, right? Like there's some tasks that we think that would require some pretty
complicated levels of thinking in order to do. Perhaps playing Starcraft, you need to like think
many moves ahead and imagine what the opponent's going to do in order to like take actions. And
neural networks are not very good at imagining what to do yet. Maybe that will change, but we'll see.
And Andrewing likes to say that as a heuristic of what neural networks can do is anything a
human can do in less than one second. But I mean, if that's a hard limitation, then there's a lot
of tasks that take more than one second for people to do. And will this solve generally I for us,
maybe not like when you phrase it that way, right? So it should be possible, right? Like it's
modular in theory. Like you can't just have architectures that give in a magic set of parameters
would solve that task. So this question is how do we do that, right? And there's just many tricks
on that. And I talk a little bit about the downsides of local learning, how we don't pay attention
to exploration in supervised learning. And like mostly it's paid attention,
enforcement learning, but we treat it as like obviously the plane, like there is some implicit
exploration because you're, you know, you're using stochastic gradient descent. So your
gradients noisy. But roughly if it wasn't noisy, you'd, you know, be blocked on a point and you
just till climb down some direction and be stuck there. And like you don't even know how good of
a solution that is, right? So that's that can be, I don't know, like that can be a very unsatisfying
because if the answer is, I mean, this goes back to what I was talking about like in terms of
limitations, like maybe local learning just can't solve this, right? And that would be super
duper unsatisfying because local learning is like our most scalable learning algorithm we have,
like using gradients is really, really good for turning lots of parameters. Like we're going to have
to have to make like a lot of plant, like a lot of different plans we want generally with
our gradient descent. So yeah, we're going to have to figure it out. So we're going to have to
figure out tricks and how to do this better. Maybe tricks for more principled exploration. And maybe
this will make it such that these won't be problems anymore. At least our will find much harder
problems, right? Though hopefully always be problems. And that would, that's what keeps the field
going, right? Yeah. Yeah. But hopefully they're not intrinsic to the way we do optimization.
And people are making better optimizers. Yeah. You know, it's quite slow, the progress.
Right. So data software optimization and understanding, and we talked a little bit about that earlier.
Are there, are you going to post your slides up somewhere?
Um, probably. I think that, well, the, I think I've, I think that rarely people put the slides
up somewhere. Okay. But they haven't asked me for the slides yet. I think they're supposed to do
that after the presentation. Okay. Which is probably good since there was like last minute editing going
on. Um, but it'll almost certainly be up somewhere. Okay. And how can folks, if folks want to learn
more about what you're up to or find you do you have a GitHub or Twitter or. I had do have a GitHub.
It's, even though that's probably not a great way to contact someone. What's it? What's it?
I'm not. I'm not. Right. GitHub.com slash Diego. Diogio. 149. Okay. And, uh, probably email would be
the best way. This is something that I love chatting about. It would be Diogio at. Oh, God, my
company name's hard to spell. Um, analytic, which is E N L I T I C dot com. Okay. Great.
Cool. Thank you. Awesome. Hey, thanks so much.
All right, everyone. That's it for today's interview. Please leave a comment on the show notes page
at twimlai.com slash talk slash eight or tweet to me at at Sam Charrington or at twimlai to discuss
this show or let me know how you liked it. Thanks so much for listening and catch you next time.
