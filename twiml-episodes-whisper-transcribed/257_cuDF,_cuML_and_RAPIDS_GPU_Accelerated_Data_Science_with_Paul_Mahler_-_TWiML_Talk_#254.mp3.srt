1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,440
I'm your host Sam Charrington.

4
00:00:32,440 --> 00:00:36,680
This week's shows are drawn from some of the great conversations I had at the recent Nvidia

5
00:00:36,680 --> 00:00:40,960
GPU technology conference and they're brought to you by Dell.

6
00:00:40,960 --> 00:00:44,840
If you caught my tweets from GTC, you may already know that one of the announcements this

7
00:00:44,840 --> 00:00:49,320
year was a new reference architecture for data science work sessions powered by high

8
00:00:49,320 --> 00:00:54,440
NGPUs and accelerated software such as Nvidia's Rabbids.

9
00:00:54,440 --> 00:00:58,960
Dell was among the key partners showcased during the launch and offers a line of workstations

10
00:00:58,960 --> 00:01:03,040
designed for modern machine learning and AI workloads.

11
00:01:03,040 --> 00:01:07,720
To learn more about Dell precision workstations and some of the ways they're being used by customers

12
00:01:07,720 --> 00:01:12,320
in industries like media and entertainment, engineering and manufacturing, healthcare

13
00:01:12,320 --> 00:01:23,520
and life sciences, oil and gas and financial services, visit Dell EMC.com slash precision.

14
00:01:23,520 --> 00:01:26,440
Alright everyone, I am on the line with Paul Moller.

15
00:01:26,440 --> 00:01:31,680
Paul is a senior data scientist and technical product manager for machine learning at Nvidia.

16
00:01:31,680 --> 00:01:35,080
Paul, welcome to this week in machine learning and AI.

17
00:01:35,080 --> 00:01:36,400
Thanks for having me.

18
00:01:36,400 --> 00:01:37,400
Absolutely.

19
00:01:37,400 --> 00:01:42,960
I'm looking forward to jumping into our conversation which will be focused on what Nvidia is doing

20
00:01:42,960 --> 00:01:47,800
around rapids and QML and all of the interesting stuff in that area.

21
00:01:47,800 --> 00:01:51,160
But before we do that, you were a philosophy major.

22
00:01:51,160 --> 00:01:55,760
How did you make your way to working in machine learning?

23
00:01:55,760 --> 00:02:02,320
I was a philosophy major and two years before I was set to graduate, I added economics as

24
00:02:02,320 --> 00:02:09,040
a major because I read the economist magazine and thought that it was a fascinating collection

25
00:02:09,040 --> 00:02:13,640
of a bunch of different articles about different aspects of the world.

26
00:02:13,640 --> 00:02:19,440
So I figured if that's what economists read, I wanted to be an economist.

27
00:02:19,440 --> 00:02:27,200
I went on to do a master's degree in economics where I mostly focused on quantitative methods.

28
00:02:27,200 --> 00:02:34,360
In an earlier life, I went out to Washington, D.C. where I worked as an economist.

29
00:02:34,360 --> 00:02:42,040
I began my career at the World Bank, serving on health and human welfare issues in sub-Saharan

30
00:02:42,040 --> 00:02:43,520
Africa.

31
00:02:43,520 --> 00:02:49,200
And then I worked for the office of the chief economist at Fannie Mae.

32
00:02:49,200 --> 00:02:54,480
Now one day, while waiting for the bus to get home from Fannie Mae, there was an article

33
00:02:54,480 --> 00:02:59,800
from the New York Times about a gentleman from SUNY Buffalo that had written an algorithm

34
00:02:59,800 --> 00:03:03,040
that was offering notes on screenplays.

35
00:03:03,040 --> 00:03:09,560
And I always had like a hobby interest in the different creative arts aspects of our

36
00:03:09,560 --> 00:03:10,560
culture.

37
00:03:10,560 --> 00:03:16,680
And when I saw that somebody had written essentially a big block of math and code that was telling

38
00:03:16,680 --> 00:03:23,440
writers how to write their screenplays better, I immediately decided that I needed to switch

39
00:03:23,440 --> 00:03:30,160
into data science or big data, which was the more popular term at the time, because that

40
00:03:30,160 --> 00:03:34,240
was where some of the most interesting things in the world were going on.

41
00:03:34,240 --> 00:03:35,760
That's a great story.

42
00:03:35,760 --> 00:03:42,120
You remember the moment, literally, the bus ride that triggered the, that set you off

43
00:03:42,120 --> 00:03:43,880
down this path.

44
00:03:43,880 --> 00:03:48,680
Yeah, it was raining at the time.

45
00:03:48,680 --> 00:03:49,680
Awesome.

46
00:03:49,680 --> 00:03:51,480
And so, what do you do now?

47
00:03:51,480 --> 00:03:54,200
I've been doing for the last year.

48
00:03:54,200 --> 00:04:00,320
I had been at a couple of startups, and I joined in the video to work on what we've been

49
00:04:00,320 --> 00:04:05,240
calling the Rapids project or the Rapids ecosystem.

50
00:04:05,240 --> 00:04:13,000
Now what that began as is that our director of engineering, who I'd worked with previously

51
00:04:13,000 --> 00:04:21,440
at Accenture, for years had been saying that we see this great acceleration in neuro-acceleration

52
00:04:21,440 --> 00:04:26,040
neural network methods as a result of getting them on GPUs.

53
00:04:26,040 --> 00:04:31,640
But you are not seeing any of those advantages for more of the bread and butter data science

54
00:04:31,640 --> 00:04:37,200
that happens at a lot of places like Fannie Mae or the World Bank, where they may have large

55
00:04:37,200 --> 00:04:41,920
data sets and questions they want to address through machine learning, but we aren't talking

56
00:04:41,920 --> 00:04:49,120
about convolutional neural networks to understand images.

57
00:04:49,120 --> 00:04:56,520
So, you know, a lot of the time when I was working at a data scientist at a couple of startups,

58
00:04:56,520 --> 00:05:03,360
I like to joke that I was really a bar trivia champion, because while I was waiting for

59
00:05:03,360 --> 00:05:09,200
my code to finish running and spit out my result, I had plenty of time to read all the news

60
00:05:09,200 --> 00:05:13,280
of the world on the internet.

61
00:05:13,280 --> 00:05:19,040
And I guess, you know, unfortunately, for me and my compatriots in data science, what

62
00:05:19,040 --> 00:05:25,880
we've done with QML and QDF in particular is, you know, if somebody knows pandas or they

63
00:05:25,880 --> 00:05:33,080
know the Pi data ecosystem, they can immediately jump right in and start seeing just crazy

64
00:05:33,080 --> 00:05:39,880
speed ups, like, you know, 50X, like, sometimes more than that on doing their end-to-end workflows.

65
00:05:39,880 --> 00:05:46,640
And that includes, you know, reading from a disk to GPU memory, doing all your data

66
00:05:46,640 --> 00:05:53,400
munging and merging and variable creation through actually executing your algorithm and, you

67
00:05:53,400 --> 00:05:56,080
know, making inferences.

68
00:05:56,080 --> 00:06:02,560
So the idea was that since all of this, well, I mean, some of it is not obviously tractable

69
00:06:02,560 --> 00:06:10,520
to GPUs, we are able to process strings in the latest iteration of QDF, which to me seems

70
00:06:10,520 --> 00:06:16,480
like a miracle, but it really, I like to joke that it's kind of like before, you know,

71
00:06:16,480 --> 00:06:22,640
the team that I work with had delivered these big pieces of QDF, it's like I could drive

72
00:06:22,640 --> 00:06:25,680
a car, and now suddenly I can fly a plane.

73
00:06:25,680 --> 00:06:32,840
And I don't need to be an expert in CUDA or parallel algorithms or anything except the

74
00:06:32,840 --> 00:06:35,280
tools that I've worked with most of my career.

75
00:06:35,280 --> 00:06:38,320
Now, let's take a step back, maybe.

76
00:06:38,320 --> 00:06:45,080
When I introduce you, I mentioned QML, you've mentioned QDF, mentioned Rapids.

77
00:06:45,080 --> 00:06:52,720
Can you kind of paint a picture of the broader ecosystem of software and libraries and

78
00:06:52,720 --> 00:07:00,280
tools that comprise our makeup, Rapids, and, you know, how they all fit together?

79
00:07:00,280 --> 00:07:01,280
Yeah.

80
00:07:01,280 --> 00:07:06,920
So Rapids is the overall name of the project, and that's made up of smaller sub libraries

81
00:07:06,920 --> 00:07:12,200
that all start with CUDA, because that's inherited from CUDA.

82
00:07:12,200 --> 00:07:14,640
Rapids is built on Nvidia CUDA.

83
00:07:14,640 --> 00:07:22,440
And CUDA is the, for anyone who's not familiar with the underlying library or API for

84
00:07:22,440 --> 00:07:24,680
doing things on Nvidia GPUs.

85
00:07:24,680 --> 00:07:25,680
Right.

86
00:07:25,680 --> 00:07:30,520
It's the general purpose computing library for Nvidia GPUs.

87
00:07:30,520 --> 00:07:31,520
Okay.

88
00:07:31,520 --> 00:07:40,160
And if you think about the more broad, pie data ecosystem, I think a lot of people do

89
00:07:40,160 --> 00:07:45,680
a lot of their initial data cleaning and exploration in pandas.

90
00:07:45,680 --> 00:07:52,400
And so that's what CUDA is meant to replace for people that are moving their workload

91
00:07:52,400 --> 00:07:54,080
on to GPUs.

92
00:07:54,080 --> 00:08:04,280
And so the API is very, very close, and you're able to, in some cases, just change the import

93
00:08:04,280 --> 00:08:09,560
statements at the top of your program, and it will just work.

94
00:08:09,560 --> 00:08:16,520
And so pandas has this kind of core abstraction of a data frame, and so CUDA is just a kind

95
00:08:16,520 --> 00:08:19,080
of, you can think of it as a CUDA-powered data frame.

96
00:08:19,080 --> 00:08:20,080
Yeah.

97
00:08:20,080 --> 00:08:21,680
I think that's the best way to think about it.

98
00:08:21,680 --> 00:08:22,680
Yeah.

99
00:08:22,680 --> 00:08:32,920
QML is our machine learning toolkit, and we aspire to, one day have almost all the functionality

100
00:08:32,920 --> 00:08:36,200
that exists in Scikit Learn.

101
00:08:36,200 --> 00:08:40,640
Scikit Learn is an eminent package built by some of the world's greatest developers.

102
00:08:40,640 --> 00:08:42,840
So we've got a ways to go there.

103
00:08:42,840 --> 00:08:49,080
But we've been rapidly adding algorithms in the last release, for example.

104
00:08:49,080 --> 00:08:58,120
We have stochastic gradient descent regression, ordinary linear regression, ridge regression,

105
00:08:58,120 --> 00:09:04,200
principle components analysis, and some other things like Kalman filtering.

106
00:09:04,200 --> 00:09:12,160
What we're trying to do is start with the things that are the real workhorses of day-to-day

107
00:09:12,160 --> 00:09:17,680
machine learning in business and other parts of industry.

108
00:09:17,680 --> 00:09:20,360
And it's been exciting to watch the package grow.

109
00:09:20,360 --> 00:09:30,000
In fact, when we launched back in October, we had, I think, four algorithms in QML, and

110
00:09:30,000 --> 00:09:32,280
we've doubled that over the last couple of months.

111
00:09:32,280 --> 00:09:40,120
It was very exciting to present at the GPU developer's conference in San Jose, California,

112
00:09:40,120 --> 00:09:46,280
a couple of weeks ago, to the wider community, all the things that we had been able to deliver

113
00:09:46,280 --> 00:09:48,480
in such a short amount of time.

114
00:09:48,480 --> 00:09:52,360
You mentioned that you kind of aspired to Scikit Learn.

115
00:09:52,360 --> 00:09:58,040
Does that mean that QML replaces Scikit Learn?

116
00:09:58,040 --> 00:10:05,560
It sounds like it does, for folks that are trying to take advantage of the GPU.

117
00:10:05,560 --> 00:10:11,440
And was there an opportunity to rather than replacing Scikit Learn kind of fit in underneath

118
00:10:11,440 --> 00:10:19,200
it so folks can that use that API or that have existing work that uses Scikit Learn could

119
00:10:19,200 --> 00:10:25,680
take advantage of the GPU acceleration without having to rewrite their apps?

120
00:10:25,680 --> 00:10:33,680
I mean, at least with the algorithms we've delivered, we've tried to keep the API one-to-one.

121
00:10:33,680 --> 00:10:38,120
For any of your listeners, I would encourage them to take a look at the API and just see

122
00:10:38,120 --> 00:10:39,920
how close it is to Scikit Learn.

123
00:10:39,920 --> 00:10:46,560
I'd also like to add that we've partnered with Enrea, the French research institution

124
00:10:46,560 --> 00:10:54,840
that does a lot of work on Scikit Learn and over the next few months and few years we're

125
00:10:54,840 --> 00:10:58,240
going to be building that collaboration with them.

126
00:10:58,240 --> 00:11:04,640
I don't think we'll ever replace Scikit Learn because there are still problems where I

127
00:11:04,640 --> 00:11:11,600
don't think it's big enough or the use case is right to necessarily go to full GPUs.

128
00:11:11,600 --> 00:11:18,840
So I think of certain analyses I did as an economist which would look like machine learning

129
00:11:18,840 --> 00:11:26,720
but we're maybe a few thousand rows and this was much more traditional frequentist statistics.

130
00:11:26,720 --> 00:11:32,080
I think that there's always going to be a lot of that work being done and I think that

131
00:11:32,080 --> 00:11:37,320
with any data science work it's about finding the right tool for the job.

132
00:11:37,320 --> 00:11:48,000
But I will tell you when I was testing out our code earlier in the summer, our demo workflow

133
00:11:48,000 --> 00:11:55,880
involved reading in, I think around like a gigabyte of CSV data to pandas and on my MacBook

134
00:11:55,880 --> 00:12:04,800
Pro it took like five and a half minutes and on a single GPU it took like 15 or 20 seconds.

135
00:12:04,800 --> 00:12:15,320
When you talk about data analysis is an iterative interactive process and the faster you can

136
00:12:15,320 --> 00:12:21,160
move the more fluid your conversation with the data will feel to you as a user.

137
00:12:21,160 --> 00:12:26,400
There won't be the long wait times to see results or see if you made a coding error in

138
00:12:26,400 --> 00:12:27,400
my case.

139
00:12:27,400 --> 00:12:31,360
That's an opportunity to become a bar trivia master.

140
00:12:31,360 --> 00:12:34,960
Yeah, it's a shock.

141
00:12:34,960 --> 00:12:39,920
So before we dig into that because that's an interesting point there, we're kind of

142
00:12:39,920 --> 00:12:41,640
talking about the landscape.

143
00:12:41,640 --> 00:12:44,360
You mentioned Kudieff, Kuh-A-Mal.

144
00:12:44,360 --> 00:12:48,680
Are there other major pieces that we should be keeping track of in this conversation?

145
00:12:48,680 --> 00:13:00,920
Yeah, we are working on a graph analytics package called Kuh-Graph and yeah, our minds

146
00:13:00,920 --> 00:13:05,640
are so fixated on accelerating the algorithms, we're totally out of bandwidth for fancier

147
00:13:05,640 --> 00:13:06,640
names.

148
00:13:06,640 --> 00:13:12,160
But everyone knows that Jensen does all the naming it in videos, why would anyone else

149
00:13:12,160 --> 00:13:16,000
spend any time thinking about that?

150
00:13:16,000 --> 00:13:21,040
Kuh-Graph is embarrassing in a sense that we compare it to a graph analytics package

151
00:13:21,040 --> 00:13:26,560
in Python and it's one of those things where you see the numbers, you just really want

152
00:13:26,560 --> 00:13:31,320
to double check them, like 10,000 time speed ups over network X for certain algorithms.

153
00:13:31,320 --> 00:13:37,280
Well, that was my reaction to the loading the data frame.

154
00:13:37,280 --> 00:13:40,440
And I still want to get through the kind of broad landscape before we dig deep into

155
00:13:40,440 --> 00:13:44,680
that, but that's the first place I'm going to come back to once we do.

156
00:13:44,680 --> 00:13:51,520
So you've got a graph analytics piece in Kuh-Graph, any other major components here?

157
00:13:51,520 --> 00:13:55,800
Some of the things that began as major components are now under the hood.

158
00:13:55,800 --> 00:14:00,600
So we put a bunch of effort into building a string reader.

159
00:14:00,600 --> 00:14:07,320
So you could directly parse data sets with strings, it's a very common thing that happens

160
00:14:07,320 --> 00:14:13,080
in data science, GPUs do not like strings.

161
00:14:13,080 --> 00:14:21,240
But now you can do things like just easily create your dummy variables from strings on

162
00:14:21,240 --> 00:14:26,920
the GPU, which sounds kind of ho-hum, but it's actually a pretty major achievement.

163
00:14:26,920 --> 00:14:29,000
It's just part of the whole speeding things up.

164
00:14:29,000 --> 00:14:34,160
I don't think our case would be as compelling if we said that it could only be numerical

165
00:14:34,160 --> 00:14:37,840
data in the Kuh-Graph data frame.

166
00:14:37,840 --> 00:14:40,680
That simply will not work for many use cases.

167
00:14:40,680 --> 00:14:51,200
So we also have a package called Kuh-Cross filter, it's written as Kuh-X filter, and we're

168
00:14:51,200 --> 00:14:58,640
going to be building out some GPU-accelerated visualizations.

169
00:14:58,640 --> 00:15:07,240
So if you think of the workflow from munging to analysis to insight through visualizations,

170
00:15:07,240 --> 00:15:11,480
we want to be able to offer every piece of that puzzle.

171
00:15:11,480 --> 00:15:17,600
The other thing, we're heavily using a software called DASC.

172
00:15:17,600 --> 00:15:24,240
And DASC is a package that handles distributed computing.

173
00:15:24,240 --> 00:15:30,720
It has been used to scale out pandas, for example, to multiple cores.

174
00:15:30,720 --> 00:15:37,960
And we were lucky enough that the creator of DASC has joined our team, and is helping

175
00:15:37,960 --> 00:15:45,240
us use that as a way to distribute workloads when we're talking about moving beyond single

176
00:15:45,240 --> 00:15:47,280
node of GPUs.

177
00:15:47,280 --> 00:15:54,280
Let's maybe go back to the initial example you gave of loading the pandas data frame.

178
00:15:54,280 --> 00:15:56,640
We're loading the data frame.

179
00:15:56,640 --> 00:15:58,040
You said it was a terabyte?

180
00:15:58,040 --> 00:15:59,640
It was a gigabyte.

181
00:15:59,640 --> 00:16:05,000
It's pretty easy to choke pandas, and I'm sure a lot of your listeners have experienced

182
00:16:05,000 --> 00:16:07,160
this before.

183
00:16:07,160 --> 00:16:16,560
The workflow was Fannie Mae makes loan-delinquency data going back, I think, 16 years available

184
00:16:16,560 --> 00:16:18,200
for free.

185
00:16:18,200 --> 00:16:26,440
And this is the whole payment history for a subset of all the loans that Fannie Mae has

186
00:16:26,440 --> 00:16:27,840
acquired.

187
00:16:27,840 --> 00:16:35,600
And as a demo workflow, what we wanted to do is read in however many quarters of data

188
00:16:35,600 --> 00:16:45,160
we could fit, or were relevant, and then apply XGBoost to predict default, which reminds

189
00:16:45,160 --> 00:16:49,160
me of another sort of under-the-hood improvement that we've made.

190
00:16:49,160 --> 00:16:56,720
It's not really under-the-hood, but we have made contributions to the DMLC XGBoost library,

191
00:16:56,720 --> 00:16:59,560
and we'll continue to do so.

192
00:16:59,560 --> 00:17:06,960
That has appeared in a lot of our early presentations and webinars.

193
00:17:06,960 --> 00:17:12,800
I think XGBoost is almost like magic, and it's a good, broad workhorse for the first thing

194
00:17:12,800 --> 00:17:14,200
that we were going to introduce.

195
00:17:14,200 --> 00:17:24,000
But we are working with the community to make certain changes to XGBoost that make it more

196
00:17:24,000 --> 00:17:29,920
amenable for the rapid ecosystem, but then giving those back to the community.

197
00:17:29,920 --> 00:17:41,200
So pardon that sidebar, but before any real data processing had happened, just bringing

198
00:17:41,200 --> 00:17:49,400
in this dense, large data set, at a rule of thumb, we couldn't do more than a couple of

199
00:17:49,400 --> 00:17:50,720
quarters of data.

200
00:17:50,720 --> 00:17:56,240
And then you would really see the time to load and go through the data preparation and

201
00:17:56,240 --> 00:18:00,760
execute the algorithm increase substantially.

202
00:18:00,760 --> 00:18:10,000
So I think that the largest we could do on pandas quarter-wise was like two or three quarters

203
00:18:10,000 --> 00:18:11,000
of data.

204
00:18:11,000 --> 00:18:12,000
They were small.

205
00:18:12,000 --> 00:18:15,040
So I think the biggest we tried was about one and a half gigabytes.

206
00:18:15,040 --> 00:18:21,840
And that's where you saw those really kind of frustrating load times of more than five

207
00:18:21,840 --> 00:18:22,840
minutes.

208
00:18:22,840 --> 00:18:31,800
And so what's happening here in these two different scenarios is on the panda side, you've got a gig

209
00:18:31,800 --> 00:18:37,840
and a half on both sides, you've got a gig and a half of information on a disk, probably

210
00:18:37,840 --> 00:18:45,000
a CSV file, perhaps, and you're loading it into a data frame on the panda side.

211
00:18:45,000 --> 00:18:48,080
It's a data frame that's located in RAM.

212
00:18:48,080 --> 00:18:54,760
And on the rapid side, it's a QDF frame that's located on the GPU itself.

213
00:18:54,760 --> 00:18:55,760
Is that right?

214
00:18:55,760 --> 00:18:56,760
Yep.

215
00:18:56,760 --> 00:18:57,760
In GPU memory.

216
00:18:57,760 --> 00:18:58,760
Okay.

217
00:18:58,760 --> 00:19:05,040
And that's where the kind of the five minute versus 15 seconds differentiation come from.

218
00:19:05,040 --> 00:19:06,040
Yeah.

219
00:19:06,040 --> 00:19:11,080
And this is where this really is more for like our hardcore C guys, but there's some

220
00:19:11,080 --> 00:19:15,320
problems with pandas that have been known for a while.

221
00:19:15,320 --> 00:19:16,320
It's great.

222
00:19:16,320 --> 00:19:23,840
I can't thank West McKinney and the community enough for open sourcing it because it's put

223
00:19:23,840 --> 00:19:25,840
food on my plate for five or six years.

224
00:19:25,840 --> 00:19:30,600
But it's also single threaded on the CPU.

225
00:19:30,600 --> 00:19:37,720
So even in the world of CPUs, you started to see people look for things like desk to help

226
00:19:37,720 --> 00:19:39,320
better leverage.

227
00:19:39,320 --> 00:19:44,000
And the multiple cores of CPUs you might have on your MacBook Pro.

228
00:19:44,000 --> 00:19:48,760
I think it's, you know, I think it's kind of funny, you know, every, every data science

229
00:19:48,760 --> 00:19:53,200
gig I've had, they've given me like a shiny MacBook Pro and I mostly work in Jupyter

230
00:19:53,200 --> 00:19:54,200
notebooks.

231
00:19:54,200 --> 00:20:00,000
And most of that stuff is only taking advantage of a single core of the processor.

232
00:20:00,000 --> 00:20:01,000
Right.

233
00:20:01,000 --> 00:20:02,000
Right.

234
00:20:02,000 --> 00:20:07,800
I mean, I, I don't want to exaggerate too much, but it's almost like you still want

235
00:20:07,800 --> 00:20:09,800
the next MacBook Pro.

236
00:20:09,800 --> 00:20:10,800
Yeah.

237
00:20:10,800 --> 00:20:11,800
Well, wait and see.

238
00:20:11,800 --> 00:20:13,000
I want to see what they do with that touch bar.

239
00:20:13,000 --> 00:20:21,320
I got some thoughts on the touch bar, but so let's not even go there.

240
00:20:21,320 --> 00:20:27,720
But you know, and so the other thing is that when we, when we move past the, you know,

241
00:20:27,720 --> 00:20:36,920
the massive parallelism that you can get from using GPUs, when you get to the side,

242
00:20:36,920 --> 00:20:37,920
I know better.

243
00:20:37,920 --> 00:20:44,400
This is all for the most part, matrix algebra and GPUs love matrix algebra.

244
00:20:44,400 --> 00:20:53,280
They are designs to do it and, you know, in our algorithms like doing, um, Ridge regression,

245
00:20:53,280 --> 00:21:00,720
which can take a some time to run on, uh, in the conventional, pydated ecosystem, um,

246
00:21:00,720 --> 00:21:08,800
I gave a, uh, tutorial at the GPU, uh, developers, uh, GPU technology convention, I was just

247
00:21:08,800 --> 00:21:13,720
like, we're going to just do a hyper parameter search and run through like a thousand Ridge

248
00:21:13,720 --> 00:21:19,720
regressions on this Black Friday data set because it's just fast enough that we can kind

249
00:21:19,720 --> 00:21:23,800
of brute force hyper parameter search on certain data set sizes.

250
00:21:23,800 --> 00:21:29,320
And, uh, do you have comparative results for that particular scenario?

251
00:21:29,320 --> 00:21:31,680
We aren't baking off every algorithm these days.

252
00:21:31,680 --> 00:21:32,760
Ridge regression is fast.

253
00:21:32,760 --> 00:21:33,760
I don't know.

254
00:21:33,760 --> 00:21:38,840
Like doing a Ridge regression, I think like 800,000 rows took me like, uh, less than a second

255
00:21:38,840 --> 00:21:39,960
couple of seconds.

256
00:21:39,960 --> 00:21:44,240
I remember being fast enough where I could do a live demo and run through a hundred iterations

257
00:21:44,240 --> 00:21:45,240
of it.

258
00:21:45,240 --> 00:21:48,600
Normally doing two or three would have taken quite some time.

259
00:21:48,600 --> 00:21:55,440
Let's jump into the, uh, to this, this part of it, the QML library.

260
00:21:55,440 --> 00:22:02,080
Can you maybe talk us through the, uh, technical underpinnings of this?

261
00:22:02,080 --> 00:22:06,480
I mean, is it as simple as, hey, these things love matrix multiplication and we're just

262
00:22:06,480 --> 00:22:12,880
doing matrix multiplication using kuda and it's just faster or they're kind of interesting

263
00:22:12,880 --> 00:22:19,280
nuances to the way some of these algorithms work that, uh, might be worth chatting about.

264
00:22:19,280 --> 00:22:24,720
Um, well, to start with a little bit about the, uh, architecture of QML.

265
00:22:24,720 --> 00:22:32,000
Um, so QML is built on top of what we're calling ML prams.

266
00:22:32,000 --> 00:22:40,840
Um, these are, uh, primitive functions that are composed of even lower level math libraries

267
00:22:40,840 --> 00:22:46,480
or various things that have been developed at Nvidia for certain linear algebra purposes.

268
00:22:46,480 --> 00:22:53,640
And so, um, we take these primitives and they are, uh, delivered in C++.

269
00:22:53,640 --> 00:23:01,880
So then when we need something new, um, like, uh, I have a colleague working on, um, doing

270
00:23:01,880 --> 00:23:05,480
massively parallel arena regressions.

271
00:23:05,480 --> 00:23:12,580
And so when he began working on that, we already had, uh, Kalman filter primitive and an

272
00:23:12,580 --> 00:23:14,040
OLS primitive.

273
00:23:14,040 --> 00:23:21,000
And so the amount of new work that he needed to begin composing a prototype was dramatically

274
00:23:21,000 --> 00:23:22,400
reduced.

275
00:23:22,400 --> 00:23:27,840
One day in the future, I actually want to see these ML prams wrapped in Python.

276
00:23:27,840 --> 00:23:33,360
So, um, different, you know, machine learning researchers or graduate students that aren't

277
00:23:33,360 --> 00:23:39,920
experts in parallel programming would be able to, um, mock up the new algorithms they're

278
00:23:39,920 --> 00:23:44,680
inventing and be able to, uh, leverage the advantages of the GPU.

279
00:23:44,680 --> 00:23:47,880
Yeah, that sounds like a, uh, that sounds like a no brainer.

280
00:23:47,880 --> 00:23:52,680
The ML Prams open source or they locked up in a binary or something.

281
00:23:52,680 --> 00:23:57,520
I mean, even if they're in a binary, they want to be able to, yeah, that's a mix.

282
00:23:57,520 --> 00:24:01,040
I mean, so some of the stuff is, uh, Nvidia proprietary stuff and we've tried to wrap it

283
00:24:01,040 --> 00:24:04,680
in a binary, but other ones are more open source.

284
00:24:04,680 --> 00:24:07,360
And it's a, it's a discussion that we're going to continue to have.

285
00:24:07,360 --> 00:24:09,720
But wherever that ends up, oh, sorry, go ahead.

286
00:24:09,720 --> 00:24:17,440
Is it a well documented, uh, you know, set of primitives or is it kind of internal, nobody

287
00:24:17,440 --> 00:24:21,880
really knows about them outside of the company?

288
00:24:21,880 --> 00:24:26,080
It's something that, you know, um, I've tried to mention publicly, whatever we speak about

289
00:24:26,080 --> 00:24:27,080
it.

290
00:24:27,080 --> 00:24:28,560
It's not super well documented right now.

291
00:24:28,560 --> 00:24:33,840
So you'd need to, uh, you need to be able to go into, uh, our GitHub folder and look

292
00:24:33,840 --> 00:24:39,920
at the primitives folder and be able to read a little bit of C C plus plus got it.

293
00:24:39,920 --> 00:24:40,920
Okay.

294
00:24:40,920 --> 00:24:47,080
Um, but, uh, yeah, I hopefully, uh, want to, you know, wrap these in Python and introduce

295
00:24:47,080 --> 00:24:50,640
them to the greater development community to see what else they can do with them.

296
00:24:50,640 --> 00:24:54,680
Um, now you've mentioned, are there, are there some algorithms that are more or less

297
00:24:54,680 --> 00:24:55,680
tractable to that?

298
00:24:55,680 --> 00:25:03,400
I mean, right now we're working on building a lot of, uh, different solvers for more exotic

299
00:25:03,400 --> 00:25:12,040
kinds of, um, regressions and one of the challenges in developing those has been, um, and this

300
00:25:12,040 --> 00:25:17,760
really is where, you know, I'm, uh, you know, all of some of the, the thinking of my colleagues,

301
00:25:17,760 --> 00:25:22,760
they're essentially sequential, uh, algorithms, the way that they originally designed, right?

302
00:25:22,760 --> 00:25:28,400
If you look at the most basic version of like, uh, a gradient descent, you, you know, start

303
00:25:28,400 --> 00:25:32,960
some place and you keep taking little steps until you're satisfied and then you stop.

304
00:25:32,960 --> 00:25:40,240
Now that's, um, uh, sequential operation, um, when we've been doing some early inspections

305
00:25:40,240 --> 00:25:47,240
on different solvers, um, this morning, in fact, um, a colleague told me that he was disappointed

306
00:25:47,240 --> 00:25:52,560
that we were only getting a 3x speed up because he was still trying to think around how to

307
00:25:52,560 --> 00:25:55,400
make the algorithm less sequential.

308
00:25:55,400 --> 00:26:01,000
So there will be things just by the way that the underlying math works that aren't going

309
00:26:01,000 --> 00:26:08,680
to necessarily be another 10,000 x speed up, but a two or three x speed up, I think is still

310
00:26:08,680 --> 00:26:10,040
pretty great.

311
00:26:10,040 --> 00:26:16,800
And the other, you know, really heavy intellectual work that, um, is going on right now, um, that

312
00:26:16,800 --> 00:26:23,360
we hope to wrap up by the end of the summer or the fall is going to be on, um, multi-node,

313
00:26:23,360 --> 00:26:24,680
multi-GPU algorithms.

314
00:26:24,680 --> 00:26:26,720
Um, that's using DASC.

315
00:26:26,720 --> 00:26:34,960
Uh, in some cases, it'll use DASC and we're currently working our way through what, um,

316
00:26:34,960 --> 00:26:40,880
other kind of, uh, communications layers could be helpful in trying to, you know, block

317
00:26:40,880 --> 00:26:47,200
up this data and distribute it across, uh, a cluster of GPUs in a way that creates, uh,

318
00:26:47,200 --> 00:26:49,440
you know, a wow moment for the user.

319
00:26:49,440 --> 00:26:50,440
Mm-hmm.

320
00:26:50,440 --> 00:26:55,040
I know that's a little marketing, but that's what I'm, I'm looking at, I'm looking at what,

321
00:26:55,040 --> 00:26:58,320
you know, we're doing with QML, like that's what I'd really like.

322
00:26:58,320 --> 00:27:03,200
You know, we've, we've been lucky so far that, um, we have gotten some wows with what

323
00:27:03,200 --> 00:27:10,840
we deliver, but the, the underlying, uh, you know, algebra and algorithms of breaking

324
00:27:10,840 --> 00:27:18,560
some of these things into parallel jobs is, uh, very far from trivial.

325
00:27:18,560 --> 00:27:25,560
You mentioned that, uh, some of the things that you're doing are allowing you to load

326
00:27:25,560 --> 00:27:29,200
your strings onto the GPU.

327
00:27:29,200 --> 00:27:40,160
Are you able to utilize the GPU for kind of, uh, heavy, heavy kind of NLP types of algorithms?

328
00:27:40,160 --> 00:27:45,160
I guess for a lot of those, you're kind of numericalizing the textual data anyway, but,

329
00:27:45,160 --> 00:27:48,480
are there any limitations there, one way or the other?

330
00:27:48,480 --> 00:27:54,400
There are some limitations, and we do want to do ML, uh, NLP, um, we're not quite there

331
00:27:54,400 --> 00:27:55,400
yet.

332
00:27:55,400 --> 00:28:01,520
So, um, we're on a, uh, implementation of word to veck, um, in terms of, uh, preparing

333
00:28:01,520 --> 00:28:08,720
or understanding your data, we have a lot of, um, ordinary string functions like, uh, token

334
00:28:08,720 --> 00:28:17,440
izers, um, we have a regular expressions engine, so you can, um, search for regular expressions

335
00:28:17,440 --> 00:28:22,120
on strings and substrings and use that to create variables.

336
00:28:22,120 --> 00:28:26,160
I think probably closer towards the end of the year would be the soonest I expect us

337
00:28:26,160 --> 00:28:29,840
that we're going to deliver that, but it's certainly something that we're interested

338
00:28:29,840 --> 00:28:33,000
in and are currently working on.

339
00:28:33,000 --> 00:28:40,120
And in just in terms of like the, the less, the kind of the pre-processing to NLP is,

340
00:28:40,120 --> 00:28:41,440
is where we started.

341
00:28:41,440 --> 00:28:47,720
Um, and so, uh, now that we have, you know, released and continue to iterate on our, our

342
00:28:47,720 --> 00:28:54,120
string manipulations package, that's going to lay the foundation for, um, NLP practitioners

343
00:28:54,120 --> 00:28:58,840
to be able to work in a way that they're used to and have algorithms like word to veck,

344
00:28:58,840 --> 00:29:06,560
um, or LDA or other things like that, um, open and available to them.

345
00:29:06,560 --> 00:29:16,640
One of the things I'm curious about is in the Clomel library are all of these algorithms

346
00:29:16,640 --> 00:29:21,440
and everything you're doing with that library only dependent on the GPU.

347
00:29:21,440 --> 00:29:29,120
In other words, are you doing all of the compute in the GPU, uh, or are you using the, the

348
00:29:29,120 --> 00:29:35,920
CPU, you know, as needed or, or where appropriate and, you know, and I'm wondering more, you

349
00:29:35,920 --> 00:29:40,440
know, more broadly is it kind of an all or nothing kind of thing or is, is the focus

350
00:29:40,440 --> 00:29:46,200
really on doing a given operation in the best place for that operation?

351
00:29:46,200 --> 00:29:53,960
I think it's doing, uh, a given operation, um, where it's best suited, but, um, we're

352
00:29:53,960 --> 00:30:00,520
really just looking at things that are, are suited to the GPU, uh, that's actually,

353
00:30:00,520 --> 00:30:08,280
so doing everything where it best, where it's best suited, but everything is best suited

354
00:30:08,280 --> 00:30:16,040
in the GPU.

355
00:30:16,040 --> 00:30:21,600
Our work has been focused on, um, you know, once the, especially on the Clomel side, once

356
00:30:21,600 --> 00:30:25,120
the data is in GPU memory, we don't want to move it around.

357
00:30:25,120 --> 00:30:31,600
Um, that's kind of one of the big advantages that we have by doing int and data science.

358
00:30:31,600 --> 00:30:34,120
We're dramatically cutting down on these read rights.

359
00:30:34,120 --> 00:30:40,120
So like the data will come in and it'll sit as a coup df data frame and I can immediately

360
00:30:40,120 --> 00:30:45,200
pass it into my algorithm and it all stays in one place.

361
00:30:45,200 --> 00:30:51,960
So, um, we're able to cut down some of the overhead by thinking really hard about, um,

362
00:30:51,960 --> 00:30:57,560
reducing, uh, the copies that happen in the course of doing this and all of those.

363
00:30:57,560 --> 00:31:04,640
I imagine that there's a bit of a dichotomy or decision point around, you know, do you

364
00:31:04,640 --> 00:31:11,600
kind of optimize around these NN workflows and then everyone who, you know, wants to do

365
00:31:11,600 --> 00:31:16,840
anything that utilizes the GPU and takes advantage of what you're doing needs to wait

366
00:31:16,840 --> 00:31:23,320
for you to build their algorithm, you know, or, you know, is there some way, you know,

367
00:31:23,320 --> 00:31:28,800
if I want to do something that, you know, that QML offers an optimized version of, but

368
00:31:28,800 --> 00:31:34,760
I also need to do stuff that, uh, for which there's not a QML optimized version, do I

369
00:31:34,760 --> 00:31:40,800
need to load, you know, do the five minute pen to load in the RAM and the load into the

370
00:31:40,800 --> 00:31:48,360
GPU and kind of go back and forth each time or, you know, is there some kind of scenario

371
00:31:48,360 --> 00:31:57,600
where I can load into the GPU and keep the data there, but also do CPU based operations

372
00:31:57,600 --> 00:31:58,600
against it.

373
00:31:58,600 --> 00:32:02,200
I don't even know if technically that makes any sense, you know, or is feasible.

374
00:32:02,200 --> 00:32:07,000
We have a bunch of different formats you can export data from QDF.

375
00:32:07,000 --> 00:32:12,280
So if you had an algorithm that we haven't built at, um, and I'd say anybody is welcome

376
00:32:12,280 --> 00:32:13,280
to join in.

377
00:32:13,280 --> 00:32:14,480
This is an open source project.

378
00:32:14,480 --> 00:32:18,960
We'd love to have, uh, any help we can get building these algorithms out, but if you have

379
00:32:18,960 --> 00:32:23,120
something that you need to do for work today and you're like, could you have sounds like,

380
00:32:23,120 --> 00:32:28,400
like a great way to speed up the pandas part of my workflow, but I am going to do a bunch

381
00:32:28,400 --> 00:32:32,920
of algorithms, for example, um, I want to do a bunch of Bayesian stuff.

382
00:32:32,920 --> 00:32:40,520
Um, we can export data from the GPU data frame into a pandas data frame.

383
00:32:40,520 --> 00:32:46,560
If that's what we'd like to work with it, um, into an empty array, um, we support the

384
00:32:46,560 --> 00:32:54,360
arrow data format, um, and we also just introduced support for DL pack, which plays nicely with

385
00:32:54,360 --> 00:32:57,800
a handful of the deep learning packages like PyTorch.

386
00:32:57,800 --> 00:33:04,440
So, um, while we're working as hard as we can to add more algorithms to it for the user

387
00:33:04,440 --> 00:33:11,400
community, um, you can, uh, pick and choose what's, what's most useful to you at this time.

388
00:33:11,400 --> 00:33:16,240
I'd also like to add that, um, QML can, uh, take in Numpy arrays.

389
00:33:16,240 --> 00:33:22,480
So these packages were designs to be closely used together, but, um, we know that that's,

390
00:33:22,480 --> 00:33:28,480
uh, not going to cover all users and, um, that's not, uh, necessarily a requirement.

391
00:33:28,480 --> 00:33:34,680
The other piece of this is QGraph, and I gather that's, uh, a newer, kind of more emerging

392
00:33:34,680 --> 00:33:37,000
part of the rapid ecosystem.

393
00:33:37,000 --> 00:33:39,000
Uh, yeah it is.

394
00:33:39,000 --> 00:33:43,720
Um, I don't have the deepest knowledge of, uh, QGraph on the team.

395
00:33:43,720 --> 00:33:49,840
Uh, I'm not, uh, really a, uh, graph guy, but, um, I know that their benchmarks have been

396
00:33:49,840 --> 00:33:55,520
fabulous and we're hoping to make more graph algorithms available to people that heavily

397
00:33:55,520 --> 00:33:58,840
rely on graph theory in their, uh, day to day work.

398
00:33:58,840 --> 00:34:06,080
And then maybe switching gears a little bit beyond the work that's happening, uh, in

399
00:34:06,080 --> 00:34:07,080
rapids.

400
00:34:07,080 --> 00:34:15,320
Uh, one of the things that was mentioned at this recent GTC was some, uh, announcements,

401
00:34:15,320 --> 00:34:21,400
uh, and partnerships around, uh, creating a reference architecture for data science

402
00:34:21,400 --> 00:34:22,400
workstations.

403
00:34:22,400 --> 00:34:24,080
What can you tell us about that initiative?

404
00:34:24,080 --> 00:34:25,080
Yeah.

405
00:34:25,080 --> 00:34:32,040
The reference architecture for, um, data science workstations is, uh, very new, but, um,

406
00:34:32,040 --> 00:34:37,520
what I think is exciting about it is that, uh, for, for people that are able to get an

407
00:34:37,520 --> 00:34:42,720
Nvidia data science workstation, um, we are going to have the, uh, the software that's

408
00:34:42,720 --> 00:34:49,200
going to be heavily based on rapids laid out, um, ideally so once you get the data science

409
00:34:49,200 --> 00:34:55,040
workstation, um, it's loaded with the software that you need to have that, that reference

410
00:34:55,040 --> 00:35:01,760
architecture will also refer to what we think the best, uh, hardware lay out is, um, and

411
00:35:01,760 --> 00:35:09,040
we're just trying to, in another way, make GPU data science more, um, accessible to people.

412
00:35:09,040 --> 00:35:14,760
Um, sometimes, uh, and it's a common project in data science circles to try to build your

413
00:35:14,760 --> 00:35:16,240
own deep learning rig.

414
00:35:16,240 --> 00:35:20,240
I think that's a great exercise, but it's not for everybody.

415
00:35:20,240 --> 00:35:25,560
And I've been in some very, you know, um, serious corporate environments where IT is not

416
00:35:25,560 --> 00:35:29,400
going to let you bring in the computer that you built to start working on their proprietary

417
00:35:29,400 --> 00:35:30,400
data.

418
00:35:30,400 --> 00:35:31,400
Right.

419
00:35:31,400 --> 00:35:32,400
Right.

420
00:35:32,400 --> 00:35:39,920
And the, the data science workstation initiative is, um, really about making it as easy

421
00:35:39,920 --> 00:35:45,040
as possible for an organization that wants to dive into GPU data science to get started.

422
00:35:45,040 --> 00:35:46,040
Cool.

423
00:35:46,040 --> 00:35:53,680
Any parting thoughts from you on rapids or a QML or advice for folks who haven't really,

424
00:35:53,680 --> 00:35:58,320
you know, been exposed to, uh, what Nvidia is doing on the software side that want to

425
00:35:58,320 --> 00:36:00,480
explore more?

426
00:36:00,480 --> 00:36:03,840
I would just really encourage, uh, everybody to take a look.

427
00:36:03,840 --> 00:36:12,360
If you go to, um, rapids.ai, that's, uh, kind of a portal, uh, landing page that will

428
00:36:12,360 --> 00:36:16,840
do somebody to everything that, um, I haven't covered here as well, some of the things that

429
00:36:16,840 --> 00:36:21,000
I have, um, links to documentation, links to GitHub.

430
00:36:21,000 --> 00:36:28,640
We've begun, uh, Google group, uh, we encourage everybody that, uh, touches rapids and find

431
00:36:28,640 --> 00:36:34,280
something that they don't like or that doesn't work to file a ticket.

432
00:36:34,280 --> 00:36:39,120
You can see, um, our roadmaps and our current work on GitHub.

433
00:36:39,120 --> 00:36:44,040
And we really want, um, the community to be involved like, uh, you know, as I think about

434
00:36:44,040 --> 00:36:49,840
the machine learning algorithms that I'm going to road map next for the team to develop

435
00:36:49,840 --> 00:36:55,120
on, a lot of that has been informed by customer and community feedback.

436
00:36:55,120 --> 00:36:59,360
And it's going to continue to be informed by customer and community feedback.

437
00:36:59,360 --> 00:37:06,680
And so, um, I would just, you know, uh, ask anybody that, uh, is interested in taking

438
00:37:06,680 --> 00:37:11,760
a look, um, you know, uh, please try to get involved with this because that's, that's

439
00:37:11,760 --> 00:37:14,200
really what's going to measure the success of our project.

440
00:37:14,200 --> 00:37:19,560
Uh, there's a real open source project and, um, we've done a great job of building community

441
00:37:19,560 --> 00:37:20,560
so far.

442
00:37:20,560 --> 00:37:26,640
We've got lots of stars and forks, but we want to see more of those and, uh, we're always

443
00:37:26,640 --> 00:37:29,080
happy to see issues opened on GitHub.

444
00:37:29,080 --> 00:37:30,080
Awesome.

445
00:37:30,080 --> 00:37:34,720
Well, Paul, thanks for taking the time to share with us what you're up to.

446
00:37:34,720 --> 00:37:37,720
Well, uh, thanks for having me.

447
00:37:37,720 --> 00:37:43,520
All right, everyone, that's our show for today.

448
00:37:43,520 --> 00:37:50,040
For more information on any of the shows in our GTC 2019 series, visit twimmaleye.com

449
00:37:50,040 --> 00:37:52,040
slash GTC19.

450
00:37:52,040 --> 00:37:55,760
Thanks again to Dell for sponsoring this series.

451
00:37:55,760 --> 00:38:00,520
Be sure to check them out at delemc.com slash precision.

452
00:38:00,520 --> 00:38:04,040
As always, thanks so much for listening and catch you next time.

