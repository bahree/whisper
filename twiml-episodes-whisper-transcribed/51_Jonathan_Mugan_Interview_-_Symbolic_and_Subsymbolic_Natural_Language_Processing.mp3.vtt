WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.360
I'm your host Sam Charrington.

00:23.360 --> 00:28.160
This past week, I spent some time in San Francisco at the Artificial Intelligence Conference

00:28.160 --> 00:30.800
by O'Reilly and Intel Nirvana.

00:30.800 --> 00:36.560
I had a ton of fun and got a bunch of great interviews from some amazing people doing

00:36.560 --> 00:39.360
awesome work in ML and AI.

00:39.360 --> 00:45.440
I got to talk to folks like Gunner Carlson of IAZD and Stanford who's applying topological

00:45.440 --> 00:51.440
models to machine learning like Ian Stoica of UC Berkeley whose RISE lab is building

00:51.440 --> 00:57.280
Ray, a distributed computing platform for reinforcement learning, and like Mo Patel and

00:57.280 --> 01:02.640
Laura Furlick of Think Big Analytics who shared a bunch of great use case stories with me.

01:02.640 --> 01:06.800
I'm super excited about my interviews from the conference and I'm looking forward to

01:06.800 --> 01:08.240
sharing them with you.

01:08.240 --> 01:13.520
Make sure you check back with us on October 9th to catch the full series.

01:13.520 --> 01:17.600
In the meantime, I've got a great interview for this week.

01:17.600 --> 01:21.840
Like last week's interview with Bruno Gunn's office on Word to Vic and Friends,

01:21.840 --> 01:27.520
this week's interview was also recorded at the last O'Reilly AI conference back in New

01:27.520 --> 01:29.880
York in June.

01:29.880 --> 01:34.640
Also like last week's show, this week's is focused on natural language processing and

01:34.640 --> 01:36.400
I think you'll enjoy it.

01:36.400 --> 01:41.920
I'm joined by Jonathan Mugen, co-founder and CEO of Deep Grammar, a company that's building

01:41.920 --> 01:47.320
a grammar checker using deep learning and what they call deep symbolic processing.

01:47.320 --> 01:51.600
This interview is a great compliment to my conversation with Bruno and we cover a variety

01:51.600 --> 01:57.400
of topics from both subsimbolic and symbolic schools of NLP, such as attention mechanisms

01:57.400 --> 02:04.040
like sequence to sequence and ontological approaches like wordnet, syn sets, frame net and sumo.

02:04.040 --> 02:06.720
I'm looking forward to your feedback on this show.

02:06.720 --> 02:12.880
Jump over to the show notes at twimmalei.com slash talk slash 49 to let me know what you liked

02:12.880 --> 02:13.880
and learned.

02:13.880 --> 02:20.160
Finally, before we dive into the show, the details for the upcoming Twimo Online Meetup

02:20.160 --> 02:22.240
have been set.

02:22.240 --> 02:29.480
On October 18th at 3pm, Pacific Time, we'll discuss the paper visual attribute transfer

02:29.480 --> 02:34.880
using deep image analogy by Jing Liao and others from Microsoft Research.

02:34.880 --> 02:40.400
The discussion will be led by Duncan Stothers, for anyone who's missed the last two meetups

02:40.400 --> 02:46.160
or for those who haven't yet joined the group, please visit twimmalei.com slash meetup for

02:46.160 --> 02:47.440
more information.

02:47.440 --> 02:52.920
There, you'll find video recaps of the last two meetups along with the link to the paper

02:52.920 --> 02:55.400
we'll be reviewing next month.

02:55.400 --> 02:59.840
If you'd like to present your favorite paper, we'd love to have you do it.

02:59.840 --> 03:05.920
Just shoot us an email at teamattwimmalei.com to get the ball rolling.

03:05.920 --> 03:08.800
And now on to the show.

03:08.800 --> 03:19.680
All right, everyone, I am here at the O'Reilly AI conference.

03:19.680 --> 03:25.680
And I am with Jonathan Mugan, who's the founder and CEO of Deep Grammar, Jonathan, welcome

03:25.680 --> 03:26.680
to the podcast.

03:26.680 --> 03:28.080
Oh, thanks for having me.

03:28.080 --> 03:30.400
I'm excited to get into this conversation.

03:30.400 --> 03:34.720
So you are speaking later today at the conference.

03:34.720 --> 03:37.720
And I'm looking forward to having you walk us through your presentation.

03:37.720 --> 03:41.200
But why don't we start by having you tell us a little bit about your background and

03:41.200 --> 03:42.480
how you got into AI?

03:42.480 --> 03:43.800
Yeah, sure.

03:43.800 --> 03:47.800
So I started off in psychology.

03:47.800 --> 03:53.120
Went and got my undergraduate in psychology and I wanted to understand the human mind.

03:53.120 --> 03:57.800
And as my advisor used to say, the interesting parts weren't scientific and the scientific

03:57.800 --> 04:00.200
parts weren't quite interesting.

04:00.200 --> 04:02.400
And I love that.

04:02.400 --> 04:07.640
Yeah, we didn't quite have a firm grasp on concrete principles that we could

04:07.640 --> 04:10.560
that we could use to really understand what was going on.

04:10.560 --> 04:15.920
So I became a little disillusioned and got my MBA and company called Pricewaterhouse Coopers

04:15.920 --> 04:20.360
trained me up in computer programming and set me off in a consulting world.

04:20.360 --> 04:21.360
Join the ranks.

04:21.360 --> 04:22.360
I did.

04:22.360 --> 04:23.360
Yes.

04:23.360 --> 04:26.720
And then as I was, I was programmed and I was like, you know, you have to tell a computer

04:26.720 --> 04:28.160
exactly what to do.

04:28.160 --> 04:33.360
This might be the kind of, the kind of rigor that we need if we're going to, we're going

04:33.360 --> 04:34.360
to do psychology.

04:34.360 --> 04:35.360
Uh-huh.

04:35.360 --> 04:39.520
And then of course, AI is a mix of psychology and computer science.

04:39.520 --> 04:40.720
So it seemed natural.

04:40.720 --> 04:45.000
So I decided to go back and get my PhD, but my undergraduate was a bachelor.

04:45.000 --> 04:46.480
And when did you do that?

04:46.480 --> 04:50.680
Well, I went back in 2003.

04:50.680 --> 04:55.120
I went back to get my masters because my undergraduate was in psychology.

04:55.120 --> 04:58.320
So I couldn't get into a PhD program straight away.

04:58.320 --> 05:01.360
So I had to take calculus and all that kind of stuff and all the kids.

05:01.360 --> 05:02.360
Yeah.

05:02.360 --> 05:07.120
The masters at UT Dallas and then got into the PhD program at UT Austin and started

05:07.120 --> 05:08.320
working with Ben Kipers.

05:08.320 --> 05:09.320
Okay.

05:09.320 --> 05:10.920
And so what was the focus of your research there?

05:10.920 --> 05:11.920
Yeah.

05:11.920 --> 05:13.160
My focus was on developmental robotics.

05:13.160 --> 05:17.760
So how can you get a robot to learn about the world in the same way a child does?

05:17.760 --> 05:24.280
And so the idea is the robot just wakes up or is born and has some knowledge built in,

05:24.280 --> 05:25.280
but not much.

05:25.280 --> 05:29.960
And it wants to build it all up from the beginning and the robot pushes objects around

05:29.960 --> 05:37.560
and learns relationships between its hand and the object and it learns how to form actions

05:37.560 --> 05:42.400
and how to build up perceptions like, oh, my hand is to the left of the block.

05:42.400 --> 05:46.880
That's actually significant because that determines when I can hit it to the right.

05:46.880 --> 05:49.120
And so it built it up that way.

05:49.120 --> 05:55.080
And then I finished that up and graduated in 2010.

05:55.080 --> 06:02.040
And at that time, AI was not hot like it is now and it's amazing to change.

06:02.040 --> 06:08.920
And so I got a postdoc at Carnegie Mellon and I studied under Norman today and at the

06:08.920 --> 06:14.120
intersection between kind of computer science and human computer interaction.

06:14.120 --> 06:19.480
So we studied if you have this location device that gives you your location or excuse me

06:19.480 --> 06:25.240
that broadcast your location to family and friends, when exactly do you want to share your

06:25.240 --> 06:26.240
location?

06:26.240 --> 06:30.080
So at the time, this was somewhat novel being able to share your location.

06:30.080 --> 06:31.080
Right.

06:31.080 --> 06:34.400
And there were a lot of privacy issues around it then, like, of course, they're still

06:34.400 --> 06:35.400
hard now.

06:35.400 --> 06:40.840
And so the device would learn through interaction with you when you want to share based on who's

06:40.840 --> 06:44.320
asking and where you are in time of day.

06:44.320 --> 06:45.320
Interesting.

06:45.320 --> 06:46.320
Interesting.

06:46.320 --> 06:50.640
So my wife actually used a Texas gal and when I went to Pittsburgh, she didn't come with

06:50.640 --> 06:51.640
me.

06:51.640 --> 06:54.520
I flew back home every weekend.

06:54.520 --> 06:55.520
Oh, wow.

06:55.520 --> 06:56.520
Yeah.

06:56.520 --> 06:57.520
It was quite a deal.

06:57.520 --> 07:00.480
And so I flew back home every weekend and eventually she says, you know, we're not going

07:00.480 --> 07:01.480
to Pittsburgh.

07:01.480 --> 07:11.120
So I got a job in Austin at a small company called 21CT where we do defense contracting

07:11.120 --> 07:13.080
work for the Department of Defense.

07:13.080 --> 07:19.600
So data mining and at that job, I pushed into natural language processing because one

07:19.600 --> 07:26.800
problem I found with the development of robotics is it was really hard to get funding

07:26.800 --> 07:30.120
unless you're at a university because it's so far off.

07:30.120 --> 07:35.120
You know, we have robots and factories all over the place, but we don't want them staring

07:35.120 --> 07:37.640
at their navel and wondering about life.

07:37.640 --> 07:43.860
And so most of the funding, most of the push is towards robots that can do actual concrete

07:43.860 --> 07:46.040
things right now, right?

07:46.040 --> 07:49.800
And I'm more interested in the fundamental concepts below that.

07:49.800 --> 07:54.640
The fundamental concepts that enable a child to just grab the world and understand it.

07:54.640 --> 08:00.400
And so I saw that language might be a good kind of in between.

08:00.400 --> 08:02.480
So language is very important right now.

08:02.480 --> 08:05.320
It's very useful chat bots are a thing.

08:05.320 --> 08:09.000
Language interfaces are important, computers have to read tons of documents.

08:09.000 --> 08:15.560
I thought, okay, well, language might be a way that I can both feed my family and study

08:15.560 --> 08:17.200
the stuff I care about.

08:17.200 --> 08:22.080
And then of course, along the way, I found a deep grammar co-founded with Christian Storm.

08:22.080 --> 08:26.160
But that ties into my talk today because my talk today is about how can we go from natural

08:26.160 --> 08:32.120
language processing down to these fundamental concepts into real understanding.

08:32.120 --> 08:40.560
Because right now, natural language processing is kind of sad because we're just at the surface.

08:40.560 --> 08:41.560
We treat these tokens.

08:41.560 --> 08:43.320
I was amazed when I first got in the field.

08:43.320 --> 08:47.640
We treat these words, we tokenize in the words and maybe we parse, but we just have

08:47.640 --> 08:49.120
this string of tokens.

08:49.120 --> 08:51.920
And then we do stuff with the tokens.

08:51.920 --> 08:53.800
The computer has no idea what these tokens mean.

08:53.800 --> 08:56.360
We just look for patterns in the tokens.

08:56.360 --> 09:00.960
And so in my talk, I start off with a TF IDF, which you take a document and convert it

09:00.960 --> 09:02.400
into a vector.

09:02.400 --> 09:06.840
If you're vocabulary is 50,000 words, it's a 50,000 long vector.

09:06.840 --> 09:09.920
And you lose, you lose the word ordering.

09:09.920 --> 09:12.920
So man by its dog is the same as dog by its man.

09:12.920 --> 09:16.520
And TF IDF is term-frequency inverse document.

09:16.520 --> 09:17.520
That's right.

09:17.520 --> 09:21.680
So the term frequency is, you know, if ardvark shows up twice, then you get a two in the

09:21.680 --> 09:23.040
ardvark's one.

09:23.040 --> 09:27.520
And then you scale that with the inverse document frequency by how often ardvark shows

09:27.520 --> 09:29.040
up in your corpus.

09:29.040 --> 09:33.920
So the less frequently it shows up, the more important it is in the context of the document

09:33.920 --> 09:34.920
is that's the idea.

09:34.920 --> 09:35.920
That's right, that's right.

09:35.920 --> 09:41.720
And then that way, that vector helps discriminate that document better because it has that

09:41.720 --> 09:42.720
scaling.

09:42.720 --> 09:49.040
And so it's interesting that you start off by talking about, you know, the fact that

09:49.040 --> 09:55.960
NLP is not, you know, it's not based on a lot of inherent structure because previous

09:55.960 --> 09:59.400
conversations I've had with folks, I might, the general kind of understanding I've come

09:59.400 --> 10:04.880
into is that that's where, you know, that lack of structure, meaning taking a statistical

10:04.880 --> 10:10.280
approach as opposed to a linguistic approach, is been the source of all of the advancements

10:10.280 --> 10:15.040
in, or much of the advancements in NLP over the last few years.

10:15.040 --> 10:16.800
Do you disagree with that generally or?

10:16.800 --> 10:20.040
Well, it's definitely true that we've been able to do a lot of cool stuff.

10:20.040 --> 10:25.000
And so in my talk, I talk about two paths, the symbolic path, and the sub symbolic path,

10:25.000 --> 10:27.400
which is the deep learning stuff that everybody's doing now.

10:27.400 --> 10:28.400
Okay.

10:28.400 --> 10:33.240
And yeah, with deep learning, we're able to generalize across tokens.

10:33.240 --> 10:38.760
So one problem we had before, if you said, I got into my car and went to the store versus

10:38.760 --> 10:43.560
I got into my truck and went to the supermarket, those looked like very different sentences

10:43.560 --> 10:45.080
in TF IDF.

10:45.080 --> 10:49.720
And you had to manually go in and say truck and car are pretty similar.

10:49.720 --> 10:50.720
Yeah.

10:50.720 --> 10:53.200
And store and supermarket are pretty similar.

10:53.200 --> 10:58.360
And you can do that for a few things, but you just can't think of all of these possibilities.

10:58.360 --> 11:01.400
And deep learning is really great from that, the word to VEC.

11:01.400 --> 11:05.920
So everything's a vector, and it turns out that, of course, car and vehicle are going

11:05.920 --> 11:08.880
to be very similar in car and truck and supermarket and store.

11:08.880 --> 11:13.960
And so if you, instead of due to TF IDF, you do like a, you can just even average the

11:13.960 --> 11:18.520
word vectors, or you can do an RNN where the last state is the meaning of sentence, you're

11:18.520 --> 11:24.440
able to really capture similarity across sentences in a way you can't do as well with symbolic

11:24.440 --> 11:25.760
methods.

11:25.760 --> 11:30.360
But you still don't have any understanding there.

11:30.360 --> 11:36.200
So when you do word to VEC, what you're doing is you're learning a vector for a word based

11:36.200 --> 11:40.280
on the words that typically go around it.

11:40.280 --> 11:45.000
And so the algorithm is, is you go through your whole corpus and for every word in the

11:45.000 --> 11:49.200
corpus, you go through one by one, you take the vector for that word, and you push the

11:49.200 --> 11:53.000
vectors for the other words closer to it, and you push all the vectors for the other words

11:53.000 --> 11:56.440
that aren't close to it away, and then you move to the next one, and you keep doing

11:56.440 --> 11:59.800
that over and over again until you've converged.

11:59.800 --> 12:04.440
And that's great, but it only captures what people say.

12:04.440 --> 12:08.560
So most of the knowledge that's needed to understand language is so obvious that we

12:08.560 --> 12:10.320
never mentioned it.

12:10.320 --> 12:13.800
And so that kind of stuff just doesn't show up in word vectors.

12:13.800 --> 12:19.040
And so even when you get this vector at the end, you still not clear what to do with it.

12:19.040 --> 12:23.040
And so you think about some of the biggest advances have been, or most exciting ones

12:23.040 --> 12:29.840
have been in machine translation, the machine still has no idea what it's just spitting

12:29.840 --> 12:30.840
out tokens.

12:30.840 --> 12:37.000
It encodes it with the encoding RNN, and then the decoder, it spits out the next word

12:37.000 --> 12:40.840
based on the previous state, the previous word, and then if it has attention, all of the

12:40.840 --> 12:46.240
previous encodings in the encoder, but it's just a softmax putting out tokens, it doesn't

12:46.240 --> 12:53.200
have any understanding of what it's doing, which is in some degree why it's so applicable

12:53.200 --> 12:57.400
in so many different domains, you can create a parse tree with it, you can even encode

12:57.400 --> 13:03.280
a picture into a vector using using a CNN, and then run the decoder, and that's how you

13:03.280 --> 13:04.280
get this captioning work.

13:04.280 --> 13:09.160
That's really exciting, but there's still no understanding there.

13:09.160 --> 13:14.840
And so you end up with this vector, so now we're on the sub symbolic path, but what can

13:14.840 --> 13:15.840
you do?

13:15.840 --> 13:19.560
And so the next thing that people started doing was, well, so attention, what you're doing

13:19.560 --> 13:24.400
when they added attention to the encoder decoder method, you're, when you're about to generate

13:24.400 --> 13:29.440
a word in your translation, you're looking at all of the previous words in the sentence,

13:29.440 --> 13:35.840
or they're encoded representation, the hidden state representation of the sequence.

13:35.840 --> 13:39.600
And so what it's doing is it's looking at facts about the world to figure out which ones

13:39.600 --> 13:42.160
are relevant for generating the next word.

13:42.160 --> 13:47.040
And so what people started doing was they said, well, what if I just feed it in a story?

13:47.040 --> 13:52.080
And so I can feed it in a story where like Tom went to the store, Tom came home, Tom

13:52.080 --> 13:58.800
picked up a jar, Tom went to the airport, and now the question is, where is the jar?

13:58.800 --> 14:02.800
And if you feed it enough of these stories, it's pretty amazing.

14:02.800 --> 14:08.280
The computer can answer, it's at the airport, presuming that you never put down this jar

14:08.280 --> 14:11.760
that's just carry with you for your life.

14:11.760 --> 14:16.760
And that's really cool, but you have to generate these stories automatically.

14:16.760 --> 14:20.800
And the reason you have to generate them automatically is because you need so many stories

14:20.800 --> 14:24.560
that it needs to be able to find these statistical patterns underneath.

14:24.560 --> 14:25.560
Okay.

14:25.560 --> 14:31.640
And this mechanism that's enabling this as attention, can we maybe double click on that

14:31.640 --> 14:37.040
to talk about how that's implemented to maybe get a...

14:37.040 --> 14:41.360
I've heard attention come up a bunch of times, but I haven't dug into it in any level

14:41.360 --> 14:46.680
of detail, and I'm wondering how that manifests itself in some of these deep networks and stuff

14:46.680 --> 14:47.680
like that.

14:47.680 --> 14:48.680
Yeah.

14:48.680 --> 14:54.200
So I'm thinking that Google just came out with this new tensor to tensor thing, which is,

14:54.200 --> 14:55.680
and I'm thinking of how they do attention.

14:55.680 --> 15:00.880
So you have like a set of keys and a query and keys and values.

15:00.880 --> 15:06.720
And what you're doing is for some query, you're looking at all of the keys to find the

15:06.720 --> 15:10.400
most similar key, and then you take that value.

15:10.400 --> 15:16.200
And the similarity between the query and the key is the weight that you use for the value.

15:16.200 --> 15:19.200
So you end up doing a weighted average of the values.

15:19.200 --> 15:21.200
Is that implemented in a neural network?

15:21.200 --> 15:22.200
Are we talking about...

15:22.200 --> 15:26.440
There's an external structure like a database or a key value store or something there.

15:26.440 --> 15:27.680
No, it's all neural network.

15:27.680 --> 15:32.000
And so when I say key and query and value, these are all vectors.

15:32.000 --> 15:33.000
Okay.

15:33.000 --> 15:34.000
Yeah.

15:34.000 --> 15:35.000
Got it.

15:35.000 --> 15:40.680
And so in the sequence-to-sequence model, what you're looking at is these, and maybe

15:40.680 --> 15:45.480
that the keys and values are one and the same, but you're looking at where your query is

15:45.480 --> 15:47.600
my current state when I'm trying to generate.

15:47.600 --> 15:52.760
So, you know, you could have, I went to the store, and then you're translating to Spanish

15:52.760 --> 15:57.080
and Yofuya Super Mercado, and when you're trying to, my great accent, when you're trying

15:57.080 --> 16:03.360
to generate a Super Mercado, you look back at, I went to the, and you went to, you look

16:03.360 --> 16:10.200
at those encoded representations along the way, and you take your state at Super Mercado

16:10.200 --> 16:15.600
or at the state before you generate Super Mercado, and you compare how similar those are.

16:15.600 --> 16:21.080
And then you take the weighted average of those values, and then that value comes in to

16:21.080 --> 16:26.320
where you would normally generate Super Mercado, and that value is taken into account.

16:26.320 --> 16:30.920
This is just another vector along with the vector for the previous state in your decoder

16:30.920 --> 16:34.320
and the vector for the previous word you generated.

16:34.320 --> 16:39.400
And then for each vector, you have another matrix, which you multiply it by, and then you

16:39.400 --> 16:42.640
add those things up, and you throw that in the softmax, and then that's your output.

16:42.640 --> 16:43.640
Okay.

16:43.640 --> 16:49.560
Yeah, and so neural network at each point is generally a, or very often, a multiplication

16:49.560 --> 16:55.720
of a matrix by a vector, and then you put some nonlinearity on the result.

16:55.720 --> 16:56.720
So.

16:56.720 --> 16:57.720
Okay.

16:57.720 --> 17:03.760
So attention basically is, is your storing kind of, you're kind of storing up these vectors

17:03.760 --> 17:10.280
and referencing them from the past, essentially, to be able, and including those in your,

17:10.280 --> 17:11.760
your end calculation.

17:11.760 --> 17:12.760
That's right.

17:12.760 --> 17:17.000
And so what they're doing in the story generation, or excuse me, in a story question answering,

17:17.000 --> 17:21.360
is they're encoding the parts of the story as vectors.

17:21.360 --> 17:25.320
And then when they want to ask, answer a question, they go back and look at the parts of

17:25.320 --> 17:29.280
the story and figure out which parts of the story are most relevant to answering that

17:29.280 --> 17:30.280
question.

17:30.280 --> 17:33.120
And then they do a little computation on top of that, and that's, that's where your answer

17:33.120 --> 17:34.120
comes from.

17:34.120 --> 17:35.120
Interesting.

17:35.120 --> 17:40.640
And are there limitations to the amount of memory that you're able to refer back to?

17:40.640 --> 17:45.200
Well, so generally, there's not limitations in the amount of memory, but you're generally

17:45.200 --> 17:46.760
taking a weighted average.

17:46.760 --> 17:47.760
Mm-hmm.

17:47.760 --> 17:52.120
And you do that because if you just take kind of a hard attention, then you can't do the

17:52.120 --> 17:53.920
back propagation as well.

17:53.920 --> 17:58.520
And so you take a weighted average, so it, things kind of get watered down a little bit.

17:58.520 --> 18:02.480
But I don't think that's a huge problem.

18:02.480 --> 18:07.520
More of the problem is it's just a very simple mechanism, and it can only do so much.

18:07.520 --> 18:11.400
And I think that's where you were going before I kind of interrupted you to push into this

18:11.400 --> 18:17.400
attention, is starting to approximate things that look and feel like meaning, but not,

18:17.400 --> 18:18.400
it's still not quite there.

18:18.400 --> 18:22.560
Yeah, you're going back over your previous experiences and say, oh, this one's relevant.

18:22.560 --> 18:23.560
Right.

18:23.560 --> 18:26.480
Yeah, which is cool.

18:26.480 --> 18:29.000
But the robot doesn't have any previous experiences.

18:29.000 --> 18:34.480
So these story generating, or these story question answering systems are really cool,

18:34.480 --> 18:37.200
but there's no built-in knowledge.

18:37.200 --> 18:38.200
Right.

18:38.200 --> 18:42.880
So when we answer questions about stories, we bring a whole lifetime of knowledge.

18:42.880 --> 18:44.600
And these all start from scratch.

18:44.600 --> 18:51.440
And what we need to do, I think the next step on the sub symbolic path is we need to have

18:51.440 --> 18:56.920
systems that interact in our world with the objects and relationships in our world.

18:56.920 --> 19:02.440
And so you can imagine like a little robot that can pick up objects and move them around.

19:02.440 --> 19:08.120
And then it knows what a bottle is, because a bottle is partially the hand fixture that

19:08.120 --> 19:09.360
it needs in order to pick it up.

19:09.360 --> 19:13.120
A bottle is partially that, if it knocks it off the table of bricks, a bottle is partially

19:13.120 --> 19:15.720
that that turns it to the side.

19:15.720 --> 19:16.720
Water comes out.

19:16.720 --> 19:19.080
All these things are part of the definition of a bottle.

19:19.080 --> 19:24.760
And so when it's pulling up memory, it's not just pulling up parts of a story.

19:24.760 --> 19:30.520
It's pulling up huge banks of things that it is experienced before.

19:30.520 --> 19:34.400
And then you can make inference from that that you wouldn't be able to otherwise.

19:34.400 --> 19:40.240
Now we don't quite know how to do these advanced inferences based on experience other than

19:40.240 --> 19:45.360
the kind of basic models we have now, which is like sequels of sequence and CNN and some

19:45.360 --> 19:46.360
other ones.

19:46.360 --> 19:50.280
And it's going to be exciting to see one of my, one of the things I really enjoy about

19:50.280 --> 19:55.120
the deep learning is every time a new configuration comes out or you know, a new one that goes

19:55.120 --> 19:56.120
into zoo.

19:56.120 --> 19:57.680
I'm like, oh cool, we're getting a little closer.

19:57.680 --> 19:58.680
Right.

19:58.680 --> 20:02.680
I envision in the brain, you know, there's just thousands upon thousands of different kinds

20:02.680 --> 20:08.200
of configurations and neurons and at least to some approximation, one of them might be

20:08.200 --> 20:09.200
a sequence of sequence.

20:09.200 --> 20:12.880
And another one might be a CNN, but there's, you know, hundreds more that we haven't discovered.

20:12.880 --> 20:13.880
Right.

20:13.880 --> 20:14.880
Right.

20:14.880 --> 20:17.760
It would be cool as we get better and better with each new one.

20:17.760 --> 20:23.840
So I think most of what we covered now, I mean, it sounds like a lead up to, you know,

20:23.840 --> 20:30.880
a specific area of research or interest that you have that kind of promises to help address

20:30.880 --> 20:31.880
this issue.

20:31.880 --> 20:37.040
Like where does it, where do we go from the sub symbolic, maybe another way to ask this

20:37.040 --> 20:42.920
is, is it your observation that a more symbolic approach is kind of the answer to the ills

20:42.920 --> 20:46.960
of the sub symbolic approach, or do you think the path forward is still sub symbolic,

20:46.960 --> 20:51.760
but extending it to incorporate more understanding?

20:51.760 --> 20:54.120
I don't know.

20:54.120 --> 20:55.120
Yeah.

20:55.120 --> 20:59.760
So in my talk, I cover both approaches as if they're separate approaches.

20:59.760 --> 21:05.520
And there's been surprisingly little overlap in the approaches.

21:05.520 --> 21:10.320
And what we've talked about the sub symbolic mostly, have we talked, have you, should we

21:10.320 --> 21:13.080
take a few minutes talking about the symbolic stuff and what's happening there?

21:13.080 --> 21:14.080
Sure.

21:14.080 --> 21:15.080
Sure.

21:15.080 --> 21:16.080
We can do that.

21:16.080 --> 21:19.560
So we can, yes, we were talking about a TIF IDF factor and that is, it throws out word

21:19.560 --> 21:21.400
order.

21:21.400 --> 21:22.440
But you can do a lot of stuff with it.

21:22.440 --> 21:25.080
You can say this document is similar to this other document.

21:25.080 --> 21:29.440
You can even throw it into machine learning classifier and do sentiment analysis or document

21:29.440 --> 21:31.360
classification.

21:31.360 --> 21:32.760
And that's pretty neat.

21:32.760 --> 21:34.760
And I mentioned sentiment analysis.

21:34.760 --> 21:41.800
So the next step in sentiment analysis, getting a little closer to actual meaning is a sentiment

21:41.800 --> 21:42.800
dictionary.

21:42.800 --> 21:43.800
Okay.

21:43.800 --> 21:48.720
So often you'll have a dictionary that says, okay, the word terrible has a negative sentiment.

21:48.720 --> 21:51.800
And the word good has a positive sentiment.

21:51.800 --> 21:55.680
And you have some simple mechanism that says, well, not terrible.

21:55.680 --> 21:58.400
You have the inverse, reverse to word.

21:58.400 --> 22:00.400
And that can get you pretty far.

22:00.400 --> 22:04.080
And but you're, for each symbol now, you're going into your dictionary and you're assigning

22:04.080 --> 22:05.680
some very simple meaning.

22:05.680 --> 22:10.320
So that's kind of the first step to assigning meaning or one, you could consider it one

22:10.320 --> 22:11.640
first step.

22:11.640 --> 22:15.160
But there's also a whole set of representations that people have built.

22:15.160 --> 22:19.520
And so when you build a representation, what you're doing is you're taking symbols and

22:19.520 --> 22:22.200
you're creating relationships just between symbols.

22:22.200 --> 22:26.640
And then presumably if you set up the symbol system, you can map what people say to the

22:26.640 --> 22:32.240
symbol and then you can map what the computer should do based on whatever symbol got lit

22:32.240 --> 22:33.240
up.

22:33.240 --> 22:37.440
So if you had like, let's say you're a tire company and you want to watch Twitter to

22:37.440 --> 22:41.680
see who you should try to sell tires to, you could mention anybody, you could have set

22:41.680 --> 22:49.000
up a symbol system where it says, okay, a car has tires, a truck has tires, you know, Toyota

22:49.000 --> 22:50.600
is a kind of car.

22:50.600 --> 22:54.160
And therefore anybody mentions a Toyota, it links in the tires.

22:54.160 --> 22:55.160
Okay.

22:55.160 --> 22:58.880
And then you can just, that helps you when you take a sentence, you say, given the sentence,

22:58.880 --> 23:02.280
can I find tires linked anywhere in there?

23:02.280 --> 23:03.920
And I don't mention tires explicitly.

23:03.920 --> 23:07.800
But of course, this is kind of brittle and there's been a lot of work in setting up these

23:07.800 --> 23:08.800
symbol systems.

23:08.800 --> 23:14.240
The most famous is probably wordnet, okay, where you have these set of synths, which is

23:14.240 --> 23:17.520
a synth set is a group of words that all mean the same thing.

23:17.520 --> 23:18.920
It's like a meaning.

23:18.920 --> 23:24.400
And so vehicle might be one synth set in that vehicle, you'd have, well, maybe car is

23:24.400 --> 23:25.400
one synth set.

23:25.400 --> 23:30.600
And then car, you'd have motor car, car, and you could have, you know, a car in all different

23:30.600 --> 23:33.160
languages, but it means car.

23:33.160 --> 23:37.880
And then you set up relationships between these things like car is a kind of vehicle.

23:37.880 --> 23:40.840
And then you would have sports car as a subset of that.

23:40.840 --> 23:42.760
And so wordnet is really popular and it's really good.

23:42.760 --> 23:47.200
It kind of gives a, it kind of gives a sense of a definition for most words.

23:47.200 --> 23:50.120
And you can also have a word being two different synths set.

23:50.120 --> 23:56.680
So bank would be in the synth set for river bank, but also a bank where you deposit money.

23:56.680 --> 23:57.680
Okay.

23:57.680 --> 24:00.960
That's one, another one is frame net.

24:00.960 --> 24:04.560
And so frame net builds up a little bigger situation.

24:04.560 --> 24:07.840
So wordnet is about individual words.

24:07.840 --> 24:09.160
Frame net is about situation.

24:09.160 --> 24:16.320
So one example is the frame commerce by, which means somebody buys something from someone

24:16.320 --> 24:17.320
else.

24:17.320 --> 24:23.840
And so that frame is triggered by some set of keywords, like bought, purchased, sold.

24:23.840 --> 24:27.360
And when that frame is triggered, what frame net does, or at least an implementation that

24:27.360 --> 24:30.160
uses frame net, goes into tries to find the roles.

24:30.160 --> 24:31.160
Who was the buyer?

24:31.160 --> 24:32.160
Could be Bob.

24:32.160 --> 24:33.560
Bob bought a car from Tom.

24:33.560 --> 24:34.560
Okay.

24:34.560 --> 24:39.640
Buyers, Bob, the seller is Tom and the thing purchases car.

24:39.640 --> 24:46.600
And so you've converted this sentence into a frame with roles and now that's machine understandable.

24:46.600 --> 24:47.600
Okay.

24:47.600 --> 24:51.520
And it's kind of nice because you move up from individual words into kind of meaning

24:51.520 --> 24:52.520
of situations.

24:52.520 --> 24:53.520
Yes.

24:53.520 --> 24:54.520
Yeah.

24:54.520 --> 24:56.400
But frame net doesn't go very deep.

24:56.400 --> 25:01.520
You know, if frame net doesn't say, you don't have like the fundamental things going on,

25:01.520 --> 25:05.000
like forces and, and well, there's a little bit of that.

25:05.000 --> 25:07.800
But you don't have the things that a child knows, a very young child.

25:07.800 --> 25:10.840
And it turns out in, in AI, that's the hardest part.

25:10.840 --> 25:15.080
You know, we started off thinking that chess was the pinnacle of intelligence.

25:15.080 --> 25:21.840
And now it turns out that picking up a bottle of water is really hard.

25:21.840 --> 25:23.680
And who would have thought?

25:23.680 --> 25:29.000
So all the things, you know, they say, all of the things a kid knows by three or four,

25:29.000 --> 25:32.560
if we could get those in their computer, that would just be amazing.

25:32.560 --> 25:35.640
And that's what I would really love to try to do.

25:35.640 --> 25:39.840
And so if we're going to build that with a symbol system, we have to go deeper.

25:39.840 --> 25:43.400
And so one symbol system that does go deeper is Sumo.

25:43.400 --> 25:49.120
And so what Sumo is is, is a full ontology, meaning it goes all the way down.

25:49.120 --> 25:52.560
And so you look at like cooking, what does the word cooking mean?

25:52.560 --> 25:58.320
Well cooking is, I don't remember the exact, the exact thing, but cooking is a process,

25:58.320 --> 26:03.400
which is a thing, which is an entity, it goes all the way, it takes it all the way down.

26:03.400 --> 26:10.560
And so, so that's really useful, but what we need to do now is figure out how we can get,

26:10.560 --> 26:13.080
how we can get things like Sumo tied in the word net.

26:13.080 --> 26:16.800
And there has been some, some linkages, Sumo already does tie in word net.

26:16.800 --> 26:21.120
But how we can get all these different representations together, because what we want to do next

26:21.120 --> 26:26.600
is build, if we're staying in a symbolic land, build a causal model of how the world works.

26:26.600 --> 26:30.440
And here at this conference, Josh Chenabong yesterday was talking about that.

26:30.440 --> 26:36.840
And so we need, you know, an entity needs to understand that when it pushes a table, all

26:36.840 --> 26:39.800
the things on top of the table are going to move.

26:39.800 --> 26:43.440
And if you try to put that in logic, it's hard.

26:43.440 --> 26:47.040
And you need like a model where you can just read it off the model.

26:47.040 --> 26:49.960
So in some sense, frame net is kind of like that.

26:49.960 --> 26:57.800
So if, if there's a frame where Bob sold a car to Tom, and then you ask, well, who has

26:57.800 --> 26:59.760
the car afterwards?

26:59.760 --> 27:00.760
It's Tom.

27:00.760 --> 27:04.200
You can just read it right off the frame, or you can have that associated with the frame.

27:04.200 --> 27:06.760
You can put that in with the frame.

27:06.760 --> 27:10.720
And so what we need to do is build deep causal models that go all the way down to these things

27:10.720 --> 27:16.280
called image schemas, or image schemas are the language independent concepts that we

27:16.280 --> 27:20.960
use to understand everything in our world. So like Lake often Johnson and, and these kind

27:20.960 --> 27:22.400
of guys, Manler.

27:22.400 --> 27:26.760
And so you put a, she's a psychologist and you put a developmental psychologist.

27:26.760 --> 27:30.040
And so like one is containment.

27:30.040 --> 27:34.480
So when you have a bottle of water, the water is contained in the bottle, which means

27:34.480 --> 27:37.400
that if you move the bottle, the water goes along with it.

27:37.400 --> 27:38.400
Another support.

27:38.400 --> 27:40.320
So the bottle is on the table.

27:40.320 --> 27:45.040
And so you need these concepts before you can understand language, because the language

27:45.040 --> 27:47.560
understanding is built on all this stuff.

27:47.560 --> 27:51.000
When we talk to each other, we never, we never say these things.

27:51.000 --> 27:52.000
Sure.

27:52.000 --> 27:57.040
So yeah, one of my, one joke I like to say is you can imagine a romance novel where there's

27:57.040 --> 28:01.360
a table in between two lovers, and the man pushes the table aside, and then the novel

28:01.360 --> 28:05.000
they would never say, and as he pushes the table aside, all the objects on the table mood

28:05.000 --> 28:06.000
because they were spushing down.

28:06.000 --> 28:07.000
Right.

28:07.000 --> 28:08.000
Like that's just not in there.

28:08.000 --> 28:11.680
And it was a sound of the light scraping across the floor, right?

28:11.680 --> 28:19.320
That produces gashes in the floor, and suddenly the objects were at a new location.

28:19.320 --> 28:21.320
We make a lot of assumptions when we talk.

28:21.320 --> 28:22.320
We do.

28:22.320 --> 28:24.440
I mean, if we didn't, we'd never get anything done.

28:24.440 --> 28:25.440
Right.

28:25.440 --> 28:26.440
Yeah.

28:26.440 --> 28:30.360
And so, so what we need to do is build up cause and models of the world onto which we

28:30.360 --> 28:32.400
can put these symbols that we define.

28:32.400 --> 28:38.720
I can't decide if it would be fun or extremely boring and tedious to run these models in

28:38.720 --> 28:41.720
reverse and generate that romance novel.

28:41.720 --> 28:45.720
That'd be the worst novel ever.

28:45.720 --> 28:48.520
That'd be awesome.

28:48.520 --> 28:52.880
It's like, you know, you have the, you have different editions of books, the big print,

28:52.880 --> 28:56.120
and then this is the computer edition.

28:56.120 --> 28:57.120
Yeah.

28:57.120 --> 28:58.120
Yeah.

28:58.120 --> 29:03.520
And so those are, those are basically the two paths and to get from where we are now

29:03.520 --> 29:07.400
in natural language processing, which is just working at the symbols either with the

29:07.400 --> 29:11.520
sum symbolic, where we're able to learn vectors for these individual things or the symbolic

29:11.520 --> 29:15.680
where we just write down what they are in the computer that can really understand because

29:15.680 --> 29:17.560
it understands the fundamentals.

29:17.560 --> 29:25.520
So it's hard to say where to go from here because one problem is you need to find a commercially

29:25.520 --> 29:31.520
viable need for the simplest possible common sense knowledge.

29:31.520 --> 29:40.840
So we all have chatbots everywhere now, but they require already way too much knowledge

29:40.840 --> 29:41.840
to be good.

29:41.840 --> 29:45.520
If you go out out of, if you stay within a particular domain and you basically just

29:45.520 --> 29:50.320
hard code everything, and you can, and you can have chatbots where it's learned using

29:50.320 --> 29:53.480
sequence sequence models, but that's just gibberish back and forth.

29:53.480 --> 29:54.840
It's no different from Eliza, really.

29:54.840 --> 29:59.520
If we have a chatbot that actually is general, if you ask it things off script and can answer

29:59.520 --> 30:02.880
your questions, we're going to need these fundamental concepts.

30:02.880 --> 30:10.560
In fact, one of my dreams is to build a chatbot for children that you get at age three or

30:10.560 --> 30:13.640
four, and it lives in your mom's phone.

30:13.640 --> 30:18.480
And it teaches you concepts about the world, and it's also your friend, and it learns

30:18.480 --> 30:19.800
about you.

30:19.800 --> 30:25.440
And the cool thing about being a teacher is that if it teaches you, then it knows what

30:25.440 --> 30:26.440
you know.

30:26.440 --> 30:29.600
So then it explains other things to you.

30:29.600 --> 30:34.040
It can explain it to you and things you understand in terms you already understand.

30:34.040 --> 30:37.640
And it can also make things interesting, because let's say it knows that your favorite animals

30:37.640 --> 30:38.640
are draft.

30:38.640 --> 30:42.240
And I can say, when it teaches you math, I can say, if you have six drafts, and you buy

30:42.240 --> 30:44.760
two more, how many do you have?

30:44.760 --> 30:48.680
And that's the kind of thing that engaged parents do.

30:48.680 --> 30:51.800
And it would be cool to do that in an app, and I have this dream that you put that in

30:51.800 --> 30:55.520
for children, and you have your developers feverishly working behind the scenes, making better

30:55.520 --> 31:00.160
and better and better technology, so that when a child gets older, the app just turns

31:00.160 --> 31:02.640
into the operating system for the child.

31:02.640 --> 31:07.400
And so the child now uses this app to interface with its whole world.

31:07.400 --> 31:11.520
And since the app has been with the child since the beginning, the app really knows the

31:11.520 --> 31:15.240
child, and so it can be the ultimate and customized.

31:15.240 --> 31:19.280
And when it's, you know, and then as an adult, when it's guiding me through how to fix

31:19.280 --> 31:23.800
my dishwasher, it knows that I know nothing, and it literally has to tell me, lefty Lucy

31:23.800 --> 31:29.720
Rady Tidy, remember, as opposed to going Wikipedia or on the web, you just have no idea.

31:29.720 --> 31:34.400
Yeah, and then even when you're old, you know, if you become, your faculty start to go,

31:34.400 --> 31:37.800
and if you're standing in the kitchen, you can't remember how to make coffee, you know,

31:37.800 --> 31:42.760
the app can then be in the cameras in the room and say, hey, you make coffee this time

31:42.760 --> 31:43.760
of day.

31:43.760 --> 31:44.760
The filters are in the cupboard over there.

31:44.760 --> 31:45.760
That's the first step.

31:45.760 --> 31:49.280
And it guides you through, and maybe we could stay independent longer.

31:49.280 --> 31:50.280
That's...

31:50.280 --> 31:51.280
I would have great vision for an app.

31:51.280 --> 31:52.880
That would be awesome, yeah.

31:52.880 --> 31:56.280
Now, how much of that is deep grammar trying to take on?

31:56.280 --> 31:57.280
None.

31:57.280 --> 31:59.120
So, yes.

31:59.120 --> 32:01.440
Deep grammar is trying to take on.

32:01.440 --> 32:03.880
When I write, I make a lot of really dumb mistakes.

32:03.880 --> 32:04.880
Okay.

32:04.880 --> 32:06.760
It's just the human in me, right?

32:06.760 --> 32:10.400
I think one thing in my hands just outputs something different.

32:10.400 --> 32:15.760
And I've always been amazed that grammar checkers couldn't capture that.

32:15.760 --> 32:18.760
And you know, spell checkers came along and they were amazing.

32:18.760 --> 32:19.760
They really...

32:19.760 --> 32:22.800
I don't know how many people around remember days with four spell checkers, but it was

32:22.800 --> 32:23.800
a huge advance.

32:23.800 --> 32:24.800
Yeah.

32:24.800 --> 32:27.120
And, you know, I always told my teachers, I'm not going to have to know how to spell.

32:27.120 --> 32:32.800
And it turns out, I was right about one thing, very few times was I right, but that

32:32.800 --> 32:33.800
I was right.

32:33.800 --> 32:37.680
So, the grammar checkers always, you know, they were in word for a long time and they

32:37.680 --> 32:42.080
just would miss obvious wrong stuff, and it really bugged me.

32:42.080 --> 32:44.560
And I always thought machine learning would be the way to go.

32:44.560 --> 32:49.880
And so, I started working with endgrams, which is sequences of tokens, this is a few years

32:49.880 --> 32:50.880
ago.

32:50.880 --> 32:55.720
And it turns out, if you think about it for five minutes, it turns out that, you know,

32:55.720 --> 33:00.240
for endgrams of like sequences of three or four, if you take the, you know, I went to

33:00.240 --> 33:03.360
the, that's like four words.

33:03.360 --> 33:05.880
And then you have a distribution over the next word.

33:05.880 --> 33:12.120
And so, if you write a word that is not in that distribution, or not, you know, doesn't

33:12.120 --> 33:17.440
have a lot of weight in that distribution like donkey, but it's similar to a word that

33:17.440 --> 33:21.440
should have high weight like store, although donkey and store are similar, then you've

33:21.440 --> 33:23.120
probably made a mistake.

33:23.120 --> 33:27.880
So, if it's a, I went to the store, right, that's a mistake.

33:27.880 --> 33:28.880
It should be obvious.

33:28.880 --> 33:32.160
Stored is very different, very similar to store, and store is going to have a very

33:32.160 --> 33:33.760
low probability.

33:33.760 --> 33:39.360
The problem with endgrams is you can't, it's that similarity problem talked about before,

33:39.360 --> 33:45.680
because I went to this, I went to the store is, you know, I went, I drove, I meandered,

33:45.680 --> 33:50.480
I walked, all those are very different to an endgram probability thing.

33:50.480 --> 33:56.280
And so, in order to train such a thing, you would have to have seen all these things.

33:56.280 --> 34:03.000
And you'd have to store the vocabulary size to the fourth to store all this probability.

34:03.000 --> 34:07.760
And so, when I started working in deep learning, and I said, oh, sequence of sequences, the

34:07.760 --> 34:08.760
way to go for this.

34:08.760 --> 34:13.600
You're in code this thing, and then you decode it, and you get the power of deep learning

34:13.600 --> 34:17.920
that power we talked about before, that similar words are going to have similar vectors.

34:17.920 --> 34:21.640
And similar sentences are going to have similar vectors to other similar sentences.

34:21.640 --> 34:24.560
You know, the first thing is, oh, okay, I got to write a patent on this, this is, this

34:24.560 --> 34:27.920
is going to be how we're going to do grammar checking, and that, that's how we got started.

34:27.920 --> 34:28.920
Yeah.

34:28.920 --> 34:33.040
And so, what we do is we encode it, and then we decode, and if the thing we decode is different

34:33.040 --> 34:37.520
from what you wrote, then there's a problem, especially if what you wrote is different

34:37.520 --> 34:41.320
than it's similar to something that would have high probability.

34:41.320 --> 34:42.320
Okay.

34:42.320 --> 34:45.800
And then we also sort of, how do you capture that similarity?

34:45.800 --> 34:50.280
We do, it's just typical, typical, you know, the easiest thing you can do is like, what

34:50.280 --> 34:54.720
Levinstein distance, which is edit distance on the letters, you can do that with similarity,

34:54.720 --> 34:58.560
but there's also a bunch of other little similarity things we do, okay, that we take advantage

34:58.560 --> 35:02.000
of a lot of the acquired knowledge over the years and grammar.

35:02.000 --> 35:06.040
So we, yeah, we have a kind of sophisticated similarity measure, and then in addition

35:06.040 --> 35:11.120
to sequence sequence, we throw the kitchen sink of deep learning at it, a bunch of different

35:11.120 --> 35:15.600
CNNs and stuff, and so we've got it pretty good now.

35:15.600 --> 35:21.320
So sometimes it still fails in a way that's disturbing, but if you make a mistake like

35:21.320 --> 35:27.400
the wrong version of there or 222 is really good at that, it can catch it better than anything.

35:27.400 --> 35:28.400
Oh, wow.

35:28.400 --> 35:31.640
So, there's a lot of different markets, so the biggest market, as you might imagine, is

35:31.640 --> 35:36.520
English as a second language, but, and we get people all the time emailing me, please

35:36.520 --> 35:39.240
get to sing going, please, please, please, you know.

35:39.240 --> 35:44.800
The English as a second language is particularly challenging, because sometimes when you're

35:44.800 --> 35:50.240
not familiar with language, which you write is so far from correct, that the machine just

35:50.240 --> 35:54.640
can't, it just doesn't know where to start, and so that's a particular challenge.

35:54.640 --> 35:59.040
And then sometimes there's even a bigger fundamental challenge that the whole sentence has

35:59.040 --> 36:01.040
to be rewritten.

36:01.040 --> 36:05.440
And the only way to do that is to understand what the person said, and we've been talking

36:05.440 --> 36:09.200
this whole whole interview about how computers just can't do that, right?

36:09.200 --> 36:15.560
So that's going to be a problem for a lot of, a lot of time to come, but we can finally,

36:15.560 --> 36:19.160
these dumb mistakes I look at them, I can't believe that the grammar tracker didn't catch

36:19.160 --> 36:20.160
that.

36:20.160 --> 36:21.160
Now it can catch those.

36:21.160 --> 36:22.840
So, so that's really exciting.

36:22.840 --> 36:31.160
Do you offer this as a service for folks, like I use a service for writing called Grammarly,

36:31.160 --> 36:37.400
which you may be familiar with, that does a decent job for some things, some aspects of

36:37.400 --> 36:43.360
their implementation are kind of bad, I don't, just they use it, the UI, user experience

36:43.360 --> 36:45.040
is kind of wonky.

36:45.040 --> 36:51.040
But I can imagine that as a go-to-market model, I can imagine more of a platform-ish approach

36:51.040 --> 36:55.960
where you're offering APIs to developers to build things around, how are you guys going

36:55.960 --> 36:56.960
at it?

36:56.960 --> 37:01.960
We are in the process of trying to decide where exactly we're going to focus because Grammarly

37:01.960 --> 37:07.040
is now really big, they've got a lot of smart people working, and it's going to be hard

37:07.040 --> 37:08.480
to go head-to-head with them.

37:08.480 --> 37:13.560
I think we've got some ideas that are really good, and I think we do some things better,

37:13.560 --> 37:16.960
but they're just hiring like crazy.

37:16.960 --> 37:22.640
You can start by plugging into any of the editing apps on the Mac, which they don't support,

37:22.640 --> 37:26.920
or I don't think that they plug into Google Docs or anything like that.

37:26.920 --> 37:31.000
So, there are some holes there, now, what kind of a note that gives you, that's another

37:31.000 --> 37:32.000
issue.

37:32.000 --> 37:33.000
And also, Grammarly is pretty expensive.

37:33.000 --> 37:36.960
It thinks like $10 a month, and there's a lot of people around the world who really need

37:36.960 --> 37:39.040
this, but $10 a month is a lot of money.

37:39.040 --> 37:43.240
So we can, if we have a service that's really good, better than things that used to be

37:43.240 --> 37:47.560
around before, but maybe we don't have all the bells and whistles at Grammarly, especially

37:47.560 --> 37:52.520
if we can fix those things that are really hard for a computer to catch.

37:52.520 --> 37:56.440
So Grammarly, looking at what they've done, it looks like they spend a lot of time implementing

37:56.440 --> 38:02.280
a lot of rules, like calmer rules, but we can catch the subtle things that just pure learning

38:02.280 --> 38:03.680
can find.

38:03.680 --> 38:06.680
And that's what a lot of people need, because when you're English as a second language,

38:06.680 --> 38:10.160
you don't have the ear that we do, the ear for the language.

38:10.160 --> 38:13.680
And the ESL market is really interesting.

38:13.680 --> 38:18.120
Language is a hobby of mine, and one of the apps that I've been using recently that I really

38:18.120 --> 38:20.320
enjoy is this app called Tandem.

38:20.320 --> 38:25.920
That basically allows you to, it's kind of a global language learning community, and there

38:25.920 --> 38:28.800
have been a bunch of these, but it's the best implemented by far.

38:28.800 --> 38:33.200
You basically go on this app, you tell it what language is your learning, and it'll match

38:33.200 --> 38:38.320
you with people, the people that speak those languages natively that are trying to learn

38:38.320 --> 38:40.480
your languages that you know.

38:40.480 --> 38:47.880
But you'll get in these conversations with folks, and depending on their level, I think

38:47.880 --> 38:52.600
the interesting conversations are when folks are beyond the, hey, I'm going to Google

38:52.600 --> 38:57.040
translate everything I want to say, because you know the failure mode, like you can spot

38:57.040 --> 38:59.680
those, you know, really quickly.

38:59.680 --> 39:04.200
But then there are folks that know enough English that they're just typing what they

39:04.200 --> 39:10.000
think is right, and sometimes it's a little hard to decipher, but most of the time you

39:10.000 --> 39:14.160
can kind of get what they're trying to say, they're just not saying it wrong.

39:14.160 --> 39:23.040
And if your stuff was plugged into this process, as like a kind of a side channel trainer

39:23.040 --> 39:28.800
or coach or something like that, I think it would, you know, the big challenge for language

39:28.800 --> 39:34.480
learners is like decreasing the cycle time of, you know, learning and iteration and accelerating

39:34.480 --> 39:35.480
the process.

39:35.480 --> 39:37.360
Something like that could be really interesting.

39:37.360 --> 39:38.360
Yeah.

39:38.360 --> 39:43.840
No, I hadn't thought of that as like a coach, you know, that you said this and maybe change

39:43.840 --> 39:44.840
it to this other thing.

39:44.840 --> 39:46.400
No, that's a good idea.

39:46.400 --> 39:50.920
Another place that's like video transcription is big now, and a lot of times you have to

39:50.920 --> 39:52.440
pay a human to do it.

39:52.440 --> 39:55.920
Well, it's done automatically, but then you need to pay a human to make sure it's done

39:55.920 --> 39:56.920
right.

39:56.920 --> 39:57.920
Right?

39:57.920 --> 40:00.800
Because you get this text out and sometimes it doesn't quite hear.

40:00.800 --> 40:03.600
And so that's very much like a grammar correction problem.

40:03.600 --> 40:04.600
So we could do that.

40:04.600 --> 40:08.960
Yeah, we're trying to say what exactly, what niche, you know, we should go make it cheap

40:08.960 --> 40:15.040
or make it an API, do transcription, maybe there's some publishers that have reached out

40:15.040 --> 40:19.200
to us, they say, look, we write, we send out all these books and we have to pay people

40:19.200 --> 40:21.280
to go in and read each one.

40:21.280 --> 40:26.440
And so if we use you guys, then we have to pay them, you know, we can have them do more

40:26.440 --> 40:31.240
books per person because they would have less, you know, less tedious stuff to do.

40:31.240 --> 40:32.760
You'd catch the idea of stuff.

40:32.760 --> 40:33.760
And so that's another option.

40:33.760 --> 40:37.880
So we're kind of standing at the crossroads right now, trying to figure out what we're

40:37.880 --> 40:39.040
going to do with it.

40:39.040 --> 40:44.920
Does the technology get into or give you the ability to address stylistic issues as

40:44.920 --> 40:47.960
opposed to correctness?

40:47.960 --> 40:55.400
It kind of does both at the same time, but it doesn't help you rewrite things.

40:55.400 --> 41:00.520
So it's basically going to help you write the way it was trained.

41:00.520 --> 41:06.280
So we started out training on Wikipedia, but then there was everything that it wanted

41:06.280 --> 41:08.760
to fix everything to be very Wikipedia.

41:08.760 --> 41:13.840
Well, you know, the thought that came to me was, you know, the artistic style transfer

41:13.840 --> 41:17.000
stuff where, you know, take this picture and make it Picasso-esque.

41:17.000 --> 41:18.000
Yeah.

41:18.000 --> 41:22.560
Like, you know, I'd love to take my writing and, you know, make it, you know, the form

41:22.560 --> 41:23.560
of some other author.

41:23.560 --> 41:24.560
Right?

41:24.560 --> 41:25.560
That would be cool.

41:25.560 --> 41:29.960
Now, so it doesn't work as well in languages, it doesn't vision because in language,

41:29.960 --> 41:33.320
you're making a set of discrete decisions.

41:33.320 --> 41:39.920
And so in vision, you have pixels which are much more amenable to small gradients.

41:39.920 --> 41:42.320
And that's why they've had such huge success with vision.

41:42.320 --> 41:43.320
And language is harder.

41:43.320 --> 41:45.880
They're starting to get some work in that area.

41:45.880 --> 41:49.520
So someone a new stuff is applying GANs to sequence of sequence models.

41:49.520 --> 41:54.680
And so what you do is instead of using the cost of generating each token while you're

41:54.680 --> 41:59.760
training in the decoder, you use some other measure of the sentence.

41:59.760 --> 42:04.480
And so, and again, it would be the probability based on some discriminator function, the

42:04.480 --> 42:08.240
probability that this is generated by the computer or by a human.

42:08.240 --> 42:14.320
And then you have to back prop, or well, you have to get that answer back into the system

42:14.320 --> 42:15.320
so it can learn.

42:15.320 --> 42:17.920
And that's usually done like with reinforcement learning.

42:17.920 --> 42:22.760
And that's not very efficient right now for language and it kind of works and there's

42:22.760 --> 42:26.360
a lot of advancements, but still got a ways to go.

42:26.360 --> 42:27.360
Okay.

42:27.360 --> 42:35.240
I just finished a report on industrial applications of AI, and ended up being like 30 pages.

42:35.240 --> 42:45.600
And I'd love to put that through like the Hemingway translator, or that will be awesome.

42:45.600 --> 42:46.600
Yeah.

42:46.600 --> 42:47.600
I think we'll get there.

42:47.600 --> 42:51.600
I can assure you the sentences, I think for Hemingway will be a lot shorter, I think

42:51.600 --> 42:52.600
shorter.

42:52.600 --> 42:53.600
Yeah.

42:53.600 --> 42:55.400
Or run on sentence kind of guy.

42:55.400 --> 42:58.400
Yeah, that would be great.

42:58.400 --> 42:59.800
And there is some of that.

42:59.800 --> 43:05.680
So if you, you know, you train the system on Hemingway, it's going to want to generate

43:05.680 --> 43:07.800
tokens that are Hemingwayish.

43:07.800 --> 43:12.240
So you feed your sentence in and it's going to translate it into be shorter and something

43:12.240 --> 43:13.640
about a fish probably.

43:13.640 --> 43:14.640
Mm-hmm.

43:14.640 --> 43:15.640
Nice.

43:15.640 --> 43:16.640
Awesome.

43:16.640 --> 43:21.120
Well, what's the best way for folks to kind of keep tabs on what you're up to and you

43:21.120 --> 43:25.360
know, follow along as you guys iterate on this model and figure stuff out.

43:25.360 --> 43:26.360
Yeah.

43:26.360 --> 43:28.360
So we have a website, dogrammer.com.

43:28.360 --> 43:30.760
On that website, you can try it out, type in a sentence.

43:30.760 --> 43:34.600
Only does one sentence at a time right now, just because we have a cheap server up on

43:34.600 --> 43:35.600
Amazon.

43:35.600 --> 43:37.720
And then you can join our mailing list.

43:37.720 --> 43:41.600
And I tweet my life out at Twitter, at J-M-U-G-A-N.

43:41.600 --> 43:42.600
Okay.

43:42.600 --> 43:43.600
Awesome.

43:43.600 --> 43:44.600
Well, thanks so much.

43:44.600 --> 43:45.600
It was great chatting with you.

43:45.600 --> 43:46.600
Oh, thanks.

43:46.600 --> 43:47.600
It's been fun.

43:47.600 --> 43:48.600
Awesome.

43:48.600 --> 43:54.360
All right, everyone, that's our show for today.

43:54.360 --> 43:56.280
Thank you so much for listening.

43:56.280 --> 44:00.840
And of course, for your ongoing feedback and support.

44:00.840 --> 44:05.840
For more information on Jonathan and the topics we covered in this episode, head on over

44:05.840 --> 44:10.520
to twimmolai.com slash talk slash 49.

44:10.520 --> 44:15.240
If you liked this episode or you've been a listener for a while and haven't yet done

44:15.240 --> 44:20.080
so, please take a moment to jump on over to Apple Podcasts or your favorite podcast

44:20.080 --> 44:23.680
app and leave us that five star review.

44:23.680 --> 44:30.480
We love to read these and it lets others know that the podcast is worth tuning into.

44:30.480 --> 44:33.800
If you've already done this, then thank you so much.

44:33.800 --> 44:35.800
We greatly appreciate it.

44:35.800 --> 44:41.160
One last note, you've probably heard me mention Strange Loop, a great technical conference

44:41.160 --> 44:43.800
held each year right here in St. Louis.

44:43.800 --> 44:48.400
I'll be attending later this week and I encourage you to check it out.

44:48.400 --> 44:54.560
Also, the following week, on October 3rd and 4th, I'll be at the Gartner Symposium IT

44:54.560 --> 44:59.800
Expo in Orlando, where I'll be on a panel on how to get started with AI.

44:59.800 --> 45:03.040
If you plan on being there, send me a shout.

45:03.040 --> 45:14.320
Thanks once again for listening and catch you next time.

