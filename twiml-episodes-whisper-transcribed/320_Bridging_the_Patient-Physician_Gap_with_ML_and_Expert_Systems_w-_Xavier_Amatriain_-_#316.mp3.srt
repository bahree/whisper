1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:24,240
I'm your host Sam Charrington.

3
00:00:24,240 --> 00:00:27,360
Did you miss TwimalCon AI platforms?

4
00:00:27,360 --> 00:00:32,800
If so, you'll definitely want to check out our TwimalCon Video Packages.

5
00:00:32,800 --> 00:00:38,320
Featuring over 25 sessions, discussing expert perspectives on ML and AI at scale and in

6
00:00:38,320 --> 00:00:43,800
production, you'll hear from industry leaders such as Facebook, Levi's, Zappos, and more

7
00:00:43,800 --> 00:00:50,400
about their experiences automating, accelerating, and scaling machine learning and AI.

8
00:00:50,400 --> 00:00:56,080
In each video package, you'll receive our keynote interviews, the exclusive team Teardown

9
00:00:56,080 --> 00:01:04,960
panels featuring Airbnb and SurveyMonkey, case studies, and more, over 13 hours of footage.

10
00:01:04,960 --> 00:01:10,920
Once again, visit twimalcon.com slash videos for more information or to secure your advanced

11
00:01:10,920 --> 00:01:17,520
purchase today.

12
00:01:17,520 --> 00:01:25,520
All right everyone, I am on the line with Chavier Amatria, Chavier is the co-founder and CTO of Curai.

13
00:01:25,520 --> 00:01:29,600
Chavier, welcome back to the Twimal AI Podcast.

14
00:01:29,600 --> 00:01:32,080
Yeah, thanks for having me, Sam.

15
00:01:32,080 --> 00:01:40,160
So for those that don't recognize the name, Chavier was actually our third guest after

16
00:01:40,160 --> 00:01:42,640
switching to the interview format.

17
00:01:42,640 --> 00:01:50,280
So this was over three years ago, and so much has happened for both of us.

18
00:01:50,280 --> 00:01:56,840
We last had an opportunity to catch up at the AWS Remarice Conference, almost back in June

19
00:01:56,840 --> 00:02:02,720
or so, and I thought it makes sense to get Chavier back on the show to get a little bit

20
00:02:02,720 --> 00:02:06,280
of an update as to what he's been up to.

21
00:02:06,280 --> 00:02:12,800
So when we last spoke to Chavier, he was leading the engineering team at Cora, doing a ton of

22
00:02:12,800 --> 00:02:19,040
work on recommendation systems and other machine learning use cases.

23
00:02:19,040 --> 00:02:24,960
Prior to that, he led the machine learning algorithms team at Netflix, and again, he's

24
00:02:24,960 --> 00:02:31,120
currently the co-founder of Curai, a startup in the healthcare AI space.

25
00:02:31,120 --> 00:02:35,880
Chavier, why don't we just jump right in and have you bring us up to date on Curai and

26
00:02:35,880 --> 00:02:37,560
what's your up to there?

27
00:02:37,560 --> 00:02:38,560
Sure.

28
00:02:38,560 --> 00:02:46,400
Yeah, so Curai, we are using state-of-the-art AI and machine learning for a very big

29
00:02:46,400 --> 00:02:54,320
and bold mission, which is to basically bring the world's best healthcare to everyone.

30
00:02:54,320 --> 00:03:02,240
And of course, that is, as I said, a very bold and very big mission, and we are making

31
00:03:02,240 --> 00:03:06,800
it concrete by basically focusing first on primary care.

32
00:03:06,800 --> 00:03:14,680
So we want to bring the cost down of providing good, not good, but best quality healthcare

33
00:03:14,680 --> 00:03:20,000
to everyone by using AI and machine learning to bring it down to a place where it can be

34
00:03:20,000 --> 00:03:22,520
affordable and it can be scalable.

35
00:03:22,520 --> 00:03:30,480
And everyone in the world who has a phone can have primary care in a very convenient

36
00:03:30,480 --> 00:03:34,640
accessible and affordable way.

37
00:03:34,640 --> 00:03:41,320
And so when you're talking about allowing people to use their phones for primary care,

38
00:03:41,320 --> 00:03:46,160
are we talking about like turning your phone into a tricorder, or are we talking about

39
00:03:46,160 --> 00:03:52,320
using your phone as a kind of a vehicle for accessing human physicians or something

40
00:03:52,320 --> 00:03:53,320
in between?

41
00:03:53,320 --> 00:03:55,680
Or something totally different?

42
00:03:55,680 --> 00:04:05,240
It's a combination of the above, plus something slightly different, but the realization

43
00:04:05,240 --> 00:04:16,920
is that a lot of what can be solved in primary care and in healthcare, it really boils down

44
00:04:16,920 --> 00:04:23,080
to having conversations between patients and physicians.

45
00:04:23,080 --> 00:04:29,640
And of course providing input to those conversations from different sensors and labs and other

46
00:04:29,640 --> 00:04:31,120
places, right?

47
00:04:31,120 --> 00:04:36,800
But the core of what happens in any kind of like medical visit is a conversation between

48
00:04:36,800 --> 00:04:38,760
the patient and the doctor.

49
00:04:38,760 --> 00:04:47,200
And that's the part that can be really automated and not only automated, but actually brought

50
00:04:47,200 --> 00:04:55,680
to a point where you can do it from anywhere that you have a phone with any kind of connection

51
00:04:55,680 --> 00:05:00,480
and you can start having a conversation and chatting with a doctor.

52
00:05:00,480 --> 00:05:01,480
That's all you need.

53
00:05:01,480 --> 00:05:06,400
And of course, there's always going to be things that you can do over the phone like you

54
00:05:06,400 --> 00:05:08,800
can get your medication over the phone, right?

55
00:05:08,800 --> 00:05:13,320
But that's okay because we can always deliver medication to your home or we can always

56
00:05:13,320 --> 00:05:19,760
refer you to a lab that is nearby and get the results from the lab and whatnot.

57
00:05:19,760 --> 00:05:25,680
But the really key issue in the part that we're focusing on is in that conversation that

58
00:05:25,680 --> 00:05:28,720
happens between patients and doctors.

59
00:05:28,720 --> 00:05:34,760
And we have a service where we employ physicians to basically be on the other end of the line

60
00:05:34,760 --> 00:05:36,920
to have those conversations.

61
00:05:36,920 --> 00:05:43,120
And what we're doing is applying this AI machine learning approaches to automate as much

62
00:05:43,120 --> 00:05:48,880
as possible this conversation so we can augment and scale the doctor.

63
00:05:48,880 --> 00:05:54,520
So an important piece here is we're not replacing the doctors with the AI machine learning.

64
00:05:54,520 --> 00:05:58,440
What we're making them is we're giving them, I usually say we're giving them superpowers

65
00:05:58,440 --> 00:06:07,080
that instead of being able to see say 100 patients a day, they'll be able to see 10,000 of

66
00:06:07,080 --> 00:06:12,320
the conversations and most of the easy stuff will be handled by the AI machine learning

67
00:06:12,320 --> 00:06:16,360
and they'll be able to focus only in the places that they're needed the most.

68
00:06:16,360 --> 00:06:22,600
Is 100 patients a day a typical metric for practicing physician?

69
00:06:22,600 --> 00:06:32,720
It really depends. In primary care, the numbers are roughly about, the average is I think 12 minutes

70
00:06:32,720 --> 00:06:40,280
per patient. So you can, it depends on what the working hours are for for doctors.

71
00:06:40,280 --> 00:06:46,000
That's actually that's in the, in the U.S. Okay, all the places like India, for example,

72
00:06:46,000 --> 00:06:52,240
where it's much less than that and doctors can see way more than 100 patients in a day.

73
00:06:52,240 --> 00:06:56,360
And the reality is that they're not even seeing them, the nurses are taking care of them

74
00:06:56,360 --> 00:07:01,120
before they get to see the doctor, but they count as having seen the doctor.

75
00:07:01,120 --> 00:07:07,520
So it's, yeah, it's not a, it's not a, but you can, obviously, you can imagine that

76
00:07:07,520 --> 00:07:14,280
a doctor with 15 minutes or less to see a patient and to remember, first of all, the

77
00:07:14,280 --> 00:07:19,560
history of the patient, what's going on, ask the right questions, get the right answers,

78
00:07:19,560 --> 00:07:25,600
remember everything they know about medical school and come up with a diagnosis and come

79
00:07:25,600 --> 00:07:33,240
up with a recommendation. It's really hard for any, you know, human being to do that at

80
00:07:33,240 --> 00:07:36,440
that rate, right? And of course, there's a lot of mistakes and a lot of things that happen

81
00:07:36,440 --> 00:07:41,560
because of this. What we're trying to do is say, hey, hand that off as much as possible

82
00:07:41,560 --> 00:07:46,280
to the algorithms and the machines and then make sure that when the doctor comes in,

83
00:07:46,280 --> 00:07:50,320
they come at the right time and they come at the point where they have all that information

84
00:07:50,320 --> 00:07:55,760
laid out for them and they can verify the decisions and make sure that they're saying

85
00:07:55,760 --> 00:08:00,840
the right thing. And at the same time, that's what we mean by augmenting, right, the

86
00:08:00,840 --> 00:08:06,640
doctor. We are, of course, giving them information that is state of the art and based on real

87
00:08:06,640 --> 00:08:12,120
science and they can get that information in a way that they can parse it and they can

88
00:08:12,120 --> 00:08:17,040
say, okay, yeah, this is the right decision. I agree. Instead of sort of like having to

89
00:08:17,040 --> 00:08:22,520
deal with all the messiness of gathering that information, parsing it, remembering things,

90
00:08:22,520 --> 00:08:27,480
going through the electronic health record and then making a decision, all of that in

91
00:08:27,480 --> 00:08:29,880
less than 15 minutes, right?

92
00:08:29,880 --> 00:08:38,320
Now there are aspects of this that sound very much like, you know, from the kind of technology

93
00:08:38,320 --> 00:08:47,280
I'd expect to see like other conversational agents where you've got some backend resources

94
00:08:47,280 --> 00:08:56,920
or team that you want to optimize the use of the time and allow some AI system to handle

95
00:08:56,920 --> 00:09:03,600
the kind of easy, easy responses. I've got to imagine that, you know, things get a lot

96
00:09:03,600 --> 00:09:10,760
more complicated and messier, different, certainly more important and healthcare side of things.

97
00:09:10,760 --> 00:09:16,360
Can you talk about some of the unique challenges associated with applying this kind of technology

98
00:09:16,360 --> 00:09:17,360
in healthcare?

99
00:09:17,360 --> 00:09:25,640
Yeah, yeah, definitely. There's a lot of challenges and you're right. You could think that, you

100
00:09:25,640 --> 00:09:32,200
know, the typical approach to dialogue systems and all the advances that we're having recently

101
00:09:32,200 --> 00:09:42,120
on this kind of chatbots and things like transformers and birds and GPG2s and things like that

102
00:09:42,120 --> 00:09:48,320
are useful and they are. I mean, we are using all of the above in different ways, but the

103
00:09:48,320 --> 00:09:56,120
reality is in a domain like healthcare medicine where the stakes are so high, you cannot

104
00:09:56,120 --> 00:10:08,360
leave things out to, you know, chance or just to a model to rely on this kinds of conversations

105
00:10:08,360 --> 00:10:12,720
to actually following the right path. And there's a lot of examples out there where you

106
00:10:12,720 --> 00:10:19,560
can trick any of these models to say things that seem reasonable for any human being, but

107
00:10:19,560 --> 00:10:24,200
they're medically completely wrong, right? And there's been a few examples of that. And

108
00:10:24,200 --> 00:10:32,640
of course, that's the key issue that we were tackling with is like, how do we combine

109
00:10:32,640 --> 00:10:37,760
prior knowledge about what's correct and incorrect in science and in medicine with some

110
00:10:37,760 --> 00:10:44,760
of this automation, right? We do have a key insight here. A very important thing of what

111
00:10:44,760 --> 00:10:52,640
we're doing is we control the end to end. So we have both sides of the conversation and

112
00:10:52,640 --> 00:10:58,680
we're meaning the patient and the expert, the doctor writing this case. So the interesting

113
00:10:58,680 --> 00:11:07,000
thing in our, in the way that we're applying this technology is that we can deploy the conversation

114
00:11:07,000 --> 00:11:14,560
helpers in both ways, right? We can decide to serve something directly to the user if we

115
00:11:14,560 --> 00:11:21,160
want to, but we can also serve it to the doctor and the doctor can use it in an assistant

116
00:11:21,160 --> 00:11:25,880
and make a call whether that makes sense or not if it's being helpful, right? That's

117
00:11:25,880 --> 00:11:31,480
a really important thing, right? Because then you go, you're basically walking the line

118
00:11:31,480 --> 00:11:36,560
between a chatbot and an assistant, kind of like a Gmail assistant, if you will, when

119
00:11:36,560 --> 00:11:42,120
you get an auto response suggestion, right? And the doctor can decide, okay, yeah, this

120
00:11:42,120 --> 00:11:46,880
makes sense. I'll just take it as it is or this doesn't make sense, but they're still

121
00:11:46,880 --> 00:11:55,240
sort of like making sure that that's medically correct. And at the same time, we are getting

122
00:11:55,240 --> 00:12:04,960
training data on how our model, the model that we're building are accurate or not and

123
00:12:04,960 --> 00:12:12,440
in what way they are or not. We have actually an upcoming publication in one of the works

124
00:12:12,440 --> 00:12:18,760
of Neuribs, which basically talked about this, how to constrain the flexibility of this

125
00:12:18,760 --> 00:12:26,320
sort of like deep neural dialogue systems with expert feedback in order to make sure that

126
00:12:26,320 --> 00:12:34,760
the information is accurate for a domain, particularly in medicine, in this case. So

127
00:12:34,760 --> 00:12:41,560
we need to combine the best of both worlds. And by the way, we do the same thing in other

128
00:12:41,560 --> 00:12:49,480
parts of our modeling strategies like diagnosis. In diagnosis, we're also combining expert

129
00:12:49,480 --> 00:12:56,200
systems, which is, you know, old school day eye with deep learning. And I think you cannot

130
00:12:56,200 --> 00:13:03,920
rely 100% on any of the two, but you could get much better if you combine those two strategies

131
00:13:03,920 --> 00:13:10,520
in some smart ways, which I think it's a key insight for medicine, but it's also something

132
00:13:10,520 --> 00:13:15,880
that will happen. And it's being advocated for by many people in machine learning in general,

133
00:13:15,880 --> 00:13:22,040
right? Like you can't blindly trust models that come only with it from the data with no prior

134
00:13:22,040 --> 00:13:27,160
knowledge or some form of knowledge constraints. And a lot of people are trying to figure out

135
00:13:27,160 --> 00:13:31,160
like how do you combine those two things, right? How do you combine all the power you get from

136
00:13:31,160 --> 00:13:39,480
models that are basically just being trained from lots and lots of data with knowledge that we have

137
00:13:39,480 --> 00:13:45,480
and structuring that knowledge and form of prior into the models? Right, right. That is a theme

138
00:13:45,480 --> 00:13:52,200
that continues to recur here on the podcast and in my conversations. One interesting thought

139
00:13:52,200 --> 00:14:01,240
there is, you know, certainly on the probabilistic side, we've benefited from a huge recent

140
00:14:01,240 --> 00:14:13,000
explosion in available tools and algorithms and the like. You've mentioned a bunch of those

141
00:14:13,000 --> 00:14:19,240
already, Bert, et cetera. And, you know, we've got tools like TensorFlow and PyTorch and many,

142
00:14:19,240 --> 00:14:27,160
many, many others. Whereas expert systems, you know, we think of as kind of a throwback to

143
00:14:27,160 --> 00:14:35,160
pre-winter, you know, AI. And I can't think of, you know, not being deep in that space. I can't

144
00:14:35,160 --> 00:14:41,880
think of kind of what the leading open source expert system software might be. Is there a tools

145
00:14:41,880 --> 00:14:46,600
ecosystem there or is it, you know, are people building, you know, when people have this realization

146
00:14:46,600 --> 00:14:50,840
that they need both and not one or the other, are they kind of building it from scratch?

147
00:14:50,840 --> 00:14:58,520
Yeah, I don't think there is such a thing. I don't think there's a, you know, there's a

148
00:14:58,520 --> 00:15:03,800
next-for-system component for TensorFlow or PyTorch. Or should there be, does that make sense?

149
00:15:03,800 --> 00:15:09,320
You know, would something like that have benefited you or is it, you know, is it basically just kind

150
00:15:09,320 --> 00:15:14,200
of rules that we know how to code them because, you know, it's not probabilistic?

151
00:15:14,760 --> 00:15:20,360
Not really. I mean, and by the way, they can be probabilistic, right? At the end of the day,

152
00:15:20,360 --> 00:15:25,080
what you have with these expert systems is a graph. And then you can do probabilistic

153
00:15:25,080 --> 00:15:29,480
inference on the graph and you can do different things on that graph. So basically,

154
00:15:30,840 --> 00:15:36,280
I'm thinking a generic tool for expert systems would be rather simple in the sense that all you

155
00:15:36,280 --> 00:15:45,800
need is a way to represent sort of like graphs and make inferences on those graphs. So it wouldn't be

156
00:15:45,800 --> 00:15:54,840
that complicated to sort of like have a component for TensorFlow or for PyTorch that basically

157
00:15:54,840 --> 00:16:04,040
does that for you. So the key thing here is those expert systems rely a lot still on sort of like

158
00:16:04,040 --> 00:16:11,560
manual labor. And just to give you an example, in the case of some of the expert systems we're using,

159
00:16:11,560 --> 00:16:19,960
we're using some that have been developed for over 50 years, right? So there's a couple of

160
00:16:20,520 --> 00:16:29,000
expert systems for medical diagnosis that go back 50 years. And we, we're using both of them,

161
00:16:29,000 --> 00:16:36,120
actually. And interestingly, you know, there's a lot of knowledge in there, right? You can think

162
00:16:36,120 --> 00:16:42,440
about, you know, 50 years of a bunch of hundreds of really well trained physicians encoding

163
00:16:43,320 --> 00:16:48,840
knowledge and information about medicine and a graph, right? And that's really valuable. And

164
00:16:48,840 --> 00:16:53,560
and it's really something that if you can then inject it into any learning system,

165
00:16:54,440 --> 00:16:59,560
you get a lot of a lot from it, right? To your question, there's no, you know, there's no

166
00:16:59,560 --> 00:17:06,920
tooling for that. On the other hand, you can do interesting things like one of the things we've

167
00:17:06,920 --> 00:17:14,120
done is use this expert system as a, basically a data generator to generate synthetic data and

168
00:17:14,120 --> 00:17:18,760
train learning models from the data that is generated from the expert system, right? That's an

169
00:17:18,760 --> 00:17:25,720
example of something that I think is very useful and really, really valuable because then you can

170
00:17:25,720 --> 00:17:31,640
you can even merge synthetic data with natural data and you can tweak it in ways that you can

171
00:17:32,360 --> 00:17:38,120
learn a model that actually now has some prior knowledge that has been injected in the form

172
00:17:38,120 --> 00:17:42,920
of a ground truth data, so to speak. Can you speak to that particular point in a little bit more

173
00:17:42,920 --> 00:17:50,520
detail? Yeah, sure. Okay, so in this case, the thought process was like that, right? Like,

174
00:17:50,520 --> 00:17:58,440
we know that, you know, if we have very good data and we train a deep learning neural net,

175
00:17:59,320 --> 00:18:06,280
we could get sort of like a really high accurate diagnosis system, the reality is that high quality

176
00:18:06,280 --> 00:18:13,560
data does not exist. If you go to electronic health records, which we have used ourselves, I mean,

177
00:18:13,560 --> 00:18:18,760
we have a project with Stanford where we have been working with them on using the

178
00:18:18,760 --> 00:18:23,320
electronic health records, and this has been something that others have done, like Google and

179
00:18:23,320 --> 00:18:28,360
DeepMind, and you name it, it's like learning predictive models from electronic health records,

180
00:18:28,360 --> 00:18:35,240
electronic health records, the data quality is really, really poor notoriously so. Yes, yes,

181
00:18:35,240 --> 00:18:39,960
and there's a lot of reason for it, but one of it is, you know, they weren't designed

182
00:18:41,000 --> 00:18:45,080
for the purpose of diagnosis, they were designed for the purpose of billing and to make sure

183
00:18:45,080 --> 00:18:51,080
the insurance company's got their money back, so there's a ton of issues with them. So,

184
00:18:52,040 --> 00:18:57,480
but again, that data is valuable, it's not like it's totally noise, there is something in it.

185
00:18:57,480 --> 00:19:05,960
So how can you generate some kind of data that is more, you know, solid and sort of like,

186
00:19:05,960 --> 00:19:10,520
can treat more as a ground truth? Well, you can go to this expert system, which again,

187
00:19:10,520 --> 00:19:17,240
they're, all they are is, you know, a graph, and you can start activating notes and generate

188
00:19:17,240 --> 00:19:24,280
data from that graph that basically becomes sort of other cases that you use to train your

189
00:19:24,280 --> 00:19:30,920
deep learning model, and that's what we showed in this, this is a paper we published last year,

190
00:19:30,920 --> 00:19:37,720
where we basically generated data from the expert system. We injected noise to that data,

191
00:19:37,720 --> 00:19:43,000
because an interesting and important thing is you want to train a model that is robust to noise,

192
00:19:43,000 --> 00:19:48,360
right? The problem with expert systems, one of the problems, is that they're not capable of

193
00:19:48,360 --> 00:19:53,800
dealing with noise. So in other words, if, you know, if the patient doesn't say exactly the

194
00:19:53,800 --> 00:19:58,200
symptom they have and they make a mistake, because they didn't understand the question, or the doctor

195
00:19:58,200 --> 00:20:05,960
enters a wrong thing, the expert system is basically doomed and and we're going to give you an

196
00:20:05,960 --> 00:20:12,280
incorrect output. That's not the case for, you know, you can train machine learning models are,

197
00:20:12,280 --> 00:20:16,840
you know, relatively robust to noise, because you can even do adversarial training and you can do

198
00:20:16,840 --> 00:20:23,240
a lot of different things to make them robust. So how do you combine both? Well, you can also

199
00:20:24,120 --> 00:20:29,240
inject noise to the expert system, that's basically what we did. So we generated data from the

200
00:20:29,240 --> 00:20:36,920
expert system. We injected different kinds of noise. One example, which I think will be very obvious,

201
00:20:36,920 --> 00:20:41,960
is you can inject noise by saying, hey, I'm just going to randomly inject things, symptoms that

202
00:20:41,960 --> 00:20:45,880
are very common, right? I'll just add coughing to everything, because, you know, coughing is

203
00:20:45,880 --> 00:20:51,000
something that people have in general, no matter whether they have one disease or not, right? It's like,

204
00:20:51,000 --> 00:20:57,400
it's, you know, you always can cough or sneeze or something like that that is very prevalent and

205
00:20:57,400 --> 00:21:03,880
very common. It's a typical thing and can confuse, really confuse an expert system, but it's,

206
00:21:03,880 --> 00:21:11,080
if you train a machine learning model on ignoring cough, because it's something that's very common,

207
00:21:11,080 --> 00:21:17,400
and it's not very, it's not going to have determined what the diagnosis is, well, then you build

208
00:21:17,400 --> 00:21:25,640
a robust model. So we again, generated synthetic data from, from these systems, injected noise

209
00:21:25,640 --> 00:21:33,000
in ways that we made the learn model more robust, and we also combined that synthetic data

210
00:21:33,000 --> 00:21:40,760
with natural data that we had from EHRs and other sources to also prove that you can, you don't

211
00:21:40,760 --> 00:21:45,720
need to constrain yourself to just one single kind of data, right? All you need to do is combine it

212
00:21:45,720 --> 00:21:56,120
in smart ways to sort of like understand, because there's obviously value to training from real world

213
00:21:56,120 --> 00:22:03,400
data. All you need to do is figure out how to combine it with more clean data and data that you

214
00:22:03,400 --> 00:22:10,520
can trust. You mentioned this kind of injecting noise via adding symptoms that are frequently

215
00:22:10,520 --> 00:22:15,880
recurrent. What are some other examples of the kind of noise that you're injecting in and more

216
00:22:15,880 --> 00:22:24,440
broadly? How do you quantify the value of this synthetic data in building out your models?

217
00:22:25,720 --> 00:22:34,440
Yeah, so, okay, so to the first question, I mean, I think that the key insight to adding

218
00:22:34,440 --> 00:22:38,840
noise in a domain like medicine is that you do need, you need to have some domain knowledge,

219
00:22:38,840 --> 00:22:43,880
right? When I give you the example of adding symptoms that are very common, that makes sense,

220
00:22:43,880 --> 00:22:48,360
right? Because it makes sense because we know about medicine like, okay, the explanation makes

221
00:22:48,360 --> 00:22:56,120
sense. Another example is like, well, you can remove symptoms that are very rare or are likely

222
00:22:56,120 --> 00:23:01,320
to be missed, right? That's another thing that makes sense once you explain it, right? But you need

223
00:23:01,320 --> 00:23:06,840
to have some insight and you need to talk to doctors, and that's something we do all the time,

224
00:23:06,840 --> 00:23:16,680
right? This kind of strategies don't come up by sheer imagination. They come up because we talk

225
00:23:16,680 --> 00:23:21,960
to our physicians and we talk to them and say, hey, what's, how do you deal with this issue? Where

226
00:23:21,960 --> 00:23:27,800
are issues that are common and that lead to mistakes in diagnosis? How can we make sure that our model

227
00:23:27,800 --> 00:23:35,960
doesn't make the same mistake? So I think that is a key and important thing is you need to work with

228
00:23:35,960 --> 00:23:40,680
the main experts and that leads me to answer your second question. Let me just pause there because

229
00:23:40,680 --> 00:23:45,560
that's a kind of an interesting point. I think, you know, and I think of noise, at least from a

230
00:23:45,560 --> 00:23:51,400
classical engineering perspective, I think of noises like this junk that's, you know, uncorrelated

231
00:23:51,400 --> 00:23:57,160
from your signal. But what you're suggesting is that at least when you're creating synthetic

232
00:23:57,160 --> 00:24:01,800
data, your noise needs to be correlated with your actual noise that you need to expect. You can't

233
00:24:01,800 --> 00:24:07,480
just have, you know, purely random noise because that won't help your model.

234
00:24:08,280 --> 00:24:15,800
Yeah, that's pretty much it. I mean, here it's slightly different, right? And notion of noise

235
00:24:15,800 --> 00:24:25,560
if you will. But what you have is synthetic data that is strictly true, if you will, because

236
00:24:25,560 --> 00:24:30,280
true in a scientific sense, because it's been generated by kind of like an expert system that

237
00:24:30,280 --> 00:24:38,280
has been designed on science. But what you need to do is inject noise that mimics more the

238
00:24:38,280 --> 00:24:45,240
reality of nature, right? And the messiness, right? But that noise needs to model some of the

239
00:24:46,760 --> 00:24:53,320
natural messiness that you see in real life. And you need to not inject it. Yeah, it's not white

240
00:24:53,320 --> 00:25:00,200
noise, right? In that sense, right? It's noise that tries to sort of turn that synthetic data into

241
00:25:01,320 --> 00:25:06,200
something that is more real, right? If you think about it, I mean, I use some time for the metaphor

242
00:25:06,200 --> 00:25:10,520
of like the self-driving cars, also use synthetic data that is generated from video games. And it's

243
00:25:10,520 --> 00:25:18,360
like, well, you can imagine that you're training your self-driving model on data from, from Grand

244
00:25:18,360 --> 00:25:24,920
Flip Auto, but you need to inject, I don't know, a flock and you need to inject rain and you need to

245
00:25:24,920 --> 00:25:31,640
inject things that are not maybe, you know, in your synthetic data. And they're adding noise to the

246
00:25:31,640 --> 00:25:38,200
capture of the image, but in a way that mimics real life situation, right? Not just white noise.

247
00:25:39,080 --> 00:25:44,680
And that sense, it's a bit like the concept of domain adaptation.

248
00:25:44,680 --> 00:25:51,640
Yeah, I mean, you could consider that for sure. And that's another, it is a very, I mean,

249
00:25:51,640 --> 00:25:56,040
domain adaptation in itself. I mean, we could go into that. It's another important thing that

250
00:25:56,840 --> 00:26:03,400
you need to do in many cases because, and yeah, you're right. It could be seen as that, right? Because

251
00:26:03,400 --> 00:26:10,600
sometimes you are training on ideal data, but then you're going to be faced with real life data that

252
00:26:10,600 --> 00:26:18,760
it's going to have to be interpreted in the context of the ideal data that you use for training. So,

253
00:26:18,760 --> 00:26:23,160
yeah, it is, yeah. Okay, so you're about to take on that second question.

254
00:26:23,160 --> 00:26:30,680
Yeah, the second question was about how do you even, you know, how do you know that the data is good

255
00:26:30,680 --> 00:26:39,080
or even the model that your training is good? And, and, you know, beyond that, the relative

256
00:26:39,080 --> 00:26:45,000
advantage of, you know, how do you compare with and without using this synthetic data, you know,

257
00:26:45,000 --> 00:26:51,480
is it a, is it a training time or is it a, you know, accuracy or some combination of all these things?

258
00:26:52,520 --> 00:27:01,240
It's mostly about accuracy, right? And, and the, the problem is that the definition of the accuracy

259
00:27:01,240 --> 00:27:09,960
is, again, really tricky and, and, and, and not that obvious, right? And accuracy in the context

260
00:27:09,960 --> 00:27:20,280
of medical diagnosis is a very, very tricky thing to define, particularly because you would hope

261
00:27:20,920 --> 00:27:28,440
that by asking physician, you would get a ground truth, but that's not the case, right? There's

262
00:27:28,440 --> 00:27:35,560
a studies out there, for example, the human DX project that published some studies that

263
00:27:37,000 --> 00:27:43,240
the, on their dataset, the average accuracy of a single physician was 60 percent, right?

264
00:27:44,280 --> 00:27:51,400
Which is really low. Now, if you, if you take the consensus of 20 physicians that got up to

265
00:27:51,400 --> 00:27:57,800
over 80 percent, which is much better, but then, of course, you need to have 20 physicians agree,

266
00:27:57,800 --> 00:28:02,760
and you still have to 80 percent, which is a lot better, but not necessarily comforting if

267
00:28:02,760 --> 00:28:10,200
you're the patient. Exactly. And, and, and I think that's, that's a key issue in like, what do we

268
00:28:10,200 --> 00:28:17,000
treat as ground truth? So, in our case, we, we, I mean, we use a combination of a lot of things,

269
00:28:17,000 --> 00:28:21,720
we use a combination of sort of, like, publicly known datasets, which there's not that many,

270
00:28:21,720 --> 00:28:28,760
unfortunately, for, for this domain, and they're just, you know, a few, what's called medical

271
00:28:28,760 --> 00:28:37,320
vignettes that you can use to evaluate. We also use our own physicians to QA, and we make sure that

272
00:28:37,320 --> 00:28:43,960
we have sort of, like, several of them agreeing on the cases, so we know that, that we're right.

273
00:28:43,960 --> 00:28:49,240
And then, at the end, it, there's also this kind of, like, synthetic data, right? It's like,

274
00:28:50,120 --> 00:28:56,120
you need to treat that synthetic data as pseudo ground truth, in the sense that, as I mentioned,

275
00:28:57,080 --> 00:29:02,040
if you think about it, that, that synthetic data is the result of, as I said before, 50 years

276
00:29:02,040 --> 00:29:08,280
of research from hundreds of physicians who have agreed that that's what, you know, that,

277
00:29:08,280 --> 00:29:14,120
particular disease should be defined as, and that's those are the symptoms that are related. So,

278
00:29:14,120 --> 00:29:20,680
it's, it's as good as a ground truth as you can get in many other cases, right? So, again,

279
00:29:20,680 --> 00:29:27,000
it's, I wish I had a, like, a great answer for this, but the reality is, I don't. It's like,

280
00:29:27,960 --> 00:29:33,160
it's a, it's kind of an iterative process where you, like, treat one data as a ground truth,

281
00:29:33,160 --> 00:29:37,400
but then you compare it to your other data, you let your physicians go through it and say,

282
00:29:37,400 --> 00:29:44,920
yeah, this is correct or it is not. And then you feed it back and you keep improving both over time.

283
00:29:44,920 --> 00:29:52,200
And I think that's, that's another very important lesson learned here is that you need to design

284
00:29:52,200 --> 00:29:59,640
all the systems as really learning systems, right? So, it's, it's not only about what's their

285
00:29:59,640 --> 00:30:06,920
accuracy today, it's more about how can you make sure that the accuracy and all the other

286
00:30:06,920 --> 00:30:13,000
methods you care about improve over time, right? And in the meantime, the, the, the, the important

287
00:30:13,000 --> 00:30:17,560
thing is like, we always default to humans, right? It's like, we'll always default to a human

288
00:30:17,560 --> 00:30:22,680
doctor and improve the model over time and, and just tell that human doctor, like, hey,

289
00:30:22,680 --> 00:30:27,160
our model thinks that these three things are important. You want to consider them and the doctor

290
00:30:27,160 --> 00:30:33,800
will say, yes or no, and it's their call. And, you know, we'll be as good as the, as the doctors

291
00:30:33,800 --> 00:30:41,400
are. But over time, we, we are pretty sure. Actually, even in our outline evaluation metrics,

292
00:30:41,400 --> 00:30:46,040
we think that we're already, our models are already at least as good as not better than the average

293
00:30:46,040 --> 00:30:50,680
doctor. But even with that, it's, it's not enough, right? It's like, they need to be better than

294
00:30:50,680 --> 00:30:59,000
the best doctor to even make it feasible to rely on, on, on them. But they're a good assistant

295
00:30:59,000 --> 00:31:08,120
and a good augmentation to the human physician for sure. Do you, have you made any attempts to

296
00:31:08,840 --> 00:31:18,280
benchmark the, the third party expert systems with regard to, you know, some elusive metric around

297
00:31:18,280 --> 00:31:27,080
accuracy or, you know, I guess that the thought is that, you know, even if we were confident that each

298
00:31:27,080 --> 00:31:35,400
of the elements in this expert system, you know, was vetted by the 20 doctors or whatever required

299
00:31:35,400 --> 00:31:42,920
to, you know, have a consensus that, you know, has some sufficient level of accuracy. You know,

300
00:31:42,920 --> 00:31:50,040
medical perspectives have changed significantly over 50 years. We may, I don't know the extent

301
00:31:50,040 --> 00:31:55,800
to which this is tracked in this expert system. But, you know, there are diagnostic practices that

302
00:31:55,800 --> 00:32:02,200
apply not equally across different groups of patients. So you have all the potential for all kinds

303
00:32:02,200 --> 00:32:07,960
of biases within a data set like that. Have you made any attempt at kind of evaluating that?

304
00:32:08,600 --> 00:32:16,040
I mean, we are constantly evaluating that with our data, but it's really hard to come up with a,

305
00:32:16,040 --> 00:32:24,520
you know, something that I, I would dare to publish, right? Because it's, the problem is the same.

306
00:32:24,520 --> 00:32:31,720
It's like there is no, no ground truth. There's a, there's a couple of papers on evaluating

307
00:32:31,720 --> 00:32:38,120
different systems and different online symptom checkers. And those are the ones that everyone

308
00:32:38,120 --> 00:32:47,560
is using as sort of like the benchmark. And there's a paper by semi-gram on evaluating symptom

309
00:32:47,560 --> 00:32:55,000
checkers. And there's some medical vignettes that she published, which are commonly used by a bunch

310
00:32:55,000 --> 00:33:01,000
of people, including some like Babylon in the UK and so on with they published things like,

311
00:33:01,000 --> 00:33:05,480
well, we use these vignettes because that's all we have that at least is commonly available and

312
00:33:05,480 --> 00:33:11,960
you can benchmark against. But they're far from, you know, something that it's that you could

313
00:33:11,960 --> 00:33:18,840
consider sort of like has good coverage of medical conditions and you can trust us as being

314
00:33:18,840 --> 00:33:28,760
comparable. But that being said, again, I think that the reality is as harsh as it may sound,

315
00:33:28,760 --> 00:33:35,880
it's not too hard to be better than the average physician. But again, that's not enough. That's

316
00:33:35,880 --> 00:33:41,000
not convincing. Like if I told you like, oh, I can build a self-driving car that is better than

317
00:33:41,000 --> 00:33:47,560
the average teenage driver, would you be okay? Like, well, probably not. Because the average

318
00:33:47,560 --> 00:33:56,200
teenage driver is not somebody I would trust on an automated driving machine. So I think here

319
00:33:56,200 --> 00:34:01,320
it's pretty much the same. It's not about being better than the average doctor. It's about

320
00:34:02,120 --> 00:34:09,320
being better than the best doctor and being able to augment and always sort of like fall back on

321
00:34:09,320 --> 00:34:20,280
humans. And I think that's exactly, I like that comparison to self-driving cars a lot because I

322
00:34:20,280 --> 00:34:26,920
think what we're trying to build is not completely autonomous vehicle, right? We're trying to build

323
00:34:26,920 --> 00:34:32,920
this AI automation as an assistant to the driver just like many cars do right now. But in this case,

324
00:34:32,920 --> 00:34:40,280
the driver is an expert who is a physician. One more question for you. You mentioned earlier that

325
00:34:40,280 --> 00:34:46,680
among the techniques that you're relying on, you do make some use of transformers,

326
00:34:46,680 --> 00:34:51,240
Bert, GPT-2, that kind of thing. How does that play out in what you're building?

327
00:34:52,120 --> 00:34:58,440
That plays out in many different ways. I mean, there's a lot of great things about

328
00:34:58,440 --> 00:35:05,960
those approaches that the one that I think is probably the most relevant in our case is the fact

329
00:35:05,960 --> 00:35:15,880
that it's all about transfer learning, right? It's about if you have a great model that has learned

330
00:35:15,880 --> 00:35:24,840
in general how to speak, sort of say, you can then fine tune it on some specific domain to become

331
00:35:24,840 --> 00:35:32,120
better about speaking about healthcare, right? So a lot of the approaches we take is we look at some

332
00:35:32,120 --> 00:35:39,240
of these models. We fine tune them on very specific data that we have that is focused on healthcare.

333
00:35:39,240 --> 00:35:45,480
And then we can use it to do a bunch of things. I mean, the output of those models can be used

334
00:35:46,120 --> 00:35:52,120
in the context of a chat bottle or a dialogue system, but you can also use them to generate

335
00:35:52,120 --> 00:36:00,680
features for anything, for a classifier or you name it, right? And I think they build a representation

336
00:36:00,680 --> 00:36:09,880
of language in general, right? So we use them as inputs to many of the things we do,

337
00:36:10,520 --> 00:36:16,440
but more directly, we also use them, as I was mentioned before, to generate

338
00:36:16,440 --> 00:36:23,160
assistance to the physicians as they're chatting and they're talking to the patient, right? So

339
00:36:23,160 --> 00:36:32,840
if you think about, and then that also, I think I dare to say pretty common in many applications

340
00:36:32,840 --> 00:36:39,800
of just customer service in general, like where customer service will have, sort of like assistance.

341
00:36:39,800 --> 00:36:46,840
Actually, there are some papers, I think, for example, from Airbnb, where they've done similar

342
00:36:46,840 --> 00:36:52,600
things for their customer service, where there's basically an assistant that is telling the customer

343
00:36:52,600 --> 00:36:58,840
service and suggesting things they could say, so they can basically accept them or not and decide

344
00:36:58,840 --> 00:37:05,400
whether they want to type them out or just simply select the suggested response. So that's an

345
00:37:05,400 --> 00:37:12,360
example where you can almost, you know, you can take one of these models fine-tuned training on,

346
00:37:12,360 --> 00:37:17,960
training on very specific data that it's more healthcare-oriented and you can generate sort of

347
00:37:17,960 --> 00:37:27,080
like an assistant for a physician or an expert in any given domain. Well, Chavier, it was

348
00:37:27,080 --> 00:37:32,360
absolutely wonderful catching up with you, really excited to learn more about what's your up to

349
00:37:32,360 --> 00:37:39,400
there at CURI and I'll definitely be following along. Okay, yeah, great. I would say that many of

350
00:37:39,400 --> 00:37:47,000
these things that we've, I've mentioned, we are publishing and we're, we have I think four papers

351
00:37:47,000 --> 00:37:52,600
in this machine learning for healthcare, workshop and new ribs and if people are interested in

352
00:37:52,600 --> 00:37:59,320
following up in some of the details of how we use this transformer model or how do we do diagnosis

353
00:37:59,320 --> 00:38:05,400
and so on, that's all, I mean, they can go to archive and find more details on some of these

354
00:38:06,440 --> 00:38:12,040
techniques and how we're using them and trying to solve sort of like this huge healthcare problem

355
00:38:12,040 --> 00:38:20,600
access. So yeah. Fantastic. We'll include some links to those papers on archive in the Shunuts.

356
00:38:20,600 --> 00:38:31,880
Great. So great talking to you. Thank you. That's our show for today. To learn more about today's show,

357
00:38:31,880 --> 00:38:39,560
visit Twomolai.com slash shows. Once again, if you missed Twomolcon or want to share what you learned

358
00:38:39,560 --> 00:38:46,680
with your team, be sure to visit Twomolcon.com slash videos for more information about Twomolcon video

359
00:38:46,680 --> 00:38:56,680
packages. Thanks so much for listening. Peace.

