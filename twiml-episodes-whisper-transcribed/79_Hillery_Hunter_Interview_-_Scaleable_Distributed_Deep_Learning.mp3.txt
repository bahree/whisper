Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast we're running a series of shows consisting of conversations
with some of the impressive speakers from an event called the AI Summit in New York City.
The theme of that event and of this series is AI in the Enterprise.
And I think you'll find this series really interesting in that it includes a mix of both
technical and case study oriented discussions.
Now I won't actually be attending the AI Summit this week because I'm in Long Beach
California attending the Nips Conference.
There are a bunch of Twimble listeners here and I'm hoping to meet as many of you as possible.
And yes, I have stickers.
If you're here at Nips and you're actually listening to podcasts this week, please
reach out to me either via the event app, the Nips event app where there's a Twimble
listeners thread or via Twitter where my handle is at Sam Charrington.
Before we proceed, let's quickly talk about the podcast schedule through the end of the
year.
Prior to my newsletter, you know that I've been on the road for a couple of weeks now.
After this week's series, we've got two more series coming before we break for the year
with our last show running on December 22nd.
Now, if you're lamenting two weeks without your favorite machine learning and AI podcast,
trust me with these two series.
The first from the Amazon Web Services Reinvent Conference and the next one from Nips.
You will have plenty of great content to tide you over until we get started again on January
8th.
Thanks to you, 2017 was a great year for the podcast and we plan to close it out strong.
So keep your ears open the next few weeks and we hope to hear from you.
Please note that on Wednesday, December 13th, we'll be holding our last Twimble Online
Meetup of the Year.
Bring your thoughts on the top machine learning and AI stories of 2017 for our discussion
segment.
And for our main presentation, Bruno Gensalvez, we'll be presenting the paper Understanding
Deep Learning requires rethinking generalization by Shi Yuan Zhang from MIT and Google Brain and
others.
You can find more details and register for the meetup at twimlai.com slash meetup.
This AI Summit series is brought to you by our friends at IBM Power Systems.
IBM Power Systems offers servers designed for mission critical applications and emerging
workloads, including artificial intelligence, machine learning, deep learning, advanced
analytics, and high performance computing.
IBM Power Systems benefit from a wide range of open technologies, many stemming from
collaboration with fellow Open Power Foundation members, and they're designed to deliver
performance efficiently, whether deployed in private, public, or hybrid clouds.
To learn more about the IBM Power System AC-922 platform for Enterprise AI, visit twimlai.com
slash IBM Power.
My guess for this first show in the AI Summit series is Hillary Hunter, IBM Fellow and Director
of the Accelerated Cognitive Infrastructure Group at IBM's TJ Watson Research Center.
Hillary and I met a few weeks back in New York City and I am really glad we were able to
get around the show.
Hillary joins us to discuss our team's research into distributed deep learning, which was
recently released as the Power AI Distributed Deep Learning Communication Library, or DDL.
In my conversation with Hillary, we discussed the purpose and technical architecture of the
DDL.
It's multi-ring topology, its ability to offer synchronous distributed training of deep
learning models and much more.
This is for sure a nerd alert pod, especially for the performance and hardware geeks among
us.
Be sure to post any feedback or questions you may have to the show notes page, which
you'll find at twimlai.com slash talk slash 77.
And now on to the show.
All right, everyone, I am on the line with Hillary Hunter.
Hillary is a IBM Fellow and Director of the Accelerated Cognitive Infrastructure Group
at IBM's TJ Watson Research Center.
Hillary, welcome to this week in Machine Learning and AI.
Thank you so much.
Very excited to be here.
I'm excited to have you on the show as well.
Folks won't know this until I tell them, but we had an opportunity to meet just a few
weeks ago in New York City at the NYU at a reception held in conjunction with the NYU Future
Labs event, and it was certainly great to meet you in person and even better to have
an opportunity to get you on the line and dig into some of the work that you've been
up to.
Yeah, it was a pleasure meeting you there and it was certainly a really interesting event
and a great opportunity to see some of the exciting things going on in the New York area
and AI and AI is just exploding everywhere.
But it's a pleasure to be here on your podcast.
I look forward to our discussion.
Awesome.
So why don't we get started by having you tell us a little bit about how you, your background
and how you got involved in artificial intelligence?
Yeah, it's interesting.
I like to say that AI really has exploded for two reasons.
One being the amount of data, I'm going to especially publicly available data sets and
the second being the compute.
And I come from a background technically that's really a mix of both those things.
And AI has been an opportunity to bring together a lot of different things that I've done
in prior technical work and a lot of different things done by members of my team in kind of
prior technical lives before getting into AI.
So I come from a systems perspective, from a hardware perspective.
I was an electrical engineer by training, and so, yep, I was a doubly.
Yeah, so that background is kind of where I'm coming from and approaching these problems
that we're trying to tackle in AI.
And I bring to the table background in performance, the background in data movement in systems,
and both are proving to be really fruitful for getting some of the grand challenge kind
of scale problems done that we're facing today in AI.
Awesome. And have you spent most of your career at IBM?
I have.
Yeah.
So I got my PhD at University of Illinois and started at IBM in 2005, and I've been with
IBM research since then.
Fantastic.
And have you been working on, in the same group, working on accelerated cognitive infrastructure
or have you done kind of evolved to that, this particular position?
Yeah.
This position has evolved.
You know, we really ramped up our efforts around accelerated computing a number of years
ago.
Prior to that, I was working on things related to processor design and memory technologies.
And it's interesting because, again, the memory relates to the data movement, relates
to, you know, feeding the AI, feeding the accelerator.
So it's all kind of come together in a really nice way.
Great.
And your group recently published some really interesting research on essentially scaling
deep learning performance using distributed, distributed techniques.
That was one of the big things that I wanted to spend some time talking about in this interview.
Can you give us an overview of that research?
Yeah.
We were really excited by what we were able to publish.
We're calling it the Power AI DDL or Distributed Deep Learning Capability.
And basically, what we showed is that we were able to create a framework independent.
So independent from TensorFlow or Cafe or PyTorch or your favorite way of doing deep learning.
We were able to create a framework independent communication library that achieves close
to optimal, very, very close to optimal, up to 95% scaling efficiency.
So what this meant for us is that we were able to use lots of GPUs together very, very
efficiently.
And we were able to use those mechanisms to train a neural network, to train a ResNet
to the highest published accuracy on a really hard problem.
And we were also able to beat what had been shown as a record of around an hour or 66 minutes
on a smaller neural network and a smaller problem.
And so we really were able to show that through hardware software integration, we were able
to have world-class AI capabilities.
And for us, this really kind of signaled a change in the productivity curve for deep learning
because most of the open source today just kind of handles a single node worth of performance.
So you're kind of stuck then.
You can have two GPUs, four GPUs, maybe eight.
But scaling out to many, many servers has been a really big challenge.
And the more servers and the more hardware you can use for a problem, the faster you get
that work done and the more productive you are.
Now, that last point might be one that's worth underscoring.
If you look at a framework like TensorFlow, there's, as part of the open source TensorFlow,
there's a distributed TensorFlow.
But that's more useful for scaling across GPUs than it is across servers, is that correct?
So there is a version of distributed TensorFlow and what we really are kind of looking at
in that versus the DDL capabilities is the extent of scalability and ultimately the
productivity of the servers.
So the default distributions in TensorFlow haven't been shown to be able to use as many
as 256 GPUs as far as I'm aware and based on our own internal studies.
So there are X factors to be had with every extra set of servers that you can add.
You get the work done faster.
And also we've shown that our communication patterns and communication overheads are
really close to optimal.
And so the other things that are out there appear to be less efficient.
And so that means at the end, a longer time to solution.
Okay.
And the 256 GPUs that you are running was across how many systems?
Yeah.
So we ran across 64 servers and we were using the Pascal P100 GPUs.
The work that we did was right before, right on the cusp of Volta coming out and miss
the very latest GPUs.
And I like to always kind of describe why this is a hard problem because the GPUs are so
fast that they all learn very quickly.
And so if you think of there being 256 kind of learners in the system, the hard part
is keeping everybody synced up in that process of keeping them synced up.
It's really critical that that be done with as low latency of a communication as possible.
And that's really the core of the technology that we showed is that the communication time
is really, really low.
And that enables you then to do fully synchronous training, meaning everyone is updating everyone
else with all the information that's being learned.
All the weights are being updated as they should after every batch.
And that means at the end that you're doing a type of deep learning that is opposed to
asynchronous where they're kind of occasionally updating each other in order to lower the
communication overhead, when you're able to do fully synchronous, you're able to kind
of keep everything moving forward a little bit more predictably, predictably, sorry, and
you're able to get a higher accuracy result at the end in general.
Okay.
And introducing this, you said it's called Power AI DDL for those who aren't familiar
with Power AI.
What is Power AI and to what extent are the results that you demonstrated here tied to
that Power AI architecture?
Yeah.
So for us, this is very much an effort of hardware and software co-design.
Power AI runs on the IBM SC822LC servers, sorry, actually, S822LC servers.
And those servers have two power processors, and they have four GPUs.
And everything is connected by doubled up NV links.
So Nvidia has this high bandwidth interconnect capability, and we have that high bandwidth connectivity
not just between the GPUs, but also back to the host processor.
And so that provides extra performance in moving data and moving weight updates and everything
in the system.
And our communication library also leverages all of that bandwidth to its max and to its
full potential.
And so when we talk about this whole space of deep learning and what we're doing from
a systems perspective and with things like DDL, we're talking about matching the software
to fully utilize the hardware capabilities.
And for us, this is also a storyline around collaboration between our research division
that I actually report in and our product division, our development division, because we were
able to actually put this code base out for IBM customers to try and download and try
themselves, try DDL on their servers or on the cloud at the same time that we made the
publication and the announcement of our leadership deep learning capabilities using this framework.
So Power AI is a, it's a download and go set of distributions of frameworks that run
on our accelerated servers.
And we now have the distributed deep learning capabilities available there as well for customers
to try themselves.
So we really, from a research perspective, we're very excited about this because it means
that we can kind of take this rarefied skillset of distributed deep learning, this kind of
grand challenge thing everyone's competing over and put it in the hands of our customers
to go ahead and try out and see what they can do with it on their data with their types
and neural networks.
And you mentioned that the results that you saw are framework independent is, is that
true in the strictest sense or is it rather that the software that you wrote was written
to adapt to some fixed number of frameworks?
So it's pretty true in a strict sense, but let me define strict to make sure we're not
talking past each other.
So what DDL is is it's a communication library.
So it lets things in a system talk to each other.
We have been able to use that communication library across many different frameworks.
What we released is already as part of Power AI was our integration into TensorFlow and
into Cafe, but we also published results using our integration into Torch and we have
other integrations that we have shown work just fine as well for our internal use currently.
And so it we have shown I think at this point integration into enough frameworks that I'm
pretty comfortable saying that, you know, this library can be integrated into pretty much
anything that you write.
Right, right.
Yeah, I think in the context of my question, the answer is yes and yes, right?
Yes, yes and yes, there we go.
Interesting.
And so how does how does this compare to some of the previous and even subsequent work like
you refer to Facebook and Microsoft work in the paper or in the blog posts since I think
since you posted this Uber published an open source project called Haravad that seeks
to do something similar.
Have you are you familiar with that one?
Yeah, I am.
And that's obviously a great, a great result that they put out there and we love seeing
all the different efforts that are going on in this space because it really is such an
important area for productivity.
The Haravad team showed a great set of scaling, scaling experiments with their integration
essentially of an MPI.
It was open MPI, right?
Mm-hmm.
MPI reduced and the nickel libraries into TensorFlow.
We do believe that our communication topologies are a bit better than what is there.
And so the net scaling efficiency will be better and we look forward to being able to talk
about that in a little bit more concrete detail pretty soon here.
But I think, you know, it's great to see these different efforts toward distributed deep
learning happening across different types of frameworks because ultimately the community
needs to have this type of productivity in order for deep learning really to take off.
For us though, the positioning at Power AI is really about taking open source and taking
the complexity of managing it, of installing it, tuning up the performance, optimizing it
for our systems, and then ultimately providing support on it for our customers.
And so, you know, we love to see improvements in open source and much of what's provided
in Power AI is open source and is based on open source.
And then we're, you know, providing improvements on top of that and optimizations on top of
that as well.
So, we want to both take the time to get going with open source way down, which is the
just download and go as well as we want to then be able to provide support and optimizations
and improvements that are really significant on top of what's going on in the space.
Mm-hmm.
In terms of DDL, I'm curious if you can kind of walk through the next level of detail and
why, you know, what are some of the architectural elements that you feel given an advantage
relative to, you know, other things that one could do in general?
Yeah, absolutely.
So, there's two things.
There's one, the advantages of the hardware, which are that the GPUs are double NV linked,
connected to one another, and also double NV linked connected back to the host processor.
Those features provide performance if you, you know, use them with an appropriate communication
topology, which we do do with DDL.
And in addition, then DDL at the overall system level, when you're talking about connecting
together a bunch of different learners, a bunch of different system nodes, uses a multirang
topology, not a single ring, but a multirang topology, and what this results in is our ability
to use all of the links to the greatest advantage possible.
If you do a naive mapping onto a system, or if you have a system with, you know, PCIe
interconnect, there are going to be bottlenecks.
What we do is kind of maneuver around the bottlenecks, and we use all the different bandwidths
so the bandwidth between the GPUs, the bandwidth on a node, the bandwidth getting out to the network,
etc.
We use those to their best possible efficiency.
So ideally, you know, you want to use all of the hardware and all the interconnect that
you have, and software doesn't naturally do that.
And so we've kind of taken all that work away from the developer and said, you know, we're
going to max out the system.
And if you buy the hardware, it's going to work really well because of the software
and the way the software is using all the, all the, the system bandwidths.
Hmm.
When you talk about these rings, where in the system architecture do they exist, are these
at the interconnect level, are they in memory, are they someplace else?
So these are all within the different interconnects of the system.
Okay.
So the, they're part of the double NV link that you mentioned.
Yep.
A part of the double NV link, a part of the connections out in the network, part of the
nodes being connected to each other with network connections, that kind of layer.
You know, there's another thing we could talk about.
You brought up memory.
There's another thing that we could talk about, which is the large model support that
we do, which is also a feature in Power AI.
And that is a situation where you really want to try to use the different pieces of memory
in the system.
So, if you think of it philosophically, it would distribute a deep learning.
We're using all of the, the links in the system, all the network bandwidth, all the bandwidth
available on a given system.
With large memory support, we're providing an out-of-core capability, meaning capability
of, of accessing other resources, other memory in the system greater than just what the GPU
has all by itself.
And the way to think about that is, you know, if I have a system, the GPU has a small
amount of memory today about 16 gigabytes, but the host processor has a lot more memory,
128 gigabytes or 256 or up to a terabyte these days, right?
So we provide function that enables people to explore bigger model sizes, bigger data
sizes, etc.
By accessing and using actively, all of the memory and not just the GPU's memory in the
system.
So, generally, I would say our philosophy is to kind of use all the resources available
in the system, you know, and let the AI developer explore things as bigly, you know, as bigly,
as largely, as largely, as large as they'd like to go, as big as they'd like to go.
And that includes dimensions of both memory and the number of systems in that, you know,
that you have to tie together.
And we've talked about the kind of framework transparency, nature of this.
Does that mean that all of the thinking that has to go into taking advantage of these
elements happens at the DDL, you know, and or framework layer?
And there's no, you know, no aspects of the, you know, the problem that has to be thought
through by the developer or there's still things that the developer has to be aware of in
order to be able to take advantage of what you've done here.
Yeah, it's a, it's such an important point in productivity, right, because you can run
as fast as possible.
But if you're going to sit there for multiple days, scratching your head, trying to figure
out how to run fast, it doesn't do anybody any good, right?
So, so one of the, one of the decisions that we made when we did the integration, for
example, into TensorFlow was to leverage the slim library, which is a library that kind
of creates the capability to run a particular neural network.
So in that case, we have hidden the DDL capabilities completely from the developer and they just
have to use slim.
So we're trying to hide under in general where we can abstractions that have been created
at higher levels so that people don't have to, you know, spend days and weeks trying
to figure out how to, how to use this technology.
I think we're, we're very focused on, you know, developer productivity was part of the
reason why we, you know, went from, you know, research endeavor to very quick engagement
with our development team and wanted to get it out there in the hands of people very
quickly and why we're focused on speed because speed, you know, this is one of the very
few areas of computing today where people sit around waiting for days, you know, to develop
the capability.
It's really kind of crazy.
So we want to like go from days down to hours and we want to, you know, get people there
as quickly as possible through downloading, go and then also through use of some of these
higher level abstractions like slim, okay.
One of the, one of the critiques, I guess, of some deep learning work is that, you know,
you hear it described as kind of overfitting on image net and you reference image net
in your results as well.
Do you, you know, what gives you confidence that these results will be generally applicable
beyond, you know, a specific, you know, data set and problem?
Yeah.
That's it.
It's a, it's a great thing to talk about because it gets to why we are so passionate about
the key thing here being that we were showing the capabilities, what you can do with the
framework, you know, only that that's the only thing that it does, right?
So, so as you start to look at other classes of neural networks, the kind of computation
to communication balance changes and some classes in neural networks are known not to scale
very far today, not, you know, known, you know, kind of they won't run it to 256 GPUs,
they run it maybe a small number of GPUs.
So what we see though is that if you're using the type of really close to optimal communication
methods that we have that no matter what the kind of inherent capability of that neural
network to scale is going to be, we're going to guarantee that you get, you know, out
to as much scale as is possible.
So if you're using a suboptimal communication method, you know, maybe you can only get
to 8 or 10 GPUs for other classes of neural networks, we're going to push that number
up by getting the communication latency to be as absolutely short as possible.
So you know, in our view, this is really about, you know, whatever the current state is
of a neural network and the scientific understanding of it and the ability to scale, we're going
to push that out and get that training done, done faster.
Awesome.
Awesome.
What other things are you working on in your group?
Yeah.
So, you know, I mentioned the large model support, which is something that, again, like I said,
comes from the same perspective of wanting to use the available system resource and not
just sort of, you know, offloading work to a GPU and being constrained by what that provides.
So we want to go multi-GPU, we want to go multi-system, we also want to be able to leverage
all of the capability of a system, the full system's memory capacity.
You know, we are, in general, along those lines also looking at acceleration in the machine
learning space, and we have some work that we've, you know, published and shown some really
great results on algorithms that are a bit more chatty between the GPU and the CPU and
how they leverage that NV link capability, that fat bandwidth pipe of communication between
the CPU and GPU and how they kind of get in and out of memory and, you know, use the larger
capability of the CPU memory.
And so those are a couple of different examples, but we're really trying to kind of use the
system to the max of its capability and tackle problems faster and enable people to explore
problems that are larger.
What's an example or some examples of, you know, particularly chatty machine learning
tasks or libraries?
Yeah, so in general, things like nearest neighbor computations, word-to-back technology
or glove, things that are used in semantic analysis, those are a couple of different types
of, you know, non-very deep neural network kind of things.
And the chattyness relates to, in those network architecture is kind of the way that state
needs to be shuffled around.
Yeah, so it relates to how state needs to be shuffled around, but it also relates to
how quickly the compute happens and, you know, how quickly the GPU needs to be supplied
with more data, for example.
So that's another big component in it.
The faster the compute on the GPU happens, the more likely it is that you need to, you
know, be feeding it data more quickly and getting that data fed to it more quickly can help
it just kind of be brick-walled in its execution time and not sitting around waiting for, you
know, the next data to get to it.
And sort of actually profiling the execution of your, you know, your training jobs.
How do you, you know, how do you develop an intuition for what's going to be chatty?
Like, it's, it doesn't sound like it's just the depth of the network and the deep
it is the, you know, the harder it is or is it, you know, or there are there, you know,
is it, you know, is it with?
Is it something else?
Is it kind of, you know, memory features?
Is it, are there things that, you know, you can look for it to get a sense for the kind
of the chattyness of your network architecture?
We like to talk about kind of the algebra of how deep learning happens or the pipeline
stages.
And we look at, you know, how much data is needed in order to start a computation on the
GPU?
How long does it take to move that data from, you know, the storage and then into the
GPU?
And then how long does the GPU take to communicate, sorry, to compute?
How long does the GPU take to compute?
And then how long does it take to communicate results, either back to the CPU or back to
other GPUs?
So those are kind of the canonical pieces.
And so we look at, you know, how much compute is there and how long will that take?
And then we look at how much data has to be moved.
And we look at those as clear, you know, kind of pipeline stages or phases of the overall
work you're trying to get done.
But it sounds like those are, again, kind of empirical observations of a training task
as opposed to, you know, being able to look at a picture of a network and tell by some
characteristic of the network that, oh, this is probably going to be, you know, chatty
and this kind of technology will apply well.
Yeah, I mean, it's a good question.
I mean, I think that we do have the ability to estimate pretty well based on the characteristics
of the neural network and its data extents.
I think it's, you know, it's definitely a mix of observation and the empirical characteristics.
But, you know, in general, from from looking at the neural network, you should be able
to figure out the, you know, volume of data, for example, that that needs to be moved
to do weight updates, you know, how many nodes are in it and all that other kind of stuff.
And there is, you know, some, you know, you have to figure out what the mini batch sizes
are that are appropriate for that.
So it's probably, I guess the, it's probably a mix of some things that can be determined,
but then other things that you have to know what the training characteristics of that
are and some of that is still somewhat experimental today.
Okay.
So what are you, you know, when you think about the, you know, this kind of work and all
of the, you know, you're working on it, there are other folks working on it.
Like, what do you see is the impact overall, you know, of it and, you know, what, you
know, what's the, what's the time frame?
Do you have a kind of a crystal ball vision around, you know, how this, you know, impacts
the way folks develop deep learning models?
Yeah, you know, it's, it's interesting because I think that there's been a lot of, you
know, concern, as you mentioned, that, you know, folks are overfitting image net.
And the question really is the rate and pace that deep learning will take off on other
types of data.
I mean, it's been proven as highly successful for image and speech, but we're seeing so
many other use cases happen.
You know, we're seeing use cases across, you know, risk and fraud and predictions and
forecasting and lots of different areas that, you know, to some extent, the same thing is
happening where there are classical machine learning techniques that are being subsumed
by the capabilities and sort of the ease of use of deep learning.
And the thing that excites me in this, in this capability conversation around DDL is
the thought that with this type of speed up that we will see those fields and those use
cases develop much more quickly because with a more productive system and a more productive
hardware and software solution, people will be able to discover and explore their data
and define the right models for those data sets more quickly, right?
So if, if, you know, if you are able to get through more of your data more quickly and
you are able to turn through more models and get to higher accuracy on these new types
of outcomes that you're looking for around like I said, you know, risk and fraud and predictions
and forecasting and those things, then those fields will develop and mature and those use
cases will develop mature that much more quickly, right?
And so, you know, this as an inflection point and kind of the rate and pace of enterprises
ultimately, you know, to apply deep learning and increase their confidence in it and increase
the accuracy and things that aren't just images and speech is ultimately what we're really
excited about.
And again, that, you know, that goes back to why we wanted to get it out into the hands
of customers because as a productivity enabler, and we hope that it will change that rate
and pace of, you know, adoption of these techniques and a productivity of data scientists
teams working on their own data sets.
Yeah, I think that's a really important point and one that I hear a lot in terms of,
you know, deep learning becoming deeply, you know, pun intended, I guess associated with
kind of image types of data, but there being these much broader applications and implications,
are there any particular use cases that, you know, you think of as, you know, kind of
the next big killer app for deep learning?
You know, I think it's hard to necessarily forecast the next big killer app.
I mean, I would say that, you know, we're seeing more than you might imagine use cases across
enterprise environments of even speech and image because people want to be able to interact
more, interact with us in a more natural way and such.
So I would say that, you know, kind of two things, one, the speech and image, I think, is
taking off and in other industries that have other data types, but that want to use more
natural interaction capabilities and such.
And so that's one thing that, you know, is great to see is the little bit more creative
use, you know, in other contexts, especially as relates to, you know, interacting with
people.
The, you know, beyond just, you know, talking to your mobile phone, for example, right?
So that's one thing.
And then I think, you know, there's a lot of discovery and exploration right now.
I think it's really hard to make a call is to, you know, what the next killer app and
what the next data set is.
But I think we are seeing a good degree of productivity on a lot of different types of data.
And, you know, they're very interesting things happening as well in, you know, automation
techniques.
We have a, we have a tool set that helps to label data and other things like that.
So as, as more folks adopt those techniques, you know, they'll be able to kind of get access
to use of deep learning on more data sets.
So as we use automation and, and tool sets like we have an AI vision to enable more quick
labeling of data, then, you know, that really should also, you know, help accelerate the
rate at which people enter into these new spaces.
Hmm.
The tool set you mentioned that helps with data annotation.
Is that open source?
It's not open source, but it's also available through the Power AI frameworks and folks can
try it on the Nibbix cloud.
They have the Power AI stuff available there.
So yeah, that's, that's available for, for people to try.
Yeah, it seems like that is also, you know, that clearly comes up constantly in these kind
of conversations and, you know, when I think about the, you know, the full life cycle of,
you know, these AI deep learning development projects, you know, there's certainly a lot
of emphasis on the deep learning framework and the model development and training.
But, you know, there's a much broader set of activities that has to take place and
very, you know, we're just starting to see projects come on, you know, open source projects
come online to provide, you know, a broader platform for handling all that.
Like the data annotation is one big area, kind of model life cycle and versioning and,
you know, production, performance management and tracking is another, you know, whole set
of areas, you know, any thoughts on how all this stuff evolves?
Yeah, I think that the, this is about maturity of the field of deep learning and it's about,
you know, people moving past just fully labeled public data sets that they, you know, get
online, right?
And so I think, you know, the overall topic of management, of management of, you know,
what models are being done when we're in how overall life cycle, integration with various
data sources, all those kind of things are what we see as being necessary from an enterprise
environment perspective because people will be, you know, wanting to improve the AI specific
to their use cases and specific to their, to their data.
So we have a good number of IBM tool sets out there today, again, folks can try them.
But the data science experience is a really nice Jupyter Notebooks based environment that
has a lot of the features we just discussed.
You know, we also have other tool sets that are available that help with, you know, data
integration from various sources through our spectrum family, for example.
And then in, you know, within Power AI, you know, we have that integrated as well with,
with the data science experience, the DSX that I just mentioned, and we have additional
capabilities that include the, you know, sort of clicker data scientist type capabilities,
including you're on data labeling and choosing your models and, you know, starting with some
default models, other things like that.
So we see these things, you know, that you mentioned as, you know, absolutely essential
to do, you know, kind of more mature deep learning, which is, you know, going beyond,
you know, just the, you know, the fully-lated label data sets that we have in the public
domain.
Right. And certainly enterprises need to, you know, need to get there in terms of maturity,
you know, perhaps to kind of wrap things up.
Any words for enterprises that are earlier on in the cycle and just getting started?
Yeah, I think that one of the things that I always like to try to clarify in these discussions
is that, you know, our intention in showing distributed deep learning capabilities at
256GPUs was to show the capability of what you can get if you do hardware and software
optimization, to show the flexibility across frameworks of what we have created, et cetera,
and put it in people's hands.
But everything that we did was done with kind of that enterprise consumer in mind, knowing
that many, you know, may also be early in their journey. And so the, the DDL environment
supports any number of notes, any number of systems that someone would like to use it
on. It supports different types of networks. You don't have to use what we published our
results on. And so, you know, what we really want to do is to kind of start with where people
are at, start with the data volume that they have, start with the, you know, neural network
capabilities that they have, and provide that ability to grow over time, you know, as they
hit that stride of productivity of really figuring out how they want to do their deep learning
and they want to apply it to the rest of their data sets, as they discover new areas within
their enterprise that they want to apply deep learning and replace classical techniques.
Perhaps they can grow out, they can scale out their deep learning environment, add more
systems to it, and still get that, you know, kind of productivity. So I think, you know,
that's, that's always kind of important to point out because it's not just the taking
the really long day-as-long jobs down to hours for use of a huge system. It's that no
matter what the system size is, you know, no matter what the data set size is, we want
to try to meet people where they're at and provide, you know, this flexibility and elasticity
and capability. Great, great. Well, hello, thank you so much. I really appreciate you taking
the time to chat with us. I learned a ton about what you're doing with DDL and I'm looking
forward to following the effort. It was a pleasure talking to you. Thanks so much for the opportunity.
Alright, everyone, that's our show for today. Thanks so much for listening and for your continued
feedback and support. For more information on Hillary or any of the topics covered in this
episode, head on over to twomlai.com slash talk slash 77. To follow along with this AI Summit
series, visit twomlai.com slash AI Summit. Of course, you're encouraged to send along your
feedback or questions to us by leaving a comment right on the show notes page or via Twitter
to at twomlai or at Sam Charrington. Thanks again to IBM Power for their support of this series.
For more about the IBM Power Systems platform for Enterprise AI, visit twomlai.com slash IBM Power.
Thanks once again for listening and catch you next time.
