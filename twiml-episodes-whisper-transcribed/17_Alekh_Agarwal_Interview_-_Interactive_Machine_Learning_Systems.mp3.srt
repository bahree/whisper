1
00:00:00,000 --> 00:00:19,320
Hello and welcome to another episode of Twimble Talk, the podcast rye interview interesting

2
00:00:19,320 --> 00:00:24,240
people doing interesting things in machine learning and artificial intelligence.

3
00:00:24,240 --> 00:00:27,560
I'm your host Sam Charrington.

4
00:00:27,560 --> 00:00:32,280
Once again, thanks so much to everyone who sent in their favorite quote from last week's

5
00:00:32,280 --> 00:00:33,280
podcast.

6
00:00:33,280 --> 00:00:35,440
Your stickers are on the way.

7
00:00:35,440 --> 00:00:39,360
We've had a blast receiving and reviewing all of your quotes.

8
00:00:39,360 --> 00:00:43,280
Don't forget to send us your favorite quote from today's show as well because this contest

9
00:00:43,280 --> 00:00:46,600
will continue while our sticker supply lasts.

10
00:00:46,600 --> 00:00:51,960
You can do that via a comment on the show notes page or a comment or post on our Facebook,

11
00:00:51,960 --> 00:00:55,280
Twitter, YouTube or SoundCloud pages.

12
00:00:55,280 --> 00:00:59,320
If you've been listening to the podcast for a while, you know that I really enjoy hearing

13
00:00:59,320 --> 00:01:04,520
from listeners and I appreciate all of the comments and feedback I get from all of you.

14
00:01:04,520 --> 00:01:09,280
Now I haven't mentioned this in a while, but one of the most important ways you can provide

15
00:01:09,280 --> 00:01:12,600
feedback is through an iTunes review.

16
00:01:12,600 --> 00:01:17,760
According to our stats, the vast majority of our listeners come from iOS and iTunes

17
00:01:17,760 --> 00:01:23,720
and many of them, one can only suppose, find the podcast via the iTunes directory.

18
00:01:23,720 --> 00:01:26,640
That's why your reviews there are so important.

19
00:01:26,640 --> 00:01:30,640
The more and better they are, the more people will want to check out the podcast.

20
00:01:30,640 --> 00:01:34,160
So please take a moment to review the show on iTunes.

21
00:01:34,160 --> 00:01:38,480
We'll have our link to the iTunes page in the show notes so you can click right through

22
00:01:38,480 --> 00:01:43,760
from there and you don't need to be a regular iTunes user to leave a review.

23
00:01:43,760 --> 00:01:48,640
Of course, if iTunes just isn't your thing and you've got other ways you prefer to spread

24
00:01:48,640 --> 00:01:52,360
the word about the podcast, those are appreciated as well.

25
00:01:52,360 --> 00:01:55,600
That's so much for spreading the word.

26
00:01:55,600 --> 00:02:00,440
The interview you'll hear on today's show was recorded last fall at the O'Reilly AI

27
00:02:00,440 --> 00:02:03,160
conference in New York City.

28
00:02:03,160 --> 00:02:07,880
On the subject of that conference, it's returning to New York in June and this time around

29
00:02:07,880 --> 00:02:11,720
we'll be giving away passes to two lucky Twomo listeners.

30
00:02:11,720 --> 00:02:15,720
Stay tuned because that giveaway is coming soon.

31
00:02:15,720 --> 00:02:19,600
If you happen to be in New York City now, I want to call your attention to another event

32
00:02:19,600 --> 00:02:22,800
that'll be taking place, this one next week.

33
00:02:22,800 --> 00:02:29,600
That event is called the Future Labs AI Summit and it'll be held at NYU on Wednesday afternoon.

34
00:02:29,600 --> 00:02:35,240
Speakers at the event will include Facebook and NYU's Jan LaCoon and NYU's Gary Marcus.

35
00:02:35,240 --> 00:02:38,040
I'll drop a link to this one in the show notes.

36
00:02:38,040 --> 00:02:39,200
Check it out if you're in the city.

37
00:02:39,200 --> 00:02:41,200
It sounds like a great event.

38
00:02:41,200 --> 00:02:46,680
Finally, I'd like to take a minute to remind you to check out my upcoming event, the

39
00:02:46,680 --> 00:02:53,240
Future of Data Summit, which will be held May 15th and 16th in Las Vegas, Nevada.

40
00:02:53,240 --> 00:02:57,440
If you haven't already checked out the event, I really encourage you to take a look.

41
00:02:57,440 --> 00:03:01,240
The person I had in mind when I created this event is someone who's in a role where they're

42
00:03:01,240 --> 00:03:06,400
responsible for helping to chart an organization's data strategy, or someone who wants to be

43
00:03:06,400 --> 00:03:11,800
in that kind of role, or someone who needs to understand how all of this will come together.

44
00:03:11,800 --> 00:03:16,680
This isn't the place where we'll go deep on neural nets, pun intended, or the latest research

45
00:03:16,680 --> 00:03:17,680
paper.

46
00:03:17,680 --> 00:03:22,080
Rather, this is an interdisciplinary summit where you'll get to hear from and engage with

47
00:03:22,080 --> 00:03:26,960
experts presenting on various aspects of our data-centric future.

48
00:03:26,960 --> 00:03:31,920
You'll hear from a soft Iraqi, an expert in big data infrastructure at Intel, on the coming

49
00:03:31,920 --> 00:03:36,840
advances in hardware and what they'll allow us to do in machine learning AI analytics

50
00:03:36,840 --> 00:03:38,400
and more.

51
00:03:38,400 --> 00:03:43,880
You'll hear about how cloud, IoT, and big data shift the cybersecurity threat landscape,

52
00:03:43,880 --> 00:03:49,120
and how we can secure these systems from IBM's global executive security advisor, Diana

53
00:03:49,120 --> 00:03:50,120
Kelly.

54
00:03:50,120 --> 00:03:56,080
I'll be leading a discussion on data privacy and algorithmic ethics, with Accenture AI's

55
00:03:56,080 --> 00:04:01,000
Roman Shoudry and Ericsson's Jonathan King.

56
00:04:01,000 --> 00:04:05,760
Erics Samar, founder and CTO of Reconna, will talk about the role of AI in optimizing the

57
00:04:05,760 --> 00:04:09,680
enterprise data center, so-called AI ops.

58
00:04:09,680 --> 00:04:14,680
And Endeavor VR founder Amy Peck will talk about the emerging role of virtual and augmented

59
00:04:14,680 --> 00:04:17,000
reality in the enterprise.

60
00:04:17,000 --> 00:04:20,600
These are just a few of the speakers I've lined up for you and I'll be announcing more

61
00:04:20,600 --> 00:04:21,920
shortly.

62
00:04:21,920 --> 00:04:27,320
To learn more about the summit, visit twimmolai.com slash future of data.

63
00:04:27,320 --> 00:04:30,080
Now, about today's show.

64
00:04:30,080 --> 00:04:32,960
This week, my guest is Alec Agarwal.

65
00:04:32,960 --> 00:04:37,520
Alec is a researcher with Microsoft Research in New York City, where his work is focused

66
00:04:37,520 --> 00:04:40,000
on interactive machine learning.

67
00:04:40,000 --> 00:04:44,240
As I mentioned before, Alec and I recorded this show at the ORILE AI conference where

68
00:04:44,240 --> 00:04:50,720
he delivered a talk called interactive learning systems, why now and how.

69
00:04:50,720 --> 00:04:53,840
Interactive learning systems are different from traditional supervised machine learning

70
00:04:53,840 --> 00:04:58,120
systems and that they need to explore and learn from their environments.

71
00:04:58,120 --> 00:05:02,440
This is an exciting area of research and one that really interests me personally.

72
00:05:02,440 --> 00:05:06,640
In part because it incorporates concepts like active learning, reinforcement learning,

73
00:05:06,640 --> 00:05:08,920
contextual bandits and much more.

74
00:05:08,920 --> 00:05:12,760
If you're interested in this topic, when you're done with this show, you should listen

75
00:05:12,760 --> 00:05:17,160
to the show I did with Georgia Tech's Charles Isbell, if you haven't already.

76
00:05:17,160 --> 00:05:18,960
That was show number four.

77
00:05:18,960 --> 00:05:24,800
The notes for this show can be found at twimmolai.com slash talk slash 17.

78
00:05:24,800 --> 00:05:33,920
Now on to the show.

79
00:05:33,920 --> 00:05:34,920
Hey, everyone.

80
00:05:34,920 --> 00:05:42,320
I'm here with Alec Agarwal with Microsoft Research and we're here at day two of the ORILE AI

81
00:05:42,320 --> 00:05:43,320
conference.

82
00:05:43,320 --> 00:05:48,240
Alec just did a great presentation on interactive learning systems and he was kind

83
00:05:48,240 --> 00:05:52,880
enough to join us to talk a little bit about that presentation.

84
00:05:52,880 --> 00:05:55,760
Alec wanted to start off by introducing yourself.

85
00:05:55,760 --> 00:05:56,760
Yeah, sure.

86
00:05:56,760 --> 00:05:58,000
So I'm Alec Agarwal.

87
00:05:58,000 --> 00:06:02,840
I'm a researcher at Microsoft Research actually here in New York City.

88
00:06:02,840 --> 00:06:05,040
I've been here for four years prior to that.

89
00:06:05,040 --> 00:06:10,160
I was doing my PhD at UC Berkeley and my work really touches upon a lot of teams in AI but

90
00:06:10,160 --> 00:06:15,120
one that is particularly in machine learning but one that has particularly been off

91
00:06:15,120 --> 00:06:18,760
interest lately is what I call interactive machine learning.

92
00:06:18,760 --> 00:06:26,960
So think that thing about problems where the machine learning algorithm is not just learning

93
00:06:26,960 --> 00:06:32,560
from a static pool of data that was hand annotated and collected by somebody else but think about

94
00:06:32,560 --> 00:06:37,840
really how the algorithm has to interact within a larger system, within a larger environment

95
00:06:37,840 --> 00:06:44,040
to collect that data to gather learning cues and then incorporate learning cues into

96
00:06:44,040 --> 00:06:52,400
the model in order to improve over time and just leads to several kind of paradigms in machine

97
00:06:52,400 --> 00:06:58,520
learning, things like active learning, reinforcement learning or you know, subsets of reinforcement

98
00:06:58,520 --> 00:07:03,640
learning like contextual bandits and so on, all of which I worked on and were touched

99
00:07:03,640 --> 00:07:05,280
upon in the talk today.

100
00:07:05,280 --> 00:07:06,280
Great, great.

101
00:07:06,280 --> 00:07:12,000
So you mentioned the machine learning, learning from the environment and one of the ways you

102
00:07:12,000 --> 00:07:17,960
illustrated that in your talk was, you showed a demonstration of a Super Mario Brothers

103
00:07:17,960 --> 00:07:18,960
game.

104
00:07:18,960 --> 00:07:23,640
Can you talk about what you were intending to show with that demo and?

105
00:07:23,640 --> 00:07:24,640
Yeah.

106
00:07:24,640 --> 00:07:30,160
So in some sense, whenever we are thinking about interactive learning systems, right?

107
00:07:30,160 --> 00:07:35,680
So one of the question is, what are environments in which we can safely run these experiments

108
00:07:35,680 --> 00:07:41,480
in which we can have this are basically algorithmic agents interact with their environment or

109
00:07:41,480 --> 00:07:43,560
manipulate the environment in a safe manner.

110
00:07:43,560 --> 00:07:48,600
Of course, you know, the natural or maybe canonical embodiment even in everybody is thinking

111
00:07:48,600 --> 00:07:52,400
of such agents are robots, right?

112
00:07:52,400 --> 00:07:54,920
But robots are hard to program.

113
00:07:54,920 --> 00:07:59,960
It's hard to in fact, like control all of their sensors and actuators and it takes time,

114
00:07:59,960 --> 00:08:03,880
it takes resources even to get one.

115
00:08:03,880 --> 00:08:11,120
And so a lot of people have found designing agents for various sort of games, either computer

116
00:08:11,120 --> 00:08:13,680
games or even traditional board games, right?

117
00:08:13,680 --> 00:08:20,960
Like baggammon, chess, gold, now, and other such games as kind of more controlled environments

118
00:08:20,960 --> 00:08:28,120
in which we can still have this interaction either with another player or with the environment

119
00:08:28,120 --> 00:08:30,040
of the game.

120
00:08:30,040 --> 00:08:36,680
And we can run these experiments kind of over and over again with actually remarkably

121
00:08:36,680 --> 00:08:37,680
high throughput.

122
00:08:37,680 --> 00:08:42,200
And overclock these games, so it allows for very fast experimentation.

123
00:08:42,200 --> 00:08:51,720
And so that's kind of the reason why people have really gravitated towards using games

124
00:08:51,720 --> 00:08:52,720
in particular now.

125
00:08:52,720 --> 00:08:56,640
There is, you know, this Atari learning environment, which is basically using Atari games again

126
00:08:56,640 --> 00:09:00,840
as a platform to test interactive learning situations.

127
00:09:00,840 --> 00:09:05,560
And what I was trying to show in the particular Super Mario demo actually, so I should give

128
00:09:05,560 --> 00:09:08,480
a credit to Stefan Ross, whose work that video comes out of.

129
00:09:08,480 --> 00:09:16,520
So, you know, what they were trying to demonstrate was supervised learning is not adequate or a

130
00:09:16,520 --> 00:09:21,600
static pool of data is not adequate to do well in interactive learning problems because

131
00:09:21,600 --> 00:09:28,920
when you manipulate your environment, then if you act a certain way, you see situations

132
00:09:28,920 --> 00:09:32,800
that may or may not be present in your training data.

133
00:09:32,800 --> 00:09:39,560
So you know, if I know how to drive well, then I might not get into a car accident or at

134
00:09:39,560 --> 00:09:44,920
least not an obvious one, or I might not get stuck behind a slow driving car very often.

135
00:09:44,920 --> 00:09:49,480
And now, of course, you don't, if you don't see any data for how to recover from those

136
00:09:49,480 --> 00:09:53,680
errors, then, you know, your driving agent is in trouble.

137
00:09:53,680 --> 00:09:58,800
So that's the part I was trying to emphasize that when we try to do supervised learning

138
00:09:58,800 --> 00:10:04,720
in these interactive scenarios, then often our algorithms tend to make mistakes that we

139
00:10:04,720 --> 00:10:09,280
don't have in the training data and then they don't know how to recover from those.

140
00:10:09,280 --> 00:10:15,600
And so they get, you know, stuck in corners and they just fall maybe in pits and do all

141
00:10:15,600 --> 00:10:17,560
sorts of silly things that we just don't do.

142
00:10:17,560 --> 00:10:18,560
Right.

143
00:10:18,560 --> 00:10:22,560
So, walk us through how the approach is to interactive learning that you talked about

144
00:10:22,560 --> 00:10:23,560
addressed that problem.

145
00:10:23,560 --> 00:10:29,680
Yeah, so in some sense, here's an alternative we could think about, right?

146
00:10:29,680 --> 00:10:36,720
So let's say I was thinking about a conversational agent like a chatbot.

147
00:10:36,720 --> 00:10:37,720
So I have two options.

148
00:10:37,720 --> 00:10:42,480
I could look through many, many transcripts of, you know, how people have talked to

149
00:10:42,480 --> 00:10:47,240
each other, maybe in a controlled domain, even like a call center or something.

150
00:10:47,240 --> 00:10:50,480
And I can try to train an agent from this data.

151
00:10:50,480 --> 00:10:55,040
Again, this might have issues that maybe I said something that doesn't make sense to

152
00:10:55,040 --> 00:11:02,080
the user, maybe the respond in a way that a call center human agent would never encounter

153
00:11:02,080 --> 00:11:04,720
and I don't know how to respond now.

154
00:11:04,720 --> 00:11:09,760
Now imagine in this situation, instead of just being stuck and floundering, I could

155
00:11:09,760 --> 00:11:14,800
actually sort of fall back to a human agent and say, hey, I'm kind of uncertain about

156
00:11:14,800 --> 00:11:15,800
what to do right now.

157
00:11:15,800 --> 00:11:17,320
Can you bell me out, please?

158
00:11:17,320 --> 00:11:23,040
Right now I have learned how to recover from this mistake.

159
00:11:23,040 --> 00:11:27,120
So in future, even if I'm about to make this mistake, I'll be able to recover from it,

160
00:11:27,120 --> 00:11:28,120
right?

161
00:11:28,120 --> 00:11:36,440
And so there is this concept of learning in situations that I encounter that interactive

162
00:11:36,440 --> 00:11:40,120
learning algorithms usually embody, right?

163
00:11:40,120 --> 00:11:46,440
And another kind of thing that often comes up is just even, you know, these are like

164
00:11:46,440 --> 00:11:48,040
very ambitious tasks, of course, right?

165
00:11:48,040 --> 00:11:55,480
So let's think about something even which seems much more mundane to us, like, you know,

166
00:11:55,480 --> 00:12:00,840
ranking search results or recommending news articles, personalizing news articles.

167
00:12:00,840 --> 00:12:06,000
So maybe I have data of what users have been clicking on and I learned from this data

168
00:12:06,000 --> 00:12:11,880
and I think I have found something better to choose, right?

169
00:12:11,880 --> 00:12:13,080
How do I evaluate it?

170
00:12:13,080 --> 00:12:17,560
Well, if this other, these other sort of choices are never displayed to the users by my previous

171
00:12:17,560 --> 00:12:22,880
system, I have no data that can actually back that these choices are good, right?

172
00:12:22,880 --> 00:12:27,560
And so we only know the performance of the thing that the user actually clicked on.

173
00:12:27,560 --> 00:12:30,920
We don't have any information about how the other things would have performed.

174
00:12:30,920 --> 00:12:31,920
Exactly.

175
00:12:31,920 --> 00:12:36,120
And so we run into this problem that we don't even know how to evaluate a system that does

176
00:12:36,120 --> 00:12:40,760
something different from what we have in the data and basically interactive learning

177
00:12:40,760 --> 00:12:47,480
exactly tries to break this gap or tries to come up with techniques for essentially how

178
00:12:47,480 --> 00:12:49,280
do you have to kind of learn on the job?

179
00:12:49,280 --> 00:12:52,440
You have to learn on the fly in order to address these situations.

180
00:12:52,440 --> 00:12:59,320
No, it seems like you've talked about two different things.

181
00:12:59,320 --> 00:13:03,400
One is, you know, you've got your machine prediction.

182
00:13:03,400 --> 00:13:07,880
How do we kind of pop out of that loop and get input from the human and the other makes

183
00:13:07,880 --> 00:13:13,640
me think more of like multivariate testing, AB testing, that kind of explore exploit kind

184
00:13:13,640 --> 00:13:14,960
of scenarios.

185
00:13:14,960 --> 00:13:15,960
How are those related?

186
00:13:15,960 --> 00:13:16,960
Yeah.

187
00:13:16,960 --> 00:13:27,440
So in some sense, explore exploit is kind of one step further from this, you know, like

188
00:13:27,440 --> 00:13:31,960
evaluation problem that I'm mentioning.

189
00:13:31,960 --> 00:13:38,880
If you think about exploration, why do you explore, right, you explore because sometimes

190
00:13:38,880 --> 00:13:42,280
you'll, you'll try one thing, sometimes you'll try another thing.

191
00:13:42,280 --> 00:13:47,880
So let's kind of in a statistical sense, if we zoom out and think about what's happening

192
00:13:47,880 --> 00:13:51,040
in expectation, roughly, you're trying everything.

193
00:13:51,040 --> 00:13:54,520
And in expectation, at least sort of as a population level, you're getting feedback

194
00:13:54,520 --> 00:13:59,880
about everything, right, and what, as a result, that enables you to do is it enables you

195
00:13:59,880 --> 00:14:03,640
to evaluate every choice that you could have made, right?

196
00:14:03,640 --> 00:14:09,280
So implicit in explore exploit is this ability to evaluate in interactive settings.

197
00:14:09,280 --> 00:14:13,720
And so what I try to do is I, I mean, explore exploit kind of in my mind consists of two parts.

198
00:14:13,720 --> 00:14:18,560
There is this ability to evaluate, which I think is very crucial in its own because people

199
00:14:18,560 --> 00:14:22,520
currently do often evaluation by doing, like you're saying, A, B testing or multivariate

200
00:14:22,520 --> 00:14:26,760
testing, which is a horribly inefficient way of going about the task.

201
00:14:26,760 --> 00:14:30,160
And explore exploit basically gives you a much more data efficient way of doing the same

202
00:14:30,160 --> 00:14:31,160
thing.

203
00:14:31,160 --> 00:14:34,800
And then it lets you, in fact, do something much more, it lets you actually refine your

204
00:14:34,800 --> 00:14:40,000
models in real time and update them and, you know, do this sort of really online learning,

205
00:14:40,000 --> 00:14:41,000
which is, which is great.

206
00:14:41,000 --> 00:14:44,720
But even if you don't want to do that, just this evaluation is something you can already

207
00:14:44,720 --> 00:14:45,720
get a lot of value out of.

208
00:14:45,720 --> 00:14:50,480
So that's why I kind of try to present both the pieces in their own right.

209
00:14:50,480 --> 00:14:57,440
But in some sense, yes, explore exploit is the is the general sort of overarching solution

210
00:14:57,440 --> 00:15:02,160
that addresses basically both of the issues.

211
00:15:02,160 --> 00:15:06,960
And why is it, why is it in that being so much more data efficient than the traditional

212
00:15:06,960 --> 00:15:07,960
purchase?

213
00:15:07,960 --> 00:15:08,960
Right.

214
00:15:08,960 --> 00:15:12,920
So there are two two important hallmarks.

215
00:15:12,920 --> 00:15:17,040
So so let's, let's first start from a non contextual setting, right?

216
00:15:17,040 --> 00:15:20,480
So let's just say you're trying to basically find the most popular new story that works

217
00:15:20,480 --> 00:15:22,800
for everyone.

218
00:15:22,800 --> 00:15:29,480
Then the a B testing way, so to say of going about it would be, let's say you have 10,

219
00:15:29,480 --> 00:15:33,360
you give one 10th of traffic to each one of them, week later, you see which one did the

220
00:15:33,360 --> 00:15:34,360
best.

221
00:15:34,360 --> 00:15:35,360
Okay.

222
00:15:35,360 --> 00:15:40,560
The explore exploit we are doing is you say, okay, well, initially I'll start giving 10%

223
00:15:40,560 --> 00:15:42,160
traffic to each one.

224
00:15:42,160 --> 00:15:46,280
The moment something starts to look better than the other second, I dynamically adjust

225
00:15:46,280 --> 00:15:47,280
my traffic.

226
00:15:47,280 --> 00:15:48,280
Right.

227
00:15:48,280 --> 00:15:52,840
So everything is getting only enough traffic that I need to rule it out as being inferior,

228
00:15:52,840 --> 00:15:53,840
right?

229
00:15:53,840 --> 00:15:54,840
And so that's already data efficient.

230
00:15:54,840 --> 00:15:55,840
All right.

231
00:15:55,840 --> 00:16:01,440
Now let's think about personalization, about contextualization, right?

232
00:16:01,440 --> 00:16:07,560
So so you said you're from St. Louis, you're hopefully not the only user from St. Louis

233
00:16:07,560 --> 00:16:09,960
who visits MSN.

234
00:16:09,960 --> 00:16:18,360
So so you visit, I, I maybe give you a randomized choice of new story.

235
00:16:18,360 --> 00:16:22,800
And maybe another user who has more or less the same features as you visits, right?

236
00:16:22,800 --> 00:16:26,280
I maybe try a different new story for them.

237
00:16:26,280 --> 00:16:29,680
Now what I've done is, she let's back up.

238
00:16:29,680 --> 00:16:36,560
So so so usually for personalization, let's again contrast with it with an a B test.

239
00:16:36,560 --> 00:16:43,320
So maybe test would say, okay, so, you know, I'll, I'll run this experiment to on, on some

240
00:16:43,320 --> 00:16:47,960
percent of data to evaluate, be on some percent of data to evaluate a, right?

241
00:16:47,960 --> 00:16:50,880
So each data point is either going to a or to be, right?

242
00:16:50,880 --> 00:16:53,400
There is no sort of data sharing happening.

243
00:16:53,400 --> 00:16:59,800
How, when you have a large class of available options, available models that you might want

244
00:16:59,800 --> 00:17:00,800
to pick out of.

245
00:17:00,800 --> 00:17:01,800
Right.

246
00:17:01,800 --> 00:17:08,440
If you've got tons of news, one set of parameters for neural network defining one model.

247
00:17:08,440 --> 00:17:10,000
So you really have an infinite number of them.

248
00:17:10,000 --> 00:17:13,960
But let's just pretend for now, they're a billion, okay?

249
00:17:13,960 --> 00:17:20,400
So of course, for a given user, many of these models would make the same choice, right?

250
00:17:20,400 --> 00:17:24,800
So if you randomize the new story that you present to the user, then and you look the,

251
00:17:24,800 --> 00:17:29,360
look at the outcome, then suddenly you have information about all the different models

252
00:17:29,360 --> 00:17:35,720
that made the story you displayed that made the same recommendation to this user, right?

253
00:17:35,720 --> 00:17:40,360
And you can use, share this user's information across all of them.

254
00:17:40,360 --> 00:17:44,960
So you're effectively training a billion models in parallel?

255
00:17:44,960 --> 00:17:48,080
You're effectively evaluating a billion models you parallel, right?

256
00:17:48,080 --> 00:17:51,560
And then because you're evaluating them in parallel, you can just choose the best one

257
00:17:51,560 --> 00:17:52,560
at the end.

258
00:17:52,560 --> 00:17:53,560
Right.

259
00:17:53,560 --> 00:17:54,560
Right.

260
00:17:54,560 --> 00:17:55,560
Well, you have optimization, right?

261
00:17:55,560 --> 00:18:00,800
So it's this data sharing that's crucial and there is a certain sense, there's a precise

262
00:18:00,800 --> 00:18:05,280
mathematical sense in which you can prove that this is, as a result, if you think about

263
00:18:05,280 --> 00:18:09,520
doing the same evaluation of a billion things through A, B testing versus what we call

264
00:18:09,520 --> 00:18:17,160
multi-world testing, then you require exponentially fewer samples in our approach.

265
00:18:17,160 --> 00:18:24,120
And there is kind of a precise theory in papers backing up that claim.

266
00:18:24,120 --> 00:18:26,120
So that's kind of the crux.

267
00:18:26,120 --> 00:18:27,120
Okay.

268
00:18:27,120 --> 00:18:28,120
Okay.

269
00:18:28,120 --> 00:18:33,560
So we've touched on this throughout the discussion, but one of the big areas that you apply

270
00:18:33,560 --> 00:18:35,760
this on is impersonalization.

271
00:18:35,760 --> 00:18:39,720
Can you talk a little bit about the use case and some of the background there?

272
00:18:39,720 --> 00:18:40,720
Yeah.

273
00:18:40,720 --> 00:18:49,520
So, you know, I think really, I mean, you know, it's 2016, I think it's great that some

274
00:18:49,520 --> 00:18:54,560
of the things we use actually do adapt to our tastes over time, but it's a pity that more

275
00:18:54,560 --> 00:18:55,880
of them don't, right?

276
00:18:55,880 --> 00:19:00,960
And I feel like everything I do should learn about me, especially because they are collecting

277
00:19:00,960 --> 00:19:01,960
a ton of data about me.

278
00:19:01,960 --> 00:19:05,600
So, it might as well put it to some good.

279
00:19:05,600 --> 00:19:12,320
And basically, I think this is, personalization is the one that's most interesting to me also

280
00:19:12,320 --> 00:19:17,640
is because like I was describing, once you start thinking, if you're just trying to

281
00:19:17,640 --> 00:19:22,400
evaluate 10 things, fine, there is some difference between A, B testing and multiple testing, but

282
00:19:22,400 --> 00:19:28,240
it's really when you start thinking about creating personalized, and if you're only doing

283
00:19:28,240 --> 00:19:32,640
10 things, then in some sense, a very smart person might just look at them and by their

284
00:19:32,640 --> 00:19:35,520
gut feeling, pick out the best one, right?

285
00:19:35,520 --> 00:19:40,400
But once you start thinking about personalized models, then first of all, it doesn't scale

286
00:19:40,400 --> 00:19:41,800
for humans to do it.

287
00:19:41,800 --> 00:19:43,840
It doesn't scale for A, B testing to do it.

288
00:19:43,840 --> 00:19:50,240
You really need a different technology to do it, and that's why I think it's a very

289
00:19:50,240 --> 00:19:55,080
well-suited scenario to something like multiple testing and contextual bandits.

290
00:19:55,080 --> 00:20:00,840
And so, that's where we've found the most, also, excitement from, you know, we've

291
00:20:00,840 --> 00:20:07,960
talked to a broad range of product managers, and that's the aspect that usually receives

292
00:20:07,960 --> 00:20:11,840
the most appeal to them.

293
00:20:11,840 --> 00:20:14,160
And what kind of results are you seeing with that?

294
00:20:14,160 --> 00:20:18,640
So with, I mean, I have most substantive results with MSN.

295
00:20:18,640 --> 00:20:24,800
We have a lot of other kind of experiments going on, but I think MSN ones are the one

296
00:20:24,800 --> 00:20:29,760
I can speak of most authoritatively, and basically, they've reliably, so, you know, they

297
00:20:29,760 --> 00:20:34,760
have like this web page with many different A, which they kind of logically partition

298
00:20:34,760 --> 00:20:39,280
into many different areas, so they've kind of applied our system to many different

299
00:20:39,280 --> 00:20:46,280
areas now, and reliably in all of those, they found kind of with minimal to no tuning,

300
00:20:46,280 --> 00:20:54,320
they found always that they were getting actually improvement in most kind of user engagement

301
00:20:54,320 --> 00:20:56,960
metrics that they track.

302
00:20:56,960 --> 00:21:01,720
What was even more interesting, and this kind of really reflects the personalization aspect,

303
00:21:01,720 --> 00:21:02,720
right?

304
00:21:02,720 --> 00:21:07,600
So they had been running these experiments in the US market, and then the Olympics came

305
00:21:07,600 --> 00:21:10,880
around, and somebody had an idea, hey, let's try this in Brazil, right?

306
00:21:10,880 --> 00:21:17,520
So this Portuguese, we've not changed anything in the system, except they, you know, like

307
00:21:17,520 --> 00:21:23,000
the user browsing history, they had it for those users on the Portuguese articles, of course,

308
00:21:23,000 --> 00:21:27,240
and they did like a Portuguese topic model for, so just the feature extraction part was

309
00:21:27,240 --> 00:21:33,120
a little language specific, nothing about the machine learning changed, and we got, you

310
00:21:33,120 --> 00:21:38,560
know, double to triple digit improvement over the existing system, just deploying this

311
00:21:38,560 --> 00:21:45,760
out of the box, and they started running it on 100% traffic, so this, it really does work.

312
00:21:45,760 --> 00:21:50,600
Is that level of improvement on par with what you saw in English, or greater, or slightly

313
00:21:50,600 --> 00:21:51,600
less?

314
00:21:51,600 --> 00:21:57,200
You know, that's kind of difficult to judge very well, because also they have different

315
00:21:57,200 --> 00:22:02,680
amounts of data and different level of performance of the baseline system in different markets,

316
00:22:02,680 --> 00:22:07,640
so it's a bit hard to compare that.

317
00:22:07,640 --> 00:22:14,120
But I think the important thing for them was that it was really with very little customization

318
00:22:14,120 --> 00:22:21,040
things were robustly able to transfer from one market to another, right?

319
00:22:21,040 --> 00:22:28,760
And so this approach, you've talked a little bit about it being more data efficient,

320
00:22:28,760 --> 00:22:34,520
but how does that translate to the actual implementation and computational efficiency?

321
00:22:34,520 --> 00:22:39,520
Yeah, so, I mean, is that an issue for these kinds of problems at that scale, or?

322
00:22:39,520 --> 00:22:44,160
I mean, there, yes, you have to be mindful about computation, we are mindful about computation,

323
00:22:44,160 --> 00:22:49,560
we've tried to make these things as efficient as possible, so with MSN, for instance, we

324
00:22:49,560 --> 00:22:54,840
were running in, you know, the front end of their servers, and we had like five percent

325
00:22:54,840 --> 00:22:59,160
of show performance overhead on their system, which was deemed fine for the value, so there

326
00:22:59,160 --> 00:23:06,560
is obviously some performance that is lost, that is incurred, but I mean, we've implemented

327
00:23:06,560 --> 00:23:13,640
these algorithms very carefully, and a lot of the costs can be amortized essentially.

328
00:23:13,640 --> 00:23:18,760
So now the 5% running on the front end, that's for model evaluation, is there a training

329
00:23:18,760 --> 00:23:19,760
step?

330
00:23:19,760 --> 00:23:23,920
Yeah, so the nice thing is that training step is running asynchronously in the cloud,

331
00:23:23,920 --> 00:23:29,880
and that, I mean, that's entirely, again, I mean, currently, even at MSN, first of all,

332
00:23:29,880 --> 00:23:36,520
one thing that's nice about doing things online is, right, you're not really thinking about

333
00:23:36,520 --> 00:23:40,760
having to deal with a billion or a trillion records all at once, you're just streaming

334
00:23:40,760 --> 00:23:41,840
over them.

335
00:23:41,840 --> 00:23:47,480
So scales become nicer, but, but, so with MSN, we were still able to do all the training

336
00:23:47,480 --> 00:23:51,960
on one machine, in fact, in the background, but even if that's not the case, it's pretty

337
00:23:51,960 --> 00:23:55,960
easy to parallelize the training algorithms, and we support that.

338
00:23:55,960 --> 00:24:02,040
So, yeah, I mean, you can, and we've, the only thing you have to be careful about is, so

339
00:24:02,040 --> 00:24:07,960
we've made sure that, we try to keep the system very reproducible, because one of the frustrating

340
00:24:07,960 --> 00:24:11,440
things we've encountered over and over again is when something goes wrong in these complex

341
00:24:11,440 --> 00:24:16,280
systems, it's really hard to trace downward, with parallelization, sometimes it can be

342
00:24:16,280 --> 00:24:21,280
a little bit more tricky, because all of the order of events and so on, so we recommend

343
00:24:21,280 --> 00:24:25,320
as far as possible to avoid it, but it's definitely possible to do it.

344
00:24:25,320 --> 00:24:29,920
All right, if you can do it for MSN on one machine, then a lot of people will be able to get

345
00:24:29,920 --> 00:24:33,200
pretty far on a single machine.

346
00:24:33,200 --> 00:24:37,400
And you've published papers about this, like, what have someone wanted to, to try out this

347
00:24:37,400 --> 00:24:38,400
approach?

348
00:24:38,400 --> 00:24:40,400
Like, what's the best way to, for them to learn about it?

349
00:24:40,400 --> 00:24:50,000
Yeah, so there is a short URL, aka.ms-mwt, that website has a ton of resources on it, and

350
00:24:50,000 --> 00:24:58,000
it has both, you know, more, like, do it yourself, type, guide, type things, if you directly

351
00:24:58,000 --> 00:25:02,080
want to get your hands dirty, if you want to learn more about the science, it provides

352
00:25:02,080 --> 00:25:08,720
extensive, there's a very extensive white paper that provides links to you in more extensive

353
00:25:08,720 --> 00:25:14,080
research papers, and so on, so really, depending on how much detail you want, all of the resources

354
00:25:14,080 --> 00:25:15,680
are available on that website.

355
00:25:15,680 --> 00:25:16,680
Awesome, awesome.

356
00:25:16,680 --> 00:25:22,680
And the project itself is on GitHub, so, you know, all of the code is open source.

357
00:25:22,680 --> 00:25:26,280
You can play with the machine learning algorithms, the machine learning algorithms actually

358
00:25:26,280 --> 00:25:29,920
have been open source for several years now, so they've been, you know, tried out not

359
00:25:29,920 --> 00:25:33,720
just by us, but by others across the research community as well.

360
00:25:33,720 --> 00:25:36,960
And what are the algorithms based on?

361
00:25:36,960 --> 00:25:40,880
Like, what general classes of algorithms do these look like?

362
00:25:40,880 --> 00:25:48,800
So, I mean, so the way we do things is we take these interactive or, you know, contextual

363
00:25:48,800 --> 00:25:54,680
bandit learning problems, and we basically sort of what the learning algorithms look like,

364
00:25:54,680 --> 00:25:59,160
their massage and massage and massage the data, till you can essentially think of it as

365
00:25:59,160 --> 00:26:02,320
some sort of a multi-class classification problem.

366
00:26:02,320 --> 00:26:03,320
Okay.

367
00:26:03,320 --> 00:26:07,160
And so, now, go to party with your favorite multi-class classification algorithm.

368
00:26:07,160 --> 00:26:14,200
I mean, we implement our own for the complete pipeline, but, and we support a lot of different

369
00:26:14,200 --> 00:26:19,960
types of models, like, you know, linear models, shallow, feed-forward nets.

370
00:26:19,960 --> 00:26:23,480
We have, you know, matrix factorization type models.

371
00:26:23,480 --> 00:26:29,400
We have a whole variety of sort of very, very quick feature manipulations that are ingrained

372
00:26:29,400 --> 00:26:33,680
into the models that you can just do, so it's, so all of this is happening in a software

373
00:26:33,680 --> 00:26:38,320
called Wobbit, the all of the machine learning part, which has been around for several years

374
00:26:38,320 --> 00:26:43,960
and is one of the more performant software tools for machine learning out there.

375
00:26:43,960 --> 00:26:51,720
So, it's, it, it provides a lot of functionality, and if you want something that's not in it,

376
00:26:51,720 --> 00:26:57,920
then, you know, there are ways to plug into other machine learning libraries as well.

377
00:26:57,920 --> 00:27:04,640
And the, your use case there was on personalization, are there other use cases that you've seen

378
00:27:04,640 --> 00:27:06,840
to supply to?

379
00:27:06,840 --> 00:27:13,320
So, I mean, it depends on how far you want to extend personalization, the definition, right?

380
00:27:13,320 --> 00:27:20,720
So, one of the things we are currently trying to work on is, so, so we have users of Microsoft

381
00:27:20,720 --> 00:27:27,720
Bank and, you know, a lot of people in this country suffer from sleep disorders or just, you

382
00:27:27,720 --> 00:27:28,880
know, is the health band?

383
00:27:28,880 --> 00:27:29,880
Yes.

384
00:27:29,880 --> 00:27:34,600
Because of like stress or other reasons, they're just not sleeping well.

385
00:27:34,600 --> 00:27:40,240
And so, the band was for a while trying to give sleep insights, basically some, some

386
00:27:40,240 --> 00:27:47,400
recommendation to change your lifestyle in some small way that might help you sleep better.

387
00:27:47,400 --> 00:27:53,400
And so, for instance, we are trying to now do an experiment where we would choose which

388
00:27:53,400 --> 00:27:58,880
recommendations to show based on how the users sleep then respond to the recommendations

389
00:27:58,880 --> 00:27:59,880
or do this.

390
00:27:59,880 --> 00:28:05,880
So, I mean, I think of this as within the realm of personalization, but, you know, and,

391
00:28:05,880 --> 00:28:14,720
I mean, again, we haven't had conversations or more medical domain so far, but we are

392
00:28:14,720 --> 00:28:18,120
really hoping that we can get there in future.

393
00:28:18,120 --> 00:28:27,120
So, that's definitely one realm, other definitely, definitely a good chunk of the conversations

394
00:28:27,120 --> 00:28:32,520
we've had other than that are, I would say, around personalization of various sorts.

395
00:28:32,520 --> 00:28:37,360
But even like one of the interesting use cases, some of our actually, so some of the people

396
00:28:37,360 --> 00:28:42,280
in our research team are, so because of the system, we also have some systems researchers

397
00:28:42,280 --> 00:28:50,760
on the team, and, you know, system itself does a number of resource allocation choices

398
00:28:50,760 --> 00:28:57,800
and, you know, server is kind of distributing, doing load balancing and a lot of other resource

399
00:28:57,800 --> 00:28:59,200
allocation problems, right?

400
00:28:59,200 --> 00:29:05,320
So, what they've been curious is if they can apply even some of these techniques to core

401
00:29:05,320 --> 00:29:07,800
systems problems.

402
00:29:07,800 --> 00:29:13,400
We have some preliminary experiments with that, nothing I would call convincing yet, but

403
00:29:13,400 --> 00:29:17,920
it's actually pretty broadly applicable.

404
00:29:17,920 --> 00:29:23,960
And by core systems problems, are you thinking things like, you know, allocation of resources

405
00:29:23,960 --> 00:29:24,960
within a data center?

406
00:29:24,960 --> 00:29:30,720
Yeah, you can apply it at several different scales, so you can think about applying it

407
00:29:30,720 --> 00:29:35,640
at the level of a router, at the level of a NIC, at the level of an OS, level of data center

408
00:29:35,640 --> 00:29:38,920
or scale, do you learn in a data center, there are many different places you can think

409
00:29:38,920 --> 00:29:39,920
of it.

410
00:29:39,920 --> 00:29:46,560
And in some sense, I mean, a lot of these are basically currently working on top of hand-designed

411
00:29:46,560 --> 00:29:50,000
rules that some very smart people thought about the problem very carefully and designed

412
00:29:50,000 --> 00:29:51,000
it, right?

413
00:29:51,000 --> 00:29:55,520
But there's no reason why we can't make them more adaptive and more intelligent.

414
00:29:55,520 --> 00:29:59,840
So yeah, I think there is definitely a lot of potential there in like machine learning

415
00:29:59,840 --> 00:30:04,920
for systems type of area for these interactive learning situations.

416
00:30:04,920 --> 00:30:08,160
And so how would you kind of taking a step back to summarize?

417
00:30:08,160 --> 00:30:12,520
How would you characterize at the highest level, you know, if you've got a problem that

418
00:30:12,520 --> 00:30:15,560
looks like X, you know, this is, you know, a solution.

419
00:30:15,560 --> 00:30:16,560
Wonderful question.

420
00:30:16,560 --> 00:30:21,960
I actually do, when I start to talk to people who are interested, I usually give them a template

421
00:30:21,960 --> 00:30:24,800
and ask them if their problem fits into that template, right?

422
00:30:24,800 --> 00:30:30,200
So at the high level, there is this loop of, you observe the world, you take an action

423
00:30:30,200 --> 00:30:33,440
and you observe a reward, right?

424
00:30:33,440 --> 00:30:36,800
So it's important that you face this loop over and over again.

425
00:30:36,800 --> 00:30:41,800
One of the things you have to be careful about, for instance, is often when we start this

426
00:30:41,800 --> 00:30:45,840
conversation, people don't necessarily have a well-defined notion of a reward they can

427
00:30:45,840 --> 00:30:46,840
point to.

428
00:30:46,840 --> 00:30:47,960
And that's very important.

429
00:30:47,960 --> 00:30:52,760
If we don't define it well, the system will just learn some garbage, right?

430
00:30:52,760 --> 00:30:56,760
The other thing that's kind of important to think about is, like I said, contextual bandage

431
00:30:56,760 --> 00:31:02,920
problem, I was saying in the top, makes this assumption that when I take an action, it

432
00:31:02,920 --> 00:31:06,000
does not have influence on the next context I see, right?

433
00:31:06,000 --> 00:31:11,280
So something like a conversation just does not fit this, you know, what you say is not

434
00:31:11,280 --> 00:31:15,000
independent of what I said before.

435
00:31:15,000 --> 00:31:21,440
But something like recommendation systems is largely true.

436
00:31:21,440 --> 00:31:25,680
So that's something you have to keep in mind when you're thinking about how good a fit

437
00:31:25,680 --> 00:31:27,000
this might be to your problem.

438
00:31:27,000 --> 00:31:35,120
Now, that said, we have a lot of research expertise and research advances in also working

439
00:31:35,120 --> 00:31:42,080
out the situations where your actions modify the state is just that they're the software

440
00:31:42,080 --> 00:31:43,080
not quite there yet.

441
00:31:43,080 --> 00:31:47,480
I mean, we have some software, but it's not really a full-fledged system yet.

442
00:31:47,480 --> 00:31:48,480
Okay.

443
00:31:48,480 --> 00:31:49,480
Great.

444
00:31:49,480 --> 00:31:50,480
Great.

445
00:31:50,480 --> 00:31:55,760
Any other considerations or things that folks should know when they're thinking about

446
00:31:55,760 --> 00:31:57,360
this space?

447
00:31:57,360 --> 00:32:02,400
No, I would say, again, think, if you're thinking about these problems, think really hard

448
00:32:02,400 --> 00:32:03,400
about the reward.

449
00:32:03,400 --> 00:32:05,600
That's the one that usually goes wrong.

450
00:32:05,600 --> 00:32:11,440
And the other thing is, I think we've tried very carefully in the various materials we've

451
00:32:11,440 --> 00:32:17,520
prepared to outline all the usual things that go wrong because one of the things we find

452
00:32:17,520 --> 00:32:23,720
is even after we talk to people, they often fall back into those traps, so it's very important

453
00:32:23,720 --> 00:32:27,560
to think through those carefully and make sure you don't fall into them.

454
00:32:27,560 --> 00:32:29,720
And what are some of those traps?

455
00:32:29,720 --> 00:32:35,720
So a lot of those traps are, well, essentially, even, I mean, it's really tempting to say that

456
00:32:35,720 --> 00:32:39,520
I have some observational data collected from my system.

457
00:32:39,520 --> 00:32:42,000
Let's just do some machine learning with it.

458
00:32:42,000 --> 00:32:45,400
And this almost never works in a reliable manner.

459
00:32:45,400 --> 00:32:47,840
And there are various levels at which this manifests.

460
00:32:47,840 --> 00:32:52,040
So one thing you might want to do is, oh, I ran this experiment and actually things are

461
00:32:52,040 --> 00:32:53,040
working quite well.

462
00:32:53,040 --> 00:32:54,720
Now, why don't I turn off the experimentation?

463
00:32:54,720 --> 00:32:56,040
I turn off the randomization.

464
00:32:56,040 --> 00:32:57,040
No.

465
00:32:57,040 --> 00:33:09,480
I mean, you know, preferences change or just various, various subtle bugs arise just due

466
00:33:09,480 --> 00:33:11,800
to the way people are recording things.

467
00:33:11,800 --> 00:33:14,760
So of course, if you do everything with our system, then, you know, we've designed things

468
00:33:14,760 --> 00:33:21,160
in a way that they shouldn't arise, but often people want to use their own custom

469
00:33:21,160 --> 00:33:23,160
components for parts of things.

470
00:33:23,160 --> 00:33:28,920
So if you're thinking about doing that, it's really important that you look into the

471
00:33:28,920 --> 00:33:33,640
failure modes that we emphasize and make sure you don't fall into those.

472
00:33:33,640 --> 00:33:38,480
And the other thing is, yeah, even on top of our system, when you're building something,

473
00:33:38,480 --> 00:33:42,600
it's important to think about the reproducibility of everything, because that's the one thing

474
00:33:42,600 --> 00:33:46,120
we found really was key when even with MSN, right?

475
00:33:46,120 --> 00:33:52,480
It wasn't all sort of a bed of roses initially, there were quite a few hiccups and because

476
00:33:52,480 --> 00:33:56,240
we kept everything reproducible, we could quickly figure out where the problem was.

477
00:33:56,240 --> 00:33:57,240
Right.

478
00:33:57,240 --> 00:33:58,240
Right.

479
00:33:58,240 --> 00:33:59,240
Awesome.

480
00:33:59,240 --> 00:34:00,240
Awesome.

481
00:34:00,240 --> 00:34:01,240
But why don't you repeat that URL once more time?

482
00:34:01,240 --> 00:34:02,240
Yeah.

483
00:34:02,240 --> 00:34:04,240
It is aka.ms slash MWT.

484
00:34:04,240 --> 00:34:05,240
Okay.

485
00:34:05,240 --> 00:34:09,320
And can folks, if they've got questions, can they contact you through that URL?

486
00:34:09,320 --> 00:34:10,320
Yes.

487
00:34:10,320 --> 00:34:11,320
Absolutely.

488
00:34:11,320 --> 00:34:12,320
Awesome.

489
00:34:12,320 --> 00:34:13,320
Awesome.

490
00:34:13,320 --> 00:34:14,320
Well, thanks so much, Alex.

491
00:34:14,320 --> 00:34:15,320
Great to meet you and appreciate the talk.

492
00:34:15,320 --> 00:34:16,320
Yeah.

493
00:34:16,320 --> 00:34:17,320
I'm talking to you.

494
00:34:17,320 --> 00:34:18,320
Thanks.

495
00:34:18,320 --> 00:34:20,320
All right, everyone.

496
00:34:20,320 --> 00:34:22,440
That's our show for today.

497
00:34:22,440 --> 00:34:27,240
Once again, thanks so much for listening and for your continued support.

498
00:34:27,240 --> 00:34:31,040
Don't forget to share your favorite quotes for a twimmel sticker.

499
00:34:31,040 --> 00:34:32,040
These stickers are great.

500
00:34:32,040 --> 00:34:33,760
You're going to love them.

501
00:34:33,760 --> 00:34:38,560
You can share your favorite quote via the show notes page, via Twitter, via our Facebook

502
00:34:38,560 --> 00:34:42,480
page, or via a comment on YouTube or SoundCloud.

503
00:34:42,480 --> 00:34:45,920
And don't forget to hit that iTunes link and leave us a review.

504
00:34:45,920 --> 00:34:52,160
The notes for this show will be up on twimmelai.com slash talk slash 17, where you'll find links

505
00:34:52,160 --> 00:34:56,520
to Alex and the various resources mentioned in the show.

506
00:34:56,520 --> 00:35:12,640
Catch you next time.

