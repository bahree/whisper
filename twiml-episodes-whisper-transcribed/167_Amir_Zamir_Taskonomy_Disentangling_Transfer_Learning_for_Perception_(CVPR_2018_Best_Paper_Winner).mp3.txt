Hey everybody, Sam here. We've got some great news to share, and also a favorite to ask.
We're in the running for this year's People's Choice Podcast Awards, in both the People's Choice and Technology categories, and we would really appreciate your support.
To nominate us, you'll just head over to Twomlai.com slash nominate, where we've linked to and embedded the nomination form from the awards site.
There, you'll need to input your information and create a listener nomination account.
Once you get to the ballot, just find and select this week in machine learning and AI on the nomination list for both the Adam Curry People's Choice Award and the this week in tech technology category.
As you know, we really, really appreciate each listener and would love to share in this accomplishment with you.
Remember, that url is twomlai.com slash nominate. Feel free to hit pause and take a moment to nominate us now.
Hello and welcome to another episode of Twomlai Talk, the podcast why interview interesting people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington.
Consider this your final update for tomorrow's Twomlai online meetup.
On Tuesday, July 17th at 5 p.m. Pacific time, Nick Teague will lead a discussion on the paper Quantum Machine Learning by Jacob Biamante and his co-authors,
which discusses how to devise and implement concrete quantum software for machine learning tasks.
Visit twomlai.com slash meetup for more details or to sign up.
In this episode, I'm joined by Amir Zameer, postdoctoral researcher at both Stanford and UC Berkeley.
Amir joins us, fresh off of winning the 2018 CVPR Best Paper Award for co-authoring taskonomy, disentangling task transfer learning.
In this work, Amir and his co-authors explore the relationships between different types of visual tasks and use this structure to better understand the types of transfer learning that will be most effective for each.
Resulting in what they call a computational taxonomic map for task transfer learning.
In our conversation, Amir and I discussed the nature and consequences of the relationships that he and his team discovered and how they can be used to build more effective visual systems with machine learning.
Along the way, Amir provides eight ton of great examples and explains the various tools his team has created to illustrate these concepts.
Now on to the show.
All right, everyone. I am on the line with Amir Zameer. Amir is a postdoc researcher with get this both Stanford and Berkeley. Amir, welcome to this weekend machine learning in AI.
Thank you. Thanks for having me.
Let's get started by having you tell us a little bit about your background and how you got involved in machine learning in AI and how you ended up with an appointment at both Stanford and Berkeley.
Sure. Yeah, I mean, regarding my background, I was always interested in map and physics, especially in middle school and high school that turned into interesting signal processing when I was in college.
And you know, one of the sensors that produced a signal from the world is camera. And that's the way I essentially got exposed to visual perception and making sense of this raw sensory data, like an image that we use on a daily basis.
But getting the computers to make sense of that struck me as a really fascinating problem. It appeared both simple and nearly impossible at the same time. And that was really the point that made it fascinating and challenging.
When you see an object, see a dog, when you walk, see something new, or you're at a particular location, and you know where you are, you can recognize objects, you can just do this without any particular difficulty.
And it can do with that actually a very young age, like children, they're just a few months old, they can solve pretty much all of these problems. So it appeared like a very simple problem that anyone can do. But when I was looking at the machinery that we have back then to get computers to do some of these very basic things that just detect an object or teleport with their like two things that you're looking at is the same thing or not basically matched them.
It looked far from solved. And that's where it all started from.
Wow. And we've certainly come a long way in that dimension. I remember taking a DSP class in grad school. The prof was focused on computer vision, a guy named Agalos Katsagalos at Northwestern. And the way we approached that problem now was very different.
Right. Yeah, that isn't the correct. A lot more machine learning isn't play than then, you know, when I was in early years of college, that's actually interesting.
And you know, a few years ago, just let's say, you know, five or seven years ago, we were doing a lot more hand crafting or solutions to rely on our intuition to solve some of these problems, like what makes an object, what makes a dog a dog. And you would extract some certain like, you know, low level features.
We would come up with your sticks for putting together some representation of that. And then maybe there would be some machine learning that you're sitting on the in on top and classifying them.
We have certainly think this scene has changed a lot since then.
For better, the judging by the metrics that we have in the field, we are doing a lot better than what we're doing back then. It doesn't mean the problem is nearly solved.
I have to admit that I have a better understanding of the complexity of this problem now than, you know, a few years ago, and I see as much on solve problems that it could see back then.
But the progress made is something that can't be denied.
When you look at the problem space and you see, as you said, as many on solve problems now is then the problems, the same in scope or texture or like do you have a mental model for text animizing the problems and how the problems have shifted over time.
Yeah, that's a good point.
The problems have shifted, you know, not trying to not get into too much details of it. I would basically abstract it this way is that, you know, a few years ago, we were defining a problem in terms of some input and output.
Let's say input was an image and output was like the objects that you're interested in or the input was an image and the output was let's say some 3D properties of the scene.
It's a layout or the scene or surface of the normal, the normal of the surface and so on.
So back then, what we were mostly struggling with was that we just couldn't solve this problem good enough.
Like object detection is a very good example. ImageNet, say looking at 2010, imagines the challenge is a pretty complex challenge.
There's like 1000, there's like 1 million images and 1000 objects, object categories, they need to classify these images into those or detect them.
So people were trying really hard. It was getting quite a bit of attention and energy.
So it's not like people weren't trying hard enough. A good part of the committee was trying, but in the end, you know, when you would look at the failure cases, they would look very simple.
You would detect a car in the sky or you would hallucinate certain animals just in the middle of the wall and so on.
We knew why they were happening, but did you know how to fix them?
So since then, the horizon of the problem have shifted. I think we have basically pushed down a little higher. And so at this point, after namely the deep learning wave happened in committee vision back in 2012, 2013, with Alex Kirchevsky at all paper and ImageNet, which significantly improved the performance.
So since then, there is this summary that, you know, more or less, a lot of people have in mind is that if you're interested in solving a particular problem, like problem X, you collect a large amount of data for it and you train a big comnet, these neural networks, that will probably give you satisfactory results.
It's a matter of throwing in more compute and throwing in more data, if you want to improve, and that's the only problem that you care about. So I think we have grown out of that local minima that we are just really interested in one problem.
We are trying to do anything we can to solve it, but it is still we're facing like difficult issues.
Right now, I think a lot of these things have changed. Like I said, for large part of the problems, if you can really clearly define the problem that you're interested in in terms of input and output, having a lot of compute, doing some search over different architectures that it can employ.
And, you know, if you can afford having a large data set for it, you can get a reasonable accuracy, probably something that would be useful to even turn into a product for users. And we have seen examples of that, like face detection or recognition and, you know, certain filters that use and Facebook and YouTube and so on, they're, they're using these tools.
I think the horizon now has shifted towards basically, okay, well, if we want to solve these problems, but if we don't have enough data for it, or we don't want to use an unreasonable amount of compute for it, or, you know, a little bit more futuristic is that like why are we solving the problems that you're trying to solve like what is the genuine basis for some of these problems that we have defined say objects or normals or depth and so on.
And so on so forth, because these are purely based on human intuition.
And ultimately, I think integration of a visual perception module into a bigger system like robotics system. These are the things that we, we are, we have started to basically approach.
And I think these are will be the biggest, you know, problems in a few years to come to be tackled by the research community.
You mentioned tackling this challenge of solving these kinds of problems with less data and one of the solutions that the research community is come up with there is transfer learning.
And that's the, the subject or at least the context of a recent paper that you were a co-author on that won the best paper award at the recent CVPR conference.
Can you tell us about that paper? Yeah, sure. So that paper is is is called test economy.
It's actually we call it test economy because it's task taxonomy. It gives you a taxonomical transfer policy over different perceptual tasks.
So the idea there is that, you know, let me give you actually a non perceptual example. Let's say if you want to learn how to play a board game.
The first board game that you that you learn takes time because you're just learning the very basics. You realize that well board games are usually composed of a board some, some, some surface and on which to sit some pieces probably.
And these pieces have some categories like different turning to different colors and there's usually some flow. You want to move some pieces from one part or another.
So none of these have to be 100% true for all games, but more or less there's some structure there. And the first time that you're exposed to such a thing, you're just learning everything from scratch.
So it takes more time. Now imagine you have learned how to play 20 board games. The next one will become a lot faster because you have understood the basic structure of the game.
Even some of the moves that you have learned in the previous games will be useful for the next one that you're going to play. So essentially you're transferring some useful knowledge that you learned before you're distilling it and applying it towards a new problem that you're facing.
And that process of successfully transferring and not having to start from complete scratch is what makes you faster.
And that generally applies in a broad sort of domains. You know, games was just one example, but literally everything that we do as as only working example of intelligence as humans are generally the brain.
So we leverage this transfer principle a lot. What makes something transferable or not? Those are basically very open questions there, but leveraging this concept has a lot of benefits.
Test economy is that for perception. So we define a lot of problems in computer vision that they're deemed useful and research community has been trying to solve them for long time examples of that.
Like I said, the most popular ones are like object detection or 3D properties of the scene like depth, normals, curvature, estimating the lighting of the scene or what is the layout.
Of the room that you're at or or some other ones like self localization, if you're in a room and you're moving or let's say you're just watching the YouTube video and the camera is moving, you can predict how the camera is moving, you can draw a trajectory of that.
So the problem of like localization or camera pose estimation, if you want to put it in vision terms. So these are problems that we generally solve and pretty much all the papers that come out in computer vision community.
The majority of them are just think one of these problems. They focus on one problem and try to come up with better and better and better machinery to improve the performance and every single one of those problems.
But these problems are related to each other. They're not independent, just like the board games are related to each other. The relationship might not be trivial, but some relationship exists.
The example I gave actually when I was at CBR giving the talk was layout of the room and the objects that are in there.
So when you're in a room and you just open your eyes and you're in a certain room in a conference room or anything, you have an understanding of the 3D layout, where the floor is, where the ceiling is, where the walls are.
And that gives you a pretty strong prior about where the objects would be. You would never look for the chairs and the ceiling. You would just look at look for them somewhere close to the ground plane because they need to receive support from the ground to stay or you would look for lights actually close to the ceiling because that's just the way the world works.
So that is special prior to the get from problem A in this case was layout of the room was very useful for problem B, which in this case is object detection.
So the relationship between problems clearly exists. And if we can develop some machinery to to make use of these relationships to solve the new problems that you face without having to start from scratch by borrowing those things that you have learned already.
And an employee towards solving a new problem that has a lot of benefits and taxonomy was is a method for actually trying to do that the challenges there are obviously is that how you want to do this.
But the premise of is that let's view the tasks that we solved in computer vision and concert let's recognize that they are related to each other and and now solve them in concert rather than starting from scratch and individually approaching them, which is one of the biggest reasons that in general machine learning and neural network systems that that are working today they are very data hungry.
So that's what we call massive amount of data to start producing the good results and using this like transfer transfer learning approaches is a potential remedy to that problem.
Some of what you're describing reminds me of something I wrote about in my newsletter a couple of months ago, I think there was a data set that was published called I think home 3D.
The broader project was to provide a data set. I was part of a challenge, I believe, that was to try to come up with an agent or a model for determining what room you're in based on 3D images.
Is this a related project to some of the work that you're doing? Do you know that lab?
Yeah, I'm aware of home 3D. There are alternative efforts also similar to that. So in general, these all related in the sense that we are generally moving from data sets to environments in computer vision.
So the notion of a data set is something that in image data sets like image net again is the most popular example for all days that we collect a set of images and we put some metal information on it.
For instance, somebody sits down and amitates that these are the objects that you see in there or this is the layout of the room or these are the 3D normals of the surfaces and so on and so forth.
The main point about data sets is that they are static, so they can't change. You cannot say that, okay, I'm looking at this image.
Let me just reposition myself with respect to the object to get a different perspective. That's just fundamentally not possible because these are passives and they're offline. They're recorded once and that's it.
A large part of perception is related to agents that need to be ultimately active in the world like robots. We need to create a home for them to learn in this particular way because that's the way they're going to be operating.
So the trend of moving from data sets to environments is mostly motivated by this desire. Now how this is related to transfer learning and astronomy in general, the relationship is that in my opinion, ultimately we need to throw an agent in an environment and ask it to do something for us, something useful.
That is not perceptual. Let's say if I have a robot at home that they want to ask it to bring the can of soda, perception is something that is in service to that downstream goal, which is just fetch a can of soda from the fridge and bring it to my desk.
Now what problems that entails, like what set of perceptual problem and bring a can of soda entails is not a trivial question despite what it sounds like we cannot really easily make a list of perception problems we need to solve to enable an agent to do some of these tasks.
In that sense, then we have to view the problems that we want to solve. Let me actually make that a little more understandable. Let's say a can of soda, the agent needs to obviously know where it is. So it needs to self localize when it's moving, it needs to understand how it moved.
So the one step that it took, how it changed, what direction it went. On the way, of course, it shouldn't hit obstacles. You need to realize that then in a room, the way to get out of the room and go to the kitchen is through the door.
So you need to do certain semantic understanding. And when it sees a can of soda, you need to recognize it and have enough skills in terms of control to grasp it and bring it on so far.
So what are the perceptual problems? I just made a list, but is that complete? No, a lot of things can actually go wrong if you think a little bit more about it. And ultimately also we don't want to have robotic agents that are single tasks.
We don't want to have an agent that just can bring a can of soda for you, but cannot make coffee perhaps or bring a cup of coffee or fetch, you know, clothes from from the washing machine.
So ultimately, the set of perception problems we need to solve to put it on an autonomous agent to do something useful for us is a non-trivial problem to create a list of them.
And the list is going to be long. That's at least what we can, what we can say now. Now, if you have a long list of problems, even if we can make that list.
First of all, if you want to individually solve them on the fly, you will burn a lot of compute, which is unnecessary because like I said, these problems have relationships and certain redundancies.
So you should view these tasks in concert, rather than individual concepts living in isolation.
And also, it's not even feasible, in my opinion, to just collect a certain amount of data for every single one of those problems and assume that you can just solve them to perfection individually and then put them, integrate them in an agent.
Again, there has to be some connection with being these and transferred like so. So it should be some highways of information between different tasks and that should be very fluid built into the system.
So the relation that comes from like, the economy and transfer learning in general and those like environments is in that sense that that makes the problem a little, a little more exposed that we really need to view problems in concert.
So you're really dealing with is a set of tasks, not individual tasks. And there's no escape from using the redundancies and like regularities within this set of tasks to make what you're trying to do.
I think a lot of people listening to this podcast will be familiar with the concept of transfer learning in the sense of, for example, object recognition, you train a neural network on lots of images based on, say, image net.
And then you use that model and maybe fine tune it for some other data set are using transfer learning in the sense of language or there's some commonality in the technical approach that you're taking between, you know, task transfer learning and the type of transfer learning that we think of in the object detection sense.
Right. Actually, the object detection example is great. What we are doing is actually similar to that. A lot more similar to that than language.
So the object detection approach that you mentioned is actually pretty interesting and pretty useful. Like you said for a long time, the most established way of reducing the need to label data was that we have one great data set,
image net and we have models that are trained on it to do object detection.
People train a model on that and then they usually get that network and then fine tune it for another task, which is not object detection, something else, but they have way less data for it.
They essentially use the image net as a good initialization for solving the other problem that they are interested in. So what we are, we didn't taxonomies actually something similar to that, but you know, with a lot more structure and a bigger scope.
So let me, let me pose, this is a question is that why object detection, a system developed for object detection should be useful for solving another task. Why should be the case. I mean, that's, let's say the other task could be seen classification.
Right. You want to know whether you're in an office or in a living room or in a library, et cetera, et cetera.
Why a model trained for object detection should be useful for that purpose. I mean, it makes sense, right, because usually the objects that you see in a scene, very well correlated with the category of the scene.
If you're in a living room, you see a lot of couches, you might see a TV, if you're in an office, you usually see, you know, conference tables and chairs and so on.
If your system has some understanding of the objects, you would expect that knowledge to be very useful and very transferable to another problem like scene classification.
So that's a successful case of transfer. Now, many of the problems that we want to solve are not that close to imagine that or generally semantic problem.
Right. Like I said, for instance, 3D understanding of the scene. It's not clear why an network that can do object detection would be useful for you to understand, let's say, normals of the surface is in a scene or just depth.
Devs is actually quite like important problem because, you know, for instance, at minimum, it can allow an agent to not run into things. You need to understand how far you are from the thing that you are in front of you.
It's an important problem, but the relation between that and something like object detection is not clear. And, you know, the research committees also found experimentally that image that features transfer to different problems with different success.
So if the problem somehow has a reason to be related to object detection, usually transfer is more successful and and and or failure cases to certain problems that initially just does not transfer well to it.
So this is actually the premise where taxonomy builds on we are recognizing that some problems are more related to other problems and and there is some structure in this space.
Let's say if I make a list of all tasks that, you know, you can think about like perceptually solve this will be some 3D tasks and some low level 2D tasks like what are the edges and so on and some semantic tasks like what are the objects, what are the, what are the, what are the category of the scene that you're looking at.
A lot of things that are not based on image content, but also returning information about the agent, like I said, self-localizing agent and and so on. So there are some, you know, modes of tasks.
There's some cluster of tasks. So there's some structure there. So if you want to successfully transfer from the knowledge that you have right now to solving a novel problem, you need to understand this structure.
You need to understand that if your new problem that you're facing is something related to 3D, you probably should transfer from the other 3D tasks as compared to semantic.
If the problem that you're facing is something related to semantic, sure, something similar to object detection is a better idea.
But doing all this completely automatically without having a human in the loop that sister and says, oh, this problem sounds 3D.
So let me transfer from other 3D tasks and this problem sounds semantically. Let me transfer from other semantic tasks. So having a machinery that in a fully computational way discovers this is structure of this space.
And now when it faces a new problem, it can say that I want to transfer from those set of tasks, not the other set of tasks is basically what taxonomy does.
So we are not really forced anymore to transfer from image net to any problem that we face. There's a system now in place that it can advise you on where you should transfer from.
And the good thing is that we can evaluate that. So in all of our experiments that that we have actually there are plenty of live demos on our website too.
If you visit there, you can actually play around a lot with the tools that are there. You will see that as a baseline, we also show how what result you would get if you transfer from image net rather than this kind of adaptive transfer policies.
And you will see that the adaptive transfer policies actually outperform the image counterpart significantly.
That essentially tells us the concept of making use of the structure among tasks and adaptably deciding where you should transfer from and how you should transfer is something that is useful.
And we were also able to successfully extract that structure and have a really computational method for enforcing that without having a human in the loop to tell us about this, you know, let's say intuitive, intuitive relationship between problems.
So several ideas starting to form about what exactly task on me was trying to do throughout this first part of our conversation. And I think it started to firm up right there at the end.
One of the ideas earlier on was some system that was, I guess you could think of it as kind of some ensemble of a bunch of different types of networks trained to do different types of tasks and some, you know, what you were trying to do with task on me was.
You know, that unsombling or that meta decision layer doesn't sound like that's it, but maybe you can comment on on that. It sounds more like what you've developed is again grossly simplifying some kind of heuristic or decision algorithm that if you've got these pre trained models that it can, you know,
the system knows about some number, some catalog of pre trained models and what their capabilities are and you can it can maybe take a look at your, your problem and your data and tells you which of those models might work best.
I feel like neither of these gets to exactly what you're doing, but they're kind of, you know, maybe you can use those to help me get there.
Yeah, exactly. So I can explain the little more details. Let's say, essentially, the tasks that we saw in computer vision are an abstraction, you know, you have a set of pixels, you want to read something abstract out of it.
So that's what we call a task here. So again, like I said, popular examples of that is object detection and so on. So we create a dictionary of these tasks. We have 26 of them in our dictionary.
Now, we train, we have a data set that that each image that's four million images in it. These are real images. They're not synthetic. So each of these four million images that they have our data set has annotations for every single one of those 26 tasks that we have there.
So we can't afford training a really good neural network to solve every single one of those 26 tasks. So we call them task a specific network. So there's one network that is completely specialized in solving one of these problems very well down to perfection nearly and not care about any other problems.
Now, we get the internal representation of these tasks, specific networks, and we try to evaluate if you can group produce the results of another task using the internal representation of one of these tasks.
Essentially, we are trying to understand that their information for solving another task is contained in this representation that was trained for another problem, but can possibly include the information.
So for example, something along the lines of looking at semantic distances between activations at different layers in the network or.
Right, so the machinery for that quantification is again another small neural net, but like to give you an example of the task, let's say the example they gave you earlier that lay out of the scene and objects or no more of the surfaces and say 3D curvature, so 3D properties of the scene.
Let's say if you want to evaluate whether the task of surface normal includes the information for solving 3D properties of an object like curvature, how the shape of it looks like or lay out of the scene.
Let me just do lay out of the scene is probably more intuitive. So we get the network that was trained for surface normal estimation.
We get the internal representation of that like one of these layers of neural nets in this big network and we train a very small neural net that we call a readout function to read a layout of the scene from the representation of the network that was only developed for surface normal estimation.
We train it using a small amount of data just a couple of thousand images that trained us like little readout function and then we evaluate it whether it was successful or not.
So if this very small neural net trained with a very limited amount of data 2000 images was able to successfully extract the layout of the scene from the internal representation of a network that was trained for normal estimation.
Then very like closely related tasks we create the complete graph and put the edge rate representing going from normals to lay out of the scene, put it put it, give it a very large weight.
And if it wasn't successful, we would give it a small weight and we do this evaluation for all pairwise sets of tasks.
And that gives us that complete graph that gives us basic tells us how every pair of problems that be related to each other.
And just to calibrate on that one example, use surface normal estimation is basically you've got an image and you're trying to calculate a vector that's perpendicular to the surface at each pixel in the image.
Exactly, exactly. Yeah, it's a 3D vector. So it's the gradients of say edges, it could be confusing in that sense. You can think of it as you're looking at an image, you click on a point and it gives you the normals of the surface.
Which respect to you is this point is the surface that this pixel is laid on? Is it like facing you is facing different direction? What would be the 3D angle of that?
You calculate these pairwise relationships and that is the objective to tell you through this research, the general relationships between these tasks or is it specific to a given data set?
Well, these are not too specific to a data set. Of course, everything that we do is is data dependence because it's a fully computational method and whatever we extract is from the data set that we have.
So we have spent a lot of effort in making sure that this data doesn't suffer from too much bias. It's from indoor scenes. So obviously, I would not expect the observations to generalize to say outdoor scenes or underwater imagery and so on.
But you know, as long as you what you care about, it's a indoor scenes, indoor images. So these relationships are expected to remain stable. We have actually some studies on on universality of these findings.
We basically fix many of these design choices except for one at some basically variance there and see whether how much of these structures that we have extracted changes or not. So we do have these studies and in general, I can say that it's not actually very data dependence.
We observe the pretty good stability in terms of high level observations. When we see that two tasks are being related in a particular form, and this data set, then we go on evaluate an image net or MIT places, the same classification that is said, usually the same trend comes out.
And we have evaluated experimentally. So I would I feel confident at this point that many of these trends that the observed there and these structures are not data specific, but they are not general also in the sense that there exist many types of relationship between tasks and these relationships are evaluated in terms of transfer learning, essentially trying to to see how much useful information one task can supply to another one.
Other types of relationships also exist, let me just give you an example is that like computational. So maybe somebody doesn't care about reducing the need for label data, but they just want to say I want to reduce the number of amount of computation that I can do like number flops that my GPU.
So the relationship in that sense would have to be measured in terms of how much compute you can save. These things usually correlated a lot like supervision and compute, but in principle, you would need to measure the relationship for what you're trying to optimize and make the system more efficient.
So what we are trying to do is basically reduce supervision, reduce label data. So our metric or relationship is curated for that. If one was interested in computation or let's say storage, you want to have, you want to store the say a neural net that solves a set of problems, but you don't want to just individually solve them.
You want to do something like compression on top of them. You don't want to save something twice if there is something in common between them. So you would again have to measure the relationship in terms of in terms of their redundancy in terms of the storage.
In that sense, these relationships are about supervision and transfer learning. They're not completely general, but a correlation between these different things is a thing too, but I'm not aware of any formal study towards towards that.
Is it fair to say that if someone was looking at trying to build a system that performed one of these tasks and was constrained and labeled data, and so this wanted to use transfer learning to accelerate building or to allow them to build a machine that required that was more data efficient.
That the goal of the paper is that they should be able to look in the paper at a chart that you produce that illustrates these pairwise relationships, and essentially they'd be told what transfer learning models would be helpful for them.
Exactly. If they had another problem, such as computational efficiency, storage efficiency, whatever, they might look at the paper as guidance and need to build out the machinery that you've built out to tell them which of these relationships is most critical for their tasks.
Yeah, that's completely true. The second part is right. In principle, if what you want to reduce is not labeled data and supervision, what we have, the study that we have done serves as a guideline, but not a direct optimization of that objective in principle.
Now, regarding the first part is that it actually ends up being a little more complicated than a chart than any fixed chart. One thing that I actually skipped over and didn't explain in detail is that after you find all these relationships, there are a lot of them.
Just the pairwise, what we call first order relationships, start 26 tasks in the dictionary, and then these are directed graphs. So there's 26 by 25 number of relationships. So it's a huge matrix.
Making sense of that is not too trivial. It ends up being a chart that can go into a handbook that you can just look up a row and say, okay, this is the way I, this is the way to go.
So when you look at these graphs, I mean, these visuals are actually on a website and a paper, but it's hard to explain them verbally.
But when you look at the many of these relationships are weak, they are not useful, obviously, because many tasks are not well related to each other, but there are many strong ones too. And you can see pretty strong patterns.
So in the end, to make this useful, we want to extract this parser structure out of it. There's no point in having this complete graph there, because you can just reduce a lot of these and get rid of them and not lose anything in terms of performance.
So formally, that becomes a sub graph selection. So that's when actually it becomes a taxonomy that becomes a tree like a structure is that you want the problems of being that, you know, the user says, I want to solve this problem or this set of problems, but I don't have enough data for it.
But I know how to solve these problems, like these 20 other problems, and then tell me how I can solve this set of problems using those basically from what I should transfer to these target tasks, what we call the problems like targets, from what sources I should transfer to these targets.
And also, let's say, maybe the user doesn't have the compute to run all these networks that we have trained in parallel as source. So the user would say that I can't afford having more than three networks running in parallel in my computer or in my agent.
So what are those three sources that best supports solving these target tasks that I care about and how I should go from those to these, what exactly is the transfer policy. And that's why it becomes a sparse structure and tree structure.
So when you look at the examples of this taxonomy in the paper on the website, you don't see a dense graph. It's actually a rather sparse, but it's curated for this very particular question that the user has is that I care about this set of part target tasks and how I should do it.
And the reason it doesn't become like a fixed chart, like I said, is that these arguments are the very case by case, the user, like every user cares about the different set of targets.
So they specify that and the taxonomy to transfer policy changes according to those arguments and actually we have we have API on our website where where you can go and basically part, you can see a partitioning of tasks like these are tasks that I know how to solve these are tasks that I want to solve.
But I don't have data for it. We call them target only like I said, and you can just press button and it runs the optimization live right there and it puts the transfer policy in front of you.
So it's not basically since they there are many arguments in play, it doesn't end up being a fixed chart that you can just print in a PDF, but the API is the best way to see that.
And maybe I'm reading into the way you've described this too much, but it almost sounds like you're describing transfer or a transfer policy, including transfer from a given source as a binary decision.
And is that always the case like is there for a given task or there is there only one way to transfer from image net or are there multiple ways to transfer from image net either computationally from kind of waiting the you know that network in different ways or transferring from different layers.
It seems like there's more than one way to do it and to what extent does your model take that into account. Yeah, great question. So there are two basically points to to to be made and ask you a question.
First of all, like one thing that we do is is not we don't come into one task when you want to solve a target task, like basically there's for solving one target, you can have multiple sources that basically corresponds to the case that we're like multiple source tasks get together to solve one problem.
We call them higher order transfers that is also included in the in the formulation and if you like I said, if you run for instance, the API, you will see that certain targets are have incoming arrows from multiple sources that basically means it found that it's useful to use more than let's say just imagine that put
it next to something else and that was the one that produced the best results. So it doesn't have to commit to a certain source now, but just, you know, having how we can we quantify the how well a transfer can happen from a source to a target.
There are many different ways actually to do that and every few months actually there's one new method that comes up. What we used was we used actually a fixed way, like I said, we used readout functions, but these are small new on that themselves.
And as far as I know actually still today, they're the best performing ways of transferring from one thing to another subject to some constraints. But yeah, in general, we commit to a single way of doing this transfer and that time is the best way and then today it is still the best.
I mean, they're like fine tuning and so on. You can view them as also as a way of transferring, but there are some issues with them because let's say if you find you in a model for a task, you can then that becomes a copy of that source. It doesn't remain the source.
So if you want to go from one, let's say one source to 20 different targets, you have to create 20 different fine tune version of that source network, which again, a storage wise and computation wise becomes intractable. Yeah, there's some details there.
Awesome. Well, this is really, really interesting work. Is there anything else that you'd like to share before we close up?
I think that's it. And I think it's a it's a very interesting topic to think about. And I believe in general, we have really just a scratch the surface of this problem.
Ultimately, what I would expect and at least I would like to see is that now that we have some understanding about this space of task, like we're really now viewing problems, like I said, coming from a space rather than isolation. So we have a model to cover this space with less computation, less supervision, essentially bringing efficient into picture.
So ultimately, what I'm interested in and I would like to see is that we can turn this into a general list perception model that can quickly compose solutions to unknown problems that it comes its way and and possibly kind of on the fly, like in a lifelong learning matter.
Now that we know this structure of this space, at least we have version one of it. Yeah, what I would hopefully see perhaps in a few years is that they can turn these journalists perception models and maybe we can put them on autonomous agents and see them really doing perceptually complex task and get better two time and not require a large amount of data for every single problem that they are asked to solve for us.
We'll see how it goes, but it's exciting. Awesome. Well, Amir, thanks so much for taking the time to share this with us. Thank you for having me.
All right, everyone, that's our show for today. For more information on Amir or any of the topics covered in this episode, head over to twimlai.com slash talk slash 164.
Don't forget to visit twimlai.com slash nominate to cast your vote for us in the people's choice podcast awards. As always, thanks so much for listening and catch you next time.
You
