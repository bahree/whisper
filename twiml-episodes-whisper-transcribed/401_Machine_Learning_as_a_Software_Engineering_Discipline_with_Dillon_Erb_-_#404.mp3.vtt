WEBVTT

00:00.000 --> 00:12.080
All right, everyone. I am here with Dylan Herb. Dylan is the co-founder and CEO of PaperSpace.

00:12.080 --> 00:16.800
Dylan, welcome to the Twomble AI podcast. Awesome. Exciting to be here.

00:16.800 --> 00:22.240
It's great to have you on the show. I was thinking back to when we first met and you introduced me

00:22.240 --> 00:28.000
to this great coffee shop across the the street, I guess, or around the corner from your old offices

00:28.000 --> 00:33.920
in Brooklyn. Looks like you've got a new space now where you've got lots of cool plants.

00:34.800 --> 00:40.400
Yep, still in Brooklyn, just down the street, but yeah, I guess it's been a while. Glad to be here.

00:41.120 --> 00:50.960
Awesome. So we are going to be focusing in on the general topic of machine learning as a software

00:50.960 --> 00:58.640
engineering discipline, touching on ideas like ML ops and CICD and what all that means for folks

00:58.640 --> 01:04.480
that are trying to get ML into production. But before we do that, let's start with a little bit

01:04.480 --> 01:11.920
of your background and how you came to, you know, found a company that is, you know, in the center

01:11.920 --> 01:19.440
of helping folks, you know, work on machine learning problems. Yeah. So time flies, but we've

01:19.440 --> 01:24.640
actually been at this for about five years now. My background and this will kind of inform some

01:24.640 --> 01:28.960
of the, probably the conversation today, but my background is a little bit nontraditional. I

01:29.920 --> 01:33.360
studied building architecture and kind of on the architecture engineering side.

01:34.640 --> 01:40.560
And what used to be probably more in like the HPC world, so working more with like genetic algorithms

01:40.560 --> 01:46.320
and I was doing things around topology optimization, that led to an interest in parallel compute

01:46.320 --> 01:52.000
and GPUs were really the name of the game and have continued to be so. And so I started paper space

01:52.000 --> 01:59.680
with my co-founder and good buddy Dan, Dan Cobran. And yeah, so we, you know, our general premise

01:59.680 --> 02:05.920
and it's definitely evolved was that, you know, GPUs as sort of a kind of parallel compute device

02:05.920 --> 02:09.520
would make its way into the cloud and there would be all sorts of, you know, new applications that

02:09.520 --> 02:14.480
would emerge from that. And I think what we, you know, weren't aware of at the time was that the

02:14.480 --> 02:19.360
big driver of that was going to be this, you know, shift from like visual compute into just,

02:19.360 --> 02:24.080
you know, massive data processing and the emergence of like deep learning as kind of a practical

02:24.080 --> 02:30.960
discipline as well. So yeah, that's how I got into it. And now I primarily just think about that,

02:30.960 --> 02:36.320
which is sort of how do these, you know, new tools evolve and support kind of emerging workflows,

02:36.320 --> 02:42.160
tools, products, things like that. Yeah, I think you and Nvidia maybe have that in common,

02:42.160 --> 02:49.280
you know, we're going after a totally different use case for GPUs, one that was graphics focused

02:49.280 --> 02:56.160
and were quick to recognize the activity or the shift that was happening around ML and AI.

02:56.800 --> 03:02.800
What were, what were some of the first use cases that you, you saw that kind of clued you into

03:02.800 --> 03:08.240
interesting activity in this space? Yeah, I mean, it was, when you're an early stage company,

03:08.240 --> 03:12.240
you kind of, you know, I was manning support as well as doing everything else to company. And so

03:12.240 --> 03:16.880
you email everyone who signs up and you're just like, hey, what are you up to? And we started,

03:16.880 --> 03:20.960
you know, this was new to me, but we started getting more requests for people saying, hey, I'm

03:20.960 --> 03:26.480
trying this thing out called TensorFlow. You guys have GPUs. This was before really the big cloud

03:26.480 --> 03:31.840
providers had offered GPUs as just like a, you know, a native compute device. Obviously today,

03:31.840 --> 03:37.360
they're, they're pervasive. And so we're like, tell us more, you know, what are you building? And

03:37.360 --> 03:43.840
so that's kind of been the format of the conversation for, you know, the last four years. And so,

03:43.840 --> 03:48.320
you know, in that time, we've seen, well, actually two things interestingly. One, you know,

03:48.320 --> 03:53.920
the emergence of like this deep learning practice where you need a lot of compute. So you need a

03:53.920 --> 03:57.600
lot of things. You need data, you need compute, you need expertise, but compute is a big part of it.

03:57.600 --> 04:01.520
You know, you can, you know, you're, you're not going to make a driverless car algorithm on a

04:01.520 --> 04:06.400
Chromebook. So there is a compute requirement. And the kind of the interesting thing is even today,

04:06.400 --> 04:10.560
and I'm sure this is, you know, bumped up in, in your universe as well, which is sort of the,

04:11.920 --> 04:19.040
the blending of the visual compute kind of side of things or everything from like image generation,

04:19.040 --> 04:24.320
deep fakes being sort of the big example, or now we're seeing things like synthetic data. So the

04:24.320 --> 04:28.720
compute pipeline and the visualization pipeline, I think, are collapsing in a really interesting way,

04:29.760 --> 04:35.520
where, you know, visual effects are, are, you know, just as interested in deep learning as like a,

04:35.520 --> 04:39.760
you know, thing in their tool belt. So, so it is a, it's been a weird few years where,

04:40.560 --> 04:43.600
you know, the first pitch deck, I remember the first slide was, what's a GPU?

04:44.800 --> 04:48.720
Which luckily, we don't, we don't have to bring that one up anymore, people kind of get it.

04:48.720 --> 04:57.920
Nice, nice, nice. And paper space has been popular in our community for quite a while. We

04:57.920 --> 05:04.560
do a bunch of study groups around courses like Fast AI and others. And, you know, for years now,

05:04.560 --> 05:10.800
folks, in trying to do those courses, you look for infrastructure in which to run it on and paper

05:10.800 --> 05:18.320
space has always been one of those options. But the company is doing more now than just, you know,

05:18.320 --> 05:25.760
a server instance with the GPU, kind of going up the stack. Can you talk a little bit about that

05:25.760 --> 05:30.560
and the motivation there? Yeah, definitely. So, paper space is the name of the company and our kind

05:30.560 --> 05:35.840
of main product or kind of collection of products is called Gradient. And that encompasses

05:36.880 --> 05:41.680
really a handful of components around what, you know, we'd probably call an ML ops platform.

05:41.680 --> 05:46.720
And, you know, we definitely should dive in to what exactly that comprises. But, yeah,

05:46.720 --> 05:52.800
today we, you know, provide tools for pretty much every step of the kind of building the machine

05:52.800 --> 05:58.640
learning model process. But really with an emphasis, again, coming from our kind of background in

05:58.640 --> 06:04.560
the infrastructure side of things, like a focus on where the kind of machine learning world intersects

06:04.560 --> 06:09.600
with the, you know, infrastructure or networking issues or more broadly, just how does it interface

06:09.600 --> 06:14.400
with, you know, traditional software development and application development that, you know, has also

06:14.400 --> 06:19.440
been changing quite quickly, but has, you know, become a relatively established pattern for, you know,

06:19.440 --> 06:25.680
all sorts of companies to build on top of. Yeah, and as you mentioned, this has been a quickly

06:25.680 --> 06:33.520
evolving space in terms of the, you know, your perspective on, you know, slash takes,

06:33.520 --> 06:40.320
slash interest in the ML ops and infrastructure side, or, you know, to what degree are,

06:42.640 --> 06:50.000
are you, I guess, productizing or, or, you know, drawing learnings from the experiences that you

06:50.000 --> 06:57.520
had, you know, standing up these, you know, these data centers with GPUs versus, you know, stuff

06:57.520 --> 07:02.080
that you're seeing evolve elsewhere outside of the company. Like, what's kind of the core,

07:04.080 --> 07:10.080
the core around which you're, you know, building out an approach to ML ops?

07:10.800 --> 07:14.640
Yeah, so I think there's kind of two levels of it. The first level is just like, you know,

07:14.640 --> 07:20.880
what does the thing do? And to date, you know, we offer, you know, one click sign up any developer

07:20.880 --> 07:24.800
and one of the most popular ways of onboarding is, you know, through a course like Fast AI,

07:25.520 --> 07:29.360
which will be, you know, partnering with them the next version as well. This sort of the entry

07:29.360 --> 07:33.680
point and the question is, you know, how can you kind of get right into the media, the problem,

07:33.680 --> 07:38.480
start looking at some pie torches or TensorFlow or, you know, try out GPT-2 or something like that,

07:38.480 --> 07:42.320
without even thinking about infrastructure. So there, you know, there's kind of the practical

07:42.320 --> 07:48.880
piece of that, just onboarding to it. And then on top of that, the question is, you know, what are,

07:48.880 --> 07:53.680
I think, across the industry, there's this a lot of interest in creating purpose-built tools

07:53.680 --> 07:57.600
for machine learning because we can all agree or people that, you know, spend some time with it will

07:57.600 --> 08:02.480
say will acknowledge that, you know, it does have some fundamental differences from traditional

08:02.480 --> 08:06.320
software development. You know, we can talk about the programming languages. Some folks are coming

08:06.320 --> 08:12.800
from R or their backgrounds in stats or math. The hardware itself, obviously, you know,

08:12.800 --> 08:18.480
GPUs are not required for building out a web app. And within machine learning, there's obviously

08:18.480 --> 08:25.200
a whole host of types of machines. So there is a particular need to think about the hardware as

08:25.200 --> 08:31.280
a primitive in your system. So, you know, so when you zoom out kind of collectively, I think the

08:31.280 --> 08:38.240
bigger problem in the way we talk about, you know, the way we evangelize it is to say, we believe

08:38.240 --> 08:43.360
machine learning is a part of a traditional software engineering practice. And the good thing about

08:43.360 --> 08:48.640
that is when you start diving into something like an ML ops platform, you have a lot to draw on,

08:48.640 --> 08:54.640
which is, you know, a whole 10 years of, at least, of, you know, modern software development practices.

08:54.640 --> 08:59.520
So that encompasses things like continuous integration, continuous deployment, obviously,

08:59.520 --> 09:02.560
tight integrations with source control management, and in the case of machine learning, you know,

09:02.560 --> 09:08.160
data management, and, and provenance. And, and you start thinking about it in a different way. So,

09:09.440 --> 09:14.880
the benefit of a traditional software development practice that might be like agile or, you know,

09:14.880 --> 09:20.400
based around CICD is that you can, you know, we're a software company that builds a web application,

09:20.400 --> 09:25.360
a web console, a lot of tools. And we can, you know, bring new people into that collaborative effort,

09:25.360 --> 09:30.640
and they can work in sort of guidelines that means that they can very quickly start, you know,

09:30.640 --> 09:35.120
shipping production code. So machine learning is newer in the sense that, you know,

09:35.120 --> 09:38.880
not every company is in the, like, we need to ship this into production. But if you look at the

09:38.880 --> 09:42.960
process of, like, testing some assumptions, building a little model out, and then trying to get into

09:42.960 --> 09:49.280
production, the, the kind of, the framework that you need to think about it in is just like any

09:49.280 --> 09:53.840
other software, you know, development paradigm. And we feel strongly that, you know, this,

09:53.840 --> 09:59.600
the closest analogy is probably something like mobile app development, where, you know, historically,

09:59.600 --> 10:03.520
it was like, you have your iOS team when the iPhone came out and you have your, you know,

10:03.520 --> 10:07.280
regular software team or your website team, and eventually those two merged, and the,

10:07.280 --> 10:11.760
the baseline practices are the same. And we think that's the, you know, the case for machine learning

10:11.760 --> 10:17.680
as well. We're just at the beginning of that curve. Now, a lot of folks pushed back on that and

10:17.680 --> 10:24.000
say that there are fundamental differences in the way that machine learning and data science more

10:24.000 --> 10:34.080
broadly should be practiced. And it's not, you know, it's not as predictable that as software

10:34.080 --> 10:41.120
engineering. You know, what's your, you know, how do you respond to that kind of? Do you,

10:41.120 --> 10:45.760
well, first of all, do you even encounter that? Like the, the, yeah, I kind of push back and what's

10:45.760 --> 10:52.000
your, what's your take on that? Well, I think it's an emerging discipline that has, that has moved

10:52.000 --> 10:56.000
quite quickly. And it's also happened at the same time that we've, you know, it's, it's happening

10:56.000 --> 11:00.240
the same time that we're also seeing pretty large, large shifts in how software is developed. And

11:00.240 --> 11:05.040
I think the biggest trends here we can talk about are, you know, containerization, the idea that

11:05.040 --> 11:09.680
you can have kind of reproducibility at the, at the, as like a primitive in your pipeline.

11:10.400 --> 11:14.800
The emergence of these kind of more complex tools like Kubernetes, which is still, you know,

11:14.800 --> 11:20.320
a complex thing to set up and to manage. And you have companies that are seeing the benefits

11:20.320 --> 11:26.560
of this. But fundamentally, we're starting to, you know, software development is shifting. And

11:26.560 --> 11:30.400
that's happening right as machine learning is trying to figure out sort of what its relationship is.

11:30.400 --> 11:35.520
So I think there is a lot to that. And I think it, in the sense that there are people that would

11:35.520 --> 11:40.320
argue that maybe the foundational part of machine learning is more exploratory. It doesn't quite

11:40.320 --> 11:46.480
have, you know, one-to-one mappings into, you know, software development. But I don't think that is

11:47.840 --> 11:52.160
true across the board. And by that, I mean, you know, there are a lot of really useful analogies

11:52.160 --> 11:58.400
we can draw from. You know, we, we think a lot about, you know, if you think of Git or GitHub as

11:58.400 --> 12:04.480
sort of a standard in collaboration on software projects today, there's some fundamental act,

12:04.480 --> 12:08.480
you know, things you can do, you know, you can fork, you can, you have pull requests, you have

12:08.480 --> 12:13.600
branches, they don't map perfectly to the machine learning model. You know, your iteration

12:13.600 --> 12:17.040
could be very quick on a machine learning model or it could be really slow if it takes a long time

12:17.040 --> 12:23.440
to train. But in either case, there are useful things to draw on, you know, development staging

12:23.440 --> 12:27.840
and production environments, you know, being able to promote things. And that's, that's a traditional

12:27.840 --> 12:31.840
software engineering practice largely. So the question is how do they interface? And that's what we,

12:31.840 --> 12:36.880
you know, spent a lot of time on gradient trying to tease out where, where you can get the best

12:36.880 --> 12:41.600
to both worlds. Like, where can you draw on those kind of best practices? And, and where does it

12:41.600 --> 12:45.760
diverge? Like the reality is, if, if it were just a software development practice, you could use,

12:45.760 --> 12:52.000
you know, Jenkins or, you know, a traditional kind of CICD pipeline runner for machine learning.

12:52.000 --> 12:55.680
And you really can't do that. There are, you know, the artifacts are different. The machine learning

12:55.680 --> 13:00.800
model is a very important artifact at the center of this. Data is a, is a, you know, as important as

13:00.800 --> 13:07.040
code as a primitive, as an input to the pipeline. So, maybe dig into that a little bit, in a little

13:07.040 --> 13:14.640
bit more detail. I am imagining that folks that are coming at things from the more traditional data

13:14.640 --> 13:23.040
science perspective don't really have any idea what a Jenkins is or CICD pipeline. You know,

13:23.040 --> 13:28.240
maybe talk through, you know, kind of your take on that traditional world from a software

13:28.240 --> 13:34.240
engineering perspective. And then, you know, we can talk through like how it maps to, you know,

13:34.240 --> 13:41.040
the way you see the ML ops world. Yeah, definitely. I mean, I think at, at the foundational level,

13:41.040 --> 13:47.360
the idea is, you, you commit to, you know, you, certain things get committed into your development

13:47.360 --> 13:52.160
process. Even, you know, literally as part of like a config file that goes into your repo or

13:52.160 --> 13:58.800
something that says like here are the steps of the process, meaning, you know, we transform some

13:58.800 --> 14:05.280
data. We send it to this machine type with this, you know, container and this framework tenser

14:05.280 --> 14:10.400
flow 2.1 or whatever. And this has an artifact that we then deploy. So, there's kind of a pipeline.

14:10.400 --> 14:15.120
And so the foundation of CICD and there are a lot of, you know, components to it is that if you

14:15.120 --> 14:20.080
can make those more reproducible and deterministic, then ultimately is a software developer or an

14:20.080 --> 14:24.640
organization trying to, you know, build an impactful machine learning model that you can do that

14:24.640 --> 14:29.360
more quickly and, you know, more safely because you can sort of see what every step of the process

14:29.360 --> 14:34.240
was. So, you know, when you talk about machine or folks talk about machine learning is sort of a

14:34.240 --> 14:39.760
black box like you put in data and you get out some predictive model that's like hard to interrogate.

14:39.760 --> 14:45.120
I think part of it is that this is really powerful, powerful algorithms that are still hard to

14:45.120 --> 14:49.200
fully understand. But the other part of it is the normal way of developing them at least, you

14:49.200 --> 14:56.400
know, we work with everyone from, you know, university students to, you know, organizations like

14:56.400 --> 15:00.960
that are very much coming at it from like a business intelligence perspective. And in both cases,

15:00.960 --> 15:06.480
you know, because it's a new practice, there aren't patterns fully established yet. So people

15:06.480 --> 15:12.000
are frantically looking for the, you know, the tool stack to make it happen. And I think that ML

15:12.000 --> 15:17.040
ops has kind of done itself a bit of a disservice in that it presents itself as such a totalizing

15:17.040 --> 15:20.800
thing, which is really not if you're thinking about it from a software development perspective.

15:20.800 --> 15:26.800
It's not the, that's really, there aren't good parallels for that. Like in traditional software

15:26.800 --> 15:30.720
development or if we're making our web application today or your iOS app or whatever, you're using

15:30.720 --> 15:35.040
30 different tools. But you're doing it in your structuring it in a way that is, you know,

15:35.040 --> 15:39.120
roll an agreement on it. You know, you commit sort of the important parts of the pipeline,

15:39.120 --> 15:42.640
you add tests, you add checks so that someone can come in. They can, you know, everything is

15:42.640 --> 15:47.600
tied to source control. So, you know, you can, you can experiment in a branch and then come back

15:47.600 --> 15:52.000
in. And I think a lot of those are really useful for thinking about, you know, machine learning,

15:52.000 --> 15:55.680
not, not, you know, in its entirety, but certainly a lot of the components that people struggle

15:55.680 --> 15:59.360
with today and taking this from R&D into production, which is really the name of the game.

16:00.880 --> 16:07.920
You know, I think my, my guess is that folks who know paper space and, and gradient in particular

16:07.920 --> 16:14.080
from a user perspective, think of it as, you know, primarily what the interface that it's

16:14.080 --> 16:20.160
providing, kind of a notebook like experience and, you know, maybe notebooks as a service we could

16:20.160 --> 16:29.840
call it. And when you think of ML ops platform, you know, you think of all this enterprise-y stuff,

16:29.840 --> 16:37.840
containers and Kubernetes and things like that. How do you get from one to, to the

16:37.840 --> 16:42.640
next is all that stuff, just the stuff that you're using to allow folks to spin up these notebooks

16:42.640 --> 16:48.080
or what's the relationship? Yeah, so, so we call it kind of like the gradient notebook lab,

16:48.080 --> 16:53.200
which is, you know, the wrapper around Jupyter. You actually use other kind of IDE-like

16:53.200 --> 16:57.280
environments, but Jupyter has certainly become the de facto standard. That's really only one

16:57.280 --> 17:02.800
part of gradient, but it's all built on the same foundation. And that foundation is committed to

17:02.800 --> 17:08.320
the idea of reproducibility adding UUIDs to every step of the process. So even if someone comes in,

17:08.320 --> 17:13.120
and the most popular kind of product we have is certainly notebooks. Anyone can come in, we have

17:14.240 --> 17:18.560
a tool called the ML Showcase where they can one click, run it on a free instance in the cloud and

17:18.560 --> 17:24.560
kind of, you know, get their feet wet with the machine learning tooling. And that's important

17:24.560 --> 17:29.520
because our general view is that for every thousand software developers, you have maybe one

17:29.520 --> 17:33.280
who's a machine learning expert. So the question is, how do you onboard the machine learning technology

17:33.280 --> 17:37.120
for everyone else? And they have to try it out. So notebooks are certainly one part of it,

17:38.000 --> 17:43.840
but as folks kind of get more advanced within gradient, we offer a lot of like kind of

17:44.800 --> 17:49.680
ways of making the pipeline sort of building on top of it. So the notebook is always running

17:49.680 --> 17:54.000
in a container. When you shut it down, we do a Docker commit to make sure that you have sort of

17:54.000 --> 17:59.040
the reproducibility in the history. And then you can go a step further and then connect that to a

17:59.040 --> 18:04.320
GitHub repo. You can actually, we have like a job run or architecture, very similar to how

18:04.320 --> 18:09.200
kind of the paradigm has been established where you can use a CLI or Python SDK and then just

18:09.200 --> 18:15.120
send a task and have it get scheduled across a single node or a large scale distributed set of

18:15.120 --> 18:22.080
nodes. And then tools like we have one called gradient CI, which is just a GitHub bot or application

18:22.080 --> 18:28.560
where we give you rich pull request information back. So notebooks are the entry point, but they're

18:28.560 --> 18:33.040
not exclusive or they're certainly not the end of it. It's really the question of, where are you on

18:33.040 --> 18:37.520
the kind of data processing or development, machine learning development lifecycle?

18:40.000 --> 18:47.040
As you've built this out, where the areas that, you know, what areas presented the most

18:47.040 --> 18:53.760
challenges? And, you know, we can kind of take this from different levels. One thing that I'm

18:53.760 --> 18:59.280
thinking about you mentioned, like you're transparently committing stuff to get like, are you

19:00.480 --> 19:07.120
are you doing that? You know, whenever I close out of a notebook or every time I, you know, run a

19:07.120 --> 19:12.160
new cell or like, how do you? Yeah, that's the connection between, you know, challenges and that

19:12.160 --> 19:17.280
question is like, you know, there's all this questions about granularity and I imagine that,

19:17.280 --> 19:22.320
you know, you've had to figure out a lot of that stuff along the way. Yeah, so I would say to

19:22.320 --> 19:27.520
that particular one, I think that is still unresolved in the sense that I don't think there's

19:27.520 --> 19:31.680
the best version of it that will exist in a year or two that exists today and I think it's been a

19:31.680 --> 19:38.080
lot of back and forth. You know, you know, Jupiter notebooks are formatted JSON objects with

19:38.080 --> 19:41.840
sort of a special cell type and everything else and they don't really map super well to the code

19:41.840 --> 19:45.440
that's actually running in it, at least from like a code differing or source control management

19:45.440 --> 19:53.120
perspective. And you got folks like interact coming out of Netflix that are trying to, you know,

19:53.120 --> 19:57.760
do interesting things with notebooks to make them more runnable. You have folks that are trying to

19:57.760 --> 20:02.640
take notebooks and turn them into executable objects. There's this whole ecosystem of innovation

20:02.640 --> 20:07.120
happening around the notebook itself, not to mention stuff that Jupiter is doing. So it's a

20:07.120 --> 20:11.440
good moving target from that perspective. Totally. And from our perspective, it's just a container

20:11.440 --> 20:16.240
and I think that's the important part. You know, we actually, big fans of Interact, I think that's

20:16.240 --> 20:21.920
a really important step in the, you know, basically they've created a TypeScript and JavaScript,

20:21.920 --> 20:27.200
like React Library, a web framework kind of interface for Jupiter. And I think that's doing a

20:27.200 --> 20:31.840
lot to move this as, you know, another kind of traditional primitive that can be used in the

20:31.840 --> 20:37.920
Webstack. But from our perspective, it's a container that has to be scheduled on a node somewhere

20:37.920 --> 20:44.400
and even within that kind of part of the process, there's a lot of complexity. You know, there

20:44.400 --> 20:49.680
are a million types of instances you can run on. There are a million ways of, you know,

20:49.680 --> 20:53.840
setting up how you do storage and artifact management and things like that. And our perspective is,

20:55.120 --> 21:00.000
you know, you should, there are sort of like best practices of like, hey, we can, you know,

21:00.640 --> 21:05.360
one example here, we, within gradient, there are experiments that you can run, which are just,

21:05.360 --> 21:09.840
you give us a container in some code and we're going to execute that as well as notebooks. And

21:09.840 --> 21:14.080
both of them, you know, at the fundamental level of gradient, which is actually built on top of

21:14.080 --> 21:19.760
Kubernetes, is that we're thinking about these as containers with a lot of nice kind of glue

21:19.760 --> 21:25.360
in between. So, for example, providing a single persistent high performance directory that's

21:25.360 --> 21:30.320
available across notebooks, experiments, and ultimately your deployments as well. And so,

21:30.320 --> 21:36.480
thinking about as like an infrastructural problem where you want to, you know, deploy a model,

21:36.480 --> 21:41.440
be able to see what, you know, what experiment created it, who, what line of code generated that,

21:41.440 --> 21:45.760
and then also use the feedback from that, you know, in the kind of holy grail here, which is to

21:45.760 --> 21:51.360
create a kind of continuous life cycle. And, you know, I think that's also an unsolved problem,

21:51.360 --> 21:56.640
but the way to solve it is, you know, I strongly believe is one that is looks more like a traditional

21:56.640 --> 22:02.800
software, you know, engineering stack issue. And so, you know, we, you've seen a lot of like,

22:02.800 --> 22:07.280
a lot of the dialogue in the, in this ecosystem is around like build versus buy or, you know,

22:07.280 --> 22:12.240
what's the one true platform? Is it this, you know, is it the SageMaker or Qflow or gradient?

22:12.240 --> 22:17.600
I think that the, the, the way this will resolve itself is that it'll be just like everything,

22:17.600 --> 22:22.080
everything else. Your every software team will build certain components by certain components,

22:22.080 --> 22:26.400
and mix and match them in a way that looks probably a lot like how, you know, software gets developed

22:26.400 --> 22:32.000
today. You know, when we send our website out, we have like every release, we, we use tools like,

22:32.000 --> 22:37.440
you know, code coverage, regression testing libraries, visual testing libraries. You know,

22:37.440 --> 22:43.360
we use CircleCI for our CICD engine. We use a lot of ARGO. There's a whole host of tools,

22:43.360 --> 22:47.760
and within this organization, we have our own practices. Like, you know, we've defined among

22:47.760 --> 22:54.480
the engineers here, myself definitely less now, but, you know, how the, you know, what's the,

22:54.480 --> 22:58.000
how, when do you make a branch? When do you make a pull request? When is it okay to make a release

22:58.000 --> 23:01.680
on something? So a lot of that, you still have to build a lot of your own methodology around software

23:01.680 --> 23:05.920
development. That's what makes a, you know, an organization effective. But the tooling itself, you

23:05.920 --> 23:10.080
know, you will need purpose-built tools, and we're, we're kind of making the case that we're offering

23:10.080 --> 23:15.200
a version of this that we think is, you know, built to, you know, grow into the next iteration,

23:15.200 --> 23:19.600
where, you know, machine learning becomes more prolific, not as, you know, as a domain, but as just

23:19.600 --> 23:25.120
to software engineering practice, kind of more fundamentally. Do you think that the, at a given

23:25.120 --> 23:32.160
organization, the tooling is best provided by folks that are familiar with the, this

23:32.160 --> 23:39.680
soft existing software engineering, you know, practices, or, you know, folks that are more

23:40.320 --> 23:47.120
coming at it from the, the data science side. And, and, you know, curious what you've seen in

23:47.120 --> 23:53.760
that regard? Yeah, I mean, I guess I'll add to the controversy. I do think this is something that,

23:53.760 --> 23:59.760
you know, at least at the very least has to engage the kind of CISOPS DevOps person, or the,

23:59.760 --> 24:04.320
or the team that's responsible for shipping production grade code, you know, whatever that looks like.

24:05.120 --> 24:09.200
You know, I don't think it's the domain of the data scientists or machine learning engineer

24:09.200 --> 24:14.800
necessarily. But I also think that the interface needs to be better defined, meaning, you know,

24:14.800 --> 24:21.840
we have to find a way that people can easily integrate with source control management and,

24:21.840 --> 24:25.840
you know, the primitive artifact that gets deployed. Like, for example, you know, you've heard stories,

24:25.840 --> 24:30.080
I'm sure of, you know, a model gets developed and then it gets rewritten in Java by somebody else

24:30.080 --> 24:34.480
so that they can actually deploy into their system. I think the Kubernetes, you know, containerized

24:34.480 --> 24:38.800
future and that in the world we're living in presents a nice kind of in-between model,

24:38.800 --> 24:42.400
which is that if you can containerize this thing and sort of define its inputs and outputs,

24:42.400 --> 24:46.720
then the artifact that comes out is something that you can hand off to the DevOps team and they

24:46.720 --> 24:50.160
can they can deploy effectively, you know, and they can deploy tools like, you know,

24:50.160 --> 24:54.560
Prometheus or, you know, metrics and logging and things that really are not within the scope of the

24:55.600 --> 24:59.920
machine learning kind of domain. And I think that's it's the separation of concerns that I think

24:59.920 --> 25:04.240
isn't really well defined. And so the outcome of that is that really sophisticated companies,

25:04.240 --> 25:08.080
you know, Facebook, Spotify, et cetera, you know, they have to build their own tools because they

25:08.080 --> 25:12.720
need to push this discipline forward. That's not going to be the case for most companies in the

25:12.720 --> 25:17.200
world and then the question is sort of what does that tool stack look like, you know, in ours as a

25:17.200 --> 25:22.880
proposal that is very much designed for, you know, a company that wants to produce production

25:22.880 --> 25:26.400
grade software and how machine learning integrates inside, you know, into that process.

25:27.040 --> 25:33.360
Yeah, that's a really interesting example you gave. Like everything or like many things in tech

25:33.360 --> 25:38.640
maybe it's there's a bit of a pendulum going back and forth. When I first started tracking this

25:38.640 --> 25:46.640
space eight something years ago or more, a lot of the early folks, a lot of the early

25:48.400 --> 25:54.480
goals and messaging of these early tool vendors, most of which are not still around was,

25:54.480 --> 26:00.800
you know, your data scientists are developing software or developing models in R and SAS and things

26:00.800 --> 26:05.840
like that and you have no way to put them in a production. And so you have to hand them over to

26:05.840 --> 26:10.240
folks that are, you know, like you said, building them in Java or building them in C, C++, whatever.

26:11.440 --> 26:17.600
And the, you know, the whole idea of a lot of these early tools was that you can, you know,

26:17.600 --> 26:25.760
develop and deploy and Python, right? And a lot of the argument was that you wouldn't have to pull

26:25.760 --> 26:33.840
in a separate person. The data scientists could just do that. And then the, the next wave of that

26:33.840 --> 26:37.920
was, well, you know, ah, maybe that's not going to work. We've got our data scientists. Let's

26:37.920 --> 26:42.320
hire a bunch of machine learning engineers and, you know, they'll be part of this process. And

26:43.040 --> 26:49.840
as opposed to kind of letting the tooling handle the productionalization, we've got people that

26:49.840 --> 26:56.400
come from more software engineering background. Um, and it sounds like you're talking about a further

26:56.400 --> 27:02.400
evolution of this pendulum, which is, well, containers will just do it for us. Yeah, I mean, I think

27:02.400 --> 27:08.560
that, I don't know how much I'll commit to that particular version. I, I agree it's a pendulum.

27:08.560 --> 27:12.880
I, I do think that, um, you know, like I've said, I think this is a software engineering practice

27:12.880 --> 27:17.200
or at least there are a lot of, you know, pieces we can pull from that. Yeah. I do think it's made

27:17.200 --> 27:22.080
leaps and bounds forward on in the sense that, you know, containers are really, really helpful for,

27:22.080 --> 27:25.600
you know, building out web applications. They're portable, they're reproducible, you know,

27:25.600 --> 27:30.080
my dev environment is similar to yours. It's, you can't map that perfectly to, um,

27:30.720 --> 27:34.800
uh, you know, the machine learning development process. But there's, there's also, you know,

27:34.800 --> 27:39.280
newer concepts like there's, uh, in the Kubernetes world, there's a lot of talk about, you know,

27:39.280 --> 27:44.480
solving the inner loop of development, which is, um, basically saying, you know, we, we have the,

27:44.480 --> 27:48.800
as a software software engineering problem, we have these complicated systems, you know,

27:48.800 --> 27:52.640
uh, paper space and gradient is a whole lot of services at a bunch of microservices,

27:52.640 --> 27:58.160
a bunch of big services. Um, how do, how do you, uh, basically keep a fidelity between your

27:58.160 --> 28:01.200
production environment and your development environment in the, you know, because maybe your

28:01.200 --> 28:05.280
laptop isn't going to run the whole host of production environments. So, you know, a lot of

28:05.280 --> 28:09.280
interest in, or a lot of software development from coming out from Google, we have,

28:09.280 --> 28:13.600
got tools like scaffold and the idea here is that, you know, you want to, uh, they call the inner

28:13.600 --> 28:16.960
loop of development, which is like, before it goes into production, someone's just iterating

28:16.960 --> 28:20.480
on it, which I think actually is what a lot of the data scientists who push back on the machine

28:20.480 --> 28:26.240
learning or the ML ops kind of paradigm that is totalizing is, you know, uh, they're like, oh,

28:26.240 --> 28:29.280
well, I'm going to, I'm going to work in a notebook here locally and I just, you know, I don't think

28:29.280 --> 28:32.960
every line of code here needs to be committed to source control. And that's, and that's true,

28:32.960 --> 28:37.040
you know, you don't need to, you don't need to get, you know, heavy handed with it. Um, but it is

28:37.040 --> 28:42.960
an open problem and open discussion in even the Kubernetes world way, you know, uh, totally distinct

28:42.960 --> 28:46.800
from machine learning world. Um, and I, you know, I think that the conversations are most productive

28:46.800 --> 28:53.120
when the two, you know, engage in that way. Um, but yeah, I mean, I, like, people that are interested

28:53.120 --> 28:57.680
in the kind of deriving insight from a machine learning model, uh, you know, they want to be working

28:57.680 --> 29:02.240
in these amazing tools like TensorFlow and PyTorch. But at the same time, if you look at like PyTorch's

29:02.240 --> 29:09.200
evolution, um, is like another kind of, uh, proxy of this, you know, um, they have native C++

29:09.200 --> 29:14.560
bindings and there's a lot of, there's a lot of, um, uh, push to make this a regular software

29:14.560 --> 29:18.640
development thing because PyTorch has all sorts of limitations. It's a, it's a very expressive

29:18.640 --> 29:23.280
language, um, very powerful in machine learning, uh, but isn't maybe the best one for, you know,

29:24.000 --> 29:28.160
squeezing out the performance from your incredibly expensive cloud GB resources or something like

29:28.160 --> 29:31.920
that. Um, so I think it's coming from both dimensions, you know, if you've got like the software

29:31.920 --> 29:35.040
engineers trying to be more machine learning stuff, you've got the ML folks that are like, yeah,

29:35.040 --> 29:39.760
we want to, you know, become more, uh, kind of pervasive in an organization or have higher impact

29:39.760 --> 29:44.640
in what gets shipped, um, and it's an evolving practice. But I, I do think that, you know, uh,

29:44.640 --> 29:50.320
uh, I, I, or sorry, I would say I don't believe that there's one ML ops platform to rule them all

29:50.320 --> 29:55.840
and it has to do the best training, you know, uh, model aggregation, quantizing, and, uh, you know,

29:56.480 --> 30:01.520
AB testing and like the full end end. But I do think that there's a lot of value in tools,

30:01.520 --> 30:05.920
you know, like cube flow, which I would say is probably the closest analogy to, to gradient, um,

30:05.920 --> 30:08.720
where, you know, you're setting up something that's kind of foundational and there's,

30:08.720 --> 30:11.840
you know, in the case of cube led, and there's a lot of setup that's required. It is a complicated

30:11.840 --> 30:15.680
tool stack and in the case of gradient, it's kind of like, you know, we're going to manage it for

30:15.680 --> 30:22.080
you a bit more. Um, but yeah, I think that enlisting the, you know, DevOps systems people who

30:22.080 --> 30:26.800
have to manage the same, keep it running early on the process sets you up for success and allows

30:26.800 --> 30:30.400
you to ultimately get all the things we always talk about, you know, more models faster, you know,

30:30.400 --> 30:36.960
higher impact, all that kind. Right. Right. And is there a relationship between gradient and,

30:36.960 --> 30:42.880
and cube flow is gradient, you know, does gradient use parts of cube flow or is, uh,

30:42.880 --> 30:50.720
do we think of gradient as a paper space analogy to cube flow, but one that, you know, has features

30:50.720 --> 30:55.920
that are specific to the way you view the world? Yeah. Um, so it's an interesting question.

30:55.920 --> 30:59.520
I think it's evolving as well. I think cube flow is a really interesting project. It's still

30:59.520 --> 31:04.640
relatively early. And I think, um, you know, Kubernetes is also something that, you know, not a lot

31:04.640 --> 31:08.960
of organizations have expertise in yet. So it is something that is a little bit harder to get going,

31:08.960 --> 31:14.400
but very powerful. Um, where we overlap is we are also based on Kubernetes. I mean, Kubernetes is

31:14.400 --> 31:18.880
just container orchestration, some networking stuff, and some nice best practices. And I'm

31:18.880 --> 31:24.480
privilizing Kubernetes, but it doesn't do anything really around machine learning or auto scaling

31:24.480 --> 31:29.840
is in particular to GPUs and things like that. Cube flow does a lot in that way, but the best

31:29.840 --> 31:35.520
part of cube flow, um, at least what we've seen work really well are, uh, is, is it's,

31:35.520 --> 31:40.560
is the fact that it's based on top of Argo as the, um, kind of pipeline tool. So Argo CD.

31:41.360 --> 31:47.360
So we also use Argo. Uh, the difference is, you know, cube flow, uh, compiles down to Argo YAML,

31:47.360 --> 31:52.560
ultimately. Um, we have our own syntax that also compiles down. And so the newest iteration of

31:52.560 --> 31:57.680
this is that they are compatible at a kind of fundamental level, but, um, you know, we buy a certain

31:57.680 --> 32:01.920
things. Um, we also, I mean, the most important differentiator between those two projects is that

32:01.920 --> 32:07.200
we have a, um, very sophisticated like GUI console where you can do most things through, you know,

32:07.200 --> 32:11.040
again, because most data science, you know, data science and kind of statisticians and,

32:11.040 --> 32:16.000
and mathematicians are not the same people that, um, can, can string up very complex, uh, you know,

32:16.000 --> 32:22.240
Kubernetes tooling. Mm hmm. And for those that don't know, Argo is the underlying workflow

32:22.960 --> 32:30.240
engine. How is that workflow engine used in, uh, gradient or, uh, cube flow for that matter?

32:30.240 --> 32:35.360
Yeah. So the, I mean, the, the, the kind of underlying piece is this, uh, continuous integration,

32:35.360 --> 32:41.200
continuous deployment, uh, engine, which basically says, you know, um, here are the steps in the

32:41.200 --> 32:45.600
process. Like this thing happens, then this thing happens. Uh, I need to build a container, I need to

32:45.600 --> 32:49.680
pull my data in, I need to process that data and each thing is sort of a step in that process.

32:50.400 --> 32:53.360
Um, and then you start thinking about like what the triggers to that. So this is another area

32:53.360 --> 32:58.880
where the, the kind of CICD paradigm doesn't map fully between machine learning and, you know,

32:58.880 --> 33:04.960
web app development or containerized, you know, um, no JS apps, which is, uh, the,

33:06.640 --> 33:11.440
the types of triggers can be very different. Like normally the trigger in a, in a, in a, in a,

33:11.440 --> 33:16.000
when you make a web app or an iOS app is, you, you push new code, you have a code commit, and that

33:16.000 --> 33:20.720
causes, you know, that's the primary trigger. Um, in the case of machine learning, there are some

33:20.720 --> 33:25.920
interesting ones that I think still, you know, early, you know, early days, but, um, you know,

33:25.920 --> 33:31.040
there are concepts like model drift and data drift. Um, you know, we joke around, it's like,

33:31.040 --> 33:34.960
code is just, or a code commit is just code drift. It's just like something has changed. So let's,

33:34.960 --> 33:38.560
let's kick off this pipeline again. And Argo is a really nice one, which gives you,

33:38.560 --> 33:42.640
it's a Kubernetes native CI CD engine, and there are a bunch of components to it. And, and that is

33:42.640 --> 33:49.840
the, um, a big part of, of cube flow. Um, and we also use that as, as, uh, this sort of our

33:49.840 --> 33:54.160
underlying engine. Um, but on top of that, there are a lot of other things with just like the UI,

33:54.160 --> 33:58.880
how you expose it, the command line interface, the, the way you expose the syntax and semantics.

33:58.880 --> 34:03.360
And I think it's a race you talked earlier about the, um, you know, the right level of granularity.

34:03.360 --> 34:08.000
And I think that is the big question. I think that's where, you know, any startup in this space or

34:08.000 --> 34:12.960
any, you know, software coming to space really has to keep an eye on how it's moving because, um,

34:12.960 --> 34:17.760
best practices change every day and, you know, uh, new frameworks come alive. But I think we've

34:17.760 --> 34:21.760
seen some stabilization. I do think that the machine learning landscape from like a fundamental

34:21.760 --> 34:26.080
framework and tooling perspective is, is somewhat stabilized. Whereas, you know, two years ago,

34:26.080 --> 34:30.480
it was the pre-cambering explosion, whereas like every day something new came up. So I don't know

34:30.480 --> 34:33.920
your thoughts on that. If that's like something you're, you also would agree with or if it's, uh,

34:33.920 --> 34:40.080
we're still in the wild west. Now, I definitely agree with, with that statement, um,

34:40.080 --> 34:44.880
I've written quite a bit about it. We've got an ebook, the definitive guide to machine learning

34:44.880 --> 34:51.600
platforms that try to map out the early phases of this Cambrian explosion. Uh, and then, uh,

34:51.600 --> 34:57.680
another one, Kubernetes for ML ops, uh, which, you know, for anyone who's still listening who,

34:57.680 --> 35:01.760
you know, has alphabet soup in their head and wants to know what all these terms are and

35:01.760 --> 35:07.600
how they fit together. That will be a good one to check out. But I certainly agree that we're

35:08.320 --> 35:20.320
starting to see more, um, more kind of coalescing of the major components. There are well-defined

35:20.320 --> 35:28.000
subcategories now within kind of this tooling and platform landscape. And it is continuing to be

35:28.000 --> 35:36.800
interesting. Uh, I'm curious from your perspective for, you know, folks that are coming at this

35:36.800 --> 35:46.240
from the data science machine learning side, haven't done much with, uh, ML ops or processes,

35:46.240 --> 35:51.920
you know, do the usual, you know, I've got a notebook on a server somewhere, write some code,

35:51.920 --> 35:57.840
you know, pull that code out of the notebook and, uh, you know, somehow get it into production. Like,

35:57.840 --> 36:05.680
what, what's the first thing to do or, or the first three things to, to think about, to be more,

36:06.720 --> 36:10.880
as discipline the right word, uh, to advance your, your, your process.

36:11.680 --> 36:17.360
Yeah, I, I think that, um, I would, I would encourage the use of A tool, you know, obviously

36:17.360 --> 36:22.800
on bias. I think we, you know, we were, uh, we are approaches to onboard people into all this,

36:22.800 --> 36:26.400
these complicated tools that we're talking about, you know, the alphabet soup of Kubernetes and

36:26.400 --> 36:30.240
Argo and everything else. Um, the interface for gradient is actually quite simple, you know,

36:30.240 --> 36:35.280
the notebook is the starting point. Um, and, you know, it, it rewards closer inspections. So

36:35.280 --> 36:39.520
the deeper you go into it, the more you'll see where it adds value. Um, but I should have said,

36:39.520 --> 36:45.840
one of the first three steps besides use gradient. Yeah. Um, you know, I think I, I, the, the,

36:45.840 --> 36:49.760
they're really the ones that I think probably most people with space would agree on. It's like, um,

36:49.760 --> 36:53.760
find a, a problem that you want to solve that's relatively narrowly scoped because that,

36:53.760 --> 36:57.840
you know, we've worked on, we internally, we have a lot of machine learning projects and the ones

36:57.840 --> 37:01.440
that do really well are the ones that have a very clean sort of scope and we know what we're trying

37:01.440 --> 37:06.160
to solve. Um, I think when it becomes unbounded, like find something interesting in the data,

37:06.160 --> 37:10.400
it, it gets really hard because not only are you having tooling issues, you're trying to figure out

37:10.400 --> 37:16.240
what the, the question is. Um, and that's it, that's very attractive given that, you know, um,

37:16.240 --> 37:19.840
you can download TensorFlow and you kind of get superpowers like it's in, it's a, it's a,

37:19.840 --> 37:24.160
it is a fundamentally transformative thing that can do things that you, you know, can't do otherwise.

37:24.160 --> 37:29.680
But, um, the fundamentals are still there, which are, you know, you, you should commit to a process

37:29.680 --> 37:33.760
of, or sorry, define the problem that you want to solve. Any organization trying to onboard

37:33.760 --> 37:38.560
to machine learning broadly today, um, if they haven't already, would have to locate where they

37:38.560 --> 37:43.760
want to apply it. In our case, the first thing we ever did was, um, just, uh, uh, kind of like

37:43.760 --> 37:47.760
fraud detection and lead scoring. We get people to sign up every day, um, you know, at this point,

37:47.760 --> 37:52.960
we've had, I think over 350,000 people sign up. This is during the Bitcoin boom as well, so we

37:52.960 --> 37:56.720
want to make sure that, you know, who's a, who's a real actor, who's not. And we had a very narrow

37:56.720 --> 38:01.680
sense of that. And that's, that's a production, um, thing that we use today. But when you, when

38:01.680 --> 38:07.680
it's more unbounded, it becomes much harder. Um, I think, you know, developing the practice around

38:07.680 --> 38:12.720
it in software development, it is the collection of the, you know, GitHub readmys and the,

38:12.720 --> 38:16.480
whether you do standups, you know, it's like, there is a software development practice way beyond

38:16.480 --> 38:21.600
the tooling. Um, and, you know, I think that's important. Uh, also source control management is

38:21.600 --> 38:26.480
great. You know, uh, you don't, you don't want to be, you know, naming your files, um, you know,

38:26.480 --> 38:31.920
final, final, final three dot ipinb, uh, because it's just, you know, you're, you're going to put

38:31.920 --> 38:37.760
get yourself in, uh, in, uh, you know, complicated mess over time. So I think source control management

38:37.760 --> 38:42.160
is, is key. But, um, yeah, I don't know if that was two or three, but I, you know, I think the

38:42.160 --> 38:46.320
fundamentals are true. Regardless of, and again, this goes back to like, I think this is, uh, you

38:46.320 --> 38:51.600
know, more well understood problem than maybe we all have, uh, realized. And so that's coming at

38:51.600 --> 38:57.040
things from the data science perspective. What about coming at things from the engineering

38:57.040 --> 39:05.440
perspective, you know, say you've been working on, on these kinds of problems, uh, at a relatively

39:05.440 --> 39:10.560
modest scale, you know, what are the kinds of what are the gnarliest, you know, technical issues

39:10.560 --> 39:18.000
you run at trying to run this at, um, you know, provider scale. Yeah, that's a really good one. Um,

39:19.360 --> 39:24.080
open AI actually outlined a lot of them, which are when you start doing this a lot. So for example,

39:24.080 --> 39:29.840
we run kind of a public gradient instance, which is where we run all the free notebooks. Um,

39:29.840 --> 39:34.560
and so it's basically like one big managed instance, uh, you know, there are some issues of scale

39:34.560 --> 39:38.720
for sure. Uh, you know, you're going to hit issues of pulling containers down, caching them

39:38.720 --> 39:42.640
at CD, which is the tool that kind of, you know, an essential tool for communication within the

39:42.640 --> 39:47.040
process. It does break down at a certain level. Um, you start have to, you know, look, you have to

39:47.040 --> 39:51.760
look for things like where, you know, your bottlenecks are. And so we've spent, you know, many years now

39:51.760 --> 39:57.760
optimizing this, um, you know, the way the pieces connect to one another because it's very likely

39:57.760 --> 40:02.320
in your first app at it, you will have an expensive GPU and an expensive CPU and a lot of memory,

40:02.320 --> 40:06.880
but you're actually using not of it due to something totally outside of the, uh, you know, of that

40:06.880 --> 40:12.240
thing. Um, and figuring that out is hard. I think big teams, you know, it pays off to invest in that.

40:12.240 --> 40:16.720
I think Uber, you know, does great in that they have an expertise here. And this is a, you know,

40:16.720 --> 40:23.040
an internal effort, but I think a lot of companies would benefit from this tooling as well, um,

40:23.040 --> 40:27.680
without having to go down that path of building custom, you know, or entirely purpose-built tooling.

40:27.680 --> 40:30.560
There will always be something that gets built internally. We have all sorts of, you know,

40:30.560 --> 40:35.440
admin tools that we build, but, um, fundamentally, we committed to a containerized process. We've

40:35.440 --> 40:40.880
adopted newer practices. Um, uh, and I think that pays off and, you know, it allows us to ship,

40:40.880 --> 40:44.960
ship a new version of the software every two weeks, you know, or whatever the release cadence is.

40:44.960 --> 40:49.360
Um, and that's, that, that's the holy grail. It's like you want to be able, you know,

40:49.360 --> 40:53.840
machine intelligence broadly is like, can you build out a model that is predictive and powerful?

40:53.840 --> 41:00.240
And also, um, is in a system that can then be fine-tuned or, or modified as it reacts to the

41:00.240 --> 41:05.280
environment. So it's like we built these predictive models that are incredible. Um, how do you,

41:05.280 --> 41:09.840
you know, now work that into, to everything else? And I think that's the open question around,

41:10.480 --> 41:13.360
uh, how software development, you know, and maybe there's something to learn there,

41:13.360 --> 41:16.960
and, and some of it has to be redefined. You know, like we didn't talk a lot about hardware,

41:16.960 --> 41:20.720
but that's, that's a, you know, I think we might have had a conversation about that in the past,

41:20.720 --> 41:26.320
but, um, accelerator, you know, GPUs were, were video game cards, and now they're, uh, you know,

41:26.320 --> 41:30.960
ML processing cards. And so there's a whole host of new things coming out. And I think that's

41:30.960 --> 41:35.680
going to lead to a lot of interesting developments in the space that we're in, which is largely

41:35.680 --> 41:40.560
involved with like the interface between hardware and physical infrastructure and, you know,

41:40.560 --> 41:46.080
uh, sending, uh, software into production, and then the actual media, the problem of like,

41:46.080 --> 41:51.040
I want to build a model and do something amazing. Anything you're particularly excited about,

41:51.040 --> 41:59.040
and can talk about on that front? Um, yeah, I, I think broadly actually gets back to one of the

41:59.040 --> 42:03.840
things I kind of earlier in the conversation. I, I am particularly interested in, uh,

42:03.840 --> 42:09.520
uh, where the visualization pipeline and the compute pipeline collapse. Uh, we actually,

42:09.520 --> 42:14.880
I was at, uh, SIGGRAPH, the big visual effects conference, um, in LA, maybe three, three,

42:14.880 --> 42:19.920
three years ago or so. Uh, and it was clear that AI was sort of making its way in there,

42:19.920 --> 42:25.920
but everything from, you know, better rendering to bots in games, um, and then on the other end,

42:25.920 --> 42:31.040
you have, you know, uh, driverless cars, simulators that can simulate environments. And I think the

42:31.040 --> 42:35.520
newest iteration of that is this, um, you know, a lot of talk around synthetic data, you know,

42:35.520 --> 42:39.200
if one of the limitations of building powerful machine learning models is generating data,

42:39.200 --> 42:43.680
how can that be done more quickly? Um, you know, can you use Unity? And I think the,

42:43.680 --> 42:47.840
the blend of those gets really interesting and I don't know exactly where it goes, but it is an

42:47.840 --> 42:52.320
area that I think is fascinating right now, um, which is now you're roping in the, you know,

42:52.320 --> 42:56.560
the content creators as well into this, this tool and it speaks to the power of it, you know,

42:56.560 --> 43:01.040
it speaks to you of this, this tool, you know, machine learning and its current, you know, iteration,

43:01.040 --> 43:05.280
it's so powerful, it can do things that are, you know, mind-boggling and complex and, you know,

43:05.280 --> 43:11.600
we need to regulate in probably lots of ways, but, um, uh, you know, it's making its way into, uh,

43:11.600 --> 43:17.520
you know, the universe way beyond sort of traditional ML stats and, you know, data science. And so,

43:18.560 --> 43:22.160
there will be a lot of dimensions that emerge. Software development is the one I'm thinking a lot

43:22.160 --> 43:27.680
about, but I think they're, you know, yeah, it's an interesting time for sure. Yeah, over the years,

43:29.280 --> 43:38.800
Jensen's visualization demos at NVIDIA's GPU conference have gotten extremely impressive,

43:38.800 --> 43:44.640
like all the ray tracing stuff and some of the simulation stuff that they're doing. And again,

43:44.640 --> 43:50.400
to your points, the same hardware. Yeah, I, yeah, to me, that's where it gets really, really,

43:50.400 --> 43:54.160
really interesting. It looked like those were two totally diversion paths, and now it looks like,

43:55.600 --> 44:00.400
you know, we had, we had one point, it might still be live. We had a guy who was, um,

44:00.400 --> 44:06.080
training a model to, to drive his, uh, grand theft auto car on Twitch, and people could pay into his

44:06.080 --> 44:09.840
account, and it would, you know, pay for his paper space credit so we could run the whole thing.

44:10.880 --> 44:15.120
And this, this car was like, you know, it was not, I wouldn't, I wouldn't get in the car, but it was

44:15.120 --> 44:18.560
a funny, you know, experience for us. Like, we're going to train this thing using, you know,

44:18.560 --> 44:22.000
grand theft auto. And it's like, those are interesting to me. Like, it's just like,

44:22.000 --> 44:26.320
a totally emerging use case and, and, you know, something as possible that was never possible before.

44:26.960 --> 44:33.040
That's awesome. That's awesome. Uh, so one last question for you. I'm curious if, you know, given that you,

44:34.000 --> 44:43.520
uh, you, you've got all these users doing machine learning stuff, uh, on, on gradient, any unique insights

44:43.520 --> 44:49.280
into what's coming based on what folks are doing. Do you get that kind of visibility?

44:50.240 --> 44:55.200
Yeah. I mean, I think, candidly, I think a lot of people are still at the beginning of understanding

44:55.200 --> 44:59.440
this technology. And, and I don't mean the, the folks that, you know, we hang out with at NERIPS.

44:59.440 --> 45:03.600
I mean, the, the, you know, the software engineers that are, you know, building, building every

45:03.600 --> 45:08.960
other software tool we use, I think within that universe, machine learning is still exotic, new,

45:08.960 --> 45:14.880
um, it, you know, it speaks to why, you know, Jeremy, uh, you know, can, can get so much interest in

45:14.880 --> 45:19.120
fast AI, you know, he present, he presents fast AI is like the pragmatic way of getting started with

45:19.120 --> 45:25.760
machine learning. Yeah. Exactly. And I think that that resonates a lot because it's still new. So,

45:25.760 --> 45:31.120
from, you know, at this point, we've run something like 60 million hours of compute, you know,

45:31.120 --> 45:35.520
through the platform. And most of that is probably training, you know, or, you know, a lot of it is

45:35.520 --> 45:39.280
probably the first model that you train and trying to swap the data set out to see like what, what

45:39.280 --> 45:44.960
you can do. Um, I think it gets more interesting as that you're like, you know, wow, this actually

45:44.960 --> 45:50.640
worked in some way. Now how do I blend it and, uh, you know, or, or do something with it? Um,

45:50.640 --> 45:55.040
in most cases, like, it's like, how do I take this model and turn it, you know, make it on an iPhone

45:55.040 --> 46:00.400
or, you know, deploy it? I think those are still really hard things. Um, so where's it going? I,

46:00.400 --> 46:05.920
I think that generally you will see more regular software engineers, whatever the, you know, whatever

46:05.920 --> 46:10.640
that encompasses, you know, using these tools and understanding them more because it's sort of

46:10.640 --> 46:18.080
moving past academia. Um, academia is still mind-blowing in how, you know, how fast it can move ahead.

46:18.080 --> 46:22.640
Every conference, it's like, wow, this is something that was totally unfathomable even a,

46:22.640 --> 46:26.400
you know, a couple of years ago. But there's still a lot of catch up to do in the kind of, um,

46:26.400 --> 46:33.120
machine learning as a professional practice, I would say. Yeah. Awesome. It's been amazing

46:33.120 --> 46:38.320
catching up with you. Uh, too bad we work. We, uh, it's been a while since we've seen each other

46:38.320 --> 46:44.560
in person, uh, due to circumstances. Yep. We're controlled. But, uh, maybe one of these days,

46:44.560 --> 46:47.600
we'll be able to grab a coffee or something at a conference.

46:47.600 --> 46:57.920
Yeah. Cheer with. Thank you. Thank you.

