WEBVTT

00:00.000 --> 00:16.320
All right, everyone. I am here with Francesque Riera. Francesque is an applied machine learning

00:16.320 --> 00:21.280
engineer at the Lego group. Francesque, welcome to the Twomo AI podcast.

00:22.000 --> 00:26.720
Thanks. And thanks a lot for having me as well. I think it's a pleasure and super excited

00:26.720 --> 00:33.600
about the talk tonight as well. Same year. Same year. And thanks for taking the call at night

00:33.600 --> 00:39.600
or being on at night. It's a bit later for you. You're in Denmark. Yeah, that's correct. Denmark,

00:40.400 --> 00:44.560
well, as you know, headquarters for Lego was born in the small town of Pilon,

00:44.560 --> 00:51.280
here in Denmark. So living just across it. Very nice. Very nice. Why don't we get started by

00:51.280 --> 00:56.400
having you share a little bit about your background and how you came to work in machine learning

00:56.400 --> 01:03.600
and at Lego? Yeah. So, well, I think it's, it's not going to be a very long story,

01:03.600 --> 01:08.720
because I've been on the, I guess, on the market for roughly three years now.

01:09.520 --> 01:16.800
But I think my enthusiasm for ML and actually I should say my enthusiasm for computer vision

01:16.800 --> 01:23.680
started back in my bachelor's in industrial electronics in Spain. And that's just because

01:23.680 --> 01:30.320
I was studying the, the bachelor's in electronics. And then the last semester was focusing on robotics

01:30.320 --> 01:36.160
and then robotics. We had an introduction to computer vision. And I don't know why, but I thought

01:36.160 --> 01:41.840
this is then impressive. It's super interesting what you can do well in in reality with mathematics.

01:41.840 --> 01:47.360
And then matrices, right? For all the pictures and pictures and all of these sort of things.

01:47.360 --> 01:54.240
And then that drove me to then actually catching up on, on a master's in Denmark. So that's

01:54.240 --> 01:59.120
when I moved to Denmark to do a full on masters in computer vision and machine learning in the

01:59.120 --> 02:05.680
University of all work, which is in the north of Denmark. And after that, well, I guess I

02:05.680 --> 02:12.880
become sort of an expert in the matter. I hope so at least. And that got me then. I got a,

02:12.880 --> 02:17.280
a small job as a supporting engineer for a couple of months. It was not my thing. And then I

02:17.280 --> 02:24.320
found this splendid opportunity to live where I got to actually work with ML on active products,

02:24.320 --> 02:30.240
running in the cloud as well. Awesome. Awesome. It was speaking of industrial robotics and

02:30.240 --> 02:38.720
computer vision. One of the early, I think this was, I think this was early many years ago,

02:38.720 --> 02:52.160
demos of AI was like somebody built a Lego sorter using a treadmill kind of thing and a paddle

02:52.160 --> 02:56.160
or some kind of robot that would like sort the Legos into different pieces. Did you ever

02:56.160 --> 03:02.560
have you ever seen that one? It's funny. You mentioned it. And we are trying to

03:02.560 --> 03:09.040
and did that one from the internet to maybe exploring the options also to use it as a product

03:09.040 --> 03:14.160
for us. Oh, really? So it's quite funny that you actually mentioned it. I think we talked about it

03:14.960 --> 03:25.440
two, three weeks ago. Oh, that's funny. If I remember correctly, I remember reading a hacker news thread

03:26.320 --> 03:32.000
when this was published. Again, I think it was probably like five years ago or four years ago or

03:32.000 --> 03:40.080
something. There were apparently a bunch of people that would like go on eBay and buy these

03:40.080 --> 03:46.240
gigantic bags of miscellaneous Legos and they were talking about using these kinds of devices

03:46.240 --> 03:52.480
to sort them and then resell them. Apparently, there's a bit of a cottage market, so to speak,

03:52.480 --> 04:01.040
in remarketing Legos. And it's also one of the big campaigns that we're also running is

04:02.080 --> 04:08.640
even though Lego is primarily made of plastic wrap, so you want to give your bricks a

04:08.640 --> 04:14.720
longest life ever, right? And I mean, if you had some Lego set from the 50s or the 60s, you

04:14.720 --> 04:20.480
would still probably use it today. So I think that's also what drives this enthusiasm, right?

04:20.480 --> 04:26.240
On you being able to, all right, let's try and make this, you know, circular economy,

04:26.240 --> 04:33.360
you know, way right for the brick. Yeah. Yeah. Well, I brought that up not thinking that it was

04:33.360 --> 04:43.840
something that you were actually thinking about. But you know, maybe we can jump into how ML is actually

04:43.840 --> 04:54.400
used today at Lego. Yeah, so maybe I can just start putting it aside my area. So my team going

04:54.400 --> 05:02.160
from, I guess, the bottom to the highest. So I'm part of a team where we are three ML engineers

05:02.160 --> 05:09.440
and then full stack and then all of the surroundings right. And in my team, we have two main

05:09.440 --> 05:15.440
products, which I can just quickly describe now. We're going to go into details a bit later if you

05:15.440 --> 05:23.120
want as well. So the main product that started also now to two and a half years ago, approximately,

05:23.120 --> 05:31.440
it's a moderation service. So as you know, Lego is of course a brand for aimed at children, right?

05:32.400 --> 05:38.560
And children are our main responsibility. So of course, all of these, for example, social media

05:38.560 --> 05:46.560
applications or games or anything where children can upload their content, let's say images,

05:47.520 --> 05:52.800
text, videos, anything that can be shared and can be created by them on the phone or on the computer

05:53.440 --> 05:59.520
needs to be moderated by law before it's actually available or online for all the children to

05:59.520 --> 06:06.560
sit right as to avoid the obvious things right. And I mean, we know Facebook does that. We know

06:06.560 --> 06:11.680
Instagram does that. A lot of companies do it, but not in the level I would say as level as it.

06:12.800 --> 06:21.520
And that's because in Facebook, you could post something that is obscene and it's not necessarily

06:21.520 --> 06:26.080
going to be deleted as you upload it, but it's going to be deleted after the fact, maybe because

06:26.080 --> 06:32.000
nobody reported you, right? That cannot happen in our applications because the damage could already

06:32.000 --> 06:36.640
be inflicted and that's not, it's definitely not, you know, the image you want to give to the brand.

06:37.680 --> 06:44.000
So yeah, so our product, what it does is receive the content created by the users and we do an

06:44.000 --> 06:50.080
initial profiting of these content. So on images, on text and on videos, we try to project already

06:50.080 --> 06:53.840
what is the most obvious things that should not be in the app. Like if you uploaded a very bad,

06:53.840 --> 07:00.640
bad image, if you showed your face or some sort of identifiable information, you cannot also

07:00.640 --> 07:08.000
share that. So we also remove that. Of course, the obvious profanities. And we are working also

07:08.000 --> 07:14.080
in extending a couple more detectors. And detectors, by detectors, I mean machine learning algorithms,

07:14.080 --> 07:21.600
actually. So we do this profiltering, what gets rejected by us just gets feedback to the users

07:21.600 --> 07:28.000
and hey, we are very sorry, but your creation is not allowed because of this. Try to take a new

07:28.000 --> 07:33.040
picture or try to be nice writing the text. So some some some post different for that.

07:34.160 --> 07:37.920
If the content is approved by us, it then goes to manual moderation. So there's a company

07:38.560 --> 07:44.160
moderating every single piece of content before it is published. So what we accomplish with our

07:44.160 --> 07:50.560
product, right, is what for the students, right, one of them being monetary aspect because

07:51.520 --> 07:56.880
each piece of content that is very obvious does not need to go to a moderator that

07:56.880 --> 08:04.160
costs per piece, right? So in that sense, what that's what we manage to prove. And of course,

08:05.120 --> 08:12.960
if let's say, you know, there's like, I don't know, a big fair with Lego and people are

08:13.920 --> 08:19.120
asked to upload pictures because they are in the fair. It's like, you need to build a car and then

08:19.120 --> 08:24.160
you need to post a picture of the car. So of course, they would like to post a picture, but if the

08:24.160 --> 08:29.200
picture was bad for some reason, it would be nice that they get the feedback instantly rather than

08:29.200 --> 08:34.160
having to wait. So when when it goes to manual moderation, there is a time frame of five minutes.

08:34.160 --> 08:40.800
I think it's five to 15. I'm not 100% sure because it's not my part. But the thing is our

08:40.800 --> 08:47.680
our service provides a response on 10 seconds for whatever time. So if you uploaded something,

08:47.680 --> 08:52.800
let's say you take a selfie with your car, like, oh, we really like your car, but you cannot have

08:52.800 --> 08:58.080
your face there. So they get the response in seconds, like, oh, my car is still here. I take another

08:58.080 --> 09:06.240
picture, life's good again, right? And then the second product we have, so after everything has

09:06.240 --> 09:15.520
been approved. So when your creation is live on the social media app, then we have an automatic

09:15.520 --> 09:22.640
job every morning that will take all of the creations from the users. And then again, with two

09:22.640 --> 09:29.920
sorts of algorithms, so one for image and one for text, we try to identify what is the theme,

09:29.920 --> 09:37.360
the predominant theme on the on the on the creation. So for example, you have a big Star Wars,

09:37.360 --> 09:41.600
you have a big Star Wars fan, then you have a lot of Star Wars sets, you take picture of your

09:41.600 --> 09:48.800
Star Wars set, we probably will identify it as a Star Wars. And then what we do is like, okay,

09:49.440 --> 09:55.680
this kid has uploaded the Star Wars picture or Star Wars album collection. So what we do is then

09:56.560 --> 10:04.320
we send the like with an NPCs on the application, and we also use different NPCs to send comments,

10:04.320 --> 10:09.440
reinforcing them and encouraging them to keep building, saying, oh, that's a very nice

10:09.440 --> 10:14.000
Star Wars creation you have. I would love to see more from you and see more like that.

10:14.560 --> 10:21.280
And the NPCs are bots or characters and, yeah, they've already existed in the app,

10:22.560 --> 10:29.680
and they've been used. So before we set up this environment for this product, it was used by

10:30.640 --> 10:39.040
my people. So me as a, I guess, consumer, engagement person on the team, I would go in,

10:39.040 --> 10:45.600
I would log in as the, let's say Chihuacca, and then I would comment out on a couple of things.

10:46.480 --> 10:51.200
So now what we're doing is we are automating the job. Of course, there's still some people

10:51.200 --> 10:56.400
looking at the comments and maybe doing more specific comments, depending on the scenario.

10:58.000 --> 11:02.400
But it's been proven also to actually give a lot of value because we have received a lot of

11:02.400 --> 11:09.920
uploads saying, oh, look, legal life, Emmett, like my comment, like my post, and they are super happy

11:09.920 --> 11:16.240
right? So it's also very, it's very fulfilling for us as well to see that the reactions are positive,

11:16.240 --> 11:23.680
right? And you mentioned there are there are three ML engineers on that team.

11:23.680 --> 11:34.240
And how, what's kind of the broader size or scope of ML and data at like a?

11:35.920 --> 11:45.360
Well, so back in March, we had also a big reorganization where we then found that also what

11:45.360 --> 11:51.360
is now the department called the data office. So my team resides within the data office and

11:51.360 --> 11:56.320
then the data science products. But from the data science products, for example, we have a couple

11:56.320 --> 12:02.560
of others. So of course, in the legal.com page, where you go to the shop, right? And you buy

12:02.560 --> 12:08.560
a start-wall set, we have, of course, a recommended engine saying, oh, you like the dits there,

12:08.560 --> 12:15.200
maybe you're all generated in the X-wing, right? So the recommended team is also, I guess, our

12:15.200 --> 12:23.840
sister team working using data as well and developing ML solutions. That's also true. Yeah,

12:23.840 --> 12:31.440
two other product teams, one working on marketing effectiveness. So for example, if you have a

12:31.440 --> 12:38.320
new release of Legonin Chago Netflix, how is that performing compared to, I don't know, Barbie or

12:38.320 --> 12:46.160
or whatever other cartoon is for children at the time, right? And then the last one is the

12:46.160 --> 12:52.960
man forecasting. So trying to be ahead of the curve, which I guess the team had a lot of fun in

12:52.960 --> 12:59.200
Corona times because it's very difficult now. There was no curve. It was just a pick and it's like,

12:59.200 --> 13:05.600
well, we are running out of stock everywhere. So that's it. Yeah. And then a way, of course,

13:05.600 --> 13:10.560
having an incubation team as well. So the incubation team is trying to analyze different

13:10.560 --> 13:17.600
areas, sectors and departments at Lego where automation could be beneficial, for example,

13:17.600 --> 13:23.920
using machine learning. And one of the examples is this brick shorter using a treadmill. Well,

13:24.640 --> 13:30.480
it's a bit more high-scale. That's the idea, right? Nice, nice.

13:30.480 --> 13:40.400
With the, you mentioned the moderation app, it reminds me of a recent interview that we published

13:41.760 --> 13:47.680
with someone who works in cybersecurity. And you know, in that environment, as you can imagine,

13:47.680 --> 13:52.800
it's kind of very adversarial. You, you know, plug one hole, someone's trying to poke another.

13:52.800 --> 14:00.160
And moderation in your environment is it, you know, quite the case that there are bad actors,

14:00.160 --> 14:06.320
as opposed to like, are you, are you, is the task primarily identifying passive,

14:09.120 --> 14:15.040
passive behavior that you don't want, or are there bad actors, so to speak, that are trying to

14:15.040 --> 14:24.240
abuse the system? I think there's a, there's a bit of both actually. Generally speaking, we see

14:24.240 --> 14:30.160
the cases where, you know, maybe, maybe you had the phone in your pocket, right? And accidentally,

14:30.160 --> 14:35.120
it took a picture. I mean, it happens to everybody, right? And then you post a picture by mistake.

14:37.120 --> 14:42.560
And that happens quite often. Sometimes we also actually caught some bugs in the app, because it's

14:42.560 --> 14:48.000
like, well, is it not about we're getting, I don't know, a thousand black images or a full white

14:48.000 --> 14:52.880
images, you know, no, it should not make any sense. It's like, okay, well, maybe you should look

14:52.880 --> 15:01.840
into the latest police, you made, and so that was a funny, funny fact. But also, and that's maybe a

15:01.840 --> 15:06.960
bit above the, where the product, our protocols, the migration product, and that's more on the,

15:06.960 --> 15:15.200
on the manual moderation, because they do track behaviors and bad actors. And as far as I

15:16.320 --> 15:21.760
know, I think there's been a couple of cases where somebody got banned, they opened more accounts,

15:21.760 --> 15:29.040
they keep doing the same, got banned again, the game. So there's a little bit of that. I was curious

15:29.040 --> 15:38.320
if that was, if that was a something that you had to deal with how that played into the way you

15:38.320 --> 15:45.840
approach the model development, but it sounds like that's kind of downstream of what you're doing.

15:46.560 --> 15:53.520
It is because it is, again, we are not Facebook or Instagram, right? We need to make sure nothing

15:53.520 --> 16:00.000
back gets published. Right. So that's why there's the extra layers of security here. Do you

16:00.000 --> 16:09.600
incorporate the input of the human moderators? Like, is it a kind of a human and a loop type of

16:09.600 --> 16:18.000
situation where you're using their judgments to evolve your models? Yeah, yeah, that's right. And

16:18.000 --> 16:24.880
that's something we started doing, I think, at the beginning of this year, maybe the end of last

16:24.880 --> 16:30.320
year, you know, with the Corona year, everything specifically started for me and the calendar. But

16:31.920 --> 16:38.400
what we managed to do, yeah, link was this external moderation company. So when they've

16:38.400 --> 16:45.600
reject an image or a text or a video, we get the feedback that saying, okay, this content was

16:45.600 --> 16:54.080
rejected because of this. And what we did is we made a feature store. And it's a very simple

16:54.080 --> 17:00.160
reason for making a feature store rather than just a database, I guess. It's that the data by

17:00.160 --> 17:08.960
law, so GDPR, it cannot be stored forever. Meaning that let's say you are a user of the social media

17:08.960 --> 17:14.400
and one day you call to consumer service and say, hey, I want all my data to be deleted. We cannot

17:14.400 --> 17:20.720
have this link because we are in a way a standalone product, right? So we are not linked to the app.

17:21.920 --> 17:26.560
Meaning if you call to consumer service, you said, I want to delete it. Your data is deleted.

17:27.440 --> 17:31.760
But nobody's going to tell us, hey, this user called because, well, first of all,

17:31.760 --> 17:36.000
I don't have your user data, for example. So it will be impossible for me to track who you are.

17:36.000 --> 17:43.680
But what we accomplished with this also is we generate features and the features are generated by

17:43.680 --> 17:52.320
well-known machine learning algorithms. For images, we are talking ResNet50. For text samples,

17:52.320 --> 17:58.240
we're talking about multilingual bird, for example. So in that sense, we get the images,

17:58.240 --> 18:06.640
text samples, we get the features out of it, which are anonymized because if you have pulling

18:06.640 --> 18:12.560
layers in a neural network, right, you cannot undo a pulling. You could, but you could not get

18:12.560 --> 18:18.640
a one-mile script. And then those features are saved and then labeled with the human interloop.

18:19.840 --> 18:25.760
Got it. So rather than saving the original images, you're saving these representations from

18:25.760 --> 18:37.680
these neural nets. And one of the driving reasons behind that is to kind of avoid the GDPR

18:38.640 --> 18:42.160
responsibilities if that was personally identifiable data.

18:43.360 --> 18:51.760
Yeah. And whether it is or it is not, if the user calls and wants the data deleted, it needs to be

18:51.760 --> 18:58.400
deleted. Otherwise, yeah, you are reaching the contract. I guess, yeah, I guess that's also within

18:58.400 --> 19:06.960
the GDPR. Meaning if they call whatever the consumer GDPR line is and ask for their data to be

19:06.960 --> 19:12.880
deleted, these representations get deleted from the feature store. Not the representations,

19:12.880 --> 19:17.680
because they have anonymized data. So that's fully anonymized data. There's no way to backtrack

19:17.680 --> 19:25.760
where and what it comes from. Yeah. Yeah. And so you said that, you described that as a rationale

19:25.760 --> 19:31.840
for a feature store versus a database, but you could still put those representations in a database.

19:31.840 --> 19:37.360
Well, yeah. I guess we like to make a fancy words, so I can just call it a feature store rather than

19:37.360 --> 19:44.400
a database, right? Yeah. And I say that to probe a little bit deeper into that and are there other

19:44.400 --> 19:54.800
capabilities that you've built into this feature store that are specific to the way you're using it

19:54.800 --> 20:01.440
for machine learning. Yeah. Well, so as I said, right, so the features, which is anonymized data,

20:01.440 --> 20:08.000
gets labeled by regulators. In this case, it's, so actually we have labels for the moderation part,

20:08.000 --> 20:15.840
but we also have labels for the theme part. Now, and as I said before, so the images that are

20:15.840 --> 20:21.920
approved, they are published on the application right, which I mean are open source, not open source,

20:21.920 --> 20:28.480
but they are open to everybody because they come from a backend that publicly shares the images,

20:29.680 --> 20:34.960
so then what we do for this, for example, is that those rather than keeping the features because

20:34.960 --> 20:42.000
that model is a bit more complex, we keep a pointer to where the image is stored in the database

20:42.000 --> 20:47.200
for the application. And then we'll reuse that if the image was deleted by the customer,

20:47.200 --> 20:52.000
well, we don't have the image. It's a pity that it's not there, but at least it's not our problem anymore.

20:53.760 --> 20:58.880
What we do with this feature store and actually something where currently working on, hopefully,

20:58.880 --> 21:05.920
to be done by the end of the year, it's a sort of retraining pipeline plus a bit testing framework

21:05.920 --> 21:12.640
as well with both actually. So the idea is checking this feature store ever now and then, right?

21:13.200 --> 21:18.960
Okay, we have no features for retraining, you know, the personal detector, right? Yeah, we don't,

21:18.960 --> 21:25.280
all right. Then let's retrain. Before we launch live, even though we think the model is better,

21:25.280 --> 21:29.200
right? Let's have a maybe testing where both models are running the production at the same time.

21:30.080 --> 21:36.720
And then it's up to of course us as developers, but also part of the business side of moderation

21:36.720 --> 21:40.560
to decide, all right, the new model is better or no, the new model is no better.

21:40.560 --> 21:55.680
With the AB testing framework, I'm curious how you're kind of packaging and deploying the models.

21:57.040 --> 22:04.720
Yeah, so for model experimentation and model registry, for example, we use the open

22:04.720 --> 22:13.120
source version of MLflow, which is the, I guess, the package from Databricks. We, what we did is

22:13.120 --> 22:20.320
we created our own, the Moster with the MLflow backend. And then within the model registry,

22:20.320 --> 22:27.840
so we have the most in either production or staging or of nothing. And in the core of moderation,

22:27.840 --> 22:35.200
the core of moderation is built upon a step function in AWS. So what we have in this step

22:35.200 --> 22:39.680
function is, okay, I have the person detector here. And if I have a staging model for the person

22:39.680 --> 22:47.600
detector, then the task is a parallel task where the inputs will flow towards both so that I can

22:47.600 --> 22:53.040
have a one-to-one analysis to say, okay, which model is performing better in all of the images, right?

22:53.040 --> 23:05.040
Okay, and so you mentioned step functions. Do you use serverless technologies pretty broadly

23:05.040 --> 23:12.240
in deploying ML? Yeah, I think, and that's also one of the biggest changes we did. I think

23:12.240 --> 23:19.680
this year, it's been quite a, quite a change here, I think. We are full on serverless, so everything

23:19.680 --> 23:26.960
is deployed with AWS sound, for example. And then everything is, yeah, step function, lambda,

23:27.760 --> 23:35.440
we have also an ECS Fargo running for the biggest model. But that's generally its own. Yeah,

23:35.440 --> 23:41.920
you could say everything is so interesting. And so I'm curious how you're, you mentioned,

23:41.920 --> 23:50.880
it sounds like you're evolving the infrastructure. You've evolved it quite a bit over the past year

23:50.880 --> 23:58.880
with COVID year or so. And I'm wondering if you can share kind of what the prior state was,

23:58.880 --> 24:07.840
and what were some of the drivers for moving to serverless and container service?

24:07.840 --> 24:14.560
Yeah, so, well, as I said, so the moderation products started two and a half years ago,

24:14.560 --> 24:21.680
and when we started, we thought it would be best if we tried to manage everything ourselves,

24:21.680 --> 24:27.760
which meant, all right, let's go full on lambdas, and then rather than, you know, having step

24:27.760 --> 24:34.560
function logic or things like that, let's go SQL, SNSs, Dynamos with streams.

24:34.560 --> 24:42.000
If I had a picture of the old setup, I think it would be scared, and many people may be

24:42.000 --> 24:48.000
listening also to be scared to see that image. But then that evolved a bit into, all right,

24:48.000 --> 24:54.000
let's go ECS, so let's go full on Fargo desks. The problem in that scenario is that

24:55.520 --> 25:00.880
we, in the moderation setup, because the application is, I wasn't going to say deployed,

25:00.880 --> 25:06.800
I guess, the moderation exists in 26 markets, and we get there constantly, all day, everything.

25:07.840 --> 25:13.520
It makes no sense to have a Fargo task that is stopped, and then has to be started when new

25:13.520 --> 25:19.520
data comes, because there's data coming in maybe a bit true for a second, which meant that the

25:19.520 --> 25:26.480
Fargo task I run at 24 hours a day, and then for the Fargo task, which is basically a Docker container,

25:26.480 --> 25:33.120
right, to receive whatever needs to be moderated, is that we needed to then enable a queue,

25:33.120 --> 25:38.640
so then that queue would get some data, and then at some point in the thread looping, right,

25:38.640 --> 25:44.800
you would get the message. I mean, it worked very well, everything was perfectly fine, and

25:46.160 --> 25:53.360
it was not a big deal, but then we consider that whenever we get new employees, new colleagues

25:53.360 --> 26:01.520
in my team, it's hard to explain the flow, it's hard to explain or maybe try to understand what

26:01.520 --> 26:10.720
is happening, right, and I think it was in December 2020 that AWS Sam came with an update,

26:11.600 --> 26:17.520
no, not AWS Sam, Lambda came with an update, where you can deploy your custom Docker images to Lambda,

26:17.520 --> 26:25.280
and that just, yeah, that made our work much easier. Life was better that day,

26:27.520 --> 26:31.360
because what we did is, well, we have to do very, which is really, let's just transfer them to Lambda,

26:31.360 --> 26:36.640
because we know how Lambda works, Lambda is amazing. AWS, not sponsored, I'm gonna say now.

26:40.000 --> 26:44.560
But having the Docker container running in Lambda, then we could also integrate it into

26:44.560 --> 26:50.400
the step functions without having to have task tokens waiting for the callbacks and a lot of

26:50.400 --> 26:55.920
complications, right, and effectively with this meant, and what this means up to today is that

26:55.920 --> 27:01.840
today we can deliver responses under 10 seconds when we talk for, you know, many images with comments

27:02.560 --> 27:10.080
and everything. Whereas before, we were below the minute, so we actually managed to reduce the

27:10.080 --> 27:15.040
average times from a minute to 10 seconds, which is quite the accomplishment, I think, as well.

27:16.160 --> 27:26.400
And do you run into runtime constraints for using Lambda when you're doing image inference?

27:27.840 --> 27:34.240
No, no, and that's because so what we do, and as I said, right, so the feature store, right,

27:34.240 --> 27:40.960
again, and clicking back to the feature store, so the images, they are passed through a resident 50

27:40.960 --> 27:44.800
without the specification layer, which is, I think, a very common approach, right, transfer

27:44.800 --> 27:49.120
learning. You've already got the representation, and you're just doing classification.

27:50.640 --> 27:58.240
And it's funny, because there was this Tesla AI event, a couple of weeks ago, maybe a couple of

27:58.240 --> 28:04.960
months ago, where they mentioned they are doing classification, image classification with something

28:04.960 --> 28:12.320
called Hydra. And what is Hydra? I mean, it's just a resident network, where then you have

28:12.320 --> 28:16.800
different heads, which classify different things. And I was like, well, this is exactly what we

28:16.800 --> 28:27.520
do. Why didn't you think of cool name like that before? We've talked quite a bit about the feature store.

28:30.000 --> 28:36.800
Can you talk a little bit about how that evolved or any challenges you have run into

28:38.240 --> 28:44.400
in bringing that project to fruition? Yeah, so the feature store was, I think it was,

28:44.400 --> 28:53.040
one of the biggest problems was from idea to reality, actually, because we were like, of course,

28:53.040 --> 28:58.720
a feature store makes sense, right? I mean, if you want our models to improve, you need data,

28:58.720 --> 29:04.800
you need labeled data, right? And I guess that is the suffering of every ML engineer, right? It's

29:04.800 --> 29:08.960
not about the data. The spread of data in the world is not, but it's not labeled, right? That's

29:08.960 --> 29:17.600
the problem. So the aviation was there, but we were, it took us quite a, quite a period of time

29:17.600 --> 29:23.840
to try to figure out what's the best approach. How do we do this best, you know, also future thinking?

29:24.400 --> 29:27.760
Because it's like, well, I mean, I could just, you know, get all the data from the manual

29:27.760 --> 29:35.680
moderation and stack it up in some one drive and, you know, let it work there forever,

29:35.680 --> 29:39.280
which is what a lot of people and a lot of companies do with the data, I'm afraid to say.

29:40.720 --> 29:46.080
But then we came to the realization, and I think it was also really some posts on other companies,

29:46.080 --> 29:50.880
how I think maybe it was Cooper, actually, how they did feature stores in Uber.

29:54.080 --> 29:59.680
So then we came to that to making a feature store client, everything we do is in Python,

29:59.680 --> 30:07.440
so it was in Python too. Well, when using the backend as AWS, because we are full on AWS,

30:08.160 --> 30:13.520
but of course, we tried to make it also cloud-agnostic so that if one day we move to

30:13.520 --> 30:19.920
Azure, or we'll move to, I don't know, any other else, then there's not that many problems,

30:19.920 --> 30:25.600
or it can also be integrated, right? And I think one of the learnings also we've got in the

30:25.600 --> 30:29.840
feature store in the first steps is that, all right, I have features, so let's say the feature,

30:29.840 --> 30:36.320
right? It's an image that went through Resident 50, so we go from a couple of megabytes image

30:36.320 --> 30:41.840
to seven kilobytes, a feature array, right? And I know it because I've seen many of them,

30:41.840 --> 30:42.960
so I know it's seven kilobytes.

30:45.280 --> 30:51.600
With all right, because we're going to get a lot of data, I think on average, we get around

30:51.600 --> 31:01.360
10 to 15,000 images per day on the evaluation platform, so you can think, right? So 10 to 15,000

31:01.360 --> 31:09.040
images per day, 10 to 15,000 features. Let's store them in S3, so we store them in S3,

31:09.040 --> 31:14.880
and then we have a catalog in no SQL database, which is DynamoDB and Amazon Work Services.

31:14.880 --> 31:22.960
Everything was fine. The data is starting, it's increasing in numbers, everything looks to be

31:22.960 --> 31:27.840
wonderful. Until the day we want to query the data for the first time, because we sell that.

31:28.480 --> 31:35.600
Let's see how the query works. Well, it turns out that S3 is not the ideal scenario when you want

31:35.600 --> 31:43.760
to query a lot of small files, because that is, and I'm not an expert in this sort of

31:43.760 --> 31:48.880
infrastructure things, but there's all of these handshake things that come between requests and

31:48.880 --> 31:53.520
get into data. That was taken longer than actually fetching the data itself, and because there's a

31:53.520 --> 32:02.800
lot of... Why was the query against S3 and not the Dynamo? Well, the DynamoDB was to keep a catalog,

32:02.800 --> 32:08.560
so in the Dynamo, you would be like, okay, this is the feature path on S3, and this is the label,

32:08.560 --> 32:16.240
so you just need to get the data from S3, because in our eyes, it's where data is supposed to be

32:16.240 --> 32:22.960
stored, it's in a bucket rather than a table. The problem that you ran into was just the latency

32:22.960 --> 32:31.920
for requesting a single file in S3 when your bucket had a lot of files. Yeah, exactly, and also

32:31.920 --> 32:37.840
requesting a big dataset, or a big feature dataset, because I guess for a couple of hundreds,

32:37.840 --> 32:42.960
for features doesn't matter, but I think that one of my colleagues did the calculation right,

32:42.960 --> 32:49.600
so it was, I think, 40 milliseconds per feature, and I'm not exactly sure of the number now,

32:49.600 --> 32:55.360
but what we learned is that we're probably into something else, so what we did is migrate the data

32:55.360 --> 33:01.680
itself as well to DynamoDB, so that the catalog, because I mean, the feature can also be converted

33:01.680 --> 33:07.760
to a bytes array, so then the bytes array can also be stored as, yeah, as no SQL entry write.

33:08.560 --> 33:15.120
So by moving the data into the catalog itself, what we managed to do is that the data now is

33:15.120 --> 33:23.520
fetched 16 milliseconds per feature, which is, you know, howling at the time, and that

33:23.520 --> 33:32.240
that actually proved to be very efficient. So is the data community there at LEGO? You know,

33:32.240 --> 33:38.640
is everyone kind of full stack, or do you have your more traditional data scientists, and then,

33:38.640 --> 33:45.360
you know, folks that are more ML engineers, how does the, what's kind of the range of skill sets

33:45.360 --> 33:53.360
or culture there in terms of full stackness, if that's a thing? We are very happy

33:53.360 --> 33:58.240
we have full stack development in our team, because she's a mastermind in doing our front-end

33:58.240 --> 34:07.760
tool. Before we had him, our think our front-end app was maybe something out of university,

34:07.760 --> 34:17.600
you could say. But so we are split actually, so we have, like myself, so we have machine learning

34:17.600 --> 34:24.400
slash data engineers, you would say, right? We also have the more standard data scientists

34:24.400 --> 34:36.240
as well, and then we have data management, folks, and platform people, so that once that are helping

34:36.240 --> 34:44.640
with the more standard infrastructure, like setting as the account, the security, and of the

34:44.640 --> 34:52.560
basic things that you need when you are an enterprise company. Interesting. We haven't talked

34:53.200 --> 34:59.360
in detail about the engagement tool, or some of the interesting challenges you ran into

34:59.360 --> 35:06.640
in developing that tool. Well, yeah, there's also been a couple, right? Because when you come out,

35:06.640 --> 35:16.160
fresh from university, right? You think data sets are beautiful, clean, and, you know, I think

35:16.160 --> 35:20.320
that's what a lot of people think about them, because when you read research papers, the data is

35:20.320 --> 35:26.880
just beautiful, right? And you have, I don't know, 16 GPUs, and all the money in the world, apparently.

35:28.560 --> 35:33.200
Well, that's not our case, even though it also goes well, I guess monetarily speaking of labor,

35:33.200 --> 35:40.320
but I think one of the, so the main issue, this is this all started, so the whole idea for

35:40.320 --> 35:47.840
this engagement tool started also, maybe two, two and a half, three years ago, and the initial idea

35:47.840 --> 35:54.480
was that we would get the data to then train only image, so we started on with images, to train

35:54.480 --> 36:00.640
an image classifier that could recognize themes. Then the constraint was we want this algorithm

36:00.640 --> 36:10.080
to be on device. So what we did is we went from the smallest network known, which is, well,

36:10.080 --> 36:18.640
at the time was mobile net actuators, and that's three, five megabytes, and that was to be for them.

36:20.240 --> 36:27.600
So, so that we had a problem because it's a wall. We can try to go down, we can try to minimize,

36:27.600 --> 36:32.960
we can try to reduce weights between layers, we can try to do a lot of this sort of optimizations,

36:32.960 --> 36:40.160
but the results were already a bit clumsy. So, you know, you maybe had a Jurassic World build,

36:40.160 --> 36:46.880
and I could recognize it as styles. So it goes far from idea, and I actually the idea of the project

36:46.880 --> 36:53.680
stopped there. Until a couple of months after somebody said, well, when we tried to do this theme,

36:53.680 --> 37:00.800
the texture for actually interacting with the customers or with the users, because then

37:00.800 --> 37:05.280
it's a wall. You have room to do the network you want, we don't really care because it's just

37:05.280 --> 37:10.560
going to run in the cloud, and it's going to run after the fact, right? So it's going to run

37:10.560 --> 37:15.760
at some point in the day to just, you know, get the themes and send some likes and comments, right?

37:18.800 --> 37:23.280
And that went very fine. I think we started with five themes, which is the top five categories,

37:23.280 --> 37:29.680
also, from the application, and probably the biggest issue there again was the data quality, right?

37:31.360 --> 37:37.920
So you can think of our application like the level life app as Instagram, right? So when you're a

37:37.920 --> 37:43.360
user, you take a picture of your level build, you will write a title in the description,

37:43.360 --> 37:49.120
say, you know, this is my cool creation, this is a Star Wars save file. And you can also then

37:49.120 --> 37:55.840
add the hashtags back on Instagram. So those hashtags could be, you know, hashtags, Star Wars,

37:55.840 --> 38:05.840
hashtags, a lot of other things. So because this data is available in the social media

38:05.840 --> 38:12.480
as backend, in this sort of, you know, you could say clumsy labeling, which is all right,

38:12.480 --> 38:16.880
well, we could try maybe running the first iteration, trying to crawl all of this data

38:16.880 --> 38:24.640
with some specific themes and maybe also even keywords. We know are used for specific themes,

38:24.640 --> 38:32.400
right? So let's say, okay, if a user has published a Star Wars image and says, this is Chihuahua,

38:32.400 --> 38:39.840
I know Chihuahua is Star Wars minifigure. Well, I can probably make it in my dataset saying it's

38:39.840 --> 38:45.600
Star Wars. So that's how we collected data the first time for five classes. And it worked

38:45.600 --> 38:51.920
wonderfully well. There were also a lot of misclassifications, but that was also expected from us.

38:52.960 --> 38:57.040
And we tried to clean, so we tried to, I think we did a couple of iterations from the keywords

38:57.040 --> 39:04.080
and the hashtags we used as well. Because you could think, if you said the spaceship,

39:04.080 --> 39:07.760
the spaceship could be Star Wars, but it could also be in Jailor, it could also be City.

39:08.640 --> 39:12.560
There's a lot of different spaces. That's the problem at labor, right? That imagination is the

39:12.560 --> 39:18.240
limit, right? So it's not the real world unfortunately or fortunate. It makes it a bit more fun.

39:20.560 --> 39:26.000
But yeah, so then we got the data. The first model with five classes worked. It was accepted by

39:26.000 --> 39:32.960
the business side. And after that happened, then we extended to 10, no nine classes. And then

39:32.960 --> 39:38.640
now we're up to 13 classes. And I think the latest issues we've been having are not issues,

39:38.640 --> 39:45.760
but the latest challenges we have to overcome was growing to 15 classes. Now we have a three third

39:45.760 --> 39:49.920
by data set of images, which you might think, well, it's not that much. You know, there's like

39:49.920 --> 39:57.280
ImageNet has 15 million images, I guess. It's quite a, it's quite a bunch when it's the first time

39:57.280 --> 40:05.520
we work with such a big data set for a production-ready solution, right? And using SageMaker,

40:05.520 --> 40:11.600
we also learned a lot of SageMaker because we don't have on on premise GPUs. So well,

40:11.600 --> 40:19.680
I have it here, but I have it for gaming on my free time. And so we also have to do some learning

40:19.680 --> 40:25.840
on how to do to let SageMaker the best possible. And to be fair, that also was very helpful when

40:25.840 --> 40:32.240
we also got support from them from the enterprise side. Because then yeah, so we were using SageMaker

40:32.240 --> 40:40.800
notebooks, which is just a full on Jupyter notebook. It turns out that even though you can choose

40:41.440 --> 40:47.840
GPU instance for the notebook, it is not the recommended scenario. The training on a GPU has to

40:47.840 --> 40:54.880
be done on the SageMaker training instead. The notebook is more just for data prep and the

40:54.880 --> 41:03.120
visualization. Well, we didn't know that. Until of course, we needed to run this 13-class training,

41:03.840 --> 41:09.280
actually the 9-class 13 training, we did it on the SageMaker notebook. And I think it took

41:10.320 --> 41:18.960
a week, maybe, a week and a half. Whereas for the 15-class, we then got these estimators

41:18.960 --> 41:25.440
running on the training jobs. And that took, I think, half a day. So more data less time.

41:25.440 --> 41:34.160
It was a good trade-off, I think. Got it. Got it. Very cool. Very cool. What are the,

41:34.960 --> 41:41.760
kind of what's next at LEGO with ML? What are you excited about or looking forward to?

41:41.760 --> 41:50.720
I think it's a very open question, right? Because again, so we are a company that makes a lot of,

41:51.680 --> 41:57.360
I guess, out of the ordinary things, right? And one of the projects, for example, we've

41:57.360 --> 42:05.040
been looking at is a taxonomious, right? So let's say how many cards we have, how many

42:05.040 --> 42:12.320
motorbikes, how many cats we have at LEGO, right? And you could say, well, we are LEGO, we own

42:12.320 --> 42:18.240
all of our data, we know what we have, and that's true. But what is, well, who is telling me that

42:18.240 --> 42:29.280
tomorrow there's not going to be a unicorn with a fish legs, for example, right? So this makes this

42:29.280 --> 42:34.080
taxonomious complicated, right? Because how do you evolve an algorithm, or a set of algorithms

42:34.080 --> 42:40.000
that can get a new class that has never seen before, it does not look like anything, right? Because

42:40.000 --> 42:45.520
you can have a fish in a unicorn, but if they are combined, who's the winner here? How do you

42:46.240 --> 42:50.400
make sure that your algorithm can learn the new class if you're coming? And it's something we're

42:50.400 --> 42:57.920
looking into, and I think it's hopefully coming next year. It requires a lot of prep work, that's

42:57.920 --> 43:05.280
for sure. But there's also a couple of other things. So we're also trying to, to upgrade some of the,

43:06.080 --> 43:15.920
of the A4, which is A4 is an adult fan of LEGO. So all of these adult oriented experiences,

43:17.200 --> 43:22.480
we're also trying to put on a bit more spice to them for making it all exciting for the adults to

43:22.480 --> 43:29.840
use, saying, you can have this specific application here, can you recognize what you're building,

43:29.840 --> 43:37.680
things like that. So trying to interact with the users as well. But it's complicated, I think,

43:37.680 --> 43:43.120
right? Because you can have very big dreams, but it's always the, it's always bureaucratic,

43:43.120 --> 43:48.800
right? And it's always about the data, right? It's like, well, sure, you can ask me to classify

43:48.800 --> 43:53.840
all of the red LEGO bricks you have, but I need to know what a LEGO brick is first, right? And what is

43:53.840 --> 44:04.160
red? Right. Very good, very good. Well, Francesc, thanks so much for joining us. It was great

44:04.160 --> 44:11.040
learning about your projects, and especially how you've built out the platform and infrastructure

44:11.040 --> 44:19.280
to support them at LEGO. Yeah, and well, thanks again for having me. And I guess maybe one last learning,

44:19.280 --> 44:49.200
you don't need to go to Kubernetes, you know, you can always go sellers. Awesome, thanks so much. Yes, thank you.

