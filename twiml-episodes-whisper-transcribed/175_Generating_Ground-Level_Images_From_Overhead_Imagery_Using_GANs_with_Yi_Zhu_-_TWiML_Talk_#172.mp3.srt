1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,400
I'm your host Sam Charrington, a quick update for our faithful listeners and fans.

4
00:00:34,400 --> 00:00:37,960
We recently asked you to help us in our bid to secure our nomination in the People's

5
00:00:37,960 --> 00:00:39,520
Choice Podcast Awards.

6
00:00:39,520 --> 00:00:44,960
Well, thanks to you, we're a finalist in the best technology podcast category, along

7
00:00:44,960 --> 00:00:49,440
with other noteworthy shows like Riko Diko and the Verge Cast.

8
00:00:49,440 --> 00:00:54,480
The award ceremony will be stream live on September 30th, which turns out to be international

9
00:00:54,480 --> 00:00:56,400
podcast day.

10
00:00:56,400 --> 00:01:00,400
Keep your fingers crossed for us and a huge thanks to everyone who voted.

11
00:01:00,400 --> 00:01:05,160
You have our utmost gratitude.

12
00:01:05,160 --> 00:01:12,360
Today we're joined by EZU, a PhD candidate at UC Merced focused on geospatial image analysis.

13
00:01:12,360 --> 00:01:17,360
In our conversation, E and I discussed his recent paper, what is it like down there?

14
00:01:17,360 --> 00:01:22,800
Using dense ground level views and image features, from overhead imagery, using conditional

15
00:01:22,800 --> 00:01:25,080
generative adversarial networks.

16
00:01:25,080 --> 00:01:30,120
E and I discussed the goals of this research, which is to train effective land use classifiers

17
00:01:30,120 --> 00:01:35,500
on proximate or ground level images, and how he uses conditional gans along with images

18
00:01:35,500 --> 00:01:40,960
sourced from social media to generate artificial ground level images for this task.

19
00:01:40,960 --> 00:01:46,320
We also explore future research directions, such as the use of reversible generative networks,

20
00:01:46,320 --> 00:01:51,800
as proposed in the recently released open AI glow paper to produce higher resolution

21
00:01:51,800 --> 00:01:53,320
images.

22
00:01:53,320 --> 00:01:54,320
Enjoy.

23
00:01:54,320 --> 00:01:59,560
All right, everyone, I am on the line with EZU.

24
00:01:59,560 --> 00:02:03,080
E is a PhD candidate at UC Merced.

25
00:02:03,080 --> 00:02:05,840
E, welcome to this week in machine learning and AI.

26
00:02:05,840 --> 00:02:06,840
Hi, Sam.

27
00:02:06,840 --> 00:02:07,840
Hi, everybody.

28
00:02:07,840 --> 00:02:08,840
So thank you for having me.

29
00:02:08,840 --> 00:02:10,800
It's really excited to be here.

30
00:02:10,800 --> 00:02:11,800
Absolutely.

31
00:02:11,800 --> 00:02:15,520
I'm really looking forward to digging into your work today to get us started.

32
00:02:15,520 --> 00:02:19,400
Why don't you tell us a little bit about your background and how you got started in machine

33
00:02:19,400 --> 00:02:20,400
learning.

34
00:02:20,400 --> 00:02:25,080
So actually, my major at first is wireless communication, because my undergraduate study

35
00:02:25,080 --> 00:02:28,120
was on like signal processing and information theory.

36
00:02:28,120 --> 00:02:32,960
But then at the year of 2012, when deep learning first the show is great potential on image

37
00:02:32,960 --> 00:02:38,080
net challenge, so I was fascinated by simplicity and surprisingly good results.

38
00:02:38,080 --> 00:02:43,480
So I started to attend some like cargo challenges using deep learning, including the famous dog

39
00:02:43,480 --> 00:02:45,920
and cat classification challenge.

40
00:02:45,920 --> 00:02:50,120
So although at that time, most of the challenge winners are still using random forest of XG

41
00:02:50,120 --> 00:02:51,120
boost.

42
00:02:51,120 --> 00:02:55,800
But later I found myself like attractive to this machine learning field and especially

43
00:02:55,800 --> 00:02:56,800
deep learning.

44
00:02:56,800 --> 00:02:58,720
So I'm thinking so why not change a major?

45
00:02:58,720 --> 00:03:04,040
So in my understanding, so images are still signals captured by optical sensors.

46
00:03:04,040 --> 00:03:06,920
So they are not that different from my previous study.

47
00:03:06,920 --> 00:03:12,920
So I switched my major to computer vision and start my PhD study at UC Merced in 2014.

48
00:03:12,920 --> 00:03:16,160
So basically, I have two lines of researcher directions.

49
00:03:16,160 --> 00:03:19,560
Right now, one is geospatial image analysis.

50
00:03:19,560 --> 00:03:24,400
The other is a video analysis, but I think today I would just focus first one.

51
00:03:24,400 --> 00:03:29,160
You mentioned that you started competing in cargo competitions.

52
00:03:29,160 --> 00:03:31,760
What were some of the competitions that you competed in?

53
00:03:31,760 --> 00:03:34,600
So the first one will be the dog and cat challenge.

54
00:03:34,600 --> 00:03:39,480
So that's the object, like a recognition problem, like a binary thing.

55
00:03:39,480 --> 00:03:47,080
And later, I also competing the challenge of the eye contact of the driver to try to

56
00:03:47,080 --> 00:03:51,640
understand if the driver is sleepiness or is it a safe driver or something.

57
00:03:51,640 --> 00:03:57,000
So there's also an autonomous driving challenge.

58
00:03:57,000 --> 00:04:01,080
And then also for the insurance to see a car's image.

59
00:04:01,080 --> 00:04:04,600
So whether it is like a damage or not, is it a new car?

60
00:04:04,600 --> 00:04:09,040
So how much price should it sell for a second hand car?

61
00:04:09,040 --> 00:04:11,400
So that's for the insurance company.

62
00:04:11,400 --> 00:04:14,760
So there are several more, but I cannot remember the details.

63
00:04:14,760 --> 00:04:15,760
Yeah.

64
00:04:15,760 --> 00:04:16,760
Okay.

65
00:04:16,760 --> 00:04:17,760
Awesome.

66
00:04:17,760 --> 00:04:22,160
I don't know the last two, the driver eye contact or the insurance one, but the dog and

67
00:04:22,160 --> 00:04:25,360
cat one is one that we have gone through.

68
00:04:25,360 --> 00:04:30,920
I'm working with a group of folks to kind of go through the fast AI course and they talk

69
00:04:30,920 --> 00:04:34,640
about that dog and cats one in there quite extensively.

70
00:04:34,640 --> 00:04:37,680
How do you do and the Kaggle challenges you did?

71
00:04:37,680 --> 00:04:41,560
So firstly, I just tried like a linear regression.

72
00:04:41,560 --> 00:04:43,760
So that's the most basic model.

73
00:04:43,760 --> 00:04:48,440
I think most people still use it today for normal questions.

74
00:04:48,440 --> 00:04:52,040
And then I also tried like a random forest and XG boost.

75
00:04:52,040 --> 00:04:58,040
So at that time, the most winners are using those two techniques and then model assembling

76
00:04:58,040 --> 00:05:00,840
to get to the best score they can.

77
00:05:00,840 --> 00:05:02,800
But then I also tried like deep learning, right?

78
00:05:02,800 --> 00:05:06,400
So convolutional neural networks for this image recognition task.

79
00:05:06,400 --> 00:05:11,240
And at first, they cannot compete with like random forest because the images are not that

80
00:05:11,240 --> 00:05:12,240
large.

81
00:05:12,240 --> 00:05:15,120
So you know, like deep learning is a data hangry model, right?

82
00:05:15,120 --> 00:05:19,880
So if you have more data and if you have like a clean data, so you can get a very good

83
00:05:19,880 --> 00:05:20,880
result.

84
00:05:20,880 --> 00:05:25,680
But if you don't have enough data, so maybe sometimes it doesn't compete with those traditional

85
00:05:25,680 --> 00:05:26,680
algorithms.

86
00:05:26,680 --> 00:05:27,680
Yeah.

87
00:05:27,680 --> 00:05:30,880
That basically the four algorithms I'm using for Kaggle challenges.

88
00:05:30,880 --> 00:05:33,400
How did you rank in the competitions?

89
00:05:33,400 --> 00:05:35,760
Did you rank highly in any of them?

90
00:05:35,760 --> 00:05:40,960
I didn't rank like a top one or top 10 thumbs in, but I'm usually in the top 10%.

91
00:05:40,960 --> 00:05:41,960
So.

92
00:05:41,960 --> 00:05:42,960
Okay.

93
00:05:42,960 --> 00:05:43,960
Nice.

94
00:05:43,960 --> 00:05:48,600
And so your research now is on understanding images using deep learning.

95
00:05:48,600 --> 00:05:51,560
And you recently published a paper.

96
00:05:51,560 --> 00:05:56,360
The paper is called what is it like down there generating dense ground level views and

97
00:05:56,360 --> 00:06:01,960
image features from overhead imagery using conditional Gans.

98
00:06:01,960 --> 00:06:03,760
That there's a ton in there.

99
00:06:03,760 --> 00:06:07,880
Yeah, yeah, the title is quite long.

100
00:06:07,880 --> 00:06:09,680
The title is quite long.

101
00:06:09,680 --> 00:06:11,480
I mean, maybe let's get started.

102
00:06:11,480 --> 00:06:15,720
I think, you know, we've talked about Gans quite a bit on the podcast, but why don't you

103
00:06:15,720 --> 00:06:19,200
start us out by talking about conditional Gans?

104
00:06:19,200 --> 00:06:20,200
Okay.

105
00:06:20,200 --> 00:06:21,200
Yeah.

106
00:06:21,200 --> 00:06:24,520
So yeah, I have seen those like a Gans episode here and there.

107
00:06:24,520 --> 00:06:29,680
So it's very hot topic like last year and there's a trend is still going on.

108
00:06:29,680 --> 00:06:33,400
So as we know, so Gans consists of two components.

109
00:06:33,400 --> 00:06:36,160
So one generator, one discriminator.

110
00:06:36,160 --> 00:06:40,640
But the thing is when you generate something, so you're generating from a random noise.

111
00:06:40,640 --> 00:06:46,160
So each time you will get different results, different output, but sometimes you really

112
00:06:46,160 --> 00:06:49,840
want to have some like fixed output, right?

113
00:06:49,840 --> 00:06:53,400
So that is what conditional Gans for.

114
00:06:53,400 --> 00:06:59,520
And for like, for example, sometimes a like a fashion, like a shoe designer, they want

115
00:06:59,520 --> 00:07:03,840
to design using again to design a new shoe.

116
00:07:03,840 --> 00:07:09,520
But if you didn't control the output, sometimes it will begin something really random.

117
00:07:09,520 --> 00:07:15,240
So if we can give the model some information like prior information, like some texturing

118
00:07:15,240 --> 00:07:17,400
bad things like I want a shoe.

119
00:07:17,400 --> 00:07:20,840
So then the output will definitely be a shoe image.

120
00:07:20,840 --> 00:07:22,840
So that's what Gans for.

121
00:07:22,840 --> 00:07:23,840
So see again.

122
00:07:23,840 --> 00:07:28,680
So the conditional again, learn the distribution condition on some auxiliary information.

123
00:07:28,680 --> 00:07:35,600
So those auxiliary information can be class labels, or which is the most standard form,

124
00:07:35,600 --> 00:07:41,240
or texting bad things for generating images, or like image to image translation.

125
00:07:41,240 --> 00:07:46,800
So here in our work, so we use conditional again because we know what we want, right?

126
00:07:46,800 --> 00:07:51,120
We want ground level images corresponding to the overhead image.

127
00:07:51,120 --> 00:07:56,640
So given me overhead image, I want to generate a similar looking ground level images.

128
00:07:56,640 --> 00:08:01,560
So the overhead image will be considered as a prior information.

129
00:08:01,560 --> 00:08:02,560
Yeah.

130
00:08:02,560 --> 00:08:04,800
So that's why we choose a conditional again.

131
00:08:04,800 --> 00:08:13,240
And what you're going for kind of visually is if you send in to the system, the image

132
00:08:13,240 --> 00:08:18,960
of, you know, what an area that looks like farmland, you want to generate ground level

133
00:08:18,960 --> 00:08:21,080
pictures that look like farmland.

134
00:08:21,080 --> 00:08:26,080
Similarly, if you're sending in overhead images of an urban setting, you want it to generate

135
00:08:26,080 --> 00:08:27,080
urban images.

136
00:08:27,080 --> 00:08:28,080
Yes.

137
00:08:28,080 --> 00:08:29,080
Exactly.

138
00:08:29,080 --> 00:08:34,360
Tell us about the different data sources that you use to make all this happen.

139
00:08:34,360 --> 00:08:35,360
Okay.

140
00:08:35,360 --> 00:08:38,560
So the ground level images, the problem there is their spars.

141
00:08:38,560 --> 00:08:42,760
So what we can have is a satellite images, right?

142
00:08:42,760 --> 00:08:44,120
Satellite images is dense.

143
00:08:44,120 --> 00:08:45,120
So in everywhere.

144
00:08:45,120 --> 00:08:49,800
So it has all the satellite images at every spatial location.

145
00:08:49,800 --> 00:08:53,480
So that is our image input source.

146
00:08:53,480 --> 00:08:58,600
So the ground chooses and cover map is from the LCM data set.

147
00:08:58,600 --> 00:09:06,000
So our study region is a 77 by 71 kilometer region containing that in London.

148
00:09:06,000 --> 00:09:10,680
So the geographic images are labeled as urban or rural based.

149
00:09:10,680 --> 00:09:15,200
So then we can do a very simple like a binary classification problems.

150
00:09:15,200 --> 00:09:18,600
So our input sources are like a lot.

151
00:09:18,600 --> 00:09:25,240
So from like Google maps, statistics and a geographic API and also the satellite images.

152
00:09:25,240 --> 00:09:28,680
So those are the three image sources we're using.

153
00:09:28,680 --> 00:09:35,240
The LCM data set is that your overhead images or those your ground images.

154
00:09:35,240 --> 00:09:36,880
Those are the overhead images.

155
00:09:36,880 --> 00:09:37,880
Okay.

156
00:09:37,880 --> 00:09:41,680
And the ground level images is from the geographic.

157
00:09:41,680 --> 00:09:48,640
And that is a website and by some London researchers, so those are for the whole United Kingdom.

158
00:09:48,640 --> 00:09:53,680
So the length of our classes are provided on one kilometer grade.

159
00:09:53,680 --> 00:09:57,800
So for every grade, they have the user provided ground level images.

160
00:09:57,800 --> 00:09:59,840
So that's pretty accurate.

161
00:09:59,840 --> 00:10:02,240
So that is our ground level images.

162
00:10:02,240 --> 00:10:05,160
And the LCM is the overhead images.

163
00:10:05,160 --> 00:10:10,840
So we have the corresponding relationship to train our models to do some experiments.

164
00:10:10,840 --> 00:10:17,760
And so I realized that what you're doing here with Gantt is fundamentally generative.

165
00:10:17,760 --> 00:10:21,880
But part of the way the problem is set up sounds like an information retrieval problem,

166
00:10:21,880 --> 00:10:26,960
like you have this overhead image and you have this corpus of ground level images kind

167
00:10:26,960 --> 00:10:28,200
of find the best one.

168
00:10:28,200 --> 00:10:29,880
Are those related in any way?

169
00:10:29,880 --> 00:10:32,440
Oh, so there are there is difference there.

170
00:10:32,440 --> 00:10:38,080
So for like a image retrieval problem is so given me overhead images, I want to find

171
00:10:38,080 --> 00:10:40,720
the best matching like ground level images, right?

172
00:10:40,720 --> 00:10:44,160
So there is like a there's a database of the ground level images.

173
00:10:44,160 --> 00:10:47,080
So we try to find the like most matching one.

174
00:10:47,080 --> 00:10:52,360
So but for generate models, it's not where we want some specific images.

175
00:10:52,360 --> 00:10:57,400
So we don't want some specific ground level images to corresponding to the overhead image.

176
00:10:57,400 --> 00:11:00,320
We want the whole data distribution to fit.

177
00:11:00,320 --> 00:11:06,160
So the we want the generator to generate some real looking images to fit the data distribution.

178
00:11:06,160 --> 00:11:11,560
We don't care about so which images we're generating, we just want it to look realistic

179
00:11:11,560 --> 00:11:12,840
for that category.

180
00:11:12,840 --> 00:11:14,960
So that's the only difference.

181
00:11:14,960 --> 00:11:20,320
And do you do any kind of information retrieval as part of the solution?

182
00:11:20,320 --> 00:11:27,560
In other words, is your GAN conditioned only on the overhead image or is it conditioned

183
00:11:27,560 --> 00:11:33,560
on the subset of images that you retrieve from a database based on the overhead image?

184
00:11:33,560 --> 00:11:34,960
That's a good question.

185
00:11:34,960 --> 00:11:42,120
So actually right now we only like based our model on GAN, so we're not we're not using

186
00:11:42,120 --> 00:11:44,280
any information retrieval thing.

187
00:11:44,280 --> 00:11:49,680
So but in the future, we want to use that because the land use classification problem is

188
00:11:49,680 --> 00:11:50,680
really challenging.

189
00:11:50,680 --> 00:11:52,160
It's really, really hard.

190
00:11:52,160 --> 00:11:54,960
So maybe later I will introduce my another work.

191
00:11:54,960 --> 00:12:00,160
So it's also about large-scale land use classification, but the classification accuracy

192
00:12:00,160 --> 00:12:03,480
is only like 20%, 30%, so it's pretty challenging.

193
00:12:03,480 --> 00:12:08,360
So we need to have some like a human prior knowledge inside it.

194
00:12:08,360 --> 00:12:12,680
So like the information retrieval, like exciting information you mentioned about.

195
00:12:12,680 --> 00:12:15,120
So we will do that in the future work.

196
00:12:15,120 --> 00:12:21,160
So in this work, what are some of the big challenges that you had to overcome in the

197
00:12:21,160 --> 00:12:22,920
spaper and this research?

198
00:12:22,920 --> 00:12:23,920
Okay.

199
00:12:23,920 --> 00:12:27,720
So can I like a backup a little bit to talk about the motivation of this work?

200
00:12:27,720 --> 00:12:30,040
So that would be more clear.

201
00:12:30,040 --> 00:12:31,040
Okay.

202
00:12:31,040 --> 00:12:37,120
So the motivation to like all of our work is because although we have like enormous

203
00:12:37,120 --> 00:12:42,440
amount of online images like from Flaker, Instagram, so all those images, all those videos,

204
00:12:42,440 --> 00:12:46,200
we could use it to do some geospatial analysis.

205
00:12:46,200 --> 00:12:52,520
But if we want to do a detailed land use classification map, so land use I mean is, for

206
00:12:52,520 --> 00:12:57,480
example, so whether a building is a hospital or whether it's a shopping mall or something.

207
00:12:57,480 --> 00:12:59,160
So how do we use the land?

208
00:12:59,160 --> 00:13:02,560
So whether it's residential or for office use.

209
00:13:02,560 --> 00:13:08,840
So but overhead images cannot handle such case because overhead images is from like above,

210
00:13:08,840 --> 00:13:09,840
right?

211
00:13:09,840 --> 00:13:11,640
So from above, you can only see a building.

212
00:13:11,640 --> 00:13:13,680
So you cannot see inside of the building.

213
00:13:13,680 --> 00:13:15,960
So you don't know what that design for.

214
00:13:15,960 --> 00:13:18,240
So that's what we come up with.

215
00:13:18,240 --> 00:13:21,800
So we want to use social multimedia to do this kind of like a land use classification

216
00:13:21,800 --> 00:13:25,680
because social media is captured by phones or cameras.

217
00:13:25,680 --> 00:13:30,440
So they can see inside the building to easily infer what it is of this building.

218
00:13:30,440 --> 00:13:31,760
So where it is.

219
00:13:31,760 --> 00:13:38,840
But to the challenge, the most big challenge of using social media is it's uneven spatial

220
00:13:38,840 --> 00:13:39,840
distribution.

221
00:13:39,840 --> 00:13:44,280
So simple speaking because most images are coming from the famous landmarks, right?

222
00:13:44,280 --> 00:13:48,920
So not to the general spatial locations, like there were tons of people having the golden

223
00:13:48,920 --> 00:13:52,480
gate bridge in San Francisco, like after towering Paris.

224
00:13:52,480 --> 00:13:57,280
So it's very easy to tell these landmarks from the photographs, but for like residential

225
00:13:57,280 --> 00:14:02,240
areas or privacy, preserve the regions, which so we don't have enough images.

226
00:14:02,240 --> 00:14:05,480
So that's a very serious problem if we don't have images.

227
00:14:05,480 --> 00:14:10,520
So how can we infer the location and all those land use classification information?

228
00:14:10,520 --> 00:14:13,600
So that's the challenge we really face about.

229
00:14:13,600 --> 00:14:16,720
So traditional zero several ways to do that.

230
00:14:16,720 --> 00:14:20,040
So one simple way is to interpolate the features.

231
00:14:20,040 --> 00:14:26,360
So if we have, for example, we have several images at location A and we have several images

232
00:14:26,360 --> 00:14:30,760
at the land location B and these two locations are close to each other.

233
00:14:30,760 --> 00:14:34,320
But between these two locations, there is no images at all.

234
00:14:34,320 --> 00:14:38,720
So in this case, we just computed the image features of these all the images at those

235
00:14:38,720 --> 00:14:43,200
two locations and interpolate them along this line.

236
00:14:43,200 --> 00:14:49,040
But here we made assumption that the assumption is we hope the image features will change

237
00:14:49,040 --> 00:14:53,960
smoothly in the spatial domain, but actually in most cases, it's not a case.

238
00:14:53,960 --> 00:15:00,280
So if we have several images of like a residential and at the location A and a residential and

239
00:15:00,280 --> 00:15:05,520
location B, then if you do the interpolation things, it will interpolate.

240
00:15:05,520 --> 00:15:09,520
So between these two locations, all the areas are residential based.

241
00:15:09,520 --> 00:15:16,280
But actually in most cases, the space in between is like forest or park or just a river

242
00:15:16,280 --> 00:15:17,280
or something.

243
00:15:17,280 --> 00:15:19,320
It's like natural things.

244
00:15:19,320 --> 00:15:24,000
So the other kind of method is, so we try to use other information like remote sensing

245
00:15:24,000 --> 00:15:28,360
images or Google Street Wheels because those two information sources are like a dance

246
00:15:28,360 --> 00:15:31,080
so everywhere in the Earth.

247
00:15:31,080 --> 00:15:37,560
So there is a work from like Professor Nason Jacob's lab in ICCV 2017, so they just

248
00:15:37,560 --> 00:15:41,200
use Google Street Wheels to do these kind of things.

249
00:15:41,200 --> 00:15:47,200
But we find that so all those techniques are based on image features and are based on

250
00:15:47,200 --> 00:15:48,800
the assumption I just said.

251
00:15:48,800 --> 00:15:53,240
So the image features change smoothly in the spatial domain, but usually that's not

252
00:15:53,240 --> 00:15:54,240
the case.

253
00:15:54,240 --> 00:15:56,000
So that's why we do this work.

254
00:15:56,000 --> 00:16:00,440
So if we cannot use the image features, why not we just use images?

255
00:16:00,440 --> 00:16:02,960
But the images are missing at those locations.

256
00:16:02,960 --> 00:16:05,240
So how do we come up with new images?

257
00:16:05,240 --> 00:16:07,640
So fortunately, we have guns.

258
00:16:07,640 --> 00:16:10,800
So that's the motivation of the work.

259
00:16:10,800 --> 00:16:17,120
You mentioned that early in your research career, you spent some time looking at

260
00:16:17,120 --> 00:16:23,360
information theory and the like and it strikes me in that context that this is a pretty

261
00:16:23,360 --> 00:16:28,800
difficult problem and that there's just not enough information in these satellite images

262
00:16:28,800 --> 00:16:34,600
to do a very good job coming up with ground level images.

263
00:16:34,600 --> 00:16:36,160
How do you get around that?

264
00:16:36,160 --> 00:16:37,160
Yeah.

265
00:16:37,160 --> 00:16:43,080
So both like a overhead images and ground level images has this like advantages and drawbacks.

266
00:16:43,080 --> 00:16:48,880
So as I said, so the overhead images is very accurate and it stands in everywhere.

267
00:16:48,880 --> 00:16:52,600
So we can totally use this kind of information.

268
00:16:52,600 --> 00:16:56,960
But if you want to see inside the building, so overhead images cannot do that.

269
00:16:56,960 --> 00:17:01,840
But ground level images, we can see inside the building, but the biggest problem is the

270
00:17:01,840 --> 00:17:03,400
uneven distribution.

271
00:17:03,400 --> 00:17:09,560
So from the information theory side of like point of view, so we should use both information

272
00:17:09,560 --> 00:17:14,800
sources to how to combine them in the best way to get the best result.

273
00:17:14,800 --> 00:17:15,800
Yeah.

274
00:17:15,800 --> 00:17:16,800
This is a good point.

275
00:17:16,800 --> 00:17:23,320
Yeah, maybe to make my question more concrete, it has to do with what you considered your

276
00:17:23,320 --> 00:17:26,400
error function to be or something along those lines.

277
00:17:26,400 --> 00:17:32,600
And I guess the thing that I am trying to articulate here that strikes me as being particularly

278
00:17:32,600 --> 00:17:37,560
interesting and challenging here is with a ground level image, you know, beyond just

279
00:17:37,560 --> 00:17:44,560
trying to generate like high, you know, whether a particular image looks kind of urban or

280
00:17:44,560 --> 00:17:49,640
looks like, you know, greenery or forest or things like that, you know, there are things

281
00:17:49,640 --> 00:17:55,400
that you're trying to generate that aren't at all represented in your input.

282
00:17:55,400 --> 00:17:59,680
So for example, your satellite image, you know, has no building facades.

283
00:17:59,680 --> 00:18:05,000
But if you're trying to generate imagery around an urban area, those ground level images

284
00:18:05,000 --> 00:18:09,160
will have building facades, so it's just kind of making that stuff up.

285
00:18:09,160 --> 00:18:16,240
And I'm wondering, does that fact play into kind of how you build out the model and what

286
00:18:16,240 --> 00:18:18,720
the loss function is and stuff like that?

287
00:18:18,720 --> 00:18:19,720
Does that make any sense?

288
00:18:19,720 --> 00:18:20,720
Yeah.

289
00:18:20,720 --> 00:18:21,720
Yeah, it totally makes sense.

290
00:18:21,720 --> 00:18:22,720
Yeah.

291
00:18:22,720 --> 00:18:23,720
That's a good point.

292
00:18:23,720 --> 00:18:27,320
So, but actually our work is in the very like a initial stage.

293
00:18:27,320 --> 00:18:31,800
So currently we're just using a very traditional conditional again to do that.

294
00:18:31,800 --> 00:18:36,440
And that the loss function is just like generating realistic looking images.

295
00:18:36,440 --> 00:18:42,640
So we're not considering like whether there is a discrepancy between the ground level

296
00:18:42,640 --> 00:18:47,240
images or overhead images or any other like objective function.

297
00:18:47,240 --> 00:18:50,440
We're just using conditional again without other stuff.

298
00:18:50,440 --> 00:18:56,360
So the loss function only thinking about realism versus not realism.

299
00:18:56,360 --> 00:18:59,120
Is that kind of axiomatic for conditional gains?

300
00:18:59,120 --> 00:19:00,120
Yes, it is.

301
00:19:00,120 --> 00:19:06,160
So, yeah, for most of conditional gains, it's just a, so for the discriminator as job

302
00:19:06,160 --> 00:19:09,280
is just to say, so whether this image is fake or real.

303
00:19:09,280 --> 00:19:13,120
So it's a binary problem and the objective function is just that.

304
00:19:13,120 --> 00:19:14,120
So entropy loss.

305
00:19:14,120 --> 00:19:15,120
Okay.

306
00:19:15,120 --> 00:19:16,120
Got it.

307
00:19:16,120 --> 00:19:22,600
So you built this GAN based system to produce these images.

308
00:19:22,600 --> 00:19:27,800
We were just talking about loss functions like what did you find in terms of its performance?

309
00:19:27,800 --> 00:19:29,120
How did the system do?

310
00:19:29,120 --> 00:19:30,120
Yeah.

311
00:19:30,120 --> 00:19:36,520
So in terms of the image quality, so I don't like have a monitor to show the quality here,

312
00:19:36,520 --> 00:19:38,920
but basically it makes sense.

313
00:19:38,920 --> 00:19:45,160
So we can generate like some realistic images, like ground level images according to the

314
00:19:45,160 --> 00:19:46,760
overhead images.

315
00:19:46,760 --> 00:19:50,320
And but we don't have like a evaluation metric.

316
00:19:50,320 --> 00:19:53,480
So how real they are or how accurate they are.

317
00:19:53,480 --> 00:19:55,600
So we use another task.

318
00:19:55,600 --> 00:19:58,080
So that's a land use classification, right?

319
00:19:58,080 --> 00:20:05,680
Because if we can do a better land use classification, given these like fake images, so we can create

320
00:20:05,680 --> 00:20:07,800
fake images all over the ground, right?

321
00:20:07,800 --> 00:20:12,400
So if we can use these images to like to do better land use classification.

322
00:20:12,400 --> 00:20:17,480
So that of accuracy can be a good indicator of how our model works.

323
00:20:17,480 --> 00:20:22,120
So let me like share the performance of our land use classification problem.

324
00:20:22,120 --> 00:20:26,240
So if you if we are using the conditional GAN generated features to do the land use

325
00:20:26,240 --> 00:20:34,040
classification, we can achieve like a land cover classification accuracy with like 73% accuracy.

326
00:20:34,040 --> 00:20:40,760
So actually it's kind of like it's a reasonable, it's not high, but it's reasonable.

327
00:20:40,760 --> 00:20:47,120
The problems we're thinking here is because our generated images are not realistic enough.

328
00:20:47,120 --> 00:20:49,000
So that's some of our future work.

329
00:20:49,000 --> 00:20:51,520
So we have like a three future directions to go.

330
00:20:51,520 --> 00:20:56,200
So I can talk about later like if you like want.

331
00:20:56,200 --> 00:21:01,000
In the land use classification, how many classes are there?

332
00:21:01,000 --> 00:21:03,320
It's only binary classification.

333
00:21:03,320 --> 00:21:06,840
So like as I mentioned, it's a rural and urban areas.

334
00:21:06,840 --> 00:21:10,960
So because that's that's only the ground truth we have.

335
00:21:10,960 --> 00:21:11,960
Okay.

336
00:21:11,960 --> 00:21:14,600
So you've got a bunch of satellite data.

337
00:21:14,600 --> 00:21:18,640
You feed it into your conditional GAN.

338
00:21:18,640 --> 00:21:28,600
It generates an image that is meant to be representative of a ground level view of the

339
00:21:28,600 --> 00:21:32,520
area that you indicate.

340
00:21:32,520 --> 00:21:39,440
And then you are sending that into a classifier that is meant to determine whether it's

341
00:21:39,440 --> 00:21:41,520
urban or rural.

342
00:21:41,520 --> 00:21:53,880
And the is the 73% is that the accuracy of your classifier or the accuracy of the generator

343
00:21:53,880 --> 00:21:55,720
based on a trained classifier?

344
00:21:55,720 --> 00:21:58,120
Oh, it's a it's a the classifier.

345
00:21:58,120 --> 00:21:59,120
It's a classifier.

346
00:21:59,120 --> 00:22:00,120
Okay.

347
00:22:00,120 --> 00:22:01,120
The classifier itself.

348
00:22:01,120 --> 00:22:02,120
That's what I thought you were saying.

349
00:22:02,120 --> 00:22:10,800
And so, you know, how did the GAN itself perform relative to with that classifier?

350
00:22:10,800 --> 00:22:11,800
Okay.

351
00:22:11,800 --> 00:22:16,840
So the baseline of that is, so we compare to like a traditional approaches, right?

352
00:22:16,840 --> 00:22:20,720
So the first approaches I'm talking about is interpolated features.

353
00:22:20,720 --> 00:22:26,960
So if we have some images, we just do the feature extraction first and interpolate the features

354
00:22:26,960 --> 00:22:29,080
on those regions without images.

355
00:22:29,080 --> 00:22:32,840
So the baseline is like 65%, okay, so 65.

356
00:22:32,840 --> 00:22:38,360
So if we're using GANs without conditional, like we're getting like a two or three percent

357
00:22:38,360 --> 00:22:44,480
improvement, like to the almost the 68% and if we're using conditional again, we're

358
00:22:44,480 --> 00:22:50,080
having like 70% and if we're using conditional again, generated features, we're having the

359
00:22:50,080 --> 00:22:53,120
best accuracy like 73%.

360
00:22:53,120 --> 00:22:56,560
So that's the progress of our work.

361
00:22:56,560 --> 00:22:59,120
And I just want to make sure I understand this.

362
00:22:59,120 --> 00:23:05,000
I thought what you were saying was that the classifier itself, like totally separate

363
00:23:05,000 --> 00:23:10,680
from the GAN part of the system, had a 73% accuracy.

364
00:23:10,680 --> 00:23:16,800
Did you model the land use classifier separately and that was 73%.

365
00:23:16,800 --> 00:23:17,800
Yes.

366
00:23:17,800 --> 00:23:22,240
So the land use classifier is totally separate from the GAN, right?

367
00:23:22,240 --> 00:23:26,000
So the GAN, the GAN is about generated discriminator.

368
00:23:26,000 --> 00:23:30,480
The job is to like create, create these images, yes.

369
00:23:30,480 --> 00:23:37,560
And then we use the features from the discriminator as the input to the land use classifier,

370
00:23:37,560 --> 00:23:38,560
okay?

371
00:23:38,560 --> 00:23:39,560
Right.

372
00:23:39,560 --> 00:23:44,160
I guess what strikes me as odd is that, or at least curious, is that if I understood

373
00:23:44,160 --> 00:23:51,840
you correctly, your ultimate accuracy of your GAN turned out to be exactly the same as

374
00:23:51,840 --> 00:23:54,640
the accuracy of your classifier, 73%.

375
00:23:54,640 --> 00:23:57,640
No, maybe I'm saying wrong.

376
00:23:57,640 --> 00:24:00,680
So because for GAN, there's no accuracy, right?

377
00:24:00,680 --> 00:24:04,240
So GANs for the discriminator is just a real or fake.

378
00:24:04,240 --> 00:24:07,560
So we don't care about that accuracy or not.

379
00:24:07,560 --> 00:24:12,400
So it's usually very high, so like 80% or 90% is very high.

380
00:24:12,400 --> 00:24:16,240
But what we are concerned about is the land use classifier, yeah.

381
00:24:16,240 --> 00:24:20,840
So what I'm saying, the accuracy is all for land use classifier, it's not for GAN

382
00:24:20,840 --> 00:24:21,840
classifier.

383
00:24:21,840 --> 00:24:22,840
Okay.

384
00:24:22,840 --> 00:24:27,200
I think I'm confusing the issue here and not doing a good job explaining.

385
00:24:27,200 --> 00:24:33,960
So I guess I'm thinking that there are, as we established, there are two separate systems.

386
00:24:33,960 --> 00:24:36,560
There's the generating system.

387
00:24:36,560 --> 00:24:41,720
It's input is an overhead image and its output is a ground level image.

388
00:24:41,720 --> 00:24:47,880
And there's a classifying system and its input is a ground level image and its output

389
00:24:47,880 --> 00:24:52,400
is a binary land use classification.

390
00:24:52,400 --> 00:24:59,240
And so the performance of the GAN is a subset of that first system.

391
00:24:59,240 --> 00:25:03,360
And it's responsible for generating these images and it's kind of judged on whether

392
00:25:03,360 --> 00:25:06,200
the images are realistic or not.

393
00:25:06,200 --> 00:25:11,280
But that whole first system, the generator system as a whole, right?

394
00:25:11,280 --> 00:25:16,080
You're giving it a satellite image and it's spitting out a ground level image.

395
00:25:16,080 --> 00:25:22,640
You can measure that its accuracy with respect to producing kind of the correct land use,

396
00:25:22,640 --> 00:25:23,640
right?

397
00:25:23,640 --> 00:25:24,640
Yes.

398
00:25:24,640 --> 00:25:29,800
So that's one kind of accuracy measure and then there's another which is given any kind

399
00:25:29,800 --> 00:25:34,400
of image, whether it's generated from our generator or not, you know, is this land use

400
00:25:34,400 --> 00:25:40,280
classifier model accurate in classifying the input image correctly.

401
00:25:40,280 --> 00:25:45,680
And then there's like a third performance metric, which is the end to end, right, given

402
00:25:45,680 --> 00:25:50,240
the satellite image, can we identify the land use correctly?

403
00:25:50,240 --> 00:25:53,880
And so which of these is the 73 percent?

404
00:25:53,880 --> 00:25:54,880
It's the second one.

405
00:25:54,880 --> 00:25:58,320
So for the third one, we're not doing end to end right now.

406
00:25:58,320 --> 00:26:00,440
So we're just, we're doing two stage.

407
00:26:00,440 --> 00:26:04,560
So first stage is GAN, the second stage is then to use, we're not doing end to end at

408
00:26:04,560 --> 00:26:05,560
this point.

409
00:26:05,560 --> 00:26:09,480
Yeah, I guess if you're not doing end to end, you're not really looking at the first,

410
00:26:09,480 --> 00:26:13,160
the performance of the generator either because that really is the end to end.

411
00:26:13,160 --> 00:26:14,160
Uh-huh.

412
00:26:14,160 --> 00:26:15,160
Okay.

413
00:26:15,160 --> 00:26:23,520
So the main focus of the research around modeling and testing this, the classifier then

414
00:26:23,520 --> 00:26:29,520
or is it the generator system, uh, the classifier, right, because we're, um, so for geospatial

415
00:26:29,520 --> 00:26:32,800
analysis, we're more caring about the land use system.

416
00:26:32,800 --> 00:26:34,880
So basically it's a zoning system, right?

417
00:26:34,880 --> 00:26:39,920
So usually the government and city will make a zoning map so every year or so.

418
00:26:39,920 --> 00:26:42,240
So that is the most helpful things.

419
00:26:42,240 --> 00:26:45,120
So for the generator, it's just a technique we use.

420
00:26:45,120 --> 00:26:49,000
So if we don't use GAN, we can use other generator to generate images.

421
00:26:49,000 --> 00:26:54,600
Yeah, it's, it's kind of funny in that like, you know, GANs are so popular and quote unquote

422
00:26:54,600 --> 00:26:55,600
sexy term.

423
00:26:55,600 --> 00:26:59,640
It's almost like a head fake that kind of pulls your attention away from what you're actually

424
00:26:59,640 --> 00:27:01,000
trying to do in this paper.

425
00:27:01,000 --> 00:27:02,000
Yeah, yeah, yeah.

426
00:27:02,000 --> 00:27:03,000
Actually it is.

427
00:27:03,000 --> 00:27:05,720
So, um, okay.

428
00:27:05,720 --> 00:27:11,000
So I'm not sure you've even talked much about the classification model.

429
00:27:11,000 --> 00:27:15,640
So for land use, it's a pretty basic, like a, like a convolutional network.

430
00:27:15,640 --> 00:27:18,880
So like we use ResNet 101 to do that.

431
00:27:18,880 --> 00:27:24,080
Um, so actually, um, we have like a previous work like a three years ago.

432
00:27:24,080 --> 00:27:29,720
Um, so it, it's also doing land use classification with like a convolutional neural network.

433
00:27:29,720 --> 00:27:33,760
I think at that time where the first batch of work that used deep learning for this kind

434
00:27:33,760 --> 00:27:38,480
of work, we also received the best postal word in the SEM six spatial conference.

435
00:27:38,480 --> 00:27:39,480
Yeah.

436
00:27:39,480 --> 00:27:43,640
Hopefully it's you, um, at that time, it's also the similar problems we're using social

437
00:27:43,640 --> 00:27:44,640
media.

438
00:27:44,640 --> 00:27:45,640
We use deep learning.

439
00:27:45,640 --> 00:27:50,400
We do the land use classification, but the biggest problem is we don't have the ground

440
00:27:50,400 --> 00:27:51,400
juice, right?

441
00:27:51,400 --> 00:27:54,440
If we don't have ground juice, we cannot evaluate our model.

442
00:27:54,440 --> 00:27:58,800
So at that time, which is to check and like a university campus to do the land use classification

443
00:27:58,800 --> 00:27:59,800
task.

444
00:27:59,800 --> 00:28:00,800
It's a tribute example.

445
00:28:00,800 --> 00:28:03,720
It's a toy example, but we get like a very good accuracy.

446
00:28:03,720 --> 00:28:08,600
So since that time, we start to build a very large data set and actually we took like

447
00:28:08,600 --> 00:28:13,400
two years to build a data set and continue this line of land use classification system.

448
00:28:13,400 --> 00:28:14,400
Yeah.

449
00:28:14,400 --> 00:28:19,000
But then in this work in this game paper, we're just using a very standard resident one

450
00:28:19,000 --> 00:28:22,040
on one network to do the land use classification.

451
00:28:22,040 --> 00:28:25,480
Does the social media images come into play in this paper?

452
00:28:25,480 --> 00:28:26,480
Oh, yes.

453
00:28:26,480 --> 00:28:30,480
So for the ground level images from the geographic data set.

454
00:28:30,480 --> 00:28:33,280
So those are all like the ground level images.

455
00:28:33,280 --> 00:28:34,480
So social media, right?

456
00:28:34,480 --> 00:28:38,200
Those social media are contributed by just resident in United Kingdom.

457
00:28:38,200 --> 00:28:42,280
So anybody can submit an image to the website and the website will show it.

458
00:28:42,280 --> 00:28:46,120
So where the photo is taken and what it's about.

459
00:28:46,120 --> 00:28:50,720
So yeah, for that data set, it is totally like a social multimedia.

460
00:28:50,720 --> 00:28:54,560
And remind me how where that comes into play in the system?

461
00:28:54,560 --> 00:28:58,360
The system is when you do the discriminator, right?

462
00:28:58,360 --> 00:29:01,960
You need to tell this image is fake or real, right?

463
00:29:01,960 --> 00:29:05,080
But how do you determine whether it's real or fake?

464
00:29:05,080 --> 00:29:08,760
You have to look at some ground level images to know it is real or fake, right?

465
00:29:08,760 --> 00:29:09,760
Oh, right.

466
00:29:09,760 --> 00:29:11,640
So yeah, that's what they're coming to play.

467
00:29:11,640 --> 00:29:16,400
So although the input is overhead imagery, but the ground shows like something you compare

468
00:29:16,400 --> 00:29:18,640
to is the ground level images.

469
00:29:18,640 --> 00:29:22,040
So that's when the social multimedia coming to play.

470
00:29:22,040 --> 00:29:27,640
And those images are they're not labeled at all with regard to land use or anything

471
00:29:27,640 --> 00:29:28,640
like that.

472
00:29:28,640 --> 00:29:33,520
It's just that's the training the discriminator to understand real versus fake images.

473
00:29:33,520 --> 00:29:34,520
Yes.

474
00:29:34,520 --> 00:29:35,520
That's the beauty of gain, right?

475
00:29:35,520 --> 00:29:38,240
So we don't care about the labeling.

476
00:29:38,240 --> 00:29:40,560
We don't need to know the land use process.

477
00:29:40,560 --> 00:29:43,800
We just need to know this is a real image that is a fake one.

478
00:29:43,800 --> 00:29:44,800
So I.

479
00:29:44,800 --> 00:29:45,800
Okay.

480
00:29:45,800 --> 00:29:46,800
Cool.

481
00:29:46,800 --> 00:29:47,800
I think I've got it now.

482
00:29:47,800 --> 00:29:57,000
So the to sum it all up, right, you've got this challenge of being able to develop accurate

483
00:29:57,000 --> 00:30:02,800
land use classifiers, but you've got this problem of sparse data, right?

484
00:30:02,800 --> 00:30:07,680
So you've got all of this area that you might want to classify based on satellite images,

485
00:30:07,680 --> 00:30:12,320
but that you don't have specific ground level data for.

486
00:30:12,320 --> 00:30:18,640
So you generate some realistic looking ground level data using the conditional gain and

487
00:30:18,640 --> 00:30:22,280
then use that to train your classifier.

488
00:30:22,280 --> 00:30:28,280
And you've been able to kind of incrementally improve the performance of the classifier

489
00:30:28,280 --> 00:30:35,320
using this kind of data as opposed to previous data sources that are trying to approximate

490
00:30:35,320 --> 00:30:36,800
this data that you don't have.

491
00:30:36,800 --> 00:30:37,800
Yes.

492
00:30:37,800 --> 00:30:38,800
Yes.

493
00:30:38,800 --> 00:30:39,800
Correct.

494
00:30:39,800 --> 00:30:40,800
Yeah.

495
00:30:40,800 --> 00:30:41,800
Got it.

496
00:30:41,800 --> 00:30:42,800
Exactly.

497
00:30:42,800 --> 00:30:43,800
Got it.

498
00:30:43,800 --> 00:30:44,800
Okay.

499
00:30:44,800 --> 00:30:45,800
Awesome.

500
00:30:45,800 --> 00:30:46,800
Yeah.

501
00:30:46,800 --> 00:30:45,560
I don't know why this one was so difficult for me to wrap my head around, but I think a part of it

502
00:30:45,560 --> 00:30:48,760
had to do with this, this, this GAN head fake.

503
00:30:48,760 --> 00:30:49,760
Yes.

504
00:30:49,760 --> 00:30:51,400
So most of the people just focus on the GANs.

505
00:30:51,400 --> 00:30:54,520
Oh, what's GAN can structure you're using?

506
00:30:54,520 --> 00:30:57,320
Are you using the state of art again?

507
00:30:57,320 --> 00:30:58,320
So yeah.

508
00:30:58,320 --> 00:31:03,480
So actually, we're doing something like for geospatial analysis, so for the land use classification.

509
00:31:03,480 --> 00:31:05,120
So that's how we're finally aimed.

510
00:31:05,120 --> 00:31:06,120
Awesome.

511
00:31:06,120 --> 00:31:09,520
Are there other interesting aspects of this paper that we haven't touched on yet?

512
00:31:09,520 --> 00:31:10,520
Yeah.

513
00:31:10,520 --> 00:31:14,560
So for this paper, there's, no, not, but for the future work, right?

514
00:31:14,560 --> 00:31:16,720
I want to talk about the future work a little bit.

515
00:31:16,720 --> 00:31:18,040
So that's very interesting.

516
00:31:18,040 --> 00:31:22,520
So the first thing about the future work is, so right now, our generated ground level

517
00:31:22,520 --> 00:31:25,200
images, like, it's not real enough.

518
00:31:25,200 --> 00:31:27,560
So they lack the image details.

519
00:31:27,560 --> 00:31:32,600
So for some like houses or like animals, they don't, they don't look real enough.

520
00:31:32,600 --> 00:31:35,640
So there's like a plenty of room for improvement.

521
00:31:35,640 --> 00:31:41,080
So currently we're thinking so we want to use a technique called a progressive GAN.

522
00:31:41,080 --> 00:31:47,080
So from immediate, so that method is the key idea is to grow both the generator and

523
00:31:47,080 --> 00:31:48,880
discriminator progressively.

524
00:31:48,880 --> 00:31:53,840
So from a small network, from a small resolution, we add new layers to the model and

525
00:31:53,840 --> 00:31:57,600
increasing the model in a progressive manner.

526
00:31:57,600 --> 00:32:02,720
So this can both speed up the training procedure and stabilize the model.

527
00:32:02,720 --> 00:32:06,800
Because GAN is really hard to train, so sometimes it's just a model collapse.

528
00:32:06,800 --> 00:32:14,080
So this could eventually lead us to a very good image resolution, because currently our

529
00:32:14,080 --> 00:32:20,040
generated images only like 32 by 32 or 64 by 64, so it's very coarse.

530
00:32:20,040 --> 00:32:24,840
But eventually, we want to have some images like 1K by 1K or 2K or even 2K.

531
00:32:24,840 --> 00:32:29,600
So because most of the remote sensing images is like 2K by like 3K.

532
00:32:29,600 --> 00:32:31,960
So it's very large and it's very details.

533
00:32:31,960 --> 00:32:36,480
So we want our generated ground level images is also large and details.

534
00:32:36,480 --> 00:32:40,640
So I think that will bring up our performance by a large margin.

535
00:32:40,640 --> 00:32:41,640
Okay.

536
00:32:41,640 --> 00:32:43,440
So that's the first direction.

537
00:32:43,440 --> 00:32:48,840
And the second direction is, so there is also a recent work by OpenAI called GLOW.

538
00:32:48,840 --> 00:32:51,440
So it's there using reversible generated models.

539
00:32:51,440 --> 00:32:55,400
They're not using GANs, but reversible generated models.

540
00:32:55,400 --> 00:33:01,960
So that model, the most, the good thing about it is their latent space, I mean the features.

541
00:33:01,960 --> 00:33:05,440
So the features are useful for downstream tasks.

542
00:33:05,440 --> 00:33:11,280
Because in GANs, the data points can usually not be directly represented in a latent space.

543
00:33:11,280 --> 00:33:15,920
So because they have no encoder and they don't have all the data distribution.

544
00:33:15,920 --> 00:33:21,640
And for reversible generated models, they can, they can interpolate between these features

545
00:33:21,640 --> 00:33:22,640
space.

546
00:33:22,640 --> 00:33:23,640
So it's very smooth.

547
00:33:23,640 --> 00:33:26,760
So in that case, we can directly use the features.

548
00:33:26,760 --> 00:33:28,320
We don't need to use images.

549
00:33:28,320 --> 00:33:31,240
So I hope that can be a better solution.

550
00:33:31,240 --> 00:33:35,480
And then I think like a lot of people are interested in this paper as well.

551
00:33:35,480 --> 00:33:36,880
So the GLOW one.

552
00:33:36,880 --> 00:33:39,800
So it can generate very realistic images.

553
00:33:39,800 --> 00:33:44,200
And the third direction will be using more information as you talk about.

554
00:33:44,200 --> 00:33:49,080
So maybe using like a information retrieval thing, we're using some like a text.

555
00:33:49,080 --> 00:33:50,320
Text information, right?

556
00:33:50,320 --> 00:33:55,480
Because for example, if we want to generate like a forest like image.

557
00:33:55,480 --> 00:34:01,840
So if we just give him the overhead image, so maybe he cannot like give us a good image.

558
00:34:01,840 --> 00:34:08,200
But if we can say so I want the forest to wheel with like with one house inside it.

559
00:34:08,200 --> 00:34:14,120
So maybe in our generated GLOW images, we will see exactly one house in a forest.

560
00:34:14,120 --> 00:34:15,280
Like scenery.

561
00:34:15,280 --> 00:34:16,800
So that's very promising.

562
00:34:16,800 --> 00:34:19,160
So that's the third direction to go.

563
00:34:19,160 --> 00:34:27,640
So so this works, this can work is our initial attempt to do this dance dance interpolations

564
00:34:27,640 --> 00:34:28,640
in this direction.

565
00:34:28,640 --> 00:34:31,160
So we have a lot of work coming up.

566
00:34:31,160 --> 00:34:34,600
So hopefully we can get better results in the future.

567
00:34:34,600 --> 00:34:38,760
You mentioned with regard to the GLOW paper, I've seen it.

568
00:34:38,760 --> 00:34:43,440
But I haven't looked at it in any detail at all.

569
00:34:43,440 --> 00:34:49,520
You mentioned that part of what it allows you to do is to get these smooth representations

570
00:34:49,520 --> 00:34:52,320
in a feature space.

571
00:34:52,320 --> 00:34:57,760
And are you would you then be trying to use that feature space or is the only benefit

572
00:34:57,760 --> 00:35:01,480
to you of that that it produces better images?

573
00:35:01,480 --> 00:35:02,840
I think we will try both.

574
00:35:02,840 --> 00:35:04,840
So better images and the features.

575
00:35:04,840 --> 00:35:10,160
I think maybe the image maybe the features makes more sense because for GLOW work, they're

576
00:35:10,160 --> 00:35:11,160
reversible, right?

577
00:35:11,160 --> 00:35:16,920
But reversible means if you have some input and go into the output, it can also use output

578
00:35:16,920 --> 00:35:18,320
to get you the input.

579
00:35:18,320 --> 00:35:21,800
So the reversible makes the features more robust.

580
00:35:21,800 --> 00:35:22,840
So it makes sense.

581
00:35:22,840 --> 00:35:25,840
It is like explainable.

582
00:35:25,840 --> 00:35:30,880
So in that case, the features might be more powerful than the again features.

583
00:35:30,880 --> 00:35:33,280
Are the features in this feature space?

584
00:35:33,280 --> 00:35:34,880
Are they semantically related?

585
00:35:34,880 --> 00:35:40,480
Like is it kind of an embedding in the feature space where you can get these semantic relationships

586
00:35:40,480 --> 00:35:43,520
between these different types of generated images?

587
00:35:43,520 --> 00:35:44,520
Yeah, definitely.

588
00:35:44,520 --> 00:35:49,040
I think the features should be like semantic related.

589
00:35:49,040 --> 00:35:56,280
So if we do like a feature space visualization, we can see like a tree forest cluster together

590
00:35:56,280 --> 00:36:00,280
under the river water lake, those images are clustered together.

591
00:36:00,280 --> 00:36:03,040
So the features are definitely semantic related.

592
00:36:03,040 --> 00:36:07,560
And so do you think you'll get to a point where you can start with a ground truth image

593
00:36:07,560 --> 00:36:12,560
of a field and say like I want a little bit more, you know, rivers and then get a stream

594
00:36:12,560 --> 00:36:16,760
and a little bit more rivers and get like a bigger body of water or something like that?

595
00:36:16,760 --> 00:36:18,520
Well, you're really like smart.

596
00:36:18,520 --> 00:36:21,800
So that's something we're trying to do right now.

597
00:36:21,800 --> 00:36:25,960
So that's more like a manipulative of the image, right?

598
00:36:25,960 --> 00:36:32,000
So yeah, if you can do that, so that's very interesting to the geospatial communities.

599
00:36:32,000 --> 00:36:37,160
Just to kind of wrap things up, I'm curious you mentioned how difficult training the

600
00:36:37,160 --> 00:36:39,480
GANs has been.

601
00:36:39,480 --> 00:36:47,240
Can you maybe share with us some things that you've learned as you try to work with GANs

602
00:36:47,240 --> 00:36:48,480
and conditional GANs?

603
00:36:48,480 --> 00:36:49,480
Okay, sure.

604
00:36:49,480 --> 00:36:54,560
So for GANs, so because we're using a very standard conditional GANs, which is proposed

605
00:36:54,560 --> 00:36:58,840
like 2015 or 2016, so it's a very earlier stage of the GAN.

606
00:36:58,840 --> 00:37:03,400
So the training is like on stable, so I have a lot of problems.

607
00:37:03,400 --> 00:37:09,800
So as mentioning another paper, I forgot the exact name, but it should be like a good

608
00:37:09,800 --> 00:37:13,120
practice is during like training, implementing GANs.

609
00:37:13,120 --> 00:37:17,640
So they propose like we should use straight-ed convolution, not pooling because pooling can

610
00:37:17,640 --> 00:37:21,960
hurt the like image resolution thing during the down sampling.

611
00:37:21,960 --> 00:37:26,120
And we should like using smaller crop size and the smaller learning rates, something

612
00:37:26,120 --> 00:37:27,120
like that.

613
00:37:27,120 --> 00:37:33,280
But I don't think that's a major problem right now because most of GAN models is

614
00:37:33,280 --> 00:37:36,520
like easier to train at this moment.

615
00:37:36,520 --> 00:37:43,200
So because the loss matrix changes to the loss system loss, so that is a very stabilized

616
00:37:43,200 --> 00:37:48,080
loss function and also the network architecture change.

617
00:37:48,080 --> 00:37:51,520
So right now we can train very deep networks using GANs.

618
00:37:51,520 --> 00:37:56,640
So for example, like a ResNet 101 and the output can be several hundred resolutions

619
00:37:56,640 --> 00:37:58,280
or even one K resolutions.

620
00:37:58,280 --> 00:38:01,280
So the training of the GAN is not that hard right now.

621
00:38:01,280 --> 00:38:05,840
Awesome, well E, thank you so much for taking the time to share with us what you're working

622
00:38:05,840 --> 00:38:06,840
on.

623
00:38:06,840 --> 00:38:07,840
It's really interesting stuff.

624
00:38:07,840 --> 00:38:09,120
Yeah, no problem.

625
00:38:09,120 --> 00:38:14,160
Alright, everyone, that's our show for today.

626
00:38:14,160 --> 00:38:20,280
For more information on E or any of the topics covered in this episode, head over to twomolei.com

627
00:38:20,280 --> 00:38:23,320
slash talk slash 172.

628
00:38:23,320 --> 00:38:34,600
As always, thanks so much for listening and catch you next time.

