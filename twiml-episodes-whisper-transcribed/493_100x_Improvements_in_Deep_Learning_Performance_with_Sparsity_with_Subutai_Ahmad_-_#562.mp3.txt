We think with the right hardware, you know, optimizations, 90%
sparse should give you more than a 10x gain. So it's not, we're not talking
about 50%, 20%, 30%, but we're talking about orders of magnitude here.
All right, everyone. I am here with Subatai Ahmed. Subatai is VP of research at
Numenta. Subatai, welcome to the Twoma AI podcast. Thank you so much for having me, Sam.
Let's get started like we do here on the show by having you share a little bit about your
background and your journey into the world of AI. Sure. So I've actually been in deep learning
and something called computational neuroscience since the late 80s, long before it was called deep
learning and I did research in, you know, understanding models of the brain and, you know,
how that impacts machine learning in the late 80s and early 90s. I couldn't really see how to
do anything practical with that at that time. And so I sort of switched gears and did research
in more traditional machine learning and then did a couple of startups. About 17 years ago,
I ran into Jeff Hawkins and Donna Dabinsky when they were founding Numenta. And it's sort of all
my worlds kind of came together at that point. You know, Numenta as a company is really trying to
show how we can take the neuroscience and really translate it eventually into practical systems.
And so I've been at Numenta doing research for the last 17 years on that topic,
very fortunate to be able to do that. Oh, that's fantastic. When we throw around this idea of deep
learning being biologically inspired all the time, but I think Numenta is one of several places
that's trying to push that idea of biological inspiration even further. Can you talk
about the company and kind of the core ideas that you're researching?
Yeah, I think deep learning has been somewhat biologically inspired and they're definitely,
you know, the idea of modeling a neuron and, you know, some of the ideas have been convolutional
networks and so on are directly taken from the neuroscience. But by and large, you know,
if you look at neuroscience, it's pretty far from, you know, a lot of the details. It's very,
very abstracted out. And so the core idea behind Numenta is, you know, deep learning is great.
If you can really solve a lot of practical problems, but if we really want to understand intelligence
and build truly intelligent systems, we need to go back and see what, you know, what have
neuroscientists learned and how can we, you know, take those properties and create sort of
algorithms, understand it from the algorithmic standpoint and then incorporate those ideas into
deep learning. You know, we feel just, you know, building bigger computers and faster computers
and throwing more data at it is all great, but that's not, though, that's not going to lead us
to intelligent systems. So we think it's, you know, we have an existence proof. There's an
amazing thing, the brain that is super intelligent, it's far more intelligent than any deep learning
system out there. Why not try to understand, you know, what's going on there and see if we can
apply that to practical systems? When you think about some of those core things that neuroscientists
have learned or that neuroscience teaches us that we can put to use in building learning systems,
what are those things? What does that landscape look like? Yeah, there's, there's quite a lot.
But neuroscientists have field has really exploded in the last 20 or 30 years. You know, many,
many people may not realize this, but the experimental techniques have really gotten extremely
sophisticated. And so there's many, many things in the neuroscience now that we can actually
apply to practical systems. At the same time, there's a ton of detail in neuroscience that is not
applicable to practical systems. So it's, it's a bit of a challenge to figure out exactly where to
draw that line. You know, what aspects are really important and what aspects are just sort of more
biological detail that don't impact practical systems. So kind of the, you know, some of the big
items that hopefully we can get into more detail on just ideas like a cortical column idea that
there's a common micro circuit that's underlying all of intelligent function. The idea of the neuron
itself, the neuron neurons in biological brains are much more sophisticated and powerful than the
neurons we use in deep learning. You know, even core representational ideas like sparsity, the
brain is extremely sparse and leads to a lot of efficiencies and other properties. Yeah,
can we extract some of those ideas and incorporate it in? Even some ideas like the, you know, the brain
is inherently a sensory motor system or constantly moving around the world and we learn by movement
and we learn the structure of the world through movement and this ties into how cortical columns
are set up. So all of those ideas are somewhat different than what's in deep learning today and
those are concepts that we think is a lot that's understood now and it behooves us to kind of
incorporate that into deep learning systems today. So this idea of a cortical column as this
fundamental micro circuit is a compelling one is not something that I've heard of previously. Can
you dig into what that means, what this micro circuit looks like and what we know about it?
Sure. It's actually a fantastic idea. It's an amazing idea and this was, I think first
proposed by a neuroscientist called Vernon Mount Castle in the 70s and what he noticed is that
wherever you look in the neocortex. So first of all, the neocortex is kind of the largest
structure in our brain. It's really where most of intelligent function actually happens and,
you know, whether it's visual processing, auditory processing, language, you know, high level thought
that all occurs in the neocortex. So what he saw is that no matter where you look in the
neocortex, you see a very, very similar micro circuit in a very similar connections between the
layers and between neuron similar neuron types and a sort of a prototypical architecture.
And there are entire neocortex has somewhere around 150,000 cortical columns. So each cortical
column is about the size of a grain of rice and it sort of sprinkled throughout our neocortex.
And like I said, they all have this common architecture. And so do you say that when you say
common architecture and connections, the picture that pops up in my mind is kind of the neuron
and the dendrites and these connections. But it sounds like you're talking about a higher level
structure. Is that right? Yeah. So the new cortex, the way it's structured is it's like a
big flat 2D sheet that sort of scrunched up and stuck in your brain. But if you were to flatten it
out, it's about a couple of millimeters thick. And there's several layers, you know, down that
thickness in that dimension. And so there's about, you know, scientists say roughly six layers.
And these layers have a, you know, intricate connectivity between them. So there's different neuron
types that are in different layers and they connect in a recurrent circuit. And that architecture
is what what we mean when we say that's repeated throughout the, you know, cortex. So it is,
it is a little bit complicated to go through. But this, I'd say probably 50,000 or 100,000 neurons
inside a cortical column and a fairly sort of prototypical connectivity structure between them.
And what have we learned about the columns themselves and their behavior? And I'm imagining this
is something that you, you know, might try to model computationally as well. Yeah, there's a lot
we can do computationally. And what neuroscientists have found, you know, there's this common structure
and the proposal by Mount Castle is that, you know, the reason of visual area as a visual area is
not because there's anything really fundamentally different. It's just that the inputs are different.
The learning algorithms and the architecture is largely similar to the auditory cortex or to
the language areas. And neuroscientists have even done this experiment where experiments where you
sort of swap modalities and you take an auditory auditory cortex and you feed it visual information.
And what you see is you actually see visual feature detectors show up in the auditory cortex.
And, you know, so that tells you there's a very similar kind of algorithm. It's it's not about
the sensory modality. It's about the common algorithm. So just to interject the meaning that this,
you know, we talk about the convolution being neurologically inspired. Is the suggestion that
that's a software feature as opposed to a hardware feature? Exactly. Yeah, so to speak. Yeah,
so convolution sort of says basically that you have sort of similar feature detectors, you know,
throughout your visual field. So that's and you do see that in the in the visual cortex. You see
very similar features across what this says is even more it takes that sort of one notch off. It's
like the entire architecture. It's not just one little feature as sort of the entire architecture is
really preserved across sensory modalities. And this is an amazing thing. If this were true and we
believe this is largely true is in order to really understand how to implement intelligent systems
in a new or cortical way, you sort of have to understand how one cortical column works and how
multiple cortical columns interact. And then it's just essentially a scaling problem, you know,
you just build more and more of them. So that really simplifies the process and and from a
computer scientist point of view, that's great. It's like there's one thing we need to understand.
It may be complex, but it's not that complex. You know, we can understand it. And once we understand
it, it's a scaling issue. And so how far along are we in this journey to understand the cortical
column? Yeah, I think we've made a lot of progress in understanding it. There's still a lot of work
to do. You know, one of the so, you know, if you look at convolutional neural networks, they
may make maybe one or two layers of this six layer structure. So they, you know, so they may make
a small piece of that. Some of the things we've learned is that every cortical column is actually
inherently a sensory motor system. And what I mean by that is every cortical column gets sensory
inputs and every cortical column actually sends out motor commands. So when you look at the visual
area, it's not just a feed, it's not just an area which gets visual input. It actually sends out
signals, you know, to your eyes, for example, and your head to move your, you know, head and eyes
around. If you look at the so-called motor areas, they actually get sensory input as well. So
there's no such thing as a sensory area or a motor area. Everything's an inherently sensory motor
area. And what we know is that mammals in particular and humans, we really learn by moving around
the world. So this algorithm is inherently one that's constantly driving action, constantly making
predictions. We see when our predictions are correct and when they're not correct and we learn
from those mistakes. And that's really how we learn about the world. We're not just passive
systems that, you know, getting billions of images as input and then suddenly we can recognize
cats and dogs. You know, we inherently understand the world and the structure of the world by moving
around and making actions and stuff. So that's a really big, you know, component of it.
Another sort of really interesting thing is that, you know, if the cortical column is the same
everywhere, the implication of that is that if some area of the brain is doing something,
it must be doing that algorithm everywhere. So if you look at high-level thought and what's
required for high-level thought, well, the same processes must be occurring also in low-level
visual areas and other areas. And what our theory says is that every cortical column is actually
its own independent modeling system. So every little cortical column is building a structured
model of the world through movement by understanding how movement shapes perception and builds up
a model of the world. And all of these cortical columns, the way they work is they interact
and they're with each other and they come to a consensus about what is the current percept that's
coming in. And through this sort of voting algorithm, that's essentially what we are consciously
aware of. You know, every with thousands and thousands of these cortical columns, each working
independently. And then they come to a consensus about what actually we are seeing and I think.
So it's an extremely distributed system of lots of independent modeling. Again,
this is very different from the way deep learning systems are struggling today.
Right. It raises a lot of really interesting questions for me. Like,
you know, we talk about these distributed system with lots of common components. You know,
one view is like a swarming kind of thing where everything's the same and you have these voting
mechanisms, but you've also described in talking about the visual versus auditory function,
a degree of specialization. And I'm still trying to wrap my head around like, is that hardware
or software specialization, meaning does the, you know, almost like kind of is there like a bone
marrow kind of is it kind of like a bone marrow and it like physically changes to specialize or
is it just changing thresholds or software things? Yeah. Yeah. That's the answers to questions
like these. I mean, we know some of it. You know, that's it's a great question and it's something
that's puzzled nor scientists for a long time. If you look at the auditory area, you see auditory
neurons. And if you look at the visual area, you see visual neurons. So what's, you know, what's
common there? Well, what's so, you know, it's similar to a deep learning system in the sense that
you can take a convolutional network. If you feed at visual information, it's going to learn edges
and, you know, corners and so on. If you feed at auditory information, it's going to learn auditory
features. So it's, it's the same thing. It's these are, there's a learning algorithm that's going
on. And because the input is different, it's just going to learn different things. And you can't
just look at the end of the day and just probe it and see what it's learned. You have to understand
the learning process itself. So it is, you know, the hardware is essentially very, very common.
But the sort of emergent functionality is very different because the inputs against is very
different throughout throughout its life. I mean, there are some differences between auditory areas
and visual areas. So there's some specialized color detectors and things like that in the
visual areas that you don't see in the auditory areas. And so, but those are really in the, in the
details. If you step back and look at the large scale architecture, there's huge commonalities
across them. Yeah, it is, it is interesting. It's fascinating to think through.
Absolutely. So the cortical column is kind of this higher level architecture. You also mentioned
the neuron itself and the way we've been modeling that for deep learning versus the way we think
about that from a neuroscience perspective. Can you elaborate a bit on that? Yeah. So if you look
at a deep learning system, you know, the basic idea of what a neuron does is it takes a very simple
function of the input and computes a very simple outputs and basically takes linear weighted sum
off its inputs and passes it through non-linearity, like a sigmoid or a relu and then outputs and
there's a very simple idea. That idea has been around for more than a hundred years. It's called
the point neuron model. And that basic idea in our mathematical system and practicalism really
hasn't changed. But if you look at the biology, real neurons are nothing like that. Real,
real neurons, you know, you mentioned these dendrites. The neurons have really complicated
dendritic structures and these dendritic structures as where neurons get input. And the neuron actually
does very sophisticated processing of its input through these dendritic structures before actually
computing an output. And so there are many, many layers of this. But you know, neuron is a pretty
sophisticated computing device in the biological brains. And so, you know, we've been sort of thinking,
you know, what, what aspects of that do we actually need to model in deep learning systems?
Are there advantages to these dendritic structures? And when we think about things like
continual learning, you know, the fact that, you know, humans can constantly learn new things
without forgetting stuff, you know, deep learning systems are not good at that. And we think the
dendritic structure and the non-linear processing that goes on in these dendrites and neurons is
actually critical to how we learn new things without forgetting stuff. So that's an example of,
you know, some functionality that that's difficult today in deep learning that could be improved
if we, if we could incorporate some of these properties. I'm curious what the neuroscience says
around like the, are these cortical columns and the neurons? Are they fundamental for
memory as well as kind of the processing? Like, is it uniform across these very different
modes as well? Some of the more recent deep learning research directions are like incorporating
memory into deep learning systems as a way to, you know, get closer to the kind of intelligence
that we exhibit. But I'm wondering if you're saying that, well, there's a single thing underneath
from a biological perspective. Yeah, I mean, it all has to be done by neurons, right? Some are
on. Yeah, exactly. It's nothing else. So it's, you know, in the brain, you know, memory, learning,
language, all of those functions, a large part of it happens in the neocortex and a lot of it
happens in what's called the hippocampus and hippocampus structures as well. And all of those both,
you know, both of those structures have exact same type of neurons that I was talking about,
what's called pyramidal neurons. So they have the same complicated, you know, nonlinear
integration. So those properties of neurons are common for all function, you know, all intelligent
function, you know, memory, language, speech, you know, as you and I are talking and listening to
each other, that's what we're doing. Our dendrites are processing away and creating these representations.
To what degree are the notions that notions around like spiking something that you're focused on
in your research? Yeah, so neurons send signals to each other by initiating a spike. So it's an
electrical signal that, you know, goes from one end to the other. We think we're not sure that
that's really critical to a model or not. You know, we do need to communicate from one neuron to
another, but it may be okay to just, you know, put a number in a memory location. That may be,
that's what happens in deep learning. And that may be just fine. You know, it is true that spiking,
you know, is primarily binary, like it either a neuron, either spikes or it doesn't,
whereas in deep learning system, we tend to transmit really high precision numbers.
And that's part of the reason deep learning systems are so expensive and energy inefficient.
So if you could go to the point where we can transmit essentially binary information, there's a
chance we could make deep learning systems really, really efficient. And so that's one area where
the spiking, it may be important to model spiking, but by and large, you know, in our work,
we haven't found it really necessary to model kind of the details of all the details of spiking as
it is in biology. So, you know, again, with neuroscience, it's always a question of where do you draw
the line? You know, what level of detail do you incorporate and what level you've done? And so
this is where we're kind of on the border of. And I imagine that that is a kind of a combination
of, you know, intuition and experimentation and seeing what works. Exactly. Yeah. Yeah. We try a
lot of stuff. Our bias is that if something is really prevalent in the neuroscience, if it's really
common and a big feature, it probably has some views. And so we do look at all of this stuff pretty
carefully, but we don't incorporate it into our models until we can actually come up with a
functional reason for it. Like there's some, you know, some benefit to be gained, but we do look at
a lot of these details pretty closely and talk to neuroscientists constantly. So we've talked about
cortical columns, the neuron model, kind of from the biological neuroscience perspective,
how do these concepts translate into things that, you know, learning machines for to say broadly?
You know, from a cortical column perspective, you know, if we can, you know, understanding that
architecture helps us, what should help us design better kind of modules and layers and deep
learning system. In the deep learning system today, it's very, very simple, you know, you have a
convolutional layer or maybe even a transformer linear layer or something like that. They're very
simple layers. What the neuroscience tells us is that each layer is a lot more complicated,
has a recurrent structure and it gets sensory input and motor output. So we can actually take the
notion of what a layer is in deep learning and incorporate some of these other elements into it and
make it a lot more, you know, complex. And that will provide a lot of benefits as I mentioned,
the idea of continual learning, for example, you know, the ability to learn new things without
forgetting stuff. But more importantly, if we can make these layers inherently sensory motor,
what we can build in are layers that really understand the 3D structure of the world and the
physical structure of the world. They inherently understand it. They build 3D models at every level
of the hierarchy and this will allow you to have extremely robust neural networks that don't get
fooled very easily with, you know, input that's slightly different from what it's seen before.
It should lead to neural networks that are much more invariant to distortions and things like
that. Today's neural networks, you really have to show it, you know, let's say you're doing a
visual system, you have to show it images of every object in every possible pose and every possible
lighting condition and all of that stuff. Whereas if you can really inherently understand the
structure of stuff, the amount of training data you will need will be dramatically smaller.
And the representations you build will be much more robust and invariant. So these are the
kinds of properties. I think we can really build in at a very fundamental level into deep learning
systems by taking clues from biology. What does it mean for a model to have an inherent 3D
understanding? I mean, it's a number, right? Are we talking about like changing the coordinate
system, the something polar? It's a fantastic question. It's something we've worked on a lot
exactly that question. We published this theory called the 1000 brain theory. So this idea that
cortical column, the thousands of these cortical columns and each cortical column is kind of this
independent modeling system. So what the 1000 brain theory says, and this is a lot of its derived
from neuroscience data, is in each cortical column, you have, you are building up models that are
based on coordinate systems, just like you mentioned. So there's something essentially reference
frames in each cortical column. And you can think of us, think of cortical columns as building maps
of the world. So just like you have a physical map that might show a city and shows how you can
move around a city and what's located at different GPS coordinates along the map. In the same way,
when we inherently build up a structured model of an object, we create a 3D map. And in that map,
we associate locations with features and properties of the object. And we know how you can manipulate
that object, how you can make predictions about what will happen if you go from one part of the
object to the next. And that map is in what we call in the reference frame of the object itself.
It's not in the observer's reference frame. It's in the coordinate system of the object.
And so that's essentially what we mean by inherently understanding the object. You would build up
this really detailed map life structure of objects in its own reference frame. And we may be viewing
it from very different angles in many different ways. But all we have to do is translate that new
viewpoint into this map life structure. And now we can navigate and understand how that object
works. So that was quite a mouthful. There was a lot of stuff in there. But inherently, we're
building up these map life structures that contain the full physical and geometric structure of
objects and concepts and so on. Yeah, it's reminding me in some ways to the, I'm blanking on the
the model, but like Jeff Hinton's, the name of the model, but Jeff Hinton has been talking about
this post convolutional model that has these properties of being kind of more spatial and
translation invariant and things like that. Yeah, so he came up with this idea capsule,
maybe they're the same as capsules. Yeah, so Jeff Hinton has actually been thinking about these
ideas since the 70s. We found papers of him writing in the 70s about how we need to build up
object-centered reference representations that are an object-centric reference frame. So he's
been thinking about this a long time. And so everything I mentioned is definitely very much
in the same kind of line of thinking. I would say what we've learned from the neuroscience is that
these particle columns are much more powerful than we thought. It's much more than what a capsule
does. They're really independent modeling systems and they're inherently sensory motor.
So motor actions and predictions are an inherent piece of this puzzle. And so these are all
aspects that have to be incorporated in as well. But definitely very, I think there's some
truism in deep learning that no matter what idea you think of, Jeff Hinton has probably thought
of it 20 years ago. He's great. A similar question to the last one. What does it mean for
these computational models to be inherently sensory motor? Is that implying like a system level
connection between input sensors and representations or something else? Yeah. So basically
cortical columns send out motor commands. And so there are 150,000 of these cortical columns. They're
all sending out signals to your motor systems. So whether it's manipulating your hands, your
eyes, your head, your speech systems, all of that stuff is getting those what we call sub-cortical
structures. Motor systems are getting sort of input from the neocortex. And there's some sort
of a reconciliation that has to happen. And again, sort of like a voting process. And then the
motor system decide, okay, these are the muscles I have to move in this way to achieve the
the goal that the neocortex is telling me. So basically there's some sort of to use reinforcement
learning terms as an action policy. There's a lot of possible actions that could happen. And there's
some arbiter that's figuring out, okay, what is the best thing I should do at this point in time?
It's still not clear to me how that necessarily what that looks like from a systems perspective,
although it does prompt this really interesting thought that kind of echoes RL or even like an
active learning where, you know, today we collect a bunch of data, throw it at some model and train
it. And the model doesn't really have anything to say about the data that it receives. And this is
kind of suggesting an active learning-esque kind of inherent capability to the model where it is
telling some downstream system what it needs to perform. Is that that sounds aspirational as
opposed to what we're doing today? Yeah, yeah, I mean, it is, it's exactly that. It's an active
system. And it's what brains are doing constantly. You and I are doing this right now. And so we
are not passive systems. And so we are inherently active. And that is a large part of why we are
so able to learn, you know, fundamentally what how our world works. And so, you know, it's a
predictive system. So when we make, when we send out motor commands and send out actions,
we make predictions about what we're going to see. And based on what we actually end up
sensing, we can update our internal models using the error and the predictions. And this is an
extremely efficient way of learning. And so, you know, and we're going to take actions that
are going to tell us most about the world. You know, if I've seen, you know, something over and
over again, I'm just, I'm mostly going to ignore it. I'm going to go for the novel stuff. And,
you know, that's how I'm going to learn most quickly. So it's, you know, as opposed to seeing the
same data, you know, over and over again, you'll, you'll really become efficient at how you learn,
learn stuff. Yeah. Yeah. I think part of the way I asked the last question was trying to get at. Where
are we today with this line of research? You know, granted that is research and it's trying to
to do, you know, big things that are modeled on biology. But what indications you have that it
is a promising direction to go to build the kind of systems that, you know, we want to build whether
that's what today's deep learning is doing or, you know, what, you know, we want it to do in 10
years. Yeah, it's definitely still research. I think we're making really good progress on it.
We are focused on building sort of initial machine learning based models of these cortical columns,
trying to figure out exactly how these reference range transformations should happen. How do you
build up these object centric models? And then, you know, some of the issues we discussed about how do
you, you know, take the motor signal or the action output and then translate it into movements of
your sensor and so on. So we're making a really good, you know, progress on that, but we definitely
don't have, you know, the full system working at. But it, you know, I don't want to put timelines on
it, but it's something we're really excited about and hopefully we'll have stuff to announce on that
soon. Yeah, nice. You mentioned sparsity as kind of this foundational property of the way that
you build out these models. Can you elaborate on that a bit? Yeah, yeah, I can talk quite a bit
about that, you know, in contrast to the cortical column stuff, which is there's still quite a bit
to figure out and we're making good progress on it. I would say the sparsity stuff is something
that's really practical and we've shown use, you know, a lot of use cases today on large scale
models and small scale models and deep learning. So sparsity is basically when you have weights that
are weight matrices or weights that are mostly zero, meaning that there's a very sparse set of
connections between layers. In the brain, you see another type of sparsity, something we call
activation sparsity, meaning very few neurons are firing at a time. So, you know, only about 1%
of neurons in your cortex are about firing at a time. And so there's sort of connectivity,
sparsity in connections and sparsity in activations. And both of those factors can actually lead to
tremendous efficiencies in processing deep learning systems. The brain uses only about 20 or 30 watts
of power, which is pretty amazing when you consider, you know, the billions, tens of billions of
neurons in there. And you can compare that with today's GPU based systems, which are power,
you can probably power a small village with some of the clusters that are out there.
And the way that this works is basically, you know, if you have zeros in your weight matrix,
you can skip that multiplication because you know already that multiplication is going to be zero.
There's no point doing that multiplication. And if you have both activations that are sparse
and weights that are sparse, you get this multiplicative effect where a fractional
percentage of the multiplications actually have to be performed. So one way to think about this
is suppose you have 90% weight sparsity. So only 10% of the weights are non-zero. So you could
imagine like a 10x gain, you know, you can skip, you know, 910s of the multiplications. But if you
also have 90% activations sparsity, only 1% of the products are going to have non-zero on both
sides. So you can skip 100 times the computation. There's this sort of multiplicative effect that
happens. And what we've focused a lot on is how do you translate that into deep learning systems
that are accurate at the same time? How can you change that? That into hardware architectures
and actual implementations that can actually exploit that efficiency? And those are both areas
that we've started to make tremendous progress on and learn quite a bit about.
And how do you go about approaching that? It strikes me as kind of the classical 10 cent for the
not-and, you know, $1,000 for knowing where to put it. Yeah, exactly. That's actually a great way
to say it because, you know, what we've been doing in deep learning is throwing more and more
compute at the problem. And it's great for some companies that are making GPUs. It's fantastic
for them. They just want to throw more compute at it. But it would be even better if you just
didn't have to do the compute, right? At all. And that's what sparsity allows you to do is sort of
knowing where to skip the compute. And so there are two challenging aspects of that. One is like,
how do you even train networks that are accurate and but have all of these zeros flying around
all over the place? And the second piece is how do you actually translate into hardware architectures?
And so with the first part in training, what we have found is that there are a number of a bunch
of different ways you can go about it. One sort of critical aspect is that the way you train
sparse networks is different from the way you train dense networks. And in particular,
you know, with deep learning you have to do really be careful about your parameters and how you set
up the network and how you set up the experiments and really explore, you know, the hyperparameters
and so on. And with sparse networks, what we found is that you need to do your own hyperparameter
exploration in a way that's quite different from the way you do dense networks. And so we've,
you know, that's one of the big things that we've learned coming out of this.
What is it about sparsity versus density denseness that drives this different way that you
need to approach it? So, you know, a lot of deep learning systems are trained using back
propagation and back propagation inherently assumes you're in this dense and dimensional space
and it's trying to move around this end dimensional space, whereas if you have a sparse system,
you know, an exponentially large percentage of that space is just out of bounds. And so you just
can't go there. And so you're really fighting against what back propagation wants to do.
And so we've had to use a lot of tricks and hyperparameter optimization techniques. We use a technology
from a company called Sigopt that has this sort of Bayesian hyperparameter optimization techniques
that's worked really, really well for us. And we've used that to figure out exactly what
combination of hyperparameters really allow back propagation based networks to effectively
kind of navigate that space and find global optimal or something close to global optimal.
Have you kind of measured the, like, or how do you measure even the difference between trying
kind of standard back prop versus more of an optimization type of an approach?
Yeah. So, so back prop, you know, you're typically computing some sort of a loss function or
error function and you kind of measure that. What's important for us is not just that error function,
but also other things like sparsity, like activation sparsity and stuff. So we have to measure
multiple things at once. So it becomes a more complicated optimization process. It's not as simple
as just finding the lowest error. You need to find the lowest error in conjunction with networks that
are as sparse as possible from a connection standpoint and as sparse as possible from an activation
standpoint. And so doing this sort of multimetric optimization is quite tricky and this is where
sort of the sigop technology that I mentioned really helped us and really shines in that respect.
So yeah, it is tricky to figure out, yeah, how do you measure it and how do you go about doing it?
That's, there was a lot of learning that was involved in that. And is the, is the,
this optimization process, is it telling you, is it telling you broadly, like, is it giving you
some broad parameter around sparsity, like, you know, level of sparsity, or is it telling you
specifically, you know, at any given time step in a training loop, like what neurons you don't
need to worry about? Yeah, it can tell us both actually. So yeah, and so in some cases, we know
what level of sparsity we might want in the weights, but we have a lot of freedom in how, how much
activation sparsity can have as one example. And so you want to be able to balance accuracy versus
sparsity in that case, and it can help guide in both areas. I think what I'm trying to reconcile is if
if, if you're, if each of the, each of the weights, or if each of the weights is a, you know,
a metric that you're trying to optimize the, the spaces, the dimensionality of the space is
ridiculous. Right, right. That, is that what you're doing? Yeah, it's something we call,
well, it's not, it's something we're doing. It's also something the brain is doing. So what's,
what people may not realize is that the connectivity in our brain is actually not fixed.
The neurons in our brain are constantly adding and dropping connections. So it's something we
call dynamic sparsity. The connectivity itself is being learned, which is kind of mind boggling
to think about. Something like, I saw a study that in the adult brain, something like 30% of the
connections are different every few days, which is just a staggering number to think about.
It says your brain is going to be quite different a few days from now. And what's going on is that
neurons are constantly trying to learn new things and forget about the stuff that's no longer relevant.
So you have your core memories that are, that are going to be stable. And then you're constantly trying
to learn new things. And this gets back to the continuing learning thing I alluded to earlier.
We're constantly trying to learn new things. And the brain does that by growing new connections
really quickly. And then if something sticks, those connections will become stronger. But most of the
stuff we see day to day are just random connections and curious connections. So those will,
we'll drop off. So that's kind of what's going on. Now we have to translate that to deep learning
where you have to actually learn the mask over the weights, like which weights are going to be
on or off. And yeah, that makes the whole process quite interesting. And I'm still like, is the,
are you learning the mask at the individual weight level or is that parameterized in some way?
There are some smaller dimensionality of like mask patterns that you're optimizing over.
Yeah, it's a great question actually. We just published a paper called two sparsities are
better than one where we showed that when you think about hardware architectures, you actually
have to be careful about the sparsity patterns because some sparsity patterns will map really
well to the hardware and others won't. And so we've come up with some techniques called complementary
sparsity where there's a set of patterns that map really, really efficiently to the hardware.
And so you want your training algorithms to understand the constraints of the hardware
in this optimization process. And if you can do all of that stuff well and balance everything
well and still get really accurate networks, we've shown you can get actually two orders of
magnitude improvement in performance. So you can get networks that a hundred times faster
on the same hardware, then a corresponding dense network would be on that architecture.
By hardware here, we're talking about GPUs or FPGAs or exactly. Yeah, so the paper we published
was a proof of concept on FPGAs and the nice thing about FPGAs is we can really design the
circuits to be exactly what we want. We actually think you can take the same ideas and apply them
to CPUs and GPUs as well. There's some additional tricks that are involved, but we're making good progress
on that. So we think, you know, sort of stepping back a little bit, we think if you can be really
smart about where you place the zeros and what computations devoid, we think there's a potential
of making deep learning orders of magnitude more power efficient and more compute efficient
than it is today. And if you think about kind of the carbon impact of deep learning today,
which is just insane, you know, it sort of behooves us as an industry to really pay attention to
this and really improve the energy usage of our deep learning systems. It's kind of completely
out of control today. And you're saying deep learning and just to be absolutely clear, you're talking
about conventional deep learning for lack of a better term, as opposed to what we were talking
about earlier, cortical columns, different neuron models, things that we're working on in the
research. This is applying sparsity and optimization to, you know, drive greater efficiency and
something that's roughly akin to what we're doing today. Is that fair? Yes, yes. The sparsity
stuff could be applicable in today's deep learning networks. It seems it's becoming more and more
clear that these sparse techniques can be applied to basically all of the network architectures
that are out there today, whether it's, you know, convolutional systems, transformers,
conformers, you know, ResNet, you know, all the different architectures that are common today
can benefit from sparsity. Having said that, the cortical column
implementations when we get there will also be sparse because the brain is sparse and then
sparsity gives you, you know, tremendous benefits. But the good thing about the sparsity work is
it can be actually applied today to today's deep learning systems. And is sparsity,
is it an emergent property of the architecture or is it, you know, the use case or the data?
Is it something that you can, you know, always apply and it has some benefit because it's
broader is it only if your data or your problem looks a certain way? You can apply to almost any
problem domain. Where the thing you do need is that you need networks that are larger.
It's sort of a little bit paradoxical thing about, but you need larger networks in order to
allow really sparse processing. And what happens mathematically is as you get larger networks,
you get these exponentially larger spaces that you're working with. And it's so it's easier and
easier to find solutions that can be extremely sparse in there. So even if you look at the total
number of non-zero weights, if you have a small network, maybe you can get to, let's say,
you know, a thousand non-zero weights at a layer, just to pick a number. If you have a larger
network, you might actually be able to get to 500 or 100 non-zero weights. So it's it's it's
counterintuitive, but by making the dimensionality's bigger, you can actually get by with smaller
absolute number of weights. Yeah. Is that a property of Bayesian optimization in particular or
something else that this is just a space? Yeah, this is just in the mathematics of sparsity.
The way it works is that as the spaces get larger, you can get the same amount of information with
a smaller number of weights. So it's an information theoretic result there. Now from a, I mean,
obviously from a Bayesian parameter optimization standpoint, it just makes it even harder to
find all these hyperparameters and create networks that can train and that's so.
You mentioned in the application of the Bayesian optimization stuff that you were doing,
you know, we talked about the multimetric nature of it. You also says some things that
sounded like kind of constraint, you know, constrained optimization, which I know SIGUP does as
well. Is that something that you're using as part of the formulation? Yeah, yeah. So with the
multimetric, we can optimize multiple quantities simultaneously, like sparsity and error and so on,
with the constraints, we are using constraints. So with SIGUP, you can put all of these constraints
in your parameters. So we make sure that we stay within ranges that we know will work well.
So for example, constraints that we know are imposed by the hardware.
Or there's combinations of hyperparameters that just don't make sense.
And so we want to put those constraints so that the hyperparameter optimization doesn't
visit parts of the hyperparameter space. That's a mouthful. That just don't make sense.
Yeah, I mean, with hyperparameter optimization, every point in the space is an entire training run.
And so you have to be judicious in how you, you know, picking those points efficiently.
And so the constraints, any constraints you can put in that you know will just reduce the
space of possibilities and make the entire process a lot more efficient.
And you mentioned transformers a couple of times. Have you applied any of the sparsity techniques
that we're talking about to transformers and language models and, you know, some of the things
that are potentially causing, you know, the large environmental impact that you alluded to?
Yeah, yeah. It's exactly because of that that we are actually spending quite a bit of time
on transformers. They're becoming very popular. But at the same time, these are just massive
models that are taking up consuming huge amounts of power and, you know, creating a pretty big
negative environmental impact today. We've been, we've having quite a bit of success in sparsifying
transformers. Because these models are larger, it's, it's, you know, it's possible to get
quite sparse with that. So we've been able to get transformer models. We've worked with the
BERT model, which is kind of the canonical transformer model that everyone uses as kind of a template.
We've been able to get to 90% sparse and even higher without losing accuracy in these kind of
models. So there's a potential of really accelerating these transformer models and making them much,
much more power efficient. So we published a blog post on that recently and we should be
having, you know, a lot more on that in the, in the few coming few months. We'll definitely
include a link to that in the show notes pages. Is there a rule of thumb that 90% sparse
translates to, you know, 50% power or something like that? We think with the right hardware,
you know, optimizations, 90% sparse should give you more than a 10x gain. So it's not, we're not
talking about 50%, 20%, 30%, but we're talking about orders of magnitude here. And so with 90%,
you can skip 9 out of 10 computations, but your model also gets really smaller. So you get additional
advantages over that. So if you, in hardware, you know, you have, you know, memory hierarchies,
the some memories are faster than others. And a smaller model means most of your model can fit
in these faster memory areas. And, you know, memory contention gets much smaller. So there's
a bunch of other practical things that come in. So, you know, we eventually think that we can get
more than a 10x for that. The brain is 95 to 98% sparse. So if we can get anywhere close to the
levels of the brain, we're talking, and again, remember, there's multiple types of sparsities
that, that interact and have multiplicative benefits. So we're talking multiple orders of magnitude
efficiency gains if we can really get close to how it is in the brain. And, and, you know, I think
a lot of deep learning researchers used to think that was impossible, but I think the brain shows
that it's not only is it possible, it's actually the best example we have today.
All right. I'm curious about you. We've been talking about training slash learning. What about the
inference side of things with regards to sparsity? Yeah. So, but both inference and training will
speed up with sparsity. We focused actually a little more on the inference side because that's the
more kind of common use case, but training will speed up too. But the fundamental mathematical
operations that occur during, during inference and training, they're very similar. And so they
should both benefit from this. Awesome. Awesome. But we've covered a ton of ground. What are you
most excited about over, you know, say the next 10 months or some other arbitrary time horizon?
Yeah. What's, you know, there's this big question in the machine learning community, like
can brain inspired understandings actually impact machine learning positively. And I think I'm
really excited that the stuff that we've mentioned is now coming into real practical domain. And
with sparsity, I think 10 months from now or year from now, we'll see really dramatic improvements in
what we can do from an efficiency standpoint. And I'm really hopeful in that time frame that we
will be able to take many more of the ideas of the cortical column and start showing really
concrete hard core benefits to machine learning systems. And to really showcase that we can take,
understanding the brain is not just an academic scientific exercise. It can actually have practical
engineering benefits. And to me, that's what I'm really passionate about as a computer scientist.
And hopefully, you know, year from now will be in a dramatically better space with respect to all
that than we are now. Fantastic. Fantastic. Well, Subatai, thanks so much for taking the time to
share a bit about what you're working on. Very cool stuff. Yeah, thank you so much, Sam. It was
a pleasure talking in there. There's a great question. And I'm glad we got it. Thank you
going to a lot of the detail on that. Absolutely. Thank you.
