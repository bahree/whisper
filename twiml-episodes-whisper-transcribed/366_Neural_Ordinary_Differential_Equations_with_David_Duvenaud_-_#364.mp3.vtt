WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimal AI Podcast.

00:13.400 --> 00:18.960
I'm your host, Sam Charrington.

00:18.960 --> 00:26.840
Alright everyone, I am on the line with David Duveno, David is an assistant professor

00:26.840 --> 00:29.560
at the University of Toronto.

00:29.560 --> 00:34.840
David, welcome back to the Twimal AI Podcast, thank you Sam, it's nice to be back.

00:34.840 --> 00:38.400
It is great to catch up with you, I'm really looking forward to this.

00:38.400 --> 00:44.880
You have been super busy since the last time we spoke, which was back in January of 2018.

00:44.880 --> 00:49.440
So just about two years ago, or at least that's when we published the show, we might have

00:49.440 --> 00:54.520
actually caught up a little bit before then, but that show was on composing graphical

00:54.520 --> 01:01.440
models with neural networks and you've been quite prolific since then and we will hopefully

01:01.440 --> 01:04.560
get a chance to talk about a bunch of what you've been up to.

01:04.560 --> 01:10.360
Our fur folks back to that show for your full background and how you got started in machine

01:10.360 --> 01:16.560
learning, but why don't we start out by having you share a little bit about your current

01:16.560 --> 01:17.560
research interests.

01:17.560 --> 01:22.680
So yeah, obviously one big thing that's happened since two years ago is that we started

01:22.680 --> 01:25.480
working on differential equations a lot.

01:25.480 --> 01:29.960
And we had the first paper, you know, at the neural ODE's paper and we've just actually

01:29.960 --> 01:30.960
released it.

01:30.960 --> 01:33.600
And I'll just interrupt you to say that was a huge paper last year.

01:33.600 --> 01:40.320
Yeah, it was, yeah, it was a, this was Neurf's 2018 where that paper was presented and

01:40.320 --> 01:41.320
you've been on my list.

01:41.320 --> 01:46.800
We actually did try to catch up around that time, but you were, you were super busy and

01:46.800 --> 01:48.400
we'll talk about it this time.

01:48.400 --> 01:49.880
Okay, great, great, great.

01:49.880 --> 01:51.640
There's a lot of, I think, pretty interesting follow ups.

01:51.640 --> 01:57.600
I'm trying to not let that take over all the research that happens here, or at least

01:57.600 --> 02:01.800
among my students, but we just published a follow up which I'm really happy about where

02:01.800 --> 02:07.440
we figured out how to train stochastic differential equations in a scalable way.

02:07.440 --> 02:11.480
And that was actually really surprising that it hadn't been worked up before.

02:11.480 --> 02:14.480
It was one of these things where I thought, oh, you know, when we looked at ordinary differential

02:14.480 --> 02:19.920
equations, the basic math for how to efficiently do backprop through them was already no one

02:19.920 --> 02:21.240
in the numeric community.

02:21.240 --> 02:24.760
So I assumed that, you know, SDEs have been around almost as long and it would have been

02:24.760 --> 02:25.760
worked out.

02:25.760 --> 02:27.040
But actually, it hadn't.

02:27.040 --> 02:34.360
And there was sort of a few, there was things called like backwards SDEs and a few other

02:34.360 --> 02:39.800
approaches for trying to build the same sorts of algorithms for doing a grading based training

02:39.800 --> 02:40.800
of SDEs.

02:40.800 --> 02:41.800
But none of them were scalable.

02:41.800 --> 02:45.320
And I think it's one of these things where the differential equations community typically

02:45.320 --> 02:48.840
hasn't been focused on computational efficiency.

02:48.840 --> 02:52.680
So the idea is that, you know, showing that there exists these dynamics is, that had

02:52.680 --> 02:55.960
been worked out with the dynamics where, but how to do these things efficiently hadn't

02:55.960 --> 02:56.960
been.

02:56.960 --> 03:00.920
So I teamed up with a probableist here at University Toronto, Leonard Wong, an amazing

03:00.920 --> 03:04.120
undergrad who's now a Google Brain resident, Joy Chen Lee.

03:04.120 --> 03:08.200
And we worked out all the details and, you know, they're the ones who really did the mathematical

03:08.200 --> 03:09.680
heavily lifting.

03:09.680 --> 03:13.840
And I just sort of convinced them that there had to exist a simple algorithm or rather

03:13.840 --> 03:17.840
an efficient algorithm because for everything else that people have looked at, there's always

03:17.840 --> 03:18.840
one.

03:18.840 --> 03:23.720
Backpropagation always has the same asymptotic time complexity as the forward pass.

03:23.720 --> 03:27.760
And this was one area where people sort of thought, oh, maybe it's not the case that there

03:27.760 --> 03:31.640
is such a efficient reverse algorithm and we eventually worked out what it was.

03:31.640 --> 03:33.160
So I'm really happy about that.

03:33.160 --> 03:39.920
I feel like we're just diving into the neural ODE stuff and we'll circle back to maybe

03:39.920 --> 03:45.400
how all of the different things your lab works on kind of connect together.

03:45.400 --> 03:50.520
But for the neural ODE stuff, let's just start from the beginning.

03:50.520 --> 03:54.920
I am sure there are folks that are listening that don't really understand what a differential

03:54.920 --> 03:55.920
equation is.

03:55.920 --> 03:56.920
So.

03:56.920 --> 03:57.920
Yeah.

03:57.920 --> 03:59.760
And the only thing is that I actually never learned these in undergrad.

03:59.760 --> 04:02.560
Like I never took one of these courses on ODE's.

04:02.560 --> 04:03.560
Oh, really?

04:03.560 --> 04:04.560
Yeah.

04:04.560 --> 04:07.520
I just kind of picked it up, you know, from talking to people who knew about them and

04:07.520 --> 04:08.520
reading about them.

04:08.520 --> 04:09.520
It's almost good.

04:09.520 --> 04:14.480
So if you take a course on ODE's, at least the ones that I've looked at, most of the material

04:14.480 --> 04:18.320
is based around solving them exactly for special cases.

04:18.320 --> 04:24.080
So if you have like linear ODE or some sort of structure in a second order ODE, there

04:24.080 --> 04:27.400
is this special case is where the answer is like sign or cosine or X or something like

04:27.400 --> 04:28.400
that.

04:28.400 --> 04:34.200
And most of I think most often, at least when I encountered them in undergrad, it was

04:34.200 --> 04:42.360
the context that were was provided was typically like physical systems, like physics or, you

04:42.360 --> 04:47.000
know, the relationships between things in the physical world or, you know, often governed

04:47.000 --> 04:50.200
by these differential equations.

04:50.200 --> 04:51.200
Yeah.

04:51.200 --> 04:52.200
Yeah.

04:52.200 --> 04:53.200
That's where they typically come from.

04:53.200 --> 04:56.880
And that's actually another big difference is that the numeric community is used to looking

04:56.880 --> 04:59.200
at differential equations that are given by nature.

04:59.200 --> 05:01.960
And they have to work out how am I going to solve these equations?

05:01.960 --> 05:03.320
I can't choose which ones I want to solve.

05:03.320 --> 05:06.640
I have to see all the ones that are there.

05:06.640 --> 05:11.320
And one thing I want to kind of talk about a bit later is that when we use neural networks

05:11.320 --> 05:14.880
to specify these differential equations, it's actually a pretty different game because

05:14.880 --> 05:20.080
we think that there's many different sets of dynamics that will encode the same or

05:20.080 --> 05:22.640
that will roughly solve the problem.

05:22.640 --> 05:28.880
So we can maybe try to choose dynamics that are easy to solve that give almost the same

05:28.880 --> 05:33.200
answer as like the very best dynamics that might be really hard to solve.

05:33.200 --> 05:38.120
How was frame out the problem that you're trying to solve with this line of work?

05:38.120 --> 05:41.960
Is it solving a differential equation?

05:41.960 --> 05:46.960
Like you have a given differential equation and you're trying to, what exactly are you

05:46.960 --> 05:47.960
trying to do?

05:47.960 --> 05:48.960
Sure.

05:48.960 --> 05:50.320
So that's a great question because there is some work on trying to use neural networks

05:50.320 --> 05:53.160
to solve differential equations.

05:53.160 --> 05:54.800
And we're not really doing that.

05:54.800 --> 05:57.960
Although I do want to mention that one place where I did learn a lot about these was actually

05:57.960 --> 06:02.760
when I visited Philip Henning, who's an amazing researcher at the Max Planck Institute.

06:02.760 --> 06:07.200
When I was a PhD student, I did an internship with him in Germany for one summer and we worked

06:07.200 --> 06:11.120
on a project where we were trying to, well, where he had worked out this correspondence

06:11.120 --> 06:19.040
between the standard ODE solvers called like Runga Kuda methods and probabilistic models

06:19.040 --> 06:23.280
that like, well, the ocean processes that would extrapolate what these functions would

06:23.280 --> 06:28.280
do in the future given observations about their gradients in the present.

06:28.280 --> 06:32.560
So basically people had come up with Runga Kuda algorithms by asking how could we extrapolate

06:32.560 --> 06:36.520
in such a way that all these errors cancel out and then he realized that actually you

06:36.520 --> 06:42.160
can drive these automatically by just putting a Gaussian process prior on these functions.

06:42.160 --> 06:46.400
And then if you ask what the predictive posterior looks like, it actually gives you these

06:46.400 --> 06:48.160
Runga Kuda algorithms for free.

06:48.160 --> 06:51.920
Anyway, but the point is that all this work on neural adhesives is not actually focused

06:51.920 --> 06:53.560
on building better solvers.

06:53.560 --> 06:57.560
We're saying let's inherit all these amazing solvers that the numeric community has built

06:57.560 --> 07:04.280
and just try to repurpose them to train even bigger models than people normally have before.

07:04.280 --> 07:09.480
And from a technical point of view, all the tricks in the neural adhes paper were already

07:09.480 --> 07:13.720
existed in the numeric literature, we just put them all together in one place.

07:13.720 --> 07:20.840
And so when you say train even bigger models, meaning you're trying to come up with algorithms

07:20.840 --> 07:26.040
that are either alternatives to gradient descent or enhancements to gradient descent

07:26.040 --> 07:32.960
that facilitate converging on different weights for neural networks that the networks themselves

07:32.960 --> 07:36.920
are solving arbitrary problems that don't necessarily have to do with differential

07:36.920 --> 07:37.920
equations.

07:37.920 --> 07:39.040
Oh, that's a great question.

07:39.040 --> 07:46.720
So what I mean specifically is let's find efficient ways to compute gradients of predictive

07:46.720 --> 07:51.880
loss or some sort of training lots with respect to all the parameters of some differential equations.

07:51.880 --> 07:56.760
And then use standard training algorithms like Adam or whatever in the standard ways on

07:56.760 --> 07:58.240
the standard losses.

07:58.240 --> 08:04.960
And so when we think of a gradient, the way that that's typically described is it's a slope

08:04.960 --> 08:11.240
which is a differential equation in a sense.

08:11.240 --> 08:16.880
And so you're applying these methods to identify these gradients more quickly.

08:16.880 --> 08:17.880
Well, yeah.

08:17.880 --> 08:22.120
So it's pretty good for me to be specifying because almost every question you ask about

08:22.120 --> 08:26.000
could we use ODE's to compute gradients or gradients to solve ODE's like the answer is

08:26.000 --> 08:27.000
always yes.

08:27.000 --> 08:30.120
So there's a lot of different ways that these tools combine and different people are working

08:30.120 --> 08:31.840
on different aspects of the problem.

08:31.840 --> 08:39.520
So to be precise, we're saying people have often like parameterized differential equations

08:39.520 --> 08:44.120
based on some few parameters and like simple functions to maybe specify like, you know,

08:44.120 --> 08:47.200
how planets evolve or how chemical concentrations change.

08:47.200 --> 08:51.320
And then they call ODE solvers to run these forward and find what the trajectories of these

08:51.320 --> 08:53.120
systems look like.

08:53.120 --> 08:58.920
And sometimes they want to fit those systems to data, which requires computing the gradient

08:58.920 --> 09:04.200
of the training loss like the mismatch between the predictions of your model and the data

09:04.200 --> 09:08.200
back through these ODE solutions to the parameters that specify them.

09:08.200 --> 09:12.080
So those were the in the neural ODE paper, we basically imported all the tricks from

09:12.080 --> 09:16.480
the numerical community into one algorithm and said, oh, this is a scalable way to compute

09:16.480 --> 09:23.080
gradients where we can use the like fanciest ODE solvers that also the numeric community

09:23.080 --> 09:24.080
developed.

09:24.080 --> 09:28.360
So it was really, it was really kind of just showcasing a bunch of things that the numeric

09:28.360 --> 09:32.520
community knew and putting them all together in such a way that it would scale to very

09:32.520 --> 09:33.520
large systems.

09:33.520 --> 09:34.520
Got it.

09:34.520 --> 09:40.240
And so what I heard you just say was that you kind of people have in the numeric community

09:40.240 --> 09:47.640
have, you know, long studied, you know, how to kind of do the, you know, forward projection

09:47.640 --> 09:52.560
of differential equations to trajectories of physical things.

09:52.560 --> 10:00.000
And then they want to do kind of the backwards reconciliation so that they can determine how

10:00.000 --> 10:02.680
accurate their predictions are.

10:02.680 --> 10:08.920
And there were a bunch of different techniques or are a bunch of different techniques for

10:08.920 --> 10:10.320
doing that.

10:10.320 --> 10:14.680
And what's the relationship between, you know, all of that and neural networks?

10:14.680 --> 10:19.400
Did you then pull those into like a deep learning or neural network framework or doing this

10:19.400 --> 10:21.040
backward gradient calculation?

10:21.040 --> 10:22.040
Yeah.

10:22.040 --> 10:23.880
So there's a few different ways we can use these tools.

10:23.880 --> 10:26.560
And one of them is to fit these physical systems that people have been doing.

10:26.560 --> 10:31.760
But what was I think most exciting to the whole deep learning community was to say, oh,

10:31.760 --> 10:38.680
there's also a potential that this sort of network, this sort of ODE network could replace

10:38.680 --> 10:42.720
some of the backbone of the neural networks that we used to train today.

10:42.720 --> 10:47.120
So in particular, residual networks is like the standard way you build a very deep network.

10:47.120 --> 10:52.920
And it's just adding together the contributions of a whole bunch of small neural network

10:52.920 --> 10:54.320
layers.

10:54.320 --> 10:57.960
And so the connection that we talked about in the neural ODE paper, which had been made

10:57.960 --> 11:05.760
before, was, oh, well, if you ask, if you look at how one of these ODE solvers solves

11:05.760 --> 11:10.200
or like confused one of these long trajectories, it also adds up many different calls to these

11:10.200 --> 11:13.800
smaller functions, the only thing that we really did knew was to sort of take this really

11:13.800 --> 11:19.560
seriously and say, OK, let's actually then use ODE solvers to solve or to compute the

11:19.560 --> 11:20.920
answer of our neural network.

11:20.920 --> 11:25.640
And then it can decide, you know, how many function evaluations to make and where.

11:25.640 --> 11:28.360
And yeah, so that was the new part.

11:28.360 --> 11:31.400
And so this has a few advantages.

11:31.400 --> 11:34.880
It's kind of a different way of formulating the problem, instead of saying, here's the

11:34.880 --> 11:38.880
algorithm for computing my residual network, which is like, you know, train together 100

11:38.880 --> 11:39.880
times.

11:39.880 --> 11:45.880
We say, here's the dynamics of this trajectory, ODE solver, it's your job to figure out how

11:45.880 --> 11:50.840
many times and we need to evaluate this function and where to tell me what the exact or to approximate

11:50.840 --> 11:52.640
what the exact trajectory would be.

11:52.640 --> 11:57.800
So the cool thing is that if the problem is easy, it might only need a few calls to the

11:57.800 --> 12:00.040
function and if it's hard, it might need a lot.

12:00.040 --> 12:04.200
But this is something that's sort of being determined adaptively on the fly, instead

12:04.200 --> 12:08.160
of at training time, where normally right now people have to just sort of try different

12:08.160 --> 12:09.160
steps.

12:09.160 --> 12:11.840
They say, oh, I tried into their neural network with, you know, 10 layers.

12:11.840 --> 12:13.320
It didn't do as good as one with 20.

12:13.320 --> 12:16.880
I, you know, it started doing better and better than where I added, but then everything

12:16.880 --> 12:17.880
was too expensive.

12:17.880 --> 12:20.440
So I had to cap it.

12:20.440 --> 12:25.680
The hope is that we can now say, oh, let's at training time, let the, like, just tell

12:25.680 --> 12:30.240
our optimizer, here's my tradeoff between accuracy and speed.

12:30.240 --> 12:34.000
It's up to you to figure out how to, like, trade these things off.

12:34.000 --> 12:37.720
So that was something we sort of said you might be able to do, or rather this idea of at

12:37.720 --> 12:41.600
training time, trading off accuracy and speed is something that we, you know, thought we

12:41.600 --> 12:44.000
could do, but we didn't really work out how to do it.

12:44.000 --> 12:48.520
And, but this is one of the papers that it was up late last night working on for somebody

12:48.520 --> 12:54.560
to ICML was me and my, since then, me and my student, Jesse Pettincourt also working with

12:54.560 --> 13:01.280
an undergrad, Jacob Kelly and my friend at Google, Matt Johnson, we were saying, oh, well,

13:01.280 --> 13:07.000
maybe we can add some sort of term, some sort of regularizing term to the loss that makes

13:07.000 --> 13:09.120
the dynamics easy to solve.

13:09.120 --> 13:10.120
And so we got that to work.

13:10.120 --> 13:13.640
And now we can see, oh, there's this tradeoff that we can explore between having, having

13:13.640 --> 13:19.920
training a ODE network that exactly minimizes our training loss versus one that is cheap

13:19.920 --> 13:22.880
to solve and sort of requires fewer layers.

13:22.880 --> 13:26.760
And so now, you know, if depending on your compute budget, you can just move along this

13:26.760 --> 13:29.600
pre-do front and trade these things off however you want.

13:29.600 --> 13:30.600
Nice.

13:30.600 --> 13:31.600
Nice.

13:31.600 --> 13:33.760
And that was the neural networks with, no, that's not the neural networks with cheap,

13:33.760 --> 13:34.760
this is unpublished stuff.

13:34.760 --> 13:43.000
This is an unpublished paper that was just submitted February 7th in the, we hours

13:43.000 --> 13:46.960
of the morning for the upcoming ICML conference.

13:46.960 --> 13:47.960
Exactly.

13:47.960 --> 13:50.760
It doesn't get any harder off the press than that, yeah.

13:50.760 --> 13:51.760
Nice.

13:51.760 --> 13:54.240
And is it already up on archive or?

13:54.240 --> 13:55.240
No, it's not an archive.

13:55.240 --> 14:01.280
Although Jesse gave a talk about it at the program transformation workshop at NRIPS, although

14:01.280 --> 14:04.240
it wasn't working at that point, we were just sort of saying, here's the math that we're

14:04.240 --> 14:05.560
going to try to implement.

14:05.560 --> 14:09.160
And so is this related to the cheap differential operator's paper?

14:09.160 --> 14:10.160
Sort of.

14:10.160 --> 14:13.440
So the cheap differential operator's paper, again, that was actually Ricky's idea.

14:13.440 --> 14:18.200
He just sort of came to me and said, hey, I think we can actually constrain the dynamics

14:18.200 --> 14:23.680
of our neural networks such that these quantities that we need to compute are cheap.

14:23.680 --> 14:28.800
So the motivation from that was there was a follow up to the neural ODE's paper called

14:28.800 --> 14:29.800
Fjord.

14:29.800 --> 14:34.520
Or the name of the method was Fjord, and it basically said, we can build normalizing

14:34.520 --> 14:40.720
flows out of a continuous time using ordinary differential equations.

14:40.720 --> 14:45.680
So normalizing flows are a family of density estimators, which just means, you know, a generic

14:45.680 --> 14:51.040
way to model any data in an unsupervised way that work by taking a simple density like

14:51.040 --> 14:57.800
a Gaussian and somehow warping it into some non-Gaussian complicated parametric density.

14:57.800 --> 15:03.240
And so one of the nice follow-ups from the neural ODE's stuff, we're saying, oh, it turns

15:03.240 --> 15:09.240
out that if you think of this transformation happening continuously, then the math that

15:09.240 --> 15:13.080
you need to compute the change in density is a little bit nicer.

15:13.080 --> 15:17.760
So it goes from having to compute the determinant of the Jacobian of the dynamics to just the

15:17.760 --> 15:22.360
trace of the Jacobian of the dynamics, and that's actually a lot easier to approximate.

15:22.360 --> 15:27.560
So it's still expensive, though, and it's still kind of a downside of this method.

15:27.560 --> 15:33.760
So Ricky worked out that actually if we constrain the architecture of these dynamics networks,

15:33.760 --> 15:38.400
we can give them exact trace, we can compute their traces exactly.

15:38.400 --> 15:44.520
And this is, I kind of like it, so I want to talk about engineering neural network architectures.

15:44.520 --> 15:48.720
So there's a lot of work that has been totally foundational to the field where people sort

15:48.720 --> 15:51.560
of do trial and error and they say, oh, what if I add more layers here?

15:51.560 --> 15:56.520
Or if I change the non-linearities of my network, what if I add noise here or there?

15:56.520 --> 16:00.280
And sometimes these are well-motivated theoretically, sometimes or not, sometimes it's just sort

16:00.280 --> 16:02.960
of the trial and error that we need to get any technology to work.

16:02.960 --> 16:03.960
Right.

16:03.960 --> 16:09.640
I think in the first year of my podcast, there was this period where many of my conversations

16:09.640 --> 16:13.040
were asking, okay, how is this done?

16:13.040 --> 16:14.040
How are people doing?

16:14.040 --> 16:19.040
And the answer that I finally came to understand was that it was just trial and error, and

16:19.040 --> 16:25.440
graduate student descent was the term thrown around, and a lot of the great models that

16:25.440 --> 16:32.440
we use to solve problems then and now, like came out of this just iterative exploratory

16:32.440 --> 16:33.440
process.

16:33.440 --> 16:34.440
Yeah.

16:34.440 --> 16:36.000
And there's nothing wrong with that.

16:36.000 --> 16:40.440
But it's more satisfying when we can say, oh, I want to have a network that's going

16:40.440 --> 16:41.920
to learn functions with this property.

16:41.920 --> 16:46.840
Therefore, I know that I need that my network has this architecture or that, or saying,

16:46.840 --> 16:50.440
if it has architecture, it'll definitely be able to learn functions that enforce this

16:50.440 --> 16:53.240
property, like maybe some sort of invariance.

16:53.240 --> 16:57.800
So there was some really nice work on this called deep sets, which said, what if I want

16:57.800 --> 17:01.480
to have a network that takes in a set of things and gives me an answer that doesn't depend

17:01.480 --> 17:04.920
on the order of the things in that set, because that's sort of what makes it a set is that

17:04.920 --> 17:05.920
the order shouldn't matter.

17:05.920 --> 17:09.600
But on a computer, you do have to give things in a particular order.

17:09.600 --> 17:13.600
So sometimes people just budget and they randomize the order, but these guys worked out the

17:13.600 --> 17:18.880
math to say, oh, here's the family of architectures that will always be invariant to the order

17:18.880 --> 17:20.560
of things in a set.

17:20.560 --> 17:22.280
So I really liked that work.

17:22.280 --> 17:26.200
And then I liked Ricky's idea because it was the same sort of thing saying, okay, well,

17:26.200 --> 17:28.160
we know we want our networks to have this property.

17:28.160 --> 17:30.480
Here is the general way.

17:30.480 --> 17:34.200
Here's the general trade-off where we can say we have to make this sacrifice, but then

17:34.200 --> 17:35.880
we will achieve this nice property.

17:35.880 --> 17:41.440
And we can actually interpolate between networks that are, let's say, very restricted, but

17:41.440 --> 17:46.560
we give you exact traces or ones that are less restricted, but then you have to approximate

17:46.560 --> 17:48.400
the trace.

17:48.400 --> 17:58.040
And the trace corresponds only to the cost of determining the weight source, the trace,

17:58.040 --> 18:01.400
more fundamental or have broader implications.

18:01.400 --> 18:06.600
Yeah, which we had a way forward, but when I say trace, I mean, the sum of the diagonal

18:06.600 --> 18:13.600
terms in the Jacobian of these networks, and the Jacobian is just the matrix that says,

18:13.600 --> 18:17.280
what is the gradient of all the outputs of the network with respect to all of its inputs?

18:17.280 --> 18:18.280
Yeah.

18:18.280 --> 18:24.280
So it's just one quantity that we sometimes need to evaluate, but it's actually quite expensive

18:24.280 --> 18:27.680
to evaluate it exactly from first standard neural networks.

18:27.680 --> 18:38.640
So you're able to, by fixing the network architecture, fixing the trace to be easier to compute,

18:38.640 --> 18:42.520
does, is characterizing the trace in that way?

18:42.520 --> 18:49.960
Is it just an issue of computational complexity, or does the trace have other implications

18:49.960 --> 18:54.560
on the network or its performance or its characteristics?

18:54.560 --> 18:58.480
It's an issue of computational complexity.

18:58.480 --> 19:01.360
Yeah, that's a great question.

19:01.360 --> 19:06.320
So I think one very valid question that people asked is basically saying, okay, how does

19:06.320 --> 19:10.280
the trade-off look empirically like where along this curve should I go?

19:10.280 --> 19:12.920
What does this restriction actually mean in practice?

19:12.920 --> 19:17.920
And in this initial paper, all we basically did was lay out the trick and show that it

19:17.920 --> 19:20.640
worked in a bunch of settings.

19:20.640 --> 19:22.200
But I think that's a great question.

19:22.200 --> 19:29.200
We know that this restriction hurts the expressive capacity of these networks, but we haven't

19:29.200 --> 19:31.360
characterized them exactly what way.

19:31.360 --> 19:34.280
So that's a great question, and I wish I knew the answer.

19:34.280 --> 19:35.280
All right, cool.

19:35.280 --> 19:43.560
And then another paper that you presented at or that your team worked on at this last

19:43.560 --> 19:52.280
nerve was the latent ODEs for a regularly sampled time series, how does that one tie into this

19:52.280 --> 19:53.280
body of work?

19:53.280 --> 19:57.000
Well, that one was kind of satisfying because the original motivation for looking at ODEs

19:57.000 --> 20:01.960
in the first place was Yulia, who's the first author of that paper and the, well, second

20:01.960 --> 20:06.880
but co-first author of the neural ODE's paper was working on some medical applications

20:06.880 --> 20:14.000
where we had some gene assays of people's tumors that were evaluated at like a week apart

20:14.000 --> 20:19.240
and then a month apart and then maybe another week apart and then a year apart.

20:19.240 --> 20:23.440
And it's not quite clear how to fit that sort of data into a standard recurrent neural

20:23.440 --> 20:24.920
network or something like that.

20:24.920 --> 20:28.160
So that made us look at these continuous time models in the first place.

20:28.160 --> 20:31.520
But then we wrote the neural ODEs paper, which didn't, it just had proof of concept.

20:31.520 --> 20:35.280
It just said, oh, here's something you can do, we look, we explore it on toy data.

20:35.280 --> 20:41.360
So I think academics, including myself, have a bad habit of taking an applied problem,

20:41.360 --> 20:44.720
saying, oh, if we did solve this theoretical thing, we could tackle this applied problem,

20:44.720 --> 20:48.080
write a paper about solving the theoretical thing and never go back to the problem.

20:48.080 --> 20:52.880
So we still haven't applied it to the original data set that Yulia was looking at, but

20:52.880 --> 20:57.280
we did apply it in that paper to a standard medical records data set where it's like people

20:57.280 --> 21:01.040
in the intensive care unit and there's also different measurements being made of them

21:01.040 --> 21:03.520
from different people at different times, like what is their temperature or their blood

21:03.520 --> 21:08.720
pressure or whatever, and being able to combine this all into one model is something that's

21:08.720 --> 21:11.840
not very natural for the discrete time models we normally use.

21:11.840 --> 21:15.840
So we sort of showcase that, yeah, once you have continuous time, here's a set of architectures

21:15.840 --> 21:20.800
you can explore in the advantages and yeah, it worked, it was satisfying paper to write.

21:20.800 --> 21:30.000
What is the role that this continuous time versus these regularly sample time series played

21:30.000 --> 21:36.880
in the original paper, was it a big motivator or just a side note, given this machinery,

21:36.880 --> 21:40.640
we can probably tackle this continuous time problem differently.

21:40.640 --> 21:46.320
Yeah, so it was the original thing that maybe started to revisit these models,

21:46.320 --> 21:49.680
but then in the paper it ended up being secondary.

21:49.680 --> 21:52.560
I think in most people's eyes who are just interested in supervised learning,

21:52.560 --> 21:56.560
right, like the bread and butter of the machine learning community is like I want to train a

21:56.560 --> 22:01.200
giant classifier or something, and so we put that front and center because we knew that would

22:01.200 --> 22:08.240
be a lot of broader interest, but the thing is that until we make noise faster or at least

22:08.240 --> 22:14.000
as fast as standard architectures, I don't think people are going to, I don't think people should

22:14.000 --> 22:19.520
use them, and so that's why that motivated the work with Jesse on regularizing them,

22:19.520 --> 22:20.800
regularizing them to be fast.

22:20.800 --> 22:24.400
We're still at the proof of concept stage there, we just got to working in some standard

22:24.400 --> 22:30.560
endless sort of things, but right now I am really excited about the time series setting

22:30.560 --> 22:37.920
for two reasons, so one is that it's really, right now, one of the main areas where you definitely

22:37.920 --> 22:42.080
do need a continuous time model, or rather you definitely need differential equations, if you

22:42.080 --> 22:45.280
start talking about continuous time, you're basically already said you're using differential

22:45.280 --> 22:49.760
equations, like I don't want to go and shoehorn differential equations in where they don't

22:49.760 --> 22:55.680
actually make sense empirically or practically just because it's like a cool thing, and so the other

22:55.680 --> 22:57.120
thing is that I think the...

22:57.120 --> 23:04.480
What is it about continuous time models or problems that necessitates differential equations?

23:04.480 --> 23:09.920
Oh, well you need to be able to say how the system changes for any arbitrarily small amount of time,

23:09.920 --> 23:14.480
and so once you've done that, the only way to do that is to basically describe the derivatives

23:14.480 --> 23:22.400
of the state. Like maybe you don't like discrete time, the relationship is between something

23:22.400 --> 23:29.200
happening at time n and something happening at time n plus one, whereas with continuous time,

23:29.920 --> 23:37.760
you need a continuous function to relate things happening at different times, and if that's the case,

23:38.960 --> 23:40.880
you hope that it's differentiable.

23:40.880 --> 23:45.680
Yeah, or rather, if it is a continuous function that says how the thing changes, then that meets the

23:45.680 --> 23:48.880
definition of a differential equation as far as like my turn. Like maybe there could be some...

23:48.880 --> 23:49.440
Yeah, I got it.

23:49.440 --> 23:55.280
But, and the funny thing is that I think the business community and like the medical community haven't,

23:55.280 --> 24:00.400
I think kind of hilariously underserved by the machine learning community in the sense that almost

24:00.400 --> 24:07.120
all the data sets that they... Like when I talk to the sponsors of vector or people at like

24:07.120 --> 24:11.120
big companies, they say, okay, so my data looks like I have a bunch of interactions with my

24:11.120 --> 24:14.800
customer that happen over years, and they're irregularly sampled and they're different types of

24:14.800 --> 24:19.120
observations, and I want to be able to predict... I'm going to be able to model this data and deal

24:19.120 --> 24:22.400
with the fact that it's like all missing... or almost all missing almost all the time.

24:23.600 --> 24:28.880
How do I do this? And I say, well, I mean, I guess you could kind of shoehorn it into an R&N maybe,

24:28.880 --> 24:34.960
like vending data. There's some nice work on deep common filters by David Santag and some other

24:34.960 --> 24:41.840
people at NYU and MIT that said, okay, if you move things to discrete time, here's how to deal with

24:41.840 --> 24:48.000
missing data. But it's really just like the bread and butter of like most of industry doesn't,

24:48.560 --> 24:54.400
like their data sets just don't fit with what's coming out of most machine learning labs,

24:54.400 --> 24:58.800
who are more focused on things like video or audio or text, where you really can't say that

24:58.800 --> 25:02.880
there's an observation at every time step. Well, a lot of times don't we just throw away the

25:02.880 --> 25:09.840
sequential nature of the problem and just treat each individual sample as, you know, unrelated

25:09.840 --> 25:14.000
training data drawn from a distribution? Oh, yeah, that's one of the ways that you can force the

25:14.000 --> 25:18.720
data to match your model. And I guess I'll just see, you know... But the point being that you,

25:18.720 --> 25:24.560
yeah, if actually there's some sequence there, you're throwing away information. And what this

25:24.560 --> 25:30.720
is doing is proposing a way to take advantage of that information yet still be tractable. Yeah,

25:30.720 --> 25:36.320
exactly. We want to meet the data where it lives. I think in the future, like statisticians

25:36.320 --> 25:40.000
should take some sort of Hippocratic oath where they swear not to just like destroy data.

25:42.560 --> 25:47.440
I mean, I mean, you know, a lot of data is not particularly valuable, but the point is I see

25:47.440 --> 25:52.240
oftentimes, including, you know, in my own work, if we don't have the tools, then you just say,

25:52.240 --> 25:56.000
okay, well, we have to throw away a bunch of data and we're already crippling our ability to make

25:56.000 --> 26:02.560
good predictions when you do that step. So that's kind of how I view, like maybe not just my recent

26:02.560 --> 26:07.040
research, but how the field moves in general is how do we get closer and closer to the raw sensors

26:07.040 --> 26:11.680
and put more and more of the modeling problem into the hands of one giant model that's jointly

26:11.680 --> 26:15.280
looking at everything instead of a sequence of people who are each looking at their little piece

26:15.280 --> 26:20.000
and throwing away what they think isn't necessary. So we've talked about the cheap differential

26:20.000 --> 26:28.400
operator's paper. We've talked about the irregularly sample time series paper. There's a paper

26:28.400 --> 26:33.840
residual flows for invertible generative modeling. Is that the residual flows paper that we talked

26:33.840 --> 26:37.360
about or is that a different residual flows paper? Yes, that's confusing. So that's a different

26:37.360 --> 26:41.840
residual flows paper. And it's kind of funny because so we had this follow up to the neural

26:41.840 --> 26:48.640
ods paper called fjord, which was the continuous time version. And the cool thing about that was

26:48.640 --> 26:53.760
that it let you use totally unrestricted neural network architectures. So I was talking before

26:53.760 --> 26:57.600
about how sometimes you can restrict the architecture to allow you to have some nice property. So

26:57.600 --> 27:03.360
that's what the normalizing flows community did from about 2015 to 2019. There's like real NDP,

27:03.360 --> 27:08.800
slow, all these big models where they said, oh, if we restrict our architecture, we can compute the

27:08.800 --> 27:15.120
change in density cheaply. But then it's kind of hard to figure out how to make these restrictions

27:15.120 --> 27:18.720
without requiring a whole bunch of layers. And these models end up being very deep and

27:18.720 --> 27:23.040
very expensive train. And so we said, oh, if you go to continuous time, you can just use any

27:23.040 --> 27:29.120
network architecture. And it's fine. But then a couple years ago, Yorne Jacobson and Jens

27:29.120 --> 27:35.600
Bareman came to the vector institute. And we're thinking about the same problems. And they said,

27:35.600 --> 27:41.680
okay, well, fjord is great. But you know, people don't like to have an odsolver inside of their

27:41.680 --> 27:45.600
model. And I think that's the reason of all thing to not want. Because now you have to fiddle,

27:45.600 --> 27:50.000
you have to worry about some numeric issues. You have to choose an error tolerance.

27:50.880 --> 27:56.000
There are similar issues with like floating point, but not as bad. Anyway, and they worked out,

27:56.000 --> 28:01.600
they said, okay, well, what if we use the same math, but for discrete time, could we come up with

28:02.320 --> 28:06.640
some version of fjord that actually used standard neural network architectures and fixed

28:06.640 --> 28:12.000
number of layers and the sort of standard way of setting things up that we that everyone's comfortable

28:12.000 --> 28:16.480
with, but inherited some of the nice mathematical properties. And then they did work it out and

28:17.200 --> 28:22.000
found and basically worked out over the course of these two papers. Another way to get an unbiased

28:22.000 --> 28:28.480
estimate of the change in density, but for finite time discrete flows. So it's kind of funny

28:28.480 --> 28:33.760
because it's like this detour into continuous time led to a better discrete time model.

28:33.760 --> 28:41.040
And that's the invertible generative modeling paper. Yeah, well, there's two. One of them,

28:41.040 --> 28:45.360
it was called the newer one was called residual flows. And then the first one was called invertible

28:45.360 --> 28:50.080
resonance. And maybe we were polluting the the namespace with all these like minor variations.

28:52.240 --> 28:58.560
I mean, Miss, that what is the review the invertible characteristic for me?

28:58.560 --> 29:01.040
Oh, well, so one thing. What is that saying?

29:01.040 --> 29:05.600
Yeah, so what that means is that if I have two different possible inputs to my network,

29:06.720 --> 29:13.040
they won't ever map to the same output. So if you want to use the change of variables formula,

29:13.040 --> 29:17.680
you need to make sure that you never take the original density and somehow like

29:18.480 --> 29:24.640
tear it or squish it into a point. All of these things cause sort of infinities in the in the

29:24.640 --> 29:31.120
likelihood. It makes me think of like a some kind of hash relationship. Is there anything interesting

29:31.120 --> 29:35.200
there? Yeah, maybe you could say we want to avoid we want to write a hash function that avoids

29:35.200 --> 29:39.200
collisions. I mean, the thing is we don't necessarily want to scramble our density.

29:39.920 --> 29:43.440
But yeah, we definitely do want to control when we lose information in these networks.

29:44.240 --> 29:47.760
Because if we ever lose this right information, then we can't use this this change of variable

29:47.760 --> 29:57.600
formula. And are you also saying that networks that are thus characterized if given a prediction,

29:57.600 --> 30:03.360
you can go back to the original inputs? Is it that invertible? Yeah, yeah. So it's the same,

30:03.360 --> 30:07.760
when we say an invertible function, it's the same like in one dimensional, like I say, for

30:07.760 --> 30:12.400
instance, f of x equals x. Okay, that's a silly example. Okay, f of x equals three times x square

30:12.400 --> 30:16.240
root and square, for example. Yeah, so square is not invertible. Well, that's not square

30:16.240 --> 30:21.840
invertible because of the signs. Right, right. But okay, square root is invertible on the positive

30:21.840 --> 30:26.960
reals square is not invertible because you can square the positive number or the negative number

30:26.960 --> 30:32.000
and get the same answer. Right, right. It's a little bit getting into the weeds, but basically

30:32.960 --> 30:37.920
the residual networks mean that we can use any architecture we want to train normalizing flows.

30:38.720 --> 30:43.600
And this is in contrast to the previous methods like glow and real MVP that had to restrict the

30:43.600 --> 30:51.600
architecture. And so the remaining of the four papers that you had at NURBS is efficient graph

30:51.600 --> 30:58.800
generation with graph for current attention networks. Yeah. Is that also related to this ODE thread

30:58.800 --> 31:03.360
or is that off on its own? No, and that was really, you know, one of the papers that,

31:04.160 --> 31:09.920
well, it was driven mainly by the first author, Renji Lau, who's amazing. And there's been a sort of

31:09.920 --> 31:15.200
race once people said, oh, you know, we could build generative models over graphs to try to make

31:15.200 --> 31:19.920
them scale. And again, it's related to this question of how do we enforce some invariance? So

31:20.480 --> 31:23.680
people are always trying to say, well, the funny thing about a graph is that the order of the

31:23.680 --> 31:28.480
nodes doesn't matter. And this is sort of the central, this makes a lot of things hard because

31:28.480 --> 31:34.880
if we like graph isomorphism is a standard sort of known to be harder than polynomial time. Oh,

31:34.880 --> 31:39.280
I think it might have just been recently shown to be in some sort of like quasi polynomial time.

31:39.760 --> 31:43.840
Anyway, it was thought to be hard for a long time. I forget the exact complexity classes in

31:45.360 --> 31:50.000
which is given to graphs like a list of nodes and edges determined whether the same graph that's

31:50.000 --> 31:54.880
that's not trivial. So we want to make sure that our models of graphs also don't care about the

31:54.880 --> 31:59.120
ordering. And so we were building up some work recently that said, oh, well, there was a lot of progress

31:59.120 --> 32:02.960
being made sort of by just ignoring the problem to some extent and saying, well, let's just choose

32:02.960 --> 32:08.720
unordering. And as long as we can assign high likelihood to like one of the many orderings that

32:08.720 --> 32:15.040
matches the data, that's probably good enough. And so people like Will Hamilton now at McGill was

32:15.040 --> 32:19.520
using recurrent neural networks to gradually iteratively add one node to the graph as we generate

32:19.520 --> 32:26.400
them. And he was sort of using recurrent networks both at each node addition. And then

32:26.400 --> 32:34.240
within each iteration of node addition, he went over the existing nodes in the graph and it fixed

32:34.240 --> 32:40.400
order with another RNN. So it was kind of like an RNN within an RNN. So completely breaking this

32:40.400 --> 32:45.280
order and variance like like twice over. And then we did that paper says, oh, we actually only

32:45.280 --> 32:50.800
have to bring it once. We have to choose an order. If we use graph neural networks, the great

32:50.800 --> 32:55.360
thing about graph neural networks is that their answer doesn't depend on the order of the nodes in

32:55.360 --> 33:05.520
the graph. Those were the papers that you had at Nureps more broadly. This neural ODE thing is

33:05.520 --> 33:12.160
just kind of one of many things that you're focused on in your lab. I've got a list of those here.

33:12.160 --> 33:18.320
Automatic chemical design using generative models. That is sounds more applied than anything that

33:18.320 --> 33:21.840
we've talked about thus far. Yeah, maybe that's a little old. I mean, that was stuff that I did mostly

33:21.840 --> 33:26.320
in my postdoc at Harvard. Oh, really? Working with the alumnus for music, who he's really

33:26.320 --> 33:31.360
taking the mantle on that one. And now he actually moved to Toronto. Yeah. Well, maybe I should just

33:31.360 --> 33:36.160
abandon this list and let you pop it up a level and tell us what are some of the other cool things

33:36.160 --> 33:40.320
you're excited about? Yeah. Where did you publish on everything you were excited about at Nureps?

33:40.320 --> 33:44.320
Oh, no, no. Yeah, still got. So we got lots of stuff. And the pipeline lots of stuff I still

33:44.320 --> 33:48.720
don't even understand well enough to publish on. But one thing I'm kind of starting to appreciate

33:48.720 --> 33:52.480
now as an academic in my like fourth year of being a professor is that once you're known for one

33:52.480 --> 33:56.480
thing, the incentive to just double down on that one thing are enormous. And it's kind of you know,

33:56.480 --> 33:59.920
like when a band like releases an album, but it has a different sound than the old one, everyone's

33:59.920 --> 34:04.400
like, well, wait, I thought you were going to talk about that other line of work. But I'm really

34:04.400 --> 34:10.480
trying to resist the incentives to pigeonhole myself just because in the really long run, you know,

34:10.480 --> 34:15.920
things change. We have to keep it up in mind. So the general area that I've been excited about

34:15.920 --> 34:21.840
for a few years, but it's really hard to make progress on is let's say learning to search.

34:21.840 --> 34:27.280
So I just thought a grad topic course on this last term. As a way, grad topic courses are a great

34:27.280 --> 34:34.640
way to get a feel for an area and several other recent papers and get some students to start projects.

34:34.640 --> 34:39.760
And it totally worked out. So actually, and a lot of people at DeepMind have been working on

34:39.760 --> 34:47.120
and are publishing similar ideas along this theme. It sounds like a kind of mash up between

34:47.120 --> 34:53.120
meta learning and neural architecture search. Is that kind of the direction? Yeah, maybe those

34:53.120 --> 34:58.720
are related things. I mean, the basic idea is that we have algorithms like Monte Carlo

34:58.720 --> 35:05.760
research that now we're starting to understand how to embed in other hard machine learning problems

35:05.760 --> 35:11.040
in particular inference and planning. So the idea is that most of what reinforcement learning has

35:11.040 --> 35:16.320
done or meta learning just says, oh, yeah, I'll just sort of brute sort brute force try to learn

35:16.320 --> 35:21.200
a policy that does the right thing in every situation. It has a giant lookup table of when you're

35:21.200 --> 35:25.760
in this situation, you should take this action. And that works if you can train the policy, but it's

35:25.760 --> 35:29.760
really expensive and it requires it puts a lot of strain on this one neural network, this policy.

35:30.480 --> 35:33.920
And, you know, I think most people agree that what humans do is they have this hybrid approach

35:33.920 --> 35:37.760
where they say, well, I know roughly what to do, but whenever I realize I'm in a tough or novel

35:37.760 --> 35:41.760
situation, I'm going to stop and plan and I'm going to think imagine a few steps ahead.

35:42.640 --> 35:46.160
What would happen if I did this, what would happen if I do that and then evaluate what I like

35:46.160 --> 35:51.760
that outcome. And so doing a little bit of search on the day when you need to make a decision in

35:51.760 --> 35:55.520
your mind takes a lot of pressure off the policy and makes you a much more powerful agent without

35:55.520 --> 36:01.120
having to in your head prepare for every possible contingency ahead of time. And so like nothing

36:01.120 --> 36:06.240
I'm saying is like new or groundbreaking. I'm just saying that now we finally have the tools

36:06.240 --> 36:10.320
to build such systems. And there's been a lot of work coming out of deep mind in this way.

36:10.960 --> 36:16.320
One paper that I feel like is waiting to be written is about intrinsic motivation or curiosity.

36:16.320 --> 36:21.760
So there's all these papers talking about, oh, you know, how is it that people somehow know not

36:21.760 --> 36:27.040
to not just pursue their goal directly, but also try to learn and do some exploration or stuff.

36:27.040 --> 36:32.080
Maybe something of something evolution, something something like the value of randomness.

36:33.200 --> 36:37.760
And it kind of so I think these papers are making sensible technical suggestions, but the

36:38.400 --> 36:43.040
philosophical sort of speculation about why is this curiosity necessary and making it sound

36:43.040 --> 36:48.560
like some serious thing bugs me because if you just say I am going to have to solve a task,

36:48.560 --> 36:52.640
but I don't know exactly what it is yet or I don't know exactly what the dynamics of my environment

36:52.640 --> 36:59.040
are, then the optimal plan will include doing some exploration, some learning, practicing skills.

37:00.320 --> 37:05.360
So in particular, if we formalize this as a Palm D.P. partially observable Markov decision

37:05.360 --> 37:08.640
process, this is like the bread and butter of reinforcement learning since like the 70s.

37:08.640 --> 37:12.720
We can't solve these because it's too expensive, but if we could, all these behaviors would

37:12.720 --> 37:18.240
emerge automatically. And it's not clear to me how much other people agree with me about this,

37:18.240 --> 37:21.840
because for a long time I thought I was the only one who thought this. But then when I started

37:21.840 --> 37:26.320
to talk to some other, and I thought that probably because the motivations of these curiosity papers

37:26.320 --> 37:30.320
didn't seem to understand this point. But then when I talk to some of the serious RL people that I

37:30.320 --> 37:36.560
know, they're like, oh yeah, of course, yes, that's I totally agree. So you know, it's that's

37:36.560 --> 37:39.840
reassuring. It's also kind of sad because you want to be the one guy with the idea that no one

37:39.840 --> 37:48.720
else realizes, right? So yeah, so right now we can't scale up these amortized planning algorithms

37:48.720 --> 37:54.240
to do effective exploration in really tricky domains. Like what you'd like is if you put your agent

37:54.240 --> 37:58.560
in a gym and you said, you know, you're going to have to play a game tomorrow. I'm not going to

37:58.560 --> 38:03.040
tell you what it would, you know, learn to dribble a basketball or like, you know, pick up the soccer

38:03.040 --> 38:07.360
ball and learn to kick it or, you know, just invent games that it might have to play and then

38:08.000 --> 38:11.520
invent practice drills for itself to learn the dynamics of how it could, how it should play them.

38:12.160 --> 38:16.640
And I think that's there's no remaining foundational problems there, but there's just a lot of

38:16.640 --> 38:20.800
engineering problems for which we have now promising tools to tackle those.

38:21.920 --> 38:28.800
Yeah, as a researcher, how do you approach engineering problems? And do you approach them differently

38:28.800 --> 38:32.560
than an engineer might or, you know, is it about framing them?

38:33.120 --> 38:37.840
Yeah, that's a great question. I mean, I would say the one of the blessings in the

38:37.840 --> 38:41.520
curse of this job that really feels like a blessing most of the time is that I don't spend a lot

38:41.520 --> 38:45.920
of time engineering. And I really like getting my hands dirty and coating up to the point of a

38:45.920 --> 38:51.680
proof of concept, but it is a bit of a slog to get these things to work. And my students are

38:51.680 --> 38:57.200
amazing at that. It's been said of my student, Will Graftwell, that he can get a potato to get

38:57.200 --> 39:05.920
state of the arts on C-Far 100 if he has to. I mean, and of course, you don't want the engineering

39:05.920 --> 39:09.840
skill to be the determiner of which method ends up looking best in your paper. You know, we take

39:09.840 --> 39:16.400
that seriously. I'm just saying, the students here are really amazing at finding, diagnosing

39:16.400 --> 39:20.560
these problems and getting them to work. It's still really fiddly. I feel like, you know,

39:20.560 --> 39:23.920
we're still in the dark ages of understanding what's happening when we're training or on that

39:23.920 --> 39:32.160
works or building models, even in, yeah. Is that something that you're focused on from our research

39:32.160 --> 39:37.120
perspective? Yeah. So actually, one of the other ICML submissions we published yesterday,

39:37.120 --> 39:42.240
or we didn't publish, we submitted yesterday, was a collaboration with Philip Henning trying

39:42.240 --> 39:48.880
to automate the step size selection during training. And the idea is that we said, oh, well,

39:48.880 --> 39:53.040
right now when we run stochastic gradient descent, we just, we, you know, if we'd have a

39:53.040 --> 39:56.400
momentum or we do atom, we kind of have a little bit of averaging of the previous gradients.

39:57.120 --> 40:03.200
But we actually know how to combine noisy observations of different things in, like,

40:03.200 --> 40:06.800
perfectly well on principle with common filters. And this is just like a simple,

40:07.600 --> 40:12.720
latent variable linear ocean model. And so Phil and his student Lucas were saying, oh, yeah,

40:12.720 --> 40:17.840
and one cool thing that we can do now that we couldn't do, at least for modern autodip systems,

40:17.840 --> 40:22.320
is get cheap hash and vector products and get them for every example in a mini batch.

40:22.320 --> 40:26.000
Anyway, I'm getting into the weeds here, but the point is we can look at all the statistics of

40:26.000 --> 40:32.560
the gradients within a mini batch that we're observing, put that in a really cheap and scalable model.

40:32.560 --> 40:37.520
And now when we're trying to choose which direction to go and how far to go, we have a lot more

40:37.520 --> 40:43.760
information available than normal. So our step size can trade off. You can say, okay, well,

40:44.400 --> 40:48.800
I know I'm this certain about the gradients. But I also think there's curvature in this direction.

40:48.800 --> 40:54.800
I also think there's, you know, I'm uncertain about the curvature. I think my future gradient

40:54.800 --> 40:58.640
observations are going to have this much noise. You can trade off all those things to ask, like, what

40:58.640 --> 41:03.920
direction and how far would give me the greatest expected improvement or probability of improvement

41:03.920 --> 41:08.800
or whatever else you want. So we kind of hope that is it fair to characterize this as using

41:10.000 --> 41:16.000
using a model within the machine learning training process in a place that you would otherwise

41:16.000 --> 41:22.480
kind of hard code, hard code a parameter like step size or do like cyclical step sizes or

41:22.480 --> 41:26.160
something like that. You got it exactly. And we just took a lot of care to make sure that there

41:26.160 --> 41:29.600
was no inner training leap. Like we don't have to train this model. All the updates are closed

41:29.600 --> 41:34.000
for them. So the algorithm looks like something like Adam where such a bunch of

41:34.000 --> 41:38.880
vectorized operations that don't have any training leaps. Yeah. And then the hope is that

41:39.680 --> 41:42.640
using this session information or this information that was always available, but we don't really

41:42.640 --> 41:48.320
use it, we can design optimizers that at least always make progress. Maybe they, you know,

41:48.320 --> 41:53.360
some fancy tuned hyper parameter schedule will be able to do better in principle. But if you could

41:53.360 --> 41:56.960
say I'm just going to run my optimizer for a really long time and it's never going to stop

41:57.840 --> 42:02.880
because the learning rate was stuck at being too high or too low. I hope we, you know,

42:03.600 --> 42:08.640
we hope that that will make this sort of engineering struggle that every deep learning student

42:08.640 --> 42:13.760
faces all day every day at least have one less hyper parameter. And you know, and it's a pretty

42:13.760 --> 42:22.080
important hyper parameter. Is there a way to combine this with other methods that have been worked out

42:22.080 --> 42:27.840
that, you know, it sounds like what you are fundamentally trying to solve or at least the result

42:27.840 --> 42:32.800
you presented is that you're less likely to get stuck in some kind of local optima. Is it also

42:32.800 --> 42:39.280
possible to to combine this with, you know, some kind of, you know, acceleration or momentum or

42:39.280 --> 42:44.720
something like that so that you can both converge faster and not get stuck in a optima?

42:44.720 --> 42:48.560
Yeah, that's a great question. And that's sort of where we left the research or that's how far

42:48.560 --> 42:53.280
we've gotten so far is that we got this method to work. We got the step size to work. But

42:53.280 --> 42:57.680
finding the enough, we've kind of found that it converges a little too well or rather that

42:58.160 --> 43:01.920
so we have this one experiment where if we just want our automatic step size selection,

43:01.920 --> 43:06.640
it makes a lot of progress really early and it ends up getting in stuck in a local optimum,

43:06.640 --> 43:09.760
which is normally not really okay. I thought that's what I thought that was the

43:10.800 --> 43:15.920
the opposite of what it was doing by the way you described it, you know, what it really does

43:15.920 --> 43:20.000
as well is, you know, eventually converge. Yeah, well, exactly. Well, when we say converge,

43:20.000 --> 43:25.360
though, you know, we can only talk, we only can guarantee local convergence. So these heuristics

43:25.360 --> 43:28.960
for saying, oh, what maximizes my probability of improvement or expected improvement, that only

43:28.960 --> 43:33.600
looks one step ahead. Yeah, in principle, we do need to somehow look ahead like multiple steps

43:33.600 --> 43:38.240
ahead, but in practice, that's just always going to be really hard. It's like as hard as actually

43:38.240 --> 43:42.480
solving the original problem. So, you know, we had some reason to believe that this might or to

43:42.480 --> 43:45.760
expect that this might be a problem. There's like Roger Gross, my colleague here has some nice work

43:45.760 --> 43:50.720
on the short horizon bias basically saying, if you optimize to do one, well, one step ahead, you won't

43:50.720 --> 43:54.560
make good long term progress. So the thing is that there's also reason to believe that this wouldn't

43:54.560 --> 43:59.360
be a problem because we think that the local optimum that we in training deep nets isn't such a big

43:59.360 --> 44:03.360
problem. So, you know, we're not totally sure if these are actually local optimum or just places where

44:03.360 --> 44:09.200
the optimizer can't make progress. But we did find that if we run for a fixed step size for a while

44:09.200 --> 44:13.280
and then switch to our adaptive step size, that works the best. So it's one of these things where

44:14.320 --> 44:19.120
we did get this thing to work better, but because it's myopic, like it only looks a few steps ahead,

44:19.120 --> 44:25.760
if you have a really long computational budget, just using fixed step sizes for a while and then

44:25.760 --> 44:29.440
switching works best. And so that's an unsatisfying answer. So I think that's the remaining question

44:29.440 --> 44:34.560
to get this to be really practical is it could be that if we just run this adaptive thing for

44:34.560 --> 44:37.760
long enough, it will be able to escape these local optimum. We haven't really looked into this

44:37.760 --> 44:44.160
in depth yet. But anyway, I mean, I'm impressed that you identified the the weakness in this whole

44:44.160 --> 44:50.800
story and just guessing it. Yeah, maybe taking a step back, you know, there's kind of a lot of

44:50.800 --> 44:58.240
contemporary debate around, you know, the role that deep learning plays in artificial intelligence,

44:58.240 --> 45:04.320
moving us to AGI. What kind of motivates you and where do you see this all going?

45:04.320 --> 45:11.120
Right. So as I said, I think from a practical point of view in the short term, just being able to

45:11.120 --> 45:17.680
meet the data where it is and deal with the actual huge piles of real problems and data sets that

45:17.680 --> 45:22.640
can't even really be touched by standard deep learning or at least supervised deep learning or

45:22.640 --> 45:27.040
discrete times here's models. That's like huge area of low hanging fruit. And there's a ton of

45:27.040 --> 45:32.240
people who are just saying, oh, you know, I was promised that AI would revolutionize my industry,

45:32.240 --> 45:38.240
but it's still kind of very bespoke and only can be applied here there. And then, but yeah,

45:38.240 --> 45:42.000
that's sort of partly why I'm also spending a lot of time thinking more about these

45:42.800 --> 45:51.040
amortized search and planning algorithms. Because I do think also that we are about to have a big

45:51.040 --> 45:55.760
improvement in the sort of general reasoning abilities of machines. And I think this is still

45:55.760 --> 46:03.280
going to be like mostly toy demos for a few years. And do you think that that mostly comes from a

46:03.280 --> 46:09.760
reinforcement learning type of problem formulation? Well, the thing is that reinforcement learning is

46:09.760 --> 46:15.600
such a vague word. And I guess I'll say model three reinforcement learning, you know, has it's like

46:16.240 --> 46:19.280
now no one's excited about it. And everyone was super hyped about it like, you know, three or four

46:19.280 --> 46:26.720
or even two years ago. I guess I would say right, like model-based planning or model-based control

46:27.520 --> 46:32.720
is really starting to become practical now for the first time. I think a lot of people have said,

46:32.720 --> 46:36.960
you know, been leaving in this for 40 years. There's this amazing book by Britsikis Neural

46:36.960 --> 46:40.240
Linguistic Programming that basically outlines most of the methods that people are excited about

46:40.240 --> 46:45.120
today. And I think it's from like the 80s. Yeah, but it's just that we now have a pile of

46:45.120 --> 46:49.680
or a set of tools that we have an idea of how to combine. We're starting to understand how they can

46:49.680 --> 46:57.680
be scaled up. So what I hear you saying is that you're mostly not placing bets on this kind of,

46:58.320 --> 47:04.480
you know, what's going to get us to AGI kind of question and you're focused on like, you know,

47:04.480 --> 47:09.200
how we can use this technology to solve current problems. Well, no, I guess I would say

47:10.400 --> 47:14.880
working backwards from what gets us AGI is a really fun research agenda, right? And I love all

47:14.880 --> 47:22.000
the papers of the Google machine. There's a recent work on logical inductors that try to sort of

47:22.000 --> 47:24.960
sketch out what these would look like. And then they always have a part where it's like, and now

47:24.960 --> 47:28.000
you do a search over all possible programs or something like that, which we don't know how to do.

47:28.560 --> 47:32.000
But I love the idea of working backwards in there. And I guess I'll just say we also have a huge

47:32.000 --> 47:37.200
amount of low-hanging fruit, like strong gradients saying, oh, if we combine these two tools that

47:37.200 --> 47:42.640
we have, we can figure out how to do that. We know we'll have a big step towards more general

47:42.640 --> 47:47.440
reasoning capabilities. So I think we don't even have to, we shouldn't think of you thinking hard,

47:47.440 --> 47:51.760
but for the near future, I think we can make a bunch of progress without even thinking that hard

47:51.760 --> 47:57.680
about the long term. Well, David, thanks so much for taking the time to update me on what you're

47:57.680 --> 48:04.400
up to and, you know, generally share with all of us what you're working on at your lab and with

48:04.400 --> 48:13.360
your students looking forward to catching your ICML papers. Oh, it's been a pleasure. Thank you, Sam.

48:15.760 --> 48:20.720
All right, everyone. That's our show for today. For more information on today's show,

48:20.720 --> 48:37.520
visit twomolai.com slash shows. As always, thanks so much for listening and catch you next time.

