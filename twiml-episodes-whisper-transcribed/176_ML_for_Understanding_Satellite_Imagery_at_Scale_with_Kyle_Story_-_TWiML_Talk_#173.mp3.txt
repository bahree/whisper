Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Today we're joined by Kyle's story, computer vision engineer at Descartes Labs.
Kyle and I caught up after his recent talk at the Google Cloud next conference titled
How Computers See the Earth, a machine learning approach to understanding satellite imagery
at scale.
Kyle and I discussed some of the interesting computer vision problems he's worked on
at Descartes, including custom object detectors and the company's geo-visual search engine.
We cover everything from the models they've developed and the platform they've built
to the key challenges they've had to overcome in scaling them.
Enjoy.
Alright everyone, I am on the line with Kyle's story.
Kyle is a computer vision engineer at Descartes Labs.
Kyle, welcome to this weekend machine learning and AI.
Thank you so much for having me on.
How are you?
I am wonderful.
You are joining an esteemed group of physicists who have been on this podcast.
It seems like there are quite a number of folks that come out of cosmology like you and
astronomy and particle physics and make their way to machine learning.
How did you do that?
Yeah, it's actually a fairly natural transition so my background is in cosmology as you said
which is a branch of astrophysics and so I spent almost a decade in graduate school
and then as a postdoc at Stanford working on a mostly on it with data from a telescope
that's in Antarctica where we were measuring what's called the cosmic microwave background.
So this is radiation left over from the big bang.
You can think of it as an afterglow of the big bang and you can actually map it and what
you see in those maps are basically a baby picture of the early universe or variations
in density, about 100,000 years after the big bang is the furthest back you can see
in time and so I spent a decade studying this using the information and the statistics
in those data to try to understand the universe as a system, model the universe as a whole
and then relatively recently this past fall came out of academia to join day cart labs
and basically instead of looking upwards using a satellite I'm now using a telescope on
the ground and kind of inverted that problem so I'm now using data from satellites to look
downwards at the earth and so it's a lot of there are a lot of similarities and a number
of important differences as well so in one sense they are both remote sensing problems
so they are both trying to learn something about phenomenon purely based on the light
that you see or the signals that you're receiving from that source and then on the machine
learning side before we go too far I've got to ask have you did you ever get to spend
time in Antarctica like I'm envisioning I forget the movie but like the you know the
one camp where you go basically under I don't know if it's on the ground probably not
under under eyes but you go into this into this encampment was it anything like that
it's it's not quite like that I have spent just shy of a year of time total in Antarctica
that was spread across six different trips and so that the telescope that I worked at
is aptly named the South Pole telescope and it is literally at the South Pole and so
I would go there during the austral summer season to maintain the telescope set it up
for observations and then most of our observations were taken during the austral winter so when
you're at the pool there is one day and one night per year so I like to say that I've
spent almost a year in Antarctica and I've never seen the sunset just kind of interesting
but it it's it's a base that's funded by the National Science Foundation and the South
Pole Station is fairly small during the summer it usually has a hundred to a hundred and
fifty people who live there we do have a building that's heated so kind of very small
dormitory style rooms and a cafeteria and then where there works we just work all the
time but it's it's a really interesting environment to be in it's just a super cool place to
be and I really value having had the opportunity to spend time down there it sounds like an
incredible experience so you were recently at the Google next conference talking a little
bit about the work you're doing at Descartes and one of the things that you made sure to
touch on is really the types of problems that you encounter the computer vision problems
that you encounter at Descartes and some of the unique characteristics of those problems
can you maybe review those for us?
Sure so the types of problems that we're working on in Descartes labs is trying to understand
the world using machine learning and using satellite imagery and other geospatial or like
georeference data sets so it's it's really is a big data problem and I know that's kind
of a you know thrown around as a phrase that people like to use but the the size and scale
of the data are are quite large and because of that it really does require automated analysis
approaches and this is where the machine learning comes in is trying to instead of just
saying we have a bunch of pictures let's set somebody down in front of computer to go
look it through of all of those pictures that that doesn't really scale to the globe right
so that's where that's where we take a machine learning approach where you say let's instead
build an algorithms that can detect either objects or patterns and then we can automatically
scale these to large regions so and just to give you a concrete or two concrete examples
one of the places that Descartes labs started as a company was trying to understand corn
production in the United States so being able to measure and predicts corn production
over the the Midwest United States and then another example just to have something concrete
is mapping infrastructure in a higher resolution is satellite imagery so for example being
able to find just simply find buildings be able to map make maps of buildings and you'd
be surprised how incomplete open source data sets are even on on basic infrastructure
like this so the idea with that latter example is we don't have good lists of where the
nuclear reactors are or the power stations or things like that so we need to actually regenerate
those lists using satellite imagery so some of the specific types of buildings like nuclear
power plants are pretty well listed but even just mapping you know residential buildings
and industrial buildings and simply just knowing where they are the fast open source data
set that I'm aware of his open street map and so some of your listeners may be familiar
with that and it's an incredible data source but even in the United States when I'm looking
at that data set I can go and pull up and see entire neighborhoods that just are blank
just don't have anything filled in and that's largely because that community has primarily
been based on people hand labeling things and largely volunteer effort and it's really
impressive what they've been able to do but being able to scale that to an up-to-date
map of buildings across the United States or even better across the world is really
just not it's not up to the task of hand labeling all of you know segmenting out each building
in a neighborhood and so so when you step outside of the United States of course it the
problem changes as well in that there's just a lot less information in a lot of countries
around the United States and so by training convolutional neural nets or other machine-land
approaches we can take a first pass at being able to map out where buildings are and that
provides an incredible amount of information that would just be unfeasible to achieve
in a so person sitting in front of a computer screen hand labeling type of way.
I think many listeners of the podcasts may be familiar with for example the planet
data set on Kaggle kind of understanding the Amazon from space and I know for those
of us who have worked through the fast AI course you kind of spend some time playing
with that data set and learning to you know automatically classify tiles that are forested
from tiles that contain water features things like that and so we may like come to this
conversation say oh yeah sounds easy pretty standard stuff like bring us to the real world
what's the you know what are some of the challenges that you deal with that you know the
folks doing this on Kaggle don't have to worry about yeah sure so I so I come at this
from you know from my physics background as it's all about the data and the information
that's in that data and so the stepping away from machine learning a little bit at first
you simply need to be able to access the data in an efficient way so that's and you know these
datasets are quite large you know when you when you come to a Kaggle competition that that step
has pretty much already been taken care of for you but there was a huge amount of effort in being
able to get the satellite imagery and then have it quickly accessible in you know a way that you
can incorporate into even starting to take a machine learning approach and so this is actually
one of the things that Descartes labs realized fairly early on was that a lot of the power for
being able to work with these datasets is going to come from having a platform that allows
uniform access and quick access to these types of data so we one of the things that we have done
and it has been a primary goal and continues to be a primary objective of the company is to
build out a platform that provides access quick access to a huge amount of data we do so this is
why we are at the Google next conference we've built out this entire platform within Google Cloud
so it's so we pretty much don't own any hardware beyond our own laptops and are working almost
exclusively within the cloud and what that gives us is being able to host a large amount of data
in a way that we can access very quickly so that's that's one problem then another so then a more
machine learning specific problem is the is the ground truth question right so when you come to
a cargo competition oftentimes you have a lot of well labeled ground truth and how basically
how much ground truth you have will dictate what types of algorithms you're able to use and then
what types of problems you're able to solve so those are at least just two problems that
present present themselves when you start dealing with questions in the real world on that first
point the platform talk about I'm curious what that means for you and or rather I'm curious what
that all that that encompasses you know is the platform you know just some set of Google Cloud
services that you're using off the shelf or have you built a lot of capability on top of kind of
raw level hardware that you're using via Google Cloud you know walk me through like what the
the elements of this platform are sure so it's all the above everything you mentioned okay
uh the so the first step is getting access to these data the so there are a number of openly
available sources like Landsat or Sentinel generally government sources and they all have their own
unique ways of accessing the data so in principle it's it's open freely available but each one has
it you know each one has a slightly different format or has slightly different image registering
or has been tiled in a different way so the first big step is ingesting all of those data and
then hosting them in the cloud so figuring out how to access the data stream pulling out all of
that data in uploading it into the cloud the next the the next step that the platform accomplishes
is tiling those images across the globe so in so I as a because I'm working within the you know
within our platform I don't have to think too carefully about how to stitch together a bunch of
different images in order to make a picture that covers the region that I'm interested in that's
all handled in an automatic way so a predetermined way of tiling the data uh that just simply removes
one of the steps that I would otherwise have to deal with then there's the search question right
so let's say I want to let's continue working with the building model example since that's the one
that I've personally worked on directly so let's say I want to collect a set of imagery over
California to use as training data how do I figure out which exactly which images I need to pull
from you know from the giant database so we have a search capability that has where each
each image that gets uploaded has a list of metadata and then we can using the google's big query
we can or actually this is the last search we can search through that metadata and pull down
exactly which images over which region over what time in a really efficient way and then turning
those images from so it's from whatever format compress format they've been saved into into an
actual image that I can applaud on my computer and then do things with and so this actually brings
up a point that maybe we can get into a little bit later of how we think about these types of analysis
and what I mean by that is thinking about imagery instead of something you pull up on your computer
and then you flip through images kind of hand-level things or look at things thinking about that
imagery as input to algorithms input to machine-learning algorithms and when you when you you start
thinking in in those terms it's you definitely take a different approach to how you work with that
data how you store it on disk how you make it available interesting so for example is that
latter point meaning it as opposed to storing the images on disk as you know in a native image
format your form your storing them as like TensorFlow records or things like that close it's that's
right yes so we actually chose to store the majority of our data in jpeg 2000 and it's a the
reason for doing so is it that format basically decomposes the images by scale so and and then
allows you to use the correlations between different bands so these images you know can have
spectral bands like red, green, blue, and year and red, swirr etc so you can take advantage of
some of the fact that these bands are correlated to be able to compress them significantly
and so when you say store them by scale is that in a sense of like a progressive image where you've
got kind of a low resolution version of the entire tile and then like progressively more detailed
tiles on a yeah yeah so it it allows you to do that without having to save duplication duplicate
copies of the data right so if you're so if then you so this is one way that we're able to zoom in
and out you know so on the visual side when a person is actually looking at it being able to zoom
in and out quickly is taking advantage of that kind of layered structure of the way that the data
is saved and then sticking to this theme of platform you kind of talked about these three steps I
guess this ingest and the indexing and then the way you manage and manipulate these images I don't
know if I captured exactly as the way you described it but you know looking at like the ingest
side is that are you what are the different ways that you're getting the imagery like are you
I don't recall if Google has a you know ship a hardware box type of feature like a AWS snowball
but is that an issue for you are you mostly getting them online and landing them in cloud storage
or something similar yeah it's the second it's the second option so we're pretty much always
getting them online and then depending on the most of the data sources that we're interested in
are data sources that are continually updating right because when you're thinking about what
types of problems you actually want to use these data for a lot of it is going to be looking for
change or monitoring what's what's new on the surface of the earth and so it's important to have
as you know it's close to real-time data as the sources can provide so what so we're stepping
a little bit outside my specific expertise but what those in general look like are pipelines
that are built within the cloud that can at check for new imagery as that new imagery becomes
available pulled into the cloud and then using the you know using parallel parallel compute they
can process those imagery filter out all the metadata get that get that all set up and then store
it into our cloud storage system so it can then be searched and accessed by a user like myself
and then so from a machine learning platform once you've got all of this imagery in place what are
some of the types of models that you're building and the specific problems that you're trying
to solve so let's say a little bit in general about how I think about the data and then we can
talk about some specific problems so with these data and there are kind of three dimensions sort of
that are that provide at least me a useful framework for thinking about how to put together analyses
by that I mean a spatial dimension a spectral dimension a temporal dimension so the spatial
dimension is just where are you in xy and perhaps z position on the surface of the earth the
spectral dimension is for each image you may have taken data in different bands or different
frequencies so optical frequencies red and blue like a ccd camera would take near infrared imagery
there's thermal bands so that that type of spectral information can be really important
and then there's the temporal dimension so what I mentioned about seeing how things change
over time for a given place on the earth and then given these three you can combine them in
different ways to address different types of problems so to be concrete about that
usually a computer vision problem where I've spent most of my time working is generally combining
spatial and spectral information so you're trying to say find where things are on the earth by
looking at these images you can also combine spectral and temporal information to say given a
place I know so let's say like a crop field if I want to measure how that field is doing over time
I'm probably going to be primarily using spectral and temporal information and then obviously
you'll pull all these all together in different in different ways so should I jump into a
specific example or two sure please yeah so on the computer vision side where one of the places
that we're kind of starting is with mapping what you can see on the ground so this is related to
your earlier comment about the the cargo competition on planet data so that you can think of that as
basically making masks of where's forest where's water where other types of things like that
taking that to the next step would be like like the building example that I said so being able to
define an algorithm that you can run over imagery and it will return result of that algorithm
will be a probability map where each just says for each pixel are you in a building or not so
this is a semantic segmentation problem and we can use pretty pretty standard semantic segmentation
tools and then there are also if you don't need to segment out entire image you just want to say
where are all of the x in the world so an example of something we've done is found electric
substations right so saying we even in the United States we do not there are not comprehensive
lists of all the electric substations across the United States but you can see them in high
resolution satellite imagery so in this case we were working with the freely available
nape that's the national agriculture imagery program and so we can use an object detection approach
to find where all the substations are if you want I can go into more detail about what that looks like
yeah please sure so for let's just keep running with the the electric substation example
for that one there are so from a machine learning approach you have you need to define your data
you define your inputs they need to find the model architecture train and then run that model
right this is kind of the life cycle of these types of problems so for here the input data as I
said is high resolution imagery from nape so this is either one meter or 60 centimeters on a side
per pixel so it's enough to actually be able to see the kind of the structure of an electric
substation where you see the the wires coming off and you see the the stands that are holding
pulling all the wires and the transformers and things like that there are enough substations
that are labeled in open street map and so for this problem that makes it makes the ground truth
fairly fairly straightforward we can just grab the locations of substations in open street map and
for training we don't need to know where all of them are we just need enough to be able to train
a good model and then you know using the platform I can create images and then I can burn those polygons
that say where the the known substations are into a similar target image so I now have an image
and a target pair that I can use for a supervised supervised learning approach and then we picked
ssd model it's a single shot multi box detector model as our object detection model of choice
and one of the reasons for this is also kind of an interesting that we might talk a little bit more
about is this is taking advantage of transfer learning ideas so with ssd the so in two sentences
for listeners who aren't familiar with it the way that algorithm works in general is you take an
image you split it up into a set of predefined boxes for each of those small boxes you use a
compost from a neural net to create a set of features and then the final step is basically
predicting whether or not each box contains the object you care about so in my case an electric
substation and we don't necessarily have enough examples to train something all the way from scratch
but we can take advantage of in this case ResNet to be able to generate those features and then we
only have to train you to fine tune those those features and train a final classification stage
so even with a relatively small number of examples we're able to train an effective algorithm to
locate these electric substations and so when you say ResNet ResNet trained on what image net or
yeah exactly yeah exactly so actually I'm sorry I misspoke this one is the VGG network
we do use ResNet in in similar contexts but in other models yeah so this one is using VGG as
the neural net to produce features and then we are just fine tuning the last I guess training the
last classification step to say whether or not each box contains the object you care about
with VGG and ResNet and the like typically these networks are trained particularly the ones that
are pre-trained on image net and the like are typically looking at relatively small
image sizes and like three channel images you know 240 pixel square or something along those lines
or what approaches do you use to apply those to these you know super high resolution images that
you have yeah so for for this particular model we used 512 by 512 images and week so one
one advantage even though we're trying to use this over large regions the earth is still in
in some sense an embarrassingly parallel problem right as long as your objects are small enough that
they fit into a tile you can chop up a region into you know almost an arbitrary a number of large
number of small tiles so it so it still works quite well we even even with the constraints that you
said and then in this case we are just taking so you mentioned the three bands that's actually
that is an important difference between a lot of standard computer vision problems where you're
just using a you know RGB image versus what I've out what I've mentioned here with satellite imagery
in this case we are still just taking the I think I think I'm using an NIR and R&G bands in this
case but in other approach and other algorithms that we worked on we have used basically data
compression so you know PCAD composition things like that to be able to compress a larger number
of bands down into three bands that will then work with these pre-trained feature generators
huh that's interesting so in that case you've got you know some larger number of bands you've
got your you know I don't know if for the satellite images the color bands a visual bands or
three or more but you've got some number of visual bands and then you've got like these infrared
bands how many total do you start within some of those cases it depends on the satellite so for
example the sentinel satellite has 12 bands that extend from visible to a near infrared all the
up into sphere bands that are sensitive to thermal emission so it yeah it can vary a lot
so you've got you've got 12 bands and you're able to use PCA or some some kind of alternate
projection of these bands down to three bands and yet a model that's trained on kind of visual
image net objects still works yes and no it's it's it's surprising I mean because I'm
imagining that the these visual I guess in one sense like that you know the the result of this PCA
thing I'm not imagining that to really mean anything visually right and so your model that's been
trained on this highly visual data you know in one sense I wouldn't expect to work on the other
sense like all it's doing is learning a bunch of different kinds of patterns that's what you're
counting on is that it learns a bunch of different types of patterns that themselves aren't necessarily
visual so maybe it should work yeah exactly and it's it's a lot of trial and error right it's a lot of like
hey this I don't know this might work let's you know let's do a small test and just see what happens
and in some cases this type of transfer of compression has worked well and then in other in
other cases I hopefully we'll talk about a geobigial search in a little bit there was insufficient
and we had to do some further fine tuning to be able to get effective features so it just you
know it's it's it's a lot of a lot of trial and error and a lot of simply digging into the data and
doing a few tests seeing what works seeing what doesn't and then moving on from there you mentioned
that you to get your your ground truth you start with this open street map data that has identified
some number of electrical substations how many approximately are there that are identified
so in the the ground truth set that I ended up with I had about a few thousand okay so it kind of
gives you a gives you an order of magnitude yeah so you've got a few thousand of these electrical
substations and you said you burn them into the images meaning you like your yeah just simply saying
that I the output from OSM is a polygon that has lat long coordinates for to trace out a boundary
and I'm just simply turning that into an actual image of zeros and ones ones with inside that
polygon and zeros outside that line up with the corresponding image from the satellite and is
that type of training data arrangement while you know we're on you've you've essentially colored on
a pixel by pixel basis is that kind of what SSD is looking for so then for SSD there is one final
step of then drawing a bounding box around the the ground truth object and so I guess I we
probably could skip the burning the raster step to make it slightly more efficient but then the
actual training is done against the coordinates of the well I guess at that point they're actually
in pixel coordinates so we do need to go through the image to to get the pixel coordinates of the
bounding box for the object of interest did that make sense yeah I mean it sounds like you could
potentially do some kind of math going from a polygon from the OSM data set to pixel coordinates but
the way you did it is by just manipulating the pixels on the image themselves yeah that's right and
ends you know with a lot of these things it's a question of where do you invest your time
absolutely and yeah so something like that yeah you could probably make that better but there's
a thousand other things that are more important to to put my time in at my desk into
so you've trained this model and I'm curious like are you with you mentioned some places you
use VGG other places you use ResNet are you you know is it kind of you you know for some projects you
do have you built up in an intuition around where you use specific things that works or is it
more like kind of looking at the the rings in a tree like the you know this project is older and
so you use VGG which was state of the art at the time and now if you were to do it again you'd use
ResNet yeah there's there's a lot of that right exactly kind of exactly what you said you yeah
you build up models as you go and then as new tools come come online and depending whether or not
it's worth going back to fix up a model we'll dictate whether or not you go back and try out new
tools or new feature generation I hope we'll talk about GeoVigil search in a bit and that's an
example of that where I we've done something that's worked pretty well but then are now the process
of going back using new tools using new data trying to make it better okay well let's do that before
we do that have we covered all of the the important bits on the the substation identification
but maybe one last thing just because it really with this being the google next series and relating
to the cloud an important part of these models is once you're done with the model you need to be able
to run it efficiently over large regions and so it's you know not machine learning specific
necessarily but it's definitely cloud related so as I mentioned a few minutes ago the earth is
somewhat of an embarrassingly barrel problem where you can just chop it up into a lot of little
pieces and this really lends itself very naturally to a cloud environment so once we have a model
then say I want to run over the United States I can just grab all of the imagery that to cover
the entire United States chop it up into a little 512 by 512 pixel pieces and then hand a set of
those and the model to a worker in the cloud and then I can spawn effectively an arbitrary number
of workers to go process all of these data through the model and then save those results directly to
the cloud so just by by working in that highly parallel fashion that's how we're able to automate
analyses and run them in a recently you know a reasonably quick amount of time so like I've been
able to I mentioned the building detector I was able to run that over the entire state of California
kind of overnight and so I come back and work in the morning and I have a map of all the buildings
across the state so that's just the kind of the final piece of the model life cycle and I even with
that final you know that specific piece and you know the the constraints you mentioned of taking
this model handing it to something that lives in the cloud and having it run in a distributed fashion
and even within the confines of Google's cloud offerings there are still probably I can think of
five ways off the top of my head to do that specific thing like do you get involved in the
infrastructure pieces of putting that all together or maybe more generally how have you
architected this distributed execution or training is it like is it one of the higher level you know
the machine learning engine type services or is it all running on just cloud engine workers
that you guys build up and manage or is it running on Kubernetes yeah so we so the engineers and
our team have built up a system in Kubernetes that are using primarily preemptible VMs and then
being then they have we've written our own task scheduling software that will and then
so we've written our own task and new software that will launch jobs into the into
criminal VMs all of the work environment so capturing the code and then all you know all the
different dependencies things like that using Docker and then inside of Kubernetes and so then
you can set up a environment in each of those workers run the code save your results to the cloud
and then break that all down so it's all it's all Kubernetes-based we're now working on getting
Istio which is one of the new monitoring packages out of Google to be able to monitor all of these
processes in an effective and effective way so and then we're we we're not using the
ML engine yet but it's something that we're looking into and are you also looking at the
Qflow package for Kubernetes yeah so that is that that's also something that we're looking into
at this point so far we're running you're basically running our own models we're largely
cares models but yeah Qflow is something that we're starting to look into now how long has the
platform been around how long have you been working with Kubernetes so the company in total has
been around for about three years so we're pretty pretty company and I'm not on the engineering team
but we've been working with Kubernetes for for a while so I think most of that time
cool yeah well we won't drill into necessarily the Kubernetes details here but I may be interested
in talking to someone on the engineering side as well just for background we talk through the
substation detection problem and you've been chomping at the bit to tell us about the geovisual
search tell us about that problem sure yeah so this kind of chomping at the bit just because I think
it's cool and interesting and a little bit unique so the idea with visual search is to be able to
click on anywhere on the earth and then be able to see other areas that look like this so at a high
level you can think of it as kind of a generalist model that hasn't been trained to find anything
specific but is able to you know is able to search over a large region for lots of different
types of things on so then the way it actually works under the hood is kind of cool so maybe I'll
take a few minutes to walk through that now yeah please cool so the the way it works under the hood
pretty actually fairly similar to many of the ideas that we've been talking about so far in this
podcast take a set of data so in our first pass we've worked with either Nape that I've mentioned
before over the United States or compositive Landsat over the entire globe chop that imagery up
into a bunch of 128 by 128 images and then use a ResNet based feature generator to create a set of
features and then simply search for similarity between features for each tile so so when you click
on a tile it's doing a similarity search to say what other tiles have similar visual features
so this this touches a number of the things that we've already mentioned on this podcast so far
the first one being whether or not the ResNet features are sufficient so in this case they we found
that the features that are generated by ResNet were not good enough to be able to have a really
clean visual search and so we did actually do some fine fine tuning of those weights basically
for Nape where it's high resolution enough to you'll see a lot of objects we took an approach of
grabbing a bunch of different objects from osm again from the open street map and then doing a
supervised fine tune training of the features to be able to match to osm objects that just kind of
cleaned up what types of features that the network was generating and so specifically to
drill in on that point for a second what you mean is that the features that the pre-trained
presumably on ImageNet model was had as opposed to the architecture itself you weren't ever
training from scratch this was all we're still talking about pre-trained ImageNet models exactly
that's exactly right so yeah taking advantage of all of that that computation and pre-training
that went into creating the ImageNet based ResNet weights and then just fine tuning those
and I'm curious can you provide us some scope or order of magnitude of the fine tuning effort
just to get a sense for you know how much you benefit from someone else doing the heavy lifting
of training ImageNet I don't really have a good way of quantifying that at the moment yes
are so you've you start with the pre-trained ResNet model on ImageNet you you fine tune and then
when you talk about using the features of this ResNet model are you basically chopping off the
classifier at the end and using that last layer or you're using intermediate layer activations or
something like that yeah so we're we're basically just chopping off that last classification layer
and then an additional step there is if we store all of those features the features numbers in
their full size I would take up a future amount of disk space so we've actually binarized those
features by injecting a bunch of noise at the end of the training process to basically force the
those features to choose between either zero one and then in the you know in the final network we
threshold it so the output of running a tile through the trained network is a list of 512 bits
zero ones and that's so that's at the cost of the you'd be basically the precision of being
able to differentiate between so we lose a little bit of differentiation power but we gain a lot
in both speed for the search later and in terms of having it take up a reasonable amount of
disk space can you elaborate on on that process the noise injection process and how that translates
to allowing you to isolate these individual pixels yeah sure so at the so basically at the
the last step we injected noise during training with an apple too that was comparable to the
width of the layers activation function and so this means that the the network needs to either
basically either decide you know the activation needs to either be one or zero otherwise there's
enough noise to kind of destroy the information that the layer is trying to pass on so it forces
the network to learn to output either very close to one or very close to zero and then once we have
that then we can just once that's trained then we can in the final model we can just add a
thresholding step does that make sense and you're injecting that noise at the last layer itself as
opposed to in the input is by manipulating the image or that's right at the okay yeah it's just at
that last step oh it's interesting um is there a name for that technique not that I'm aware of
they're they're very well maybe okay I'm not aware of a specific name okay at that last layer of
resonant how many features are there so we're we're ending up with 512 features per input image
so you're what you're what you're trying to do is it is not you're not trying to necessarily
reduce the number of features you're trying to basically compress them on off yeah that's right
got it exactly okay and then just to complete because it's also interesting idea for the
Landsat dataset this same fine tuning process but instead of using osm uh there aren't really
uh enough useful information in osm around the across the entire globe so we used took an auto
encoder approach basically um and use that as a way to to fine tune the um satellite imagery
future generation for the Landsat dataset and what's the relationship between the fine tuning process
and osm or the you know we'll get to the auto encoder but how does the osm data help you with
the fine tuning it just makes the the features that you're generating more responsive to the
the types of things you've seen satellite imagery so it kind of exactly what you mentioned
earlier where image net is trained on you know a bunch of pictures that may or may not
have the size and shapes of the types of things that you're interested in in satellite imagery
this just allows us to fine tune those weights a little bit to uh make them more descriptive about
the the types of visual features that we see you know in satellite imagery so does that mean
I mean you've got satellite imagery in your original data um it is what you're doing
are using osm to give you landmarks so that you can train on specific tiles that you know are
interesting as opposed to training on a bunch of ocean or something like that or is it more
nuanced than that it's partially that but it's more recognizing the fact that
up overhead picture of you know a city park and a golf course that has doesn't look exactly the
same as a picture of a cat or a dog and so the the features that a network learns to consider
interesting from one of those images won't be exactly the same as the features that a network
learns to consider interesting from the other one and so it's it's really more accounting for
that type of difference knowing that we're going to be applying it to satellite imagery so trying
to just fine tune a little bit uh what information the network is pulling out of those images
okay uh yeah so you're you're doing this before you take off the classifier and you're training
on osm tiles because you've got labels for them exactly got it I missed that part I was thinking
we'd already chopped off the the classifier and so how do you use an auto encoder to do that
you're basically doing it in more of an unsupervised kind of way yeah exactly that's right so because
the Solanza imagery is a lot lower resolution 15 15 meters per pixel instead of one meter per pixel
so the labeling from the osm simply wasn't as useful and then additionally there weren't as many
labels around other go in other continents around the globe and so we still need to so we use an
auto encoder to uh basically compress those images so compress the final image net features down
into the same number of 512 feature bits so you so in that case the auto encoder is we're basically
using it as an compression algorithm to get from the output image net features to the small number
of binary features that we want to use for the visual search cool so yeah so then the result of
that process is we've now tiled up the United States and the earth into small tiles and we've trained
a algorithm to create visual features for each of those tiles and so now the the search part of
the visual search comes in uh so the basic idea there is to you want to look for tiles that are
close in feature space and so we took kind of a two-step approach to making this work first
we used a hamming distance as kind of a first pass filter so this is just simply looking at the
difference you're just lining up the bits and looking for differences between bits and that
allows you to zoom into a smaller number of of candidates that may be visually close to the tile
that you're searching for and then as a final step for the Landsat data the that first first step
was enough to limited it down to where we could simply do a direct search of tag because it's just
comparing bit by bit a group for search over all the the close images and then return return the
image that's closest in feature space for the NAPE data set which starts with about two billion
of these little tile images that was it still a director works but it's too slow for interactive
use so we needed to do something a little more creative to be able to make this so you could click
on something on a website and have have information come back to you in real time and so we used
the approximation method called bit sampling which is basically takes a set of 32 hash functions
and then it hashes sets of bits to reduce down that 512 feature bit vector into smaller chunks
and then looks for images that have similar outputs of that hash function and then you can do a
group for a search of that second filtered set of images did did that make sense it does I'm
curious whether the notion of using some kind of projecting this feature space into some kind of
embedding layer is that does that make sense in this context and using like the distance in
an embedding layer to do image similarity that I think that approach could make sense yeah this
this type of search is basically what we landed on first but an embedding approach could also work
well okay interesting so yes so then the output of that is we now have a search capability where
you can click on a little tile and get back results in other tiles that look visually similar
basically in real time it's you know public available on our website at search.dakerlabs.com
you can go play around with it if you want and so it forms a it's not a it's as I said in intro
it's kind of a generalist model that's looking for things that look visually similar
and then if so for some types of problems that's enough for other types of problems where you
you want a higher precision recall in the objects that you're detecting or the objects aren't
quite as visually striking then you need to take a more you know expensive more but more standard
computer vision approach with like the electric substation as I mentioned before for example
I'm curious with the the geovisual search have you developed a way to
characterize the accuracy of the end to end process like you're you know the performance metrics
that you would typically look at for a training process are you know don't really apply because
you're kind of shifting the the use domain and I'm wondering if there's a way to measure
the extent to which this feature similarity maps to what people think is similar or what people
want to see like you know is it figuring out built you know tiles that have the same number of
buildings or tiles that have some you know maybe there's something that's not really you know
that doesn't jump out at us that causes feature similarity but it's not but it's not necessarily
what an analyst wants to see when they're clicking around how do you characterize that performance
so we haven't done anything quantitative in what you described you certainly could say I have
a class I care about let's see whether or not a visual search you you could calculate a precision
recall for if you had basically a labeled data set that you could check against and here it's
you know we we haven't done that type of quantification of the process and because that'll also
depend a lot on which object you choose you know for example so we're still we thought about
this more as just kind of a general first pass search rather than a quantifiable like precision
recall toward right right if that makes sense but yeah it's I mean it's definitely the type of
thing that you you could do we just haven't performed that exercise with any specific examples
and you mentioned that that's you know once a user cares about that type of precision you would lean
more on the type of process that we described earlier where you're modeling around a specific
type of entity or object and it sounds like you know presumably you what you have found is that
you know when people click around in this map they tend to get results that are appealing to them
that are that satisfy the base promise of the system yeah exactly so it's it's it's it's kind
of an interesting I think of it as an interesting starting place or you can even say if if I'm
thinking about investing time and therefore money into a more exact model do you will are there
enough features that will look you know visually distinctive do I have a good chance of being able
to build an effective model but then yeah for for a solution where you really want a quantified
precision then taking one of the other approaches or object detection or semantic segmentation
and building a specialist model for the exact thing you're looking for has almost always
been more much more effective great well just as we kind of wind down I appreciate you being
flexible with time here run over a little bit but this has been really interesting are there any
other things that you'd want to leave folks with I just thanks for having me on it's been
fun to discuss these things and if people want to play around with some of these things go check
us out awesome well thank you Kyle thanks so much for taking the time to chat with me about this
stuff all right thank you all right everyone that's our show for today for more information on Kyle
or any of the topics covered in this episode head over to twimmol ai.com slash talk slash 173
as always thanks so much for listening and catch you next time
