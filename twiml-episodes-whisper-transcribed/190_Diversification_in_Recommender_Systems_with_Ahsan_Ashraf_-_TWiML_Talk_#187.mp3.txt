Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. In this episode of our Strada Data Conference series, we're
joined by Essen Ashrough, data scientist at Pinterest. In our conversation, Essen and I discuss
his presentation from the conference, diversification and recommender systems, using topical variety
to increase user satisfaction. We cover the various experiments his team ran to explore
the impact of diversification in user boards. The methodology is team used to incorporate
variety into the Pinterest recommendation system, the metrics they monitored throughout the
process, and how they performed sensitivity and sanity testing.
Before we move on, I'd like to send a huge shout out to our friends at Capital One and
Cloud era for their continued support of the podcast and their sponsorship of this series.
At the NIPPS conference in Montreal in December, Capital One will be co-hosting a workshop
focused on challenges and opportunities for AI and financial services, and the impact
of fairness, explainability, accuracy and privacy. A call for papers is open now through
October 25th. For more information or submissions, visit twimbleai.com slash C1 NIPPS. That's
the letter C, the number one NIPS.
Cloud era is a modern platform for machine learning and analytics optimized for the cloud
that you build and deploy AI solutions at scale efficiently and securely anywhere you want.
In addition, Cloud era Fast Forward Labs expert guidance helps you realize your AI future
faster. To learn more, visit Cloud era's machine learning resource center at cladera.com
slash ML. Now, if you're a fan of this show, you've got to love our sponsors because they
help make it possible. So, please take a look at what they're up to and be sure to let
them know that twimble sends you. And now, onto the show.
I am here in New York City with SN Ashraf. SN is a data scientist with Pinterest. SN
welcome to this week in machine learning and AI. Thanks, Sam. So, you gave a talk yesterday
on the way you do recommendation systems at Pinterest. What was the specific title of
the talk? So, the talk was called diversification and recommender systems and how we can use
content diversity to try and increase user satisfaction. Awesome. Before we get into that,
a little bit about your background, you have a PhD in physics. PhDs in physics are not
unfamiliar to this podcast. There are a lot of physics PhDs in ML and AI. How did you get
from physics to machine learning? Yeah. So, my background, as he said, is in physics.
I used to study condensed matter physics and the physics of complex systems. So, complex
systems are often systems that are kind of hard to define using simple equations and physicists,
as you might know, I really love to put an equation to everything. We like to say, hey,
this is how the world works. I can put that on a t-shirt. So, complex systems are often
systems that have a lot of interacting, a lot of moving parts essentially. I've always
been interested in those kinds of systems. A system like Pinterest is similar to that.
Before that, I also studied a little bit in neural networks and worked in sort of soft
matter physics, which is polymer physics, and anything, any other materials that don't
behave as we might expect. And you also were an insight fellow. I know
Ross and Emmanuel and some of the other folks that have gone through that program. How
you made the transition into machine learning and AI? Yeah, absolutely. So, towards the end
of my PhD, I started realizing that a lot of the work that I was doing was quite relevant
to the stuff that people were doing out in the Bay Area. It was a great way for me to
be able to transition into data science. Awesome. So, what's your focus at Pinterest?
Yeah. So, at Pinterest, we have a sort of small data science team. I worked very closely
with the ranking and blending team, and they essentially build what we call our main
home feed. And so, when you land at Pinterest, you kind of see a set of bins, and that feed
is what we call the home feed. So, what powers that feed is a set of interacting machine
learning technologies. And one of our teams is responsible for kind of the ranking and
the blending of a lot of that content, and I work really closely with those engineers.
Okay. And so, the project that you described in your talk was on introducing diversification
into that feed. What motivated you to start looking at that?
Yeah. So, that's a great question. So, machine learning systems often are a burping system.
Recommender systems try to say, I have this user and app, this item, and how likely is this
user to engage with or to click on or to whatever your system might be with this specific
item. And the interesting thing is that they don't often are at least naive or simple
the recommender systems don't often think about looking at things as a set of bins.
Most recommender systems try to think of stuff as a burping or a burping item basis.
But we obviously as human beings can feel that if, so the example that I gave in my talk
was that personally, I love using Pinterest for street art. And, you know, one of my favorite
street artists is Banksy. And the recommender system can learn that I like Banksy, but what
ends up happening is that rather than just giving me one Banksy pin, it ends up giving me
a hundred of them. And so recommender systems say, hey, if I know what this person likes,
I'm going to give you a thousand of these things. But obviously, we have the notion of
diminishing returns, right? So, this is a pretty standard thing in economics where if
you give me one of something, I might really like it, but by the hundred one, the additive
value that you're giving me by showing me that piece of content is actually pretty low.
And so that's kind of the primary motivation behind thinking about entropy or thinking
about randomness and sort of diverse content diversification within recommender systems.
And to maybe jump to the punchline a little bit and I'll elaborate on why, did you in
fact find that diversification provided some additional lift or engagement?
Yeah, absolutely. So, that's actually, I like to do that as well where I try to give
away the punchline. And so, yeah, the punchline is the diversity makes users happy, right?
And the reason why is that Pinterest specifically is a visual search engine, right? People come
into Pinterest to discover and do what the things that they love. And so they want to be
able to, they want to be introduced to a bunch of content that they can explore and browse
and then kind of dive deep into the things that they really enjoy. And sometimes the things
that you really find, I mean, if you think about the last time someone recommended a good
book to you or a good art show to you, it often happens by chance. And the word for
it is serendipity, right? And so what we want to do is we want to create serendipitous
experiences on Pinterest where people come on and they're able to define those things.
And when there's more diversity, that is more likely to happen.
So what prompted the question was, in fact, another interview that I had with the speaker
at Stratta from Reuters who looked at essentially recommendations, they were trying to determine
the best articles to surface to a user in their infinite scroll redesign. So you go to
an article and when you're finished with that article, you see the next article automatically.
And they looked at a couple of different experiments. One was surfacing similar articles, another
was surfacing dissimilar articles. And the third was surfacing articles, just top world
news articles. And they found that, in fact, in their case, similar articles had the greatest
performance as opposed to dissimilar, which is kind of analogous to the diversification
that you're describing. But I think I guess all this speaks to the fact that it's really
about your users and the way they want to use your site.
Yeah, absolutely. I mean, that's a really interesting point. So the interesting thing about
Reuters, are any news, or most news agencies, I should say, is that they're not personalized
usually, right? Like Reuters is just trying to, and they are not exactly.
So they're trying to give you content. Most of the content that they serve the users will
probably be homogenous. However, for us, if you look at a specific user's home feed,
it's entirely personalized to that specific user. From the language that that user speaks
to the type of content that they've engaged with in the past, to the users that they follow
to all these different things. And therefore, for us, diversification is actually a really
important part of that. And I should also caveat that I don't think that the extreme end
of this, like if you could take it to the, to the, you know, an infinitive, it would
be that everything should be random. And that's not what I'm saying either, right? There's
some sweet spots. So in machine learning, we use the term explore versus exploit. And
the idea is that if you really personalize, if you really say, hey, this user really likes
Banksy Street art, all I'm going to show this user is Banksy Street art, that's like
extreme exploitation. And extreme exploration is randomness. And there's some sweet spot
between those two, which allows for this kind of discovery experience that we want to
try and give our users. And so how did you organize the presentation?
Yeah. So basically, I start off by talking generally about the motivation. And then I
sort of give a few examples of how do we actually measure this diversity. So one of the really
important problems within, like in the past and literature as well as now is we can intuitively
feel that, like I can tell you that this was a Banksy Street art pen. But how do you actually
give that information to a machine learning algorithm as a number or as something that
could be an input to your, to your system? And so I primarily focused like more than half
my talk was trying to focus on how do we measure that? And what are the kinds of tests
that we can do to be able to understand that this measure is actually working for us?
And so these things are things like is my metrics stable, which means that how, as I
give it more information, does it actually kind of converge to a specific value? And how
quickly does it converge? Because that's an important thing as well. Like I don't, I want
to be able to give it, you know, 10 pins or a few pins, pins here are just our basic
unit of content on Pinterest. And it should be able to give me a pretty good value. So
that's stability. The second thing is giving it 10 pins and it's giving you a pretty good
value of what? Of diversity. Okay. Yeah. So you can imagine that with one image, so let's
imagine we have a thousand pins, right? There's some value of diversity that I can give
that set of pins. And so you can, for example, if all those thousand pins were related to
street art, the diversity of that would be pretty low because they're all street art.
If they were entirely random, the diversity should be pretty high. So the number that I
give it should be pretty high. Now, if I sub sample, let's say one pin from that thousand
set of pins, maybe it's randomly happens to be not a street art pin, right? And then now
I take two samples and now I can maybe get a better value of diversity. Now I take three
and so on and so forth. And so how many pins do I need in order to be able to get to that number
pretty quickly? This is what I mean, my stability. Okay. And then the second important aspect of it
is sensitivity, which means that does it behave the way we wanted to behave? So the example here
would be again, like let's say I have a set of pins and a hundred percent of them are
taxi street art, right? And so the diversity value here should be pretty low. Now let's say we
start introducing a completely orthogonal topic into this set of pins. So imagine something like
scooters or cars or whatever. Something that we think is like different. And then
we introduce some of these pins into this system. And we should see this diversity value kind of
go up. When we're at 50, 50 where we have 50 percent like scooters, 50 percent street art,
then this diversity value should be at its max. And then as you keep introducing more and more
scooters into it until we have a hundred percent scooters, then this diversity value should come
down again. And so if this measure doesn't move very much or it doesn't move the way we actually
be expected to behave, then it's not working. And so this kind of a study of trying to understand
if the measure is working is a sensitivity analysis. And so I talk a little bit about this.
One quick question on that. So you said that when you have this 50, 50 distribution of
pin topics, assuming we know how to find those, that this diversity measure should be at its max,
that's like one, that's a design decision, right? You could also argue that the diversity
metric should be at its max when every pin is of a different topic. Yeah, absolutely. Yeah,
so what I mean by saying max is max in this spectrum of mixing these two topics. So you're
absolutely right that if I introduced a third topic into this, now that value should be even higher
than if I only have two topics. So that's actually another analysis that I didn't talk about in my
talk, but we did do, which is that let's say I have 50, 50, two topics, and now we have 30, 30,
33 topics. And then let's say we keep going with like 25, 25, 25, like four topics and so on.
We actually keep seeing that this value increases until some max point where it flatens out.
And so that's also another aspect of sensitivity, which we also checked.
Okay. Yeah, but that's a great, that's a great question.
Okay. And so that's a word. Yeah. And so the last thing is basically, in physics, we just call
the sanity checking, right? So we want to make sure that this metric is sensible for the system
that we have now constructed. So in Pinterest, we have something called boards. Boards are collections
of bins or collections of these units of images that people that our users have kind of put together.
And these are thematically often quite similar. So users create boards, for example, for
you know, flowers for their wedding. And so all of the, depends on that, on that board,
will be like purple flowers or something of that sort. People will create a board for scooters.
People, I have a board, for example, for street art, you know, and so on and so forth.
And we, we generally know that these boards should be thematically quite consistent and quite
similar. So one of the things that I did was I sampled lots of users boards and tried to give
those set of bins a value for diversity. And that creates a distribution. And this distribution now
should be lower than the lower in diversity from the distribution, if I just randomly sample bins
from our corpus, from our system. And so we basically created multiple distributions like this,
just to make sure that the distribution for boards is actually like the mean and everything is
much lower than the distribution for random bins. And then the distribution for sessions,
or when users come on, what does this see on Pinterest? It's actually kind of overlapping
between those two things, because sometimes users come on to see pretty random content,
but sometimes users may come on just to see something very specific. And so we see that the sessions,
not only is the, the mean kind of that distribution making sense, but the standard deviations a
lot bigger too. And so we know that it's more spread out. And that generally kind of makes sense as
well. And so given those three checks of stability, sensitivity and sanity, we can then become
quite confident that this measure is something that we can use. We're able to look for boards that
were titled miscellaneous or random and compare them to compare them in diversity to other types of
boards. Yeah, that's a good question. I tried doing some stuff where I looked at specific topics and
saw that within, within boards, topics of, you know, one, boards of one topic can be different in
diversity than boards of a different topic. Okay. I think the meaning consistently. Yeah, exactly.
Or in general, like if you create those distributions, they often are non-overlapping,
one being greater than the other. I can't remember right now what a specific example, but like one
of the, one of the things I think was that crockpot recipes, which much lower in diversity,
than a board title, it's like wedding or a board title, you know, something like that. And I guess
the thing is that wedding is a much broader category like you could have, you know, anything from
dresses to flowers to all sorts of stuff in wedding. Whereas if you're, if you have crockpot
recipes, they're quite specific. I guess. So things like that we definitely saw.
But that's a good point about boards type being titled random. And if they were, in fact,
random or not. Yeah. Yeah. You may be coming to this, but I'm curious how you got at what the
pin's content was. Did you, you know, we're looking at metadata? Are we doing deep learning on
the images? Yeah. So that's a great question. So when we, when users put a pin onto our
system, they generally put it on a specific board, which has a title and a description and stuff.
And, and then the pin itself has a description to where users can write some stuff about,
about the images that they've uploaded. And then often, pins actually have a web page is associated
with the content, right? And so when you, when you try to, what, when we have all this information
about the pin, we can actually associate certain words with these pins. And so the example that I
gave was, if you have a street art pin, the words that are associated with this are things like art
and graffiti or a bank see and so on and so forth. And we call these words associated with
pins pin annotations. And so that's kind of step one, which is that try to get to the key topics
of the pin. And we do this by, by, by this kind of tokenization off of these words that we can
get from the descriptions, the board titles, the web age of sounds of worth. Once we have these
annotations, we can actually use, so I walked in my, in my talk, I walk through several measures of
diversity that they, that you can use. One of the problems with words, though, is that there's
synonyms. And so when, when, when you say like art versus street art versus graffiti or something
of that sort, these have overlapping meanings. And because of that, we actually couldn't get very
stable measures of diversity using purely words, using purely annotations. So there's a lot of
work on lexical diversity in linguistics literature. And then so you can use some of those
standard techniques. We also tried using some techniques like entropy or the gene coefficient,
which are from like physics or economics. And then, but at the core of it, the issue is that
annotations or words can often have synonyms. And there's, there's the inherent instability in that.
And so what we ended up doing was creating these embeddings using a matrix factorization. And so
what embeddings try to do is that they try to map these words onto vectors, right? And not just
any vectors, but something quite special vectors in the sense that you can add and subtract these
vectors and they still have meaning. So the very standard example that people give is that if you
have the vector for king, you can subtract man and add a woman and then get queen out of that.
And the fact that that works actually still blows my mind because it's quite incredible that we
can do that in such a such a such a way that that we can still interpret it makes a lot of sense.
And so we actually do exactly that where we take these annotations and we know that annotations
on a specific border related to each other. And so we use that matrix of annotations as columns
and boards as rows and factorize that to be able to get annotation embeddings. And so once we
have these annotation embeddings and we know that the the each pin has associated like let's say we
take the top 10 annotations for each pin. Now, because of the fact that these these embeddings
can be added and subtracted and stuff, we can actually just take the average of it and that
average still has meaning. And that embedding we call the pin embedding. And so once we have that
pin embedding, now we can actually do simple things like calculate the similarity or dissimilarity
in this case of different pins and be able to give that kind of get that embedding diversity value
out of that. Is there a single pin embedding at Pinterest or do you have multiple different types
of pin embeddings that use like is that a pin annotation embedding and there are other types of
pin embeddings? Yeah, that's a great question. I mean, we definitely have a lot of embeddings
and we're trying to converge to a single unified embedding which encapsulates everything. And so a
lot of the stuff that I've talked about so far has been about the words that are on the pin,
but you can imagine that there's there's a visual embedding right embedding that's purely based on
the image itself. And we definitely have a team that entirely focuses in trying to create a
visual embedding. And we actually recently launched something which tries to combine the two
and there's a block post about that that I can link at some point.
So you talk for the properties of the metric and then you're using embeddings to develop the
metric. What else did you talk about? Or what was next in your talk? I mean, so the final step
obviously is that we have the problem, we've kind of motivated it. We have sort of the solution
which is how do we measure the diversity? But then the final step is how do we actually implement
this into our system? How do we use it? And actually before you jump into that, you mentioned
a couple of alternatives, some lexical work from the linguistic space and entropy and things from
physics. How far did you go down those paths? What did you kind of look down those paths and say,
hey, we don't want to go down there? Well, we definitely, we went far enough where we were able to
understand what the pros and cons of those approaches are and whether it works for us.
And when we found that something doesn't work for us, we kind of moved on from there. And so
the very first example, you know, there's something called the type token ratio, which just takes
the unique number of annotations and divides the total. And so this is something that's like very
easy to understand. And that's, I think, there are, that is the pro of some of these approaches,
which is that, you know, interpredability is in a really important aspect in machine learning,
which people sometimes ignore these days. But that does have its value and merits. And so,
I think the point is being that like, you kind of have to try it out, see what the pros and cons
are, see if it, you know, matches the three conditions that I mentioned of like stability, sensitivity,
and sanity. And if it doesn't, then you kind of move on to the next approach and stop where you
find something that works. So this type token, for example, pretty simple, but it really trips up
with the synonyms. Exactly. Yeah. And so you can imagine that if I have some two pins that are
there that are quite different, but then, you know, might have overlapping annotations, it'll, it just
won't understand. Or if they're quite similar, like imagine slow cooker and crock bot, which are
two separate words, but, you know, I have quite the same meaning. It'll actually think that
they're very different. And so it just doesn't behave the way that we wanted to behave. Right.
Right. So the implementation of the system, for where some of the big challenges there. Yeah.
So before jumping into the implementation, I'll just quickly describe what our recommendations
stack looks like. Okay. And so we start off with, you know, our pin corpus, which is sort of
billions and billions of pins. We narrow that down to a state called Canada generation, where we
have a bunch of different ways of generating candidates for, you know, recommendations. And so
some of these involve purely content to content type recommendations, but some of them involve
more like collaborative filtering type approaches, and so on and so forth. Like we have a lot of
these different approaches. And at this stage, we kind of get about off the order of thousands of
pins for users. The next step is pure ranking. Meaning for a given user's home feed, you'll get
thousands of candidate pins. Exactly. Okay. So when a user comes onto Pinterest, we're doing all
of this stuff in the background before they even land on the on the home feed. So the next
step is ranking where we just blend all of the, or put all of these things together and try to say,
well, how do they actually perform against each other? What is the probability that this user
will in fact engage with this specific thing or not? And then the final step is actually blending.
And at this stage, where we're taking content from the users followers, from the users, from the
the user that the viewer follows, from the topics that they're interested in, and finally from their
recommendations, which are these machine learning systems. And we're putting them all together
into a single feed or creating a chunk as we call it. And so at this final step, you can imagine
that there's a lot more control in trying to tune this explore exploit balance of, do we want to
try and show users 100% things that we know that they will engage with? Or do we want to sneak in
some more exploratory stuff? And so this is the stage at where we started adding this diversification.
And the way that we essentially did it was that we would calculate this embedding
dissimilarity metric for all the pins that we were about to show the user. And we said,
is it too similar? Is it the first few pins that this user's going to see? Are they all very,
very similar? And if they are, then we push some of the pins down and we actually introduce more
sort of randomness and more entropy into the system. Do you consider this at all a personalization
parameter? Like user A wants more diversification in their field than user B? Yeah, that's a great
question. So we did, we are also trying other approaches of introducing diversity. And so one of
the ways of doing that is actually adding it into our ranking function essentially. And these
things are often called submodular functions. And so there is an approach of doing something like
that as well. We specifically found that this approach of just kind of looking at the final product
and then seeing if it's too similar and just, you know, penalizing at that step works slightly
better than actually trying to do it the other way. That's not to say that it won't work for anyone
else. I think it's just a matter of the system and how it behaves. But that's definitely one of
the options as well. So the pin embeddings did that pre-exist this particular project? Was it
already available for you? Did you have to create that as part of building out this system?
So we actually had to create new pin embeddings for this. And the reason why often is again,
the characteristics of the embeddings that you want can often differ from different use cases,
from use case to use case. And so the given pin embedding, the embeddings that we already had
just didn't behave the way that we wanted them to behave. And this is again the thing of
any, for a lot of these systems, the technologies are often already out there. You kind of just have
to pick and choose like what works for me and what is the problem that I'm really trying to solve.
At the end of the day, you know, it is great to try and build at something new, but at the same time,
if something already works, you just use that. And in this case, we just realized that, you know,
we had to create something new to be able to solve this problem.
So you haven't talked much about kind of the operating characteristics,
scalability requirements. I mean, clearly, you talked about how many pins there are and
the number of candidates did the solution that you, the direction you started going,
did it just kind of work fine or did it have to be massaged in order to get it to perform
it to meet the requirements of the site? Yeah, that's actually a great question. So when we think
about the pros and cons between, let's say like the annotation diversity that I talked about in
the embedding diversity, that was actually a big thing that we took into account when deciding
on which approach to choose. And so the reason why, and so in this case, embedding diversity actually
does, it is better in terms of scalability. And the reason why is because embeddings are numbers.
And numbers are easier to, you know, store and add and subtract and do all these things to,
instead of actual words, which are the annotations. So each pin has, you know,
hundreds of annotations associated with it and we're trying to find what's the overlap between
two things. You literally have to go through every single one and kind of, you know, see which
one's overlap. And then that's actually a very expensive process if you want to do it at scale.
And so for embeddings, which are just vectors, we can actually add and subtract them fairly easily.
And there's libraries that do matrix factorization and stuff like that pretty, pretty fairly.
Having said that, matrix factorization is actually something that's very hard to do at scale.
And in that case, what comes into play is sampling, but sampling in the correct way,
in the sense of like, sampling in a way that doesn't introduce biases into your system.
And that in itself is a topic that's really hard to do. We ended up using reservoir sampling.
But can you elaborate on this whole area? What are you using sampling in place of
creating embeddings for every pin? Is that what you're suggesting?
No, so what I'm, I guess, if you come back to the matrix that we're factorizing, we're essentially
looking at annotations as the columns and the rows being boards here. And so the reason why we're
using boards is because we think boards are more thematically similar. And if we do it on boards,
we're able to actually get at the meaning of that annotation a little bit better,
or the vector that we're going to associate with this annotation a little bit better.
And so the sampling that I'm talking about is actually at that board level,
which is that how many boards do we sample? What are the topics that we sample these boards from?
And you can imagine that if I just do it randomly, I might create biases for
things that people create, a lot of boards for versus things that people don't create,
all right, and so we have to be quite smart about that when we're when we're trying to do this.
Makes sense. And how did you approach that?
Yeah, so I mean, obviously we started simple, we started with the, you know, trying to do it the
very simple way where we just did a random sample, recognize what is a granularity at which we
want to be able to find differences between these boards, and then try to skew it in the opposite
way of that. So if we have a lot of boards from a specific piece of content, we try to sample less
from there and then we over sample places where we don't have enough content.
If you have a lot of boards from, meaning a lot of boards that a specific piece of content belongs to,
a lot of boards that are thematically quite similar. And so the idea here is that let's say,
we have a lot of users creating wedding boards. And so it could be that if I just sample randomly,
I end up getting just a lot of wedding boards. But, you know, we also want to make sure that we have
boards for scooters or boards for street art and all this other stuff. And then if there's not
enough boards that are just being created for that type of content, we actually have to over sample
there and under sample boards that are being created just, just in volume a lot more.
Is there, would it be a reasonable approach to look at creating an embedding space for boards
and then using that embedding space for boards to try to determine diverse boards and then using
that to feed into your pin embedding? Yeah, I mean, so at Pinterest we are actually by lucky where
we have this really nice graph of, you know, a user creating a board and that board having a bunch
of pins on it and then often these pins overlap with other boards. The most smallest unit in this
obviously is the pin. And so we can roll up to the boards if we want. But, but you're right in
the sense that we could we could do it at the board level and then roll it up to the user or something
of that sort. But given that we have the pin embeddings, we can actually just roll it up to the user,
roll it up to the board and then be able to get that value for the board as well. And so it's often
good to be able to start at the most basic unit and kind of roll it up if needed because then you
have the granularity, right? Yeah, I guess that makes sense. And so implementation.
Yeah, so as I said, at the blending level, we're able to kind of tune this Explorer exploit balance.
And the idea here simply is that we take this out of pins and what's basically known as first
page optimization. And so the idea is that one of the things that we'd found with these embeddings
is that the top of user's feeds are often the least diverse in the sense that
recommender systems often try to shove everything that they think that the user will really engage with
right at the top of the feed, which kind of makes sense because again, recommender systems are
trying to do this at a per item or a per pin basis. They're not really thinking too much about
like what does the page look like? How does this pin look like in association with the pins that
it's surrounded by? And so that's the reason why this is actually quite effective at that
blender level because now we have a set of pins. Now we have this page that before we show the
user, we can kind of take a look at and be like, hey, does this kind of look good together?
And so the lower diversity of the top of the feed actually often makes users feel that,
you know, this is all the stuff that I'm going to see. But if you actually scroll a little bit lower,
there is a lot of content that's more diverse. We're just not showing it right at the top.
And so if you actually push some of that content further up the feed, you know, you can kind of
users are able to be exposed to more diversity right at the top, which kind of encourages them to
explore more. And so when you talk about this, the focus being this first page diversity, does that
mean that after the first page of recommendations, you're not going through this same process and
assuming that the recommended is already going to produce a more diverse feed? So we do go through
the process even after the first page, but it becomes more crucial right at the top right because
that's where the issue that we found was. We also found that the issue was actually more severe in
users that are the most active. So the more active that you are, the more we know about you and the
more we're able to kind of overoptimized on the interests that we know you have as opposed to
the interests that you may or may not have. Right. And so there's a bunch of things here that
the quantitative measure of diversity allowed us to do. And so the first thing as I mentioned is
the fact that we understood and recognized that the problem was more severe at the top of the
feed versus lower down. The problem was more severe, for example, for users that was the most active.
And then we can also do things like try to understand, well, is this problem more severe for
users that speak a different language or users that there's a lot of other properties of this
that we can now kind of understand a little bit more and tackle those problems head on as opposed
to just kind of thinking about diversity as a more qualitative problem. We can try to focus on
specific quantitative approaches of doing that. All of this is based on understanding what
engagement means for these pins. And there are kind of the obvious things like
likes, thumbs ups, those kinds of things, comments. But when thinking about the board and diversity
on the board, you know, a lot of times it's just, you know, what you see when the board comes up
and that having a qualitative, making a qualitative impression on the user.
Did you do user studies or things like that outside of just kind of the pure click engagement metrics?
Yeah, absolutely. So this issue actually was, so we have a entire team that's devoted to try
and talk to users. There is do a lot of qualitative research to be able to understand what are the
problems that users are facing. And then we obviously listened to a lot of the comments and stuff
that users give us as well. And so this problem actually we studied quite in depth where we got a
bunch of users to come in from varying backgrounds and we tried to understand what are the problems
that you're facing? How, you know, and so that in my talk, I actually give a few quotes from some
of our users that specifically talk about this problem where users say, oh, I see the same stuff
all the time or I want to more diversity and I want to more type new type of new content or new
topics and, you know, I want Pinterest to kind of broaden the types of things that it shows me and
stuff like that. And so we kind of approach it from a bunch of different ways like we, you know,
we have very like in-depth studies where we sit down and talk to users for an hour. We have,
you know, a way for users to write to us and do all sorts of stuff like that. And then we have
user surveys, which are sort of quick responses. And then finally, we have the experiments that we
run where we try something out. And in this case, we tried this specific approach that I described
at an actual experiment. And then we try to see, do one users come back more often? And two,
when they do come back, do they spend more time? And do they actually like feel like they're engaging
more? And so we saw a lot of these approaches kind of converging on the same thing, which kind
of tells us that we're doing something right. So another way to come at that is that for this
type of problem, you're optimizing more on site engagement and then cart, then pin engagement,
per se. Is that fair? Yeah, I think that's fair. I think often those two things correlate
where site engagement often correlates with how often people engage more with content.
But, you know, users come on for a bunch of various reasons. And users have different ways
of engaging. So some users may come in and, you know, check a lot or what we call repending,
which is that they take pins that they see and put them on their own boards. But then some
users just come on to explore, right? Like, users are coming on to like, you know, design things and
actually take actions on those things. But they're also just coming to, you know, for entertainment.
They just want to see a bunch of content. They want to try and explore new new ideas or explore
new aspects of themselves. And we kind of want to we don't want to over optimize on one specific
type of user base. We try want to try and do it more holistically. And site engagement is a good
proxy for that. So you've inserted this into your pipeline. You've got it up and running on the
site. You didn't done some experiments. You know, we know that you found that increased diversity
worked. But, you know, how well, how exactly did you measure that? Yeah. So finally we did an
experiment. We actually did a bunch of series of experiments. And the final one that we decided
to launch was was one where, as I mentioned, a users are coming back more often to the site.
And so that's one of that that basically tells us inherently that users did find value when the
when they came once and therefore they've decided to come back. And that's a really important
measure for us. And then the second thing is when they do come on, do do they engage more to
the reap and more do they spend more time. And so we actually saw that we increased time spent
by about 1%, which is quite huge. And then we were also able to increase the number of pin
impressions that that users were seeing. So that just meant that every time they came in,
they actually scrolled deeper and had longer and hopefully more meaningful sessions.
And that was finally our sort of proxy for saying that this is this is something that's actually
providing the idea to users. But to kind of start to wrap up any, what were the key takeaways
that you left the audience with? Yeah. So sort of the main thing that I think that this shows is
that measuring content diversity, a measuring diversity in any recommender system is incredibly
important. And the reason why it's important is because it's not just a tack on that you can
put right at the end of your system. It's actually something that's inherently fixing a pretty
common flaw that recommender systems have, which is that recommendations are often per bin or per
item. And they're not taking into account things like, you know, diminishing returns or how do people
look at something when they see lots of stuff at the same page, which is how we as human beings
actually interact with the content. We don't see things like, hey, look at this one thing and
then looks at something and so on and so forth. We actually often see things in a holistic view.
So it's incredibly important. It's non-trivial in the sense that it has, you can't just use the
first thing that comes to mind. You often have to do a lot of analysis and a lot of like kind of
deep studies in terms of stability, sensitivity, and things like that to be able to understand
is this measure right for the system that I'm working with. And then finally, it's important,
it's non-trivial. And then when it's actually done well, it can have a lot of real user impact.
It can make your users much happier. And this is something that we finally showed through these
experiments and this implementation that we had. But the final takeaway just is that measuring
diversity in recommender systems is actually important, non-trivial, and can have real user impact.
Awesome. Awesome. Well, as in, thank you so much for taking the time to share this with us.
Yeah, thank you so much. All right, everyone, that's our show for today.
For more information on Essen or any of the topics covered in this show,
visit twimlai.com slash talk slash 187. For more information on the entire
Stratidata podcast series, visit twimlai.com slash strata and y 2018.
Thanks again to our sponsors Capital One and Cladera for their sponsorship of this series.
As always, thanks so much for listening and catch you next time.
