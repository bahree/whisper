1
00:00:00,000 --> 00:00:04,960
The conversation you're about to hear was recorded live at Twomokon AI

2
00:00:04,960 --> 00:00:11,720
platforms. For more coverage of Twomokon, visit Twomokon.com-news or follow us

3
00:00:11,720 --> 00:00:17,680
on Twitter at Twomokon AI. But first, a word from our sponsor.

4
00:00:17,680 --> 00:00:23,840
Thanks to our friends at SIGOPT for being a founding sponsor of Twomokon AI

5
00:00:23,840 --> 00:00:29,120
platforms. SIGOPT then bites you to watch CEO Scott Clark's upcoming webinar

6
00:00:29,120 --> 00:00:33,600
outlining the critical capabilities customers prioritize when building

7
00:00:33,600 --> 00:00:38,160
machine learning platforms. He'll draw on experiences working with algorithmic

8
00:00:38,160 --> 00:00:41,840
trading firms that represent over 300 billion dollars in assets under

9
00:00:41,840 --> 00:00:47,160
management and enterprises with over 500 billion in market capitalization to

10
00:00:47,160 --> 00:00:53,280
summarize these trade-offs. Head over to twomolai.com-slash-sIGOPT to register.

11
00:00:53,280 --> 00:01:04,400
Our next guest is Deepak Agrawal. Deepak is vice-president of engineering at

12
00:01:04,400 --> 00:01:10,560
LinkedIn and he's particularly passionate about the connection between the

13
00:01:10,560 --> 00:01:16,200
organization's investment in machine learning and AI and the value that it

14
00:01:16,200 --> 00:01:19,600
creates. And we're going to explore that in our chat.

15
00:01:19,600 --> 00:01:24,960
Let's get started by, I think everybody knows LinkedIn. It's not something

16
00:01:24,960 --> 00:01:27,840
that we need to spend a lot of time explaining. How many of you don't have an

17
00:01:27,840 --> 00:01:35,040
account on LinkedIn? Okay, everybody, market saturation.

18
00:01:35,040 --> 00:01:39,280
But let's maybe get started by talking about some other ways that LinkedIn is

19
00:01:39,280 --> 00:01:44,400
using machine learning. Yeah, so we often say that LinkedIn machine learning is

20
00:01:44,400 --> 00:01:49,040
like oxygen, right? So everything we do has machine learning built

21
00:01:49,040 --> 00:01:53,040
inside it. Like if you go to LinkedIn, the first thing you want to do when you

22
00:01:53,040 --> 00:01:57,840
join LinkedIn is get connected to people who can help you. Now, how do we do

23
00:01:57,840 --> 00:02:00,640
that? We recommend you people that you can connect to. That's all powered

24
00:02:00,640 --> 00:02:04,720
through machine learning. Once you get connected to people, then you start

25
00:02:04,720 --> 00:02:09,280
consuming content that they produce on your news feed. And you know, there is

26
00:02:09,280 --> 00:02:13,440
information overload on the feed. You can see so many content. So what kind of

27
00:02:13,440 --> 00:02:17,040
content do you want to see? And when do you want to see? Like, for instance, if

28
00:02:17,040 --> 00:02:21,840
you're looking for a job, you want to see job recommendations. But if you're

29
00:02:21,840 --> 00:02:24,720
not looking for a job, if you're very happy and you want to learn more deep

30
00:02:24,720 --> 00:02:29,680
learning and if you are connected to Andrew and he publishes something, then you

31
00:02:29,680 --> 00:02:34,000
want to see that on your feed. So you need algorithms to scale this process.

32
00:02:34,000 --> 00:02:38,240
Again, that's all powered through machine learning. If you are a market

33
00:02:38,240 --> 00:02:42,800
here and you want to target the right audience, the entire advertising

34
00:02:42,800 --> 00:02:46,240
ecosystem, we all know works through machine learning. If you're recommending

35
00:02:46,240 --> 00:02:50,960
jobs that you can, that, you know, even if you're not looking for a job, we

36
00:02:50,960 --> 00:02:54,320
still recommend you jobs because there is always a better opportunity out

37
00:02:54,320 --> 00:02:58,160
there for all of you. That's all through machine learning. If you're a recruiter

38
00:02:58,160 --> 00:03:00,560
and trying to source candidate, it's all through machine learning. If you're a

39
00:03:00,560 --> 00:03:04,240
salesperson trying to close a deal, who are the decision makers? How do you

40
00:03:04,240 --> 00:03:08,480
reach to a decision maker? So everything we do on LinkedIn, product, whatever

41
00:03:08,480 --> 00:03:12,400
you see on the app, it's all powered through machine learning. And you know,

42
00:03:12,400 --> 00:03:15,280
finally, you know, this is something that goes behind the scene. We have to keep

43
00:03:15,280 --> 00:03:18,560
the site safe, right? There are, you know, there are a lot of bad actors out

44
00:03:18,560 --> 00:03:22,960
there producing content that should not even reach you. You know, there are

45
00:03:22,960 --> 00:03:26,560
people who create fake profiles. I mean, I've seen a lot of fake profiles of

46
00:03:26,560 --> 00:03:30,960
famous people. And that's not a good thing, right? So just to keep the ecosystem

47
00:03:30,960 --> 00:03:35,600
clean, that's again, machine learning plays a very important role in that as well.

48
00:03:35,600 --> 00:03:39,520
So everything we do at LinkedIn is powered through machine learning. In fact,

49
00:03:39,520 --> 00:03:43,360
when we create a new product idea, in addition to product managers,

50
00:03:43,360 --> 00:03:46,960
engineering managers, designers, we also have a machine learning force

51
00:03:46,960 --> 00:03:50,960
and sitting there, right? When we are designing the product, because we believe

52
00:03:50,960 --> 00:03:56,560
that's the right way to do things. I mean, you know, the UI that you create,

53
00:03:56,560 --> 00:04:01,600
if that can actually build some important feedback loop, that can play a big

54
00:04:01,600 --> 00:04:04,320
difference. I mean, Andrew was talking about collecting label data. Well,

55
00:04:04,320 --> 00:04:07,920
how do you actually ensure that you collect the right label data? We have to

56
00:04:07,920 --> 00:04:11,280
actually start working on it at the design phase. It's too late if you don't

57
00:04:11,280 --> 00:04:14,800
pay attention to it at that stage. Then you have to build very complicated

58
00:04:14,800 --> 00:04:18,320
model that essentially do guessing, right? So you don't have to guess if you can

59
00:04:18,320 --> 00:04:22,080
actually get the right data. And so that's why it's very important to start that

60
00:04:22,080 --> 00:04:27,440
process from the very beginning of machine learning process of product

61
00:04:27,440 --> 00:04:33,360
process. One of the things that has always fascinated me in my conversations with

62
00:04:33,360 --> 00:04:39,120
folks at LinkedIn is we think of LinkedIn relative to a more traditional

63
00:04:39,120 --> 00:04:42,560
enterprise as kind of a digital native company born on the web.

64
00:04:42,560 --> 00:04:49,520
You know, the product is web. But in a lot of ways, the company has evolved

65
00:04:49,520 --> 00:04:53,760
similarly. You know, it's initial investments in machine learning and

66
00:04:53,760 --> 00:04:57,680
what the way it's supporting machine learning today are very different. Can

67
00:04:57,680 --> 00:05:01,360
you talk a little bit about the journey at LinkedIn and how

68
00:05:01,360 --> 00:05:06,000
ML and AI has evolved over the years? Yeah, so LinkedIn was always a data

69
00:05:06,000 --> 00:05:10,240
first company, right? Like if you all remember, the word data science was

70
00:05:10,240 --> 00:05:14,960
coined by DJ Patil at LinkedIn. So we were always very savvy about data. We

71
00:05:14,960 --> 00:05:21,040
knew our businesses all about the data, the unique data we possess, right? So

72
00:05:21,040 --> 00:05:23,440
we were always doing data science. We were always doing data product

73
00:05:23,440 --> 00:05:28,000
innovation. We also started doing machine learning very early on. Like in 2007,

74
00:05:28,000 --> 00:05:32,640
the first machine learning product, real product was the people recommendations,

75
00:05:32,640 --> 00:05:36,560
right? So in those days, we would compute, we will have simple machine learning

76
00:05:36,560 --> 00:05:40,240
models, of course. You know, so we'll have a simple model, the features of

77
00:05:40,240 --> 00:05:42,960
these models. There were a handful of features, but they're very carefully

78
00:05:42,960 --> 00:05:47,120
tweaked based on intuition. Once we have that model, then productionizing these

79
00:05:47,120 --> 00:05:51,920
models at that scale was still very difficult. So we will actually build

80
00:05:51,920 --> 00:05:56,800
Hadoop systems that will do the ranking and scoring offline, right? So because

81
00:05:56,800 --> 00:06:00,800
online was not very well developed. And then we'll run these processes

82
00:06:00,800 --> 00:06:05,360
every day, right? So search was another system that we actually developed

83
00:06:05,360 --> 00:06:09,680
very early on that used machine learning. Fast forward 2012, we got more

84
00:06:09,680 --> 00:06:13,280
sophisticated, right? So the first sophistication we added in terms of

85
00:06:13,280 --> 00:06:16,640
machine learning was in our advertising system, right? So the advertising

86
00:06:16,640 --> 00:06:19,600
system, we most of our other recommended systems are based on simple

87
00:06:19,600 --> 00:06:23,360
collaborative filtering idea at those times. You know, people who bought this,

88
00:06:23,360 --> 00:06:26,800
also bought this. But then advertising was the first place where we added a lot

89
00:06:26,800 --> 00:06:30,960
of sophistication. We added, we build near real-time systems, we build online

90
00:06:30,960 --> 00:06:35,520
systems that can score things at runtime, more complex models,

91
00:06:35,520 --> 00:06:40,320
and you know, encouraged by the success we got there, we then

92
00:06:40,320 --> 00:06:43,360
attacked the feed problem, the news feed problem. For those of you who have been

93
00:06:43,360 --> 00:06:47,440
using LinkedIn for a long time, I'm sure most of you will tell me today the

94
00:06:47,440 --> 00:06:50,960
news feed is much better than what it was five years ago. That's all due to

95
00:06:50,960 --> 00:06:54,160
machine learning, right? I mean, so a lot of work happened to kind of add

96
00:06:54,160 --> 00:06:58,000
sophistication to the news feed algorithm. And once we got success in these

97
00:06:58,000 --> 00:07:01,760
two big applications, then we started thinking of how do we generalize it across

98
00:07:01,760 --> 00:07:06,400
the board, right? Why, why just advertising? Why is this news feed? Why can't we build

99
00:07:06,400 --> 00:07:10,160
a platform that can actually generalize it to everywhere? And that's what we

100
00:07:10,160 --> 00:07:13,200
have been doing for the last few years. And so we have a program at LinkedIn

101
00:07:13,200 --> 00:07:16,560
called ProML, Productive Machine Learning. And again, I think a lot of

102
00:07:16,560 --> 00:07:21,040
companies have a platform. But I mean, one unique thing about our platform is,

103
00:07:21,040 --> 00:07:25,120
you know, we are building a platform with a very strong opinion, right? So

104
00:07:25,120 --> 00:07:29,280
you can build a machine learning platform that can cater to a lot of

105
00:07:29,280 --> 00:07:32,080
tail users, right? So if you're a cloud company, you're going to build a

106
00:07:32,080 --> 00:07:35,760
machine learning platform that can cater to the needs of a diverse set of

107
00:07:35,760 --> 00:07:40,240
customers. That's not our goal, right? We know that our ROI is going to come from

108
00:07:40,240 --> 00:07:44,160
a few big applications. And the platform we have built is really

109
00:07:44,160 --> 00:07:47,520
suitable more for that, right? So large scale recommender systems,

110
00:07:47,520 --> 00:07:51,680
large scale search systems, large scale classification problems. These are the

111
00:07:51,680 --> 00:07:56,160
problems that we face. And our platform is really geared towards that, right? So

112
00:07:56,160 --> 00:08:00,960
we also know that we have reached a point where without adding more sophistication

113
00:08:00,960 --> 00:08:04,080
to our systems, you're not going to get the ROI that we used to get.

114
00:08:04,080 --> 00:08:06,960
So I give you an example, like, you know, two years ago,

115
00:08:06,960 --> 00:08:11,440
for our job recommendations system, we revamped the model. We kind of moved

116
00:08:11,440 --> 00:08:16,640
away from a simple linear model to something more complex, involving deep learning.

117
00:08:16,640 --> 00:08:19,520
And, you know, involving something that is a homegrown technology called

118
00:08:19,520 --> 00:08:22,800
generalized mixed model. So I mean, I'm not going to technical detail, but these

119
00:08:22,800 --> 00:08:26,000
are very high dimensional model. We had a model with GMM.

120
00:08:26,000 --> 00:08:30,240
Yeah, exactly. This is the old technique in statistics, known since the 70s.

121
00:08:30,240 --> 00:08:35,680
And in statistics, it was applied to application that 500 patients.

122
00:08:35,680 --> 00:08:39,440
Now, you know, those 500 patients become half a billion patients. And then suddenly

123
00:08:39,440 --> 00:08:44,320
the explosion in the number of parameters in complexity increases a lot, right?

124
00:08:44,320 --> 00:08:50,240
So we applied that and, you know, we found a 30% improvement in result. That was stunning.

125
00:08:50,240 --> 00:08:54,320
And so we were all very happy. But for the next six months, nothing happened.

126
00:08:54,320 --> 00:08:56,800
And then like, it was very surprising. Like, okay, well,

127
00:08:57,760 --> 00:09:01,920
did the engineers all go to Hawaii or what's happening? Like, why is nothing moving?

128
00:09:02,480 --> 00:09:06,160
And what we realized is when we introduced that complexity,

129
00:09:06,880 --> 00:09:11,200
the tooling did not keep pace with that, right? So it became very hard for the subsequent

130
00:09:11,200 --> 00:09:15,280
engineers to kind of iterate on this model, because we didn't build the appropriate tooling that

131
00:09:15,280 --> 00:09:19,040
will enable them to kind of iterate. So that was the realization.

132
00:09:19,040 --> 00:09:23,120
And I know this is not going to work as we actually start introducing more sophistication,

133
00:09:23,120 --> 00:09:28,080
the industrial process will only work if the engineers are still productive.

134
00:09:28,080 --> 00:09:33,200
And in order to improve productivity, when you add more complexity, especially for such large

135
00:09:33,200 --> 00:09:36,960
skill distributed systems, if you really want them to run efficiently, if you want them to run

136
00:09:36,960 --> 00:09:42,720
in a reliable fashion, you have to make sure that the tooling and infrastructure can keep pace

137
00:09:42,720 --> 00:09:47,280
with the innovation that we are doing. And so that was really the impetus of this project that

138
00:09:47,280 --> 00:09:51,040
we kind of run called ProML. And we actually run it very rigorously. We are not just building

139
00:09:51,040 --> 00:09:56,400
platform components. We actually measured the success of that. So every week, we measured the

140
00:09:56,400 --> 00:10:02,800
number of successful experiments we have run. So there are a lot of experiments, our engineers run,

141
00:10:02,800 --> 00:10:07,040
but we only track the number of successful experiments, right? Because otherwise you can start

142
00:10:07,040 --> 00:10:11,200
cheating, right? Like someone can just go parameters sweep on the grid and say, okay, I did a

143
00:10:11,200 --> 00:10:15,440
parameter sweep of 100 different values. And so I ran 100 experiments. I don't care. I mean,

144
00:10:15,440 --> 00:10:20,160
you know, you can run 100 experiments or two experiments. Did how many of them succeeded, right? So

145
00:10:20,160 --> 00:10:25,440
that's our metric. And we have seen like more than 30% improvement in the number of successful

146
00:10:25,440 --> 00:10:32,000
experiment that we run on the site after introducing this program. We've still not done. There is still

147
00:10:32,000 --> 00:10:36,880
a long way to go, but you know, this has been really useful for us. It has kind of also bought

148
00:10:36,880 --> 00:10:40,960
the teams together. So earlier, you know, if you don't have a standardized way of doing things,

149
00:10:40,960 --> 00:10:45,520
no matter how hard you try, the culture in the feed team would be different from the culture in

150
00:10:45,520 --> 00:10:49,440
the jobs team, right? And that's not good, right? I mean, we don't want to create different

151
00:10:49,440 --> 00:10:54,160
cultures in the same company. In fact, we want, given that we are a centralized organization,

152
00:10:54,160 --> 00:10:59,040
we want people to flow from one area to the other, right? So you did your tour of duty on the

153
00:10:59,040 --> 00:11:03,120
feed and you should just go to the jobs team and learn about the jobs product. And you should be

154
00:11:03,120 --> 00:11:08,240
productive in a day, right? And that is only possible if you standardize things. And so this project

155
00:11:08,240 --> 00:11:13,440
has also helped us to standardize things and not kind of deviate too much. That's a really

156
00:11:13,440 --> 00:11:22,640
interesting observation. I talked to a lot of people who put the idea of culture against the idea

157
00:11:22,640 --> 00:11:29,360
of technology in a sense. You know, technology is not most important in its culture, but you're

158
00:11:29,360 --> 00:11:34,320
talking about the relationship between the two and technology supports creating the culture

159
00:11:34,320 --> 00:11:40,960
in an important way. Yes, so in engineering, the process, which is called CICD, right? Continuous

160
00:11:40,960 --> 00:11:46,000
integration, continuous development. If you look at different companies, they use different tools.

161
00:11:46,000 --> 00:11:53,200
And to me, that process, the tools you use is actually a reflection of your culture, right? So the way

162
00:11:53,200 --> 00:11:58,240
you do machine learning really reflects the culture you're trying to build. Like for instance,

163
00:11:58,960 --> 00:12:03,360
there are many companies who actually build standardized tooling just for model creation.

164
00:12:04,320 --> 00:12:07,600
Once you've created the model, the deployment can happen in very different ways.

165
00:12:08,320 --> 00:12:13,440
Yeah, I'm not saying that's good or bad, but that kind of tells you the culture of the company,

166
00:12:13,440 --> 00:12:19,760
right? We don't do that. We just want to standardize the entire end-to-end machine learning

167
00:12:20,320 --> 00:12:24,640
culture. And then, you know, if you work at LinkedIn, that's really the culture that we have,

168
00:12:24,640 --> 00:12:30,640
right? Because we believe that's the best way for us to increase the ROI of machine learning.

169
00:12:30,640 --> 00:12:36,560
To increase ROI, there are several components. You have to be cost effective. How do you become cost

170
00:12:36,560 --> 00:12:42,000
effective? You have to be efficient. You have to be productive. And you also have to innovate,

171
00:12:42,000 --> 00:12:45,680
right? Because if you don't innovate, then no matter how productive you are, you're going to run

172
00:12:45,680 --> 00:12:51,520
experiment that will only give you marginal returns after a while. So the innovation has to continue,

173
00:12:51,520 --> 00:12:55,520
because innovation is not easy. We all know that. If you don't innovate for the next six

174
00:12:55,520 --> 00:13:00,640
months, you certainly would not stumble upon a breakthrough that's going to completely change

175
00:13:00,640 --> 00:13:06,320
the game, right? So you have to continuously innovate. And you cannot give your engineers time

176
00:13:06,320 --> 00:13:10,800
to innovate if they are not productive. If they take too much time to do their job, when are they

177
00:13:10,800 --> 00:13:18,080
going to get the time to innovate, right? So innovation, productivity, efficiency, and those are three.

178
00:13:18,080 --> 00:13:22,960
So innovation, productivity, efficiency, like these things, they are all entangled. They all have

179
00:13:22,960 --> 00:13:27,200
to be attacked together. You cannot just say, oh, I'm not going to worry about productivity. I'm

180
00:13:27,200 --> 00:13:31,760
just going to worry about innovation. It doesn't work that way, right? Because, you know, the time that

181
00:13:31,760 --> 00:13:37,600
an engineer has and, you know, the kind of investment you can make in the program is fixed, right?

182
00:13:37,600 --> 00:13:42,400
You have a fixed budget problem and you have to really solve these three components in the best

183
00:13:42,400 --> 00:13:50,000
possible way, depending on your business needs. Yeah. So we talked about platform abstractly thus far.

184
00:13:50,000 --> 00:13:56,800
Can you give us an overview of ProML and the kind of major components, what's in place now,

185
00:13:56,800 --> 00:14:00,960
the direction you're heading with it, et cetera? Yeah. So our aspirational goal is the entire

186
00:14:00,960 --> 00:14:06,480
end-to-end machine learning process should become completely automated, right? Like once a scientist

187
00:14:06,480 --> 00:14:10,720
understands the business problem, they understand the kind of models they need to build,

188
00:14:11,280 --> 00:14:16,240
to solve that machine learning problem. From there on, everything else should become automated.

189
00:14:16,240 --> 00:14:21,120
And obviously that's very easy to say, we all know it's not very difficult to do. I don't think

190
00:14:21,120 --> 00:14:26,640
anyone has solved that, but that's the aspiration at least. So just like any other end-to-end machine

191
00:14:26,640 --> 00:14:31,120
learning platform, we have a model creation process, right? So how do you create models very easily?

192
00:14:31,120 --> 00:14:35,600
How do you compose different kinds of ideas together? Like if someone wants to take XGBoost and

193
00:14:35,600 --> 00:14:41,680
GLM makes and neural network and try it out, like it should be very easy to try that. So model

194
00:14:41,680 --> 00:14:47,680
creation, data preparation, you know, that's one major component. Once you've built that,

195
00:14:47,680 --> 00:14:52,080
then how do you deploy these models in production? Once you've deployed the model in production,

196
00:14:52,080 --> 00:14:56,000
then how do you make sure that you're not babysitting the model, right? We don't want our scientists

197
00:14:56,000 --> 00:15:00,000
to become babysitters, right? Because that's not a good way to run a machine learning program.

198
00:15:00,000 --> 00:15:04,080
So the maintenance of these models, once they're in production, should be almost automated.

199
00:15:04,080 --> 00:15:08,240
And what do I mean by that? If the model is running in production, you still have to retrain

200
00:15:08,240 --> 00:15:12,240
the model because the world around you is changing every day. It's not a stationary process.

201
00:15:12,240 --> 00:15:16,880
So every day, the data should flow in the directory automatically. If for whatever reason,

202
00:15:16,880 --> 00:15:21,840
the data does not flow, there should be an alert and the person who's running the model should

203
00:15:21,840 --> 00:15:26,960
kind of know about it and then they should have a fall-off graceful degradation. If everything

204
00:15:26,960 --> 00:15:31,040
is going well, then the data should come in and you should be automatically building systems that

205
00:15:31,040 --> 00:15:36,320
can retrain the model and deploy it in production without any issues, right? And so this should not

206
00:15:36,320 --> 00:15:42,400
require any human intervention, right? The rest of the things, model creation and all that stuff,

207
00:15:42,400 --> 00:15:48,160
that's where the scientists need to spend the time. So one unique feature that we have added

208
00:15:48,160 --> 00:15:53,680
to this entire ProML, which may be distinct from other is we created this notion of a feature marketplace.

209
00:15:54,480 --> 00:15:58,560
Right? So we all know in order to create machine learning models, features are the most important thing.

210
00:15:58,560 --> 00:16:02,560
And so we don't want the scientists to be starting from the scratch, right? So there is a marketplace

211
00:16:02,560 --> 00:16:06,720
where we have actually created a lot of features for you, right? Think of them as prefabrication,

212
00:16:07,360 --> 00:16:13,040
prefabricated features, right? For instance, based on all the activity of users on the site,

213
00:16:13,040 --> 00:16:17,520
we know who has a strong job intent. We know who clicked on an ad in the last two. So these are

214
00:16:17,520 --> 00:16:22,560
features that are all available to you. Based on your profile, we know what are your standardized

215
00:16:22,560 --> 00:16:27,280
title and all that. So if you want to build a machine learning task, you can just grab these features

216
00:16:27,280 --> 00:16:33,440
from the feature marketplace using a consistent API, we call it frame. And that makes it super easy

217
00:16:33,440 --> 00:16:39,120
for you to start building a model right from the very word go. And you will actually reach 80%

218
00:16:39,120 --> 00:16:44,240
there, right? So that's very easy. Now, if the additional 10% improvement can add a lot of

219
00:16:44,240 --> 00:16:48,560
business ROI, then you better spend more time on that because otherwise the first cut should be

220
00:16:48,560 --> 00:16:54,320
something that you should be able to do very quickly. So I alluded to this earlier, but

221
00:16:54,320 --> 00:17:01,280
and you did as well. LinkedIn was very early on in this space. You made initial investments based

222
00:17:01,280 --> 00:17:07,680
on your technology landscape at the time, very Hadoop centric, but that's needed to evolve

223
00:17:07,680 --> 00:17:14,000
over the years. Can you talk about that evolution and the challenges that it presented the way

224
00:17:14,000 --> 00:17:20,560
you managed it? Yeah, so that was not an easy journey. So I think when we started the advertising

225
00:17:20,560 --> 00:17:26,000
problem, for instance, we wanted to do very large scale modeling as I told you, but Hadoop is not

226
00:17:26,000 --> 00:17:32,800
very amenable to that kind of computation. So we actually did algorithmic innovations to do

227
00:17:32,800 --> 00:17:38,080
those distributed computations. So for instance, the first version of model fitting algorithm we

228
00:17:38,080 --> 00:17:44,560
launched on the site was based on ADMM, right? So ADMM can actually help you do computation in pieces

229
00:17:44,560 --> 00:17:48,320
and then you can patch it all together by using a simple computer. And that was very amenable to

230
00:17:48,320 --> 00:17:55,120
Hadoop and that's how we started. We got the ball rolling and we saw enormous ROI system and

231
00:17:55,120 --> 00:17:59,280
then Spark came along. We are one of the very early adopters of Spark even when it was not

232
00:17:59,280 --> 00:18:03,920
very stable. And so we believed that Spark is going to solve a lot of our problems. So we invested

233
00:18:03,920 --> 00:18:10,960
very early in Spark. We worked in fact with folks in MLlib. In fact, a couple of the folks who actually

234
00:18:10,960 --> 00:18:18,080
joined MLlib were at LinkedIn. So we had a great relationship with them and we then really

235
00:18:18,080 --> 00:18:23,040
focused on its Spark first-class citizen in our ecosystem. That helped us a lot to scale these

236
00:18:23,040 --> 00:18:28,400
algorithms. And then finally TensorFlow came in, right? And so, you know, when with the deep learning,

237
00:18:28,960 --> 00:18:36,480
again, TensorFlow, TensorFlow in those days had a lot of great things, but it was not, it didn't have

238
00:18:36,480 --> 00:18:41,040
everything that would help us deploy it into it. So we had to build things like Tony and other

239
00:18:41,040 --> 00:18:46,000
things that we have open source. So that is that spin really our journey. And I think all along,

240
00:18:46,000 --> 00:18:50,800
we had very strong support from the business to do what we really needed to because the ROI

241
00:18:50,800 --> 00:18:55,360
was always there. So that is the key lesson for me, right? Whenever you're building a machine

242
00:18:55,360 --> 00:19:00,640
learning program, you know, you know, when you're going to your executives, just show them the money

243
00:19:00,640 --> 00:19:07,600
and then everything else becomes much easier. If you can't prove the ROI, then it becomes hard.

244
00:19:07,600 --> 00:19:14,880
So be the money. Yeah. You can do a lot of research, but the program should act tangible and

245
00:19:14,880 --> 00:19:18,800
direct value to the business. And if you can ensure that balance, I think you are in great shape.

246
00:19:18,800 --> 00:19:24,080
Then you get all the support you need. Because otherwise, you know, going to your executive team

247
00:19:24,080 --> 00:19:29,600
in the early days of deep learning and telling them, okay, you need to help us get a GPU cluster.

248
00:19:29,600 --> 00:19:34,480
It's going to cost $20 million. It's not a very easy argument to make a way. You have to wait

249
00:19:34,480 --> 00:19:40,240
for a while before they actually buy into that. But you know, if you if you have a track record

250
00:19:40,240 --> 00:19:46,720
of delivering, then these things work out much much much more easily. We alluded to this and my

251
00:19:46,720 --> 00:19:52,960
conversation with Andrew earlier, you have to manage that portfolio and expectations very carefully.

252
00:19:52,960 --> 00:19:57,360
Right. You know, what you can deliver in your term, what the overall vision is. How have you done

253
00:19:57,360 --> 00:20:05,040
that? Yeah, I think you're just just described my job. But yeah, so I mean, that's what a portfolio

254
00:20:05,040 --> 00:20:09,840
manager would do. Right. So I mean, I have this philosophy, as I said, like, you know, so there are

255
00:20:09,840 --> 00:20:14,240
three components to your portfolio. One is the core investment. So this is something that you do to

256
00:20:14,240 --> 00:20:19,120
add value to your business on a consistent basis. Like is machine learning able to improve

257
00:20:19,840 --> 00:20:25,200
the number of the engagement that members have on our side? Is machine learning able to help us

258
00:20:26,160 --> 00:20:31,920
create more revenue? Help our customers have a better experience? Is machine learning able to

259
00:20:31,920 --> 00:20:37,760
keep our sites safe and clean? Right. So these are core investments. We cannot for not to do those.

260
00:20:37,760 --> 00:20:44,320
Right. So that's a bulk of the investment. Like I would say that's 60%. The other 30% is more strategic

261
00:20:44,320 --> 00:20:49,600
investment. So we know that deep learning is going to help us in the next six months. And after

262
00:20:49,600 --> 00:20:54,880
all the model iteration that our engineers are doing with ProML, you know, even if they can do a

263
00:20:54,880 --> 00:20:58,960
lot of iteration after a while, these methods are going to only give marginal gains. In fact, you

264
00:20:58,960 --> 00:21:03,600
know, it will, it will actually put us in a lot of trouble because there will be so many experiments

265
00:21:03,600 --> 00:21:08,960
queued up. And the amount of experimental budget you have is still constant. You cannot run

266
00:21:08,960 --> 00:21:13,840
hundreds of experiments on the side, even if you can, because the experimental budget is fixed.

267
00:21:13,840 --> 00:21:19,440
Right. So you have to invest in strategic initiatives. So that in six months, you get something new,

268
00:21:19,440 --> 00:21:24,000
which is going to give you a big gain. Right. And so the experiments you're running will actually

269
00:21:24,000 --> 00:21:28,560
still produce a lot of ROI. So that's the strategic bucket. And the 10% is more venture bed. Right.

270
00:21:28,560 --> 00:21:34,720
So new product ideas, things like, okay, how can we do deep reinforcement learning and production

271
00:21:34,720 --> 00:21:40,320
is it in the future? How do we build chat boards that can change the entire way users interact

272
00:21:40,320 --> 00:21:45,040
with each other on LinkedIn? These are all venture beds and ideas. So that's roughly the portfolio

273
00:21:45,040 --> 00:21:52,000
we tried to maintain like 70, 20, 10. And so far it has worked pretty, pretty well. We also do

274
00:21:52,000 --> 00:21:57,040
other things like to encourage grassroots innovation. We have something called the ideas program.

275
00:21:57,040 --> 00:22:02,000
So every quarter, anyone in the organization can actually submit an idea they want to work on.

276
00:22:02,560 --> 00:22:07,440
And then there is a committee who actually looks at all the ideas and we'll select the top 10 ideas.

277
00:22:07,440 --> 00:22:12,800
And those ideas get funded through our normal resource pool. So this is just to make sure that

278
00:22:12,800 --> 00:22:18,160
everyone, it's not always top down. The ideas are not always top down. And some of the ideas,

279
00:22:18,720 --> 00:22:22,400
some of the best ideas we have seen actually come from that grassroots innovation.

280
00:22:22,400 --> 00:22:28,320
Our organization is also very energized. They know that we are in the innovation business here.

281
00:22:28,320 --> 00:22:34,320
This is how we actually produce ROI. So it's also important to create that culture of innovation.

282
00:22:34,320 --> 00:22:39,920
And you also organize hackathons so that folks can actually experiment with new product ideas

283
00:22:39,920 --> 00:22:46,000
or prototypes and so on and so forth. So it's really that culture that is very important I think

284
00:22:46,000 --> 00:22:50,640
that helps you create that energy and keep doing that innovation. And then we have an awesome

285
00:22:50,640 --> 00:22:56,560
infrastructure team that actually partners with us to make sure that we are productive and we are

286
00:22:56,560 --> 00:23:06,560
efficient. Okay. You mentioned earlier that you're very deliberate about measuring the experimental

287
00:23:06,560 --> 00:23:13,680
velocity of teams at LinkedIn. Can you dig into that a little bit deeper? Yeah. So just as we

288
00:23:13,680 --> 00:23:18,720
have a centralized AI organization at LinkedIn, we also have a central experimentation platform.

289
00:23:18,720 --> 00:23:24,000
Yeah. Right. So every AB test, you run, we document that, we log that data.

290
00:23:25,120 --> 00:23:31,280
And based on those AB experiments, we have readouts of things that succeeded in production.

291
00:23:31,280 --> 00:23:37,120
So we kind of aggregate all of that information and make sure that we kind of have a dashboard

292
00:23:37,120 --> 00:23:42,240
and we look at it every week and we measure things. Just as we will measure any business metric,

293
00:23:42,240 --> 00:23:46,480
just as you will, you know, anyone's just as in your business metric, there are seasonality,

294
00:23:46,480 --> 00:23:52,480
right? Like in the summer, people take vacation and we see a drop. If I see a very big drop,

295
00:23:52,480 --> 00:23:57,520
then that's an alarm bell, like what is happening. And then I realize, oh, while everyone is attending

296
00:23:57,520 --> 00:24:02,480
a conference, so okay, well, that's why we didn't have an hour experience. So we are very deliberate

297
00:24:02,480 --> 00:24:07,200
and we are very particular about that. We in fact, enhanced it now, right? So now we started with

298
00:24:07,200 --> 00:24:12,320
the total number of successful experiments. Now I'm asking even the teams to label their experiments.

299
00:24:12,320 --> 00:24:18,320
So is it a large t-shirt or medium-sized t-shirt or a small t-shirt, right? And it's very interesting

300
00:24:18,320 --> 00:24:23,680
how teams kind of put those labels and then we discuss about it. Like, you know, you can give a

301
00:24:23,680 --> 00:24:30,880
hard time to your team. You know, you're a good way. I'm a good manager. So why do you think this

302
00:24:30,880 --> 00:24:36,240
is a large experiment? And you know, it's okay. Do you think you can't think bigger than this?

303
00:24:36,240 --> 00:24:42,560
And so it's actually a nice thing for you to even get a sense of the culture that you have in

304
00:24:42,560 --> 00:24:47,680
your organization. Like, who thinks what is big? What is medium? What is large? And over time,

305
00:24:47,680 --> 00:24:52,320
you can actually create this culture where you are actually asking folks to run fewer numbers of

306
00:24:52,320 --> 00:24:57,440
larger experiment because it is great to say we can run a lot of experiment where every experiment

307
00:24:57,440 --> 00:25:02,720
has a cost associated with it. And I don't mean infrastructure cost only. I mean opportunity cost.

308
00:25:02,720 --> 00:25:06,880
Right? If someone is running an experiment, they're going to wait. Then for two, after two days,

309
00:25:06,880 --> 00:25:10,720
they're going to look at the results. And then they're going to move on to the next idea.

310
00:25:10,720 --> 00:25:15,440
So this is a hidden, economists call it the hidden cost, right? So there is a cost to every experiment

311
00:25:15,440 --> 00:25:22,080
you run. And the best way for an organization to become really effective is to run a large number

312
00:25:22,080 --> 00:25:27,920
of high value experiment, not a large number of experiments. That is not all enough, right? I mean,

313
00:25:27,920 --> 00:25:33,040
because in many cases, I've seen people run parameter sweep experiments. I mean, those experiments

314
00:25:33,040 --> 00:25:37,840
should be all automated. So you have to actually have forensics on the experiments as well.

315
00:25:38,320 --> 00:25:43,200
Once you become a large organization, the next step is to kind of analyze, have telemetry on your

316
00:25:43,200 --> 00:25:48,080
experiments and then figure out what experiments can be automated and encourage your engineers to

317
00:25:48,080 --> 00:25:54,640
work on the big things rather than on things that can be automated. You mentioned in opening things

318
00:25:54,640 --> 00:26:03,840
up that LinkedIn was very early in adopting data science. And in that period of time, you know,

319
00:26:03,840 --> 00:26:09,280
once that term caught on, you know, everyone who worked in this space was a data scientist.

320
00:26:09,280 --> 00:26:14,960
Since then, the roles have evolved. We've got machine learning engineers, we've got platform

321
00:26:14,960 --> 00:26:19,760
teams. Yeah, what does that evolution look like at LinkedIn and where do things stand now?

322
00:26:19,760 --> 00:26:26,720
Yeah, so that evolution, like in any other place, is still evolving, like we all know. I mean,

323
00:26:26,720 --> 00:26:31,680
to me, there are four pillars of AI or data science, whatever you call it. And that's how, actually,

324
00:26:31,680 --> 00:26:37,120
five machine learning computer science, of course, that's the key. Then statistics,

325
00:26:37,920 --> 00:26:43,520
optimization and economics and systems engineering, right? So these are really the five pillars. And

326
00:26:43,520 --> 00:26:48,720
when people from these five disciplines come together and work together to solve hard

327
00:26:48,720 --> 00:26:54,960
problem, that is what creates the magic, right? Now, I think labels are labels that keep changing

328
00:26:54,960 --> 00:27:00,800
over time. I don't think at LinkedIn, we pay too much attention to the labels. We just make sure

329
00:27:00,800 --> 00:27:05,520
that these five disciplines can always come together and work. And then, you know, once, so that's

330
00:27:05,520 --> 00:27:10,640
the technology side, but technology side is also not enough to create awesome machine learning

331
00:27:10,640 --> 00:27:14,800
solutions, like for instance, the most important thing in a machine learning problem is to define

332
00:27:14,800 --> 00:27:21,920
the problem. What are you trying to solve? And you cannot define the problem unless you interface

333
00:27:21,920 --> 00:27:26,160
very closely with the domain experts, you know? So you have to work very closely with the product

334
00:27:26,160 --> 00:27:31,360
team. And in this day and age, you have to worry about security, you have to worry about legal. So

335
00:27:31,360 --> 00:27:39,280
you have to interface with the legal team, right? So it takes a village to get AI, right? And so,

336
00:27:39,280 --> 00:27:44,640
I think that's how we think about AI at LinkedIn. And in order to actually make sure that everyone

337
00:27:44,640 --> 00:27:48,800
understands the basic of AI at LinkedIn, we have actually created what we call the AI Academy.

338
00:27:49,440 --> 00:27:54,800
And so it has three levels of courses. The AI 100 is general awareness. In fact, I also

339
00:27:54,800 --> 00:27:58,320
teach there, we're passionate about teaching, right? So this is like a two-day course. We have our

340
00:27:58,320 --> 00:28:03,200
best people kind of teach to everyone in the company what AI is all about. And it starts from

341
00:28:03,200 --> 00:28:09,120
data, right? I mean, you cannot be AI first company without being a data first company.

342
00:28:10,000 --> 00:28:14,000
Because at least today, we don't know how to do AI without data. Maybe in the future,

343
00:28:14,000 --> 00:28:17,440
we may create a new theory that can help us do AI without data. But right now,

344
00:28:18,160 --> 00:28:25,120
we all know we need data to do AI. So data first to AI first. And that education is a very

345
00:28:25,120 --> 00:28:30,240
important component. Then we have AI 200. So if you are an engineer who has taken Andrew's course,

346
00:28:30,240 --> 00:28:34,160
but now want to get your hands dirty, you can actually get your hands dirty through AI.

347
00:28:34,160 --> 00:28:40,000
200. And AI 300 is an internship program. You can be an intern in the AI organization. Someone

348
00:28:40,000 --> 00:28:44,320
is going to work very closely with you. And you will be actually deploying things in production,

349
00:28:44,320 --> 00:28:51,040
right? So the education and this collaboration and tooling, this is really how we run AI at LinkedIn.

350
00:28:51,040 --> 00:28:56,160
And we always say, it takes a village to get AI, right? And we have seen that first time.

351
00:28:56,160 --> 00:29:01,280
You know, if you try to create an AI program in a company in a silo, it would never work.

352
00:29:01,920 --> 00:29:07,840
You just have to do it together. It's too big to be solved by one discipline is my opinion.

353
00:29:07,840 --> 00:29:19,360
So maybe to start to close out, I'd love to hear your vision for where ML is going at LinkedIn.

354
00:29:19,360 --> 00:29:22,160
And maybe even more broadly in the industry.

355
00:29:22,160 --> 00:29:27,120
Yeah, so I think we have reached a point where deep learning is really helping us a lot.

356
00:29:27,120 --> 00:29:32,240
And you know, we are able to solve some very traditional problems that were very hard to solve,

357
00:29:32,240 --> 00:29:38,240
in a very spectacular way. But I still think Andrew was also talking about that.

358
00:29:38,240 --> 00:29:42,480
Machine learning is still not very accessible broadly, right? Only a few companies

359
00:29:42,480 --> 00:29:49,920
that have the talent and that have the culture that they've built in this space for a long time,

360
00:29:49,920 --> 00:29:54,160
they are the ones who are really reaping a lot of benefit now. So how do we make machine learning

361
00:29:54,160 --> 00:29:59,920
more accessible broadly so that everyone can benefit from that? I think that's a very challenging

362
00:29:59,920 --> 00:30:04,800
problem. It's not easy, right? Like, you know, if you don't even understand how to manage data

363
00:30:04,800 --> 00:30:09,520
to think that one fine day you'll just wake up and run a deep learning model and we'll all start

364
00:30:09,520 --> 00:30:15,280
working automatically, that's a pipe thing. That doesn't happen, right? So getting there would be a

365
00:30:15,280 --> 00:30:21,440
very big task for us. Once we get there, then I think the computational cost is also something

366
00:30:21,440 --> 00:30:26,720
that we all have to think about. So GPUs are not just costly, they are also bad for the environment.

367
00:30:26,720 --> 00:30:31,760
Right? And so, you know, we don't want a planet where we are actually doing so much compute

368
00:30:32,560 --> 00:30:36,320
without being careful of what we're doing to our environment and to our planet, right? So

369
00:30:37,520 --> 00:30:41,840
if you want to be cost effective, you have to figure out ways to kind of do the computation

370
00:30:41,840 --> 00:30:46,240
in a more efficient way. I think that I believe it's going to become a very important

371
00:30:46,960 --> 00:30:51,600
topic for all of us. And finally, responsible AI, right? And again, I don't want to steal

372
00:30:51,600 --> 00:30:57,840
thunder. There is a panel on this later at the conference, but doing AI in an ethical way,

373
00:30:57,840 --> 00:31:04,240
doing AI in a responsible way, making sure the privacy of individuals and everyone else is kind

374
00:31:04,240 --> 00:31:08,720
of a dear to and we're not having data breaches and all that stuff, right? So that's again,

375
00:31:08,720 --> 00:31:12,960
a very important topic for all of us because there's no point in creating a very powerful technology

376
00:31:12,960 --> 00:31:17,200
if it works against the humans, right? I mean, we need to create technology that helps us

377
00:31:17,200 --> 00:31:23,840
become better, right? And so this is again another big area for all of us to think about in the

378
00:31:23,840 --> 00:31:28,480
future. It's not about where this will go. It is all about where we want to take it to, right?

379
00:31:28,480 --> 00:31:32,320
So this is one thing where I don't want to make a prediction. I want to kind of appeal to all of

380
00:31:32,320 --> 00:31:37,520
you that we all think about it very deeply and make sure that we are all using the technology

381
00:31:37,520 --> 00:31:43,280
in a way that is going to help the human race, not hurt the human race. Awesome. Well, Deepak,

382
00:31:43,280 --> 00:31:55,120
thanks so much for joining us. Thank you. All right, everyone. I hope you enjoyed our show

383
00:31:55,120 --> 00:32:01,680
straight from the main stage at TwomoCon AI Platforms. For more information about today's show,

384
00:32:01,680 --> 00:32:09,120
visit TwomoAI.com. And for more TwomoCon coverage, visit TwomoCon.com slash news.

385
00:32:09,120 --> 00:32:12,640
Thanks so much for listening and catch you next time.

