1
00:00:00,000 --> 00:00:18,160
All right, everyone. I am here with Greg Brockman. Greg is co-founder and CTO at OpenAI. Greg,

2
00:00:18,160 --> 00:00:24,880
welcome back to the Twomol AI podcast. Thanks for having me, Sam. Hey, it's been a while since we

3
00:00:24,880 --> 00:00:33,280
spoke. It was back in November of 2017. Believe it or not, episode 74 of the podcast, we're over 500

4
00:00:33,280 --> 00:00:40,640
now and we were then talking about AGI. I am really looking forward to this chat where we'll be talking

5
00:00:40,640 --> 00:00:48,960
about something new that OpenAI has been working on for a while, Codex. But before we do, why don't

6
00:00:48,960 --> 00:00:54,720
you reintroduce yourself to our audience and tell them how you came to work in the field of AI?

7
00:00:56,000 --> 00:01:01,200
Hey, everyone. I'm Greg as I, Sam said, and I am one of the co-founders of OpenAI.

8
00:01:02,880 --> 00:01:08,800
You know, for me, I've read the Alan Turing 1950 paper, Computing Machinery and Intelligence

9
00:01:08,800 --> 00:01:13,520
Paper back before I knew how to code. And I remember reading it. You know, it lays out the Turing

10
00:01:13,520 --> 00:01:18,960
Test. But then it says, look, you're never going to be able to program a solution to this test.

11
00:01:18,960 --> 00:01:23,840
The only way to do it is you have to have a learning machine. And he goes into quite some detail.

12
00:01:23,840 --> 00:01:27,040
You know, he says, like, look, you're going to have to do this, like, to have a little machine.

13
00:01:27,040 --> 00:01:31,120
It's almost like a child machine that you give rewards when it does good things, punishment when it

14
00:01:31,120 --> 00:01:36,720
does bad things. And from there, you can hope to build up a solution to this. Really visionary

15
00:01:36,720 --> 00:01:40,800
stuff, honestly. And for me, I was captivated by the idea that you could build a machine that

16
00:01:40,800 --> 00:01:45,840
could understand problems that you yourself could not. And I just saw being able to build machines

17
00:01:45,840 --> 00:01:50,160
that could themselves help you solve problems that were outside of your reach, be the thing I wanted

18
00:01:50,160 --> 00:01:56,320
to do. So I went to a professor and was like, hey, can I do some MLP research with you? And he's

19
00:01:56,320 --> 00:02:01,360
like, great. Yep. Here's these, like, parse trees and things like that. And sadly, it didn't look

20
00:02:01,360 --> 00:02:06,400
like that was going to quite get you there. So I got distracted by programming languages, which,

21
00:02:06,400 --> 00:02:10,320
you know, I think kind of captures the same idea, right? If you can build a compiler,

22
00:02:10,320 --> 00:02:14,400
you can kind of understand this program and can really amplify what a human can do.

23
00:02:16,800 --> 00:02:23,120
And then, you know, it did startups and it was really 2015 that I first encountered deep learning.

24
00:02:23,120 --> 00:02:28,000
And for me, I was watching hacker news every day. And I felt like there was a new deep learning

25
00:02:28,000 --> 00:02:31,520
for this, deep learning for that. But I didn't know what deep learning was. And it was actually

26
00:02:31,520 --> 00:02:36,400
surprisingly difficult to just Google around and learn what deep learning actually meant. So I

27
00:02:36,400 --> 00:02:41,360
asked some friends about it. And as I started going around, I realized all of my smartest friends

28
00:02:41,360 --> 00:02:45,920
from school were now in deep learning. And that for me was a real sign of, okay, maybe there's

29
00:02:45,920 --> 00:02:51,920
some real substance here. And the deeper I dug, the more it felt to me like the old direction,

30
00:02:51,920 --> 00:02:56,720
that it just didn't feel right, the new direction actually did feel right. And to me,

31
00:02:56,720 --> 00:03:01,040
you know, looking back at it now, the thing I find most fascinating is that really this neural

32
00:03:01,040 --> 00:03:05,680
in that direction, it's not a, you know, five, 10-year thing. It's really a 70-year journey to get

33
00:03:05,680 --> 00:03:10,400
to where we are. So it's just exciting to be pushing the frontier of what these neural networks

34
00:03:10,400 --> 00:03:13,280
can do. And that's basically what we've been doing in OpenAI the whole time.

35
00:03:14,160 --> 00:03:19,680
Nice. You mentioned your interest in programming early on and parse trees and all that kind of

36
00:03:19,680 --> 00:03:25,680
stuff. And you know, there's maybe a connection to what we're going to be talking about today.

37
00:03:25,680 --> 00:03:36,720
Again, which is Codex. OpenAI recently announced co-pilot, which is another project in the same

38
00:03:36,720 --> 00:03:42,560
vein. Maybe tell us a little bit about these projects, what they are and how they're related to one

39
00:03:42,560 --> 00:03:48,800
another. Yep. So we've been building the Codex model for about a year now. We really started

40
00:03:48,800 --> 00:03:55,920
when we saw GPT-3 be released. And people's, you know, most excited reactions were actually

41
00:03:55,920 --> 00:03:59,120
using it for programming. And we looked at that and we said, well, we didn't build this model

42
00:03:59,120 --> 00:04:03,520
of the program at all. What happens if we actually put some effort into it? And so we've actually

43
00:04:03,520 --> 00:04:07,440
teamed up with GitHub and Microsoft. You know, GitHub, I think, is probably best in the world that,

44
00:04:08,000 --> 00:04:12,960
you know, knowing what developers want and have great, your great community. And obviously,

45
00:04:12,960 --> 00:04:17,200
you know, that they have, they have lots of data as well. And so we worked really closely with

46
00:04:17,200 --> 00:04:23,040
them to try to build a product that people wanted, right, to really validate that what we were doing

47
00:04:23,040 --> 00:04:27,840
wasn't just a cool research project, but was actually useful from day one. So a month ago,

48
00:04:27,840 --> 00:04:35,440
we released co-pilot together with GitHub, which is the first product built on top of Codex.

49
00:04:35,440 --> 00:04:40,080
And that they use the Codex API that is the same API that we, I guess by the time people

50
00:04:40,080 --> 00:04:47,440
watch this podcast, will have released on Tuesday. And so, you know, talk a little bit about the

51
00:04:47,440 --> 00:04:54,720
relationship between Codex and GPT-3. Is it an entirely separate model? Are they the same model

52
00:04:54,720 --> 00:05:00,320
with different training data, different training processes? Yep, I would think of Codex as a descendant

53
00:05:00,320 --> 00:05:06,960
of GPT-3. So spiritually, you do the same kind of task. GPT-3 is take all of the text on the internet

54
00:05:06,960 --> 00:05:13,520
and just do an auto-complete task, predict what word is going to come next. Codex is take all

55
00:05:13,520 --> 00:05:19,200
the text on the internet and all of the public code and do that same process. And we made lots of

56
00:05:19,200 --> 00:05:23,760
improvements all across the board. Really, this has been an effort of a quarter of open AI to make

57
00:05:23,760 --> 00:05:29,440
it happen. So we've really had to put in efforts from everything to, you know, we have architectural

58
00:05:29,440 --> 00:05:33,840
improvements, we have training improvements, we have a lot of just like the good old-fashioned

59
00:05:33,840 --> 00:05:38,720
engineering, maybe these models be fast and responsive has been a huge amount of work as well.

60
00:05:38,720 --> 00:05:44,080
So it's really been improvements all across the board. So kind of talking about Codex relative

61
00:05:44,080 --> 00:05:49,360
to GPT-3, you mentioned, take all of the text on the internet and all of the code on the internet.

62
00:05:51,360 --> 00:05:54,800
In creating something like a Codex, are those given equal weight or

63
00:05:56,080 --> 00:06:01,440
is the code somehow, you know, more relevant for the task that Codex is likely to see?

64
00:06:01,440 --> 00:06:07,600
Yeah, the short answer is I think we're still figuring out exactly the right way of doing it

65
00:06:07,600 --> 00:06:13,920
ultimately. I mean, I think that, you know, right now our process is, you know, I think you basically

66
00:06:13,920 --> 00:06:22,240
end up with, you know, you end up seeing much more code more recently than text. But it's still

67
00:06:22,240 --> 00:06:26,000
an open question, I think, exactly what you want. And we've kind of found that when you actually

68
00:06:26,000 --> 00:06:30,560
look at the models in terms of how people want to use them, that part of what makes Codex really

69
00:06:30,560 --> 00:06:36,800
shine is the fact that it has all this world knowledge built in. And so you can actually end up with

70
00:06:36,800 --> 00:06:41,760
a model that's very, very good at doing just like, you know, sort of very narrowly defined,

71
00:06:41,760 --> 00:06:47,680
complete this programming, you know, this function or something, without actually being very useful

72
00:06:47,680 --> 00:06:53,600
to people. So I think that finding the right evaluations is actually one real trick to make

73
00:06:53,600 --> 00:06:58,560
this model work. And so, you know, what we really focused on is, and has actually got it,

74
00:06:58,560 --> 00:07:03,360
it's pretty well so far, is at the very beginning of the project, we've wrote down this data set,

75
00:07:03,360 --> 00:07:10,720
this now open source called, we call it human eval, which is a list of problems written by humans

76
00:07:10,720 --> 00:07:14,880
that are just programming puzzles. And we kind of designed them to be ones that are kind of,

77
00:07:14,880 --> 00:07:20,320
they're a little bit of tricky wording and a little bit like, you know, it's different from what

78
00:07:20,320 --> 00:07:26,560
you would find in, you know, some already out there in the training corpus. So kind of intentionally

79
00:07:26,560 --> 00:07:32,880
chosen to have some twist them and that sort of thing. And what we found is that by pursuing

80
00:07:32,880 --> 00:07:37,360
that metric, it is actually our best north star metric, like everything else, if you just look at

81
00:07:37,360 --> 00:07:42,160
perplexity, you know, basically like, how good it is exactly at predicting next token in text,

82
00:07:42,720 --> 00:07:46,880
that that particular metric breaks down a little bit for us, because you kind of want this holistic,

83
00:07:46,880 --> 00:07:51,680
not just how certain do you get that there should be a period here, but you really want just given

84
00:07:51,680 --> 00:07:56,240
a pretty natural description of what the problem is going to be, can you solve that problem?

85
00:07:58,240 --> 00:08:06,160
And so when you created that that data set and that metric, did you, was there a closed loop

86
00:08:06,160 --> 00:08:13,120
there where the things that the programs that Codex created against that training set had to

87
00:08:13,120 --> 00:08:18,800
actually run and produce the desired result? I understand, yes. I'm mostly asking just from

88
00:08:18,800 --> 00:08:24,480
the perspective of evaluating the formats of Codex in terms of producing runable code.

89
00:08:25,680 --> 00:08:31,360
An aspect of that data set. 100%, 100%. So you literally take, you provide the model with,

90
00:08:31,360 --> 00:08:35,120
you know, maybe a doc string and maybe a little bit of a function definition, it generates a bunch

91
00:08:35,120 --> 00:08:40,240
of code, you literally eval that code. Now the details of the eval actually I think are pretty

92
00:08:40,240 --> 00:08:46,320
interesting because you just had some code come out from your model. What's it going to do?

93
00:08:46,320 --> 00:08:50,400
Is it going to lead all the files on your computer? It's all possible, right? And so you really

94
00:08:50,400 --> 00:08:55,360
need to have a good sandbox. And so, you know, I think that one thing people miss in this field is,

95
00:08:55,360 --> 00:08:59,520
you know, it's all about the big idea, but what people miss is that actually it's about the small

96
00:08:59,520 --> 00:09:04,160
ideas, right? It's about getting the engineering really right. And so yeah, you want to actually train

97
00:09:04,160 --> 00:09:08,320
a model to run some arbitrary code and eval that makes sure it's doing the right thing. You need to

98
00:09:08,320 --> 00:09:13,360
have a world class sandbox to make that happen. And so you need to make sure both that the execution

99
00:09:13,360 --> 00:09:17,760
is like not able to do anything, you know, tamper with your system, but also that, you know,

100
00:09:17,760 --> 00:09:20,960
just even little things like resource consumption and being able to crash your system and things

101
00:09:20,960 --> 00:09:26,000
like that are held in check. And we actually have found multiple times that the model would generate

102
00:09:26,000 --> 00:09:32,160
code that kind of broke our current sandbox. So we've upgraded it since then. Interesting, interesting.

103
00:09:32,160 --> 00:09:38,320
So I think that is suggesting the folks that play this play around with this via the API that they

104
00:09:38,320 --> 00:09:42,720
take care to inspect the results they get before they just run them if they don't have a sandbox

105
00:09:42,720 --> 00:09:47,600
environment that they're. Yeah. I definitely, I definitely recommend that for any code you take

106
00:09:47,600 --> 00:09:52,480
from the internet. You know, if you just download some code from even my GitHub, I will not take

107
00:09:52,480 --> 00:09:57,360
a fence if you double check it before just running it. I think it's, it's just an important thing

108
00:09:57,360 --> 00:10:02,160
generally. But I would say that this, this like the model doing unpredictable things is really

109
00:10:02,160 --> 00:10:07,440
early in training, right? So when the model isn't very smart, isn't very capable. It's sort of

110
00:10:07,440 --> 00:10:12,240
less predictable exactly what it'll do. The more capable the model gets, the more it's going to be

111
00:10:12,240 --> 00:10:17,040
faithful to your instruction. So I've been, I've been using this model for, for, you know, I spent

112
00:10:17,040 --> 00:10:21,120
quite, quite a bit of time playing with it. And that I've actually, you know, found that it's,

113
00:10:21,120 --> 00:10:24,800
that it's quite reliable in contrast in some ways to GPT-3.

114
00:10:26,720 --> 00:10:31,680
Can you talk about those distinctions a little bit more in terms of the types of

115
00:10:32,480 --> 00:10:37,600
results that it tends to see versus GPT-3 relative to the, the prompts that you're giving it?

116
00:10:37,600 --> 00:10:43,200
Yeah, see the thing about GPT-3 is that I always, and I really, like when we get these new models,

117
00:10:43,200 --> 00:10:47,520
I really spend a lot of time with them trying to really understand them, trying to like just sort of

118
00:10:48,080 --> 00:10:53,840
feel like I get the personality of these models if you'll forgive the term because these models,

119
00:10:53,840 --> 00:10:57,040
you know, they're not one thing, right? They're really this whole distribution of things.

120
00:10:57,040 --> 00:11:01,920
But so for GPT-3, I really spend a lot of time trying to teach it. You know, I have this whole chat

121
00:11:01,920 --> 00:11:06,160
session where I was a teacher and I was explaining to it how to sort of list of numbers. And it would

122
00:11:06,160 --> 00:11:10,560
do one example, get it right, and I'd be like, wow, I've really taught it the process of sorting.

123
00:11:10,560 --> 00:11:13,520
And then I'd give it another example and it would totally go off the rails and do something wrong.

124
00:11:14,080 --> 00:11:18,880
And I think that the feeling that I had was GPT-3 didn't really want to list. And like it really

125
00:11:18,880 --> 00:11:22,960
felt like this, this, you know, this, this being that like had a short attention span and would

126
00:11:22,960 --> 00:11:27,680
just kind of like do random things sometimes. And I think that that's probably a reflection of

127
00:11:27,680 --> 00:11:31,280
the training data in some ways, right? If you're out there on the internet and you read some text saying,

128
00:11:31,280 --> 00:11:34,240
okay, now I'm going to sort of list of numbers. I mean, maybe you're in the middle of the fiction

129
00:11:34,240 --> 00:11:38,080
story, right? And then like, you know, that some aliens arrive or something. And so it's actually

130
00:11:38,080 --> 00:11:43,200
reasonable for GPT-3 to make pretty, pretty arbitrary predictions when it's not very confident

131
00:11:43,200 --> 00:11:50,000
what should come next. But by contrast and code, what I found with Codex is that when it fails,

132
00:11:50,000 --> 00:11:54,320
it does half my instruction, but not the full instruction, right? And sometimes, you know,

133
00:11:54,320 --> 00:11:58,640
sometimes you can end up with the traditional failure modes of autoregressive models where it fails

134
00:11:58,640 --> 00:12:02,800
by repeating a token over and over if that's the most certain one. And basically most of my

135
00:12:02,800 --> 00:12:07,600
experiments have been, I haven't had to fuss with hyper parameters and I really just set

136
00:12:07,600 --> 00:12:12,160
temperature equals zero, so it's just always picking the most likely token. And it's worked out

137
00:12:12,160 --> 00:12:16,720
way, way better than for any model that I've tried before. And I think that a lot of this comes

138
00:12:16,720 --> 00:12:21,680
back to the structure of the data, right? That in code, if I have a comment saying, now I'm going to

139
00:12:21,680 --> 00:12:25,360
sort some numbers, you're really going to sort numbers next, right? There's really nothing else

140
00:12:25,360 --> 00:12:30,160
that's about to happen. And so it's almost like we have this great data set that we've built up

141
00:12:30,160 --> 00:12:36,640
of instruction following. And I think that that idea we found in GPTland was pretty key to

142
00:12:36,640 --> 00:12:40,080
getting something that's even more useful to people. And in code, it's almost built in.

143
00:12:41,520 --> 00:12:50,640
Yeah, I'm very curious about this idea of, you know, the code is a data set and the self-documenting

144
00:12:50,640 --> 00:12:56,000
nature of it. When you think about just kind of raw code that you might find in GitHub,

145
00:12:56,000 --> 00:13:02,800
you know, there's documentation that's going to be at a, I would think a pretty low kind of

146
00:13:02,800 --> 00:13:09,040
semantic level, like, you know, this loop is going to do thing X. You know, I think of something

147
00:13:09,040 --> 00:13:15,840
like a stack overflow that's talking about the code that you might see in a post at a much higher

148
00:13:15,840 --> 00:13:25,680
level. And I wonder a little bit about, you know, as all of, you know, to what degree is the code

149
00:13:25,680 --> 00:13:30,400
that codex is trained on, you know, GitHub style versus something that might have some more like

150
00:13:30,400 --> 00:13:35,280
higher level semantic meaning. And, you know, just your thoughts on whether that matters and,

151
00:13:36,400 --> 00:13:43,520
you know, how codex might evolve with different types of data that you trained it on.

152
00:13:43,520 --> 00:13:47,200
Yep. I think the answer for this stuff has probably got to catch them all.

153
00:13:47,200 --> 00:13:52,080
Like, I think we're at a point with these models and I think GPT kind of set the stage for it,

154
00:13:52,080 --> 00:13:58,080
is that the broader you go, the more capable you're going to get. Yeah. And the part of it is that

155
00:13:58,080 --> 00:14:03,280
when we do a task, it's kind of impossible to predict exactly what skills people want to bring to

156
00:14:03,280 --> 00:14:08,880
bear, right? Like, that the, you know, it's almost like, if you rewind to the, before the, you know,

157
00:14:08,880 --> 00:14:13,840
general purpose computers were obviously the right solution, which by the way, they're not even

158
00:14:13,840 --> 00:14:16,480
obviously the right solution for all problems, but for most problems they are.

159
00:14:17,120 --> 00:14:21,920
There were specialized machines for each individual application. And people were always like,

160
00:14:21,920 --> 00:14:27,200
well, your general purpose computer is cool and all. It's great demo, but if you really want to do,

161
00:14:27,200 --> 00:14:31,840
like, you know, your, your, your contact book, you need to use this specialized, you know,

162
00:14:31,840 --> 00:14:38,240
IBM, whatever, you know, machine that existed at the time. And I think that the basically just turns

163
00:14:38,240 --> 00:14:44,400
out that many tasks require mixing and matching between lots of different things. And so it's kind

164
00:14:44,400 --> 00:14:50,480
of hard to pre-bake one answer to everything. And so where we've started has been, you know, again,

165
00:14:50,480 --> 00:14:54,720
kind of all the text out there and all the public code. But I think that within code, you know,

166
00:14:54,720 --> 00:15:02,080
it's not just like, you know, Django and projects like that. It's also think about all the ipython

167
00:15:02,080 --> 00:15:08,880
notebooks that people put on GitHub, right? And that those ones tend to be very much like a

168
00:15:08,880 --> 00:15:13,440
tutorial, right? There's lots of lots and lots of casual tutorials and things like that that

169
00:15:13,440 --> 00:15:18,880
are out there. And so you get kind of a very different slice of intelligence from those.

170
00:15:18,880 --> 00:15:22,960
And I think that what we've been looking at, like, I think kind of a big next step really is

171
00:15:22,960 --> 00:15:27,120
figuring out what are the best sources? You know, what do you learn from each one? How do you

172
00:15:27,120 --> 00:15:34,480
figure out what you want to balance in that model? And one thing that people probably might be

173
00:15:34,480 --> 00:15:39,520
surprised to hear is that codex, you know, it can do lots of things in lots of different languages.

174
00:15:39,520 --> 00:15:43,840
You know, it's probably pretty good at about it doesn't different languages. But we really trained

175
00:15:43,840 --> 00:15:47,200
it just for Python. Like, we actually were just like, we just want this thing to be as good

176
00:15:47,200 --> 00:15:54,000
as Python as we can. And all the other stuff kind of fell out as almost an accident. So I think that,

177
00:15:54,000 --> 00:15:59,120
you know, if you test it, it'll be interesting to see, you know, do people find it very useful

178
00:15:59,120 --> 00:16:03,680
for the broad range of languages? Or is, you know, sort of that focus on Python? Does that shine

179
00:16:03,680 --> 00:16:08,160
through? Nice. Nice. I can tell you that it does do hello world and list.

180
00:16:08,160 --> 00:16:12,400
Okay, good. There we go. Believe me, we did not try to make a good list.

181
00:16:16,560 --> 00:16:23,120
You know, also fascinated by this, this idea that we talked about earlier, you know, the

182
00:16:23,120 --> 00:16:28,400
language, the natural language, plus the code. And there's part of me that would love to like

183
00:16:28,400 --> 00:16:35,360
tweak some hyper parameter that lets you wait one versus the other. Any, you know, any thoughts

184
00:16:35,360 --> 00:16:41,760
on that? Or I suspect that your answer is going to be similar to the last one, which is kind of

185
00:16:41,760 --> 00:16:46,480
the more the merrier, all having all the data, you know, is going to get you better results and

186
00:16:46,480 --> 00:16:51,360
trying to over optimize or yeah, yeah, I think some of the stuff you're you're hitting on the

187
00:16:51,360 --> 00:16:55,680
right front here, as I think, right? And look, like just to zoom out to the big picture,

188
00:16:56,640 --> 00:17:01,360
to me, the most fascinating thing, first of all, is that this is all just a neural net, right? Like,

189
00:17:01,360 --> 00:17:06,960
you rewind back to the 40s and, you know, pits and McCullough, like that model of the information

190
00:17:06,960 --> 00:17:11,120
processing of the brain, like that's the thing we're still doing today. And so, you know, you can

191
00:17:11,120 --> 00:17:16,560
actually find this great paper on Wikipedia called like, you know, an interpretation of the

192
00:17:16,560 --> 00:17:22,240
history of the, of the perceptron. And the, you know, the story ever and always told about

193
00:17:22,240 --> 00:17:26,480
the perceptron was like, hey, in the 60s, these neural net people overhyped everything and,

194
00:17:26,480 --> 00:17:30,160
you know, all the funding went away. And if you actually look at the historical documents,

195
00:17:30,160 --> 00:17:33,440
kind of what was going on is there were two competing camps. There was the symbolic systems

196
00:17:33,440 --> 00:17:37,280
people. And then there were the neural net people. And that this symbolic systems people had a

197
00:17:37,280 --> 00:17:41,840
very concerted campaign to try to drive all the funding for the neural net people. And that they

198
00:17:41,840 --> 00:17:45,920
had all these disparaging things to say, like those neural net people, they have no new ideas.

199
00:17:45,920 --> 00:17:50,320
They just want to build a bigger computer. Like that's all they want to do. And, you know, here we are,

200
00:17:50,320 --> 00:17:55,120
you know, 40 years later, 50 years later. And yeah, we just want bigger computers and more data.

201
00:17:55,120 --> 00:17:59,520
And so I think that is actually the most core answer. Like, you know, I think that we all kind of

202
00:17:59,520 --> 00:18:03,440
want the great scientific insight of like, you know, to, to figure things out and to figure out

203
00:18:03,440 --> 00:18:08,000
the exact theory of mixing. And I think actually the funny thing is I think we can make progress

204
00:18:08,000 --> 00:18:13,200
on those problems. But the highest, highest order bit is you need to have a big machine with

205
00:18:13,200 --> 00:18:18,080
thoughts of compute and pour in all the data you can. And like, you know, at some point that that

206
00:18:18,080 --> 00:18:22,080
that the details that mix start to really matter. But the highest order bit is actually achieving

207
00:18:22,080 --> 00:18:29,440
that first thing. Does that, you know, to what degree does that like cap innovation? If you've

208
00:18:29,440 --> 00:18:34,160
already, you know, pulled all the language in the world, all the text in the world into GPT-3

209
00:18:34,160 --> 00:18:41,360
and all the code in the world into codex. And it's all about, you know, data and size of compute,

210
00:18:41,360 --> 00:18:46,240
where do you go to innovate? Yeah. So I think it's a great question. So on the one hand,

211
00:18:46,240 --> 00:18:51,040
you can look at what I said as a pretty depressing thing. I'm just like, okay, it's just, you know,

212
00:18:51,040 --> 00:18:55,360
just a simple matter of, you know, doing this, this large-scale engineering. And you need to have

213
00:18:55,360 --> 00:19:01,120
your particle accelerator equivalent in order to do it. But actually, if you dig into the source

214
00:19:01,120 --> 00:19:06,320
of progress in recent years, you know, we've published a couple studies on this. And so we have one

215
00:19:06,320 --> 00:19:10,800
study that shows the compute ramp is insane. Like, it's just faster than any exponential that I'm,

216
00:19:10,800 --> 00:19:17,360
that I'm aware of. But we also have another study showing the algorithmic ramp, showing the

217
00:19:17,360 --> 00:19:22,400
efficiencies due to algorithms is also exponential. And you know, rather than being like, you know,

218
00:19:22,400 --> 00:19:26,800
doubling every 3.5 months, it's like, you know, more like, you know, doubling every year, year and a

219
00:19:26,800 --> 00:19:31,520
half, you know, something much more like Moore's Law. I forgot the exact number. But that's still

220
00:19:31,520 --> 00:19:36,880
a pretty insane rate of progress. And so I think that the truth of all of this is that if you have

221
00:19:36,880 --> 00:19:41,600
a paradigm that is worthwhile, right, that like, making it a more capable neural net, clearly a

222
00:19:41,600 --> 00:19:46,640
worthwhile thing at this point, you're going to innovate to the max in all dimensions. And yeah,

223
00:19:46,640 --> 00:19:50,400
we've had a pretty big compute overhang because people just weren't willing to spend lots of money

224
00:19:50,400 --> 00:19:54,800
on computers. And now people are. So they just spend more money to get ahead of Moore's Law. So that's

225
00:19:54,800 --> 00:19:58,960
one dimension. Similar story for data, you know, that there's been lots of data out there, just like,

226
00:19:58,960 --> 00:20:02,560
it just wasn't really worth people's effort to collect it or people didn't really know to do it,

227
00:20:02,560 --> 00:20:07,360
whatever it is, there's an overhang of just gather all that data. But on the algorithmic front,

228
00:20:07,360 --> 00:20:10,480
you know, I think that's been the one that people have been pushing on. And so there isn't as much

229
00:20:10,480 --> 00:20:14,080
of an overhang, you know, there's not like low hanging fruit left around that just no one's

230
00:20:14,080 --> 00:20:17,520
thought of that just, you know, you just show up and you're just like, gold's at my feet,

231
00:20:17,520 --> 00:20:23,120
it takes effort. But I think that the fruit is still there, right, that it's still the case that

232
00:20:23,120 --> 00:20:26,720
we are making this exponential progress there. So I think it's like, just because we're making

233
00:20:26,720 --> 00:20:33,280
big progress in certain dimensions, you know, that's just temporal, right, that like we cannot

234
00:20:33,280 --> 00:20:39,200
keep up the rate of improvement from those dimensions. And so, yeah, once you've saturated them,

235
00:20:39,920 --> 00:20:43,040
the only thing left is going to be this other dimension. So I think it's really important,

236
00:20:43,040 --> 00:20:45,920
we as a community don't lose that muscle that we really build it up.

237
00:20:47,440 --> 00:20:52,560
And now if I asked you to comment on that algorithmic dimension, would it be,

238
00:20:52,560 --> 00:21:00,240
would it be asking you to speculate into the future or is there, you know, a set of kind of,

239
00:21:01,040 --> 00:21:05,840
you know, relatively low hanging fruit that, you know, things that you know that directions

240
00:21:05,840 --> 00:21:10,240
that you know that you want to head on the algorithmic side? Yep. Well, I want to talk again about

241
00:21:10,240 --> 00:21:15,360
just, you know, first of all, my personal philosophy, you know, is very much like, you know,

242
00:21:15,360 --> 00:21:20,560
greatness through a thousand small steps that I really, you know, I think that there are some

243
00:21:20,560 --> 00:21:25,680
people who are extremely good at the like one big idea to change everything. And I think that,

244
00:21:25,680 --> 00:21:30,000
you know, like Ilya, who's one of my co-founders is extremely good at that. You look at like,

245
00:21:30,480 --> 00:21:34,560
you know, I think he's, you know, with work like Alex, and I think he's very good at sort of

246
00:21:34,560 --> 00:21:39,520
setting the direction. But for me, I tend to think in terms of, okay, like what are all the small

247
00:21:39,520 --> 00:21:44,080
details we have to get right to make this happen? And if you look at the current models, you know,

248
00:21:44,080 --> 00:21:50,160
the funny thing about GPT-3 is that it actually used the same, the tokenizer that Alec Radford,

249
00:21:50,160 --> 00:21:56,320
who works at OpenAI, wrote kind of like overnight, right before the deadline, you know, three years

250
00:21:56,320 --> 00:22:03,440
prior for GPT-1. And like, you know, that thing is not optimal. It's actually become kind of the

251
00:22:03,440 --> 00:22:07,520
standard lots of people use it. I mean, you know, people have done a little bit to, to, you know,

252
00:22:07,520 --> 00:22:11,840
play with different organizations and retrain the things like that. But fundamentally, I think that

253
00:22:11,840 --> 00:22:16,400
that there's like a big, you know, big shift in some ways in kind of small detail in other ways

254
00:22:16,400 --> 00:22:20,480
of just, we should be really doing by level models, right? We shouldn't be doing this like, let's

255
00:22:20,480 --> 00:22:24,560
like, you know, sort of tokenize things and duck and chunk them up in this like way that kind of maps

256
00:22:24,560 --> 00:22:29,200
to, you know, it's almost like hard coding that's in the model that probably would do a lot better

257
00:22:29,200 --> 00:22:33,200
if it wasn't there. I think a lot of the story of neural nets has been removed the hard coded stuff

258
00:22:33,200 --> 00:22:38,400
and add in learning. So I think that's one example of the kind of thing that I would really love

259
00:22:38,400 --> 00:22:43,520
to see someone work on and just see, to see great results from and for us to incorporate that.

260
00:22:43,520 --> 00:22:49,680
So I think basically little bits of the architecture that are still like, yeah, we really should be

261
00:22:49,680 --> 00:22:54,400
doing this differently. I think that that for me is actually where I put a lot of focus.

262
00:22:55,280 --> 00:23:01,200
I wanted to kind of transition to, you know, how we should think about codex as like users and

263
00:23:01,200 --> 00:23:06,160
practitioners. Those folks that want to play with it, like, how should we, you know, think about

264
00:23:06,160 --> 00:23:11,440
interacting with this API to get the most out of it? And let's maybe start with, what is it best

265
00:23:11,440 --> 00:23:19,280
that versus where the, you know, the soft edges? Yes. Well, I will first say, I think that no one

266
00:23:19,280 --> 00:23:24,880
knows yet to the answer of, what is it best at? Like, I can tell you what I've discovered in my

267
00:23:24,880 --> 00:23:29,120
efforts, right? And I'll say, for me, I know I'm scratching the surface. Like, I know I am.

268
00:23:29,120 --> 00:23:33,760
Fair enough. But, and that's, that's the wonderful thing, by the way, you know, if you train a vision

269
00:23:33,760 --> 00:23:38,480
model on an image net, you know what it's good at, right? It's very, very good at all the dog breeds.

270
00:23:38,480 --> 00:23:44,320
Um, this model, general purpose, so it's quite good at lots of things. Um, I have, so for, for me,

271
00:23:44,320 --> 00:23:49,120
you know, I really latched on to this, uh, being able to provide instructions in natural language

272
00:23:49,120 --> 00:23:54,560
and have it generate an executable output, right? So basically talk to your computer, does it?

273
00:23:54,880 --> 00:23:59,680
Um, when, you know, when we first started playing with the model, like, it wasn't clear that it

274
00:23:59,680 --> 00:24:03,840
would be good at that. And I just kind of realized, like, hey, this model, when I give it these, like,

275
00:24:03,840 --> 00:24:06,960
because I actually started out the other, on the other side, I started out with trying to say,

276
00:24:06,960 --> 00:24:10,800
if I just want to provide one big instruction and have it write a whole program. And, you know,

277
00:24:10,800 --> 00:24:16,240
it's quite reliable at doing things like, I'd say make it to Kinter UI that, like, has a button that

278
00:24:16,240 --> 00:24:20,720
says, hello world, then you click it and send an email, like, that level of instruction, it could

279
00:24:20,720 --> 00:24:25,040
actually write, like, you know, the 30, 40 lines of Python to do it. And sometimes I make a little

280
00:24:25,040 --> 00:24:28,720
mistake. You'd forget to, like, wire up the button, or, you know, it kind of like a placeholder for

281
00:24:28,720 --> 00:24:33,040
whatever. Um, but the way it would fail was, again, very interpretable, right? Because look at

282
00:24:33,040 --> 00:24:36,720
it, I'd be like, oh, okay, just kind of forgot this piece. And so then I started thinking about, well,

283
00:24:36,720 --> 00:24:40,000
what I really want is I want to be able to chunk this instruction up into smaller pieces,

284
00:24:40,000 --> 00:24:44,160
because, you know, it did 80% of it. And so if I just had a 50% size instruction,

285
00:24:44,160 --> 00:24:50,560
maybe I'll do 100% of it. Um, and so I think that that's kind of the highest level picture of

286
00:24:50,560 --> 00:24:56,400
where we are is the model. I think it's not yet ready to do big things, right? On its own. But

287
00:24:56,400 --> 00:25:01,040
it's ready to do small things. And honestly, for programming, like, I like doing the big things.

288
00:25:01,040 --> 00:25:04,640
I don't like doing the small things myself, right? The, uh, the like, you know, okay, like,

289
00:25:04,640 --> 00:25:08,960
here's this very specific fiddley thing. And like, get the details of the indexing right. Like

290
00:25:08,960 --> 00:25:13,520
that kind of thing, the model, it knocks it out in the park, or memorizing the details of,

291
00:25:13,520 --> 00:25:17,360
you know, this, whatever framework, like, you know, I used to write and Ruby on Rails. And most

292
00:25:17,360 --> 00:25:21,440
of Ruby on Rails is just knowing what the Railsism is to do. Any particular thing. Right. Right.

293
00:25:21,440 --> 00:25:26,560
And, um, yeah. And by the way, I mean, like, ID's are just not very good at Ruby on Rails,

294
00:25:26,560 --> 00:25:30,080
because there's so much dynamism and things like that. But this model, I think, would be,

295
00:25:30,080 --> 00:25:34,080
would be quite good at it. So I think basically figuring out how to work with those strengths is

296
00:25:34,080 --> 00:25:39,360
important. And so part of it is, I think, like, one dimension that I think is very exciting is

297
00:25:39,360 --> 00:25:45,600
baking it in as an interface to lots of existing applications. So we have a example of baking to

298
00:25:45,600 --> 00:25:51,600
Microsoft Word. I think that for any website, really, you should now be able to very easily build

299
00:25:51,600 --> 00:25:56,080
an interface where you just say, like, you know, you know, what, if you're depending on what

300
00:25:56,080 --> 00:26:01,200
your web app is, you know, go and look up the, you know, go send an email to this person or,

301
00:26:01,200 --> 00:26:06,000
I, you know, like, yeah, any of the functionality that's in your website should become voice

302
00:26:06,000 --> 00:26:09,920
controllable or, you know, natural language controllable without having to nest or you click through

303
00:26:09,920 --> 00:26:15,520
a bunch of buttons in order to get there. You mentioned that some of your initial observations

304
00:26:15,520 --> 00:26:22,240
were that you, you construct this prompt and it will spit out results that, you know,

305
00:26:22,240 --> 00:26:29,440
were 80% there or missing some detail or something like that. And that you solve that by kind of

306
00:26:30,640 --> 00:26:37,520
chunking your prompts and making them smaller and more compact. Do you think the issue that you

307
00:26:37,520 --> 00:26:47,040
experienced originally was, you know, was on the input side or the the output side, if that makes

308
00:26:47,040 --> 00:26:52,880
sense, was it a, you know, issue in the, a fault in kind of the generation process or it, you

309
00:26:52,880 --> 00:26:58,160
know, couldn't pull the piece for that, you know, couldn't make the connection necessary required

310
00:26:58,160 --> 00:27:03,360
for that last 20% or was it, you know, forgot it in the parsing stage, using really rough language

311
00:27:03,360 --> 00:27:08,720
here to, yeah, yeah, you're asking the great mysteries of the what's going on inside the neural net,

312
00:27:09,280 --> 00:27:15,600
which, you know, I think is always very interesting. And, you know, for me,

313
00:27:15,600 --> 00:27:23,600
well, first of all, I also think that if you asked literally me to do the same task without

314
00:27:23,600 --> 00:27:28,720
access to an interpreter, so I just have to write the program once without ever being able to push

315
00:27:28,720 --> 00:27:34,320
backspace, I'm not going to do a good job either, like trust me, like I will not. Most of programming

316
00:27:34,320 --> 00:27:39,200
for me is I write a little bit and I run it and it doesn't work and I change it and I fix it and

317
00:27:39,200 --> 00:27:45,120
I iterate and I fix it, you know, and that, that other piece, this model doesn't get to do it at all.

318
00:27:45,120 --> 00:27:50,960
So I think that it's very possible that the model simply cannot, like, you know, just from reading

319
00:27:50,960 --> 00:27:55,200
all that text and really deeply thinking through all the details about the interface should work

320
00:27:55,200 --> 00:28:00,000
is a bottleneck. And then secondly, it's very possible that just like, it just as it's writing,

321
00:28:00,000 --> 00:28:04,080
it just runs, oh no, I really wish that I'd like implemented this function beforehand. So you

322
00:28:04,080 --> 00:28:08,080
know what, I'll just pretend that it's implemented later and like, you know, that never gets to it.

323
00:28:08,080 --> 00:28:13,280
So I don't know which of those stories is more true, my guess is that it's a mix of both.

324
00:28:14,240 --> 00:28:18,720
And partly, I just look at myself, like, you know, look, this is not a human-like intelligence,

325
00:28:18,720 --> 00:28:25,040
so it may be too, you know, a little bit too egocentric to think that I can look to what I'm good at

326
00:28:25,040 --> 00:28:32,160
and bad at to map to where the model makes mistakes. But I will say that for me, it's been actually,

327
00:28:32,160 --> 00:28:39,360
like, I feel much more in tune with the failures and successes of Codex than I did with GPT-3.

328
00:28:39,360 --> 00:28:43,200
For me, it does feel like when it fails, I'm a little bit like, you know, sometimes,

329
00:28:43,200 --> 00:28:46,320
sometimes the way it fails, by the way, is it'll just put in pass, you know, so it's like, you know,

330
00:28:46,320 --> 00:28:50,400
I have a nice Python, you know, death, whatever, and like, I put it in a doc strand, like, okay,

331
00:28:50,400 --> 00:28:53,840
model, you go now, and that's solution is just to put in pass or, you know, comment to do,

332
00:28:54,720 --> 00:28:58,960
something like that. And I get it, it's a little bit like, it's like, okay, I'm not going to be

333
00:28:58,960 --> 00:29:02,960
able to do this, so I'm not even going to try, right? And, you know, I don't think that's necessarily,

334
00:29:03,680 --> 00:29:08,400
the, you know, the only characterization. But it really feels like, you know, if you think of

335
00:29:08,400 --> 00:29:14,400
how code is usually structured, that I think that it actually starts to feel a little bit more,

336
00:29:14,400 --> 00:29:18,720
like, constrained in terms of the, the, again, you know, you have this pattern of comment,

337
00:29:19,280 --> 00:29:22,720
complete, or total out, and kind of nothing in between.

338
00:29:22,720 --> 00:29:34,000
Yeah, you just mentioned the structure that code tends to have the, you know, codex operates

339
00:29:34,000 --> 00:29:41,200
like GPT, GPT-3 in this kind of input, you know, process output paradigm. Have you done any

340
00:29:41,200 --> 00:29:48,800
playing around or experimentation to try to force fit structure into that input in a way that

341
00:29:48,800 --> 00:29:55,280
it understands that it can produce more, you know, structure on the output? Well, so I have one,

342
00:29:55,280 --> 00:29:59,360
so I have, I have a couple of different dimensions that I think are very interesting, right? So,

343
00:30:00,320 --> 00:30:05,040
look, there's one dimension that I think is kind of fun, which is translating between languages.

344
00:30:05,040 --> 00:30:12,000
And so I have a little demo of a writing, writing a Python program, so I wrote a Python program,

345
00:30:12,000 --> 00:30:17,520
that then you run it, make some calls to the API, generate some Ruby code, and that Ruby code is

346
00:30:17,520 --> 00:30:23,280
just a program that calls the API to generate some Python code, and you get this Python Ruby oscillator

347
00:30:23,280 --> 00:30:27,440
forever and ever. It's a little bit like writing a coin. It's just like kind of a fun, fun little thing.

348
00:30:28,560 --> 00:30:33,280
I actually tried doing the same thing for Python to Ruby to JavaScript to Python to Ruby JavaScript.

349
00:30:33,280 --> 00:30:39,360
I got it to do like six cycles of Python Ruby JavaScript before it broke. So it actually was like

350
00:30:39,360 --> 00:30:44,080
each time writing a little bit of unique code, which is kind of a cool thing to see. So setting it

351
00:30:44,080 --> 00:30:48,480
up for that, I think, was a very interesting challenge, because there you really have to make sure

352
00:30:48,480 --> 00:30:53,200
that your your prompt which is kind of contained within the program is something that kind of like

353
00:30:53,200 --> 00:30:58,400
gives the enough context to the API for it to actually generate the whole new program, but

354
00:30:58,400 --> 00:31:02,800
you know, it's like you really got to play some some some nice fiddley games to make it happen.

355
00:31:02,800 --> 00:31:06,800
So, you know, that I think is more of a proof of concept. It's more of like an interesting

356
00:31:06,800 --> 00:31:11,840
exercise than it is something very practical. But there's actually another direction that I was

357
00:31:11,840 --> 00:31:17,360
experimenting with that I I think it's like interesting and very fruitful. Someone can make it work of

358
00:31:18,240 --> 00:31:22,880
you know, look, programming is two things. It's understand the super hard problem and decompose it.

359
00:31:22,880 --> 00:31:28,160
Right. So it's basically problem decomposition and then mapping the small problem to code. We've

360
00:31:28,160 --> 00:31:33,120
already said codex is really good at that second thing, probably better than I am. That first one

361
00:31:33,120 --> 00:31:37,360
is it actually bad to it. And all I know is that the obvious ways of making it good at it,

362
00:31:37,360 --> 00:31:42,960
I haven't succeeded at, but I but using codex for task decomposition is something I've tried a

363
00:31:42,960 --> 00:31:48,240
little bit and got some interesting results on. And you know, you can do things like you have

364
00:31:48,240 --> 00:31:53,760
codex call into, you know, you basically tell it, oh, there's this like magical oracle function.

365
00:31:53,760 --> 00:31:58,640
And so oracle is you give it some natural language and then just like the machine will magically

366
00:31:58,640 --> 00:32:02,960
implement it for you. And then you say, okay, do this hard task and you get access to call

367
00:32:02,960 --> 00:32:08,160
the oracle thing. And then you can see it can codex generate good calls, subcalls to oracle.

368
00:32:08,160 --> 00:32:12,720
And I've actually gotten it to as a little bit of like a, you know, together working with codex

369
00:32:12,720 --> 00:32:16,800
to be able to get it to do things like, you know, go on Google and like download an image of,

370
00:32:16,800 --> 00:32:20,480
you know, a particular person and put into a website and things like that. And you know, you

371
00:32:20,480 --> 00:32:27,520
use Selenium to to orchestrate all of this. And I think that ideals like this are very interesting

372
00:32:27,520 --> 00:32:34,000
because maybe you can actually have codex as a tool that helps in more of the cognitive domain

373
00:32:34,000 --> 00:32:42,640
in addition to this like very mechanical like code emission domain. Is there a, you know, input

374
00:32:42,640 --> 00:32:49,520
pattern that you've seen or a hyper parameter that can kind of guide it towards a degree of

375
00:32:49,520 --> 00:32:58,080
complexity in the solution? Like there's a, the length of the output, you know, as a, as a, you

376
00:32:58,080 --> 00:33:03,440
know, one idea that might be that, you know, hey, if I say, you know, give me hello world and I

377
00:33:03,440 --> 00:33:07,680
want it to be, you know, 300 characters in length or a thousand characters in length, that's going

378
00:33:07,680 --> 00:33:12,640
to be, you know, one thing. If I say, you know, 10,000 like, is it going to give me the, you know,

379
00:33:12,640 --> 00:33:20,240
the J2EE enterprise? I mean, I think the best, the best starting point, by the way, for all

380
00:33:20,240 --> 00:33:24,160
these things, the only real answers you got to try it, right? Like you really seem to play with it.

381
00:33:25,840 --> 00:33:30,160
But I think the place to start is just by asking the model for what you want. And if the model

382
00:33:30,160 --> 00:33:35,760
doesn't quite seem to get it, you try to spell it out more clearly, expand how you're asking,

383
00:33:35,760 --> 00:33:40,000
like really think about if this were a junior programmer and I had to really hold their hand and

384
00:33:40,000 --> 00:33:44,160
walk them through it, how would I do it? Right? And sometimes that's breaking up into multiple

385
00:33:44,160 --> 00:33:49,760
instructions. Sometimes that's just expand more of what you're asking for. So I think that's

386
00:33:49,760 --> 00:33:54,160
definitely the starting point. Another very powerful thing is providing more examples, right? So

387
00:33:54,160 --> 00:34:00,400
one thing we really haven't done very much of yet is trying to do GPT-3 style prompt engineering

388
00:34:00,400 --> 00:34:05,280
and trying to provide prompts to the model that really show examples of the behavior you want.

389
00:34:05,280 --> 00:34:09,280
And like all the indications so far and all the times that we've tried is that it's quite good at

390
00:34:09,280 --> 00:34:14,000
that. But we just haven't really pushed it in the way that we push GPT-3. In part because

391
00:34:14,000 --> 00:34:19,920
it's already capable of the tasks we want simply by asking. So we just kind of didn't have to go

392
00:34:19,920 --> 00:34:24,880
down that road. And then the third thing, of course, is fine tuning. And so we have a GPT-3

393
00:34:24,880 --> 00:34:32,240
fine tuning API these days. We'll be wrong that out for codex. And I think that that will open

394
00:34:32,240 --> 00:34:38,640
a new dimension to what you're able to make it do. Awesome. One of the interesting examples I saw

395
00:34:38,640 --> 00:34:46,400
in some of the materials was a, you know, not your traditional kind of creative program like,

396
00:34:46,400 --> 00:34:52,960
you know, XYZ, but it was to solve this word problem like, you know, from an elementary school,

397
00:34:52,960 --> 00:35:02,400
you know, Jason has six apples and four apples, something like that. But it created a program to

398
00:35:02,400 --> 00:35:08,880
figure out this word problem. Yes, right? I thought that was really interesting. And it made me immediately

399
00:35:08,880 --> 00:35:16,160
think about the implications of something like this in education, you know, both coding education,

400
00:35:16,160 --> 00:35:22,240
but, you know, more broadly in education. Any thoughts on that? Yeah. So the funny thing is when

401
00:35:22,240 --> 00:35:28,240
we were starting open AI, I, you know, I left my previous job and I knew I wanted to start a company.

402
00:35:28,240 --> 00:35:33,760
And I had three possible domains on my list. Number one was AI, which turned out to pan out.

403
00:35:34,560 --> 00:35:40,000
Number two was VR slash AR and I kind of scratched that off very quickly. But number three was

404
00:35:40,000 --> 00:35:44,560
programming education. You know, this is an area that's very near and dear to my heart. I feel like,

405
00:35:44,560 --> 00:35:48,480
you know, for my programming education, it was, you know, I started out very self-taught,

406
00:35:48,480 --> 00:35:53,120
just building stuff that I was excited about. And it was just hard, you know, it's just not very

407
00:35:53,120 --> 00:35:57,040
much fun to like, you know, it's like, you do W3 school's tutorial back in the day. I'm sure

408
00:35:57,040 --> 00:36:01,200
there are better tutorials now. But then you're just stuck staring at an editor and thinking about what

409
00:36:01,200 --> 00:36:05,280
do I build, right? And you run your thing and it doesn't work. And what do you do, right? And you

410
00:36:05,280 --> 00:36:09,200
don't know about a lot of concepts, you know, I didn't know about serialization. And so I was building,

411
00:36:09,200 --> 00:36:12,800
actually I built one of the first things I built was a chatbot game. So it was that you had a

412
00:36:12,800 --> 00:36:17,520
little chatbot that you could train by talking to it. And then you can have a little chatbot

413
00:36:17,520 --> 00:36:22,240
battle where you would like play this game where one window that you were talking to with a chatbot,

414
00:36:22,240 --> 00:36:26,800
one was a person and you had to distinguish which which which which which was which before your

415
00:36:26,800 --> 00:36:31,760
opponent would. And all this stuff, you know, I didn't know what serialization was. So I just like

416
00:36:31,760 --> 00:36:35,360
had this like I came up with a magical identifier that or you know, like a string of characters

417
00:36:35,360 --> 00:36:39,760
that I thought no one else would whatever type. And I use that as my record separator.

418
00:36:41,200 --> 00:36:45,920
And just looking back, I just wish that someone was there to say, oh, you should probably use JSON here.

419
00:36:45,920 --> 00:36:50,240
And then I'd be like, what's JSON, right? I'd go around, I'd figure out how to use JSON. And I would

420
00:36:50,240 --> 00:36:54,720
have just sort of cut off this whole tree. You know, there's a little bit of the tree that was very

421
00:36:54,720 --> 00:36:58,880
useful for me to figure out why is it useful to have serialization? Like, you know, why don't you just

422
00:36:58,880 --> 00:37:02,560
want to do your own record separator? You know, what are the problems? But there's a bigger tree

423
00:37:02,560 --> 00:37:05,840
of really implementing it and building up the library and trying to make it work and like that kind

424
00:37:05,840 --> 00:37:11,280
of thing that was a little bit of wasted effort. And so what I am excited to see with Codex is that

425
00:37:11,280 --> 00:37:15,760
we have a model that for the first time you can show it code and can actually kind of understand it.

426
00:37:15,760 --> 00:37:19,120
And so we've done a little bit of playing around with code explaining, right? And actually can do

427
00:37:19,120 --> 00:37:24,000
a decent job of taking a function and explains how it works or can generate comments for it or

428
00:37:24,000 --> 00:37:29,360
generate doc strings, generate unit tests. I think that all those things really open up the

429
00:37:29,360 --> 00:37:34,720
possibility of having a personalized programming tutor, right? And that to me is just like, it's

430
00:37:34,720 --> 00:37:40,320
amazing. I would love to be able to see programming education fall out from, you know, pursuing the

431
00:37:40,320 --> 00:37:45,200
AI passion and we will get there. It's just a question of, you know, I'm hopeful that Codex is

432
00:37:45,200 --> 00:37:54,960
enough at least to take the first steps. Does there need to be an element of, I guess I made a

433
00:37:54,960 --> 00:38:00,480
mental connection to like explainability in these kinds of models and, you know, tutor, you want

434
00:38:00,480 --> 00:38:09,120
your tutor to be able to explain to you the connections beyond just, you know, showing you an example

435
00:38:09,120 --> 00:38:16,720
which is kind of what Codex does now. That kind of called a mind, the whole explainability around

436
00:38:16,720 --> 00:38:21,520
these kinds of models to you and do you think that's a piece that would be interesting in that

437
00:38:21,520 --> 00:38:26,480
context? Yeah. So I think maybe in a non-traditional way. Like I think that the traditional explainability

438
00:38:26,480 --> 00:38:30,400
has been, we want to look at the connections of the neural net and explain why it made a decision

439
00:38:30,400 --> 00:38:34,560
that it made, right? But I mean, if you think about the equivalent problem for humans, we're not very

440
00:38:34,560 --> 00:38:37,920
good at either, right? You know, we don't open up the neurons of the brain and be like, oh, wow,

441
00:38:37,920 --> 00:38:42,000
look at the connection between these two neurons, right? You asked someone, why did you make that

442
00:38:42,000 --> 00:38:47,120
decision? And I think most of behavioral science is basically realizing that our own explanations

443
00:38:47,120 --> 00:38:51,920
of our actions are quite poor, right? That like, you know, you kind of do something and you come up

444
00:38:51,920 --> 00:38:58,640
with some like back narrative or why you did it. So I kind of feel like the baseline we should

445
00:38:58,640 --> 00:39:03,120
shoot for is that we should shoot, you know, look, we should get to a better place than where we are

446
00:39:03,120 --> 00:39:07,760
with humans in terms of being able to explain why decisions were made. But at the very least, I

447
00:39:07,760 --> 00:39:11,920
think it's a good baseline to hit. And so I think that what we should be trying to focus on with these

448
00:39:11,920 --> 00:39:17,280
models is that, you know, they rate some, you know, they're given a function and that they

449
00:39:17,280 --> 00:39:20,880
should explain how it works, that they wrote their own function, they explain why they wrote it,

450
00:39:20,880 --> 00:39:26,320
and that explanation actually adds up. You know, and like, maybe it turns out that in fact, just like

451
00:39:26,320 --> 00:39:31,360
the human version, that it doesn't quite correspond to, you know, sort of objective truth in some

452
00:39:31,360 --> 00:39:34,240
ways and that it says, well, I mean, the decision because of this variable and that variable,

453
00:39:34,240 --> 00:39:38,080
I mean, it changed that variable and it still does the same thing. You know, that kind of experiment,

454
00:39:38,080 --> 00:39:42,720
I think would be very interesting to see. But on the other hand, I think that for the super

455
00:39:42,720 --> 00:39:46,880
complicated task, and let's not hit ourselves, I mean, like, even writing the simplest program is

456
00:39:46,880 --> 00:39:51,280
a super complicated task of like, you just got to understand so many different concepts, you need to

457
00:39:51,280 --> 00:39:55,520
know this whole library of all these different functions. Like, that is really hard and I think that

458
00:39:55,520 --> 00:40:00,320
to even fit in our brain exactly like, okay, like, you know, how do I translate, how would I even

459
00:40:00,320 --> 00:40:07,040
write a program for, you know, say, you know, say it five times, you know, like something like that,

460
00:40:07,680 --> 00:40:11,360
what is it supposed to reference? Like, you know, five, like, how is that represented? Like,

461
00:40:11,360 --> 00:40:16,560
all the different ways you could see it. I think that for us to write a program that can do that

462
00:40:16,560 --> 00:40:22,000
is just going to be such a giant complex tree that even a trace through it would be extremely

463
00:40:22,000 --> 00:40:26,560
complicated and probably, you know, something that's outside of humans, humanity's ability to

464
00:40:26,560 --> 00:40:32,320
understand. So I think the trick is number one, having the, you know, just focusing these models on

465
00:40:32,320 --> 00:40:38,880
being able to provide good explanations that feel right at an intuitive level, but the feel like

466
00:40:38,880 --> 00:40:42,400
they were written by a person. And I think that that, we're on trajectory four, you know, I think

467
00:40:42,400 --> 00:40:46,880
that you can ask codex for this stuff today, and maybe it'll do a good job, you know, maybe it's

468
00:40:46,880 --> 00:40:51,440
not exactly what it was trained for, so maybe it won't, but I think that you can at least get started.

469
00:40:51,440 --> 00:40:56,720
But I think there's a next step, and this is actually part of our alignment work at OpenAI,

470
00:40:56,720 --> 00:41:02,160
is thinking about models that themselves are really optimized for explaining what another model did,

471
00:41:02,160 --> 00:41:06,000
right? Because here we have these, you know, with the super complicated problem that this model

472
00:41:06,000 --> 00:41:10,160
I came up with a solution for, and that it did in a super complicated way that we can't understand,

473
00:41:10,160 --> 00:41:13,760
but hey, we know what to train models that can do super complicated things that we don't understand,

474
00:41:13,760 --> 00:41:18,160
and so maybe you can't explain your model to do it. And I think that really finding the right

475
00:41:18,160 --> 00:41:22,640
balance here where you can have a very trustworthy model, and you know, that there's ideas that we

476
00:41:22,640 --> 00:41:28,400
have for how to actually do it, but maybe you can bootstrap your way to models that can actually

477
00:41:28,400 --> 00:41:32,640
solve problems where we don't even understand the solution, but then they explain, and they have

478
00:41:32,640 --> 00:41:37,440
to really prove to these other models that what they're doing is legit. And I think that this

479
00:41:37,440 --> 00:41:42,080
kind of thing is in our, you know, might take a while to get there, but that isn't our future.

480
00:41:42,080 --> 00:41:52,000
Yeah, some of the broader societal issues that, you know, something like a codex, codex gives rise

481
00:41:52,000 --> 00:42:01,440
to or questions like jobs, copyright, and potentially fairness bias. Who may be dig into those

482
00:42:01,440 --> 00:42:10,320
really quick thoughts on kind of job implications? Yeah, so I think that the interesting thing about

483
00:42:10,320 --> 00:42:16,240
codex in particular, as an example of AI in general, is that it's just not playing out how people

484
00:42:16,240 --> 00:42:21,840
expected, right? I think that the expectation was that AI is going to take this job, and then that

485
00:42:21,840 --> 00:42:28,160
job, and then this job, and the only question is just ordering the jobs in order of automation.

486
00:42:28,880 --> 00:42:34,720
But in reality, I think AI is kind of taking no jobs, and it's taking a percentage of all jobs

487
00:42:34,720 --> 00:42:40,240
at once, and that percentage tends to be the kind of boring, drudge work stuff. And I think that's

488
00:42:40,240 --> 00:42:44,640
actually a pretty inspiring picture, right? You look at it in the case of codex that programming,

489
00:42:44,640 --> 00:42:50,320
you know, being a software engineer requires you to talk to users, understand what users want,

490
00:42:50,320 --> 00:42:54,960
come up with an idea of the thing that they are going to be excited to use, and have this picture

491
00:42:54,960 --> 00:42:58,400
of like how you're going to build it. So there's the architecture of the system. When you come

492
00:42:58,400 --> 00:43:03,360
to implementing, you want to design in a way that will be future compatible. So, you know, tomorrow

493
00:43:03,360 --> 00:43:06,720
users are going to ask you for something else, and you should make it so, you should make it so

494
00:43:06,720 --> 00:43:10,640
it's really easy to build that feature, right? So you kind of anticipate all the different ways

495
00:43:10,640 --> 00:43:16,320
that you might want to modify your system. None of that is, and then you also want to write, you

496
00:43:16,320 --> 00:43:19,680
know, you want to implement using a framework and, you know, know that exactly. After all that,

497
00:43:19,680 --> 00:43:26,160
it's API docs and Stack Overflow. Exactly. Exactly. Exactly. So we actually have very poor tools

498
00:43:26,720 --> 00:43:33,040
for those those that last piece, but that's not what we want to spend our time on. And so I think

499
00:43:33,040 --> 00:43:36,560
what we're going to see with codecs, and I think that this again, I think is representative

500
00:43:36,560 --> 00:43:42,400
of the kind of AI we're building is we're going to find that the kind of like the hard, you know,

501
00:43:42,400 --> 00:43:48,640
the drug work, the part that is like you need to know the whole encyclopedia of your field that,

502
00:43:48,640 --> 00:43:53,360
you know, just like even coming with an idea of where to start, like those problems that I think

503
00:43:53,360 --> 00:43:58,720
are real barriers to people getting started, those are going to start really melting away.

504
00:43:58,720 --> 00:44:02,240
And then that will free up people to actually work on the exciting stuff.

505
00:44:06,080 --> 00:44:10,480
Copyright is the the next one I know that, you know, the big issue here is that there are no

506
00:44:10,480 --> 00:44:15,840
answers, and the system hasn't quite figured it out yet. But I'm wondering what your quick take

507
00:44:15,840 --> 00:44:20,240
is on that. Yep. So, you know, I think that, you know, our position is definitely that, you know,

508
00:44:20,240 --> 00:44:26,640
training training on, you know, publicly available code and and text is very use. But I think that

509
00:44:26,640 --> 00:44:32,560
it's definitely the case that the technology here is running ahead of the law, right? I think that,

510
00:44:32,560 --> 00:44:37,200
you know, that's something that I think is has happened many times in the past. And so I think

511
00:44:37,200 --> 00:44:41,520
that it's time for a public conversation about this, like part of the reason that we're doing

512
00:44:41,520 --> 00:44:46,960
a preview here, you know, that this is a API that will be available starting to roll out now,

513
00:44:46,960 --> 00:44:50,400
is that we want that feedback. We want to start that conversation. And, you know, technologies

514
00:44:50,400 --> 00:44:54,480
like Codex, you know, I think they have a lot of potential. I think we would, you know, be doing

515
00:44:54,480 --> 00:44:59,920
it to service to ourselves if they weren't easy to build on the work, lots of people weren't able

516
00:44:59,920 --> 00:45:04,640
to use them. So I'm very hopeful that we can figure out how do we get the good of these systems

517
00:45:04,640 --> 00:45:09,600
and get lots of benefits. And, you know, just really help, help it, help it supercharge the economy

518
00:45:09,600 --> 00:45:13,040
in a way that we think is, you know, doing the right thing for everyone.

519
00:45:14,080 --> 00:45:20,480
And are there fairness bias types of issues that have come up through in the context of Codex?

520
00:45:20,480 --> 00:45:26,320
For sure. Yeah, I think that fairness and bias are kind of a key part of AI. And I think that,

521
00:45:26,320 --> 00:45:29,600
you know, one thing that, you know, first of all, I think that those issues themselves, I think,

522
00:45:29,600 --> 00:45:34,400
you know, deserve a lot of space because, you know, we're building these systems that, you know,

523
00:45:34,400 --> 00:45:39,840
that they are being trained on data that is generated by all of us, right? And that if you're,

524
00:45:39,840 --> 00:45:43,920
if you're, you know, sort of not careful, you're going to lash onto the wrong things or help amplify

525
00:45:43,920 --> 00:45:47,520
biases that exist in the system. So I think that this is always going to be an important thing.

526
00:45:47,520 --> 00:45:54,080
And the stakes are just going to raise as we go. But I also want to point out that I think that

527
00:45:54,080 --> 00:46:00,080
Codex also represents a bit of a raising of the stakes of the kinds of fallout that you can get from,

528
00:46:00,080 --> 00:46:04,960
from a misbehaving system, right? You know, that if you generate some code with Codex and it does

529
00:46:04,960 --> 00:46:10,560
decide to delete all your files, that's probably not something you want, right? So I think that we need

530
00:46:10,560 --> 00:46:16,320
to figure out what values go into these systems and that, you know, we have some preliminary work on

531
00:46:16,320 --> 00:46:22,400
this that I think we've, we've, you know, published, published a bit on already. But I think you

532
00:46:22,400 --> 00:46:27,360
also need to think about how do you really align these, how do you technically align these systems

533
00:46:27,360 --> 00:46:31,200
with whatever values should be in there? And I think that, you know, look like we've got some

534
00:46:31,200 --> 00:46:35,360
technical problems ahead of us, but I think the question of, you know, both who are the people who

535
00:46:35,360 --> 00:46:39,840
are actually building it and making sure that that that is diverse and representative enough,

536
00:46:39,840 --> 00:46:45,600
I think is pretty, pretty critical. But also the question of, you know, how exactly are those values

537
00:46:45,600 --> 00:46:49,920
chosen, you know, who makes that decision? I think one day that's going to be kind of the most

538
00:46:49,920 --> 00:46:54,720
important problem that we as a community and, you know, we as a society are facing. And so I think

539
00:46:54,720 --> 00:46:58,560
that, you know, it's never too soon to start really, really working hard on these problems.

540
00:47:00,160 --> 00:47:07,280
Related issue is access and accessibility. And that's maybe a segue to kind of the rollout plan

541
00:47:07,280 --> 00:47:13,120
for Codex. Yes. A little bit about that. Yeah. So we really want this technology to be out there

542
00:47:13,120 --> 00:47:17,280
and used. We think you can deliver a lot of value. And we think that it's like a little taste of

543
00:47:17,280 --> 00:47:22,800
a future to come. So that's really important to us. We're going to do the same kind of playbook

544
00:47:22,800 --> 00:47:26,800
we did with the GPT-3, where we're going to have a private beta. We're going to roll it out as

545
00:47:26,800 --> 00:47:31,680
quickly as we can safely. We're going to be scaling it up that the invites will start flowing on

546
00:47:31,680 --> 00:47:36,960
Tuesday. So again, whenever we seize this podcast, that the first invites will all be out. And,

547
00:47:36,960 --> 00:47:40,640
you know, honestly, we just want to learn, right, that we have a new technology here in the best

548
00:47:40,640 --> 00:47:45,280
way to understand how it will impact the world, is by actually seeing it impact the world.

549
00:47:45,280 --> 00:47:51,920
And our philosophy is very much try to get, you know, a broad slice of usage at smaller scale

550
00:47:51,920 --> 00:47:56,160
and scale it up as you go. And there's very particular things that we did for GPT-3. You know,

551
00:47:56,160 --> 00:48:00,320
we have an academic access program in order to make sure that the, you know, the academics are

552
00:48:00,320 --> 00:48:04,480
able to get access. I think that, you know, for this, I think that there's going to be different

553
00:48:04,480 --> 00:48:08,080
segments that are going to be excited about using it. You know, I think that people who are

554
00:48:08,080 --> 00:48:13,040
programming, you know, students, I think are like one segment who we want to make sure that this

555
00:48:13,040 --> 00:48:17,840
is accessible to. So we really want feedback. We really want to see how people are excited about

556
00:48:17,840 --> 00:48:22,240
using our technology. And we are very excited about you using it. And honestly, we need,

557
00:48:22,240 --> 00:48:27,920
need your help to understand it. Awesome. Awesome. And is there, was there something about a

558
00:48:27,920 --> 00:48:34,080
competition that you're hosting for this? Yes. So Thursday, 10 AM. So I don't know what time

559
00:48:34,080 --> 00:48:40,000
you're planning on releasing the podcast. But Thursday, 10 AM, we are going to have a new kind

560
00:48:40,000 --> 00:48:46,800
of programming competition. So you will be able to use Codex as both your teammate and a competitor.

561
00:48:46,800 --> 00:48:52,240
So everyone's going to get access to some number of queries to Codex while doing Python programming

562
00:48:52,240 --> 00:48:57,680
challenges. And it should be very exciting. There will be a leaderboard for the whole internet

563
00:48:57,680 --> 00:49:02,720
racing to solve these challenges. But really, the goal is to get a sense of what does it like to

564
00:49:02,720 --> 00:49:07,200
work alongside Codex. And this is one way we can really accelerate access to everyone and give

565
00:49:07,200 --> 00:49:12,880
them a chance to get a little taste of it. Awesome. Well, of course, we'll have pointers in the

566
00:49:12,880 --> 00:49:19,120
show notes for this episode. But Greg, thanks so much for taking the time to give us what is

567
00:49:19,120 --> 00:49:25,600
effectively a preview, a sneak peek, although it will be released by the time this shows public.

568
00:49:25,600 --> 00:49:35,600
Like, great to have you on the show once again. Yep, great to be back. Thank you so much.

