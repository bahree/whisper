1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:28,720
I'm your host Sam Charrington.

4
00:00:28,720 --> 00:00:33,040
Before we get going, I'd like to send a huge thanks to our friends at HPE for sponsoring

5
00:00:33,040 --> 00:00:38,080
this week's series of shows from the O'Reilly AI Conference in New York City.

6
00:00:38,080 --> 00:00:43,160
At the conference, HPE presented on InfoSight, which is the company's cloud-based AI ops

7
00:00:43,160 --> 00:00:49,240
solution for helping IT organizations better manage and ensure the health of their IT infrastructure

8
00:00:49,240 --> 00:00:50,240
using AI.

9
00:00:50,240 --> 00:00:55,440
I've previously written about AI ops and it's definitely an interesting use case for machine

10
00:00:55,440 --> 00:00:56,440
learning.

11
00:00:56,440 --> 00:01:05,760
To check out what HPE InfoSight is up to in this space, visit Twimbleai.com slash HPE.

12
00:01:05,760 --> 00:01:11,440
Alright everyone, I am here in New York City for the O'Reilly AI Conference and I'm with

13
00:01:11,440 --> 00:01:12,440
Joanna Bryson.

14
00:01:12,440 --> 00:01:15,240
Joanna is a reader at the University of Bath.

15
00:01:15,240 --> 00:01:19,920
Apparently, a reader is something like a super associate professor.

16
00:01:19,920 --> 00:01:25,240
And Joanna's here to speak on maintaining human control of artificial intelligence.

17
00:01:25,240 --> 00:01:30,480
We've been interacting on Twitter for quite some time and it is a pleasure to finally meet

18
00:01:30,480 --> 00:01:31,480
you in person.

19
00:01:31,480 --> 00:01:33,320
I am so bad with these things.

20
00:01:33,320 --> 00:01:35,480
I totally forgot you were that guy on Twitter too.

21
00:01:35,480 --> 00:01:36,480
That is awesome.

22
00:01:36,480 --> 00:01:39,120
Yeah, I'm super happy to be here.

23
00:01:39,120 --> 00:01:41,000
We've said super twice in the same 10 minutes.

24
00:01:41,000 --> 00:01:42,000
I hope that's okay.

25
00:01:42,000 --> 00:01:43,600
It is not a problem at all.

26
00:01:43,600 --> 00:01:46,160
It is not a problem at all, not a problem at all.

27
00:01:46,160 --> 00:01:49,520
Before we got started, we shared quite a bit about Chicago.

28
00:01:49,520 --> 00:01:51,760
We both spent about 10 years or so on Chicago.

29
00:01:51,760 --> 00:01:52,760
Yeah.

30
00:01:52,760 --> 00:01:55,040
And we went to the competing universities there, right?

31
00:01:55,040 --> 00:01:56,040
North Western for me.

32
00:01:56,040 --> 00:01:58,960
And University of Chicago for me as an undergraduate.

33
00:01:58,960 --> 00:01:59,960
Uh-huh.

34
00:01:59,960 --> 00:02:00,960
Uh-huh.

35
00:02:00,960 --> 00:02:05,960
So starting from your undergrad in Chicago, how did you make your way to working in AI?

36
00:02:05,960 --> 00:02:06,960
Oh, yeah.

37
00:02:06,960 --> 00:02:11,080
Well, that is absolutely critical to understand who I am that I did do that undergraduate

38
00:02:11,080 --> 00:02:13,400
degree, the liberal arts degree at Chicago.

39
00:02:13,400 --> 00:02:18,240
So my first degree was basically behavioral sciences, which at that time meant nonclinical

40
00:02:18,240 --> 00:02:19,240
psychology.

41
00:02:19,240 --> 00:02:22,080
They've annoyingly rebranded.

42
00:02:22,080 --> 00:02:25,160
So now it means a business degree, I did not do a business degree.

43
00:02:25,160 --> 00:02:29,200
Anyway, so I actually am interested in the fundamentally what makes, you know, what

44
00:02:29,200 --> 00:02:30,200
is intelligence?

45
00:02:30,200 --> 00:02:31,480
How does it work?

46
00:02:31,480 --> 00:02:33,320
Why does some species use it more than others?

47
00:02:33,320 --> 00:02:35,000
Why do some people use it more than others?

48
00:02:35,000 --> 00:02:38,000
Why do individuals use it more sometimes than other times?

49
00:02:38,000 --> 00:02:42,400
I mean, I am really interested in the blue sky, basic nuts and bolts of intelligence.

50
00:02:42,400 --> 00:02:46,160
And I got my start at Chicago with the liberal arts degree.

51
00:02:46,160 --> 00:02:50,640
And so then when I worked for five years in industry and I went into the master's degree

52
00:02:50,640 --> 00:02:54,840
at Edinburgh, which was at that time, one of the very few places that you could do just

53
00:02:54,840 --> 00:02:56,720
a degree in artificial intelligence.

54
00:02:56,720 --> 00:03:00,720
In fact, their AI department was older than their computer science department there.

55
00:03:00,720 --> 00:03:05,320
So it was no more computer science than it was linguistics or philosophy or music.

56
00:03:05,320 --> 00:03:07,320
They actually had a big music component.

57
00:03:07,320 --> 00:03:09,080
And of course, psychology and neuroscience.

58
00:03:09,080 --> 00:03:14,720
So that was a really great opportunity, too, to really get at what intelligence was.

59
00:03:14,720 --> 00:03:18,280
And I think Europe is still more inclined to be focused on that.

60
00:03:18,280 --> 00:03:24,680
Although there are some great programs in America like Indiana and Colorado, where AI sort

61
00:03:24,680 --> 00:03:28,360
of falls under psychology and not under computer science.

62
00:03:28,360 --> 00:03:35,960
Anyway, I already knew that I kind of wanted to go to MIT because that was the best thing.

63
00:03:35,960 --> 00:03:36,960
I thought.

64
00:03:36,960 --> 00:03:41,000
So I managed to get the letters of reference.

65
00:03:41,000 --> 00:03:46,920
I needed more than the grades and got into MIT for my PhD, which was super interesting

66
00:03:46,920 --> 00:03:51,600
and opened a lot of doors, but was also some kind of a nightmare in many ways.

67
00:03:51,600 --> 00:03:56,440
And especially for someone coming in with a psych degree into the computer science department.

68
00:03:56,440 --> 00:04:01,000
But I did learn some really, again, some really important things about computer science.

69
00:04:01,000 --> 00:04:05,320
Like for example, that there's something that is computational tractability.

70
00:04:05,320 --> 00:04:07,720
There are limits to what we can know.

71
00:04:07,720 --> 00:04:08,720
And there are physical limits.

72
00:04:08,720 --> 00:04:12,320
They take time, space, and energy to do computation.

73
00:04:12,320 --> 00:04:16,680
So then also while I was at MIT, I noticed the people projected.

74
00:04:16,680 --> 00:04:20,560
They over-identified with AI, and they had really weird beliefs about it.

75
00:04:20,560 --> 00:04:24,520
But only in some context, only if it was like, for example, I was working on a robot shaped

76
00:04:24,520 --> 00:04:25,520
like a human.

77
00:04:25,520 --> 00:04:26,520
Right?

78
00:04:26,520 --> 00:04:28,320
There were other robots everywhere that were actually better.

79
00:04:28,320 --> 00:04:32,480
This robot, it turned out, wasn't grounded properly.

80
00:04:32,480 --> 00:04:36,480
So it's like you're going to have to figure out that it wasn't grounded.

81
00:04:36,480 --> 00:04:39,760
We couldn't figure out why the different process, it was supposed to be like a parallel brain.

82
00:04:39,760 --> 00:04:42,360
We couldn't figure out why the different processors didn't talk to each other.

83
00:04:42,360 --> 00:04:47,320
So anyway, the person that figured that out was the one that they hired when they fired

84
00:04:47,320 --> 00:04:48,320
me.

85
00:04:48,320 --> 00:04:53,200
At some point, they cut all the sort of liberal arts people off the project because they weren't

86
00:04:53,200 --> 00:04:55,160
getting the funding they were hoping for.

87
00:04:55,160 --> 00:04:59,280
And so anyway, while I was on that, as I said, I noticed people, well, I couldn't

88
00:04:59,280 --> 00:05:00,680
help but notice they would interrupt me.

89
00:05:00,680 --> 00:05:04,120
I was sitting there and they'd be saying, hey, it'd be unethical to unplug that robot.

90
00:05:04,120 --> 00:05:06,640
And I'd be like, well, it's not plugged in.

91
00:05:06,640 --> 00:05:08,680
So that was how I got into AI ethics.

92
00:05:08,680 --> 00:05:12,760
So I wrote my first paper with a philosopher friend I had named Phil time.

93
00:05:12,760 --> 00:05:18,040
And so that was the way I've got my first AI ethics paper.

94
00:05:18,040 --> 00:05:22,920
And then I was already a mature student because as we also discussed that, that worked in

95
00:05:22,920 --> 00:05:27,280
Chicago in the financial industry for a few years and paid off my undergraduate debts.

96
00:05:27,280 --> 00:05:35,560
So I didn't get many calls when I was looking to work as a professor in America.

97
00:05:35,560 --> 00:05:39,840
So I applied to like two places in the UK and I got offered five interviews.

98
00:05:39,840 --> 00:05:43,160
So I was like, oh, okay, I guess these guys are still into this stuff.

99
00:05:43,160 --> 00:05:47,360
So I went back there and again, the British, like it's not the tenure system.

100
00:05:47,360 --> 00:05:49,960
So I could do whatever I wanted.

101
00:05:49,960 --> 00:05:54,280
They have a really high, or they did at the time have a very high toleration of eccentricity.

102
00:05:54,280 --> 00:05:59,320
And so I was able to pursue the blue sky stuff I was interested in as long as I was really

103
00:05:59,320 --> 00:06:03,880
good at teaching programming and I got the occasionally AI paper out.

104
00:06:03,880 --> 00:06:11,960
So then in things kind of checked the line and then in 2007, the last time we had a president

105
00:06:11,960 --> 00:06:18,160
we were terrified of, George Bush was talking, his administration gave like tens of millions

106
00:06:18,160 --> 00:06:23,880
to Ron Arkin to make ethical AI robots warriors.

107
00:06:23,880 --> 00:06:27,160
Now I thought, I didn't know that Ron Arkin had been trying to get that money for ages.

108
00:06:27,160 --> 00:06:30,680
I thought that this was Bush trying to do like, I would grade with robots because as I

109
00:06:30,680 --> 00:06:32,880
said, people over identified with them.

110
00:06:32,880 --> 00:06:37,480
So I started cranking up the publishing in the AI ethics area.

111
00:06:37,480 --> 00:06:42,200
And so then I started getting like called to the table by governments.

112
00:06:42,200 --> 00:06:43,920
The EU is really good at this stuff.

113
00:06:43,920 --> 00:06:45,960
The British are very good at it.

114
00:06:45,960 --> 00:06:50,600
And they would be asking questions that like nobody knew the answers to like, how is

115
00:06:50,600 --> 00:06:52,360
AI changing society?

116
00:06:52,360 --> 00:06:56,960
But because of the other work I've been doing, all that stuff about like, what is intelligence

117
00:06:56,960 --> 00:07:00,880
for and how does it change the species and what do you use it for?

118
00:07:00,880 --> 00:07:03,800
I was able to more answer those questions than a lot of other people.

119
00:07:03,800 --> 00:07:09,840
So I've found up doing a huge amount of sort of consultations into really interesting

120
00:07:09,840 --> 00:07:20,080
obviously companies, but also more NGOs like OECD and the Red Cross, Chatham House, you

121
00:07:20,080 --> 00:07:25,080
know, people that I was, I didn't think I would have anything to offer, but what's cool

122
00:07:25,080 --> 00:07:28,520
is they bring in like these round tables and then I would be one of the two or three

123
00:07:28,520 --> 00:07:32,000
AI experts and then I'd learn about the other stuff.

124
00:07:32,000 --> 00:07:36,080
So right now what I think is actually the most important stuff I'm working on in terms

125
00:07:36,080 --> 00:07:40,520
of research is that I started working on political economy trying to understand things

126
00:07:40,520 --> 00:07:46,880
like why polarization is correlated with wealth, wealth inequality, not wealth, I mean,

127
00:07:46,880 --> 00:07:48,520
wealth is fine, right?

128
00:07:48,520 --> 00:07:56,520
But when society gets to, when the amount of income is too different and we don't think

129
00:07:56,520 --> 00:07:59,480
it's probably the income that really is doing it, we think it's like how comfortable

130
00:07:59,480 --> 00:08:07,640
you feel, then that tends, although that always, to lead to a high political polarization.

131
00:08:07,640 --> 00:08:13,200
So that again goes back to the original stuff I was interested in about how do we cooperate,

132
00:08:13,200 --> 00:08:16,760
why do we have these conversations, why am I here, you're not paying me, right?

133
00:08:16,760 --> 00:08:20,040
But I think it's a worthwhile thing to do is if somebody wants my opinion, I think it's

134
00:08:20,040 --> 00:08:23,640
the obligation of academics, I mean, that's basically what we are, we're getting paid

135
00:08:23,640 --> 00:08:28,920
by taxpayer money to be receptacles of information and also to do some research, which people

136
00:08:28,920 --> 00:08:32,880
tend to focus on more, but really we were the ones who were good at answering questions.

137
00:08:32,880 --> 00:08:37,040
So then when people ask questions, we should answer, that was a long answer, I'm sorry,

138
00:08:37,040 --> 00:08:38,040
I like that.

139
00:08:38,040 --> 00:08:39,040
Do you want, is that okay?

140
00:08:39,040 --> 00:08:41,280
That is totally fine, that's totally fine.

141
00:08:41,280 --> 00:08:49,160
So one of the things that you touched on and it's a big focus of your, your research is

142
00:08:49,160 --> 00:08:52,120
understanding natural intelligence.

143
00:08:52,120 --> 00:08:56,400
And do you think of that as a prerequisite to understanding artificial intelligence or

144
00:08:56,400 --> 00:08:57,920
talking about artificial intelligence?

145
00:08:57,920 --> 00:09:02,640
No, I mean, obviously not because there's a lot of people who are leaders in AI that

146
00:09:02,640 --> 00:09:06,640
know nothing about natural intelligence and government, unfortunately.

147
00:09:06,640 --> 00:09:12,680
So I mean, one of the other things about Chicago is that politics is like a sport there.

148
00:09:12,680 --> 00:09:19,800
It's like, I watched the Cubs and I read about politics and it was, a lot of people just

149
00:09:19,800 --> 00:09:24,600
show a striking amount of ignorance about that, but there are great, great people in AI,

150
00:09:24,600 --> 00:09:27,160
they're making real contributions to society.

151
00:09:27,160 --> 00:09:31,320
But we need now at this point to realize that once you get to the point where you're really

152
00:09:31,320 --> 00:09:35,680
altering, even if you aren't making that much money like Twitter.

153
00:09:35,680 --> 00:09:39,400
But certainly if you're making a lot of money and also you're altering that landscape

154
00:09:39,400 --> 00:09:44,120
of what it is to be human, you've got to talk with people, you've got to go in and participate

155
00:09:44,120 --> 00:09:45,120
with government.

156
00:09:45,120 --> 00:09:48,440
I mean, really participate, not just try to block them from doing things to you.

157
00:09:48,440 --> 00:09:52,280
I mean, actually think about the problems you're causing in society, you know, the great

158
00:09:52,280 --> 00:09:56,920
power equals great responsibility thing, you know, that we need to get those guys talking.

159
00:09:56,920 --> 00:10:01,920
So no, no, unfortunately, it's not at all true that you have to understand this stuff.

160
00:10:01,920 --> 00:10:06,480
How can we best use our understanding of natural intelligence or even taking a step back?

161
00:10:06,480 --> 00:10:10,040
What do we understand about natural intelligence and how can we apply that to AI?

162
00:10:10,040 --> 00:10:12,960
Is that even a narrow enough question to start to answer?

163
00:10:12,960 --> 00:10:17,640
I mean, I think what academics are is we look at things in nature that are the world

164
00:10:17,640 --> 00:10:22,440
or anything that we can't understand and then we try to say, especially if we feel like

165
00:10:22,440 --> 00:10:26,120
we ought to be able to understand that, I almost understand it by don't quite.

166
00:10:26,120 --> 00:10:31,280
So one of the big questions that motivated me a lot as a PhD student was why are the different

167
00:10:31,280 --> 00:10:32,800
regions of the brain?

168
00:10:32,800 --> 00:10:35,040
Why do they have different architectures, right?

169
00:10:35,040 --> 00:10:38,200
Because why wouldn't nature just find one best architecture, right?

170
00:10:38,200 --> 00:10:40,200
Because it's had billions of years to do that, right?

171
00:10:40,200 --> 00:10:41,600
Why wouldn't it do that?

172
00:10:41,600 --> 00:10:45,080
And again, this was like, I wasn't taking to heart that part about computation I told

173
00:10:45,080 --> 00:10:46,360
you about before.

174
00:10:46,360 --> 00:10:49,920
Now I understand and that was something and it was interesting because I mean, I literally

175
00:10:49,920 --> 00:10:53,240
like really smart people like, you know, Sandy Pentland, some of these people when I was

176
00:10:53,240 --> 00:10:58,400
finishing my PhD, they were the ones who helped me see what I had found, which was that

177
00:10:58,400 --> 00:11:02,640
this explained why the modular approaches to AI were working well, right?

178
00:11:02,640 --> 00:11:08,680
It's that basically you have a lot of learning problems to solve and learning again, it's

179
00:11:08,680 --> 00:11:15,080
about tractability and it's about making it likely that you'll learn it in time, right?

180
00:11:15,080 --> 00:11:20,240
So you can prove sometimes, for example, we don't need in some deeply theoretical sense,

181
00:11:20,240 --> 00:11:25,200
we don't need deep learning, we don't need multiple layers to neurons in theory, like,

182
00:11:25,200 --> 00:11:28,960
you know, a single hidden layer is sufficient to solve any problem.

183
00:11:28,960 --> 00:11:33,920
But in practice, it's incredibly unlikely that you're going to get there with that single

184
00:11:33,920 --> 00:11:34,920
middle layer.

185
00:11:34,920 --> 00:11:39,640
And when you add lots of other layers, you're adding information in that can accelerate

186
00:11:39,640 --> 00:11:40,640
the learning.

187
00:11:40,640 --> 00:11:44,960
Well, of course, if you're accelerating some kinds of learning, you're decelerating other

188
00:11:44,960 --> 00:11:45,960
kinds.

189
00:11:45,960 --> 00:11:49,160
And that's the whole point is that you want to make something that's likely to learn

190
00:11:49,160 --> 00:11:50,840
the problem you're setting it.

191
00:11:50,840 --> 00:11:54,800
And so basically the different parts of the brain are sitting there solving different

192
00:11:54,800 --> 00:11:58,320
kinds of problems, like the problems that I have and the problems that ears have and

193
00:11:58,320 --> 00:12:04,640
the problems of planning and of choosing of all the options before you and of meeting

194
00:12:04,640 --> 00:12:05,640
the goals you have.

195
00:12:05,640 --> 00:12:07,880
I mean, these are just different kinds of problems.

196
00:12:07,880 --> 00:12:12,120
And so they have different architectures that are best able to facilitate you learning

197
00:12:12,120 --> 00:12:15,480
how and also, of course, controlling doing that.

198
00:12:15,480 --> 00:12:22,040
So yeah, I mean, I remember I was just laughing when, you know, the deep mind got sold

199
00:12:22,040 --> 00:12:24,160
for 400 million pounds.

200
00:12:24,160 --> 00:12:26,960
And then they're going, oh, we're providing artificial general intelligence.

201
00:12:26,960 --> 00:12:30,920
It's like, there's never going to be an algorithm that gives you a missing, you know,

202
00:12:30,920 --> 00:12:32,720
there's never going to be a single solution.

203
00:12:32,720 --> 00:12:34,440
And you guys, it's like 14 of them.

204
00:12:34,440 --> 00:12:37,840
And the reason they were getting 400 million pounds was they were really good at tweaking

205
00:12:37,840 --> 00:12:43,560
the parameters and nobody else was as good as they were at it.

206
00:12:43,560 --> 00:12:47,880
That was like, if there was a GI, they would have been worth anything, right?

207
00:12:47,880 --> 00:12:52,720
They were worth a lot of money because they had specialist capacity, some of which were

208
00:12:52,720 --> 00:12:56,760
intuitive, but some of which were obviously that they're smart people and they ran to good

209
00:12:56,760 --> 00:12:57,760
strategies.

210
00:12:57,760 --> 00:13:02,520
Well, maybe let's transition a little bit to the talk that you'll be giving tomorrow

211
00:13:02,520 --> 00:13:05,760
on the topic of maintaining human control of artificial intelligence.

212
00:13:05,760 --> 00:13:11,120
I think, you know, I think of that as a headline or a topic.

213
00:13:11,120 --> 00:13:15,960
In a lot of ways, it's obvious that we need to maintain control of artificial intelligence.

214
00:13:15,960 --> 00:13:19,560
But when you think about that, like, what does that mean to you and why is that important?

215
00:13:19,560 --> 00:13:21,040
Well, it's really interesting.

216
00:13:21,040 --> 00:13:23,160
There's two, there's two big differences.

217
00:13:23,160 --> 00:13:28,760
One is that some people really, really, really want to replace themselves.

218
00:13:28,760 --> 00:13:35,920
Like, it's part of their own drive for immortality is to have these AI offspring.

219
00:13:35,920 --> 00:13:36,920
Yeah.

220
00:13:36,920 --> 00:13:40,240
So there's a surprising number of people are still talking about.

221
00:13:40,240 --> 00:13:43,880
They think like AI is these aliens we've discovered and we need to make friends with

222
00:13:43,880 --> 00:13:44,880
them, you know?

223
00:13:44,880 --> 00:13:49,560
And I wish we would make friends with other people, you know, like the amount of devotion

224
00:13:49,560 --> 00:13:50,560
to that idea.

225
00:13:50,560 --> 00:13:55,520
I mean, strong, emotional, and also some, I mean, some of them are very well informed

226
00:13:55,520 --> 00:13:58,680
philosophers and things like that, you know, this is immoral necessity.

227
00:13:58,680 --> 00:14:04,920
And just like, no, it's like, so I tried for a long time to communicate that, look,

228
00:14:04,920 --> 00:14:06,520
look, we designed something.

229
00:14:06,520 --> 00:14:10,840
So if we build something that required our obligation, we would be being mean to it.

230
00:14:10,840 --> 00:14:14,280
But I think I, in a way, gave the whole idea of more credit than it deserved.

231
00:14:14,280 --> 00:14:17,640
And I should have focused more on the fact that you, I was just doing this logical thing

232
00:14:17,640 --> 00:14:20,400
and saying, look, you know, we shouldn't do that for logical reasons.

233
00:14:20,400 --> 00:14:24,480
But actually pragmatically, we're never going to build something that experiences the world

234
00:14:24,480 --> 00:14:26,280
as much like us as a fruit fly does.

235
00:14:26,280 --> 00:14:30,360
And people kill fruit flights left right in the center, you know, that just the, the,

236
00:14:30,360 --> 00:14:36,240
the phenomenological experience is dependent on all these sorts of, the sensors we have,

237
00:14:36,240 --> 00:14:41,560
the subset of all the computation we can do that we've evolved into, the space that we're

238
00:14:41,560 --> 00:14:42,560
in.

239
00:14:42,560 --> 00:14:44,960
So, but that's not the most important part.

240
00:14:44,960 --> 00:14:48,160
And I think I've just spent too much time on it probably again.

241
00:14:48,160 --> 00:14:54,040
But even that makes me, makes me wonder, granted, we can't create something that has the

242
00:14:54,040 --> 00:14:58,800
same sensory experience as we do because it won't have the same sensors.

243
00:14:58,800 --> 00:15:03,160
But does that also imply that you don't think that we can create something that has a degree

244
00:15:03,160 --> 00:15:04,160
of self-awareness?

245
00:15:04,160 --> 00:15:05,160
Oh.

246
00:15:05,160 --> 00:15:06,160
Yeah.

247
00:15:06,160 --> 00:15:07,160
See, that's again, there's a lot of points.

248
00:15:07,160 --> 00:15:09,400
Am I falling into some trap that everybody falls into?

249
00:15:09,400 --> 00:15:10,400
No, exactly.

250
00:15:10,400 --> 00:15:17,520
Everybody, like, so there's so many terms that, that we, we use to mean human, right?

251
00:15:17,520 --> 00:15:21,680
Actually, the best one, I think the one you're really interested in is moral agent and

252
00:15:21,680 --> 00:15:22,680
moral patient.

253
00:15:22,680 --> 00:15:23,680
That's two terms.

254
00:15:23,680 --> 00:15:27,260
moral agent is something that if it does something that's responsible, we are society

255
00:15:27,260 --> 00:15:28,880
considers it responsible.

256
00:15:28,880 --> 00:15:30,760
And that varies by society.

257
00:15:30,760 --> 00:15:35,400
No, there isn't, there isn't universal agreement about how old you have to be before you're

258
00:15:35,400 --> 00:15:36,400
an adult, right?

259
00:15:36,400 --> 00:15:39,800
I mean, some cultures, you're an adult when your dad dies, and obviously you're only an

260
00:15:39,800 --> 00:15:41,040
adult if you're male.

261
00:15:41,040 --> 00:15:42,040
Right.

262
00:15:42,040 --> 00:15:48,520
So, so this is, on the other hand, the moral patient is the thing you have to take care of.

263
00:15:48,520 --> 00:15:53,880
That's something that, that the society realizes it has, it has obligations towards.

264
00:15:53,880 --> 00:15:57,880
And so that can include things like the environment, you know, things that don't, or babies, right?

265
00:15:57,880 --> 00:16:00,240
Things that are not moral agents themselves.

266
00:16:00,240 --> 00:16:03,760
But, but that we, we think we realize we need to take care of.

267
00:16:03,760 --> 00:16:06,920
Although a lot of philosophers think the reason we have to take care of those things is

268
00:16:06,920 --> 00:16:08,280
because it's in our own interests.

269
00:16:08,280 --> 00:16:13,760
So that basically only moral agents have the, the, the, the foundation of moral patient

270
00:16:13,760 --> 00:16:18,800
agency, but that as we realize the, the sort of interconnected world we live in, then we

271
00:16:18,800 --> 00:16:23,840
extend our needs through these other organisms that, that we have identity with.

272
00:16:23,840 --> 00:16:29,520
And incidentally, that's the best explanation for why we would ever take care of AI is that

273
00:16:29,520 --> 00:16:33,960
if we don't take care of things that we feel empathy for, then we might, then we, we

274
00:16:33,960 --> 00:16:38,960
harden our hearts and we, and we learn to treat things we do.

275
00:16:38,960 --> 00:16:40,760
We should empathize with like other people badly.

276
00:16:40,760 --> 00:16:45,760
So that's like what the British have come up with, with response to that is say, okay,

277
00:16:45,760 --> 00:16:50,440
so don't make AI the reminds you of people, you know, there's two sides of that, right?

278
00:16:50,440 --> 00:16:55,920
So yes, of course we have, I would say we, you know, if, if you think that consciousness

279
00:16:55,920 --> 00:17:01,680
is self, self-awareness, then AI and robots have more of that than we do, right?

280
00:17:01,680 --> 00:17:02,680
They have RAM.

281
00:17:02,680 --> 00:17:04,240
I mean, any computer science has a RAM.

282
00:17:04,240 --> 00:17:07,080
It can know exactly what all the parameters are, right?

283
00:17:07,080 --> 00:17:10,520
Part of what it is to be human is not to have access to all that stuff, actually, because

284
00:17:10,520 --> 00:17:14,360
a lot of it will slow you down and clutter your way forward.

285
00:17:14,360 --> 00:17:18,920
So we, learning for us is about consolidating all that knowledge into the stuff that's

286
00:17:18,920 --> 00:17:22,200
actually going to be most likely to be useful in the future.

287
00:17:22,200 --> 00:17:24,200
So that's not, it's not quite the same thing.

288
00:17:24,200 --> 00:17:28,720
But anyway, let me skip on to the thing that's actually really important, which is I tend

289
00:17:28,720 --> 00:17:34,360
to talk about AI as necessarily extending from humans, like the A is artifact.

290
00:17:34,360 --> 00:17:36,880
So it's something you've made for some purpose.

291
00:17:36,880 --> 00:17:42,840
And so the other side of maintaining human control is actually basically not making something

292
00:17:42,840 --> 00:17:48,000
that's going to be a big dumpster fire, right?

293
00:17:48,000 --> 00:17:51,920
And I was really expecting a technical term there.

294
00:17:51,920 --> 00:17:58,560
So it's like, it almost got, it almost went south from that, but I was good at the last

295
00:17:58,560 --> 00:18:00,560
minute.

296
00:18:00,560 --> 00:18:08,720
But anyway, so the, I am particularly concerned about organizations that deliberately try

297
00:18:08,720 --> 00:18:11,160
to evade responsibility for what they do.

298
00:18:11,160 --> 00:18:17,000
So like when a government comes in and decides that they want to cut taxes for some of their

299
00:18:17,000 --> 00:18:23,040
friends and that the way they'll do that is by stopping services for people that didn't

300
00:18:23,040 --> 00:18:24,920
vote for them.

301
00:18:24,920 --> 00:18:29,840
And then they hide that within like some complicated software that they sort of outsource

302
00:18:29,840 --> 00:18:32,600
and try to obfuscate, right?

303
00:18:32,600 --> 00:18:34,720
So there's been cases of this.

304
00:18:34,720 --> 00:18:40,280
And if you don't have proper rules about accountability, well, if you have proper accountability,

305
00:18:40,280 --> 00:18:45,080
and now just to cut to the boring chase of this, all you need is decent DevOps, right?

306
00:18:45,080 --> 00:18:49,280
You just need to be able to go in and see, you know, what was the rationale to writing

307
00:18:49,280 --> 00:18:51,080
the software, right?

308
00:18:51,080 --> 00:18:57,120
And so you can see what was intended if people documented and logged that properly, right?

309
00:18:57,120 --> 00:19:01,960
Or you can even guess that I mean, they may not say, you know, we want to, you know, do

310
00:19:01,960 --> 00:19:06,680
bad things to the people on the wrong side of the town, but you can still see like, you

311
00:19:06,680 --> 00:19:10,720
know, save money in this neighborhood or something, you know?

312
00:19:10,720 --> 00:19:17,200
So the, the, but for quite a lot of what's happened is that people aren't doing the kind

313
00:19:17,200 --> 00:19:22,280
of standard standard stuff you do in software engineering in AI, again, because it's over

314
00:19:22,280 --> 00:19:25,560
identification, maybe because too many, you know, psychologists or whatever are dropping

315
00:19:25,560 --> 00:19:28,360
in and haven't had their computer science courses.

316
00:19:28,360 --> 00:19:33,240
But maybe, maybe because people just think, oh, you don't have to do that with AI, it's

317
00:19:33,240 --> 00:19:34,400
going to teach itself.

318
00:19:34,400 --> 00:19:37,480
Like it's the, the machine is supposed to responsible for itself.

319
00:19:37,480 --> 00:19:38,480
And we can't do that.

320
00:19:38,480 --> 00:19:40,040
We cannot hold machines responsible.

321
00:19:40,040 --> 00:19:44,160
There's no way, you know, the penalties of law that we've invented dissuade humans.

322
00:19:44,160 --> 00:19:47,520
In fact, they would dissuade sheep if the sheep could understand, right?

323
00:19:47,520 --> 00:19:51,000
So a lot of it is based on, you don't want to lose social status.

324
00:19:51,000 --> 00:19:55,320
You don't want to lose, you know, freedom, you don't want to lose time, right?

325
00:19:55,320 --> 00:19:59,880
Yeah, you're not going to build AI that is sincerely going to care about those kinds

326
00:19:59,880 --> 00:20:05,640
of things in the, in the pervasive way that we do, you know, it's just systemic for us.

327
00:20:05,640 --> 00:20:10,400
When you isolate someone, it is a form of torture, I mean, for any length of time, right?

328
00:20:10,400 --> 00:20:16,000
So the, and yeah, you're not going to make AI that, I mean, if you could, it would be unethical,

329
00:20:16,000 --> 00:20:20,440
but you're not going to make AI that, that, that, that not AI that you can maintain safely,

330
00:20:20,440 --> 00:20:21,440
right?

331
00:20:21,440 --> 00:20:25,360
Again, this is me being, um, probably overly generous.

332
00:20:25,360 --> 00:20:29,880
I don't think there's any way you can do a whole brain uploading, but if you could,

333
00:20:29,880 --> 00:20:34,200
then that would be more like a clone and then that would, and then all the stuff I'm

334
00:20:34,200 --> 00:20:37,440
saying wouldn't apply to that, but you wouldn't be able to maintain an extended, like,

335
00:20:37,440 --> 00:20:38,440
you can suffer.

336
00:20:38,440 --> 00:20:41,280
So it's, it wouldn't be a very good product, right?

337
00:20:41,280 --> 00:20:44,240
It would be, I mean, obviously some people would buy it because as I said, they want

338
00:20:44,240 --> 00:20:46,280
to have perpetual, I mean, so stupid.

339
00:20:46,280 --> 00:20:51,200
Why, why do people think that like AI, their AI child is going to live longer than they

340
00:20:51,200 --> 00:20:52,200
are, right?

341
00:20:52,200 --> 00:20:53,200
Because like, what is it?

342
00:20:53,200 --> 00:20:56,200
Like the meantime to failure of a software system, like five years and humans last like

343
00:20:56,200 --> 00:20:57,200
80 years, right?

344
00:20:57,200 --> 00:21:00,800
You know, that, that's just like, it reminds me of everyone being so excited about an Apple

345
00:21:00,800 --> 00:21:01,800
car.

346
00:21:01,800 --> 00:21:05,680
It's like, have you used a computer, an Apple phone, do you really want an Apple car?

347
00:21:05,680 --> 00:21:11,360
Well, yeah, I'm kind of excited about that, but yeah, but no, I assume that they would

348
00:21:11,360 --> 00:21:15,000
get together with some people who knew how to handle those things, you know, but they,

349
00:21:15,000 --> 00:21:18,520
like, yeah, no, that if you want to do something that's going to last longer than you're going

350
00:21:18,520 --> 00:21:22,040
to live, then you should get, you know, like carve some stuff in stone or something.

351
00:21:22,040 --> 00:21:27,960
It's not about building an eye, you know, or, you know, build a big cathedral thing.

352
00:21:27,960 --> 00:21:30,440
People seem to care about those.

353
00:21:30,440 --> 00:21:31,440
Right.

354
00:21:31,440 --> 00:21:32,440
I cared.

355
00:21:32,440 --> 00:21:33,440
I was really upset.

356
00:21:33,440 --> 00:21:34,440
I lost two hours.

357
00:21:34,440 --> 00:21:37,080
Really, really about that, Twitter.

358
00:21:37,080 --> 00:21:42,560
One of the things that struck me in your right up for your talk and just now, I don't

359
00:21:42,560 --> 00:21:45,600
know that I've ever heard an academic say the word DevOps.

360
00:21:45,600 --> 00:21:50,840
Well, that's, you know, okay, well, there's two parts of that.

361
00:21:50,840 --> 00:21:53,920
First of all, I did spend that five years doing software engineering.

362
00:21:53,920 --> 00:21:54,920
Uh-huh.

363
00:21:54,920 --> 00:21:58,400
Secondly, you know, part of the reasons I mentioned before that was when I was at MIT,

364
00:21:58,400 --> 00:22:01,200
I was a psychologist and I was having a little bit of a struggle.

365
00:22:01,200 --> 00:22:05,400
Actually, the very first course I took every other week was like doing a proof, every

366
00:22:05,400 --> 00:22:07,680
other week was doing like pseudo code.

367
00:22:07,680 --> 00:22:09,800
And I was getting like seas on the proofs and aids.

368
00:22:09,800 --> 00:22:13,960
I was like one of the leading in the class, my first, my, you know, like the, MIT doesn't

369
00:22:13,960 --> 00:22:17,920
program, they build computers from scratch or like out of free or transforms or something.

370
00:22:17,920 --> 00:22:18,920
Right.

371
00:22:18,920 --> 00:22:22,320
So, anyway, but, you know, so, but there were certain struggles I was having.

372
00:22:22,320 --> 00:22:26,680
But I just realized that the whole thing about systems engineering of AI was wide open.

373
00:22:26,680 --> 00:22:28,680
No one was paying attention to it.

374
00:22:28,680 --> 00:22:33,520
So I used to say I did, I did systems AI and, and unfortunately, I guess I wasn't influential

375
00:22:33,520 --> 00:22:34,520
enough then.

376
00:22:34,520 --> 00:22:36,680
Maybe I should try again now to make that a thing.

377
00:22:36,680 --> 00:22:37,680
Not good to talk.

378
00:22:37,680 --> 00:22:38,680
Yeah.

379
00:22:38,680 --> 00:22:39,680
Well, seriously.

380
00:22:39,680 --> 00:22:42,320
But anyway, it's the systems engineering of AI and I kind of want people to be able

381
00:22:42,320 --> 00:22:46,760
to Google that and find out the systems engineering predates even computers.

382
00:22:46,760 --> 00:22:49,840
You know, because there was like these people claiming there's only four people working

383
00:22:49,840 --> 00:22:50,840
on AI safety.

384
00:22:50,840 --> 00:22:51,840
It's such rubbish.

385
00:22:51,840 --> 00:22:56,040
I mean, everybody, there's AI is pervasive in software now.

386
00:22:56,040 --> 00:22:57,040
Everybody's using AI.

387
00:22:57,040 --> 00:23:00,000
Everybody's using machine learning, which is, you know, it's a huge change.

388
00:23:00,000 --> 00:23:02,960
But that's why there's all these guys from like, you know, Accenture and whatever here.

389
00:23:02,960 --> 00:23:03,960
Right.

390
00:23:03,960 --> 00:23:05,840
You know, everybody is using this stuff now.

391
00:23:05,840 --> 00:23:09,920
And everyone who's been developing a system and trying to make sure it stands up is obviously

392
00:23:09,920 --> 00:23:13,920
doing all kinds of AI safety, they're doing all kinds of security.

393
00:23:13,920 --> 00:23:15,960
And they're trying to figure this stuff out.

394
00:23:15,960 --> 00:23:19,560
And so there's thousands of people working in this area and there's probably hundreds

395
00:23:19,560 --> 00:23:20,560
who've published on it.

396
00:23:20,560 --> 00:23:25,360
But they just didn't go out and say, oh, yes, I'm saving the world, you know, I, I, there

397
00:23:25,360 --> 00:23:29,000
are some real problems because there are certain billionaires that only fund existential

398
00:23:29,000 --> 00:23:30,000
threats.

399
00:23:30,000 --> 00:23:33,800
And then there's certain academics that want to bring in millions of dollars or pounds

400
00:23:33,800 --> 00:23:34,800
or something.

401
00:23:34,800 --> 00:23:37,400
And, and so they say, oh, yeah, AI is an existential threat.

402
00:23:37,400 --> 00:23:39,280
It's like, you know, that's not helpful.

403
00:23:39,280 --> 00:23:42,680
But what is an existential life, it's an existential threat, but something that really

404
00:23:42,680 --> 00:23:47,040
screws up a lot of people's lives and can cause war and things like that is when we lose

405
00:23:47,040 --> 00:23:52,160
control of our governments or our corporations, you know, we, we need to figure out how to

406
00:23:52,160 --> 00:23:54,920
plug things together when we change the landscape so much.

407
00:23:54,920 --> 00:24:00,440
There are, you know, serious, serious problems, but it's not the AI itself.

408
00:24:00,440 --> 00:24:05,200
It's, it's, I actually think that the changes are more about the changes that are making

409
00:24:05,200 --> 00:24:10,200
it harder that we need to reinvent governance and that we need to really think about how

410
00:24:10,200 --> 00:24:17,280
to plug the software industry into regulation, have more to do with the digital, you know,

411
00:24:17,280 --> 00:24:24,360
the fact that we can move perfect replicas of things across the planet in an instant.

412
00:24:24,360 --> 00:24:25,680
That's what's really changing things.

413
00:24:25,680 --> 00:24:30,320
And it doesn't matter whether the intelligence is something that's coming from a computer

414
00:24:30,320 --> 00:24:34,160
or the intelligence is coming from eight billion connected humans, right?

415
00:24:34,160 --> 00:24:37,760
Either way, you've just got a totally new situation.

416
00:24:37,760 --> 00:24:41,640
And I think that's where a lot of the regulatory challenges are coming from.

417
00:24:41,640 --> 00:24:46,440
So then when you have a talk called maintaining human control of artificial intelligence,

418
00:24:46,440 --> 00:24:50,200
is it fair to say it's not really about human control of artificial intelligence at all,

419
00:24:50,200 --> 00:24:55,920
but rather about human control of human institutions in a world of artificial intelligence?

420
00:24:55,920 --> 00:24:59,640
I'm going to finish with that stuff, but I will focus on the DevOps.

421
00:24:59,640 --> 00:25:04,400
And what I'm going to, what I do when I'm talking to people that are mostly industry is I try

422
00:25:04,400 --> 00:25:08,080
to reach out with this idea that look, we can do this responsibly.

423
00:25:08,080 --> 00:25:10,880
We know how and we need to help people.

424
00:25:10,880 --> 00:25:16,040
I mean, the main thing is that we want to help the government enforce so that when we do

425
00:25:16,040 --> 00:25:20,800
good practice, other people doing bad practice don't suddenly get lots of money in all the

426
00:25:20,800 --> 00:25:22,680
VC or whatever, you know?

427
00:25:22,680 --> 00:25:27,720
So for the vast majority of actors, it's in all of our interests to do things that make

428
00:25:27,720 --> 00:25:29,960
the software stable, right?

429
00:25:29,960 --> 00:25:33,880
So yeah, so I've learned DevOps just by going to these kinds of meetings, right?

430
00:25:33,880 --> 00:25:38,360
That's not what we call it in the East, but that's okay, you know?

431
00:25:38,360 --> 00:25:44,040
But the focus on the problem and the solution, it's just unbelievable that people are sitting

432
00:25:44,040 --> 00:25:45,040
here.

433
00:25:45,040 --> 00:25:51,640
I mean, I've talked to the head of, I'm trying to say this, I will go ahead and say this.

434
00:25:51,640 --> 00:25:55,040
There's this thing that European Union, I have unbelievable respect for them.

435
00:25:55,040 --> 00:25:58,400
They are leading an AI in regulation, I'm about to say something bad, you can see it's

436
00:25:58,400 --> 00:26:01,880
because I'm going to say out the caveats first, they're about great, they are, right?

437
00:26:01,880 --> 00:26:05,320
So you know, it's 550 million people, I don't want to like run it down.

438
00:26:05,320 --> 00:26:08,560
I think it's like one of the, they're leading the free world right now and they're certainly

439
00:26:08,560 --> 00:26:14,840
leading an AI ethics and regulation, but they put together this high level experts group

440
00:26:14,840 --> 00:26:19,080
and there's like 52 people in it, which it was supposed to be small, but somehow, you

441
00:26:19,080 --> 00:26:20,480
know, things happened.

442
00:26:20,480 --> 00:26:21,480
Those are super smart people.

443
00:26:21,480 --> 00:26:26,160
There's some of my friends and colleagues that I work with, I have lots of respect for,

444
00:26:26,160 --> 00:26:27,160
you know?

445
00:26:27,160 --> 00:26:31,400
But I wound up at this European Union meeting with the guy that was the chair of that group

446
00:26:31,400 --> 00:26:37,000
and he had never heard about logging, you know, about revision control.

447
00:26:37,000 --> 00:26:43,520
He had no idea how easy it is to demonstrate that you follow good practice on AI and he

448
00:26:43,520 --> 00:26:47,880
had never thought about AI as a product being developed so that you have the same kinds

449
00:26:47,880 --> 00:26:51,800
of due diligence responsibilities as any other product.

450
00:26:51,800 --> 00:26:56,120
Now this stuff is already happening in the automotive industry, for example.

451
00:26:56,120 --> 00:27:00,800
That's why every time there's an accident or incident happening with a driverless car,

452
00:27:00,800 --> 00:27:01,800
it's on the front page.

453
00:27:01,800 --> 00:27:02,800
What went wrong?

454
00:27:02,800 --> 00:27:03,800
Why it went wrong?

455
00:27:03,800 --> 00:27:05,560
You know, what the car perceived?

456
00:27:05,560 --> 00:27:06,560
How it was developed?

457
00:27:06,560 --> 00:27:08,040
Why that thing happened?

458
00:27:08,040 --> 00:27:09,040
Why?

459
00:27:09,040 --> 00:27:12,760
Because that's a decently regulated space and AI doesn't change the fact that automotive

460
00:27:12,760 --> 00:27:14,320
is decently regulated.

461
00:27:14,320 --> 00:27:17,360
So what's not regulated because it's new and nobody's written the rules of stuff like

462
00:27:17,360 --> 00:27:19,320
social media and things like that.

463
00:27:19,320 --> 00:27:23,400
But there's nothing about the AI that's being used there that makes it any harder for

464
00:27:23,400 --> 00:27:28,040
us to know about what's going on in those companies and whether they were paying attention

465
00:27:28,040 --> 00:27:31,520
to the right things, then it does an automotive company.

466
00:27:31,520 --> 00:27:34,600
It's just that there hasn't been the history and so the people coming in, you know, the

467
00:27:34,600 --> 00:27:38,560
jot out of college when they were, you know, first years or whatever, just didn't realize

468
00:27:38,560 --> 00:27:40,600
what they needed to do.

469
00:27:40,600 --> 00:27:46,040
And the government hasn't, it's possible that governments need some kind of specialists

470
00:27:46,040 --> 00:27:51,280
like they have for environmental enforcement, right, you know, or medical that they need

471
00:27:51,280 --> 00:27:56,520
to bring together people that can go through and check, you know, the admin logs.

472
00:27:56,520 --> 00:28:01,920
But you know, we've just gotten incredibly sloppy and you can read one of my colleagues

473
00:28:01,920 --> 00:28:06,960
say to Gears has written some amazing thing about the agile turn that talking about how

474
00:28:06,960 --> 00:28:10,960
everybody's throwing their software together so fast they have no idea where their software

475
00:28:10,960 --> 00:28:12,440
libraries are coming from.

476
00:28:12,440 --> 00:28:15,680
And of course, famously, Frank Pasquale has done the same thing about data, just showing

477
00:28:15,680 --> 00:28:20,120
that data is coming from all over incredibly unethically the way it's passing between

478
00:28:20,120 --> 00:28:21,120
hands.

479
00:28:21,120 --> 00:28:26,360
We need to be able to demonstrate the provenance of our, the libraries we link to and the

480
00:28:26,360 --> 00:28:31,280
data we train our systems from because there are bad actors out there, you know, but it's

481
00:28:31,280 --> 00:28:33,520
just, it is basic DevOps.

482
00:28:33,520 --> 00:28:36,760
It's really not, it's not, it's not rocket science at all.

483
00:28:36,760 --> 00:28:42,040
It really is something that it's basic administration, it's basic bookkeeping and that's how we

484
00:28:42,040 --> 00:28:43,040
need, you know.

485
00:28:43,040 --> 00:28:47,200
And so they got super, I got to say these guys, it wasn't just, it was like several of

486
00:28:47,200 --> 00:28:50,640
the people that were looking at this, that there was, again, apparently a dumpster fire

487
00:28:50,640 --> 00:28:53,200
that was this, this high-level expert group.

488
00:28:53,200 --> 00:28:56,480
And they're just like, nobody told us this, I'm like, I don't know what's going on with

489
00:28:56,480 --> 00:29:02,320
the other 52 people, right, but, but maybe, I mean, I think this is an important thing.

490
00:29:02,320 --> 00:29:05,680
We were talking about our education and our bank and everybody's a little different.

491
00:29:05,680 --> 00:29:09,680
One of my colleagues at Princeton, our veterinarianin, has this wonderful paper showing, well, it's

492
00:29:09,680 --> 00:29:13,760
both terrifying and wonderful, that you can uniquely identify someone if you have like

493
00:29:13,760 --> 00:29:17,640
15 of their T.co links from their browser history.

494
00:29:17,640 --> 00:29:19,840
So T.co means it's Twitter.

495
00:29:19,840 --> 00:29:24,440
And so you can tell which tweets those links came from because they're compressed, you

496
00:29:24,440 --> 00:29:25,880
know, you are else, right?

497
00:29:25,880 --> 00:29:31,200
And then from seeing what links you've got it's about to click, you can, you can, for

498
00:29:31,200 --> 00:29:36,560
most, like for 95% of people, you can uniquely identify them because we all follow different

499
00:29:36,560 --> 00:29:38,600
people on Twitter, right?

500
00:29:38,600 --> 00:29:41,480
And so you can just, and that's the mother of public, right?

501
00:29:41,480 --> 00:29:44,040
So in the one hand, it's terrifying from a privacy perspective.

502
00:29:44,040 --> 00:29:46,680
But what's cool about it is it just shows we're all different.

503
00:29:46,680 --> 00:29:49,480
We all have different combinations and sets of knowledge.

504
00:29:49,480 --> 00:29:54,000
And yeah, so I'm the one person that happens to be in AI ethics because of the paper I wrote

505
00:29:54,000 --> 00:29:59,400
on as a PhD student about people over identifying with my robot and someone who knows what DevOps

506
00:29:59,400 --> 00:30:00,400
is, right?

507
00:30:00,400 --> 00:30:04,280
You know, like, and so, and so that's the, you know, that's something useful.

508
00:30:04,280 --> 00:30:11,560
It does strike me that the DevOps and specifically the disciplines that we're talking about, understanding

509
00:30:11,560 --> 00:30:17,280
logging and versioning and all of these things, they're necessary, but not sufficient for

510
00:30:17,280 --> 00:30:20,280
providing the kind of control that you're talking about.

511
00:30:20,280 --> 00:30:21,280
Right.

512
00:30:21,280 --> 00:30:24,880
But what that stuff is for is for providing accountability.

513
00:30:24,880 --> 00:30:30,600
If the companies want to defend themselves, if something goes horribly wrong, then you

514
00:30:30,600 --> 00:30:36,760
have to say who should pay for it, basically, you know, the taxpayers paying for it or,

515
00:30:36,760 --> 00:30:39,040
you know, are the companies paying for it?

516
00:30:39,040 --> 00:30:41,760
And in particular, when you're looking at things where you're saying that, you know, did

517
00:30:41,760 --> 00:30:45,840
some third country interfere with your election or whatever, you know, there, it really matters

518
00:30:45,840 --> 00:30:50,720
that you get to the nuts and bolts about why was things written the way they were written?

519
00:30:50,720 --> 00:30:54,600
And so if people don't follow DevOps, right?

520
00:30:54,600 --> 00:30:57,400
So first of all, you're right, you need somebody who would go through and read that stuff,

521
00:30:57,400 --> 00:30:58,400
right?

522
00:30:58,400 --> 00:31:00,920
And secondly, of course, they're still going to be unanticipated things.

523
00:31:00,920 --> 00:31:05,600
They will happen like the, you know, like flash crashes and stuff, like we just, we get

524
00:31:05,600 --> 00:31:07,960
better and better at handling those things, right?

525
00:31:07,960 --> 00:31:10,280
So I, you know, people freak out about flash crashes.

526
00:31:10,280 --> 00:31:13,880
I think the fact that they're only flash crashes, you know, like, okay, some people lost

527
00:31:13,880 --> 00:31:14,880
money.

528
00:31:14,880 --> 00:31:19,120
I understand that couldn't devastate some lives, but compared to like the 1929 when people

529
00:31:19,120 --> 00:31:23,920
were starving in the streets, which did not happen at least in America in 2008, incidentally,

530
00:31:23,920 --> 00:31:27,800
as bad as 2008 was, it wasn't like what 1929 was, right?

531
00:31:27,800 --> 00:31:31,080
So we are getting better and better at recovering from this stuff and we can dwell too much

532
00:31:31,080 --> 00:31:32,560
on the negative.

533
00:31:32,560 --> 00:31:37,200
But no, it is one part, but it's a core part and it's part, part of why it's important

534
00:31:37,200 --> 00:31:39,000
is about the demystification.

535
00:31:39,000 --> 00:31:43,720
Part of it is because then we can make sure that corporations absorb their fair share

536
00:31:43,720 --> 00:31:48,960
of the, of the costs when we're cleaning up from messes, right?

537
00:31:48,960 --> 00:31:54,320
And also, hopefully by making them do that, we make it less likely messes happen, right?

538
00:31:54,320 --> 00:31:58,680
But part of it is just to make it clear that you can do that stuff, right?

539
00:31:58,680 --> 00:32:03,600
And to help people understand, all right, oh, yeah, it's just, it's an ambient technology

540
00:32:03,600 --> 00:32:04,600
now.

541
00:32:04,600 --> 00:32:08,480
You know, just like we're going to look back and before there was newspapers, you know,

542
00:32:08,480 --> 00:32:10,560
or whatever, we can say what changed.

543
00:32:10,560 --> 00:32:16,040
You know, this, I had no idea about this, that polarization increased when people started

544
00:32:16,040 --> 00:32:17,640
getting their newspapers.

545
00:32:17,640 --> 00:32:20,240
Do you have an economic polarization?

546
00:32:20,240 --> 00:32:22,040
Yeah, no, political polarization.

547
00:32:22,040 --> 00:32:23,040
Political polarization.

548
00:32:23,040 --> 00:32:25,960
That would actually help people to get more information.

549
00:32:25,960 --> 00:32:30,120
But unfortunately, what it happened was they then started focusing at the national level

550
00:32:30,120 --> 00:32:35,600
and using and getting these national identity things and not, and so it actually, yeah,

551
00:32:35,600 --> 00:32:40,240
that was like, apparently that was part of what, getting mail delivered or something,

552
00:32:40,240 --> 00:32:44,800
that they, they, they, you know, it's like, I can't even keep all this stuff on my head.

553
00:32:44,800 --> 00:32:45,800
I get it from other people.

554
00:32:45,800 --> 00:32:46,800
But it was amazing.

555
00:32:46,800 --> 00:32:50,720
And that's only 100 years ago, the, the rate at which were changing society in terms

556
00:32:50,720 --> 00:32:52,720
of information is outstanding.

557
00:32:52,720 --> 00:32:57,320
And so again, projecting this onto AI and saying something's going to happen in 60 years

558
00:32:57,320 --> 00:32:59,840
or 30 years is not helpful.

559
00:32:59,840 --> 00:33:05,040
What we need to do is to see what we've already done and to recognize how much things are

560
00:33:05,040 --> 00:33:09,520
already changing and who's explaining that, you know, although one of the scary things,

561
00:33:09,520 --> 00:33:13,120
of course, is that every time you do discover something like this, you're also handing the

562
00:33:13,120 --> 00:33:18,800
levers to the bad actors, you know, as we saw with the, that PNAS article that came out,

563
00:33:18,800 --> 00:33:19,800
what was it?

564
00:33:19,800 --> 00:33:21,800
It was like 2012 or something.

565
00:33:21,800 --> 00:33:27,280
It showed that the one that you showed that you, you need like, I don't know, 80 Facebook

566
00:33:27,280 --> 00:33:30,400
likes and you could predict who someone was going to vote for better than their partner

567
00:33:30,400 --> 00:33:31,400
could.

568
00:33:31,400 --> 00:33:32,400
Right?

569
00:33:32,400 --> 00:33:36,840
You know, I remember seeing that and I thought, oh, but I actually thought a bigger deal

570
00:33:36,840 --> 00:33:42,000
was a paper that came out of Microsoft and like NYU about how you could tell how people

571
00:33:42,000 --> 00:33:45,840
are going to vote from their connect, their connect to sitting on top of their television

572
00:33:45,840 --> 00:33:46,840
set.

573
00:33:46,840 --> 00:33:50,800
And you can also tell when people are going to get divorced and you can tell, you know,

574
00:33:50,800 --> 00:33:53,480
as you could just see whether they're looking at each other, whether they're looking at

575
00:33:53,480 --> 00:33:58,080
the TV, when the commercials come on for the different presidential candidates, you

576
00:33:58,080 --> 00:34:03,120
know, you have all this information there and like no one is talking about this, right?

577
00:34:03,120 --> 00:34:07,040
And then in the end, that was, I thought I wrote a paper in 2014 that came out in 2015

578
00:34:07,040 --> 00:34:12,080
that predicted the 2016 might be altered by AI, but unfortunately I said by that Microsoft

579
00:34:12,080 --> 00:34:13,280
connect stuff, right?

580
00:34:13,280 --> 00:34:19,400
So I was near miss, but you know, obviously if you have this information, people are going

581
00:34:19,400 --> 00:34:20,880
to use it.

582
00:34:20,880 --> 00:34:27,800
And so apparently this is another crazy thing, autocrats usually really do their populists,

583
00:34:27,800 --> 00:34:30,720
but they usually really do make make the poor better off.

584
00:34:30,720 --> 00:34:32,560
They usually wind up giving their money.

585
00:34:32,560 --> 00:34:36,360
So it's possible, and again, we haven't seen this yet, but maybe the Trump is the first

586
00:34:36,360 --> 00:34:38,040
one who doesn't do that.

587
00:34:38,040 --> 00:34:43,120
And that might be because he has the social, the political science that says, oh, you know,

588
00:34:43,120 --> 00:34:47,080
decreasing polar, decreasing inequality actually decreases polarization.

589
00:34:47,080 --> 00:34:50,560
So you'd probably be less likely to get reelected if you actually helped the people that voted

590
00:34:50,560 --> 00:34:51,560
you for you.

591
00:34:51,560 --> 00:34:56,040
I'm wondering, do you have kind of three key takeaways for that you're planning to add

592
00:34:56,040 --> 00:34:57,040
it to us?

593
00:34:57,040 --> 00:35:02,280
I'm sorry.

594
00:35:02,280 --> 00:35:07,320
People usually come up with cool edits that at the end, but I realize it's a lot of work.

595
00:35:07,320 --> 00:35:11,000
So Joanna, you're giving this talk tomorrow, human control of artificial intelligence is

596
00:35:11,000 --> 00:35:12,000
a necessity.

597
00:35:12,000 --> 00:35:15,040
What are the three key takeaways that you're hoping folks will walk away with?

598
00:35:15,040 --> 00:35:17,920
Well, I think, I think there's actually only two.

599
00:35:17,920 --> 00:35:21,240
I mean, no, me, okay, I'll make it three.

600
00:35:21,240 --> 00:35:22,240
One is fun.

601
00:35:22,240 --> 00:35:23,240
That is possible.

602
00:35:23,240 --> 00:35:24,240
Right.

603
00:35:24,240 --> 00:35:27,040
So the most important thing is just realizing that it is an artifact.

604
00:35:27,040 --> 00:35:29,880
It's not only something we can do but it's something we should do.

605
00:35:29,880 --> 00:35:37,880
Secondly, that all maintaining responsibility involves is keeping track of what humans do.

606
00:35:37,880 --> 00:35:41,040
We don't need to know what every weight in a deep neural network does.

607
00:35:41,040 --> 00:35:45,160
We need to know who trained it when they thought they were done.

608
00:35:45,160 --> 00:35:49,840
What tests did they use, what libraries did they use?

609
00:35:49,840 --> 00:35:51,320
And that's about it, right?

610
00:35:51,320 --> 00:35:54,480
And maybe we also want to be able to go back and do some forensics.

611
00:35:54,480 --> 00:36:00,040
If it comes up with a result we don't like, and then we say, okay, let's test to make

612
00:36:00,040 --> 00:36:06,080
sure that it wasn't picking up on some defended characteristics, as we say in the UK.

613
00:36:06,080 --> 00:36:10,840
And try changing a few things in the resume and see if we can get the result we expected.

614
00:36:10,840 --> 00:36:11,840
Right.

615
00:36:11,840 --> 00:36:15,960
But those are things that we can do, even if we have a completely black box around the

616
00:36:15,960 --> 00:36:20,600
AI from the outside, just as long as we have government inspectors can go through and check

617
00:36:20,600 --> 00:36:23,560
the logs and make sure people file a good practice, right?

618
00:36:23,560 --> 00:36:25,440
So that's the main, the key thing.

619
00:36:25,440 --> 00:36:32,240
And then the third thing is that, or the original second thing, the first one, the third thing

620
00:36:32,240 --> 00:36:37,120
is that it isn't that AI is threatening us.

621
00:36:37,120 --> 00:36:42,400
That we are finding new ways to express power over each other.

622
00:36:42,400 --> 00:36:46,880
And it's really in the interest of everybody, all the big companies, all the little companies,

623
00:36:46,880 --> 00:36:53,560
all the medium sized companies, the ordinary people to get involved and the users, I fight

624
00:36:53,560 --> 00:36:54,560
for the users.

625
00:36:54,560 --> 00:36:55,560
I'm sorry, I digress.

626
00:36:55,560 --> 00:36:56,560
Right.

627
00:36:56,560 --> 00:37:05,000
It's in everybody's interest to live in a society where there is a stable and foreseeable

628
00:37:05,000 --> 00:37:10,960
economy and politics, so it makes sense to pay our taxes and to participate in governments.

629
00:37:10,960 --> 00:37:11,960
Awesome.

630
00:37:11,960 --> 00:37:13,960
Well, Joanna, thanks so much for taking the time.

631
00:37:13,960 --> 00:37:16,200
Again, I made something hard to add to that.

632
00:37:16,200 --> 00:37:17,200
Sorry.

633
00:37:17,200 --> 00:37:21,440
No, well, yeah, thanks for doing this.

634
00:37:21,440 --> 00:37:22,440
Absolutely.

635
00:37:22,440 --> 00:37:23,440
Absolutely.

636
00:37:23,440 --> 00:37:24,440
It's kind of fun.

637
00:37:24,440 --> 00:37:25,440
Thank you.

638
00:37:25,440 --> 00:37:29,440
And nice to meet you too.

639
00:37:29,440 --> 00:37:30,440
All right, everyone.

640
00:37:30,440 --> 00:37:32,160
That's our show for today.

641
00:37:32,160 --> 00:37:36,840
If you like what you've heard here, please do us a huge favor and tell your friends about

642
00:37:36,840 --> 00:37:38,000
the show.

643
00:37:38,000 --> 00:37:42,040
And if you haven't already hit that subscribe button yourself, make sure you do so you

644
00:37:42,040 --> 00:37:45,600
don't miss any of the great episodes we've gotten in store for you.

645
00:37:45,600 --> 00:37:50,840
For more information on any of the shows in our AI conference series, visit twimolei.com

646
00:37:50,840 --> 00:37:54,640
slash AINY19.

647
00:37:54,640 --> 00:37:57,440
Thanks again to HPE for sponsoring the series.

648
00:37:57,440 --> 00:38:01,800
Make sure to check them out at twimolei.com slash HPE.

649
00:38:01,800 --> 00:38:05,120
As always, thanks so much for listening and catch you next time.

