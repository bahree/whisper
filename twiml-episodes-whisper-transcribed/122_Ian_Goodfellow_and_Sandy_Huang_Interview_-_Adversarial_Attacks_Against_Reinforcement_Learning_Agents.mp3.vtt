WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.560
I'm your host Sam Charrington.

00:32.560 --> 00:37.200
Before we hop into today's interview, I'd like to send a huge shout out to everyone

00:37.200 --> 00:41.360
who participated in the Twimble Online Meetup earlier this week.

00:41.360 --> 00:46.760
In our community segment, we had a very fun and wide-ranging discussion about freezing

00:46.760 --> 00:50.840
your brain, and if you missed that startup's announcement this week, you probably have

00:50.840 --> 00:55.560
no idea what I'm talking about, as well as machine learning in AI in the healthcare

00:55.560 --> 00:58.560
space and more.

00:58.560 --> 01:04.040
Community member Nicholas Teague, who goes by underscore Nick T underscore on Twitter, also

01:04.040 --> 01:08.880
briefly spoke about his essay, A Toddler Learns to Speak, where he explores connections

01:08.880 --> 01:11.680
between different modalities in machine learning.

01:11.680 --> 01:17.320
Finally, a hearty thanks to Sean Devlin, who presented a deep dive on deep reinforcement

01:17.320 --> 01:21.640
learning and Google deep mind seminal paper in the space.

01:21.640 --> 01:27.320
Be on the lookout for the video recording and details on next month's meetup at twimblei.com

01:27.320 --> 01:30.800
slash meetup.

01:30.800 --> 01:36.360
Now you all know I travel to a ton of events each year, and event season is just getting

01:36.360 --> 01:38.000
underway for me.

01:38.000 --> 01:43.320
One of the events I'm most excited about is my very own AI summit, the successor to the

01:43.320 --> 01:47.440
awesome future of data summit event I produced last year.

01:47.440 --> 01:52.240
This year's event takes place April 30th and May 1st and is once again being held in

01:52.240 --> 01:57.800
Las Vegas in conjunction with the interop ITX conference.

01:57.800 --> 02:02.680
This year's event is much more AI focused and is targeting enterprise line of business

02:02.680 --> 02:08.480
and IT managers and leaders who want to get smart on AI very quickly.

02:08.480 --> 02:14.880
Think of it as a two day no fluff technical MBA in machine learning and AI.

02:14.880 --> 02:18.960
I'll be presenting a machine learning and AI bootcamp and I'll have experts coming

02:18.960 --> 02:24.600
in to present many workshops on topics like computer vision, natural language processing

02:24.600 --> 02:31.400
and conversational applications, machine learning and AI for IOT and industrial applications,

02:31.400 --> 02:37.600
data management for AI, building an AI first culture in your organization and operationalizing

02:37.600 --> 02:40.240
machine learning and AI.

02:40.240 --> 02:49.360
For more information on the program, visit twimmolai.com slash AI summit.

02:49.360 --> 02:54.720
In this episode, I'm joined by Ian Goodfellow, staff research scientist at Google Brain

02:54.720 --> 03:01.320
and Sandy Huang, PhD student in the EECS department at UC Berkeley to discuss their work on the

03:01.320 --> 03:05.840
paper, adversarial attacks on neural network policies.

03:05.840 --> 03:10.200
If you're a regular listener here, you've probably heard of adversarial attacks and have

03:10.200 --> 03:15.080
seen examples of deep learning based object detectors that can be fooled into thinking

03:15.080 --> 03:20.520
that, for example, a giraffe is actually a school bus by injecting some imperceptible

03:20.520 --> 03:22.760
noise into an image.

03:22.760 --> 03:28.320
Well, Sandy and Ian's paper sits at the intersection of adversarial attacks and reinforcement

03:28.320 --> 03:29.320
learning.

03:29.320 --> 03:33.440
Another area we've discussed quite a bit on the podcast.

03:33.440 --> 03:37.920
In their paper, they discuss how adversarial attacks can be effective at targeting neural

03:37.920 --> 03:41.200
network policies and reinforcement learning.

03:41.200 --> 03:45.920
Sandy gives us an overview of the paper, including how changing a single pixel can throw off

03:45.920 --> 03:49.440
performance of a model train to play Atari games.

03:49.440 --> 03:55.040
We also cover a lot of interesting topics relating to adversarial attacks in RL individually

03:55.040 --> 04:00.560
and some related areas such as hierarchical reward functions and transfer learning.

04:00.560 --> 04:04.720
This was a great conversation that I'm really excited to bring to you.

04:04.720 --> 04:07.560
And now on to the show.

04:07.560 --> 04:11.680
All right, everyone.

04:11.680 --> 04:15.880
I am on the line with Ian Goodfellow and Sandy Huang.

04:15.880 --> 04:22.880
Ian is a staff research scientist at Google Brain and Sandy is a PhD student at UC Berkeley.

04:22.880 --> 04:25.720
Ian and Sandy, welcome to the podcast.

04:25.720 --> 04:26.720
Thank you.

04:26.720 --> 04:27.720
Thanks.

04:27.720 --> 04:28.720
Fantastic.

04:28.720 --> 04:31.120
I am so excited to have you both on the show.

04:31.120 --> 04:36.680
You recently published a paper called Adversarial Attacks on Neural Network Policies and I'm

04:36.680 --> 04:41.000
really looking forward to digging into some of what you worked on together.

04:41.000 --> 04:45.400
But before we do that, why don't we take a moment to have each of you introduce yourselves

04:45.400 --> 04:46.880
to the audience?

04:46.880 --> 04:47.880
Ian?

04:47.880 --> 04:49.200
Hi, I'm Ian.

04:49.200 --> 04:54.680
I lead a team at Google where we study adversarial machine learning that can be generative adversarial

04:54.680 --> 04:59.800
networks that can also be adversarial examples, which will be the main thing we talk about today.

04:59.800 --> 05:04.000
I've been working on deep learning since about 10 years ago, really.

05:04.000 --> 05:07.160
I got into deep learning when it was a very academic thing.

05:07.160 --> 05:11.640
I spent my PhD studying deep learning and then when I came and did an internship at Google,

05:11.640 --> 05:16.600
I got interested in the security side of machine learning when I helped Christian Zegity

05:16.600 --> 05:19.200
write the first paper on adversarial examples.

05:19.200 --> 05:23.000
And that's my main focus today is making sure that machine learning is secure.

05:23.000 --> 05:28.520
Awesome. And what prompted you to study machine learning for your graduate degree?

05:28.520 --> 05:31.800
I was really interested in figuring out how intelligence works.

05:31.800 --> 05:36.880
As an undergrad, I started out taking a psychology class freshman year and then I decided that

05:36.880 --> 05:39.680
it wasn't quite concrete and technical enough.

05:39.680 --> 05:44.560
So I moved on to cognitive science and then neuroscience and then eventually I decided

05:44.560 --> 05:49.120
that I'd be more likely to figure out how intelligence works by studying machine learning

05:49.120 --> 05:50.120
directly.

05:50.120 --> 05:54.760
It's really, really hard to reverse engineer the human brain partly just because so far

05:54.760 --> 05:58.800
we haven't had the tools to look in and measure the activity of all the neurons.

05:58.800 --> 06:03.080
We can't measure as many neurons simultaneously as we would like to.

06:03.080 --> 06:07.160
During my PhD, I was really interested just in getting machine learning to work, just

06:07.160 --> 06:09.440
making AI start to happen.

06:09.440 --> 06:14.040
And now that that ball has started rolling, I'm a lot more interested in making sure that

06:14.040 --> 06:18.080
there's a good outcome as AI develops further.

06:18.080 --> 06:21.800
Like Google, we have a lot of different teams like the people and AI research group that

06:21.800 --> 06:25.680
study different aspects of how AI relates to society.

06:25.680 --> 06:29.080
I started a group that works on adversarial machine learning because I want to make sure

06:29.080 --> 06:32.640
that systems with machine learning in them are secure.

06:32.640 --> 06:38.120
That people on the outside can't intentionally mess with the machine learning system and

06:38.120 --> 06:41.360
cause it to do what they would like it to do rather than what the designers would like

06:41.360 --> 06:42.360
it to do.

06:42.360 --> 06:43.360
Awesome.

06:43.360 --> 06:44.360
How about you, Sandy?

06:44.360 --> 06:47.000
How'd you get involved in machine learning and AI?

06:47.000 --> 06:52.520
Yeah, I think like Ian, my interest also started when I was at undergrad.

06:52.520 --> 06:58.520
I started off being really interested in computer science, but more from a bio computation side

06:58.520 --> 07:03.440
of things, I was really interested in understanding how DNA works, understanding genetics, that sort

07:03.440 --> 07:04.440
of thing.

07:04.440 --> 07:10.240
But then, as I was thinking more biocomp classes, I realized that I was actually most interested

07:10.240 --> 07:15.280
in the machine learning discoveries that had been made, like machine learning driven discoveries

07:15.280 --> 07:18.360
that were made in biocomputation.

07:18.360 --> 07:23.560
For example, clustering, breast cancer genes to figure out those two different types.

07:23.560 --> 07:29.720
That's when, junior year, I started taking more AI, more machine learning classes, and

07:29.720 --> 07:33.520
that's when I realized that, okay, there's a lot more to learn here, and I wanted to

07:33.520 --> 07:37.960
do a PhD in this area and see what else I can figure out.

07:37.960 --> 07:38.960
Awesome.

07:38.960 --> 07:44.360
And you're advised by both Peter Biel, who's been on the show before and Anka?

07:44.360 --> 07:45.360
Is that right?

07:45.360 --> 07:46.360
Yes.

07:46.360 --> 07:47.360
And Anka, drawing, yep.

07:47.360 --> 07:51.440
And so does that mean that you spend a lot of time thinking about robotic applications

07:51.440 --> 07:53.200
of machine learning and AI?

07:53.200 --> 07:54.840
Yes, exactly.

07:54.840 --> 08:00.040
Recently, I've been thinking a lot about how we can make machine learning systems more

08:00.040 --> 08:06.160
interpretable and more predictable, and so that ties in very closely with helping human

08:06.160 --> 08:10.040
robot interaction be more feasible in the future.

08:10.040 --> 08:11.040
Fantastic.

08:11.040 --> 08:15.000
So who wants to get started by telling me a little bit about the paper that you worked

08:15.000 --> 08:16.000
on together?

08:16.000 --> 08:17.000
Yeah, sure.

08:17.000 --> 08:19.080
I can give a summary.

08:19.080 --> 08:25.200
So the idea here was that there had already been a lot of work that shows that adversarial

08:25.200 --> 08:29.840
examples are really effective at attacking classifiers.

08:29.840 --> 08:35.880
So things like object recognition, if you train something to recognize objects in a scene

08:35.880 --> 08:41.000
in an image, it's pretty straightforward to find a small perturbation that will

08:41.000 --> 08:46.000
get your neural network to output a completely different label than what you had anticipated

08:46.000 --> 08:48.040
and then what the correct label is.

08:48.040 --> 08:54.960
And so we were thinking, we wanted to see if this would apply to neural network policies

08:54.960 --> 08:57.440
that were trained with deep reinforcement learning.

08:57.440 --> 09:02.560
And in particular, we were really interested in, to what extent, these adversarial perturbations

09:02.560 --> 09:07.960
could disrupt the performance of these policies and how transferable they were.

09:07.960 --> 09:13.160
And if you didn't know how a particular policy was trained, for example, which deep reinforcement

09:13.160 --> 09:18.640
learning algorithm was used to train it, could you still attack that policy?

09:18.640 --> 09:22.640
And so that was the overall question that we were trying to answer.

09:22.640 --> 09:30.840
So you referenced the work on adversarial examples for classifiers and these are examples.

09:30.840 --> 09:35.640
Like actually before spouting out some examples, do you do each of you have your own kind of

09:35.640 --> 09:40.160
favorite example of adversarial attacks against classifiers?

09:40.160 --> 09:42.640
Well, a lot of them are pretty similar.

09:42.640 --> 09:47.880
I would say one of my favorite observations is a paper called Delving into Transferable

09:47.880 --> 09:55.120
Adversarial Examples, where the authors found that if they fool several different classifiers

09:55.120 --> 09:59.520
simultaneously, if they actually use an optimizer to search for an adversarial example

09:59.520 --> 10:05.280
that fools very many different classifiers, then that input is extremely likely to fool

10:05.280 --> 10:07.960
another classifier that wasn't involved.

10:07.960 --> 10:12.600
You can design these attacks that will actually fool more or less anything without access

10:12.600 --> 10:16.280
to the target model that you want to fool.

10:16.280 --> 10:25.440
So in that example, if you somehow manage to create an example that visually looks like

10:25.440 --> 10:33.400
an ostrich but is classified as a school bus for multiple classifiers, they've demonstrated

10:33.400 --> 10:37.800
that it's likely to work on some broader number of classifiers.

10:37.800 --> 10:38.800
Exactly.

10:38.800 --> 10:44.160
Like suppose that you're a malicious attacker and you want to fool somebody's computer

10:44.160 --> 10:47.160
vision system, you don't know what they're using.

10:47.160 --> 10:48.160
Right.

10:48.160 --> 10:51.560
Let's say for the sake of argument, they're using VGGnet, but the attacker doesn't know

10:51.560 --> 10:52.560
that.

10:52.560 --> 10:57.560
The attacker could do something like fool inception and fool a resonant with the same input

10:57.560 --> 10:58.560
image.

10:58.560 --> 11:02.320
And if they go ahead and fool those two and a few other models, it's much more likely

11:02.320 --> 11:08.360
that they'll fool VGG, even if they never actually worked on fooling VGG specifically.

11:08.360 --> 11:16.200
Is the fooling specific to the network architecture as opposed to the specific parameters of a given

11:16.200 --> 11:18.200
model?

11:18.200 --> 11:22.000
That's what this paper is able to overcome.

11:22.000 --> 11:26.680
It's able to fool models regardless of their parameters or their architecture.

11:26.680 --> 11:30.920
As long as the models are trying to solve the same task, like recognize school buses

11:30.920 --> 11:36.280
and ostriches, this attack can find a reliable way of fooling pretty much any architecture

11:36.280 --> 11:37.760
that we've tested.

11:37.760 --> 11:42.160
And that's a lot of what Sandy was saying about how in this paper that we just published

11:42.160 --> 11:47.680
on adversarial policies, we wanted to find out if this property of adversarial examples

11:47.680 --> 11:52.280
transferring from one policy to another holds up in the same way that a transfer between

11:52.280 --> 11:54.480
classifiers holds up.

11:54.480 --> 12:00.240
And the context here is policies applied to reinforcement learning.

12:00.240 --> 12:05.400
Can you give us a concrete example of what you've got in mind there?

12:05.400 --> 12:09.760
Sure, maybe Sandy, do you want to talk about the Atari games?

12:09.760 --> 12:10.760
Yeah, sure.

12:10.760 --> 12:15.520
So I think when you think deeper reinforcement learning, the first really impressive example

12:15.520 --> 12:23.440
of this was when DeepMind was able to train DeepQ networks, DeQin, use that to play Atari

12:23.440 --> 12:27.440
games at human level performance.

12:27.440 --> 12:33.400
And so the idea there is that you basically start from a random initialization network

12:33.400 --> 12:35.000
that knows nothing.

12:35.000 --> 12:39.000
And then very slowly over time, just by getting this reward as feedback, this network is

12:39.000 --> 12:43.400
able to learn which actions will help it maximize reward.

12:43.400 --> 12:44.400
What's the next step?

12:44.400 --> 12:50.240
How does that apply or how does adversarial attacks apply in that example?

12:50.240 --> 12:57.240
Yeah, so the kind of adversarial attacks we were looking at, we assume that we've already

12:57.240 --> 13:01.960
got a policy that was trained with deep reinforcement learning.

13:01.960 --> 13:07.040
So it's fully trained and it's able to get really high performance on the game.

13:07.040 --> 13:11.920
So for example, a policy that's trained to play space invaders.

13:11.920 --> 13:17.760
What we can do is compute these small adversarial perturbations in the image of the game.

13:17.760 --> 13:21.240
So we do things like, for example, change a single pixel in the game.

13:21.240 --> 13:26.960
And that's able to significantly decrease the performance of this fully trained policy.

13:26.960 --> 13:29.960
And so the policy itself is fixed at that point.

13:29.960 --> 13:33.280
All we're changing is the input that the policy is given.

13:33.280 --> 13:34.280
Wow.

13:34.280 --> 13:43.000
And so, and did you find that across, like how broadly did you find that this applies,

13:43.000 --> 13:48.840
that you're able to change a single pixel value and dramatically impact the performance

13:48.840 --> 13:50.440
of the model?

13:50.440 --> 13:51.440
Yeah.

13:51.440 --> 13:53.680
So we did look at a few different games.

13:53.680 --> 13:59.240
All the only domain we looked at in this paper was Atari, but we looked at chopper command,

13:59.240 --> 14:02.880
sequests, space invaders, and pong.

14:02.880 --> 14:08.400
And so across all those games, and across three different ways of training these policies,

14:08.400 --> 14:15.280
A3C, TRPO, DQN, adversarial examples are, you can pretty easily find adversarial examples

14:15.280 --> 14:19.680
that will significantly decrease performance, like at least by half.

14:19.680 --> 14:24.360
We looked at a range of different perturbations, there are graphs in the paper that show

14:24.360 --> 14:33.160
this more concretely, but basically no matter which Atari game you're looking at or how

14:33.160 --> 14:38.480
it was trained, it's definitely true that adversarial examples exist.

14:38.480 --> 14:45.160
In some training methods, adversarial examples are more effective at decreasing performance,

14:45.160 --> 14:46.960
but yeah, they're pretty prevalent.

14:46.960 --> 14:55.440
Yeah, it's fascinating that you found this, you know, and I think about it in the context

14:55.440 --> 15:01.280
of reinforcement learning, I guess my initial take is that it seems different from just

15:01.280 --> 15:09.520
looking at an image, but you know, now that you've got me thinking about it, I can see

15:09.520 --> 15:15.560
how because the training methods are so similar, you would expect to have similar occurrences

15:15.560 --> 15:20.560
of adversarial examples in reinforcement learning types of models.

15:20.560 --> 15:25.720
One thing that I find really exciting about Sandy's results is that they help us answer

15:25.720 --> 15:29.240
a question that's almost more philosophical than technical.

15:29.240 --> 15:34.000
Most of the previous work on adversarial examples was about object recognition, looking

15:34.000 --> 15:39.840
at a photo and saying whether that photo is of an ostrich or a school bus.

15:39.840 --> 15:43.920
But a lot of the time we were making up unusual photos.

15:43.920 --> 15:49.200
They weren't photos made by taking a camera and snapping a picture of a school bus in

15:49.200 --> 15:50.200
the real world.

15:50.200 --> 15:56.560
They were made by a computer program and there's a deep philosophical question about how

15:56.560 --> 16:02.160
you can say what the objectively true answer is in such a photo.

16:02.160 --> 16:06.400
We've mostly evaluated our systems based on whether they agree with human judgment, but

16:06.400 --> 16:08.400
maybe the human is making a mistake.

16:08.400 --> 16:13.200
So it's hard to say that what the system does in the end is objectively wrong.

16:13.200 --> 16:24.880
And are you, if I can interrupt, is that critique to the specific examples that have become

16:24.880 --> 16:30.880
popularized of adversarial attacks like the ostrich and school bus?

16:30.880 --> 16:34.800
I mean, I guess it's not clear to me.

16:34.800 --> 16:35.800
Exactly.

16:35.800 --> 16:36.800
Yeah.

16:36.800 --> 16:43.000
I guess one of the strange things about adversarial examples is we're studying how to make computers

16:43.000 --> 16:45.160
make mistakes.

16:45.160 --> 16:51.560
And we happen to have done that mostly on, on kinds of data where we don't know objectively

16:51.560 --> 16:53.880
what a mistake is or isn't.

16:53.880 --> 16:57.920
If you make up an entirely new image, it's hard to say objectively how that image should

16:57.920 --> 16:58.920
be categorized.

16:58.920 --> 17:04.320
But for things like playing Atari games, it's, it's objective how the points are awarded.

17:04.320 --> 17:08.880
For pong, you need to actually make the ball go through the opponent's goal post.

17:08.880 --> 17:12.840
It's not really a question of whether a human observer thought the ball went through

17:12.840 --> 17:13.840
the goal post.

17:13.840 --> 17:18.600
It's just a question of how the pong game physics defined the scoring.

17:18.600 --> 17:25.160
And so in these experiments, we can actually say that the machine learning system is objectively

17:25.160 --> 17:28.280
compromised, that it really is doing worse.

17:28.280 --> 17:31.920
It's not just that it's playing pong differently than a human would play.

17:31.920 --> 17:35.840
It's actually playing pong in a way where it receives fewer points.

17:35.840 --> 17:42.160
Were you able to produce specific failure modes via these attacks or are we only looking

17:42.160 --> 17:48.440
at this from the perspective of subpar performance or reduced scores?

17:48.440 --> 17:57.000
Could you always make, could you by manipulating a single pixel or some number of pixels always

17:57.000 --> 18:06.520
make the pong paddle, the agent playing pong, missed the ball in the upper left corner?

18:06.520 --> 18:10.880
There's some work that came out after ours that focuses on what they call an enchanting

18:10.880 --> 18:11.880
attack.

18:11.880 --> 18:16.400
Where they attract the agent toward a particular state.

18:16.400 --> 18:20.040
We were just trying to reduce the score that the agent receives.

18:20.040 --> 18:24.240
I don't know, Sandy, did you notice any particular failure modes like any specific kind of mistakes

18:24.240 --> 18:26.440
that the agent would do over and over again?

18:26.440 --> 18:32.640
No, I think because when we were computing the adversarial examples, all we were trying

18:32.640 --> 18:38.000
to do is to get the agent to not do what it thought was the best action.

18:38.000 --> 18:42.840
It could take any other action besides the best action, and that would be a successful

18:42.840 --> 18:45.840
adversarial attack.

18:45.840 --> 18:52.680
I didn't see any specific failure modes, although I did see patterns in terms of what particular

18:52.680 --> 18:58.160
adverse profound and so in particular, you mentioned changing one pixel.

18:58.160 --> 19:04.440
In something like chopper command, when you change one pixel in this game, the highest

19:04.440 --> 19:09.360
impact you can get from that is by changing it in this small miniature map of the entire

19:09.360 --> 19:10.360
game.

19:10.360 --> 19:13.280
That's at the bottom of your screen.

19:13.280 --> 19:19.640
When we found adversarial examples for this, actually, most of the ones where we only

19:19.640 --> 19:26.640
changed one or two pixels, those pixels would get changed in that miniature map.

19:26.640 --> 19:30.080
That's the optimal way to fold the agent.

19:30.080 --> 19:35.920
That does shed some light on what the agent has learned to pay attention to, because you

19:35.920 --> 19:40.840
could imagine that maybe if you're just training a deep reinforcement learning agent on images,

19:40.840 --> 19:46.040
maybe it just would ignore this miniature map and not realize it's important.

19:46.040 --> 19:50.480
In that way, adversarial examples are also kind of interesting because they can make

19:50.480 --> 19:54.760
policies more interpretable in terms of what they're paying attention to in the scene.

19:54.760 --> 19:55.760
Okay.

19:55.760 --> 20:02.640
Other things that surprised you in terms of the things you learned in doing this?

20:02.640 --> 20:09.400
I guess one other interesting result that we haven't actually posted yet, but we've

20:09.400 --> 20:14.960
talked about at presentations and stuff, is it's actually possible to have dormant adversarial

20:14.960 --> 20:16.560
examples as well.

20:16.560 --> 20:22.200
We looked at policies that are recurrent, which means they have some sort of memory.

20:22.200 --> 20:27.880
One canonical example is if you have an agent trying to navigate through a maze and you

20:27.880 --> 20:31.920
show the agent, a particular indicator, say it's a certain color at the beginning of

20:31.920 --> 20:35.920
the maze, it has to remember that in order to figure out which goal to go to at the end

20:35.920 --> 20:38.880
of the maze.

20:38.880 --> 20:44.440
If you just did playing adversarial attacks, you could have an agent that just while it's

20:44.440 --> 20:48.760
navigating through this maze starts acting randomly and never reaches the goal at all.

20:48.760 --> 20:55.040
But if you're using, if you compute dormant adversarial examples, which means that you

20:55.040 --> 21:00.080
perturb a particular input, that's given to the agent, but then the agent keeps acting

21:00.080 --> 21:04.360
correctly until some time point in the future.

21:04.360 --> 21:08.800
In this maze, it would be the agent still navigating the maze correctly and then all

21:08.800 --> 21:12.640
of a sudden at the very end actually goes to the wrong goal.

21:12.640 --> 21:19.480
But the key there is that the point at which the adversarial example was introduced is

21:19.480 --> 21:23.800
actually significantly earlier than the point at which the agent makes the mistake, which

21:23.800 --> 21:25.840
is what makes it dormant.

21:25.840 --> 21:29.200
And so these also exist for recurrent policies.

21:29.200 --> 21:33.040
They're a little bit harder to find, but you can find them in very much the same way, just

21:33.040 --> 21:35.400
frame you as an optimization problem.

21:35.400 --> 21:37.760
What does harder to find mean?

21:37.760 --> 21:43.160
Hard to find as in, it takes more computational power to find it.

21:43.160 --> 21:48.600
The problem itself is a little bit, there are more local minima, for example, you'll have

21:48.600 --> 21:53.320
a lot of examples that won't meet all the constraints of your authorization.

21:53.320 --> 21:57.880
You're constrained to basically that you do the right thing for the next, let's say,

21:57.880 --> 22:02.080
10 time steps and the wrong thing on the 11th time step.

22:02.080 --> 22:06.600
You can think of dormant adversarial examples as being a little bit like post hypnotic

22:06.600 --> 22:13.240
suggestion in a cheesy spy movie, where there's a character who has been pre-programmed to

22:13.240 --> 22:18.160
suddenly carry out an assassination, and even that person themselves doesn't know that

22:18.160 --> 22:19.960
they've been programmed in that way.

22:19.960 --> 22:20.960
Kind of the mentoring.

22:20.960 --> 22:21.960
Kind of.

22:21.960 --> 22:22.960
Kind of.

22:22.960 --> 22:23.960
Exactly.

22:23.960 --> 22:27.320
Yeah, that was the movie we were talking about when we first had the idea for this project.

22:27.320 --> 22:28.320
Interesting.

22:28.320 --> 22:35.040
From a security point of view, dormant adversarial examples are more worrisome because they could

22:35.040 --> 22:40.400
be presented to an agent before it enters the area that you've secured.

22:40.400 --> 22:44.240
You could imagine if you have some kind of room where you're careful about what objects

22:44.240 --> 22:45.240
are there.

22:45.240 --> 22:49.960
You could make sure that nothing in that area can confuse your robot, but if your robot

22:49.960 --> 22:55.400
could be confused by something it saw before it came into the secure environment, then you

22:55.400 --> 22:59.680
actually have to secure it at the level of the machine learning software, rather than

22:59.680 --> 23:04.320
securing it by making sure that there's nothing unusual in its physical environment.

23:04.320 --> 23:14.640
Are you aware of any publicized examples of adversarial attacks in the wild?

23:14.640 --> 23:20.680
I've heard people in finance say that they spend a lot of effort obfuscating their trading

23:20.680 --> 23:26.240
algorithms so that their competitors don't reverse engineer their trading algorithm and

23:26.240 --> 23:30.040
fool them into making unprofitable trades.

23:30.040 --> 23:35.680
I don't know of anything very similar to the computer vision, object recognition examples

23:35.680 --> 23:38.120
that we're studying so far.

23:38.120 --> 23:40.600
Do you know of anything like that, Cindy?

23:40.600 --> 23:42.760
I don't know of anything in the wild.

23:42.760 --> 23:50.000
I mean, there have been more examples of real world adversarial examples where you do

23:50.000 --> 23:55.520
something like print out a poster of a stop sign and paste that on top of a real stop

23:55.520 --> 24:04.080
sign, and that's able to fool a classifier from a lot of different angles and distances.

24:04.080 --> 24:10.000
So, I've been fortunate that the real malicious people don't seem to be using these techniques

24:10.000 --> 24:11.000
yet.

24:11.000 --> 24:13.760
I think that people probably well in the future.

24:13.760 --> 24:16.800
At the moment, I think we're protected by a few factors.

24:16.800 --> 24:21.000
One is I think there's a lot of other malicious things you can do that are easier.

24:21.000 --> 24:28.320
And two, if you have the deep learning expertise, there are less risky ways to turn a profit.

24:28.320 --> 24:35.240
Have you come across, I don't know if you would call these adversarial examples, but what's

24:35.240 --> 24:40.640
the analog accidental adversarial examples, like natural adversarial examples, is there

24:40.640 --> 24:41.640
such a thing?

24:41.640 --> 24:48.320
I mean, I think that would just be called like training and test, well, it would be like

24:48.320 --> 24:53.520
training and test distribution shift, but something that you saw it test time that you didn't

24:53.520 --> 24:58.600
train on and you didn't expect to see, which I think does happen all the time.

24:58.600 --> 24:59.600
Sure.

24:59.600 --> 25:00.600
Yeah.

25:00.600 --> 25:01.600
Yeah.

25:01.600 --> 25:05.600
There are optical illusions and things like that that fool humans, even when they're not

25:05.600 --> 25:06.920
really designed to.

25:06.920 --> 25:10.800
I'm sure you've seen silly images posted on Reddit, where the first time you look

25:10.800 --> 25:13.800
at it, you think you see something entirely different.

25:13.800 --> 25:18.960
There's lots of party photos where people standing near each other, it looks like someone's

25:18.960 --> 25:20.960
arms are actually someone else's legs.

25:20.960 --> 25:24.680
So it looks like there's somebody with four legs or something like that.

25:24.680 --> 25:27.920
That kind of thing comes up in machine learning too, just by chance.

25:27.920 --> 25:32.280
So where do you see the work in this paper going?

25:32.280 --> 25:36.600
One thing that I'd be really interested in is going further in the direction of what

25:36.600 --> 25:42.640
you were alluding to earlier about controlling the agent to do complicated behaviors that

25:42.640 --> 25:48.360
are different from what the designer wanted, rather than just doing worse at the main task.

25:48.360 --> 25:52.640
One of the main things making that harder for researchers to do is that we only have access

25:52.640 --> 25:54.560
to one reward function.

25:54.560 --> 25:57.480
We have a reward function that says play pong very well.

25:57.480 --> 26:01.880
And as an attacker, we can choose actions that make that reward go down.

26:01.880 --> 26:06.320
But if we had two different reward functions for the same environment, like if we had a robot

26:06.320 --> 26:10.600
that can cook, and it's been asked to cook scrambled eggs, but we also have a reward

26:10.600 --> 26:12.840
function for making a birthday cake.

26:12.840 --> 26:17.280
It would be interesting to show that we could trick it into making a birthday cake.

26:17.280 --> 26:18.720
It's really easy to break things.

26:18.720 --> 26:22.640
It's harder to create something that wasn't there already.

26:22.640 --> 26:26.800
And so if as an attacker, we could show we have so much control that we're able to actually

26:26.800 --> 26:31.360
create a birthday cake rather than just interfere with making scrambled eggs.

26:31.360 --> 26:36.160
That would be a lot more impressive in terms of the capabilities of the attacker.

26:36.160 --> 26:44.360
In the example that you use, the robot has multiple reward functions corresponding to what

26:44.360 --> 26:49.200
are, you could view as largely different tasks are there.

26:49.200 --> 26:59.240
Are you seeing a move in reinforcement learning towards something other than a single reward

26:59.240 --> 27:02.800
function, something that's more kind of nuanced or complex?

27:02.800 --> 27:06.560
I'm not sure exactly what that means or if the question makes sense, but you're saying

27:06.560 --> 27:12.680
is there a different way to evaluate performance or to give a learning signal to the agent?

27:12.680 --> 27:14.320
I don't work on reinforcement learning as much.

27:14.320 --> 27:17.440
Sandy is probably better qualified to comment on that.

27:17.440 --> 27:23.000
I personally feel like we need to move beyond the paradigm of just maximizing reward for

27:23.000 --> 27:25.080
a lot of different reasons.

27:25.080 --> 27:28.880
One reason that I have is that it's a strange way to communicate with an agent.

27:28.880 --> 27:33.160
Imagine we wanted a reinforcement learning agent to plan a mission to Mars and we give it

27:33.160 --> 27:37.040
a reward of one when it gets there and a reward of zero otherwise.

27:37.040 --> 27:41.360
How would it even know we wanted it to go to Mars until it got there?

27:41.360 --> 27:45.920
You could imagine having two different super intelligent agents and we want one to cure

27:45.920 --> 27:50.480
cancer and one to go to Mars and we just let them both lose on Earth.

27:50.480 --> 27:54.520
How would each one know which task it had been assigned until it finished its task?

27:54.520 --> 27:59.960
Conceivably, one could cure cancer but it was supposed to go to Mars and get no reward

27:59.960 --> 28:02.160
and the other vice versa.

28:02.160 --> 28:06.040
I should say I'm saying all of this is someone who mostly studies classifiers in

28:06.040 --> 28:11.400
generative models, so maybe I'm being very inferred at the reinforcement learning point.

28:11.400 --> 28:15.280
Sandy, do you have any thoughts about the future of reward functions?

28:15.280 --> 28:21.160
Well, I think there is a lot of complexity that can go into a reward signal.

28:21.160 --> 28:29.600
I do sort of agree with Ian that it is a pretty severe constraint on how you can give

28:29.600 --> 28:35.000
information to your agent if all you're giving it is this particular reward but you can

28:35.000 --> 28:41.480
do things like shape the reward or do something more like curriculum learning where you start

28:41.480 --> 28:49.160
off the agent with smaller tasks even if the reward is sparse and over time give it

28:49.160 --> 28:53.760
harder task but it's already learned how to solve the easier smaller tasks so it should

28:53.760 --> 28:57.240
be able to more easily solve more difficult ones.

28:57.240 --> 29:04.920
I think, yeah, I mean there has been a lot of work recently on trying to do things like

29:04.920 --> 29:12.840
transfer learning or meta learning in the context of reinforcement learning and so that's

29:12.840 --> 29:14.880
also a promising direction.

29:14.880 --> 29:16.920
You mentioned curriculum learning.

29:16.920 --> 29:19.760
Can you elaborate on that?

29:19.760 --> 29:27.160
Are you essentially iteratively training with more comprehensive or longer term rewards?

29:27.160 --> 29:29.720
Is it related to transfer learning in that sense?

29:29.720 --> 29:33.920
Yeah, it is related transfer learning in the sense that you're trying to transfer knowledge

29:33.920 --> 29:36.640
from easier tasks to harder tasks.

29:36.640 --> 29:41.840
So there's been some work Carlos Lorenza and Peter's group has done some work where you're

29:41.840 --> 29:46.880
in a setting with sparse reward like Ian was talking about where you get a one if you succeed.

29:46.880 --> 29:53.240
But you can do things like start your robot from starting states that are closer to your

29:53.240 --> 29:54.240
goal.

29:54.240 --> 29:59.000
For example, if you're trying to get a robot to insert a key into a lock and turn it, you

29:59.000 --> 30:03.000
could start it with the key already inside the lock and it just has to turn the key and

30:03.000 --> 30:06.560
then start with the key just a tiny bit outside the lock so it just has to figure out and

30:06.560 --> 30:11.920
start the key and then turn it and so you just slowly you have a set of states where the

30:11.920 --> 30:17.840
robot succeeds from and you slowly expand that set to starting states that are just a

30:17.840 --> 30:21.880
tiny bit more difficult than the ones the robot can already succeed from.

30:21.880 --> 30:27.000
And so that and then at the end you have a robot that can insert this key into the lock

30:27.000 --> 30:31.840
and turn it from any point within a large range of different points.

30:31.840 --> 30:36.520
It's called curriculum learning as an analogy to the way that we teach people in school

30:36.520 --> 30:37.520
schools.

30:37.520 --> 30:41.640
We start out teaching people how to read the alphabet in kindergarten and then build

30:41.640 --> 30:48.200
up to very easy C spot run type books and then gradually to more and more difficult

30:48.200 --> 30:53.080
reading tasks and then once people can read fluently we start teaching them subject matter

30:53.080 --> 30:55.720
that they read in textbooks.

30:55.720 --> 31:02.040
We don't on day one of kindergarten start throwing everyone questions sampled uniformly

31:02.040 --> 31:07.520
from the set of all knowledge we expect them to have by age 18 we don't give anyone questions

31:07.520 --> 31:11.360
from their algebra two class on day one of kindergarten.

31:11.360 --> 31:16.560
We arrange the order of the experiences so that it gets harder and harder as they go through.

31:16.560 --> 31:22.000
It seems really obvious in retrospect but in machine learning we actually usually do

31:22.000 --> 31:26.840
uniformly sample all the experiences that we test the machine learning system on.

31:26.840 --> 31:30.560
So curriculum learning is a pretty big change from what's the standard practice.

31:30.560 --> 31:36.840
And I was thinking ahead a little bit to how the adversarial attacks might apply in

31:36.840 --> 31:45.520
that example and what are the extent to which we've looked at transferability of adversarial

31:45.520 --> 31:51.640
attacks in transfer learning cases in general have either of you looked into that.

31:51.640 --> 31:55.280
I've been a co-author of some work and I've followed a lot of other work with a lot of

31:55.280 --> 32:01.920
interest. In 2013 when Christian Zeggedy wrote one of the first papers on adversarial

32:01.920 --> 32:07.520
examples he found that if you just make adversarial examples for one model and don't do anything

32:07.520 --> 32:12.560
to try to make them transfer they will often fool other models just by chance without

32:12.560 --> 32:15.320
needing to do anything special to cause it to happen.

32:15.320 --> 32:21.440
And then later Nikola Paparno in a paper that we wrote together showed that if you train

32:21.440 --> 32:26.880
one neural network to copy another neural network you can actually train it to copy the

32:26.880 --> 32:32.840
behavior of the target network on unusual inputs that don't correspond to any kind of real

32:32.840 --> 32:36.880
data as well as regular inputs that look like data.

32:36.880 --> 32:40.480
But once you've managed to copy all the decision boundaries of this target network in that

32:40.480 --> 32:45.240
way then you know that adversarial examples for your copy are very likely to fool the target

32:45.240 --> 32:46.240
as well.

32:46.240 --> 32:50.000
You can copy a network like that without actually having access to its parameters or its

32:50.000 --> 32:51.000
architecture.

32:51.000 --> 32:54.760
You can just send inputs to it and see what output it assigns them and then you train

32:54.760 --> 32:58.320
your own model to copy that input to output mapping.

32:58.320 --> 33:04.400
So is that you basically using the model that you're copying or the network that you're

33:04.400 --> 33:08.440
copying is that essentially generating your label data so you've got some inputs you're

33:08.440 --> 33:15.520
sending it to that and then you're training your model on the inputs and labels that

33:15.520 --> 33:17.000
that thing generates.

33:17.000 --> 33:18.000
Exactly.

33:18.000 --> 33:21.560
The attacker doesn't even need to have enough resources to label their own data set.

33:21.560 --> 33:22.560
Right.

33:22.560 --> 33:27.480
And so any specific thoughts on how adversarial attacks might apply in this curriculum learning

33:27.480 --> 33:30.400
type of use case?

33:30.400 --> 33:36.840
One thing that's kind of interesting and related to curriculum learning is a machine learning

33:36.840 --> 33:40.440
security problem called training set poisoning.

33:40.440 --> 33:43.400
It's almost like the opposite of curriculum learning.

33:43.400 --> 33:49.280
That curriculum learning is when a benevolent designer of the system structures the training

33:49.280 --> 33:53.160
set to be really easy for the model to learn from.

33:53.160 --> 33:57.400
Training set poisoning is when an attacker sneaks something into your training set that

33:57.400 --> 33:59.200
you didn't know was there.

33:59.200 --> 34:04.400
And then it can make the machine learning model do something that the attacker chose at

34:04.400 --> 34:08.720
test time based on what it learned from the training set poisoning.

34:08.720 --> 34:11.400
Another really similar idea was introducing it.

34:11.400 --> 34:15.320
Before you move on from there, can you give a specific example of that?

34:15.320 --> 34:16.320
Yeah.

34:16.320 --> 34:22.480
There's a paper from Stanford that came out last year where they showed that they can

34:22.480 --> 34:26.760
introduce a specific picture of a dog that has been altered a little bit just like an

34:26.760 --> 34:28.160
adversarial example.

34:28.160 --> 34:33.040
And if you include that dog in your training set, it will misrecognize lots and lots and

34:33.040 --> 34:36.080
lots of dogs as fish at test time.

34:36.080 --> 34:37.080
Wow.

34:37.080 --> 34:38.080
Okay.

34:38.080 --> 34:39.080
Interesting.

34:39.080 --> 34:40.080
Yeah.

34:40.080 --> 34:41.080
I've not come across that one.

34:41.080 --> 34:45.960
Another similar thing is introducing a paper called Badnets and they call it adding a

34:45.960 --> 34:47.720
backdoor to the network.

34:47.720 --> 34:52.200
The idea is that the person who trains the network might intentionally add something weird

34:52.200 --> 34:57.920
to the training set that then lets the network do something they want later on.

34:57.920 --> 35:04.000
So for example, suppose that I was training a face recognition system that I would give

35:04.000 --> 35:06.560
to other people to use to secure their facility.

35:06.560 --> 35:11.440
If I was a bad person, I might put myself in the training set and tell it to always let

35:11.440 --> 35:12.440
me in.

35:12.440 --> 35:15.960
And then I could go break into their warehouse later without needing to do anything special

35:15.960 --> 35:18.320
to get through security.

35:18.320 --> 35:22.400
Detecting those kinds of backdoors in neural networks is a really interesting research

35:22.400 --> 35:28.000
challenge because when a network does something unusual, it can be hard to tell whether the

35:28.000 --> 35:30.480
network is making a random mistake.

35:30.480 --> 35:33.840
The network is making a mistake that someone built into it as a backdoor.

35:33.840 --> 35:37.240
Or in some cases, the network isn't even making a mistake, it's doing something smarter

35:37.240 --> 35:39.800
than you, the human, have thought of.

35:39.800 --> 35:42.600
And it's actually correcting one of your own mistakes.

35:42.600 --> 35:46.640
So when you disagree with it, you don't actually know a priority who's right or wrong.

35:46.640 --> 35:51.480
Yeah, the thing that this makes me think about is, I guess some of the conversations we've

35:51.480 --> 35:54.400
had as an industry around code reuse.

35:54.400 --> 36:00.800
I forget the specific example, but there was an example about a year ago or so of an NPM

36:00.800 --> 36:06.720
library that I don't think anything malicious happened, but someone either changed it or

36:06.720 --> 36:09.000
unpublished it or something like that.

36:09.000 --> 36:17.560
And because so many people had used this library in their code, it had the potential to disrupt

36:17.560 --> 36:20.320
a whole bunch of applications.

36:20.320 --> 36:26.360
And I think the NPM folks, the node folks came in and did something extraordinary to make

36:26.360 --> 36:30.720
sure this library didn't go away and break all these applications.

36:30.720 --> 36:39.520
And we're seeing a lot of the same, the analogy of code reuse in the machine learning world

36:39.520 --> 36:42.040
is like reusing these data sets.

36:42.040 --> 36:50.200
But I don't know that we have any real standards for certifying data sets as being untempered

36:50.200 --> 36:51.200
with.

36:51.200 --> 37:00.360
And the idea that you can introduce back doors or make neural networks misbehave and

37:00.360 --> 37:08.360
really bad ways by manipulating the training data sets suggests that we need these kinds

37:08.360 --> 37:09.360
of standards.

37:09.360 --> 37:12.880
And even a lot of people create their training data sets by crawling the web.

37:12.880 --> 37:20.560
And we know that you can kind of poison web search results by creating linking architectures

37:20.560 --> 37:28.640
and things like that so that certain things could make certain results become more popular.

37:28.640 --> 37:32.160
And that could have a downstream impact on these models as well.

37:32.160 --> 37:33.160
Exactly, yeah.

37:33.160 --> 37:37.240
Sandy, do you have any war stories from training sets at Berkeley?

37:37.240 --> 37:42.160
No, I guess in the, well, in the context of reinforcement learning, you don't really

37:42.160 --> 37:48.440
have the training set like you do in supervised learning, you have your simulator.

37:48.440 --> 37:52.960
And so I think, I mean, something similar does apply.

37:52.960 --> 38:01.200
You could have a simulator that somehow gets the agent to learn some correlation that actually

38:01.200 --> 38:07.440
impairs it when it's launched on, unlike a test simulator.

38:07.440 --> 38:12.080
I mean, I think this is sort of related to reward hacking.

38:12.080 --> 38:16.920
If there is something that the agent can exploit in your simulator, a lot of times if you

38:16.920 --> 38:22.200
are training it with deep reinforcement learning, it will find that and it will exploit that.

38:22.200 --> 38:26.120
But the difference is in the context of reward hacking, it's pretty obvious when your agent

38:26.120 --> 38:31.080
has done that if you just watch a rollout of the agent, you can usually detect that, okay,

38:31.080 --> 38:33.800
it's not actually doing what I want it to do.

38:33.800 --> 38:39.560
And just to make sure folks are familiar with the term reward hacking, you know, these

38:39.560 --> 38:46.040
are examples in the case of video games where the one that I remember was kind of this

38:46.040 --> 38:50.440
agent that's a boat that figures out that if it swirls around in a circle that racks

38:50.440 --> 38:54.840
up a whole bunch of points, even though it's not making progress towards its end goal,

38:54.840 --> 38:57.560
or what you might want it to end goal to be.

38:57.560 --> 38:58.560
Yeah.

38:58.560 --> 38:59.560
Exactly.

38:59.560 --> 39:01.120
That's a really popular example.

39:01.120 --> 39:05.840
And the problem there is that you told the agent to maximize score.

39:05.840 --> 39:10.640
And by swirling around, it's able to get all these points by getting these things in

39:10.640 --> 39:12.840
the environment that appear periodically.

39:12.840 --> 39:14.840
And so that's what's doing while swirling around.

39:14.840 --> 39:19.000
But really, you wanted the agent to win the game, but you couldn't really specify that

39:19.000 --> 39:21.160
particular reward function.

39:21.160 --> 39:25.160
So that's why you gave it, just told it to max line points.

39:25.160 --> 39:29.840
But that goes along with the discussion we were having earlier about how the reward function

39:29.840 --> 39:31.680
is really important.

39:31.680 --> 39:35.520
And you need to be really careful in selecting your reward function and make sure you're

39:35.520 --> 39:37.840
actually telling the agent what you want it to do.

39:37.840 --> 39:38.840
Yeah.

39:38.840 --> 39:42.440
And I think the question that I was, the way I was trying to ask the question previously

39:42.440 --> 39:51.160
was, is there a notion at all of either hierarchical reward functions or multiple reward functions

39:51.160 --> 40:00.560
that you try to optimize simultaneously, or is that just beyond the frontier of complexity

40:00.560 --> 40:01.960
for us right now?

40:01.960 --> 40:07.160
There was a really interesting recent result from DeepMind with a system called Impala.

40:07.160 --> 40:12.000
They showed that they could train on several different tasks at the same time and actually

40:12.000 --> 40:18.920
do better on task A because they had also studied task B, C, and T. That was actually pretty

40:18.920 --> 40:20.920
hard to get even that much working.

40:20.920 --> 40:28.040
And so would you expect it to be easier or harder to, I'm assuming in this case, there are

40:28.040 --> 40:32.280
separate rewards functions for each of these tasks.

40:32.280 --> 40:37.560
But I'm envisioning at least conceptually that you can have a single task with multiple

40:37.560 --> 40:40.440
reward functions.

40:40.440 --> 40:43.720
Would you expect that to be easier or harder than what they did?

40:43.720 --> 40:45.680
I would expect that to be easier.

40:45.680 --> 40:50.600
It's mostly that for most of the simulators we have, there's really one thing that you

40:50.600 --> 40:51.600
want to do.

40:51.600 --> 40:56.320
In the pung simulator, you want to play pung by knocking them all through the opponent's

40:56.320 --> 40:57.320
goal post.

40:57.320 --> 40:58.320
Yeah.

40:58.320 --> 41:02.800
I guess I'm thinking of something, maybe the simple example of the kind of thing I'm

41:02.800 --> 41:07.200
thinking of is the whole explorer exploit.

41:07.200 --> 41:14.320
Maybe there's, you know, you have a game that, you know, it's like a map based or world

41:14.320 --> 41:22.520
based game and you want, you know, one reward function to be when the game.

41:22.520 --> 41:29.280
But you also want to encourage your agent to explore the world.

41:29.280 --> 41:33.880
And so you might have, at least I'm envisioning, you'd have one reward function that correlates

41:33.880 --> 41:39.440
to the amount of the world that's been explored and another that correlates to winning the

41:39.440 --> 41:40.440
game.

41:40.440 --> 41:44.520
And, you know, what does it mean to kind of maximize both of those?

41:44.520 --> 41:45.520
Yeah.

41:45.520 --> 41:50.560
I think that that reminds me of hierarchical reinforcement learning, which is definitely

41:50.560 --> 41:53.800
a topic that is being studied.

41:53.800 --> 41:58.800
So in that sense, you have sort of different levels of agents.

41:58.800 --> 42:05.120
You have a planner that essentially decides, okay, what do I want to do?

42:05.120 --> 42:07.160
What is most important to do at this point in the game?

42:07.160 --> 42:08.160
Do I explore?

42:08.160 --> 42:09.160
Do I exploit?

42:09.160 --> 42:10.160
Do I do something else?

42:10.160 --> 42:17.880
And then that, they pass down that, I guess, that sub-reward to a policy that's been

42:17.880 --> 42:22.840
trained to, for example, explore really efficiently.

42:22.840 --> 42:26.440
And so you can learn this whole thing into end.

42:26.440 --> 42:29.360
I mean, DeepMine also had a paper on this last year.

42:29.360 --> 42:33.760
I forget what the, what the new hood it is, but yeah, where they're able to show that

42:33.760 --> 42:40.040
it is possible to train a giant policy that does hierarchical reinforcement learning and

42:40.040 --> 42:41.760
passes down these sub-rewards.

42:41.760 --> 42:47.400
I think this is really important for complex tasks, like trying to win a game of starcraft,

42:47.400 --> 42:52.360
for example, it might be, it might take a lot longer to train this end to end compared

42:52.360 --> 42:55.520
to if you're doing it in a hierarchical way.

42:55.520 --> 43:01.760
You know, we've talked about examples of, you know, fooling classifiers or fooling, you

43:01.760 --> 43:09.560
know, reinforcement-trained agents, which could be robots.

43:09.560 --> 43:14.880
But I'm wondering, given the focus of your work is on robotics, I'm wondering if there

43:14.880 --> 43:22.760
are any more subtle examples that you've come across or things that, you know, are areas

43:22.760 --> 43:32.320
of concern for the application of adversarial examples or adversarial training in the

43:32.320 --> 43:34.360
context of robotics.

43:34.360 --> 43:42.440
Yeah, I think one of the key problems is that adversarial examples make policies and

43:42.440 --> 43:49.520
they make robots less predictable, and it's harder to anticipate their behavior.

43:49.520 --> 43:58.160
And so if you are a human trying to interact with a robot or riding in a car, you have this

43:58.160 --> 44:04.000
mental model of how you think this robot is going to behave in the next, like, one, two

44:04.000 --> 44:05.000
seconds.

44:05.000 --> 44:09.040
And so the dangerous thing about adversarial examples is that that basically breaks your

44:09.040 --> 44:14.320
model and puts you in a position where you're not sure how to respond as a human, and so

44:14.320 --> 44:16.680
that's really dangerous.

44:16.680 --> 44:23.600
I think the fact that it does seem possible to introduce adversarial examples, whether

44:23.600 --> 44:31.240
you train this policy with reinforcement learning or with supervised learning, does seem,

44:31.240 --> 44:36.440
I mean, that's pretty, pretty scary.

44:36.440 --> 44:41.360
Although it is hard to get adversarial examples in the real world, and like Ian said, there's

44:41.360 --> 44:46.520
a lot of other ways in which robots can misbehave that don't depend on adversarial

44:46.520 --> 44:47.520
examples.

44:47.520 --> 44:55.600
And so the whole challenge of getting robots unpredictable and interpretable robots is

44:55.600 --> 45:00.280
much bigger than just trying to solve the problem of adversarial examples.

45:00.280 --> 45:06.520
What I'm really excited about is that adversarial examples give us a way of studying how robust

45:06.520 --> 45:11.080
a robotic system is in very concrete mathematical terms.

45:11.080 --> 45:16.480
We specify a model where we say all the things that an attacker can do, and then we try

45:16.480 --> 45:22.200
to prove that our policy will still work, even in the last case where the attacker chooses

45:22.200 --> 45:26.480
the thing that is the most likely to interfere with with the robot's abilities.

45:26.480 --> 45:30.160
So far we're not able to defend against that kind of attack, but in the future when our

45:30.160 --> 45:35.560
defense algorithms get better, if we're able to perform well in the worst case, it should

45:35.560 --> 45:40.720
also mean that we're always able to guarantee good performance in the average case, that if

45:40.720 --> 45:47.480
we're able to resist actual tempering, we can also be robust to things that interfere

45:47.480 --> 45:49.160
with robot policies right now.

45:49.160 --> 45:54.240
Like when you train a policy on one robot body and then run it on another robot body that

45:54.240 --> 45:58.960
is slightly different due to mechanical imperfections, that can be enough to interfere

45:58.960 --> 46:02.000
with that policy.

46:02.000 --> 46:06.960
That's actually a good segue to what I think will be our last question here.

46:06.960 --> 46:18.280
A lot of the work on the supervised learning camp of adversarial examples has been on

46:18.280 --> 46:25.160
architectures or methodologies for creating robustness in the networks to these kinds of

46:25.160 --> 46:27.360
attacks.

46:27.360 --> 46:34.360
Have you done or seen anything in the reinforcement learning world along those lines yet?

46:34.360 --> 46:38.960
Some of the follow-up work on our first paper actually used some of the techniques from

46:38.960 --> 46:42.120
classifiers to increase the robustness.

46:42.120 --> 46:48.960
So there was some work from CMU last year, it was called robust adversarial reinforcement

46:48.960 --> 46:49.960
learning.

46:49.960 --> 46:55.240
They were trying to, well, their definition of adversarial attacks was more physical,

46:55.240 --> 47:00.640
so you're training this locomotion agent in Mujoko, and the adversary can apply these

47:00.640 --> 47:03.240
forces to the agent.

47:03.240 --> 47:08.320
And you train the adversary actually in parallel with the policy that you're trying to train

47:08.320 --> 47:13.840
to get this agent to walk or make forward progress.

47:13.840 --> 47:21.000
And what they were able to find is that by training this agent to be able to walk despite

47:21.000 --> 47:25.800
these forces applied to it by the adversary, they're able to get an agent that was more

47:25.800 --> 47:32.360
robust in terms of being able to locomot across many different parameters of the environment

47:32.360 --> 47:36.760
in terms of friction or mass, different body parts of the agent, things like that.

47:36.760 --> 47:40.120
And Mujoko, that's a simulator, that's right.

47:40.120 --> 47:41.360
Yeah, that's a simulator.

47:41.360 --> 47:45.960
So that's where you have things like the half-cheetah and the humanoid on the swimmer,

47:45.960 --> 47:46.960
yeah.

47:46.960 --> 47:47.960
Great, great.

47:47.960 --> 47:53.440
Well, any final words from either of you, any parting thoughts or things that you'd

47:53.440 --> 47:57.960
like to point folks to if they're interested in learning more about this stuff?

47:57.960 --> 48:02.880
You could summarize a lot of what we talked about today as Goodheart's law in action.

48:02.880 --> 48:10.400
Goodheart's law is an idea that came from economics that says, once you use some value as a metric

48:10.400 --> 48:14.920
that you make it your target to optimize, it's no longer a good metric.

48:14.920 --> 48:20.640
And we see that happen with both adversarial examples and with reward hacking.

48:20.640 --> 48:25.640
If we use the output of a classifier as something that we're going to optimize, we find

48:25.640 --> 48:30.560
an adversarial example instead of a good input from a particular target class.

48:30.560 --> 48:36.160
And similarly, if a reinforcement learning agent optimizes its reward function too well,

48:36.160 --> 48:41.240
it can find ways of obtaining rewards that are serious and not doing what we actually

48:41.240 --> 48:43.160
hoped that our agent could do.

48:43.160 --> 48:45.560
Yeah, I think that's a great summary.

48:45.560 --> 48:51.480
Are there any specific implications of thinking of this stuff in terms of Goodheart's law?

48:51.480 --> 48:56.600
I guess one thing is just that it lets us see that there are many different things that

48:56.600 --> 49:01.040
all fall in the same category that you could think of reward hacking as a kind of adversarial

49:01.040 --> 49:02.800
example or vice versa.

49:02.800 --> 49:05.840
And you can see that solutions to one might help with the other.

49:05.840 --> 49:06.840
Got it.

49:06.840 --> 49:07.840
Awesome.

49:07.840 --> 49:12.360
Well, Ian Sandi, thank you both so much for taking the time to chat with us about this

49:12.360 --> 49:15.320
stuff is really interesting and important work.

49:15.320 --> 49:16.320
Thank you.

49:16.320 --> 49:17.320
Oh, thank you.

49:17.320 --> 49:18.320
Thank you.

49:18.320 --> 49:23.640
All right, everyone, that's our show for today.

49:23.640 --> 49:29.160
For more information on Ian, Sandi, or any of the topics covered in this episode, you'll

49:29.160 --> 49:35.120
find this show notes at twemolei.com slash talk slash 1-1-9.

49:35.120 --> 49:39.520
If you have any questions for Ian or Sandi, please post them there and we'll make sure

49:39.520 --> 49:41.400
to bring them to their attention.

49:41.400 --> 49:45.480
If you're new to the podcast and you like what you hear or you're a veteran listener

49:45.480 --> 49:48.280
and haven't already done so, please take a moment

49:48.280 --> 49:53.200
to head on over to your podcast app of choice and leave us your most gracious rating and

49:53.200 --> 49:54.360
review.

49:54.360 --> 49:57.800
It helps new listeners find us, which helps us grow.

49:57.800 --> 50:01.560
Thanks in advance and thanks so much for listening.

50:01.560 --> 50:27.040
Catch you next time.

