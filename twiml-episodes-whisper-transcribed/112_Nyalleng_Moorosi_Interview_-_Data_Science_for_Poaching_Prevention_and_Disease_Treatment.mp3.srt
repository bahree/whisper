1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,360
I'm your host Sam Charrington.

4
00:00:31,360 --> 00:00:35,800
You may have listened to one or more of the shows from our AI and consumer electronic series

5
00:00:35,800 --> 00:00:36,800
last week.

6
00:00:36,800 --> 00:00:42,320
There's a ton of interesting work happening in that space, but the reality is that I came

7
00:00:42,320 --> 00:00:48,720
back from CES both excited and also somewhat disillusioned about personal AI.

8
00:00:48,720 --> 00:00:53,040
Don't get me wrong, there's a bunch of cool stuff coming for sure, but it's taking

9
00:00:53,040 --> 00:00:55,000
so long.

10
00:00:55,000 --> 00:00:59,800
In the meantime, the privacy sacrifices we're being asked to make for modest conveniences

11
00:00:59,800 --> 00:01:02,280
seem pretty steep at times.

12
00:01:02,280 --> 00:01:06,480
I want to know what you think about the state of personal AI.

13
00:01:06,480 --> 00:01:11,040
I really want to hear from listeners on their thoughts, experience and desires on the role

14
00:01:11,040 --> 00:01:16,160
AI is playing in your home and personal life, your favorite examples of home or personal

15
00:01:16,160 --> 00:01:17,360
AI.

16
00:01:17,360 --> 00:01:22,240
The home or personal AI that you really want to see in your lifetime, and just where

17
00:01:22,240 --> 00:01:30,120
you see this all going, please check out TwimbleAI.com slash my AI to share your thoughts.

18
00:01:30,120 --> 00:01:34,360
We've got some really great entries so far, and you can check them out on that page,

19
00:01:34,360 --> 00:01:38,160
but we're missing a very important one, yours.

20
00:01:38,160 --> 00:01:41,560
Let me know your thoughts, and you'll be automatically entered into the running for

21
00:01:41,560 --> 00:01:44,400
some great prizes.

22
00:01:44,400 --> 00:01:50,200
For today's show, I'm joined by Nialing Marosi, senior data science researcher at the Council

23
00:01:50,200 --> 00:01:56,320
for Scientific and Industrial Research, or CSIR, in Pretoria, South Africa.

24
00:01:56,320 --> 00:02:02,120
We discuss two major projects that Nialing is a part of at the CSIR, one a predictive

25
00:02:02,120 --> 00:02:07,760
policing use case which focused on understanding and preventing rhino poaching in Kruger National

26
00:02:07,760 --> 00:02:13,440
Park, and the other a healthcare use case which focuses on understanding the effects of

27
00:02:13,440 --> 00:02:18,640
a drug treatment that was causing pancreatic cancer in South Africans.

28
00:02:18,640 --> 00:02:23,480
Along the way, we talk about the challenges of data collection, data pipelines, and overcoming

29
00:02:23,480 --> 00:02:24,800
sparsity.

30
00:02:24,800 --> 00:02:28,720
This was a really interesting conversation that I'm sure you'll enjoy.

31
00:02:28,720 --> 00:02:30,040
Let's do it.

32
00:02:30,040 --> 00:02:36,320
Alright everyone, I am on the line with Nialing Marosi.

33
00:02:36,320 --> 00:02:42,880
Nialing is a senior data science researcher at the Council for Scientific and Industrial

34
00:02:42,880 --> 00:02:49,040
Research, or CSIR, in Pretoria, South Africa, and Nialing, welcome to this weekend machine

35
00:02:49,040 --> 00:02:50,760
learning and AI.

36
00:02:50,760 --> 00:02:51,760
Thank you.

37
00:02:51,760 --> 00:02:52,760
Thank you.

38
00:02:52,760 --> 00:02:56,040
I'm really happy to be on today.

39
00:02:56,040 --> 00:02:58,960
And I'm happy to have you on.

40
00:02:58,960 --> 00:03:06,360
I got a chance to hear your talk at the Black and AI workshop at Nips, and you are doing

41
00:03:06,360 --> 00:03:09,040
some really amazing work.

42
00:03:09,040 --> 00:03:14,080
And I'm looking forward to learning even more about it.

43
00:03:14,080 --> 00:03:19,400
But before we get into what you're up to nowadays, why don't you spend a little bit of time

44
00:03:19,400 --> 00:03:24,240
telling the audience about your background and how you got involved in data science and

45
00:03:24,240 --> 00:03:25,240
machine learning?

46
00:03:25,240 --> 00:03:35,720
Alright, so I am actually originally from Lesutu, but I did most of my university in

47
00:03:35,720 --> 00:03:40,000
the US, so I went to my Calista College for my undergraduate.

48
00:03:40,000 --> 00:03:46,200
I was going to be an econ major because all the cool kids were doing it, but I know

49
00:03:46,200 --> 00:03:49,760
and I've been distracted halfway through.

50
00:03:49,760 --> 00:03:57,640
So yes, in St Paul, Minnesota, I decided during my degree that I ought to know a little

51
00:03:57,640 --> 00:04:03,680
bit more about computing in order to be a really good economist, of course.

52
00:04:03,680 --> 00:04:06,840
But then the bug caught.

53
00:04:06,840 --> 00:04:08,400
I just couldn't stop.

54
00:04:08,400 --> 00:04:13,200
I took an AI class in undergrad, and I was taught by this really brilliant woman, Susan

55
00:04:13,200 --> 00:04:14,200
Fox.

56
00:04:14,200 --> 00:04:17,680
I'm completely plugging her in right now shamelessly.

57
00:04:17,680 --> 00:04:22,720
She was completely brilliant, and I was just wowed by it and the area.

58
00:04:22,720 --> 00:04:27,120
But generally, the whole reason I was into econ to even start with was because I was really

59
00:04:27,120 --> 00:04:29,200
obsessed with trying to understand things.

60
00:04:29,200 --> 00:04:31,240
Like, why do things happen the way they happen?

61
00:04:31,240 --> 00:04:33,040
Why do people do the things they do?

62
00:04:33,040 --> 00:04:37,560
And for what I, as far as I knew, economic theory was one of the best way to describe why

63
00:04:37,560 --> 00:04:39,680
people do the things they did.

64
00:04:39,680 --> 00:04:46,440
But then when I got into AI and the modeling of AI, and then later on, I went to, for my

65
00:04:46,440 --> 00:04:52,520
PhD, which is very unfinished, I went to University of Minnesota and I was doing machine learning.

66
00:04:52,520 --> 00:04:59,480
Then it was also the development of these models that can help you not only observe, which

67
00:04:59,480 --> 00:05:05,560
sometimes in econ, you observe, and then you plan back, but also try to model these things

68
00:05:05,560 --> 00:05:09,040
and try to extract these features that explain to you.

69
00:05:09,040 --> 00:05:14,320
And if you can, you know, in mechanical design, mechanism design, you know, the whole thing

70
00:05:14,320 --> 00:05:18,760
of like, if you can just tweak it a little bit, you can get the output you want, that really

71
00:05:18,760 --> 00:05:20,680
got me very interested.

72
00:05:20,680 --> 00:05:25,640
So overall, I will say I am a true student of liberal arts.

73
00:05:25,640 --> 00:05:31,440
So my machine learning is tainted by econ, is tainted by biology, and it's tainted

74
00:05:31,440 --> 00:05:33,520
by a couple of other things.

75
00:05:33,520 --> 00:05:37,520
So but I've always just wanted to know how things work the way they work, why they work,

76
00:05:37,520 --> 00:05:38,680
the way they work.

77
00:05:38,680 --> 00:05:39,760
And I love Paxons.

78
00:05:39,760 --> 00:05:43,320
I love just discovering what's embedded in a process.

79
00:05:43,320 --> 00:05:44,800
And yeah, that's that.

80
00:05:44,800 --> 00:05:46,320
Yeah, that's amazing.

81
00:05:46,320 --> 00:05:51,960
The more and the deeper I get into, you know, the machine learning AI and the kind

82
00:05:51,960 --> 00:05:57,880
of the opportunities around it, the more I am convinced that, you know, everyone, you

83
00:05:57,880 --> 00:06:03,240
know, we need people from a very broad cross section of backgrounds who understand

84
00:06:03,240 --> 00:06:09,360
this and are working on it and can kind of, you know, both, you know, bring their perspectives

85
00:06:09,360 --> 00:06:15,160
to it, but also apply it to, you know, their disciplines and I'll end doing that.

86
00:06:15,160 --> 00:06:19,040
Can we really advance this to where it needs to be?

87
00:06:19,040 --> 00:06:20,880
No, I agree with you.

88
00:06:20,880 --> 00:06:27,840
I actually think it's in the problem that the worth of these processes is.

89
00:06:27,840 --> 00:06:33,360
I mean, the mathematics can be really beautiful and understand this, the structures under.

90
00:06:33,360 --> 00:06:38,920
But when you sit in an area and you're able to dissect an area and actually apply it,

91
00:06:38,920 --> 00:06:44,480
I've always been drawn to that kind of machine learning, so very applied.

92
00:06:44,480 --> 00:06:46,920
I like seeing the results.

93
00:06:46,920 --> 00:06:47,920
What is CSIR?

94
00:06:47,920 --> 00:06:53,520
Oh, yeah, CSIR, so it's, I think in the US, we call it the government lab.

95
00:06:53,520 --> 00:06:54,520
Okay.

96
00:06:54,520 --> 00:06:55,520
National labs.

97
00:06:55,520 --> 00:06:57,320
We have a national lab, yeah.

98
00:06:57,320 --> 00:06:58,320
Yeah.

99
00:06:58,320 --> 00:06:59,320
Okay.

100
00:06:59,320 --> 00:07:00,320
Yeah.

101
00:07:00,320 --> 00:07:02,160
So it's just this campus and we've got different units.

102
00:07:02,160 --> 00:07:06,800
We've got nano-centile study, nano-particles and the applications, biosciences and these are

103
00:07:06,800 --> 00:07:08,360
the biologies, the wet labs.

104
00:07:08,360 --> 00:07:09,360
Okay.

105
00:07:09,360 --> 00:07:10,360
We've got security.

106
00:07:10,360 --> 00:07:16,080
We've got, so it's a bunch of research lab, national research labs and we serve both the

107
00:07:16,080 --> 00:07:18,800
government and industry.

108
00:07:18,800 --> 00:07:22,840
And academia, actually, sometimes.

109
00:07:22,840 --> 00:07:27,880
And so what are some of the types of projects that you get involved in there?

110
00:07:27,880 --> 00:07:38,200
Yeah, so going back to my all-ever-so-curious, I've been in, I know, but I do try to segment

111
00:07:38,200 --> 00:07:43,360
them, but I've been in a couple of projects and maybe just for today I'll talk about just

112
00:07:43,360 --> 00:07:44,360
the two of them.

113
00:07:44,360 --> 00:07:50,080
One that I'm currently working on and one that I just recently parted with a little bit.

114
00:07:50,080 --> 00:07:56,360
And so some of one of the projects that we worked with was an understanding rhino poaching.

115
00:07:56,360 --> 00:08:02,480
So I have a feeling a lot of people will be aware of this that there is a huge problem

116
00:08:02,480 --> 00:08:04,960
in rhino poaching.

117
00:08:04,960 --> 00:08:11,080
And so we were contracted by the South African park rangers who were the guys that police

118
00:08:11,080 --> 00:08:12,920
the parks in South Africa.

119
00:08:12,920 --> 00:08:20,000
And they contracted us to say, can you guys provide for us some model to understand how

120
00:08:20,000 --> 00:08:21,240
these poaching happens?

121
00:08:21,240 --> 00:08:26,640
And if there is any way that maybe we can have a sort of predictive system that we can

122
00:08:26,640 --> 00:08:30,600
work with, to just cut down on the search space.

123
00:08:30,600 --> 00:08:31,600
So we worked with them.

124
00:08:31,600 --> 00:08:37,480
We were contracted to go work at the Kruger National Park, which is a fairly big national

125
00:08:37,480 --> 00:08:38,480
park.

126
00:08:38,480 --> 00:08:39,480
It's the size of Israel.

127
00:08:39,480 --> 00:08:40,480
Yeah.

128
00:08:40,480 --> 00:08:41,480
Kruger.

129
00:08:41,480 --> 00:08:43,480
Yeah.

130
00:08:43,480 --> 00:08:45,440
It spans all the way into Mozambique.

131
00:08:45,440 --> 00:08:47,440
It's on the border with South Africa and Mozambique.

132
00:08:47,440 --> 00:08:48,920
It's fairly big.

133
00:08:48,920 --> 00:08:55,280
But we work with one of the units in the CSI heart that does security.

134
00:08:55,280 --> 00:09:01,360
And we just, we were building a model to sort of try to narrow down a probability distribution

135
00:09:01,360 --> 00:09:03,400
map over the land.

136
00:09:03,400 --> 00:09:07,680
So to say, you look at the different features there as in things like when was the last

137
00:09:07,680 --> 00:09:12,400
poaching, how far from water, what's the weather like, what's the moonlight like, how far

138
00:09:12,400 --> 00:09:16,640
from the road, and things like those or how dense is the forestry.

139
00:09:16,640 --> 00:09:20,640
And try to put all those features and see how much each one of them contributes to an

140
00:09:20,640 --> 00:09:26,080
area being very high that, you know, a rhino is going to be poached there.

141
00:09:26,080 --> 00:09:31,560
And by doing that, then, you know, maybe the parks can then be allocated there in areas

142
00:09:31,560 --> 00:09:35,000
that we suspect is going to be high poaching.

143
00:09:35,000 --> 00:09:41,440
Obviously, this is, this is a model that can get quite compromised to somebody knows how

144
00:09:41,440 --> 00:09:42,440
it works.

145
00:09:42,440 --> 00:09:46,480
Because then, you know, shifting resources from one area to the other.

146
00:09:46,480 --> 00:09:48,560
And so, you know, maybe that's a lot of time.

147
00:09:48,560 --> 00:09:50,600
But rhinos are fairly territorial.

148
00:09:50,600 --> 00:09:54,960
And so, like, the poachings are going to happen around where they are going to be.

149
00:09:54,960 --> 00:09:59,640
And so we studied those things like how they migrate and all of that kind of stuff.

150
00:09:59,640 --> 00:10:01,480
So that was a project that we did.

151
00:10:01,480 --> 00:10:03,640
It's currently running.

152
00:10:03,640 --> 00:10:11,000
And it ended some early this, this, oh, it's a new year in 2017 and March, 2017.

153
00:10:11,000 --> 00:10:12,000
Okay.

154
00:10:12,000 --> 00:10:13,000
So that was that.

155
00:10:13,000 --> 00:10:14,560
But currently, yes.

156
00:10:14,560 --> 00:10:15,560
You make that one.

157
00:10:15,560 --> 00:10:18,240
That project sounds so easy.

158
00:10:18,240 --> 00:10:24,240
But when you kind of rattle off some of the features of this model, it strikes me that

159
00:10:24,240 --> 00:10:26,920
data is coming from all over the place.

160
00:10:26,920 --> 00:10:29,200
Like lots of different data sources.

161
00:10:29,200 --> 00:10:33,600
Can you talk a little bit about the, you know, the kind of the pipeline.

162
00:10:33,600 --> 00:10:37,760
And the challenges associated with that particular project.

163
00:10:37,760 --> 00:10:38,760
Yeah.

164
00:10:38,760 --> 00:10:41,560
So we actually worked a lot with the experts.

165
00:10:41,560 --> 00:10:45,080
So as one of those models that, like, one was informed by data and the other one was

166
00:10:45,080 --> 00:10:50,040
informed by the experts to start off with, we built this thing where, as the ranges are

167
00:10:50,040 --> 00:10:52,040
patrolling, they can start to input data.

168
00:10:52,040 --> 00:10:53,040
Okay.

169
00:10:53,040 --> 00:10:57,280
So, for example, data about you can see some of the leftover food steps by maybe people

170
00:10:57,280 --> 00:10:58,760
that should not have been in the park.

171
00:10:58,760 --> 00:10:59,760
Okay.

172
00:10:59,760 --> 00:11:03,760
There will be some areas of the parks that are closed off, but also if they had been approaching,

173
00:11:03,760 --> 00:11:06,120
you can see like food steps towards there.

174
00:11:06,120 --> 00:11:11,920
And so like that would information that like a park range would, would, would input.

175
00:11:11,920 --> 00:11:14,600
But also the park does have sensors in there.

176
00:11:14,600 --> 00:11:20,720
And so the sensors would be another, you know, input, data input.

177
00:11:20,720 --> 00:11:28,560
The park we do have knowledge about where the water sources are and generally what, like,

178
00:11:28,560 --> 00:11:36,040
vegetation and what's the word, like, you know, the level, like, how steep it is, how mountain

179
00:11:36,040 --> 00:11:37,840
else it is and stuff like that.

180
00:11:37,840 --> 00:11:38,840
Okay.

181
00:11:38,840 --> 00:11:40,000
And like, how far from the road it is.

182
00:11:40,000 --> 00:11:44,520
So there was already some of that data that was there because obviously this is a very

183
00:11:44,520 --> 00:11:46,960
important area in South Africa.

184
00:11:46,960 --> 00:11:54,120
So that information was there, but that combined with literally the ranges just walking in the

185
00:11:54,120 --> 00:11:55,120
park.

186
00:11:55,120 --> 00:11:56,120
Okay.

187
00:11:56,120 --> 00:12:01,240
So something can be noted as the water source, but then the water is all gone if it hadn't

188
00:12:01,240 --> 00:12:04,760
been raining, things like those, you know, then they would correct it.

189
00:12:04,760 --> 00:12:11,240
So we were seriously learning and we actually have this as a, as it's continuously ingesting

190
00:12:11,240 --> 00:12:12,240
new data.

191
00:12:12,240 --> 00:12:18,920
So every day we get new data from, you know, the last night patrols or things like those.

192
00:12:18,920 --> 00:12:22,560
We also get data from helicopters that will fly over.

193
00:12:22,560 --> 00:12:23,560
Yeah.

194
00:12:23,560 --> 00:12:28,320
So you're right, it came from multiple sources and we just put it together.

195
00:12:28,320 --> 00:12:34,600
Not just, but so some of the problems that we run into here, one of the biggest problems

196
00:12:34,600 --> 00:12:39,840
actually that we ran into was the problem of statistics and the problem is that we would

197
00:12:39,840 --> 00:12:42,360
have a lot of spots.

198
00:12:42,360 --> 00:12:46,880
So we, we had very sparse data, it's a vast land.

199
00:12:46,880 --> 00:12:52,720
And so we will have a lot of areas where we hadn't seen any data points, right?

200
00:12:52,720 --> 00:12:56,400
We don't, we don't have any information on whether there had been a poaching or something

201
00:12:56,400 --> 00:12:57,400
like that.

202
00:12:57,400 --> 00:13:01,800
And we really could only predict based, you know, like everything else that there was

203
00:13:01,800 --> 00:13:03,640
no poaching.

204
00:13:03,640 --> 00:13:08,760
We had to also say whether there was no poaching because there can be a poaching there

205
00:13:08,760 --> 00:13:10,960
or it just hadn't occurred yet.

206
00:13:10,960 --> 00:13:11,960
Okay.

207
00:13:11,960 --> 00:13:12,960
Yeah.

208
00:13:12,960 --> 00:13:18,560
So that was, I think the spacity of the data was one of the things that was quite difficult

209
00:13:18,560 --> 00:13:22,360
when we're trying to generalize over the whole park.

210
00:13:22,360 --> 00:13:26,680
When you, you, you do see it change.

211
00:13:26,680 --> 00:13:29,840
We did see the patterns change over the years.

212
00:13:29,840 --> 00:13:34,320
So, you know, that's why I go back to that thing of saying, you know, just because you haven't

213
00:13:34,320 --> 00:13:37,680
seen a poaching happen in an area doesn't mean that it's not going to be the next area

214
00:13:37,680 --> 00:13:38,680
of interest.

215
00:13:38,680 --> 00:13:39,680
Right.

216
00:13:39,680 --> 00:13:40,680
Right.

217
00:13:40,680 --> 00:13:41,680
Yeah.

218
00:13:41,680 --> 00:13:46,640
How did you deal with attacking that or overcoming that sparsity issue?

219
00:13:46,640 --> 00:13:52,320
Well, we are, I don't know if we, this is cheating or what, but we, we did it.

220
00:13:52,320 --> 00:13:53,320
We did a lot of smoothing.

221
00:13:53,320 --> 00:13:55,320
So, we did a lot of things.

222
00:13:55,320 --> 00:13:56,320
Okay.

223
00:13:56,320 --> 00:14:00,960
It takes things out, but, you know what though, I have to say this.

224
00:14:00,960 --> 00:14:05,640
It turned out that it wasn't even the big problem with our motto.

225
00:14:05,640 --> 00:14:08,680
It seemed, it was pretty good.

226
00:14:08,680 --> 00:14:12,640
So we had cut up the park into Kilometre square grid.

227
00:14:12,640 --> 00:14:13,640
Okay.

228
00:14:13,640 --> 00:14:17,680
It's pretty good, but it turned out that even at the granularity of Kilometre square

229
00:14:17,680 --> 00:14:20,040
grid, it's still too difficult.

230
00:14:20,040 --> 00:14:24,720
Like a poaching can actually happen in this square where we said it would happen.

231
00:14:24,720 --> 00:14:27,160
And they will still not get the poachers.

232
00:14:27,160 --> 00:14:28,160
Ah, okay.

233
00:14:28,160 --> 00:14:30,160
And so it has to make it bigger.

234
00:14:30,160 --> 00:14:31,160
So it has to make it bigger.

235
00:14:31,160 --> 00:14:32,160
That granularity was even too big.

236
00:14:32,160 --> 00:14:33,160
Ah, even that.

237
00:14:33,160 --> 00:14:38,560
And this is, I think, we were working, I think, overall, it's like very square Kilometres

238
00:14:38,560 --> 00:14:42,360
and we broke it down into Kilometres and even that wasn't, wasn't good enough in terms

239
00:14:42,360 --> 00:14:47,560
of truly pinpointing this in order to make it effective enough.

240
00:14:47,560 --> 00:14:51,680
So I, you know, it's one of those where you're like, that's, I mean, sometimes it worked,

241
00:14:51,680 --> 00:14:58,120
but then it was, that part of it was a bit difficult because it's like, oh, you know, you were

242
00:14:58,120 --> 00:15:06,480
so close and our model, certainly, took that into effect because for us, our model punished

243
00:15:06,480 --> 00:15:09,160
a lot more if we missed the poaching, right?

244
00:15:09,160 --> 00:15:10,160
Right.

245
00:15:10,160 --> 00:15:14,360
And if you say there's going to be a poaching and the poaching doesn't happen.

246
00:15:14,360 --> 00:15:17,480
So those are some of the things that we had to take care of.

247
00:15:17,480 --> 00:15:22,920
And like, you know, making sure that the cost truly reflects the outcome that we want.

248
00:15:22,920 --> 00:15:30,080
And was the, you talked about the granularity in terms of the area, but what was the granularity

249
00:15:30,080 --> 00:15:32,000
in terms of time?

250
00:15:32,000 --> 00:15:38,640
Like how did you, did you, were you trying to predict that a poaching might happen in

251
00:15:38,640 --> 00:15:41,280
a given day or week or month?

252
00:15:41,280 --> 00:15:42,280
It was a day.

253
00:15:42,280 --> 00:15:43,280
Okay.

254
00:15:43,280 --> 00:15:48,080
And we were getting updated data every day and so the predictions were daily within

255
00:15:48,080 --> 00:15:49,080
you data.

256
00:15:49,080 --> 00:15:50,400
Yeah.

257
00:15:50,400 --> 00:15:58,240
And did you find that the predictions in terms of area very dramatically day to day or

258
00:15:58,240 --> 00:16:01,120
were they relatively static?

259
00:16:01,120 --> 00:16:05,920
Yeah, the shifts would take a little bit longer.

260
00:16:05,920 --> 00:16:10,520
So the migration pattern in terms of where the poachings would happen would actually shift

261
00:16:10,520 --> 00:16:14,960
a little bit like we take multiple, multiple days to actually move.

262
00:16:14,960 --> 00:16:17,440
But like I said, that's the problem.

263
00:16:17,440 --> 00:16:21,160
And if you say, you know, it's going to happen in this grid and then like next, it's going

264
00:16:21,160 --> 00:16:26,920
to happen in the next grid or, you know, it was still around the same area.

265
00:16:26,920 --> 00:16:30,680
So the policy wasn't as bad.

266
00:16:30,680 --> 00:16:34,280
But once again, it's the limited resources that the park rangers have.

267
00:16:34,280 --> 00:16:36,040
It's the danger of the job.

268
00:16:36,040 --> 00:16:37,040
Right.

269
00:16:37,040 --> 00:16:38,040
You can't just complain side, right?

270
00:16:38,040 --> 00:16:41,280
I mean, just because you are there doesn't mean that they're going to stop poaching.

271
00:16:41,280 --> 00:16:42,280
Right.

272
00:16:42,280 --> 00:16:43,280
Yeah.

273
00:16:43,280 --> 00:16:48,760
And so there were some of those things that actually, you know, really complicated the problem.

274
00:16:48,760 --> 00:16:49,760
But you know what?

275
00:16:49,760 --> 00:16:50,760
I have to say this.

276
00:16:50,760 --> 00:16:53,120
And maybe there's a little bit of a plug.

277
00:16:53,120 --> 00:16:55,800
It was, it was very interesting work.

278
00:16:55,800 --> 00:16:57,120
It was very useful work.

279
00:16:57,120 --> 00:17:00,400
And if there's people, I know there's work like this happening in Kenya.

280
00:17:00,400 --> 00:17:03,840
The groups that are doing the same thing around poaching in Kenya.

281
00:17:03,840 --> 00:17:07,440
And you know, it's one of those things that, you know, it just needs, maybe needs more

282
00:17:07,440 --> 00:17:14,440
men power because it's, it's the only way to truly beat it because it's a social problem

283
00:17:14,440 --> 00:17:16,560
as long as the incentives are there.

284
00:17:16,560 --> 00:17:17,560
Right.

285
00:17:17,560 --> 00:17:21,880
I mean, definitely that gets made by poaching is an offer somebody to risk their life and

286
00:17:21,880 --> 00:17:25,240
go to jail and or what jail, right?

287
00:17:25,240 --> 00:17:26,680
So that's the, that's the thing.

288
00:17:26,680 --> 00:17:30,720
That's why I'm saying like, even when we made it fairly difficult, there would still

289
00:17:30,720 --> 00:17:32,720
be occurrences.

290
00:17:32,720 --> 00:17:33,720
Right.

291
00:17:33,720 --> 00:17:37,160
And and it's mostly because of the geographical space.

292
00:17:37,160 --> 00:17:42,920
So when this model runs in smaller spaces, so there were privately owned firms around

293
00:17:42,920 --> 00:17:43,920
there.

294
00:17:43,920 --> 00:17:44,920
And those are not as big.

295
00:17:44,920 --> 00:17:49,880
And their problem there wasn't as bad because, you know, they're managing a smaller space.

296
00:17:49,880 --> 00:17:54,400
But given how big the space is, given the incentives, given that this is a project that

297
00:17:54,400 --> 00:18:00,160
goes between two countries, you know, it gets very difficult to pull this.

298
00:18:00,160 --> 00:18:01,160
Right.

299
00:18:01,160 --> 00:18:07,320
We lend a lot and the South African park rangers are still using the two.

300
00:18:07,320 --> 00:18:09,200
So they are still getting some value for it.

301
00:18:09,200 --> 00:18:14,000
So I always say sometimes when you're working with Israel world problems, it's just cutting

302
00:18:14,000 --> 00:18:15,000
the search space.

303
00:18:15,000 --> 00:18:16,600
It's not finding the solution.

304
00:18:16,600 --> 00:18:18,320
It's cutting the search space.

305
00:18:18,320 --> 00:18:19,320
Mm hmm.

306
00:18:19,320 --> 00:18:20,320
Yeah.

307
00:18:20,320 --> 00:18:28,400
And in this particular example, we're talking about search space both from a modeling perspective,

308
00:18:28,400 --> 00:18:32,240
but also literally the land perspective.

309
00:18:32,240 --> 00:18:42,240
Yes, the landmass, the little red, yeah, like, yes, in kiloveturn, in by, so no, it's

310
00:18:42,240 --> 00:18:43,240
in kiloveturn.

311
00:18:43,240 --> 00:18:44,240
Right.

312
00:18:44,240 --> 00:18:45,240
Yes.

313
00:18:45,240 --> 00:18:46,240
Right.

314
00:18:46,240 --> 00:18:55,080
But yeah, that project came to, well, our contribution, my contribution in that project

315
00:18:55,080 --> 00:19:01,760
kind of came to an end when we were done developing the model, and that was in March.

316
00:19:01,760 --> 00:19:05,000
And pieces of those are published.

317
00:19:05,000 --> 00:19:09,720
And then of course, like I said, the software was delivered to the park rangers.

318
00:19:09,720 --> 00:19:17,160
And so I moved on from that one, from that project, but it's certainly an interesting one.

319
00:19:17,160 --> 00:19:20,640
And it's been one where we actually have been contacted by other people.

320
00:19:20,640 --> 00:19:25,120
For example, the cash in transit robberies, you know, where they're saying, you know, can

321
00:19:25,120 --> 00:19:29,600
you tell us, spatial, because we've worked with spatial data now, spatially, you know,

322
00:19:29,600 --> 00:19:33,960
what are the chances that this one area is going to be a hotspot for the next, you know,

323
00:19:33,960 --> 00:19:42,360
a hit on a delivery, but yes, you know, they all link up, well, somewhat.

324
00:19:42,360 --> 00:19:46,120
It's different, but you know, at least they have that spatial component.

325
00:19:46,120 --> 00:19:47,120
Right.

326
00:19:47,120 --> 00:19:48,120
Right.

327
00:19:48,120 --> 00:19:54,720
So that one ended about a year ago, just over a year ago.

328
00:19:54,720 --> 00:19:57,840
And so now you're working on what?

329
00:19:57,840 --> 00:20:02,560
So now I actually, it's funny because my life sort of came full circle, and I went back

330
00:20:02,560 --> 00:20:03,560
to biology.

331
00:20:03,560 --> 00:20:07,680
I know I did say that I started off as an economist, but people have to forgive me because

332
00:20:07,680 --> 00:20:12,280
like I say, I am a liberal art student.

333
00:20:12,280 --> 00:20:18,240
So I actually have my undergrad is both computer science and biology, and then, you know, my

334
00:20:18,240 --> 00:20:22,120
PhD work was the science economics and bioinformatics a little bit.

335
00:20:22,120 --> 00:20:23,560
I was just doing a project.

336
00:20:23,560 --> 00:20:29,200
That's also why the PhD never got finished, there was just too much, too much going on.

337
00:20:29,200 --> 00:20:35,040
But so I got contacted by the bio sciences group here at the CSIR.

338
00:20:35,040 --> 00:20:41,120
So these are the white lab biology, so we actually run the experiments and run samples.

339
00:20:41,120 --> 00:20:49,320
And so they had run a project like last year, and that project was on understanding an

340
00:20:49,320 --> 00:20:54,400
effect of one HIV treatment drug, and they found that, you know, there were biomarkers

341
00:20:54,400 --> 00:21:00,040
in African populations that had not been studied and were causing renal failure in the patients

342
00:21:00,040 --> 00:21:02,840
that were taking that specific drug.

343
00:21:02,840 --> 00:21:08,160
And so based on that, it was a signal that no, actually we really have to take this problem

344
00:21:08,160 --> 00:21:09,800
very carefully.

345
00:21:09,800 --> 00:21:16,680
So we know that generally people have the same processes, generally, as human beings.

346
00:21:16,680 --> 00:21:25,600
But also there are some manifestations of diseases that are race or geography dependent.

347
00:21:25,600 --> 00:21:31,600
And so we wanted to, so South Africa is going through a whole thing.

348
00:21:31,600 --> 00:21:38,600
So there's such an increase in cancer incidence in South Africa, as is in the world over.

349
00:21:38,600 --> 00:21:45,400
But there's several research that will show that the rates of these incidence is actually

350
00:21:45,400 --> 00:21:47,600
different in different populations.

351
00:21:47,600 --> 00:21:53,840
So for example, black men will have a higher incidence rate of prostate cancer.

352
00:21:53,840 --> 00:21:59,320
Or for example, it's a lot more difficult to detect breast cancer in black women, and

353
00:21:59,320 --> 00:22:05,280
actually in Latina women, even more, mostly because of how dense the breast tissue is.

354
00:22:05,280 --> 00:22:09,880
So there are some of these things that we know that there are some differential biomarkers

355
00:22:09,880 --> 00:22:14,560
in different populations that make the disease manifest itself differently.

356
00:22:14,560 --> 00:22:17,720
And therefore the therapy ought to be different.

357
00:22:17,720 --> 00:22:26,520
And so we decided to study pancreatic cancer in African population, but specifically South

358
00:22:26,520 --> 00:22:32,360
Africans, which literature will show that they can be quite different, you know, they

359
00:22:32,360 --> 00:22:35,280
can be quite genetically different from maybe let's say West African.

360
00:22:35,280 --> 00:22:36,280
So I want to make that clear.

361
00:22:36,280 --> 00:22:39,680
We're not looking at all people of African descent.

362
00:22:39,680 --> 00:22:44,040
One of the really biggest things I have to say, so besides understanding these biomarkers

363
00:22:44,040 --> 00:22:47,640
is actually just to create the data, make sure the data is out there.

364
00:22:47,640 --> 00:22:53,040
So there may be however much we get to in identifying these biomarkers in pancreatic

365
00:22:53,040 --> 00:22:58,240
cancer, but the most important thing is to create this data so that other researchers

366
00:22:58,240 --> 00:23:03,360
in the rest of the world can study the data because one of the biggest problem with diseases

367
00:23:03,360 --> 00:23:09,160
in African populations is that African populations are not usually included in medical trials.

368
00:23:09,160 --> 00:23:14,720
So there isn't enough information and enough data to go about to see how things affect

369
00:23:14,720 --> 00:23:16,600
people of African origin.

370
00:23:16,600 --> 00:23:21,240
The same happens with the amount of genomic data that's out there, which of course also

371
00:23:21,240 --> 00:23:23,320
the amount of proteomic data.

372
00:23:23,320 --> 00:23:26,160
We are looking at proteomic data.

373
00:23:26,160 --> 00:23:33,640
So this is the data, it's just the proteins and that in actually it's going to be peptides.

374
00:23:33,640 --> 00:23:38,600
But I'm going to stick with proteins just for the sake of just removing it.

375
00:23:38,600 --> 00:23:43,760
It's not really complex, but you know, just for we're having a chip chat, right?

376
00:23:43,760 --> 00:23:51,880
Anyway, the scientists hopefully will not kill me for saying proteins when actually.

377
00:23:51,880 --> 00:23:58,440
So yeah, so you know, we just want to collect like try to find some of these proteins and

378
00:23:58,440 --> 00:24:04,360
see if there are some of them are actually biomarkers in this disease and maybe it might

379
00:24:04,360 --> 00:24:08,640
actually help to say what actually a biomarker is because there's a technical definition

380
00:24:08,640 --> 00:24:14,760
of the biomarker is that a biomarker has to be a protein that is related to a disease.

381
00:24:14,760 --> 00:24:18,040
It is actionable and it is measurable.

382
00:24:18,040 --> 00:24:21,840
So it is measurable enough that you can make a decision.

383
00:24:21,840 --> 00:24:28,040
So there are, so this is why we always talk about biomarkers because if you can truly identify

384
00:24:28,040 --> 00:24:33,040
biomarkers, which means they are actionable, then you can hopefully start to think about

385
00:24:33,040 --> 00:24:39,920
drug development or some kind of corrective thing because you can clearly pinpoint it

386
00:24:39,920 --> 00:24:41,760
and you can take some action.

387
00:24:41,760 --> 00:24:47,400
And some of that action can just be that you can predict and say if I see this biomarker,

388
00:24:47,400 --> 00:24:54,400
I know it is differentiated enough that I can actually predict your susceptibility to

389
00:24:54,400 --> 00:24:58,760
the disease or actually whether the disease or the stage of the disease.

390
00:24:58,760 --> 00:25:00,320
So that's what we wanted to do.

391
00:25:00,320 --> 00:25:08,000
We want to identify these biomarkers and then we want to populate this data set so that

392
00:25:08,000 --> 00:25:09,800
other research has had it.

393
00:25:09,800 --> 00:25:15,920
And for us, we certainly want to find these biomarkers and pass them on to our own, you

394
00:25:15,920 --> 00:25:23,640
know, drug development companies or, you know, we, it's a whole pipeline, this whole system.

395
00:25:23,640 --> 00:25:27,800
So you've got doctors in one end and then you've got academic and research in such it

396
00:25:27,800 --> 00:25:28,800
on the other.

397
00:25:28,800 --> 00:25:33,480
You've got drug companies and then it goes right back to the doctors and we're working

398
00:25:33,480 --> 00:25:35,160
with the local hospitals here.

399
00:25:35,160 --> 00:25:36,800
That's where we're getting the samples.

400
00:25:36,800 --> 00:25:43,440
So in the samples from the local hospitals here, in most of them, I in Johannesburg,

401
00:25:43,440 --> 00:25:48,120
I'm sorry, but it's like 40 kilometers, 50 kilometers away from each other, so practically

402
00:25:48,120 --> 00:25:49,520
close by.

403
00:25:49,520 --> 00:25:56,920
Maybe let's talk a little bit about that, that initial element of collecting the samples

404
00:25:56,920 --> 00:26:00,960
and processing them, and turning them into a data set that you can use.

405
00:26:00,960 --> 00:26:03,520
How, how do you go about that?

406
00:26:03,520 --> 00:26:08,560
Well, you get a sample and you have to prepare this sample.

407
00:26:08,560 --> 00:26:13,520
So I mean, you know, like the doctor goes in and then they diagnose you, oh, that sounds

408
00:26:13,520 --> 00:26:14,520
bad.

409
00:26:14,520 --> 00:26:17,120
I'm sorry, it's always so morbid this whole discussion.

410
00:26:17,120 --> 00:26:24,240
But, you know, like, you know, there's cancer and they, you know, then we ask, you know,

411
00:26:24,240 --> 00:26:28,840
when they are going to do biopsies and stuff like that, right, they'll have like a sample.

412
00:26:28,840 --> 00:26:35,760
And so we get this sample and then it goes into a lab, it gets prepared, it gets run through

413
00:26:35,760 --> 00:26:40,240
mass spec, and the actual process that we use is the SWAT process.

414
00:26:40,240 --> 00:26:44,440
It really is not that important, but it goes through mass spec, and then mass spec is going

415
00:26:44,440 --> 00:26:47,520
to tell us, you know, which proteins are present in this sample.

416
00:26:47,520 --> 00:26:51,920
It's also going to tell us how much of each one of these proteins are present.

417
00:26:51,920 --> 00:26:57,320
And it's in that how much, well, assuming, first of all, you can clearly and neatly say

418
00:26:57,320 --> 00:27:01,920
which proteins are present, because that's the whole science of its own too.

419
00:27:01,920 --> 00:27:06,760
And then you then, after you have defined your, your, your identify them, then you find

420
00:27:06,760 --> 00:27:10,640
out how much of them are present in the sample.

421
00:27:10,640 --> 00:27:17,560
And then you are then, if, if you can find differences between either a protein or a group

422
00:27:17,560 --> 00:27:22,680
of protein between the disease cells and the non-disease cells, then you can say that,

423
00:27:22,680 --> 00:27:26,080
you know, you're starting to get biomarkers for that specific disease.

424
00:27:26,080 --> 00:27:31,880
Now, some of the problems that we run into here is, so I actually had one.

425
00:27:31,880 --> 00:27:36,840
I worked a little bit with data, with genomics data, and anyone that has done like genomics

426
00:27:36,840 --> 00:27:42,120
data and done PCR, I mean, the whole point of PCR is to, what's it called, is to make

427
00:27:42,120 --> 00:27:43,120
more of it.

428
00:27:43,120 --> 00:27:44,440
I can't think of the word.

429
00:27:44,440 --> 00:27:46,440
It's to amplify it.

430
00:27:46,440 --> 00:27:47,440
Expression or amplification?

431
00:27:47,440 --> 00:27:48,440
Yeah.

432
00:27:48,440 --> 00:27:49,440
Yeah.

433
00:27:49,440 --> 00:27:50,440
You know, you can amplify DNA, right?

434
00:27:50,440 --> 00:27:53,280
Unfortunately, for us, you can amplify the protein.

435
00:27:53,280 --> 00:27:56,280
So the amount of sample you have is the amount of sample you have.

436
00:27:56,280 --> 00:27:57,280
Okay.

437
00:27:57,280 --> 00:28:03,440
As always, you have like, so proteomics is one of these things that reproducibility is

438
00:28:03,440 --> 00:28:06,040
a big thing.

439
00:28:06,040 --> 00:28:10,400
And so also, you know, like the whole thing of like confidence in your results.

440
00:28:10,400 --> 00:28:17,400
So this whole statistical packages that are just developed to do this, because the sample

441
00:28:17,400 --> 00:28:20,320
will degrade, but now you can't do anything.

442
00:28:20,320 --> 00:28:22,920
And so you don't get as high a signal.

443
00:28:22,920 --> 00:28:24,920
And so, yeah.

444
00:28:24,920 --> 00:28:30,200
So one of the first thing actually we always do is to standardize our equipment so that

445
00:28:30,200 --> 00:28:36,320
when we do get the sample, we try to get everything run as quickly as possible.

446
00:28:36,320 --> 00:28:41,200
I know I've actually just gone into so many other things, besides where we start the

447
00:28:41,200 --> 00:28:42,200
question.

448
00:28:42,200 --> 00:28:45,600
But hopefully there is a thread that runs through.

449
00:28:45,600 --> 00:28:51,720
Well, you know, one of the things that, you know, one of the things that comes up in

450
00:28:51,720 --> 00:28:57,760
this area is that the data collection itself is often pretty noisy.

451
00:28:57,760 --> 00:29:03,240
And I'm wondering if that's something that you experienced and, you know, how you dealt

452
00:29:03,240 --> 00:29:05,560
with that in your pipeline.

453
00:29:05,560 --> 00:29:06,560
Yeah.

454
00:29:06,560 --> 00:29:10,840
So we are actually just now starting to collect the data.

455
00:29:10,840 --> 00:29:12,160
So you're right.

456
00:29:12,160 --> 00:29:16,920
We get, we get like by like, no, this is a three year project.

457
00:29:16,920 --> 00:29:18,440
And so year one just passed.

458
00:29:18,440 --> 00:29:21,960
And so that was the data collection for the biologists.

459
00:29:21,960 --> 00:29:25,160
And for us, it was to standardize the workflow.

460
00:29:25,160 --> 00:29:28,840
So that then takes care of the technical noise.

461
00:29:28,840 --> 00:29:31,800
So what they call, what is it called, like technical or something?

462
00:29:31,800 --> 00:29:34,480
Anyway, I'm going to call it technical noise because I can't remember.

463
00:29:34,480 --> 00:29:37,120
And I think for people in this audience, it makes sense.

464
00:29:37,120 --> 00:29:40,920
But this is, you know, given the tools that you are using, right?

465
00:29:40,920 --> 00:29:45,840
How much noise do you just get from those tools and try to measure that?

466
00:29:45,840 --> 00:29:47,280
So that's the first thing.

467
00:29:47,280 --> 00:29:50,720
And then yes, we do have sample noise as well.

468
00:29:50,720 --> 00:29:56,640
And the thing, generally, the thing you do in this area is to just try to get as many

469
00:29:56,640 --> 00:30:01,160
samples as possible, which we can't always hide.

470
00:30:01,160 --> 00:30:05,120
And maybe that's a good thing because I mean, it's not as many people have counts are.

471
00:30:05,120 --> 00:30:11,680
But then of course, it has those other implications.

472
00:30:11,680 --> 00:30:17,240
But yeah, I mean, you just, I mean, like I say, it's just like any other statistical exercise.

473
00:30:17,240 --> 00:30:21,480
You just have to have more data in order to be confident in your results.

474
00:30:21,480 --> 00:30:22,480
You have to have more data.

475
00:30:22,480 --> 00:30:25,280
You have to run the samples multiple times.

476
00:30:25,280 --> 00:30:30,080
So for each sample, we run it multiple times and to find out, you know, the noise that's

477
00:30:30,080 --> 00:30:34,280
embedded in just that one sample and the variations that are in that one sample.

478
00:30:34,280 --> 00:30:36,960
And then we run it.

479
00:30:36,960 --> 00:30:41,320
Sometimes we run two samples at the same time and then look at like that, that are supposed

480
00:30:41,320 --> 00:30:43,360
to come from the same cluster.

481
00:30:43,360 --> 00:30:47,280
And then we look at like the variation of the protein there.

482
00:30:47,280 --> 00:30:50,960
And then that will help, you know, at least there we know that they just ran through the

483
00:30:50,960 --> 00:30:51,960
same run.

484
00:30:51,960 --> 00:30:55,240
So at least the technical variation is not exactly there.

485
00:30:55,240 --> 00:30:58,440
And then we can just look at the sample variation.

486
00:30:58,440 --> 00:30:59,440
But this is the place.

487
00:30:59,440 --> 00:31:00,440
This is what biology is.

488
00:31:00,440 --> 00:31:01,440
You're right.

489
00:31:01,440 --> 00:31:05,160
It's always, it's always a variation.

490
00:31:05,160 --> 00:31:12,920
And so the only decisions you really can make is when your data is sufficiently separated.

491
00:31:12,920 --> 00:31:18,080
So one of the first things you do is just look at the means of the two samples.

492
00:31:18,080 --> 00:31:21,600
So you look at mean expression of the disease cell and you look at mean expression of the

493
00:31:21,600 --> 00:31:23,840
non-disease cell.

494
00:31:23,840 --> 00:31:25,920
And then you compare the two means.

495
00:31:25,920 --> 00:31:30,640
And if, if, for example, the standard error, you know, Chris crosses, you really have to

496
00:31:30,640 --> 00:31:34,280
go back and just work through your, your workflow.

497
00:31:34,280 --> 00:31:37,480
So we just start off simple as that.

498
00:31:37,480 --> 00:31:40,720
Because then you can't tell, obviously there you can't draw in conclusions.

499
00:31:40,720 --> 00:31:45,440
But if we can start to see some of those differences, then we get a little bit more confident.

500
00:31:45,440 --> 00:31:49,840
And then we can start to see per sample, where each sample falls.

501
00:31:49,840 --> 00:31:55,160
And then we start to look at, you know, can we predict per sample?

502
00:31:55,160 --> 00:32:01,600
And but yeah, this is something that can be one protein or it can be a combination of

503
00:32:01,600 --> 00:32:02,600
proteins.

504
00:32:02,600 --> 00:32:08,840
Is the process that would happen on the medical side the same as your process?

505
00:32:08,840 --> 00:32:15,920
Meaning you, you got these biopsy samples and you ran them through the mass spec to

506
00:32:15,920 --> 00:32:18,600
separate out all the proteins.

507
00:32:18,600 --> 00:32:26,280
Is that how the absent, your data collection is that how the disease would be diagnosed

508
00:32:26,280 --> 00:32:32,960
or would it be like a radiologist and imaging data and that kind of thing, a different kind

509
00:32:32,960 --> 00:32:33,960
of process?

510
00:32:33,960 --> 00:32:37,800
Oh, I think usually it is still the imaging part.

511
00:32:37,800 --> 00:32:45,280
I have to also say, I don't know as much, I have absolute, everyone's, well, yeah, I have

512
00:32:45,280 --> 00:32:52,560
a lay only a lay on knowledge on how the diseases themselves are diagnosed, like in the hospital

513
00:32:52,560 --> 00:32:53,560
itself.

514
00:32:53,560 --> 00:32:59,280
Because yeah, I get to interact with the data once with the bioscientists and they run

515
00:32:59,280 --> 00:33:06,240
through and all I do is help them standardize the equipment and then later on to do the data

516
00:33:06,240 --> 00:33:07,240
analysis.

517
00:33:07,240 --> 00:33:08,240
Okay.

518
00:33:08,240 --> 00:33:13,280
And so the data that you get is already labeled as to whether the sample is cancerous or

519
00:33:13,280 --> 00:33:14,280
not?

520
00:33:14,280 --> 00:33:15,280
Yes.

521
00:33:15,280 --> 00:33:16,280
Okay.

522
00:33:16,280 --> 00:33:17,280
Yes.

523
00:33:17,280 --> 00:33:18,280
Yes.

524
00:33:18,280 --> 00:33:20,680
The thing is we are getting, yes.

525
00:33:20,680 --> 00:33:24,960
So after a patient has been diagnosed with a certain kind of cancer, then we get a sample.

526
00:33:24,960 --> 00:33:29,400
So if, you know, like you've been diagnosed with pancreatic cancer, then we get the sample

527
00:33:29,400 --> 00:33:30,400
from the pancreas.

528
00:33:30,400 --> 00:33:36,640
We also get a sample from the biofluids, which are like bloods and the lymph something.

529
00:33:36,640 --> 00:33:37,640
Yes.

530
00:33:37,640 --> 00:33:38,640
The biofluids.

531
00:33:38,640 --> 00:33:40,520
So that's the information that we get.

532
00:33:40,520 --> 00:33:45,360
And then yes, that's the one that gets segmented with the mass spec.

533
00:33:45,360 --> 00:33:46,360
Okay.

534
00:33:46,360 --> 00:33:51,360
And so you're, you're kind of in this data collection process now.

535
00:33:51,360 --> 00:33:53,560
Have you started?

536
00:33:53,560 --> 00:34:00,240
And you've also started kind of exploratory work with the data, but you're pretty kind

537
00:34:00,240 --> 00:34:02,280
of the modeling stage.

538
00:34:02,280 --> 00:34:03,280
Yes.

539
00:34:03,280 --> 00:34:04,280
Yes.

540
00:34:04,280 --> 00:34:06,720
Yes, we are definitely before that.

541
00:34:06,720 --> 00:34:11,160
So like I had mentioned that other study with using data.

542
00:34:11,160 --> 00:34:18,040
So we actually are using that data to sort of standardize the workflow.

543
00:34:18,040 --> 00:34:23,740
So all the way, like to see how things shift from, you know, the point of them getting

544
00:34:23,740 --> 00:34:29,440
segmented all the way until they get quantified and start to, you know, to understand that.

545
00:34:29,440 --> 00:34:33,920
And then that way we understand our equipment and we understand what we need to change.

546
00:34:33,920 --> 00:34:42,760
So there are multiple workflows in proteomics, multiple, they spy ex, they is, they is open

547
00:34:42,760 --> 00:34:49,360
swathes, you know, there's trans proteomic pipeline and and all of them really are like

548
00:34:49,360 --> 00:34:50,360
these pipelines.

549
00:34:50,360 --> 00:34:54,280
So, you know, it goes into mass spec and then it gets into another thing that's going to

550
00:34:54,280 --> 00:35:00,080
pick like a peak, like it's going to find all the areas that that are present and then

551
00:35:00,080 --> 00:35:04,680
it gets labeled and then it gets quantified and then, you know, all of this.

552
00:35:04,680 --> 00:35:09,640
And then like we're still at the peptide level and then we go to the good old string alignment.

553
00:35:09,640 --> 00:35:11,680
But what do they call in proteomics?

554
00:35:11,680 --> 00:35:17,040
We do alignment to try to figure out if you see this set of peptide, what's the probability

555
00:35:17,040 --> 00:35:20,840
that it's this protein and then it gets labeled.

556
00:35:20,840 --> 00:35:23,440
So that is always a pipeline.

557
00:35:23,440 --> 00:35:28,880
And so, you know, all these tools in the pipeline are pieces of code and sensors that come

558
00:35:28,880 --> 00:35:32,400
with their own variation and have to be standardized.

559
00:35:32,400 --> 00:35:34,320
So that's what we've been doing.

560
00:35:34,320 --> 00:35:39,880
Once we have that, once we have data that we think is good, that we think we can trust

561
00:35:39,880 --> 00:35:42,560
which will hopefully be at the end of this year.

562
00:35:42,560 --> 00:35:48,960
So the process of actual data analysis is going to be this year.

563
00:35:48,960 --> 00:35:56,240
And then, then we can start really running the biggest thing in this area is to do classification

564
00:35:56,240 --> 00:36:00,640
right, when an extracting those features that put one sample one side over the other.

565
00:36:00,640 --> 00:36:02,000
Because those are the biomarkers, right?

566
00:36:02,000 --> 00:36:07,200
Like they're going to, in this area, be like those features that are seeing your disease

567
00:36:07,200 --> 00:36:08,280
on one disease.

568
00:36:08,280 --> 00:36:12,920
So for us, we're looking at it as a classification problem and to put it on one side over the

569
00:36:12,920 --> 00:36:14,120
other.

570
00:36:14,120 --> 00:36:18,560
We are also very interested in actually just running a clustering problem, even on the

571
00:36:18,560 --> 00:36:19,560
data.

572
00:36:19,560 --> 00:36:24,720
So maybe this goes back to your question of, so it comes labeled to maybe even see if

573
00:36:24,720 --> 00:36:28,480
like, are these expressions, you know, all that different?

574
00:36:28,480 --> 00:36:33,120
Can we, when we cluster them, are there, you know, like some similarities, even between

575
00:36:33,120 --> 00:36:34,280
the disease and non-disease?

576
00:36:34,280 --> 00:36:38,160
So you just cluster the whole data set and hopefully we'll get the separation that these

577
00:36:38,160 --> 00:36:42,640
are the disease cells and they'll cluster together and these are the non-diseased, you know,

578
00:36:42,640 --> 00:36:45,280
and they'll cluster together, the data from both will cluster together.

579
00:36:45,280 --> 00:36:49,000
But you can do that, but that's a lot of the work that you do when you're just exploring

580
00:36:49,000 --> 00:36:50,000
your data.

581
00:36:50,000 --> 00:36:54,680
The same thing, which is, you know, like understanding the differences of the means, you go through

582
00:36:54,680 --> 00:36:59,000
and you just understand your data and then like, you know, like chopping up your data into

583
00:36:59,000 --> 00:37:04,280
blocks so that you're not observing some other phenomena, let's say, maybe you're just

584
00:37:04,280 --> 00:37:09,120
seeing this disease only in like old people or something like that so that you can mix

585
00:37:09,120 --> 00:37:15,120
like, you know, old people and young people and still try to detect these differences.

586
00:37:15,120 --> 00:37:22,760
It ends up being something a process that has a lot of data and I do think one other thing

587
00:37:22,760 --> 00:37:27,960
that makes the problem a little bit complicated is that actually in the whole process of running

588
00:37:27,960 --> 00:37:33,240
your mass spec and everything, though we might run to one to run as many samples as possible.

589
00:37:33,240 --> 00:37:36,400
Remember, I did say that's the sample degrades.

590
00:37:36,400 --> 00:37:41,480
So we always have like a timeline of how much we can take to even run experiments and

591
00:37:41,480 --> 00:37:45,160
that actually is one of the things that limits how much data we can collect.

592
00:37:45,160 --> 00:37:49,760
So ideally we'd collect as much as possible, but you know, we get limited by that.

593
00:37:49,760 --> 00:37:55,400
It's not just that we get too few samples, it's also that the samples we have can degrade

594
00:37:55,400 --> 00:37:59,400
and it can be difficult to rerun them later.

595
00:37:59,400 --> 00:38:06,120
Which actually is why sometimes this process of swath is good because all it does is just

596
00:38:06,120 --> 00:38:07,920
breaks down everything, all it wants.

597
00:38:07,920 --> 00:38:12,360
So without even necessarily needing to label it, you don't need to pick it just segments

598
00:38:12,360 --> 00:38:13,360
everything.

599
00:38:13,360 --> 00:38:18,200
So later on you can actually go back to the old samples and study them when you have

600
00:38:18,200 --> 00:38:23,080
more information of how to actually understand this data, at least it will exist.

601
00:38:23,080 --> 00:38:30,120
One question that occurs for me in thinking about the way you describe your workflow, even

602
00:38:30,120 --> 00:38:34,440
like at the super macro level, right?

603
00:38:34,440 --> 00:38:42,120
The first year is spent on data collection and kind of refining this data workflow and

604
00:38:42,120 --> 00:38:49,000
then you kind of transition to an analysis phase in the second year.

605
00:38:49,000 --> 00:38:54,960
But often these two are like the overall process of getting to a model is very iterative and

606
00:38:54,960 --> 00:39:00,000
you end up tweaking the way you collect data and the kinds of data you collect and building

607
00:39:00,000 --> 00:39:01,480
out features and things like that.

608
00:39:01,480 --> 00:39:07,480
And I'm wondering how, if that's something that you observe as well and how you fit that

609
00:39:07,480 --> 00:39:12,560
into the way you work there on this project?

610
00:39:12,560 --> 00:39:13,880
Yes, definitely.

611
00:39:13,880 --> 00:39:19,320
We definitely do run the problem of getting to some point and then just not having the

612
00:39:19,320 --> 00:39:26,320
results that we wish to have, but or just not having results that are reliable.

613
00:39:26,320 --> 00:39:32,440
But yeah, so it won't mean that we will stop collecting data on yet when year three, that

614
00:39:32,440 --> 00:39:34,800
data will keep coming.

615
00:39:34,800 --> 00:39:40,840
And so while we are running this analysis, we might find that actually we have to check

616
00:39:40,840 --> 00:39:48,520
away some of that data and have to inform the doctors on how to collect the data.

617
00:39:48,520 --> 00:39:58,040
The nice thing is that, like I said, this kind of study is not like we are breaking ground

618
00:39:58,040 --> 00:40:00,880
in the processes, it's not.

619
00:40:00,880 --> 00:40:05,960
We are using processes that have been developed and we're trying to make them better.

620
00:40:05,960 --> 00:40:10,240
There's a lot of work to still like for improvement in this area, like technically.

621
00:40:10,240 --> 00:40:16,440
So yes, we can contribute those kind of technical, you know, like contributions.

622
00:40:16,440 --> 00:40:20,960
But the processes, there are multiple studies, there are multiple universities that are

623
00:40:20,960 --> 00:40:24,560
doing the work and material mix and in cancer.

624
00:40:24,560 --> 00:40:30,680
So you know, we have an idea generally of how to prepare samples of how, you know, we

625
00:40:30,680 --> 00:40:33,480
have a lot of literature that we can lean on to.

626
00:40:33,480 --> 00:40:39,880
So hopefully, you know, fingers crossed, things will work out fine and generally they really

627
00:40:39,880 --> 00:40:40,880
should.

628
00:40:40,880 --> 00:40:41,880
Right.

629
00:40:41,880 --> 00:40:47,320
I don't know what could be so different because we expect the difference is going to be

630
00:40:47,320 --> 00:40:48,760
in the population.

631
00:40:48,760 --> 00:40:49,760
Right.

632
00:40:49,760 --> 00:40:53,600
Because in terms of collections and stuff like those that we are just, it's standardized

633
00:40:53,600 --> 00:40:54,600
methods.

634
00:40:54,600 --> 00:40:55,600
Yeah.

635
00:40:55,600 --> 00:40:56,600
Yeah.

636
00:40:56,600 --> 00:41:03,800
So ultimately, the goal here isn't to, you know, pioneer some new, you know, approach

637
00:41:03,800 --> 00:41:12,000
to collecting the data or modeling, it's rather to apply things that are fairly well understood,

638
00:41:12,000 --> 00:41:16,400
but to a population that has been underrepresented and understudied.

639
00:41:16,400 --> 00:41:17,400
Exactly.

640
00:41:17,400 --> 00:41:26,480
I'm thinking about the, you know, the conversations that I've had with folks in data science

641
00:41:26,480 --> 00:41:35,440
and researchers in, in Africa in particular, I'm forgetting the name of the gentleman

642
00:41:35,440 --> 00:41:46,640
who presented at the black and AI workshop in, from Kenya and, yeah, I think the common

643
00:41:46,640 --> 00:41:54,080
thread through, you know, the things that you presented is this idea of applying these,

644
00:41:54,080 --> 00:41:58,760
actually both of the projects, I think he was working on a similar project with regard

645
00:41:58,760 --> 00:42:06,920
to, to natural resources, as well as kind of applying some of these methods to, to kind

646
00:42:06,920 --> 00:42:09,440
of understudied populations.

647
00:42:09,440 --> 00:42:15,400
And I don't know, I, I guess the question that I'm kind of struggling with is, is, you

648
00:42:15,400 --> 00:42:20,440
know, is a little bit of, is that the fact that you're kind of both working on similar

649
00:42:20,440 --> 00:42:27,440
things is that, to what degree is that kind of representative of the unique, you know,

650
00:42:27,440 --> 00:42:35,360
challenges that are kind of expressing themselves in African countries, you know, versus maybe

651
00:42:35,360 --> 00:42:39,920
a selection bias, that's what the organizers of black and AI thought would be interesting.

652
00:42:39,920 --> 00:42:40,920
Yeah.

653
00:42:40,920 --> 00:42:46,920
So, you know, but I think I actually had a little bit of a difference with the, with a difference

654
00:42:46,920 --> 00:42:52,160
of understanding with the work that I think Shira actually presented.

655
00:42:52,160 --> 00:42:57,720
I, I thought there was a lot of innovation in that, because it's, it's understanding

656
00:42:57,720 --> 00:43:04,000
the resources that you have and having to make these processes work for you.

657
00:43:04,000 --> 00:43:08,600
So, I have to say, and this is actually something I learned about when I moved here, there

658
00:43:08,600 --> 00:43:12,480
is a certain way that you work with these problems when, you know, you are in the US

659
00:43:12,480 --> 00:43:15,960
and you have the resources, you've got all the computing resources and you've got, you

660
00:43:15,960 --> 00:43:21,480
know, other resources, and then you move here and then you don't have as many resources

661
00:43:21,480 --> 00:43:26,160
and the level actually that the innovation comes at that of trying to still do the same

662
00:43:26,160 --> 00:43:34,680
quality of work, but, but at fairly low resources, because the work still has to be done.

663
00:43:34,680 --> 00:43:41,120
So I, I actually really think that is, so we have two areas of opportunity to contribute

664
00:43:41,120 --> 00:43:46,560
I think to, to, you know, the scientific dialogue in the world. Number one is how do you do

665
00:43:46,560 --> 00:43:51,560
science when you don't have as many resources, but how do you do good, usable, useful

666
00:43:51,560 --> 00:43:52,560
science?

667
00:43:52,560 --> 00:43:55,400
And I think for us, that's the most important thing. We don't have the luxury of, you

668
00:43:55,400 --> 00:44:00,160
know, dwelling on the smallest of problems, like, I think it's going to sound bad.

669
00:44:00,160 --> 00:44:06,840
I, my, I, please don't give me on Twitter, but, you know, for goodness, but, you know, like,

670
00:44:06,840 --> 00:44:11,760
if I'm going to be asking for a grant, like, the, the level I am going to get questioned

671
00:44:11,760 --> 00:44:17,120
at, you know, like, it's not just like there's all of this just a line there. So you have

672
00:44:17,120 --> 00:44:21,360
to learn how to work with, with, with those things. And I think that's where innovation

673
00:44:21,360 --> 00:44:26,400
comes from, because in fact, then that sort of thing can be pushed back into the developed

674
00:44:26,400 --> 00:44:30,160
world and say, no, you can actually do it with less resource. And so, you know, there

675
00:44:30,160 --> 00:44:33,760
isn't, you know, that can actually help. And there's been instances where this sort of

676
00:44:33,760 --> 00:44:37,840
thing has happened. And processes have gotten removed because of that. Because, you know,

677
00:44:37,840 --> 00:44:42,040
as they say, what is it called deprivation is the mother of invention. I'm sure I've

678
00:44:42,040 --> 00:44:51,440
just made that up, but I think this is a specific example of interviewing somebody who's

679
00:44:51,440 --> 00:44:58,040
English is a second language, right? We make things like that. But anyway, so, so that's

680
00:44:58,040 --> 00:45:04,720
the first thing. But the second thing is that we have very interesting problems. Like,

681
00:45:04,720 --> 00:45:11,480
we have problems that actually you can make an impact on. Like, just study in this data,

682
00:45:11,480 --> 00:45:17,000
however much we go through, and, you know, can make a difference. And that's the luxury

683
00:45:17,000 --> 00:45:21,600
that we have. That's the thing that really motivates us. So I think, I mean, there's a lot

684
00:45:21,600 --> 00:45:28,400
of basic research that gets done here too. I mean, Amazon has a very, very good lab in

685
00:45:28,400 --> 00:45:33,200
Cape Town. So that's where that's coming out of here. There's a lot of other labs actually

686
00:45:33,200 --> 00:45:37,360
that are running around. There's research institutes. Even here at the CSI, there's a lot

687
00:45:37,360 --> 00:45:44,360
of basic, basic work that gets done. But like I said, I like applied work. So I can't

688
00:45:44,360 --> 00:45:49,680
really say that generally captures the type of work that gets done here. But I do think

689
00:45:49,680 --> 00:45:55,560
we do more of that kind of work here. And I think you'll see maybe that same thing happening

690
00:45:55,560 --> 00:46:00,880
with maybe Latin America also, some of the, you know, the results you see coming out.

691
00:46:00,880 --> 00:46:07,920
And I think it's because we stand to truly make an impact. And it's really attractive

692
00:46:07,920 --> 00:46:13,520
to work in an area where you feel you can actually make an impact to a process to people's

693
00:46:13,520 --> 00:46:25,440
lives. Right. Outstanding. So that's that. That's my walk around. Yeah. And so what are

694
00:46:25,440 --> 00:46:30,960
some of the other things I came across a couple of other things that you've been involved

695
00:46:30,960 --> 00:46:38,000
in? I know last year there was the deep learning in Daba. Were you involved in that?

696
00:46:38,000 --> 00:46:44,960
Yes. Yes. Yes. I was, I was one of the organizers of the deep learning in Daba. I still

697
00:46:44,960 --> 00:46:50,680
am. Okay. So for folks that are familiar with that, what was that? I was, I was cheering

698
00:46:50,680 --> 00:46:55,240
you on from afar. Like, you know, I saw the tweets all the time and I was like, Oh, man,

699
00:46:55,240 --> 00:46:59,400
I need to find a way to go to that and just never was able to make it happen. But it

700
00:46:59,400 --> 00:47:07,840
looked like an awesome event. It was so great. So what happened is just a team of,

701
00:47:07,840 --> 00:47:11,920
our friends, I think actually, these are people that already knew each other from before.

702
00:47:11,920 --> 00:47:20,000
I mean, but overall, we all kind of also met or over Skype and, you know, but we, the

703
00:47:20,000 --> 00:47:25,920
whole purpose of the in Daba is to strengthen machine learning research in Africa. So one of

704
00:47:25,920 --> 00:47:31,120
the big problem actually, it's not the problem of even computing resource. Sometimes the problem

705
00:47:31,120 --> 00:47:36,960
of human resource, you know, like building a team that's big enough to truly, you know,

706
00:47:36,960 --> 00:47:44,400
build that momentum. And so we wanted to find out who else is here doing this work. It can

707
00:47:44,400 --> 00:47:49,520
be really difficult to find out who because we are so sparse, you know, like placed all over.

708
00:47:49,520 --> 00:47:53,200
Find out who's doing it. And for people that are interested, is there where we can

709
00:47:53,200 --> 00:47:58,320
capacity change them? So it was, it was a summer school. I urged people to go check out our

710
00:47:58,320 --> 00:48:05,680
videos on on YouTube. And we managed to get a, you know, a lot of people that actually

711
00:48:05,680 --> 00:48:11,360
this time around have the main planning people were people that have their roots in Africa.

712
00:48:11,360 --> 00:48:18,880
These are Africans. So we managed to all come together. And some of them were in deep

713
00:48:18,880 --> 00:48:25,360
mind and some somewhere in the US and a lot were here in South Africa. And the next year,

714
00:48:25,360 --> 00:48:30,560
so 2018 in Daba is still going to happen in September. And we're looking even for even more

715
00:48:30,560 --> 00:48:36,400
Africans that are practicing in this area because we don't want to have this idea that then no

716
00:48:36,400 --> 00:48:42,640
Africans are doing this work because then it's too wide. Looks unreachable. It looks foreign.

717
00:48:42,640 --> 00:48:48,880
And it's not foreign. It's a technique that can be applied to our problems in our world. So

718
00:48:49,440 --> 00:48:55,120
we have been trying to find more Africans or I'm also trying to find experts, not just, you know,

719
00:48:55,120 --> 00:49:02,800
Afro-African descent, but other experts to come and teach and you know, just be around here.

720
00:49:02,800 --> 00:49:08,800
And it was a fantastic, fantastic event. We had Nando, we had Anima,

721
00:49:10,000 --> 00:49:16,880
who just came, we had George from Brown. We had, and everybody was so, it was a really,

722
00:49:16,880 --> 00:49:24,480
really good contagious. And I can tell you already the type of research and the kind of relationships

723
00:49:24,480 --> 00:49:30,480
collaborations we've formed just from that week. Like it really energized our work.

724
00:49:31,200 --> 00:49:37,040
Oh, that's great. Because even with the guys at Black and AI, I got to meet them because they

725
00:49:37,040 --> 00:49:44,240
saw that we're doing this in Daba. And I really have never worked with it like nicer planning

726
00:49:44,240 --> 00:49:50,080
committee. We generally, I don't know if they'll be mad at me for this, but we fight like crazy

727
00:49:50,080 --> 00:49:58,240
by the nice thing is we are all interested in the same thing. And we work like crazy too. So

728
00:49:58,240 --> 00:50:04,000
anyone that wants to work with us wants to help us see this thing come true, please. Like we are

729
00:50:04,000 --> 00:50:09,280
always open. And that's what we want to do. We just want to strengthen African machine learning.

730
00:50:09,280 --> 00:50:15,600
We have very interesting problems in Africa because the dynamics are different. So if you get

731
00:50:15,600 --> 00:50:22,560
a model in US, it may not work here just because the dynamics are different. So we bring in a whole

732
00:50:22,560 --> 00:50:28,240
new way of turning around some of these theories to get to understand how they may work in our

733
00:50:28,240 --> 00:50:33,520
environments. And we can't do that if we don't have truly fundamental knowledge of how these

734
00:50:33,520 --> 00:50:38,800
things work. We can't expand and stretch them to fit our problems if we don't fully understand

735
00:50:38,800 --> 00:50:44,240
how far they can stretch and expand. So that's why we want to embed like deep, deep, you know,

736
00:50:44,240 --> 00:50:50,720
theoretical knowledge. So people we can really grasp these concepts. And then I think then we

737
00:50:50,720 --> 00:50:57,760
can really do something for ourselves. Well, you're passion for all of this is very palpable,

738
00:50:57,760 --> 00:51:04,640
very tangible. And I appreciate you taking the time to chat with me about it. This is really

739
00:51:04,640 --> 00:51:14,320
fun. Yeah, anything else that you'd like to mention before we close out? I think maybe I'll actually

740
00:51:14,320 --> 00:51:21,120
go back to the whole thing of collaboration partners. If anyone is interested in any of the things

741
00:51:21,120 --> 00:51:25,680
we said or has been interested in a problem that maybe they observed well, they took, you know,

742
00:51:25,680 --> 00:51:34,320
a trip down here to South Africa or anywhere else in Africa. We like collaborators because,

743
00:51:35,120 --> 00:51:41,360
you know, we don't necessarily, it need not be that we are isolated, you know, we get isolated.

744
00:51:41,360 --> 00:51:46,000
So if there are people that are generally interested in working with some of these projects,

745
00:51:46,000 --> 00:51:51,680
one thing I can really tell them is they are rewarding. The findings can be quite rewarding.

746
00:51:51,680 --> 00:51:56,560
And that we're all looking for collaborators because we don't also want to get myopic and how

747
00:51:56,560 --> 00:52:02,320
we understand our own problems. Perhaps looking at them from the outside can be a way to solve them.

748
00:52:02,320 --> 00:52:08,640
So we're open to discussions. Great. And is there any particular best way for folks to connect

749
00:52:08,640 --> 00:52:16,320
with you? Sure. They can get in touch with me. I am on Twitter. Oh, I don't know if I should disclose

750
00:52:16,320 --> 00:52:28,000
who I am on Twitter, given the things I said. I want to tell my, my handle is Ed Nunewska.

751
00:52:28,000 --> 00:52:35,440
That's the name my father gave me and it's stuck. Ed Nunewska. So, Ed Nunewska. So that's that.

752
00:52:35,440 --> 00:52:42,160
And then of course, my email is and more CSI R.C.O.D. Zere. And maybe you can just link that

753
00:52:42,160 --> 00:52:46,560
to the link to the web link. We'll link all of that together in the show notes.

754
00:52:47,600 --> 00:52:50,800
Okay. Great. Yeah. No. If you're just getting in touch with me,

755
00:52:50,800 --> 00:52:55,440
if you want to get in touch with the deep learning Indava to please just link our link over there.

756
00:52:55,440 --> 00:53:01,040
It's deeplearningindava.com. If you want to get in touch with our combatee, please check us out.

757
00:53:02,080 --> 00:53:06,800
Check, you know, maybe there's somebody they've always wanted to work with. Maybe you just like us.

758
00:53:06,800 --> 00:53:13,280
But anyway, yes. I think collaboration is good. Yeah. Great. Well, Nyalin, thank you so much.

759
00:53:14,480 --> 00:53:23,520
And well, yeah, just thank you so much. Thank you so much. It's been interesting.

760
00:53:24,080 --> 00:53:28,960
And please, you know, everybody, I'm always open for discussion. If I said something,

761
00:53:28,960 --> 00:53:34,320
that was not correct. Please, you know, guide me in the right direction. That's what I said,

762
00:53:34,320 --> 00:53:40,000
collaboration, conversation and exploration. That's it. Awesome. Awesome. Thank you.

763
00:53:43,040 --> 00:53:49,120
All right, everyone. That's our show for today. Remember, we want to hear your thoughts on personal

764
00:53:49,120 --> 00:53:57,680
AI head on over to twimlai.com slash my AI to share. For more information on Nyalin or any of the

765
00:53:57,680 --> 00:54:04,000
topics covered in this episode or to share your feedback, head on over to twimlai.com slash talk

766
00:54:04,000 --> 00:54:11,840
slash 109. Thanks so much for listening and catch you next time.

