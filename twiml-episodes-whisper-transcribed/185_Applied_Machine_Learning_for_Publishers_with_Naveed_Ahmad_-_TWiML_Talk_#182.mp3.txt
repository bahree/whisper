Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington.
In today's show, we're joined by Navid Ahmad, Senior Director of Data Engineering and
Machine Learning at Hearst Newspapers.
A few months ago, Navid gave a talk at the Google Cloud Next Conference on how publishers
can take advantage of machine learning.
In this conversation, we dive into the role of ML at Hearst, including their motivations
for implementing it and some of their early projects, the challenges of data acquisition
within a large organization, and the benefits they enjoy from using Google's BigQuery as
their data warehouse.
Enjoy the show.
All right, everyone.
I am on the line with Navid Ahmad.
Navid is Senior Director of Data Engineering and Machine Learning at Hearst.
Navid, welcome to this weekend machine learning and AI.
Thank you.
Thank you for having me on your show.
So you've been in the publishing industry for about 10 years now.
Can you tell us a little bit about both your current role and some of the past things
you've done in publishing?
Sure.
So I'm currently a Senior Director for Data Engineering and Machine Learning at Hearst,
being here for about two years.
My rule has been building the data warehouse, building personalization, and also doing
predictive analysis using that data.
And before Hearst, I was at New York Times.
I worked in the subscription space where I'm working on the CRM system, and also some
of the aspects of machine learning, like churn modeling and email content detection.
And before that, I was in Thomson Reuters, where I also built their data and used a distribution
platform, as well as part of their CMS team contributed to recommendation systems.
And before that, I was mostly in telecommunication.
And so you recently presented at the Google Cloud Next Conference on Applied Machine Learning
for Publishers.
Before we jump into that topic, maybe you should take a second to provide an overview of Hearst
for those who aren't familiar with the company.
First.
So Hearst, I especially work in Hearst newspaper department.
Hearst is a very large organization with more than 300 businesses, which includes magazines,
investment, and television channel, and within even newspapers, there's about 40 plus websites,
which includes names like San Francisco Chronicle and Houston Chronicle and Times Union.
It is, and Hearst had quarters located here at 57th Street in New York.
And that's some background about Hearst.
So one of the things that you mentioned in describing your background and in our conversation
before the interview started was the role of the data warehouse and enabling you to
perform the types of machine learning that you want to be able to perform at Hearst.
Can you talk a little bit about the data warehouse and the process for establishing it?
Yeah.
So one of the first tasks for me when I joined Hearst was to build a data warehouse, especially
I used Google BigQuery as a tutorial data warehouse.
And the idea was that we need all the data sets in one place to be able to look at what
is the relationship between newsletter and web and subscriptions.
And the first use case was to build business and tell it on like regular reporting on
top of this data.
And the same data can be used to, that's used to give data reports about current status
can be used to do predictive modeling.
So using BigQuery and basically we build our data sets include Google Analytics, be our
content, our newsletters, so all sorts of data related to our business is in BigQuery.
And this forms as a foundation for machine learning.
So the data warehouse is kind of central to your ability to perform machine learning and
analytics and it was one of the first things that you established at Hearst.
Can you talk a little bit about any challenges that you experienced trying to centralize all
of the data from these various sources?
The challenges were that our data was sitting in different formats like before the warehouse,
would get either data from individual systems or there's Excel worksheets going around.
So really figure out the data sources and then building a platform for ETL, that was
great challenge.
Some of the bigger data sources like Google Analytics and DFP, they were easier to get into
BigQuery but some of the ones we have to build specific code to ETL that into BigQuery.
And what is DFP?
Double click for publishers.
Okay, so that's an advertising, your advertising system?
Yeah, this is all the log data for each advertisement that impressions.
And so the data warehouse has your analytics.
So the click stream data for people that are visiting the site, it's got information about
the advertising, interactions and maybe those click streams.
Does it also contain content information?
Yeah, so all our CMS content is in BigQuery.
All our newsletter, our bird data, that's in BigQuery.
And is that data replicated to BigQuery for
analytics or is that it's native place and you're publishing directly to and from BigQuery?
Yeah, it is replicated.
Like the other, we have a separate CMS system, it has our own database.
So we're replicating that BigQuery, you know.
And these are very different types of data.
What's the goal for pulling this all into a single place?
Yeah, so each of these data, they have identifiers that link them up with other datasets.
For example, a newsletter with their hashed email address.
You can link it up with subscription data.
And from subscription data, we can link it up to Google Analytics data.
So the idea is to be able to connect all the aspects of a user
using these identifiers.
It sounds like the primary focus from an analytics, a predictive analytics,
and a machine learning perspective is based on this link data, this user-centric link data.
Yes, absolutely.
And so maybe let's talk through some of the different things that you've done with the data
from a machine learning perspective. What are the different challenges that you're trying to address?
So we built a few different machine learning-based products and predictive models
using this data we have in BigQuery.
So number one, the one we are currently is a churn model, churn prediction.
The churn prediction is that be able to predict how likely a subscriber is to cancel their subscription.
And it's very relevant in the media space because it's a bit a challenging environment to keep
subscribers. Since we have subscription data, how people, the different attributes of their usage,
behavior, how many newsletters they signed up to, also customer service data associated with
each subscriber. And the way it works is we take like a year-old worth of data and take their
attributes of month and feed it into a machine learning model. And we know that in the next month,
how many of those subscribers were canceled and versus sudden cancel. So this is a binary
classification problem. And using six months of data, we have training data of subscribers who
didn't cancel versus cancel and we can build a predictive model on it. So one of the things
that we've done is since we're on BigQuery, we've used this new, the launch feature called BigQuery
ML, which was released in Google next back in August. And the great thing about BigQuery ML is that I
can do a training and prediction model right using SQL syntax, i.e., you don't have to write code
or Python code. And it does some of the like machine learning goodies, like normalization of
features and also fine-tuning the model. And one of the things that I emphasize that this enables
people who don't even have machine learning background, like people in business intelligence or
who have background in SQL can easily write machine learning code just by using SQL syntax.
So we built out our first model on San Francisco. And we're just working on
integrating with our marketing and CRM systems. And so this was one of the use cases
for machine learning. Then we've also built one of the things that I've done over here's use.
If I could jump in before you go to the next one, one of the things that really struck me
at the Google next conference and not to turn this into a BigQuery commercial, but I was really
surprised by the enthusiasm for BigQuery. People seem to really love that database. Can you
like maybe net out for me why folks are excited about it and the BigQuery ML piece that they just
announced? Yeah. So in general, BigQuery is a fully managed data warehouse. So you don't have to
have a DBA to fine-tune or optimize the database. And it's based on Google's own internal
Dremel technology. What I've heard is heavily Dremel technologies is heavily used inside of Google.
And so they expose that as BigQuery. And their philosophy is it's full scan. Any data set
basically spawns off machines or compute in the back end and is able to very quickly get your
results. Especially if you have large data sets and terabytes or petabyte scale, it only takes a
few minutes to run SQL. So it's very easy in terms of maintenance. It's based on SQL syntax.
That's why it's very an attractive option for data mining. And BigQuery ML, so they built out
machine learning on top of BigQuery, which even further aids very fast because it's using the same
compute infrastructure. And it abstracts out the machine learning as an SQL SQL.
So it becomes, I foresee, that it'll enable machine learning for a lot of people, especially
who are on BigQuery. And so it's the idea then that you'll have like, you know, where you might
have aggregators in SQL like, you know, average or max or something like that, you can apply
some kind of model to or is it different? Yeah, it's absolutely. So just like these functions,
you mentioned they've introduced a few different functions to be able to train and predict and even
get machine learning and metrics like once the machine learning model is built to get like,
for example, what's the AUC of this machine learning or precision and recall. So you can do all
of those things just by a function called with a SQL syntax. Yeah, as compared to using a tool like
scikit-learn or TensorFlow, the other advantage is you don't have to take the data outside of the
system. You do everything in one place. Like if you were using scikit-learn or TensorFlow, there's
this whole process of extracting the data, massaging into a format that machine learning framework
can understand, build a machine learning model, pull that data back into your data warehouse.
So this whole cycle just gets reduced because you're doing everything in line and BigQuery.
You were about to go into another use case beyond the well, actually back on the on the churn.
Yeah, I'm curious. This isn't necessarily a machine learning question. You know, I've talked
about churn prediction in many cases across many different industries and I understand broadly
how it's applicable. But I'm curious in the case of a publisher, you know, what specifically
is Hurst going to do on the business side once it's able to predict that a user has a
high likelihood to churn? Yeah, that's a good question. And we're working with marketing and people
in the subscription business. What are the different ways? So some of the ways is that we,
in our marketing platform, we were able to give messaging and be able to send emails.
Typically a user who is not engaged a lot are some of the people. Those are some of the attributes
that are indicative that a person is going to cancel their subscription.
So the email to nudge somebody, hey, you know, this, did you know that there's a certain feature
or educating our print subscribers that they have free digital benefits are some of the things
that they can, we can employ on getting people to less or cancel their subscription.
You built this on BigQuery ML using this new SQL-like interface. Can you talk about how
that experience of building these models using a SQL type of interface different from
traditional approaches? You mentioned Scikit-learn, like how were they different? And I guess
I'm particularly curious about the different skill level involved, but also any differences in
the way you need to manage the models or the way you productize them, those kinds of things.
Yeah, that's a good question. So one thing is getting started with BQML is really fast.
I actually built the first prototype within a day. I got a very basic churn model up and running.
I've done this before in my previous job using Scikit-learn and the difficulty really was
getting aggregating data from different sources. Like in my previous job, getting data from
like an Oracle database and, you know, a Hadoop database. So seemed like that took most of the time
getting data from different data sources. Typically, the machine learning technology piece is the
easier part. And the more time it's spent on getting the data and then figuring out the right
features to use in our machine learning model. And so using BigQuery ML, getting used to the
SQL syntax is, again, anybody with no SQL can be trained to do it within, I think, half a day.
I think the tricky part is, like, for someone new is to learn some of the machine learning concepts.
For example, if you train a machine learning model, how do you measure that it's, you've succeeded,
like basic concept like precision recall and accuracy and AUC area and the curve. So those are more
where more time is spent, like figuring out the features and then doing measurement of the result.
And I feel like that doesn't differ from versus using Scikit-learn or BigQuery ML. The feature
engineering and the measurement. But getting used to BigQuery ML is very quick. You don't have
to use any language. And then the cycle I was talking about that data export, train model, data
import, that cycle gets really reduced. So the number of experiments that you can do using BigQuery
ML, you can do a lot more experiments with this technology.
And is part of that because presumably they're scaling out the training behind the scenes and
it's just a lot faster or are there other aspects to reducing that cycle time?
Yeah, A is that it's a lot faster because of the BigQuery technology. And second is the
what I mentioned before exporting data for machine learning model outside of the system,
training it and fetching that data back in. So that gets significantly reduced.
And the third one is in our case, since our data was all sitting in one place,
we didn't have to incur this ETL cost of getting data from different sources. Everything was
sitting in BigQuery. What are some of the other types of machine learning projects that you've
done at Hearst? Yeah. So another one was application of natural language processing.
There's a case study published back in November. I think if you just Google Hearst and Google
that probably the first link. So we've applied Google's natural language processing to all our content.
So each hour tag basically we're using two features of natural language. One is classification of
content. Putting this into broad categories like if content is about food or wine or is it about
real estate? And then also a more detailed version of it that it tags entities and the content.
And the entities could be proper nouns or common nouns with their metadata,
with their how much salient they are to that article and also their Wikipedia link for their
popular entities. So we did that for all our content from all our websites and that stored in
our CMS as well as also replicated into BigQuery for their analysis. So some of the use cases
on this NLP data is so we built out BI business intelligent reports. For example, if you want to
see how is for example a particular personality trending over time. So using Google NLP data and
Google Analytics data in our content data in BigQuery we can build out reports. And also if you want to
see which content categories get more traffic versus the content that we publish. Like which
categories should we be focusing on for publishers? So we build all sorts of reports using these
three data sets. So this is one of the use cases for Google NLP. The other one is which is more
interesting is that we also pass like when we render our ads on thing we've integrated that with
double click for publishers. So whenever an ad is rendered we also pass the key value pair
of which category that ad belongs to. The category of the content that ads being displayed.
So we built over a month this builds a database where we can say show me all the ads that are
displayed on for example Olympics content or ads on our food contents. So if there's a new advertiser
that comes on board and he says that I want to run a campaign on let's say basketball or
Olympics content. We build this capability using our tag tagging technology. So they can just
specify a criteria and double click for publisher said specify for this campaign advertise this
campaign on all the content which is related to let's say tennis or basketball. So that's another
use case for natural language processing. I know you've done some work both with Google's
AutoML NLP that allows you to kind of fine tune some of their models but the classification was
there off the shelf NLP service that you that one if I'm not mistaken you're not able to train
that using your own data is that right? Yeah it's a good question. So the AutoML for NLP which is
another tool released recently it works on top of the classification. So they give a
a taxonomy of about 700 categories by default but let's say if somebody wants to train
their own category like for some unique or novel content they can use AutoML for
a natural language to train their own custom categories. So and that works on top of their
they existing categories. So if you make a web service call to this AutoML it not only returns
their default categories but also the trained version as well. And so you found that for the
types of articles that you were initially trying to categorize the were these 700 built-in
categories were they sufficient were they was it you know what was the experience of trying to
map your business to this pre-canned AI service? So most of most of the time it works very well.
So some of the categories like the like for most of our use cases those categories were fine.
There's a few categories which don't work well especially their sensitive content so they
don't really split out. So any article about crime or you know some violence everything gets
categorized into sensitive content. We would have ideally liked that to be split down into more
granular but Google thinks that they're like some restrictions they just don't want to split that
out. But one of the categories that we wanted that I did a POC was to detect evergreen content
because we had labeled about several thousand articles with evergreen label. So whatever green
means is content that has a life shelf of more than a few days or a few weeks. For example an article
which is a review of a museum or an article about some real estate. So those articles have a longer
shelf life and it's very important in the newspaper if you can detect that type of content because
that kind of content can be used in recommendations. Even an article written two years ago can be
reused which otherwise would just be sitting there and nobody reads that content. So I built using
AutoML for text. I built a classifier which basically detects evergreen versus non-evergreen content
and our editorial they helped us label this content. We asked all the different markets
to take their content and label them with evergreen content. Initially I did a very quick prototype
using TensorFlow and then I had a hunch that their Google is working on this AutoML feature for
NLP and then once this feature came out doing a POC was really quick. Within a day I was able to train
an AutoML model that can differentiate evergreen content. So really the hard part in this is to
get the labeled data. And the interesting thing is once this model is trained I even try to
on CNN and New York Times content it was able to differentiate evergreen and non-evergreen content.
And right now we're incorporating that into one of our recommendation systems.
Yeah one of the things that I noticed a couple of times in our conversation you're the senior director
of data engineering in ML. But it sounds like on a couple of these POCs you just kind of
played around and put some stuff together you make it sound really easy. It can be easy to
experiment but then if you actually want to use this in a way that the business is going to depend
on it there's at least traditional a lot more that needs to go into it in terms of engineering.
I guess there's maybe several questions in here but you know part of it is like does the cloud
change that dynamic in your perspective or you know are these projects that you built and then
kind of threw them over the wall to you know some team that then had to maintain these projects.
Are you a question specifically about auto ML or just about recommendations and
the other stuff I've been talking about? Specifically the I think with term prediction and the auto
ML the impression I got was that it almost sounded like you got bored one weekend and kind of
work on these and kind of you know you came up with these models in a you know a day or so.
Yeah so that they weren't done in a day they were sort of like the turn model we started
when BQMO was an alpha state only available for certain customers but I think it was about
May we start building it so and it took a couple of months like refining and fine tuning it
and so and we're now we're working on another market building out doing this prediction for Houston.
Auto ML it was in phase that we started off like last year we said we need to collect the data
since we want to be able to do saying so one mini project was to figure out how to get this data
and get it labeled and and then once we had it label it was just sitting there for a while till
I did some prototyping work for we're using TensorFlow and then when auto ML came out
it was since we had it did already had done the hard work on labeling that data it was
very something very quick to do so I guess one piece of the project wasn't done like in in a weekend
there were different phases and happen in different times and the other thing is in general the
the newer features in cloud especially the the higher level API that's much more quicker to do
prototyping and getting things out quicker and auto ML is a feature that's meant for
people to do things much quicker like you don't have to learn TensorFlow to be able to do it it's
just upload the data give it label data set and then it takes a few hours to train a model and start
using it you've done some work on recommendation systems as well yeah so as I was talking so one of
the use cases with the NLP data that's sitting in our BigQuery where data warehouses to build a
recommendation system so the the idea is that if two piece of content if they have overlapping
NLP entities they're highly likely to be related to each other so this is content to content
recommendation and since we had this again since we had this in our BigQuery database
we built out a recommendation system which actually works using a big SQL which takes entities
from one set of articles and matches that with another one and corporate saliency score and
so there's there's a whole bunch of rules that we built out in our SQL and produces a table of
recommendations and that's fronted by a web service layer that our website San Francisco Chronicle
is I think three websites that are using that content content recommendation and this was a
release as part of San Francisco's user interface rewrite so a few months ago we released a revamp
of SF Chronicle so this was project this content to content recommendation was part of
that deployment so you've got this content in the in BigQuery and then you're using
Google NLP to essentially you're adding fields for each of the entities that are recognized and
then you're using BigQuery ML to generate a recommendation for from a given piece of content to
other recommended pieces of content based on these shared entities that have been recognized by
the NLP service yeah so and just a minor correction that this one doesn't use BQML this is a plain
SQL running which basically is looking at the number of overlaps of entities between two contents
got it got it so it's basically scoring of how like based on the overlap how
relevant one content is to another one okay and then you said you're frontending that with
a website are you serving the data directly up from BigQuery or do you push it out to some
cache or database yeah so yeah so we'd push it out this computation is done on a periodic
basis about every 15 minutes and that's about the frequency that we're getting your content so we
build out a table in a postgres database and then that web service layer is using a memcache as
well as this postgres database to serve these out and you've also done some video content recommendations
yeah so it's so another project we've done is we've taken our video converted that to sound
and then extracted text from that sound and then applied Google's NLP to extract entities from
the sound so so our content has the NLP tags and our video using voice transcription
and using that text to also extract entities we can recommend video to content based on this
technology and so to transcribe the audio did you use the the Google speech to text API for that
yeah yeah so Google has a Google sound API which given any sound it can convert that to text
and what was your experience getting reliable results from that yeah so our like full objective
was to also have captioning on the videos so we did most of the time it works very well the
sound transcription but in certain cases it doesn't like it if like it there was some few
cases when it didn't work very well so we haven't used this for captioning but we took this text
and it seems to work well if we apply NLP on it if we just want to extract categories and tags
from it for that purpose it works well and those tags are used for it eventually being able to
recommend a contextual content along with the videos any final thoughts or words of wisdom that
you shared as you were wrapping up your presentation on these use cases yeah so some of the advice I
gave was that like some of the things that we want to do in the future is to use applied deep learning
for recommendations there's a lot of research done in the past couple of years for application
deep learning and recommendation we're actually doing some research and we want to launch very soon
with a recommendation system that applies user deep learning so I'd recommend that we should use
like everybody should look into deep learning and TensorFlow so it's really everything you don't
have to build from scratch if a problem is already solved by higher level APIs such as the NLP
image video APIs or BQML and AutoML we should be using that it's really a trade-off do you want to
hire data scientists to rebuild this thing or do you want to just pay that for that service and apply
it and only in cases where the problem is novel or the data set is novel is when you want to build your
in-house machine learning model for example we want to be able to do recommendations using deep
learning so this is something that we're building in-house and other things that we're exploring
as more use cases for NLP we identify that we can use this with our newsletters to be able to
generate newsletters automatically that's something that we're looking at too and also more
uses for BigQuery ML just like churn we can do the converses do propensity modeling how life
be heard what's the likelihood of a user to subscribe to our whenever websites so this is
overall our future plans awesome well Naveed thanks so much for taking the time to share what you've
been up to with us it's been really interesting and I appreciate it okay thank you very much
all right everyone that's our show for today for more information on Naveed or any of the topics
covered in this episode visit twimmelai.com slash talks slash 182 if you're a fan of the podcast
we'd like to encourage you to visit your Apple or Google podcast app and leave us a five-star
rating and review your reviews help inspire us to create more and better content and they help new
listeners find the show as always thanks so much for listening and catch you next time
