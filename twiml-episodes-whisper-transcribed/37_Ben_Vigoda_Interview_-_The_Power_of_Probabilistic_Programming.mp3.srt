1
00:00:00,000 --> 00:00:16,000
Hello and welcome to another episode of Twimultoth, the podcast why I interview interesting people

2
00:00:16,000 --> 00:00:20,960
doing interesting things in machine learning and artificial intelligence. I'm your host,

3
00:00:20,960 --> 00:00:26,880
Sam Charington. The show you're about to hear is part three of our five part O'Reilly AI New York

4
00:00:26,880 --> 00:00:32,640
series sponsored by Intel Nirvana. I'm very grateful to them for helping make this series possible

5
00:00:32,640 --> 00:00:36,880
and I'm excited about the cool stuff they launched at the O'Reilly AI conference including

6
00:00:36,880 --> 00:00:43,120
version 2.0 of their neon framework and their new Nirvana graph project. Be sure to check them out

7
00:00:43,120 --> 00:00:49,280
at intelnervana.com. And if you haven't already listened to the first show in this series where I

8
00:00:49,280 --> 00:00:56,000
interview Navine Rao who leads Intel's newly formed AI products group and Hanlon Tang and algorithms

9
00:00:56,000 --> 00:01:00,960
engineer on that team, it's Twimultoth number 31 and you definitely want to start there.

10
00:01:01,840 --> 00:01:08,800
My guess for this episode is Ben Vigota. Ben is the founder and CEO of Gamalon, a DARPA funded

11
00:01:08,800 --> 00:01:15,520
startup working on Bayesian program synthesis. We dive into what exactly this means and how it enables

12
00:01:15,520 --> 00:01:21,760
what Ben calls idea learning in this show. Note that Ben and I go pretty deep in this discussion,

13
00:01:21,760 --> 00:01:26,160
so I'm issuing the Nerd Alert. All right everyone onto the show.

14
00:01:33,440 --> 00:01:39,760
Hey everyone, I am here with Ben Vigota of Gamalon and I've been meaning to catch up with Ben

15
00:01:39,760 --> 00:01:45,920
for a long time and I was just telling him before we got started about how we do these Nerd Alerts

16
00:01:45,920 --> 00:01:52,400
now and he's like, oh, I'm going full Nerd Alert on this show. So if you like the Nerd Alert,

17
00:01:52,400 --> 00:01:57,040
you're probably going to enjoy this show. So Ben, why don't we start by talking a little bit about

18
00:01:57,040 --> 00:02:03,520
your background and what you're doing, the company, all that stuff. My background, well that

19
00:02:03,520 --> 00:02:10,800
that requires a full Nerd Alert, a super full Nerd Alert. Well, I was at MIT PhD and I was actually

20
00:02:10,800 --> 00:02:18,640
in a physics group second today. Oh, okay. Yeah, so physics is a good background, especially

21
00:02:18,640 --> 00:02:24,160
statistical physics and a lot actually at Gamalon, a lot of our, a lot of folks there are

22
00:02:25,120 --> 00:02:29,840
from physics background as well. I was kind of obsessed actually with machine learning and

23
00:02:29,840 --> 00:02:34,560
AI stuff back in high school and middle school and it was lucky enough to

24
00:02:34,560 --> 00:02:41,520
you get a few audiences with a few of my heroes and some of them recommended that I went

25
00:02:41,520 --> 00:02:46,800
going to physics. That's what I did. It's kind of, it's like if you want to learn how to play

26
00:02:46,800 --> 00:02:52,320
rock guitar where you go study jazz or classical, so you have good chops or something. So you know,

27
00:02:52,320 --> 00:02:57,280
that kind of thing. Yeah, so that's my background, did a postdoc and it was a visiting scientist at

28
00:02:57,280 --> 00:03:02,320
MIT for a little while and then I founded my, my first company was, we called them probability

29
00:03:02,320 --> 00:03:06,720
processors. So they're microchips. Okay. And it was really the first microchip architecture,

30
00:03:06,720 --> 00:03:12,640
first processor for machine learning, you know, before kind of GPUs and before the Google

31
00:03:12,640 --> 00:03:17,680
Tensor processor unit, we created a sparse dense tensor processing processor. Oh, wow.

32
00:03:17,680 --> 00:03:24,480
About five years actually before the TPU CPU. So that's in a bunch of stuff that's in like

33
00:03:25,520 --> 00:03:29,920
every cell phone call you place probably goes through one of those because it's in most like

34
00:03:29,920 --> 00:03:35,280
cell phone towers. Okay. Or a lot of them. And it's in cars and, oh, wow, might be going into

35
00:03:35,280 --> 00:03:40,880
the Amazon echo. So it's in a bunch of places. And is that company still going or was it like

36
00:03:40,880 --> 00:03:45,840
outside or acquired? Yeah. So that's kind of the basis of, you know, it's a big part of analog

37
00:03:45,840 --> 00:03:50,240
devices now, just the company that acquired us. Big Boston, Boston, based semiconductor, yeah,

38
00:03:50,240 --> 00:03:54,320
one of the best semiconductor companies and definitely the best one in Boston.

39
00:03:54,320 --> 00:04:01,520
Right. So, you know, but through that experience, you know, we always thought, you know,

40
00:04:02,160 --> 00:04:05,520
if we're going to have these processors, we got to make them easy to program.

41
00:04:05,520 --> 00:04:09,040
Right. And so we came up with what we call probabilistic programming.

42
00:04:09,040 --> 00:04:14,800
Okay. And, you know, back then you could kind of count on two hands, the other people who thought

43
00:04:14,800 --> 00:04:19,600
they were working on probabilistic programming. We were a small cult. We all knew each other.

44
00:04:19,600 --> 00:04:24,640
Yeah. Now it's just exploded. But, you know, we built one of the first industrial scale,

45
00:04:24,640 --> 00:04:28,560
probabilistic programming systems and compiler systems. Back around the same time,

46
00:04:28,560 --> 00:04:32,320
Microsoft was building in fur.net, okay, which they used. I don't know that one.

47
00:04:32,320 --> 00:04:37,200
Oh, you don't know that one? Yeah. So, you know, it was used to prototype some cool stuff for gamers.

48
00:04:37,760 --> 00:04:43,840
It matches like Xbox people, like if you're an Xbox player, and they used in fur.net to build

49
00:04:43,840 --> 00:04:47,600
a machine learning model that'll find you really good people to play against. Oh, wow.

50
00:04:47,600 --> 00:04:52,560
Okay. Kind of well matched your skill level. Yeah. And it's called true skill. And they also

51
00:04:53,120 --> 00:04:57,600
did some stuff for Bing that I heard about that. Okay. Obviously not an expert on all the things

52
00:04:57,600 --> 00:05:02,640
they didn't tell me. But yeah. So that's kind of where I came from in my background.

53
00:05:02,640 --> 00:05:05,840
And were you in the keynote this morning here at O'Reilly?

54
00:05:06,320 --> 00:05:11,040
I didn't get chance to catch it. It was, uh, I forget the name Jonathan, I think, from MIT.

55
00:05:11,760 --> 00:05:16,160
Professor there at CSAIL was talking about some of the work that they were doing was

56
00:05:16,160 --> 00:05:20,320
that involved probably ballistic programming. Oh, maybe Professor Josh Tannenbaum.

57
00:05:20,320 --> 00:05:26,320
Josh Tannenbaum, yeah. Yeah. Okay. Yeah. He came to my talk and then we were hoping to get to work

58
00:05:26,320 --> 00:05:30,720
together somewhere. Oh nice. Like we have great reverence for for what those folks do in their

59
00:05:30,720 --> 00:05:36,240
lab at MIT. And there's a lot of cross fertilization there. Also one of his former postdocs,

60
00:05:36,240 --> 00:05:40,480
Noah Goodman, who's not Professor at Stanford. Okay. He's a real leader in this field. There's

61
00:05:40,480 --> 00:05:45,840
a few others. David Blight at Columbia here in New York. Okay. So probabilistic programming seems

62
00:05:45,840 --> 00:05:49,360
like a good thing to dig into, but I don't think we got to what Gamalon is up to.

63
00:05:49,360 --> 00:05:53,920
Well, Gamalon started as the largest investment by DARPA in probabilistic programming.

64
00:05:53,920 --> 00:05:58,240
Okay. Over the last few years. And thus you can't really talk about any of the things you're doing.

65
00:05:58,240 --> 00:06:04,320
No, of course you can. Of course you can. What they call a six-to-one technology readiness level one,

66
00:06:04,320 --> 00:06:09,280
which is the academic and commercial research. Okay. You know, so we basically set out to make it

67
00:06:09,280 --> 00:06:16,720
really easy for data scientists or human statisticians or modelers to create large-scale complex

68
00:06:16,720 --> 00:06:23,280
Bayesian models and solve them with data. And then I had this breakthrough where we realized

69
00:06:23,280 --> 00:06:26,720
with the tools we had built, we could actually get a computer to write its own probabilistic

70
00:06:26,720 --> 00:06:32,560
programs. That's a new thing. That's not really an academia. That's just a Gamalon thing.

71
00:06:33,360 --> 00:06:37,520
We call the underlying technology Bayesian programs synthesis, but we branded it idea learning.

72
00:06:37,520 --> 00:06:42,320
Okay. Kind of compete with deep learning a little bit, like good branding, so we needed a good

73
00:06:42,320 --> 00:06:47,680
term. So idea learning. There's some wind behind these sales now, the deep learning. Yeah.

74
00:06:47,680 --> 00:06:53,520
It's interesting to see. So you have kind of a standard way. You walk folks through kind of

75
00:06:53,520 --> 00:06:59,600
Gaussian stuff, probabilistic programming, kind of how that all fits together. Yeah, I do.

76
00:07:00,320 --> 00:07:05,120
You know, you can also check out Gamalon.com. You can follow along on some of the examples that

77
00:07:05,120 --> 00:07:12,160
we have on the technology tab. Like one good example would be say I wanted to make a little drawing

78
00:07:12,160 --> 00:07:17,200
app, right? So we actually did this in one of our hackathons at Gamalon. It's pretty fun.

79
00:07:17,200 --> 00:07:23,040
The idea is basically like Microsoft Surface or a tablet, you know, you would want to

80
00:07:23,040 --> 00:07:26,800
with your finger or stylus, you know, straw triangle or square or whatever.

81
00:07:27,840 --> 00:07:33,440
If you're like me, you're not I'm not I'm a terrible artist. So my squares and rectangles

82
00:07:33,440 --> 00:07:38,240
and triangles come out super lumpy and messed up. But the system should still be able to recognize

83
00:07:38,240 --> 00:07:44,320
them. And then it replaces what you drew with what you meant to draw. Okay. So I've seen there,

84
00:07:44,320 --> 00:07:49,520
there are apps to do this and for that. Absolutely. They kind of I think ours is the coolest.

85
00:07:49,520 --> 00:07:54,640
Cause I mean, I was totally just a toy. Like it's not a product. It's just to kind of play with

86
00:07:54,640 --> 00:07:58,800
the technology. But one of the things that's really satisfying about it is most of those apps,

87
00:07:58,800 --> 00:08:03,440
you sort of draw a triangle and it's like, Oh, that's a triangle. Yeah. But then the triangle it

88
00:08:03,440 --> 00:08:09,040
puts there isn't the triangle you meant to draw. It just puts down like a 60, 60, 60, 60,

89
00:08:09,040 --> 00:08:16,880
60s on the bottom, you know, standard size. So it's not really your triangle. It's their triangle.

90
00:08:16,880 --> 00:08:24,320
Right. The thing, you know, so our app puts it's that map our experiment prototype hackathon thing.

91
00:08:24,320 --> 00:08:30,400
But it's a good pedagogical tool. It puts your triangle. Nice. Okay. So the way it does that.

92
00:08:30,400 --> 00:08:34,080
So I'm going to just take you through what's basically a cooking recipe. You could do a cooking

93
00:08:34,080 --> 00:08:38,320
recipe on a podcast, right? Like take two eggs or some flour. Yeah. So it's going to be like that.

94
00:08:38,320 --> 00:08:44,000
Nice. So what we're going to do is a cooking recipe to draw a triangle. Okay. All right. So

95
00:08:44,000 --> 00:08:49,440
triangle has three corners. You know, we're going to have to pick a x y coordinates for the first

96
00:08:49,440 --> 00:08:53,280
corner of the triangle. Cause this is going to be a random triangle. The corners could be all over

97
00:08:53,280 --> 00:08:56,480
the place. It's going to be anywhere. So we're going to pick those at random. So let's pick the

98
00:08:56,480 --> 00:09:01,760
first triangle. Maybe we're doing this. I'm going to date myself. But this is going to be a 1024

99
00:09:01,760 --> 00:09:07,040
by 768 JPEG image or something, right? I'm going to draw this triangle. So we're going to pick a

100
00:09:07,040 --> 00:09:11,680
number between one and 1024. And that's going to be the x coordinate. And I'm going to pick another

101
00:09:11,680 --> 00:09:20,880
number at random between one and seven 68. I guess zero and seven 67 on the vertical axis. So we

102
00:09:20,880 --> 00:09:25,920
picked that first coordinate. And then we're going to pick a second coordinate. We have two points

103
00:09:25,920 --> 00:09:31,040
and we draw a line between them. And then, okay. So now we got a line, right with two ends.

104
00:09:32,000 --> 00:09:37,120
And then pick a third point. And then we draw another line. Now we got almost a triangle.

105
00:09:37,120 --> 00:09:41,920
Then we just got to bring it on home, close the triangle, draw the last line. We didn't need any

106
00:09:41,920 --> 00:09:47,440
new points. We just need to connect to the opening. So that was a good probabilistic program.

107
00:09:47,440 --> 00:09:51,760
We had some randomness because we picked the corners at random. We also had some determinism

108
00:09:51,760 --> 00:09:56,880
because we drew the lines straight and just, you know, they're the same every time. If you tell

109
00:09:56,880 --> 00:10:01,920
the points, what points are at the ends, the line is the same every time. That's what probabilistic

110
00:10:01,920 --> 00:10:07,920
programs are good at, sort of combining randomness and determinism. And I could ask a question about

111
00:10:07,920 --> 00:10:13,520
this specific app. Sure. Yeah. And I'm totally biased by this thing that I've seen on the iPad.

112
00:10:13,520 --> 00:10:18,960
Okay. Why are you picking the random points if I'm drawing with my finger or triangle? Yeah.

113
00:10:18,960 --> 00:10:23,600
And I have seen the thing where it'll draw the isosceles triangle and what I drew is like nothing

114
00:10:23,600 --> 00:10:28,000
like that. Right. Right. But why not just figure out what the vertices were that I drew. The

115
00:10:28,000 --> 00:10:33,280
corners changes the direction and the lines. Yeah. You're asking the exact right question. Right.

116
00:10:33,280 --> 00:10:38,640
So we've got this cookbook recipe, but we want the real corners of the triangle you meant to draw.

117
00:10:38,640 --> 00:10:42,640
So here's what we're going to do. We're going to take this cookbook recipe and we're going to run it

118
00:10:42,640 --> 00:10:47,680
over and over and over again. And every time we run it, we're going to adjust those random three

119
00:10:47,680 --> 00:10:53,280
corners and move them a little. And then we're going to move them and then we're going to re-render

120
00:10:53,280 --> 00:10:57,920
the triangle and we're going to compare it to the triangle you drew. And we're going to score it.

121
00:10:57,920 --> 00:11:03,760
And if it got better, if it got closer to the triangle that you drew this random cookbook

122
00:11:04,720 --> 00:11:10,800
recipe output, then we're going to keep that new set of corners. And if the corners got,

123
00:11:10,800 --> 00:11:14,960
if the output got worse, like it doesn't, it's moved away from being like the triangle you drew,

124
00:11:15,600 --> 00:11:20,880
then a lot of the time where we're going to reject that and try again, try some new corners.

125
00:11:21,520 --> 00:11:26,720
Is this like some of the squares of the distances between the vertices or something like that?

126
00:11:26,720 --> 00:11:31,200
Yeah, any distance metric. This is the red light district, the CD underbelly

127
00:11:32,640 --> 00:11:37,040
that side at the railroad tracks of probabilistic programming is you always have to have a good

128
00:11:37,040 --> 00:11:41,920
way of scoring at the bottom there. Mean squared errors gives any. That's basically saying,

129
00:11:41,920 --> 00:11:46,240
hey, I think the triangle that the cookbook output, if I add some Gaussian noise to it,

130
00:11:46,240 --> 00:11:50,960
it would look like your triangle. That's really what mean squared distance means there in that context.

131
00:11:51,520 --> 00:11:55,520
So anyway, so yeah, so you could pick absolute value, L1 distance, you could, and that's

132
00:11:56,080 --> 00:12:00,880
it's another thing that's really saying there's pass on noise between your cookbook recipe and what

133
00:12:00,880 --> 00:12:06,720
I drew the real observed triangle. So anyway, so that's the south side of town. But once you get

134
00:12:06,720 --> 00:12:13,920
that figured out, then you can just keep adjusting those corners until it matches. And once it matches,

135
00:12:13,920 --> 00:12:20,560
as best as it's going to, then you could take, say, the most likely run you had of the cookbook

136
00:12:20,560 --> 00:12:25,040
recipe. And you say, what are the corners there? You're like, okay, I've got a set of corners.

137
00:12:25,040 --> 00:12:29,360
They've matched as, you know, the best ones we found. And you could draw a triangle with those

138
00:12:29,360 --> 00:12:34,240
corners. You take that triangle and you draw it right on the tablet. And the person goes, wow,

139
00:12:34,240 --> 00:12:39,920
that looks like the triangle I meant to draw. Now, what I just described to you is totally not all we do

140
00:12:41,520 --> 00:12:46,560
because it is insane idea to run a program over and over again and adjust the parameters

141
00:12:46,560 --> 00:12:51,040
or random and hope you hit the output. So that would be like, I don't know, throwing

142
00:12:51,760 --> 00:12:56,960
from our hotel room here at a top Manhattan, we would be throwing darts and trying to hit the

143
00:12:56,960 --> 00:13:01,600
statue of Liberty or something. So we don't actually do that. But I think that's a good way to think

144
00:13:01,600 --> 00:13:05,920
about it. And for simple programs, simple policy programs, you can actually do it that way.

145
00:13:06,560 --> 00:13:12,400
What it is, it's a good way to think about what's happening. And what this algorithm will do is

146
00:13:12,400 --> 00:13:17,760
it also those talked on the show recently in the context of industrial applications of AI.

147
00:13:17,760 --> 00:13:23,440
We've talked about simulation a lot. Simulation. This is a simulator, right? This is a simulator.

148
00:13:23,440 --> 00:13:29,840
So what did you just did? You just simulated where your data came from. This is a story or in fact,

149
00:13:29,840 --> 00:13:34,640
the way we usually teach us to new employees or new people that we're trying to teach it to is

150
00:13:34,640 --> 00:13:38,640
we say, oh, you have some data and you want to do machine learning on it. You want to do

151
00:13:38,640 --> 00:13:42,720
probabilistic programming on your data. Okay. Well, the first thing you should do is in English,

152
00:13:43,840 --> 00:13:50,800
get out your literary cap in your quill pen. And write down in English, the story of how you think

153
00:13:50,800 --> 00:13:55,680
your data came to be, right? Which is a simulation. It's a simulation of the system that made your

154
00:13:55,680 --> 00:14:02,240
data. Whether that's a biological thing, you know, this is heartbeat, data, or whether that's

155
00:14:02,240 --> 00:14:07,360
financial data or whatever. And the characters in your story, those are going to be the variables in

156
00:14:07,360 --> 00:14:12,480
your probabilistic program. And then the things they do, the actions they take are going to be some

157
00:14:12,480 --> 00:14:17,440
of the determinism like like the straight lines we drew. And if they have choices to make, those

158
00:14:17,440 --> 00:14:22,160
are going to be random coin flips or dice rolls in your probabilistic program. You know, if you

159
00:14:22,160 --> 00:14:26,800
knew exactly your system, if you could deterministically simulate it, it wouldn't be a probabilistic

160
00:14:26,800 --> 00:14:32,000
program, it would be a regular program. So the whole point of this is that there are certain things,

161
00:14:32,000 --> 00:14:35,440
you know, you have a pretty good idea of the story that created your data, but there's some stuff

162
00:14:35,440 --> 00:14:39,520
you're not sure about. Whatever that is, that's where you put the random variables in your

163
00:14:39,520 --> 00:14:44,400
probabilistic program. And then whatever those random variables are, the program's going to try

164
00:14:44,400 --> 00:14:49,760
as many possibilities through those random variables as a kin to try to match your data. And that's

165
00:14:49,760 --> 00:14:55,680
what gives you the fancy term, the posterior distribution. That's what tells you if I made a coin flip

166
00:14:56,320 --> 00:15:02,160
and, you know, if I flip the coin heads, I choose to draw a triangle. If I drop flip the coin tails,

167
00:15:02,160 --> 00:15:09,440
I choose to draw square. And I've got my data, you know, looks like a square. Then if I run this

168
00:15:09,440 --> 00:15:14,400
over and over again, that coin flip is going to tend toward the side where the, yeah, I'll put

169
00:15:14,400 --> 00:15:20,720
ahead and square. And so you'll, you'll figure out that's the kind of choice that this square-drawing

170
00:15:20,720 --> 00:15:26,640
system is doing is making a lot of the time. Probably probabilistic programming then is you just

171
00:15:26,640 --> 00:15:31,280
really summarize it for us. It's kind of a methodology for programming that allows us to introduce

172
00:15:31,280 --> 00:15:38,880
randomness, right? And, yeah, and for, and really for simulating systems, simulating systems that

173
00:15:38,880 --> 00:15:45,200
create data. And I think when people say probabilistic programming, they don't just mean the programming

174
00:15:45,200 --> 00:15:49,760
language with random variables, because like Python has random variables. Technically, Python's

175
00:15:49,760 --> 00:15:53,920
a probabilistic programming language, but that's not what we mean in the field. We also mean the

176
00:15:53,920 --> 00:15:59,360
solver, the thing that tried lots of different values of the random variables and tried to find

177
00:15:59,360 --> 00:16:05,360
good ones. And that was actually a segue to my next question, which is, you know, when I think about,

178
00:16:05,360 --> 00:16:11,600
you know, not knowing a whole lot about probabilistic programming applied, you know, and I think about

179
00:16:11,600 --> 00:16:17,600
how I might try to approach this. It's like, you know, Python either, you know, do, you know,

180
00:16:17,600 --> 00:16:22,640
get a loop and do a bunch of random stuff, you know, or get a loop and, you know,

181
00:16:22,640 --> 00:16:27,760
deterministically do like a brute force search of my space or something like that.

182
00:16:27,760 --> 00:16:33,760
Right. Is, you know, how are you doing this, you know, or what different techniques and

183
00:16:33,760 --> 00:16:38,560
the programming languages, platforms, frameworks, that kind of thing are you are being used for

184
00:16:38,560 --> 00:16:43,360
probabilistic programming? Yeah. Well, may I give you a little quick overview of, you know,

185
00:16:43,360 --> 00:16:48,880
three sentence overview of what people are doing in general on this? So I would say people are

186
00:16:48,880 --> 00:16:54,480
doing all kinds of different things. So some people, if they want to figure out, was it a square

187
00:16:54,480 --> 00:16:58,640
or triangle? And they write a program that flips a coin and if it's a heads, it draws it square,

188
00:16:58,640 --> 00:17:03,760
and if it's a tail draw triangle, there's a bunch of different ways you could try to evaluate the

189
00:17:03,760 --> 00:17:08,960
way to that coin. So you could run it over and over again, and that's called a sampling method.

190
00:17:09,600 --> 00:17:14,560
And you could do a Monte Carlo style thing there. And then within Monte Carlo methods, there's all

191
00:17:14,560 --> 00:17:20,000
the tons of those. There's metropolis Hastings and important sampling and Gibbs sampling and all

192
00:17:20,000 --> 00:17:27,440
those things. And then other people will use variational methods. And there's black box variational

193
00:17:27,440 --> 00:17:31,760
methods and there's mean field and there's, you know, all kinds of things there.

194
00:17:32,320 --> 00:17:37,440
And what are the principal differences between Monte Carlo, Sim, and variational methods?

195
00:17:38,160 --> 00:17:43,680
If I had to like summarize it well, just like in a sentence, I would say a sampling method picks

196
00:17:43,680 --> 00:17:50,880
a value over and over again, and a variational method fits a curve or a function. Okay. So like,

197
00:17:50,880 --> 00:17:56,160
it's basically think of a regular old Gaussian distribution, right? And how did it get made? Well,

198
00:17:56,160 --> 00:18:02,000
say there's some, your data actually has a real Gaussian distribution. And you want to fit that.

199
00:18:02,000 --> 00:18:06,640
So one thing you could do is you could pick a mu, pick a sigma, draw the curve,

200
00:18:07,760 --> 00:18:13,440
take the mean square to the actual data, didn't get it right. Okay, adjust a little,

201
00:18:13,440 --> 00:18:18,720
keep doing that, keep moving the mean, keep moving the sigma until it fits. That's a variational method.

202
00:18:18,720 --> 00:18:23,280
Okay. Because what you did was you changed some parameters of this curve and fit the curve model.

203
00:18:23,280 --> 00:18:27,760
Yeah. Fitting a curve is basically a variational method. And then the other way you could do it

204
00:18:27,760 --> 00:18:32,400
is Monte Carlo method, which is you could just sample a bunch of samples from a Gaussian with some

205
00:18:32,400 --> 00:18:39,520
mean invariance and then compare the histogram you got to the data and keep adjusting the parameters

206
00:18:39,520 --> 00:18:43,600
of where you're sitting. So one way to do it, you know, the science museum, they got those pegboards

207
00:18:44,160 --> 00:18:48,880
and the ping pong balls fall down and they make a Gaussian pile at the bottom, right? So you could,

208
00:18:48,880 --> 00:18:54,560
you know, you could change the spacings of the pegs in there until that would be a more like a

209
00:18:54,560 --> 00:19:00,160
sampling method. You know, change all the maybe there's biases of those pegs like they push the

210
00:19:00,160 --> 00:19:04,560
balls more to the right or the left, you get a skew. Okay, stuff like that. So, you know, so there's

211
00:19:04,560 --> 00:19:11,280
all different kinds of ways to sort of fit variables to data in a probabilistic program. And I would

212
00:19:11,280 --> 00:19:16,240
say the community is in this massive exploration of ways to do it. Another way is gradient-based

213
00:19:16,240 --> 00:19:19,920
methods, which is what's usually done in neural networks. You can do backpropagation through a

214
00:19:19,920 --> 00:19:24,560
probabilistic program using auto differentiation methods and there's a bunch of papers on that.

215
00:19:25,280 --> 00:19:32,400
What we do at Gamalon is a little different. We sort of look at each of these core methods

216
00:19:32,400 --> 00:19:39,040
as or even some of their sub components like taking a gradient as kind of like an atom

217
00:19:39,920 --> 00:19:45,760
or a subatomic particle or like an amino acid like a building block. And then what our system does

218
00:19:45,760 --> 00:19:52,160
is kind of evolves solvers on the fly to do the best job of solving the probabilistic program you

219
00:19:52,160 --> 00:19:57,840
throw at it. So, it's a pretty complex piece of engineering. Yeah, what does that mean? Does that

220
00:19:57,840 --> 00:20:06,480
mean that the system knows about some, you know, a handful of archetypal solvers and it kind of

221
00:20:06,480 --> 00:20:12,400
picks one and fits its parameters or is it it's kind of like more evolutionary than that? It's more

222
00:20:12,400 --> 00:20:18,960
evolutionary than that. It's kind of like doing some approximation of what maybe a human mathematician

223
00:20:18,960 --> 00:20:25,840
who want to design a solver for a model would do. What's the cookbook for that? There's no good

224
00:20:25,840 --> 00:20:29,680
cookbook, right? In fact, there's a theorem that says there's no good cookbook. It's called the

225
00:20:29,680 --> 00:20:35,360
no free lunch theorem, which basically says given a problem, there's no one solver that will solve

226
00:20:35,360 --> 00:20:40,720
all problems best. So, every problem, you know, they're definitely always going to be for any given

227
00:20:40,720 --> 00:20:45,360
problem. You know, you might there may be a general class of methods that work pretty well,

228
00:20:45,360 --> 00:20:50,000
but better than some other class of methods. But that class of methods would work that worked well

229
00:20:50,000 --> 00:20:54,720
on that problem could work terribly on some other problem. So, there's no guarantees, right?

230
00:20:54,720 --> 00:20:58,720
That a particular solver will be particularly good on a particular prompted program. So,

231
00:20:58,720 --> 00:21:04,000
you really just have to try stuff, which is what humans do. You know, if you look at like, you know,

232
00:21:04,000 --> 00:21:09,520
nips the conference or AI stats or any of these conferences, you kind of look through a lot of the

233
00:21:09,520 --> 00:21:15,600
papers historically, like somebody said, here's a model I invented. I want to solve, you know,

234
00:21:15,600 --> 00:21:20,720
this problem. I wanted to, I don't know, cluster documents by topic or something. And then,

235
00:21:20,720 --> 00:21:26,560
okay, this model was hard to solve. So, I had to spend a year of grad school or two years or five

236
00:21:26,560 --> 00:21:32,720
years figuring out a solver method that would work on it. Right. And sometimes for very complex

237
00:21:32,720 --> 00:21:36,080
models, people cobble together a couple of different solver methods for different parts of the

238
00:21:36,080 --> 00:21:40,080
model. And then, you know, at the end of grad school, they get it working and they publish their

239
00:21:40,080 --> 00:21:44,800
paper. Yeah. And, you know, if you're real good, you can do one of those a year. And so, most

240
00:21:44,800 --> 00:21:52,080
papers are are a choice of a model and a choice of a solver kind of made it. Uh-huh. And what we're

241
00:21:52,080 --> 00:21:57,520
trying to do is automate that, you know, grad, we're trying to, I guess, automate grad school. It's

242
00:21:57,520 --> 00:22:04,000
funny. It's funny that you describe it like that that I've spent a lot of time and have asked a

243
00:22:04,000 --> 00:22:09,920
bunch of folks on the podcast about, you know, trying to understand deep neural network architectures

244
00:22:09,920 --> 00:22:16,000
and how do folks get to Google Net or, you know, one of these kind of, how do you arrive at that?

245
00:22:16,000 --> 00:22:21,440
Yeah. And the best explanation, the best sarcastic explanation that I heard recently was

246
00:22:21,440 --> 00:22:27,520
gradient descent by graduate student. And there's an acronym for it totally. But here's a non-sarcastic

247
00:22:27,520 --> 00:22:33,600
because there's a theorem for that too, which is if a model is at its core, a simulation of where

248
00:22:33,600 --> 00:22:39,120
your data came from. Think about what that, what a synonym is for that. Another synonym for that

249
00:22:39,120 --> 00:22:45,600
is scientific theory. A scientific theory, a good one is a very clean, accurate simulation

250
00:22:45,600 --> 00:22:51,760
for your data came from. Whether it's a black hole or a mitochondria or something. And a good

251
00:22:51,760 --> 00:22:56,960
scientific theory will, you know, fit the data. It'll predict future events. It'll do all these

252
00:22:56,960 --> 00:23:02,720
things great because it's a great simulation. It really models the system well. Well, we know that

253
00:23:02,720 --> 00:23:09,200
the space of scientific theories is super exponential. There's a ton of them. There's way more than

254
00:23:09,200 --> 00:23:17,120
even NP complete. Right. So the search for scientific theories is by its definition, a computationally

255
00:23:17,120 --> 00:23:21,840
hard exercise. Like, you know, there's never going to be, well, we hope there's not going to be

256
00:23:21,840 --> 00:23:26,800
a polynomial time algorithm that just gives, well, there'd be some pros and cons. So if there was

257
00:23:26,800 --> 00:23:31,360
like a fast, you could run on your laptop and it could find a scientific theory to fit any data,

258
00:23:31,360 --> 00:23:37,200
if that algorithm existed. The good news is science, we'd know all science, right? Any data we

259
00:23:37,200 --> 00:23:41,760
got from anything we'd understand it immediately, science would be done. That'd be kind of cool in a

260
00:23:41,760 --> 00:23:48,400
way. Problem is if the aliens come, if the aliens come, they probably discovered this too. How

261
00:23:48,400 --> 00:23:52,560
they get here, they had like really good science. They've got a fascinating light speed travel.

262
00:23:52,560 --> 00:23:57,600
And they're really not interested in our culture, right? They don't want to hear about our science

263
00:23:57,600 --> 00:24:02,400
and our theorems and trade because we came up with different ones because it's hard. Like,

264
00:24:02,400 --> 00:24:05,760
they already have them all because they have this polynomial time algorithm. So the only reason

265
00:24:05,760 --> 00:24:10,000
they're here is for our water and our, you know, probably going to eat us or something.

266
00:24:11,040 --> 00:24:16,400
So that would be, so we most of us hope that there's no polynomial time algorithm for finding

267
00:24:16,400 --> 00:24:20,800
good models. You don't think there are benevolent aliens that just want to explore the universe and

268
00:24:20,800 --> 00:24:27,440
make friends? Well, if they have, if they have all theorems, why don't they stay home, right? Still play.

269
00:24:28,640 --> 00:24:31,120
Anything we would say would be boring to them. Yeah. Yeah.

270
00:24:33,360 --> 00:24:36,560
Because they could simulate, they could make a theory of everything about the Earth.

271
00:24:36,560 --> 00:24:40,480
They could just have us running in an aquarium and simulation. Yeah, exactly.

272
00:24:40,480 --> 00:24:44,640
Why come all the way here? My father. Yeah. Yeah. Interesting. I guess that's the other

273
00:24:44,640 --> 00:24:49,040
alternative is we would be, if they had this algorithm, then we would be that simulation.

274
00:24:49,040 --> 00:24:54,160
Right. Yeah. So I need to get you. If anyone has a connection with Elon Musk,

275
00:24:54,160 --> 00:24:58,800
so I can get him on to talk about this, you know, we are a simulation theorem, make it happen.

276
00:24:59,680 --> 00:25:06,320
So the triangle example, I think, is interesting and illustrative because it's of its simplicity,

277
00:25:06,320 --> 00:25:13,040
but it's not satisfying because it doesn't tell me where I would use this. And more specifically,

278
00:25:13,040 --> 00:25:19,840
I guess, what are the classes of problems for which this Gaussian approach is better than any of

279
00:25:19,840 --> 00:25:24,560
the other things that I would have otherwise tried to do? And by the way, it's much more than

280
00:25:24,560 --> 00:25:28,640
Gaussian. So we use all kinds of different random distributions and positive programs and

281
00:25:28,640 --> 00:25:33,920
also the determinism. So in fact, programming is more much more than Gaussian.

282
00:25:33,920 --> 00:25:37,680
Because in fact, we can express any probability distribution.

283
00:25:37,680 --> 00:25:41,040
Progressive programs are sort of turning complete, if you will, for describing probability

284
00:25:41,040 --> 00:25:48,240
distributions, they can make any probability distribution, any space. So here is where we should

285
00:25:48,240 --> 00:25:53,680
probably make a distinction between the model and the solver. So on the model side, a process

286
00:25:53,680 --> 00:26:00,560
program can express a model of any data. It's a great scientific tool. It's a great, you know,

287
00:26:00,560 --> 00:26:04,560
I mean, what if all scientific theories were written in a clear stochastic lambda calculus,

288
00:26:04,560 --> 00:26:09,360
so across all different branches of the sciences, we could all just have this clear

289
00:26:09,360 --> 00:26:16,480
language that we could trade our theories in. So someone said, you know, Bayesian modeling is,

290
00:26:17,120 --> 00:26:22,000
oh, you know, who was this Pedro Domingos said to me, you know, the Bayesian tribe think they're

291
00:26:22,000 --> 00:26:26,880
they're right, they're doing things the right way. He wrote the master algorithm, yeah.

292
00:26:26,880 --> 00:26:31,520
Yeah, because Bayesian is like clearly they're kind of the like correct way, you know,

293
00:26:31,520 --> 00:26:37,040
you need like a British accent. So like express, express a theory on anything, you know,

294
00:26:37,040 --> 00:26:41,120
a probability theory is the theory of uncertainty, like this is the right way to do. But of course,

295
00:26:41,680 --> 00:26:48,000
all but the simplest Bayesian models are computationally intractable to solve. And so, you know,

296
00:26:48,000 --> 00:26:53,200
I think people who are in other tribes, like neural networks would say, well, you know, we restricted

297
00:26:53,200 --> 00:26:59,040
ourselves to set of probabilistic programs that you can use gradient based methods on. And that's

298
00:26:59,040 --> 00:27:04,160
how we're getting all these great results. And so we would say, oh, great, gradient methods sound

299
00:27:04,160 --> 00:27:08,240
good to us. We're incorporating that into our probabilistic programming solver platform,

300
00:27:08,240 --> 00:27:13,440
along with all these other methods. And when they come in handy, we'll use them. And so,

301
00:27:14,080 --> 00:27:19,920
we do everything that TensorFlow does. In fact, our system, like David Blisystem in Columbia,

302
00:27:19,920 --> 00:27:26,000
Edward actually is built on top of TensorFlow. So there's probably a programming system on top of

303
00:27:26,000 --> 00:27:34,240
TensorFlow. Okay. Interesting. Yeah. So David BliB-L-E-I. Okay. David Bli. Brilliant. Faculty member at Columbia

304
00:27:34,240 --> 00:27:40,800
University and good colleague. Okay. So yeah, absolutely. And so, you know, there are some

305
00:27:40,800 --> 00:27:45,680
architectural limitations in using TensorFlow. But you get the benefit of, you know, this

306
00:27:45,680 --> 00:27:50,960
open source project that has a lot of good momentum and energy behind it. And so, you know,

307
00:27:50,960 --> 00:27:57,280
probably the takeaway is it's not an either or. But I like to start my design flow when I'm

308
00:27:57,280 --> 00:28:03,360
thinking about machine learning with a probabilistic program with an English story in English with

309
00:28:03,360 --> 00:28:09,840
my quill pen of where my data came from. Because I think it helps you do model discovery by

310
00:28:09,840 --> 00:28:14,800
graduate student creating descent to know what that generative story is of your data. That's a good

311
00:28:14,800 --> 00:28:22,480
starting point. So can you maybe walk us through that design thinking process for application that

312
00:28:22,480 --> 00:28:27,680
you've applied this to? Yeah. That's a good question. I hope to think of one. Yeah. I mean, so think

313
00:28:27,680 --> 00:28:33,840
about enterprise data dirt. You're trying to clean up enterprise data. So you can think about,

314
00:28:33,840 --> 00:28:39,360
well, what are all the ways that data gets corrupted? Oh, it gets abbreviated. So someone deleted

315
00:28:39,360 --> 00:28:45,040
some letters, gets permuted. There's a whole bunch of things you can you know, sat down for a half

316
00:28:45,040 --> 00:28:50,160
hour with your friend and brainstormed ways that data could get corrupted. You could write all

317
00:28:50,160 --> 00:28:55,360
those down and then you could make a story of data dirt. Okay. So that would be an example. There's

318
00:28:55,360 --> 00:29:02,400
a really beautiful example. My friend Julian at MIT who used, he didn't actually use a probabilistic

319
00:29:02,400 --> 00:29:07,680
programming system because they didn't exist at the time yet. But essentially did this for finding

320
00:29:07,680 --> 00:29:14,960
exoplanets. So exoplanet is a planet that is circling a star, you know, in a distant, not galaxy,

321
00:29:14,960 --> 00:29:22,720
but distant solar system. And he found clouds on a planet that was a thousand light years away.

322
00:29:24,160 --> 00:29:29,840
And he did that by writing a story. So he used Kepler space telescope to receive the data.

323
00:29:30,560 --> 00:29:35,920
And he wrote a story of where the data came from. You know, the light came from the star. I had to

324
00:29:35,920 --> 00:29:42,000
pass through the atmosphere of the solar of the planet. And then it had to disperse, you know,

325
00:29:42,000 --> 00:29:47,840
through the scatter through through molecules in its atmosphere. And then it had to then disperse

326
00:29:47,840 --> 00:29:53,840
through the traversal to Kepler. And then Kepler's optics, he had to model all that. And so

327
00:29:54,720 --> 00:30:01,440
he went from that to all the way back to what was the density of the atmosphere at different

328
00:30:01,440 --> 00:30:06,080
locations on the planet, just inverting that that model. And he had all kinds of different

329
00:30:06,080 --> 00:30:10,800
solver methods. So the orbital dynamics were continuous variables. He did gradient descent on

330
00:30:10,800 --> 00:30:18,080
those. But the, I think the distribution over the scattering, that stuff was deterministically

331
00:30:18,080 --> 00:30:23,440
done because they had put hot gases into a kiln and they knew their scattering properties

332
00:30:23,440 --> 00:30:27,840
at very high densities and temperatures. And so, you know, that was like a deterministic system

333
00:30:27,840 --> 00:30:32,160
with some parameters that needed to be Markov chain money, Carlos sampled. All these things kind

334
00:30:32,160 --> 00:30:37,120
of cobbled together. So doing that kind of science, I think astrophysics is a really great example.

335
00:30:37,120 --> 00:30:40,960
You know, that's the kind of thing you can do really beautifully with the probabilistic programming.

336
00:30:42,240 --> 00:30:47,920
And so in one of the cases that you see on the enterprise side, whether it's this dirty data,

337
00:30:47,920 --> 00:30:52,560
a problem or something else, like so you start out, you write out your problem in English,

338
00:30:52,560 --> 00:30:58,560
like what next, where how do you evolve that to a solution? We translated into Python. Okay.

339
00:30:58,560 --> 00:31:03,760
So just take your English and turn into Python simulation. And that, did you mention DSL earlier,

340
00:31:03,760 --> 00:31:09,760
you have a descriptive language for doing this or we just use Python. So straight up Python.

341
00:31:09,760 --> 00:31:13,920
Okay. You know, if you look at what we call particle is our probabilistic programming platform.

342
00:31:13,920 --> 00:31:17,840
Okay. An idea learning platform. Get an idea learning at the end here.

343
00:31:17,840 --> 00:31:22,560
The simulations are just straight up Python. And you can do anything you can, you can do

344
00:31:22,560 --> 00:31:28,560
in Python. If you have a science library or a business process thing that you wrote that

345
00:31:28,560 --> 00:31:31,840
simulates part of your business process or whatever, you can just, you know, patch that right in.

346
00:31:31,840 --> 00:31:36,000
Python's a great glue language. So it's a really great way of putting together simulation.

347
00:31:37,120 --> 00:31:41,680
And so that's the first step is you just build that Python model of your data. And it should

348
00:31:41,680 --> 00:31:47,680
spit out random data, kind of nonsense data, but it should look statistically like your real data.

349
00:31:49,440 --> 00:31:55,200
So you've got a problem. You model your problem in English. You create a Python

350
00:31:56,160 --> 00:32:02,800
version of your model. What we're calling a simulation here. And I think I guess what

351
00:32:02,800 --> 00:32:07,280
is interesting here is you're not, this Python isn't trying to solve your problem. It's just

352
00:32:07,280 --> 00:32:12,320
trying to model your problem. That's what's different than like traditional approaches. And then

353
00:32:12,320 --> 00:32:18,880
yes, kind of apply the solver to that exactly back. Yes. It works backwards to figure out what

354
00:32:18,880 --> 00:32:24,560
the actual model is to the actual parameters are right. Okay. Exactly. We call it posterior. Yeah.

355
00:32:24,560 --> 00:32:30,240
Okay. Yeah. Exactly. That's it. Yeah. Okay. Oh, very interesting. So it's a very rational design

356
00:32:30,240 --> 00:32:34,960
flow. Mm-hmm. You know exactly what you put into your model and you know exactly what you left

357
00:32:34,960 --> 00:32:41,040
free for the solver to noodle with. So on that last point, one of the big challenges

358
00:32:41,040 --> 00:32:48,480
in applying neural networks to, you know, problems that matter, if you will, is explainability.

359
00:32:48,480 --> 00:32:55,200
Yeah. Does the approach that we're talking about here, because you know your model,

360
00:32:55,200 --> 00:32:59,600
better it is at lead to better results from an explainability perspective, or there's still

361
00:32:59,600 --> 00:33:06,080
challenges there. I mean, it gives you a knob you can turn from perfect explainability to the

362
00:33:06,080 --> 00:33:10,880
same situation you're in with a neural network. Okay. So neural network is just a probabilistic

363
00:33:10,880 --> 00:33:14,880
program. You can write it. Right. A probabilistic program. The neural network written as a probabilistic

364
00:33:14,880 --> 00:33:22,480
program is flip these million coins. Uh-huh. Now multiply their heads and tails by some weight,

365
00:33:22,480 --> 00:33:28,400
which sampled from a Gaussian. Right. And then add them some number of times. Yeah. Do all that,

366
00:33:28,400 --> 00:33:33,040
you know, how that looks. And then add them up and do the sigmoids and then get to make that be

367
00:33:33,040 --> 00:33:37,760
the weight of another batch of coins. Okay. The weights of another batch of coins. And that's

368
00:33:37,760 --> 00:33:40,880
the next layer of the neural network. So there's your neural network as a probabilistic program.

369
00:33:40,880 --> 00:33:45,600
Okay. So you can do that. That's a pretty, you know, you can run that model. You can train it on

370
00:33:45,600 --> 00:33:50,480
data. You can use back prop in the probabilistic program as the solver. And now you're just doing

371
00:33:50,480 --> 00:33:55,040
tensorflow. You're doing neural network. Right. So the fact that it is a probabilistic programming

372
00:33:55,040 --> 00:33:59,760
language, it's just one that only has Bernoulli variables, Gaussian variables, and gradient descent

373
00:33:59,760 --> 00:34:04,960
solver. Okay. This is very limited probabilistic programming system. Okay. And it only can express

374
00:34:04,960 --> 00:34:09,600
models which tend to be not very explainable because that there's no deterministic code. You can't

375
00:34:09,600 --> 00:34:16,320
really put any ideas in there. But we can turn a knob all the way over to a super explainable model

376
00:34:16,320 --> 00:34:22,880
if you want to, like a very causal model, where if, you know, you flip a coin to decide whether

377
00:34:22,880 --> 00:34:27,040
the person was crossing the street and you flip a coin to decide whether the bus was coming at

378
00:34:27,040 --> 00:34:32,560
that same time. And then if they were crossing the street and the bus was coming at the same time,

379
00:34:32,560 --> 00:34:38,080
they get hit by the bus else they don't. And then you sample the statistics of people getting hit

380
00:34:38,080 --> 00:34:42,880
by buses and like someone gets hit by, gets hit by a bus. You go in the model and you say like,

381
00:34:42,880 --> 00:34:47,040
why do you think they got, you know, we're like good to get hit by a bus and say, oh, look,

382
00:34:47,040 --> 00:34:52,480
you know, it's very likely for them to cross the street when the bus was coming.

383
00:34:52,480 --> 00:34:58,240
Right, right. It's kind of a morbid example. But, but super explainable, super simple. You can

384
00:34:58,240 --> 00:35:03,280
make them. Did you happen to see the video on Twitter of the bus that hit the guy and the guy

385
00:35:03,280 --> 00:35:07,280
like bounce off the bus and got up. This happened today. And I think in the UK somewhere.

386
00:35:07,280 --> 00:35:12,000
I'm so glad that so that's what happens in this process. Every time someone does get hit by a bus,

387
00:35:12,000 --> 00:35:17,200
he just bounces off. Everybody's fine. That's great. And it's 100% probability of that happening.

388
00:35:17,200 --> 00:35:26,000
Just good. So yeah, so what you do is you get a knob. And the knob is what we call model capacity.

389
00:35:26,000 --> 00:35:31,280
Not so the neural network is of extreme. It's a universal function approximator. It's a super wide

390
00:35:31,280 --> 00:35:36,880
variance model. It can if with enough training data can be do me anybody be anybody it wants to be

391
00:35:36,880 --> 00:35:41,920
right. It can learn any to fit any data. And it's totally unexplainable and it takes a ton of data

392
00:35:41,920 --> 00:35:47,680
to or very unexplainable and it takes a ton of data to train it. And as you turn this knob toward

393
00:35:47,680 --> 00:35:53,360
narrower variance, you get models which are tighter, more deterministic, more understandable,

394
00:35:53,360 --> 00:35:59,040
more explainable. And if they're the right model, they also take a lot less data to fit and they

395
00:35:59,040 --> 00:36:03,920
also make much better predictions. If they're the wrong model of their biased, then you got a problem.

396
00:36:04,560 --> 00:36:09,120
And so that's the second thing. In addition to making this really fancy solver system for the

397
00:36:09,120 --> 00:36:15,280
second thing our system has is a bunch of it's basically the first IDE for probabilistic programming.

398
00:36:15,280 --> 00:36:20,240
Because so it lets our staff basically get a lot of the kinds of profiler and debugger

399
00:36:20,960 --> 00:36:25,200
feedback kind of the analogy to profiler and debugger feedback that you get from regular

400
00:36:25,200 --> 00:36:30,480
program. So it tells you, you know, is this line of code helping fit your data or hurting you when

401
00:36:30,480 --> 00:36:34,960
you try to fit your data? Is it helping your solver converge or is it hurting you with your

402
00:36:34,960 --> 00:36:40,880
solver conversions? Things like that. And that's super important. We need development tools for

403
00:36:40,880 --> 00:36:45,920
a machine learning next. We, you know, like take tensor flow, you know, it doesn't give you a ton

404
00:36:45,920 --> 00:36:51,760
of feedback when your model is the wrong model. You mentioned wanting to get to idea learning.

405
00:36:51,760 --> 00:36:59,440
Oh, idea learning. Yeah. So that's our kind of new aha at Gamlon. And I don't know if we

406
00:36:59,440 --> 00:37:06,640
have really time to get into it. But the simple idea is that instead of inputting the probabilistic

407
00:37:06,640 --> 00:37:12,080
programs by probabilistic programmers programming probabilistically, you just talk to the system.

408
00:37:12,960 --> 00:37:19,520
And tell it stuff. And it interprets what you're telling it as a probabilistic program. And so

409
00:37:19,520 --> 00:37:24,240
you can insert ideas into the middle of it. So be as if you could talk to tensor flow in

410
00:37:24,240 --> 00:37:30,800
English and have it adjust its weights. Not in terms of supervisor unsupervised data. But in

411
00:37:30,800 --> 00:37:37,120
terms of like, but almost be like saying, Hey, tensor flow, I want weight number 219 to be 1.3.

412
00:37:38,240 --> 00:37:44,000
You'd be adjusting the insides of it. Of course, that wouldn't make a lot of sense in the case

413
00:37:44,000 --> 00:37:48,320
of a neural network. But in the case of the kinds of models we build, you can actually have a

414
00:37:48,320 --> 00:37:53,440
pretty nice little conversation with your system. So the analogy here is, you know, training a dog.

415
00:37:53,440 --> 00:37:59,200
So Pavlovian conditioning, right? If you want a normal supervised neural network, whatever machine

416
00:37:59,200 --> 00:38:05,760
learning, you ring the bell. You show the food, the meat, delicious meat, the dog salivates.

417
00:38:05,760 --> 00:38:10,000
You do that over and over again. Now the dog salivates every time they hear the bell.

418
00:38:10,000 --> 00:38:14,560
Right? So what did you do? You basically give it, it's just like when we teach tensor flow,

419
00:38:14,560 --> 00:38:19,600
how to recognize a cat. You show a picture of a cat. You say, this is a cat. If it gets it right,

420
00:38:19,600 --> 00:38:24,560
then you reward it with that prop. If it's a wrong, you punish it. It's the same thing. Stimulus

421
00:38:24,560 --> 00:38:30,320
and a response. And then you course it to give you the right response for the stimulus, desired

422
00:38:30,320 --> 00:38:34,960
response for the stimulus. Do that over and over again. With a dog, it's like 12 repetitions.

423
00:38:34,960 --> 00:38:40,800
With a tensor flow, it tends to be, you know, 30,000 cat images or labeled data points for each

424
00:38:40,800 --> 00:38:46,480
category. And that's how we ordinarily, but when we were the human, you know, with my kids,

425
00:38:46,480 --> 00:38:51,200
you know, when I went on the comfort dinner, you know, and they was just a summer now. So we

426
00:38:51,200 --> 00:38:56,560
got it, they're out playing. So we got the dinner bell. And you just say, look, this is the dinner

427
00:38:56,560 --> 00:39:01,760
bell. And when I ring it, that means it's dinner's ready and you should come. And, you know, normal

428
00:39:01,760 --> 00:39:06,800
people would hear that once. And they would just come to the dinner. They don't need repetitions.

429
00:39:06,800 --> 00:39:12,080
They're not heavy in learning stimulus responses. We're not conditioning them. I mean, if you're my

430
00:39:12,080 --> 00:39:16,000
kids, you have to repeat yourself like three or four times. But, you know, I mean, what did you,

431
00:39:16,000 --> 00:39:20,480
you did something different? You didn't put an input output stimulus response and train now. What

432
00:39:20,480 --> 00:39:24,880
you did was you stuck an idea in their head in between the input and output right in there,

433
00:39:24,880 --> 00:39:30,720
like conception, by talking to them. So that's what we need for machine learning because there's no

434
00:39:30,720 --> 00:39:35,760
way we're going to just like, how do you build modern civilization if humans could only train

435
00:39:35,760 --> 00:39:40,000
each other through stimulus and response? It's nuts. That's crazy. This isn't going to work.

436
00:39:40,000 --> 00:39:46,000
Right. Interesting. Interesting. Great. Well, what's the, for folks that want to learn more about

437
00:39:46,000 --> 00:39:51,280
this, you've mentioned the Gamalon website is one place. There are other. If you want to play with

438
00:39:51,280 --> 00:39:57,760
models, you don't even have to install any software. There's probmods.org, P-R-O-B, M-O-D-S. I guess

439
00:39:57,760 --> 00:40:03,280
prob stands for probabilistic. I don't know what mod stands for. Models. Dot org. There's a lot of

440
00:40:03,280 --> 00:40:08,880
good examples. That's Josh Tenenbaum, no goodman. Fikashman, Shinga, some others collaborate on that.

441
00:40:08,880 --> 00:40:12,560
It's fun to play with because it's all in JavaScript. I think it's, it's all in the browser.

442
00:40:13,120 --> 00:40:16,960
You just literally go this web page and you can live play with probabilistic programs and edit them

443
00:40:16,960 --> 00:40:24,800
and run them. It's kind of nice start. Any papers that are seminal and this area?

444
00:40:24,800 --> 00:40:31,920
Yeah, there's the, there's a lot of seminal papers and I would say go play. The papers are really

445
00:40:31,920 --> 00:40:40,000
pretty tough to read. They tend to combine programming languages, semantics with compiler theory

446
00:40:40,000 --> 00:40:46,000
with heavy vision math and everything in between. It's going to be an expert in like three different

447
00:40:46,000 --> 00:40:51,760
terrible vocabularies, mathematical notation. So yeah, I think playing is maybe the, could be

448
00:40:51,760 --> 00:40:55,920
more fun. Awesome. Awesome. Well, it's so great to have you on the show. I really appreciate you

449
00:40:55,920 --> 00:41:05,680
taking the time out. Yeah, thanks, Robbie. All right, everyone. That is our show. Thanks so

450
00:41:05,680 --> 00:41:11,600
much for listening and for your continued support, comments and feedback. A special thanks goes

451
00:41:11,600 --> 00:41:17,040
out to our series sponsor, Intel Nirvana. If you didn't catch the first show in this series where

452
00:41:17,040 --> 00:41:22,480
I talked to Naveen Rao, the head of Intel's AI product group about how they plan to leverage

453
00:41:22,480 --> 00:41:27,920
their leading position and proven history and silicon innovation to transform the world of AI,

454
00:41:27,920 --> 00:41:33,440
you're going to want to check that out next. For more information about Intel Nirvana's AI

455
00:41:33,440 --> 00:41:40,960
platform, visit intelnervana.com. Remember that with this series, we've kicked off our giveaway

456
00:41:40,960 --> 00:41:47,520
for tickets to the AI conference. To enter, just let us know what you think about any of the podcasts

457
00:41:47,520 --> 00:41:53,200
in the series or post your favorite quote from any of them on the show notes page on Twitter

458
00:41:53,200 --> 00:41:59,680
or via any of our social media channels. Make sure to mention at Twomo AI, at Intel AI,

459
00:41:59,680 --> 00:42:08,000
and at the AI come so that we know you want to enter the contest. Full details can be found on

460
00:42:08,000 --> 00:42:13,680
the series page. And of course, all entrants get one of our slick Twomo laptop stickers.

461
00:42:13,680 --> 00:42:19,680
Speaking of the series page, you can find links to all of the individual show notes pages

462
00:42:19,680 --> 00:42:49,520
by visiting TwomoAI.com slash O'Reilly AINY. Thanks so much for listening and catch you next time.

