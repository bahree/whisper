Hey everyone, hope you all had a wonderful holiday.
For the next few weeks we'll be running back the clock with our second annual AI Rewind
series.
Join by a few friends of the show, we'll be reviewing the papers, tools, use cases,
and other developments that made us splash in 2019 in key fields like machine learning,
deep learning, NLP, computer vision, reinforcement learning, and ethical AI.
Be sure to follow along with the series at twomolai.com slash rewind 19.
As always, we'd love to hear your thoughts on this series, including anything we might
have missed.
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via
a comment on the show notes page you can find at twomolai.com.
Happy New Year, let's get into the show.
Alright everyone, I am on the line with Chelsea Finn.
Chelsea is an assistant professor of computer science at Stanford University.
Chelsea, welcome back to the twomolai podcast.
Thank you Sam.
Alright, so we have not spoken since it's been quite a while.
You were at Berkeley at the time, it was back in June of 2017 for twomol talk number 29,
we're over 300 shows now, so quite a while.
Before we jump into our focus for this conversation, which is a look back at 2019 and all the
exciting developments in reinforcement learning, why don't you catch us up on what you've
been up to over the past couple of years?
Yeah, so I finished my dissertation at UC Berkeley, now I'm an assistant professor at
Stanford, and I guess in my research, some of the things I've been really thinking about
recently are how we can build machine learning systems and especially embodied systems such
as robots that can generalize two different objects, two different environments, two different
settings.
And this is through the lens of reinforcement learning algorithms, as well as what's called
metal learning algorithms, where you try to accumulate previous experience in a way that
allows you to quickly learn new things or quickly adapt to new settings, rather than
trying to learn from scratch for every new thing that you might want to do.
So my group at Stanford has been starting to study some of these problems in generalization
and reinforcement learning and robotics, and yeah, excited to be on the show today.
Thanks, and you're also teaching a course at Stanford now on metal learning, is that right?
Yeah, absolutely.
I'm teaching a new course that is just wrapping up.
We have our last class on Monday of next week, and then after actually after the course,
all of the videos are going to be available online for the public to see all the videos
of the lectures.
Oh, that's fantastic.
That is fantastic.
So the format of this particular conversation is going to be, again, focused on 2019
in review and reinforcement learning before we jump into the specific papers or topics
that you thought weren't interesting this year.
Can you kind of characterize the year for us, you know, relative to other years that you've
been following RL, you know, how was 2019 shaped up?
Yeah, I think that research and reinforcement learning has really been picking up, and
there have been an increasing interest in it, so there are more labs that were crucially
focusing on supervised learning or unsupervised learning that are now going into this setting
where agents need to make multiple decisions, and people have different motivations for
doing that.
But I think that really the field is expanding, and that's been an exciting time, and also
with that expansion, I think that people have been studying a broader range of reinforcement
learning problems.
So before people were kind of narrowly focused on a few benchmarks, and I think that now
that is opening up, and people are kind of reconsidering different formulations and
different problem settings within the context of reinforcement learning, and with that
there's also been some of the same big players that have been trying to advance the capabilities
of our reinforcement learning systems as well.
So I think that the, there's been progress on a lot of fronts, and it's been a pretty
exciting year.
Now, the benchmarks that have traditionally been used in RL, at least the ones that come
to mind most immediately for me are video games, and in particular, historically it's been
kind of simple Atari-style video games, but these have been getting a lot more complex
over time.
Yeah, absolutely.
So one of the big focuses in reinforcement learning has been looking at Atari games,
and there's still actually worse and really interesting progress made on those benchmarks,
but I think that also a lot of research has been opening up and focusing on other problems
as well.
Another very common benchmark in previous years has been the, these continuous control tasks
of these simulated robots in the Bajako physics engine, and using like open-air gym, for
example.
An example of that would be like the cart pull.
It's like cart pull, but also what's called the half cheetah or the ant, which is actually
a four-legged creature, these types of locomotion, simulated locomotion tasks.
Wow, okay.
So these are, we'll see these videos with an agent, essentially learning to walk or trying
to get from one place to another most efficiently.
Those types of benchmarks got it, okay.
All right.
Awesome.
So your task in preparing for this session is to come up with a few or was to come
up with a few papers that you thought were significant.
You, in fact, came up with topics, some of which had multiple papers that you thought were
really interesting.
Why don't we just jump in and have you walk us through these?
Yeah.
So I think that it, there's often never just one single paper that really solidifies, and
a result is often many multiple results that actually really show you what has been,
what is capable and what you can, what these algorithms are capable of and what these
what can be done with their current technology, and so that's why I wanted to focus on these
different topics because I think that there are, it's not just one paper for each thing.
So the kind of the first thing that I wanted to highlight was thinking about reinforcement
learning in the real world.
So there's been some pretty impressive progress on reinforcement learning on real robots
for dexterous manipulation tasks.
So think like a five-fingered hand that can do things such as turn, turn Rubik's cube size
or can manipulate to what's called bounding balls in the palm of the hand and rotate
them with a single hand.
And so we've seen reinforcement learning algorithms that are able to learn both of these
very dexterous manipulation tasks with five finger hands in the real world.
So one, but one thing that was interesting, there were actually two papers that showed
these results, one that actually trained completely in simulation and then tried to transfer
what was learned in simulation to the real robot, and one which learned completely in
the real world and was actually efficient enough to run in the real world.
And so one of the things that I found really exciting about these results is it showed
the sort of complexity that we could learn in the real world using reinforcement learning
algorithms, and it was interesting to see how divergent the two approaches were for
accomplishing somewhat similar objectives.
How would you characterize the divergent nature of these two approaches?
What were the key things that they did differently?
Yeah, so the key thing was simulation versus real.
So one of them was, and it's a little bit more that one of them was trying to take a
really powerful reinforcement learning method and train it in a variety of different simulated
settings in a way that allowed it to transfer to the real world.
And one was extremely focused on efficiency.
So if you're running on a real robot, a five-fingered hand that is a bit fragile, it isn't
something that you can really put a lot of wear and tear on, then you need to be extremely
efficient.
You need to be learning in a way that you don't break the hand in the process of reinforcement
learning.
So, and so the two algorithms that were developed here were in many ways just like completely
different from each other, but yet achieved a kind of similar result in the real world.
And the Rubik's Cube paper, that's a relatively recent one in the year, and that was some
results by OpenAI, and those caused a bit of a stir in the community that the results
they presented were overhyped or over marketed.
Do you have a take on that?
Yeah, so I think that the concern was that the kind of the title of the approach or of
the blog post was about solving Rubik's cubes using reinforcement learning, and what they
were actually doing was they were using a Rubik's Cube solver, and then figuring out how
you could do the, they were using a Rubik's Cube solver to figure out which face to turn
in which way, and so that kind of was in some ways pretty specified, and then the reinforcement
learning part was figuring out how you can turn that face with a physical hand.
And so this was, this was a bit controversial because it was people got the impression
that they were learning all of the moves of solving the cube with reinforcement learning
and not just the physical aspect of it.
In many ways, I actually think that the physical aspect of it, while it seems like it should
be simpler, because we are like basic manipulation skills are so intuitive and basic to us.
I think that actually in reality, these sort of physical contact is actually much harder than
solving the Rubik's Cube.
We've had solvers for Rubik's cubes for a very long time.
This is one of the first times that we've actually seen, or robotic hand, be able to have
the dexterity that allows it to rotate one of the paces.
Well, and there are so many more degrees of freedom with the hand than the cube itself.
Yeah, yeah, and so yeah, there was, there was some controversy there, and also just generally
the way that things were handled with like the press and everything, because they also
put a significant effort into producing a video and like marketing the work, whereas
in contrast, a lot of research labs don't do that as you might expect.
So there was also some controversy there.
For the record, we'll include links to all of these papers in the show notes, the ones
we just spoke about were solving Rubik's Cube with a robot hand, and deep dynamics for
learning dexterous manipulation.
Yeah, and the second paper was by Anusha Nagarbadi, who is a PhD student at UC Berkeley, and
was doing an internship at Google Brain at the time.
Okay, great, great.
So lots of progress on applying reinforcement learning to manipulating physical objects
with robotic hands.
So the second approach that we talked about by Anusha was using a technique called model
based reinforcement learning, where you learn a dynamics model of the world, and then you
do reinforcement learning using that dynamics model to optimize a policy or to optimize
over actions.
And so this kind of approach in general, I think, has in some ways received less attention
by the machine learning community than model free reinforcement learning methods.
But this year, I think that we actually saw an increased interest in model based reinforcement
learning methods, and one of the reasons why people do like to use them is they tend to
be more sample efficient.
But the challenge is if you're in a vision based domain, if you only see images, if you
only see image pixels as your observations, then in principle, learning a model involves
potentially even predicting pixels forward into the future, like learning a video prediction
model, and that can be very challenging.
And so this year, we saw some real progress, I think, on model based reinforcement learning
on vision based domains, one approach that predict pixels into the future.
So I was actually learning a video prediction model and showed how you can use this for things
like Atari Games, and one which bypassed the need to predict pixels by pushing forward
in a latent representation learned by neural network, and using that latent representation
to predict things like the future values and the actions of a policy.
And so I guess one of the things that I was really impressed by and thought was interesting
with these two works is that it really showed the viability of model based reinforcement
learning as an approach, even for some of the benchmarks that have been so heavily studied
by model free approaches.
So I think there are a number of benefits of model based, and I wasn't necessarily thinking
that those benefits would come on things like Atari, for example, but we were able to
see significant progress from these works on those domains.
In the case of the Atari work, what does a model look like in that context?
Yeah, so for the first paper, the model was actually generating images into the future.
So it's a big neural network.
It's fully convolutional.
So it's basically taking in an input frame, it produces convolutional feature maps and
produces a representation and then has deconvolutions to produce the next image.
And it also takes us and put the action to produce the next image.
So it's trying to predict what will the next image or the next sequence of images look
like given the current image and a sequence of actions.
And then that future predicted image becomes additional input to the RL learner.
So yeah, so then that image you can use to essentially generate more data for your
reinforcement learner.
So instead of always taking actions in the real environment, you could also take actions
in this learned environment and use that to improve your policy.
Sometimes when we use model-based approaches, the model isn't necessarily a learn model.
Is it a learn model in both of these cases?
Yeah, so in both of these cases, it is a learn model.
There's a field, especially prominent in robotics called model-based control that typically
assumes that you know the model of the environment.
And I guess it's not always, it's not just in robotics as well.
There are a number of approaches where it assumes that you know the model.
You know the true simulation of your system and use that known simulation in order to
learn the task.
And in many ways, in Atari, it's in some ways a little bit silly to actually learn the
model because you actually have the real simulation system.
But I think that in many ways, the reason why people care about these approaches that actually
learn the model is that it means that it's applicable in settings such as the real world
where you don't know the model.
And so in both of these cases are, is the approach essentially the same where you're using
model to predict images into the future?
So the first one is predicting images into the future and the second one is actually
predicting other quantities into the future.
So it's predicting things like values, rewards, and actions into the future.
And this is all, all of these predictions are conditioned on some latent representations.
You can think of it as predicting the only the quantities that are relevant for the game.
But in some latent representation, rather than having to predict all of the pixels, which
include things that aren't necessarily relevant to doing well on the game.
Got it.
And so the pixels that you're predicting here might be pixels that represent things on the
screen that are specific to specific to actions that might be taken, like I don't know
a score or some kind of trying to come up with a good example of what that might be.
But specific symbols on the screen, or are we out of, is the model predicting out of
pixel space?
Yeah.
So in the second one, it's not actually predicting at all in the pixel space.
It's basically, as a neural network that takes the image and produces a vector of representation.
And then it predicts that representation forward, rather than predicting the pixels.
But the fundamental, and both of these cases, we're starting from vision and pixels and
using that to using models based on these pixels to kind of inform an RL learner, an
RL agent.
Yeah.
Okay.
All right.
And so the next category that you wanted to cover is focused on batch off policy RL.
What is that?
Yeah, so, and actually the next two topics are focusing really on generalization and
reinforcement learning in a way.
So what batch off policy RL is, is say that you have, so I guess, let's first talk about
kind of the standard reinforcement learning settings.
So in the standard reinforcement learning setting, your agent collects some data, you learn
from that data, then you collect some more data, you learn from that data, and you iterate
this process.
And what off policy reinforcement learning methods are methods that, I guess, maybe I'll start
with on policy methods, on policy methods, they collect data, learn from that data, and
then they throw away that data and collect a new batch of data and learn.
And so they're always collecting data from their current, their current policy, their current
actor, and they can't, they don't have a way to reuse any data that they collected previously
because they need the data to be from their current policy.
Now off policy methods are ones that can actually leverage data that they collected from
previous policies.
One that can leverage what's called off policy data, data that's not from your current policy,
your current behavior.
And these algorithms tend to be a lot more efficient because you don't, you're not throwing
away data at every single iteration of your algorithm.
Now batch off policy RL algorithms take this to an extreme where they assume that you
actually are just given a batch of data from some, from some policy from, according to
some behavior, and then try to learn from that and I don't have any ability to collect
more data in that environment.
And the reason why this is really important and interesting is that first, if you think
of just the majority of machine learning, you consider like, you often have settings
where you have some data set, maybe something like image net, you're just given a batch
of data and you want to learn from that data.
And if we have algorithms that can just learn behaviors and learn policies from a batch
of data, that means that we can start just accumulating very large and diverse data sets
and allowing algorithms to learn from them without having to kind of have this iterative
data collection process in the loop.
And the second reason why it's important is that if there are a number of settings
where it's just, it's unsafe or not possible to collect more data such as if you imagine
for example, wanting to learn how to make medical decisions.
You want to learn from data of doctors' decisions or maybe you have another system that's interacting
with users in a way that isn't safe to take actions in random ways, then you want to
just be able to use data from doctors, for example, and learn from that data without having
to experiment or collect more data by kind of randomly taking actions or by exploring.
Does that make sense?
It does.
One question I have is, does the sequence of actions, is that necessarily included from
or excluded from the typical problem set up for batch off policy RL?
Yeah, so typically you do assume that you have the actions that were taken as well.
So you know what action or what decision the doctor made in the medical decision making
example.
But one quite interesting setting would be, maybe you have some data with actions, but
some data without actions, like you're just observing humans on the internet doing stuff
and maybe you could try to learn how to manipulate objects from that data and that's a setting
that some students in my lab have actually been studying recently.
Interesting.
Yeah, that question was prompted by your idea that at some point we might just be able
to take some collection of data and learn from it using off policy RL agent, but that
would seem to assume that there's some known sequence of actions that you'd have to
have that and that seems to make it less of a natural data set if that makes any sense.
With that background in mind, what were the specific advancements in the couple of papers
you identified on this topic?
Yeah, so I guess getting back to benchmarks, we actually don't have really good benchmarks
for this setting that actually have meaningful real world settings, but the algorithms themselves
showed a lot of promise towards enabling good learning in these settings.
So the first one actually took the replay buffer of an agent on Atari and when they only
took this replay buffer, they were actually able to outperform the policy in that replay
buffer just by learning from that data.
So that result I think was quite impressive and also is just a suggestion that we should
be able to learn very well from these types of data if we set up our algorithms well.
And the second approach, which actually, the second paper that I linked, which actually
predates the first one, also showed quite strong results.
They weren't looking at the Atari domain.
They're looking more at these continuous control domains, but they also showed the ability
to learn behavior from these batches of data, not quite to the extent of the replay
buffer, if I remember correctly, but we're showing pretty strong results there.
The paper that's looking at the replay buffer, the result seems counterintuitive, if I'm
interpreting it correctly, you basically have this replay data from a RL agent.
So everything the agent kind of saw as it was exploring these games and then you give
it to another agent to learn off of and the agent somehow performs better than the
original agent, but it doesn't see necessarily anything more than the original agent saw
as it that it has access to all of it at the same time, whereas the original agent only
saw it in snapshots.
So yeah, that's a good point.
I think that the reason is that they didn't give it, they didn't train the first agent
completely to convergence.
They took a batch of the log data from that agent before it had reached its max performance
and then showed that there was some room for improvement.
If you give the online agent more data, it would have achieved the full maximum performance
or would have achieved as well as the offline agent.
It was just that they stopped things early and then wanted to test, can you do better
with this batch of data?
This gets to the question of how should we set up these experiments with batch off policy
reinforcement learning methods.
I think that things like training on logged DQN data or logged data from Atari isn't a
great experimental set because it doesn't necessarily test the types of things that we want
from these algorithms necessarily because we may not have that sort of data when we're
performing reinforcement learning in real world settings, but it still I think is a step
in the right direction and at least people are starting to study these kinds of problem
settings.
And proposing benchmarks for them.
Yeah, so I guess one of the reasons why I found it interesting wasn't just the kind of
the results, but also the visualizations and some of the, I think you provided an understanding
of the problem in terms of understanding where what we can do in these problem settings
to do better.
And basically what are the kind of sorts of things that we might try to do and like in
terms of solving this problem.
So when you move from this kind of batch setting to a policy that you're learning from that
batch of data, you have this distribution mismatch between the states and actions visited
by the first policy and that batch of data and the states and actions visited by the policy
that you're learning, the behavior that you're learning.
And so it provides some nice visualizations and to understanding how we might try to handle
this distribution mismatch and in particular they focus on the action setting, the distribution
mismatch and the actions and made some nice visualizations for understanding what is actually
happening under this distribution mismatch.
And this distribution mismatch is what they're calling the bootstrapping error.
I believe so, yes.
Got it.
You mentioned that this work on batch off policy as well as the next paper that you had in
mind, RL with diverse offline data sets are kind of common in that they're both tackling
generalization for RL.
That can mean a lot of things in what sense are these focused on generalization?
Yeah.
So I guess the papers that we just talked about, they actually aren't really focused
on generalization, but I think that if we build better batch off policy reinforcement
learning methods, we'll have the ability to learn from more diverse data sets because
we won't be collecting data for every, like within the context of our algorithm.
So for example, in the context of a robotics, say, if you want to generalize to something
at the level of ImageNet, that would mean that you'd have to collect an ImageNet style,
ImageNet diverse size data set in the context of your reinforcement learning experiment.
That just isn't practical, right?
So we need to think about how we can have algorithms accumulate data into a large data set
and then actually start sharing data just like the rest of machine learning does.
So if we can build these very large data sets by having robots collect data and then store
that into a very large buffer of data and then have algorithms that can learn from that
very large buffer of data without having to kind of recollect it in the loop of reinforcement
learning.
And then we can start to see the generalization that we see in supervised learning settings.
So this next paper is actually trying to start to study that, like can we accumulate,
can we build a very large and diverse data set and then learn from it?
Is it too far of a stretch to say that this is kind of pointing us in the direction
of kind of a transfer learning type of approach as applied to RL?
That's a good question.
So in this particular paper, it is in many ways pointing at a transfer learning setting
where you start, where you learn from this data set and then you learn representations
for control and then you take that representation and then try to transfer that to a new setting.
So analogous to ImageNet pre-training, for example, like ImageNet, you can pre-training
on the ImageNet data set and then fine tune your features to a new task that allows you
to transfer all of the rich diversity from ImageNet to your new problem setting.
And we did something similar in this paper where we collected a very large data set,
learned on it and then transferred to new robots, transferred to new experimental setups.
But it's what I think that it potentially points towards that.
But I also think that we may be able to ideally be able to not just study transfer learning
and also be able to just generalize in zero shot to new domains and new problems.
And so this paper that we're talking about is RoboNet large scale multi robot learning.
If I remember correctly, you did some work in grad school at Google Brain
on a kind of a large scale parallel robot platform. Is this similar?
Right. So the work back at Google in 2017 or like 2016, 2017,
it was actually paralyzing across 10 robots at Google,
but they were all the same robot platform in the same environment.
And so if we want, if we care about, it was really an important step towards this work here,
but it wasn't. And it mainly served as the foundation for the work that we're doing in this paper
in RoboNet. The key question that we're trying to answer now is if we don't have 10 robots,
like which most labs don't, my lab at Stanford does not have 10 robots.
Can we share data across institutions in a way that allows us to get the diversity
of having multiple robots, having many robots. And if we can think about having institutions
in universities share data across robots and across labs and across experimental setups,
then we can get away from having each individual lab to have to collect their own like image net size thing.
What is the nature of the data that we're talking about sharing it? Is this image data?
Is it some other type of data?
Yeah, so in this paper, the data corresponded to trajectories of images and the actions
the robot took. So essentially video plus a sequence of actions that corresponded to that
to that video, to that outcome of images. And is it video kind of off robot video or video
from the perspective of a manipulator or something else?
Yeah, so we actually included multiple camera viewpoints in the data sets such that some of them
were somewhat of a first person's perspective. Some of them were more of a third person perspective,
really corresponding to just different camera placements around the robot.
And the total number of viewpoints we had in the data set was actually over 100.
Okay. And the data set also included data not just from like, so in the Google data set,
it was like 10 robots, one robot platform. And this data set, it's around 10 robots, but it's like
seven robot platforms. So a significant diversity actually, the kinds of robots and the colors of
the robots and the kinematics of the robots across across these different labs and across actually,
across four different institutions. Okay. And the specific learning objective in this set up
is what? Right. So that's where this is where off policy, batch off policy reinforcement learning
methods come in. So if you have, if you have all this data, you need to figure out how to learn from
it. And then yeah, the good question is like, what is the reward function, right? What should you
learn from this sort of data? In this particular setting, we actually use model-based
reinforcement learning methods. So we were learning to predict video based off of the actions that
the robot took. And then once we had this this model of the of the world, then we would actually
use that model of test time to accomplish different tasks. So we would give it a you kind of give
it a new task at test time and then it would use its model to plan to plan to achieve that task,
using its learn model rather than trying to learn a policy during during training. So it actually
is kind of learning behavior on the fly at test time using its learn model. And then the kind of
the tasks that we were testing on correspond to like manipulation tasks like picking up a cup
and positioning it next to other cups or pushing a pencil to be next to some other pencils.
Okay, so in the case of a cup, you've got a model that you've learned off of the data set offline
that has basically kind of built out a view of the world. If I do take some action on the cup,
the future world is probably going to look like this. And then you then give a live robot access
to this and it's also being trained in a RL. Is it is it also an RL agent that's training that
when it's live or is it some other kind of approach? It's so it's it's not quite reinforcement learning.
It's more just in in some ways like model based control in some in some ways as we were talking
about before, but with the learn model. So it's what's called planning. So if you have a model,
if you know what kind of view you have an understanding of what will happen if you take some actions,
and you can use that model to plan a sequence of actions for trying to accomplish a goal. So
some more like an optimization problem across the likely outcomes from the model that you already have.
Yeah, exactly. It's this optimization problem over actions given a goal and your model.
Mm-hmm. Okay. But one of the things that we're doing next is we're we're trying to look into if we
can annotate rewards in the context of in his data set for different tasks. And then that would
allow us to try to do some of this training, do some of this optimization over policies or behaviors
at training time so that we could study algorithms like model free methods. And a reward annotating
a reward in this context means that if the goal is to stack cups that the cup is you know a
picture where the cup is stacked, or are we talking about something more liable than that?
Yeah, it could be something like that. And it may one of the things that's a little bit
challenging in like in robotics context is that if you have a picture of cups that are stacked,
that doesn't mean that you actually have a good reward function for that. So it's actually hard
to compare two images like if you have an image of cup stack, how do you know that another image
also has cup stacked or also isn't achieving that goal in some way. So one of the things we'd
be thinking about is just really simple things like label if an object is being grasped by the robot.
Like one if it's being grasped and zero otherwise. And then we could use that that reward
function for grasping to learn a policy for grasping. So we're thinking about very simple
reward functions like that like are there cup stacks in this image or is there about holding
something things like that. All right, so then the next couple of papers that you called out are
focused on the the broad curriculum learning area. What's been going on there? I remember
correctly we talked briefly about that a couple years ago on that podcast we did. There were two
curriculum learning curriculum based approaches that I found to be quite interesting and exciting
this year. And in both the cases the agent was actually coming up with its own curriculum
for solving tasks. So one of them was this paper from some folks at Uber AI that was actually
using genetic algorithms and evolutionary methods to generate increasingly complex environments
for the agent to learn. So it was a locomotion based task where there was this agent
this legged robot insinuation that needed to move forward. And then it was generating
different different stepping stones and different environments that made it more challenging
for the robot to to traverse the terrain. And the second one was a single environment but
that what it was trying to do it was essentially trying to play sort of a kind of hide and seek game
where there were some agents that were trying to that were trying to hide. And so they were given
like five seconds or some some amount of time to like go hide. And then the secret agents after
that had to go try to find the agents that were hiding. And all this was insinuation. But the one
of the interesting things was that it would kind of learn this sort of curriculum where initially
the hiders were doing very obvious things that were pretty easy to find. But then later they would
actually start making barricades and making it harder for the the secret agents to actually go
and find the hiders. So the curriculum was in that latter case the learning agent was the
hider or the finder. So in this case actually both the hider and the finder were learned.
Okay. They were both being learned simultaneously and the curriculum emerged simply from the fact
that this was a multi agent optimization. And so as the first the kind of hiders would learn some
sophisticated behaviors and then they would be winning and then the the the seekers or the
finders would learn a would start to learn more sophisticated behaviors as well to go find them.
And then you would have this kind of alternating thing where one of them like starts winning and then
the other agents will need to learn more sophisticated behaviors to overcome the types of strategies
that the other set of agents learned. Right. It's probably worth taking a step back and
and kind of providing a high level overview of curriculum learning. As I understand it again,
I think for me, I think you were the first person that explained this to me. The idea is that
in a you know any kind of scenario like that you might have a reinforcement learning agent
in a big part of the problem is that the the state space is huge. And you know, that's not not
unlike how we as humans learn, you know, if we had to learn everything, you know, the possibilities
are huge. So in school, for example, we create a curricula that has us learn, you know, X,
then Y, then Z and in doing so that kind of creates a more narrow path for us to get through this,
you know, the possibilities and curriculum learning is trying to do something similar where first
you teach the agent to do the first thing, then you teach it to do like in the case of,
you know, some of these locomotion examples, you first you teach it to crawl, then you teach it
to stand up, then you teach it to run, then you teach it to jump that kind of thing. Yeah, exactly.
Does it count as a as curriculum learning if it's just something that the agent does without,
you know, some special capability to learn curricula? Does that make sense? Particularly around that
second example with the hiders and the the seekers, you know, what we're doing, we're kind of
observing after the fact, you know, the things that these agents did and calling it curriculum
learning, but was there something specific to the agent that, you know, made it learn in that way
or is that kind of orthogonal to the point anyway? It doesn't not matter. Yeah, so to me,
I think that a curriculum is characterized by an increase in complexity and if for the kind of
that the beginning, you learn very simple things and at the end, you're doing much more sophisticated
and complex behaviors than it seems like there's this there's this progression of simple to complex.
In the case of the first paper, this was fairly explicit. In the case of the second paper,
I think that this was more of an emergent property of the algorithm or an emergent property of
this multi agent optimization and it wasn't necessarily something that was built in to the algorithm
from the start. And I think that just this progression of from very simple behaviors to very complex
behaviors is the thing that's very exciting to me because it means that if we can if we can move
from very simple behaviors to more complex behaviors, then maybe we can also move from very complex
behaviors to even more complex behaviors. And it seems like a path towards agents that are
increasingly sophisticated and increasingly intelligent. And so from that perspective,
it doesn't matter to you whether that's because of a training regime that has curricula built in
that we create or whether it's just something that this complex that emerges in a complex system.
Yeah, and I actually think that potentially it's maybe even more exciting if it emerges or if the
agent creates it itself because that means that it won't rely on us for moving to the next level.
The next couple of papers were focused on exploration problems?
Yeah, so these are kind of getting back to Atari Land. And in terms of the applications that
they were studying. And in general, exploration is kind of a huge challenge and reinforcement
learning. It's the problem of discovering the right thing to do in settings where you're not
given a lot of supervision oftentimes about what the right thing to do is you basically need to explore
your environment and find the parts of your environment, the parts of your state space that give
you high reward. And in the past, one of the kind of classic problems or benchmarks in exploration
and reinforcement learning has been a Atari game called Monizuma's Revenge. I don't think it's
the only problem that we care about in the context of exploration, but it's one that has been notably
challenging for our current reinforcement learning systems. And these two papers were both
proposed means for actually solving Monizuma's Revenge to a very large degree, scoring tens of
thousands of points on this game when I think previous approaches were often only getting zero
points or just a few points on the game. So I think that these approaches were making a
lot of progress there. And the key insight of the first one, which is actually a little bit
controversial, was to find the parts of the game where you're getting some, making some progress.
And then actually, if you then die at those parts of the game, actually reset to that state and
start kind of exploring in that region of the state space again. And so really trying to
remember, really trying to remember the visits states that are promising, returning to those
promising states and then explore from it. And so one of the reasons why it was controversial is
that they were using this reset ability to kind of go to a particular state that you had been to before.
And in many real world contexts, that's of course not something that you can do, but if you're
an Atari game, that is something that you could conceivably do. And then the second paper
took a very similar approach, but they lifted this assumption by using an imitation learning approach
to figure out, to kind of remember how to get back to that state. So both of these papers
are a share in common that they take advantage of ways to spend more time exploring difficult
areas of much as soon as revenge, one using some kind of God mode reset capability. And the other
is just remembering how they got to given states and getting there before they start doing
exploration. Exactly, yeah. Okay, I can see how the first one is controversial because in the
in the paper, they're kind of report their scores. But if you keep doing these unnatural things
like resetting and going to hard places and kind of accumulating more score, it doesn't seem
to be comparable to the other scores that are reported for this game. Yeah, I think it's just
worth briefly mentioning that they're, I focused on a number of, I think, conceptual advances
in various approaches. And at the same time, there have been these fairly large efforts to try
to use reinforcement learning and other approaches to solve harder types of games. So StarCraft,
there's a large team of deep mind that was trying to solve a version of StarCraft
through reinforcement learning and also, and also imitation learning. And there was also a fairly
large team at OpenAI that was trying to study reinforcement learning algorithms on the game Dota 2.
And both of them made quite notable progress in those settings. And I feel like I can't,
I can't summarize 2019 without at least making a very brief mention of those two results.
And OpenAI on the Dota front has been working on this for a while. This goes back to 2018, at least,
first potentially earlier than that. Yeah, and I think that StarCraft actually also goes back
into 2018 as well. I think that they've been long efforts. So one of the other things that we
like to cover in these AI rewind segments is more kind of practical, tangible advances in terms
of new tools and libraries and open source projects, things like that. And you had a number that
that come to mind. Where do you like to start? Yeah, so I'll start with some of the libraries that
have been made available. So in general, reinforcement learning, building reinforcement learning
algorithms is challenging. And we're actually still in the stage where for for deep reinforcement
learning algorithms, many, many algorithms can be, I guess the first is just so many design
decisions when designing a reinforcement learning agent such as when do you collect data, how do
you collect data and like at a per time step level versus like an episodic level? Do you normalize
your state's in actions? Do you how do you kind of estimate your return? There's just all of these
really tiny design decisions that that can actually make a pretty large difference on the result
of the algorithm. And so having implementations of reinforcement learning algorithms that are
trustworthy and and are actually kind of ready to use out of the box for for different applications.
I think it's quite important for the advancement of reinforcement learning. For kind of specific
algorithms, there have been a number of open source implementations released by the authors of
those papers and may wait in may times those those implementations are in some ways the most
trustworthy because they're the ones that should reproduce the results in the paper. But there's also
been a library called TF agents or TensorFlow agents that provides actually a platform of many
different algorithms, many of it reinforcement learning algorithms. And it's trying to basically
provide a kind of a unified code interface and framework for running these different types of
algorithms and making it with the goal of making it easier for people to use these algorithms on
their problems. And so how much of the the problem of getting an RL agent up and running does
does TF agent solve? Does it get you all the way there? It's always struck me with RL there are
just so many moving pieces. You've got your simulation environment or your game environment. You've
got your agents. You've got to figure out your optimization, your loss function. How much of that
does TF agents take care of for you? So if you want to use an environment like OpenAI
Jim, for example, then I think that you should be able to run it like it basically will solve
everything for you. If your environment interface is different from Jim, then of course it you'll
need to do some plumbing essentially to hook it up with that. And it also doesn't solve the
fact that if you have a new environment, these algorithms may not work out of the box on that
environment because of you may need to tune it or maybe just that your problem is harder than
the current kinds of reinforcement learning problems that we can solve. But if you want to reproduce
one of the standard algorithms using a known environment like Jim, you should be able to do it
fairly handily. I think you should be able to do it fairly handily with with this code base.
It's also worth mentioning I think that this is from very late 2018, but there was also a
a framework called dopamine that was trying to do something somewhat similar, but they were
had a more narrow and narrow focus, which was to study Q learning algorithms, various types of
Q learning algorithms for Atari games. So things like deep Q networks, distributional reinforcement
learning, prioritize experience, replay all the bells and whistles that you might want in your
DQ Ed agent, specifically looking at Atari games. That code base I think provided a very reliable
implementation of that kind of suite of algorithms, which are basically with a more narrow
focus than TF agents. And you also mentioned PyTorch higher. What's that all about?
Yeah, so this isn't necessarily a reinforcement learning thing, but it's a library that's been
very useful for metal learning research. So I don't know how much of the details I want to get
into with metal learning, but kind of the goal is to learn, as I kind of mentioned at the beginning,
the goal is to learn priors from previous experience in a way that allows you to learn very quickly,
like learn with only a few data points for new tasks. And one very popular approach for metal
learning is to perform a what's called a bi-level optimization, where you're actually embedding
a optimization process inside another optimization process, and higher provides a way to
perform these higher order optimizations, like for example, if you're embedding one optimization
inside another, you have a second order optimization process. And this this library allows it,
makes it very easy for people to do that. And I haven't used it personally myself, but my PhD
students have been using it and have said have said wonderful things about it.
Is the idea with bi-level optimization and metal learning that you're optimizing whatever
problem you're trying to solve at one level and at another level you're optimizing the way you
learn how to solve that problem? Exactly. Yeah, that's a great way to put it.
And so higher isn't necessarily a metal learning library, but it's a general bi-level optimization
library, or is it more specifically geared towards metal learning? It is a more general thing,
but they also provide a number of optimizers that make it good specifically for metal learning
use cases. Okay. And maybe to further kind of skirt this line of going too deep in on metal
learning, is there a classic problem setup or hello world of metal learning that would help
make it more concrete for folks that aren't familiar with it? It's a good question. Yeah,
so I think that maybe one one very simple like one that kind of very standard problem in metal
learning is can you learn to recognize characters of a new alphabet with a few examples? So can you
give an example of five different handwritten digits? Can you learn a classifier that can
distinguish those five handwritten digits? And the way that these metal learning algorithms work
is that they take a handwritten digits from a number of different alphabets and languages
and learn at the lower level they're trying to learn how to recognize distinguished digits from
a particular alphabet and at the higher level they're trying to change the way that that lower
level learns across alphabets in a way that makes it generalize faster and in a way that makes
allow it to learn from only five examples. So for example, if you trained a deep neural network
on five examples it would probably overfit massively or not be able to learn very much.
And these algorithms try to actually train for the ability to generalize from a few examples by
changing the way that neural networks are learning. So we've talked a little bit about open AI gem
and some of these other simulation environments. There were also some new ones that were introduced
this year. Absolutely. And I think that kind of maybe jump me ahead a little bit too much is
one of my predictions for next year is that we're going to really need better environments for studying
the kinds of problems that we care about. So I think that if we care about things like generalization
like the ability to learn new tasks quickly like the ability to solve longer horizon tasks maybe with
the use of demonstration data, then we need environments that allow us to actually evaluate those
abilities and our algorithms. And there have been a number of environments that were introduced
this year that tried to focus on different aspects of this. So for example, there was the AI habitat
environment that was developed specifically for visual navigation. And I was specifically trying
to target the setting where you want to learn how to navigate environments with photo realistic
rendering where basically you're dealing with images with where your observations are highly
realistic images such that you aren't learning from these kind of really simple game graphics.
So it's such that you're actually studying both the vision aspect of the problem as well as
the control aspect. The second environment is the meta world environment which is an environment
that one of my PhD students and some of my collaborators have been working on where we've been
trying to study or allow ourselves to study generalization across tasks. So we create a benchmark of
50 manipulation tasks and simulation. And with the hope of seeing if it can allow us to study one
whether or not we can have algorithms learn across all of the tasks and two whether or not we have
algorithms that can use say 45 of the tasks in a way that allows you to quickly learn new tasks
like the next five tasks. And that was kind of targeting meta-reinforce and learning algorithms
that can learn how to learn from small amounts of data or small amounts of data for new tasks.
The minor realm competition that I also mentioned has been looking at Minecraft environments which
I think is a really interesting taskbed for reinforcement learning methods because it's more open
ended. And they specifically one of the things that they've been focusing on there was the ability
to learn from example behavior and from demonstrations. And so they collected a very large data set of
humans playing in these Minecraft environments with the hope of building reinforcement learning agents
that can use this data as well as their own data collected that they collected themselves in the
environment in order to learn in order to learn complex and long horizon skills. The recism that I
mentioned is a kind of a simulation platform for studying whether or not you can learn a recommender
system. And I think that one of the things that I really like about this is that it actually allows you
to study a more real world problem that's quite different from things like games and things like
robotics. Yeah, I was not expecting recommender systems to come up here.
Yeah, and I think that people like in many ways the reinforcement learning community has been so
focused on control robotics and video games. And I think that maybe we're overfitting to some of
the challenges in those domains. And if we care about building reinforcement learning algorithms
that are useful in a variety of settings, then we should be testing them on things like recommender
systems. And I hope to see more of that too. Like you could imagine education as I mentioned before
medical decision making. I think that there are a lot of potential applications of a reinforcement
learning where a sequential decision making where you need a really reason about the effect of your
actions on future states. Yeah. And then the last thing is this Google research football
environment. And this one I believe is specifically focusing on the ability to study multi-agent
reinforcement learning in the context. And by football for those of us that are Americans,
they're referring to soccer. Yeah, and actually I did an interview with one of the principles in
this work not too long ago. Actually, this is relatively recent, but it was episode 293 of the
show and it was with Olivier Bachin. Yeah, so those were all the environments that I've seen come
up that look quite promising and really filling a gap that we I think don't have in our current
environments. Well, you started kind of foreshadowing into your predictions there. One of them
being that we need to see even more of these types of environments. What are some of the other
things that you expect to see in the field in the next year? And I guess we're going into 2020,
so we could we could even talk about the next decade if you dare. Yeah, so I think that in addition
to an increase in environments that will hopefully allow us to meaningfully study things like
generalization and batch off policy RL instead of repurposing old environments for those things.
I think we'll also start to see an increase in papers that study settings like batch off policy
reinforcement learning. So I think that this year we saw a pretty big increase in in people
that are studying that and I think that that's probably going to continue because that's really a
problem that matters in the context of real world, the real world deployment of reinforcement learning
systems. So greater focus on batch off policy RL? Yeah, and then I also think that we didn't quite
this was the kind of one of the papers that I was going to mention on meta reinforcement learning
and multitask reinforcement learning, which we didn't quite get to cover in 2019. And I think that
I think that we're also really going to see an increase in papers that study this. So I think
that in general the the community has been fairly focused on like trying to solve individual tasks.
And that's I think in some ways that actually been by nature of the environments that are focusing
on let's learn one Atari game, let's learn to run with this one agent in this one environment.
And I think that a lot of people really do care about generalization and we did actually see an
increase in paper this paper is this year that we're focusing on generalization. To some degree
and I think that we'll see that actually kind of the breadth of generalization that we try to study
increase and in particular trying to study generalization across tasks across goals across objectives
such that we can move towards general purpose reinforcement learning agents rather than these very
narrow and specialized agents. And do you have a sense for what that will likely look like? Is it
analogous to what you were doing with the collaboration across institutions and
in the off-policy work where you were looking at multiple different robotic platforms and trying
to train on different environment simultaneously? Or is it some new I don't know some new training
technique or something that results in greater generalization? Yeah so I think that I would love
for everyone to start studying robotics but I think that in practice people won't be
a little bit scared of robots and not scared in terms of them being dangerous but just scared of
the effort that goes into actually getting things working on a real robot in the real world.
And so I think that in practice what that mean will mean is that people will be studying
simulation agents like simulate control agents, simulate robots, maybe simulate different
like video game levels for example and studying how agents can learn across, can generalize across
these video game levels that can generalize across reward functions of a robot for example.
And yeah I think that basically that the algorithms are mostly ready for this. I think that
there's still advances to be had on interest in terms of the ability for kind of just basic
reinforcement algorithms to be stable and work well with images etc. But I think that many of
the algorithms are ready to take the jump towards these more complex settings where you need to be
doing multiple things and not just one thing. And maybe actually I mentioned Muldi's task RL
that the ability to do these different tasks but it could also be doing tasks and sequence
or kind of hierarchical reinforcement learning where you're performing like picking up an object
and then placing it into a bin and then taking that bin and putting it somewhere else.
And I think that in general people have been studying some of these problems for a long time
but I think that the will actually start to see meaningful advances in these problems
in the deep RL setting and in more complex and challenging environments.
Any other predictions?
I think that the batch of policy multi task RL and metarole in environments are my main ones.
I would guess that I guess we also talked to Affair about model based and model free. I think
that people will continue to show interest in model based methods and will also continue to see
a number of hybrid methods that combine elements of model based and model free algorithms as well.
Yeah, so I think that kind of the things that are going up will continue to actually maybe
kind of increasingly more popular things like batch off policy model based RL and metarole
reinforcement learning and multi task free reinforcement learning.
And one of the things that I guess I'm really excited about is that I think people will actually
really start to meet and study generalization and I think that this is something that's been overlooked
for a very long time in reinforcement learning.
One of the questions that I get all the time about reinforcement learning, particularly deep
reinforcement learning is who's using it and for what? Have you come across any notable
kind of real-world use cases for deep RL this year?
Yeah, that's a really good question. I think that in general people aren't using it
because these algorithms are they work in some context but they are still a long ways away from
being just like a plug and play thing like like deep learning for example with neural networks
we like with batch norm and and all the and like resnets and stuff like that. We've really figured
out a way to kind of really be able to up deploy these deep neural networks in a wide range of
settings if you have enough data and can actually formulate your problem as a supervised learning
problem. In reinforcement learning I don't think that we're quiet at that stage yet
and there have been some some applications so I know that there are some folks that use it
actually for recommender systems. So like Craig Bhutiliyare for example at Google is someone who's
notably who's been studying reinforcement learning in these sorts of settings. There was also one
example I think a couple years ago of using reinforcement learning for like data center power
management but I haven't really seen that see not really expand yeah at least from from what I've
seen. I think it's also hard to say because it's not necessarily people aren't always public about
the sorts of things the sorts of algorithms are using in industrial applications but my guess is
that they really aren't being used much in real world applications. One of the things that makes
this particular question challenging is you'll see a lot of people talking about
using reinforcement learning but when then you go under the covers it's not deep reinforcement
learning it's like single step kind of traditional reinforcement learning which is different.
Yeah I guess one other thing worth mentioning is that there's a startup company by
led by Peter Biel and others that is looking at robotic automation and they're looking at
deep imitation learning and deep reinforcement learning for doing this. That's covariance.
covariant yeah but I also don't know they are very public about the the kinds of techniques that
they're using or the applications. I recently saw the Mark Hammond who has been on this show he
founded a company called Bonsai that was acquired by Microsoft they're still doing interesting
stuff with RL like I think it is happening but in terms of to your point kind of public
detailed case studies about what folks are doing they are difficult to come by.
Yeah and like Osaro is another startup company that is looking at deep reinforcement learning but
they I don't know they're like if they're actually using it for their for the use cases or not
and there's also I guess another kind of maybe more real-world examples that they're but the not
deep is that there was some work I think a couple years ago by Joel Pinou looking at
reinforcement learning for for brain simulation for seizures to try to stimulate the brain
in a pattern that used less stimulation than kind of a standard thing while also still preventing
seizures but that was using that was by no means using a deep reinforcement learning it was just
using reinforcement learning with smaller models because they didn't have enough data in order to
deploy these techniques with deep networks. Cool anything else we should keep our eyes peeled
for in 2020? I think that that's it I'm excited to see what's to come and then I think that there
is maybe one other thing worth mentioning is that there's been increasingly other labs other
research labs that have really been entering the reinforcement learning space and it's been
exciting to see that people are getting more interested in in the problem setting labs that
were traditionally doing things like supervising and unsupervised learning and I guess I mentioned this
also at the beginning of the podcast. Any particular ones that folks should be
keeping an eye on I mean there are the you know deep mind open AI or you know traditional
folks that have been publishing a lot in this space certainly yourself and Sergei Levine and Peter
Biel and many others you know academic labs what who are some of the new ones that might be worth
taking a look at? Yeah so in addition to the folks that you mentioned some of the kind of existing
ones are like the group brain team in addition to to divine efforts and folk labs at McGill and
Montreal like Joanna Precap and Joel Panou folks at Oxford like Ash Shaman Whiteson and then also
folks at Michigan like Satinder Singh and Hungluck Lee and then also at Stanford is also Emma
Brunskill and I think that these folks study different aspects of the problem for example Emma
has been looking at applications like education and really kind of human impact settings and
really actually focusing on like off policy methods for example while Satinder's group and
Hungluck's group are often looking at more video game applications and Joel has done kind of at
McGill has done a variety of different applications so I think that it's been interesting to see
how different methods are using different approaches and then in terms of newer labs I think that
it's hard to list them but I think that some some maybe people who are probably rather recognizable
in the deep learning community and then are starting to do more reinforcement learning or
folks like Gashua Bengeo and Jan Lecune, Jan it's more on the model-based reinforcement learning
side of things and Gashua I think has been been looking at representation learning in the context
of embodied agents and then also folks like Jimmy Baw for example has had some interesting work
in in model-based reinforcement learning recently and then also Tany Maw at Stanford has
Jimmy is at University of Toronto and then Tany Maw at Stanford has done a lot of really great work
on theoretical deep learning and has been starting to move in towards towards the kind of look at
the theoretical aspects of deep reinforcement learning. Well Chelsea that was a ton of stuff to
cover thanks so much for you know doing this show for helping us get caught up on RL and giving
us a peek into what's coming next. Yeah absolutely my pleasure.
All right everyone that's our show for today for more information on today's guest or for links
to any of the materials mentioned check out twimmelai.com slash rewind 19. Be sure to leave us a five-star
rating and a glowing review after you hit that subscribe button on your favorite podcast catcher.
Thanks so much for listening and catch you next time.
