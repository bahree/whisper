1
00:00:00,000 --> 00:00:12,640
Welcome to the Twimal AI podcast. I'm your host, Sam Charrington.

2
00:00:14,640 --> 00:00:20,080
All right, everyone. I'm here with Fuck Lay. Fuck is a research scientist at Google.

3
00:00:20,080 --> 00:00:29,360
Quack, welcome to the Twimal AI podcast. Hi, everyone. It's great to have you on the show. I've

4
00:00:29,360 --> 00:00:36,160
followed your research for your work for quite some time. And I'm looking forward to digging into

5
00:00:36,160 --> 00:00:42,320
some of the new things that you're working on. But before we do that, I'd love to have you share

6
00:00:42,320 --> 00:00:46,560
a little bit about your background and how you got started working in machine learning.

7
00:00:46,560 --> 00:00:55,680
Okay, so I was born in Vietnam and I did my undergrad in Australia.

8
00:00:58,000 --> 00:01:06,160
And in my second year of my undergrad, I started a summer project, doing machine learning

9
00:01:06,160 --> 00:01:12,640
with Alex Mola, back in Australia. And back then, I was a player that I would call

10
00:01:12,640 --> 00:01:23,360
Toronto Methods. And then I did my PhD at Stanford on a lot of deep learning back in the day

11
00:01:23,360 --> 00:01:41,440
when deep learning was very cool. And that's around 2007. And around 2011, I did a summer

12
00:01:41,440 --> 00:01:50,800
internship at Google. And that was when Google Brain Project was kind of founded. So when I was there,

13
00:01:52,000 --> 00:01:59,360
that was Andrew and Jack Dean and Greg Coraro was there. And I was the intern. So we started

14
00:01:59,360 --> 00:02:07,840
up quite small. That's all right. Yeah. And then I did some of the, you know, scaling up

15
00:02:07,840 --> 00:02:17,840
neural networks with the Google Brain Fox. And then, you know, and then after two years,

16
00:02:17,840 --> 00:02:27,600
I did some work on machine translation with Ilya and Oryo Vinyal. He's now at it mine.

17
00:02:28,720 --> 00:02:35,200
Ilya is now at OpenAI. And we developed some of the N2N translation methods. And

18
00:02:35,200 --> 00:02:45,040
and then around 2016, I started looking into more like, you know, auto-email neural architecture

19
00:02:45,040 --> 00:02:52,160
search. Yeah. And more recently, I looked into more like together with auto-email. I also

20
00:02:52,160 --> 00:02:59,920
looked into semi-subvised learning and so on. Awesome. Awesome. Now you mentioned early on doing

21
00:02:59,920 --> 00:03:06,880
work with Alex Mola. Was he, was this before he was at Carnegie Mellon or was he visiting in

22
00:03:06,880 --> 00:03:17,200
Australia? He was a professor in Australia. Yeah. Yeah. Yeah. I went to a university in

23
00:03:17,200 --> 00:03:26,400
small air in the capital city of Australia. And he was, yeah, camera and he was a professor there

24
00:03:26,400 --> 00:03:35,520
doing research. So I thought, you know, I had, I have been long very interested in AI and machine learning.

25
00:03:37,840 --> 00:03:41,920
And I took, before that I took a class in data mining and so on. And I thought, you know,

26
00:03:41,920 --> 00:03:47,840
it's a little bit boring, but machine learning and the ability to actually learn is actually

27
00:03:47,840 --> 00:03:58,160
super fascinating. So I contacted him and he, he, he, he was doing like auto methods.

28
00:04:00,400 --> 00:04:06,480
Machine learning and and we, we worked together for maybe a few years. Yeah.

29
00:04:07,440 --> 00:04:16,640
Before he went to, he went to America and then CMU and Amazon. Okay. Okay.

30
00:04:16,640 --> 00:04:24,720
Yeah. So a lot of your recent work has been focused on this idea of, you know,

31
00:04:24,720 --> 00:04:30,720
automating machine learning and neural architecture search to allow machines to find the best

32
00:04:30,720 --> 00:04:34,480
deep learning architectures and like, you know, talk a little bit about how you

33
00:04:36,160 --> 00:04:43,120
arrived at working in that area and what some of the motivations were for getting started

34
00:04:43,120 --> 00:04:51,680
digging into that problem. Yeah. Yeah. So I've been long interested in this idea of self-improvement.

35
00:04:52,560 --> 00:05:01,520
Machine should be self-improving itself, machine learning, right? And even, and when I started

36
00:05:01,520 --> 00:05:09,200
doing kernel methods with Alex, I always asked him, you know, how the, the code, the kernel bandwidth

37
00:05:09,200 --> 00:05:17,520
and so on or how some of the hyper parameters in kernel methods decided and, you know, apparently,

38
00:05:17,520 --> 00:05:24,720
they decided by using things like cross validation and so on. And then when I work on kernel methods,

39
00:05:24,720 --> 00:05:30,480
sorry, neural networks, my hope is to make the hyper parameters go away.

40
00:05:30,480 --> 00:05:39,280
Right. But that's how it's the opposite. So if now if you look at a, a convolution neural networks,

41
00:05:39,280 --> 00:05:46,880
it's, it has a lot of hyper parameters, right? Like how many, how many layers you want it to be

42
00:05:47,360 --> 00:05:53,360
and how many channels you want it to be and what are some of the size of hyper parameters since

43
00:05:53,360 --> 00:06:01,120
are like kernel widths and so on. And so, not all the training parameters. Yeah, all the, yeah,

44
00:06:01,120 --> 00:06:08,800
and learning, right? And as researchers develop more and more techniques for neural networks,

45
00:06:08,800 --> 00:06:15,520
there's more decisions that you have to make that feel like, you see, like a problem that

46
00:06:15,520 --> 00:06:27,440
you know, can be helped by a little bit of automation. So in, so I, I, I observe a lot of my

47
00:06:27,440 --> 00:06:32,720
colleagues at Google when they design neural networks and I asked them about the principles of

48
00:06:32,720 --> 00:06:38,000
designing neural networks and, you know, you started out having some really solid principles.

49
00:06:39,360 --> 00:06:45,360
Like you add skip connection so that gradient can flow through the network and so on.

50
00:06:45,360 --> 00:06:51,040
But as you tune the network harder and harder, you no longer have the principle. It's a,

51
00:06:51,040 --> 00:06:57,200
it's around, you know, trial and error, right? You, you try this a little bit and it seems a little

52
00:06:57,200 --> 00:07:05,120
bit better. So you try, you try that more. So I think that that is something that may be ready

53
00:07:05,120 --> 00:07:12,240
for automation. So even during my grad school, I already talked about trying this, but I thought,

54
00:07:12,240 --> 00:07:18,800
you know, maybe we didn't have enough compute because training in that already takes, took me days.

55
00:07:20,400 --> 00:07:26,800
So when I saw that neural, you can train neural networks in 30 minutes or something like that,

56
00:07:26,800 --> 00:07:33,120
you know, from on CIFA, I thought, oh, maybe this is the right time to try this. So that's when I

57
00:07:33,120 --> 00:07:42,640
started doing this neural architecture search in 2016. It's interesting that, you know, even with all

58
00:07:42,640 --> 00:07:48,800
of the compute resources of Google, you had to wait until the time was compressed enough in order

59
00:07:48,800 --> 00:07:57,520
to be able to tackle the problem. Yeah, yeah. Third cell that to get really good results, you want

60
00:07:57,520 --> 00:08:07,280
the networks to be really big and that will take a long time to train. Yeah, and it's funny coming

61
00:08:07,280 --> 00:08:12,400
from me that we have so much resources at Google, but training neural networks is still taking a long

62
00:08:12,400 --> 00:08:23,360
time. Yeah. And so maybe talk about the first steps in that area, did you jump right into

63
00:08:23,360 --> 00:08:32,240
the neural architecture search or was that the, you know, an end stage or a end result of this work?

64
00:08:34,320 --> 00:08:46,240
Oh, well, you know, I work on some of the related ideas on and off since 2012, like Blender

65
00:08:46,240 --> 00:08:51,440
with how to do better hyperparameter tuning for new networks. And none of that is really published

66
00:08:51,440 --> 00:08:57,440
because I didn't have good results and, you know, I didn't have enough compute and so on. So,

67
00:08:58,560 --> 00:09:01,280
so I tried it on and off.

68
00:09:06,880 --> 00:09:14,160
Over the time, you know, every year I would set out some time to try this idea for a few months

69
00:09:14,160 --> 00:09:22,480
and, you know, and it didn't work very well because lack of compute and so on. And then around 2016,

70
00:09:22,480 --> 00:09:30,640
I met Beretsov, who is my colleague now at Google, and he's very talented. So we say, oh,

71
00:09:30,640 --> 00:09:44,560
let's let's try this idea of using like a reinforcement learning to generate a network, like a

72
00:09:44,560 --> 00:09:55,760
little layer in a network for, for a SIFA model. SIFA model is already at that time, you could say

73
00:09:55,760 --> 00:10:01,920
that, you know, train enough a few depends on how, where you want it to be, but, you know, from

74
00:10:01,920 --> 00:10:09,440
30 minutes to a few hours. And that seems like about the right amount of time to get this going.

75
00:10:09,440 --> 00:10:16,240
And my prediction is that you have to train maybe either between from 1000 to 10,000 models. And

76
00:10:18,480 --> 00:10:22,640
I did a backup, our envelope calculation and I thought, oh, this might be the right time to do it.

77
00:10:22,640 --> 00:10:28,560
But, you know, I tried this, some of these related ideas in, you know, much before that.

78
00:10:28,560 --> 00:10:34,000
So you're doing a SIFAR, which is an image recognition for object detection and images.

79
00:10:36,640 --> 00:10:44,240
And so you're doing, you've got this, when you say you're doing,

80
00:10:44,240 --> 00:10:56,400
when you say you're doing tens of thousands of models, is that part of the optimization process

81
00:10:56,400 --> 00:11:01,760
that you're describing here? You're expecting that you need to do 10,000 in order to optimize the

82
00:11:01,760 --> 00:11:10,000
hyper parameters. Yeah. So, so in this process, you have a controller, which is also a machine learning

83
00:11:10,000 --> 00:11:24,000
model. And every time it makes an update, that's basically, it has to, it has to try training one

84
00:11:24,000 --> 00:11:30,880
model to conversion, one SIFAR model, two conversions. And it will take the signal from the

85
00:11:30,880 --> 00:11:38,720
conversions of the, the SIFAR model, you know, maybe the SIFAR model will get 70%. So that 70%

86
00:11:38,720 --> 00:11:45,280
would be used as a signal to make one update for them controller, one update, right? So,

87
00:11:46,000 --> 00:11:51,920
typically machine learning models take a long time to train. So it requires, you know, 10,000

88
00:11:51,920 --> 00:11:56,560
of updates. So that's basically the number of models that we had to try.

89
00:11:57,520 --> 00:12:04,240
Were your initial attempts at this doing, you know, how do you distinguish between like,

90
00:12:04,240 --> 00:12:12,320
doing your hyper parameter optimization and kind of architecture search, because there is a varying

91
00:12:13,440 --> 00:12:19,680
degree of complexity in trying to come up with these new architectures. Yeah, and your more

92
00:12:19,680 --> 00:12:27,280
recent work on this is like using evolutionary algorithms and a like to do this. Can you maybe talk

93
00:12:27,280 --> 00:12:35,920
through kind of the progression of complexity that you went through? Yeah, so the first project that

94
00:12:35,920 --> 00:12:48,800
we did was architecture search for like a SIFAR model. And that's already was already very

95
00:12:48,800 --> 00:12:56,560
expensive back when we did it. And we didn't choose hyper parameters. You only, in other words,

96
00:12:56,560 --> 00:13:05,520
we didn't choose hyper parameters like learning rate or way decay or dropout, which is focused on

97
00:13:05,520 --> 00:13:14,000
architectural hyper parameters, basically, you know, number players. What kind of player do you use

98
00:13:14,000 --> 00:13:21,520
in what stage in the network? And that's already took us like almost a week for every time we try

99
00:13:21,520 --> 00:13:30,240
this, it takes like a week on a few hundred GPUs. So after that, we moved to ImageNet. And because

100
00:13:30,240 --> 00:13:38,400
ImageNet, the network is bigger, we decided to use this idea called transfer learning, which is

101
00:13:38,400 --> 00:13:46,880
basically find a module that works well on SIFAR 10 and transfer to ImageNet, because searching

102
00:13:46,880 --> 00:13:55,360
directly on ImageNet would be very difficult. This is very expensive. Now in parallel with that,

103
00:13:55,360 --> 00:14:02,160
we also try a lot of methods in not only in reinforcement learning and but also in evolution.

104
00:14:04,240 --> 00:14:10,560
And we also observe the evolution that does as well or even better than reinforcement learning.

105
00:14:10,560 --> 00:14:21,680
So we slowly adopt more, sorry, evolutionary methods. And then, so some of the first

106
00:14:21,680 --> 00:14:26,320
of any intuition for why evolution works better than reinforcement learning?

107
00:14:28,080 --> 00:14:37,760
Oh, I see. So evolution is very flexible and very easy to implement, right? For example,

108
00:14:37,760 --> 00:14:45,520
in evolution, you just need to decide mutation and cross-over mutation meaning you have a network

109
00:14:45,520 --> 00:14:52,080
and then it just changes a little bit and across-over meaning take two networks and make them.

110
00:14:52,640 --> 00:15:00,960
So implementing evolution is actually quite easy. Now in contrast, reinforcement learning,

111
00:15:00,960 --> 00:15:07,200
because we're not experts in reinforcement learning. So we have, we try a lot of reinforcement

112
00:15:07,200 --> 00:15:13,520
learning methods like we started out with reinforced and then we tried something like PPO and so on

113
00:15:13,520 --> 00:15:21,600
and TIPO recently. And they're good, but they also require a fair bit about tuning to get working

114
00:15:21,600 --> 00:15:30,720
very well. So on the other hand, evolution seems to be very flexible in terms of implementation.

115
00:15:30,720 --> 00:15:38,000
And it's also wanting that evolution does quite well is that it's easy to diversify

116
00:15:39,840 --> 00:15:47,920
the models. So you can just try to, in reinforcement learning, once it happens, it will zoom in

117
00:15:47,920 --> 00:15:57,520
into a particular area, area in the surface and optimize the particular model. Whereas in evolution,

118
00:15:57,520 --> 00:16:04,560
it will diversify the population over time. It's easy to control the diversification process

119
00:16:04,560 --> 00:16:17,840
to get better models. So we use more evolution methods now and then feed forward.

120
00:16:19,840 --> 00:16:24,720
Okay, so in the second project, we already found one network that was kind of stay-of-the-art

121
00:16:24,720 --> 00:16:36,080
in computer fusion on power or slightly better on the on the stay-of-the-art of image net.

122
00:16:36,080 --> 00:16:41,600
So that was super exciting because we didn't think that it was possible. And then

123
00:16:44,080 --> 00:16:50,640
after that, we realized that this transfer learning has a problem that you know, you transfer the

124
00:16:50,640 --> 00:16:58,160
cell. And maybe the type of things that you want on cipher and the kind of cells that you want

125
00:16:58,720 --> 00:17:05,840
for image net is quite different. So we started searching on image net directly. Basically,

126
00:17:05,840 --> 00:17:11,440
you search a model on image net directly, but that became super expensive. You know, our

127
00:17:11,440 --> 00:17:18,160
back-up envelope calculation will take a few months for this to finish. So we realized that maybe

128
00:17:18,160 --> 00:17:24,480
one idea that we can have is search on small scale. Search a smaller model on cipher. Let's say,

129
00:17:25,280 --> 00:17:32,880
is that of searching the biggest model possible that you could find? Search for a small model,

130
00:17:32,880 --> 00:17:40,080
like that train only like five epochs in, you know, eight hours or something like that, right?

131
00:17:40,080 --> 00:17:47,600
So that's small. And after that, after we found a good model, we figure out a way to scale it up to

132
00:17:47,600 --> 00:17:57,200
be a size. So basically, make it new with larger image or make it deeper or make it wider.

133
00:17:58,400 --> 00:18:05,120
Is that scaling up in a learning way or scaling it up? Yeah, scaling up in a learning way.

134
00:18:05,120 --> 00:18:10,720
Okay. We're scaling up in a learning way. So the second stage of scaling up, basically,

135
00:18:10,720 --> 00:18:18,480
what we did was, you know, develop like a, we learned the way that we should be scaling up.

136
00:18:21,120 --> 00:18:27,200
And it looks like it works very well and that became something called efficient. Now it's been

137
00:18:27,200 --> 00:18:36,320
used quite a bit in various places at Google and in academia. And the smallest network that we

138
00:18:36,320 --> 00:18:43,920
found turns out to be super helpful for mobile devices. So people, because the network, small network

139
00:18:43,920 --> 00:18:51,280
seemed to be quite fast for mobile devices. That became something like a mobile network V3.

140
00:18:53,600 --> 00:19:01,120
Yeah, at Google. And after that, we say, you know,

141
00:19:01,120 --> 00:19:09,280
okay, now that we can get stay of the art on image net, but the problem is that a lot of

142
00:19:09,280 --> 00:19:16,640
building blocks that we used are very much, you know, building blocks that pre-describe or

143
00:19:18,240 --> 00:19:24,720
pre-describe by human experts. Let's say we have to make use of the rail law layer design,

144
00:19:24,720 --> 00:19:32,320
but design by human experts or we made use of a convolutional layer design by human experts

145
00:19:32,320 --> 00:19:39,920
or batch norm layer design by human experts. So we say, can we, can we design everything from

146
00:19:39,920 --> 00:19:48,320
scratch, basically assume that we know a non-pile library, like non-pile is this basically just,

147
00:19:48,320 --> 00:19:54,640
you know, matrix vector multiplication and bunch of non-linearity. Can you use a non-pile library

148
00:19:54,640 --> 00:20:02,320
to evolve the concept of machine learning? And that became something like auto-email 0.

149
00:20:03,280 --> 00:20:08,640
Which basically, yeah, that's auto-email 0. And that's basically the thought process behind it.

150
00:20:09,680 --> 00:20:16,080
In auto-email 0, 0, we didn't get stay of the art yet. But what's exciting about it is,

151
00:20:16,080 --> 00:20:26,640
it generate a program from just matrix vector multiplication and to do machine learning,

152
00:20:28,000 --> 00:20:33,360
which is super exciting. And I hope that using this method we can discover

153
00:20:34,560 --> 00:20:40,640
fundamental new fundamental new building blocks for machine learning.

154
00:20:40,640 --> 00:20:48,880
Yeah, folks haven't, if anyone listening hasn't taken a look at any of the blog posts or the paper

155
00:20:48,880 --> 00:20:56,320
for auto-email 0, really interesting. There's one particular diagram that I've seen a couple

156
00:20:56,320 --> 00:21:03,760
different versions of it, but it kind of walks through the process that this algorithm takes to learn

157
00:21:03,760 --> 00:21:13,600
a model and shows the various steps that it introduces and as well as the program that it outputs.

158
00:21:14,560 --> 00:21:18,000
And it's super interesting, you know, talk a little bit about this idea of,

159
00:21:19,360 --> 00:21:25,360
you know, having this model work by evolving a program. Where did that come from?

160
00:21:25,360 --> 00:21:34,880
So, go ahead, sorry. I was just going to say, you know, a lot of what we're doing is all software,

161
00:21:34,880 --> 00:21:40,080
it's all programs, but this in particular is like arithmetic arithmetic operations that

162
00:21:41,520 --> 00:21:49,040
in a very kind of simple way define all the steps that are used to evolve these algorithms.

163
00:21:49,040 --> 00:22:03,920
Yes, yes. So, if you think about what computer, sorry, machine learning experts are doing now

164
00:22:03,920 --> 00:22:08,400
are they? Is that basically they look at a computer program, you know, they use TensorFlow, right?

165
00:22:09,120 --> 00:22:14,480
They look at TensorFlow or PyTorch and they have a bunch of players and then they figure out

166
00:22:14,480 --> 00:22:22,720
to develop to write a computer program to write a new program to do machine learning.

167
00:22:22,720 --> 00:22:29,600
Let's say you want to do forecasting or something like that, right? Basically, you look at

168
00:22:30,560 --> 00:22:37,680
how people use LSTM and then you put together a computer program to do forecasting.

169
00:22:37,680 --> 00:22:46,560
Now, the act of writing that program is still now not learned, right? So, basically, it's basically

170
00:22:46,560 --> 00:22:52,720
human expert knowledge and changing that program can affect the quality of the model greatly.

171
00:22:53,520 --> 00:23:02,000
Now, stepping back a little bit is that that program, the auto and the program you just put

172
00:23:02,000 --> 00:23:09,040
assume a lot of knowledge about machine learning, right? The fact that the concept of gradient, you know,

173
00:23:09,040 --> 00:23:17,120
by propagation is assumed during this process, right? Because the different automated differentiation

174
00:23:17,120 --> 00:23:24,800
is built in into TensorFlow and PyTorch. So, we said that maybe what we can do is

175
00:23:24,800 --> 00:23:35,120
to step back a little bit from PyTorch and TensorFlow and start it from NumPy and using NumPy,

176
00:23:35,120 --> 00:23:45,200
can you put together a small computer program that can do, you know, SIFA classification or something

177
00:23:45,200 --> 00:23:54,800
like that? Yeah, and in the setup that we have, we have three functions. So, one function is

178
00:23:56,160 --> 00:24:05,120
setup, meaning that, you know, it's like a DNA that you start with, right? And then there's a

179
00:24:05,120 --> 00:24:12,240
predict function, meaning that whatever you have learned, you have to use it to force survival,

180
00:24:12,240 --> 00:24:18,480
right? So, you have to make, you've given a given situation view in, you have to make some

181
00:24:18,480 --> 00:24:25,840
prediction. And then the third function is the learn function. You know, it's, it has to learn

182
00:24:25,840 --> 00:24:32,240
so that the predict function is better over time. So, there's only this template have only three

183
00:24:32,240 --> 00:24:42,720
functions, setup, predict, and learn. And AutoML0 has to fill in the rest of the program,

184
00:24:43,440 --> 00:24:48,640
the rest of the instruction within these building blocks. Right, it's starting with those

185
00:24:48,640 --> 00:24:54,800
three functions being totally empty. Yeah, it started out with these three functions totally empty.

186
00:24:54,800 --> 00:25:01,520
So, at the beginning, it will start it, you know, amazing, right? At the beginning, it will do

187
00:25:01,520 --> 00:25:08,080
nothing. So, most of the programs will be garbage. So, you have no signal at all.

188
00:25:09,440 --> 00:25:16,080
You have no signal at all. So, by some random block, right? It will find some kind of

189
00:25:18,240 --> 00:25:27,360
dot product linear kind of layer that somehow does more than better than random. Just slightly a

190
00:25:27,360 --> 00:25:33,120
little bit, sorry, better than random, right? That's just basically the predict function does

191
00:25:33,120 --> 00:25:41,200
a little bit better than random. And then it will basically slowly put together one more,

192
00:25:41,840 --> 00:25:47,360
one more layer to become like a neural net. And then it will invent the concept of gradient.

193
00:25:47,360 --> 00:25:54,240
So, over time, it will come, it started from a very small program that is like do linear to

194
00:25:54,240 --> 00:26:02,000
go through many, many steps to eventually do a neural network. Yeah, and I referenced this

195
00:26:02,000 --> 00:26:08,240
diagram. If I'm understanding the diagram correctly here, identifying all these points where

196
00:26:09,120 --> 00:26:17,840
the algorithm evolves these techniques that, you know, humans do now. Like it eventually figures out

197
00:26:17,840 --> 00:26:25,760
how to do SGD. It eventually figures out like Rayloo and other things and it figures out techniques

198
00:26:25,760 --> 00:26:29,840
like gradient normalization and stuff. Am I reading that correctly that this is all stuff that

199
00:26:30,880 --> 00:26:37,120
has figured out? Yeah, yeah, yeah. So, basically, if you put it in the whole sequence,

200
00:26:37,840 --> 00:26:44,080
you know, the first step it will find like something like linear model, it find logic clipping,

201
00:26:44,080 --> 00:26:56,240
it will find learning rate, and it will find Rayloo, you know, and then normalizing the input norm,

202
00:26:56,240 --> 00:27:05,760
the gradient, and then, you know, doing having malinear interaction, things like that. So, things

203
00:27:05,760 --> 00:27:14,080
that, you know, like over the time in the last maybe 30 years of neural networks evolution.

204
00:27:14,800 --> 00:27:20,720
Just so that I understand there's no, there's no kind of priors, there's no like

205
00:27:22,320 --> 00:27:28,960
recipe book or techniques that are given to the model, the algorithm, and all it's figuring out

206
00:27:28,960 --> 00:27:40,240
all of this from nothing. Yeah. So, so the caveat is that AutoML 0 has access to 64 functions

207
00:27:42,080 --> 00:27:49,120
from Lampai. Okay, that's there's some bias here. So, this 64 function from Lampai,

208
00:27:50,960 --> 00:27:57,040
because the way that linear algebra works is very in favor of neural nets, right? So it will

209
00:27:57,040 --> 00:28:02,800
tend to develop things like neural nets, because linear algebra. But that's the only thing.

210
00:28:02,800 --> 00:28:06,720
There's no product there for it to use, eventually it's going to try and use it on some stuff.

211
00:28:08,480 --> 00:28:13,040
But it would be hard for it to find something like trees, because, you know,

212
00:28:13,680 --> 00:28:17,760
Nampai doesn't have the functions that kind of suitable for trees.

213
00:28:19,600 --> 00:28:24,880
But it's because Nampai, you can argue that Nampai is like a library that's very suitable for

214
00:28:24,880 --> 00:28:29,280
neural nets, right? So, it will evolve things that eventually look like a neural net.

215
00:28:31,600 --> 00:28:36,160
Now, what's surprising is that it did the whole process of developing models that started from

216
00:28:36,160 --> 00:28:44,240
linear and then put in development learning rate and normalizing the gradient and things like that.

217
00:28:44,240 --> 00:28:50,720
It looks very much like the evolution, our own evolution process of developing machine learning

218
00:28:50,720 --> 00:29:03,120
models. And so do you have you, have you, has this allowed you to see future techniques that we

219
00:29:03,120 --> 00:29:13,280
may learn to apply? Yes, but we haven't found anything extremely novel in the sense that like we

220
00:29:13,280 --> 00:29:19,280
never, we haven't seen it before. But we haven't found something that we haven't looked into

221
00:29:19,280 --> 00:29:29,520
more closely. So, in particular, it found this bilinear layer that normally, if you do a neural net,

222
00:29:29,520 --> 00:29:39,360
you would take X, multiply by some W, and then you apply some nonlinearity. Now, what we found

223
00:29:39,360 --> 00:29:54,560
during AutoML 0 is that it prefer X, WX. So, and then apply some nonlinearity. So, that basically,

224
00:29:54,560 --> 00:30:03,120
what they say is some kind of malinear interaction, right? And apparently this concept has been developed

225
00:30:03,120 --> 00:30:14,240
by other colleagues at Google. But we never, we didn't know that before project. So, we are in the

226
00:30:14,240 --> 00:30:22,000
process of trying out this layer on larger problems. But it looks like that layer is actually quite

227
00:30:22,000 --> 00:30:30,240
promising. Interesting. Yeah. So, the other thing is the concept of gradient normalization. So,

228
00:30:30,240 --> 00:30:36,960
basically you take the gradient and then you normalize it before you make the update. So, this is not

229
00:30:36,960 --> 00:30:40,640
not something new. So, people have done this before, but it's not very popular.

230
00:30:43,520 --> 00:30:54,000
And I think maybe one thing is that we are also trying this on bigger networks now ourselves.

231
00:30:54,000 --> 00:31:02,880
But if you can think about this process, it will aid the process of discovering either new idea

232
00:31:02,880 --> 00:31:08,720
or discovering older ideas, but we actually overlook because we have so many ideas in

233
00:31:08,720 --> 00:31:17,520
machine learning that we tend to overlook them. So, but some of the recent data is look promising

234
00:31:17,520 --> 00:31:24,080
or some of the ideas that it found. Do you think there's an opportunity to use a technique similar

235
00:31:24,080 --> 00:31:32,160
to what you did earlier with CFAR, where you apply AutoML 0 in a small way and then scale it up

236
00:31:32,160 --> 00:31:40,240
to bigger problems or networks? Yeah, I still we still thinking about how to do it effectively,

237
00:31:40,240 --> 00:31:47,920
but basically, basically you're right. So, the problems that we did in AutoML 0 is like a,

238
00:31:47,920 --> 00:31:55,840
it's not even CFAR, it's a Dow scale CFAR. It's a small version of CFAR. So, and then you know,

239
00:31:55,840 --> 00:32:09,360
the concept like gradient normalization or things like, you know, malinia interaction is something

240
00:32:09,360 --> 00:32:14,160
that it found and then we can take some of these layers and transfer into big problems. Now,

241
00:32:14,160 --> 00:32:20,560
unfortunately, the problem in CFAR, it's so downscale that you we cannot present it like an image.

242
00:32:20,560 --> 00:32:25,840
So, it is only 1D. And in 1D, you cannot find things like convolution on neural networks.

243
00:32:26,480 --> 00:32:34,080
So, that's a that's a limitation, but in the next step, we try to make it have like make it see an

244
00:32:34,080 --> 00:32:42,800
image rather than just a 1D image. Now, that's that's one aspect, which is basically how to scale

245
00:32:42,800 --> 00:32:49,040
this more effectively, which we did it before. The other thing that we did is to zoom in into a

246
00:32:49,040 --> 00:32:54,560
particular aspect of the neural network and can you do better. So, related to this is the

247
00:32:55,680 --> 00:33:02,720
paper that we published, you know, a couple days ago, on evolving a better activation and

248
00:33:02,720 --> 00:33:12,080
normalization layer. So, basically, in a neural network, people use this layer called BASHNOM

249
00:33:12,080 --> 00:33:19,760
and RELU a lot, right? This is in Dresnet. If you use Dresnet, you have BASHNOM and RELU

250
00:33:19,760 --> 00:33:25,680
and then there's a skip connection, right? And we say, let's use this into a single layer and

251
00:33:25,680 --> 00:33:34,240
search for a new mathematical operation to replace this BASHNOM and RELU. So, we accept the rest

252
00:33:34,240 --> 00:33:43,360
of the network, but we search for a mathematical operation from to to find a new layer. And it

253
00:33:43,360 --> 00:33:51,520
seems like to find a very good layer as a replacement and it works better than BASHNOM and RELU.

254
00:33:51,520 --> 00:33:58,880
And is the motivation there, primarily, network performance or is it computational or

255
00:34:00,000 --> 00:34:09,360
what is driving you to focus on those particular layers? So, BASHNOM and RELU, one thing the BASHNOM

256
00:34:09,360 --> 00:34:15,360
RELU is a good thing about it is it allows you to train with very big batch size, right? But when

257
00:34:15,360 --> 00:34:21,040
it's very small batch size, it doesn't work very well. So, it's a layer that Google would like,

258
00:34:21,040 --> 00:34:28,400
but most data scientists would not like because you don't have a big computer to train with a

259
00:34:28,400 --> 00:34:34,480
big batch. So, there's a replacement called Group NOM and RELU, which is very good, but it works

260
00:34:34,480 --> 00:34:44,400
very well on small batch size, but on the big batch size it's still a little bit worse than BASHNOM.

261
00:34:44,400 --> 00:34:55,520
So, it's still confusing to many people what layers to use, even at Google. So, developing a new layer

262
00:34:55,520 --> 00:35:01,360
is that, first of all, BASHNOM RELU is if it can be a good replacement for BASHNOM and RELU,

263
00:35:01,360 --> 00:35:07,280
that means the layer can be used by both internal researchers and external researchers.

264
00:35:07,280 --> 00:35:13,440
Well, and the second thing is BASHNOM RELU helps training, it stabilizes the training a lot,

265
00:35:13,440 --> 00:35:22,240
it speed up the training. So, we use it a lot in our work, so we really want to improve on that

266
00:35:22,240 --> 00:35:29,120
aspect. It plays a huge role. There's a paper where they just say that you just train only the

267
00:35:29,120 --> 00:35:37,920
BASHNOM layers and don't train the convolutional nets and you still can get a good performance.

268
00:35:37,920 --> 00:35:44,400
You know, it's not great, but it's good performance. It means that these layers play a very good

269
00:35:44,400 --> 00:35:52,480
important role. And is that paper? Talking about training those layers, only those layers from scratch

270
00:35:52,480 --> 00:35:59,760
or fine-tuning only those layers? Training those from scratch. Yeah. And so,

271
00:36:00,720 --> 00:36:06,480
is the idea that you're applying techniques like what you've done at AutoML Zero to finding

272
00:36:06,480 --> 00:36:17,840
these new layers or is that a totally separate approach? Oh, it's highly related. And I would say,

273
00:36:17,840 --> 00:36:26,560
you know, it's like an application of this idea of AutoML Zero. Yeah. Yeah.

274
00:36:27,520 --> 00:36:32,720
Cool. So, you've also been working on semi-supervised or self-supervised

275
00:36:34,720 --> 00:36:42,160
learning recently. Yes. Can you describe some of that work and how it relates to this stuff?

276
00:36:42,160 --> 00:36:48,960
Yeah. Sure. So, I've been working on this automated machine learning and

277
00:36:50,160 --> 00:36:58,240
automated architecture design and so on. And I gave talks about this new development and a lot

278
00:36:58,240 --> 00:37:02,320
of people came to me and complained. They say, you know, you automate the design of the neural networks,

279
00:37:02,320 --> 00:37:12,960
but I have more data than you. So, I bid you. So, I say, oh, that's a good point. So, I thought about,

280
00:37:13,600 --> 00:37:19,840
okay, automate machine learning is cool, but can you automate the labeling process?

281
00:37:21,200 --> 00:37:27,040
Can you automate labeling, right? Because most people would prefer to have more data.

282
00:37:27,040 --> 00:37:33,280
Because more data is very important, right? I don't mean that architecture is not important,

283
00:37:33,280 --> 00:37:41,040
but having more data is also very important. So, the question is, can you automate the process

284
00:37:41,040 --> 00:37:48,560
of labeling data? So, today, you don't have anything. Today, you basically get some

285
00:37:48,560 --> 00:37:53,120
unlabeled data and you give it to some human experts and then they label the data for you, annotate

286
00:37:53,120 --> 00:37:56,960
the data for you. So, you get techniques like active learning that can help you

287
00:37:56,960 --> 00:38:01,760
out the best labels, the best data to label, which helps. That's right. Yeah, active learning

288
00:38:01,760 --> 00:38:08,160
will speed up that process by selectively choose the example to annotate the data.

289
00:38:09,120 --> 00:38:18,240
Now, so, one idea that we had with the concept of pseudo labels, so fake labels,

290
00:38:18,240 --> 00:38:26,800
can you take your model and generate and evaluate on the unlabeled set?

291
00:38:28,160 --> 00:38:33,360
And now you have weekly label data. Our observation is this, right? Like, can you take your own model?

292
00:38:34,560 --> 00:38:39,200
Generate labels on a new set of unlabeled data and then put it back,

293
00:38:39,920 --> 00:38:45,440
assuming that they are correct labels and train the new model on that. Well, actually,

294
00:38:45,440 --> 00:38:52,880
I tried this idea many, many years ago too and it didn't work. And the problem is that if you

295
00:38:53,600 --> 00:39:01,840
take the model and generate the new label data, the new weekly label data, some of them are

296
00:39:01,840 --> 00:39:07,200
accurate, you know, a three would get a three, like that label of three, but sometimes a three

297
00:39:07,200 --> 00:39:14,960
would get a label of five. And this error would propagate, propagate into the new training and it would hurt

298
00:39:16,080 --> 00:39:20,000
the new training and the new training would not get better result than the old training.

299
00:39:21,440 --> 00:39:24,160
Right? Because it's the confirmation bias going on.

300
00:39:26,400 --> 00:39:32,800
Now, you, so I did not know any way to fix that problem.

301
00:39:32,800 --> 00:39:43,680
So, so I thought the concept of self labeling is a, is too good to be true. But recently,

302
00:39:43,680 --> 00:39:51,760
we realized that there's a way to overcome this process is when you train the new training,

303
00:39:51,760 --> 00:39:59,760
you inject a lot of noise into the new student. So that's, so you have a teacher that generate

304
00:39:59,760 --> 00:40:07,760
labels on a label data. You have a combined set of true label and weekly label data

305
00:40:09,040 --> 00:40:14,720
or pseudo label data and you train new students on this new combined set. When you train the

306
00:40:14,720 --> 00:40:23,360
student, make sure that you insert a lot of noise. So the a lot of noise in the student will,

307
00:40:23,360 --> 00:40:31,600
we still don't know this, how this happened yet. But the noise in the students seems to have

308
00:40:31,600 --> 00:40:38,960
this process that will overcome the confirmation bias. Probably because it will make the student

309
00:40:38,960 --> 00:40:44,560
more robust, right? Because it has, it has new, not trust the labels all that much. Exactly.

310
00:40:45,280 --> 00:40:49,840
Right? So it learned not to trust the label that much because it has to cope with so much noise.

311
00:40:49,840 --> 00:41:00,320
And amazingly, it actually outperformed the teacher. So the noise that we use is basically things like,

312
00:41:00,320 --> 00:41:08,720
you know, drop out and drop certain parts of the model, data augmentation, and super aggressively

313
00:41:08,720 --> 00:41:15,680
doing this data augmentation and noise. And eventually it would do better than the student.

314
00:41:15,680 --> 00:41:22,720
And you can just iterate the process, right? Once you have a better student, you label new data

315
00:41:22,720 --> 00:41:27,840
and then you put back. So we keep doing this and it seems to work very well.

316
00:41:28,720 --> 00:41:34,080
And how what's your performance metric or your benchmark?

317
00:41:34,080 --> 00:41:50,480
Yeah. So we worked on this data set called ImageNet. So I think when we work on this data set,

318
00:41:50,480 --> 00:41:58,080
the say of the art was like 82% and then using architecture search, we've pushed it into 85%,

319
00:41:58,080 --> 00:42:08,880
85.4% or something like that. And then using this auto label process, you get to 88.4. So

320
00:42:08,880 --> 00:42:20,960
3% improvement. So and keep in mind that 1% improvement on ImageNet at that high range is very

321
00:42:20,960 --> 00:42:28,480
difficult. And I'm talking about top one accuracy. Okay. And so is this, are you,

322
00:42:30,160 --> 00:42:38,320
is the setup here that you are you starting with your standard kind of 70% training and 30%

323
00:42:39,600 --> 00:42:48,400
test ratio or does that matter in this? Oh, so you're having your student

324
00:42:48,400 --> 00:43:00,320
label that 30% like did these ratios come into play into this? Okay. So we just follow the

325
00:43:00,320 --> 00:43:06,960
conventional ImageNet setup. So ImageNet has already had a split of maybe 1.2 million

326
00:43:06,960 --> 00:43:15,520
example of training and 100,000 for validation. And for other label data, you would use a different

327
00:43:15,520 --> 00:43:22,320
compass. So at Google, we have this data set called JFT that has about 300 million images.

328
00:43:24,240 --> 00:43:35,280
And we are operating other data beyond ImageNet that you've labeled using a model trained on ImageNet

329
00:43:36,160 --> 00:43:41,120
that you don't have any labels for. I mean, meaning that you don't have any labels for your

330
00:43:41,120 --> 00:43:46,240
external data set. You're just labeling it based on ImageNet. Some of those are going to be wrong.

331
00:43:46,240 --> 00:43:53,520
So you introduce the noise and it all seems to. Yeah. Yeah. So basically, yeah. So here's something

332
00:43:53,520 --> 00:43:58,800
that we find really surprising. There's one experiment in the paper that people did not check.

333
00:43:58,800 --> 00:44:04,800
But it's super exciting. Interesting is that we can propagate back images that so, you know,

334
00:44:04,800 --> 00:44:12,720
images has a 1,000 categories, right? Like, you know, some flowers, some dogs, and some cats,

335
00:44:12,720 --> 00:44:19,520
and so on. Now, we propagate back images that don't have, that don't look like any,

336
00:44:20,240 --> 00:44:26,320
like any categories in 1,000 category. So it could be like some kind of very strange animal.

337
00:44:27,200 --> 00:44:32,400
And the model which is just like us is the ImageNet. Yeah. It doesn't belong to any,

338
00:44:32,400 --> 00:44:42,000
yeah, any categories. And it still helps. Just by saying that, you know, this image is not any of

339
00:44:42,000 --> 00:44:47,360
these categories. And this put a lot of low, low percentage, low probability on a lot of these

340
00:44:47,360 --> 00:44:53,920
categories from propagate back is still okay. And that consistent with the consistent maybe with

341
00:44:53,920 --> 00:45:02,160
the idea that the images are even if they're not, you know, helping turn your classifier layers

342
00:45:02,160 --> 00:45:09,040
at the end, they're helping the network learn textures and, you know, low level features better.

343
00:45:10,000 --> 00:45:13,760
Yeah. So I think what really happened is that it just tried to learn

344
00:45:15,120 --> 00:45:22,640
information about natural images, right? And the fact that, basically, you give it a label.

345
00:45:22,640 --> 00:45:26,960
So the teacher give it a label. And then when you do data augmentation, you ship the image

346
00:45:26,960 --> 00:45:35,040
a little bit. Right. And then you say that the label is the same. So, you know, the probability table

347
00:45:35,040 --> 00:45:42,320
looks very similar. So the model has to work really hard to stay consistent in the production.

348
00:45:42,320 --> 00:45:50,240
So that's, and natural image in general is just very similar in similar ways, right? So it learns

349
00:45:50,240 --> 00:45:57,200
to be consistent in terms of labeling for new images. And maybe that's the reason why it's

350
00:45:57,200 --> 00:46:03,040
very helpful. Even though the images might not have anything to do with your other set.

351
00:46:04,320 --> 00:46:07,120
And you know, I'm curious in kind of articulating that

352
00:46:08,480 --> 00:46:14,320
intuition for what's happening is that based on, you know, all of your experience working with

353
00:46:14,320 --> 00:46:20,080
these kind of networks. So did you perform specific experiments to try to understand, you know,

354
00:46:20,080 --> 00:46:23,920
what the effect might be and, you know, what were those experiments?

355
00:46:25,600 --> 00:46:31,440
Oh, okay. So network introspection or any kinds of things. I'm just curious, you know,

356
00:46:31,440 --> 00:46:36,560
one of the kinds of things you've done, you know, to deepen your understanding of why this is working.

357
00:46:37,280 --> 00:46:43,040
Oh, you mean this particular experiment or in general?

358
00:46:43,040 --> 00:46:50,560
This particular experiment. Okay, so we try to lower the threshold, right? So when you take

359
00:46:50,560 --> 00:46:58,720
the your model and then label a new, a compass of 300 million images, we feel that we have a

360
00:46:58,720 --> 00:47:03,840
chance to feel that our low confidence images, right? So things that, you know, have very low

361
00:47:03,840 --> 00:47:13,280
prediction, low probability of having to be in any way of the class. So we can keep, you know,

362
00:47:13,280 --> 00:47:20,880
only 10,000 images or we can keep 30, sorry, 1 million images or 30 million images or,

363
00:47:20,880 --> 00:47:27,520
you know, 300 million images or anywhere in between, right? So we vary the threshold.

364
00:47:27,520 --> 00:47:38,720
And it seems like actually the threshold can be quite really low, right? So it could be, you know,

365
00:47:38,720 --> 00:47:44,480
in the 130 million or something like that, it's still okay. And then we visualize the image that

366
00:47:44,480 --> 00:47:53,920
are actually very low accuracy and a low confidence. And then we visualize them and see what they

367
00:47:53,920 --> 00:47:59,120
look like. And they don't, apparently, they don't look like anything like on image net. So that's what

368
00:47:59,120 --> 00:48:06,400
we found why the fact that they're helpful is very surprising. But why they are helpful, we still

369
00:48:06,400 --> 00:48:14,320
don't know. So is this a hypothesis? We don't, we don't know. Yeah. Yeah. Yeah. I, which is basically

370
00:48:14,320 --> 00:48:21,920
our current work is trying to analyze why it's helpful. Okay. And what do you expect your

371
00:48:21,920 --> 00:48:32,640
direction to be in analyzing that? We're probably going to look into the hidden state of the

372
00:48:32,640 --> 00:48:37,280
neural network. And you see, we see, you know, with these low confidence,

373
00:48:41,040 --> 00:48:46,960
where is it, is it trying to make the hidden state more consistent to each other,

374
00:48:46,960 --> 00:48:52,000
which is basically the same phenomenon that a lot of people do in contrastive learning in

375
00:48:52,000 --> 00:48:57,280
self-supervised learning is that they also have two images of data augmented image. And they're

376
00:48:57,280 --> 00:49:03,520
trying to make sure that the prediction is the same. And if you have two images of unrelated

377
00:49:03,520 --> 00:49:09,840
images, you make sure the prediction is different. Right? So I think this is what happened in here.

378
00:49:09,840 --> 00:49:14,800
So we're going to visualize some of the hidden state of the neural network with and without

379
00:49:14,800 --> 00:49:21,040
these low-confident example and see, you know, what happened during training. Okay.

380
00:49:21,040 --> 00:49:25,360
That's that's one direction that we think that we will be doing. Okay. Okay. Yeah.

381
00:49:27,120 --> 00:49:33,840
Cool. You were also an author on Mina. Yes. Were you involved in that? Can you talk a little

382
00:49:33,840 --> 00:49:41,280
bit about that work? Yeah. So many years ago, I worked on something called sequence learning,

383
00:49:41,280 --> 00:49:49,440
and to end neural networks through NLP. And that's still for translation. And I spent like two

384
00:49:49,440 --> 00:49:54,960
years after that trying to be like a chatbot to chat with me, because I always fascinate like,

385
00:49:54,960 --> 00:49:59,760
can you have an agent that can talk to me, you know, in the intelligence? I've been all your emails

386
00:49:59,760 --> 00:50:06,240
and you know, there's a down mic here or were you trying to, were you trying to have a communication

387
00:50:06,240 --> 00:50:10,480
with the chatbot or were you trying to replace yourself, quote unquote, with the chat by having

388
00:50:10,480 --> 00:50:18,720
other people be able to talk to you? Both. Both. You know, just talking to computers is fascinating.

389
00:50:18,720 --> 00:50:27,760
Yeah. Yeah. And so I failed. And then we, I, why, and then we ended up meeting this person

390
00:50:27,760 --> 00:50:35,520
called at Google and a very great engineer at Google called Daniel. And he, he said, how about

391
00:50:35,520 --> 00:50:42,160
let's work together to make this better. And we did a lot of work on scaling up some of the models

392
00:50:42,160 --> 00:50:50,400
that we built. We collected a huge amount of social media data, you know, people talking on the

393
00:50:50,400 --> 00:50:56,560
internet. And then we train a huge model, like a model that I, I, you know, maybe a hundred times

394
00:50:56,560 --> 00:51:07,120
because of what I train back in the day. And it can do multi-tone conversations. It started doing

395
00:51:07,120 --> 00:51:14,720
multi-tone conversation very well. And one of the magic moments that I really think that

396
00:51:14,720 --> 00:51:26,080
truly magic is that it actually invented like a joke. It invented a joke. So people talk, it's, it's, it

397
00:51:26,080 --> 00:51:38,960
made this pun that, you know, horses go to Harvard. So cows go to Harvard, cows go to Harvard. And horses

398
00:51:38,960 --> 00:51:47,440
go to Harvard. But it's very fascinating. I remember, it's been a while since I looked at that one,

399
00:51:47,440 --> 00:51:54,320
but I remember it was the, you know, when you describe it as inventing this joke, it wasn't anywhere

400
00:51:54,320 --> 00:52:00,320
in the training data that you could find, right? Yeah. The only instance of mentioning

401
00:52:00,320 --> 00:52:08,800
Hayward is, doesn't have any, anywhere, there's only one instance of mentioning the word Hayward in the

402
00:52:08,800 --> 00:52:15,600
training data. And we look at that context, and it has nothing that looks like what we are like

403
00:52:15,600 --> 00:52:20,400
horses go to Harvard at all. And so what do you think was happening there? How did that work?

404
00:52:21,360 --> 00:52:27,120
I guess unlike what we see in kind of conventional language models, like even the big ones,

405
00:52:27,120 --> 00:52:32,320
they're picking up stuff that they've seen before generally, right? Okay. So I can tell you my

406
00:52:32,320 --> 00:52:38,560
version. My version is a following. My version is a following. We still don't know, right? That's

407
00:52:38,560 --> 00:52:46,960
what I say. It's a magic moment. I think, first of all, a lot of social media jokes about puns.

408
00:52:46,960 --> 00:53:00,320
Right? A lot of, you know, like we find puns kind of funny. And so, and so, and a punny. And so

409
00:53:02,000 --> 00:53:09,040
in the training data, we train with my pair at coding, meaning that we take a world and we break it

410
00:53:09,040 --> 00:53:18,000
down into disease. So it's, Hayward is not a world, it's right, two words, Hayward, and Harvard,

411
00:53:18,000 --> 00:53:24,000
and etc. So it must have learned the concept of puns. And you wouldn't understand that, you know,

412
00:53:24,000 --> 00:53:30,880
Harvard and Hayward somehow is kind of related. So it made up this pun. So two things, learning

413
00:53:30,880 --> 00:53:39,600
the concept of puns and learning how to put together like a new concept. And yeah, it does a lot

414
00:53:39,600 --> 00:53:46,240
of new things like it make a joke. Like it makes jokes about why chicken crossed the road,

415
00:53:46,240 --> 00:53:51,440
some things like that. I don't remember the particular jokes I can find it and send it to you.

416
00:53:53,200 --> 00:54:00,160
It makes all kind of new jokes that we never found in the training data. And it's truly

417
00:54:00,160 --> 00:54:08,080
fascinating. And so where do you, you know, that line of work, where do you see that going?

418
00:54:09,360 --> 00:54:15,360
Well, what were the key, you know, was this, was this kind of simply, you know, quote unquote,

419
00:54:16,400 --> 00:54:24,640
an instance of scaling up the training or where there are some, you know, new techniques or

420
00:54:24,640 --> 00:54:32,640
novel things that were developed, you know, in the model that you can see applying elsewhere.

421
00:54:33,760 --> 00:54:42,240
I see. Well, first of all, it basically tells a lot of people that scaling up is very

422
00:54:42,240 --> 00:54:52,240
effective. Why to be, you know, NLP models, right? Like I spent two years failing doing that

423
00:54:52,240 --> 00:54:57,280
chatbot. And then suddenly someone came along and found a solution. And the solution is kind of

424
00:54:57,280 --> 00:55:05,440
simple, which is basically just make the model a lot larger. We also found a lot of new interesting

425
00:55:05,440 --> 00:55:11,120
insights is that during the process of building the bar, we have a lot of difficulty how to measure

426
00:55:11,120 --> 00:55:15,920
the performance of the bar. So basically, we, we, one thing that we want to measure is how human

427
00:55:15,920 --> 00:55:22,560
like it is, right? We want to be able to have a conversation like a human like. So we, to measure

428
00:55:22,560 --> 00:55:26,800
human life, we always want to ask human to look at a conversation that we have with about because

429
00:55:26,800 --> 00:55:33,680
we built many models of the box, right? Right. Many, many models of the box. And every model,

430
00:55:33,680 --> 00:55:37,600
we have to look at a used human expert to look at the conversations and say, hey, how many human

431
00:55:37,600 --> 00:55:46,400
like this is? So during the process, what in that we observe is that as the complexity of the model,

432
00:55:46,400 --> 00:55:51,200
so, you know, as you train the model better, so the objective function is called the complexity,

433
00:55:51,200 --> 00:55:57,120
is like a very logo objective. This is basically how, how well you predict the next world, right?

434
00:55:57,840 --> 00:56:03,760
And we've noticed that this objective function correlates very well with human,

435
00:56:03,760 --> 00:56:13,920
a judgment of human likeness of the bar. And, and then we did a real plot of perplexity,

436
00:56:14,720 --> 00:56:18,560
which is the local objective function, you know, tricked me in the next world. And,

437
00:56:20,080 --> 00:56:27,200
human likeness, and we saw like a, a strong correlation. So that's basically another contribution

438
00:56:27,200 --> 00:56:32,000
for NLP is that actually this local objective function, which is just predicting the next work,

439
00:56:32,000 --> 00:56:38,320
next work, if you do a good job at it, turns out it's also a more global objective function,

440
00:56:38,320 --> 00:56:47,120
which is human likeness, how, how I can behave like a human. So local mimicking is global mimicking.

441
00:56:48,720 --> 00:57:00,480
What's interesting about that is that, you know, perplexity being kind of predicting the next work,

442
00:57:00,480 --> 00:57:07,280
what, what makes jokes and puns work is that the next word is a surprise compared to what you saw.

443
00:57:07,280 --> 00:57:12,720
So how do you get something that's good at jokes, but also is optimizing on perplexity?

444
00:57:16,320 --> 00:57:22,960
Yeah, that's, that's the reason why I say it's very simple, it's surprising that low, so

445
00:57:22,960 --> 00:57:29,840
these are, let me try to say it again. So there must be a low, a global objective function that we

446
00:57:29,840 --> 00:57:37,840
are optimized, we like making fun, so that we can get engagement, right, making fun, saying something

447
00:57:37,840 --> 00:57:44,480
meaningful, right, that's, that's global, right, but something that's not just optimizing the next

448
00:57:44,480 --> 00:57:50,000
world. But what I'm just claiming is that local objective function is that actually just predicting

449
00:57:50,000 --> 00:58:00,880
the next world is highly common, global, that we're going to relate to the global. And I mean,

450
00:58:02,240 --> 00:58:08,080
I guess you could argue that your what perplexity doing is doing in the case of the joke is

451
00:58:08,080 --> 00:58:13,280
predicting surprise, it's not like flying to the surprise, what is predicting the surprise based

452
00:58:13,280 --> 00:58:21,040
on? It's predicting the surprise. Yeah, you could say that. Yeah, yeah. Huh, interesting. Cool.

453
00:58:23,040 --> 00:58:29,200
Well, thank you so much, Quark, for taking the time to share with us what you're up to and

454
00:58:31,280 --> 00:58:37,200
and kind of walk us through these recent works here is really, really interesting stuff.

455
00:58:37,200 --> 00:58:45,440
Oh, thank you so much. It's a pleasure to talk to you. Same, same. Awesome. Awesome. Thank you.

456
00:58:47,760 --> 00:58:53,200
All right, everyone. That's our show for today. To learn more about today's guest or the topics

457
00:58:53,200 --> 00:58:59,600
mentioned in this interview, visit twimmelai.com. Of course, if you like what you hear on the podcast,

458
00:58:59,600 --> 00:59:05,600
please subscribe, rate, and review the show on your favorite pod catcher. Thanks so much for

459
00:59:05,600 --> 00:59:07,600
listening and catch you next time.

