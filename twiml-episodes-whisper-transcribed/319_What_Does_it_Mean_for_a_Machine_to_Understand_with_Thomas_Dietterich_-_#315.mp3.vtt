WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimal AI Podcast.

00:13.400 --> 00:24.240
I'm your host Sam Charrington.

00:24.240 --> 00:27.360
Did you miss TwimalCon AI platforms?

00:27.360 --> 00:32.880
If so, you'll definitely want to check out our TwimalCon Video Packages.

00:32.880 --> 00:38.320
Featuring over 25 sessions, discussing expert perspectives on ML and AI at scale and in

00:38.320 --> 00:43.800
production, you'll hear from industry leaders such as Facebook, Levi's, Zappos, and more

00:43.800 --> 00:50.400
about their experiences automating, accelerating, and scaling machine learning and AI.

00:50.400 --> 00:56.160
In each video package, you'll receive our keynote interviews, the exclusive team Teardown

00:56.160 --> 01:05.000
panels featuring Airbnb and SurveyMonkey, case studies, and more, over 13 hours of footage.

01:05.000 --> 01:10.920
Once again, visit twimalcon.com slash videos for more information or to secure your advanced

01:10.920 --> 01:17.440
purchase today.

01:17.440 --> 01:20.440
Alright everyone, I am on the line with Tom Dietrich.

01:20.440 --> 01:25.640
Tom is a Distinguished Professor Emeritus at Oregon State University.

01:25.640 --> 01:28.080
Welcome to the Twimal AI Podcast.

01:28.080 --> 01:29.080
It's nice to be here.

01:29.080 --> 01:30.360
Thank you for inviting me.

01:30.360 --> 01:31.360
Absolutely.

01:31.360 --> 01:34.560
I'm really looking forward to digging into this conversation.

01:34.560 --> 01:39.840
I'd love to, before we dive in, get a little bit of your...

01:39.840 --> 01:44.480
I'm really looking forward to diving into our conversation, which is going to be focused

01:44.480 --> 01:46.720
on a recent blog post that you wrote.

01:46.720 --> 01:51.600
There's really exploring what it means for a machine to understand.

01:51.600 --> 01:55.600
But before we do that, I'd love to learn a little bit about your background.

01:55.600 --> 02:00.600
And the context to which you bring to this conversation.

02:00.600 --> 02:06.200
Okay, well, I'm started out as one of the very first graduate students working in machine

02:06.200 --> 02:07.720
learning.

02:07.720 --> 02:12.560
My advisor, Richard McCalsky, was one of the three professors, along with Tom Mitchell

02:12.560 --> 02:19.080
and Jaime Carbonell at Carnegie Mellon, who launched the series of workshops and now

02:19.080 --> 02:22.640
conferences, the International Conference on Machine Learning.

02:22.640 --> 02:28.120
So back in 1980, there were about 30 of us in a classroom in Carnegie Mellon, and I've

02:28.120 --> 02:30.760
been working in the field since then.

02:30.760 --> 02:31.760
Wow.

02:31.760 --> 02:39.120
Over that time, I made some technical contributions in ensemble methods and in hierarchical

02:39.120 --> 02:45.560
reinforcement learning and various applications, for example, in pharmaceuticals, and I work

02:45.560 --> 02:52.840
in applications in weather network data and data cleaning, internet of things, kinds

02:52.840 --> 02:58.960
of things, but my primary research focus these days is on reliable, robust artificial

02:58.960 --> 03:04.320
intelligence, particularly in safety critical applications.

03:04.320 --> 03:11.400
So I've done a lot of service activities for the field I edited the machine learning

03:11.400 --> 03:17.200
journal for six years, and then moved into a position as the founding president of the

03:17.200 --> 03:21.560
International Machine Learning Society, which is the organization that runs the machine

03:21.560 --> 03:24.160
learning conference, the ICML conference.

03:24.160 --> 03:32.280
I also chaired the NURP's conference in 2000, and I served a term as president of the

03:32.280 --> 03:35.760
Association for the Advancement of Artificial Intelligence.

03:35.760 --> 03:40.960
My main service activity these days is I'm one of the moderators on archive for machine

03:40.960 --> 03:46.320
learning, which is between machine learning and computer vision, the two most active categories

03:46.320 --> 03:49.280
for the archive pre-print server.

03:49.280 --> 03:56.280
And so I mentioned this in the opening, but we're really digging into this topic of what

03:56.280 --> 04:01.440
it means for a machine to understand, a recent blog post of yours, and I thought to get

04:01.440 --> 04:05.920
things kicked off, I'd read your couple of the opening sentences.

04:05.920 --> 04:10.840
You wrote critics of recent advances in artificial intelligence complained that although these

04:10.840 --> 04:15.400
advances have reduced remarkable improvements in AI systems, these systems still do not

04:15.400 --> 04:19.160
exhibit real true or genuine understanding.

04:19.160 --> 04:23.920
The use of words like real true and genuine imply that understanding is binary, a system

04:23.920 --> 04:27.880
either exhibits genuine understanding or it does not.

04:27.880 --> 04:31.760
The difficulty with this way of thinking is that human understanding is never complete

04:31.760 --> 04:33.560
and perfect.

04:33.560 --> 04:39.400
So certainly the way you've laid that opening argument out, it resonates with me.

04:39.400 --> 04:46.680
I recently had Gary Marcus on the show, this was back in September, and we spoke about

04:46.680 --> 04:56.640
the book that he recently launched rebooting AI, and he's very outspoken as a critic of

04:56.640 --> 05:01.640
deep learning, and maybe that's not the way he would put it, maybe he'd say a critic

05:01.640 --> 05:09.360
of deep learning as a standalone path to artificial general intelligence, but reading

05:09.360 --> 05:16.120
the blog post, I couldn't help but think of Gary Marcus as being, although you didn't

05:16.120 --> 05:21.560
name him kind of the person in absentia that you were writing this to.

05:21.560 --> 05:29.760
Maybe talk a little bit about the broader context for this post and maybe how what prompted

05:29.760 --> 05:35.440
you to write it, if Gary was part of what you were thinking about or not, I'd love

05:35.440 --> 05:38.280
to kind of get a sense for where you were coming from here.

05:38.280 --> 05:45.680
Well certainly Gary and I have had an opportunity even to engage in formal debates, and Gary's

05:45.680 --> 05:50.600
as I was saying, I think Gary's main points I generally agree with, which is that there

05:50.600 --> 05:56.400
are lots of obvious shortcomings in our existing AI systems and in particular these systems

05:56.400 --> 06:03.600
based on deep learning, but Gary and other people can't seem to stop saying things like

06:03.600 --> 06:10.160
well, when we look at the behaviors say of Google Translate, it's clear that it doesn't

06:10.160 --> 06:17.280
have, it's not exhibiting real understanding of the language it's translating, or when

06:17.280 --> 06:23.800
we talk about Siri, that Siri doesn't really understand what we're talking about.

06:23.800 --> 06:30.160
And I've been making the counter argument, yes, these systems are understanding and it's

06:30.160 --> 06:34.160
real understanding, but it is narrow understanding.

06:34.160 --> 06:41.400
So I am criticizing the use of the word real to mean deep and complete understanding, because

06:41.400 --> 06:47.680
that denies that these systems are doing anything that is intelligent or that is exhibiting

06:47.680 --> 06:52.200
real understanding, and I think that puts you in the position that you will never be

06:52.200 --> 06:57.640
happy with any AI system because no matter how good it gets, it will make mistakes and

06:57.640 --> 07:00.120
exhibit failures and it's understanding.

07:00.120 --> 07:05.760
And are you going to say, well, when it understands 95% of what people say, that it's still

07:05.760 --> 07:11.080
not real understanding, I mean, you're pushing yourself into a belief that there's some

07:11.080 --> 07:15.560
magic threshold, that if you could somehow cross it, you would have a system that had

07:15.560 --> 07:17.520
real understanding.

07:17.520 --> 07:19.200
And I don't think that's the way it works.

07:19.200 --> 07:24.560
I think that the way it works is that we make incremental progress, sometimes bigger

07:24.560 --> 07:30.080
leaps, sometimes no progress for periods of time, as we were doing in speech recognition

07:30.080 --> 07:36.440
for a while in the 90s, but our systems get better, they are able to understand something.

07:36.440 --> 07:43.440
So as I say, when I tell Siri, please call Dan and it calls the right person, it has understood

07:43.440 --> 07:47.400
me for the purposes of that utterance for that task.

07:47.400 --> 07:53.120
Now if I said, you know, Siri, tell me what Dan means to me, and it doesn't really know

07:53.120 --> 08:01.760
anything about what it means, say, to be a best friend, but many people, of course, have

08:01.760 --> 08:07.680
remarked that it's impossible for two human beings to fully understand each other.

08:07.680 --> 08:12.240
So that brings me back again to what is it we're really trying to achieve when we build

08:12.240 --> 08:13.880
AI systems.

08:13.880 --> 08:20.600
And as an engineer, I would say, I want systems that can make the appropriate response when

08:20.600 --> 08:26.480
I ask them to do something, or if they're warning me of some situation in the world that

08:26.480 --> 08:28.920
I should be paying attention to, and so on.

08:28.920 --> 08:33.800
And to the extent that they do that correctly, I would say they understand what I want them

08:33.800 --> 08:34.800
to do.

08:34.800 --> 08:38.880
Yeah, when we have these kinds of conversations, I think it's, you know, there's a slippery

08:38.880 --> 08:47.160
slope of kind of devolving to defining every term in the argument.

08:47.160 --> 08:51.520
But in this case, I wonder the extent to which we're talking about different types of

08:51.520 --> 08:52.520
understanding.

08:52.520 --> 08:56.600
Do you think that that is the case at all here?

08:56.600 --> 09:00.200
Well, I don't know that there are different types, but certainly different definitions

09:00.200 --> 09:01.200
or degrees.

09:01.200 --> 09:04.240
Well, obviously we are arguing about definitions.

09:04.240 --> 09:11.920
And in my blog post, I was supporting the view that instead of arguing about our definitions,

09:11.920 --> 09:16.680
we should be trying, we should be asking ourselves, well, what tests would you give to

09:16.680 --> 09:21.960
a system in order to evaluate whether it's understanding, doing a particular type of

09:21.960 --> 09:22.960
task well?

09:22.960 --> 09:23.960
Right.

09:23.960 --> 09:32.120
If you say, well, the system is narrow, show me all the things that you would like it

09:32.120 --> 09:35.160
to do that it is failing to do now.

09:35.160 --> 09:41.720
And I drew the analogy to test driven development in software engineering, write the tests first,

09:41.720 --> 09:46.240
and then use those to decide how to engineer the system to try to meet those tests.

09:46.240 --> 09:49.280
And then keep writing more tests.

09:49.280 --> 09:54.920
And I think Gary has actually jumped on that.

09:54.920 --> 09:59.800
And on Twitter, he's been asking people, you know, what's wrong with our current natural

09:59.800 --> 10:03.040
language understanding tasks.

10:03.040 --> 10:09.880
And because it seems that we can often get an AI system to do well on a particular benchmark

10:09.880 --> 10:10.880
task.

10:10.880 --> 10:16.720
And yet, again, it turns out that it's very narrow and it's not doing well on any like

10:16.720 --> 10:22.480
immediately adjacent tasks that we would like it to do.

10:22.480 --> 10:25.960
And so some people have been, there's been a bit of a discussion now about that.

10:25.960 --> 10:30.400
And I think that's really where the discussion needs to go is, you know, and Gary himself

10:30.400 --> 10:35.760
in our Twitter conversations, I thought articulated beautifully, said, okay, I want the computer

10:35.760 --> 10:41.920
to be able to say, read a story and tell me the answers to the journalist questions.

10:41.920 --> 10:44.320
Who, when, where, why, how?

10:44.320 --> 10:46.000
So why did this person do this?

10:46.000 --> 10:47.000
What did they do?

10:47.000 --> 10:48.000
When did they do it?

10:48.000 --> 10:51.160
You know, ordered these events for me correctly in time.

10:51.160 --> 10:57.200
And those are way beyond what we can, the state of the art in AI and natural language understanding.

10:57.200 --> 11:00.920
Some of the people in the natural language community said, just stop using the word understanding

11:00.920 --> 11:06.160
at all, we just call it language processing, because we know that, that this word understanding

11:06.160 --> 11:12.280
sets expectations for something that is very broad and deep.

11:12.280 --> 11:17.560
You know, Doug Hofstetter had a very interesting piece that came out last year, where he analyzed

11:17.560 --> 11:24.680
Google Translate and showed how many, many cases where Google translates understandings

11:24.680 --> 11:28.160
clearly extremely surface oriented.

11:28.160 --> 11:34.240
And often it can't understand anything about did, you know, did John do something to Mary,

11:34.240 --> 11:39.440
was Mary doing something to John, it just knows that John and Mary need to both be translated

11:39.440 --> 11:41.720
into the different language.

11:41.720 --> 11:47.320
And it certainly doesn't have any of the, you know, connotations and depth to say that

11:47.320 --> 11:53.200
would be required to translate more poetic language or metaphorical language.

11:53.200 --> 12:02.280
It really has no understanding of human social relationships, what might make Mary angry,

12:02.280 --> 12:08.240
what might make John happy, you know, just, it's completely clueless about that, because

12:08.240 --> 12:13.440
all it has been taught to do is to translate from say Chinese into English or English into

12:13.440 --> 12:20.640
Chinese for fairly straightforward everyday sentences, and certainly not trying to translate

12:20.640 --> 12:23.360
Shakespeare at all.

12:23.360 --> 12:30.320
And one can imagine that it could make very serious mistakes as a result in say highly

12:30.320 --> 12:36.720
emotional and complex social situations.

12:36.720 --> 12:41.800
It's fine for where is the nearest bus stop, but not so good for, you know, why aren't

12:41.800 --> 12:44.480
you talking to me anymore or something.

12:44.480 --> 12:48.840
I do want to encourage us to move beyond saying, well, it either understands or it doesn't,

12:48.840 --> 12:54.120
and this understanding is true or it isn't to say, well, this understanding is incomplete

12:54.120 --> 12:58.200
in these important ways and what we would need these to do.

12:58.200 --> 13:03.960
And so for example, when we think about reading a story or just engaging in a dialogue,

13:03.960 --> 13:08.560
we need AI systems that can be building and maintaining an interpretation of the dialogue.

13:08.560 --> 13:12.000
And this is well known in the natural language community, we just don't know really how

13:12.000 --> 13:13.560
to do it at scale.

13:13.560 --> 13:20.120
We can build applications in a narrow domain, say purchasing airline tickets or something,

13:20.120 --> 13:27.480
where we can cover a lot of different linguistic phenomena and have quite good performance,

13:27.480 --> 13:31.080
but as soon as you step out of that narrow domain, that breaks down.

13:31.080 --> 13:37.480
Yeah, maybe kind of returning to the definition or debate over what understanding a self

13:37.480 --> 13:45.600
means, you refer to Cyril's Chinese room argument and an article by Cole in your blog.

13:45.600 --> 13:48.240
Can you talk a little bit about those?

13:48.240 --> 13:57.840
Well, yes, so there is this famous article in which Cyril put forward the sort of following

13:57.840 --> 14:00.640
thought experiment, right, which was his Chinese room.

14:00.640 --> 14:08.200
So a person is in a room and in this room are, you know, files and files or books and

14:08.200 --> 14:14.600
books full of rules that basically say if you are given these Chinese characters as input,

14:14.600 --> 14:19.840
you should produce these Chinese characters as output and maybe their intermediate rules

14:19.840 --> 14:20.960
and so on.

14:20.960 --> 14:25.080
And there's a human being in there and what that human being is essentially like the central

14:25.080 --> 14:29.640
processing unit of a computer, it takes the input from the outside world that matches

14:29.640 --> 14:34.720
it somehow against all these rules, follows the rules until it produces an output and outputs

14:34.720 --> 14:35.720
it.

14:35.720 --> 14:42.280
And Cyril was arguing that obviously this room could not, that was not truly or generally

14:42.280 --> 14:46.760
understanding Chinese, even though to an outside observer, it looked like it was doing

14:46.760 --> 14:49.360
just fine.

14:49.360 --> 14:56.200
And several people criticize this at the time and it has been a source of lots of interesting

14:56.200 --> 15:06.040
analysis over the years, but one of the main points I would like to link this back to

15:06.040 --> 15:16.320
my previous discussion is that the person inside the box, who he, Cyril asserts is just

15:16.320 --> 15:21.200
an English-only speaker and doesn't understand any Chinese at all, will still not understand

15:21.200 --> 15:24.920
any Chinese even though they are executing this computer program.

15:24.920 --> 15:29.280
And that's what we should expect, I mean, he doesn't make this point, he tries to pull

15:29.280 --> 15:35.720
out the intuition, you know, why doesn't this human understand Chinese and even argues,

15:35.720 --> 15:41.000
well, suppose the human wouldn't actually be using books full of rules but had memorized

15:41.000 --> 15:45.480
all of these rules, but still they would just be executing rules and they wouldn't be understanding

15:45.480 --> 15:46.480
Chinese.

15:46.480 --> 15:52.200
Well, a lot of people then criticize this as saying, well, the intuition is somehow that

15:52.200 --> 15:56.400
I'm looking, if the box is the entire system and I'm looking at some subpart of the box

15:56.400 --> 16:02.840
like the central processing unit of a computer, I'm not going to be able to find a locus of

16:02.840 --> 16:08.440
understanding, looking at any single component inside the box, it's a sort of system-level

16:08.440 --> 16:14.280
property that, given inputs, the box produces appropriate outputs, which would be my sort

16:14.280 --> 16:18.640
of functional definition of understanding.

16:18.640 --> 16:23.800
And I would say that understanding is deeper and broader to the extent that you can ask

16:23.800 --> 16:28.600
it a wider and wider of more challenging and deeper questions and it produces the appropriate

16:28.600 --> 16:31.360
responses to those also.

16:31.360 --> 16:36.120
But we shouldn't expect when we open up, let's say some day we have produced an omni-intelligence

16:36.120 --> 16:40.880
system that is broad and deep intelligence and we open it up, what are we likely to find

16:40.880 --> 16:41.880
inside?

16:41.880 --> 16:46.200
Well, at some level, we might find things we would describe as reasoning and memory and

16:46.200 --> 16:52.440
knowledge and if we go down deeper, we might find pattern matching and probabilistic, you

16:52.440 --> 16:54.400
know, random sampling or something.

16:54.400 --> 16:59.280
If we go down lower, we're just going to find, you know, and in or gates turning on

16:59.280 --> 17:05.680
and off, why should we expect to look inside the box and find intelligence there?

17:05.680 --> 17:10.800
It's going to be the entire system that exhibits this behavior.

17:10.800 --> 17:16.160
Just as with us, when we open up our brains, we just see neurons firing and we keep trying

17:16.160 --> 17:18.920
to find, well, how does that produce intelligent behavior?

17:18.920 --> 17:25.080
Yeah, I think part of this, you know, that most recent argument reminds me of my conversation

17:25.080 --> 17:31.280
with Greg Brockman of OpenAI, where I think towards the end of that conversation, you know,

17:31.280 --> 17:32.600
we pause it.

17:32.600 --> 17:38.080
I forget whether this was, you know, his perspective or mine or, you know, how we arrive at it,

17:38.080 --> 17:44.320
but, you know, one way of kind of projecting what AGI might look like is that it's not

17:44.320 --> 17:50.760
some single universally intelligent system, but something that looks more like an ensemble

17:50.760 --> 17:53.720
of narrower things.

17:53.720 --> 18:00.560
And I think that kind of ties into your functional argument in the sense of, you know, at the

18:00.560 --> 18:06.880
end, the individual subcomponents of this thing won't necessarily have broad understanding,

18:06.880 --> 18:10.320
but they could produce something that, you know, if you look at it, it functionally

18:10.320 --> 18:14.720
has a broader understanding.

18:14.720 --> 18:15.720
Yes.

18:15.720 --> 18:19.720
And of course, this has also been a topic of long discussion in the AI community.

18:19.720 --> 18:25.000
I think researchers and artificial intelligence want to believe that there are some common core

18:25.000 --> 18:30.200
mechanisms that apply across a wide range of intelligent behaviors.

18:30.200 --> 18:37.360
And certainly we, one of the things we have a commitment to or have had since the 50s

18:37.360 --> 18:43.400
in reaction to behaviorism is that there should be some sort of mental models or mental representations

18:43.400 --> 18:45.720
of the world.

18:45.720 --> 18:51.920
And for instance, I should have a representation, someone in my head of you and your goals

18:51.920 --> 18:55.400
and questions in this conversation, what we've already talked about, what we might talk

18:55.400 --> 18:57.000
about, and so on.

18:57.000 --> 19:03.440
And there's been a almost a reflex reaction against the idea that intelligence is just

19:03.440 --> 19:08.600
a big switch that, given the task, I switch on a different expert in my head, and I just

19:08.600 --> 19:10.000
have all these narrow experts.

19:10.000 --> 19:12.600
There should be some shared mechanism.

19:12.600 --> 19:19.560
On the other hand, you know, when you look at, say, Nobel Prize winners in physics, who

19:19.560 --> 19:26.800
decide to start talking about artificial intelligence, and they don't have any deeper insight

19:26.800 --> 19:33.200
than any of the rest of us when they move out of their field of expertise, and this

19:33.200 --> 19:37.760
has been established across many different, you know, fields of expertise is that, you

19:37.760 --> 19:42.600
know, you can be an expert in one thing, but you're basically a novice in other things.

19:42.600 --> 19:47.800
And yet, of course, we do see that some people seem to be faster learners than others.

19:47.800 --> 19:53.720
And so that argues, well, maybe there are some shared mechanisms, and certainly our

19:53.720 --> 19:57.880
perception, as we get older, maybe we're fooling ourselves, is that as we learn more and

19:57.880 --> 20:03.160
more about more things, we're able to go into a new area and learn faster because

20:03.160 --> 20:10.140
we have some frameworks that we can plug new knowledge into that help us learn faster.

20:10.140 --> 20:20.200
So there's definitely this conflict in AI between generality and narrow specificity.

20:20.200 --> 20:25.400
And I don't think it's a big switch that just completely switches from one expert to

20:25.400 --> 20:26.400
another.

20:26.400 --> 20:31.880
But nor do I think that there is some universal module that if we just get that right,

20:31.880 --> 20:38.760
we'll be able to learn everything, because certainly we see in ourselves and in other

20:38.760 --> 20:44.040
people that different people have different strengths and weaknesses in terms of the kinds

20:44.040 --> 20:47.880
of knowledge that they can master, the kinds of skill they can exhibit.

20:47.880 --> 20:52.040
So there's a lot of heterogeneity, and that suggests that there isn't a single thing

20:52.040 --> 20:56.360
like our heart or our lungs that has the same function across everyone.

20:56.360 --> 21:05.760
There's certainly elements of your argument that resonate with me very strongly, the idea

21:05.760 --> 21:10.360
that understanding isn't binary just makes sense to me.

21:10.360 --> 21:16.880
At the same time, when I think about what Gary is reacting to and trying to speak out

21:16.880 --> 21:27.160
against, it's this hype engine that wants to portray with intent or not.

21:27.160 --> 21:33.800
Maybe it's not the initiators of the news that want to intent, but once you put it through

21:33.800 --> 21:43.600
the media filter starts to portray this idea that we're at already systems that exhibit

21:43.600 --> 21:52.400
some kind of intelligence or that we should be worried about, or that have understanding.

21:52.400 --> 21:59.240
I think that's really what he's trying to put a damper on for the benefit of the industry

21:59.240 --> 22:07.840
as a whole, and certainly when I talk to laypeople about what's happening in artificial intelligence

22:07.840 --> 22:16.720
and what a theory really is, what I most immediately refer to as a misconception that these

22:16.720 --> 22:26.280
systems do have understanding our intelligent, and my immediate reaction is to try to contextualize

22:26.280 --> 22:33.600
that, and most often I'm saying, no, they're not really, they don't really understand.

22:33.600 --> 22:42.680
It seems like the right counterbalance to what the hype creates or just a lack of understanding

22:42.680 --> 22:47.960
of what's really happening in these systems conveys.

22:47.960 --> 22:54.040
It seems like the right way to kind of guide their understanding of the degree of understanding

22:54.040 --> 23:00.280
of modern AI, so why do you kind of object to that?

23:00.280 --> 23:07.200
Well, I don't object to that, and I am 100% with Gary on the trying to dampen down the

23:07.200 --> 23:08.200
hype.

23:08.200 --> 23:12.200
I think it creates those misconceptions that you're talking about.

23:12.200 --> 23:19.920
I think it's leading investors to put money behind things that are not going to work out.

23:19.920 --> 23:26.960
I think that it's also an issue of intellectual honesty in the computer science field that

23:26.960 --> 23:32.120
as we do research on these things, we need to point out not only the new capabilities

23:32.120 --> 23:37.480
that we can create, but also their weaknesses.

23:37.480 --> 23:44.200
I totally agree with Gary, and I think Gary would make another point in his argument against

23:44.200 --> 23:48.280
a sort of all deep learning approach.

23:48.280 --> 23:53.280
He's making essentially the ladder to the moon argument, which you may have heard before,

23:53.280 --> 23:59.520
which is that if our goal is to get to the moon, and we can demonstrate that we went

23:59.520 --> 24:03.560
from six-foot tall ladders to ten-foot tall ladders to fifty-foot tall ladders, and

24:03.560 --> 24:08.360
we say, look, we're making progress and extrapolate from our ladder technology to claim that

24:08.360 --> 24:12.640
it's going to get us to the moon, and it just isn't.

24:12.640 --> 24:22.080
We have to, of course, as researchers and engineers, we place our bets on certain technologies,

24:22.080 --> 24:25.920
and we want to see how far we can push them.

24:25.920 --> 24:31.440
In the past, people have placed bets on explicit knowledge representation in reasoning systems

24:31.440 --> 24:36.800
where they're hand-coding as much of human knowledge as they can, and now deep learning

24:36.800 --> 24:44.800
is placing its bets on deep neural networks and, more generally, undifferentiable programming.

24:44.800 --> 24:49.480
This certainly gives you a way of writing programs that you can now train in an end-to-end

24:49.480 --> 24:53.720
fashion, or you can train individual modules, and then glue them together with end-to-end

24:53.720 --> 25:04.240
fine-tuning, and there's a feeling among the connectionist or deep learning community

25:04.240 --> 25:09.960
that there's still a lot of headroom to go in the things that we could do with this technology.

25:09.960 --> 25:15.040
So people are putting in memory, and they're connecting this technology to external knowledge

25:15.040 --> 25:21.680
sources. They're doing the meta-learning, which allows them a system that's been trained

25:21.680 --> 25:27.400
on some initial tasks to also then very rapidly learn new tasks and transfer their knowledge

25:27.400 --> 25:34.600
from one to another, and so partly the people inside the deep learning community feel

25:34.600 --> 25:41.080
like this is still a productive research paradigm or research program, which is another

25:41.080 --> 25:48.360
another thing I mentioned in my blog post is this idea that if we think about the development

25:48.360 --> 25:53.960
of science in terms of Thomas Coons analysis in terms of paradigms, or this analysis in

25:53.960 --> 26:02.960
terms of research programs, that these programs continue until they cease to be productive

26:02.960 --> 26:08.520
and fruitful, and Gary, on the one hand, is arguing, well, there are all these things

26:08.520 --> 26:13.040
that deep learning systems, today's deep learning systems cannot do, and that he doesn't

26:13.040 --> 26:15.880
see how they're ever going to do.

26:15.880 --> 26:21.480
So he's arguing, well, we know we have these other systems, these symbolic reasoning systems

26:21.480 --> 26:26.120
that can do some of these other things, so the obvious path forward is to build hybrid

26:26.120 --> 26:31.240
systems that combine both symbolic and connectionist components.

26:31.240 --> 26:36.240
But I would say that the connectionist deep learning reply to that is, well, we're going

26:36.240 --> 26:43.120
to keep pushing, because we see that our methods are still fruitful in giving us new capabilities,

26:43.120 --> 26:47.200
we're going to keep pushing on them, and we want to maintain an open mind.

26:47.200 --> 26:53.480
I mean, maybe we can't reach the moon with only connectionist ladders, but maybe we can

26:53.480 --> 26:58.400
build rocket ships out of connectionist material, and maybe we can get there.

26:58.400 --> 27:04.040
And so, of course, the connections have from, for decades, criticized symbolic systems

27:04.040 --> 27:11.200
for their inability to capture nuances and similarities, and because they're very symbolic

27:11.200 --> 27:18.960
and Boolean, and I don't think we have a good, that the symbolic community has a good

27:18.960 --> 27:21.400
response to that criticism.

27:21.400 --> 27:26.360
So both methods have their weaknesses, and maybe, I mean, if I were building a system

27:26.360 --> 27:29.520
today, I would build a hybrid system.

27:29.520 --> 27:33.400
And certainly, if you look at the successful AI systems out there, like let's look at

27:33.400 --> 27:41.840
Google search or Bing, these systems are hybrids of deep learning and symbolic reasoning

27:41.840 --> 27:43.160
systems.

27:43.160 --> 27:49.080
They are also collections of experts, so that an incoming question is routed to multiple

27:49.080 --> 27:53.720
subquery engines that say, well, is this a question about stock prices?

27:53.720 --> 27:56.080
Is this a question about a product for sale?

27:56.080 --> 27:59.120
Is this a question about geography, right, and so on?

27:59.120 --> 28:04.400
The candidate answers bubble up and are ranked and assessed, and then one or more of them

28:04.400 --> 28:06.600
are displayed to the user.

28:06.600 --> 28:10.360
And so these are certainly, if you look at what is the state of the art in terms of actual

28:10.360 --> 28:13.640
practical systems, they are all hybrid systems.

28:13.640 --> 28:23.920
It strikes me that the critique of deep learning isn't going to get us to AGI is similar

28:23.920 --> 28:32.840
to, or there's maybe an adjacent critique that kind of touches on your own area of research

28:32.840 --> 28:39.560
into robust and safe AI that kind of says that, you know, a lot of the research in that

28:39.560 --> 28:47.120
area kind of presupposes some super intelligence, ALA, Nick Bostrom.

28:47.120 --> 28:48.240
And we're nowhere near there.

28:48.240 --> 28:51.000
We have no idea how we're going to get there.

28:51.000 --> 28:59.400
And therefore, how useful really is that whole line of reasoning is kind of that the parallel

28:59.400 --> 29:04.600
critique there, part of what you're responding to or in what ways do you see that playing

29:04.600 --> 29:05.600
out?

29:05.600 --> 29:11.880
Well, so my interest in robust AI is much more immediate than Bostrom's paperclip maker

29:11.880 --> 29:15.080
or any of those things.

29:15.080 --> 29:16.080
Okay.

29:16.080 --> 29:18.680
I think those are all fantasies, basically.

29:18.680 --> 29:25.520
But we have very, very practical issues confronting us right now, because all of the AI systems

29:25.520 --> 29:30.840
that we build today are limited to fairly closed worlds.

29:30.840 --> 29:36.840
I mean, I guess I'd have to say the Google search engine is much more open, but it has

29:36.840 --> 29:43.400
a fallback, which is to just go and do a search of the web and try to find matching documents.

29:43.400 --> 29:47.440
So if it can't understand something, that's kind of what it falls back on, just like Siri

29:47.440 --> 29:48.440
does.

29:48.440 --> 29:55.320
But when we think about, say, a self-driving car, and we train it to recognize basketballs

29:55.320 --> 30:00.600
bouncing across the street, children on bicycles and tricycles and dogs and deer and what

30:00.600 --> 30:05.280
have you, but there's always the possibility that something new, there'll be some new kind

30:05.280 --> 30:08.360
of obstacle that the car hadn't seen before.

30:08.360 --> 30:13.440
And so I used to use the made up example, well, suppose we train the system in North America,

30:13.440 --> 30:17.760
but then we deployed in Australia, what does it do the first time it sees a kangaroo?

30:17.760 --> 30:24.560
It turned out that was actually a real case, and the kangaroos were confusing some of

30:24.560 --> 30:30.640
the self-driving car systems that had been engineered in Europe, and then were being

30:30.640 --> 30:32.840
tested in Australia.

30:32.840 --> 30:38.080
So this is known as kind of the open world problem or the open category problem.

30:38.080 --> 30:41.120
There's some new kinds of objects out there.

30:41.120 --> 30:47.240
And our machine learning systems, say for supervised learning, are trained on some fixed

30:47.240 --> 30:48.520
set of categories.

30:48.520 --> 30:56.680
So the most famous benchmark, which is ImageNet, has 1,000 categories of objects, and it's

30:56.680 --> 31:00.360
trained to discriminate among those 1,000 categories.

31:00.360 --> 31:05.120
So it basically assumes every new image it sees contains one of those, objects from

31:05.120 --> 31:07.320
one of those 1,000 categories.

31:07.320 --> 31:11.320
And so if there's something new there, it will just classify it as one of the thousand

31:11.320 --> 31:13.400
things it knows about.

31:13.400 --> 31:19.240
So if it hadn't been trained on kangaroos, maybe it classifies it as a rabbit or something,

31:19.240 --> 31:23.160
maybe it's confused about the scale, I don't know, or a deer, who knows what kind of mistake

31:23.160 --> 31:28.320
it might make, maybe because of the way it moves it classifies it as a paper bag blowing

31:28.320 --> 31:30.720
across the road.

31:30.720 --> 31:38.040
And in that case, the automatic car might make a mistake and not break to stop for it.

31:38.040 --> 31:44.320
So I'm interested in this question of can we build systems that can work in open environments?

31:44.320 --> 31:48.880
Can we get probability estimates, confidence estimates out of the system that we can

31:48.880 --> 31:51.240
trust in open worlds?

31:51.240 --> 31:52.400
And in close worlds too.

31:52.400 --> 31:58.000
So if we think about, there's been a lot of discussion of face recognition and these

31:58.000 --> 32:04.480
face recognition systems, and it's well established that they are not equally accurate on all

32:04.480 --> 32:10.120
people, right, and particularly black people and darker skinned people, the systems are

32:10.120 --> 32:17.640
not nearly as accurate on, and particularly black women are very inaccurate on these, and

32:17.640 --> 32:23.120
partly that is because the images of black people are underrepresented in the training

32:23.120 --> 32:27.280
data, but they are also just a minority of the population.

32:27.280 --> 32:33.600
So if you belong to a subpopulation that is a tiny minority, machine learning systems

32:33.600 --> 32:40.120
tend to go for the common case, and they tend to be less accurate on the edge cases.

32:40.120 --> 32:42.680
You know, you can say, well, I'm 98% accurate.

32:42.680 --> 32:46.960
It's just that every one of my mistakes turns out to be a black woman.

32:46.960 --> 32:50.280
And so if you're a black woman, you're really going to have problems with these face recognition

32:50.280 --> 32:52.240
systems.

32:52.240 --> 32:58.320
And you know, this has been really called to attention, I think, really nicely, by a

32:58.320 --> 33:08.160
joy-blown lenient MIT and Timnick Gibru, who I think is still with Google, and their collaborators.

33:08.160 --> 33:16.680
And I want these systems to be able to give confidence scores that say, well, when I see

33:16.680 --> 33:22.200
an image of a black woman, I have a confidence score that is very low, so that we know not

33:22.200 --> 33:25.880
to trust these systems in these situations.

33:25.880 --> 33:32.960
Even for the self-driving car, same for, say, using computer vision in medicine, in reading

33:32.960 --> 33:40.200
X-rays and so on, we need systems that can give us confidence numbers that we can trust.

33:40.200 --> 33:45.720
The computer vision system that Amazon sells for face recognition gives confidence numbers,

33:45.720 --> 33:49.680
but they don't tell you what they mean.

33:49.680 --> 33:56.000
And in any case, you would need to calibrate your confidence numbers to the data you're

33:56.000 --> 34:00.280
using the model on, which is different from the data it was trained on.

34:00.280 --> 34:02.720
And so that's one of the things I'm studying in my work.

34:02.720 --> 34:10.000
Yeah, I think at the end of the day, it strikes me that, you know, what Gary's saying is,

34:10.000 --> 34:16.240
hey, you know, we're calling these systems intelligent, talking about their understanding,

34:16.240 --> 34:22.440
but at AGI, deep learning, you know, almost certainly isn't going to get us to AGI by

34:22.440 --> 34:29.240
itself, which is kind of the benchmark of, quote unquote, you know, capital I intelligence

34:29.240 --> 34:35.920
capital you understanding at the same time, what you're saying is, hey, let's not throw

34:35.920 --> 34:42.800
the baby out with the bathwater, there's still significant value in deep learning as well.

34:42.800 --> 34:48.880
And also a lot of room for additional research and exploration and uncovering future value

34:48.880 --> 34:50.280
there.

34:50.280 --> 34:56.400
And it, you know, it sounds like both of these are correct, both of these are great and

34:56.400 --> 34:59.080
right perspectives to take.

34:59.080 --> 35:00.080
Do you agree with that?

35:00.080 --> 35:08.000
Well, I mean, I'm not willing to say, oh, well, you know, deep learning style technology

35:08.000 --> 35:11.880
can't get us to say broadly intelligent systems.

35:11.880 --> 35:15.200
I don't like them from AGI because it's an undefined term.

35:15.200 --> 35:25.960
But so I think we should continue to push in that direction with our deep learning systems.

35:25.960 --> 35:31.160
But we obviously should not be declaring victory prematurely.

35:31.160 --> 35:37.240
And we do get a lot of press releases out of companies and out of academic labs that

35:37.240 --> 35:44.560
exaggerate the significance of the work and, and, you know, maybe they don't explicitly

35:44.560 --> 35:52.280
say it, but they are open to the misinterpretation that, that, that, you know, broad, I'm the intelligence

35:52.280 --> 35:57.080
systems are right around the corner when, when, in fact, of course, we are nowhere near

35:57.080 --> 35:58.080
having those.

35:58.080 --> 36:06.440
Yeah, and I should be clear, Gary's perspective is that deep learning as a style of computation

36:06.440 --> 36:07.440
won't get us there.

36:07.440 --> 36:14.520
And it needs to be supplemented by symbolic computing and, or other techniques.

36:14.520 --> 36:22.080
My personal view is that, you know, there's some discontinuous innovation that's going

36:22.080 --> 36:25.040
to be required to get us there.

36:25.040 --> 36:27.960
Deep learning may be a big part of it, but there's something else.

36:27.960 --> 36:30.120
And we don't really know what that something else is.

36:30.120 --> 36:36.000
It may be some other style of compute that, you know, quantum computing, you know, heaven

36:36.000 --> 36:43.680
forbid or something else that allows us to, you know, to, to far surpass where we are

36:43.680 --> 36:46.000
today.

36:46.000 --> 36:49.600
But I don't think we know what that is today.

36:49.600 --> 36:55.280
And, you know, it's certainly, I think it's a very safe bet that we're going to need

36:55.280 --> 36:58.280
more innovation, discontinuous innovation, as you say.

36:58.280 --> 36:59.280
Exactly.

36:59.280 --> 37:04.520
And then, you know, but also there, there's, you know, it's very clear that we're just

37:04.520 --> 37:14.600
at the beginning of unlocking the value that deep learning offers to society and that,

37:14.600 --> 37:17.520
you know, there's more work to be done and that we should be doing that work.

37:17.520 --> 37:18.520
Yes.

37:18.520 --> 37:24.920
I think we could continue this discussion for quite some time, but I think we've covered

37:24.920 --> 37:30.760
a lot of good ground and it was great checking with you on your perspective on this.

37:30.760 --> 37:34.200
And I really appreciated the blog post, Tom.

37:34.200 --> 37:35.960
Well, thank you very much.

37:35.960 --> 37:41.000
It's always fun to talk about these, these challenging questions in artificial intelligence.

37:41.000 --> 37:42.000
Awesome.

37:42.000 --> 37:45.840
Thanks so much.

37:45.840 --> 37:47.960
That's our show for today.

37:47.960 --> 37:53.480
To learn more about today's show, visit twomolei.com slash shows.

37:53.480 --> 37:58.360
Once again, if you missed twomolei or want to share what you learned with your team, be

37:58.360 --> 38:04.280
sure to visit twomolei.com slash videos for more information about twomolei.com video

38:04.280 --> 38:05.880
packages.

38:05.880 --> 38:32.600
Thanks so much for listening and we'll see you next week.

