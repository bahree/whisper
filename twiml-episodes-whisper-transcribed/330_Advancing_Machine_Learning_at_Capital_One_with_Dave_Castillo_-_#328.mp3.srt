1
00:00:00,000 --> 00:00:13,400
Welcome to the Tumel AI Podcast.

2
00:00:13,400 --> 00:00:16,400
I'm your host Sam Charrington.

3
00:00:16,400 --> 00:00:24,480
Hey, what's up everyone?

4
00:00:24,480 --> 00:00:29,920
I recently attended the AWS Reinvent Conference in Las Vegas, and I'm excited to share

5
00:00:29,920 --> 00:00:38,960
a few of my many interesting conversations from that event here on the podcast this week.

6
00:00:38,960 --> 00:00:44,160
Before we dive in, I'd like to thank our friends at Capital One for sponsoring our Reinvent

7
00:00:44,160 --> 00:00:45,760
series.

8
00:00:45,760 --> 00:00:50,120
Capital One has been a huge friend and supporter of this podcast for some time now, and

9
00:00:50,120 --> 00:00:54,600
I'm looking forward to sharing my interview with Dave Castillo, Capital One's managing

10
00:00:54,600 --> 00:00:58,640
VP of machine learning with you on Thursday.

11
00:00:58,640 --> 00:01:02,400
Dave and I discussed the unique approach being taken at the company's center for machine

12
00:01:02,400 --> 00:01:07,480
learning, as well as some of the interesting AI use cases being developed at the bank,

13
00:01:07,480 --> 00:01:11,880
and the platform they're building to support their ML and AI efforts.

14
00:01:11,880 --> 00:01:17,640
To learn more about Capital One's machine learning and AI efforts and research, visit capitalone.com

15
00:01:17,640 --> 00:01:20,960
slash tech slash explore.

16
00:01:20,960 --> 00:01:25,760
And now on to the show.

17
00:01:25,760 --> 00:01:28,880
All right, everyone.

18
00:01:28,880 --> 00:01:34,560
We are here at Reinvent in Las Vegas, and I am with Dave Castillo.

19
00:01:34,560 --> 00:01:39,360
Dave is the managing vice president for machine learning at Capital One.

20
00:01:39,360 --> 00:01:41,640
Dave, welcome to the Twimal AI podcast.

21
00:01:41,640 --> 00:01:42,640
Oh, thanks, Sam.

22
00:01:42,640 --> 00:01:44,040
Happy to be here.

23
00:01:44,040 --> 00:01:47,400
Let's just jump right in and talk a little bit about your background.

24
00:01:47,400 --> 00:01:51,240
Tell us about your work in the machine learning field.

25
00:01:51,240 --> 00:01:52,240
Absolutely.

26
00:01:52,240 --> 00:01:54,160
So I'm actually very fortunate.

27
00:01:54,160 --> 00:01:59,200
I got to start machine learning way back when it was still handcrafted knowledge, and

28
00:01:59,200 --> 00:02:01,400
we were really focused on knowledge representation.

29
00:02:01,400 --> 00:02:02,800
So I started out in the NASA days.

30
00:02:02,800 --> 00:02:10,440
I did a lot of work for NASA, and it was about solving problems that were very theoretical,

31
00:02:10,440 --> 00:02:14,240
but never really saw the light of day in a production setting.

32
00:02:14,240 --> 00:02:16,040
That wasn't NASA?

33
00:02:16,040 --> 00:02:20,520
Actually, I worked with all the NASA bureaus, but I was stationed actually in Florida,

34
00:02:20,520 --> 00:02:27,520
and I kept the Kennedy Space Center, Johnson Space Center, and JPL, Jet Propulsion Lab.

35
00:02:27,520 --> 00:02:30,640
So I worked with all those three, and it was a lot of fun.

36
00:02:30,640 --> 00:02:35,760
We did robotic vision systems, which are obviously a whole different technology set back

37
00:02:35,760 --> 00:02:37,480
then, than convolutional known.

38
00:02:37,480 --> 00:02:44,120
That sort of annihilated all of that work, and put a lot of people out of work in the

39
00:02:44,120 --> 00:02:45,120
process.

40
00:02:45,120 --> 00:02:51,680
I started with the start of there, and really watched the transition go from knowledge representation

41
00:02:51,680 --> 00:02:57,680
into statistical learning, into machine learning, into deep learning, and then explainability

42
00:02:57,680 --> 00:03:05,760
on top of that, and then more recently, just really looking into how do we do large-scale

43
00:03:05,760 --> 00:03:09,240
graph related machine learning with graph machine learning.

44
00:03:09,240 --> 00:03:11,600
So I've had this very diverse career.

45
00:03:11,600 --> 00:03:17,520
graph had two startups, wind up in the media industry, as a CTO of an advertising company

46
00:03:17,520 --> 00:03:24,640
in New York City for several years, and I left that in 2015 and joined banking with a company

47
00:03:24,640 --> 00:03:30,200
called Early Warning, most popular for Zell, the master payment system, and I ran machine

48
00:03:30,200 --> 00:03:34,000
learning there, the innovation labs, and their data transformation.

49
00:03:34,000 --> 00:03:37,720
That company's owned by the seven largest banks in the U.S., including Capital One.

50
00:03:37,720 --> 00:03:40,400
That's how I got familiar with Capital One.

51
00:03:40,400 --> 00:03:46,240
I started here last June in 2019, 2018, sorry.

52
00:03:46,240 --> 00:03:47,240
Right, right.

53
00:03:47,240 --> 00:03:49,000
So what's your role at Capital One?

54
00:03:49,000 --> 00:03:56,400
I run what's called the Center for Machine Learning, and it's a fairly large organization.

55
00:03:56,400 --> 00:04:01,280
It's 200 plus people focused on a center of excellence, and I can get into a little bit

56
00:04:01,280 --> 00:04:03,480
more of what that means later if you like.

57
00:04:03,480 --> 00:04:04,480
Yeah.

58
00:04:04,480 --> 00:04:09,360
It's folks to Adam Wenzhel on the podcast about a year and a half ago, and he went off

59
00:04:09,360 --> 00:04:13,440
to do a startup, I think, now, is that the same, are you in the same role?

60
00:04:13,440 --> 00:04:15,520
I'm in the same role, absolutely.

61
00:04:15,520 --> 00:04:23,320
I actually met Adam, and I thank Adam for the incredible job he did in amassing a great

62
00:04:23,320 --> 00:04:28,360
talent to sort of incubate the Center for Machine Learning, and Adam had to start in

63
00:04:28,360 --> 00:04:34,120
a place where he was really the skill set, and the expertise around machine learning,

64
00:04:34,120 --> 00:04:42,160
so the machine learning opportunities and solutions all came in and fanned into Adam's organization.

65
00:04:42,160 --> 00:04:43,560
And that's no longer the case now.

66
00:04:43,560 --> 00:04:48,320
We've been able to transform that where we can scale it, and we federated a lot of

67
00:04:48,320 --> 00:04:49,760
capability out.

68
00:04:49,760 --> 00:04:54,880
So our model is more of a model of enablement now, where we actually join teams.

69
00:04:54,880 --> 00:05:02,040
We provide skill sets, technology, platforms, tools, education, boot camps, basically just

70
00:05:02,040 --> 00:05:05,960
trying to empower the other teams to be self-sufficient, and we stay with them for a

71
00:05:05,960 --> 00:05:08,000
period of time, and then we extract ourselves.

72
00:05:08,000 --> 00:05:14,040
Yeah, one of the things that I'm seeing all over the enterprise landscape, it's happening

73
00:05:14,040 --> 00:05:20,600
right now is this kind of transition from doing machine learning in kind of a lab type

74
00:05:20,600 --> 00:05:27,280
of environment, doing a lot of experimentation to, and alongside that, you know, selling

75
00:05:27,280 --> 00:05:31,000
it across the enterprise, and now it seems like everyone's bought in, everyone wants to

76
00:05:31,000 --> 00:05:36,560
figure out how to do it, how to build models, get their pet model into production, and

77
00:05:36,560 --> 00:05:41,840
it's causing shifts in the way organizations are organizing it to support that.

78
00:05:41,840 --> 00:05:44,760
It sounds like you're going through a similar transition.

79
00:05:44,760 --> 00:05:50,640
Yeah, so 18 months ago, we had a lot of sighted desks, bespoke initiatives, where people

80
00:05:50,640 --> 00:05:56,360
were creating their own solutions, they were very narrow, solve very narrow use cases,

81
00:05:56,360 --> 00:06:01,200
and they had to go through all the pain associated with getting something from a proof of concept

82
00:06:01,200 --> 00:06:07,080
into whether it be a challenger model or ultimately into production, and what's happened since

83
00:06:07,080 --> 00:06:11,920
then is that we've become highly collaborative because we understand the challenges associated

84
00:06:11,920 --> 00:06:18,600
with getting a machine learning into production in a well-managed, compliant way, and that's

85
00:06:18,600 --> 00:06:23,560
not an easy task, especially when you have a material model, a model that's under regulatory

86
00:06:23,560 --> 00:06:29,840
scrutiny, and even our immaterial models, which are not so heavily scrutinized, but still

87
00:06:29,840 --> 00:06:33,680
require a well-managed discipline about them.

88
00:06:33,680 --> 00:06:38,440
Anyway, there was a sentiment about how do we actually take all these learnings and come

89
00:06:38,440 --> 00:06:44,320
together as an enterprise and leverage what each other has done, and throw away some

90
00:06:44,320 --> 00:06:49,280
of these sighted desks solutions that don't scale, and really look toward more of an enterprise

91
00:06:49,280 --> 00:06:50,440
offering.

92
00:06:50,440 --> 00:06:55,520
And we've done a lot of work in that area, so now we have collaboration, Center for Machine

93
00:06:55,520 --> 00:07:00,440
Learning Collaborates with all the lines of business, we're driving an enterprise, what

94
00:07:00,440 --> 00:07:05,880
we call the enterprise machine learning ecosystem now, it's driven by myself and another guy

95
00:07:05,880 --> 00:07:10,800
named Annie Gupta, who's in our card business, and underneath that initiative, we have our

96
00:07:10,800 --> 00:07:17,000
feature platform, our machine learning platform, and our monitoring platform, all really

97
00:07:17,000 --> 00:07:24,280
looking at how do you go from inception to deployment, and then keep the 360 moving through

98
00:07:24,280 --> 00:07:28,320
monitoring and refit and retraining.

99
00:07:28,320 --> 00:07:33,080
So there's been a huge sort of awakening that has happened in Capital One, it's really

100
00:07:33,080 --> 00:07:39,120
exciting to see everyone coming together, it's not so territorial anymore, it's really,

101
00:07:39,120 --> 00:07:41,480
how do we help each other actually get to the finish line?

102
00:07:41,480 --> 00:07:42,680
Awesome, awesome.

103
00:07:42,680 --> 00:07:46,760
Before we dig into a little bit of the platform that you've built, can we talk a little

104
00:07:46,760 --> 00:07:49,120
bit about the use cases?

105
00:07:49,120 --> 00:07:57,880
I have spoken to folks at Capital One that are doing things like Fraud and AML, and there

106
00:07:57,880 --> 00:08:03,320
are a bunch of things that are kind of obvious ones for a financial services company, maybe

107
00:08:03,320 --> 00:08:07,680
we can talk about the broad landscape, but I'm also curious if you can share any ones

108
00:08:07,680 --> 00:08:14,600
that are maybe less obvious or more interesting or that you're personally excited about.

109
00:08:14,600 --> 00:08:19,320
We've been Capital One, we are looking at applying machine learning across every facet

110
00:08:19,320 --> 00:08:24,080
of our business, and we've done some very cool and interesting things that you wouldn't

111
00:08:24,080 --> 00:08:28,720
think it would be associated with a bank, so for example, when you join Capital One,

112
00:08:28,720 --> 00:08:33,160
you get assigned access privileges to data and to systems, that's all done by machine

113
00:08:33,160 --> 00:08:34,640
learning today.

114
00:08:34,640 --> 00:08:40,040
And many as an employee, as an employee, when you join, and that's something that we built.

115
00:08:40,040 --> 00:08:46,120
So that's in production, and it's actually compressed the time for associates to get

116
00:08:46,120 --> 00:08:50,240
up to speed and actually be productive because they've had to wait weeks and weeks and weeks

117
00:08:50,240 --> 00:08:56,400
to get access to certain data, to certain systems, and now it's all assigned based on these

118
00:08:56,400 --> 00:08:57,560
algorithms.

119
00:08:57,560 --> 00:08:58,920
That's kind of cool.

120
00:08:58,920 --> 00:09:03,440
We have space allocation that happens when we look at how badges are navigating through

121
00:09:03,440 --> 00:09:09,560
our buildings, and we can look at how to plan space for people, and that's an interesting

122
00:09:09,560 --> 00:09:11,400
application of machine learning.

123
00:09:11,400 --> 00:09:18,480
We did one last year where we're assigning our first year associates, they go on a rotation.

124
00:09:18,480 --> 00:09:24,200
So the next job is an assignment that's actually a recommendation engine that's actually

125
00:09:24,200 --> 00:09:27,320
made by a machine learning algorithm for that job assignment.

126
00:09:27,320 --> 00:09:31,920
That was all done manually on a huge spreadsheet, it was very painful for people to do that

127
00:09:31,920 --> 00:09:32,920
across the board.

128
00:09:32,920 --> 00:09:35,040
That's all automated.

129
00:09:35,040 --> 00:09:42,080
When we're doing, what I used to do in the ad tech industry, we're doing adward bidding

130
00:09:42,080 --> 00:09:48,920
with multi-arm bandits, we're doing creative and personalization, aspiring toward getting

131
00:09:48,920 --> 00:09:55,160
granular and personalization with multi-arm bandits, a tiny bit of reinforcement learning.

132
00:09:55,160 --> 00:10:02,840
So those are not typical of mapping, but I think it's a testimony of that we're serious

133
00:10:02,840 --> 00:10:06,960
about where we want to apply this technology.

134
00:10:06,960 --> 00:10:14,080
There's a plethora of other examples I could give you on reading documents from our vendors

135
00:10:14,080 --> 00:10:18,880
and extracting salient information out of those documents, etc.

136
00:10:18,880 --> 00:10:23,680
The thing that jumped out to me about a couple of those examples, the space planning and

137
00:10:23,680 --> 00:10:31,600
the employee example, the onboarding example, is that the functions that own those processes

138
00:10:31,600 --> 00:10:40,200
are HR and facilities, probably not necessarily the most technology forward or AI-enabled organizations.

139
00:10:40,200 --> 00:10:46,200
So did they come to you with those opportunities and ideas, or is this part of the COE process

140
00:10:46,200 --> 00:10:48,280
where you're already embedded with them?

141
00:10:48,280 --> 00:10:52,040
How does the ideation process work there?

142
00:10:52,040 --> 00:10:55,320
Yeah, it was really an outward reach on our part.

143
00:10:55,320 --> 00:10:59,880
And then once we started to educate as to what was possible, then dialogue started happening

144
00:10:59,880 --> 00:11:05,680
within these non-technical organizations, and then something was more and out of that.

145
00:11:05,680 --> 00:11:07,240
So yeah, it was an outward reach.

146
00:11:07,240 --> 00:11:13,040
We started up a year ago, we started up what we call advanced ML within the Center for

147
00:11:13,040 --> 00:11:14,360
Machine Learning.

148
00:11:14,360 --> 00:11:18,160
This is a new group that is focused on three things.

149
00:11:18,160 --> 00:11:22,920
One is we had to get better about managing our university research partnerships, and

150
00:11:22,920 --> 00:11:25,000
we can talk about that a little bit later.

151
00:11:25,000 --> 00:11:30,440
The second item was we wanted to initiate an advocacy function where we could talk to

152
00:11:30,440 --> 00:11:36,920
our lines of businesses about what we were doing and how we might look at doing their

153
00:11:36,920 --> 00:11:40,920
jobs a little bit differently with machine learning, not necessarily automating the tasks

154
00:11:40,920 --> 00:11:44,640
that they're doing, but thinking about their jobs differently.

155
00:11:44,640 --> 00:11:53,560
The third component of that was really focused on getting a collaboration across Capital

156
00:11:53,560 --> 00:12:02,400
One on what are the challenges that we're all facing when it comes to machine learning,

157
00:12:02,400 --> 00:12:08,840
whether it is at the beginning of the machine learning lifecycle or at the end, let's identify

158
00:12:08,840 --> 00:12:13,560
what those challenges are, and then let's look at trying to come together as an organization

159
00:12:13,560 --> 00:12:17,360
rather than as individual victims to solve them.

160
00:12:17,360 --> 00:12:22,840
Yeah, it strikes me that when thinking about the types of examples that you gave, there

161
00:12:22,840 --> 00:12:26,480
have to be so many examples like that.

162
00:12:26,480 --> 00:12:35,720
But there are also these very high value, cord of financial services, types of use cases

163
00:12:35,720 --> 00:12:41,760
that I guess I'm trying to formulate a question around how do you prioritize when you've

164
00:12:41,760 --> 00:12:49,560
got this broad portfolio of opportunities, some of which are cord of the business and

165
00:12:49,560 --> 00:12:58,120
drive revenue and profitability and others are supporting organizations that aren't necessarily

166
00:12:58,120 --> 00:13:03,280
central to that conversation or aren't not often central to that conversation.

167
00:13:03,280 --> 00:13:06,480
Again, the HR facilities, that kind of thing.

168
00:13:06,480 --> 00:13:11,920
This group that I mentioned earlier, the advocacy group, is really was meant to bring all

169
00:13:11,920 --> 00:13:20,560
these people together, and then what that group also does is it opines on what are the

170
00:13:20,560 --> 00:13:24,400
higher priority areas that we should be focused on.

171
00:13:24,400 --> 00:13:29,400
There's a lot of discussion, the group got very large, it got to like 35 people across

172
00:13:29,400 --> 00:13:34,400
the company, and so we actually split it up into sort of a hierarchy of groups.

173
00:13:34,400 --> 00:13:39,640
But the idea is that there are high priority opportunities for us that we definitely have

174
00:13:39,640 --> 00:13:46,360
to be plugged into, but there is also enough bandwidth with this new enablement model that

175
00:13:46,360 --> 00:13:52,440
we can actually address some of these non-high priority use cases, just to really allow us

176
00:13:52,440 --> 00:13:57,080
to get some bedrock out there in terms of proliferating and democratizing ML.

177
00:13:57,080 --> 00:14:02,520
But certainly the high-price items get discussed and they are triaged, and it's no longer

178
00:14:02,520 --> 00:14:06,560
someone in the center for machine learning saying, I think this is the highest priority,

179
00:14:06,560 --> 00:14:08,760
this is coming from the business now.

180
00:14:08,760 --> 00:14:13,640
That's a flip on how we operated a year and a half ago.

181
00:14:13,640 --> 00:14:18,320
So it sounds like kind of putting some of the pieces together, transitioning the models

182
00:14:18,320 --> 00:14:25,560
of one where you don't necessarily own the development of the model and getting it into production,

183
00:14:25,560 --> 00:14:30,080
but you're supporting the business, as well as the platform and infrastructure pieces

184
00:14:30,080 --> 00:14:36,680
that you mentioned have created a velocity or acceleration or efficiency to being able

185
00:14:36,680 --> 00:14:41,320
to work on these projects that allows you to get more done in more areas.

186
00:14:41,320 --> 00:14:47,880
So most of the models are owned by the owners of the use cases, so the lines of business

187
00:14:47,880 --> 00:14:51,960
if you will, or even our tech partners, because we often partner with our tech partners

188
00:14:51,960 --> 00:14:54,800
for anomaly detection, let's say.

189
00:14:54,800 --> 00:14:59,000
So they own those use cases and they will typically own those models.

190
00:14:59,000 --> 00:15:05,840
There are some models that we own as well, and that we will start and take into production.

191
00:15:05,840 --> 00:15:12,480
But what we're trying to do really is to empower others to be self-sufficient and to be able

192
00:15:12,480 --> 00:15:19,640
to leverage these best practices, leverage the tools and tech that either are emerging

193
00:15:19,640 --> 00:15:26,000
or have been built so that we can do things in a very consistent, well-managed and again,

194
00:15:26,000 --> 00:15:27,600
compliant way.

195
00:15:27,600 --> 00:15:36,800
Can you give us a sense of scope for how many data scientists exist within Capital One

196
00:15:36,800 --> 00:15:42,560
relative to the number within your COE?

197
00:15:42,560 --> 00:15:49,840
Just trying to understand the leverage that you're trying to create with the COE, and

198
00:15:49,840 --> 00:15:53,160
maybe to just tack another question onto that.

199
00:15:53,160 --> 00:16:01,160
One of the things that I see a lot of in industry is the shifting away from thinking of the

200
00:16:01,160 --> 00:16:08,040
data scientists as this monolithic unicorn that can do everything from statistical analysis

201
00:16:08,040 --> 00:16:14,320
to production-ready code, and you are starting to see a lot more of machine learning engineers

202
00:16:14,320 --> 00:16:20,520
as a separate and distinct role that works with the data scientists as part of the team

203
00:16:20,520 --> 00:16:27,160
and brings more of a software engineering perspective and wondering how that plays out

204
00:16:27,160 --> 00:16:28,360
at Capital One.

205
00:16:28,360 --> 00:16:35,520
There are data scientists within the lines of business that have come up through the traditional

206
00:16:35,520 --> 00:16:41,080
ranks of statistical analysis and data science.

207
00:16:41,080 --> 00:16:48,560
There have been transformations in those skill sets to more toward understanding how to

208
00:16:48,560 --> 00:16:53,400
address this life cycle for machine learning, so the machine learning engineering part

209
00:16:53,400 --> 00:16:55,160
if you will.

210
00:16:55,160 --> 00:17:02,480
We have worked very closely with our tech college organization to do a couple of things.

211
00:17:02,480 --> 00:17:10,600
One is to provide curriculum on how to round out a data scientist to understand what that

212
00:17:10,600 --> 00:17:16,760
whole spectrum looks like, and also a rescilling program that we have not officially launched

213
00:17:16,760 --> 00:17:23,560
yet, we developed all the content for, but to take data engineers or software engineers

214
00:17:23,560 --> 00:17:30,440
and to get them converted into machine learning engineers, this rescilling program is all

215
00:17:30,440 --> 00:17:32,200
about that.

216
00:17:32,200 --> 00:17:38,360
Within the Center for Machine Learning, we do focus on bringing in machine learning engineers

217
00:17:38,360 --> 00:17:42,680
so that they will have a little software engineering, they will have data science, they will have

218
00:17:42,680 --> 00:17:52,800
DevOps, data engineering, and so they tend to cover all the entire life cycle of machine

219
00:17:52,800 --> 00:17:55,640
learning if you will.

220
00:17:55,640 --> 00:18:04,080
But as you might expect, there isn't enough supply of those types of people, but we are

221
00:18:04,080 --> 00:18:09,480
seeing the lines of business actually hiring this type of talent and then it becomes sort

222
00:18:09,480 --> 00:18:16,600
of viral, it's contagious and they are able to leverage that skill set and teach others

223
00:18:16,600 --> 00:18:21,880
and get some critical mass around the MLE.

224
00:18:21,880 --> 00:18:27,400
We still call our machine learning engineers software engineers from an organizational

225
00:18:27,400 --> 00:18:35,000
perspective, it's just kind of a legacy thing, but what I would say is that the concentration

226
00:18:35,000 --> 00:18:40,600
of machine learning engineers within the Center for machine learning is probably at

227
00:18:40,600 --> 00:18:46,360
par with some of the largest lines of business that we have.

228
00:18:46,360 --> 00:18:52,520
Now they certainly have many more business analysts, many more data scientists than we have

229
00:18:52,520 --> 00:18:54,680
because they have to.

230
00:18:54,680 --> 00:19:02,040
And as we begin to automate a lot of these processes involved in the machine learning life cycle,

231
00:19:02,040 --> 00:19:08,040
these data scientists are going to be a lot more capable of doing much more than they

232
00:19:08,040 --> 00:19:14,920
have before because they don't know how to do a deployment, it's going to be push button.

233
00:19:14,920 --> 00:19:19,920
If they don't know how to access a feature set, that's going to be a library call from

234
00:19:19,920 --> 00:19:23,680
a Python notebook, it's going to be fairly easy for them.

235
00:19:23,680 --> 00:19:32,880
And so part of our strategy for our platform or our MLE ecosystem is to really take a

236
00:19:32,880 --> 00:19:40,240
giant step, a step function, if you will, to make these data scientists a lot more capable.

237
00:19:40,240 --> 00:19:46,920
While we're also teaching them the rest of the, all the other parts that encompass what

238
00:19:46,920 --> 00:19:49,760
you need to know for machine learning engineering.

239
00:19:49,760 --> 00:19:57,320
Because your organization end up owning the MLE ecosystem, the platform and responsibility

240
00:19:57,320 --> 00:20:02,920
for its maintenance and upkeep and all that, or you more defining the requirements and

241
00:20:02,920 --> 00:20:09,760
partnering with an IT or other technology organization to get it out and keep it up.

242
00:20:09,760 --> 00:20:13,640
Now we're an enterprise function, so that's part of the new identity we've taken on is

243
00:20:13,640 --> 00:20:19,000
that we are going to own and support production systems, production machine learning systems.

244
00:20:19,000 --> 00:20:26,320
That is a new responsibility that has come into our organization and we embrace that and

245
00:20:26,320 --> 00:20:29,240
we look forward to dealing with that.

246
00:20:29,240 --> 00:20:35,080
Yeah, going back to the platform you mentioned, feature store feature component.

247
00:20:35,080 --> 00:20:39,160
And I guess this is fresh on my mind because of a conversation I had yesterday.

248
00:20:39,160 --> 00:20:49,480
But when a data scientist or an MLE engineer builds a feature and contributes it to a feature

249
00:20:49,480 --> 00:20:54,920
store type of environment, there's a certain level of responsibility now for that feature

250
00:20:54,920 --> 00:21:01,560
it becomes kind of a mini product in a sense and it limits them in the way they evolve it

251
00:21:01,560 --> 00:21:02,560
potentially.

252
00:21:02,560 --> 00:21:06,400
They are maybe an implied support requirement.

253
00:21:06,400 --> 00:21:10,000
Anything necessarily specific to features but anytime you're trying to take advantage

254
00:21:10,000 --> 00:21:15,880
of reuse, there's this implied contract that comes from kind of contributing your code

255
00:21:15,880 --> 00:21:17,480
or your feature.

256
00:21:17,480 --> 00:21:23,280
Is that an issue that you grapple with and how does it play out at Capital One?

257
00:21:23,280 --> 00:21:31,080
Yeah, that's actually a very good point and it's like careful what you wish for, because

258
00:21:31,080 --> 00:21:34,640
there's always the flip side of something that you have to deal with.

259
00:21:34,640 --> 00:21:41,200
One who contributes a feature to the feature platform has to understand the responsibility

260
00:21:41,200 --> 00:21:44,680
that goes with that and there is a responsibility.

261
00:21:44,680 --> 00:21:49,440
There's a responsibility to be able to field questions on that feature.

262
00:21:49,440 --> 00:21:53,240
There's a responsibility stand behind the engine that calculates that feature.

263
00:21:53,240 --> 00:21:56,960
There's a responsibility to have gone through the governance associated with that feature

264
00:21:56,960 --> 00:21:59,120
and the use cases that it serves.

265
00:21:59,120 --> 00:22:01,080
And so it's a much bigger deal.

266
00:22:01,080 --> 00:22:05,800
We can't just casually commit something back, you have to understand exactly what you're

267
00:22:05,800 --> 00:22:10,000
doing and where that accountability lies.

268
00:22:10,000 --> 00:22:11,400
The same thing is true with the platform.

269
00:22:11,400 --> 00:22:16,080
When we took that on, we had to understand that there may be calls at two o'clock in the

270
00:22:16,080 --> 00:22:18,800
morning associated with this platform.

271
00:22:18,800 --> 00:22:21,480
The Center for Machine Learning has never had that before.

272
00:22:21,480 --> 00:22:30,000
And when we added talent to the Center for Machine Learning, we brought people in who

273
00:22:30,000 --> 00:22:36,800
had production experience, enterprise, platform experience, because a lot of people there were

274
00:22:36,800 --> 00:22:39,840
machine learning engineers, some of them were fresh out of school, some of them were fresh

275
00:22:39,840 --> 00:22:41,760
out of their PhD programs.

276
00:22:41,760 --> 00:22:46,240
Not many really had that level of experience, so we had to retool and we brought in a very

277
00:22:46,240 --> 00:22:50,920
large group to integrate into the Center for Machine Learning.

278
00:22:50,920 --> 00:22:56,480
But the mindset is that there is an operational accountability that we have and that anyone

279
00:22:56,480 --> 00:23:02,320
is going to have who contributes features into the feature platform.

280
00:23:02,320 --> 00:23:06,160
Now if you think about our data transformation, the same thing is true there.

281
00:23:06,160 --> 00:23:11,920
If you manufacture data, you're almost in the same position where you have to be able

282
00:23:11,920 --> 00:23:16,360
to, you know, you're going to put data into an ecosystem and make it available for someone

283
00:23:16,360 --> 00:23:21,000
else and there has to be some level of accountability that goes with that.

284
00:23:21,000 --> 00:23:24,840
It just can't be something that you casually contribute.

285
00:23:24,840 --> 00:23:30,640
So this is all part of our well-managed initiative and I think, you know, to scale machine learning

286
00:23:30,640 --> 00:23:35,560
across our organization to really transform banking, we have, we understand that we're

287
00:23:35,560 --> 00:23:38,880
going to have to be more disciplined about how we conduct ourselves.

288
00:23:38,880 --> 00:23:41,240
You mentioned something interesting in there.

289
00:23:41,240 --> 00:23:46,440
It sounded like you brought in an organization that had existing platform capability.

290
00:23:46,440 --> 00:23:49,920
Was this kind of an internal transfer of a team?

291
00:23:49,920 --> 00:23:55,280
It was, that's exactly what it was, there was a platform team that was in another group.

292
00:23:55,280 --> 00:24:00,960
And when I started to have these conversations with our executive management, we knew that

293
00:24:00,960 --> 00:24:07,000
we had to seed our talent with this other type of talent.

294
00:24:07,000 --> 00:24:11,840
And so we went in and started to look at organizational adjustments and we wound up integrating

295
00:24:11,840 --> 00:24:14,680
an entire group and we actually did it in two ways.

296
00:24:14,680 --> 00:24:18,200
We brought in, brought in half the group and then we brought in the second half of the

297
00:24:18,200 --> 00:24:22,360
group after we had built up some critical mass.

298
00:24:22,360 --> 00:24:28,400
And so now what we've done is we actually have a platform's organization within the

299
00:24:28,400 --> 00:24:33,560
center for machine learning that is all about enterprise level, enterprise platform.

300
00:24:33,560 --> 00:24:38,560
So we're talking about resilience, scalability, logging, you know, all the things that you'd

301
00:24:38,560 --> 00:24:41,080
expect to see with the platform.

302
00:24:41,080 --> 00:24:45,520
This team has the experience in actually doing all those things.

303
00:24:45,520 --> 00:24:51,000
Yeah, it's a really great example of something that I talked to folks about all the time,

304
00:24:51,000 --> 00:24:55,920
you know, whereas machine learning and building out machine learning platforms might be new

305
00:24:55,920 --> 00:25:03,920
to an organization, the core kind of requirements and engineering practices probably aren't

306
00:25:03,920 --> 00:25:04,920
new.

307
00:25:04,920 --> 00:25:09,520
Capital One, for example, has, you know, I've known Capital One for many years to be investing

308
00:25:09,520 --> 00:25:15,400
heavily in platform as a service and container orchestration and all kinds of, not to mention

309
00:25:15,400 --> 00:25:21,680
we're here at Reinvent Cloud and building, you know, platforms to support traditional enterprise

310
00:25:21,680 --> 00:25:22,680
applications.

311
00:25:22,680 --> 00:25:29,960
A lot of the principles that go into supporting, you know, enterprise apps, web apps are similar

312
00:25:29,960 --> 00:25:34,640
to, you know, some of the things you need to be thinking about when you're supporting machine

313
00:25:34,640 --> 00:25:38,720
learning and AI-based systems, is that your experience as well?

314
00:25:38,720 --> 00:25:39,720
Absolutely.

315
00:25:39,720 --> 00:25:40,720
Yeah.

316
00:25:40,720 --> 00:25:41,720
A spot on.

317
00:25:41,720 --> 00:25:42,960
It's definitely my experience.

318
00:25:42,960 --> 00:25:49,760
One of the things that you've mentioned is kind of a common thread throughout the discussion

319
00:25:49,760 --> 00:25:52,960
is the interface with risk and governance.

320
00:25:52,960 --> 00:25:56,960
Can you talk a little bit about more about that process?

321
00:25:56,960 --> 00:25:59,160
Yeah, absolutely.

322
00:25:59,160 --> 00:26:05,080
I believe that we are actually in a very unique and interesting position at Capital One when

323
00:26:05,080 --> 00:26:10,280
it comes to the governance around machine learning.

324
00:26:10,280 --> 00:26:17,000
We have been living in a regulated world for many, many years, obviously, and with varying

325
00:26:17,000 --> 00:26:22,360
degrees of regulatory oversight, as I mentioned before, material models and non- and immaterial

326
00:26:22,360 --> 00:26:29,080
models, the ones that don't have to go through such high levels of regulatory scrutiny, but

327
00:26:29,080 --> 00:26:36,840
we understand the process and we understand all these little sort of phases and artifacts

328
00:26:36,840 --> 00:26:45,600
and responsibilities and the laws of making sure that at the end of the day, we are compliant

329
00:26:45,600 --> 00:26:50,760
with the regulatory frameworks that we have to navigate within today manually.

330
00:26:50,760 --> 00:26:53,280
So let me give you an example.

331
00:26:53,280 --> 00:26:58,640
When we create a machine learning model, we have to go through a process to validate

332
00:26:58,640 --> 00:27:03,400
the outcomes and how this model has actually performed.

333
00:27:03,400 --> 00:27:07,480
And one way one of the approaches that we might take is to build what are called these

334
00:27:07,480 --> 00:27:09,920
surrogate validation models.

335
00:27:09,920 --> 00:27:16,240
So we go in and handcraft these other models and these other models either support the

336
00:27:16,240 --> 00:27:21,160
outcomes of the model that you are trying to get through governance or they might refute

337
00:27:21,160 --> 00:27:22,160
it.

338
00:27:22,160 --> 00:27:24,320
They might show something that is a little bit different.

339
00:27:24,320 --> 00:27:27,440
And then you have to write all this stuff up in a document and you have to get that document

340
00:27:27,440 --> 00:27:30,640
through and you have to defend it and so on and so forth.

341
00:27:30,640 --> 00:27:35,760
Is the surrogate validation model, is that like a test suite or is it?

342
00:27:35,760 --> 00:27:36,760
You can think of it.

343
00:27:36,760 --> 00:27:40,240
You can think of it as kind of model or yeah, you can think of it as that, right?

344
00:27:40,240 --> 00:27:45,000
But like for example, if you're doing a gradient boosted machine, you might have surrogate

345
00:27:45,000 --> 00:27:51,080
models that are logistic regression models that are something that are types of models

346
00:27:51,080 --> 00:27:55,840
that we've been able to linear models that we've been able to deal with for many, many

347
00:27:55,840 --> 00:28:02,840
years and we can see how these two models behave with, you know, again, certain types

348
00:28:02,840 --> 00:28:05,760
of data sets that they're scoring.

349
00:28:05,760 --> 00:28:11,160
The opportunity for us is because we know this process so well, we can actually use ML

350
00:28:11,160 --> 00:28:14,000
for governing ML.

351
00:28:14,000 --> 00:28:19,880
We can use natural language processing for auto-document generation.

352
00:28:19,880 --> 00:28:25,800
We can use auto ML to generate the surrogate models automatically and then run them.

353
00:28:25,800 --> 00:28:30,880
You know, and we can interpret those documents on the other side.

354
00:28:30,880 --> 00:28:39,760
We can double click into the validation models and surface the, either the supporting argument

355
00:28:39,760 --> 00:28:42,600
or for those models.

356
00:28:42,600 --> 00:28:46,080
If you think about it even beyond that, when you talk to these model risk officers, their

357
00:28:46,080 --> 00:28:48,760
minds are, I mean, the wheels are turning in their heads.

358
00:28:48,760 --> 00:28:53,320
They see so many opportunities for doing some really cool things like one of one model

359
00:28:53,320 --> 00:28:59,480
risk officers said to me, I'd like to have this sort of surrogate recommendation engine

360
00:28:59,480 --> 00:29:05,400
that's saying, hey, I just picked up this bias in this data set and your feature set.

361
00:29:05,400 --> 00:29:07,640
Here are the artifacts that I picked up.

362
00:29:07,640 --> 00:29:11,560
This thing's running kind of alongside you, you know, and it's picking this kind of

363
00:29:11,560 --> 00:29:18,960
bias up and recommend you make these changes or you take a look at these particular features.

364
00:29:18,960 --> 00:29:20,360
That's cool stuff.

365
00:29:20,360 --> 00:29:23,640
Then you can actually do, if you could actually do something like that.

366
00:29:23,640 --> 00:29:30,200
So we're looking into piloting and doing some POCs around a lot of these kind of interesting

367
00:29:30,200 --> 00:29:33,680
model governance kinds of tasks.

368
00:29:33,680 --> 00:29:38,240
Now I don't think we'll ever take the human out of the loop, especially for those highly

369
00:29:38,240 --> 00:29:43,680
regulated, regulated use cases, but I think we're going to make their workload significantly

370
00:29:43,680 --> 00:29:45,240
less laborious.

371
00:29:45,240 --> 00:29:50,800
If you think about the proliferation of machine learning models at Capital One, we don't

372
00:29:50,800 --> 00:29:55,560
want to just stack them up on the model risk officers back door and we're in the same

373
00:29:55,560 --> 00:29:56,560
boat we're in today.

374
00:29:56,560 --> 00:29:59,960
We're just, they just have a huge backlog that they have to work through.

375
00:29:59,960 --> 00:30:04,080
We want to give them the tools to actually get through this and we want to be able to

376
00:30:04,080 --> 00:30:06,120
build that right into our platform.

377
00:30:06,120 --> 00:30:11,080
I think that's where we are thinking about the ideas that we're thinking about are far

378
00:30:11,080 --> 00:30:16,320
beyond what others are, as they're looking at machine learning, just based on the conversations

379
00:30:16,320 --> 00:30:23,120
that I've had with them and that I'm seeing in the trade racks, is that the regulated,

380
00:30:23,120 --> 00:30:27,640
highly regulated models are certainly going to undergo tremendous scrutiny.

381
00:30:27,640 --> 00:30:28,880
We know that.

382
00:30:28,880 --> 00:30:31,840
But there's this other class of models that they're starting to look at as well and they're

383
00:30:31,840 --> 00:30:38,400
saying even though the regulatory frameworks don't cover those models, they need to be

384
00:30:38,400 --> 00:30:43,760
explained and interpreted and bias detected just like these other models and I think there's

385
00:30:43,760 --> 00:30:48,360
going to be, the sentiment is going to be that all models have to go through this sort

386
00:30:48,360 --> 00:30:52,560
of scrutiny and I think that tech companies are going to be facing the same thing.

387
00:30:52,560 --> 00:30:58,520
So what kind of advice would you give them, companies that don't have the history of

388
00:30:58,520 --> 00:31:05,800
operating in a regulatory framework but want to either see the writing on the wall or

389
00:31:05,800 --> 00:31:11,680
they are internally motivated to use machine learning in a responsible way.

390
00:31:11,680 --> 00:31:18,240
How do they kind of bootstrap the internal frameworks to do so?

391
00:31:18,240 --> 00:31:22,800
There are some, I mentioned earlier about the rate of innovation in the industry.

392
00:31:22,800 --> 00:31:29,080
There are some emerging companies that are providing some libraries for some of this,

393
00:31:29,080 --> 00:31:31,000
some types of bias detection.

394
00:31:31,000 --> 00:31:37,920
So there's some basic functions that you can do today with libraries and Python and then

395
00:31:37,920 --> 00:31:44,280
there's companies that are springing up weekly, it seems like that are addressing explainable

396
00:31:44,280 --> 00:31:46,480
and interpretable AI.

397
00:31:46,480 --> 00:31:53,240
So I would say that to those companies that don't have the expertise to just get, at

398
00:31:53,240 --> 00:31:59,720
least get savvy about what's happening and there's so much literature out there about

399
00:31:59,720 --> 00:32:00,720
this.

400
00:32:00,720 --> 00:32:06,720
So if you're in the cloud with the AWS, AWS does have some basic capability with Shapley

401
00:32:06,720 --> 00:32:14,000
values that you can use to help explain or interpret your models.

402
00:32:14,000 --> 00:32:20,720
So there's some fundamental pieces there that I think people should at least become aware

403
00:32:20,720 --> 00:32:21,720
of.

404
00:32:21,720 --> 00:32:25,840
But it's certainly something you should not ignore because I believe that this is going

405
00:32:25,840 --> 00:32:30,120
to be, in five years from now, this is going to be something that is expected of everything

406
00:32:30,120 --> 00:32:31,880
that we do with machine learning.

407
00:32:31,880 --> 00:32:34,560
We're going to have to be able to explain that model and I think it's going to be all

408
00:32:34,560 --> 00:32:35,560
be built in.

409
00:32:35,560 --> 00:32:40,600
Well, Dave, thanks so much for taking the time to share what you're up to.

410
00:32:40,600 --> 00:32:46,440
It sounds like it continues to be an interesting run and you're doing a lot of interesting things

411
00:32:46,440 --> 00:32:47,440
there at Capital One.

412
00:32:47,440 --> 00:32:49,880
I appreciate the opportunity to learn about it.

413
00:32:49,880 --> 00:32:51,160
Thanks for having me, Sam.

414
00:32:51,160 --> 00:32:52,160
Appreciate it.

415
00:32:52,160 --> 00:32:53,160
Thank you.

416
00:32:53,160 --> 00:33:01,920
All right, everyone, that's our show for today to follow along with our reinvent series.

417
00:33:01,920 --> 00:33:07,360
Visit twimmelai.com slash reinvent 2019.

418
00:33:07,360 --> 00:33:11,480
Thanks once again to Capital One for their sponsorship of this series.

419
00:33:11,480 --> 00:33:17,320
Be sure to check out capital one calm slash tech slash explore to learn more about their

420
00:33:17,320 --> 00:33:20,040
ML and AI research.

421
00:33:20,040 --> 00:33:22,960
Thanks so much for listening and catch you next time.

