Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In this episode, we're joined by Nina Mielehn, researcher and lecturer at Stanford University.
Nina and I recently spoke about her work in the field of geometric statistics and machine
learning.
Specifically, we discussed the application of reminion geometry, which is the study of
curve surfaces to ML.
Reminion geometry can be helpful in building machine learning models in a number of situations,
including in computational anatomy and medicine, where it helps Nina create models of organs
like the brain and heart.
In our discussion, we review the differences between reminion and euclidean geometry and
theory and practice and discuss several examples from Nina's research, which is a Python
package that simplifies computations and statistics on manifolds with geometric structures.
Before we dive into the conversation, I'd like to send a huge thanks to our friends at
IBM for their sponsorship of this episode.
Interested in exploring code patterns, leveraging multiple technologies, including ML and AI,
then check out IBM Developer.
With more than 100 open source programs, a library of knowledge resources, developer advocates
ready to help and a global community of developers, what in the world will you create?
Dive in at IBM.biz slash ML AI podcast, and be sure to let them know that Swim will
send you.
And now on to the show.
Alright everyone, I am on the line with Nina Mielehn.
Nina is a researcher and lecturer in machine learning for computational anatomy at Stanford
University.
Nina, welcome to this week in machine learning and AI.
Hello, hello Sam, thanks for calling me today and hello everyone.
Absolutely, absolutely.
So, let's just start with a little bit of background.
And in particular, you started out in theoretical physics.
How did you make your way from general relativity to machine learning?
Yes, that's correct, I started with theoretical physics, I did my master in theoretical physics
and mathematical physics, which I think it might be an original background for someone
in machine learning these days.
And so yeah, initially I was looking at different mathematical models of reality, specifically
at the infinitesimally small and the very big, so I was looking at models of particles
elementary particles and their collisions at very high energies and also at models of
the very big, like the stars, the galaxies, space, time and all these things.
And so yes, I had courses like general relativity, black holes, and this was very interesting
because after my master, in fact, I moved to machine learning for the medical imaging field.
And at that time, a lot of people asked me the question like, what does this have to
do with machine learning now?
Why, why are you switching?
What's the link?
I started a PhD in machine learning and how could I do that after a matter in mathematical
physics.
And in fact, both fields are really related and the main link they have is geometry.
So in theoretical physics, when I was looking at space time, in fact, I was looking at
the geometry of space time.
So you might know that Einstein in the early 90s used geometry, actually a particular kind
of geometry, which is called remanion geometry.
And so Einstein used geometry to describe space time and how the geometry of space time
was changing depending on the massive objects that are in space time.
So for example, if you go very close to a black hole, then time will slow down.
And so the geometry of time near the black hole is changed.
So geometry has a lot to do with the mathematical model that you put in theoretical physics.
And then between my master and my PhD, I went to computational anatomy.
So medical imaging, I was looking at organ shapes.
And here again, I got to do a lot of geometry and remanion geometry.
Because in fact, the data, the data in machine learning that I was looking at are geometrical
data.
So they live in a space, in a data space that also has a particular geometry.
And so I was studying the geometry of this data space just in the same way that I was studying
the geometry of space time during my master.
So that's how I got from theoretical physics to machine learning for data belonging to
these geometric spaces.
And then after that, I kind of stayed in machine learning.
So I was a software engineer for a little bit in a machine learning startup.
And now I've come back to academia.
And I'm now a postdoc at Stanford University, as you said it.
And from my PhD to my work in software engineering and my postdoc through these three stages,
three in these three steps, I was looking at data belonging to geometric spaces.
And these data have been mostly organ shapes.
So you said it, I'm in computational anatomy.
So I built computational models of the human body.
And so the data, I have are organ shapes.
During my PhD, I looked at brain shapes, how brain shapes vary when you have a disease.
For example, isomers disease or other forms of dementia.
When I was a software engineer, I looked at hot shapes.
So how the shape of your heart may vary if you have different cardiovascular diseases.
And now back as a postdoc, I do two things.
I look at brain shapes again, but I also look at the shapes of abstract data spaces.
So that's how yeah, I'm kind of merging all the geometric background that I have gathered
to come to the field of machine learning.
Maybe we should start from the beginning and have you explain what rhymanian geometry
is and how is it distinguished from the Euclidean geometry that we tend to think of?
Yes, sure.
So remanian geometry is this theory of mathematics that allowed Einstein to describe the geometry
of space time.
And actually, it's a very powerful mathematical theory, which is why I think Einstein has
used it to describe this very complicated space.
And it's a generalization of Euclidean geometry.
Actually, rhymanian geometry is locally Euclidean.
So let's take an example.
If you consider the earth as the planet, you look at the surface of the earth locally.
So for you and me in a given room, in a building, locally, the geometry is flat.
It looks like we live on a flat to dimensional plane.
But if you look at the global geometry of the earth's surface, it's a sphere.
It's a 2-d sphere.
So the geometry is in fact curved.
No way.
No way.
I know.
I'm just trolling the flat earthers out there.
Sorry.
No, no, no.
But that's a simple example to explain the generalization of Euclidean geometry to
rhymanian geometry, that's one example that everybody uses, because it's more intuitive for
everybody.
But it describes very well, actually, what it is.
So rhymanian geometry is locally Euclidean.
But if you look at the global shape of your space, then it's going to be curved.
And when you start to do statistics on the curved spaces, then actually, everything
that we know about usual statistics breaks down.
So let me come back to the example of the earth for a little bit.
And then we'll go into other examples if you prefer.
But for the earth, imagine you take two cities on the surface of the earth.
And you want to do statistics on the position of these two cities.
So very simple example.
We have only two data points.
And an example of statistics that you may want to compute if, for example, an average
in position of these two cities.
If you use usual statistics in Euclidean space, you're going to compute a city that will
be in between the two cities that you have on the surface of the earth.
But this city, if it's the middle of the chord joining the two cities that you have on the
surface of the earth, the average city will be inside the earth, will be on the surface
of the earth.
And so you are in a situation where you want to compute an average of cities that cannot
possibly be a city, because it's not even on the earth.
And that's a very simple example again, but that's a very, very important example because
it shows that if you do statistics for data that belongs to curved spaces, even the very
definition of mean cannot apply, because the very definition of mean is a linear definition.
The mean is a weighted sum of the elements, so it's linear applying this linear definition
to a nonlinear space, this completely breaks down.
And that's why we got into developing this new field that we call geometric statistics.
And we are asking the question, how should we generalize everything that we know about
statistics to these curved spaces?
When we're thinking about geometric statistics and Romanian geometries, are we exclusively
looking at statistics as it relates to the points on these curves, as opposed to the relationships
between different curves, for example?
Yeah, so when we look at geometric statistics, we mean statistics for data that belong to
these curved spaces.
But actually, you can look at curves on these curved spaces, and the space of curves on curved
spaces is also itself a space that is nonlinear.
So you can look at different kind of data space, and as soon as you have something curved
somewhere, it's going to propagate, and you're going to have some geometric somewhere.
And I think the thought that was the origin of that question was trying to apply this to
your computational anatomy types of problems.
I'm imagining in those fields, yes, I'm interested in describing statistically the surface
of the heart.
But I'm also interested in things that are happening within the heart, and maybe there
are complex surfaces that are composed of different things that are easier described with
multiple curves as opposed to a single curve.
So maybe you can use that as a segue to explain how this is applied in that field.
Yes, so for sure, there are two types of geometry that you can consider when you talk about
these geometric objects, like the surface of the heart.
First of all, you can take one heart and look at its surface, and this surface of this
one heart will have a particular geometry, but I'm not looking at one heart and the surface
of one heart.
Actually, I'm saying, in the way we model things currently, I'm saying that this heart
is just a point in very high dimensional space, abstract space, that represents its shape.
So the shape of the space I'm looking at is the shape of the data space.
So actually, for example, if you go back to the example of the heart, imagine you have
a heart and you want to study its shape, what we usually do is that we say, okay, the
shape of this heart is actually going to be the deformation of a template heart.
So we assume that there is a template shape, that is, for example, the healthy heart
shape.
So we fix that and then we represent each patient heart by the deformation of the template
shape to the patient shape.
And so now each heart shape is represented by a deformation.
So one data is a deformation.
And so if you have different patients, each of them is represented by the deformation
from the template heart to the heart.
And so your data are these deformations that are a point in a high dimensional space,
space, which is the space of deformation.
So it's a bit more abstract that looking at the surface of one heart is actually one
data is a deformation from a template to the patient space.
And the space of data is the space of deformations, which is curved.
Okay, okay.
I think I'm following here.
So a question that pops up for me is I can imagine representing these, you know, a particular
heart, a particular deformation as, you know, some set of parameters that kind of describe
in the physical world, you know, how the heart is deformed from the template.
But I can also imagine something more abstract, kind of like an embedding applied to, you
know, this heart space.
Does that concept apply here?
Yeah, the completely makes sense.
So I think initially people while looking in physical models, like mechanical models and
imagining which clinical parameters may control the deformation from template heart shape
to a patient heart shape.
But then there are so many parameters that you can learn and maybe you're not going to
find all of them and then even if you find this clinical parameters that govern the transformation
from a heart shape to another, then how do you translate that to the next organ shape?
You will need to do that all over again to do a deformation from a healthy brain shape
to another brain shape.
And so in our case, we are mourning the second scenario that you've described.
We just look at the abstract deformation and we parameterize it just in terms of geometry.
So this deformation, this point in our curved space, we represent it mathematically as a
deformorphism.
And then we look at how many parameters we need to describe this deformorphic transformation.
So we look at that in the abstract way.
And also because, you know, I don't have a medical background, so I come from the mathematical,
from a mathematical background.
So I think that's also a reason why we use our mathematical tools to describe transformation.
Now the hope is that eventually both fields will merge together.
So by describing these transformations, these deformations of organ shapes of heart shape
by mathematical or geometric parameters, maybe by doing so we will discover new clinical
parameters.
And actually we started to see a hint of that when we did a study on heart shapes.
So we had many hearts, together many heart shapes, that are set of many heart shapes.
And we performed a principal component analysis on these heart shapes by using deformations.
So we were looking at what were the main variations in terms of geometric shapes within these
heart shapes.
And the main variation geometric deformation that we found was the size of the heart, which
is a clinical irrelevant parameter because it's very correlated with the body mass index.
So basically just by looking at which general direction we were seeing in this data set
of heart shapes, we found one principal component, which was linked to the body mass index.
So yes, we have an abstract mathematical geometric approach to describing this transformation,
but the hope is that we will recover the clinical, the usual clinical parameters and maybe
even discover more clinical parameters.
And so I think this is a very good instance of a pretty disciplinary approach because
historically medicine and computer science have not been that linked.
And now that we have more medical images being produced everyday in hospitals and also
more computing power to actually analyze these images and do machine learning on these
images.
Now is a very good time to have both fields of mathematics and medicine or of computer
science and medicine merging to see if we can establish a dialogue between the geometric
parameters and the clinical parameters.
Can you elaborate a bit more on that example?
What did the data set look like?
Was it an image-based data set?
How did you go from presumably again some set of images in two or three-dimensional Euclidean
space, again, another assumption to geometric representations and then how exactly did you
apply PCA to that to determine these deformations, right, the deformations and derive this size
factor?
Yes.
So originally it was an image data set, but we extracted, so image of the heart, but we
extracted heart surfaces in terms of measures.
So in order to do that, you have in medical imaging community what we call segmentation algorithm,
so there are many data available and they basically do that.
They are able to take an image and extract the surface of the heart.
So for us, the starting point was this data set of meshed surfaces of the heart.
In order to go from the surface, one data point to the deformation, one data point, we
use this template modelization.
In the sense that we took one heart shape and we named it the template.
And so each of the other heart surfaces was a deformation from this template to the
data point.
You arbitrarily picked a heart shape to what you call the template.
I envisioned earlier that the template was derived in some way, akin to an average
or some statistical template.
Exactly.
So yeah, and actually template shape estimation is the main point of my PhD thesis on his
whole.
I wanted to simplify a little bit, but you're completely right.
So in fact, the template shape and the deformations are jointly computed.
So you have an iterative algorithm that is called template shape estimation that works
as follows first, you take all the heart surfaces and you register them.
So you align them in order to correct them.
Find a centroid and try to make them overlap as much as possible.
Exactly.
So that the surface themselves, the meshes overlap as much as possible.
This is called registration.
After this registration step, you compute an average.
If you're in images, you do that in terms of pixels, if you're with surfaces, you find
a middle for each corresponding meshes.
And so after the registration step, you have another edging step.
This averaging step gives you an initial estimate of the template.
And then you iterate the algorithm.
Now that you have an initial estimate of the template, you register again, everybody,
not among themselves, but to the template.
And you have reg them again.
You have a second estimate of the template.
And you do that over and over again until you converge to a template.
And so in the end, you have a template shape.
And each single image or each single surface being described as a deformation from this
template.
And yeah, actually, as I said, my PhD thesis was exactly on this algorithm because this
is an algorithm that is very well known both in medical imaging community, but also in
signal processing community, where you want to align signals and then compute an average
of signals.
So in terms of signals, you might imagine spikes on neurons or stuff like that, temporal signals.
And so they also have, if they want to see the shape of a signal, they also often compute
a template shape and do registration.
So it's a very well known algorithm in the field.
And the goal of my PhD study was to analyze the statistical properties of this algorithm,
which can be done using geometry, and which hadn't been done before.
And so I'm imagining if you've got this template heart and you've got a corpus of other heart
meshes or representations, that one of the things that you might want to do is like
find a distribution of the way that a heart or heart's in general differ from the template
and that part of what makes that difficult in trying to do that in Euclidean geometry
is that you have to do it in any given point on the surface, a bunch of different directions.
And is that part of what makes geometric models easier to apply here?
Yeah, so that's exactly what we want to do.
We want to describe the viability, for example, in a hard shape.
And we do that on a geometric space.
Not necessarily because it's going to be easier, but it's because it is what it makes sense.
So if we go back, you know, to the example, when we were averaging two CDs on the surface
of the earth, if we are computing Euclidean average, then we get something that is not a
city.
Going to the heart now, if we want to compute the average of all the healthy heart, for
example, to get a template healthy heart, if we were to do that using Euclidean geometry,
we will get something that doesn't look like a heart at all.
So it's more in order to be able to describe the variability of heart by summary statistics
like a mean, and then a standard deviation, we want this summary statistics to make sense.
So we want to the mean of hard surfaces to be a hard surface.
And that's why we use geometric statistics.
Makes sense, makes sense.
And then to describe the geometry of the hard shapes, we can do many things.
So I'm taking the example of the mean heart shape, but then we can look at variations.
So that's where PCA comes in.
What are the healthy variations of a heart shape?
For example, size can be a healthy variation of a heart shape.
But then we can also look at clusters.
And the intuition is that the different clusters that you might see will correspond.
One will be for healthy heart shapes, and another one may be for a certain pathology that
can be seen in a heart shape.
And I'll show you what this is what we do for brain shapes, where you can, if you do that,
a clustering on brain shapes, and you want to separate healthy brain shapes versus brain
shapes with Alzheimer's disease, then you can easily separate that because Alzheimer's
disease is a disease that you can see on the brain shapes.
So there is an atrophy of the cerebral cortex that changes the brain shapes.
How do you describe these types of geometric statistics in terms of, are there different
sets of distributions or different fundamental laws, what's different about the way things
work in the geometric world compared to what we're used to?
Yes.
So basically every time that you had something that was linear in the Euclidean word, then
you need to say that into something that is not linear in the Riemannian word.
And even the very basic operations don't work anymore.
So if you're in a Euclidean space and you have two points on this Euclidean space, you
can do a subtraction of a point to another and you get a vector.
So if you have point a and point b, if you do b minus a, you get vector a b.
Now even this very simple operation, the subtraction, you cannot do it in Riemannian geometry.
So this variation has been translated to the Riemannian geometric framework and we call
that the logarithm.
It's not linked to the logarithm as a function, it's just called like that.
Same thing in the Euclidean word, if you have a point a, you can add it a vector u a plus
u and you get another point.
You use that vector to shoot from point a and you arrive at another point.
So the addition, this addition doesn't even exist in Riemannian geometry neither because
if you imagine that you're in the surface of the earth, if you shoot a long attention
vector and you see what you arrive, you're not going to be on the earth.
So you want to generalize this operation in order to do very basic computation in Riemannian
geometry.
And so I took the example of the subtraction of the addition, basic operation that don't
exist as is in Riemannian geometry and it we needed to generalize.
And in fact, the way that we need to generalize these very basics of operations, links to the
fact that we have generalize optimization algorithm, for example.
So if you think about an optimization algorithm, you're in a machine learning framework and
you want to find the parameter theta that optimizes your criterion.
Then when you do gradient descent, that's what you're doing.
You have your current estimate of theta and you shoot along the gradient.
So in Euclidean geometry, you add you have a point which is theta and you use a vector
which is the gradient to shoot along it.
So now this operation, which is addition of a vector to a point, we cannot do it in
Riemannian geometry.
I mean, now we can, because we have generalized the addition, but we couldn't do it as is
in Riemannian geometry, otherwise we would have gotten out of the space of parameters.
Just the same way that we would have gotten out of the surface of the earth.
So we've generalized the addition, but we've also made an analogous construct to a
vector that's, you know, a curved vector on the surface, correct?
Yes, yes, but in fact, actually in Riemannian geometry, we use the concept of tangent vectors.
So if you take your curved space, if you take the surface of the earth, remember I said
that Riemannian geometry was locally Euclidean, so on the surface of the earth, if you are
in your office on the earth, then the earth looks flat.
So Riemannian geometry is locally Euclidean, and in fact, it means that at each point of
a Riemannian manifold, so a curved space, there is a tangent space that locally approximates
very well your curved space.
And so actually we use vectors all as well, because we use at the point of a curved space,
we consider the tangent vectors at this point.
And then we transform them into curved vector, if you want, so we add a tangent vector to
a point.
This brings us to another point on the curved space through a curved vector, you can imagine
it like that.
But since we have so many tools in Euclidean geometry, we wanted also to bring these tools
back on Riemannian geometry, so we bring them back locally at each tangent space of the
curved space.
There's some vector at this tangent space that gets projected onto the surface, and that's
your Riemannian vector, so to speak.
Yes, yes, that's the way it works.
Butchering terminology of course.
That gives us a little bit of a foundation.
Part of what you've been up to recently is building out a set of tools to make this
a little bit more accessible.
Can you talk a little bit about that work?
Yes, so yeah, as I said, I'm very passionate about geometry, and even though I learned geometry
in theoretical physics, when I saw how it could be applied to computational anatomy and
computational medicine, I really wanted to democratize the use of geometry.
And so that's why starting last year, I started to build a Python package to be able to
democratize geometry.
So it's a package that is called GeomStats, it stands for geometric statistics.
And basically, it allows you to do this addition and subtraction, so generalization of addition
and subtraction on Riemannian spaces, easily.
It encapsulates the geometry so that you just have to think about, okay, I'm going to use
the generalization of the addition.
I'm going to use the generalization of the subtraction.
And I wanted to do that because we have seen that, actually, in machine learning, a lot
of spaces, data spaces have geometric properties.
For example, if you're looking at protein shapes and you want to describe the shape of a protein
by the list of angles from one carbon of the carbon bone to another, then you have a
list of angles that belong to a set of spheres.
How many ways, many examples in machine learning where data belong to geometric spaces?
And so we wanted to give a package that allows people to do the computations on these geometric
spaces.
So we have implemented various spaces like hypersphere, hyperbolic spaces, also space of matrices.
For example, symmetric positive definite matrices or rotation matrices, et cetera, so that
people could do proper averaging in these geometric spaces.
So this is a Python package that is available on GitHub.
And we're also currently writing a journal paper that explains the need and the use of this
package for the machine learning community.
Interesting.
So does Python support something like polymorphism that would allow you to just kind of drop
in and replace the operations provided by this package?
By plus and minus.
By plus and minus.
Yeah, exactly.
Or do you have to kind of update your code to use the package?
Yeah.
No.
Right now you have to update your code.
It's true that it will be a very interesting use of polymorphism.
No.
Right now we've given these operations their names, so exponential and logarithm.
The name they have in mathematics.
And it's up to the user to do the translation.
But that's actually an amazing idea we could use polymorphism.
But I guess we also don't want to completely hide the fact that it's geometric, otherwise
people might forget about it.
And then when people run into bugs, then they won't understand what's going on.
So for example, if you remember the example of averaging two hard shapes, if you do that
in the Euclidean way, you're going to get something that is not a hard shape.
And this is not a bug of your code.
This is not a precision problem.
This is a mathematical, this error as a mathematical foundation.
Because the space of hard shapes is curved.
So we don't want to completely hide the fact that there is geometry, but more encapsulate
some subtle concept into a user-friendly environment.
Yeah.
And so using this library to implement something like gradient descent, is it, besides some
of the fact that you're not using polymorphism, is it essentially substituting your pluses
and minuses with your exponents and logarithms?
Yes.
Yes.
So that's a way of thinking.
So the library, we've used it at three points, at three levels in machine learning pipelines.
So if you imagine a supervised learning, a typical supervised learning pipeline where you
have an input, it's x, and then you want to predict an output y from this input x.
And you do that by learning a function that can have a given parameter theta.
Now we can put geometry at theta.
So you say, oh, the space of parameter is actually curved.
And the gradient descent in that curve space will need to use our exponential and logarithm,
which are the generalization of the addition and the subtraction.
So that's one thing that we've put into the package, actually, by creating our own version
of keras.
So now when you, when you, we have created all of keras, no, no, no, no, so we have re-implemented
keras.
No, no, no, no.
We have forked the keras repository and just modified a little bit so that now you can
add a parameter in some keras function, this parameter is called manifold.
And basically, if you say manifold is hypersphere, then it knows that instead of using the addition
and subtraction, they will need to use, the code will need to use exponential and logarithm.
So that's the first way of putting geometry in this supervised learning pipeline is putting
it for the space of the parameter theta.
But now the way we've been using geometric statistics as well is by putting geometry
on the space of the input x, but also on the space of the output y.
And we've done an interesting example, putting geometry on the space of the output y.
So we did a regression on the groups.
So the situation was as follows, it was again, medical images and we had slices of brain
volumes, actually, a fetal brain volume.
With fetal brain MRI, actually, you have a lot of motion, so the images are motion
corrected because the fetal moves in the belly of the mother, the mother may move as well.
And so this is a case of medical images, when if you take slices, then the slices are
not going to be aligned within another.
And when you want to reconstruct the 3D volume of the fetal brain, then you're running
to problem.
And so what we've done using GM stats is that for each slice of the fetal brain, we
were predicting the optimal pose of this slice in the reconstructed brain volume.
And we did that using computational neural network, predicting a pose where a pose is
a translation.
So it's the position of the slice in the 3D brain volume and the orientation, a rotation.
The rotation of the slice in the 3D brain volume, so we were able to take which slice was supposed
to be where in the future reconstructed 3D brain volume.
And in order to do that, we put some geometry on the space of these positions and orientations,
which was the space of the output for this supervised learning algorithm.
Input is slice of brain and output is position and orientation of this slice of brain.
And by considering the fact that positions and orientations, they don't belong to your
rotating space.
They naturally belong to a curved space, which is a lead group, is the space of translations
and rotations.
And by taking into account the geometry of the space of the output of the supervised learning
algorithm, we were able to get better brain volume reconstruction.
So you can put geometry on the space of parameters, you can put geometry on the space of the input,
you can put geometry on the space of the outputs, and you might increase the accuracy of your
results.
In general, are these fundamental operations in the remanion space of the same order of
complexity as the Euclidean analogs?
Which will depend for which space, but what's interesting is that they're actually exactly
the same on the infinitesimal level.
So if you really close, if you imagine your curved space and you close and you imagine
your curved space, which has a tangent space at a given point, if you're really close to
this given point, then using your curved vector or your tangent vector is the same.
So at the infinitesimal scale, yeah, they're exactly the same.
I'm wondering if you've ever tried to apply this someplace where just to try it and were
surprised that it actually works, or are you usually applying this because you have a strong
intuition based on the problem that you should apply it?
No, usually it's based on a strong intuition.
I'm trying to think, yeah, I think it's more the second scenario because I was just wondering
if there are problems that might have some latent geometricness to them that we don't
know.
Now that you've made this easy with a library, if we could just apply it willy-nilly to
see if that has some value.
Yeah.
So I think one set of problems is every time you're dealing with rotations and translation,
but especially rotations.
So rotation is a very intuitive geometric example because we have an intuition of how it works.
You can rotate an object.
And even if it's simple, it appears in so many fields.
One example is in medical imaging when you want to register or align two shapes.
You're rotating one shape to match as closely as possible the other shape.
But also in robotics, when you want to control a robotic arm to do such and such, you want
to move it in space, but you may also want to rotate the robotic hand, that is at the
end of the arm.
So rotations is, even if it's simple, it's a very good example of a geometric space.
And a lot of people haven't been considering the geometry of this rotation space.
And so this is an example where it actually makes a real difference.
And we've shown it with this supervised learning approach where we are able to reconstruct
better brain volumes using, taking into account the geometry of that space.
OK, what I'm hearing is that as opposed to just kind of throwing this approach at a
given problem from a modeling perspective, because it's easier now that we've got gyms
that's rather use it as an opportunity to think about where geometric statistics might
apply or if it might apply to the problem more deeply than you might otherwise, because
now if it does, you've got an easier way to use that information.
Yeah, exactly.
And that's the all point of the paper that we are writing right now.
So we do a review of geometry in machine learning because our feeling is that a lot of people
are using geometric data spaces.
It might be an intuitive space like the space of rotations, but sometimes it's a more abstract
space like a hyperbolic space or the space for symmetric positive definite matrices,
which has a geometric touch, but people don't often use that.
So the first goal of the paper is to do a review and be like, oh, in that field, in fact,
you have some geometry with this data.
In fact, you have some geometry.
And then once we've convinced people that deep down their data space is curved, then
we tell them with gyms that which tools they can use.
And a very important case, a set of tools that they can use is the metrics, the set of
metrics.
So if you have a curved space and you want to measure the distance between two elements
of this curved space, you need to follow the curvature of the space and take the length
of the shortest curve that links these two points.
So we call that a geodesk.
It means that the distance between two points in a curved space is defined as the distance
as the length of the geodesk.
And this means that if you are losing loss functions, if you are learning on a geometric
space, the loss functions that you should use, which is the distance between the ground
roofs and the prediction is the geodesk.
Exactly.
You should use a geodesk distance because you will encompass, you will take into account
the geometry of your space.
So that's what gyms that is about is first telling people, oh, your space is geometric.
You should have a look at this geodesk distance because it will be more suited for your problem.
What's next in this line of research?
Oh.
So in terms of implementation, so gyms that has been around for a year now and we are
in development.
So we are currently developing at different backends.
So we want gyms that since we want to apply it to machine learning, we want gyms that
to be able to be used on GPU.
So we are currently implementing TensorFlow backend and also PyTouch backend.
So this is from an implementation point of view.
And now the goal will be to use gyms that so for me to study organ shapes, but also other
kind of shapes.
So initially I was looking at brain shapes or hard shapes, now I'd like to go into small
the scales and maybe look at cell shapes and protein shapes.
And actually use the tool that we've developed to look at these statistics on these new kinds
of shapes.
Sounds fantastic.
Well, Nina, thanks so much for taking the time to chat with us about this really interesting
work.
Thank you very much.
All right, everyone, that's our show for today for more information on Nina or any of
the topics covered in this episode, visit twimmelai.com slash talk slash 196.
If you're a fan of the podcast and you haven't already done so or you're a new listener
and you like what you hear, visit your Apple or Google podcast app and leave us a five-star
rating and review.
Your reviews are super helpful and they inspire us to create more and better content as well
as help new listeners find the show.
Thanks again to our friends at IBM for their sponsorship of this episode.
Be sure to check out the IBM Developer Portal at IBM.biz slash MLAI podcast.
As always, thanks so much for listening and catch you next time.
