1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,480
I'm your host Sam Charrington.

4
00:00:23,480 --> 00:00:28,040
A quick thanks to everyone who participated in last week's Twimble Online Meetup.

5
00:00:28,040 --> 00:00:29,960
It was another great one.

6
00:00:29,960 --> 00:00:35,480
If you missed it, the recording will be posted to the Meetup page at twimbleai.com slash

7
00:00:35,480 --> 00:00:36,480
meetup.

8
00:00:36,480 --> 00:00:37,880
Definitely check it out.

9
00:00:37,880 --> 00:00:43,680
I never cease to be amazed by the generosity and creativity of the Twimble community.

10
00:00:43,680 --> 00:00:48,480
And I'd like to send a special shout out to listener Sharon Glander for her exceptional

11
00:00:48,480 --> 00:00:50,200
sketch notes.

12
00:00:50,200 --> 00:00:55,120
Sharon has been creating beautiful hand sketch notes of her favorite Twimble episodes

13
00:00:55,120 --> 00:00:57,160
and sharing them with the community.

14
00:00:57,160 --> 00:01:00,400
Sharon, we truly love and appreciate what you're doing with those.

15
00:01:00,400 --> 00:01:02,920
So please keep up the great work.

16
00:01:02,920 --> 00:01:06,040
We'll link to her sketch notes in the show notes for this episode.

17
00:01:06,040 --> 00:01:11,440
And you should definitely follow her on Twitter at sharingglander for more.

18
00:01:11,440 --> 00:01:15,840
This is your last chance to register for the rework, deep learning and AI assistant

19
00:01:15,840 --> 00:01:22,200
summits in San Francisco, which are this Thursday and Friday, January 25th and 26th.

20
00:01:22,200 --> 00:01:26,600
These events feature leading researchers and technologists like the ones you heard in

21
00:01:26,600 --> 00:01:29,680
our Deep Learning Summit series last week.

22
00:01:29,680 --> 00:01:34,600
The San Francisco event is headlined by Ian Goodfellow of Google Brain, Daphne Kohler

23
00:01:34,600 --> 00:01:37,320
of Calico Labs and more.

24
00:01:37,320 --> 00:01:42,880
Definitely check it out and use the code Twimbleai for 20% off of registration.

25
00:01:42,880 --> 00:01:47,400
In this episode of the show, I host the largest group of guests I've ever had on a single

26
00:01:47,400 --> 00:01:48,560
podcast.

27
00:01:48,560 --> 00:01:55,760
I speak with Arthur Gretton, Wittowat Jekritam, Zoltan Zabo, and Kenji Fukumizu, who, alongside

28
00:01:55,760 --> 00:02:02,240
with Wenkai Shu, authored the 2017 Nipps Best Paper Award winner, a linear time kernel

29
00:02:02,240 --> 00:02:04,320
goodness of fit test.

30
00:02:04,320 --> 00:02:09,840
In our discussion, we cover what exactly a goodness of fit test is and how it can be used

31
00:02:09,840 --> 00:02:15,320
to determine how well a statistical model applies to a given real world scenario.

32
00:02:15,320 --> 00:02:19,960
The group and I then discuss this particular test, the applications of this work, as well

33
00:02:19,960 --> 00:02:24,640
as how this work fits in with other research the group has recently published.

34
00:02:24,640 --> 00:02:25,640
Enjoy.

35
00:02:25,640 --> 00:02:37,400
Alright everyone, I am in Long Beach, California at Nipps and I've got the stink pleasure

36
00:02:37,400 --> 00:02:42,960
of being seated with a group who is authored one of the Best Paper Award winners.

37
00:02:42,960 --> 00:02:48,080
Here at Nipps, it is by far the largest group that I've ever interviewed at one time on

38
00:02:48,080 --> 00:02:49,080
a podcast.

39
00:02:49,080 --> 00:02:56,560
So I'm going to allow them to introduce themselves and please name, role, title, affiliation,

40
00:02:56,560 --> 00:03:02,080
and maybe while you're at it, a little bit about your research and areas of interest.

41
00:03:02,080 --> 00:03:03,080
Thank you.

42
00:03:03,080 --> 00:03:04,080
So I'm Arthur Gretton.

43
00:03:04,080 --> 00:03:10,240
I'm a professor at the Gatsby Computational Neuroscience Unit at University College London.

44
00:03:10,240 --> 00:03:17,120
So my research interests, I generally know how to represent and compare probabilities.

45
00:03:17,120 --> 00:03:22,440
So on one hand, if you've got two groups of objects, you're trying to tell whether they're

46
00:03:22,440 --> 00:03:26,480
similar or different, that's one of the problems that we might address.

47
00:03:26,480 --> 00:03:27,480
Another is in reasoning.

48
00:03:27,480 --> 00:03:32,040
So if you're observing something that might have an effect on something else, you want

49
00:03:32,040 --> 00:03:37,120
to know how strong that effect is and how confident you are in whether that effect exists.

50
00:03:37,120 --> 00:03:38,120
Great.

51
00:03:38,120 --> 00:03:39,120
Thanks, Arthur.

52
00:03:39,120 --> 00:03:40,120
Hello.

53
00:03:40,120 --> 00:03:41,120
My name is Witowat Gretton.

54
00:03:41,120 --> 00:03:44,400
I'm from the Gatsby Unit University College London.

55
00:03:44,400 --> 00:03:48,320
I'm a PhD student studying computer science.

56
00:03:48,320 --> 00:03:54,200
So my interests are statistical tests, hypothesis testing, and a bit on Bayesian inference.

57
00:03:54,200 --> 00:03:55,200
Okay, great.

58
00:03:55,200 --> 00:03:56,200
Kenji.

59
00:03:56,200 --> 00:03:57,200
Hello.

60
00:03:57,200 --> 00:03:58,200
I'm Kenji Fukumizu.

61
00:03:58,200 --> 00:04:02,720
I'm a professor at the Institute of Statistical Mathematics in Tokyo.

62
00:04:02,720 --> 00:04:07,120
And I'm basically interested in statistics and the machine learning.

63
00:04:07,120 --> 00:04:13,120
And my interest is also ranging from the mathematical aspect of machine learning to the application

64
00:04:13,120 --> 00:04:14,600
of machine learning.

65
00:04:14,600 --> 00:04:15,600
Okay.

66
00:04:15,600 --> 00:04:16,600
Great.

67
00:04:16,600 --> 00:04:17,600
And Zaltan?

68
00:04:17,600 --> 00:04:18,600
Hello.

69
00:04:18,600 --> 00:04:22,520
I'm Zaltan Sable from C-MAP, Center of Applied Mathics, Equality Technique France.

70
00:04:22,520 --> 00:04:28,360
I'm a research associate professor, genuinely interested in information theory, connoe algorithms,

71
00:04:28,360 --> 00:04:34,880
how to define similarities between many different objects, graphs, time series, dynamical systems,

72
00:04:34,880 --> 00:04:40,640
vectors, and how to apply it in different information, theoretical context, including hypothesis

73
00:04:40,640 --> 00:04:41,640
testing.

74
00:04:41,640 --> 00:04:42,640
Okay.

75
00:04:42,640 --> 00:04:46,960
I want to tell us about the paper that you guys submitted.

76
00:04:46,960 --> 00:04:48,480
So I'm happy to do that.

77
00:04:48,480 --> 00:04:52,360
So the paper is a goodness of fit test.

78
00:04:52,360 --> 00:04:57,040
What this means is that we have some model of the world, and we want to tell whether that

79
00:04:57,040 --> 00:05:01,280
model reflects the world well or not, and if not, why not.

80
00:05:01,280 --> 00:05:06,440
So an example that we used in the talk was that if you have a model of where crime occurs

81
00:05:06,440 --> 00:05:11,000
in Chicago, you might want to know whether that model accurately predicts where the crime

82
00:05:11,000 --> 00:05:16,480
will occur, and if there is some shortcoming to the model, you might want to know where

83
00:05:16,480 --> 00:05:17,800
that is.

84
00:05:17,800 --> 00:05:22,240
As an example, we used a rather crude crime model, which was mistakenly predicting crimes

85
00:05:22,240 --> 00:05:26,120
in the lake next to the city where clearly there weren't any crimes.

86
00:05:26,120 --> 00:05:32,000
So this would automatically flag those, so there's no piracy at the moment in Chicago.

87
00:05:32,000 --> 00:05:34,920
So this is a specific instance.

88
00:05:34,920 --> 00:05:38,600
The difficulty we encounter with this is that if the model that you have of the world

89
00:05:38,600 --> 00:05:43,680
is very complicated, then it can be very difficult to compare this with data.

90
00:05:43,680 --> 00:05:49,200
So in our crime example, imagine trying to figure out whether it's likely that somebody's

91
00:05:49,200 --> 00:05:53,840
going to commit a crime or this is going to depend on their social network, on the state

92
00:05:53,840 --> 00:05:58,040
of the economy, on the political atmosphere, and so on.

93
00:05:58,040 --> 00:06:02,680
All of these things are very complicated and interact, so you can state what the interactions

94
00:06:02,680 --> 00:06:08,160
are, but then to figure out how these all combined to create a probability is impossible.

95
00:06:08,160 --> 00:06:10,520
It's just not mathematically feasible.

96
00:06:10,520 --> 00:06:15,440
Nonetheless, we might want to know when our test has difficulties and the way that we've

97
00:06:15,440 --> 00:06:21,640
formulated it, our test is able to decide on the, or to locate the mismatch between data

98
00:06:21,640 --> 00:06:27,840
and model without having to figure out these probabilities and do this impossible computation.

99
00:06:27,840 --> 00:06:29,800
And what was the title of the paper?

100
00:06:29,800 --> 00:06:36,040
So the title is a linear term, kernel goodness of fit test, linear time kernel goodness

101
00:06:36,040 --> 00:06:37,640
of fit test.

102
00:06:37,640 --> 00:06:45,000
How is this other standard set of goodness of fit test that folks would use otherwise?

103
00:06:45,000 --> 00:06:48,440
So in certain simple cases, there are goodness of fit test.

104
00:06:48,440 --> 00:06:53,520
So if you have a very simple model that you're comparing with like a simple Gaussian, then

105
00:06:53,520 --> 00:06:55,280
there are other goodness of fit tests.

106
00:06:55,280 --> 00:06:59,480
In the case of the Gaussian, we don't have this complexity that I mentioned where you

107
00:06:59,480 --> 00:07:05,120
have many interacting effects that make computing probabilities in close form impossible.

108
00:07:05,120 --> 00:07:11,800
So what we have tried to address is the case when these models are very complex.

109
00:07:11,800 --> 00:07:16,400
And what, out of curiosity, what brought you all together from disparate parts of the

110
00:07:16,400 --> 00:07:17,400
world?

111
00:07:17,400 --> 00:07:20,480
Yes, we are like frequent collaborators.

112
00:07:20,480 --> 00:07:22,920
So Zoltan used to be at Caspi unit.

113
00:07:22,920 --> 00:07:24,600
We worked together before.

114
00:07:24,600 --> 00:07:30,320
I and other also frequently visit Kenji at Institute of Statistical Mathematics, so kind

115
00:07:30,320 --> 00:07:33,080
of frequent collaborators.

116
00:07:33,080 --> 00:07:38,520
And so with the, you know, when I think about a best paper, you know, there are lots of

117
00:07:38,520 --> 00:07:44,640
reasons why a paper might win best paper, you know, brought applicability, elegance of

118
00:07:44,640 --> 00:07:50,560
the solution and others, you know, why do you think this paper was selected as one of

119
00:07:50,560 --> 00:07:52,000
the best papers for Nips?

120
00:07:52,000 --> 00:07:57,520
I think it's a combination of broad applicability and the analysis that comes with it.

121
00:07:57,520 --> 00:08:05,360
So one, I guess, key thread in modern machine learning is like the formulating generative

122
00:08:05,360 --> 00:08:07,640
models for complicated data.

123
00:08:07,640 --> 00:08:13,000
So one of the key research directions of DeepMind, for example, is reinforcement learning.

124
00:08:13,000 --> 00:08:18,240
They want an agent that is in a world and that is able to learn from that world.

125
00:08:18,240 --> 00:08:23,880
What this requires in practice to train it is a simulation of the world, which is very realistic.

126
00:08:23,880 --> 00:08:28,720
So one important, I guess, component of that is to know when your simulation of the world

127
00:08:28,720 --> 00:08:29,720
is not accurate.

128
00:08:29,720 --> 00:08:31,160
What is it missing?

129
00:08:31,160 --> 00:08:36,240
I think for this reason, a paper which is able to, like, troubleshoot these models of

130
00:08:36,240 --> 00:08:42,800
the world that people might use in practice is a very much interest across the community.

131
00:08:42,800 --> 00:08:49,760
I think sort of coupled with this sort of very applied, I guess, benefit is that we have

132
00:08:49,760 --> 00:08:51,160
some nice theory results.

133
00:08:51,160 --> 00:08:54,480
So one of the phrases in the title was linear time.

134
00:08:54,480 --> 00:08:59,440
So the time that it takes is no more than the time that it takes to sort of look at each

135
00:08:59,440 --> 00:09:00,440
item.

136
00:09:00,440 --> 00:09:02,760
Quadratic time means you sort of have to look at all pairs of items.

137
00:09:02,760 --> 00:09:04,520
So that becomes very expensive.

138
00:09:04,520 --> 00:09:10,240
So what another result that we showed in the paper is of the various ways that one could

139
00:09:10,240 --> 00:09:15,400
derive a linear time goodness of fit test with the properties that we've talked about,

140
00:09:15,400 --> 00:09:18,240
the way that we've proposed is provably better.

141
00:09:18,240 --> 00:09:23,400
So it's going to give you a test which has the number of true positives, I should say,

142
00:09:23,400 --> 00:09:28,040
is going to improve faster for this test and for the other alternatives.

143
00:09:28,040 --> 00:09:29,040
Yeah.

144
00:09:29,040 --> 00:09:34,840
When you say linear time, is it linear in the number of examples or the number of distributions

145
00:09:34,840 --> 00:09:38,040
that you're trying to fit to or linear in the number of examples?

146
00:09:38,040 --> 00:09:39,040
Okay.

147
00:09:39,040 --> 00:09:43,160
We have a single model, you know, our grand map of Chicago, and then we're looking, it's

148
00:09:43,160 --> 00:09:44,640
linear in the number of crimes.

149
00:09:44,640 --> 00:09:45,640
Okay.

150
00:09:45,640 --> 00:09:46,640
Okay.

151
00:09:46,640 --> 00:09:49,480
So what's the process for applying it?

152
00:09:49,480 --> 00:09:54,400
So this goodness, then this new goodness of fit test takes in two inputs.

153
00:09:54,400 --> 00:09:55,400
Okay.

154
00:09:55,400 --> 00:09:56,400
The first one is a model.

155
00:09:56,400 --> 00:09:57,400
Okay.

156
00:09:57,400 --> 00:10:00,720
For example, in the crime, Chicago crime example, the model, let's say we want to model

157
00:10:00,720 --> 00:10:05,960
spatial density of robbery events in Chicago, say, just going to be the model, then just

158
00:10:05,960 --> 00:10:07,360
some density function.

159
00:10:07,360 --> 00:10:09,680
The second input would be a collection of points.

160
00:10:09,680 --> 00:10:14,080
In this case, it's going to be observed robbery events where each point is just like

161
00:10:14,080 --> 00:10:17,840
the larger your language coordinate of one event, for example.

162
00:10:17,840 --> 00:10:23,960
And the way that this works is that the test constructs some function, it's essentially

163
00:10:23,960 --> 00:10:31,240
a scoring function that tells where the mismatch is between the model and the data.

164
00:10:31,240 --> 00:10:36,600
And once we have this scoring function, we can then use it to find sort of a region where

165
00:10:36,600 --> 00:10:38,640
the mismatch is the largest.

166
00:10:38,640 --> 00:10:42,640
And then we can pinpoint, okay, that is where there's a mismatch.

167
00:10:42,640 --> 00:10:45,800
And then this is how we criticize the model.

168
00:10:45,800 --> 00:10:50,400
So in the end, we would get a region indicating, okay, this is where the model doesn't fit quite

169
00:10:50,400 --> 00:10:51,920
well to the data.

170
00:10:51,920 --> 00:10:57,080
And then as a modeler, they can just improve the model based on the hints, on the evidence

171
00:10:57,080 --> 00:10:58,480
given by the test.

172
00:10:58,480 --> 00:11:00,520
So when you say region, region of what?

173
00:11:00,520 --> 00:11:03,880
We're not talking about a region of the map of Chicago, we're talking about a region

174
00:11:03,880 --> 00:11:04,880
of the map.

175
00:11:04,880 --> 00:11:07,480
Yeah, a physical region, actually.

176
00:11:07,480 --> 00:11:13,320
And does that correspond to, you know, if we're not talking about a map example, but it

177
00:11:13,320 --> 00:11:19,280
sounds like this applies more broadly, what's the generalization of a region?

178
00:11:19,280 --> 00:11:20,280
Right.

179
00:11:20,280 --> 00:11:25,600
So the technical term would be, it's going to be the domain where your data live in.

180
00:11:25,600 --> 00:11:28,720
So whatever that domain is, so I don't know.

181
00:11:28,720 --> 00:11:35,880
So typically, I guess if you have data, I'd represent it at a table, then you have many

182
00:11:35,880 --> 00:11:36,880
columns.

183
00:11:36,880 --> 00:11:41,200
I guess columns correspond to the number of features or the number of attributes.

184
00:11:41,200 --> 00:11:46,600
Then let's say your data live in Rd, where D is the number of columns, then that is going

185
00:11:46,600 --> 00:11:50,080
to be like the equivalent of the region that I mentioned.

186
00:11:50,080 --> 00:11:54,080
So it's going to identify regions in your feature space.

187
00:11:54,080 --> 00:11:55,080
In the feature space, yeah.

188
00:11:55,080 --> 00:11:56,080
Okay.

189
00:11:56,080 --> 00:11:57,080
Right.

190
00:11:57,080 --> 00:11:58,080
Okay.

191
00:11:58,080 --> 00:12:03,880
Let me give you maybe two other examples, because this paper is, in fact, part of a larger,

192
00:12:03,880 --> 00:12:07,880
let's say, a package of hypothesis tests in your time hypothesis tests.

193
00:12:07,880 --> 00:12:13,760
And we use this technology in, for example, computer vision or natural language processing.

194
00:12:13,760 --> 00:12:19,000
In the first case, you have, let's say, images of, let's say, happy or sad people.

195
00:12:19,000 --> 00:12:20,000
Okay.

196
00:12:20,000 --> 00:12:24,480
And then the question is whether you can detect the difference between the two emotions,

197
00:12:24,480 --> 00:12:29,960
or if there's difference, which, let's say, masses are responsible for this difference.

198
00:12:29,960 --> 00:12:30,960
Okay.

199
00:12:30,960 --> 00:12:37,640
So that's the domain, in this case, let's say, muscle activities or parts of the face.

200
00:12:37,640 --> 00:12:42,960
In the other application, natural language processing, we had, let's say, documents from

201
00:12:42,960 --> 00:12:48,800
two different topics, categories, like neuroscience, Bayesian inference.

202
00:12:48,800 --> 00:12:55,160
And then the question was, again, whether these two possibly different topics can be discriminated.

203
00:12:55,160 --> 00:12:58,840
And if this is the case, what are the keywords you should look at?

204
00:12:58,840 --> 00:13:03,040
In this case, that's the domain, what are the most distinguishing keywords?

205
00:13:03,040 --> 00:13:04,040
Mm-hmm.

206
00:13:04,040 --> 00:13:11,240
So in the case of the first example, you, in applying this result, you would identify, for

207
00:13:11,240 --> 00:13:14,000
example, regions of the face, that, what?

208
00:13:14,000 --> 00:13:17,160
That distinguish between the emotions or...

209
00:13:17,160 --> 00:13:18,320
Yeah, so that's right.

210
00:13:18,320 --> 00:13:23,920
So for instance, if you had sort of emotions that required you to frown or to, yeah, to

211
00:13:23,920 --> 00:13:27,760
put the sides of your mouth downwards, like you're unhappy, which causes the sort of lines

212
00:13:27,760 --> 00:13:32,320
on each side to stand out, these would be the regions, the spatial regions in the face,

213
00:13:32,320 --> 00:13:37,920
which distinguish the sort of contented emotions from the angry or upset emotions.

214
00:13:37,920 --> 00:13:42,080
So yeah, so the domain is sort of all of the possible regions in the face, just the spatial

215
00:13:42,080 --> 00:13:43,080
domain.

216
00:13:43,080 --> 00:13:47,200
And then the salient regions that matter in distinguishing them are the locations around

217
00:13:47,200 --> 00:13:50,720
the eyebrows and to each side of the nose and mouth.

218
00:13:50,720 --> 00:13:57,560
And so this, one of the areas that it sounds like this plays into is the domain of explainability

219
00:13:57,560 --> 00:13:59,160
of AI models.

220
00:13:59,160 --> 00:14:03,640
Is that one of the primary motivators as well for this research?

221
00:14:03,640 --> 00:14:08,880
I think so, yes, because, yeah, it's basically a way of showing the shortcoming of your model,

222
00:14:08,880 --> 00:14:11,880
like what it is that your model fails to explain about your data.

223
00:14:11,880 --> 00:14:17,680
So I think, yeah, explainability is really one of the, regions, I think that the paper

224
00:14:17,680 --> 00:14:20,200
was given the award, yeah.

225
00:14:20,200 --> 00:14:24,720
And we are using the more and the more complex model, the recent three, and so it's very

226
00:14:24,720 --> 00:14:33,000
important to, well, say that we are, that model correctly reflects our data, that's our

227
00:14:33,000 --> 00:14:34,000
motivation.

228
00:14:34,000 --> 00:14:38,760
And so what are the requirements of the model that would allow us to apply this?

229
00:14:38,760 --> 00:14:43,760
Is it applicable to like box set of models or do we need to know something about, you

230
00:14:43,760 --> 00:14:48,600
know, either the models or the distributions of our data or other things in order to apply

231
00:14:48,600 --> 00:14:50,640
this technique?

232
00:14:50,640 --> 00:14:53,120
So it does need to be a probabilistic model.

233
00:14:53,120 --> 00:14:59,520
So by contrast, one might think of these adversarial networks where there you might not have actually

234
00:14:59,520 --> 00:15:05,520
a model of the probabilities of the images that it's generating, if it's generating images.

235
00:15:05,520 --> 00:15:10,960
So what we do require is that we're able to, you know, write the model as something that,

236
00:15:10,960 --> 00:15:15,360
you know, if you were able to, like, to normalize it to take the sum over all possible states

237
00:15:15,360 --> 00:15:17,880
would be a valid probability.

238
00:15:17,880 --> 00:15:24,360
Some ways of generating data from randomness, just take the data and apply a bunch of transforms

239
00:15:24,360 --> 00:15:29,160
to it, but it's not clear how or if you could turn that into a probability of the outputs

240
00:15:29,160 --> 00:15:31,680
given the random noise that you fed in.

241
00:15:31,680 --> 00:15:36,440
We are only able to deal at this stage with the case where you have this, you know, thing

242
00:15:36,440 --> 00:15:40,200
that you could write as a probabilistic model, were you able to normalize it?

243
00:15:40,200 --> 00:15:41,200
Okay.

244
00:15:41,200 --> 00:15:47,160
And maybe taking a step back, what was the fundamental realization or innovation that

245
00:15:47,160 --> 00:15:49,560
kind of led you down the path that led to this paper?

246
00:15:49,560 --> 00:15:55,920
It sounds like it's all time you mentioned this paper is one in a series of works, but

247
00:15:55,920 --> 00:15:58,240
what inspired this particular result?

248
00:15:58,240 --> 00:16:00,720
Zoltan mentioned it was one of a sequence of works.

249
00:16:00,720 --> 00:16:03,680
So the first work was just in comparing sets of objects.

250
00:16:03,680 --> 00:16:08,080
So in Zoltan's example, sets of faces with positive and negative emotions.

251
00:16:08,080 --> 00:16:10,720
So notice that model never appeared in that description.

252
00:16:10,720 --> 00:16:15,720
It's just, I have set A set B and I want to know why and where these two sets are different.

253
00:16:15,720 --> 00:16:20,160
The second test in the series was a test of whether two things are dependent.

254
00:16:20,160 --> 00:16:26,040
So as an example, if you show to a human some movie, then their brain will be active in

255
00:16:26,040 --> 00:16:28,680
a way that depends on the movie that they're watching.

256
00:16:28,680 --> 00:16:33,000
And so this dependence, you see, can we take a step back and as you talk through each of

257
00:16:33,000 --> 00:16:37,960
these papers in this sequence, kind of what the key results were as well?

258
00:16:37,960 --> 00:16:38,960
Sure.

259
00:16:38,960 --> 00:16:45,000
So for the first paper, we were able to show again, a linear time test of where these two

260
00:16:45,000 --> 00:16:47,720
sets of objects were different.

261
00:16:47,720 --> 00:16:50,760
So again, it only costs me a time to compute.

262
00:16:50,760 --> 00:16:54,320
So that means the linear and the number of objects that we're comparing.

263
00:16:54,320 --> 00:16:59,240
And we were also given a diagnosis of where it was that the two sets were different.

264
00:16:59,240 --> 00:17:04,280
And so my first kind of flash intuition on this would be to try to do some kind of difference

265
00:17:04,280 --> 00:17:09,440
or something like that and maybe convolve the images together or something is, are you

266
00:17:09,440 --> 00:17:15,640
thinking about it in the same domain or more like from a pure probabilistic perspective

267
00:17:15,640 --> 00:17:16,640
or?

268
00:17:16,640 --> 00:17:20,480
So we need to think in terms of sets of objects rather than objects.

269
00:17:20,480 --> 00:17:27,000
So for example, if one looks at a pair of faces, you can take their difference and you

270
00:17:27,000 --> 00:17:28,960
can see where that difference occurs.

271
00:17:28,960 --> 00:17:33,560
If you have two sets, it can be that the mean is the same, but one set has a higher variance

272
00:17:33,560 --> 00:17:34,560
than the other.

273
00:17:34,560 --> 00:17:38,640
And so we care about any difference that might occur between two sets of objects.

274
00:17:38,640 --> 00:17:42,920
So the second paper was about dependence testing.

275
00:17:42,920 --> 00:17:47,240
And in this case, again, we have a linear time test.

276
00:17:47,240 --> 00:17:51,360
We also care about dependence that might be quite complicated.

277
00:17:51,360 --> 00:17:57,880
So the dependence that you might learn in statistics 101, you notice that when one thing increases,

278
00:17:57,880 --> 00:18:01,920
like, you know, I press down my accelerator, my cargo's fastest, linear dependence.

279
00:18:01,920 --> 00:18:06,280
What we might care about is dependence that is much more complicated than that.

280
00:18:06,280 --> 00:18:11,440
For example, like if you are adjusting a parameter on a robot arm, it might, on average, hit the

281
00:18:11,440 --> 00:18:12,440
same target.

282
00:18:12,440 --> 00:18:17,600
But it might be that if you under-damped the robot arm, it's half the time above the target,

283
00:18:17,600 --> 00:18:18,600
half the time below.

284
00:18:18,600 --> 00:18:21,360
So the variance is going up depending on this parameter, even though if you just looked

285
00:18:21,360 --> 00:18:23,640
at the means, you would get the same mean.

286
00:18:23,640 --> 00:18:28,600
So this, you know, this is a very trivial example, but illustrating this sort of complexity

287
00:18:28,600 --> 00:18:29,880
of dependence.

288
00:18:29,880 --> 00:18:34,840
So that's, you know, first of all, testing pairs of samples, second testing dependence.

289
00:18:34,840 --> 00:18:39,160
And then this led us to say, well, what if we don't have, you know, two sets of samples,

290
00:18:39,160 --> 00:18:42,040
but a sample in a model, what's the best way to approach that?

291
00:18:42,040 --> 00:18:43,560
And that led to our third paper.

292
00:18:43,560 --> 00:18:44,560
Okay.

293
00:18:44,560 --> 00:18:45,560
Great.

294
00:18:45,560 --> 00:18:46,560
Great.

295
00:18:46,560 --> 00:18:48,240
And does the sequence continue?

296
00:18:48,240 --> 00:18:50,880
What's next for the group of you?

297
00:18:50,880 --> 00:18:59,360
So one possible future work is that continuing from the third work, which is, so now that we

298
00:18:59,360 --> 00:19:02,000
have a model and we have one data set, right?

299
00:19:02,000 --> 00:19:06,600
But in practice, in reality, most of the time your model is probably wrong.

300
00:19:06,600 --> 00:19:09,600
And you know, a priori that it is, it is probably wrong.

301
00:19:09,600 --> 00:19:14,720
But now a more relevant question is given two models that are wrong, two competing models

302
00:19:14,720 --> 00:19:18,120
that are wrong, which one fits better?

303
00:19:18,120 --> 00:19:23,560
That is, this is now a model comparison instead of, so in the current version is model

304
00:19:23,560 --> 00:19:24,560
acquitticism.

305
00:19:24,560 --> 00:19:28,120
There's only one model when we try to criticize where it goes wrong.

306
00:19:28,120 --> 00:19:33,120
But now we can also extend to two models that we know are wrong, but we want to ask,

307
00:19:33,120 --> 00:19:34,440
okay, which one fits better?

308
00:19:34,440 --> 00:19:37,320
I think this is one possible direction.

309
00:19:37,320 --> 00:19:41,960
And then maybe which fits better where, as opposed to hiring, exactly, exactly, exactly.

310
00:19:41,960 --> 00:19:42,960
Exactly.

311
00:19:42,960 --> 00:19:44,640
That's what we are going to continue.

312
00:19:44,640 --> 00:19:45,640
Exactly.

313
00:19:45,640 --> 00:19:46,640
Nice.

314
00:19:46,640 --> 00:19:52,080
For the rest of you, any kind of parting words as we wrap up, there's another possibility

315
00:19:52,080 --> 00:19:53,280
of extension.

316
00:19:53,280 --> 00:19:58,760
So here, all these linear time tests, the underlying assumption is that the probabilistic model lies

317
00:19:58,760 --> 00:20:05,160
on essentially on vectors, like data machine, Euclidean vectors, which is possibly the simplest

318
00:20:05,160 --> 00:20:07,280
concept to understand.

319
00:20:07,280 --> 00:20:13,240
But one might argue maybe there are some dependencies between the coordinates somehow, so like if

320
00:20:13,240 --> 00:20:18,880
you, for example, think of graphs or like images, not all the pixels can vary completely

321
00:20:18,880 --> 00:20:20,400
in the pendants.

322
00:20:20,400 --> 00:20:27,600
So often there's some underlying load-dimensional structure in the background, hidden under

323
00:20:27,600 --> 00:20:28,600
the hood.

324
00:20:28,600 --> 00:20:32,600
So that's another possible direction to extend this framework.

325
00:20:32,600 --> 00:20:33,600
Awesome.

326
00:20:33,600 --> 00:20:35,840
Any other thoughts?

327
00:20:35,840 --> 00:20:37,600
How's Neb's been for everyone?

328
00:20:37,600 --> 00:20:38,600
Great.

329
00:20:38,600 --> 00:20:47,480
I'm very impressed by Neb's giving a word to this work because this is the age of artificial

330
00:20:47,480 --> 00:20:52,240
intelligence, and many people are looking at their application with deep learning, but

331
00:20:52,240 --> 00:21:01,280
our research is very basic ones, but we are using any method, we are using many complex

332
00:21:01,280 --> 00:21:08,320
models, and the model criticism is very important and basic research, and I'm very happy that

333
00:21:08,320 --> 00:21:12,760
Neb's gives respect to such type of basic work.

334
00:21:12,760 --> 00:21:17,360
And there was a call here at Neb's somewhat controversial call for kind of more basic

335
00:21:17,360 --> 00:21:20,960
work and more rigor in the way we look at machine learning.

336
00:21:20,960 --> 00:21:23,960
Any thoughts or comments on that?

337
00:21:23,960 --> 00:21:28,800
It sounds like you would all agree with that general idea.

338
00:21:28,800 --> 00:21:35,560
So there was an inspiring call to arms, which has caused a lot of discussion, and in a way,

339
00:21:35,560 --> 00:21:40,800
I think it's also a call to arms that is already being acted on in the sense that this

340
00:21:40,800 --> 00:21:46,080
year there has been a lot more thought about fundamentals of algorithms that are being

341
00:21:46,080 --> 00:21:49,040
used very successfully in the past two years.

342
00:21:49,040 --> 00:21:53,400
One example that was raised in this call to arms is understanding when you're optimizing

343
00:21:53,400 --> 00:21:59,000
a model with a huge number of parameters, as deep learning methods are, what the pathologies

344
00:21:59,000 --> 00:22:04,960
of these optimization methods are, when correlations cause you to converge very slowly or converge

345
00:22:04,960 --> 00:22:11,000
poorly, many of the, I guess, presentations by the optimization community, the SNPs were

346
00:22:11,000 --> 00:22:12,960
actually addressing that.

347
00:22:12,960 --> 00:22:17,960
So I think there was a standing ovation, as you know, when this call to arms was made.

348
00:22:17,960 --> 00:22:22,840
And I think that's because, like, the spirit of this Vali Rahimi's words was very much

349
00:22:22,840 --> 00:22:27,560
in the air that people were saying, OK, let's, we've got this amazing success on applications.

350
00:22:27,560 --> 00:22:34,120
Now let's understand what's still holding us back, and then that will help us to progress

351
00:22:34,120 --> 00:22:35,120
even further.

352
00:22:35,120 --> 00:22:36,120
Hmm.

353
00:22:36,120 --> 00:22:37,120
Great.

354
00:22:37,120 --> 00:22:42,120
Well, Arthur with Wat, Kenji, Zoltan, thanks so much for taking a few minutes to chat

355
00:22:42,120 --> 00:22:45,920
with us about your paper, and congratulations on the award.

356
00:22:45,920 --> 00:22:46,920
Thank you very much.

357
00:22:46,920 --> 00:22:48,920
Thanks, Zoltan.

358
00:22:48,920 --> 00:22:55,160
Alright, everyone, that's our show for today.

359
00:22:55,160 --> 00:23:00,240
Thanks so much for listening, and for your continued feedback and support.

360
00:23:00,240 --> 00:23:05,240
For more information on Arthur with Wat, Zoltan, Kenji, or any of the topics covered in this

361
00:23:05,240 --> 00:23:11,160
episode, head on over to twomlai.com slash talk slash 100.

362
00:23:11,160 --> 00:23:16,400
Of course, we'd be delighted to hear from you, either via a comment on the show notes page,

363
00:23:16,400 --> 00:23:22,960
or via Twitter, directly to me at At Sam Charrington, or to the show at At Twomlai.

364
00:23:22,960 --> 00:23:32,960
Thanks once again for listening, and catch you next time.

