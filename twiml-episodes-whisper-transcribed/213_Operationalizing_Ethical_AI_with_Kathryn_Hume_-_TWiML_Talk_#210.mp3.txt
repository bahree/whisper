Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Today we conclude our trust in AI series with this conversation with Katherine Hume, vice
president of strategy at integrate AI.
You might remember Katherine from our interview last year on selling AI to the enterprise,
which was Twimble Talk number 20.
This time around we discuss our newly released whitepaper, responsible AI in the consumer
enterprise, which details a framework for ethical AI deployment in e-commerce companies
and other consumer facing enterprises.
We look at the structure of the ethical framework she proposes and some of the many questions
that need to be considered when deploying AI in an ethical manner.
Thanks once again to Georgian partners for their continued support of the podcast and for
sponsoring this series.
Georgian partners is a venture capital firm that invests in growth stage business software
companies that use applied artificial intelligence, conversational AI and trust to differentiate
and advance their business solutions.
Post investment Georgian works closely with portfolio companies to accelerate the adoption
of these key technologies for increased value.
To help their portfolio companies hire the right technical talent Georgian recently published
Building Conversational AI Teams, a comprehensive guide to lead you through sourcing, acquiring
and nurturing a successful conversational AI team.
Check it out at Twimbleai.com slash Georgian.
And now on to the show.
All right everyone, I am on the line with Katherine Hume, Katherine is the vice president of
strategy with integrate AI, Katherine, welcome back to this week in machine learning and
AI.
It's wonderful to be here Sam, one of my favorite podcasts.
Well, you are one of one of my most favorite people so we are in the right place and we
are going to be talking about broadly speaking the topic of ethics and fairness and trust
in AI, but before we do that, for those who haven't had a chance to listen to the wonderful
podcast that we did a while back, I don't even remember when, but why don't you do a kind
of a brief intro and then we'll refer people back to that previous show in the show notes.
Yeah, for sure.
So as mentioned right now I'm heading up strategy for a Toronto based startup called integrate
AI.
You work with large consumer enterprises, the retail banks and telcos and retailers of
the world and help them do a better job providing relevant experiences for their customers
by applying machine learning into their customer engagement stacks.
And prior to that, I worked at a New York City based machine learning research lab called
fast forward labs that's actually now part of cloud era.
That's where we last met and during my time there I was also working with the enterprise
and sort of sitting between the world of academic research and the world of applied machine learning
in the enterprise to help companies get a handle on what was actually ready for prime time
as they tried to sort through the hype in the space and take them from the point of
prototype conceptualization to actual scaled application.
I guess the other thing to mention and it is relevant for my background for this context
is that I have a doctorate in comparative literature and an undergrad in theoretical mathematics.
And so I sort of bring a mixture of philosophy and epistemology, so the theories of knowledge
to my thinking around applied mathematics and machine learning.
That's been a decent toolkit for my thinking about ethics.
And it's made for some very interesting conversations I will say.
So jumping into this conversation around ethics, you recently spearheaded the development
of a white paper on what you called responsible AI in the consumer enterprise, which is a
really interesting paper.
What bucket do you put that in?
Do you think of it as ethics or fairness or trust there are a lot of words that we kind
of throw around when thinking about some of the ethical issues with regards to AI as applied
to the enterprise in particular, how do you kind of think through all that?
It's a great question and there are so many terms being thrown around these days as basically
we start to grapple with the implications of bringing machine learning algorithms from
the research lab into the real world.
And they sort of hit up against the surprising realities of human behavior and how human
behavior gets reflected in data and then how algorithms basically recycle some of these
reflections and refractions into products that can have sometimes surprising results.
So that's sort of a, I'd say a general statement I'm sure was lots to unpack there, but under
that this sort of general umbrella lies lots of terms that have social implications and
technical implications, terms that you mentioned like responsibility, fairness, transparency,
accuracy, bias.
And I would say we in the paper cover all of those topics because we think they're all
relevant and worth critical attention in the enterprise as well as in society as a whole.
But I'd say what we were trying to do and what's unique about this paper is we didn't design
it as a policy paper.
So it's not providing guidance to government agencies or regulators to help them think
about holistically sort of how to define these concepts at a at a at a large level so
we can agree upon what they mean in society.
It's also not oriented specifically for a data science or machine learning practitioner
audience where we would go into some of the latest and greatest research on say tackling
fairness.
This one is designed for cross functional teams that work in consumer enterprises in particular.
So it's aligned with some of the work we do at integrate.
And it's meant to operationalize ethics as a series of questions and dialogues that teams
need to be asking and having at the various points in building out a machine learning product.
So that was sort of the approach we take.
We really wanted to go from the realm of philosophy to the realm of concrete pragmatic.
What do you need to be thinking about when what are the questions and how do you sort of build
out a framework for developing a company's own ethical posture in the act of applying machine
learning?
And I'm a big fan of the use of the term operationalizing AI ethics.
It's been I think a gap that's been kind of missing in a lot of the conversation that's
been out there.
There's and you talked about some of the various perspectives and things that folks have
done.
And we've seen some of the things that I've seen that have gotten is gotten close to providing
a framework or more kind of checklist-y or manifesto-ish.
You have all these different attempts and they're all wonderful attempts.
But I love that you're kind of pushing it a little bit more formalism without kind of
diving into an academic treaties.
So maybe can you walk us through the overall structure of this framework?
Yeah, for sure.
So we actually used the steps that go into the process of building a machine learning
system as the structure for our ethical framework.
So basically we, you know, there's variations on how a machine learning product might
be built, but more or less companies follow the same process.
So in the beginning there's sort of a design phase where they have to ask, what problem
are we actually solving?
And I think one of the sort of pitfalls or the failure modes, let's say in applied machine
learning and the enterprise and times where enterprises sort of get stuck is they get caught
up with the innovative potential of some cool new capability.
And forget to ask the question, what is the business value?
What is our metric for success and how do we measure baseline performance today and an
improvement upon baseline to make a judgment call as to whether or not machine learning is
really the right solution for our problem, right?
And whether or not we want to invest in this technology in the first place.
So we sort of start off with saying giving some guidance and insight into how to go about
framing and shaping a machine learning problem in the first place.
Next we talk about data collection, where the data comes from.
If it's coming from a product interface, how one can think about the design of that
interface was to collect usable data, data that will be easier to maintain the quality
of in the long term, as well as if there's data that's coming from a third party where
there might be proven evidence issues.
We talk about then some of the data processing, depending on the different algorithmic technique
the company is using, this could include sort of standard feature engineering or more some
of the processing to prepare for a learning representation if one moves into the deep learning
sort of five things.
We talk about how different types of algorithms can present to different issues as it relates
to fairness bias and transparency.
And then we talk about putting the model into production, what it means to scale, how
there's some issues that can arise from sort of overfitting on a training set to extending
that out to larger data sets.
And then we talk a lot about what I actually think is one of the most interesting and underrepresented
portions of fairness in machine learning systems, which is maintenance, DevOps, and auditing
of the system in the long run.
So not just when we first build out that theoretical model, but how we, how one can maintain accountability
over a system over time.
So that's sort of the, those are the steps that we think through and obviously, you know,
there's iterations there and there's feedback loops and it's never quite as linear as
it seems.
But you know, that's sort of how we broke things down.
Considering a maintenance, I think, is underappreciated, not just from an ethics and fairness perspective,
but from a machine learning perspective in general, right?
There's a lot of focus in the industry on modeling and not quite enough on getting that
model into production and the care and feeding that's required to make sure that the model
is fresh and to protect the business.
Yeah, there's sort of a meta point to be made here where, you know, in this framework,
we talk a lot about the interdisciplinary cross-functional questions.
And this is sort of why it's framed more as a, we don't have the right answer.
We can't reduce this down to a checklist where you just have to press, you know, press
X plus button, press Y button.
It's rather, this is the checklist of questions you need to ask.
And the answer is going to depend upon the context and therefore it needs to be a collaboration
between, you know, the legal team, the business team, the data team, the algorithm team,
the engineering team, and that shows up at every point in the process, but if you think
about this endpoint where it's around, you know, maintenance, operationalization of
systems, et cetera, there too, it's, it's a, you know, there's some machine learning scientists
who go so far as to build modular code and then put that out in the scale of into production,
but most of the time this is going to then be collaboration with what Andrew Carpathy
would call software 1.0 folks who, you know, are working in a slightly different code
stack and are, and are representing and modifying the initial mathematics and the models to,
to turn them into scalable, reliable software.
So it's kind of all about, it's all about the handoffs, the collaboration, and then
finding a way to translate between two different language systems, two different modes of
working to, to, to do the hard work of, you know, making the systems thrive in production.
On that point of translation, you also explicitly talk about some of the softer topics,
people and communication.
I don't know that you explicitly mentioned culture, but I imagine that plays a big role
in this as well, a huge role.
Can you talk a little bit about those topics?
Yeah, for sure.
So let's break it down into two.
So in thinking about culture, it's interesting, I recently heard DJ Patil give a talk at
a ethics conference that Scotia Bank hosted up here in Toronto.
And he was mentioning that one of the key requirements for success to deploy AI ethically is to
have a channel of resistance, right, to basically, almost like a, what are those things called
where it's the, it's like the bat line, right, so it's like it's, it's like a channel
where you're allowed to descent, right, there's sort of like the accepted protocol, but
somebody's, somebody, people are empowered and they have the right to raise their hand
and say, I don't think this is okay.
So that's sort of, you know, I think there's a huge cultural implication there where, where
teams are empowered to have a voice and to think critically about what they're building.
And also to be stimulated and challenged to not just accept, you know, the, the base technical
answer and building a machine learning model, namely that, you know, we've decided that
our objective function is X.
We're going to measure accuracy using, you know, sort of the following, the following
measures.
And I guess like one of the idiosyncrasies or the uncomfortable paradoxes of, say, rendering
the model fair is that often to make a model fair requires a little bit of a trade-off
on its accuracy, if one is just reporting on the performance of the model in its, you
know, sort of in its normally functioning fashion.
And that's, that, it's, it comes off as counterintuitive, right, because like the scientists
that are building the models are, are thinking about measuring their performance according
to the, the set of constraints that they, they've learned in school and that they've been
applying and then to come into that and say, okay, we're going to actually design this
so that there's a cut on accuracy and then the question becomes, well, who makes that
call?
Do I make that call or does this hit need to be a cross-functional dialogue with like
the business team?
Did they make the call on some of the trade-offs that we're going to incur if we decide
to make a model fair as opposed to sort of more accurate?
And so, so I think the translation questions are totally intertwined with the cultural
questions, because there needs to be the foundation to enable, to enable these discussions to
occur, to, to start to get traction in the first place.
It's not just a, you know, let me download my fairness module from the cloud and the
next thing you know, the next thing you know, everything's going to be fine.
There's work to try to, to try to build out sort of open source scalable toolkits to
address fairness, but it's still, we're still so early in the process and there's still
not even alignment across the technical community on what fairness means as a statistical problem.
So, we've got a lot of work to do to figure this out.
That makes me think of a couple of things.
This notion of a channel of resistance calls to mind, an article that I came across yesterday
maybe, you know, very contemporary about how Google is experiencing, you know, internal
turmoil is probably too strong, but there are very vocal kind of employee factions on
two sides of the issue of the company's involvement in China.
And that kind of suggests in the context of this notion of a channel of resistance that
they have a very strong culture of, you know, supporting and allowing that kind of resistance,
which kind of bodes well for their ability to, at least in this particular area, you
know, support, you know, making ethical decisions around AI, does that, is that consistent
with what you see?
That's a great question.
I'm certainly not an expert in what's going on inside Google these days, but I do, for
sure.
But, yeah, I think that it's interesting, my father, who worked in technology for 45 years,
he was telling me that back in the 70s and 80s, when he was working, it wasn't even possible
for two managers to be in a room together without a director present, right?
So there was so much control over what might be gossip, what might be dissent, what might
be background blings, what's going on in the company, that they didn't even, like, culturally,
it was not possible for two people to meet, which I have trouble even imagining.
You know, it's like, well, there's always water cooler discussions, right?
Of course, there would be covert backchannels, if not explicitly permissible backchannels,
but there's a difference there where, you know, it's, like, this as a, the organization
coming together and explicitly sanctioning critical thinking, right?
And explicitly, explicitly empowering people from the technical side and as well as voices
external to the company.
So one of the things we talk about in the paper is that I think one of the failure modes
in thinking about operationalizing ethics and, let's say, from an organizational perspective
would be, let's appoint the ethics committee, okay, well, who's on the ethics committee?
And here we're thinking about large enterprises, so you can imagine it would be the chief
data officer, the chief technology officer, somebody from the risk management department.
And the question to be posed is, do the people occupy those positions have the requisite
diversity of opinion and perspective to function well as ethics officers?
And often that's not the case, right?
Because of, just because of the sort of, you know, the issues in diversity, inclusion,
and the workforce.
And so meaning it's really important, not only that there's a channel of dissent, but also
that multiple people's perspectives are brought to the table in particular during the design
phase.
You know, it's not necessarily, you don't need to have sort of community representatives
when it comes to thinking about audit logs and trails on the, on production organization
of the systems.
But in some of the early thinking on doing what we call a pre-mortem, where a team would
not do a post-mortem, where they've done their project, and then they look backwards
and say, okay, what worked, what didn't, what are we going to change, but rather taking
the time to imagine, prospectively, some of the issues that might go wrong in applying
a machine learning application.
And taking that from the embodied perspective of people who are not like, you know, some
of that, you know, not like you, right, not like potentially the composition of an existing
workforce.
And so, so we ask and close this in the paper, should there be customers in that, on
that ethics committee, or could it just be sort of different people's perspective within
the organization?
Another question is, and going back to who makes the call and who's accountable, there's
different opinions across the community as to whether or not there needs to be a neutral
third party, almost auditing function, who comes in and basically has, you know, has
no skin in the game, and can therefore look through system design and audit it for compliance
with some ethical standard.
That makes sense to me, but I don't think it's enough because, as we say in the paper,
if you sort of wait until the end, and then go back through a system and see where it
might have ethical pitfalls, there's a lot of wasted time, and there's so many in the
moment pragmatic decisions that technical teams can make to save themselves the trouble
of, say, implementing a neural network when the use case requires a level of interpretability
and transparency given regulatory requirements, and they could have sort of just selected a
different algorithm earlier on and avoided the subsequent requirement to redo to redo work
to meet regulatory or ethical requirements.
So maybe let's walk through some of the key questions that need to be asked at these
different stages, is that a good way to go through this, you think?
Yeah, let's do it.
So again, you kind of start with this notion of kind of problem definition and scoping.
What needs to be thought about there?
You just kind of talked about some of it, the way you define a problem is going to have
direct implications on, I guess I'm kind of blurring the lines of problem
definition with the whole neural network thing, but let me kind of roll that back and say,
what are some of the questions that need to be considered when we're thinking about problem
definition and scope?
Sure, so I'll talk about two that are partially just about all machine learning products
and also have implications and ethics.
So one is how much error can the business tolerate in its predictions?
And so as I think through this, it's a contextual question where, you know, you can imagine Amazon
using a collaborative filtering algorithm to recommend products and personalize products
to its customers.
So there, you know, what's the worst case scenario if Amazon recommends that I buy a toilet
as opposed to buying told stories were in peace?
I might be, I might be humorous, I might be like, well, this is funny.
It could have implications as a couple of sort of horror stories where I believe somebody
bought an earn for their, after their grandfather died and then subsequently they were sort
of presented with all of these morbid death trinkets.
So obviously that can have some emotional impact on a user, but more or less like, you
know, a recommender in a commercial setting is one that can tolerate a relatively high
level of prediction error and therefore, you know, machine learning team doesn't need
to work and work and work and work until they get to super high precision around accuracy,
recall, et cetera.
So just thinking contextually about that is, does have both pragmatic implications as
well as ethical implications.
Erase Roe has seen this going wrong is in use cases where a business like the end users
have a psychological perception that they need to have certainty from their system.
So this is classic and say auditing and I think we talked about this in the last podcast
where, you know, if you're big for consulting firm and your job is to audit 10k statements
for your clients, you know, there's not a lot of error that can be tolerated in those
predictions because the whole name of the game is to assure whatever that may be, at least
from, you know, from our psychological perspectives that yes, indeed, this 10k statement is in compliance
with the general acceptable accounting principles, right?
So that's saying that these fundamental questions in the output of machine learning algorithms
which aren't deterministic, they can vary over time, but that, you know, that this power
broadens the applicability of what we can do with software assistance today in such
amazing ways, can have impact, can have ethical impact if certain users of your population
are not as well represented in a data set and therefore the performance on the algorithm
as it relates to the accuracy of predictions for products or services or whatever, you
know, whatever for them is less sound than a population that's better represented.
One of the other ones that I really like to think about actually was inspired by Jonathan
Zinger in his super awesome article asking the right questions about AI, where he talks
about how rigorous and precise we need to be in determining what our objective function
actually optimizes for.
So he gives the example, you know, the much cited example from the pro-publica recidivism
prediction algorithms where the impression that the end users of the system had, you
know, the judges was that this system would tell them the likelihood that somebody would
recommit a crime as a function of the sentence that was given to them.
So if they're given a five-year sentence, you know, low likelihood, if it's a two-year
sentence, they haven't been punished enough and there's a high likelihood they're still
going to, they're going to come back and receive us.
He shows in that article that he's so definitely done is that what the system actually measures
is the likelihood of conviction, which is a very different problem, right?
So it becomes clear to us that it's like, oh, God, well, of course, African Americans
are going to have a higher likelihood based on in a past precedent.
And so I see that all over the place in businesses, too, where a business team might think
that they are optimizing for, let's say, long-term revenue from a credit card customer.
And actually, if you look at how the algorithm's been designed, it's just plain old conversion.
So they're not actually trying to get great customers, they're just trying to get more
customers.
And that has both business impact because, you know, you might be getting a lot of customers,
but that might not actually lead to long-term revenue.
And it has ethical impact because the customers that you end up converting might not actually
be sort of like well-served by the lines of credit that are given to them.
And therefore, just being precise on sort of where what our target variable is and what
the implications are and optimizing for that has, again, both business considerations as
well as ethical considerations.
That calls to mind a podcast that I did with Roma Rosales who is Director of AI at LinkedIn
and that one was called Problem Formulation for Machine Learning.
And the example that they gave that was really interesting was, you know, some system that
they had a recommendation system or like something about groups, I think, that kind of
optimized around engagement.
And they, you know, their first versions of this system found that, hey, if we send
more emails, people will be more engaged.
But it didn't really capture kind of the quality of the engagement that they were looking
for or the notion that they can piss people off by sending too many emails.
And the big takeaway there that I found interesting was that while this Problem Formulation
even in your model is kind of the first step, it's a very iterative process, right?
You start someplace, you experiment, you learn, and then you can continue to evolve the
way you define the problem and the nuance that you incorporate into that problem definition
to kind of more closely zero in on what's really important.
I really love that you're saying that it's an iterative process and it's absolutely
right.
So, you know, I mentioned earlier on these pre-mortems where we try to imagine what
might go wrong, but the world is always much more complex than our measly imaginations
can capture.
So, it is really important that you go in with a certain amount of assumptions and then
we learn.
And we learn not only about the algorithm learning and improving its performance by updating
parameters, et cetera, but about our paying careful critical attention to all of the assumptions,
the sort of baked in assumptions that we weren't aware of until we put things out in the
world.
It's funny.
Nick Bostrom has the famous anecdote about the paperclip maximizer in people in the
world.
So, folks aren't familiar with that.
It's basically, you know, we design the system that's optimized to create.
I think it's created as many paperclips as possible or in the fastest amount of time.
I can't remember exactly how it's formulated, but it leads to this, you know, the algorithm
realizes iteration after iteration that humans are the blocker in paperclip production.
So, it's like, right, let's exterminate the humans and then we can just mine all the
aluminum in the world to our hearts delight.
And when I first read that, I was sort of like, oh, God, like it's, it, it, it, it, it
pricked up my sensitivity around what I consider to be the sort of dangerous discussion around
existential risk from AI.
But the more I think about it, if you view it as a fable, it's a powerful fable because
as you just described, optimizing for engagement, optimizing for some, some localizable, measurable,
you know, variable, observable variable, can have drastic implications that we're not
aware of.
And what I like about the, you know, the iteration is it needn't be that we succumb to the all
powerful paperclip maximizer if we are aware of the fact that there are risks in our
reducing down the world to a quantitative, measurable, optimizable frame.
But then keeping our lights in our eyes open as we learn to know that this is indeed a
reduction to know that these systems are, are not super intelligent, you know, they're
correlation gradient descent, you know, search, search space optimizers, right?
So that, and that's it.
And, you know, and obviously there's incredible progress going on in research community.
But if we, if we humble ourselves and humble our interpretation and actually realize how
limited these systems can be, then it forces us to open our minds to think about the critical
questions we need to ask from a values perspective as we, and to empower ourselves operationally
through, you know, through these, like, this iterative learning to, to fix things.
That's hard for consumer enterprises in particular regulated enterprises because they don't
tolerate mistakes.
There's, you know, the whole culture around risk management is around, like, absolute
prevention of, of a bad outcome, adherence to compliance standards and that mindset,
a compliance mindset and a waterfall oriented legal risk management mindset comes into
a lot of tension and friction with a sort of iterative learning, let's fail, but then
we'll, you know, we'll rectify things, I approach that is often required for machine
money.
Right, right.
And it also echoes very strongly back to the, kind of the sixth, sixth step in your
process, this monitoring and maintenance, if you're not monitoring the resource consumption
of your paper clip maker and its secondary implications, you can't really effectively
refine problem definition in the next iteration.
Yeah, my VP of engineering will be delighted to know that, you know, I sort of, I used
to be really bored by things like logging, you know, measurement of stuff, because most
boring stuff I want to think about, like, algorithms and math, and now I am, I'm endlessly
fascinated, I have a lot of respect for the technical specifications and the rigor that
goes into building an auditable system.
I give you an example of one of the problems we've been thinking about.
So in the wake of GDPR, we're working in Canada and so there's still some questions related
to how the privacy regulations might be updated in keeping with, you know, what happened
in European Union and then sort of discussions around the globe or on the appropriate privacy
frameworks.
And the more progressive enterprises are acting as if GDPR were international and global.
And so one of our customers, we had it, was thinking about storing predictions.
So, you know, build out a machine learning model, it's in the marketing space, so it's
recommending which offer, which action would be the next best action or offer for different
customers.
And, you know, when that prediction is made, there's the data went into train the model,
but then there's, you know, the output of the model at a given moment in time.
And so we were thinking, what if sometime in the future, you know, right now we're
November 29, 2018, what if on April 11, 2019, a customer has a question about why they
were served some particular offer, you know, back six months ago.
And so, you know, how would one resolve that if the model is indeed updating over time
and if the output of a, you know, the output of a prediction in April is any different
than it was in November.
And so just keeping that log, right, not only thinking about logging data, not only thinking
about sort of logging, you know, anomalies in the system, but indeed logging this derived
data that, you know, GDPR would call profile, or some sort of, you know, some, some probabilistic
output of a person's likelihood to act on something, it's just like there's a new data
category that companies need to think about when they're, you know, when they're logging
things and they're thinking about oddening their system.
So we should probably move through this list a little bit more quickly.
Number two is design.
What, let's talk about that, that phase with regard to the questions that it should be
prompting.
Yeah.
So we were thinking about design here from a front end perspective.
So it's a big term, but we were sort of narrowed it down to designing product interfaces.
And the questions that we think about here are a lot around how you collect data, both
so that you are reducing some of the ambiguities.
If a question's hard for a person to ask, people are probably going to give answers that
lead to low quality data, as well as some of the representational harm that the way in
which data formats are presented could cause to people.
So an example I like here is gender.
So right now, I think Facebook has 73 and it could be off on the precise number, but
I believe it's 73 different gender categories.
And there are 73 different categories that shows that the variable is continuous and not
discrete, right?
So if there are two or if there are five categories and we can easily be like on one of these five,
it's probably discrete, but here it's this strange mapping of something that is a, it's
easy, right?
It's continuous and nonetheless our systems, we need to put them into a tabular format.
And that it leads to these interesting representational questions underneath the scenes.
And then the third thing we think about in design is, is this a human in the loop system
or is this a completely automated system because there's different legal and liability questions
associated with either a person?
And so is there as part of this, does the cross functional aspect of this framework get
represented in the design phase?
Do you need to be, you know, I guess I'm thinking of, and in fact I think I remember reading
in the paper, you kind of referring to Agile Ethics or Agile, the word Agile came up
somewhere and this in particular kind of calls to mind the whole, you know, do design
with your customers from, you know, traditional software development, Agile applications.
Yeah, so I quoted the term Agile Ethics from Alex Dunn who is the executive director
of a company called the Engine Room based in the UK.
I think she's fantastic.
And I heard her, I actually shared a blog post about the term Agile Ethics that came out
during AccessNow's Rights Con conference in Toronto in May and I met her there and I was
just like, oh my god, I love it.
That's exactly what I'm thinking about.
And so it's funny, I've used the term Agile Ethics with a bank and the, what this does
not mean is that like, you know, we can sort of update our principles on a day-to-day
basis, so one day fairness means, you know, equality of opportunity and the next day it's
equity of outcome, right?
That is not what I mean by Agile Ethics.
What we mean is that, you know, ethics need not be that one-time, final compliance review
of a system.
It should be embedded into the phases of an Agile product development methodology.
And then second is, yeah, as it relates to design, so this is going to be the front end
of the system consent is a big question here.
So in thinking about, you know, how are you going to represent a consent mechanism to
your customer?
Is it going to be, as we know, the sort of standard tacit, you know, here's a 55-page
legal document that nobody reads, but in order to play part in society and the system,
you just sort of tacitly accept it or is there a way to represent it in a, in a friendly,
so not a, like, not sort of cumbersome way, but to make privacy a little bit more legible
and meaningful to users now that we care about it.
I think it's, it's, it's heartening to seeing that more and more people are seeming
to care about it in the wake of, you know, some of the privacy breaches that have occurred
in over the past year.
So speaking of privacy breaches, data collection and retention.
Yeah.
These are big issues here.
And in fact, probably, you know, this is, this is maybe an area that's more obvious to
the people when they're thinking about responsible AI and ethics, or at least it, you know, comes
up frequently, right, from a privacy perspective.
Is it broader than people tend to think?
I think, I think if it is broader, it's that often problems that we associate to ethics
in ML and algorithms actually start with data.
And so, you know, I think by now, it's pretty well understood that it's garbage and garbage
out.
So, you know, if you've got a data set and you've got awesome representation of rich Caucasian
males and very scant representation of not-rich African-American females, then, you know,
the performance of your algorithms can vary across those, across those represented sets.
So I think what's tough is actually solving this, you know, in such a way where companies
think about modifying how they are collecting data and how they're storing it.
So as to sort of address this problem at its roots and its foundation, it's like, it's
one thing to note that it's a problem and it's another thing to go through the changes
required to like update your data collection and retention procedures.
I think the other one is from a risk management perspective and information governance.
Prior to ML, the sort of rule of thumb would be, if you don't need the data, delete it.
Do you know what I mean?
There's, there's of course staff-
You don't need a particular attribute as well.
You don't need an attribute.
Yeah, and if you don't, yeah, and if you don't and think about email, like email retention.
So, like in the information governance community, there's going to be, you discovery and evidence
and production requirements that mean that for a certain period of time, you know, things
need to be saved and stored in case there's some question in case, you know, in case files
need to be reviewed.
But once you're outside of that, it's just like, got rid of it.
And obviously it's cheaper to store massive data at scale now.
So that's the sort of, the economics of that have shifted a lot over the past 20 years.
But from an ML perspective, this came up once in a discussion with a financial services
firm that wanted to capture seasonal irregularities or seasonal regularities across multiple years
in a particular domain.
And if the retention folks going back to cross-functional collaboration say, the week
after year three, and then the machine learning folks come in and they say, oh, no, no,
but we don't want to overfit to, you know, the recent theme when actually have sort
of a more historical view, there can be misaligned objectives, I'd say, in terms of, you know,
what to retain and what not to retain.
Yeah, I was just having a conversation with a manufacturer in the transportation industry
and they were talking about, or we were talking about cultural implications and culture
clashes kind of along these lines and the same exact issue came up, they're collecting
all of the sensor data about, you know, how their vehicles are being used and their retention
people are telling them, you know, throw it away and, you know, do things like kind of
early anonymization and they are, they recognize that they don't really know what the potential
future value of this data is and how they may want to use it.
And they're kind of digging in, I guess, for lack of a better term to kind of fight this
long fight to try to shift this culture or at least come to, you know, some happy medium.
And I don't know that there's any formula for doing that.
You know, I recently became obsessed with an anecdote about Henry Ford who, as a by-product
of his work creating cars, he set up a bunch of sawmills in Michigan and he invested in
this almost so that he could reduce this bottlenecks and his supply chain for wood, so they
could, you know, make steering wheels and baseboards, et cetera, for the cars a lot faster.
And in doing so, he had all these stumps and branches and sawdust, like all this leftover
wasted wood stuff.
And so he ended up combining the wood with tar to create charcoal, briquettes, and created
a separate business line that has now become Kingsford charcoal, which is still an emphasis
today.
I did not know that Henry Ford was the father of Kingsford charcoal.
And what I love about that story is I actually think it helps us sort of rewrite history in
thinking about the legacy of Henry Ford for the data era, where we thought about, you
know, the, like, Henry Ford is all about automation and the assembly line and reducing and squeezing
out every ounce of variable cost from production systems to scale, but like for the data era
and the machine learning era, what's interesting about business processes is that they are
unique vehicles to create data and knowledge about the world that only exists by, as a
byproduct of the activity of doing that work.
And, you know, for me, this is sort of the, had you really, a lot of enterprises struggle
with disruptive innovation in AI.
It's kind of like, okay, we'll optimize something, okay, we're going to automate an existing
job.
But like, is this it?
Is this all we get here?
Is cost savings from AI?
And it's like, not if you do this creative shift in the value of your business process
and start to create a unique information asset, however, as you just said, doing that shifts
around, you know, how we think about data and how we retain it.
And when that data is created, you know, Henry Ford, he didn't, the wood itself did not
suggest charcoal briquettes, it was human ingenuity to find tar with the wood to create a new
business line.
And similarly, it's like, all right, so where does human creativity lie in this whole
AI equation?
Well, the data itself is not worth much.
It takes our designing, our framing, our applying the right algorithm, our lining up the
right back end architecture to scale this to create value.
And you know, that's not an ethical point, that's more of a strategic point.
But I think it's like the essence of innovation with AI.
When it comes to beyond the retention of the data, when it comes to processing the data,
that also has huge ethical implications.
Can you talk through some of the areas that you explore around processing?
I think the most salient point here has to do with the paradox, a paradox of interpretability.
So we tend to think that linear aggressions are more interpretable and easier to control
and govern than deep neural networks, because it's easier for us to align, you know,
which input, which feature, with which weight is correlated to which output, right?
And we say, okay, we feel like we have control over that, that's interpretable.
What I think is happening there, and it's a risk that we haven't really thought enough
about, is that we're actually displacing the lack of transparency from the machine to
the person's mind.
So when a scientist and practitioner comes in and analyzes the data and decides, you know,
which features to focus on, and when they're building out their model, there's lots of
opportunity there for bias of some sort to inflect, you know, which feature ends up
end up being featured.
I'm sorry, a little bit of fun.
Which features matter?
Part of that will be from analytic observation, you know, it will live within the data, but
there's just so much room, you know, for human bias to come in and be shaping the choices
that are made in selection of features without being aware of that.
And so that's, like for me, that's sort of this, it's an overlooked aspect of data processing
that makes us think a little bit more critically about this whole notion of sort of transparency
and interpretability.
And it's one way you have to sort of gut check yourself and be like, you know, are we
imposing our biases here?
The other thing I talk about, and I think this could be a podcast on its own, has to
do with some of the ways to think about data privacy and a machine learning first world
where we're sort of shifting from protecting a data point.
So personally identifiable information in the form of my name, my address, my social security
number to obfuscating an individual within a distribution.
So if you think about sort of, you know, we've got 15 people in a room.
How tall is everybody?
What's the average height?
It would be, you know, the extent to which one individual is contributing to that mean
and the standard deviation from the mean.
And so this is the realm of sort of differential privacy in ways in which we can make it so
that you can't reverse engineer an individual from sort of a large distribution.
So that's, there's a whole section in there on that.
But again, I think it's a topic for another podcast.
And talking about the way we feature our features, you suggested that one of the dangers
is imparting our biases into the models by the way we do feature engineering and the
like.
But it also prompted this question for me around, you know, is there this other paradox
that, you know, there's this class of models that we think of as transparent and so we
don't question them as much.
Whereas there's this other class of models that we think of as opaque and we spend a lot
of energy trying to understand them.
Is there a risk there?
I think so.
I think it goes back to, it's almost like the Nick Bostrom point where it's the risk
of focusing a lot of our attention on, on, on the one area that's been sort of talked
about like that's been discursively accepted as like, this is where the wrist lies.
And people should be working on that like the, all of the work that's going into interpretability
of neural networks is incredibly exciting and important work.
However, I think there's a, there's a larger point that relates to even sort of the social
stratus of and the social like importance of different type classes of algorithms in
the community.
That doesn't mean that linear regressions, logistic regressions, support vector machines don't
have their ethical issues too.
And it also doesn't mean that they're not worthy mathematical functions, you know, for
systems.
And at Fastword Labs, we always used to say, you know, when you're applying ML, start
with the simplest possible algorithm possible to solve your problem that can scale.
And then once you've gotten baseline performance, then maybe experiment up with sort of some
of the more complex models that can process hierarchical features and represent the data
a little differently.
But, you know, don't start with the hammer when all that you need is sort of a chisel.
So yeah, so I think that, I think there's a larger sociological point wrapped up in your
question as it relates to the sort of hierarchy, sometimes of perceived value in the scientific
and engineering community, that, you know, where, and actually, and this shows up in the
paper, in my experience, finding a lawyer, people talk a lot about the sort of scarcity
of PhDs in machine learning, finding a lawyer who knows a lot about the law and also knows
a lot about how machine learning systems work so that they can pave new ground in thinking
about how regulations should evolve or how compliance practices should evolve to encompass,
like the realities of new technical problems at a deep level and not just to sort of a conceptual
level.
Those people are really hard to find.
And I feel like they're heroes whose song needs to be sung a little more, you know?
So in the next step in your framework, which looks at model prototyping and QA testing,
does this go beyond the model evaluation, the, I guess, the statistical aspects of modeling
or is that the focus?
A lot of it is around the statistics.
So Q&A, you know, is this basically testing control, you know, having your test, having
your validation set, seeing how the model performs.
And I think the particular ethical aspect of it has to do with subsampling areas of
the model.
I mean, sort of holistic, you know, how is this performing, but rather are there pockets
of a population where the model seems to be performing well at the, you know, sort of
at the, for the majority group, but if you look at the minority, the performance is opposite.
So this is often referred to as Simpson's paradox.
So it's sort of a particular fine grained comb on Q&A with an eye towards potential fairness
and bias pitfalls.
And then some of the other, one of the other things we bring up in the Q&A testing are inspired
by some of the work that Ian Goodfellow, who I know has been on the podcast, has done
as it relates to basically tricking a model, a malicious actor, tricking a model into thinking,
well, thinking, metaphorical term, into not from a QA tester's vantage point, seeming
to perform well according to the standards that were, you know, measuring the model's
performance by when actually it's been tricked into doing a misclassification or doing
something, you know, or having the wrong output.
Yeah.
First point makes me think of kind of ethical unit tests for models.
Have you come across a scalable framework for doing that or does it not require anything
special?
I personally haven't seen it, but I wouldn't be surprised if someone's working on
that.
And I like, I like the term.
You should.
I like that.
I know we're running out of time, but the sixth step in the framework is deployment
monitoring and maintenance.
We've talked about it a little bit already.
Anything you want to add there?
I'd say the only, we've talked about a lot, you know, advice folks to have a look at
the paper if they're interested in it.
And then I guess the one thing has to do documentation, so not just sort of the, you know, the auditing
and logging controls in the system itself, as well as all the proper DevOps on the tool,
but also the, you know, the hard work of documenting what choices were made when, why.
And there's questions around whether or not that's a sort of internal technical compliance
tool, internal sort of cross-functional compliance tool, or also could be something that is presented
then to end users, so that they, you know, in a friendly, with friendly language, so that
if they have questions around the transparency of a system, you know, they, there's, there's
communication on how frequently the model is updated.
If it's possible to remove, you know, somebody decides to opt out of a system.
The retraining cycles for the model's entail and sort of a guarantee, there's a section
in the paper where we say, well, what if you were to get a confirmation if you opt out
of some data collection mechanism in a machine learning algorithm associated with it that's
like, model's been retrained, you're removed, you're no longer part of this, you know, you're,
you've been forgotten by the system.
That might be overly cumbersome, but it's, you know, something we propose as food for
thought.
Can I talk quite a bit about the paper? Is it easy for folks to find? I just pulled
up the integrate.ai site and it didn't jump out at me. Maybe we can set up a place where
people can kind of easily go to grab it.
Yeah, you bet. If you go on the integrated iSight, there's a, there's a, a tool like a,
a button or whatever called trust, and it's under that trust button and there's a page
for a monthly i, but we can definitely find a way to make it a little bit easier to find.
Because there's a, there's a website that has, like, devoted to it in particular or some
page on our website, but it's accessible through the integrate.ai website under the trust
tab.
Okay. Got it. I see it. Easy enough, any kind of final thoughts or words of wisdom?
Yeah, it's just to sum things up. You know, there's so much attention paid to ethics
these days. I think it's wonderful that we're having large discussions around AI and society
and that this technology is, is encouraging us to question our values so holistically.
However, as we started off, there's, there's still so much work to do in taking this from
sort of the philosophy, the domain of philosophy and the domain of fun conversations that dinner
parties into practical, operational decisions. And, you know, the, what we were trying to do
here is to, is to show that often ethics occurs in the trenches, right? It's like it's the
decisions that teams make in their building systems. They're hard to make and, and they
require accountability and decision rights. And it's within that tension that we think
that we can really do some work to change things. So, so the call to action is to, is to
not only talk about these issues, but to think really critically about how to put them
into practice.
Well, Katherine, thank you so much. Always wonderful to speak with you. We need to find
a way to do it more often. Thanks for being on the show.
Totally concur. It's great to talk to you, Sam.
All right, everyone. That's our show for today for more information on Katherine or any
of the topics covered in this episode. Visit twomla.com slash talks slash 210. Thanks once
again to the great folks over at Georgian Partners for their sponsorship of this series. Be sure
to head over to twomla.com slash Georgian for more information on their building conversational
AI teams guidebook. As always, thanks so much for listening and catch you next time.
