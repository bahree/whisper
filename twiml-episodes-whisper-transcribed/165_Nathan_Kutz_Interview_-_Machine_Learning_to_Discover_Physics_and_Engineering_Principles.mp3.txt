Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington, a quick update about the upcoming Twimble Online Meetup.
Next Tuesday July 17th at 5pm Pacific time, Nick Teague will lead a discussion on the paper
Quantum Machine Learning by Jacob Biamante at all, which explores how to devise and implement
concrete software for machine learning tasks.
Visit twimbleai.com slash meetup for more details or to sign up.
In this episode, I'm joined by Nathan Kuts, professor of Applied Mathematics, Electrical
Engineering and Physics at the University of Washington.
Nathan and I met a few months ago at the Prepare AI Conference here in St. Louis, where
he gave a great talk on machine learning to discover physics and engineering principles.
Our conversation is laser focused on his research and to the use of ML to help discover the
fundamental governing equations for physical and engineering systems from time series measurements.
We explore in fact the application of his work to self-tuning fiber optic lasers, as well
as to biology and other complex multi-scale systems.
And now on to the show.
All right, everyone. I am on the line with Nathan Kuts. Nathan is professor of Applied
Mathematics, Electrical Engineering and Physics at the University of Washington.
Nathan, welcome to this weekend machine learning and AI.
Thank you very much. Glad to be here.
I'm really looking forward to our conversation. I had a chance to see a talk you did at
the Prepare AI Conference here in St. Louis a few weeks ago and I thought it was a great
presentation. It was on machine learning to discover physics and engineering principles
and we are going to dive pretty deeply into that.
But before we do, Nathan, take a few minutes to introduce us to your background and how
you got involved in machine learning and artificial intelligence.
Sure. So we'll be happy to do that.
So I come from a pretty standard, let's say, physics, Applied Math training. I did
my PhD back in the early the 90s and then went off to do a postdoc at Princeton University
jointly with Bell Labs. And at the time, I was doing a lot of just standard modeling
of physical systems. I was particularly interested in optical systems. At that time at Bell Labs,
we were doing long haul communications. So the idea there was to, of course, write down
some governing equations. And typically this was some version of Maxwell's equations for
understanding how light propagates down fiber optic cables. And so I would call this coming
from the forward modeling perspective, which is you write down some set of equations.
You drive them and then you do lots of simulations. And then you build your understanding in that
architecture. And of course, there's some data that's collected. But back in the 90s,
the data wasn't the same as what we think about today. And so I spent a long time there
really building out sort of that perspective and applying it to various other types of
problems once they came to the University of Washington. And about 10 years ago, I got
very interested in using some of these dimensionality reduction techniques that were being used
in the fluid, fluid dynamics community. And there it's called proper orthogonality composition,
which really was the same as PCA. And everybody has these different terms for the same thing.
So then I started realizing that you said it was called proper orthogonality. Proper
orthogonality composition. Proper orthogonality composition. Okay. And then interestingly enough,
the atmospheric scientists called it empirical orthogonal functions. So I started realizing
that everybody's doing the same thing, which is taking your data and maybe doing a little
bit of mile pre-processing, but running it through the singularvality composition. And I
started applying it initially to optics problems and started realizing that, oh my God, this
is an amazing idea. And in fact, it was kind of interesting that I had no idea that there's
whole other community existed. And that's how I started getting into the idea of using
data methods over towards more physics engineering-type problems. And since that time, then I brought
in all of this architecture borrowed from the MLAI type communities around model discovery,
regression, dimensionality reduction. And that has completely changed the way I think
about these problems. And it's also allowing me to start thinking about using these ideas
towards control architectures. And that was ultimately the goal we had in the engineering
sciences. It's always going to come down to some kind of control theory. But this allowed
my group and my collaborators to start thinking about framing things in terms of control.
So since that time, we just are really integrating these engineering systems with machine learning
architectures. And that's how I've continued to develop in this direction is to use both
perspectives.
The context that you presented on, if I remember correctly, was in controlling, was it lasers
or controlling the laser coherence or something like that?
Yeah, I can speak to it. So we were interested. A lot of my expertise comes in this laser
physics, which I had started back all the way back in the late 90s working on laser physics.
And one of the interesting things was that over this time period, this is a big commercial
market. Laser technologies are, I mean, the amount of money in that field is enormous.
I mean, hundreds of millions of dollars a year in sales for lasers because they're pretty
much ubiquitous, right? They're from your supermarket scanner all the way to what's emerging
light art technologies. So lasers have become this really important technological piece in
regards to censored technologies. And so the data coming from this is quite large. And
we'd like to tune them. So one of the things that I noticed was that people were like trying
to tune them sort of hand-tune these lasers for performance and then environmentally sealing
it. So whatever you did to tune it, you'd lock it in and you'd sell it. And if anything
happened, then you'd have to have your rep come out and fix the laser. And then I was
thinking, well, this is a lot easier than a self-driving car. In fact, nobody gets hurt.
And it's only a few knobs. And that was the remarkable thing. This community has been
very slow on the uptake, I feel, in terms of adopting this technology. So we've been
really pushing it out there. So one of the things I highlighted at the talk was, here's
an architecture, a software architecture that you could implement today towards essentially
a laser system that would tune itself. And not only would tune itself, it would learn all
the possible behaviors that has available to itself. And even if something happens,
someone hit the laser and so you've changed the parameter space a little bit, the same
could learn its new parameters and learn how to tune itself under this new conditions.
If I can interrupt, let's maybe take a step back and talk through what it means to tune
a laser. What are those knobs that are used in tuning them?
Sure, sure. So the laser that I've mostly worked on is a fiber optic laser. So it's nice.
It has its own cavity. It's just the laser, the fiber itself contains the light. And the
way that the mode, what's called mode locking happens, in other words, you want to create
a nice, very high intensity pulse of light. And in order to shape the light, there are
a set of polarizers and some waveplates that you can turn, you know, you can rotate them
through 360 degrees. And it turns out, as you turn these set of waveplates and polarizers,
you can get very different behavior out of the laser cavity. And so it's a pretty, you
know, there's about five knobs. There's three waveplates, a polarizer, and also the
amount of pumping or energy put into the cavity. And with these five parameters, you can
get either very exceptionally high performing lasers or you can get just garbage. It's completely
in coherent light that's worthless. So the idea is to use these five parameters to
tune this thing into not only a very coherent ultra short pulse with very high peak intensities,
but among all the set of, there's these islands of stable behaviors, which one should you
be on to give you the very highest performance possible for the fiber and for the gain you
have available to you. The approach you took was an interesting one. It was ultimately,
you know, in terms of the spectrum of machine learning applications, it was fairly simple,
but the way you constructed the problem, you know, is what lent it to that simple approach.
Can you kind of talk through the problem construction and ultimately the solution?
Yeah, sure. So, of course, the first thing I, I guess, one way I think about machine learning
in general is the first thing you have to pause it is some kind of loss function, right?
So this is, this is the, I mean, I think of all of machine learning AI is just really
just some giant optimization problem. And so the main thing with an optimization is just
figuring out what your objective function. The nice thing for us in the laser system
is it's very clear what the objective function was. We wanted a very coherent high intensity
pulse. Now, interestingly enough, in optics, you, you don't actually measure the electric
field. You, you measure the intensity of electric field. So you lose phase information. So right
away, you're limited in terms of what your measurement space is. And there's also one
of the interesting thing. You might say I want the maximum energy in the cavity, but that
has actually not the best objective function available to you. It turns out if you just
make the objective function the energy, then what it will do is the best performance of
the laser cavity will put you at the edge of instability. So it's basically a walk you
up this, it'll walk you up to the edge of cliff. And the cliff, of course, has the best
performance, but you move slightly one way and you lose everything. And so we've formulated
a new objective function, which was based upon not only do I want a lot of energy in the
cavity, I also want to constrain the laser bandwidth. So we were able to construct an objective
function that had both the bandwidth and the energy. And that actually allowed us to keep
a boundary away from the instability. So once we have that objective function, by bandwidth
are you ultimately talking about coherence or, but you said you didn't, you weren't able
to directly measure coherence if I remember correctly. Well, so we can't measure the
coherence directly, but we can measure the energy. We can also measure through the
spectrogram, just what the total frequency content is. So when you look on a oscilloscope,
you'll be able to see immediately what the band, the total bandwidth is. And so you're
trying, you know, for a very short pulse, you have a nice broad bandwidth pulse. So typically,
for these laser systems, they can produce somewhere around 30 nanometers of bandwidth. So
it's pretty broad band. It's around telecom wavelengths. And so it's a great light source
this way. So we were able, so we know then that what we were trying to do roughly speaking
is, you know, produce this. And so we were able to construct the objective function.
That was the number one step initially was just to simply figure out, given for what
we think we want, how do we actually mathematically construct this for an algorithm to go after
the search, right, go after the optimization. So we were able to do and also make sure
that we stay away from the edge of instability for physical systems, right? It really matters
that you've got to have a cushion away from the boundary. You can't just walk up a gradient
and off a cliff. Otherwise, it's it's worth this technology.
And did you consider, as opposed to the objective function with two components, energy and
bandwidth, some formulation that tries to maximize energy, but then back it off or maybe even
out, even outside of the machine learning, maximize energy, but then in the implementation,
back it off a little bit. Yeah, we had various perspectives on this. We
automatically could do something like this, which is once we have the learning, we could
back off of these things. But it turned out it was so easy to use this scope tray. So
in others, that was that was probably the easiest measurement of all that we use that directly
as, okay, this is a perfect measurement because that's something that is is actually probably
the easiest thing for to actually make a measurement of you. It's actually very difficult to
even measure the light pulse itself in the cavity because typically these pulses are
in the 100 attempt to second regime. So this is ultra fast optics. And so measurement
of the light pulse itself requires you either to put on some kind of very expensive measurement
device. And of course, you don't want that in a practical setting because you'd like
a cheap device. And so we just use the bandwidth itself as a proxy. Okay. And then once
you have that, you can now you can set the thing free. It's got five input knobs, these
four or three wave plates of polarizer, the gain, and you just say, okay, go look around.
And the nice thing is these parameters, so this is another nice feature. These wave plates
and polarizers, right, they, they, they, there's two pi periodic, right? You rotate a polarizer
around, it goes around two pi, you're back where you started. So it allowed us to do what
we call the toroidal search, right? So you could start spinning these wave plates and polarizers
at irrationally related frequencies. And what this will do, it was will cover some kind
of hyper torus. And so it's a very effective search strategy because if you have parameters
that don't wrap around themselves, right? So the slowest, the slowest search, when it
goes around two pi, all the other ones have gone around even multiples times, it allows
you to map out this, this torus structure and look in the torus where the good regions
are for, for these, for these poll solutions.
There was an element that I recall from the presentation that was not just that you
were able to optimize the subjective function, but that you were also able to do it in such
a way that gave you insight into the underlying physical principles. Can you, are we at that
point in kind of the story yet? Or yeah, we can certainly talk about that because I think
that's actually one of the more exciting places. So we started with this idea of machine
learning on the laser for control, which was, you know what, if this thing can just tune
it, we're fine. It can be a sort of an AI agent sitting on top of sort of these machines
and you just give it the knobs input output and get it going. But the, where this led to
was this idea of data driven discovery of governing equations. So we started thinking
about if I measure system and I don't know it's underlying physics. So a lot of the models
I had spent time with in the past, I actually knew the governing physics. Optical fields
are governed by Maxwell's equations and of course they have these quantum interactions.
So we can write down these governing equations and, you know, we've had a lot of experience
with them. We know that they are true in, in these regimes we're operating in and so
we can do in some sense first principles modeling. But another part of my life that developed
over the last five to ten years was looking at neuroscience. So now you'd measure from
the brain and we don't know the governing equations of the brain. We don't have an F equals
M A formulation. In other words, here are the governing equations. We have posited various
models for what neurons do. But on some level you're interrogating a system where you fundamentally
don't know the governing equations. And so what we started to, what we set out to do then
is to start figuring out, is there a way from measurements alone, time series measurements,
can we infer the governing equations? And the beauty of this is it all comes down to just
Ax equal to B. So we're excited because that's, you know, I like to tell everybody that's
the height of applied math. If you actually understand Ax equal to B, that's something because
Ax equal to B is a lot harder than it looks. You know, when I think of a Maxwell's equation,
it's a lot more complicated than a linear, a linear equation. How, talk a little bit about that
background of how you kind of equate these ultimately complex and and higher order equations
to a linear relationship? Sure, you bet. So the way we've formulated the problem is we take time
series measurements of some dynamical systems. So we think of these things as systems were
measuring, they're changing in time, they're evolving. And we would like to discover the equations
of motion. So one of the my backgrounds mathematically that really framed everything I did application
wise was in dynamical systems. And the basic structure for that is to look at DXDT. The change of
some vector X is equal to F of X. So it could be a nonlinear function of X. It could be
very complicated. It could even be have parametric dependencies. So that's the simplest thing.
DXDT equals F of X. And this is already framing what we want to do. And in a sense that the
standard way we think about when we pick up a textbook on E and M or quantum, we're given F.
We know what it is. Now what we're trying to do is say you give me time series measurement X.
Can I tell you what F was? What was the F that's consistent with the data I read? So
DXDT is the first derivative of with respect to time. And what you're doing is you're taking
you've got some time series measurements that you're taking and you're using them to ultimately
work your way back to the underlying function from these time series measurements. Exactly.
And so the way we do this is we simply say the following. We say, okay, DXDT. So let's call
that X dot. So it's the rate of change of time. So if you give me time series data, I can differentiate
it. But you gave me X. And that means I can compute X dot. Okay, that's for an X dot is going to be
the B vector in AX equal to B. Now let's talk about what the A is going to be. So now what
we're going to do is say, well, what could the right hand side be? Well, the right hand side could
be a lot. There's lots of possibilities. And I don't know what they are. So what I'm going to do
is just make up a library of all kinds of potential right hand sides. Well, maybe the right hand side
depends on X or X squared or or X cubed or sine X or E to the X. It's only limited by my imagination.
So the idea is typically to put in a bunch of potential functions, which could be right hand side
terms. So the matrix A we're going to build is the following. Each column will be a candidate
right hand side functions, a candidate that for describing what F is. Okay, so I could put in
thousands of candidates. And that's what creates the columns of the matrix A. And each row is the
time measurement. So if you give me X, I can compute X squared. I can compute sine X over time.
So the rows will be time columns will be potential functions that might be in the right hand side.
That's the matrix A. So what we're going to do when we do this regression, this AX equal to B is
just say, well, here's a matrix A. The right hand side B I have, which is the time derivative,
solve it. And generally, you're going to have a lot of time measurements. So this is going to be
a system, which is highly over determined. And I want a solution. And here's the key. I want a
sparse solution. So what's going to happen is when you do this regression, it's going to select
out the smallest number of columns that they have to use so that X dot is equal to F of X.
So, and when it does this, it selects out the physical terms. So for instance, what this will do
for you is if you measure a fluid flow, just take measurements of the time series of a
vorticity of a fluid flow, what comes out in this regression process is the navier stokes equations.
If you were to look at trajectories of a flying object, thrown object, what would come out would
be F equals M A? It recovers all of these systems, all the canonical models out of mathematical
physics. You just give me the time series measurements. It recovers back the governing equations,
all by this simple regression of a linear system where the matrix contains all these potential
library terms. And by promoting a sparse solution, it just gives you back your solution.
Yeah, so I thought this part was really clever. Yeah, again, as you just said it, you are solving
just a simple linear regression, but your terms are, you know, they can be anything. They can be
quadratics and qubits and exponentials and all these different things. What is this? You know,
in a lot of ways, it's like a decomposition. Do you consider this part of what you did to be
kind of a novel contribution of your researcher is that, you know, is it really more, you know,
standard or well explored and you're applying it to these types of physical problems?
As far as we've seen, this is the first application of this towards really discovering dynamics
itself and getting out dynamical systems. What's kind of remarkable about it is, in some sense,
how to say it in a good way, we had no business discovering this. While this wasn't discovered
30 years ago, it's so simple, right, that like, you know, it's like, why didn't someone do this 30
years ago? They should have. They should have. They really should have. And it's really interesting.
I think from the physics community and the engineering community, and I'm going to talk
personally for a minute here, if I look back at my educational experience, whenever we solved
under or over determined systems that were AX equal to B, you just did it with Lee Square fitting.
There was no concept of sparse solutions there. You know, it's interesting because
Tip Shirani has a paper on the last so method from 1996, which have gotten a ton of traction,
this basically variable selection technique, but that never made it over to like this physics
engineering community. And in some sense, the technology has been just sit there, and we just
finally made the observation. Well, wait a minute. This actually can work here and look at this.
It actually works. And you can just run it on your laptop. It's not like this. It's not one of these
neural, you know, neural networks or even another competing technology developed by Hod Lipson at
he's now Columbia. They had done something similar, but they had done a genetic search. So,
you know, they basically would put a genetic algorithm that's positing some right hand side terms,
which ones are best. Let's keep them, let's modify them and have them evolve until we find this.
So it was fairly expensive computationally, because it was really going through tons of
possibility versus this regression is extremely simple. It's fast, it's effective, it's very robust.
And since that time, we've been starting to use this on more complicated problems where we
don't know the answer. So for all the problems that we know the answer, which are these canonical
models out of the mathematical physics engineering community, we've been able to discover them all.
So we've been able to sort in some sense validate or cross validate the method. And now we're
moving it on to biological applications. And one of the exciting areas we've been trying
to find this on is the sea elegance worm, which is a one millimeter worm. It's got 302 neurons.
And when you watch these neurons, they can now do whole brain imaging of this little worm.
It's amazing what people can do with this worm. We are now starting to understand that when the
worm is forward crawling or backward crawling, it's very low dimensional structure that's
being executed there. And what we really want to understand is the control architecture
and the governing equations of how 302 neurons coordinate their dynamics to produce
very simple dynamics, which are functional across the entire connectome of that worm.
So this is an exciting way forward because there is no truth in this case. Nobody has the right
answer, right? So this is going to give us the ability to get towards understanding biological
neural systems with a very new architecture, which seems to be incredibly robust and easy to use.
So I want to dig a little bit further into the biological applications. But before we do that,
I wanted to connect a couple of points that you mentioned. You mentioned the idea of
sparsity as being key to what you've done here. And the idea there is that if you collect all
these data points, even if the equations are perfectly expressed in simple underlying
relationships between these terms, you've got noise, you've got other things, and the result of
your analysis, your optimization is going to likely include some very small fractional
terms of things that don't necessarily need to be there. And so what you mentioned,
the lasso work in passing, if I remember correctly, what you did was you implied lasso to
bring out that sparsity to reduce those terms. Is that correct?
Yeah, so we use something lasso-like. So what we actually did, and this is
maybe taken as a criticism alasso, it's interesting if you really look at what people do,
people tend not to use lasso directly, but something like elastic net. So the lasso itself is
a little bit unstable in terms of actually finding for you the right sparsity pattern that you're
looking for. And so elastic net helps regularize this process. But we actually avoid those
altogether. What we do is we do a sequential least square thresholding. So essentially least
squares is extremely fast, but what least squares will do is says everybody participates. So there
is nobody that's zero. However, there's a bunch of terms that, well, you know, it depends on how
you define zero. It's 10-6-0. So we start doing this in the sense that we say, it works incredibly
well where you can just do these squares, which is very fast. All the terms below some threshold,
you throw away. And then you do it again with the terms that are left over, and this thing converges
right back right to the solution you want. So it's in some sense a more stable way. We found
it to be stable. We're working actually right now on convergence proofs around this, but we found
it to be a very stable robust way to do sparse regression. And so it's lasso-like, but it doesn't
inherit some of the known problems of lasso itself. How do you end up setting the threshold? If you're
working on modeling something that's got a constant like a Planck's constant that's ultimately
very, very small, how do you make sure you're not getting rid of important things? Yeah, so that's
always a great question. I mean, so what we've been doing so far for most of the models that we have
is we will do some kind of cross validation for this threshold level. And actually it's interesting
the threshold level, we haven't done this yet, but probably the most effective way to do it is to
have an adaptive threshold level. So if you're on threshold, if you're going to apply the threshold
and your iteration number three, it should probably be different than on iteration number two.
But even if you don't do something like that, you can typically learn it through cross validation
what the sparsity, what the threshold level should be. But that is your one tuning knob. And by the
way, it's very, very similar to last so where in the last so you have to tune what this L1 penalty
strength is. So typically there's this lambda parameter where you can make it bigger or smaller.
And the bigger you make it, the more it enforces sparsity. And that's very similar to the threshold
here, which is it's it's a tuning knob that all of these sparsity promoting techniques seem to
carry around. And again, there are ways that people suggest or argue you can kind of cross validate
to get the right values for these things. But it's still a little bit of a still a little bit of
alchemy on that one. Okay. And so you were starting to introduce us into to some of your future
application of this moving away from these underlying physical control systems where we we know a
lot of the equations to something that is more complex and unknown. And in this case, the
the relationship between neurons and the C elegans. Is that the right way to say it? Yeah, yeah,
exactly. And I would say more broadly, the thing that we're really trying to go after
sort of in sort of the grand challenge problem of this, I still remain very rooted in thinking
about engineering and physical applications. But one of the things that's become increasingly
clear to me is what we've developed over the last centuries is the ability to exceptionally well
with what I call uniscale physics problems. In other words, I can write down one set of equations
and models in the whole system. But it's increasingly clear that a lot of these complex systems we
look at that we want to understand today, they're multi-scale systems. There's a fast scale that
is doing some kind of physical process that's connected to some mezzo scale, which is connected
to some macro scale. And by the way, they're all talking to each other. And they're all very
different physics at these levels. And yet that is the dynamics that really produces the manifestation
of the dynamics on the measurement space you may be looking at. So for instance, you may not care
about the micro scale because you're watching it from the macro scale. But whatever happened
in the micro scale is in fact feeding that information up. And you need to, if you're going to model
the system well, you have to understand how these different scales are talking to each other.
So it really requires us to start moving away from uniscale physics models, where I just write
everything down and do simulations, too. Actually, there's different physics at each level.
I've got to figure out how they're connected together and discover different physics models at
each level. And that's going to be the way we make progress forward. At least that's those,
and by the way, biology is a perfect representation of this. Even in the sea elegans, there's
quite a different number of different time scales that ultimately produce time and spatial
scales that ultimately produce the thing that I'm most interested in, which is this macroscopic
behavior of the worm crawling forward or crawling backward or doing what are called omega turns,
where it turns its body around. And the only way to understand that completely is to understand
this architecture of multi-scale physics. And so to make that a little bit more concrete,
we're talking about one scale might represent the chemical, the underlying chemical reactions,
and then there's a scale above that that's representing neurological activity and connections
and things like that. And then finally, at the top scale, how the entire set of neurons are
sort of evolving together in a functional form, all being driven by the chemistry, which drives
the voltage activity of neurons, which then drives this bigger functionality of muscle activation.
Yeah, that's exactly the cascade. And you know that the muscle activation is a very different physics
than the chemical activations, right? These are different physical processes. And you can't just
write one model down that says, you know, and again, I think the discovery we've made in physics,
right? We have Maxwell's equation, we have quantum mechanics, we have the fluid dynamics,
navier strokes of fluids, we've done impressive things, right? We can build airplanes,
we can build iPhones, you know, quantum based devices. The kind of stuff we can do is quite
remarkable, but we've kind of hit a wall there in terms of our modeling capabilities.
It strikes me that in the physical case, you know, we've got these multi-scale problems do exist.
Maybe we just don't understand them because of their complexity or the number of skills or
things like that, meaning, you know, at some level, there's quantum mechanics and there are
I'm thinking even for simple kind of mechanics problems, you know, there's, you know, maybe even an
infinite number of levels, you know, ranging to, you know, atomic cohesion and all kinds of stuff,
I don't know. Yeah. Are you getting a question in there somewhere? Yeah. No, but it's actually
interesting you asked this, right? Because I think the way I think of multiple scale physics,
like, you know, if I was in a push a table and see how it moved, I push it with a certain force,
you know, typically we do that, we draw like a vector diagram, okay? Your force is this,
you know, this much at this angle and you could compute through ethical, so may what's going to
happen to the table movement. An alternative to this would be I'd model the atomic interaction
between my hand pushing the table and the atoms of the table. Of course, nobody does this, but
there are systems where you're required to do that. You really need to do that kind of, so
especially molecular dynamics simulations, this is what they're trying to do, right? Really
model the molecular interactions that produce some macro scale structure that's important to see.
Okay. Okay. So that's where we're headed, I think. Interesting. And so how far have you gotten
in applying this technique to these kinds of multi-scale problems? So far, we have a first paper that
just came out, it's on the archive, where we did essentially some toy problems where we've had mixed
dynamics, a fast scale and a slow scale dynamical system and we asked the following question. If I
just gave you the data and I'm just measuring this joint dynamics together, what would I get?
Do I actually, can I extract them from each other? Can I, is there a way to disambiguate what's
going on in the fast scale from the slow scale and could I really discover these two dynamics? And
the initial evidence is suggest yes, you can. In fact, you can discover a slow and a fast dynamics,
you can even discover how they're connected together. So we've done this on some, let's call it the
baby toy problem. I mean, that's where you always have to start. I always like to start with something
where we can test the truth against it, right? One of the things that makes me uncomfortable in some
of the application areas of some of the, these methods that I've seen is people will do hard problems
and sometimes they'll neglect to do the very simple problem to see if it even just works on the
simple problem. And I never go forward unless I can get it to work on a simple problem because
if you can't get that to work, it's certainly not going to say anything about a hard problem.
Right. And so I think the ability to test bet it and see if it works is important. So we've got
that first result along that direction and then our hope is to continue building this way.
And then also acquiring good data sets is a really important thing. You know, getting a good
data set from a, you know, physical biological system where there's good time resolution, good
spatial resolution where you can really bring these methods on. We're getting better and better
about how to handle bad data. But you know, at the end of the day, there's a great deal of pressure.
If you want to build a good model, you need good data, right? I think that's one of the really
big advantages of computer vision, right? You can, you can bring in a bunch of really good
data sets, right? Here's pictures. They're high quality HD cameras. And that helps a lot. They're
not, you imagine if you were trying to train image net where all the images were corrupt and blurred.
You know, how well would it do now on classification tasks? I think it would be much
challenging. And the same thing for us is the case is we want to be able to have really high quality
of data initially. And you know, the biologists are, are getting their job done. It's amazing
the revolutions you're seeing in biology in terms of the kind of recordings they can do.
The, just the vast sheer quantity of what they're able to extract now from even in the
C. elegans worm. They'll be able to image this thing in real time where you're seeing the whole
brain region 150 neurons just lighting up as it's doing its, its motion is, is just unbelievable.
This is something that 10 years ago, I don't think anybody thought was possible. And here we are.
We've got it. So it's really exciting. So you started to answer one of the questions I had, which
was for these different scales for C. elegans, where the data was coming from. It sounds like
at some level, at one of the levels you described is coming from imagery. But is there also a level
beneath that that is looking at actual chemical activity? And I imagine the level above where we're
talking about physical motion is coming from imagery as well. Yeah. So the motion comes from imagery.
A lot of times you can easily just see it moving around. There's the actual imaging of the neurons.
One level down from that would be electrophysiology recordings where you actually stick in a
voltage probe into neurons and you monitor those directly. And they do have this available in
monkeys and rats and other creatures like this where you can actually get some direct recordings
from neurons to see how they're spiking. And so there, of course, is a little harder because,
even if even in your best days you might be able to get a tetrode fork in the brain with
the four prongs or the fork have four recordings each. So maybe get up to 16. And so we can do this
for instance with insects and tenelopes of moths. We work with a gentleman over here in biology,
Jeff Riffle, who has this. So there is more and more data becoming available, but it's still
hard to acquire. And in humans, of course, it's very difficult, right? Because you can't just
go in and put probes in people. They have done this, of course, with certain people like, for
instance, we are working with people with where there are e-cog recordings from the brain,
often a big array. But these were from patients who needed this for because they had such severe
epilepsy that they put this into monitor that. And as a byproduct, we actually have great data
to collect on the brain itself. So we'll see what the future holds in terms of what it's, I think
it's just remarkable how they keep coming up with new and novel ways. I think what we want to do
is be prepared on our end to have the mathematical architecture in place so that when they do produce
these data sets, we can essentially squeeze out as much as possible, as much learning and science as
possible. I think that's what we're really after is that is the mathematical engine so that as
these data sets come in, we can we have a new way to start interrogating that data and discovering
underlying biophysical principles or physical principles. Well, that's fantastic. Well, Nathan,
I really appreciate you taking the time to walk us through this. It's really, really interesting work
and I'm looking forward to kind of keeping tabs on it and seeing what else you guys you all come
up with. Great. Thank you so much for having me. It's been a pleasure and hopefully, hopefully
everybody who listened got something out of this. Absolutely. I'm sure they will. Thanks Nathan.
Okay, take care.
All right, everyone. That's our show for today. For more information on Nathan or any of the topics
covered in this episode, head on over to twomolei.com slash talk slash 162. Don't forget to visit twomolei.com
slash nominate to cast your vote for us and the people's choice podcast awards. And as always,
thanks so much for listening and catch you next time.
