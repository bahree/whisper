Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In this episode I speak with Mike Delbalso, product manager for machine learning platforms
at Uber.
Mike and I sat down last fall at the Georgian Partners portfolio conference to discuss
his presentation, finding success with machine learning in your company.
In our discussion, Mike shares some great advice for organizations looking to get value
out of machine learning.
He also details some of the pitfalls that companies run into, such as not having the proper
infrastructure in place for maintenance and monitoring, not managing their expectations,
and not putting the right tools in place for data science and development teams.
On this last point, we touch on the Michelangelo platform, which Uber uses internally to build,
deploy and maintain machine learning systems at scale, and the open source distributed
TensorFlow system they've created, Horavod.
This was a very insightful interview, so get your notepad ready.
Before we jump in, over the last few weeks, you've heard me talk quite a bit about our
MyAI contest, which explores the role we see for AI in our personal lives.
We receive some outstanding entries, and now it's your turn to check them out and vote
for a winner.
Do this by visiting our contest page at twimmolai.com slash MyAI.
Voting remains open through Sunday March 4th at 11.59 pm Eastern time.
One more quick announcement.
Join us on Tuesday, March 13th for the next Twimmol Online Meetup, in which our presenter,
Sean Devlin, will be doing an in-depth overview of reinforcement learning and presenting
the Google Deep Mind paper, playing Atari with deep reinforcement learning.
Head on over to twimmolai.com slash Meetup for full details.
And now on to the interview.
Hey everyone, so I am here at the Georgian Partners Conference, and I have the pleasure
of being here with Mike DelBalso, Mike is the product manager for machine learning platforms
at Uber.
Yep, Mike, welcome to this week in machine learning and AI.
Thanks, happy to be here.
It's awesome to have you, and I especially love it when folks tell me that they actually
listen to the show before I'm about to interview them.
Yeah, I listen to almost every episode at the gym.
Oh wow, nice, nice.
Do you like, do you put it on like 1.5x or my guy?
I actually do.
I have some thing where other people can't stand it, like when I play podcasts for my
friends, but it's got some setting to skip spaces and to just go 1.5x, so it just sounds
really weird, but I got you so used to what it doesn't bother me.
That's fun.
I listen to a ton of audio books and podcasts, and I tend to listen to it one and a half
x, and you know, it's still the point where like if I hear the person slow down, they
sound all weird.
Yeah.
I'm like really weird and slow talking, and it's not the point I was expecting.
But yeah, there's actually like little jingles in front of different podcasts that when
I listen to it at 1x, it just sounds weird, like because it's all down pitched or whatever,
I don't know, it's weird.
So I guess this podcast has that too, right?
There's a little jingled.
Yeah, I'm not even going to try to repeat this.
Don't do it.
It's called robot race.
But yeah.
So you know the routine, right, how do you get started in machine learning and AI?
Yeah, so I'm a trained electrical engineer and right on, and so we're in Toronto right
now, and I went to University of Toronto, and when I graduated, I got hired as an associate
product manager at Google, and the associate product management program is the way that
they hire product managers directly out of school, and it's a rotational program.
So they put you on one team for about a year, and then you switch, and they put you on
another team for a year, and then you can switch again if you want.
And so my first role was on maps, and I worked on maps, data, kind of stuff, and then my
second role was on Google's ads auction team.
And I was the product manager for some of the teams that generate a lot of the machine learning
predictions for the ads auction.
And so the kinds of predictions you'd want to make are, you know, how relevant is this
ad to the query that somebody is searching for, or how likely do we think someone is to
click this ad, so just a tangential little product at Google.
It was super important.
It was really interesting because it was super important, and we would make changes that
would have gigantic financial impacts, but you know, it's a lot of really big numbers
and dollars that you work with, and you could be hundreds of millions of dollars change
you'd make.
But at the same time, you work with really small numbers where you are working on a project
that you think is going to improve something or some other metric by 0.01 percent, and
you'll be happy that you squeezed out another 0.1 percent, but really it translates to
a lot of money.
Wow.
It's like really interesting place to work, and there's some pretty unique things about
working on that team.
One, those machine learning models that we ran were, I think, had some of the strictest
requirements for stability in the industry, so, you know, we didn't want any downtime
in the ad system, so we had a whole bunch of infrastructure to support that.
Okay.
Also, super large scale, super real time.
We have to score a whole bunch of ads at query time and return to all these scores and
run a whole crazy auction really quickly, and it's just a complex problem to try to predict
if you're going to click on an ad, and so we tried to do a really good job of it, but
it's always like you can always do more.
So that, I learned a lot about machine learning on those teams, and probably set you up to
learn a ton about the importance of platforms for machine learning.
Yeah, we had our own infrastructure to support us in the machine learning space that helped
us train models and evaluate them in real time and all that kind of stuff.
So I think most importantly though, is I learned a lot about best practices on machine learning,
and I learned that at a time when machine learning was still pretty new for a lot of people,
and I think even today, we're at a point where best practices for machine learning aren't
as widely established as best practices for regular software development, like checking
your code and use Git and run tests before you submit stuff, and et cetera.
And that kind of stuff is just not as well established for machine learning.
So it's part of those best practices are kind of what I learned on those teams.
And so I've been there for a while, and then I joined, about two years ago, I joined
Uber as the product manager for what was pretty much the only machine learning team at the
time, and we began building a machine learning platform, which today is known as Michelangelo,
and we recently wrote a blog post and published it to explain what Michelangelo is all about.
Yeah, basically that's a system to allow internal people within Uber to build machine learning
systems, and deploy them, and monitor them, and maintain them at scale within Uber.
And so our customers are teams like the team who's trying to predict ETAs, like how long
it will take a car to get to you, or the Uber Eats team.
Can you tell them I think they need to work on that a little bit more?
I'm working on it, man, and we've got a whole roadmap to improve this stuff.
But I mean, that really touches on real kind of product implications of using machine
learning, which is like how do you communicate and design for uncertainty, because you know
machine learning, you're inherently, you're making a prediction, and often a lot of use
cases a system will ask for, give me your best prediction, give me one number back.
But often a system will kind of know like a distribution, like I think it's going to
be in this range, and so we try to, when we can, provide like a range of our confidence
intervals for how long we think it will take some process to happen, and I think we do
that well in Uber Eats, but the product requires a single number in ETAs, and it's hard
to, right?
It's funny how much of this ends up being just kind of UX UI, right?
Like if it said, you know, four to eight minutes, it would be, I think a lot more palatable
than four, and then eight the next time you look at it.
Yeah, right.
Yeah, it's also the kind of thing where we have to work with designers a lot to explain
to them the uncertainty that comes with these machine learning systems and help them
understand what level of confidence we actually have with these numbers, so that they can understand
what the user experience will be from, like once they get these values, once they get
the predictions back, but ETAs are inherently a really challenging thing.
Like there will be construction on the street, and then your model has to adapt really
quickly, and it's a tough engineering problem to solve.
Yeah, yeah.
If only users thought about how tough the engineering problems were, when they were waiting
for their Uber.
I think they shouldn't worry about it, and we should just make it a good experience,
and they don't have to even consider anything about implementation.
Should be magical, and that's kind of like what we're trying to do at Uber is make transportation
as reliable as running water, so you don't have to worry about anything, you just open
up the app and call it right, and the ride comes to you.
Right, right.
I want to talk about Michelangelo, I also want to talk about the presentation you did
here, because you spoke here at the George and Partners conference.
Do you talk about Michelangelo in your presentation?
I think I intended to mention it, plug it, but I don't think I actually got to it in the
presentation.
Okay.
I mean, I'm happy to chat about what that's about.
We'll put that on the stack, also HoraVod, you've just announced HoraVod, and that came
up in our last, the Toma Long Island meetup, someone mentioned that they saw the news,
and we discussed that for a little bit, so I'd love to hear kind of you riff on that for
a bit, but let's start with kind of the presentation, and we'll see where that takes us.
Yeah, okay.
So, I gave a 20, 30 minute talk at this conference today, and I tried to get across the
main idea that I tried to get across in this talk, is that to actually get enterprise
value out of using machine learning systems, a building of applying machine learning in
your company, there's a lot more to do, and a lot more to get right than just choosing
the right algorithm, and a lot of people focus on what's the right algorithm to use when
you see most of the news we see about machine learning is, so and so came up with an advancement
in AI that lets us beat this other thing, a new accuracy metric, and stuff like that,
and that's great, and it's totally like a research frontier that's super valuable to
have, but what practically I deal with, and a lot of the basic machine learning problems
that we deal with day to day, in most use cases in a company, are issues that can be adequately
handled with a very few algorithms, we can apply like grading boosted decision trees
or random forests to a large number of classification problems, and receive acceptable results.
And so I forget the stat, but some astounding number of Kaggle competitions are
basically just unsumbles of them, and so if you're trying to extract value practically
in your company and make systems that really use this stuff, the bottlenecks usually
are not there in which algorithm you're choosing, maybe if you're building self-driving
cars and stuff where you're really pushing the limits of what AI can do today, then
you're dealing with not a lot more, but we deal with a lot more challenges related
to infrastructure and data, and things like that that are closer to typical engineering
problems, that people usually have to put a little bit more effort to think about how
to do that correctly in the machine learning paradigm, so in my talk, I was talking about
a few different areas in which there are pitfalls where people typically don't, there could
be things that if you don't have experience, you don't think about this ahead of time
and you're not planning on dedicating time to solving these problems, or it could just
be tricky things to get right even if you know what you're doing, and so one area is
technically just infrastructure, solving the infrastructure that goes around your machine
learning system, so if you're in grad school and you're learning machine learning, you're
a data scientist, as that is, you get really good at training a Python model, or an R model
here, output you're deliverable from that often is a scikit-learn Python object, or maybe
something you can export to, like a PMML format, but that's just one part of the story.
Even if you have a great model, there's a lot more infrastructure around your machine
learning system that you need to integrate well, integrate with wealth.
For example, you need a logging system that is aware of the machine learning use case
that can store historical data in a way that's compatible with what your machine learning
system wants to do with that data, and the future one is training a model on it, or you
need a monitoring system to be able to accurately monitor and evaluate your model's accuracy
over time, so you can determine if your model that you're using in production becomes
stale, and you need to either retrain it, or if you have some other system that's automatically
retraining things, you need a whole bunch of infrastructure to manage that, automatically
train, evaluate, deploy a lot of that stuff, and so my point there is that there's a large
engineering investment to go alongside the basic work on the algorithms, that a lot
of people overlook, and when people think like machine learning is magic, it's really
not magic, you still got to put a lot of work into it, so that's kind of one area, is
that lead you to, like is there for a mature company, or a company who's doing machine learning
a scale, is there like a magic ratio of data scientists to engineers?
That's a good question, well, you know, I'm building Michelangelo, which is, we have
almost exclusively engineers building that, and our customers are a mix of data scientists
and engineers, so the teams that are using our system to actually build these models and
run them in production, it's usually like half and half engineers in data scientists,
and there's a blurry line between engineer and data scientists sometimes, and so there's
often a lot of engineers who just know enough about machine learning that they don't need
to consult data scientists and they can understand enough about these models to be confident
about them, and it doesn't frequently, I don't see it going the other way too much, there's
not often like a data scientist who's running like a production system, but I mean, that
can happen, that wouldn't surprise me if I saw that, so it seems to be that they'll
be paired, so you have data scientists working with kind of this new breed of engineer called
the machine learning engineer that has kind of that base level understanding of models.
Are you?
So what we have at Uber is kind of like the nucleus of a team is an engineer, the team
of engineers, usually one product manager, and a couple of data scientists, and being
an engineer is not just one skill set, and so there are engineers who are very data-focused
engineers or there's UI engineers and the whole spectrum of it, and so usually the folks
that partner most closely with data scientists are the more data-minded engineers, but they're
not necessarily folks with machine learning specialties, and even on my team on a Michelangelo
team, most of the work that you end up doing to run a production machine learning system
is data pipeline, work floor stuff, and so I would say probably even most of our engineers
are just like data engineers, infrastructure engineers, systems engineers, folks that have
picked up a lot of important machine learning concepts, but do not have like PhDs in machine
learning or something like that. And so the idea behind Michelangelo is that you can invest
in building this platform, and then the data scientists can focus on the machine learning
and not have to think about the logging and the monitoring and all of these, you know,
the life cycle of it deploying and managing a model of production.
Yeah, exactly. So, you know, about two years ago when we started Michelangelo, there were
a lot of teams who either were trying to put machine learning systems into production
and building their own production stack for it, and different teams were doing the similar
thing, but they were all these bespoke solutions that we're not, you know, if you're building
something for a specific use case, it's unlikely to be supported with a proper engineering
investment, and it's unlikely to generalize well when you want to expand your use case
and somewhat brittle and stuff like that. So there was a clear, a clear need for a platform
to help these teams put something into production and manage something in production, but, you
know, our goal is to support everything from the exploration side of the data science
workflow all the way to production, managing something, maintaining something in production
and a whole operational side of it. I would say that we, our particular focus is putting
things into production. It's a much harder problem to solve that helping the teams would
like the model exploration stuff. You know, data scientists have their own tools that
they like using, and it's hard to do much better than psychic learn and just letting
a data scientist just iterate really rapidly with the tools that they're used to, but when
it comes time for them to like begin using that system in production, the, and usually
need to rely on an engineering solution, and that's where our platform comes in.
Okay. And so you mentioned logging, you mentioned monitoring, are there other kind of main
features of the platform? Well, I was suggesting that there's, there's actually like all kinds
of infrastructure, whether it's a machine learning platform or not, that you want to have
aware of the machine learning use case. And there's also types of infrastructure that you
would, that is machine learning specific that you might need to build. So there's a thing
that we built, which is a feature store. And so that allows teams to share and discover
features. So if you are building the model that predicts how long a restaurant's going
to take to prepare meal for Uber Eats, maybe the model that I'm building would be able
to benefit from some of the features that you're using in your model. And so it took
way for us to share features. So I don't have to duplicate all the data pipelines to create
those features. And is that, does something like that exist as like a kind of a metadata
catalog that is, you know, mirrors a data store like a HDFS or something like that?
Yeah. So for us, practically, we have, you can do a few things, but basically we manage
metadata, metadata layer to understand where these features are stored. And it is all
in like high HDFS and other people can contribute to it. Or we have this other way where we
manage some core set of features. And, and yeah, we're still figuring out like how we can
make that most usable to other teams. Right. And really nail the, the sharing of this
data use case because there's, it's, it's, I mean, like it's super tricky because if
you're building a production system, you are going to be hesitant to rely on a data
set that you don't own. Right. And you don't know if that person is going to change that
how it's calculated the next day and it messes up your system. So we're trying to like
figure out the right contracts are to guarantee certain data stability and quality and stuff
like that. Like how far along are you on? We have what it even means to have a contract.
Like it's, it's hard enough for services, which have a fairly, you know, small surface
area. Right. Of the, the API, but the, it strikes me that data has a much more kind
of expensive surface area around what you need to try to define a contract. Yeah. I,
like, I, I haven't spent enough time on this area to, to have really figured out where,
where we actually want it to be. So I can't even say if we're 20% there or 50% there,
you know, like, because I really don't know how much you want to do in this space. So
that's, that's tricky. I think it's an area that we probably want to prioritize more.
Yeah. And like another thing that comes along with that is, you know, that's HDFS hive.
That's all offline stuff that's ready for batch processing. Right. But there's a whole
other side to this where if you're running a online prediction model, you want these features
to be generated in real time. And so you might have some real time stream processing
stuff that will calculate those same features. And you want them to be calculated in the
same way as you calculated your offline batch data that you trained your model on. But
you're calculating these features in real time and then making them available to your
model so your model can score. Right. You know, and this could be like understanding how
many, a good example is how many meals has this restaurant? How many orders does this restaurant
gotten in the past 30 minutes? And that might be like a window figure out how you're going
to accumulate. And it's like you, you've got your, this repository that says what your
features are. And then you've got your, you know, you're kind of your meta meta that's
like how you derive the features from the underlying stream of data. Yeah. It's, it's a, it's
and so you can imagine there's a large activation energy to set up a system like that. And so
that's part of the value of having a platform that builds all this infrastructure and plugs
it all together nicely. So you just need to provide a configuration and say, hey, use this
data and give me this real time feature. Right. And that, you know, we found that that
kind of infrastructure has really lowered the activation energy. So we've allowed like
those teams were building these spoke solutions. We built a good system for them and they've
kind of pivoted onto our system and begun using our system. But also there's teams that
before didn't have the resources to even take on a machine learning project, but they've
known that they want to have some kind of predictive solution in their product. And, and the
lower activation energy to get started has kind of helped them unlock that and they've been
able to fund a product because the, or such a project because the cost has come down
so much. Nice. Nice. Now, so we've talked primarily around what I kind of roughly think
of as engineering facing features and infrastructure. But there's a whole set of operational I, you
know, operationalizing features. You may remember me getting into part of this conversation
in an interview with Jennifer, Jennifer Prinky, who is at Walmart Labs at the time, not
too long ago. And kind of the direction that they were heading. And I could see Michelangelo
going in this direction. If it's not already is, you know, you've got a model in production.
Like, are you tracking kind of either statistical drift of the inputs or, you know, model accuracy,
you know, decreasing over time. And then like automatically triggering or at least like
setting a ticket or something like that to reevaluate the model. So the kind of the
full like service level impact of these models. Do you get into that at all? So this is,
we do get kind of into it. And this is an area that we're really trying to focus on over
the next couple of months is build out a lot of, there's a lot of things you would want
to do to operationalize and maintain such a model. So things like tracking the data quality
both in and out of your model. So you might want to do things like track feature distributions
over time and real time. So all the data is coming into your model. Does it look different
in this five minutes than it looked in right past? And you might compare that to the distribution
of data that you saw an hour ago, but also like the historical distribution of like the past
two years of data like that. And right. Has that changed? I mean, you just start
in a rough way. I go ahead. When we were talking about, you know, data and contracts
around data and models like one of my thoughts was, you know, is part of that contract like
a statistical distribution of the data at a point in time so that you can refer back
to, you know, what you thought that data looked like when you built the model. Right. So
you can, so that's not something that we would necessarily want the customer to explicitly
provide, but it would be something that we may be able to extract from the training data.
So we might be able to say like, look, this is the distribution of the data we saw at
training time. Right. It's kind of save a snapshot of the summer summary statistics and then
compare our normal like the data we see in real time to the statistics. And so sometimes
like, you know, your training data, you might rebalance classes and reweight things and stuff
like that. And so there's some complication to take that stuff into account. And so this
is some of the challenges that we're working through right now. Okay. But then there's also
the other side of like doing the same data quality checks on the output of your model.
Like the predictions and, you know, doing the same statistics tracking stuff. And you would
also want to have alerting happen on that kind of thing. And that's all part of this like
larger infrastructure integration story because your company should have a way to, you
know, like set up alerts for production systems. And so you just want to integrate with the
basic stuff that you have. Cool. So we didn't get very far in your presentation. So I was
talking a lot about like infrastructure side, which is the summary is you got to do a lot
of engineering work before you get into to put one of these models into production properly.
There's also another part of it, which is, you know, building tools to help the data
scientists do their job properly, both correctly and productively. And so, you know, there's
many parts of the data science workflow. And, you know, if you leave data scientists,
if you don't support them with the proper tools, then they may be building systems in a
non reproducible way. So there's not a way to like recreate the model. If you need to
retrain it, it may not be all version and like checked in and source control and stuff
like that. And you may not even have standardized ways to evaluate these models. So you may just
waste a lot of cycles comparing data scientists as evaluations to data scientists, bees evaluations.
Because there's not a common set of metrics that everybody's agreed on ahead of time. So
there's some work to provide some infrastructure for them as well. And so the future store is
one example. It gives a kind of like common source of truth. But beyond the technical stuff,
there's a lot of organizational considerations when you're trying to make the most out of
machine learning in your company. So, you know, one thing that's important to understand is that
not everyone's an expert at machine learning. And people come with different levels of
abilities and background knowledge of machine learning. So that leads to a lot of times when
people have false conclusions about how machine learning can be applied to their problem. So
some people will tell me, well, I don't think machine learning can help with my problem at all.
So we don't need to like have a collaboration. And then when we look into it, we realize, oh,
well, actually the main problem you're trying to solve is pretty appropriate for a machine learning
solution. Or there's folks who have told me have kind of conveyed that they believe machine learning
is basically magic. And they think it can solve all of their problems. And that's a kind of thing
where you really want to understand those expectations and adjust those expectations to be
much more realistic. So you don't have a problem down the line of completely missed expectations.
But then there's also people who are almost like overconfident about what they know about
machine learning and they request specific algorithms or specific implementation. And the
example I get in the talk with someone once asked me mentioned that they need unsupervised
online, deep reinforcement warning or something like that. And I was like, okay,
it's like, I don't know if that's a thing, but we should understand where your problem is. So
so like the way to handle most of these situations is to not really focus on the machine learning
part of it, but to really just like talk to these people and understand what the business
problem they're trying to solve is and not involve machine learning vocabulary. And you know,
I'm the product manager of our team. So part of my role is to understand the business problem
and find a way to translate it into a machine learning problem, something that we would be able
to solve and understand also the need to be able to solve or something that you'd be able to support
with both. So we have we also have an applied machine learning team that we that we
is like a tiger team that supports the different engineering groups. Yeah, it's kind of like a
set of machine learning specific data scientists that these folks may be able to like put on loan
to a team to help them solve a machine learning problem if they don't have the resources when it's
a particularly relevant and important machine learning problem. So yeah, so like I hope kind of
coordinate these these types of problems. And yeah, so we figure out like if it's a relevant
problem and how we might be able to solve it. And ultimately I'm trying to understand the
the business metrics that these people are trying to optimize for and understand the mechanisms
that the whole product works. So what do they think actually affects those business metrics?
And then I'm trying to translate that problem into for for our data scientists or for even for
their data scientists who might need help with it. Like a machine learning problem that
where if a data scientist is optimizing for precision recall or all the other kind of data science
metrics that they would be used to that are related to machine learning as they optimize for those
those would likely be an appropriate proxy for optimizing for these end business metrics. And so
it doesn't always work out perfectly. But it's a good way to like start things off. And ultimately
you want to have people who understand the whole problem end to end to really like think things
through think things through. But it's a good way to get started in that kind of area. So like talking
to people in the right at the right level of like vocabulary, you know focusing on the business
problems, expectations making sure they don't think that machine learning's magic setting that
properly. Another thing that I found useful is going out of my way to find a senior person in
the company who really understands machine learning or like taking the time out to educate them
about machine learning. So I can always have them on my side if I really need to ever need to defer
to someone more senior. I know that there's someone there who will will either be sufficiently
technically competent to be able to make the right call or that I've spent enough time with so
that they'll trust me to make the call when it comes to that. So that I found that to be useful.
And then another thing that is also useful for figuring out how to make the most out of machine
learning is realizing that you're not going to have a machine learning, you're not going to have
infinite machine learning experts. And so you have to find a way to make the most out of the limited
number of machine learning experts you have. And what's the answer to that challenge? It's
like how do you set up your org, right? It's like how do you how do you distribute your machine
learning folks across the different teams? And there's different ways you can do it. So I mentioned
that we have this applied group which is kind of like a solutions team in some sense where they
they're a team of a few data scientists who are experts at machine learning and they kind of
are out on loan to different teams that need help that don't have the machine learning expert
keys. And so you can imagine there's these different engineering teams and then there's one
kind of cluster of machine learning people on a separate team that loosely interacted those teams.
But another model is you could just embed one machine learning person or one or two on each
of those teams. And so that model is slightly more difficult to scale because you have a new
team and then you need another machine learning person. And so I think we've experimented with
different models. Different models will work well for different size companies. Often it's not
just a binary thing but there could be like a hybrid where you might lean more to one side or another
and in practice we might have things where our applied team might start off a project but then
hand off a project to a data scientist on a product team at some point. So like thinking those
things through explicitly ahead of time and being realistic about how many resources related to
machine learning that you have that's pretty valuable. And so that kind of covers the organizational
things you want to think about when you're launching machine learning systems. And then
finally just like team focused things which are like how do you when you're running a machine
learning project. What can you do to like organize your machine learning project to be set up for
success. And there's a few things that are kind of just like questions you want to ask yourself
and like rules of thumb you want to apply. So one is about like not artificially encapsulating
the machine learning specific part separately from the non machine learning specific part of your
project. Like you don't want the person who's building the model who's generating these scores
to not know anything about how these scores are being used. And so there's a lot of
like there's a lot of ways that these scores can be misused and a lot of like nuance and how
these scores are generated. And we talked about the the inverse of that which is the designers
and the app teams not really understanding the scores and the probabilistic nature of the scores.
Yeah. So that's that's part of like how you would design a project to to a product rather to
take make actually relevant use of the machine learning scores that you're providing. And then
there's other examples of this which are more like you know your scores can be used in a way
that they they were not intended to be used. And when they can even be used in a way that can
harm your machine learning model. If you have a certain feedback loop where you know if you're
trying to predict churn for example and then someone uses these scores to to provide some
treatment to the people who are most likely to churn. And then they solve the churn problem for
the 10% most likely to churn people. Then those people didn't churn. And then when you go to
retrain your model next month or two months from now or whenever you'll not have those labels
that those people churned. And so your training data is going to be kind of messed up. So that's not
a it's not that's not like a particularly hard problem. But you just have to if you don't think
about that ahead of time if you're not thinking this problem through end to end and you're not aware
of how these scores are being used. Then you may not include that in your design of the whole system.
And so your whole project might be flawed from the beginning. And there's a solution there to
flush your your labels when you make changes like that. There's other things you could do. One could
be just have a holdout set. So you just don't you never apply treatments to some x% of the people
that you're making predictions for. So then you can always reserve them as like the control,
the untouched people that you can train your model on in the future. Or you can even like a more
advanced way is find a way to include that treatment in your modeling in the future. So your model
is aware that maybe these people didn't turn. But it was because they got this treatment. And so
this kind of that has more of a feedback loop. And this is more complicated. You need to think
about it more deeply. But overall like I'm on the team side, it's you really want your folks that
who have who really have an attitude of ownership of these systems to be the most involved here.
You want them to be kind of almost like borderline paranoid about the the operation of these systems
and asking questions like if the world changes, is my model going to be able to react to it?
And how will my model react to it? Or like if how a feature is computed by the logging people
or if it's how it's recorded changes slightly like the user different format to the store int now
instead of a double. Is that going to mess up my number or my predictions? And how can I prevent that
to happen from happening? Or like how will I notice if something's wrong or any number of things
or even even like on these different dimensions like legal and cultural things like based on how
these predictions are being used. Is there any bias that's inherent in this model? And will that have
some impact in how the model is perceived like legally or culturally? And that's always a tricky
problem. And often not just one person can think through all of these problems and you want
like a kind of like a good citizenship attitude because no one's particularly
singularly responsible for these issues. We just want people to really think things through
and really own the system. So they are always feeling like they're doing the right thing here as well.
Okay. So we covered I think the topics in your presentation. I wanted to make sure we hit
on Horavod before we move on. What's that all about? So we have a lot of data over and we've been
trying to get started running distributed TensorFlow. And we ran into a lot of issues. I'm just setting
up the parameter server constellation and connecting everything together in the proper way. And
we've got an engineer, Alex Sergeev, who's awesome. He's a genius. And he found a way to kind of
improve upon the Bidews, a paper that Bidew published where they open source some code to
do a different way to distribute the work in TensorFlow. And basically ripping out the guts of
the distributed stuff and replacing it with open MPI. Exactly. Hearkens back to the great computing
days. Yeah. So it uses like the ring all reduced methods. And you have, you know, you have, if you
have n nodes, each node is only sending data to one other node and receiving from one other node.
And there's no parameter servers. Only all the workers are communicating with other nodes and
are averaging up gradients in that node itself. And so you kind of set up this ring of connections
between like a circle, like an online them all in a circle. And so then if you pass one message
through it will, and you keep iterating your message passing eventually, one signal will get
all the way around and be distributed to all the nodes. It's much easier to understand visually.
We run into that problem. But Alex built this awesome system that is called Horavod. Horavod is
a Russian dance that people link arms in the circle. And so it's kind of like the right imagery
there. And so he's been applying it to a lot of our distributed learning problems. And he's found
that it's much easier to set up a distributed learning job in TensorFlow. So, you know, there's a
lot of boilerplate code that you need to have in a existing parameter server TensorFlow paradigm
to like allow it to all be distributed. And he's figured out a way to have only four lines of code
that you need to add to a TensorFlow script to distribute it. And I've heard not great things about
trying to distribute out of the box TensorFlow. It's pretty challenging. I don't know exactly what
Google does internally. But I think they're probably working on an improvement.
Lewis, from what I've heard, they're trying to, or at least, you know, someone is thinking that at
some point, you know, they've got this Kubernetes thing. It's kind of good at distributed compute.
They've got this TensorFlow thing. Hey, you know, somehow we can kind of get the chocolate and
then the peanut butter together. Yeah, yeah. So I mean, I'm sure there's a whole team working on
that in Google. And so this was just like Alex on our team needed to solve a problem for himself
immediately. And so he built this system. And it's really easy. If you check out the blog posts we
have to search Horavod. And this blog post is open source. And it's open source. You can just
go download it and check out the sample code. And it's literally just add four lines to your TensorFlow
script. And you're good to go. And here you describe it. It goes higher up and ripping out the
guts than I even thought. Like I thought it was just like using kind of MPIs like a low,
low level message passing thing. But it's also kind of changing the way gradients are distributed
across the system. Yeah, it touches various layers there. So Alex has been able to use it too.
Let me see. I think there was some model that took maybe like 10 days to train. And on our cluster
with I don't remember the details. But in some real practical application, he was able to train
this model in seven hours instead. Instead of how many nodes. I don't know how many he used in
this particular thing. But we've trained stuff up to maybe 128 GPUs. That's right. Another that
I mentioned that I remember the the graph that was in the blog post. And you get pretty darn
close to ideal multipliers on the scale. It was pretty impressive. Yeah, it's pretty, I mean,
it's a very large speed up for really big applications. And you know, you can imagine if you take
10 days to train your model and suddenly you can train a model in seven hours, it changes
your whole workflow. It changes like what your job is. Right. And so that team's productivity has
been changed dramatically. And there's not that many use cases that absolutely need like so much
data. So we're still trying to figure out if there's ways that it can be useful to speed up like
smaller work a smaller use cases as well. But it's always something good to have because you know,
people aren't going to begin training on less data. People are always adding more data. And so
that's what's happening to our use cases. And is this an example of something that
would get integrated into the Michelangelo platform or yeah, this is part of the Michelangelo
platform. Yeah. And so I wasn't mentioned in our Michelangelo blog post, but we're working on
kind of like a separate development, X model exploration, deep learning specific IDE kind of
or IDs the wrong word. Like a framework or yeah, kind of like a framework for people to
easily run containers and tends to flow and iterate on TensorFlow models and get a bunch of
machines that they can like GPU machines so they can run these big distributed jobs.
Okay. And then have ultimately have those models appear at Michelangelo so they can push them
to production and evaluate stuff like that. And that's so that's how Horavod fits into Michelangelo.
And I think we're just going to see a tighter and tighter integration in the future.
So now Horavod is open source. Michelangelo is not. Michelangelo is not. I'm still trying to
prioritize that in our roadmap. It's a lot of work to open source something. So don't blame me.
A lot of people have gotten a lot of emails like, hey, so I couldn't find Michelangelo and GitHub.
And I was like, oh, nice medium post. Sorry guys. But yeah, I hope to be able to open source
at sometime. It's just not something we're working on right now. Well, it's I think
as people deploy, you know, as people kind of productionalize machine learning more and more,
like they run into this. They have to run into this. And at every place like, you know, the problem
that you're trying to solve with Michelangelo, preventing data scientists and engineers at Uber
from building this over and over again, we're doing this like on the scale of the industry.
And, you know, there are like a handful of proprietary, you know, kind of all-in-one platforms that
kind of solve some of it. And I forget the name of the company, but I recently came across a
company that is, you know, their focus is trying to solve the kind of model lifecycle and production
thing. But, you know, I think ultimately, you know, it's infrastructure, right? And infrastructure
wants to be open source. Yeah, right. So, you got to shut open source it. I think it would be
cool if we did. What's up with that? I'll bring that to feedback back. I'm arguing for it too.
So, yeah, I think it would be cool if we did. And I would definitely love to see like an industry-wide
adoption of Michelangelo. I feel like we have a lot of high priority stuff. We got to add to it
internally. But there's definitely, I mean, there's a lot of benefits to open sourcing stuff.
And I mean, that's why I think that's why Google open sourced TensorFlow from the beginning.
I mean, they worked on it a lot internally, but then when it was good enough, they open sourced it.
And as far as I understand is because they didn't want to have the same thing happen to them as
what happened with MapReduce, where they didn't open sourced it at first. And then it took,
and then there was a big deviation from the open sourced stuff. Then what happened when they
made someone else open sourced it. Yeah, and they were kind of left out. Yeah. So, awesome.
Yeah, there's a lot of benefits. We'd love to do it sometime.
Right. Oh, Mike, I really enjoyed the conversation. Thanks so much for taking the time to sit down
with us and share a little bit about what you and over up to in the realm of ML platforms.
Awesome. It was a pleasure being here. Thanks a lot.
Right. Thanks.
All right, everyone. That's our show for today. For more information on Mike or any of the topics
covered in this episode, head on over to twimmalai.com slash talk slash 115.
Definitely remember to vote on your favorite MyAI video at twimmalai.com slash MyAI.
And, of course, thanks so much for listening and catch you next time.
