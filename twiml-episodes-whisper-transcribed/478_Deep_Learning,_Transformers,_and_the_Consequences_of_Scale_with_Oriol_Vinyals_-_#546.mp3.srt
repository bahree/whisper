1
00:00:00,000 --> 00:00:12,880
All right, everyone. I am here with Ariel Vinyals.

2
00:00:12,880 --> 00:00:17,520
Ariel is the lead of the deep learning team at DeepMind.

3
00:00:17,520 --> 00:00:20,240
Ariel, welcome to the Twomo AI podcast.

4
00:00:20,240 --> 00:00:23,120
Hey, Sam. It's great to be here. I'm a big fan of the show.

5
00:00:23,120 --> 00:00:26,720
Thanks so much. I'm really looking forward to chatting with you.

6
00:00:26,720 --> 00:00:32,480
This conversation is long overdue. I'd love to get started

7
00:00:32,480 --> 00:00:35,120
by having you share a little bit about your background

8
00:00:35,120 --> 00:00:37,920
and introduce yourself to our audience.

9
00:00:37,920 --> 00:00:42,560
Yeah, absolutely. I mean, this could go a long time,

10
00:00:42,560 --> 00:00:44,960
so I'll try to keep it maybe at a maybe more

11
00:00:44,960 --> 00:00:48,240
recency bias, I guess. But yeah, I've been in the field of

12
00:00:48,240 --> 00:00:51,680
machine learning deep learning since it was quite not as

13
00:00:51,680 --> 00:00:55,600
popular as it is today. So it's been definitely a fun journey.

14
00:00:55,600 --> 00:00:59,120
I'm happy to maybe link this back at the end of our conversation

15
00:00:59,120 --> 00:01:04,000
a little bit. But you know, in the nutshell, I've been

16
00:01:04,000 --> 00:01:07,920
maybe the main passion I've had always is with sequence modeling.

17
00:01:07,920 --> 00:01:11,360
So I started in speech recognition where a lot of deep learning

18
00:01:11,360 --> 00:01:16,000
early days things were going on. And then I transition from

19
00:01:16,000 --> 00:01:20,400
doing my PhD in California in Berkeley to joining Google

20
00:01:20,400 --> 00:01:24,320
Brain in very early days, worked a lot on actually natural

21
00:01:24,320 --> 00:01:29,200
language processing. We had maybe one of the works that

22
00:01:29,200 --> 00:01:32,400
you know, links forward to many of the research that I'm doing

23
00:01:32,400 --> 00:01:35,120
these days is the sequence to sequence work on machine

24
00:01:35,120 --> 00:01:39,600
translation that we did back in the day. And then you know,

25
00:01:39,600 --> 00:01:43,120
my passion for sequences kind of was developing.

26
00:01:43,120 --> 00:01:46,800
At some point it was a good time to actually go back to Europe. I'm

27
00:01:46,800 --> 00:01:51,600
originally from Spain. So I moved to London in 2016 where I joined

28
00:01:51,600 --> 00:01:55,200
deep mind. And since then, you know, I learned a lot actually

29
00:01:55,200 --> 00:01:58,480
about the reinforcement learning. One of the, you know, most fun projects

30
00:01:58,480 --> 00:02:01,680
I've had, you know, a pleasure to work with a large team of people is the

31
00:02:01,680 --> 00:02:05,200
Alpha Star project, you know, involving creating an agent to play

32
00:02:05,200 --> 00:02:09,200
Starcraft. But actually in that creation, a lot of the sequence

33
00:02:09,200 --> 00:02:13,840
modeling background came back as we use a lot of, you know, LSTM's

34
00:02:13,840 --> 00:02:16,560
transformers, all these kind of models that are very popular

35
00:02:16,560 --> 00:02:21,920
thanks to their performance in NLP. And these days, I'm mostly just

36
00:02:21,920 --> 00:02:27,120
focusing on, you know, leading the deep learning team and trying to

37
00:02:27,120 --> 00:02:31,360
just do the usual things that we like to do in deep learning, which is

38
00:02:31,360 --> 00:02:39,440
try to unlock, you know, state of the art or new horizons and benchmarks

39
00:02:39,440 --> 00:02:42,640
in many modalities and as many modalities as we can.

40
00:02:42,640 --> 00:02:48,480
As deep learning has matured, you know, folks often get a lot more

41
00:02:48,480 --> 00:02:53,040
specialized, you know, leading the scope of deep learning,

42
00:02:53,040 --> 00:02:58,720
writ large seems like a very big scope.

43
00:02:58,720 --> 00:03:03,520
Yes, indeed. I mean, it was fun because the way the way it all started, right,

44
00:03:03,520 --> 00:03:08,400
it was literally a lot of like proving yourself in very specific fields,

45
00:03:08,400 --> 00:03:13,920
even very specific applications in those fields. So the very first

46
00:03:13,920 --> 00:03:18,160
results that caught people's attention were actually on speech

47
00:03:18,160 --> 00:03:22,080
recognition. So that started actually mostly in, you know,

48
00:03:22,080 --> 00:03:25,920
thanks to the Toronto group led by, of course, Jeff Hinton.

49
00:03:25,920 --> 00:03:29,760
I got to learn that through my internships at Microsoft Research,

50
00:03:29,760 --> 00:03:33,920
where a lot of action was happening. And then, you know, maybe the most

51
00:03:33,920 --> 00:03:37,440
obviously notable moment that most people would refer to is the image

52
00:03:37,440 --> 00:03:41,040
net moment that really happened almost three, four days, you know,

53
00:03:41,040 --> 00:03:46,000
days, no years, these days, time passes very differently.

54
00:03:46,000 --> 00:03:50,080
But yeah, three or four years after that breakthrough,

55
00:03:50,080 --> 00:03:54,640
the image net moment occurred. And then from there on, it's been an adventure

56
00:03:54,640 --> 00:03:59,040
of basically trying to find where are the frontiers

57
00:03:59,040 --> 00:04:03,680
of what deep learning models can or cannot do. And, you know,

58
00:04:03,680 --> 00:04:07,680
through the years, it turns out that by applying more or less the same

59
00:04:07,680 --> 00:04:13,120
methodology, you know, through gradient descent, new

60
00:04:13,120 --> 00:04:17,440
architectures, but refining ideas, adding the right kind of

61
00:04:17,440 --> 00:04:21,840
inductive biases to our models, et cetera, you end up

62
00:04:21,840 --> 00:04:26,080
then transferring to more and more areas that involve, of course, natural

63
00:04:26,080 --> 00:04:31,120
language, machine translation, then now all sorts of like

64
00:04:31,120 --> 00:04:34,960
language modeling, multimodal language, vision,

65
00:04:34,960 --> 00:04:39,520
generative models. One of the recent successes, of course, of the year was

66
00:04:39,520 --> 00:04:43,040
alpha fold, in which essentially all these tools just were applied to

67
00:04:43,040 --> 00:04:47,520
these very different domains. So expanding to more and more domains

68
00:04:47,520 --> 00:04:51,920
of science in general has been now, you know, it's the day to day and the way

69
00:04:51,920 --> 00:04:55,600
to think, perhaps when you're, you know, thinking more deep learning

70
00:04:55,600 --> 00:04:59,360
in 2021 and beyond. I'd love to have you elaborate a little bit on that

71
00:04:59,360 --> 00:05:06,880
when I kind of think about the field, I tend to think about it

72
00:05:06,880 --> 00:05:12,240
in terms of, you know, there's this, you know, set of work that's been like

73
00:05:12,240 --> 00:05:18,000
applying deep learning to new application areas. There has been, you know,

74
00:05:18,000 --> 00:05:23,920
applying a deep learning approach to, you know,

75
00:05:23,920 --> 00:05:28,960
kind of technical field. That's not a great way to put it, but like,

76
00:05:28,960 --> 00:05:32,720
okay, we've got graph machine learning. Like, how do we do that in deep learning?

77
00:05:32,720 --> 00:05:35,520
You know, we've got reinforcement learning. How do we do that with deep learning?

78
00:05:35,520 --> 00:05:40,000
That kind of thing. And then there's been, you know, a lot of energy, just how

79
00:05:40,000 --> 00:05:47,280
do we make deep learning more computationally efficient and, you know,

80
00:05:47,280 --> 00:05:52,880
make it easier to train that kind of thing? Are you working across all of those

81
00:05:52,880 --> 00:05:57,120
areas or do you even, you know, think about it similarly? How do you kind of,

82
00:05:57,120 --> 00:06:02,720
you know, think about that taxonomy? Yeah, I mean, it's one thing that I tend to,

83
00:06:02,720 --> 00:06:07,120
I found it useful throughout my career. And this, I think, the level of

84
00:06:07,120 --> 00:06:14,240
generality has increased over the years. But the, what you try to do mostly if you're

85
00:06:14,240 --> 00:06:19,680
a deep learning researcher, having been in the field for abilities to try to

86
00:06:19,680 --> 00:06:26,320
just identify commonalities across, you know, modalities or problem settings.

87
00:06:26,320 --> 00:06:32,160
And in a way, I have this sort of argument. And I've given more, obviously,

88
00:06:32,160 --> 00:06:37,200
more detailed technical talks at, you know, like recently about what I call the

89
00:06:37,200 --> 00:06:42,560
deep learning toolbox, right? And I think that is a reasonably, you know,

90
00:06:42,560 --> 00:06:47,440
you can see quite a few examples on one of, you know, some of the major successes

91
00:06:47,440 --> 00:06:52,240
recently on applying these toolbox approach. And by a toolbox, I mean,

92
00:06:52,240 --> 00:06:57,120
we have architectures, um, tricks of the trade on how to train the models like,

93
00:06:57,120 --> 00:07:00,960
you know, like optimization methods, how to use the hardware more efficiently.

94
00:07:00,960 --> 00:07:06,160
Like this is all part of this gigantic toolbox. And then when you're faced with a new problem,

95
00:07:06,160 --> 00:07:10,160
right? If you're truly like embracing sort of the deep learning approach,

96
00:07:10,720 --> 00:07:15,920
you're applying almost always the same first principle, which is you learn everything.

97
00:07:16,960 --> 00:07:22,000
Everything is learned and to end from the output back to the input by training a set of

98
00:07:22,000 --> 00:07:28,240
learnable weights through gradient descent in general. And then you just pick a mix and match

99
00:07:28,240 --> 00:07:33,920
from the toolbox. The precise elements beat, oh, I have a sequence. Okay, I'm going to use a

100
00:07:33,920 --> 00:07:39,520
transformer. It's a very long sequence. Maybe I'll use an LSTM. I have vision. I use

101
00:07:39,520 --> 00:07:43,680
compolutions, et cetera, et cetera. So you're mixing and matching all these components.

102
00:07:43,680 --> 00:07:48,640
And then when faced with a new problem, like folding proteins, which obviously has some

103
00:07:48,640 --> 00:07:52,720
similarities, but in the end is like at the input, you have a sequence of letters, right?

104
00:07:52,720 --> 00:07:56,560
The amino acids that form the protein. And then the output are like these

105
00:07:56,560 --> 00:08:01,840
sequence and ordered set almost of 3D coordinates, right? So when you face with this problem,

106
00:08:01,840 --> 00:08:07,360
you say, okay, I have some data that maps these inputs to these outputs that people in the

107
00:08:07,360 --> 00:08:11,280
community has built over the years, very expensive to do because you need

108
00:08:11,280 --> 00:08:16,160
crystallography and some methods to generate this training data. But then with the deep learning

109
00:08:16,160 --> 00:08:21,680
mindset, you just go at the problem and then iterate over ideally a very nice, you know,

110
00:08:21,680 --> 00:08:28,240
training, validation split, as usually is done in machine learning. Then you start playing with

111
00:08:28,240 --> 00:08:34,560
details that matter a lot actually to unlock performance to attack this particular new modality,

112
00:08:34,560 --> 00:08:40,400
right? So I think it's the right framing to think about what is the next modality? What is a

113
00:08:40,400 --> 00:08:45,200
challenge? Is it a very large graph that we currently are at the, you know, we cannot quite do,

114
00:08:45,200 --> 00:08:50,320
because yeah, we have graph neural nets, but do they scale properly and so on? And that is

115
00:08:50,320 --> 00:08:56,640
generally how then you see a lot of papers at Newribs and other conferences kind of tackle

116
00:08:56,640 --> 00:09:01,760
these new challenges. And what's beautiful about this is that even though there is a theory of

117
00:09:01,760 --> 00:09:07,280
deep learning, that's a field I've gotten a bit into like just because it's just fascinating.

118
00:09:07,280 --> 00:09:12,960
But in general, how the field has advanced is there is a problem out there. You take it, you cannot

119
00:09:12,960 --> 00:09:18,320
change it, but you apply this first principle of end-to-end learning, learnable model, powerful

120
00:09:18,320 --> 00:09:25,280
model, and then hopefully you enable something that that field was not, you know, maybe looking

121
00:09:25,280 --> 00:09:29,920
at at the time. Although nowadays many people have heard about deep learning and it's quite

122
00:09:29,920 --> 00:09:34,720
permitting everywhere. It's mostly you don't have to prove yourself in the maybe it's the same way

123
00:09:34,720 --> 00:09:41,360
that we had to by going one field at a time almost as we were doing in 2008, 2009, up to maybe

124
00:09:41,360 --> 00:09:48,800
2014-15, where things really start taking over and people start paying attention, given the successes

125
00:09:48,800 --> 00:09:56,000
where too many may be to ignore. Two, three years ago, a lot of work was happening

126
00:09:56,640 --> 00:10:07,040
in the area of just getting the basic machinery, working, tweaking the optimizer,

127
00:10:07,040 --> 00:10:14,720
tweaking learning rates, all this kind of stuff. Are you still involved in that kind of work?

128
00:10:14,720 --> 00:10:18,720
Do you think that's accelerating as more people are coming in or slowing down as we've gotten the

129
00:10:18,720 --> 00:10:25,200
basic machinery working? Yeah, it's a good question. I mean, going to kind of all the modalities,

130
00:10:25,200 --> 00:10:32,560
right? The ultimate modality if you extrapolate is, well, all the data is just a sequence of

131
00:10:32,560 --> 00:10:38,160
bytes, right? You can always represent any data structure, input or output as a sequence of

132
00:10:38,160 --> 00:10:44,160
bytes. And that is a very kind of romantic, almost way to think about machine learning,

133
00:10:44,160 --> 00:10:49,360
at least supervised learning problem. Kind of a grand unified theory of deep learning or something?

134
00:10:49,920 --> 00:10:56,880
Yeah, yeah, exactly. So in that sense, what has happened is that sequence models have evolved

135
00:10:56,880 --> 00:11:03,840
enough that we have transformers, which might not be the last iteration over the ultimate model,

136
00:11:03,840 --> 00:11:09,760
right? That will rule all the modalities with the same sort of applying the same model,

137
00:11:09,760 --> 00:11:15,920
the same formula, right? The toolbox maybe just reduces to this one model. We're not quite there yet,

138
00:11:15,920 --> 00:11:23,440
but that is one way to see it. And indeed, as the field is advancing, I think the details

139
00:11:23,440 --> 00:11:29,600
are refined enough that more people can just access these toolbox without being maybe

140
00:11:29,600 --> 00:11:36,800
necessary an expert and see successes reasonably in a reasonable, simple way. Obviously, this goes

141
00:11:36,800 --> 00:11:42,880
hand in hand with the fact that our software has also tremendously advanced with all the frameworks

142
00:11:42,880 --> 00:11:49,040
that exist currently and open source, and et cetera, that exist to kind of lower the barrier

143
00:11:49,040 --> 00:11:55,520
of entry to the field. But I would say that, yes, it's easier to get the details right

144
00:11:56,400 --> 00:12:02,000
more so than it was three years ago. Although, I mean, I think there's still quite a lot of research

145
00:12:02,000 --> 00:12:09,200
to be done indeed. Just kind of hinging off of the, you know, this point that you made around,

146
00:12:09,200 --> 00:12:19,440
you know, sequences and transformers and how we're, you know, almost there. Do you have a feeling

147
00:12:19,440 --> 00:12:25,520
or a bet on kind of what, you know, do we get there? What, you know, is transformers the thing

148
00:12:25,520 --> 00:12:30,880
that gets us there? Is it something that's a slight evolution of transformers? You know, what

149
00:12:31,600 --> 00:12:37,200
what dimensions do you think get us there if you think we get there? Yeah, I think transformers

150
00:12:37,200 --> 00:12:43,840
have evolved, you know, in a very natural, cool way from obviously what has come before transformers,

151
00:12:43,840 --> 00:12:51,840
LSTMs, attention mechanisms, et cetera. But indeed, there are also a lot of limitations and

152
00:12:52,800 --> 00:12:58,320
maybe one way, like there's a very computational sort of framing of machine learning, which is to say,

153
00:12:59,520 --> 00:13:04,400
well, we want to have any modality to any modality. Our model needs to not make many assumptions

154
00:13:04,400 --> 00:13:09,760
over the underlying data. So it kind of adapt to all the tasks that we might be interested in

155
00:13:09,760 --> 00:13:15,040
tackling. I think transformers do quite well here. An obvious challenge that if you look at the

156
00:13:15,040 --> 00:13:19,840
probably the amount of papers tackling this, this probably going to be a few, is the fact that

157
00:13:19,840 --> 00:13:25,760
transformers is still requires kind of, it's very symmetric in the way that it looks at all the data.

158
00:13:25,760 --> 00:13:32,560
I mean, if you think of language modeling, for example, it just looks at every single piece of text,

159
00:13:32,560 --> 00:13:37,680
right? And when you're reading a book, that's not how you're kind of ingesting this information

160
00:13:37,680 --> 00:13:42,800
as you read chapter after chapter. So there's some beautiful work, I think, still to be done in

161
00:13:42,800 --> 00:13:50,880
in the memory mechanism being a bit more hierarchical. Maybe that's lose inspiration from how we,

162
00:13:50,880 --> 00:13:56,320
you know, we think we work in terms of ingesting information and, you know, compressing the information

163
00:13:56,320 --> 00:14:01,440
we ingest. We don't remember every single detail. That being said, computers might not necessarily

164
00:14:01,440 --> 00:14:07,280
need to operate like we do, right? So it is possible that, well, okay, like that's not we cannot do

165
00:14:07,280 --> 00:14:12,320
that, but well, the machines can can can do it. So maybe that is fine. But I think computationally,

166
00:14:12,320 --> 00:14:18,720
there are there are definitely challenges that may make transformers have definitely some limits

167
00:14:18,720 --> 00:14:24,400
on the amount of information they can process effectively in parallel as they ingest these sequences

168
00:14:24,400 --> 00:14:31,120
of information or bytes. It sounds like you're suggesting a kind of a higher level attention

169
00:14:31,120 --> 00:14:40,640
mechanism that more more broadly shapes the way that the transformers learning from the data that

170
00:14:40,640 --> 00:14:45,760
it's presented. Yeah, I think, I mean, there again, and this exists, it just that then it's the

171
00:14:45,760 --> 00:14:52,960
matter of what details how the optimization can be made to work. But I think the indeed some more

172
00:14:52,960 --> 00:14:59,520
forms of hierarchical memory from course to find lots of these ideas existed in computer vision

173
00:14:59,520 --> 00:15:05,680
for years as well. So I think there's going to be a good mix of ideas and iterative processes

174
00:15:05,680 --> 00:15:13,520
until maybe the next model that feels maybe more efficient for for other tasks we might not even

175
00:15:13,520 --> 00:15:17,680
be thinking about. That's that's what I was saying. There's a you need to push the envelope. So

176
00:15:17,680 --> 00:15:23,440
usually it goes hand in hand with a new task that we cannot even dream of doing right now because

177
00:15:23,440 --> 00:15:29,680
yeah, the limitations of the state of the art models. But I think hierarchy in memories, one of my

178
00:15:29,680 --> 00:15:35,840
beds, definitely some work, you know, we've done and we might mean the whole research community is

179
00:15:35,840 --> 00:15:40,880
doing that I think is I'm definitely keeping an eye on. You know, also related to

180
00:15:42,000 --> 00:15:50,720
transformers and the impact that we've seen there is the work specifically happening around

181
00:15:50,720 --> 00:15:57,280
large language models. What kind of work are you doing there? Yeah, I mean large language models

182
00:15:57,840 --> 00:16:05,120
is a very fascinating field that you could you could think there's been a huge paradigm shift

183
00:16:06,000 --> 00:16:12,080
or maybe there's been non-depending on how far you zoom in, zoom back in the past, right? Because

184
00:16:12,720 --> 00:16:19,680
you know, if you look even even in the 50s, Shannon like was already intrigued by the fact that

185
00:16:19,680 --> 00:16:25,280
well, if you have, you know, statistical pieces of text, you could you could actually generate

186
00:16:25,280 --> 00:16:31,840
text from end-gram models. And then, you know, you have to go forward to maybe the neuro-language model

187
00:16:31,840 --> 00:16:38,800
error until we start to scaling this up with the data and the models and GPUs, etc. to start

188
00:16:38,800 --> 00:16:44,560
unlocking indeed state-of-the-art machine translation, which already feels a bit magical because

189
00:16:44,560 --> 00:16:50,080
in machine translation, we're asking the model to translate sentences that definitely have not been

190
00:16:51,280 --> 00:16:54,960
in, are not in the training set. As we know, most of the sentences we add are

191
00:16:55,760 --> 00:17:00,320
they're probably unique. Otherwise, we would not be talking to one another because we could always

192
00:17:00,320 --> 00:17:07,280
predict what is some going to say, what is Oriol going to say. So, but what had, but thanks to

193
00:17:07,280 --> 00:17:13,360
these kind of mix of components, I think the last definitely 10 years, last five years, and then

194
00:17:13,360 --> 00:17:19,360
transformers being the last maybe ingredient that has been added to this mix toolbox, you know,

195
00:17:19,360 --> 00:17:26,880
build that I was mentioning. We've gotten to language models that, yeah, we can sample from and

196
00:17:26,880 --> 00:17:32,560
we can query, but it starts to feel like, oh, this is, if we were talking to someone, right,

197
00:17:32,560 --> 00:17:37,280
behind the scenes, like going back to obviously Turing and Turing test ideas, although maybe that

198
00:17:37,280 --> 00:17:43,040
is not that useful these days, but just thinking like that, you have this pre-trained language model

199
00:17:43,040 --> 00:17:50,240
that you can start treating like an entity that can obviously utter any text, and that's powerful

200
00:17:50,240 --> 00:17:55,760
if it's good enough. And I think that's what changed. We were doing this all along, I would argue,

201
00:17:55,760 --> 00:18:02,240
in NLP for many years, many people have obviously tried to model language. So the basic principle

202
00:18:02,240 --> 00:18:07,280
exists. I mean, you could always query the model to, you know, ask it a question and see how it

203
00:18:07,280 --> 00:18:13,760
reacts. Does it know about color of the sky, et cetera? But recently, thanks to scale, there's

204
00:18:13,760 --> 00:18:20,640
what it's true is that the performance has been so good. And like the change from like the,

205
00:18:20,640 --> 00:18:26,720
you know, millions to billions of parameters has triggered like now, okay, a new dream almost of

206
00:18:26,720 --> 00:18:32,880
what else could we do if we kept scaling on the one side? And also like, look, it's unbelievable

207
00:18:32,880 --> 00:18:38,080
that this that felt like maybe at that end or like sure, we could use language models in a more

208
00:18:38,080 --> 00:18:44,320
complex system that was doing, you know, the traditional chatbot building with rules and so on.

209
00:18:44,320 --> 00:18:50,880
Now, this actually feels like it's in a state that for some applications and with lots of

210
00:18:50,880 --> 00:18:58,400
caveats actually can actually be used and feels like a very powerful tool. And indeed, I mean,

211
00:18:58,400 --> 00:19:05,680
open AI being probably the one that has pioneered this more notably has shown some demonstrations,

212
00:19:05,680 --> 00:19:13,600
right? So at DeepMind, we are definitely looking into this. I mean, the space of large language

213
00:19:13,600 --> 00:19:19,120
models is extremely exciting. And actually, if you look a bit at the history and perhaps

214
00:19:20,720 --> 00:19:25,600
things like you see in projects such as the one I was involved with, which is a StarCraft,

215
00:19:25,600 --> 00:19:31,280
the way I always thought about StarCraft coming from where I come from is that it's just modeling

216
00:19:31,280 --> 00:19:37,600
sequences of words. These words don't are not like English words. They're like instructions on,

217
00:19:37,600 --> 00:19:42,640
you know, that you send to the game engine, like move this piece here and it's a very rich language.

218
00:19:42,640 --> 00:19:49,280
It's an API like language. But you could already start seeing the power of these methods beyond

219
00:19:49,280 --> 00:19:56,160
language to like decision making and agents. And I think this is there's a lot of interesting

220
00:19:56,160 --> 00:20:02,720
parallels that we are already witnessing by like, you know, maybe a like human level, like go

221
00:20:02,720 --> 00:20:07,680
actually the very first steps in AlphaGo where indeed as well similar to modeling,

222
00:20:08,560 --> 00:20:13,920
precise modeling of the probability of the next word in Go, the next word is just a two-dimensional

223
00:20:13,920 --> 00:20:20,720
like position of where you would put the next, you know, piece. But in general, this principle has

224
00:20:20,720 --> 00:20:27,680
been there since ever and then the all that it took is for the performance to be at the level of,

225
00:20:27,680 --> 00:20:34,400
wow, you can really play a game or reply to almost anything you can ask these models and you will

226
00:20:34,400 --> 00:20:40,640
get somewhat sensible replies sometimes enough that this speaks now the interest of many more

227
00:20:40,640 --> 00:20:47,680
and the field is indeed expanding a lot which is super welcome. So you mentioned Starcraft

228
00:20:47,680 --> 00:20:57,920
once again and you are you've got a workshop paper at Nurebs that is a follow-up to the Alpha

229
00:20:57,920 --> 00:21:07,840
Star work. That was maybe 2019 the Alpha Star. Can you maybe give us a refresher on that work and

230
00:21:07,840 --> 00:21:14,720
then talk about what you are presenting at Nurebs this year? Sure, yeah, I mean the I think DeepMine

231
00:21:15,360 --> 00:21:21,360
as a company obviously had its own different stages where, you know, the very early beginnings

232
00:21:21,360 --> 00:21:27,360
on Atari just showing that deep reinforcement learning has to prove itself, right? It's a bit like

233
00:21:27,360 --> 00:21:34,320
deep learning but now a bit more specific. So a way to prove itself is actually indeed to master

234
00:21:34,320 --> 00:21:39,280
ever more complicated domains or environments as we call them in reinforcement learning instead

235
00:21:39,280 --> 00:21:45,200
of being datasets you are optimizing a reward but ultimately actually a lot of the deep learning

236
00:21:45,200 --> 00:21:53,840
kind of main components or ideas apply. So you saw this kind of kind of one at a time, right? Almost

237
00:21:53,840 --> 00:22:00,480
as a curriculum of increasingly, you know, domains like from Atari to Go and Chess and then,

238
00:22:00,480 --> 00:22:05,120
you know, ultimately I would say Starcraft being, you know, much more complex in many different

239
00:22:05,120 --> 00:22:10,240
dimensions as a as a game or a video game. That's what we kind of were doing at DeepMine and maybe

240
00:22:10,240 --> 00:22:16,640
that's the Alpha Star project that regarded Starcraft 2 which is a popular real-time strategy game

241
00:22:16,640 --> 00:22:22,400
was the end of that sort of sequence of demonstrations of, well, these deep parallel principles

242
00:22:23,440 --> 00:22:28,560
really apply to all the domains that we have found interesting in these sense of

243
00:22:28,560 --> 00:22:34,080
games that are complex. They require like a synchronous thinking, partial observability,

244
00:22:34,080 --> 00:22:39,600
all the right kind of interesting properties that may be the same way computer vision went

245
00:22:39,600 --> 00:22:44,320
from emnis which was very interesting until, you know, it became soft and then we moved over,

246
00:22:44,320 --> 00:22:50,720
right? So this is kind of a parallel. I like to kind of visualize almost in this complexity versus

247
00:22:50,720 --> 00:22:57,760
the kind of game that DeepRL was stacking. And Alpha Star essentially did this through an approach

248
00:22:57,760 --> 00:23:04,000
that was not purely deep reinforcement learning. It actually employed imitation learning or offline

249
00:23:04,000 --> 00:23:10,000
reinforcement learning thanks to the massive amount of games that when humans play one another

250
00:23:10,000 --> 00:23:15,520
I recorded anonymously by the company that makes the game. So there's a huge wealth of sequences

251
00:23:15,520 --> 00:23:20,720
of observations and actions, right? That as I said, you could see as a bit of a language

252
00:23:20,720 --> 00:23:28,080
just that expresses moves in the game, right? And then that was kind of the first seed of Alpha

253
00:23:28,080 --> 00:23:34,560
Star, how we reach ground master level at the game was, well, let's take a look at these sequences

254
00:23:34,560 --> 00:23:39,600
that we have at scale. There's actually millions of sequences. So we have a lot of data which is

255
00:23:39,600 --> 00:23:45,760
great in deep learning the more data, the better. And then we learn to imitate which essentially

256
00:23:45,760 --> 00:23:52,000
is applying the same principles as language modeling. So given all the words that we've seen and to

257
00:23:52,000 --> 00:23:56,560
until some point, we're trying to predict the next work or in the case of StarCraft, we're trying

258
00:23:56,560 --> 00:24:02,480
to predict the next move, which is just kind of a complex object like move this unit

259
00:24:02,480 --> 00:24:07,280
onto this position in the map, right? But it's actually very similar to modeling language.

260
00:24:07,840 --> 00:24:14,080
And that first agent was reasonably good. It was actually better than most humans in terms of

261
00:24:14,080 --> 00:24:20,240
the median performance. But then we took it to the next level by then initializing a self kind of

262
00:24:20,240 --> 00:24:27,280
play system, a multi agent system in fact, of many agents that developed different skills and

263
00:24:27,280 --> 00:24:33,920
different strategies in the game. And they played each other for many years actually, like there's

264
00:24:33,920 --> 00:24:40,160
like each agent kind of plays a StarCraft over like 150 years or so. That's what we did in the

265
00:24:40,160 --> 00:24:46,400
nature paper. And that achieves from this median level or above median level to really like the top,

266
00:24:46,400 --> 00:24:51,920
the very top top level of play that then we verified and we published, as you said, indeed in the

267
00:24:52,880 --> 00:25:00,480
in the end tail of 2019. So that was very cool. And obviously our goal was can we just really crack

268
00:25:00,480 --> 00:25:05,200
this game, right? So we took this kind of hybrid approach of initializing the model to imitate

269
00:25:05,200 --> 00:25:11,520
humans and then taking off by just doing self play. And in a way, imitate how humans have discovered

270
00:25:11,520 --> 00:25:17,920
the game by playing online against each other. Recently, we were both motivated by first actually

271
00:25:17,920 --> 00:25:24,560
trying to pose StarCraft or imitating human moves as a challenge for those who study offline

272
00:25:24,560 --> 00:25:30,320
around, which is this area of reinforcement learning where you're not allowed to interact with

273
00:25:30,320 --> 00:25:35,520
the environment. You you can observe kind of agents or people interacting with environments,

274
00:25:35,520 --> 00:25:41,360
but you're not allowed because maybe it's not practical to perhaps go there and to plug your agent

275
00:25:41,360 --> 00:25:46,400
in the environment and try to see if you get a reward. So that's a fascinating area. And what we

276
00:25:46,400 --> 00:25:52,800
thought is look, we have one of the richest offline around data sets, right? Millions of trajectories

277
00:25:52,800 --> 00:25:59,840
from humans of all levels playing the game would it would be great to first try to open this as a

278
00:25:59,840 --> 00:26:04,800
challenge for the community. So one thing we're working on very hard these days actually is to,

279
00:26:04,800 --> 00:26:10,960
you know, finalizing open sourcing. So, you know, anyone can just go download the the code and

280
00:26:10,960 --> 00:26:17,040
just train their own agent based on these imitation principle only. So we're only looking at the

281
00:26:17,040 --> 00:26:21,760
the very kind of first beginning of the whole Alpha Star agent. But at the same time, we were

282
00:26:21,760 --> 00:26:26,960
wondering, look, can we push the performance of these agents? We didn't care to do it at the time

283
00:26:26,960 --> 00:26:32,240
because we knew that multi-agent and self-play and reinforcement learning were going to be successful

284
00:26:32,240 --> 00:26:39,680
at making the agent better. But could we do that? Only imitating human moves. And that is kind of

285
00:26:39,680 --> 00:26:45,440
almost the same if you think of language modeling that you're only imitating next word prediction.

286
00:26:45,440 --> 00:26:50,160
You're never training these models further, but by themselves are quite good. And the answer is

287
00:26:50,160 --> 00:26:58,800
you can push performance. We definitely have beaten like the agent that we published, the initial

288
00:26:58,800 --> 00:27:03,920
agent that was learned to imitate. With some further refinements, we use Mu0, there are details

289
00:27:03,920 --> 00:27:11,360
in the paper that folks can go and read. But with some ideas, very key ideas using further than

290
00:27:11,360 --> 00:27:18,480
just imitating the next move. So from the offline RL community, we're able to beat that first Alpha

291
00:27:18,480 --> 00:27:23,680
Star agent that was very good and see that, of course, what yielded the nature of publication

292
00:27:23,680 --> 00:27:31,120
performance by, I think, over 90% of the time. So there is performance to be unlocked, and this

293
00:27:31,120 --> 00:27:37,360
is only the beginning. We tried a few ideas, but I think this is a very fruitful resource for those

294
00:27:37,360 --> 00:27:42,320
who are interested in this field of offline reinforcement learning to maybe go and tackle

295
00:27:42,320 --> 00:27:47,120
this challenge and hopefully with the source code available. And just the fact that it's such a fun

296
00:27:47,120 --> 00:27:53,440
game to observe and a cool domain, people might go and obviously then take it on and try to

297
00:27:54,400 --> 00:28:00,080
get to a next level of performance just with imitating human moves, which is fascinating to me at

298
00:28:00,080 --> 00:28:10,720
least. Do you think of at least in this context the offline setting and imitation as synonymous?

299
00:28:10,720 --> 00:28:18,000
It seems like they largely are, but that you could also envision other types of processing of

300
00:28:18,000 --> 00:28:24,800
the data set that has some benefit beyond just the imitation itself. Yeah, so I think, I mean,

301
00:28:24,800 --> 00:28:28,960
there's a lot of names. I mean, people call this supervised learning, behavioral cloning,

302
00:28:28,960 --> 00:28:35,040
imitation learning. I think what's powerful about, if you restrict yourself to, I will not interact

303
00:28:35,040 --> 00:28:40,720
with the environment, but I am able to see like what agents interacting with the environment achieved.

304
00:28:40,720 --> 00:28:47,120
I think that the powerful and what offline are really poses is imitation is one part, right? So

305
00:28:47,120 --> 00:28:53,120
imitating the actions very well is what language modeling regards with and it's very important.

306
00:28:53,120 --> 00:28:59,680
But there is also the reward that we observe in a game like StarCraft, who won the game? And you can

307
00:28:59,680 --> 00:29:05,200
also predict the winner, right, offline. You can say, look, I mean, we have this game to players

308
00:29:05,200 --> 00:29:09,600
playing. We have every single action they took. Let's try to imitate that to understand that

309
00:29:09,600 --> 00:29:15,440
very well, like we understand language. But also there is a fact that one of them won. And if we

310
00:29:15,440 --> 00:29:21,040
can model that precisely with a value of action, which is obviously a crucial element of most RL,

311
00:29:22,000 --> 00:29:28,720
then we can even in an offline setting try to find an action that is not just human like, but

312
00:29:28,720 --> 00:29:34,080
maximizes the probability according to our own estimate of the value and maximizes the

313
00:29:34,080 --> 00:29:40,160
probability of winning. And in fact, one of the best performance agents we showed uses Mu0,

314
00:29:40,160 --> 00:29:48,000
which is offline. We never use self play, but Mu0 basically tries to model based on

315
00:29:48,000 --> 00:29:53,920
simulating actions that you may take in the future and then pick those that maximize reward

316
00:29:53,920 --> 00:29:59,520
or value, but according to your own estimates, right. And that extra step we didn't take at the

317
00:29:59,520 --> 00:30:05,120
time. But many people in offline RL obviously are studying not only actions, but also estimating

318
00:30:05,120 --> 00:30:12,720
the reward or value. We found that already enabled this level of performance that we didn't

319
00:30:12,720 --> 00:30:18,000
unlock at the time we were doing the project. But I'm sure there is more on how you train the values,

320
00:30:18,000 --> 00:30:22,560
how do you train the actions, how, what the losses are. There's a lot of toolbox actually

321
00:30:22,560 --> 00:30:29,280
components to be discovered perhaps. And, you know, the usage of benchmarks is critical to advancing

322
00:30:29,280 --> 00:30:34,880
this. So there are quite a few benchmarks in offline RL already. I think StarCraft poses an

323
00:30:34,880 --> 00:30:40,880
interesting one given its complexity in action space and the fact that it's partial observable

324
00:30:40,880 --> 00:30:45,280
and some properties that make, you know, this a unique environment like many others.

325
00:30:45,280 --> 00:30:51,920
So what degree are you seeing, you know, the learnings from the work that deep mind and others

326
00:30:51,920 --> 00:30:58,880
are doing around games kind of translate to, you know, real world, non-game scenarios.

327
00:30:58,880 --> 00:31:05,520
Yeah, that's a great question because I think we're seeing quite a lot like, for instance,

328
00:31:05,520 --> 00:31:12,240
like actually like thinking of Alpha Star and then Alpha Fold. A lot of the work we did in Alpha

329
00:31:12,240 --> 00:31:19,040
Star early days, right. We, you know, transformers just had come up. So we started investing

330
00:31:19,040 --> 00:31:24,880
or seeing maybe good performance with transformers. Maybe the first project that we saw that was Alpha

331
00:31:24,880 --> 00:31:32,000
Star actually. And then transformers and self-attention and some further tools that were developed

332
00:31:32,000 --> 00:31:37,200
specifically thinking about the protein folding problem were developed. And, you know, there's

333
00:31:37,200 --> 00:31:41,440
loose inspiration, right, by the fact that, hey, we know that there's this group that found this

334
00:31:41,440 --> 00:31:46,560
model that, you know, a different research group that the great part of research is you take at

335
00:31:46,560 --> 00:31:52,160
learnings from not only your own company, but of course, any other research institution. And,

336
00:31:52,720 --> 00:31:56,720
you know, loosely speaking, right, there is always, you can find always these connections, right,

337
00:31:56,720 --> 00:32:03,760
that from one project learnings, then it goes to others and so on. From games and reinforcement

338
00:32:03,760 --> 00:32:11,280
learning, actually, I see a shift now with many great examples of just applying the same

339
00:32:11,280 --> 00:32:17,440
areal techniques to other places. There's some that we are applying in language, right,

340
00:32:17,440 --> 00:32:23,280
machine translation, for example. It's difficult, like we have a few works. I'm not sure they're

341
00:32:23,280 --> 00:32:29,920
quite there in terms of breaking the state of the art, but certainly, you know, there, you know,

342
00:32:29,920 --> 00:32:34,480
because you have this, again, going back to maybe the beginning of the conversation,

343
00:32:34,480 --> 00:32:40,080
these deep learning approach that the tools must be generic, then anything you discover in any

344
00:32:40,080 --> 00:32:45,280
specific domain, because you tried not to be very domain specific, then they will naturally

345
00:32:45,280 --> 00:32:49,680
translate to other domains, ideally, right. And perhaps one of the things that

346
00:32:50,800 --> 00:32:55,760
in StarCraft, maybe it's a very simple idea, but I think this one has a lot of potential when

347
00:32:55,760 --> 00:33:02,000
you mix this idea of imitating humans with reinforcement learning refinement, is that we added

348
00:33:02,000 --> 00:33:08,800
distillation laws, which is a pressure of the areal model to actually look still even if it wants to

349
00:33:08,800 --> 00:33:13,680
change the actions because reinforcement learning is a different laws and it's doing different things.

350
00:33:13,680 --> 00:33:19,440
You make some pressure for the model to always sort of imitate a little bit the policy that

351
00:33:19,440 --> 00:33:25,440
it started from that imitates humans. And that was critical there. And in fact, in a lot of now

352
00:33:25,440 --> 00:33:31,200
language model applications, for instance, this principle, I see it kind of sprinkled around,

353
00:33:31,200 --> 00:33:36,720
like because it's quite natural to not want the model to diverge. And this comes from obviously

354
00:33:36,720 --> 00:33:43,040
all their work on model distillation, knowledge distillation, et cetera. So it's, as I said,

355
00:33:43,040 --> 00:33:47,600
there's always these tools that ideally you purpose for some particular reason initially,

356
00:33:47,600 --> 00:33:53,840
but the consequences or the applications later on along the line, they're going to be unprecedented.

357
00:33:53,840 --> 00:33:59,680
And I mean, Transformers is a great example of having been developed for machine translation

358
00:33:59,680 --> 00:34:06,240
now suddenly, they're folding proteins. Even the authors of these papers, it's just sometimes

359
00:34:06,240 --> 00:34:10,560
hard to believe probably, like, how is this happening? And it's a bit random and you need a

360
00:34:10,560 --> 00:34:15,600
bit of lag and the right titles in the papers. And there's a lot of interesting actually randomness

361
00:34:15,600 --> 00:34:20,800
in the field, which makes nearly such an interesting conference, actually, and of all the other

362
00:34:20,800 --> 00:34:30,320
conferences in ML. But yeah, it's cool to think that these are basically in the end tools that we

363
00:34:30,320 --> 00:34:37,600
want to apply generally anywhere. And many times we see these successes transfer over very

364
00:34:37,600 --> 00:34:43,920
surprising areas that even the original inventors did not anticipate. One of the historical challenges

365
00:34:43,920 --> 00:34:50,160
with reinforcement learning, deep reinforcement learning in particular is the sample inefficiency.

366
00:34:50,160 --> 00:34:56,880
You know, that gives rise to topics like fuchsad or related to topics like fuchsad and one-top

367
00:34:56,880 --> 00:35:04,240
learning. You've got a poster at Nureps that is looking at multimodal fuchsad learning

368
00:35:04,240 --> 00:35:10,720
with language models. Is that in the RL context or separate from RL?

369
00:35:10,720 --> 00:35:18,000
That's not in the RL context. But it is definitely in the imitation learning context. And then it's

370
00:35:18,000 --> 00:35:25,120
it's proposing a way to try to leverage a large amount of data and a big language model that we

371
00:35:25,120 --> 00:35:32,720
trained, right? Only on language. But then can this language model with a little bit of extra training

372
00:35:32,720 --> 00:35:39,200
be then tuned to be able to talk about images that it takes as its inputs, right? And I mean,

373
00:35:39,200 --> 00:35:44,320
this is fascinating because it links to fuchsad learning, which is an area that actually has

374
00:35:44,320 --> 00:35:50,000
one of the very first works I did when I joined in mine. There was all these metal learning

375
00:35:50,000 --> 00:35:55,680
papers coming from from the groups and I was very impressed. So one of the works we did was working

376
00:35:55,680 --> 00:36:01,760
on fuchsad learning. What happened with the work is that we propose a new method that I think

377
00:36:01,760 --> 00:36:06,640
it's reasonably simple. But but actually the benchmark proposing that paper is what made,

378
00:36:06,640 --> 00:36:12,480
you know, this this paper more maybe widely known for the benchmark it proposed, which is called

379
00:36:12,480 --> 00:36:17,520
Mini ImageNet. And you know, Mini ImageNet is the task of fuchsad learning where you know,

380
00:36:17,520 --> 00:36:22,160
you just have a few images from one class you've never trained and you know, the the goal is to

381
00:36:22,160 --> 00:36:27,040
classify it better at that chance, right? And you know, there's been a lot of work following up

382
00:36:27,920 --> 00:36:32,320
nicely defined clear benchmark for the community. And the cool thing here is that

383
00:36:32,880 --> 00:36:39,280
without even training at all to do Mini ImageNet or fuchsad learning for images, these language

384
00:36:39,280 --> 00:36:45,360
models, right? Just through imitating just generic knowledge that I found on the internet about

385
00:36:45,360 --> 00:36:51,600
people talking to one another and so on. And with a little bit of fine tuning to understand correlations

386
00:36:51,600 --> 00:36:56,640
between images and language, namely captioning, image captioning is what we pre-trained these models

387
00:36:56,640 --> 00:37:02,320
with. But by freezing the language component, you can then reuse this model to not only do image

388
00:37:02,320 --> 00:37:08,000
captioning, but also Mini ImageNet was one of the tasks that we present in the paper. And it's

389
00:37:08,000 --> 00:37:13,680
impressive that it's not only doing fuchsad learning better than chance, but it's never trained

390
00:37:13,680 --> 00:37:18,960
with the purpose of doing fuchsad learning. You're in a way borrowing the capabilities of fuchsad

391
00:37:18,960 --> 00:37:25,200
learning from the language modeling. And then with a little bit of vision mixed in, you're able to do

392
00:37:25,200 --> 00:37:30,800
fuchsad tasks that you were not even thinking when you were designing the training setting. And like

393
00:37:30,800 --> 00:37:35,200
what you were doing at the time, we were doing at the time with fuchsad learning for image classification,

394
00:37:35,200 --> 00:37:39,600
where we were training the models to do fuchsad learning for image classification. And then they

395
00:37:39,600 --> 00:37:44,400
were good at that. But now it's like, almost you don't train them to do this particular task,

396
00:37:44,400 --> 00:37:50,480
but they actually generalize over different tasks that involve language and vision in different

397
00:37:50,480 --> 00:37:59,120
degrees, of course, of accuracy, not very high. But this is actually a lot of work that I see in

398
00:37:59,120 --> 00:38:03,600
the vision and language intersection is going this direction. And it's quite exciting to see all

399
00:38:03,600 --> 00:38:09,040
the amazing work that appears as well at Neurib's and computer vision conferences like. But it

400
00:38:09,040 --> 00:38:14,880
doesn't have RL, although you could always use RL to fine tune the models further. But in general,

401
00:38:14,880 --> 00:38:20,800
that has not been adopted yet too much, I guess, in these communities. We spoke earlier about

402
00:38:22,080 --> 00:38:31,440
transformers as kind of this, you know, innovation frontier, you know, for lack of a better term,

403
00:38:31,440 --> 00:38:38,880
and kind of where a lot of the activity is in the toolkit. And, you know, this poster is an example

404
00:38:38,880 --> 00:38:47,120
of, you know, transformers, you know, plus, you know, transformers trained with language. And

405
00:38:47,760 --> 00:38:53,040
in parallel, there's this broader conversation about foundation, the foundational models in the

406
00:38:53,040 --> 00:38:59,360
community. You know, what's your take on that? Do you think that, you know, language is going to

407
00:38:59,360 --> 00:39:04,640
provide this, you know, substrate that we'll be able to do a lot of non-language things with. And

408
00:39:04,640 --> 00:39:10,800
that's going to be a, you know, broadly applied tool in the toolbox. Yeah, I mean, I honestly think

409
00:39:10,800 --> 00:39:19,840
looking at what's been going on. And even taking some inspiration from not only like transformers,

410
00:39:19,840 --> 00:39:24,640
but also ideas around unsupervised learning, self-supervised learning, another big area that's

411
00:39:24,640 --> 00:39:31,120
been exploding lately, it does feel that, you know, again, going back to the very beginning of

412
00:39:31,120 --> 00:39:36,320
deep learning, right? You have this principle, right? Like the traditional deep learning, maybe,

413
00:39:36,320 --> 00:39:41,520
let's see what the next generation deep learning could look like. But traditional one is this,

414
00:39:41,520 --> 00:39:46,320
as I said, you take the data set of input outputs and you have these toolbox, you mix and

415
00:39:46,320 --> 00:39:51,120
match components, and then off you go, you train your model and it does reasonably well at many

416
00:39:51,120 --> 00:39:57,200
tasks. But this is quite unsatisfying, right? If you ask many of those who have been in the field

417
00:39:57,200 --> 00:40:02,320
for a long time, and obviously the ones that are not probably also find it annoying, the annoying

418
00:40:02,320 --> 00:40:08,480
bit is the ways are randomly initialized, which is beautiful in a way. But it's also like quite annoying.

419
00:40:08,480 --> 00:40:13,040
I mean, this, this is very wasteful, right? We're, we're starting to train from scratch,

420
00:40:13,920 --> 00:40:19,440
and we're throwing away a lot of energy used to train, you know, fine weights that we had for

421
00:40:19,440 --> 00:40:26,080
a different task that's somewhat related. And so this weight reusability, I think is what these

422
00:40:26,080 --> 00:40:31,280
foundational models and this way of thinking is getting at, which is, look, not only we want to

423
00:40:31,280 --> 00:40:37,680
take tools, we might want to take the tools with the weights associated with them, which poses a very

424
00:40:37,680 --> 00:40:44,720
interesting challenge of, can we initialize parts of the network with maybe a language modeling

425
00:40:44,720 --> 00:40:54,160
component, but practice and sadly this is at the frontier, it's still not very clear that we found

426
00:40:54,160 --> 00:41:01,200
the way to use this idea of pre-trained weights in a successful way, meaning that we start from

427
00:41:01,200 --> 00:41:05,680
the pre-trained weights rather than random weights, and we achieve better performance. Although

428
00:41:05,680 --> 00:41:11,520
more exceptions to start to appear in the field, such as the one from self-supervised learning,

429
00:41:11,520 --> 00:41:18,800
in which you train features that look at image statistics that seem superfluous maybe initially,

430
00:41:18,800 --> 00:41:24,400
but these features happen to be quite good at classifying or doing all sorts of image tasks if

431
00:41:24,400 --> 00:41:31,920
you use them as pre-trained weights. But I do believe that these probably has to be part of an

432
00:41:31,920 --> 00:41:39,600
answer for the next generation deep learning that not only reuses the tools, but reuses the weights,

433
00:41:39,600 --> 00:41:45,760
but it is extremely tricky. Again, the field has been looking at this problem not, you know,

434
00:41:45,760 --> 00:41:51,600
definitely before deep learning was deep learning, but it is very natural to think about that, and

435
00:41:51,600 --> 00:41:58,640
also again, maybe looking a bit at how we learn, we don't always like start from a fresh brain when

436
00:41:58,640 --> 00:42:05,600
we learn something new, right? There's always an accumulation of learning capabilities, and that

437
00:42:05,600 --> 00:42:11,120
feels like a big gap in the way we do deep learning. Despite the fact that you train these networks

438
00:42:11,120 --> 00:42:16,160
from scratch, and they argue that protein folding or translation and the components are the same,

439
00:42:16,160 --> 00:42:24,160
but the weights, no, right? And that, I think I believe this will be as we do research and find

440
00:42:24,160 --> 00:42:29,760
ways to make these weights useful and beating the performance without, you know, training from

441
00:42:29,760 --> 00:42:36,240
scratch, et cetera. I believe this is definitely a way forward, and, you know, it's exciting because,

442
00:42:36,240 --> 00:42:41,600
I mean, we tried, I mean, we, definitely we tried many, many things, and, you know, it's, it's

443
00:42:41,600 --> 00:42:46,800
hard. It's not, it's, you know, it feels intuitive, and it's one of these things that neural nets don't

444
00:42:46,800 --> 00:42:52,080
seem to want to do too well without, like, a lot of effort. So there must be a way, and I mean,

445
00:42:52,080 --> 00:42:56,400
finding it is definitely in the future. I'm sure many people are excited about this.

446
00:42:56,400 --> 00:43:05,600
So another thing I wanted to ask about is a panel that you're going to be on, also at NERPs,

447
00:43:05,600 --> 00:43:10,800
that is talking about a topic that's somewhat related to what we've talked about thus far,

448
00:43:10,800 --> 00:43:20,000
but is specifically focused on, you know, the consequences of the level of scale that we've achieved,

449
00:43:20,000 --> 00:43:24,400
and that we, that, you know, the way we're approaching machine learning with transformers is

450
00:43:24,400 --> 00:43:31,200
requiring. Tell us a little bit about that panel and some of the questions that it is exploring.

451
00:43:31,200 --> 00:43:37,200
Yeah, I mean, I think the panel and the question about scale, there's a more profound question,

452
00:43:37,200 --> 00:43:42,800
which links to actually in the whole field, or what it, what it, what it, what makes research good

453
00:43:42,800 --> 00:43:49,280
or interesting. And I find that that is a fascinating evolving topic, right? Definitely.

454
00:43:49,280 --> 00:43:58,080
I actually kind of recalled one of my, I think my very first NERPs paper has a theorem and a proof,

455
00:43:58,080 --> 00:44:03,280
because at the time, you know, you had to put a theorem and a proof in a NERPs paper for it

456
00:44:03,280 --> 00:44:09,520
to be accepted, right? And it's just that's what you had to do. The paper actually talks a bit about

457
00:44:09,520 --> 00:44:14,640
actually deep learning with SBMs, which was a very, you know, obviously popular model at the time,

458
00:44:14,640 --> 00:44:21,360
super vector machines. But, you know, the question, and I think a challenge is, um, scale is,

459
00:44:21,360 --> 00:44:27,760
is definitely permeating into research. And there is, I think one, one of the main aspects that

460
00:44:28,800 --> 00:44:36,320
will be discussing the panel, or we discussed in the panel is the fact that many research works

461
00:44:36,320 --> 00:44:42,800
or papers might be required or asked naturally by reviewers. Hey, like, can you try this idea at

462
00:44:42,800 --> 00:44:48,560
scale, right? If you look at reviews, there's many reviews that are actually public. I really applaud

463
00:44:48,560 --> 00:44:55,280
the usage of systems like open review that has a more transparent way to show how the review process

464
00:44:55,280 --> 00:45:00,560
work. And it's quite great for people that are new in machine learning, perhaps the most. But

465
00:45:00,560 --> 00:45:06,880
you often see that, well, does this model scale or not, right? And, you know, that is that a fair

466
00:45:06,880 --> 00:45:12,000
question to ask. Should we always aim to scale up our methods? I wish, as we show, like,

467
00:45:12,000 --> 00:45:16,880
results at the conference, like, near ribs. And I think the answer is, I mean, absolutely not.

468
00:45:17,600 --> 00:45:24,800
But the question is, if a paper claims, um, claims to have discovered a new tool or a new

469
00:45:24,800 --> 00:45:30,080
advance on an existing tool, um, the real question, perhaps, and that, that's where it gets maybe

470
00:45:30,080 --> 00:45:36,640
a bit tricky is in which data sets or in which benchmarks are you trying this idea on? And I think what

471
00:45:36,640 --> 00:45:42,560
the community has evolved towards is that, let's say, in computer vision, if you don't show something

472
00:45:42,560 --> 00:45:49,920
works on image net, um, it's probably a bit inconclusive based on, let's say, other very popular

473
00:45:49,920 --> 00:45:57,040
data sets that are smaller and does a bit easier to to run on, like, CIFAR or MNIST, um, right? And

474
00:45:57,040 --> 00:46:02,320
there's still a lot of good work and insights discovered on those data sets. But it is natural

475
00:46:02,320 --> 00:46:08,000
that you scale up, right? These experiments to image net. Otherwise, I mean, the field is full,

476
00:46:08,000 --> 00:46:12,720
like, it's very big right now. There's another big change is that there are lots of good papers

477
00:46:12,720 --> 00:46:19,520
very well written. And there's a bit, you know, challenging signal signal to noise ratio to understand

478
00:46:19,520 --> 00:46:24,800
or slice out what is a meaningful contribution or what needs more work in terms of showing

479
00:46:24,800 --> 00:46:30,240
empirically that some method works. And here, scale plays a big role. I think the role is that

480
00:46:30,240 --> 00:46:38,800
it's fair to ask maybe to run on image net certain models. And then a very interesting follow-up

481
00:46:38,800 --> 00:46:46,320
question is can everyone in the world scale to that scale of data set, right? I mean, image net is

482
00:46:46,320 --> 00:46:52,000
not, um, perhaps the most large one, you know, if you think of it of hardware and we can train

483
00:46:52,000 --> 00:46:56,320
image net. I mean, some there's some results that can train image net in 10 seconds, of course,

484
00:46:56,320 --> 00:47:01,680
using a lot of parallelism. So it feels like in this sense, it's more accessible, but I mean,

485
00:47:01,680 --> 00:47:07,840
the real question is, is it truly accessible? And that creates, I think, a challenge and part of

486
00:47:07,840 --> 00:47:15,760
the panel, right? I'm discussing about accessibility to scale and is research at scale the only or is

487
00:47:15,760 --> 00:47:22,080
is there interesting research at scale? I think the answer is yes. But is the only research we should

488
00:47:22,080 --> 00:47:28,720
look for at scale? And I think absolutely not. And one very beautiful example that I can think of

489
00:47:28,720 --> 00:47:34,800
again related to transformers is that at the time we were working on machine translation,

490
00:47:35,520 --> 00:47:42,000
at Google, we had this sequence, the sequence paper in which we use an 8,000 dimensional long,

491
00:47:42,000 --> 00:47:47,760
like LSTM basically to do machine translation. And we achieved almost a state of the art numbers

492
00:47:47,760 --> 00:47:52,880
with a technique that was completely different than what people had been doing at the time,

493
00:47:52,880 --> 00:47:58,400
which got I guess people's attention. And it was perfected further. And most of, if not all,

494
00:47:58,400 --> 00:48:02,720
the machine translation systems now, they ran with some sort of neural network underneath.

495
00:48:03,440 --> 00:48:11,040
But Montreal had less GPUs. It's not that we use a lot of GPUs. We use a GPUs for like 20 days.

496
00:48:11,040 --> 00:48:15,600
So it was a painful experiment to run, right? You had to wait 20 days and it's a GPUs. They all

497
00:48:15,600 --> 00:48:21,120
fit in one machine. But of course, it's reasonably large scale. But I think Montreal took a different

498
00:48:21,120 --> 00:48:27,920
approach. And instead of scaling up to 8,000 layers, 8,000 units, they only could use 1000 because

499
00:48:27,920 --> 00:48:33,280
maybe they run it on a single GPU. But what they did, which was I think that's maybe the most

500
00:48:33,280 --> 00:48:39,440
transformative thing. Nobody intended is to invent attention. So the attention paper came maybe

501
00:48:39,440 --> 00:48:46,080
perhaps because a scale was not as available to a group in Montreal as it was for us at Google.

502
00:48:46,640 --> 00:48:53,680
And I think this is just a lesson that we need to learn that sometimes by just being resource

503
00:48:53,680 --> 00:48:57,840
constrained, very good research has happened, right? I mean, I'm not saying that then all

504
00:48:57,840 --> 00:49:04,000
please be happy and never try to scale up. It's not like that. But indeed, there are examples and

505
00:49:04,000 --> 00:49:08,800
it should inspire creativity and a different way of thinking, which ultimately might create

506
00:49:08,800 --> 00:49:13,520
a pattern shift. And absolutely as a community, we need to be very careful, of course, at

507
00:49:13,520 --> 00:49:20,400
discarding ideas because they don't scale. And it would obviously had been a mistake to discard

508
00:49:20,400 --> 00:49:25,280
that idea that created attention, which of course, followed up with self-attention transformers

509
00:49:25,280 --> 00:49:32,400
and who knows what comes next. But that attention principle maybe was the key tool that we keep

510
00:49:32,400 --> 00:49:39,520
seeing being quite useful. And it was invented by a lack of like scaling up to achieve better

511
00:49:39,520 --> 00:49:44,800
results. They had to invent these very intuitive attention mechanism, which for translation made

512
00:49:44,800 --> 00:49:48,880
a lot of sense. You look at, you know, you look at the sentence and you attend over the words

513
00:49:48,880 --> 00:49:54,080
that look like, oh, yeah, I'm going to translate this word. And that beautiful principle now is

514
00:49:54,080 --> 00:49:59,840
folding proteins, right? It's like unbelievable. But it is what it is. Yeah. So that's one aspect

515
00:49:59,840 --> 00:50:08,000
in the panel. And I think maybe another big challenging question, I think, is I don't think

516
00:50:08,000 --> 00:50:13,920
we have many answers. Although there were some interesting parallels with different fields,

517
00:50:13,920 --> 00:50:22,320
like, for instance, the large collider that exists at CERN, the question about, I mean,

518
00:50:22,320 --> 00:50:29,200
companies obviously have access to resources like compute. And I think it is our duty to do the

519
00:50:29,200 --> 00:50:36,400
best we can in terms of research, publishing, and scaling up as responsibly as we can. But the

520
00:50:36,400 --> 00:50:42,320
real question is, how can you make that kind of research available, right? To more people. And

521
00:50:43,760 --> 00:50:51,120
you know, a very interesting thought is should, you know, who should kind of invest on the

522
00:50:51,120 --> 00:50:57,760
maybe building a computer for academic research. CERN is a very interesting example

523
00:50:57,760 --> 00:51:02,800
because the physics community kind of came together and decided, well, we need to build this

524
00:51:02,800 --> 00:51:07,200
device. It's very expensive. It actually uses a lot of energy. I mean, it's quite an interesting

525
00:51:07,920 --> 00:51:13,040
place to be actually. I was very lucky to be very recently in the bizarre real world, you know,

526
00:51:13,040 --> 00:51:19,440
experience that I had during the pandemic. But that is an interesting model. And there are

527
00:51:19,440 --> 00:51:23,520
challenges. It's not like, oh, we should just build a super computer like CERN and, you know,

528
00:51:23,520 --> 00:51:28,880
just have people access it in the same way that you do access CERN by writing grants. And

529
00:51:28,880 --> 00:51:34,000
it's a very interesting system, actually. But the problem there is that happened with a lot of

530
00:51:34,000 --> 00:51:39,520
consensus that, you know, physics is a, for many years, like they said, oh, we need to test these

531
00:51:39,520 --> 00:51:45,360
things and to do that, we need this device. So a very, another question we're discussing

532
00:51:45,360 --> 00:51:50,080
the panel is, of course, it might be too early is scaling up is something that, I mean, it has

533
00:51:50,080 --> 00:51:55,120
happened in machine learning, actually, forever. It's not new. You look at the history of data sets.

534
00:51:55,120 --> 00:52:01,280
There's a scaling up trend, of course, of everything, thanks to more laws and so on. But the

535
00:52:01,280 --> 00:52:08,240
question is, do we know, do we feel, you know, assured enough that it's investing, like, let's say,

536
00:52:08,240 --> 00:52:14,320
public money from different governments that maybe could form a coalition similar to CERN,

537
00:52:14,320 --> 00:52:20,000
is that the right moment to do it, right? Do we know that's the way to go? And that's a good

538
00:52:20,000 --> 00:52:26,160
good question. My answer to that is scaling up will be part of the solution, but it's not the

539
00:52:26,160 --> 00:52:31,520
solution, right? In terms of building intelligence, I think it's inconceivable to me that

540
00:52:33,520 --> 00:52:38,960
we need to scale up just because of the amount of the sheer amount of learning at the planetary,

541
00:52:38,960 --> 00:52:44,400
like, species level that happened, if you think of the amount of parallelism, years, etc., that,

542
00:52:44,400 --> 00:52:51,200
you know, it got us to be intelligent beings. So just the existing proof tells me that scaling

543
00:52:51,200 --> 00:52:57,360
probably will be part of the solution, but it's not the only thing that will be, like, required,

544
00:52:57,360 --> 00:53:03,840
right? So, anyways, the panel, I guess, no more spoilers, but it touches all these very interesting

545
00:53:03,840 --> 00:53:08,800
questions and a lot of them, of course, are inspired indeed by what we were discussing before on

546
00:53:08,800 --> 00:53:14,640
scaling up language models and their foundational model capabilities that they seem to exhibit,

547
00:53:14,640 --> 00:53:21,440
and, you know, if that is true, like, when is the time to think very carefully about how to,

548
00:53:21,440 --> 00:53:27,280
you know, get access to the community, similar to the hardware that was being made accessible

549
00:53:27,280 --> 00:53:32,320
with CERN or another example is obviously the Hubble telescope, right? Very, you know,

550
00:53:32,320 --> 00:53:37,600
huge endeavors from to build those things. So, yeah, very interesting questions. I don't know if

551
00:53:37,600 --> 00:53:42,160
you have any thoughts or any solutions to these either, or what do you think? You know, my,

552
00:53:44,320 --> 00:53:50,320
what occurred to me was when you talked about kind of humans as the existence proof of the

553
00:53:50,320 --> 00:53:57,120
requirement for scale, it prompted me to think about, I think we've demonstrated that

554
00:53:58,160 --> 00:54:03,680
scale is required to advance knowledge, but it's not clear that, you know, that that's the same

555
00:54:03,680 --> 00:54:12,480
as advancing intelligence. And, you know, certainly, you know, there's, you know, there's a degree

556
00:54:12,480 --> 00:54:19,200
to knowledge that becomes kind of common knowledge and, you know, we take for granted,

557
00:54:21,280 --> 00:54:28,640
but yeah, it's not obvious the impact of, you know, our scale is a species on our intelligence

558
00:54:28,640 --> 00:54:35,040
per se. I mean, I guess you could argue that our scale as a species has, you know,

559
00:54:35,040 --> 00:54:40,720
facilitated, you know, for example, nutrition, like, you know, we've, we've industrialized farming

560
00:54:40,720 --> 00:54:45,280
and we become stronger and that's, you know, made our brains bigger and, you know, that's,

561
00:54:46,560 --> 00:54:54,320
that has advanced intelligence significantly, but I'm not sure that that's the same kind of scale

562
00:54:54,320 --> 00:55:01,200
argument. Yeah, I really like, by the way, your, your, yeah, your knowledge. I think definitely

563
00:55:01,200 --> 00:55:06,960
from a knowledge standpoint, I mean, we, we store all the knowledge in different formats,

564
00:55:06,960 --> 00:55:13,520
right? Like, over time, without which, you know, things would be much harder. So even in that sense,

565
00:55:15,840 --> 00:55:22,320
accessing all the knowledge and learning how to access that feels like a natural thing we need

566
00:55:22,320 --> 00:55:28,400
to investigate from a machine learning standpoint. And what maybe, you know, these language models

567
00:55:28,400 --> 00:55:35,040
are getting at the very first stages of they know, or they have the knowledge in an imperfect way,

568
00:55:35,040 --> 00:55:41,200
et cetera, but they have the knowledge that exists in a particular corpus. And, but then, yeah,

569
00:55:41,200 --> 00:55:46,560
from there to intelligence, I agree. Like, that's what I'm saying. I think it's part of the puzzle,

570
00:55:46,560 --> 00:55:53,680
but definitely not the whole puzzle indeed. Yeah, awesome, awesome. Well, Ariel, it has been

571
00:55:53,680 --> 00:55:59,280
wonderful catching up, chatting a little bit about all the things you're working on,

572
00:55:59,280 --> 00:56:07,680
particularly with regard to nirips. You are also involved in a new to machine learning panel.

573
00:56:07,680 --> 00:56:12,160
We're not going to have a chance to talk about that, but for those who are new to the field,

574
00:56:12,160 --> 00:56:17,760
I encourage you to seek out that panel. And I'm sure there'll be lots of interesting tidbits to

575
00:56:17,760 --> 00:56:23,520
learn from there. Thanks so much, Ariel, for taking the time to chat. Excellent. Sam, it's been a

576
00:56:23,520 --> 00:56:28,800
pleasure over, long overdue and looking forward to him back in a few years. And we'll see if we change

577
00:56:28,800 --> 00:56:35,200
the discussion topics or it will, it will be like about the same, the same line of thinking about

578
00:56:35,200 --> 00:56:40,080
scaling up and so on. But the field is fascinating. So looking forward to our next chat. Absolutely,

579
00:56:40,080 --> 00:56:42,080
thanks so much. Thank you.

