1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,600
I'm your host, Sam Charrington.

4
00:00:31,600 --> 00:00:36,680
In this episode of our deep learning endoba series, we're joined by Nila Murray, senior research

5
00:00:36,680 --> 00:00:41,240
scientist and group lead in the computer vision group at Naver Labs Europe.

6
00:00:41,240 --> 00:00:44,840
Nila presented at the endoba on computer vision.

7
00:00:44,840 --> 00:00:49,880
In this discussion, we explore her work on visual attention, including why visual attention

8
00:00:49,880 --> 00:00:54,320
is important and the trajectory of work in the field over time.

9
00:00:54,320 --> 00:00:59,520
We also discuss her paper Generalize Max Pooling and her recent research interests in learning

10
00:00:59,520 --> 00:01:02,880
representations with deep learning.

11
00:01:02,880 --> 00:01:07,000
Before we jump in, I'd like to send a big shout out to our friends at Google AI for their

12
00:01:07,000 --> 00:01:10,800
support of the podcast and their sponsorship of this series.

13
00:01:10,800 --> 00:01:16,920
Google AI recently opened up applications for their 2019 residency program.

14
00:01:16,920 --> 00:01:21,720
The Google AI residency is a one year machine learning research training program, with the

15
00:01:21,720 --> 00:01:26,760
goal of helping individuals become successful machine learning researchers.

16
00:01:26,760 --> 00:01:31,280
The program seeks residents from a very diverse set of educational and professional backgrounds

17
00:01:31,280 --> 00:01:35,640
from all over the world, so if you think that this is something that interests you, you

18
00:01:35,640 --> 00:01:37,880
should definitely apply.

19
00:01:37,880 --> 00:01:42,640
Find out more about the program at g.co slash AI residency.

20
00:01:42,640 --> 00:01:47,720
And now on to the show.

21
00:01:47,720 --> 00:01:48,720
All right, everyone.

22
00:01:48,720 --> 00:01:50,520
I am on the line with Nila Murray.

23
00:01:50,520 --> 00:01:55,960
Nila is a senior research scientist and group lead in the computer vision group at Naver

24
00:01:55,960 --> 00:01:56,960
Labs Europe.

25
00:01:56,960 --> 00:01:59,560
Nila, welcome to this week in machine learning and AI.

26
00:01:59,560 --> 00:02:00,560
Thanks so much.

27
00:02:00,560 --> 00:02:01,560
I'm happy to be here.

28
00:02:01,560 --> 00:02:04,080
I'm happy to have you on as well.

29
00:02:04,080 --> 00:02:09,960
And before we jump into the kind of heart of the conversation, I'd like to have the audience

30
00:02:09,960 --> 00:02:14,120
get to know you a little bit, you did your PhD in Barcelona, Spain.

31
00:02:14,120 --> 00:02:16,040
Tell us about your focus there.

32
00:02:16,040 --> 00:02:17,040
Sure.

33
00:02:17,040 --> 00:02:22,280
So I did my PhD at the University of Tata AutÃ³noma de Barcelona, so specifically in the

34
00:02:22,280 --> 00:02:26,120
computer vision center in Barcelona.

35
00:02:26,120 --> 00:02:30,960
And so the work I did there was very focused on subjective vision.

36
00:02:30,960 --> 00:02:36,560
So this is the problem of being able to model subjective properties of human vision.

37
00:02:36,560 --> 00:02:41,800
So that's things like, in particular, was focused on visual attention, in particular,

38
00:02:41,800 --> 00:02:44,680
bottom-up attention or what we might call saliency.

39
00:02:44,680 --> 00:02:48,920
So this is really understanding if you're given some kind of, if an individual is presented

40
00:02:48,920 --> 00:02:54,760
with a visual stimulus, let's see an image or a video, how can we model which regions

41
00:02:54,760 --> 00:02:57,880
of that visual stimulus would attract the user's attention.

42
00:02:57,880 --> 00:03:01,760
And then we can have various degrees of attention that we can predict.

43
00:03:01,760 --> 00:03:04,080
So that was one focus of my research through my PhD.

44
00:03:04,080 --> 00:03:06,440
Another focus was on visual aesthetics.

45
00:03:06,440 --> 00:03:11,000
This is another, let's say, subjective visual experience where you are trying to model

46
00:03:11,000 --> 00:03:14,520
or at least trying to understand or predict, you know, to what degree with a specific visual

47
00:03:14,520 --> 00:03:20,040
stimulus, let's say, an image or a video be considered visually appealing to someone.

48
00:03:20,040 --> 00:03:23,400
And so you can consider different ways of approaching that.

49
00:03:23,400 --> 00:03:27,960
There are some that are, let's say, some approaches that are a bit more in the computational

50
00:03:27,960 --> 00:03:33,120
neuroscience perspective, really taking a sort of biologically inspired approach.

51
00:03:33,120 --> 00:03:38,200
And I did investigate that to some degree, and they're also much more data-driven approaches.

52
00:03:38,200 --> 00:03:42,360
So things that would be, you know, using machine learning techniques.

53
00:03:42,360 --> 00:03:46,880
So I investigated both aspects through my PhD.

54
00:03:46,880 --> 00:03:52,840
And during your PhD, you did a stint with Xerox Research Lab in Europe.

55
00:03:52,840 --> 00:03:58,680
You went there after your PhD, and without leaving, you ended up at Naver Lab's Europe.

56
00:03:58,680 --> 00:04:00,560
What's tell us about that whole story?

57
00:04:00,560 --> 00:04:03,600
Sure, so it's very interesting time.

58
00:04:03,600 --> 00:04:09,760
So in fact, I was a visiting researcher at Xerox Research Center Europe, XRC, for those

59
00:04:09,760 --> 00:04:12,040
in the know, during my PhD.

60
00:04:12,040 --> 00:04:16,000
So actually, I was visiting them and collaborating with them as a PhD student.

61
00:04:16,000 --> 00:04:21,480
So after my PhD, you know, I had a great time at Xerox and so I decided to stay on as

62
00:04:21,480 --> 00:04:23,120
a research scientist.

63
00:04:23,120 --> 00:04:29,840
And eventually, I started leading the computer vision team at Xerox and at a certain

64
00:04:29,840 --> 00:04:38,200
point last year, Xerox decided to transition, and you know, we were looking for acquirers

65
00:04:38,200 --> 00:04:39,600
for the lab.

66
00:04:39,600 --> 00:04:45,040
And so, Naver Labs, and so I can get in a little bit into what, you know, who are we as

67
00:04:45,040 --> 00:04:46,040
Naver Labs?

68
00:04:46,040 --> 00:04:50,320
Naver Labs acquired us eventually, acquired XRC.

69
00:04:50,320 --> 00:04:55,480
And so now XRC is now known as NLE, or Naver Labs Europe.

70
00:04:55,480 --> 00:04:57,640
And so the institute is the same, the people are the same.

71
00:04:57,640 --> 00:05:02,840
But now we're under the umbrella of our new home, which is Naver Labs.

72
00:05:02,840 --> 00:05:04,400
And Naver Labs does what?

73
00:05:04,400 --> 00:05:09,800
Okay, so this is like an unintended, unintended story.

74
00:05:09,800 --> 00:05:12,160
So let me start off with Naver Corporation.

75
00:05:12,160 --> 00:05:16,200
So our parent company is Naver Corporation and this is a South Korean Internet technology

76
00:05:16,200 --> 00:05:17,200
company.

77
00:05:17,200 --> 00:05:21,320
We're a very dominant company in South Korea and then for different services in other parts

78
00:05:21,320 --> 00:05:24,800
of South and Southeast Asia and East Asia.

79
00:05:24,800 --> 00:05:28,320
So we're very well known, we're very dominant in search, for example, in web search and

80
00:05:28,320 --> 00:05:31,960
mobile search in South Korea and then we have many other services.

81
00:05:31,960 --> 00:05:38,120
We have a lot of internet and web services for things like for translation, for navigation,

82
00:05:38,120 --> 00:05:39,640
for maps, for blogging.

83
00:05:39,640 --> 00:05:44,600
We have more than 100 services that we provide over the web of the internet over mobile

84
00:05:44,600 --> 00:05:45,600
phones.

85
00:05:45,600 --> 00:05:50,280
We also, we own line corporation, for example, which is a very popular messaging app as

86
00:05:50,280 --> 00:05:51,280
well.

87
00:05:51,280 --> 00:05:52,800
So this is Naver Corporation.

88
00:05:52,800 --> 00:05:58,440
And so Naver Labs is actually a spin-off of Naver Corporation.

89
00:05:58,440 --> 00:06:05,000
And so Naver Labs, what we are, is we're a company with a vision of ambient intelligence.

90
00:06:05,000 --> 00:06:11,240
So what this means is we are interested in having services that are intelligent and that

91
00:06:11,240 --> 00:06:16,200
take context into account when it's deciding how to best provide some kind of service.

92
00:06:16,200 --> 00:06:20,280
So that may mean understanding where you are, understanding what you're doing and using

93
00:06:20,280 --> 00:06:24,360
that to provide you different services related to, for example, mobility or entertainment

94
00:06:24,360 --> 00:06:26,760
or communications, things like that.

95
00:06:26,760 --> 00:06:28,440
So that's the vision of Naver Labs.

96
00:06:28,440 --> 00:06:33,760
And as a result of that, we do a lot of research on things like of computer vision, natural

97
00:06:33,760 --> 00:06:38,040
language processing, you know, recommendation systems, a lot of machine learning tasks in

98
00:06:38,040 --> 00:06:44,320
general and also applications like robotics, autonomous driving and quite a few other things.

99
00:06:44,320 --> 00:06:49,920
You recently returned from the deep learning in Daba and in fact this interview will be

100
00:06:49,920 --> 00:06:56,240
published as part of our deep learning in Daba series where you presented an overview

101
00:06:56,240 --> 00:06:59,440
of tutorial on CNNs.

102
00:06:59,440 --> 00:07:02,840
I'm curious about your experience at the in Daba.

103
00:07:02,840 --> 00:07:09,600
Had you been to the in Daba event previously and tell me a little bit about your experience

104
00:07:09,600 --> 00:07:10,600
there?

105
00:07:10,600 --> 00:07:11,600
Sure.

106
00:07:11,600 --> 00:07:15,760
So I hadn't been previously as far as I understand the first, this is the second edition

107
00:07:15,760 --> 00:07:19,200
of the deep learning in Daba and I wasn't there last year.

108
00:07:19,200 --> 00:07:23,840
I was really happy to be invited to give to give this overview of convolutional models

109
00:07:23,840 --> 00:07:28,640
and I have to say it was really a great experience, you know, on all dimensions.

110
00:07:28,640 --> 00:07:30,800
In terms of the organization, it was great.

111
00:07:30,800 --> 00:07:36,640
In terms of the group of students and participants that the organizers put together, they were

112
00:07:36,640 --> 00:07:40,360
really a very engaged, motivated audience.

113
00:07:40,360 --> 00:07:43,600
I was a bit, you never know when you're giving a talk how engaged the audience is going

114
00:07:43,600 --> 00:07:50,320
to be, you know, we got so many questions during the talk, which is always great after

115
00:07:50,320 --> 00:07:51,320
the talk.

116
00:07:51,320 --> 00:07:55,600
A lot of questions as well, a lot of students coming up to me afterwards saying, oh, you

117
00:07:55,600 --> 00:08:00,040
know, I saw this, you spoke about this specific kind of flavor of a confnet.

118
00:08:00,040 --> 00:08:01,560
Could do you think it could be use in my case?

119
00:08:01,560 --> 00:08:02,880
Here's what I'm trying to do.

120
00:08:02,880 --> 00:08:06,360
So people are really, really willing to explain to describe their problems, to look for advice,

121
00:08:06,360 --> 00:08:08,520
to ask questions, to share knowledge.

122
00:08:08,520 --> 00:08:12,600
So I thought the general spirits of the DL and Daba was great.

123
00:08:12,600 --> 00:08:16,760
It was really, I think, in the best spirit of conferences, in the sense that people

124
00:08:16,760 --> 00:08:21,400
are really there to share knowledge and to find collaborations and things like that.

125
00:08:21,400 --> 00:08:24,120
So it was a very nice spirit.

126
00:08:24,120 --> 00:08:26,600
I think I also learned a lot listening to some of the other talks.

127
00:08:26,600 --> 00:08:32,600
They were great talks by people like Katja Hoffman and David Silver and others, many others,

128
00:08:32,600 --> 00:08:34,840
where I learned quite a bit of stuff myself too.

129
00:08:34,840 --> 00:08:37,280
So in all knowledge, it was really wonderful.

130
00:08:37,280 --> 00:08:39,080
Oh, fantastic.

131
00:08:39,080 --> 00:08:47,160
And so your recent work has been focused on areas like visual attention and learning different

132
00:08:47,160 --> 00:08:48,880
representations for visual search.

133
00:08:48,880 --> 00:08:54,280
Let's dive into some of your recent research.

134
00:08:54,280 --> 00:08:55,280
Visual attention.

135
00:08:55,280 --> 00:08:59,760
When we talk about visual attention, is it related at all to, you know, we talk about

136
00:08:59,760 --> 00:09:02,120
attention within models?

137
00:09:02,120 --> 00:09:03,440
What are the connections between those?

138
00:09:03,440 --> 00:09:04,960
Or is it just a naming collision?

139
00:09:04,960 --> 00:09:05,960
Sure.

140
00:09:05,960 --> 00:09:07,760
So that's a great question.

141
00:09:07,760 --> 00:09:11,880
I would say there's an intersection, but it's not a complete overlap.

142
00:09:11,880 --> 00:09:17,240
So visual attention has a very specific meaning in the neuroscience community and in other

143
00:09:17,240 --> 00:09:18,240
communities, right?

144
00:09:18,240 --> 00:09:24,440
It's not something that computer vision people came up with.

145
00:09:24,440 --> 00:09:25,920
But there is some overlap.

146
00:09:25,920 --> 00:09:30,480
Basically, when I say visual attention in the context of my research, what I really mean

147
00:09:30,480 --> 00:09:38,760
is human attention, let's say that's given, or that's a result of the human visual system.

148
00:09:38,760 --> 00:09:43,680
And you can think of even higher up visual processes and cognitive processes.

149
00:09:43,680 --> 00:09:48,360
The idea being that, so for example, to give you a typical setup, in terms of what we're

150
00:09:48,360 --> 00:09:52,120
trying to model, let's say, for example, if you sit a human, you sit somebody in front

151
00:09:52,120 --> 00:09:58,520
of like a computer screen, you show them an image, and then you have an eye tracker.

152
00:09:58,520 --> 00:10:05,200
And the eye tracker is tracking exactly where fixations are landing on that image, right?

153
00:10:05,200 --> 00:10:10,320
And the goal of a visual attention model will be to predict that, to be able to say, given

154
00:10:10,320 --> 00:10:14,240
this image, I think that, let's say the average person, because of course, there's a lot of

155
00:10:14,240 --> 00:10:19,080
subjectivity that goes into this, but the average person would pay more attention to this

156
00:10:19,080 --> 00:10:20,080
part or to that part.

157
00:10:20,080 --> 00:10:23,600
Or I think that part's going to gather a fair bit of attention.

158
00:10:23,600 --> 00:10:26,600
Or this is a degree to which I think it would have that amount of attention.

159
00:10:26,600 --> 00:10:30,360
So that's what I mean when I say visual attention, and that's the type of research I did.

160
00:10:30,360 --> 00:10:36,360
But in a lot of CNNs now, and let's say general deep learning models, it's used in a very

161
00:10:36,360 --> 00:10:40,160
similar sense, the idea being that, but it's not necessarily human driven, right?

162
00:10:40,160 --> 00:10:45,040
It's basically saying, what does the algorithm, what does a model basically think needs to

163
00:10:45,040 --> 00:10:49,360
be attended to in whatever stimulus you're talking about?

164
00:10:49,360 --> 00:10:53,960
And that can be a visual stimulus, it can be textual data, it can mean many other things.

165
00:10:53,960 --> 00:10:58,600
So it's not necessarily human driven, but the concept is similar in the sense that there's

166
00:10:58,600 --> 00:11:00,720
a lot of information.

167
00:11:00,720 --> 00:11:03,760
What information is relevant for me to make whatever decision I'm trying to make?

168
00:11:03,760 --> 00:11:06,040
Why do we want to study visual attention?

169
00:11:06,040 --> 00:11:12,320
Yeah, you could step back and think about it as more like a bug than a feature or limitation

170
00:11:12,320 --> 00:11:13,320
feature, right?

171
00:11:13,320 --> 00:11:20,320
It's like we, as humans, have this limited ability to focus due to a variety of kind of

172
00:11:20,320 --> 00:11:26,760
neurophysical limitations the way we're wired, but a computer could do more, right?

173
00:11:26,760 --> 00:11:30,880
Why do we need to worry about focusing on a specific part of an image?

174
00:11:30,880 --> 00:11:36,560
You could say that, but I actually think that visual attention is kind of a miracle.

175
00:11:36,560 --> 00:11:42,280
I wouldn't say it's a miracle, but it's actually a very wonderful feature of the human, let's

176
00:11:42,280 --> 00:11:43,640
say human cognition.

177
00:11:43,640 --> 00:11:47,680
In the sense that there is a vast amount of information that we're taking in every time

178
00:11:47,680 --> 00:11:51,080
we just look around us, right, every time we look around us, every time we see.

179
00:11:51,080 --> 00:11:55,680
If you think of the amount of data that you need to store it to just store like an hour

180
00:11:55,680 --> 00:12:03,240
of video and just imagine that we're just like having this be input into our eyes constantly

181
00:12:03,240 --> 00:12:07,160
as when we're awake and when we're looking around, there's a lot of information and it's

182
00:12:07,160 --> 00:12:11,160
not rather than think that, okay, it'd be great if we could use all of this, frankly,

183
00:12:11,160 --> 00:12:16,080
I mean, it's very smart for our visual system to say, a lot of this is not even necessary.

184
00:12:16,080 --> 00:12:17,080
Why would I waste my time?

185
00:12:17,080 --> 00:12:22,080
Why would I have waste my energy of my brain to process this stuff that's not even needed

186
00:12:22,080 --> 00:12:24,280
for whatever time of task I'm trying to do?

187
00:12:24,280 --> 00:12:29,240
The human brain is very computationally efficient and that's something that we struggle a lot

188
00:12:29,240 --> 00:12:33,200
with in computation, especially in deep learning right now.

189
00:12:33,200 --> 00:12:36,040
I think it's actually something very fascinating and something that we would love to be able

190
00:12:36,040 --> 00:12:43,480
to replicate in, let's say, machines and computer systems as well.

191
00:12:43,480 --> 00:12:44,480
That's just to start off with.

192
00:12:44,480 --> 00:12:48,040
I really think, I find it really fascinating the ability we have to be able to really

193
00:12:48,040 --> 00:12:50,640
attend to things that matter.

194
00:12:50,640 --> 00:12:53,960
So that's one reason why I think it's interesting, just because if we could replicate that, I

195
00:12:53,960 --> 00:12:57,520
think it would serve as a lot of computational power and many other things, right?

196
00:12:57,520 --> 00:13:03,520
We know that right now, for example, there's a lot of issues with environmental concerns,

197
00:13:03,520 --> 00:13:04,520
for example.

198
00:13:04,520 --> 00:13:07,360
How do you cool all these, like, GPUs that we're all using and things like this?

199
00:13:07,360 --> 00:13:11,560
So if we could manage to replicate that, that'd actually be pretty cool.

200
00:13:11,560 --> 00:13:15,760
And then, so another reason we're interested in it is just for purely, let's say, scientific

201
00:13:15,760 --> 00:13:18,280
purpose to really understand how the brain works.

202
00:13:18,280 --> 00:13:19,280
So that's one thing.

203
00:13:19,280 --> 00:13:22,080
And actually, a lot of this research came out of the neuroscience community, right?

204
00:13:22,080 --> 00:13:24,480
Like, people who really want to understand how does the brain work.

205
00:13:24,480 --> 00:13:26,080
That's just a fundamental question.

206
00:13:26,080 --> 00:13:27,720
And attention is a big part of that.

207
00:13:27,720 --> 00:13:31,480
Not just visual attention, but also attention to audio signals.

208
00:13:31,480 --> 00:13:34,840
And then also just from for computational reasons, I just gave you a few, right?

209
00:13:34,840 --> 00:13:42,720
It's really good for compression purposes and for just general computational efficiency.

210
00:13:42,720 --> 00:13:47,880
Can you scope out the landscape of visual attention?

211
00:13:47,880 --> 00:13:52,400
What's the research trajectory been in this field over time?

212
00:13:52,400 --> 00:13:57,720
It sounds like it's been something we've been looking at for a long time from the perspective

213
00:13:57,720 --> 00:13:59,640
of multiple disciplines.

214
00:13:59,640 --> 00:14:06,160
How is our understanding of how to replicate visual attention and machines evolved over

215
00:14:06,160 --> 00:14:07,160
time?

216
00:14:07,160 --> 00:14:08,160
Sure.

217
00:14:08,160 --> 00:14:11,160
So, yeah, I can try to take you through a very brief overview.

218
00:14:11,160 --> 00:14:16,880
So visual attention, let's say computational models have been around for many, many decades.

219
00:14:16,880 --> 00:14:20,160
There are some very seminal ones.

220
00:14:20,160 --> 00:14:25,200
So for example, there's one by Triesman et al, which is related to what we call feature

221
00:14:25,200 --> 00:14:26,640
integration theory.

222
00:14:26,640 --> 00:14:31,240
And then from there, there have been some works by professors such as Laurent Eti and a

223
00:14:31,240 --> 00:14:36,760
lot of his students who've worked on things like really taking an image and trying to decompose

224
00:14:36,760 --> 00:14:41,120
that image into some sort of compressed representation.

225
00:14:41,120 --> 00:14:44,560
The idea being that if you have an image, a lot of, there's a lot of redundancy in images,

226
00:14:44,560 --> 00:14:45,560
right?

227
00:14:45,560 --> 00:14:46,560
We all know that.

228
00:14:46,560 --> 00:14:50,120
So images can be very highly compressed and anybody who's used, you know, JPEP compression

229
00:14:50,120 --> 00:14:52,920
or any other types of compression techniques know that.

230
00:14:52,920 --> 00:14:59,080
And the idea is that there's been a long history of work thinking about the fact that to understand

231
00:14:59,080 --> 00:15:03,760
visual attention, we have to think about what can be compressed and what cannot be in order

232
00:15:03,760 --> 00:15:06,080
to retain the same information in the image.

233
00:15:06,080 --> 00:15:11,480
So that's been like a big theme throughout a lot of visual attention research.

234
00:15:11,480 --> 00:15:17,520
In early days, let's say a lot of things was very, very much focused on trying to come

235
00:15:17,520 --> 00:15:22,360
up with, let's say from first principles, first principles of neuroscience about, for

236
00:15:22,360 --> 00:15:26,520
example, how the brain works, how the visual cortex works, how the primary cortex works,

237
00:15:26,520 --> 00:15:30,440
et cetera, trying to, from that, come up with a model.

238
00:15:30,440 --> 00:15:34,040
More recently, people have gone into very much data-driven approaches.

239
00:15:34,040 --> 00:15:39,040
So by data-driven, I mean things like, you know, you have some sort of data set that

240
00:15:39,040 --> 00:15:40,040
you collect.

241
00:15:40,040 --> 00:15:46,080
So a typical data set that is used in this field is a data set of images and corresponding

242
00:15:46,080 --> 00:15:50,160
fixations from different, different individuals, different viewers.

243
00:15:50,160 --> 00:15:54,280
The data-driven approach would basically say, okay, I have some sort of model, and this

244
00:15:54,280 --> 00:15:58,280
model can be a deep learning model, it can be some other type of model, and I want to

245
00:15:58,280 --> 00:16:04,600
optimize this model such that it's able to predict as best as possible the these fixations.

246
00:16:04,600 --> 00:16:10,800
So basically to give some sort of, for example, a probability score for given pixels, what's

247
00:16:10,800 --> 00:16:14,120
a probability that a fixation would land on that pixel?

248
00:16:14,120 --> 00:16:20,520
So a lot of work on this field has used things like image decomposition methods, for example,

249
00:16:20,520 --> 00:16:23,880
using wavelet theory, using wavelet decomposition.

250
00:16:23,880 --> 00:16:27,800
More recently, it's gone, you know, a lot of the research has gone into deep learning.

251
00:16:27,800 --> 00:16:33,320
So for example, some of the recent research that I did used a very generic, let's say,

252
00:16:33,320 --> 00:16:36,240
visual backbone, you can say, visual front end.

253
00:16:36,240 --> 00:16:42,480
So for example, a ResNet model, and then for example, train the ResNet model architecture

254
00:16:42,480 --> 00:16:46,440
for the final layer that gives you a prediction on a pixel basis, saying, okay, given this

255
00:16:46,440 --> 00:16:49,920
image, what's the probability of this pixel being fixated upon?

256
00:16:49,920 --> 00:16:57,320
And so your specific work in this field, what were some of the papers that you've done

257
00:16:57,320 --> 00:16:58,640
in this area?

258
00:16:58,640 --> 00:17:03,000
So the first, the first work I did on this, so this was before the deep learning era,

259
00:17:03,000 --> 00:17:10,360
if we can put it that way, was really based on using a psychophysical model, a vision.

260
00:17:10,360 --> 00:17:15,720
So this model was really a model that was developed initially to predict color perception.

261
00:17:15,720 --> 00:17:17,880
So how do we perceive color in images?

262
00:17:17,880 --> 00:17:22,920
And then we adapted this model to predict visual attention.

263
00:17:22,920 --> 00:17:25,400
More specifically saliency.

264
00:17:25,400 --> 00:17:29,160
So maybe before I get into this, I can give a little bit of a distinction between the

265
00:17:29,160 --> 00:17:30,240
two things.

266
00:17:30,240 --> 00:17:35,680
So when we talk about visual attention, this is something that's extremely complex,

267
00:17:35,680 --> 00:17:40,840
let's say process in cognition, it involves many, let's say, what we might call low-level

268
00:17:40,840 --> 00:17:44,080
cues and also many top-down cues.

269
00:17:44,080 --> 00:17:46,760
By that, what I mean is that typically when you're looking around you and you're paying

270
00:17:46,760 --> 00:17:49,600
attention to things, you have some sort of purpose in mind.

271
00:17:49,600 --> 00:17:52,720
So for example, you might be reading or you might be looking at a movie or you might

272
00:17:52,720 --> 00:17:57,120
be searching for something on your desk, and that impacts a lot where you look.

273
00:17:57,120 --> 00:17:59,320
So this is why it's very subjective.

274
00:17:59,320 --> 00:18:05,080
So there's a component of attention, though, that's been traditionally called saliency,

275
00:18:05,080 --> 00:18:06,840
and this is something that people call more bottom-up.

276
00:18:06,840 --> 00:18:12,680
So what that means is it's something that's more related to things like textures, like

277
00:18:12,680 --> 00:18:17,640
low-level textures and colors and things that aren't really related to the task at hand.

278
00:18:17,640 --> 00:18:21,400
So what I may mean by that, for example, is even though you're looking at your desk and

279
00:18:21,400 --> 00:18:25,600
you're trying to find something, if there are certain things that are just, let's say,

280
00:18:25,600 --> 00:18:30,320
fundamentally salient for lack of a better word in your desk.

281
00:18:30,320 --> 00:18:34,320
So there are many patterns that have been found that have this property.

282
00:18:34,320 --> 00:18:35,680
So this was more what I was focused on.

283
00:18:35,680 --> 00:18:38,960
This aspect of really a bottom-up saliency or low-level saliency.

284
00:18:38,960 --> 00:18:46,640
Is it fair to draw this distinction along the lines of bottom-up or saliency is specific

285
00:18:46,640 --> 00:18:54,280
to the content of, let's say, an image, whereas top-down is more contextual or related to

286
00:18:54,280 --> 00:18:56,920
the goal of the observer, or is that too simple?

287
00:18:56,920 --> 00:19:01,240
Yeah, I think that's a fair distinction to make, exactly.

288
00:19:01,240 --> 00:19:06,280
So we would say that, and this is why I think if you're trying to model attention, many

289
00:19:06,280 --> 00:19:09,760
people have started off trying to model what we call bottom-up, bottom-up or low-level

290
00:19:09,760 --> 00:19:15,240
attention, because that's the sort of thing that's really, it's less task-dependent,

291
00:19:15,240 --> 00:19:16,240
right?

292
00:19:16,240 --> 00:19:19,480
It really sort of depends on just the fundamentals of the visual stimulus that you have.

293
00:19:19,480 --> 00:19:24,000
And so that weighs a bit more, there's a bit more continuity of that coherence in between

294
00:19:24,000 --> 00:19:25,000
different viewers.

295
00:19:25,000 --> 00:19:28,000
When you start to get into things like looking at different tasks, it can be very subjective

296
00:19:28,000 --> 00:19:29,320
and very different.

297
00:19:29,320 --> 00:19:34,160
So your specific research was taking a kind of a neuro-physical model, is that how you

298
00:19:34,160 --> 00:19:35,160
described it?

299
00:19:35,160 --> 00:19:43,280
So that was my initial work on this, which took a psychophysical model, and it attempted

300
00:19:43,280 --> 00:19:50,640
to modify it to better predict, let's say, visual saliency and in particular to predict

301
00:19:50,640 --> 00:19:53,560
visual fixations, so eye fixations.

302
00:19:53,560 --> 00:19:59,120
So what that meant is that you take a data set with these fixations that were recorded

303
00:19:59,120 --> 00:20:03,320
by different observers, and you try to really replicate that with your model.

304
00:20:03,320 --> 00:20:09,320
So later on, in more recent years, I've looked at a more data-driven approach with some

305
00:20:09,320 --> 00:20:10,840
learning involved.

306
00:20:10,840 --> 00:20:17,680
So in this case, what we did was, we really focused on, so this is work I did with students

307
00:20:17,680 --> 00:20:21,400
at MindSomniaJetly and also a colleague of mine, Noravig.

308
00:20:21,400 --> 00:20:27,400
And so for this work, we focused on really trying to understand what kind of objective functions

309
00:20:27,400 --> 00:20:31,760
would be appropriate to learn, to learn a model of saliency.

310
00:20:31,760 --> 00:20:37,200
And so we modeled saliency as a probability distribution.

311
00:20:37,200 --> 00:20:41,920
So we said, okay, we want to really predict the probability of fixations, we consider,

312
00:20:41,920 --> 00:20:47,440
we consider all our pixels in our image to be potential fixations, and we try to figure

313
00:20:47,440 --> 00:20:51,920
okay, what would be like the best objective function to learn to train this model.

314
00:20:51,920 --> 00:20:56,240
And so we consider different ones, we propose some new ones, basically, that we're trying

315
00:20:56,240 --> 00:20:58,480
to really capture this property of probability.

316
00:20:58,480 --> 00:21:03,360
And so what were the field of objective functions that you considered?

317
00:21:03,360 --> 00:21:08,880
So very much those related to capturing differences between, or let's say, to quantify differences

318
00:21:08,880 --> 00:21:10,680
between probability distributions.

319
00:21:10,680 --> 00:21:15,160
So they're, for example, we try things like, like, kale divergences, gents and shanen

320
00:21:15,160 --> 00:21:19,760
differences, kite divergences, and then many others.

321
00:21:19,760 --> 00:21:26,560
So then, basically, we, we, finally, we proposed like an objective function that's basically

322
00:21:26,560 --> 00:21:31,320
a combination of a soft smack function, which will allow you to have a valid probability

323
00:21:31,320 --> 00:21:37,240
distribution, then to apply different types of divergences, probability divergences on

324
00:21:37,240 --> 00:21:38,240
top of that.

325
00:21:38,240 --> 00:21:40,440
And we found that that worked that worked quite well.

326
00:21:40,440 --> 00:21:44,440
And in fact, we found that the bathe charia distance worked quite well.

327
00:21:44,440 --> 00:21:45,880
The what distance?

328
00:21:45,880 --> 00:21:50,160
So there's a, there's a, it's called, it's called a bathe charia distance.

329
00:21:50,160 --> 00:21:51,160
It's quite well known.

330
00:21:51,160 --> 00:21:52,760
It's been used in many, many fields.

331
00:21:52,760 --> 00:21:56,200
I think it could be used more, actually, than it is right now.

332
00:21:56,200 --> 00:21:59,960
But actually, the kale diverge is actually works also very well.

333
00:21:59,960 --> 00:22:02,920
We compared several ones, and then we found that these two were pretty good.

334
00:22:02,920 --> 00:22:10,200
So in the case of, I'm trying to map this to, like, simple Gaussian type distributions,

335
00:22:10,200 --> 00:22:17,200
where translating from one to the other is related to kind of the mean and the variance,

336
00:22:17,200 --> 00:22:24,240
do those correspond to correspond one to one, two terms, and these different, these different

337
00:22:24,240 --> 00:22:25,240
functions?

338
00:22:25,240 --> 00:22:26,240
No.

339
00:22:26,240 --> 00:22:27,240
So these are very generic.

340
00:22:27,240 --> 00:22:32,160
So basically, so, so these, they're not necessarily some sort of generative model.

341
00:22:32,160 --> 00:22:38,360
For example, it's really, you assume that you have for every, every value you have in

342
00:22:38,360 --> 00:22:42,640
your distribution, you're assuming that, you know, you know, what the value is, right?

343
00:22:42,640 --> 00:22:46,720
So you have, like, probability of X, one probability of X2, et cetera.

344
00:22:46,720 --> 00:22:50,960
So that can, that can be modeled, for example, with a GMM or with something else.

345
00:22:50,960 --> 00:22:53,800
But this is sort of agnostic to that, these types of distances.

346
00:22:53,800 --> 00:22:58,160
They're saying once you have some sort of probability distribution, this is, this distance

347
00:22:58,160 --> 00:23:02,160
is going to compute the difference between the two, but they can be modeled differently.

348
00:23:02,160 --> 00:23:03,160
All right.

349
00:23:03,160 --> 00:23:07,120
And so how do you use this ability to compute the distance between two distributions

350
00:23:07,120 --> 00:23:12,640
to help you figure out attention?

351
00:23:12,640 --> 00:23:18,320
So what we start off with, so our ground truth can be converted into a probability distribution.

352
00:23:18,320 --> 00:23:19,320
This is what we did.

353
00:23:19,320 --> 00:23:22,320
And when we're not the first people to do this, but this is something that's, that's, that's

354
00:23:22,320 --> 00:23:24,520
very, that has been done before.

355
00:23:24,520 --> 00:23:27,760
So for example, you start off with a set of fixations.

356
00:23:27,760 --> 00:23:34,360
So you can consider it, you can consider that you have an image and you know the, you

357
00:23:34,360 --> 00:23:39,040
know the X-Y coordinates of where somebody fixated on, on that image.

358
00:23:39,040 --> 00:23:47,440
Is your data set consist of one image and a large number of captured fixations from different

359
00:23:47,440 --> 00:23:52,920
observers based against that same image, or do you have a bunch of images each with their

360
00:23:52,920 --> 00:23:56,000
corresponding fixations?

361
00:23:56,000 --> 00:23:57,000
So both.

362
00:23:57,000 --> 00:24:01,680
We have multiple images, ideally, in the beginning we had very, we have very small data sets, but

363
00:24:01,680 --> 00:24:04,600
now we have pretty, pretty big ones.

364
00:24:04,600 --> 00:24:10,880
So we have multiple images and for each image you have multiple sets of fixations.

365
00:24:10,880 --> 00:24:14,600
So basically, for example, you might have like an image and you ask like 10 people to take

366
00:24:14,600 --> 00:24:17,920
a look at the image, you know, during, during a very small amount of time.

367
00:24:17,920 --> 00:24:20,040
So maybe like up to two seconds.

368
00:24:20,040 --> 00:24:22,520
And so then you capture these fixations.

369
00:24:22,520 --> 00:24:27,560
And normally what people do is they, they apply Gaussian blurring to that.

370
00:24:27,560 --> 00:24:31,240
So you can imagine you have like an image, which is full of zeros and let's say you have

371
00:24:31,240 --> 00:24:34,280
like ones at the locations of the fixations.

372
00:24:34,280 --> 00:24:37,400
And then you can apply Gaussian blurring to this and what you're going to get is you're

373
00:24:37,400 --> 00:24:40,200
going to get this resultant image, which is diffuse, right?

374
00:24:40,200 --> 00:24:44,280
So you have high values at the fixation points.

375
00:24:44,280 --> 00:24:48,800
And then these high values sort of diffuse in the immediate area of the fixation.

376
00:24:48,800 --> 00:24:54,400
So what you end up with is sort of like, it's not a, it's not sort of like a binary image,

377
00:24:54,400 --> 00:24:59,520
but you have some diffuse attention around the regions of fixation with the modes being

378
00:24:59,520 --> 00:25:01,400
at the fixation points.

379
00:25:01,400 --> 00:25:05,880
And so this can be what you have now is rather than have like an image full of zeros with

380
00:25:05,880 --> 00:25:10,520
a few, you know, with just a few points where there's some support of the distribution,

381
00:25:10,520 --> 00:25:14,600
you know, you've diffuse a distribution where you have basically some amount of non-zero

382
00:25:14,600 --> 00:25:16,440
support at all points.

383
00:25:16,440 --> 00:25:21,720
And if you normalize this resultant image appropriately, you can consider this image

384
00:25:21,720 --> 00:25:23,760
to be a probability distribution.

385
00:25:23,760 --> 00:25:28,520
And so then this becomes your ground truth and you just apply your typical machine learning

386
00:25:28,520 --> 00:25:31,080
framework and you say, okay, this is my ground truth.

387
00:25:31,080 --> 00:25:38,840
I have a model that takes us as inputs and image and produces some sort of predicted distribution.

388
00:25:38,840 --> 00:25:44,880
And then I compare the two using my probability distribution, my difference measure.

389
00:25:44,880 --> 00:25:47,000
And you backpropagate the loss, right?

390
00:25:47,000 --> 00:25:49,480
You backpropagate the difference that you see.

391
00:25:49,480 --> 00:25:57,840
And so why is that diffusion step key here as opposed to the more binary approach of looking

392
00:25:57,840 --> 00:25:59,600
at the fixations?

393
00:25:59,600 --> 00:26:06,600
As we have a somewhat subjective process, it's a little bit too strict of a task to ask

394
00:26:06,600 --> 00:26:10,960
the model to predict exactly the fixation point because you could imagine that the user

395
00:26:10,960 --> 00:26:14,560
could have very easily looked elsewhere and you can actually see this, right?

396
00:26:14,560 --> 00:26:18,480
Because if two observers are looking at the same image, you're not going to look at

397
00:26:18,480 --> 00:26:19,800
the exact same point.

398
00:26:19,800 --> 00:26:24,280
So therefore, if you have a location, so for example, you have an image and there's

399
00:26:24,280 --> 00:26:28,920
an image that contains a face, very likely there's going to be a ton of attention paid

400
00:26:28,920 --> 00:26:29,920
to the face.

401
00:26:29,920 --> 00:26:31,400
People like to look at faces.

402
00:26:31,400 --> 00:26:34,320
But they're not going to look at the same points, not everybody's going to focus exactly

403
00:26:34,320 --> 00:26:35,320
on the eye, right?

404
00:26:35,320 --> 00:26:40,720
So if you have ten people, you might see like a distribution of fixations very much on

405
00:26:40,720 --> 00:26:42,680
the face.

406
00:26:42,680 --> 00:26:46,360
But you can imagine very easily if you had another person, if you had an eleventh person

407
00:26:46,360 --> 00:26:48,480
that looked, they might look not exactly on that point, right?

408
00:26:48,480 --> 00:26:49,920
But they would look in the general area.

409
00:26:49,920 --> 00:26:54,240
So this is why it's nice to not use just the actual fixation that you have, but to sort

410
00:26:54,240 --> 00:27:01,320
of do this diffusion process where you basically apply gouchions to those regions and have

411
00:27:01,320 --> 00:27:03,800
this diffused distribution.

412
00:27:03,800 --> 00:27:12,160
And you're doing this diffusion process to each of your fixation images, if you will,

413
00:27:12,160 --> 00:27:19,640
as opposed to after aggregating or averaging across the different fixations for a particular

414
00:27:19,640 --> 00:27:20,640
image.

415
00:27:20,640 --> 00:27:21,640
Exactly.

416
00:27:21,640 --> 00:27:24,560
So there are many different ways to do this.

417
00:27:24,560 --> 00:27:27,120
You can see that there are different choices, right?

418
00:27:27,120 --> 00:27:29,640
And none of them are exactly correct, right?

419
00:27:29,640 --> 00:27:30,640
Right.

420
00:27:30,640 --> 00:27:35,080
So in this case, we're trying to solve like a pseudo problem, like an adjacent problem,

421
00:27:35,080 --> 00:27:36,840
because we're not actually solving the exact one, right?

422
00:27:36,840 --> 00:27:40,560
Because this is the way the subject of it comes into play.

423
00:27:40,560 --> 00:27:42,560
So yeah, there are different ways of doing it.

424
00:27:42,560 --> 00:27:51,200
And of course, when you're applying this gouchion blurring, the gouchion filter itself has

425
00:27:51,200 --> 00:27:52,200
its own parameters, right?

426
00:27:52,200 --> 00:27:55,800
So you have to decide how diffused do you want this to be?

427
00:27:55,800 --> 00:28:02,120
And that itself isn't a question that sort of left up to the specific researcher.

428
00:28:02,120 --> 00:28:07,120
And do you, in that sense, is it, it's not something you're learning.

429
00:28:07,120 --> 00:28:11,680
It's a hyperparameter that you're choosing the experimentation.

430
00:28:11,680 --> 00:28:17,080
Oh, it's, it's, it's not even a hyperparameter because this is really the ground truth you're

431
00:28:17,080 --> 00:28:18,920
setting at this point.

432
00:28:18,920 --> 00:28:23,360
So really, what many people have tried to do is to set this in a somewhat principled way

433
00:28:23,360 --> 00:28:29,560
by trying to look at things like, like peripheral vision and trying to understand, okay?

434
00:28:29,560 --> 00:28:34,640
If someone actually fixates what does that, how localized is that fixation really?

435
00:28:34,640 --> 00:28:39,920
And then you can kind of have a measure on what sort of local region is really being

436
00:28:39,920 --> 00:28:41,320
attended to.

437
00:28:41,320 --> 00:28:45,440
And there's a lot of psychophysical studies on that and that can, that can inform how

438
00:28:45,440 --> 00:28:47,400
diffused you want this to be.

439
00:28:47,400 --> 00:28:53,000
And so that's, are we just finishing up your, your first work in this, in this field?

440
00:28:53,000 --> 00:28:54,000
It sounds like it.

441
00:28:54,000 --> 00:28:57,400
Yeah, but we have a bit more to cover.

442
00:28:57,400 --> 00:28:59,200
So where'd you go next?

443
00:28:59,200 --> 00:29:03,280
You know, some of this work was done actually while I was at XRC now, NLE.

444
00:29:03,280 --> 00:29:09,720
Some other work I've done while here is related to, as I said, learning visual representations.

445
00:29:09,720 --> 00:29:13,480
So I also did a favourites of work on this once again before the deep learning era.

446
00:29:13,480 --> 00:29:19,480
So this was using handcrafted features and trying to understand how to, how to learn embeddings

447
00:29:19,480 --> 00:29:23,200
in an effective way for different tasks.

448
00:29:23,200 --> 00:29:26,680
So things like fine-grained recognition and also visual retrieval.

449
00:29:26,680 --> 00:29:31,120
So maybe I can start off with some of the work I did recently with colleagues from

450
00:29:31,120 --> 00:29:36,520
NREA and also colleagues here at NLE related to what we call aggregation.

451
00:29:36,520 --> 00:29:42,000
So this is also something that has become very well known in the deep learning context.

452
00:29:42,000 --> 00:29:46,880
What I mean by that in particular is, for example, people very often refer to the same

453
00:29:46,880 --> 00:29:49,280
sort of principle as pooling.

454
00:29:49,280 --> 00:29:52,920
So, you know, for example, average pooling or actually exactly.

455
00:29:52,920 --> 00:29:54,240
This is quite well known, right?

456
00:29:54,240 --> 00:29:59,240
So this is basically saying you have some amount of it local information or maybe not even

457
00:29:59,240 --> 00:30:03,720
not very localized information and you want to be able to summarise it somehow, right?

458
00:30:03,720 --> 00:30:07,680
And very often what you want to summarise might be different vectors.

459
00:30:07,680 --> 00:30:10,320
So let's say you have a set of vectors and you want to summarise them.

460
00:30:10,320 --> 00:30:14,280
So for example, in the deep learning context, if you think of like a feature like a hyper

461
00:30:14,280 --> 00:30:19,520
column and you want to to summarise that in some way or to, let's say, compress information,

462
00:30:19,520 --> 00:30:21,400
you might use max pooling or average pooling.

463
00:30:21,400 --> 00:30:26,400
You referred to a feature as a hyper column and I haven't heard that reference before.

464
00:30:26,400 --> 00:30:28,040
What does that mean?

465
00:30:28,040 --> 00:30:33,520
So when I say hyper column, this is sort of, it sounds a bit old school now.

466
00:30:33,520 --> 00:30:39,000
It's a term that comes from neuroscience once again and it refers basically to, it's

467
00:30:39,000 --> 00:30:44,160
what you might call this typical feature tensor that you find in many confnets where

468
00:30:44,160 --> 00:30:47,480
you have, for example, you have an input image and you're applying different convolutions

469
00:30:47,480 --> 00:30:48,480
to it.

470
00:30:48,480 --> 00:30:53,560
Very often you end up at some intermediate point with multiple feature maps, right?

471
00:30:53,560 --> 00:30:58,360
So you have like a feature map of size like h prime w prime and that's your feature

472
00:30:58,360 --> 00:30:59,960
map and you have multiple ones of them, right?

473
00:30:59,960 --> 00:31:01,520
Let's say you have d feature maps.

474
00:31:01,520 --> 00:31:05,720
So in the end, what you end up with is a tensor that's like d by h prime by w prime and

475
00:31:05,720 --> 00:31:08,560
this has often been referred to as a hyper column.

476
00:31:08,560 --> 00:31:09,560
Okay.

477
00:31:09,560 --> 00:31:12,720
So that's in a deep learning context, but you can think of many contexts where you have

478
00:31:12,720 --> 00:31:16,680
feature vectors that you want to summarize in some way.

479
00:31:16,680 --> 00:31:20,640
So for example, in my work, we worked a lot with what we call Fischer vectors and so Fischer

480
00:31:20,640 --> 00:31:27,480
vectors are, it's a type of representation of visual content, particularly used a lot

481
00:31:27,480 --> 00:31:29,600
with what we call local descriptors.

482
00:31:29,600 --> 00:31:32,840
And so it's a way of basically saying, okay, I have some local region of an image and

483
00:31:32,840 --> 00:31:38,120
I want to find some vector representation of that local region that's discriminative

484
00:31:38,120 --> 00:31:40,400
and compact hopefully, okay?

485
00:31:40,400 --> 00:31:45,040
And so imagine you have an image and you have a set of these descriptors that you extracted

486
00:31:45,040 --> 00:31:48,520
let's say for different regions of the image and you say, okay, I have this image, I have

487
00:31:48,520 --> 00:31:52,640
I don't know, like n of these descriptors and I don't want n.

488
00:31:52,640 --> 00:31:53,640
I want like fewer.

489
00:31:53,640 --> 00:31:55,640
I want maybe one or I want to.

490
00:31:55,640 --> 00:32:00,960
So it is, how do I go from that end to that much, much smaller number while not losing

491
00:32:00,960 --> 00:32:01,960
too much information?

492
00:32:01,960 --> 00:32:06,560
So this is like a fundamental problem that has been tackled in many ways and so I've

493
00:32:06,560 --> 00:32:11,080
done some work on how to construct these what we call aggregated representations from

494
00:32:11,080 --> 00:32:14,720
a set of like a larger number of them.

495
00:32:14,720 --> 00:32:20,200
It sounds like, and you mentioned the term embedding previously, it sounds like you're creating

496
00:32:20,200 --> 00:32:27,400
some embedding space and then doing something akin to dimensionality reduction on that embedding

497
00:32:27,400 --> 00:32:28,400
space.

498
00:32:28,400 --> 00:32:32,560
So it's not quite in the sense that in the sense that when people talk about dimensionality

499
00:32:32,560 --> 00:32:36,200
reduction, typically they mean that you have an embedding space, right?

500
00:32:36,200 --> 00:32:39,240
It can be in, let's say, the dimension.

501
00:32:39,240 --> 00:32:43,920
So for example, I don't know, 2000 dimensions and for for dimensionality reduction, what

502
00:32:43,920 --> 00:32:48,160
you want to try to do is find some smaller dimensional space in which you're embedding

503
00:32:48,160 --> 00:32:49,160
slip.

504
00:32:49,160 --> 00:32:50,160
So for example, I don't know.

505
00:32:50,160 --> 00:32:56,960
You might want to go down from 2000 dimensions to maybe 500 or 256 or something like this.

506
00:32:56,960 --> 00:33:00,200
In our case, what we're doing is we're not changing the embedding space, but we're

507
00:33:00,200 --> 00:33:05,520
just changing the amount of samples from that space.

508
00:33:05,520 --> 00:33:06,520
That's it.

509
00:33:06,520 --> 00:33:09,160
The amount of vectors that live in that space.

510
00:33:09,160 --> 00:33:16,200
So for example, rather than have, let's say, 1000 vectors each of size, each of dimension

511
00:33:16,200 --> 00:33:19,280
to K, you might want to have only one.

512
00:33:19,280 --> 00:33:21,360
So this is what this aggregation is about.

513
00:33:21,360 --> 00:33:25,840
Is that a typical example going from 2000 to one?

514
00:33:25,840 --> 00:33:27,080
You can go even higher than that.

515
00:33:27,080 --> 00:33:28,080
You can go even higher than that.

516
00:33:28,080 --> 00:33:34,360
So very often, you might find on the order of anywhere from like 1000 to 10,000 factors

517
00:33:34,360 --> 00:33:36,800
that you want to reduce to just one factor.

518
00:33:36,800 --> 00:33:39,880
So you can see there that you can see what is that doing for you.

519
00:33:39,880 --> 00:33:42,600
Yeah, what is that compression compression?

520
00:33:42,600 --> 00:33:47,280
It's, you know, you're reducing, you're reducing the size of your image representation

521
00:33:47,280 --> 00:33:51,800
by a factor of the number of descriptors that you have.

522
00:33:51,800 --> 00:33:57,120
If you go down to one, I guess if I'm thinking of it in the sense of compression, then certainly

523
00:33:57,120 --> 00:33:59,400
we would want to do that.

524
00:33:59,400 --> 00:34:05,960
But you know, I would imagine there's a tremendous amount of loss in a scenario like that.

525
00:34:05,960 --> 00:34:11,280
But when I think about it from the perspective of like an embedding space, I mean, I guess

526
00:34:11,280 --> 00:34:12,280
it's also loss.

527
00:34:12,280 --> 00:34:15,120
Like you just lose a ton of information.

528
00:34:15,120 --> 00:34:19,680
And so maybe I'm asking you to convince me that there's, you're left with something of

529
00:34:19,680 --> 00:34:24,760
utility after you do this 10,000 to one reduction.

530
00:34:24,760 --> 00:34:30,400
Sure, so you're definitely right in the sense that they both, they have like an effective

531
00:34:30,400 --> 00:34:31,720
compression, right?

532
00:34:31,720 --> 00:34:32,720
That's for sure.

533
00:34:32,720 --> 00:34:34,480
In one case, you're going to a smaller dimensional space.

534
00:34:34,480 --> 00:34:37,640
In other case, you're just reducing the amount of representations you have.

535
00:34:37,640 --> 00:34:39,920
In both case, you're losing information, that's for sure.

536
00:34:39,920 --> 00:34:40,960
And this was the point of my work.

537
00:34:40,960 --> 00:34:47,880
So my work was really on how to represent, how to, to perform this aggregation while maintaining

538
00:34:47,880 --> 00:34:53,240
as much as possible, you know, as much information you can in this aggregated representation.

539
00:34:53,240 --> 00:35:00,400
So what we proposed was a method called generalized max pooling, which aimed to, to sort of maintain

540
00:35:00,400 --> 00:35:01,400
this property.

541
00:35:01,400 --> 00:35:05,540
And of course, it's not perfect, but we found that it gave, you know, pretty, pretty interesting

542
00:35:05,540 --> 00:35:10,240
results over other techniques, for example, like average pooling or max pooling.

543
00:35:10,240 --> 00:35:16,080
And so the benefit there is, is once again, it's in terms of, of compression, which is

544
00:35:16,080 --> 00:35:17,240
which is important.

545
00:35:17,240 --> 00:35:23,560
So in many applications, you don't want, you can't afford to store, for example, 10,000

546
00:35:23,560 --> 00:35:25,800
descriptors to represent an image.

547
00:35:25,800 --> 00:35:30,600
To give an example, so we're very interested in my research center on the problem of image

548
00:35:30,600 --> 00:35:31,600
search, right?

549
00:35:31,600 --> 00:35:37,160
So let's, let's say, for example, you have a database of images, let's say, a billion

550
00:35:37,160 --> 00:35:38,840
images, right?

551
00:35:38,840 --> 00:35:42,400
And you have some image that you want to match to that.

552
00:35:42,400 --> 00:35:45,320
And you can think of many applications, you can think of, for example, shopping, let's

553
00:35:45,320 --> 00:35:48,640
say you want to, to find us, you know, you have an image of like some kind of clothing

554
00:35:48,640 --> 00:35:55,000
item, and you want to find in a huge, let's say, catalog by multiple retailers, anybody

555
00:35:55,000 --> 00:35:58,680
who has something similar, who has the exact same one.

556
00:35:58,680 --> 00:36:02,440
If you have to, if you have to compute the similarity between that image and all the

557
00:36:02,440 --> 00:36:07,400
images in those, in that database, and each image is represented by 10,000 vectors, each

558
00:36:07,400 --> 00:36:12,680
of which has a dimension of 2,000, it's going to take forever, it's not practical.

559
00:36:12,680 --> 00:36:18,360
So this is why very often people work on how do I get from 10,000 vectors to one, or

560
00:36:18,360 --> 00:36:20,680
some smaller number, some much smaller number.

561
00:36:20,680 --> 00:36:22,000
So this is really the utility there.

562
00:36:22,000 --> 00:36:24,120
It's a compression, it's a compression utility.

563
00:36:24,120 --> 00:36:28,360
You mentioned this, this algorithm is a generalized max pooling.

564
00:36:28,360 --> 00:36:35,880
Are you taking kind of an off the shelf, CNN architecture, ResNet, for example, and kind

565
00:36:35,880 --> 00:36:40,000
of swapping out, you know, wherever it says max pooling with this generalized max pooling

566
00:36:40,000 --> 00:36:45,720
and maybe chopping off the last layer, your classifier, and that's creating your vectors

567
00:36:45,720 --> 00:36:48,280
or is it more involved than that?

568
00:36:48,280 --> 00:36:53,800
So that would make sense, as a good guess.

569
00:36:53,800 --> 00:36:57,440
But actually, and this is something that could technically be done, but actually what

570
00:36:57,440 --> 00:36:59,600
we think of this is something that's very generic.

571
00:36:59,600 --> 00:37:04,320
So it's actually not related to deep, it's not specific to deep architectures.

572
00:37:04,320 --> 00:37:08,280
It's very generic in the sense that we are trying to solve the problem where we have

573
00:37:08,280 --> 00:37:12,640
a set of vectors and we want to reduce, we want to aggregate them in some way.

574
00:37:12,640 --> 00:37:17,040
And so that can involve features extracted by deep networks.

575
00:37:17,040 --> 00:37:21,840
So let's say, for example, you have a deep network and at some point you have this hyper

576
00:37:21,840 --> 00:37:25,920
column I talked about, you have this feature tensor and you want to pull it in some way,

577
00:37:25,920 --> 00:37:27,160
you want to aggregate it.

578
00:37:27,160 --> 00:37:32,320
So you can think of generalized max pooling in this scenario as an alternative to max pooling

579
00:37:32,320 --> 00:37:35,320
or average pooling, but you can also have other descriptors, right?

580
00:37:35,320 --> 00:37:40,480
So you can have descriptors that are handcrafted or come from many different, many different

581
00:37:40,480 --> 00:37:42,360
things we can think of.

582
00:37:42,360 --> 00:37:51,080
It's more presented as a fundamental operation, you can apply to a set of vectors to aggregate

583
00:37:51,080 --> 00:37:57,880
them as opposed to something that's specifically used in a context of a deep learning model.

584
00:37:57,880 --> 00:37:58,880
Exactly, yeah.

585
00:37:58,880 --> 00:38:02,400
And in fact, in the original work we didn't apply it on deep learning, although it's

586
00:38:02,400 --> 00:38:06,960
been subsequently used with deep models in combination.

587
00:38:06,960 --> 00:38:10,960
And so you've spent this time on the visual attention side.

588
00:38:10,960 --> 00:38:14,920
It sounds like that was a bit earlier in your research.

589
00:38:14,920 --> 00:38:19,800
More recently you're working on learning these representations.

590
00:38:19,800 --> 00:38:23,120
Where do you, where you headed?

591
00:38:23,120 --> 00:38:31,840
What's kind of interesting for you nowadays and where you investing your resources in terms

592
00:38:31,840 --> 00:38:32,840
of future research?

593
00:38:32,840 --> 00:38:33,840
Sure.

594
00:38:33,840 --> 00:38:38,080
So actually, you know, everything sort of all becomes new again.

595
00:38:38,080 --> 00:38:43,400
So actually I'm doing some work now on both things at the same time, because as you mentioned

596
00:38:43,400 --> 00:38:48,240
very patiently before, attention is it's used everywhere, right?

597
00:38:48,240 --> 00:38:50,760
In deep learning right now, in many things.

598
00:38:50,760 --> 00:38:55,520
So actually right now some of the work I'm really involved in at the moment involves

599
00:38:55,520 --> 00:39:03,000
how to learn representations using deep learning with attention involved as well.

600
00:39:03,000 --> 00:39:09,920
So meaning how do you use some sort of attention mechanisms in order to pay attention to regions

601
00:39:09,920 --> 00:39:14,360
of an image that you really need to focus on in order to solve your task, right?

602
00:39:14,360 --> 00:39:18,600
So this is where once again the, this is like the overlap between the two views, let's

603
00:39:18,600 --> 00:39:20,320
say, of attention.

604
00:39:20,320 --> 00:39:25,080
Because once again, there's limited processing power, there's a lot of information images.

605
00:39:25,080 --> 00:39:32,520
And so it pays to really only spend time computing over data that's valuable.

606
00:39:32,520 --> 00:39:38,040
And then not only that, but it also can be considered some sort of clean name mechanism,

607
00:39:38,040 --> 00:39:43,480
for example, because you can have a lot of things that you can just, that might be important,

608
00:39:43,480 --> 00:39:47,280
but it can also be just distracting and can be considered clutter basically.

609
00:39:47,280 --> 00:39:51,840
So let's say, for example, you're doing once again, let's look at the fashion example.

610
00:39:51,840 --> 00:39:56,360
Let's say you want to train to learn a representation for visual search, right?

611
00:39:56,360 --> 00:39:58,760
Let's use my previous example.

612
00:39:58,760 --> 00:40:05,240
And you want to search for, you know, you're looking for shirts.

613
00:40:05,240 --> 00:40:08,800
If you have an image, there are many parts of the image, which is not going to help you

614
00:40:08,800 --> 00:40:11,160
find, you know, whether these two images match, right?

615
00:40:11,160 --> 00:40:12,720
Whether they both have the same shirt.

616
00:40:12,720 --> 00:40:16,800
So you might want to only attend to the regions of the image that are relevant for that.

617
00:40:16,800 --> 00:40:19,800
So of course, like if there, if you might want to pay attention to the upper body of the

618
00:40:19,800 --> 00:40:23,400
person, you might want to be able to find, okay, where is the upper body of this person?

619
00:40:23,400 --> 00:40:27,480
Because this is where I'm going to really be able to tell if there's a shirt in the

620
00:40:27,480 --> 00:40:28,480
image.

621
00:40:28,480 --> 00:40:30,320
And if so, if it's the same shirt, if it's the shirt I'm looking for.

622
00:40:30,320 --> 00:40:34,640
Now, there might be other parts of the image that are also useful for that.

623
00:40:34,640 --> 00:40:37,480
So maybe, for example, if I find the head, I might have a better chance of finding the

624
00:40:37,480 --> 00:40:40,480
body, but there are many parts that may not be, right?

625
00:40:40,480 --> 00:40:46,200
So what I'm focused on right now is how to, how to incorporate different models of salience

626
00:40:46,200 --> 00:40:50,360
or let's do different ways of attending to regions so that, you know, you can simplify

627
00:40:50,360 --> 00:40:54,160
these tasks of learning representations for search.

628
00:40:54,160 --> 00:40:57,600
Naila, thanks so much for taking the time to chat with us.

629
00:40:57,600 --> 00:41:03,400
It's really interesting work and I'm excited to get to learn a bit about what you're up

630
00:41:03,400 --> 00:41:04,400
to.

631
00:41:04,400 --> 00:41:05,400
Thanks so much, Naila.

632
00:41:05,400 --> 00:41:07,400
It was really great to have this conversation.

633
00:41:07,400 --> 00:41:13,120
All right, everyone, that's our show for today.

634
00:41:13,120 --> 00:41:18,240
For more information on Naila or any of the topics covered in this episode, head on over

635
00:41:18,240 --> 00:41:22,640
to twimlai.com slash talk slash 190.

636
00:41:22,640 --> 00:41:28,760
For more information on the entire deep learning and daba podcast series, visit twimlai.com slash

637
00:41:28,760 --> 00:41:31,680
in daba 2018.

638
00:41:31,680 --> 00:41:34,920
Thanks again to Google for their sponsorship of this series.

639
00:41:34,920 --> 00:41:41,760
Be sure to check out the 2019 AI residency program at g.co slash AI residency.

640
00:41:41,760 --> 00:41:44,920
As always, thanks so much for listening and catch you next time.

