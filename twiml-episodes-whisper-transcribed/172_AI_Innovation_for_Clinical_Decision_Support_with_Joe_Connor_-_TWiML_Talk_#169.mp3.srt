1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,600
I'm your host Sam Charrington.

4
00:00:31,600 --> 00:00:36,600
In this episode I speak with Joseph Conner, founder of Experto Crete.

5
00:00:36,600 --> 00:00:40,880
Joseph has been listening to the podcast for a while and he and I connected after he reached

6
00:00:40,880 --> 00:00:45,400
out to discuss an article I wrote on AI in the healthcare space.

7
00:00:45,400 --> 00:00:49,880
In this conversation we explore his experiences bringing AI-powered healthcare projects

8
00:00:49,880 --> 00:00:55,440
to market in collaboration with the UK National Health Service and its clinicians.

9
00:00:55,440 --> 00:00:59,680
We take a look at some of the various challenges he's run into when applying ML and AI in

10
00:00:59,680 --> 00:01:04,560
healthcare as well as some of his successes such as tackling effective triage of mental

11
00:01:04,560 --> 00:01:09,920
health patients using emotion recognition within a chatbot environment.

12
00:01:09,920 --> 00:01:15,240
We also discuss data protections, especially GDPR and the challenges that come along

13
00:01:15,240 --> 00:01:20,320
with building systems dependent on using patient data under these restrictions.

14
00:01:20,320 --> 00:01:25,920
Finally, we take a look at potential ways to include clinicians in the building of these

15
00:01:25,920 --> 00:01:29,600
applications and the importance of doing so.

16
00:01:29,600 --> 00:01:33,440
And now on to the show.

17
00:01:33,440 --> 00:01:38,080
Alright everyone, I am on the line with Joe Conner.

18
00:01:38,080 --> 00:01:43,560
Joe is an innovation associate in machine learning and AI with the UK's National Health

19
00:01:43,560 --> 00:01:47,200
Service and founder at EXPERTOCREDI.

20
00:01:47,200 --> 00:01:49,920
Joe, welcome to this weekend machine learning and AI.

21
00:01:49,920 --> 00:01:52,200
Hi, it's pleasure to be here.

22
00:01:52,200 --> 00:01:54,040
Well, it's great to have you on the show.

23
00:01:54,040 --> 00:01:55,040
You reached out.

24
00:01:55,040 --> 00:02:01,560
I believe following maybe an article or something I wrote up in my newsletter on the healthcare

25
00:02:01,560 --> 00:02:07,560
space and my interest in further exploring it and offered to help out.

26
00:02:07,560 --> 00:02:11,760
So I'm really glad to have you on the show.

27
00:02:11,760 --> 00:02:18,520
And why don't you start by telling us a little bit about what you're doing at NHS and your

28
00:02:18,520 --> 00:02:23,000
background and kind of how you got involved in applying ML and AI in healthcare?

29
00:02:23,000 --> 00:02:26,440
Well, you know, one of the things about coming on your show is it makes it actually look

30
00:02:26,440 --> 00:02:30,280
back and realise where you've come from.

31
00:02:30,280 --> 00:02:34,680
I'm a self-aminent engineer by first discipline and I joined the time energy authority doing

32
00:02:34,680 --> 00:02:39,600
probabilistic risk assessment back in the 85.

33
00:02:39,600 --> 00:02:46,520
So our 28 years in this particular field and then moved to the Advanced Robotics Research

34
00:02:46,520 --> 00:02:49,720
Lab at the University of Software doing VR and Telepresence.

35
00:02:49,720 --> 00:02:57,720
But then moved out to that in two commercial due diligence in the tech space, I think

36
00:02:57,720 --> 00:03:03,520
it's supply chains and modelling for banks and VCs, but got back into machine learning

37
00:03:03,520 --> 00:03:11,360
making things back in 2006 looking at computer vision based classifiers for emotion recognition

38
00:03:11,360 --> 00:03:13,640
effective computing type things.

39
00:03:13,640 --> 00:03:17,680
And for the past two and a half years I've been embedded with an organisation called NHS

40
00:03:17,680 --> 00:03:24,480
Digital, which is part of the UK National Health Service and been looking at technologies

41
00:03:24,480 --> 00:03:31,520
that it can adopt, but more of late part of an organisation for code for health.

42
00:03:31,520 --> 00:03:37,040
And its primary role is to work with clinicians, find out what they need, and make some open

43
00:03:37,040 --> 00:03:42,040
source software and tools that they can use, and it's obviously the best thing I've ever

44
00:03:42,040 --> 00:03:43,040
done in my life.

45
00:03:43,040 --> 00:03:47,040
I'm really enjoying it, the clinicians are fabulous people and it's nice to be able to help

46
00:03:47,040 --> 00:03:48,040
in that space.

47
00:03:48,040 --> 00:03:49,840
Oh, that sounds great.

48
00:03:49,840 --> 00:03:53,000
You mentioned you've been working on emotion recognition.

49
00:03:53,000 --> 00:03:56,000
Is that something that you're applying in the health care context?

50
00:03:56,000 --> 00:04:00,360
Well, yes, it's important to actually consider, well one of the things I've been looking

51
00:04:00,360 --> 00:04:07,000
at for the past two years is the subjects of mental health and how that can be assisted

52
00:04:07,000 --> 00:04:11,440
by technologies, cell chatbots, etc.

53
00:04:11,440 --> 00:04:15,520
And you can overlay effective computing type technologies in that domain.

54
00:04:15,520 --> 00:04:22,400
So for example, a clinical issue that exists is how do you actually triage people effectively

55
00:04:22,400 --> 00:04:25,880
when the demand for services is so high?

56
00:04:25,880 --> 00:04:30,200
And the clinician approached me and he wanted to know is it possible to do what's

57
00:04:30,200 --> 00:04:37,400
called PHQ9, GAD7, which are sort of scoring mechanisms for looking at mental health in

58
00:04:37,400 --> 00:04:39,360
a chatbot environment.

59
00:04:39,360 --> 00:04:47,440
So we built a tool which you could consider, which looks at scoring those triage processes,

60
00:04:47,440 --> 00:04:50,840
but all the laying on top of that effective computing.

61
00:04:50,840 --> 00:04:57,520
So analyzing emotion in text and all paid to look at suicide ideation words and expanding

62
00:04:57,520 --> 00:05:01,360
on the current models that exist, which are usually just numerical question and answer

63
00:05:01,360 --> 00:05:07,560
type things where you get a rating, but going into dialogue because I think the future

64
00:05:07,560 --> 00:05:14,520
does probably lie in augmenting services using these types of technologies.

65
00:05:14,520 --> 00:05:19,600
Has that project gone into production uses at live and are patients interacting with

66
00:05:19,600 --> 00:05:20,600
it?

67
00:05:20,600 --> 00:05:25,840
It's currently out with clinicians and the supply chain that could potentially use it.

68
00:05:25,840 --> 00:05:30,800
Obviously, it's an open source piece of software, so anybody can use it.

69
00:05:30,800 --> 00:05:36,280
But there are existing providers for mental health services and triage in Britain and we're

70
00:05:36,280 --> 00:05:40,840
saying to them, here it is, play with it, use it, test it.

71
00:05:40,840 --> 00:05:44,760
And if you find a value, here's the code, fill you books, crack on.

72
00:05:44,760 --> 00:05:50,720
But obviously it would need to go through some rigorous testing, not least because it

73
00:05:50,720 --> 00:05:52,920
would be a change of clinical practice.

74
00:05:52,920 --> 00:05:58,280
And so I think it's the early days, I think before to say it's very early days.

75
00:05:58,280 --> 00:06:03,160
You say it's open source, are the models that you're using open source as well or just

76
00:06:03,160 --> 00:06:07,120
the chatbot platform itself?

77
00:06:07,120 --> 00:06:12,240
Everything, the code, the manual, everything is open source.

78
00:06:12,240 --> 00:06:13,240
Okay.

79
00:06:13,240 --> 00:06:16,720
But the role code for health is to actually produce things that people can use quickly.

80
00:06:16,720 --> 00:06:20,120
A clinician comes to me and says, can I do this?

81
00:06:20,120 --> 00:06:23,680
And what's the art of what's possible and I can show them?

82
00:06:23,680 --> 00:06:26,480
So it's great to build things in the NHS.

83
00:06:26,480 --> 00:06:34,040
Just prior you mentioned or alluded to the process of bringing new products, projects

84
00:06:34,040 --> 00:06:42,600
to market in the healthcare sphere, maybe it's worth taking a step back and kind of starting

85
00:06:42,600 --> 00:06:47,080
there and having you talk a little bit about your experiences with machine learning

86
00:06:47,080 --> 00:06:55,880
and AI in general at NHS and in healthcare and getting projects out to clients.

87
00:06:55,880 --> 00:06:56,880
Okay.

88
00:06:56,880 --> 00:07:01,880
Well, I think like most engineers, you think you can solve problems and you go ahead

89
00:07:01,880 --> 00:07:03,200
and solve problems.

90
00:07:03,200 --> 00:07:08,720
I think one of the things that I, big slaining point I have come across is asking the

91
00:07:08,720 --> 00:07:10,720
question, how wrong can you afford to be?

92
00:07:10,720 --> 00:07:13,920
Because this is an environment that's a bit like my early days in the 80s when we're

93
00:07:13,920 --> 00:07:18,240
talking about nuclear power stations, you cannot afford to be wrong.

94
00:07:18,240 --> 00:07:19,240
Right.

95
00:07:19,240 --> 00:07:23,640
And the degree to which you could be wrong needs to be understood by both the person designing

96
00:07:23,640 --> 00:07:28,160
the system and also the person requesting the system.

97
00:07:28,160 --> 00:07:35,160
And I think most people don't, if I were set to be a broad generalisation, most clinicians

98
00:07:35,160 --> 00:07:39,240
do not actually ask that question of themselves before they consider using machine learning

99
00:07:39,240 --> 00:07:40,240
in AI.

100
00:07:40,240 --> 00:07:43,600
So that's the first thing I've learned of one of the things I'm very keen that people

101
00:07:43,600 --> 00:07:47,200
start to do is to recognise that look, how wrong can you afford to be?

102
00:07:47,200 --> 00:07:49,520
So let me give you an example.

103
00:07:49,520 --> 00:07:57,000
We have a system via which we dispatch ambulances and medication and services in Britain.

104
00:07:57,000 --> 00:08:01,480
You could look at that and you could optimise the questions because that's an existing system

105
00:08:01,480 --> 00:08:03,320
that is an expert system.

106
00:08:03,320 --> 00:08:07,200
You could put a Bayesian ranking system in there.

107
00:08:07,200 --> 00:08:11,840
You could optimise that, ask fewer questions, but chances are not because you're asking

108
00:08:11,840 --> 00:08:14,280
fewer questions, you might get it wrong.

109
00:08:14,280 --> 00:08:19,360
So 80% of the time you might get it right, 20% of the time you might get it wrong.

110
00:08:19,360 --> 00:08:23,680
The risk associated with that is significant because in the 20% of the time you don't get

111
00:08:23,680 --> 00:08:26,200
it right, that might be an ambulance.

112
00:08:26,200 --> 00:08:33,760
So there are certain situations where there are highly critical situations for health

113
00:08:33,760 --> 00:08:37,200
that I think machine learning isn't appropriate for.

114
00:08:37,200 --> 00:08:42,640
So early stage triage for low risk areas, I think it isn't appropriate for.

115
00:08:42,640 --> 00:08:46,320
But when you're building these things, the challenge always is how do you get hold of data

116
00:08:46,320 --> 00:08:47,760
to be able to build them?

117
00:08:47,760 --> 00:08:52,880
And I think there's a misconception that because the NHS has 1.7 million people in it and

118
00:08:52,880 --> 00:08:59,520
it's fourth largest organisation in the world, that it is one discreet organisation and

119
00:08:59,520 --> 00:09:01,240
one discreet set of data sets.

120
00:09:01,240 --> 00:09:03,960
And in some cases that's true with public health data, etc.

121
00:09:03,960 --> 00:09:12,720
Most of the time they sit within fairly siloed groups of NHS practices across Britain.

122
00:09:12,720 --> 00:09:18,880
So it's very difficult for me as a machine learning practitioner to say, okay, you, as

123
00:09:18,880 --> 00:09:23,400
in your trusts, will you please show your data with me, personally identifiable data?

124
00:09:23,400 --> 00:09:25,440
It just doesn't going to happen.

125
00:09:25,440 --> 00:09:30,960
So I think the Chitkey skill for myself and people who will follow on from me will

126
00:09:30,960 --> 00:09:36,080
be to actually get some skill base at a local level embedded at a local level, so that people

127
00:09:36,080 --> 00:09:43,240
can build classifiers and prediction mechanisms that can be used locally, built locally within

128
00:09:43,240 --> 00:09:46,360
the boundaries and the firewall of a particular trust.

129
00:09:46,360 --> 00:09:51,880
The only other option is to try and agglomerate all that data together and well, there's been

130
00:09:51,880 --> 00:09:59,520
further successful with that with a group of organisations providing details to Swansea

131
00:09:59,520 --> 00:10:00,520
University.

132
00:10:00,520 --> 00:10:05,160
Not on a national Welsh skill, but I think one of the challenges I, I think I'm going

133
00:10:05,160 --> 00:10:10,560
to look at probably from May onwards is the subject of federated machine learning because

134
00:10:10,560 --> 00:10:15,440
there's a better chance I think of that being used and by federated, it means different

135
00:10:15,440 --> 00:10:20,280
things to different people, but by by federated machine learning, I mean, let's build models,

136
00:10:20,280 --> 00:10:27,760
let's build models external to the NHS and apply them locally and just get the gradient

137
00:10:27,760 --> 00:10:31,600
descent or just get the error back, let's see what these things work, so we're not actually

138
00:10:31,600 --> 00:10:37,400
looking at personal data, we're not actually looking at anything relating to anything

139
00:10:37,400 --> 00:10:43,440
that could define a person in the real world, but just looking for the success or not of

140
00:10:43,440 --> 00:10:47,080
the models, I think there's a better chance of being able to deploy models in that sort

141
00:10:47,080 --> 00:10:50,640
of environment, but it's early days.

142
00:10:50,640 --> 00:10:58,120
So do you have a taxonomy of sorts or mental model for the different application areas

143
00:10:58,120 --> 00:11:03,800
within healthcare broadly or the NHS of ML and AI?

144
00:11:03,800 --> 00:11:07,680
I think within like an organisation this size, you wouldn't be surprised to find out

145
00:11:07,680 --> 00:11:14,440
that it sits within text, it sits within images, and to a lesser degree, it sits within

146
00:11:14,440 --> 00:11:20,520
video, but it's primarily text and images, so for example if you wanted to do some

147
00:11:20,520 --> 00:11:25,200
analysis of drug use and drug use effect, it's very difficult for you to do anything other

148
00:11:25,200 --> 00:11:31,520
than natural language assessment on text within a health record system, and the taxonomy

149
00:11:31,520 --> 00:11:37,120
is associated with doing that sort of analysis that you might expect to see like ICD-10,

150
00:11:37,120 --> 00:11:46,800
UMILs, SNOMED, are not the ways in which people classify a diagnosis, just don't exist

151
00:11:46,800 --> 00:11:50,880
in these systems, they are just free text systems.

152
00:11:50,880 --> 00:11:57,600
So people are busying themselves, developing, um, annotation mechanisms to understand for

153
00:11:57,600 --> 00:12:04,320
a particular medication, these are the likely ways in which a clinician will express those,

154
00:12:04,320 --> 00:12:11,320
and running those algorithms to understand the effect of drugs and the effect of treatment,

155
00:12:11,320 --> 00:12:14,960
particularly in mental health and in mind about in London they're doing some success in

156
00:12:14,960 --> 00:12:20,400
that area using products like Gates and to a lesser degree glove, the natural language

157
00:12:20,400 --> 00:12:28,160
tools, but within the realms of images, it's very difficult because most images have also

158
00:12:28,160 --> 00:12:36,040
got embedded in them, tools, techniques for maintaining personally identifiable data,

159
00:12:36,040 --> 00:12:40,880
so an x-ray will have identifiable data in it, so we've been busing ourselves over the

160
00:12:40,880 --> 00:12:48,280
past three months looking at ways in which we can, um, allow people to label data without

161
00:12:48,280 --> 00:12:54,560
actually, um, so say we had a physiotherapist who wanted to create some label datasets for

162
00:12:54,560 --> 00:12:59,320
shoulder impingement, it'd be very difficult for them to create that dataset, um, if you

163
00:12:59,320 --> 00:13:03,680
don't give them say web-based or mobile-based tools by which they can take pictures label

164
00:13:03,680 --> 00:13:09,520
the data and give that to in a controlled environment to the people who could produce the algorithms.

165
00:13:09,520 --> 00:13:15,040
So yeah, labeling of data and producing data that you can use for machine learning is

166
00:13:15,040 --> 00:13:19,080
one of the biggest challenges I think that we have, and it's not going to go away probably

167
00:13:19,080 --> 00:13:22,560
because we are a very disparate organization.

168
00:13:22,560 --> 00:13:28,960
So it sounds like you're, you kind of think of things in terms of the media type if you

169
00:13:28,960 --> 00:13:36,760
will, text versus images versus video, uh, I'm curious even at a higher level, there are,

170
00:13:36,760 --> 00:13:41,560
we could probably rattle off the different types of applications, but there you've, you've

171
00:13:41,560 --> 00:13:48,120
talked about diagnostic applications, you've talked about with the ambulances, like the

172
00:13:48,120 --> 00:13:54,400
efficiency types of applications, I was just curious what other broad categories of

173
00:13:54,400 --> 00:13:56,640
applications there might exist.

174
00:13:56,640 --> 00:14:00,600
I imagine, you know, I say sometimes look at an org chart and you can figure out where

175
00:14:00,600 --> 00:14:07,800
you can apply it, the same thing applies at the NHS in terms of HR and lots of other types

176
00:14:07,800 --> 00:14:08,960
of applications.

177
00:14:08,960 --> 00:14:19,240
I think we need to be realistic, and the doing machine learning and AI on clinical subject,

178
00:14:19,240 --> 00:14:23,920
I would break it up into two areas, what is clinical and what's process?

179
00:14:23,920 --> 00:14:28,880
If you look at it just as in process and people were not different from any other organization

180
00:14:28,880 --> 00:14:34,120
in the world, somewhat bigger, so there are ways in which you can do process improvement,

181
00:14:34,120 --> 00:14:44,360
whether that's booking people onto of treatment plans, booking people into hospitals, um,

182
00:14:44,360 --> 00:14:48,760
waiting this management, that type of, these are sort of the sort of bread and butter you

183
00:14:48,760 --> 00:14:53,680
would expect machine learning solutions to be, uh, well made for.

184
00:14:53,680 --> 00:14:58,560
And there's not little or no clinical risk associated with some of those, uh, right,

185
00:14:58,560 --> 00:15:04,120
but when you start to look at clinical applications, you come across one thing that is probably

186
00:15:04,120 --> 00:15:08,440
unique to healthcare, which is the subject of a healthcare pathway.

187
00:15:08,440 --> 00:15:12,520
So for every form of treatment, there will be a healthcare pathway, which we'll say you

188
00:15:12,520 --> 00:15:17,840
do this by this, do this, this, this, this, this, and this will be, um, under what's called

189
00:15:17,840 --> 00:15:22,840
the nice guidelines, so it's simply equivalent to the HIPAA compliance, et cetera, you would

190
00:15:22,840 --> 00:15:25,000
have in the stands.

191
00:15:25,000 --> 00:15:29,800
And some of those processes, if they're in a pathway, you could augment.

192
00:15:29,800 --> 00:15:35,320
So if we take the subject of mental health, you could augment the, the process of, um,

193
00:15:35,320 --> 00:15:40,000
triaging by getting better data to people more quickly.

194
00:15:40,000 --> 00:15:46,080
And that's important because, uh, like, not unlike other, um, conditions, the sooner you

195
00:15:46,080 --> 00:15:48,680
treat somebody, the better it's going to be.

196
00:15:48,680 --> 00:15:56,000
So gathering data and analyzing data and scoring that data more efficiently to help a clinician

197
00:15:56,000 --> 00:16:02,280
understand who they should see isn't much better than what currently exists in some areas

198
00:16:02,280 --> 00:16:04,520
of mental health, which is you get a questionnaire.

199
00:16:04,520 --> 00:16:08,640
So if you're, uh, paper questionnaire, in some cases.

200
00:16:08,640 --> 00:16:13,360
So if you've got a condition, they will, uh, they will meet you and they'll ask you

201
00:16:13,360 --> 00:16:16,000
to rate how you feel on certain subjects.

202
00:16:16,000 --> 00:16:19,640
If you work with paper questionnaire, you will see, and then maybe too excited, you have

203
00:16:19,640 --> 00:16:20,640
to fill it in again.

204
00:16:20,640 --> 00:16:24,200
The data gets lost, and that's just not the way to work.

205
00:16:24,200 --> 00:16:28,320
So you can imagine in a chatbot environment, you can capture that data, but person doesn't

206
00:16:28,320 --> 00:16:34,920
have to be there to actually, um, gather that data, and they can control that data and

207
00:16:34,920 --> 00:16:37,760
share that data as and when they see fit.

208
00:16:37,760 --> 00:16:41,760
So it doesn't, it could be a system that sits on a mobile phone, which they control their

209
00:16:41,760 --> 00:16:46,200
user, and it may not necessarily go to the NHS unless the person specifically wants it

210
00:16:46,200 --> 00:16:47,360
to do.

211
00:16:47,360 --> 00:16:53,760
So these sort of solutions, the triage in making better decisions about what to do next.

212
00:16:53,760 --> 00:16:58,680
At the early start stages of a, uh, a diagnosis is worthwhile.

213
00:16:58,680 --> 00:17:05,840
It's very difficult to suggest, um, alternatives, uh, one of the key and the most successful

214
00:17:05,840 --> 00:17:11,040
bits about being involved with code for health is the clinicians involved from the onset.

215
00:17:11,040 --> 00:17:15,160
And I think that's critical for anybody wishing to bring systems into, into any health

216
00:17:15,160 --> 00:17:20,640
care system, anything that's going to affect the current clinical practice should be designed

217
00:17:20,640 --> 00:17:26,480
with clinicians, not presented to them as a solution for increasing efficiency, which

218
00:17:26,480 --> 00:17:33,360
I've seen done, um, and also it's going to use some, some machine learning terms, you

219
00:17:33,360 --> 00:17:38,040
know, tailor it back a bit and explain exactly what these things do, because people need

220
00:17:38,040 --> 00:17:43,240
to understand specifically, because a GDPR, how these things work, and it's clinicians

221
00:17:43,240 --> 00:17:47,640
at the end of the day that will want to either use it or not or refer it or not.

222
00:17:47,640 --> 00:17:48,640
Right.

223
00:17:48,640 --> 00:17:53,040
So it's, it's how you, how you design, how you decode design a product is the most important

224
00:17:53,040 --> 00:17:54,040
thing.

225
00:17:54,040 --> 00:17:58,040
And being close to clinicians is, and clinical practice wherever you were in the world

226
00:17:58,040 --> 00:17:59,600
is, I think, is the, is a key thing.

227
00:17:59,600 --> 00:18:03,400
If you want to do useful things in machine learning, that's why it's such a good thing

228
00:18:03,400 --> 00:18:08,280
to be, we embed it in the NHS, although it's two and a half days a week and the rest

229
00:18:08,280 --> 00:18:10,240
of time I mean, I spread a creed.

230
00:18:10,240 --> 00:18:13,720
Just working with people, working with clinicians is a, is a wonderful thing, because you know

231
00:18:13,720 --> 00:18:21,720
you can remove some of the pain of doing practice, uh, by optimizing it and augmenting it.

232
00:18:21,720 --> 00:18:26,720
Um, and that, maybe that's another subject that's worth discussing is the subject of what

233
00:18:26,720 --> 00:18:29,960
is AI to most people.

234
00:18:29,960 --> 00:18:36,040
The conversations around what is AI and is it going to remove jobs, et cetera, is just

235
00:18:36,040 --> 00:18:37,840
prevalent in the press.

236
00:18:37,840 --> 00:18:43,480
If I see another discussion with a look of a humanoid robot in it, I think I'm going

237
00:18:43,480 --> 00:18:44,480
to scream.

238
00:18:44,480 --> 00:18:45,800
It drives me a party.

239
00:18:45,800 --> 00:18:49,800
It's just, it doesn't help people to understand that really, what are we looking at here?

240
00:18:49,800 --> 00:18:53,000
We're looking at maths stats and computers.

241
00:18:53,000 --> 00:18:56,960
And they are programmed to do things or not by humans.

242
00:18:56,960 --> 00:19:02,080
And they can be used to augment practice in a clever way by doing predictions and classifications

243
00:19:02,080 --> 00:19:04,040
faster than some people can do.

244
00:19:04,040 --> 00:19:05,520
It's not going to remove jobs.

245
00:19:05,520 --> 00:19:09,960
I know, I don't believe it was going to remove jobs in, in healthcare, and it neither

246
00:19:09,960 --> 00:19:15,400
is should it, because you know, unless we get, um, systems that are good at socratic

247
00:19:15,400 --> 00:19:20,440
dialogue, um, listening better, responding better, very quickly.

248
00:19:20,440 --> 00:19:22,440
And I don't see that happening at the moment.

249
00:19:22,440 --> 00:19:25,960
Uh, I think we're, say, safe to say that clinicians are safe.

250
00:19:25,960 --> 00:19:29,280
I can certainly see it changing a lot of jobs.

251
00:19:29,280 --> 00:19:30,280
Yeah.

252
00:19:30,280 --> 00:19:31,280
Right.

253
00:19:31,280 --> 00:19:36,560
Hopefully for the better one would help both from the patient perspective as well as the

254
00:19:36,560 --> 00:19:37,560
worker perspective.

255
00:19:37,560 --> 00:19:38,560
Yeah.

256
00:19:38,560 --> 00:19:43,360
I mean, the National Health Service is about making people better.

257
00:19:43,360 --> 00:19:48,520
It isn't about keeping people well, although I think increasingly people are thinking like

258
00:19:48,520 --> 00:19:49,520
that.

259
00:19:49,520 --> 00:19:52,920
So public health England does exist and there are various initiatives to keep people

260
00:19:52,920 --> 00:20:01,080
well, but I think in economies like ours that are very densely populated regions, um,

261
00:20:01,080 --> 00:20:05,440
we need to think more about how can we build AI systems that keep people well.

262
00:20:05,440 --> 00:20:07,880
That's become a pretty popular topic here.

263
00:20:07,880 --> 00:20:12,080
Uh, the whole population health management idea.

264
00:20:12,080 --> 00:20:13,080
Hmm.

265
00:20:13,080 --> 00:20:18,160
Um, I don't know that we're necessarily any further ahead than the NHS is.

266
00:20:18,160 --> 00:20:20,960
Um, I'm certainly far from an expert in that.

267
00:20:20,960 --> 00:20:25,920
Uh, it's a topic that comes up quite a bit when talking about kind of health care

268
00:20:25,920 --> 00:20:26,920
directions.

269
00:20:26,920 --> 00:20:27,920
Yeah.

270
00:20:27,920 --> 00:20:33,240
I think if we look at what we have in our phones in terms of technology, it's wholly

271
00:20:33,240 --> 00:20:34,240
believable.

272
00:20:34,240 --> 00:20:39,160
And in my experience, when we built emotion classifiers that can sit in a wholly powered

273
00:20:39,160 --> 00:20:44,600
bike, but it's not within the NHS, but within expert degree, we build emotion classifiers

274
00:20:44,600 --> 00:20:50,320
that can sit in a phone and passively understand your emotional state and put it in a, in a diary

275
00:20:50,320 --> 00:20:54,440
that you can subsequently review using computer vision technologies.

276
00:20:54,440 --> 00:20:59,080
So that sort of thing could exist and could that could enable me to reflect about what

277
00:20:59,080 --> 00:21:02,120
makes me happy and what works you sad.

278
00:21:02,120 --> 00:21:06,600
And therefore I could with the right tools that might be chatbot might give me some

279
00:21:06,600 --> 00:21:12,680
ideas to the way in which I can improve my behavior to make me more resilient.

280
00:21:12,680 --> 00:21:18,000
These sort of things can be encompassed within our day to day lives if they, if somebody

281
00:21:18,000 --> 00:21:19,000
wants to do that.

282
00:21:19,000 --> 00:21:26,000
And I know, I mean, there's quite a lot of resilience apps coming out, resilience technologies,

283
00:21:26,000 --> 00:21:33,000
monitoring, sensing technologies that exist, but behavior change, but a population level

284
00:21:33,000 --> 00:21:37,360
isn't just about the technology, it's about a country's willingness and often society's

285
00:21:37,360 --> 00:21:40,400
willingness to be assisted.

286
00:21:40,400 --> 00:21:43,160
And we've got to trust the thing that's assisting us.

287
00:21:43,160 --> 00:21:47,240
So if you've got a phone that's saying, look, Joe, you've not been happy, you've been

288
00:21:47,240 --> 00:21:50,160
grumpy for a couple of days now.

289
00:21:50,160 --> 00:21:53,160
What do you think's causing that and what do you think you could improve that?

290
00:21:53,160 --> 00:22:00,800
Now, if I, if my phone did that, which it could do, I built a bot that actually does that.

291
00:22:00,800 --> 00:22:03,720
Now, I built it, so I know it works.

292
00:22:03,720 --> 00:22:08,840
But if I had, I've got to have that given to me and I didn't involve, wasn't involved

293
00:22:08,840 --> 00:22:11,920
in the process, didn't know it would work, didn't know what its purpose would be.

294
00:22:11,920 --> 00:22:14,880
I might feel like, you know, what is this thing?

295
00:22:14,880 --> 00:22:15,880
It's an issue of trust.

296
00:22:15,880 --> 00:22:20,480
You trust AI, but you can't trust it if you're involved in the process of its design.

297
00:22:20,480 --> 00:22:27,640
And I think most people are designing tech with including people in the process a lot

298
00:22:27,640 --> 00:22:31,520
of the times, because it just doesn't get used.

299
00:22:31,520 --> 00:22:37,240
So a population level behavior change does require, I think, societies and I think it's

300
00:22:37,240 --> 00:22:42,800
incumbent on health services to say, look, we want to, we want to move our country in

301
00:22:42,800 --> 00:22:47,880
this direction, that might be mental health, etc.

302
00:22:47,880 --> 00:22:48,880
Let's do this.

303
00:22:48,880 --> 00:22:52,560
And these are the tools and techniques we're going to use on that.

304
00:22:52,560 --> 00:22:56,640
And that requires governments to want to do that as well.

305
00:22:56,640 --> 00:22:59,400
But also requires people to feel comfortable with the tech.

306
00:22:59,400 --> 00:23:03,680
And I think that's my concern at the moment is that there's too much rubbish being talked

307
00:23:03,680 --> 00:23:08,560
about machine learning and making it frightening when in fact, it isn't.

308
00:23:08,560 --> 00:23:16,600
You mentioned previously and now just the notion of centering this process around the

309
00:23:16,600 --> 00:23:17,600
clinician.

310
00:23:17,600 --> 00:23:20,560
What has to happen to fully enable that, right?

311
00:23:20,560 --> 00:23:24,800
You know, you said machine learning is maths, stats and computers.

312
00:23:24,800 --> 00:23:27,760
Clinicians are typically focused elsewhere, right?

313
00:23:27,760 --> 00:23:34,640
Do they need to, do we need to turn our clinicians into mathematicians, statisticians and computer

314
00:23:34,640 --> 00:23:35,640
geeks?

315
00:23:35,640 --> 00:23:44,440
Or have you come across some interesting ways to pull clinicians into the process?

316
00:23:44,440 --> 00:23:48,240
And do you have any hints at ways to do that at scale?

317
00:23:48,240 --> 00:23:53,720
So let's say a group of people in the NHS want to, a group of clinicians are wedded to

318
00:23:53,720 --> 00:24:00,840
the idea of doing changing customer practice within, after a clinical risk assessment of

319
00:24:00,840 --> 00:24:05,240
system, let's call it a chatbot, just because we've been talking a little bit about that.

320
00:24:05,240 --> 00:24:10,920
If they wanted to deploy that, the next challenge for, not for me, because we're in code for

321
00:24:10,920 --> 00:24:15,600
how we just give it away, but the next challenge is if you're not doing what I'm doing, which

322
00:24:15,600 --> 00:24:21,840
is giving away things free to the NHS, is who's going to buy these products?

323
00:24:21,840 --> 00:24:23,960
Because it won't be the clinician.

324
00:24:23,960 --> 00:24:29,560
The clinician may specify that they should be used and therein lies the other challenging

325
00:24:29,560 --> 00:24:35,080
issues within health boards or within NHS trusts.

326
00:24:35,080 --> 00:24:40,400
There are purchasing processes, typically three to five year contractual process that

327
00:24:40,400 --> 00:24:44,920
you need to go through to get that through the door.

328
00:24:44,920 --> 00:24:47,320
So getting it specified is one thing.

329
00:24:47,320 --> 00:24:52,480
Getting it passed in compliance with nice guidelines and in compliance with clinical

330
00:24:52,480 --> 00:24:56,520
guidelines is another thing, so that might take you two years.

331
00:24:56,520 --> 00:25:00,080
You then have to wait or you have to create a process for which that process, that product

332
00:25:00,080 --> 00:25:05,120
is subsequently specified and purchased, that might take you another three years, and

333
00:25:05,120 --> 00:25:10,400
I know very few SMEs, very few SMEs, and not many large companies.

334
00:25:10,400 --> 00:25:16,040
For particular technologies who have both the cash or the pre-deliction to want to wait

335
00:25:16,040 --> 00:25:23,360
to run for those types of processes to go through.

336
00:25:23,360 --> 00:25:28,560
It's not easy, and quite rightly it's not easy to implement technology in health care.

337
00:25:28,560 --> 00:25:35,480
But once you're in, the people who do it, stay there, but I have a slight problem with

338
00:25:35,480 --> 00:25:39,760
the fact that once they're in, they don't often innovate.

339
00:25:39,760 --> 00:25:45,120
They get there, they don't make any changes, so customer practice, clinical need changes,

340
00:25:45,120 --> 00:25:50,640
the systems don't change with it, and that's why, and that actually causes the thing that

341
00:25:50,640 --> 00:25:55,440
we have in the NHS, which clinicians will want innovation, and they will subsequently

342
00:25:55,440 --> 00:25:59,000
make it happen because they're not happy with what they've currently gone.

343
00:25:59,000 --> 00:26:05,680
So that's the strange cycle, I don't think it can be that unusual for other large organizations,

344
00:26:05,680 --> 00:26:09,760
particularly public sector organizations, it must be common, I would have thought.

345
00:26:09,760 --> 00:26:16,320
It actually calls to mind a lot of what you see on the traditional enterprise side as

346
00:26:16,320 --> 00:26:17,320
well.

347
00:26:17,320 --> 00:26:23,880
Take, for example, the evolution of cloud computing.

348
00:26:23,880 --> 00:26:30,480
A lot of that came out of pent-up frustration in being able to get infrastructure through

349
00:26:30,480 --> 00:26:35,840
the traditional procurement process and IT processes, so you'd have what they call this

350
00:26:35,840 --> 00:26:43,560
whole shadow IT, and it makes me wonder if there are a shadow health care type of thing

351
00:26:43,560 --> 00:26:50,360
arising where individual clinicians or groups are looking outside of these traditional

352
00:26:50,360 --> 00:26:56,240
structures, or as the bureaucracy, if we can call it that, is it so strong so that that's

353
00:26:56,240 --> 00:26:58,520
less possible in healthcare?

354
00:26:58,520 --> 00:27:02,920
You've got to bear in mind once I've been a beneficiary of the NHS for 55 years, I've

355
00:27:02,920 --> 00:27:08,520
only been within its walls for two and a half, but I would have to say that an outsider

356
00:27:08,520 --> 00:27:15,040
looking in what you've said is a fur representation of customer practice.

357
00:27:15,040 --> 00:27:20,000
So we've talked about some of the clinical challenges and along the way you've mentioned

358
00:27:20,000 --> 00:27:21,000
GDPR.

359
00:27:21,000 --> 00:27:28,200
Do you have any particular insights into how that lays out either generally or within the

360
00:27:28,200 --> 00:27:30,080
healthcare confines?

361
00:27:30,080 --> 00:27:35,100
Very much so, I've been pondering on the subject of GDPR since I came in through the

362
00:27:35,100 --> 00:27:43,880
door of the NHS, not least because it directly impacts on black box algorithms, which my

363
00:27:43,880 --> 00:27:51,680
particularly fond, so it's very difficult, it's very difficult for me and I would say

364
00:27:51,680 --> 00:27:57,240
difficult, I'd say impossible for me to recommend the use of a black box algorithm, and I don't

365
00:27:57,240 --> 00:28:01,960
have a problem doing that in some cases because we both know that majority of problems

366
00:28:01,960 --> 00:28:09,160
can be solved with, you know, logistical regression, experts, you're going to get very good

367
00:28:09,160 --> 00:28:16,640
responses on your dataset by using things that people can understand.

368
00:28:16,640 --> 00:28:26,200
But you seem to have accepted the labeling of black box to describe deep learning where

369
00:28:26,200 --> 00:28:32,800
I've come to refer to it as opaque as opposed to black box, it's hard to see, but you

370
00:28:32,800 --> 00:28:34,480
know, you can see some things.

371
00:28:34,480 --> 00:28:35,480
Of course.

372
00:28:35,480 --> 00:28:42,880
I mean, you can apply models on the outputs of a deep learning model and create an approximation

373
00:28:42,880 --> 00:28:49,040
of how it's created, like a decision tree, you can throw lots of data at a deep learning

374
00:28:49,040 --> 00:28:52,720
model and look at the outputs and explain them.

375
00:28:52,720 --> 00:28:59,200
But the worst case scenario from my perspective, and maybe becomes, you know, because of the

376
00:28:59,200 --> 00:29:04,520
background I come from, looking at probabilistic risk assessment type stuff, is this.

377
00:29:04,520 --> 00:29:10,680
I have an algorithm and it dispatches an ambulance to you or it doesn't, let's say it doesn't,

378
00:29:10,680 --> 00:29:14,800
you die, your family have a right to understand what happened.

379
00:29:14,800 --> 00:29:22,280
So let's say, the process by which that happened involved in some cases, a black box algorithm

380
00:29:22,280 --> 00:29:29,400
on opaque algorithm of some form, and you say to me, okay, well, you obviously made

381
00:29:29,400 --> 00:29:32,840
that decision based on this algorithm, how did it work?

382
00:29:32,840 --> 00:29:38,600
Now under GDPR, because of the issue associated with algorithmic responsibility, I have a legal

383
00:29:38,600 --> 00:29:39,600
requirement.

384
00:29:39,600 --> 00:29:42,760
There is a legal requirement on me, if I was the designer of that system, or indeed the

385
00:29:42,760 --> 00:29:46,320
user of that system, to explain that to you.

386
00:29:46,320 --> 00:29:50,760
It has not yet been tested in law as to the degree to which that explanation will be

387
00:29:50,760 --> 00:29:52,240
acceptable.

388
00:29:52,240 --> 00:29:59,440
So, the current process is that all such algorithms will have a person in the loop.

389
00:29:59,440 --> 00:30:03,360
But let's face it, you can put a person in the loop, the degree to which that person

390
00:30:03,360 --> 00:30:09,360
made that decision, or do we switch that algorithm in the decision, isn't always clear.

391
00:30:09,360 --> 00:30:16,160
Knit a couple of those together, and you might have an excellent explanation for the way

392
00:30:16,160 --> 00:30:19,200
in which a call was handled, for example.

393
00:30:19,200 --> 00:30:24,320
You might have some gap in speech type technology, which would understand that that call was

394
00:30:24,320 --> 00:30:28,800
handled very well, and then you might have some technologies which will take you through

395
00:30:28,800 --> 00:30:33,040
a questionnaire that will dispatch an ambulance.

396
00:30:33,040 --> 00:30:38,040
One might, the first one, might be a black box algorithm, I don't know, opaque algorithm,

397
00:30:38,040 --> 00:30:42,560
because the one might be an expert system.

398
00:30:42,560 --> 00:30:47,760
What actually was, played the greatest influence in that decision to dispatch the ambulance,

399
00:30:47,760 --> 00:30:52,320
was it that the system said I handled the call effectively and asked all the questions

400
00:30:52,320 --> 00:30:58,640
correctly, or was it the decision tree that I went through?

401
00:30:58,640 --> 00:31:03,080
It would be very hard to say the two things are linked, and if the former doesn't have

402
00:31:03,080 --> 00:31:10,320
a very good explanation that you will understand, it's quite a large financial hit, I mean,

403
00:31:10,320 --> 00:31:15,240
20 million euros, potentially, put instance of not being able to explain an algorithm.

404
00:31:15,240 --> 00:31:23,680
I don't know any SMEs in the world that prefer to take that degree of risk, and in practicality

405
00:31:23,680 --> 00:31:29,440
is termed, that wouldn't happen, I don't see that happening, but it's not been tested,

406
00:31:29,440 --> 00:31:36,080
so it's very difficult for me to suggest the widespread use of opaque algorithms because

407
00:31:36,080 --> 00:31:37,760
of this issue.

408
00:31:37,760 --> 00:31:44,720
Even though I know, in some cases, some specific ones that the algorithm may be more accurate

409
00:31:44,720 --> 00:31:49,440
and have less risk associated with it, but if I can't explain it, then what do I go

410
00:31:49,440 --> 00:31:50,440
with that?

411
00:31:50,440 --> 00:31:55,400
So it's been, I completely understand GDPR, I think we have to comply with it, we have

412
00:31:55,400 --> 00:32:01,880
no choice, if we want trust in machine learning, we need to be able to explain things well,

413
00:32:01,880 --> 00:32:08,760
so I've been showing a lot of those people running, well, since the days of line, when

414
00:32:08,760 --> 00:32:15,560
line was originally, how many years has that been around there, two, it's been a couple,

415
00:32:15,560 --> 00:32:21,160
so if somebody were able to solve the problem of explaining how that works, in a manner

416
00:32:21,160 --> 00:32:26,280
that would be acceptable to a member of the public who was affected by it, I'd be a very

417
00:32:26,280 --> 00:32:31,560
happy man, because I think these algorithms have a very important role for just speeding

418
00:32:31,560 --> 00:32:36,720
up the process for which we learn about things, not necessarily just doing things, how we

419
00:32:36,720 --> 00:32:38,440
learn about things.

420
00:32:38,440 --> 00:32:44,440
Do you have any views as to, well, I mean, I mean, obviously, this typically is something

421
00:32:44,440 --> 00:32:49,240
that were you ask very useful questions of the person that you're interviewing, and

422
00:32:49,240 --> 00:32:53,440
they hopefully come up with decent responses, but I'm sitting here, left scratching my

423
00:32:53,440 --> 00:32:59,120
head as to the best way to go with deep-lying algorithms in terms of the explanation

424
00:32:59,120 --> 00:33:00,440
of them.

425
00:33:00,440 --> 00:33:07,080
Given this podcast, maybe listen to by people in healthcare, what techniques do you think

426
00:33:07,080 --> 00:33:11,240
we should be considering in healthcare, because let's face it, you have one of the few

427
00:33:11,240 --> 00:33:16,840
people in the world that goes out and speaks to leading people in the area of machine

428
00:33:16,840 --> 00:33:22,680
learning in AI, so I guess from the nature in which you ask questions, you also have

429
00:33:22,680 --> 00:33:25,680
a reader of papers, and you understand how things are coded.

430
00:33:25,680 --> 00:33:29,640
Do you have any idea as to what sort of things we should be considering?

431
00:33:29,640 --> 00:33:37,840
I think the way I think of the explaining deep-learning landscape is that there are a

432
00:33:37,840 --> 00:33:41,000
couple of broad camps.

433
00:33:41,000 --> 00:33:49,000
There's one broad camp that is basically try to fit some explainable model to the output

434
00:33:49,000 --> 00:33:54,120
of your opaque model, and you've mentioned and acknowledged that camp, and the other

435
00:33:54,120 --> 00:34:05,200
is try to gain some insight into what individual neurons are doing by introspecting weights and

436
00:34:05,200 --> 00:34:08,200
things that are happening in the network.

437
00:34:08,200 --> 00:34:11,680
I think that's a step towards explaining, but it's not quite.

438
00:34:11,680 --> 00:34:20,440
I don't think it produces anything yet that necessarily meets the bar of explainability.

439
00:34:20,440 --> 00:34:23,680
I think that's more providing insight.

440
00:34:23,680 --> 00:34:31,040
There have been a couple of conversations with folks that are offering software tools

441
00:34:31,040 --> 00:34:40,240
that aim to one way or another provide the user who's deploying these tools with a dial

442
00:34:40,240 --> 00:34:48,680
that lets them dial in explainability when needed, presumably at the cost of model performance.

443
00:34:48,680 --> 00:34:56,600
But in a case where at the NHS, that dial will be pinned all the way to one end, right?

444
00:34:56,600 --> 00:35:03,640
Then I think you're back to these two types of approaches, and the various methods of

445
00:35:03,640 --> 00:35:10,120
training a decision tree or something like that against a moral-pake model is what I've

446
00:35:10,120 --> 00:35:14,400
heard the most of happening in real life.

447
00:35:14,400 --> 00:35:18,520
I'd love it to happen, but I think the other challenge we've got is when you string

448
00:35:18,520 --> 00:35:23,960
a number of them together, who takes responsibility for the decisions each one of them have made?

449
00:35:23,960 --> 00:35:26,800
That's an interesting thing in itself.

450
00:35:26,800 --> 00:35:31,200
If you've got a dial, how many dials can you afford to be be bothered to press whilst

451
00:35:31,200 --> 00:35:33,360
you're waiting for an ambulance?

452
00:35:33,360 --> 00:35:35,360
Yeah, it's a big issue.

453
00:35:35,360 --> 00:35:39,760
I think time will tell us to whether GDPR had a positive or negative effect on health

454
00:35:39,760 --> 00:35:41,560
care AI.

455
00:35:41,560 --> 00:35:46,920
At the current time, I think we need to spend more time explaining what we're doing and

456
00:35:46,920 --> 00:35:52,320
making less of a bogeyman, so maybe slowing things down as it is doing is a good thing,

457
00:35:52,320 --> 00:35:57,880
so maybe I'm being slightly pragmatic and thinking on the positive, but I think that

458
00:35:57,880 --> 00:36:03,320
wouldn't be a bad thing, and there are plenty of ways in which organisations can make money

459
00:36:03,320 --> 00:36:06,680
with deep learning algorithms and it doesn't have to be in health care.

460
00:36:06,680 --> 00:36:11,920
I think the challenge, however, when it comes when you don't apply, I'll pick algorithms

461
00:36:11,920 --> 00:36:18,400
to problems that are just intractable with any other way, so I'm talking about computer

462
00:36:18,400 --> 00:36:20,600
vision type problems.

463
00:36:20,600 --> 00:36:25,600
So looking at a label data set for say glaucoma images, which is what I'm going to be doing

464
00:36:25,600 --> 00:36:36,320
next month, and building an algorithm that could identify, say, features of those particular

465
00:36:36,320 --> 00:36:40,920
images that might help somebody understand whether glaucoma is early on say glaucoma is

466
00:36:40,920 --> 00:36:45,520
happening, I think the people who would like me to do it need to understand you can't

467
00:36:45,520 --> 00:36:52,160
do it any other way than with some form of a vague algorithm, and therefore the ramifications

468
00:36:52,160 --> 00:36:57,800
are going to be, it needs to be one where it's a very low risk, how wrong can we fall

469
00:36:57,800 --> 00:36:59,960
to be on this particular subject?

470
00:36:59,960 --> 00:37:05,320
I think that's anybody applying AI in this sector needs to think of that first, and I

471
00:37:05,320 --> 00:37:12,160
don't know enough about the subject as I'm expecting to be in May when I get briefed

472
00:37:12,160 --> 00:37:16,240
by the clinician, but that's the first question I'm going to ask is how wrong can we fall

473
00:37:16,240 --> 00:37:17,240
to be?

474
00:37:17,240 --> 00:37:23,640
Yeah, there's a podcast I did recently with Ryan Publin, who's a research scientist or

475
00:37:23,640 --> 00:37:31,640
research engineer at Google who worked on looking at retinal fundus images and trying

476
00:37:31,640 --> 00:37:39,120
to predict, I think it was diabetes in his case, and his research ended up focusing on identifying

477
00:37:39,120 --> 00:37:46,040
potential risk factors, in other words, inferring demographic information from the retinal

478
00:37:46,040 --> 00:37:49,440
scans, like age and...

479
00:37:49,440 --> 00:37:55,920
Yes, right, right, so they looked at a whole bunch of data sets and came up with inferences

480
00:37:55,920 --> 00:38:00,400
about the nature of the person that didn't relate to the condition that they were looking

481
00:38:00,400 --> 00:38:01,400
for.

482
00:38:01,400 --> 00:38:09,240
That's right, and so in that interview, there was an interesting tidbit about how his grappling

483
00:38:09,240 --> 00:38:14,240
with this explainability issue and the context of being able to go back and validate these

484
00:38:14,240 --> 00:38:24,200
results with the clinicians, and what he suggested was that you've got a deep neural network

485
00:38:24,200 --> 00:38:28,880
model, you've got this layer at the end that says yes or no or makes a prediction of some

486
00:38:28,880 --> 00:38:35,480
sort, but the roughest level it sounds like what they were able to do is kind of push back

487
00:38:35,480 --> 00:38:45,240
a layer and maybe train a layer on identifying features that are kind of known contributors

488
00:38:45,240 --> 00:38:53,840
to a particular condition, and it does strike me that maybe that might help us still use

489
00:38:53,840 --> 00:38:57,080
these opaque algorithms, but pass the explainability test, right?

490
00:38:57,080 --> 00:39:01,960
If the algorithm's not making the decision, it's just providing, it's augmenting the intelligence

491
00:39:01,960 --> 00:39:02,960
of the clinician.

492
00:39:02,960 --> 00:39:08,080
I think that I think therein lies an interesting and probably important factor that we need

493
00:39:08,080 --> 00:39:13,280
to consider is that it's different doing healthcare research than implementing systems

494
00:39:13,280 --> 00:39:14,840
that augment practice.

495
00:39:14,840 --> 00:39:21,200
So I'm particularly focused on implementing systems that can augment practice.

496
00:39:21,200 --> 00:39:28,680
The use of deep learning systems or any opaque algorithms that can improve people's understanding

497
00:39:28,680 --> 00:39:33,400
of causation of illness is wholly appropriate.

498
00:39:33,400 --> 00:39:39,120
The difficulty comes as to when somebody needs to understand why that came to that discussion,

499
00:39:39,120 --> 00:39:44,880
but I think it opens up identifying feature sets that people clinicians need to consider

500
00:39:44,880 --> 00:39:51,880
in more detail, as that one did, is highly appropriate for healthcare research and should be

501
00:39:51,880 --> 00:39:58,520
considered, because it wouldn't be implemented in this particular algorithm that's going

502
00:39:58,520 --> 00:40:01,880
to be used for diabetic myopathy.

503
00:40:01,880 --> 00:40:02,880
I believe so.

504
00:40:02,880 --> 00:40:07,480
I'm not sure I got that term right.

505
00:40:07,480 --> 00:40:21,680
Maybe to be more concrete about it to make up some terminology and apply it to this

506
00:40:21,680 --> 00:40:28,760
example, but intuitively there's a big difference between using an opaque algorithm to determine

507
00:40:28,760 --> 00:40:35,600
whether I have the diabetic condition based on an eye image and using an ensemble, let's

508
00:40:35,600 --> 00:40:42,560
say, of opaque algorithms to identify or maybe a single opaque algorithm to identify features

509
00:40:42,560 --> 00:40:48,920
that may be difficult for me as a clinician to identify or time consuming, but to raise

510
00:40:48,920 --> 00:40:54,480
the flags and say, well, there's this kind of vein structure, there are these kind of

511
00:40:54,480 --> 00:40:57,760
spots, there's this kind of coloration.

512
00:40:57,760 --> 00:41:05,440
So this is a candidate that should be explored more fully by the clinician.

513
00:41:05,440 --> 00:41:10,160
So I think the point I'm trying to make is maybe don't throw the baby out with the bathwater

514
00:41:10,160 --> 00:41:12,160
kind of thing here, right?

515
00:41:12,160 --> 00:41:20,240
There's still potential value in these opaque algorithms, but it's really where potentially

516
00:41:20,240 --> 00:41:22,680
where the decision is being made.

517
00:41:22,680 --> 00:41:23,680
Awesome.

518
00:41:23,680 --> 00:41:24,680
Awesome.

519
00:41:24,680 --> 00:41:27,920
Well, Joe, thank you so much for taking the time to chat with me.

520
00:41:27,920 --> 00:41:28,920
It's been great.

521
00:41:28,920 --> 00:41:29,920
It's been awesome.

522
00:41:29,920 --> 00:41:30,920
Well, thank you very much.

523
00:41:30,920 --> 00:41:34,760
I've really enjoyed it and keep up the good work and you're a very good communicator

524
00:41:34,760 --> 00:41:39,720
in the area of machine learning, and if I have my way, lots of people in the NHS will

525
00:41:39,720 --> 00:41:43,600
be listening to this podcast, not to me, but to what you do on a regular basis.

526
00:41:43,600 --> 00:41:44,880
Well, I really appreciate that.

527
00:41:44,880 --> 00:41:45,880
That works.

528
00:41:45,880 --> 00:41:46,880
Enjoy your weekend.

529
00:41:46,880 --> 00:41:47,880
You too.

530
00:41:47,880 --> 00:41:48,880
Have a good one.

531
00:41:48,880 --> 00:41:52,120
All right, everyone.

532
00:41:52,120 --> 00:41:53,840
That's our show for today.

533
00:41:53,840 --> 00:41:59,080
For more information on Joe or any of the topics covered in this episode, head over to

534
00:41:59,080 --> 00:42:03,880
twimmaleye.com slash talk slash 169.

535
00:42:03,880 --> 00:42:07,800
If you're a fan of the pod, we'd like to encourage you to head over to your Apple or

536
00:42:07,800 --> 00:42:14,080
Google podcast app or wherever you listen to podcasts and leave us a five star rating

537
00:42:14,080 --> 00:42:15,680
and a review.

538
00:42:15,680 --> 00:42:20,120
They're super helpful as we push to grow this show and community.

539
00:42:20,120 --> 00:42:34,160
As always, thanks so much for listening and catch you next time.

