All right, everyone. I am here with Dylan Herb. Dylan is the co-founder and CEO of PaperSpace.
Dylan, welcome to the Twomble AI podcast. Awesome. Exciting to be here.
It's great to have you on the show. I was thinking back to when we first met and you introduced me
to this great coffee shop across the the street, I guess, or around the corner from your old offices
in Brooklyn. Looks like you've got a new space now where you've got lots of cool plants.
Yep, still in Brooklyn, just down the street, but yeah, I guess it's been a while. Glad to be here.
Awesome. So we are going to be focusing in on the general topic of machine learning as a software
engineering discipline, touching on ideas like ML ops and CICD and what all that means for folks
that are trying to get ML into production. But before we do that, let's start with a little bit
of your background and how you came to, you know, found a company that is, you know, in the center
of helping folks, you know, work on machine learning problems. Yeah. So time flies, but we've
actually been at this for about five years now. My background and this will kind of inform some
of the, probably the conversation today, but my background is a little bit nontraditional. I
studied building architecture and kind of on the architecture engineering side.
And what used to be probably more in like the HPC world, so working more with like genetic algorithms
and I was doing things around topology optimization, that led to an interest in parallel compute
and GPUs were really the name of the game and have continued to be so. And so I started paper space
with my co-founder and good buddy Dan, Dan Cobran. And yeah, so we, you know, our general premise
and it's definitely evolved was that, you know, GPUs as sort of a kind of parallel compute device
would make its way into the cloud and there would be all sorts of, you know, new applications that
would emerge from that. And I think what we, you know, weren't aware of at the time was that the
big driver of that was going to be this, you know, shift from like visual compute into just,
you know, massive data processing and the emergence of like deep learning as kind of a practical
discipline as well. So yeah, that's how I got into it. And now I primarily just think about that,
which is sort of how do these, you know, new tools evolve and support kind of emerging workflows,
tools, products, things like that. Yeah, I think you and Nvidia maybe have that in common,
you know, we're going after a totally different use case for GPUs, one that was graphics focused
and were quick to recognize the activity or the shift that was happening around ML and AI.
What were, what were some of the first use cases that you, you saw that kind of clued you into
interesting activity in this space? Yeah, I mean, it was, when you're an early stage company,
you kind of, you know, I was manning support as well as doing everything else to company. And so
you email everyone who signs up and you're just like, hey, what are you up to? And we started,
you know, this was new to me, but we started getting more requests for people saying, hey, I'm
trying this thing out called TensorFlow. You guys have GPUs. This was before really the big cloud
providers had offered GPUs as just like a, you know, a native compute device. Obviously today,
they're, they're pervasive. And so we're like, tell us more, you know, what are you building? And
so that's kind of been the format of the conversation for, you know, the last four years. And so,
you know, in that time, we've seen, well, actually two things interestingly. One, you know,
the emergence of like this deep learning practice where you need a lot of compute. So you need a
lot of things. You need data, you need compute, you need expertise, but compute is a big part of it.
You know, you can, you know, you're, you're not going to make a driverless car algorithm on a
Chromebook. So there is a compute requirement. And the kind of the interesting thing is even today,
and I'm sure this is, you know, bumped up in, in your universe as well, which is sort of the,
the blending of the visual compute kind of side of things or everything from like image generation,
deep fakes being sort of the big example, or now we're seeing things like synthetic data. So the
compute pipeline and the visualization pipeline, I think, are collapsing in a really interesting way,
where, you know, visual effects are, are, you know, just as interested in deep learning as like a,
you know, thing in their tool belt. So, so it is a, it's been a weird few years where,
you know, the first pitch deck, I remember the first slide was, what's a GPU?
Which luckily, we don't, we don't have to bring that one up anymore, people kind of get it.
Nice, nice, nice. And paper space has been popular in our community for quite a while. We
do a bunch of study groups around courses like Fast AI and others. And, you know, for years now,
folks, in trying to do those courses, you look for infrastructure in which to run it on and paper
space has always been one of those options. But the company is doing more now than just, you know,
a server instance with the GPU, kind of going up the stack. Can you talk a little bit about that
and the motivation there? Yeah, definitely. So, paper space is the name of the company and our kind
of main product or kind of collection of products is called Gradient. And that encompasses
really a handful of components around what, you know, we'd probably call an ML ops platform.
And, you know, we definitely should dive in to what exactly that comprises. But, yeah,
today we, you know, provide tools for pretty much every step of the kind of building the machine
learning model process. But really with an emphasis, again, coming from our kind of background in
the infrastructure side of things, like a focus on where the kind of machine learning world intersects
with the, you know, infrastructure or networking issues or more broadly, just how does it interface
with, you know, traditional software development and application development that, you know, has also
been changing quite quickly, but has, you know, become a relatively established pattern for, you know,
all sorts of companies to build on top of. Yeah, and as you mentioned, this has been a quickly
evolving space in terms of the, you know, your perspective on, you know, slash takes,
slash interest in the ML ops and infrastructure side, or, you know, to what degree are,
are you, I guess, productizing or, or, you know, drawing learnings from the experiences that you
had, you know, standing up these, you know, these data centers with GPUs versus, you know, stuff
that you're seeing evolve elsewhere outside of the company. Like, what's kind of the core,
the core around which you're, you know, building out an approach to ML ops?
Yeah, so I think there's kind of two levels of it. The first level is just like, you know,
what does the thing do? And to date, you know, we offer, you know, one click sign up any developer
and one of the most popular ways of onboarding is, you know, through a course like Fast AI,
which will be, you know, partnering with them the next version as well. This sort of the entry
point and the question is, you know, how can you kind of get right into the media, the problem,
start looking at some pie torches or TensorFlow or, you know, try out GPT-2 or something like that,
without even thinking about infrastructure. So there, you know, there's kind of the practical
piece of that, just onboarding to it. And then on top of that, the question is, you know, what are,
I think, across the industry, there's this a lot of interest in creating purpose-built tools
for machine learning because we can all agree or people that, you know, spend some time with it will
say will acknowledge that, you know, it does have some fundamental differences from traditional
software development. You know, we can talk about the programming languages. Some folks are coming
from R or their backgrounds in stats or math. The hardware itself, obviously, you know,
GPUs are not required for building out a web app. And within machine learning, there's obviously
a whole host of types of machines. So there is a particular need to think about the hardware as
a primitive in your system. So, you know, so when you zoom out kind of collectively, I think the
bigger problem in the way we talk about, you know, the way we evangelize it is to say, we believe
machine learning is a part of a traditional software engineering practice. And the good thing about
that is when you start diving into something like an ML ops platform, you have a lot to draw on,
which is, you know, a whole 10 years of, at least, of, you know, modern software development practices.
So that encompasses things like continuous integration, continuous deployment, obviously,
tight integrations with source control management, and in the case of machine learning, you know,
data management, and, and provenance. And, and you start thinking about it in a different way. So,
the benefit of a traditional software development practice that might be like agile or, you know,
based around CICD is that you can, you know, we're a software company that builds a web application,
a web console, a lot of tools. And we can, you know, bring new people into that collaborative effort,
and they can work in sort of guidelines that means that they can very quickly start, you know,
shipping production code. So machine learning is newer in the sense that, you know,
not every company is in the, like, we need to ship this into production. But if you look at the
process of, like, testing some assumptions, building a little model out, and then trying to get into
production, the, the kind of, the framework that you need to think about it in is just like any
other software, you know, development paradigm. And we feel strongly that, you know, this,
the closest analogy is probably something like mobile app development, where, you know, historically,
it was like, you have your iOS team when the iPhone came out and you have your, you know,
regular software team or your website team, and eventually those two merged, and the,
the baseline practices are the same. And we think that's the, you know, the case for machine learning
as well. We're just at the beginning of that curve. Now, a lot of folks pushed back on that and
say that there are fundamental differences in the way that machine learning and data science more
broadly should be practiced. And it's not, you know, it's not as predictable that as software
engineering. You know, what's your, you know, how do you respond to that kind of? Do you,
well, first of all, do you even encounter that? Like the, the, yeah, I kind of push back and what's
your, what's your take on that? Well, I think it's an emerging discipline that has, that has moved
quite quickly. And it's also happened at the same time that we've, you know, it's, it's happening
the same time that we're also seeing pretty large, large shifts in how software is developed. And
I think the biggest trends here we can talk about are, you know, containerization, the idea that
you can have kind of reproducibility at the, at the, as like a primitive in your pipeline.
The emergence of these kind of more complex tools like Kubernetes, which is still, you know,
a complex thing to set up and to manage. And you have companies that are seeing the benefits
of this. But fundamentally, we're starting to, you know, software development is shifting. And
that's happening right as machine learning is trying to figure out sort of what its relationship is.
So I think there is a lot to that. And I think it, in the sense that there are people that would
argue that maybe the foundational part of machine learning is more exploratory. It doesn't quite
have, you know, one-to-one mappings into, you know, software development. But I don't think that is
true across the board. And by that, I mean, you know, there are a lot of really useful analogies
we can draw from. You know, we, we think a lot about, you know, if you think of Git or GitHub as
sort of a standard in collaboration on software projects today, there's some fundamental act,
you know, things you can do, you know, you can fork, you can, you have pull requests, you have
branches, they don't map perfectly to the machine learning model. You know, your iteration
could be very quick on a machine learning model or it could be really slow if it takes a long time
to train. But in either case, there are useful things to draw on, you know, development staging
and production environments, you know, being able to promote things. And that's, that's a traditional
software engineering practice largely. So the question is how do they interface? And that's what we,
you know, spent a lot of time on gradient trying to tease out where, where you can get the best
to both worlds. Like, where can you draw on those kind of best practices? And, and where does it
diverge? Like the reality is, if, if it were just a software development practice, you could use,
you know, Jenkins or, you know, a traditional kind of CICD pipeline runner for machine learning.
And you really can't do that. There are, you know, the artifacts are different. The machine learning
model is a very important artifact at the center of this. Data is a, is a, you know, as important as
code as a primitive, as an input to the pipeline. So, maybe dig into that a little bit, in a little
bit more detail. I am imagining that folks that are coming at things from the more traditional data
science perspective don't really have any idea what a Jenkins is or CICD pipeline. You know,
maybe talk through, you know, kind of your take on that traditional world from a software
engineering perspective. And then, you know, we can talk through like how it maps to, you know,
the way you see the ML ops world. Yeah, definitely. I mean, I think at, at the foundational level,
the idea is, you, you commit to, you know, you, certain things get committed into your development
process. Even, you know, literally as part of like a config file that goes into your repo or
something that says like here are the steps of the process, meaning, you know, we transform some
data. We send it to this machine type with this, you know, container and this framework tenser
flow 2.1 or whatever. And this has an artifact that we then deploy. So, there's kind of a pipeline.
And so the foundation of CICD and there are a lot of, you know, components to it is that if you
can make those more reproducible and deterministic, then ultimately is a software developer or an
organization trying to, you know, build an impactful machine learning model that you can do that
more quickly and, you know, more safely because you can sort of see what every step of the process
was. So, you know, when you talk about machine or folks talk about machine learning is sort of a
black box like you put in data and you get out some predictive model that's like hard to interrogate.
I think part of it is that this is really powerful, powerful algorithms that are still hard to
fully understand. But the other part of it is the normal way of developing them at least, you
know, we work with everyone from, you know, university students to, you know, organizations like
that are very much coming at it from like a business intelligence perspective. And in both cases,
you know, because it's a new practice, there aren't patterns fully established yet. So people
are frantically looking for the, you know, the tool stack to make it happen. And I think that ML
ops has kind of done itself a bit of a disservice in that it presents itself as such a totalizing
thing, which is really not if you're thinking about it from a software development perspective.
It's not the, that's really, there aren't good parallels for that. Like in traditional software
development or if we're making our web application today or your iOS app or whatever, you're using
30 different tools. But you're doing it in your structuring it in a way that is, you know,
roll an agreement on it. You know, you commit sort of the important parts of the pipeline,
you add tests, you add checks so that someone can come in. They can, you know, everything is
tied to source control. So, you know, you can, you can experiment in a branch and then come back
in. And I think a lot of those are really useful for thinking about, you know, machine learning,
not, not, you know, in its entirety, but certainly a lot of the components that people struggle
with today and taking this from R&D into production, which is really the name of the game.
You know, I think my, my guess is that folks who know paper space and, and gradient in particular
from a user perspective, think of it as, you know, primarily what the interface that it's
providing, kind of a notebook like experience and, you know, maybe notebooks as a service we could
call it. And when you think of ML ops platform, you know, you think of all this enterprise-y stuff,
containers and Kubernetes and things like that. How do you get from one to, to the
next is all that stuff, just the stuff that you're using to allow folks to spin up these notebooks
or what's the relationship? Yeah, so, so we call it kind of like the gradient notebook lab,
which is, you know, the wrapper around Jupyter. You actually use other kind of IDE-like
environments, but Jupyter has certainly become the de facto standard. That's really only one
part of gradient, but it's all built on the same foundation. And that foundation is committed to
the idea of reproducibility adding UUIDs to every step of the process. So even if someone comes in,
and the most popular kind of product we have is certainly notebooks. Anyone can come in, we have
a tool called the ML Showcase where they can one click, run it on a free instance in the cloud and
kind of, you know, get their feet wet with the machine learning tooling. And that's important
because our general view is that for every thousand software developers, you have maybe one
who's a machine learning expert. So the question is, how do you onboard the machine learning technology
for everyone else? And they have to try it out. So notebooks are certainly one part of it,
but as folks kind of get more advanced within gradient, we offer a lot of like kind of
ways of making the pipeline sort of building on top of it. So the notebook is always running
in a container. When you shut it down, we do a Docker commit to make sure that you have sort of
the reproducibility in the history. And then you can go a step further and then connect that to a
GitHub repo. You can actually, we have like a job run or architecture, very similar to how
kind of the paradigm has been established where you can use a CLI or Python SDK and then just
send a task and have it get scheduled across a single node or a large scale distributed set of
nodes. And then tools like we have one called gradient CI, which is just a GitHub bot or application
where we give you rich pull request information back. So notebooks are the entry point, but they're
not exclusive or they're certainly not the end of it. It's really the question of, where are you on
the kind of data processing or development, machine learning development lifecycle?
As you've built this out, where the areas that, you know, what areas presented the most
challenges? And, you know, we can kind of take this from different levels. One thing that I'm
thinking about you mentioned, like you're transparently committing stuff to get like, are you
are you doing that? You know, whenever I close out of a notebook or every time I, you know, run a
new cell or like, how do you? Yeah, that's the connection between, you know, challenges and that
question is like, you know, there's all this questions about granularity and I imagine that,
you know, you've had to figure out a lot of that stuff along the way. Yeah, so I would say to
that particular one, I think that is still unresolved in the sense that I don't think there's
the best version of it that will exist in a year or two that exists today and I think it's been a
lot of back and forth. You know, you know, Jupiter notebooks are formatted JSON objects with
sort of a special cell type and everything else and they don't really map super well to the code
that's actually running in it, at least from like a code differing or source control management
perspective. And you got folks like interact coming out of Netflix that are trying to, you know,
do interesting things with notebooks to make them more runnable. You have folks that are trying to
take notebooks and turn them into executable objects. There's this whole ecosystem of innovation
happening around the notebook itself, not to mention stuff that Jupiter is doing. So it's a
good moving target from that perspective. Totally. And from our perspective, it's just a container
and I think that's the important part. You know, we actually, big fans of Interact, I think that's
a really important step in the, you know, basically they've created a TypeScript and JavaScript,
like React Library, a web framework kind of interface for Jupiter. And I think that's doing a
lot to move this as, you know, another kind of traditional primitive that can be used in the
Webstack. But from our perspective, it's a container that has to be scheduled on a node somewhere
and even within that kind of part of the process, there's a lot of complexity. You know, there
are a million types of instances you can run on. There are a million ways of, you know,
setting up how you do storage and artifact management and things like that. And our perspective is,
you know, you should, there are sort of like best practices of like, hey, we can, you know,
one example here, we, within gradient, there are experiments that you can run, which are just,
you give us a container in some code and we're going to execute that as well as notebooks. And
both of them, you know, at the fundamental level of gradient, which is actually built on top of
Kubernetes, is that we're thinking about these as containers with a lot of nice kind of glue
in between. So, for example, providing a single persistent high performance directory that's
available across notebooks, experiments, and ultimately your deployments as well. And so,
thinking about as like an infrastructural problem where you want to, you know, deploy a model,
be able to see what, you know, what experiment created it, who, what line of code generated that,
and then also use the feedback from that, you know, in the kind of holy grail here, which is to
create a kind of continuous life cycle. And, you know, I think that's also an unsolved problem,
but the way to solve it is, you know, I strongly believe is one that is looks more like a traditional
software, you know, engineering stack issue. And so, you know, we, you've seen a lot of like,
a lot of the dialogue in the, in this ecosystem is around like build versus buy or, you know,
what's the one true platform? Is it this, you know, is it the SageMaker or Qflow or gradient?
I think that the, the, the way this will resolve itself is that it'll be just like everything,
everything else. Your every software team will build certain components by certain components,
and mix and match them in a way that looks probably a lot like how, you know, software gets developed
today. You know, when we send our website out, we have like every release, we, we use tools like,
you know, code coverage, regression testing libraries, visual testing libraries. You know,
we use CircleCI for our CICD engine. We use a lot of ARGO. There's a whole host of tools,
and within this organization, we have our own practices. Like, you know, we've defined among
the engineers here, myself definitely less now, but, you know, how the, you know, what's the,
how, when do you make a branch? When do you make a pull request? When is it okay to make a release
on something? So a lot of that, you still have to build a lot of your own methodology around software
development. That's what makes a, you know, an organization effective. But the tooling itself, you
know, you will need purpose-built tools, and we're, we're kind of making the case that we're offering
a version of this that we think is, you know, built to, you know, grow into the next iteration,
where, you know, machine learning becomes more prolific, not as, you know, as a domain, but as just
to software engineering practice, kind of more fundamentally. Do you think that the, at a given
organization, the tooling is best provided by folks that are familiar with the, this
soft existing software engineering, you know, practices, or, you know, folks that are more
coming at it from the, the data science side. And, and, you know, curious what you've seen in
that regard? Yeah, I mean, I guess I'll add to the controversy. I do think this is something that,
you know, at least at the very least has to engage the kind of CISOPS DevOps person, or the,
or the team that's responsible for shipping production grade code, you know, whatever that looks like.
You know, I don't think it's the domain of the data scientists or machine learning engineer
necessarily. But I also think that the interface needs to be better defined, meaning, you know,
we have to find a way that people can easily integrate with source control management and,
you know, the primitive artifact that gets deployed. Like, for example, you know, you've heard stories,
I'm sure of, you know, a model gets developed and then it gets rewritten in Java by somebody else
so that they can actually deploy into their system. I think the Kubernetes, you know, containerized
future and that in the world we're living in presents a nice kind of in-between model,
which is that if you can containerize this thing and sort of define its inputs and outputs,
then the artifact that comes out is something that you can hand off to the DevOps team and they
can they can deploy effectively, you know, and they can deploy tools like, you know,
Prometheus or, you know, metrics and logging and things that really are not within the scope of the
machine learning kind of domain. And I think that's it's the separation of concerns that I think
isn't really well defined. And so the outcome of that is that really sophisticated companies,
you know, Facebook, Spotify, et cetera, you know, they have to build their own tools because they
need to push this discipline forward. That's not going to be the case for most companies in the
world and then the question is sort of what does that tool stack look like, you know, in ours as a
proposal that is very much designed for, you know, a company that wants to produce production
grade software and how machine learning integrates inside, you know, into that process.
Yeah, that's a really interesting example you gave. Like everything or like many things in tech
maybe it's there's a bit of a pendulum going back and forth. When I first started tracking this
space eight something years ago or more, a lot of the early folks, a lot of the early
goals and messaging of these early tool vendors, most of which are not still around was,
you know, your data scientists are developing software or developing models in R and SAS and things
like that and you have no way to put them in a production. And so you have to hand them over to
folks that are, you know, like you said, building them in Java or building them in C, C++, whatever.
And the, you know, the whole idea of a lot of these early tools was that you can, you know,
develop and deploy and Python, right? And a lot of the argument was that you wouldn't have to pull
in a separate person. The data scientists could just do that. And then the, the next wave of that
was, well, you know, ah, maybe that's not going to work. We've got our data scientists. Let's
hire a bunch of machine learning engineers and, you know, they'll be part of this process. And
as opposed to kind of letting the tooling handle the productionalization, we've got people that
come from more software engineering background. Um, and it sounds like you're talking about a further
evolution of this pendulum, which is, well, containers will just do it for us. Yeah, I mean, I think
that, I don't know how much I'll commit to that particular version. I, I agree it's a pendulum.
I, I do think that, um, you know, like I've said, I think this is a software engineering practice
or at least there are a lot of, you know, pieces we can pull from that. Yeah. I do think it's made
leaps and bounds forward on in the sense that, you know, containers are really, really helpful for,
you know, building out web applications. They're portable, they're reproducible, you know,
my dev environment is similar to yours. It's, you can't map that perfectly to, um,
uh, you know, the machine learning development process. But there's, there's also, you know,
newer concepts like there's, uh, in the Kubernetes world, there's a lot of talk about, you know,
solving the inner loop of development, which is, um, basically saying, you know, we, we have the,
as a software software engineering problem, we have these complicated systems, you know,
uh, paper space and gradient is a whole lot of services at a bunch of microservices,
a bunch of big services. Um, how do, how do you, uh, basically keep a fidelity between your
production environment and your development environment in the, you know, because maybe your
laptop isn't going to run the whole host of production environments. So, you know, a lot of
interest in, or a lot of software development from coming out from Google, we have,
got tools like scaffold and the idea here is that, you know, you want to, uh, they call the inner
loop of development, which is like, before it goes into production, someone's just iterating
on it, which I think actually is what a lot of the data scientists who push back on the machine
learning or the ML ops kind of paradigm that is totalizing is, you know, uh, they're like, oh,
well, I'm going to, I'm going to work in a notebook here locally and I just, you know, I don't think
every line of code here needs to be committed to source control. And that's, and that's true,
you know, you don't need to, you don't need to get, you know, heavy handed with it. Um, but it is
an open problem and open discussion in even the Kubernetes world way, you know, uh, totally distinct
from machine learning world. Um, and I, you know, I think that the conversations are most productive
when the two, you know, engage in that way. Um, but yeah, I mean, I, like, people that are interested
in the kind of deriving insight from a machine learning model, uh, you know, they want to be working
in these amazing tools like TensorFlow and PyTorch. But at the same time, if you look at like PyTorch's
evolution, um, is like another kind of, uh, proxy of this, you know, um, they have native C++
bindings and there's a lot of, there's a lot of, um, uh, push to make this a regular software
development thing because PyTorch has all sorts of limitations. It's a, it's a very expressive
language, um, very powerful in machine learning, uh, but isn't maybe the best one for, you know,
squeezing out the performance from your incredibly expensive cloud GB resources or something like
that. Um, so I think it's coming from both dimensions, you know, if you've got like the software
engineers trying to be more machine learning stuff, you've got the ML folks that are like, yeah,
we want to, you know, become more, uh, kind of pervasive in an organization or have higher impact
in what gets shipped, um, and it's an evolving practice. But I, I do think that, you know, uh,
uh, I, I, or sorry, I would say I don't believe that there's one ML ops platform to rule them all
and it has to do the best training, you know, uh, model aggregation, quantizing, and, uh, you know,
AB testing and like the full end end. But I do think that there's a lot of value in tools,
you know, like cube flow, which I would say is probably the closest analogy to, to gradient, um,
where, you know, you're setting up something that's kind of foundational and there's,
you know, in the case of cube led, and there's a lot of setup that's required. It is a complicated
tool stack and in the case of gradient, it's kind of like, you know, we're going to manage it for
you a bit more. Um, but yeah, I think that enlisting the, you know, DevOps systems people who
have to manage the same, keep it running early on the process sets you up for success and allows
you to ultimately get all the things we always talk about, you know, more models faster, you know,
higher impact, all that kind. Right. Right. And is there a relationship between gradient and,
and cube flow is gradient, you know, does gradient use parts of cube flow or is, uh,
do we think of gradient as a paper space analogy to cube flow, but one that, you know, has features
that are specific to the way you view the world? Yeah. Um, so it's an interesting question.
I think it's evolving as well. I think cube flow is a really interesting project. It's still
relatively early. And I think, um, you know, Kubernetes is also something that, you know, not a lot
of organizations have expertise in yet. So it is something that is a little bit harder to get going,
but very powerful. Um, where we overlap is we are also based on Kubernetes. I mean, Kubernetes is
just container orchestration, some networking stuff, and some nice best practices. And I'm
privilizing Kubernetes, but it doesn't do anything really around machine learning or auto scaling
is in particular to GPUs and things like that. Cube flow does a lot in that way, but the best
part of cube flow, um, at least what we've seen work really well are, uh, is, is it's,
is the fact that it's based on top of Argo as the, um, kind of pipeline tool. So Argo CD.
So we also use Argo. Uh, the difference is, you know, cube flow, uh, compiles down to Argo YAML,
ultimately. Um, we have our own syntax that also compiles down. And so the newest iteration of
this is that they are compatible at a kind of fundamental level, but, um, you know, we buy a certain
things. Um, we also, I mean, the most important differentiator between those two projects is that
we have a, um, very sophisticated like GUI console where you can do most things through, you know,
again, because most data science, you know, data science and kind of statisticians and,
and mathematicians are not the same people that, um, can, can string up very complex, uh, you know,
Kubernetes tooling. Mm hmm. And for those that don't know, Argo is the underlying workflow
engine. How is that workflow engine used in, uh, gradient or, uh, cube flow for that matter?
Yeah. So the, I mean, the, the, the kind of underlying piece is this, uh, continuous integration,
continuous deployment, uh, engine, which basically says, you know, um, here are the steps in the
process. Like this thing happens, then this thing happens. Uh, I need to build a container, I need to
pull my data in, I need to process that data and each thing is sort of a step in that process.
Um, and then you start thinking about like what the triggers to that. So this is another area
where the, the kind of CICD paradigm doesn't map fully between machine learning and, you know,
web app development or containerized, you know, um, no JS apps, which is, uh, the,
the types of triggers can be very different. Like normally the trigger in a, in a, in a, in a,
when you make a web app or an iOS app is, you, you push new code, you have a code commit, and that
causes, you know, that's the primary trigger. Um, in the case of machine learning, there are some
interesting ones that I think still, you know, early, you know, early days, but, um, you know,
there are concepts like model drift and data drift. Um, you know, we joke around, it's like,
code is just, or a code commit is just code drift. It's just like something has changed. So let's,
let's kick off this pipeline again. And Argo is a really nice one, which gives you,
it's a Kubernetes native CI CD engine, and there are a bunch of components to it. And, and that is
the, um, a big part of, of cube flow. Um, and we also use that as, as, uh, this sort of our
underlying engine. Um, but on top of that, there are a lot of other things with just like the UI,
how you expose it, the command line interface, the, the way you expose the syntax and semantics.
And I think it's a race you talked earlier about the, um, you know, the right level of granularity.
And I think that is the big question. I think that's where, you know, any startup in this space or
any, you know, software coming to space really has to keep an eye on how it's moving because, um,
best practices change every day and, you know, uh, new frameworks come alive. But I think we've
seen some stabilization. I do think that the machine learning landscape from like a fundamental
framework and tooling perspective is, is somewhat stabilized. Whereas, you know, two years ago,
it was the pre-cambering explosion, whereas like every day something new came up. So I don't know
your thoughts on that. If that's like something you're, you also would agree with or if it's, uh,
we're still in the wild west. Now, I definitely agree with, with that statement, um,
I've written quite a bit about it. We've got an ebook, the definitive guide to machine learning
platforms that try to map out the early phases of this Cambrian explosion. Uh, and then, uh,
another one, Kubernetes for ML ops, uh, which, you know, for anyone who's still listening who,
you know, has alphabet soup in their head and wants to know what all these terms are and
how they fit together. That will be a good one to check out. But I certainly agree that we're
starting to see more, um, more kind of coalescing of the major components. There are well-defined
subcategories now within kind of this tooling and platform landscape. And it is continuing to be
interesting. Uh, I'm curious from your perspective for, you know, folks that are coming at this
from the data science machine learning side, haven't done much with, uh, ML ops or processes,
you know, do the usual, you know, I've got a notebook on a server somewhere, write some code,
you know, pull that code out of the notebook and, uh, you know, somehow get it into production. Like,
what, what's the first thing to do or, or the first three things to, to think about, to be more,
as discipline the right word, uh, to advance your, your, your process.
Yeah, I, I think that, um, I would, I would encourage the use of A tool, you know, obviously
on bias. I think we, you know, we were, uh, we are approaches to onboard people into all this,
these complicated tools that we're talking about, you know, the alphabet soup of Kubernetes and
Argo and everything else. Um, the interface for gradient is actually quite simple, you know,
the notebook is the starting point. Um, and, you know, it, it rewards closer inspections. So
the deeper you go into it, the more you'll see where it adds value. Um, but I should have said,
one of the first three steps besides use gradient. Yeah. Um, you know, I think I, I, the, the,
they're really the ones that I think probably most people with space would agree on. It's like, um,
find a, a problem that you want to solve that's relatively narrowly scoped because that,
you know, we've worked on, we internally, we have a lot of machine learning projects and the ones
that do really well are the ones that have a very clean sort of scope and we know what we're trying
to solve. Um, I think when it becomes unbounded, like find something interesting in the data,
it, it gets really hard because not only are you having tooling issues, you're trying to figure out
what the, the question is. Um, and that's it, that's very attractive given that, you know, um,
you can download TensorFlow and you kind of get superpowers like it's in, it's a, it's a,
it is a fundamentally transformative thing that can do things that you, you know, can't do otherwise.
But, um, the fundamentals are still there, which are, you know, you, you should commit to a process
of, or sorry, define the problem that you want to solve. Any organization trying to onboard
to machine learning broadly today, um, if they haven't already, would have to locate where they
want to apply it. In our case, the first thing we ever did was, um, just, uh, uh, kind of like
fraud detection and lead scoring. We get people to sign up every day, um, you know, at this point,
we've had, I think over 350,000 people sign up. This is during the Bitcoin boom as well, so we
want to make sure that, you know, who's a, who's a real actor, who's not. And we had a very narrow
sense of that. And that's, that's a production, um, thing that we use today. But when you, when
it's more unbounded, it becomes much harder. Um, I think, you know, developing the practice around
it in software development, it is the collection of the, you know, GitHub readmys and the,
whether you do standups, you know, it's like, there is a software development practice way beyond
the tooling. Um, and, you know, I think that's important. Uh, also source control management is
great. You know, uh, you don't, you don't want to be, you know, naming your files, um, you know,
final, final, final three dot ipinb, uh, because it's just, you know, you're, you're going to put
get yourself in, uh, in, uh, you know, complicated mess over time. So I think source control management
is, is key. But, um, yeah, I don't know if that was two or three, but I, you know, I think the
fundamentals are true. Regardless of, and again, this goes back to like, I think this is, uh, you
know, more well understood problem than maybe we all have, uh, realized. And so that's coming at
things from the data science perspective. What about coming at things from the engineering
perspective, you know, say you've been working on, on these kinds of problems, uh, at a relatively
modest scale, you know, what are the kinds of what are the gnarliest, you know, technical issues
you run at trying to run this at, um, you know, provider scale. Yeah, that's a really good one. Um,
open AI actually outlined a lot of them, which are when you start doing this a lot. So for example,
we run kind of a public gradient instance, which is where we run all the free notebooks. Um,
and so it's basically like one big managed instance, uh, you know, there are some issues of scale
for sure. Uh, you know, you're going to hit issues of pulling containers down, caching them
at CD, which is the tool that kind of, you know, an essential tool for communication within the
process. It does break down at a certain level. Um, you start have to, you know, look, you have to
look for things like where, you know, your bottlenecks are. And so we've spent, you know, many years now
optimizing this, um, you know, the way the pieces connect to one another because it's very likely
in your first app at it, you will have an expensive GPU and an expensive CPU and a lot of memory,
but you're actually using not of it due to something totally outside of the, uh, you know, of that
thing. Um, and figuring that out is hard. I think big teams, you know, it pays off to invest in that.
I think Uber, you know, does great in that they have an expertise here. And this is a, you know,
an internal effort, but I think a lot of companies would benefit from this tooling as well, um,
without having to go down that path of building custom, you know, or entirely purpose-built tooling.
There will always be something that gets built internally. We have all sorts of, you know,
admin tools that we build, but, um, fundamentally, we committed to a containerized process. We've
adopted newer practices. Um, uh, and I think that pays off and, you know, it allows us to ship,
ship a new version of the software every two weeks, you know, or whatever the release cadence is.
Um, and that's, that, that's the holy grail. It's like you want to be able, you know,
machine intelligence broadly is like, can you build out a model that is predictive and powerful?
And also, um, is in a system that can then be fine-tuned or, or modified as it reacts to the
environment. So it's like we built these predictive models that are incredible. Um, how do you,
you know, now work that into, to everything else? And I think that's the open question around,
uh, how software development, you know, and maybe there's something to learn there,
and, and some of it has to be redefined. You know, like we didn't talk a lot about hardware,
but that's, that's a, you know, I think we might have had a conversation about that in the past,
but, um, accelerator, you know, GPUs were, were video game cards, and now they're, uh, you know,
ML processing cards. And so there's a whole host of new things coming out. And I think that's
going to lead to a lot of interesting developments in the space that we're in, which is largely
involved with like the interface between hardware and physical infrastructure and, you know,
uh, sending, uh, software into production, and then the actual media, the problem of like,
I want to build a model and do something amazing. Anything you're particularly excited about,
and can talk about on that front? Um, yeah, I, I think broadly actually gets back to one of the
things I kind of earlier in the conversation. I, I am particularly interested in, uh,
uh, where the visualization pipeline and the compute pipeline collapse. Uh, we actually,
I was at, uh, SIGGRAPH, the big visual effects conference, um, in LA, maybe three, three,
three years ago or so. Uh, and it was clear that AI was sort of making its way in there,
but everything from, you know, better rendering to bots in games, um, and then on the other end,
you have, you know, uh, driverless cars, simulators that can simulate environments. And I think the
newest iteration of that is this, um, you know, a lot of talk around synthetic data, you know,
if one of the limitations of building powerful machine learning models is generating data,
how can that be done more quickly? Um, you know, can you use Unity? And I think the,
the blend of those gets really interesting and I don't know exactly where it goes, but it is an
area that I think is fascinating right now, um, which is now you're roping in the, you know,
the content creators as well into this, this tool and it speaks to the power of it, you know,
it speaks to you of this, this tool, you know, machine learning and its current, you know, iteration,
it's so powerful, it can do things that are, you know, mind-boggling and complex and, you know,
we need to regulate in probably lots of ways, but, um, uh, you know, it's making its way into, uh,
you know, the universe way beyond sort of traditional ML stats and, you know, data science. And so,
there will be a lot of dimensions that emerge. Software development is the one I'm thinking a lot
about, but I think they're, you know, yeah, it's an interesting time for sure. Yeah, over the years,
Jensen's visualization demos at NVIDIA's GPU conference have gotten extremely impressive,
like all the ray tracing stuff and some of the simulation stuff that they're doing. And again,
to your points, the same hardware. Yeah, I, yeah, to me, that's where it gets really, really,
really interesting. It looked like those were two totally diversion paths, and now it looks like,
you know, we had, we had one point, it might still be live. We had a guy who was, um,
training a model to, to drive his, uh, grand theft auto car on Twitch, and people could pay into his
account, and it would, you know, pay for his paper space credit so we could run the whole thing.
And this, this car was like, you know, it was not, I wouldn't, I wouldn't get in the car, but it was
a funny, you know, experience for us. Like, we're going to train this thing using, you know,
grand theft auto. And it's like, those are interesting to me. Like, it's just like,
a totally emerging use case and, and, you know, something as possible that was never possible before.
That's awesome. That's awesome. Uh, so one last question for you. I'm curious if, you know, given that you,
uh, you, you've got all these users doing machine learning stuff, uh, on, on gradient, any unique insights
into what's coming based on what folks are doing. Do you get that kind of visibility?
Yeah. I mean, I think, candidly, I think a lot of people are still at the beginning of understanding
this technology. And, and I don't mean the, the folks that, you know, we hang out with at NERIPS.
I mean, the, the, you know, the software engineers that are, you know, building, building every
other software tool we use, I think within that universe, machine learning is still exotic, new,
um, it, you know, it speaks to why, you know, Jeremy, uh, you know, can, can get so much interest in
fast AI, you know, he present, he presents fast AI is like the pragmatic way of getting started with
machine learning. Yeah. Exactly. And I think that that resonates a lot because it's still new. So,
from, you know, at this point, we've run something like 60 million hours of compute, you know,
through the platform. And most of that is probably training, you know, or, you know, a lot of it is
probably the first model that you train and trying to swap the data set out to see like what, what
you can do. Um, I think it gets more interesting as that you're like, you know, wow, this actually
worked in some way. Now how do I blend it and, uh, you know, or, or do something with it? Um,
in most cases, like, it's like, how do I take this model and turn it, you know, make it on an iPhone
or, you know, deploy it? I think those are still really hard things. Um, so where's it going? I,
I think that generally you will see more regular software engineers, whatever the, you know, whatever
that encompasses, you know, using these tools and understanding them more because it's sort of
moving past academia. Um, academia is still mind-blowing in how, you know, how fast it can move ahead.
Every conference, it's like, wow, this is something that was totally unfathomable even a,
you know, a couple of years ago. But there's still a lot of catch up to do in the kind of, um,
machine learning as a professional practice, I would say. Yeah. Awesome. It's been amazing
catching up with you. Uh, too bad we work. We, uh, it's been a while since we've seen each other
in person, uh, due to circumstances. Yep. We're controlled. But, uh, maybe one of these days,
we'll be able to grab a coffee or something at a conference.
Yeah. Cheer with. Thank you. Thank you.
