Welcome to the Twimal AI Podcast.
I'm your host Sam Charrington.
Today we close out our TwimalCon coverage with a panel on operationalizing responsible
AI, but first a quick announcement.
Have you been enjoying our TwimalCon coverage, but want more?
TwimalCon video packages are now available for advanced purchase over at twimalcon.com
slash videos.
The package features over 13 hours of content, including all the awesome keynotes and panels
you've heard on the podcast, all of the breakout sessions, including 9K study and 8 Tech
track sessions, as well as the highly regarded team tear down panels with Airbnb and serving
monkey.
Again, visit twimalcon.com slash videos for more info and now on to the show.
I am really excited to welcome up a panel that's going to talk through a really important
topic, operationalizing how we do machine learning and AI ethically and to lead us through
that.
I'm excited to have our moderator for the panel, Kari Johnson from Benchabee, and he will
introduce the rest of the panel.
Kari and panel, welcome to TwimalCon.
My name is Kari Johnson and I'm senior AI staff writer at Benchabee, and I'm really excited
to be here with our panel.
I could introduce everyone, but I think everyone will do a better job if they do it themselves.
So we'll just quickly go down the row, but we can begin with a quick icebreaker, I think,
that's all right.
I was thinking icebreakers are, they can be corny, that's possible, but sometimes they also
let you know a little bit about the other people too.
When you were a kid, what did you want to be when you grew up?
I'll go first.
I wanted to be a firefighter, but I also asked my mom to do an op-ed on local television
about why kids don't need to take showers, so it could be that I'm doing what I always
wanted to be.
I wanted to be a politician and ultimately a president.
Yeah.
That's nice.
I wanted to be a writer, I actually wanted to write novels, and I write Scalicote, so
yeah, here we are, yeah.
At various points, I wanted to be a nurse, marine biologist, archaeologist, and writer.
These are all good professions, I think we're doing good stuff too.
But again, down the line, if we could start with Perry, and can you please correct me on
the correct pronunciation of your name and tell us a little about the current work that
you do?
Sure.
I'm Perry Nas.
I'm coming from Georgian part.
I work for Georgian partners right now.
For those of you who are not familiar with Georgian partners, we are a venture capital
based in Toronto, biggest private funding in Canada, investing in grossest-stage companies
as software services, and our business plan and model is a little bit different.
We have impact in where I'm coming from in addition to the investment team.
We are like a applied research lab for our portfolio companies, to risking research and
innovation for them, and accelerating the adoption of cutting edge disruptive technologies such
as machine learning and AI, so we are a team of practitioners working on R&D projects
with our companies.
Awesome.
I'm Guillaume, which is French for William.
If my first name is too hard to pronounce, if you're coming to William, or even Bill, if
you must, essentially, I lead computational science at LinkedIn, which is a team that's
heavily invested in fairness.
I assume I don't need to introduce LinkedIn, but if you don't know what it is, you should
check it out.
It's a pretty cool network.
Essentially, what I've primarily worked on lately is specifically about the experimentation
side of fairness and responsible design.
I'm Rachel Thomas, and I'm Director of the Center for Applied Data Ethics at the University
of San Francisco, which we just launched a few months ago, so it's still very new.
I'm focused on harms that are happening now, so that includes unjust bias, surveillance
and the erosion of privacy and disinformation, and at the center, we'll be working on a
research, a mix of research, education and policy.
Co-founder of FastAI, where we're interested in getting people from more diverse backgrounds
into AI.
Just to get us started, how do you get started with operationalizing AI ethics?
What do you feel like are the first steps that an organization should be taking to set
the table?
That's a very good question.
I believe you have to start with the vision, your vision of the company, and that's what
we always value a lot when we are doing the due diligence.
Then, of course, the culture is also very important.
Teams, do you have the right team in place?
You have to understand that, of course, you are building softwares, but most of these issues
started from not having diverse teams working on these software products, so it's really
important to start by building a diverse team, not only in terms of the gender or ethnicity,
but maybe we need to have new roles in our machine learning or data science teams.
Maybe we need sociologists, maybe we need somebody with legal and compliance background, so
it's not only about, okay, let's have equal number of women and men, or from different
ethnicity, but it's also about maybe thinking about diversity in the background of the team,
and at the end, there is the question of what kind of processes do we need in place to
be able and enable the team to build responsible, ethical AI products?
Yeah, I think that makes a lot of sense.
I think values and vision are very important.
We found that we can't really have just one central team that writes one piece of code
and fixes fairness, for example, for the whole company, and so it's really everyone's
problem.
Everyone is involved in this, and so what that means is that you really have to have
the right alignment in terms of values and culture.
So when you join LinkedIn, first thing they tell you before you even sit down is members
first, right?
So if you have a doubt that you always put the member first, and also act like an owner.
And I think once everyone kind of feels like, okay, being fair, treating the members fairly
and the way I would want to be treated myself, when everyone is kind of behind that, it
makes it a lot easier.
And of course, to get started practically, you also want to have the right tools and the
ways to measure and the right processes, but really without culture and without the
values, it's actually, I think, very hard to do anything, given the fact that it's really
kind of a broad effort that everyone should take ownership of.
Yeah.
And for concrete resources, one resource I highly recommend is the Marcoola Center has
a toolkit for tech ethics, and they recommend kind of a number of processes you can implement,
but a key one is ethical risk sweeps.
And so this is kind of periodically scheduling times to really go through kind of what could
go wrong, and like what are the ethical risks, because I think a big, big part of ethics
is also kind of, you know, thinking through what can go wrong before it does and having processes
in place around what happens when there are mistakes or errors.
Diving a deeper into that, what does the panel think about in terms of favorite frameworks
or approaches or different tools that are available and out there now that teams can use?
So let's start with what are under the umbrella of responsible AI, what are main concerns,
privacy is one of them, and not only about the tools, but also about the technology, I
really believe in the financial privacy and federated learning to build privacy preserving
machine learning products and systems.
In terms of explainability and communication with the end users or actually deboking, enabling
developers to debok the systems, I really like approaches like Lime or Alime or Anchor
Lime.
In terms of fairness and bias, I believe it's really important to root out bias.
So I personally use the tool developed by EPF all university and I guess Columbia University,
its name is FairTest, discovering un-valented associations in data-driven applications
and Google Wattif tool, they are also my favorites, and I guess that's it.
And we recently also open sourced our TensorFlow tool for building privacy preserving machine
learning systems.
Yeah, so we have similar kind of areas that we care about.
I would say there's a lot of internal tools that we're also building.
Some of it is that there are some great tools that don't always scale, and we have a lot
of data that we have to process.
And we also have some tools that we're building completely our own.
So for example, we have a lot of fairness built into experimentation.
The idea is it's not just about the algorithm, it's not just about are you fair from an
algorithmic perspective, once your output is put in front of humans, what kind of outcomes
are we seeing between treatment and control, things like this.
So that's also the kinds of, we also care a lot about that, which is try to think about
it when you train your data, when you first put together your training data, when you train
your model, and then even in an ongoing basis, how does it interact with people, how does
it interact with users?
And so we try to cover all of that, at least the whole kind of machine learning and AI
life cycle.
Yeah, going along with that, I think that some of the issues that arise from not thinking
about the whole system, and so kind of how the different parts interact, I think a lot
of tech kind of encourages us to kind of hyper specialize, and it's really important
for people across different groups to be talking.
One great idea I've heard from Alex Fierst is having, like having trust and safety embedded
with engineering, product and design, because they have, you know, trust and safety is kind
of seeing what can go wrong and what happens when it does, and that engineering, product
and design tend to live in a bit more optimistic world, but I really think having kind of those
communication channels open between groups, and also having kind of all the necessary
stakeholders, like everyone's that's going to be impacted downstream involved.
What do you feel like are some of your biggest concerns as it relates to responsible AI
today, or to put it another way, if you could change one thing about the AI ethics debate
today, what would it be?
I think I might actually say, like, you know, focusing a bit more on harm and less on bias.
I think sometimes we get focused on the role algorithms, and you're like, oh, there's
bias, and I think there's a sense in which like, you know, every algorithm is going to
be biased somewhat, but you might see very different types of harm.
But so Edlington, we're about people's careers, and so we actually, very mindful of like,
are we helping everyone, you know, kind of like get ahead in their careers?
And so in terms of kind of shifting the debate, I think there's a sense in which it's sometimes
really too focused on kind of the algorithm itself, and kind of the bias of the algorithm,
versus kind of like taking a little step back and kind of talking, thinking about harm
more globally.
Yeah, I completely agree with that.
Like, I think bias is an important issue, but it's just one of many, and kind of looking
at harms, and particularly aren't harms that are already happening when there are mistakes,
you know, we've seen mistakes of an algorithm cutting off health care incorrectly, for Medicaid
benefits, of teachers wrongly being fired, that kind of, there are things already happening
and understanding those.
And I think one of the dynamics that comes into play is that algorithmic systems can make
it harder for anybody to feel responsible, and you know, bureaucracy does this as well,
but often algorithms are kind of extending bureaucracy, and this is an idea Dana Boyd has
talked about, but that's something that alarms me and we need to talk more about of kind
of how we build systems where we can feel responsible for the outcomes, because that's important.
Yeah, I believe also I do have a few of you, but I also believe that we need to have
machine learning specific quality assurance processes, and also we have to have like a
guard-lay, guard-rails fault tolerance, and we have to have a plan for when things can
go wrong, and we have to reduce the impact of models errors, especially for more critical
decision-making or applications.
One of the problems I had so far is like some people believe maybe that these systems
are perfect or they can be perfect down the road, but I personally believe because they
are probabilistic systems that we are using them for deterministic decision-making, we
can be might never reach to 100 percent performance.
So it's really important to have guard-rails.
I totally agree with that.
No doubt, one other point also is to ask about just what are the things we shouldn't be
doing at all?
I think sometimes people skip to this, like, oh, how do we de-biase the data?
You know, and we're seeing this a lot with facial recognition of like, oh, you know,
we need to de-biase it and have more faces of people of color, but we also need to ask,
like, you know, should we be using facial recognition this much at all, and in these use cases,
and really kind of pausing at the start, too?
It comes to that idea of like public perception, you know, you were talking about people believing
that it can be a perfect system.
It seems like maybe part of the process, as well as teaching or at least having public
education stuff out there so that people understand that these are probabilistic systems and they're
making predictions based on data.
Exactly.
Education is a key for both not only for the end users, but also for the developers, product
leads, or the executive teams in general.
Yeah.
Yeah.
I agree with that.
I would also add on that I think in some cases tech companies bear some responsibility
for overhyping what they're selling, and then are kind of, you know, praying on that
often people purchasing these products don't have the understanding of probability that
they need, or have these misconceptions that AI is 100% accurate, but I think we also have
a real responsibility to not over promise or over sell the capabilities of what we're
doing.
Yeah.
There's definitely a lot of marketing injected into the AI space.
Guillaume, I was curious if you could talk a little bit about sort of the practical challenges
of scaling a responsible AI within a large organization, and as an aside to that, if you
think of sort of those rules as different for the organization-wide approach as opposed
to like specific teams within the company.
Yeah.
So as I briefly mentioned, it really starts with the values kind of in the culture.
And then you know, it starts with essentially we also found that it's hard to just have
one role that applies to everybody.
And so what we try to do is kind of give ourselves the tools, the measurement tool, the
analysis tool, the experimentation tools that we can then kind of like deliver to specific
teams, and we work with them to figure out like what makes sense for their specific vertical.
And importantly, we also care about kind of the final outpost once there's an experiment
that people are interacting with it.
And so you know, we really try to have like a 360 degree view of the whole process and
make sure that kind of every part of the process is covered.
So say you train an algorithm, for example, you know, we'd like you to be a wire like
whether it's let's say a general representative or something like that.
And so you know, we try to make sure that people have the information in front of their
eyes when it's available.
And we also have, you know, we just kind of have a lot of meetings where we just kind
of discuss whether we this is the right thing or not the right thing.
You know, it my sense that it has to be decentralized to an extent.
And so this is also why, you know, when we so we analyze all the experiments and we look
at what's happening with the fairness metrics and sometimes people have like really fairness
and enhancing things, but they didn't know they did.
It's just it's just a really happy accident.
And we also want to be able to like see that and reward that and kind of learn from what
they did that worked.
Right.
And so really we're trying to have this kind of like decentralized approach where everyone's
an owner.
What are some of the sort of major AI systems that are used at LinkedIn?
So essentially it's, so we have like, we have the feed, we have the recommendations.
Yeah.
So essentially, you know, these are built into potentially they can be used by everything
basically.
So to give you a couple of examples, one that uses this heavily is the when we rank candidates.
So job candidates.
You like until LinkedIn, you see potential candidates, right.
And so those we make sure that they're representative.
And so there, you know, we actually make sure that when you see, let's say your first
list of candidates, it's actually representative of the, you know, population distribution
of the underlying talent pool and this, you know, and the bias there is like we're being
very, very careful that you're seeing something as representative.
So that's that's one example.
Yeah.
Yeah.
And for the panel, just curious what you think about in this age of AI, how this ethos
of move fast and break things with startups has evolved or changed, if at all?
Yeah.
I believe generally it's much harder for startups to proactively prioritize responsible
and ethical use of AI.
It's been kind of overwhelming for it and endeavor for them.
And that's why we gave them a framework in our team.
We provided the principles and framework for them and we, we tried to kind of educate
them that trust and responsible AI is a, you can think of it in a two in the, in terms
of two dimensional space where one where to, when one dimension is the value, you deliver
to your customers and the other dimension is the level of comfort.
So of course, the value is like the actual product you are building.
And then level of comfort can be a, but it's mainly about data ownership and do customers
have any opinion in your product roadmap or not privacy and security.
One of them most important ones and explainability and transparency.
Do your customers know how their data is used or end users and for what kind of purposes
you are using that data and do they have based on this new compliance, for example, GDPR
right to be, to delete their data from your existing databases or even fingerprints from
your Michelle and X systems.
Is this something you ran across at the Center for Applied Data?
I think Senator, you guys are just getting started, but yeah.
Oh, yeah.
That's good to say.
I think that kind of looking at like a, on a long time horizon, I think being ethical
is a kind of like a profitable decision and I think there is this real conflict of interest
though with, there's so many short term pressures and pressures to focus on the short term.
And that's something that is tough, tough to crack.
And I think that it's great to hear about like a venture firm working on that because
I think often the incentives around venture capital and kind of just our current corporate
ecosystem push people on short term and that it does take a longer time horizon to think
about ethical behavior and the financial benefits of ethical behavior.
I think that there are deeper benefits to ethical behavior in the short term as well.
Yeah.
And I think also like if there's also the risk that like if you're very biased, you might
only kind of cater to one population, right?
And so eventually that limits the growth of a user base.
So even from a business perspective, you actually want to have everyone come on board, right?
And so it's actually, I think it's actually, I agree with you, I think it's actually
good business in the long run to actually be responsible and worry about this.
I would say though, I don't think LinkedIn ever really was kind of move fast and break
things as far as I can tell.
It's, you know, I think it's, if you're an ML engineer, sometimes you can kind of get
carried away and maybe you forget why you're working on, but at least when you're working
on something that's about jobs, it kind of puts this, you know, you kind of realize,
okay, like this is important and how would I want people to treat my resume and my own
job?
And so my sense is like, and of course I wasn't around when the company was created,
but it seems like it seems like for a very long time, at least it's been about responsibility
and kind of like putting members first.
So I've got two more questions and then I'm going to open it up and see if there's any
questions from the audience.
One of them was, you know, Rachel and I were talking about this before we got started
today and it's like, so the Pope was talking about AI ethics effectively at a tech conference
at Envatican City a couple days ago, which is something I want to see pictures of or something
that must be interesting.
But I mean, and then, you know, they're essentially talking about a need for protection of the
common, of the common good, from AI perspective as well as tech.
And you know, other ones, examples of, yeah, ethics coming up in the context of power
in AI, it feels like a word that's been missing from a lot of conversations around artificial
intelligence, Alexandria, Casio, Cortez, and someone else, Joey Blumwinny having a long
exchange about the systems working best on white men and not as well on a women of color,
for example, it seems present in a lot of these.
And this is a very long-witted way of asking how the panel, if there are any specific ways
that you see this power dynamic playing out in the AI ethics conversation today.
I think, you know, this is also one place where like diversity is like super important,
right?
Because essentially the people are in the room making the decision about like what should
be fair will kind of refer to their own experience.
And so that's why, you know, it's very important to like make sure that both in the company
and also kind of like inside, you know, your development teams, like there's actually as
much diversity as possible, so that the decision kind of is made in a way that's kind of like
kind of conscious of everyone's kind of, you know, approaches.
I think, you know, it's, and really diversity is probably one of the best way to get, at
least get started on that.
Yeah, I think it's a really important point you bring up, I would say one area, I think
we see it a lot in is surveillance technology, which tends to kind of disproportionately
be applied against, against poor people.
We've also seen instances of, you know, Baltimore police, I, using facial recognition to identify
protesters protesting the death of Freddie Gray, and those things definitely have this
kind of clear power differential and, you know, even looking at history kind of how surveillance
technology has been used to suppress dissent and moves for social, positive social change
powers a very important dynamic.
And then one last thing before asking the audience if you have any questions to NIA positive
note, do you have any favorite examples of AI ethics success stories where an organization
or team was able to resolve some, some, some challenges or confront them?
I was working with one of our companies, turn it in, they are also based on Valley, and
we were building a plagiarism detection tool mainly for investigators and teachers to mostly
for universities and K-12 institutes, and so it's basically using NLP, all your kind
of authorship style, not using NEPIO, not using any student, performance, grades, anything.
But at the end of like development process, we get together and because I'm not a native
speaker, I ask a question, like, is it possible that our model has, for example, more false
positive rate against non-native speakers?
And because, you know, like, we, historically, there might be more instances of like a
plagiarism for non-native speakers and the model in humano-mission learning is all about
correlation, not necessarily causation, and the model might have picked up those kinds
of, for example, grammatical errors as the signs for, for example, plagiarism.
And we started testing our system and we used the first test and we realized, yes, there
is a potential problem and we actually could mitigate and address such bias in the system.
Oh, that's, yeah, so I have two, two small examples and the, so one that I briefly mentioned
is the representative ranker, so when, you know, when you look for candidates, you essentially
see something else representative, the second one is something that we actually discovered
when we analyzed the experiments, so it was kind of a surprise, but it turns out, so
we have this feature that sends like job notifications instantly, so, you know, when
there's a job that's good for you, you instantly get notified, and we looked at the metrics
and we're like, wow, this is like really equalizing.
So, you know, it seems like people who were not too engaged or like didn't have like
that great networks, you know, were kind of categories that, you know, we thought were
a bit disengaged.
These people just, we see the metrics go way up and we just, we wonder why, and it turns
out, you know, like people tend to self-sensor when they're playing to jobs, they're like,
oh, maybe I'm not qualified enough, maybe I've done good enough, or I can see there's
already people applied, I don't want to do it.
And so it turns out that like when an algorithm tells you, hey, we think you should apply
to this, you'd be good for this, that can also be empowering.
So that's a success story there, which is, you know, and, you know, we just found out
that like some of these algorithms actually help people and actually reduce the gap between
the categories that we might care about.
And so, yeah, these are two, two stories that I was, I was pretty happy about.
Yeah.
Yeah, a story I like is from a meetup, Evan Estola, the lead machine learning engineer
shared that when they were building their recommendation system to recommend meetups
to people, they realized there was the potential for a feedback loop of if you're a woman
or interested in technical meetups, so then the algorithm might recommend even fewer
meetups to women, tech meetups, so you would have fewer women attending and kind of create
this feedback loop pretty quickly.
And so they decided to short circuit that from the start.
And I really like that story because it's also an instance of kind of not just unthinkingly
trying to optimize a metric, but thinking about, yeah, thinking about the outcome and taking
the responsibility around feedback loop seriously.
Nice.
And any questions from the audience?
There we go.
I use this.
Speaking to Lola Pal.
Alrighty.
So with the rise of deep learning, a lot of more principled approaches have fallen to
the wayside in favor of empiricism.
It's going to be a long question.
It's not very long.
I see that a lot in natural language processing.
But in the case of ethics and AI, how do you think principled approaches fit in?
Can you give an example of what you mean by principled approaches?
So I work on machine translation and we've seen of shift towards bite pair encodings using
sub words that don't necessarily have any meaning by themselves, as opposed to using
a large vocabulary of whole words.
So we can't inspect why a model attributes the decision to the sub words themselves as
intelligently as before.
It's kind of in conflict with deep learning's empirical approach to AI.
But I'm wondering, do you think it can come back?
So kind of.
What do you do when your techniques abstract away from your ability to be fair?
In a sense?
A little bit.
A little bit.
Yeah, so I would say I think the Y is generally always a hard question.
So we try to focus kind of at first as like we're seeing some metrics about is a performance
the same for groups that we care about, how people interact with it.
And even with this, we even when the techniques are simple, the Y is always kind of a difficult
question.
Like, why is it that we're seeing this?
What is it about the algorithm?
And sometimes it's not the arguments about the people.
And so when it's about people like figuring the Y is very, very complicated.
So I think what a first step is to try and at least to an extent try to have some idea
about the metrics that we care about or the principles that we care about and maybe
comfortable kind of either delaying something or thinking more deeply about something until
we've understood the Y.
And I agree that as things get more complex or as increasingly one algorithm, the output
of one is the input of the other one.
The Y gets really, really hard and that's no exception there, if that makes sense.
But that's been my experience at least.
I also believe that it's so hard, like the root cause analysis is always hard as we use
more complex machine learning models, it's even harder, but I'm also very encouraged
to see if we can use the same optimization techniques because it's like like deep learning
any of those machine learning, they are like optimization tools for us.
Can we use the same optimization tools to reach fairness?
And there are some papers out there in the recent conferences.
They talk about, for example, rather than optimizing over the global population.
For example, optimizing for micro segments of the populations and getting to, for example
demographic parity, but of course for products like machine translation is much more complex
because the word embeddings, they can't even be racist and we know about all these problems.
And it's really harder to kind of modify the objective functions to reach to the fairness
in the context of, for example, machine translation, but I'm pretty optimistic that we can use
the same technology to get there.
And all I would add is I think it can be useful to just even simple techniques of altering
your input to see how that impacts the output, can give you a lot of insight into what
might be happening, as well as to remember that with humans we're bad at knowing why
we make the decisions we do.
We do a lot of post-talk justification of our decisions and so the causality is often
not well understood there either.
Thank you to our great panel.
Not on pause.
All right, that's our show for today.
To learn more about today's show or any of our panelists visit twomolai.com slash shows.
Make sure you visit twomolcon.com slash videos to secure your access to twomolcon video content
now.
Peace.
