1
00:00:00,000 --> 00:00:29,000
All right, everyone. I am, of course, Sam Charrington, host of the Twomo AI podcast. And today we're joined by friend of the show, Kamyar Azizadina Shelley, assistant professor at Purdue University for AI Rewind 2021 deep reinforcement learning, of course, our in our AI Rewind series.

2
00:00:29,000 --> 00:00:39,000
We talk through the trends and advancements in each of the fields we cover, as well as what's in store for 2022 and beyond.

3
00:00:39,000 --> 00:00:50,000
Kamyar last joined us for the reinforcement learning office hour session. We held in conjunction with Twomo Fest in 2020, which was, by the way, great session.

4
00:00:50,000 --> 00:01:00,000
We'll link to that in the show notes and I'm super excited to have him join us again today. Kamyar, welcome back to the Twomo AI podcast.

5
00:01:00,000 --> 00:01:05,000
Sam, thank you. Thank you so much for the introduction. Thanks for having me today.

6
00:01:05,000 --> 00:01:12,000
I'm really looking forward to diving into our chat. You pulled together a lot of great material for us to talk through.

7
00:01:12,000 --> 00:01:20,000
The bulk of our time will be spent talking through the four key themes that you identified as being important for RL in 2021.

8
00:01:20,000 --> 00:01:31,000
But before we do that, I'd love to have you kind of catch us up. The last time we spoke, you had just graduated from Caltech and transitioned over to Purdue. How's that been going?

9
00:01:31,000 --> 00:01:49,000
Well, everything is going well. I was actually visiting research at Caltech at the time. It's been a, it's been an interesting year. So the pandemic part, I would not talk about it was like a hard part, but excluding the pandemic part.

10
00:01:49,000 --> 00:02:03,000
Everything was fantastic and the transition to Purdue was awesome. Building collaborations with many people across the country or across the globe. All of them were like fantastic.

11
00:02:03,000 --> 00:02:15,000
Great colleagues here, great friends here really enjoyed and my students are amazing here. So I'm working with them and we are doing really, really nice stuff these days.

12
00:02:15,000 --> 00:02:29,000
That's great. That's great. Well, to ease us into the topic, maybe you can kind of generally characterize the past year in RL. Was it a big year, slow year, broad brush strokes.

13
00:02:29,000 --> 00:02:32,000
What was your kind of takeaway about the field?

14
00:02:32,000 --> 00:02:56,000
So we had amazing years in like in 2015, 16, 17, and 18. And in the last two years, we had another like two or more amazing years, but it was interesting in the sense that we have made many, many theoretical development in the last two years.

15
00:02:56,000 --> 00:03:05,000
From practical standpoint, we have done so many advancements and so many technologies emerged.

16
00:03:05,000 --> 00:03:16,000
But if you want me to compare again with like stuff happening in NLP, probably we were a little bit less in the media, I would say.

17
00:03:16,000 --> 00:03:25,000
There were so many great progress in in a field of RL, but not as much coverage at NLP God in the last two years.

18
00:03:25,000 --> 00:03:41,000
And do you think that was just because of the transformative nature of what was happening in NLP or because the advancements in RL were kind of more academic, less easily applicable?

19
00:03:41,000 --> 00:03:55,000
I think it was mainly so that we have made many great advancements in the last few years, but these these two years mostly we've been focusing to harvest them.

20
00:03:55,000 --> 00:04:17,000
So there were many, many, well, they have been many, many technological advancement. Now we are basically taking them to practice. And there were so many great theoretical advancements, but these days we were actually harvesting them. Basically, we are using many of those to make many, many advancements.

21
00:04:17,000 --> 00:04:30,000
And which I don't know, it's like my worth being like flashy everywhere, but for some reason they did not get to be so flashy.

22
00:04:30,000 --> 00:04:51,000
And also in the last two years, we have been not going to conferences, so we did not get to share with each other what happened in the last two years. On this something is so flashy, they make it to the public easily, but other stuff that we do.

23
00:04:51,000 --> 00:05:10,000
They might need it to be kind of communicated among people and get boosted up, but since we were not going around and talking with each other's and meeting and hanging out and that might also be a reason for things to live it, not be that on the media that much.

24
00:05:10,000 --> 00:05:25,000
We had to pick out one or two of the kind of most flashy developments in the field, what would those be. So one of them is this recent adoption of reinforcement learning methods in robotics.

25
00:05:25,000 --> 00:05:39,000
In robotics in last two years, I've been observing, okay, despite the fact that many, many researchers were not able to go to their labs to run experiments for robotics.

26
00:05:39,000 --> 00:05:55,000
There have been great progress in the field of robotics, mainly due to the deployment of oral methods, or I would not say deployment.

27
00:05:55,000 --> 00:06:10,000
The people in field of robotics are so great in knowing topics in RL that they develop oral algorithms for their problems, and these developments actually made it possible to make many, many fundamental advancement in robotics.

28
00:06:10,000 --> 00:06:26,000
For example, now you can have drones flying virtually with all everything guaranteed and like not just plug and play everything is guaranteed. You can have many robots actually walking in different trains.

29
00:06:26,000 --> 00:06:29,000
And mainly all of these things are.

30
00:06:29,000 --> 00:06:43,000
They've been made possible using learning methods. So these are, this is one one flashy thing happened last year. And actually, let me pause you because that's getting into one of the areas that you identified as a theme.

31
00:06:43,000 --> 00:07:02,000
Let's just stick, stick with that for a second and dig a little bit deeper will include in the show notes links to a bunch of the papers that we'll be talking about, or as close to all of the papers that we're talking about and demos as we can.

32
00:07:02,000 --> 00:07:21,000
Be sure to check out the show notes, but you mentioned with regard to the drones that, well, there's one paper in video that will share that's kind of high high speed, oral control flight through like a dense forest that was really interesting.

33
00:07:21,000 --> 00:07:40,000
And you just mentioned guarantees talk a little bit about the guarantees aspect of that. What, what, what is that offering? So that's a very good question. It has two folds to it. Many people in robotics, when they want to deploy an algorithm, they want to make sure that algorithm works guaranteed.

34
00:07:40,000 --> 00:07:51,000
Not just by chance, they want the algorithm to be there and work for sure. Okay, so this is one of the things that they actually need for many, many applications.

35
00:07:51,000 --> 00:08:11,000
You might call it cultural thing. It has been there for like many decades. And if you want to propose a robotics algorithm that is acceptable to robotics, robotics is basically the most of them they require you to provide a guaranteed algorithm or basically guaranteed your algorithm works.

36
00:08:11,000 --> 00:08:28,000
And so they're the good thing is one of the thing that prevented roboticists to adopt methods we develop in machine learning was the fact that we're not seeing a way to get guaranteed algorithms out of them.

37
00:08:28,000 --> 00:08:46,000
But in last few years, there have been many, many amazing researchers in robotics with robotics and control background, who are now I would say expert in machine learning, they can develop algorithms in some topics way better than I would do.

38
00:08:46,000 --> 00:09:05,000
And they are actually behind all this improvement and advancements and they are able to deploy and develop reinforcement learning or general machine learning algorithms that can be used in robotics, robotic control that is actually guaranteed to work.

39
00:09:05,000 --> 00:09:25,000
So what happens when things are guaranteed to work is, for example, you can, if you have an algorithm which is guaranteed to work, you can deployed in a very, very extreme scenario, for example, extreme unknown wind, you're having a drone, we're going to stop supposed to fly in a weird situation.

40
00:09:25,000 --> 00:09:38,000
If you have an algorithm which is guaranteed, I'm not just saying like some guarantee to actually guaranteed to work right away, you just plug and play, you just deploy the algorithm in the wild, it works.

41
00:09:38,000 --> 00:09:44,000
And you don't need to crash your drone and might cost you millions of not millions of like thousands of dollars.

42
00:09:44,000 --> 00:09:54,000
And you don't need to spend so much time tuning everything will work right away. So this is one thing that robotics is they would like to have as for their algorithms.

43
00:09:54,000 --> 00:10:06,000
So what's the next level of detail or kind of technical detail around a guaranteed like it strikes me that that's fairly ambiguous.

44
00:10:06,000 --> 00:10:21,000
Are we talking about guaranteed convergence guaranteed guaranteed predictions around control within a certain range or something else.

45
00:10:21,000 --> 00:10:25,000
So the guarantees are mainly about a stability and the performance.

46
00:10:25,000 --> 00:10:49,000
Okay, so when you have a when you come over the policy to control a drone, one thing you really want one thing that practitioners would ask you immediately is, would you guarantee that this controller is a stable means like it's not going to or we have a walking robot, you can guarantee that this controller would not crash or what would not fall would not hit or the action that

47
00:10:49,000 --> 00:11:03,000
Okay, in the control system, the actions can go to infinity mathematically. So do you have a guarantee that your machine is not going to overshoot to some unknown region.

48
00:11:03,000 --> 00:11:14,000
Okay, for example, if you have a drone, you're not going to have the power of 10,000 times what is allowed and to the drone and throw might crash.

49
00:11:14,000 --> 00:11:28,000
Or the engine might just burn down. So these are the types of guarantees for the stability and the performance guarantees like how far you're from what you actually your desire to be.

50
00:11:28,000 --> 00:11:44,000
So this is, for example, if I have a drone is going to fly like to do weird maneuver in extreme wind, when follow a specific like pattern like pathway, can I guarantee that this mod is drone would do that.

51
00:11:44,000 --> 00:11:50,000
If I use a machinery model inside the closed loop control.

52
00:11:50,000 --> 00:12:02,000
Okay, so these are like type of guarantees that we we are talking about. And what, what are the techniques that have been developed that allow us to now provide these kinds of guarantees.

53
00:12:02,000 --> 00:12:12,000
One of the coolest one I'm aware of is the notion of lipchitzness. So if you haven't let's say deep neural network to model.

54
00:12:12,000 --> 00:12:20,000
So let's read this way. I mean, I can talk about one of the works that I'm fully aware of. So let's say you have a drone is this drone is trying to land.

55
00:12:20,000 --> 00:12:26,000
Okay, and when this drone gets close to land or it gets close to ceiling this air circulation.

56
00:12:26,000 --> 00:12:39,000
The circulation of air through the wings of the drone imposes many many weird fluid dynamics based like pattern that's Newton's law would not easily give you.

57
00:12:39,000 --> 00:12:45,000
Newton's law that you have actually put in the motion equation for the drone. So those are not there.

58
00:12:45,000 --> 00:12:56,000
Okay, so can you use machinery models to learn this residual pattern or whatever is this thing is circulation of air results in the drone maneuver.

59
00:12:56,000 --> 00:13:00,000
And can you learn those things using deep neural networks.

60
00:13:00,000 --> 00:13:13,000
That's we know we can learn, but the thing is, can you make sure that these neural networks that you're using is lipchitz. If this neural network is lipchitz, which you can use some techniques like self normalization.

61
00:13:13,000 --> 00:13:28,000
And there are other techniques that make sure that the neural network is lipchitz. Given this, you can actually prove that whatever controller you get is going to be robust. Okay, this is one of the things you would get as a guarantee.

62
00:13:28,000 --> 00:13:38,000
Your ability to the guarantee that the neural network is lipchitz is around the way you formulate the.

63
00:13:38,000 --> 00:13:43,000
Class functions and things like that, you're from the cost function and architecture.

64
00:13:43,000 --> 00:13:53,000
Okay, so both of them together, I don't want to have soft guarantee. I want to know that if I put it in the cost function, it becomes a soft guarantee.

65
00:13:53,000 --> 00:13:59,000
I want to put it built in in the architecture that is going to be for sure a lipchitz function.

66
00:13:59,000 --> 00:14:18,000
Okay, and I want to know what is that lipchitz constant. If I know all of these things that can go and design a control. Okay, but the result of it is going to be after doing all this training, the first time you deployed a machine in the wild means that the first time you deployed a drone or a robot in the wild, it's going to work right away.

67
00:14:18,000 --> 00:14:23,000
You don't need to spend like two months or five months or two years of fine tuning.

68
00:14:23,000 --> 00:14:51,000
Now, historically, one of the things that has prevented us from, you know, taking things out of training and putting them into production and having them work right away is the whole idea of, you know, color what you will generalization domain adaptation, like the real world is different from the environments that we train on does do the, it sounds like the guarantees are strong enough that they overcome that that issue is at the case.

69
00:14:51,000 --> 00:15:02,000
It is the case and also the expertise in roboticist machine learning expertise in roboticist researcher is a second thing.

70
00:15:02,000 --> 00:15:09,000
For example, this problem that you brought up that's when you train things in the lab and when you change the situation things break down.

71
00:15:09,000 --> 00:15:21,000
The recent advancement that people in robotics robotics made is like this new era of doing some sort of online metal learning on the fly.

72
00:15:21,000 --> 00:15:33,000
So they are many algorithms they proposed recently that you actually can learn and setting that can adapt extremely fast to a new scenario.

73
00:15:33,000 --> 00:15:42,000
So that wind condition problem was talking about you are flying a drone in extreme wind scenario and the wind keep changing.

74
00:15:42,000 --> 00:15:54,000
Your training you probably had like five or 10 wind condition, but in the wild is keep changing is going to create this turbulent flow is going to be extremely hard and they're unknown as well.

75
00:15:54,000 --> 00:16:03,000
So they're certainly chosen to can you come with an algorithm can which can adapt to any wind condition on the fly.

76
00:16:03,000 --> 00:16:18,000
So this for example, one of this cool algorithms with again learning theoretical guarantees have been developed by our offices and control theorists who are extremely well and expert in machine learning.

77
00:16:18,000 --> 00:16:35,000
Basically this this okay, there's a general theme and trend trend in reinforcement learning with these days that people use the basic topics and core topics in reinforcement learning and take them all and develop problem specific algorithms.

78
00:16:35,000 --> 00:16:50,000
Okay, this is one of the general theme these days in reinforcement learning when I eight years ago when I was doing reinforcement and I started doing reinforcement learning we were like, okay, find the hardest problem ever could exist and try to have an algorithm solve that.

79
00:16:50,000 --> 00:17:09,000
Okay, this was a theme when I started working in reinforcement learning and was the main theme for many many years to come up with principle algorithms which actually works for a force case even like not even earth or universe like whatever can mathematically be hard it's going to work for that.

80
00:17:09,000 --> 00:17:22,000
But these days people are like hey let's use all those principles that we have developed in last 30 years or 40 years and take them to that design and develop problem specific algorithms.

81
00:17:22,000 --> 00:17:36,000
So this is one of them like you want to fly drone and you want to be fast fast and adaptive you want to be robust go and design your algorithm given those principle design your our algorithm to do such.

82
00:17:36,000 --> 00:17:41,000
So this is an interesting trend trend at that I really like.

83
00:17:41,000 --> 00:18:04,000
And I'll encourage folks once again to check out the learning high speed flight in the wild video it is super impressive that drone is going fast and the trajectory is a lot smoother than you know if you saw a drone you know ML control drone video a couple of years ago is like a lot of stops and starts and things like that this is pretty pretty smooth.

84
00:18:04,000 --> 00:18:20,000
Robotics topic that we were just talking about is related to another area that you identified as a key theme in RL over the past couple of years and that's advances in control.

85
00:18:20,000 --> 00:18:24,000
Can you talk a little bit about what you see happening in control.

86
00:18:24,000 --> 00:18:44,000
Well control theory arguably is one of those oldest settings or problems study that I firstly asked about how we can come up with a policy that can do something.

87
00:18:44,000 --> 00:18:56,000
So the field of cyber network used to be called was around this idea that how we can actually come up with a way to control a system.

88
00:18:56,000 --> 00:19:11,000
And this problem has been there for almost ever and you can find books that are like haven't been open for like last 60 years this is how old is this topic.

89
00:19:11,000 --> 00:19:27,000
So but the thing is this topic did this problem of control theory has been there for for long time and there have been so many many so many improvement and amazing developments in the end in the field.

90
00:19:27,000 --> 00:19:31,000
But recently a specific in last two three years.

91
00:19:31,000 --> 00:19:43,000
Many reinforcement learning folks there were and basically learning theory folks they were asking hey most of the almost all of the control theory is about the setting that you.

92
00:19:43,000 --> 00:19:50,000
You are not going to learn anything you are everything is given in advance and there's no learning is involved.

93
00:19:50,000 --> 00:20:04,000
So I give you a model or the environment the parameters everything and ask you come up with the optimal controller of course there are settings like adaptive control that you would learn but those settings were not that established yet.

94
00:20:04,000 --> 00:20:28,000
In last few years many of us started to learn many many things in control theory and try to see whether we can frame them as reinforcement learning problems okay so now in this problem that's like three years ago we were like we don't know how to solve control problems when they don't know.

95
00:20:28,000 --> 00:20:44,000
And we don't know the environment. Now we actually know a lot to today we know a lot about how to control control control system.

96
00:20:44,000 --> 00:21:03,000
We don't know the parameters of or how the environment works in advance basically with literally turn the problem to a reinforcement or problem that things are unknown in advance and you're going to interact with the system learn how the system works and given that come with the.

97
00:21:03,000 --> 00:21:27,000
Coming with the good controller which actually can stabilize the system is the system does not blow up and maintain some some guarantees that you want okay so this was one of the things that happened in last few years and it's a theme because it's it's one of those areas that machine learning got involved.

98
00:21:27,000 --> 00:21:46,000
And the control theorists they were kind of extremely welcoming and they are now there are many many works in this area of learning and control that actually derived by control theories that I'm extremely excited and we actually have a new conference called out for DC.

99
00:21:46,000 --> 00:22:00,000
So this is a new theme of research that's basically goes back and okay there are many many problems in the world there are control theory problems that nuclear power plant you want to control it you need to have a control theory problems.

100
00:22:00,000 --> 00:22:03,000
You want to send human tomorrow to to moon.

101
00:22:03,000 --> 00:22:09,000
You have a control problem there if you want to like I don't know anything or you want to control the.

102
00:22:09,000 --> 00:22:20,000
And the robots we were just talking about robots or it's a big things that I care about data centers the cooling of data center you want to control those basic all of these things are.

103
00:22:20,000 --> 00:22:36,000
Control problems but what we did the new theme is like hey great people back in the time people were going and analyzing the model and come on with the model themselves and then design the controller now what you're saying is like hey you don't need to spend five years doing that.

104
00:22:36,000 --> 00:22:42,000
You are doing that designing or understanding the model just give it to them to the RL algorithm if we do it.

105
00:22:42,000 --> 00:22:56,000
And the question is how they are all going to do it these are like that these are the things that researchers these days are working on and they have developed many many algorithms for and some of them are really surprising and some of them are changing the topic.

106
00:22:56,000 --> 00:23:08,000
Basically for example there is a topic in control theory called robust control which is it's been main thing for control theory for almost ever.

107
00:23:08,000 --> 00:23:13,000
And recent thing like last two years we actually altered the definition of it.

108
00:23:13,000 --> 00:23:17,000
We were like okay robust control fine but we don't want it.

109
00:23:17,000 --> 00:23:27,000
What did it mean before and what does it mean now so the thing it was saying before was okay you have a I give you and I give you a system to control.

110
00:23:27,000 --> 00:23:42,000
I give you let's put in the RL context I give you an RL environment to to to I give you an environment that you don't know exactly how the environment works but I give you an estimate of the environment.

111
00:23:42,000 --> 00:23:48,000
But the true environment is going to slide away from this model this environment.

112
00:23:48,000 --> 00:24:02,000
Okay so now that one question you can answer is like can you come up with a policy or controller which is going to when I applied a controller is going to work for even the worst choice of this environment.

113
00:24:02,000 --> 00:24:12,000
Okay so I have I have like I give you an estimate of the environment but the true I also tell you the true environment is like somewhere close to this true it is estimated environment.

114
00:24:12,000 --> 00:24:26,000
And when you say an estimate of the environment are you is it the state of the environment that is unknown or is it the you know whatever you're trying to optimize the score or the performance or.

115
00:24:26,000 --> 00:24:36,000
Let's just say the dynamics of the environment is not clear to you okay so if I have a couple of water in my hand if I want to lift it.

116
00:24:36,000 --> 00:24:48,000
I would expect the glass go like two centimeter up by my go one centimeter and half okay so this is uncertainty I have about the environment itself.

117
00:24:48,000 --> 00:24:59,000
Or uncertainty I have about the environment noise there's a noise in the environment I have uncertainty about that okay so these are the things that are not modeled.

118
00:24:59,000 --> 00:25:16,000
Okay basically these are unknown to me so I don't know how exactly the environment works in my mind if I lift this cup of water it should go to centimeter up but when I do when I apply my action on this environment the cup of water goes up one centimeter and half.

119
00:25:16,000 --> 00:25:25,000
Okay so this is basically there's a discrepancy between what happens in the world and what you have in mind okay so the robust control says.

120
00:25:25,000 --> 00:25:39,000
You I you have an estimate of the modern in mind the true environment is not the exact estimate you have the true environment is somewhat close to it but you don't know which like what is it.

121
00:25:39,000 --> 00:25:49,000
But what you do you make it a mean max problem you try to find the controller which works for the worst possible that can happen to you.

122
00:25:49,000 --> 00:26:03,000
Okay so when you apply when you come with a controller when you apply that controller the environment might be a good environment or might be a worse environment around the estimate you have in your mind okay so you were trying to come up with the robust way of doing it.

123
00:26:03,000 --> 00:26:16,000
You were saying okay I don't know what is the true environment but I know it's close to the estimate I have so I try to find worst environment possible close to my estimate and I try to be best for that okay.

124
00:26:16,000 --> 00:26:28,000
Okay so it's in somehow what it what it means is like whatever controller you're going to use is going to do somewhat good for any environment this set of environments that are possible.

125
00:26:28,000 --> 00:26:33,000
This was a robust control that has been like there since like 60s.

126
00:26:33,000 --> 00:26:49,000
So basically solving in max problem I want to come I don't know how the environment works but I hypothesize is going to be the worst possible that can happen to me and I want to find a controller which actually solve all the environments there.

127
00:26:49,000 --> 00:27:00,000
This one is going to be bad in sense that if you want to robustify yourself against the worst thing that can happen the control you are going to get is going to be really really really conservative.

128
00:27:00,000 --> 00:27:17,000
Okay but this was a practice for like 60 years yeah but in last two years we were saying like hey good fine you can design a controller at the beginning which is going to be robust to whatever is going to happen the future.

129
00:27:17,000 --> 00:27:46,000
But if you run your controller for 10 times steps and you realize that the environment is not that bad updated you don't need to keep running that very very conservative controller for next 55 billion years after like few times steps if you realize the environment is not that bad you don't need to robustify yourself against something that is worse you can just robustify yourself against the worst is going to happen.

130
00:27:46,000 --> 00:27:49,000
Not the worst that can that would have happened.

131
00:27:49,000 --> 00:28:06,000
Okay in robust control we are saying that I want to be robust against the worst that could have happened to me but what we have been saying recent is like hey you don't need to be that conservative you just make yourself robust against what the worst is going to happen to you.

132
00:28:06,000 --> 00:28:16,000
So it's kind of like you based on the observed data you try to identify a distribution and be robust for that as opposed to the worst possible.

133
00:28:16,000 --> 00:28:30,000
Yeah if the worst possible is there but if I interact with the environment the uncertain is not that bad my estimate is not that bad just update myself be more relaxed and this is going to work in practice very well.

134
00:28:30,000 --> 00:28:48,000
This was one of the themes that I really like to happen last year last years is there a name for this it sounds like it should be called meta robust learning or something like that it's called robust control is I think it's it's called improper learning in control.

135
00:28:48,000 --> 00:28:51,000
Is that the improper learning for non stochastic.

136
00:28:51,000 --> 00:29:12,000
A lot of the things that I saw in taking a look at that paper was they talked a little bit about this kind of arc and control from you know classical control which you.

137
00:29:12,000 --> 00:29:39,000
You describe very like you know all the parameters of the thing that you're trying to control to stochastic control which there's now some noise in the system but it's it's kind of random noise and this paper was really geared around non stochastic control which as opposed to assuming just kind of noise injected it assumes like adversarial perturbences.

138
00:29:39,000 --> 00:30:03,000
Is that is that non stochastic control formulation is that newer and can you talk a little bit about this like assumption of adversarial perturbences versus noise yeah sure definitely so this you in that paper you make this assumption that things are adversarial because you want to be robust against a voice that can happen to you.

139
00:30:03,000 --> 00:30:18,000
Assuming things are adversarial has been there in control forever okay but they are they were saying you don't need to robustify yourself against adversarial noises or adversarial changes of perturbation in the model.

140
00:30:18,000 --> 00:30:44,000
Without looking what happens to you they were saying that hey look what happens to you and you can adapt to the noise to that research noise as well so this was the one of the things but when you make things noisy by basically make this thing to be a stochastic noise you can do many many many things and making things a stochastic noise has been there forever like when we send human to moon.

141
00:30:44,000 --> 00:30:58,000
We use a model called LQG linear quadratic Gaussian yeah so this model basically solved by Coleman and others back in.

142
00:30:58,000 --> 00:31:13,000
Age of dinosaurs many many years ago and so this model this this solution sorry this setting is noises noise is a stochastic and you're trying to come with a controller for.

143
00:31:13,000 --> 00:31:30,000
The setting that the noise is a stochastic okay so the noise in the system is a stochastic is not adversarial okay and Coleman was able to give a solution to this problem so you know the environment yeah so you know the environment.

144
00:31:30,000 --> 00:31:49,000
You know the noise is Gaussian and for that you can come up with the optimal control design okay and people did it people send human to moon with this exact solution but the interesting thing is now assume that the for the same model I don't tell you.

145
00:31:49,000 --> 00:31:58,000
What is the for the parameters of the model I don't tell you what is a dynamics can you use data to learn the learn the dynamics.

146
00:31:58,000 --> 00:32:13,000
It's basic machine learning question I I dug down so many books from many years ago I talked to many many colleagues in control theory there was no solution for it if I give you.

147
00:32:13,000 --> 00:32:23,000
There's a few samples and ask you hey can you estimate the model parameters of that thing we use to send human to moon I couldn't find anything.

148
00:32:23,000 --> 00:32:49,000
Okay and this was another big innovation in machine learning and well intersection of reinforcement learning and control that happened like two years ago and also keep keep happening and last year and also these days that's people now actually proposing interesting and weird and cool algorithms to be able to learn the parameters of the dynamical systems.

149
00:32:49,000 --> 00:33:16,000
That are heavily used in practice okay so these are these are new things like now we know how to learn the model of that system that we used to send human to moon okay what does it mean it means that now if you want to deal with the new system so for that system we had like many years of experience many many hundreds of engineers they were like tweaking things to come up with the parameters.

150
00:33:16,000 --> 00:33:27,000
Now with this new machinery that we have you don't need five years of engineering and hundreds of engineers to go and tweak things to see what are the parameters of the model.

151
00:33:27,000 --> 00:33:38,000
You can directly deploy these algorithms to actually directly learn the parameters of the model so the machine itself learns everything which is like very very exciting.

152
00:33:38,000 --> 00:33:49,000
Is that is the paper that is the best exemplifies that is that algorithmic regret bound and partially observable linear dynamical systems.

153
00:33:49,000 --> 00:33:59,000
That's a paper the first proposes how to learn the dynamics of this partially observable control systems.

154
00:33:59,000 --> 00:34:14,000
You were saying there's a noise in the system you don't observe everything the state is noisy and you don't observe everything which is the same model people using sending human to moon and for that setting this paper actually shows how to learn the parameters of the model.

155
00:34:14,000 --> 00:34:33,000
The interesting thing is like after being able to learn the parameters of the model with the algorithm is proposing that paper you can actually get a controller which controls the system without knowing the system in advance such that the performance is almost identical to the performance of the optimal policy.

156
00:34:33,000 --> 00:34:44,000
So it's this algorithm learns so fast basically exponentially fast it learns everything the optimal controller exponentially fast which is kind of weird.

157
00:34:44,000 --> 00:35:02,000
And it's able to do that which is like I like it a lot and is the learning the environment and learning the controller is that happening in parallel or is that hat or like in the same loop where they serialized steps in the same loop.

158
00:35:02,000 --> 00:35:12,000
And interesting observation there is in order to come with a good controller you don't need to learn the model very well.

159
00:35:12,000 --> 00:35:20,000
If you have some idea about the model you can come with a really good controller that was another observation in that paper that.

160
00:35:20,000 --> 00:35:44,000
Can you don't you learn the model with a rate of one over number of square root of number of samples but your controller would converge to good controller exponentially fast which was another cool thing that strikes me as counter intuitive in the sense of learning a controller has been hard for a long time even when we were given the model.

161
00:35:44,000 --> 00:35:58,000
And so how is it that not knowing the model and kind of learning it but not very well gives us good performance and allows us to learn a good performing controller.

162
00:35:58,000 --> 00:36:22,000
That's a very very good question so it's like you don't know exactly what is a model but the cost function here is something convex okay so we're talking about linear quadratic Gaussian so the cost function is quite right is that it's called regulation regulatory cost in sense that is quadratic in the sense that if you're away from it you get penalized the distance squared.

163
00:36:22,000 --> 00:36:49,000
Okay so if you're too far you're going to get penalized very very even higher so now you have a model some estimate of the model you deploy your policy or controller in the real world and the deviation is large okay your model is not accurate but when you deploy a controller on the environment the deviation you get is large so you're not going to use the optical policy of the model you're using.

164
00:36:49,000 --> 00:37:18,000
You're using optimal you're using a policy which gains some information about the model you learn such that when you applied on the real world is not going to deviate too much okay so it's like kind of different from the paradigm we have been using in classical reinforcement learning algorithm that you come up with the estimate of the model and then build a confidence interval and find a model in that confidence interval and then find a policy for that model.

165
00:37:18,000 --> 00:37:43,000
This was a paradigm we have been using but in this paper we're saying that no you estimate the model good use this controller that is going to do well on this environment on the real world and if it's not performing well make it better despite the fact you don't know the real world almost like the initial estimate of the models like an initialization as opposed to something that you're fixed on.

166
00:37:43,000 --> 00:38:08,000
Yes so you have initialization of the model and you have basic estimate you have the you come with a controller you apply that controller on the world take five samples you see the deviation is really large what you do you do not just keep looking at it you updated you updated how you updated gradient descent you literally do gradient descent to update it and the controller here you come up with is actually gradient descent based controller.

167
00:38:08,000 --> 00:38:35,000
You're not doing you don't do anything fancy just to SGD everything works yeah this was another cool thing happen and this this this this works actually open so many new doors for many many people in control theory and reinforcement learning and general machine learning that's now they can given these tools you can go and develop many many many things for example knowing how to learn this partially observable linear dynamical systems.

168
00:38:35,000 --> 00:38:47,000
So this settings are partially observable linear dynamical system they're linear because dynamics is linear they are partially observable because of the example you gave the state is noise you don't exactly observe the state.

169
00:38:47,000 --> 00:38:58,000
Now people are developing all sorts of algorithms for these settings because now we know how to design a controller and we also know how to learn the dynamics.

170
00:38:58,000 --> 00:39:13,000
People took this stuff to non-linear control as well so now we also know how to control non-linear control problems if this interview was in two months I could share many many weirdly cool news with you but maybe maybe next time.

171
00:39:13,000 --> 00:39:29,000
Awesome awesome one of the other areas that you identified is risk sensitive RL talk a little bit about the shifts that you've been seeing and the way we're optimizing these are problems.

172
00:39:29,000 --> 00:39:48,000
So this is very good and I hope that the audience would also see it I kind of sometimes my friends they told me that I'm good at finding this seeds happening in machine learning field that start growing and blooming and I think I hope that my predictions correct for this one.

173
00:39:48,000 --> 00:40:07,000
This is one of them I think this is I mean I'm going to make a prediction but I made so many prediction before they worked I hope this works as well if it doesn't I apologize for those folks that were looking for something which is going to be big but turn out not to be big.

174
00:40:07,000 --> 00:40:23,000
But okay let me come and see it sounds like an important area it is I'll let you go ahead and describe it it is insanely important all the machine learning we have not all almost all the machine learning stuff we have been doing in last I don't

175
00:40:23,000 --> 00:40:35,000
eight years they were around this idea that hey I have a loss function I want to maximize it or I want to minimize it how I'm going to evaluate it look at the accuracy.

176
00:40:35,000 --> 00:40:46,000
This is what we have been doing a supervised learning in reinforcement learning give me an algorithm give me an algorithm which maximizes the expected return what I carry is expected return.

177
00:40:46,000 --> 00:41:08,000
Now you come to my office let's assume that I am a I'm a healthcare practitioner you tell me you have a prescription drug machine which is going to have 90% accuracy okay I would ask you hey what happens to that one person are you going to make a small mistake or you're going to kill the person.

178
00:41:08,000 --> 00:41:37,000
Okay so it's like your act the expected value of your your return is not the thing I would carry in practice I would care how you would do in different parts of the tail okay so if you're doing your your you're doing 90% your expected return is like really high good what is your variance is variance let's assume I'm I'm I'm a H1

179
00:41:37,000 --> 00:41:59,000
holder or whatever it's called H1 manager and you give me a policy and you tell me hey if I use this policy in expectation I'm going to make five billion dollars a year good and ask was the variance the variance is like or the standard deviation if the standard deviation is like 10 billion then I'm not going to use it okay

180
00:41:59,000 --> 00:42:15,000
or or if like if if you tell me this is the variance let's assume the variance is also low but if I look at the temper if if I apply your policy in the real world I'm going to make money every day right some different money or I can lose money every day

181
00:42:15,000 --> 00:42:41,000
but if I look at 10% lower quantile if it is going to be that 10% lower quantile is going to be like humongous low or these are like examples or if you are deploying a policy in societal setting or judiciary system and you say in this city I use this policy to do to help judges

182
00:42:41,000 --> 00:42:55,000
and then you say okay the crime or the wealth of the people in the in the city increased by 10% okay great but now I go and see like five people in the city became billionaire

183
00:42:55,000 --> 00:43:11,000
expected value is not that important variance also not be might not be that important 10% or 5% or even 1% upper quantile of my distribution was important okay so these are the things like now you give me a classification algorithm

184
00:43:11,000 --> 00:43:37,000
I applied on my problem great but I want to know what is the top two person when you when I show image of dog to you you tell me like you misclassify it how bad you misclassify okay so these kind of things that actually we care about the distribution of our performance not just the performance itself

185
00:43:37,000 --> 00:43:47,000
okay so now what I think is then also I've been working on it in this area is like and also there are so many works just happened last two years

186
00:43:47,000 --> 00:44:09,000
weirdly just last two years they have been like bombards of papers in this area that people are asking hey can I have a machine learning algorithm that is able to actually maximize different risk functionals instead of just expected value okay

187
00:44:09,000 --> 00:44:27,000
this is one thing another thing is given my my machine can I evaluate the performance of my machine with respect to all sort of risk functionals okay before we were you were going you if I'm a healthcare practitioner you give me your policy

188
00:44:27,000 --> 00:44:43,000
and I was going to look at the look at the past data and see what is this expected performance now the questions can I look at the top 10 person quantile can I look at this variance can I look at top 50 quantile can I look at different

189
00:44:43,000 --> 00:45:03,000
different weird things about it and decide whether your machine is good or not so these are things that I think people in practice need this is aligned with that idea was or theme was saying that we are these days we are trying to develop special specialized machine learning methods

190
00:45:03,000 --> 00:45:21,000
so for here we're trying to come up with the specialized methods that practitioners that say in healthcare or in stock market would be able to use let me give you this example dark I asked 20 companies to give me their prescription

191
00:45:21,000 --> 00:45:35,000
prescription policies and I'm I'm CDC I'm going to adopt it this is a hypothetical example and now what I would do I look at this 20 machines I look at their expected value I look at their

192
00:45:35,000 --> 00:45:49,000
variance I look at many many different risk quantities and based on all of them I'm going to decide which one is better so first question comes up is whether the thing I'm doing is the statistics that is basically valid or not

193
00:45:49,000 --> 00:46:01,000
if I look at 10 billion different tests and run the 10 billion tests on these these 20 machines the result I'm going to get are going to be valid or not these are like questions that people have been trying to answer these days

194
00:46:01,000 --> 00:46:21,000
and they have been trying to ask similar with reinforcement learning algorithms it sounds like this is a broad movement or trend that you're seeing in machine learning in the RL setting how is it accommodated what are some of the things people are doing

195
00:46:21,000 --> 00:46:35,000
actually the interesting thing is most of the fundamental development happen actually from RL which are in RL first yes yes so for example this idea of like so for this idea of this healthcare setting

196
00:46:35,000 --> 00:46:43,000
you give me your policy I want to evaluate the performance of your policy with respect to all the risk functionals

197
00:46:43,000 --> 00:46:55,000
okay so this is a contextual bandit problem that this this topics I'm talking about there are mainly or so many other risk functionals people have studied these things in the context of RL

198
00:46:55,000 --> 00:47:05,000
people have studied an MDP and contextual bandit these are what I am aware of and there are so many impossibility results there are so many positive results

199
00:47:05,000 --> 00:47:17,000
and from when you're using different risk functionals that deserve there are many many things out there coming from reinforcement learning and now people are getting ideas and generalizing to to general machine learning settings

200
00:47:17,000 --> 00:47:29,000
okay interesting interesting can you give us an overview of some of the specific papers and and the specific things that they're trying to accomplish

201
00:47:29,000 --> 00:47:39,000
there's one one another point the reason it actually emerged from reinforcement learning is most of these things arise when we are talking about decision making

202
00:47:39,000 --> 00:47:48,000
if I'm going to make a come over the policy for society or I'm going to give come over the policy to be used in healthcare I'm solving a reinforcement learning problem

203
00:47:48,000 --> 00:47:58,000
so these type of questions basically came from reinforcement learning setting another example I give you in supervised learning we don't talk about safety much

204
00:47:58,000 --> 00:48:07,000
safety is another constraint where we need to have these are this safety mainly is talked about and developed in the field of reinforcement learning

205
00:48:07,000 --> 00:48:18,000
I'm pretty sure people are taking these things to supervise learning as well because in supervised learning safety is important but these are more important issues in reinforcement learning

206
00:48:18,000 --> 00:48:42,000
so that was these are basically some of the reasons why these things emerged from reinforcement learning as some of the papers so there are some papers that actually talk about at this risk functional that Daniel Conman and his colleague they came up with this prospect theory that you would prefer to lose less than gain much gain more

207
00:48:42,000 --> 00:48:53,000
this is this whole idea prospect theory is like if I give you ten more dollars you're going to be less happy than if I get ten dollars from you

208
00:48:53,000 --> 00:49:02,000
so this idea says hey you don't care about expected value if you earn more you're going to be less happy than you are going to lose the same amount

209
00:49:02,000 --> 00:49:11,000
therefore the expected values they don't this thing they don't sum up if you lose more you're going to be really angry if you gain same amount you're not is not going to compensate

210
00:49:11,000 --> 00:49:29,000
right so lost a version should have a higher impact than the possibility of gain yeah so yeah exactly so these are the things that now if that's a case if I want to come with an RL algorithm for mdp for contextual band it if I consider these things what can I say

211
00:49:29,000 --> 00:49:42,000
can I find is it possible to learn a policy which maximizes the prospect theory objective function can I find or is there there are impossibility results

212
00:49:42,000 --> 00:49:56,000
there are some other things which say which say which come from KL divergence stuff which says if you make more you're going to be basically same thing if you make more you're going to be less happy than if

213
00:49:56,000 --> 00:50:04,000
so if you have some certain amount of money if you make a little bit more you're going to be more happy than making so much more

214
00:50:04,000 --> 00:50:14,000
saturating utility or that kind of idea yeah exactly so if you are going to use some ideas like that can we get algorithms working

215
00:50:14,000 --> 00:50:35,000
so these are like risk specific settings there are some other people working on the area that say hey you're interested in prospect theory good for you you're interested with this interesting this exponential stuff or this marginalized benefit of stuff good for you

216
00:50:35,000 --> 00:50:46,000
can we have a theory which actually answers question with respect to all possible risk functional ever developed

217
00:50:46,000 --> 00:50:56,000
okay so in insurance premium design there are people they talk about some risk functional called distorted risk functional

218
00:50:56,000 --> 00:51:06,000
what is this it just takes different part of the quantile and distorted and come up with the and then do the expectation of that distorted reward basically

219
00:51:06,000 --> 00:51:18,000
okay in in in risk management they look at things called CVR so what is CVR it looks at less expected value of your term of the 10% upper quantile

220
00:51:18,000 --> 00:51:26,000
so I don't care what is your expected value you are going to make if you are going to lose means the 10% lower contact what is the expected value there

221
00:51:26,000 --> 00:51:30,000
so this is another thing that people call use this called

222
00:51:30,000 --> 00:51:43,000
condition value at risk the CVR yes condition value at risk so condition on that you are into lower 10% of the quantile what is the condition value of your money at risk

223
00:51:43,000 --> 00:51:54,000
okay so these are the things my company cares about quantile of 10% your company cares about 15% my company might care about 15% to all

224
00:51:54,000 --> 00:52:04,000
percent 13% for 14% all of them okay my company might also care about the distorted risk functions my company might care about prospect theory risk function

225
00:52:04,000 --> 00:52:21,000
and is the is the idea to to be able to decouple any particular problem from the specific loss function or optimization function so that

226
00:52:21,000 --> 00:52:29,000
so that you can kind of get results for multiple ones at the same time like how exactly reformulate in the problem here

227
00:52:29,000 --> 00:52:45,000
that's a very good question now let's say I give you the past data set my company has been working in healthcare for last 10 years I look or last five months and I if I'm lucky last 10 years

228
00:52:45,000 --> 00:52:59,000
so I have five years of data and now you give me many many are all developed policies and I'm going to look at the performance of these policies with respect to all these

229
00:52:59,000 --> 00:53:14,000
risk functionals and then I want to give me 20 policies I use this data set to evaluate the risk performance of all these 20 policies with respect to infinitely many risk functionals

230
00:53:14,000 --> 00:53:30,000
who designs or risk function those risk functionals my experts in my company they come up with all these risk functionals that or tests basically and one of one of my my experts says let's look at the see war of 10%

231
00:53:30,000 --> 00:53:40,000
another one says let's look at see war of 15% I take this to 20 policies and apply it under my data and estimate all these quantities

232
00:53:40,000 --> 00:53:53,000
okay and after this guy seeing all these numbers they might come with new tests and they might also after seeing those results of those tests they might come with many many new tests and then in the end they might choose one of these policies

233
00:53:53,000 --> 00:54:11,000
okay so this is the setting I have this past data from like last five months you or some other companies they give me 20 or policies and they tell me these are good I'm going to go and talk with my expert to see whether they are good or not

234
00:54:11,000 --> 00:54:20,000
so I'm going to use these policies and apply it on my past data and see how they perform with respect to all these risk functionals

235
00:54:20,000 --> 00:54:35,000
and so on that I'm not very clear on what makes this interesting from an RL perspective like the first thing we talked about was you know it's clear why that being interesting and important you want to develop a policy using RL

236
00:54:35,000 --> 00:54:51,000
you used to do it based on maximizing expected reward but there are all these other things that you care about how do you do those with RL the second problem sounds like you have a policy it could be RL policy it could be any policy and you're just kind of

237
00:54:51,000 --> 00:55:10,000
chugging your past data through the policy and evaluating the results what am I missing there the first setting is called online setting that you're actually using your RL algorithm to design and come up with some policy so these are those papers I talked about that they look at the different risk functionals

238
00:55:10,000 --> 00:55:25,000
they said the second setting I'm talking about that you have to any policies they can be expert design policies they can be RL policies whatever policies this is called offline setting or off policy setting so I have the data

239
00:55:25,000 --> 00:55:30,000
you give me policies I want to evaluate them first before using them in practice

240
00:55:30,000 --> 00:55:53,000
so I want to assess their performance before using them in practice so this is we call off policy risk assessment problem or in short opera is like you have off policy data from the past or log data and you use that data set to actually assess the performance of your policy

241
00:55:53,000 --> 00:56:09,000
okay so this is a whole topic of off policy setting off we have off policy policy evaluation we have off policy policy improvement means I give you past data I hire new person I showed that person last 10 years of data that person is supposed to give me a good policy

242
00:56:09,000 --> 00:56:24,000
okay I'm not going to allow that person to deal with the real world data real world system I'm just going to give that person the last 10 years of data so this is off this is a log policy I'm going to use to design a good policy

243
00:56:24,000 --> 00:56:37,000
okay so this is called off policy policy assessment it's different from online setting that I'm like I want to solve an Atari game I want to go and in the wild try different things at different situation to see what happens

244
00:56:37,000 --> 00:56:53,000
this is a little bit different I get I get that I get the on policy versus off policy or online versus off policy and also get the

245
00:56:53,000 --> 00:57:08,000
you know there's also off policy you know training where you're developing the policy itself I get that I what I'm what I don't what's not clear to me is why the assessment or the evaluation is an interesting problem

246
00:57:08,000 --> 00:57:20,000
like I'm thinking of it in the context of an inference like him you giving me data I'm applying my policy I get some results like what's what's the hard part there

247
00:57:20,000 --> 00:57:39,000
oh the hard part so the thing is so I have multiple policies and I applied them on my data okay and I look at expected value we know statistically that this estimate of the expected value is going to be close to its mean to the true expected value

248
00:57:39,000 --> 00:57:56,000
okay now you compute the variance I go and look at the data I want I want to compute the variant performance the variance of the performance of each of those policies I can do that and I can come with an estimate that is going to be close to the true

249
00:57:56,000 --> 00:58:12,000
estimate the first one was the expected value was close to the true estimate with probability like one minus delta the second one is also close to the true estimate with probability one minus delta but both of them are valid estimate probability one minus two delta

250
00:58:12,000 --> 00:58:27,000
okay so the probability that these are going to be correct became smaller now you are asking me to test 1000 different risk functionals so all of them are going to be valid estimation with probability one minus 10,000 times delta okay

251
00:58:27,000 --> 00:58:48,000
so now the question I'm asking is can we do these things with respect to infinity many risk functionals why I care about infinity risk functionals because if I show expected value and variance and see what of 10% to an expert expert might design an arbitrary risk functional that I have not seen before

252
00:58:48,000 --> 00:59:02,000
okay so I want to come over the way to estimate all the risk functionals performance of my model with respect to all the risk functionals and make sure that the estimate I'm going to get is going to be valid estimation

253
00:59:02,000 --> 00:59:31,000
okay this was not known before like this is like a new things that people are actually sure this possible so it's the it's the it's managing the statistical assessment of the policy given a you know size of a data set number of runs all of that and trying to get as much information as you can about all of these different things that you care about without compromising the validity of the estimates

254
00:59:31,000 --> 00:59:46,000
exactly that's exactly we're putting I and also one important things I want this all these estimates which I have infinitely many of them to all those estimates I want them to hold simultaneously with the constant probability

255
00:59:46,000 --> 01:00:01,000
okay so it's like I don't want to compromise at all I want to get all of them correct can I do that or not so this was not known and now now in the last two years appeared that it's possible got it got it

256
01:00:01,000 --> 01:00:30,000
so the fourth trend that you identified and we'll try to get through this one quickly is structured MDPs what's that about this is also very fascinating area which is a gain again a line with the topic I was saying at the beginning that the new theme and trend in reinforcement learning is to come up and design problem more problem specific algorithms back in time I was doing reinforcement learning

257
01:00:30,000 --> 01:00:50,000
when I started reinforcement learning we were trying to come with algorithm which actually works for worst thing ever okay nowadays we were saying hey if you're not dealing with the worst thing ever you are dealing with the world and world is not as hard as things in mathematics can get

258
01:00:50,000 --> 01:01:17,000
okay why we're coming with our algorithms are going to work well against the worst thing can ever mathematically happen okay so this is the idea behind structure in reinforcement learning so last okay this isn't very old topic but last two or three years it became very very people got back to it basically

259
01:01:17,000 --> 01:01:45,000
so one of the things is called state abstraction you are saying okay I have many many states in my RL setting RL environment but it's not like all of the more behaving differently I can actually cluster them and they're actually going to behave similarly so this is the basic says there is an underlying clustering of the environment of the observation that I can use to reduce the complexity of my model

260
01:01:45,000 --> 01:02:05,000
if my environment has 10,000 states but if I can cluster them each 100 or 500 of them and come with the problem with 10 states I can easily solve everything okay so this is an idea let me give you another example if it's possible so like let's say I'm here I my job is to go to my blackboard

261
01:02:05,000 --> 01:02:24,000
okay so my the state of I can say this visual state I have is my state so this observation I have is my state if I move back for my observation changes if I move forward my observation changes if I turn left or right my observation would change too

262
01:02:24,000 --> 01:02:39,000
okay but the thing is this observation is humongous space it comes from humongous space but I can map this observation this big observation to my current location and if I know my current location I know the optimal action is this way

263
01:02:39,000 --> 01:02:54,000
if I look at this direction I'm seeing different I mean different state but this state or this observation I have is rich enough for me to infer my current location and I go I know the optimal action is going backward

264
01:02:54,000 --> 01:03:21,000
so this is an idea so when I'm at this state all this observation I'm looking at these are different states but all of them work they can be mapped directly to my current location and if I know my current location I know my optimal policy so this is called rich observation MDP so you says your observation is rich enough to directly infer your current location so this is one of the settings that people have been working many many more or many many papers in this area

265
01:03:21,000 --> 01:03:43,000
and it sounds like it's related to work that has been going on for a while to try to use geometry to reduce the state space from observe pixels to something that's you know maybe in this language more structured and that can be more easily operated on

266
01:03:43,000 --> 01:03:53,000
so we have this word model paper so they are able to get this whole observation of the pixels and map it to the latent state of the VA

267
01:03:53,000 --> 01:04:07,000
which latent state is continuous or I have a paper with some colleagues from Canada and here that we actually map the whole pixel space to finite many states

268
01:04:07,000 --> 01:04:19,000
or there are many works that are actually they are called by simulation they are actually trying to come with this type of geometric structure of the problem

269
01:04:19,000 --> 01:04:27,000
there's a friend of mine David Able he's also has done so many works in this area that I actually trying to do a state abstraction this is one thing

270
01:04:27,000 --> 01:04:40,000
but there are so many other structure people came up with one of them which is very interesting is called linear MDP what it says it says basically things the transition function in my MDP

271
01:04:40,000 --> 01:04:51,000
despite the fact that I have many many states the transition can be linearized it can be written in the basically somewhat let me say it in some special case

272
01:04:51,000 --> 01:04:59,000
it can be written as a summation of bunch of known MDP basically so your MDP can be written as a summation of some known MDPs

273
01:04:59,000 --> 01:05:07,000
this way your q function becomes linear in the on those feature presentations that you would get from those MDPs

274
01:05:07,000 --> 01:05:16,000
okay so this is let me tell you this way let's assume that your q function is linear with respect to some feature presentation

275
01:05:16,000 --> 01:05:28,000
okay why is good it's good because or at least might make sense is if I'm training a deep q network I have a really deep neural network

276
01:05:28,000 --> 01:05:37,000
and the last layer is a linear layer right I can assume that this feature presentation is good enough that my q function is linear with respect to this feature presentation

277
01:05:37,000 --> 01:05:49,000
okay so it actually has some meaning to the in practice so people have been studying this setting specific in last two years and and it's been like glorified

278
01:05:49,000 --> 01:06:04,000
and there's so many I think more than 100 papers in this in this specific area that the assumes that there is some there is some linearity structure in the MDP that you can exploit to come up with a good policy

279
01:06:04,000 --> 01:06:16,000
there are people making low rank assumption that the underlying system is low rank and basically transition function or transition kernel

280
01:06:16,000 --> 01:06:27,000
despite the fact that it being like humongous thing it actually has some low rank structure and this one has many many interpretation

281
01:06:27,000 --> 01:06:36,000
people making different assumptions for example in metal learning and reinforcement learning people make assumption that when you go from one environment to another environment

282
01:06:36,000 --> 01:06:48,000
everything almost stays same but some linear part of the environment changes and these are all interesting because these are all easily transferable to deep neural networks

283
01:06:48,000 --> 01:07:01,000
and so I made the assumption or the statement earlier that a lot of the efforts I've seen the ultimate benefit is trying to get to sample efficiency

284
01:07:01,000 --> 01:07:14,000
and being able to convert trashers at the primary benefit of this work or are there also performance implications or generalizability implications or other implications

285
01:07:14,000 --> 01:07:25,000
it depends on who you're asking who writes those papers some people just care about the fact that how well they can improve the sample efficiency

286
01:07:25,000 --> 01:07:37,000
and I was part of that crowd many years ago but these days there are other people there they are like hey what makes sense in practice

287
01:07:37,000 --> 01:07:46,000
how things work we know in practice we can use deep neural networks to come over the feature presentation and do metal learning on just the last layer

288
01:07:46,000 --> 01:07:56,000
if that's the case can I theoretically study this so some people their goal is to theoretically study the thing that they know is going to work in practice

289
01:07:56,000 --> 01:08:08,000
and they theoretically study it and see whether it works in theory and then come over the real world applications some people mainly care about the fact that whether they can get a better sample complexity

290
01:08:08,000 --> 01:08:21,000
both approaches are awesome I love both but I'm more in the I'm kind of I have one leg in one camp and heavier leg in other camp

291
01:08:21,000 --> 01:08:40,000
yeah awesome awesome so we started this talking about the the flashy you know demos or results or or papers that kind of caught folks's attention

292
01:08:40,000 --> 01:08:52,000
and that kind of you abandoned that and kind of went right into these trends starting with robotics because that was one of those areas did we cover all of the areas that you thought were flashing

293
01:08:52,000 --> 01:08:56,000
talking about the trends were they all represented in those trends

294
01:08:56,000 --> 01:09:17,000
yeah there's one more trend that I did not talk about it's I am still not very I would say not knowledgeable enough to talk about it but I see it's a big trend is called self supervised learning approach in reinforcement learning

295
01:09:17,000 --> 01:09:33,000
that's I'm trying to get myself involved in the sense that to be able to learn what people are doing and what are the ideas it goes things I've read so far they seem to be extremely promising

296
01:09:33,000 --> 01:09:48,000
and basically transferring knowledge from different tasks again we are going to direction of having problem specific approaches so I have many many tasks can I kind of do transfer knowledge from one to another

297
01:09:48,000 --> 01:09:55,000
can I use whatever I learn from one environment and deploy it and get help to improve my performance in different setting

298
01:09:55,000 --> 01:10:14,000
these are the things that people are doing in this area I think it's extremely promising is trending and it's a general theme is a new paradigm that needs a lot of attention from us but I'm still a very junior student in that area

299
01:10:14,000 --> 01:10:35,000
one of the historical challenges or critiques of RL has been that it's very difficult to apply you know a lot of the results that got the most press or visibility or you know games and other toy problems

300
01:10:35,000 --> 01:10:47,000
what kind of progress have we made in the applicability and kind of real world use cases commercial developments that kind of thing over the past couple of years

301
01:10:47,000 --> 01:11:08,000
yeah that's a very good point yeah for supervised learning problems people just go and play and things usually work merely because there are those problems are much easier than RL problems you can now just download one of the implementation of DQN and applied on some random problem if it wouldn't work

302
01:11:08,000 --> 01:11:24,000
okay it's like if you take the implementation of a model which works on the image and then apply to another image problem it works but if you apply DQN best implementation of DQN and take it on apply to another problem I doubt it would work

303
01:11:24,000 --> 01:11:39,000
okay so it's like you are dealing with the harder problem there are so many things to be you need to tweak there and so generally you need a harder problem and because you're solving a harder problem so you need to be more expert

304
01:11:39,000 --> 01:11:53,000
so one thing I'm seeing now is yes we have those game stuff who have worked on those game stuff basically all the experts in reinforcement learning now people are getting reinforcement learning algorithm to work in practice

305
01:11:53,000 --> 01:12:17,000
they are actually experts in reinforcement learning for example the robotics setting I'm talking about people got reinforcement learning algorithm to work in robotics not because they just downloaded something from GitHub they learn things they invented new RL algorithms those work in their setting they did not say okay let's download this one and run it

306
01:12:17,000 --> 01:12:31,000
no they wanted to solve the problem they took courses in RL I have one colleague who is a robot assistant control theorist who knows a lot of RL who can just generate and write down many many machine learning algorithms he's an expert in machine learning right now

307
01:12:31,000 --> 01:12:55,000
and some of these domain adaptation stuff and metal learning in in in deep learning stuff I was talking about are basically his work and he has all the learning theoretic guarantees and everything is like if you read the paper you feel like it's like a veteran of RL theory person wrote it but this person is like three years into the machine learning field of four years

308
01:12:55,000 --> 01:13:20,000
so he's extremely smart and he knows everything right now so people now working different areas that are deploying machine the reinforcement learning algorithms learn that hey you cannot just download and plug and play you need to know what you're doing so people are getting more expert in this area in recommendation system people have been using reinforcement learning algorithms forever and thought they've been making a lot of money

309
01:13:20,000 --> 01:13:41,000
in that many I don't know how many years this has been the case but in hedge funds have been tiring amazing RL folks and getting our algorithms to work they also understood that hey in order to make things working to know things very well so they actually got became expert in this area

310
01:13:41,000 --> 01:13:56,000
and many other manufacturing companies that are trying to get RL to work they also understood that you cannot just download and press run control or to to for the machine to work

311
01:13:56,000 --> 01:14:11,000
yeah it was not seriously it was the case like three years ago four years ago I've been hearing stories from big companies that people downloaded like DQ and algorithm and ran it on random game I was like how you're expecting to work

312
01:14:11,000 --> 01:14:20,000
and they have spent like two or three years to make that thing work and they fail of course is not you need to know the pieces of it

313
01:14:20,000 --> 01:14:38,000
you're touching on another slightly removed issue one issue is the ability to take results out of academia and kind of apply them to you know real or problems and have them easily easily work

314
01:14:38,000 --> 01:14:53,000
another related issue is reproducibility like I'm downloading the paper and the game that the paper was written to play and trying to get that to work and that should be easier in theory

315
01:14:53,000 --> 01:15:10,000
yeah but it's still hard yeah yeah genuinely this happens because the other problems are genuinely harder it's like the super wise learning is very tiny teeny special case of reinforcement learning

316
01:15:10,000 --> 01:15:24,000
so one thing I tell my students for my RL course is the first lecture is like I do not expect everything to be easy any problem you solve in reinforcement learning means that you have solved many many many problems in many many many fields

317
01:15:24,000 --> 01:15:39,000
because it subsumes many many fields in machine learning so it's genuinely hard and I don't know we are going in the right direction or bad direction because we are at this in academia

318
01:15:39,000 --> 01:15:51,000
what I'm seeing is like we are kind of weak in RL not in sense of we don't know in the sense that we don't we don't have that many people to train the next generation

319
01:15:51,000 --> 01:16:03,000
I mean you look at France like France had so many people in in RL five years ago now all of almost all of them are not there anymore or in US

320
01:16:03,000 --> 01:16:15,000
we had we had slow growth in reinforcement learning we have them in reinforcement in people like 10 years ago but now we have more but the demand is much much much higher than the number of people we have

321
01:16:15,000 --> 01:16:31,000
so who can give the next I don't know the industry or other sections of academia who can give or who can fit them with new experts in the fields this is a kind of scary thing at least for me I don't know where

322
01:16:31,000 --> 01:16:39,000
are you still looking for PhD students yeah whoever is the other who wants to do reinforcement learning please join

323
01:16:39,000 --> 01:16:50,000
yeah I'm doing everything I can to train the next generation of reinforcement learning experts but it's just me and few others darn them many people in the

324
01:16:50,000 --> 01:16:56,000
in academia who do reinforcement learning which is kind of not what we want at the moment

325
01:16:56,000 --> 01:17:10,000
top predictions for 2022 and beyond I would say at least for 2022 I'm I'm sure we're going to have so many new structure in in

326
01:17:10,000 --> 01:17:25,000
or a structural assumption in in Markov decision processes for example and people would come up with new ways of modeling MDPs or a special case of MDPs to make them suitable for practice

327
01:17:25,000 --> 01:17:40,000
so I would see that there is a huge wave of experts in the area now they're trying to cook up new assumptions or new structures for MDP problems to

328
01:17:40,000 --> 01:17:49,000
come up with algorithms that actually are more sample efficient and hopefully would help us to design efficient algorithms in practice

329
01:17:49,000 --> 01:18:03,000
okay this is one thing I think is going to be one of the main thing in 22 another thing I am sure is going to be the case given the fact that most of the control sorry most of the

330
01:18:03,000 --> 01:18:20,000
roboticists have been in lockdown and could not go to lab is going to be a lot of work intersection of reinforcement learning and robotics in 2022 I lost track of each year we are hopefully

331
01:18:20,000 --> 01:18:31,000
the pandemic would be over and we're going to get back to normal so this robotics is another thing I think is going to be huge given this techniques we have in control

332
01:18:31,000 --> 01:18:47,000
theory I think it's going to be one of the main thing in like next few years that people there are many many amazing and awesome control theory

333
01:18:47,000 --> 01:19:03,000
and all the departments you go ask whose control theories you see like or any university you go you see like 10 or 20 people they claim their control theories maybe 20 or 5 or 10 people they claim their control theories

334
01:19:03,000 --> 01:19:17,000
RL theory or RL expert like probably you get one so there's these people actually the people in control there understand the power of this reinforcement learning algorithms tools we provide in this area

335
01:19:17,000 --> 01:19:25,000
and I'm sure all of them are going to recognize the power of these things and start to deploy them in their work

336
01:19:25,000 --> 01:19:35,000
resulting in new generation of learning and learning and control algorithms for next few years basically these are

337
01:19:35,000 --> 01:19:45,000
some of the prediction but I'm sure that we are in this direction that we're going that we are trying to come up with more problem specific algorithms

338
01:19:45,000 --> 01:19:55,000
we're going to have awesome algorithms that are going to provide safe and robust reinforcement learning methods in next 5 years

339
01:19:55,000 --> 01:20:05,000
we're going to have methods and our algorithms that can handle all sort of risks functional that people carry in practice

340
01:20:05,000 --> 01:20:27,000
things that I think people would work on a lot and also this from practice point practical point of view coming up with a scenario and settings such that you can have some meta model that you can use

341
01:20:27,000 --> 01:20:39,000
okay you can transfer knowledge from different problems to problems that you deal with this is another thing that I think going to be blooming in next few years

342
01:20:39,000 --> 01:20:45,000
like in NLP you have this language model you can just use it to do many many things

343
01:20:45,000 --> 01:20:57,000
we will have something like that not very long in reinforcement learning learning of course RL is more complex so you're not going to have one model

344
01:20:57,000 --> 01:21:05,000
you're going to have one model for robotics one model for I don't know the dialect systems of one model for Atari games

345
01:21:05,000 --> 01:21:13,000
one model for self-driving cars these are the things that are probably going to be driving factors for next few years

346
01:21:13,000 --> 01:21:25,000
yeah well Kambiar thanks so much for running through this with us and specifically thanks for all of the work that you put into pulling this together

347
01:21:25,000 --> 01:21:35,000
continues to be a fascinating field and a lot of interesting stuff to talk about

348
01:21:35,000 --> 01:21:43,000
yeah thank you Sam thank you for having me RL is going to be fascinating field forever because it's that by definition

349
01:21:43,000 --> 01:21:47,000
if you want to have general intelligence you need to solve RL first

350
01:21:47,000 --> 01:21:57,000
it's going to be and there's so many open problems I encourage people out there to get involved in reinforcement learning

351
01:21:57,000 --> 01:22:01,000
there's so many open and cool problems to be solved in this area

352
01:22:01,000 --> 01:22:05,000
awesome thanks so much yeah thank you

