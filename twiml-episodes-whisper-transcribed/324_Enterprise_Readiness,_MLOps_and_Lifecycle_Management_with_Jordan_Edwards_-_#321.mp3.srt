1
00:00:00,000 --> 00:00:13,400
Welcome to the Tumel AI Podcast.

2
00:00:13,400 --> 00:00:15,600
I'm your host Sam Charrington.

3
00:00:15,600 --> 00:00:23,200
Hey, what's up everyone?

4
00:00:23,200 --> 00:00:24,200
This is Sam.

5
00:00:24,200 --> 00:00:29,040
A quick reminder that we've got a bunch of newly formed or forming study groups, including

6
00:00:29,040 --> 00:00:34,800
groups focused on Kaggle competitions and the fast.aiNLP and deep learning for coders

7
00:00:34,800 --> 00:00:36,760
part one courses.

8
00:00:36,760 --> 00:00:42,960
It's not too late to join us, which you can do by visiting TumelAI.com slash community.

9
00:00:42,960 --> 00:00:47,960
Also, this week I'm at ReInvent and next week I'll be at NURRIPS.

10
00:00:47,960 --> 00:00:50,360
If you're at either event, please reach out.

11
00:00:50,360 --> 00:00:52,360
I'd love to connect.

12
00:00:52,360 --> 00:00:57,860
Alright, this week on the podcast I'm excited to share a series of shows recorded in

13
00:00:57,860 --> 00:01:01,580
Orlando during the Microsoft Ignite Conference.

14
00:01:01,580 --> 00:01:05,820
Before we jump in, I'd like to thank Microsoft for their support of the show and their sponsorship

15
00:01:05,820 --> 00:01:07,140
of this series.

16
00:01:07,140 --> 00:01:11,900
Thanks to decades of breakthrough research and technology, Microsoft is making AI real

17
00:01:11,900 --> 00:01:18,660
for businesses with Azure AI, a set of services that span vision, speech, language processing,

18
00:01:18,660 --> 00:01:21,740
custom machine learning, and more.

19
00:01:21,740 --> 00:01:26,180
Millions of developers and data scientists around the world are using Azure AI to build

20
00:01:26,180 --> 00:01:31,260
innovative applications and machine learning models for their organizations, including

21
00:01:31,260 --> 00:01:35,260
85% of the Fortune 100.

22
00:01:35,260 --> 00:01:41,260
Microsoft customers like Spotify, Lexmark, and Airbus choose Azure AI because of its proven

23
00:01:41,260 --> 00:01:47,420
enterprise grade capabilities and innovations, wide range of developer tools and services

24
00:01:47,420 --> 00:01:49,460
and trusted approach.

25
00:01:49,460 --> 00:01:54,500
Stay tuned to learn how Microsoft is enabling developers, data scientists, and MLOPS

26
00:01:54,500 --> 00:02:00,260
and DevOps professionals across all skill levels to increase productivity, operationalize

27
00:02:00,260 --> 00:02:06,660
models at scale, and innovate faster and more responsibly with Azure machine learning.

28
00:02:06,660 --> 00:02:11,660
Learn more at aka.ms slash Azure ML.

29
00:02:11,660 --> 00:02:14,460
Alright, onto the show.

30
00:02:14,460 --> 00:02:18,100
Alright, everyone.

31
00:02:18,100 --> 00:02:23,420
I am here in sunny Orlando, Florida at Microsoft Ignite.

32
00:02:23,420 --> 00:02:26,940
I've got the pleasure of being seated across from Jordan Edwards.

33
00:02:26,940 --> 00:02:31,860
Jordan is a principal program manager for the Azure machine learning platform.

34
00:02:31,860 --> 00:02:34,180
Jordan, welcome to the Twinwall AI podcast.

35
00:02:34,180 --> 00:02:35,180
Oh, thanks.

36
00:02:35,180 --> 00:02:41,260
So, I'm really looking forward to talking with you about our subject for the day, MLOPS

37
00:02:41,260 --> 00:02:46,180
and related topics, but before we do that, I'd love to hear a little bit about your background.

38
00:02:46,180 --> 00:02:51,220
It sounds like you got started off at Microsoft where a bunch of folks that are now working

39
00:02:51,220 --> 00:02:54,900
on ML and AI, got started in the Bing group.

40
00:02:54,900 --> 00:02:55,900
Right.

41
00:02:55,900 --> 00:03:01,060
So, I started Microsoft a little over seven years ago, started off working on the big data

42
00:03:01,060 --> 00:03:07,700
platforms and related machine learning platforms, then ended up working on engineering systems

43
00:03:07,700 --> 00:03:13,860
for those platforms, then we decided to take those engineering systems and apply them to

44
00:03:13,860 --> 00:03:18,220
machine learning as well, hence the internal machine learning platform was born.

45
00:03:18,220 --> 00:03:22,380
As you mentioned, like a bunch of other folks who used to work on Bing, we all got moved

46
00:03:22,380 --> 00:03:26,020
into, hey, let's take the cool stuff we built for Bing's internal engineering platform

47
00:03:26,020 --> 00:03:29,300
and bring it to external customers on Azure.

48
00:03:29,300 --> 00:03:33,540
And so, I've been in the Azure machine learning team a little bit over a year now.

49
00:03:33,540 --> 00:03:34,540
Nice.

50
00:03:34,540 --> 00:03:35,540
Nice.

51
00:03:35,540 --> 00:03:36,540
And your role here on the team?

52
00:03:36,540 --> 00:03:37,540
Yes.

53
00:03:37,540 --> 00:03:42,460
I'm the product area lead for what we call MLOPS, which is really all about how do you bring

54
00:03:42,460 --> 00:03:44,620
your machine learning workflows to production?

55
00:03:44,620 --> 00:03:49,460
A topic that we've spent a lot of time talking about here on the podcast, as well as our

56
00:03:49,460 --> 00:03:55,220
recent Twomelcon AI Platforms event, you know, maybe starting, kind of directly connecting

57
00:03:55,220 --> 00:04:03,420
to your background, I'm curious the transition from, you know, a team that largely came

58
00:04:03,420 --> 00:04:10,340
out of this internal product or project Bing and is now trying to generalize those, you

59
00:04:10,340 --> 00:04:18,380
know, systems, but broader knowledge and learnings to the market, kind of, what are the commonalities

60
00:04:18,380 --> 00:04:22,620
and differences, I guess, that you encounter in trying to do that?

61
00:04:22,620 --> 00:04:27,300
So there's actually a lot of commonalities when you double click on it, but the biggest

62
00:04:27,300 --> 00:04:33,180
thing is, you know, Bing and Office 365 internal Microsoft teams have been doing AI and ML

63
00:04:33,180 --> 00:04:34,980
for a long time.

64
00:04:34,980 --> 00:04:39,340
And so they built up a lot of habits and tools and technologies, but also a lot of things

65
00:04:39,340 --> 00:04:44,300
that don't necessarily map to how we see enterprises getting started, right?

66
00:04:44,300 --> 00:04:48,100
So there's the, you know, most of our external customers today are coming in wanting to

67
00:04:48,100 --> 00:04:51,940
do Python-based development, and we have some of that internally, but we also have, you

68
00:04:51,940 --> 00:04:55,780
know, languages that predate the popularity of Python as a data science platform.

69
00:04:55,780 --> 00:05:01,580
So we have, you know, engineers doing machine learning work in.NET and C++, and so those

70
00:05:01,580 --> 00:05:03,100
workflows are a bit different.

71
00:05:03,100 --> 00:05:07,700
Also a lot of the machine learning platforms at Microsoft, as you would imagine, were previously

72
00:05:07,700 --> 00:05:12,940
Windows-based, whereas the new customers coming in wanted to do things using Linux and containers

73
00:05:12,940 --> 00:05:16,380
and their newer techniques that are being applied as well.

74
00:05:16,380 --> 00:05:21,700
So there's similarities in there, the ways they want to solve the problem, but just different

75
00:05:21,700 --> 00:05:27,820
tools that they're using, and also just different amounts of context that it builds up.

76
00:05:27,820 --> 00:05:30,140
There's also the matter of scale.

77
00:05:30,140 --> 00:05:34,820
So when you look at teams like Bing, they've got, you know, a thousand data scientists

78
00:05:34,820 --> 00:05:38,580
that are collaborating together to train these huge models.

79
00:05:38,580 --> 00:05:43,260
Most of the enterprise customers that we're talking to, they have small teams scattered

80
00:05:43,260 --> 00:05:48,060
all over the place, or they're trying to staff a team, or they have a team, and they're

81
00:05:48,060 --> 00:05:51,980
not sure how to make best use of their time.

82
00:05:51,980 --> 00:05:57,140
Also the most common problem that we're seeing that they come to us with is, hey, we have

83
00:05:57,140 --> 00:06:01,540
all these data scientists who are doing work in Jupyter notebooks, whatever, the work

84
00:06:01,540 --> 00:06:04,180
is happening on their local machines.

85
00:06:04,180 --> 00:06:10,660
We have no idea where the code is, if the code's even checked in, and they're doing all

86
00:06:10,660 --> 00:06:15,100
this work, but we can't leverage any of it on the business side.

87
00:06:15,100 --> 00:06:20,100
There's so many, so many problems in that problem statement, right?

88
00:06:20,100 --> 00:06:24,460
There's kind of a reproducibility problem.

89
00:06:24,460 --> 00:06:29,460
There's a business value path to production problem.

90
00:06:29,460 --> 00:06:32,900
There's kind of an accountability problem.

91
00:06:32,900 --> 00:06:33,900
Can you unpack that?

92
00:06:33,900 --> 00:06:37,260
How do you do prioritize those?

93
00:06:37,260 --> 00:06:40,620
We try to put it in terms of a process maturity model.

94
00:06:40,620 --> 00:06:42,340
It's exactly how you framed it.

95
00:06:42,340 --> 00:06:46,780
There's the reproducibility of the work.

96
00:06:46,780 --> 00:06:51,820
Another data scientist and the team could reproduce the same work that one person did, and

97
00:06:51,820 --> 00:06:55,700
then an automated system could also reproduce that work, which means you need clean modeling

98
00:06:55,700 --> 00:07:00,100
around the code and data and configuration that you're using in your model development

99
00:07:00,100 --> 00:07:01,100
process.

100
00:07:01,100 --> 00:07:06,780
Then there's the, how do you transition this model, this thing you've created, to production,

101
00:07:06,780 --> 00:07:11,300
so how do you package it, how do you certify it, and how do you roll it out in a controlled

102
00:07:11,300 --> 00:07:16,540
fashion, and then at the end, how do you determine the business value of your model?

103
00:07:16,540 --> 00:07:19,020
Is it making your business more effective?

104
00:07:19,020 --> 00:07:22,940
From a cost point of view, is it worth the amount of compute hours you're spending and the

105
00:07:22,940 --> 00:07:26,340
amount of man hours you're spending training these models?

106
00:07:26,340 --> 00:07:31,020
Then on the absolute end of the process maturity model is, okay, I've got this

107
00:07:31,020 --> 00:07:32,020
model.

108
00:07:32,020 --> 00:07:33,020
It's reproducible.

109
00:07:33,020 --> 00:07:34,020
I've got it deployed out.

110
00:07:34,020 --> 00:07:36,780
I'm using it for a production scenario.

111
00:07:36,780 --> 00:07:41,420
How do I know when I might need to retrain it, so completing the circle, and that's always

112
00:07:41,420 --> 00:07:46,300
the question that customers will come and start with is, how do we do automated retrain?

113
00:07:46,300 --> 00:07:48,700
It's like, let's walk back and begin it.

114
00:07:48,700 --> 00:07:52,980
How do you reproduce these models in the first place?

115
00:07:52,980 --> 00:07:56,740
That strikes me as a mature customer that's asking about automated retraining, right?

116
00:07:56,740 --> 00:07:57,740
Correct.

117
00:07:57,740 --> 00:08:00,580
Those people are trying to get the model into production in the first place, or many.

118
00:08:00,580 --> 00:08:04,220
They see the marketing hype where they read all the things like, oh, look at this company

119
00:08:04,220 --> 00:08:08,820
doing cool automated retraining stuff, and realistically, it takes a long time to get

120
00:08:08,820 --> 00:08:13,260
to that degree of maturity where you can trust that you have the high-quality data coming

121
00:08:13,260 --> 00:08:17,820
into your production systems to be able to analyze and compare and figure out I do need

122
00:08:17,820 --> 00:08:19,020
to retrain.

123
00:08:19,020 --> 00:08:25,860
Even in the case of being your office ML development teams, there's never a fully automated

124
00:08:25,860 --> 00:08:31,300
retraining loop. It's always there's a scorecard that gets generated in humans go and do

125
00:08:31,300 --> 00:08:36,300
some sort of review process prior to your new larger models going out, especially when

126
00:08:36,300 --> 00:08:39,740
they deal with things like how do you monetize ads, for instance?

127
00:08:39,740 --> 00:08:44,220
There's a lot there to dig into, but before we do that, one of the questions that I had

128
00:08:44,220 --> 00:08:47,740
for you is you've got ML ops in your title.

129
00:08:47,740 --> 00:08:51,340
What does that mean to you?

130
00:08:51,340 --> 00:08:56,020
That means to me that it's all about how do you take the work that data scientists are

131
00:08:56,020 --> 00:09:02,980
doing and make their lives easier, but also make it easier for others, other personas

132
00:09:02,980 --> 00:09:06,500
to come into the fold and take advantage of data science.

133
00:09:06,500 --> 00:09:11,420
The three personas I like to talk about is you have your data engineer who's got this

134
00:09:11,420 --> 00:09:15,700
giant lake of data. They want to figure out what value they can derive from it.

135
00:09:15,700 --> 00:09:20,260
You've got your data scientist who's tasked with finding interesting features in that data

136
00:09:20,260 --> 00:09:22,140
and training models on top.

137
00:09:22,140 --> 00:09:27,820
You've got this new emerging persona called the ML engineer whose responsibility it is

138
00:09:27,820 --> 00:09:32,660
to take the work the data scientist is doing and bring it to production.

139
00:09:32,660 --> 00:09:39,100
My job is to help the ML engineer be able to be successful and help the ML engineer be

140
00:09:39,100 --> 00:09:44,740
able to interact well with the data engineering and data science personas that are required

141
00:09:44,740 --> 00:09:47,260
to complete that circle.

142
00:09:47,260 --> 00:09:52,980
Of course, you also have the hub in the center of it, your IT ops persona, who's giving

143
00:09:52,980 --> 00:09:58,060
them all of the raw compute and storage resources to get started, making sure everybody plays

144
00:09:58,060 --> 00:10:02,540
nicely together and actually connects things and to end.

145
00:10:02,540 --> 00:10:13,140
There's an obvious echo to DevOps, to what extent is it inspirational, is it directly

146
00:10:13,140 --> 00:10:18,180
applicable, or is it counter applicable, meaning just don't try to do exactly what you're

147
00:10:18,180 --> 00:10:20,180
doing in DevOps because that's the...

148
00:10:20,180 --> 00:10:25,380
I think it's sort of all three of the things that you mentioned shocking, right, to me up.

149
00:10:25,380 --> 00:10:29,580
As far as how it's inspirational, definitely the practices that have been developed in

150
00:10:29,580 --> 00:10:35,260
the DevOps field over the past 20 years or so are useful.

151
00:10:35,260 --> 00:10:40,980
However, data scientists are not software engineers and they're not even engineers.

152
00:10:40,980 --> 00:10:45,780
A lot of them are scientists, so talking to them they need to care about things related

153
00:10:45,780 --> 00:10:53,020
to the infrastructure and package version management and dealing with all of the intricacies

154
00:10:53,020 --> 00:10:57,820
of how to run a production infrastructure, that's just not something that they're interested

155
00:10:57,820 --> 00:10:58,820
in at all.

156
00:10:58,820 --> 00:11:04,100
Trying to force these habits onto them, we've seen this even trying to get them to write

157
00:11:04,100 --> 00:11:05,820
tests for their code.

158
00:11:05,820 --> 00:11:10,300
It takes a lot of education on the net value add they're going to get from it before

159
00:11:10,300 --> 00:11:14,460
they're willing to onboard, so definitely inspirational from a process point of view.

160
00:11:14,460 --> 00:11:20,100
A lot of the same tools are applicable, but then you also need new tools that are domain-specific

161
00:11:20,100 --> 00:11:21,100
too.

162
00:11:21,100 --> 00:11:26,540
How do you do data versioning, how do you do model versioning, how do you validate and run

163
00:11:26,540 --> 00:11:34,300
integration testing on models, how do you release and do AB comparison on a model as

164
00:11:34,300 --> 00:11:39,820
opposed to a normal software application, and know if it's better or not, so yeah.

165
00:11:39,820 --> 00:11:45,060
You know, applicable and you'll get hit in the face by a data scientist if you tell

166
00:11:45,060 --> 00:11:48,860
them to go and implement all these things themselves.

167
00:11:48,860 --> 00:11:52,220
One of the things you mentioned earlier was testing.

168
00:11:52,220 --> 00:12:01,260
What's the role of testing in an MLOPS process and what kind of experiences if you had

169
00:12:01,260 --> 00:12:07,260
working with real customers to implement testing procedures that make sense for you?

170
00:12:07,260 --> 00:12:15,340
The place we try to start is by integrating some sort of tests on the data itself, so ensuring

171
00:12:15,340 --> 00:12:21,020
that your data is of the same schema, you have high quality data, like a columnized feature

172
00:12:21,020 --> 00:12:25,700
hasn't just been dropped or the distribution of values in that feature haven't changed

173
00:12:25,700 --> 00:12:26,700
dramatically.

174
00:12:26,700 --> 00:12:30,900
A lot of the stuff that we've built into the machine learning platform, especially

175
00:12:30,900 --> 00:12:36,620
on the data set profiling side, are designed to help you with that, to help you with skewed

176
00:12:36,620 --> 00:12:43,420
testing and analyzing, is your data too different to the point where you shouldn't need to be

177
00:12:43,420 --> 00:12:48,060
training it, or is it your data too similar, or is it in that sweet spot where the same

178
00:12:48,060 --> 00:12:51,820
training pipeline is actually applicable to go and solve the problem.

179
00:12:51,820 --> 00:12:58,380
That's on the profiling side, and then we also have some advanced capabilities on the

180
00:12:58,380 --> 00:13:04,940
drift side, so analyzing the over time, how are the inputs or features into your model

181
00:13:04,940 --> 00:13:10,300
changing, whether that's training versus scoring, or day every day we cover week and month

182
00:13:10,300 --> 00:13:15,300
every month of the data coming into your model, when it's making predictions, does that

183
00:13:15,300 --> 00:13:19,940
data, has the shape of that data changed over time, do you still trust the model based

184
00:13:19,940 --> 00:13:24,940
on the input values, and then of course you have the other end of it too, which is looking

185
00:13:24,940 --> 00:13:30,660
at the predictions, the model is making, whether it's from a business application, so say

186
00:13:30,660 --> 00:13:36,580
I'm using the Outlook app on my phone, and I've got the smart reply model running there.

187
00:13:36,580 --> 00:13:43,260
Now either they didn't click on any of my suggestions, they clicked on a different suggestion

188
00:13:43,260 --> 00:13:48,580
from the one I did, they clicked on the top suggestion that I had, or they said I didn't

189
00:13:48,580 --> 00:13:54,500
like any of these suggestions, all those types of feedback come into telling you, is the

190
00:13:54,500 --> 00:14:00,460
quality of data that you've trained your model on, giving you a useful model on the prediction

191
00:14:00,460 --> 00:14:01,460
side.

192
00:14:01,460 --> 00:14:08,860
So, yeah, skew testing, validating your data's quality, correctness, consistency between

193
00:14:08,860 --> 00:14:11,260
training and inference, it's all those things.

194
00:14:11,260 --> 00:14:12,260
Okay, okay.

195
00:14:12,260 --> 00:14:13,260
Yeah.

196
00:14:13,260 --> 00:14:15,580
So I'm kind of pulling your threads here, I mean, you're taking a step back, you talked

197
00:14:15,580 --> 00:14:21,940
a little bit about a maturity model that you, when you look at customers, they kind of

198
00:14:21,940 --> 00:14:29,220
fall in these end buckets, is there a prerequisite for, you know, starting to think about

199
00:14:29,220 --> 00:14:30,900
ML ops?

200
00:14:30,900 --> 00:14:38,340
So I think the prerequisite is you have to have a desire to apply a model to a business

201
00:14:38,340 --> 00:14:39,340
need.

202
00:14:39,340 --> 00:14:43,380
If your only goal is to write a model to say, you know, publish a paper, like, hey, I have

203
00:14:43,380 --> 00:14:47,460
this model to solve this school problem, then you don't really need any of the ML ops

204
00:14:47,460 --> 00:14:48,460
stuff.

205
00:14:48,460 --> 00:14:51,140
And if you're still, you know, if you're just mucking around in a Jupyter notebook,

206
00:14:51,140 --> 00:14:56,220
trying some different things by yourself, it's also a stretch to say, like, oh, you need

207
00:14:56,220 --> 00:15:00,940
these ML ops practices now, but the second you go beyond, you know, I'm keeping all my

208
00:15:00,940 --> 00:15:04,900
notes in Jupyter or I'm dumping them into one note somewhere and just keeping track of

209
00:15:04,900 --> 00:15:10,380
all my experiments on my own, the second you want collaboration or reproducibility or

210
00:15:10,380 --> 00:15:15,740
the ability to scale up and scale out to run your jobs in the cloud, that's where ML

211
00:15:15,740 --> 00:15:17,660
ops starts coming into play.

212
00:15:17,660 --> 00:15:23,980
I agree that kind of collaboration is a big driver, but even an individual researcher,

213
00:15:23,980 --> 00:15:28,700
you know, that's tracking hyperparameters and file names or on posted notes or some

214
00:15:28,700 --> 00:15:35,500
equipment worse can benefit from some elements of the tooling that we kind of refer to as

215
00:15:35,500 --> 00:15:36,500
ML ops.

216
00:15:36,500 --> 00:15:37,500
Would you agree with that?

217
00:15:37,500 --> 00:15:38,500
I would.

218
00:15:38,500 --> 00:15:39,500
Yeah.

219
00:15:39,500 --> 00:15:42,340
But just, you know, trying to sell them on using everything from the very beginning is

220
00:15:42,340 --> 00:15:43,340
the tougher sell.

221
00:15:43,340 --> 00:15:47,900
So we start by saying, you know, start by tracking your work, so it's the whole process

222
00:15:47,900 --> 00:15:48,900
maturity flows.

223
00:15:48,900 --> 00:15:53,900
You start with work tracking, then making sure everything's in a reproducible pipeline.

224
00:15:53,900 --> 00:15:57,060
And then making sure that others can go and take advantage of that pipeline.

225
00:15:57,060 --> 00:16:01,100
And then you actually have the model that you can go and use in other places.

226
00:16:01,100 --> 00:16:02,100
Yeah.

227
00:16:02,100 --> 00:16:06,940
Yeah, which, which I like the way you pull that together because in a lot of ways, one

228
00:16:06,940 --> 00:16:13,340
of the questions that I've been kind of noodling around for a while now is the, you know,

229
00:16:13,340 --> 00:16:18,900
where does ML ops, you know, start and end relative to kind of platforms and tooling and

230
00:16:18,900 --> 00:16:21,060
the things that enable and support ML ops?

231
00:16:21,060 --> 00:16:26,060
And it's, you know, very much like the conversation we were having around DevOps, like DevOps

232
00:16:26,060 --> 00:16:31,940
isn't, you know, containers and Kubernetes and things like that DevOps is a set of practices.

233
00:16:31,940 --> 00:16:37,420
And it's very much to your point, kind of a, you know, that end and process.

234
00:16:37,420 --> 00:16:43,180
So you might need, you know, any one of a number of the tools that someone might use to

235
00:16:43,180 --> 00:16:47,340
enable ML ops, but that doesn't necessarily mean that you need ML ops.

236
00:16:47,340 --> 00:16:48,340
Right.

237
00:16:48,340 --> 00:16:53,060
So when, sure, I work on Azure machine learning, when I'm talking to customers about how does

238
00:16:53,060 --> 00:16:58,340
ML ops actually work, you're going to have at least like three different tools and technologies

239
00:16:58,340 --> 00:16:59,340
being used, right?

240
00:16:59,340 --> 00:17:00,660
Because you have three different personas.

241
00:17:00,660 --> 00:17:04,580
You have data engineering, data science and DevOps, ML engineering, whatever it means,

242
00:17:04,580 --> 00:17:11,100
you're going to have some sort of data pipelining tool, something like data factory or, you know,

243
00:17:11,100 --> 00:17:15,060
airflow in the open source world, something to help with managing your training pipelines,

244
00:17:15,060 --> 00:17:19,220
whether it's, you know, Azure ML is a managed service or something like cube flow.

245
00:17:19,220 --> 00:17:22,940
If you're in the open source community and then same thing on the release management side,

246
00:17:22,940 --> 00:17:28,340
whether it's using Azure DevOps or get up actions or you're running your own like Jenkins

247
00:17:28,340 --> 00:17:32,020
server, either way, there's going to be at least those three different types of tools

248
00:17:32,020 --> 00:17:37,300
with different personas and they all need to work together and interoperate.

249
00:17:37,300 --> 00:17:43,340
So that's another key part of our pitch is like, make sure that you're being flexible

250
00:17:43,340 --> 00:17:49,980
in how you're producing and consuming events because ML ops is more than just model ops

251
00:17:49,980 --> 00:17:53,820
and you need to make sure it fits into your data and dev side of the house.

252
00:17:53,820 --> 00:17:54,820
Yeah.

253
00:17:54,820 --> 00:17:55,820
Yeah.

254
00:17:55,820 --> 00:17:56,820
Awesome.

255
00:17:56,820 --> 00:18:03,020
You mentioned Azure DevOps playing a role in here and Jenkins on the open source side.

256
00:18:03,020 --> 00:18:04,020
Yeah.

257
00:18:04,020 --> 00:18:08,300
These are tools that, you know, from the DevOps perspective, you associate with CICD,

258
00:18:08,300 --> 00:18:11,020
continuous integration and continuous delivery.

259
00:18:11,020 --> 00:18:14,740
The idea being that there's a parallel on the model deployment side, can you elaborate

260
00:18:14,740 --> 00:18:16,580
a little bit on how those tools are used?

261
00:18:16,580 --> 00:18:17,580
Yeah.

262
00:18:17,580 --> 00:18:22,820
So the way we like to look at it from a DevOps point of view is we want to treat a model

263
00:18:22,820 --> 00:18:28,500
as a packaged artifact that can be deployed and used in a variety of places.

264
00:18:28,500 --> 00:18:32,180
So your model is sure you have like, you know, your pickle file or whatever, but you also

265
00:18:32,180 --> 00:18:37,140
have the execution context for, you know, I can instantiate this model into a class and

266
00:18:37,140 --> 00:18:42,260
Python or I can embed it into like my Spark processing pipeline or I can deploy it as an

267
00:18:42,260 --> 00:18:46,060
API and a container onto, you know, Kubernetes cluster, something like that.

268
00:18:46,060 --> 00:18:50,140
So it's all about how do you bring the model artifact in as another thing that can be

269
00:18:50,140 --> 00:18:52,980
used in your release management process flow?

270
00:18:52,980 --> 00:18:54,980
It does not have to be a pickle file.

271
00:18:54,980 --> 00:18:55,980
It could be anything.

272
00:18:55,980 --> 00:18:56,980
It could be anything.

273
00:18:56,980 --> 00:18:57,980
Exactly.

274
00:18:57,980 --> 00:18:58,980
Yeah.

275
00:18:58,980 --> 00:18:59,980
Yeah.

276
00:18:59,980 --> 00:19:01,940
It's, this is my, you know, serialized graph representation.

277
00:19:01,940 --> 00:19:06,140
Here's my code file, my config that I'm feeding in.

278
00:19:06,140 --> 00:19:09,420
So it's a model is just like any other type of application.

279
00:19:09,420 --> 00:19:12,860
It just happens to come from some, or have some sort of association to like a machine

280
00:19:12,860 --> 00:19:18,540
learning framework or to have come from some data, which is actually, you know, another

281
00:19:18,540 --> 00:19:23,620
important part of the, the ML ops story is what's the, what's the end to end lineage look

282
00:19:23,620 --> 00:19:24,620
like, right?

283
00:19:24,620 --> 00:19:29,380
So ideally, you should be able to go from, I have this application that's using this model.

284
00:19:29,380 --> 00:19:34,140
Here's the code and config that was used to train it and here is the data set that this

285
00:19:34,140 --> 00:19:39,860
model came from, especially when we're talking to customers in more of the highly regulated

286
00:19:39,860 --> 00:19:40,860
industries.

287
00:19:40,860 --> 00:19:45,940
So, you know, healthcare, financial services, say you have a model deployed that's determining

288
00:19:45,940 --> 00:19:49,340
if it's going to approve or reject somebody for a loan.

289
00:19:49,340 --> 00:19:54,980
You need to be very careful that you've maintained your full audit trail of exactly, you know,

290
00:19:54,980 --> 00:19:59,700
where that model came from in case somebody decides to come in and ask further.

291
00:19:59,700 --> 00:20:04,660
This also becomes more complicated than more of a black box that your model is, but in

292
00:20:04,660 --> 00:20:10,100
general, the goal of having all these different technologies work together and interoperate

293
00:20:10,100 --> 00:20:16,620
is so that you can track sort of your correlation, ID or correlation vector across your entire

294
00:20:16,620 --> 00:20:21,540
data and software and modeling landscape.

295
00:20:21,540 --> 00:20:30,780
Can we talk about that kind of end-to-end lineage, is that a feature like you use tool X,

296
00:20:30,780 --> 00:20:36,340
use Azure ML and create a button and you have that or is it more than that and a set

297
00:20:36,340 --> 00:20:40,980
of, you know, disciplines that you have to follow as you're developing the model?

298
00:20:40,980 --> 00:20:45,780
So, yeah, it's kind of the ladder leads to an element of the former.

299
00:20:45,780 --> 00:20:46,780
Okay.

300
00:20:46,780 --> 00:20:51,100
So, assuming that you use the, I think you're the all of the above guy.

301
00:20:51,100 --> 00:20:52,100
Yeah.

302
00:20:52,100 --> 00:20:55,780
You're seeing it up right now.

303
00:20:55,780 --> 00:21:00,460
So yeah, when it comes to using the tools the right way is like, sure, you could just,

304
00:21:00,460 --> 00:21:05,420
you know, have a random CSV file that you're running locally to train a model on, but if

305
00:21:05,420 --> 00:21:10,940
you want to assert you have proper lineage of your end-to-end ML workflow, like that CSV

306
00:21:10,940 --> 00:21:17,260
file should be uploaded into blob storage and locked down and accessed from there to guarantee

307
00:21:17,260 --> 00:21:21,740
that you can come back, you know, a year later and reproduce where this model came from.

308
00:21:21,740 --> 00:21:25,660
Same thing on the code and packaging and the base container images that you're using

309
00:21:25,660 --> 00:21:27,380
when you're training the model.

310
00:21:27,380 --> 00:21:31,780
All that collateral needs to get, needs to be kept around.

311
00:21:31,780 --> 00:21:36,180
And what it's that allow you to do is, you know, we have the inside of the machine learning

312
00:21:36,180 --> 00:21:42,700
service, internal metastore that keeps track of all the different entities and the edges

313
00:21:42,700 --> 00:21:44,780
that connect them together.

314
00:21:44,780 --> 00:21:50,100
And right now we have sort of a one hop exposure of that, but one of the things we're working

315
00:21:50,100 --> 00:21:53,900
on is more of a comprehensive way to to peruse the graph.

316
00:21:53,900 --> 00:21:59,020
So it's like, hey, across my enterprise, show me every single model that's been trained

317
00:21:59,020 --> 00:22:05,260
using this data set, not scope to, you know, a single, a single Azure or a single project

318
00:22:05,260 --> 00:22:06,260
that my team is doing.

319
00:22:06,260 --> 00:22:11,420
But across the entire canvas, show me everybody using this data set, what types of features

320
00:22:11,420 --> 00:22:15,820
are they extracting from it, is somebody doing work that's similar to mine?

321
00:22:15,820 --> 00:22:19,020
Can I just fork their training pipeline and build on top of it?

322
00:22:19,020 --> 00:22:24,940
And you know, going back to how has this work we've done for internal teams inspired the

323
00:22:24,940 --> 00:22:30,380
work we're doing on Azure, that's probably the most powerful part of our platform for internal

324
00:22:30,380 --> 00:22:34,740
Microsoft teams is the discovery, the collaboration, the sharing.

325
00:22:34,740 --> 00:22:40,340
That's what allows you to do ML at high scale, at high velocity.

326
00:22:40,340 --> 00:22:44,020
So we want to make sure as much as we can that the tools and technologies that we have

327
00:22:44,020 --> 00:22:48,860
on Azure provide that same capability with all of the enterprise ready features that you

328
00:22:48,860 --> 00:22:54,780
would come to expect from Microsoft and Azure.

329
00:22:54,780 --> 00:23:01,260
So in that scenario, you outlined the starting place is a data set that's uploaded to blob

330
00:23:01,260 --> 00:23:02,980
storage.

331
00:23:02,980 --> 00:23:09,020
Even with that starting place, you've kind of disconnected your ability to do lineage

332
00:23:09,020 --> 00:23:13,900
from the kind of the source data set, which may be in a data warehouse or something like

333
00:23:13,900 --> 00:23:14,900
that.

334
00:23:14,900 --> 00:23:15,900
Yeah.

335
00:23:15,900 --> 00:23:21,100
Is there also the ability to kind of point back to those original sources?

336
00:23:21,100 --> 00:23:22,100
Oh, yes.

337
00:23:22,100 --> 00:23:25,860
So you know, sometimes you'll have a CSV there, but you can also connect to a SQL database

338
00:23:25,860 --> 00:23:30,740
or to your raw data lake and have a tracking of, okay, this is the raw data, here's say

339
00:23:30,740 --> 00:23:36,140
the data factory job that did all these transformations, here's my curated data set, here's all the

340
00:23:36,140 --> 00:23:40,860
derivations of that data set, here's the one I ended up using for training this model.

341
00:23:40,860 --> 00:23:45,900
Here's, I took this model and you know, transfer learned on top of it to produce this new model

342
00:23:45,900 --> 00:23:50,340
and then I deployed this model as this API and you can trace things all the way back to

343
00:23:50,340 --> 00:23:55,660
there and then going the other way, you know, when this model is now running, I can be collecting

344
00:23:55,660 --> 00:23:59,420
the inputs coming into my model and the predictions my model is making.

345
00:23:59,420 --> 00:24:05,420
I log those into Azure monitor and then my data engineer can set up a simple job to take

346
00:24:05,420 --> 00:24:10,380
that data coming in and put it back into the lake or put it back into a curated data set

347
00:24:10,380 --> 00:24:15,100
that my data scientist can now go and experiment on and say, well, you know, how is my, how's

348
00:24:15,100 --> 00:24:20,940
it coming into my model that's deployed compared to the one I trained it, that's completing

349
00:24:20,940 --> 00:24:22,580
the circle back to the beginning, yeah.

350
00:24:22,580 --> 00:24:24,820
Nice, nice.

351
00:24:24,820 --> 00:24:31,820
Which conceivably, you could, as opposed to talking about a data set, which, you know,

352
00:24:31,820 --> 00:24:37,060
this data set has produced what models you could point to a particular, you know, row

353
00:24:37,060 --> 00:24:41,220
in a data warehouse or something like that or a value and say, you know, what's been

354
00:24:41,220 --> 00:24:43,380
impacted by this particular data point.

355
00:24:43,380 --> 00:24:44,380
Exactly.

356
00:24:44,380 --> 00:24:47,300
And that's, that's, you know, the value that we're trying to get out of the new generation

357
00:24:47,300 --> 00:24:51,100
of Azure data lake store and some of the work we're doing on the Azure data catalog side

358
00:24:51,100 --> 00:24:56,900
is to give you exposure into like, what's all the cool stuff that's being done or not

359
00:24:56,900 --> 00:25:03,020
being done with this data, it goes back to letting your decision makers know, am I occurring

360
00:25:03,020 --> 00:25:07,780
business value from these, you know, these ETL pipelines that I'm spinning all these

361
00:25:07,780 --> 00:25:11,740
compute dollars to go and cook these curated data sets in.

362
00:25:11,740 --> 00:25:17,500
And that's a large part of what are the larger ML platform to team did before as well

363
00:25:17,500 --> 00:25:22,300
was we helped with creating curated data sets for, for being in office to go and build

364
00:25:22,300 --> 00:25:23,540
models on top of.

365
00:25:23,540 --> 00:25:28,420
So we had the data engineering pipelines and the machine learning pipelines and the

366
00:25:28,420 --> 00:25:33,940
release management pipelines all like in under the same umbrella which helped to inform

367
00:25:33,940 --> 00:25:37,980
the way we're designing the system now to be designed to meet enterprises where they

368
00:25:37,980 --> 00:25:41,060
are and help them scale up and out as they go.

369
00:25:41,060 --> 00:25:42,060
Yeah.

370
00:25:42,060 --> 00:25:48,540
I'm curious what are some of the key things that you're learning from customers kind of

371
00:25:48,540 --> 00:25:55,540
on the ground who are working to implement this type of stuff, kind of how would you characterize

372
00:25:55,540 --> 00:26:00,940
you know where folks are if you can generalize and what are the key stumbling blocks that

373
00:26:00,940 --> 00:26:01,940
kind of thing.

374
00:26:01,940 --> 00:26:07,100
So there if we're to think about it in terms of like four phases where phase one is kicking

375
00:26:07,100 --> 00:26:11,980
the tires, phase two is models reproducible, phase three is models deployed and being

376
00:26:11,980 --> 00:26:16,300
used in phase four is I've all the magical automated retraining wizardry.

377
00:26:16,300 --> 00:26:19,220
They're mostly between phase one and phase two right now, like very few of them have

378
00:26:19,220 --> 00:26:22,580
actually gotten a model deployed into the wild.

379
00:26:22,580 --> 00:26:26,460
If they have it deployed, it's only deployed as like a dev test API, they don't trust

380
00:26:26,460 --> 00:26:27,860
it yet.

381
00:26:27,860 --> 00:26:32,220
So that's you know one learning is customers were a lot earlier in the journey than we've

382
00:26:32,220 --> 00:26:38,100
been expecting coming from doing this for internal Microsoft teams.

383
00:26:38,100 --> 00:26:43,860
Another one is that the for the customers we're talking to their internal organizations

384
00:26:43,860 --> 00:26:50,060
are not always structured to let them innovate most effectively, elaborate on that.

385
00:26:50,060 --> 00:26:56,660
Yeah, so they'll have part of their org, you know, their data team and their IT department

386
00:26:56,660 --> 00:27:02,900
and their research teams are totally disconnected, disjointed, don't communicate to each other,

387
00:27:02,900 --> 00:27:04,620
don't understand each other.

388
00:27:04,620 --> 00:27:08,460
And so IT just sees what the researchers are doing and says there's no way you're doing

389
00:27:08,460 --> 00:27:10,340
any of this in production.

390
00:27:10,340 --> 00:27:16,420
The data engineers are unsure what data the data scientists are using, like they might

391
00:27:16,420 --> 00:27:19,680
data scientists might be off running SQL cores in the side, but they have no idea from

392
00:27:19,680 --> 00:27:23,300
which tables, tables will disappear under the data scientist.

393
00:27:23,300 --> 00:27:28,100
So we're just instead of doing a pure like here's how to use the platform.

394
00:27:28,100 --> 00:27:33,540
It's more, hey, let's get all the right people in the room together from IT and research

395
00:27:33,540 --> 00:27:38,700
and your data platform and your software development platforms and start a conversation

396
00:27:38,700 --> 00:27:45,540
and build up the domain expertise and the relationships on the people side before you get started

397
00:27:45,540 --> 00:27:47,780
with the process or the platform.

398
00:27:47,780 --> 00:27:52,020
That's been yeah, one big learning is to step back and focus on getting the right people

399
00:27:52,020 --> 00:27:55,940
involved first and then they can figure out the process that's going to work well for

400
00:27:55,940 --> 00:28:00,060
their business and then they can adopt the platform tools that we've been building to

401
00:28:00,060 --> 00:28:04,180
help them be more efficient at doing end-to-end ML.

402
00:28:04,180 --> 00:28:11,740
Are you finding that there's a pattern in organization that allows organizations to

403
00:28:11,740 --> 00:28:17,580
move more quickly like centralized versus decentralized or a quote unquote center of excellence

404
00:28:17,580 --> 00:28:24,500
or embedded into business units that is it one of the other of those works best?

405
00:28:24,500 --> 00:28:31,740
I think what we've seen work best is to have one business unit sort of act as the incubator

406
00:28:31,740 --> 00:28:36,540
to vet the end-to-end flow and actually get a model working in production, but then

407
00:28:36,540 --> 00:28:43,620
have the overall center of excellence centralized team observe what they're doing and take

408
00:28:43,620 --> 00:28:51,620
notes and let them flesh out what the canonical reference ML ops architecture pipeline should

409
00:28:51,620 --> 00:28:53,020
look like.

410
00:28:53,020 --> 00:28:58,020
So I think that's seem out of all the patterns we've seen a lot of patterns being applied.

411
00:28:58,020 --> 00:29:03,060
That one seems to be the best so far though is let a small team give them some flexibility

412
00:29:03,060 --> 00:29:08,620
to go and build a model, take it to production with some light guard rails and they can build

413
00:29:08,620 --> 00:29:14,980
out the reference architecture, get repository and CICD pipeline templates that the rest of

414
00:29:14,980 --> 00:29:17,340
the teams and the company can use.

415
00:29:17,340 --> 00:29:24,420
Is the salient point there that the end business unit that has the problem owns the deployment

416
00:29:24,420 --> 00:29:30,140
of the model as opposed to the centralized but somewhat disconnected data science or

417
00:29:30,140 --> 00:29:31,140
AI.

418
00:29:31,140 --> 00:29:32,140
Correct.

419
00:29:32,140 --> 00:29:33,140
Yes.

420
00:29:33,140 --> 00:29:37,820
So your DevOps team for your business unit needs to know and understand the fact that

421
00:29:37,820 --> 00:29:42,220
a model is going to be entering their ecosystem and needs to be able to manage it with the

422
00:29:42,220 --> 00:29:48,220
same tools they manage their other application releases with hence the integration with Azure

423
00:29:48,220 --> 00:29:52,900
DevOps to make sure that all your pipelines are tracked and managed in one place and there's

424
00:29:52,900 --> 00:29:57,660
not this one row release pipeline that's coming in and causing issues and havoc with

425
00:29:57,660 --> 00:29:59,860
the rest of your production system.

426
00:29:59,860 --> 00:30:07,020
And generally when you look at these production pipelines did the pipelines and the tooling resonate

427
00:30:07,020 --> 00:30:12,860
with the DevOps teams or are they like this strange beast that they take a long time

428
00:30:12,860 --> 00:30:13,860
for them to run around.

429
00:30:13,860 --> 00:30:17,180
So they freak out until they see the Azure DevOps integration and then they'll go,

430
00:30:17,180 --> 00:30:22,700
OK, I understand this, yeah, hence where I'm like, you need to have the tools that your

431
00:30:22,700 --> 00:30:25,020
audience can understand.

432
00:30:25,020 --> 00:30:28,260
You show them a Jupyter notebook, they'll jump out of their seats and run away scared.

433
00:30:28,260 --> 00:30:32,940
Whereas you show them like, oh, here's a managed multi-phase release pipeline with like

434
00:30:32,940 --> 00:30:36,980
clearly defined declarative ammo for the different steps like that that resonates well

435
00:30:36,980 --> 00:30:37,980
with them.

436
00:30:37,980 --> 00:30:41,180
Whereas data scientists, you show them a big complex approval flow and they're going

437
00:30:41,180 --> 00:30:42,660
to be like, I'm never using any of this.

438
00:30:42,660 --> 00:30:46,820
You show them a Jupyter notebook, they're happy or you know an ID with like low friction

439
00:30:46,820 --> 00:30:54,020
and Python and then your data engineers, again, you show them a, you know, a confusing notebook

440
00:30:54,020 --> 00:30:55,020
process flow.

441
00:30:55,020 --> 00:30:57,980
They're not going to like that as much, but you show them a clean like ETL where they

442
00:30:57,980 --> 00:31:03,060
can drag and drop and run their SQL queries and understand are there pipelines running

443
00:31:03,060 --> 00:31:07,180
in a stable fashion like that resonates with them say it's yeah.

444
00:31:07,180 --> 00:31:11,140
Different personas, different tools, they need to work together and figure out what

445
00:31:11,140 --> 00:31:14,500
process is going to work for their business needs.

446
00:31:14,500 --> 00:31:18,180
Because I've kind of looked at primarily this machine learning engineer role that has

447
00:31:18,180 --> 00:31:22,180
been emerging over the past few years and now we're talking about the DevOps engineers

448
00:31:22,180 --> 00:31:28,020
like a separate thing that the line is kind of a great movie and blurred line, right?

449
00:31:28,020 --> 00:31:32,060
What we've seen in terms of, we have customers to ask us, well, how do we hire these ML

450
00:31:32,060 --> 00:31:33,060
engineers?

451
00:31:33,060 --> 00:31:37,220
It's like, basically, you need a person who understands DevOps, but also can talk to

452
00:31:37,220 --> 00:31:41,540
your data scientists or can, can figure out the work they're doing help them get their

453
00:31:41,540 --> 00:31:47,060
work into a reproducible pipeline on the training side and help with deploying the model and

454
00:31:47,060 --> 00:31:52,100
integrating it into the rest of your application lifecycle management tools.

455
00:31:52,100 --> 00:31:58,620
So yeah, your ML engineer needs to be a DevOps person with some understanding of ML.

456
00:31:58,620 --> 00:32:05,740
And is a DevOps person necessarily a software engineer that is coding a model based on

457
00:32:05,740 --> 00:32:10,460
not necessarily, they just need to be really good at operational excellence.

458
00:32:10,460 --> 00:32:15,980
So do they understand how to write things declaratively, how to set up process control

459
00:32:15,980 --> 00:32:21,420
flows so that things work nicely and like, you don't need to understand the ML, the data

460
00:32:21,420 --> 00:32:24,700
scientists is doing, you need to understand the process they're going through to produce

461
00:32:24,700 --> 00:32:25,700
that model.

462
00:32:25,700 --> 00:32:31,380
So they have a bunch of code and a Jupyter Notebook, like help them factor right into modules

463
00:32:31,380 --> 00:32:35,700
that you can stitch together, but you don't need to understand like, you know, the machine

464
00:32:35,700 --> 00:32:40,940
learning framework that they're using specifically in that context.

465
00:32:40,940 --> 00:32:45,380
You've mentioned Jupyter Notebooks a few times, you know, one of the things that I see

466
00:32:45,380 --> 00:32:49,900
that folks are trying to figure out is like, should we do ML in Notebooks or should we

467
00:32:49,900 --> 00:32:58,740
do ML in IDEs, Microsoft, you know, has a huge investment in IDEs, but you've also been

468
00:32:58,740 --> 00:33:03,740
like in Visual Studio Code, making it more kind of interactive, integrated kind of real

469
00:33:03,740 --> 00:33:08,780
time to incorporate some of the Notebook Eskys, so interaction.

470
00:33:08,780 --> 00:33:11,940
We wanted to be fluid to go from one of the other.

471
00:33:11,940 --> 00:33:18,540
We've seen the value and the interactive canvases for doing, you know, rapid fire experimentation.

472
00:33:18,540 --> 00:33:24,540
We also talked to large companies like Netflix to learn how they use Notebooks in automation

473
00:33:24,540 --> 00:33:28,260
at scale and some of the ML project, for example, exactly.

474
00:33:28,260 --> 00:33:31,180
So we've actually integrated paper mill into our platform as well.

475
00:33:31,180 --> 00:33:36,660
So you can, if you're designing your training pipeline, you can stitch together a mix of scripts

476
00:33:36,660 --> 00:33:42,820
and Notebooks and, you know, data processing steps together, and we try to be as fluid

477
00:33:42,820 --> 00:33:43,820
as we can.

478
00:33:43,820 --> 00:33:48,580
And we're working with the developer division as well to figure out how to more cleanly

479
00:33:48,580 --> 00:33:51,580
integrate Notebooks into our ID experiences.

480
00:33:51,580 --> 00:33:55,980
And you saw some of that in the BS code side and there's more stuff coming to help with

481
00:33:55,980 --> 00:33:56,980
that.

482
00:33:56,980 --> 00:34:02,940
We've talked a little bit about kind of this automated retraining aspect of managing

483
00:34:02,940 --> 00:34:03,940
model life cycles.

484
00:34:03,940 --> 00:34:08,060
Are there other aspects of managing model life cycles that you find important for folks

485
00:34:08,060 --> 00:34:09,220
to think about?

486
00:34:09,220 --> 00:34:14,820
So yeah, knowing when to retrain the model is one thing, knowing when to deprecate the

487
00:34:14,820 --> 00:34:20,460
model is nothing to say that the data the model is trained with is stale or can't be

488
00:34:20,460 --> 00:34:21,460
used anymore.

489
00:34:21,460 --> 00:34:27,460
We've got removed for GDPR reasons, this is why having the whole lineage graph is so important

490
00:34:27,460 --> 00:34:32,060
to be able to figure out exactly what data was used to train the model.

491
00:34:32,060 --> 00:34:37,900
Other things around model life cycle management, yeah, I really know who's using it, know where

492
00:34:37,900 --> 00:34:43,500
the model's running, know if the model's adding business value, know if the data coming into

493
00:34:43,500 --> 00:34:48,540
the model has changed a lot since you trained it, know if the model is dealing with some

494
00:34:48,540 --> 00:34:53,460
type of seasonal data and needs to be retrained on a seasonal basis there.

495
00:34:53,460 --> 00:34:58,100
And then also know the resource requirements for your model.

496
00:34:58,100 --> 00:35:03,580
So another big thing we see a trip a lot of our customers up is they train the model

497
00:35:03,580 --> 00:35:08,980
on these big beefy VMs with massive GPUs and go to deploy and it's like, hey, my model

498
00:35:08,980 --> 00:35:09,980
is crashing.

499
00:35:09,980 --> 00:35:10,980
What do I do?

500
00:35:10,980 --> 00:35:11,980
Right.

501
00:35:11,980 --> 00:35:13,900
And so we've tried to build tooling in to help with that as well.

502
00:35:13,900 --> 00:35:20,500
So profiling your model, running sample queries into it, different sizes of sample queries

503
00:35:20,500 --> 00:35:24,660
to not always the same thing and making sure you know, does your model have enough CPU

504
00:35:24,660 --> 00:35:30,940
and memory and the right size GPU to perform effectively.

505
00:35:30,940 --> 00:35:35,940
And we're also doing some work on the on X framework to help with taking those models

506
00:35:35,940 --> 00:35:41,660
and quantizing them or optimizing them for a specific business use case on the hardware

507
00:35:41,660 --> 00:35:47,580
side, which is really slowly coming in, especially we have customers in the manufacturer, manufacturing

508
00:35:47,580 --> 00:35:52,060
sector who want to run models quickly on the edge on small hardware.

509
00:35:52,060 --> 00:35:55,540
So it's like, how do you, how do you manage that transition from this model I train on

510
00:35:55,540 --> 00:35:59,420
this beefy machine to this model running on this tiny device?

511
00:35:59,420 --> 00:36:08,900
You finding that that most customers are deploying models or even thinking about an individual,

512
00:36:08,900 --> 00:36:12,220
I've got this, you know, this model that I've created and I'm going to think about the

513
00:36:12,220 --> 00:36:17,500
way I deploy this model versus, I've got a model I've built it to a standard, it's

514
00:36:17,500 --> 00:36:21,460
just like any other model that I'm going to just going to throw it into my model deployment

515
00:36:21,460 --> 00:36:22,460
thing.

516
00:36:22,460 --> 00:36:24,420
Like, are they there yet?

517
00:36:24,420 --> 00:36:26,580
Some of them are there.

518
00:36:26,580 --> 00:36:30,980
The ones that have been doing this for a while longer and develop like their template

519
00:36:30,980 --> 00:36:32,500
for their model deployment flow.

520
00:36:32,500 --> 00:36:33,500
Right.

521
00:36:33,500 --> 00:36:37,260
We try to provide as much tooling as we can, you know, in the platform and in the registry

522
00:36:37,260 --> 00:36:40,540
for you to track all the relevant things about it.

523
00:36:40,540 --> 00:36:44,540
But really just getting, getting the model deployed into your existing apico system, making

524
00:36:44,540 --> 00:36:49,980
sure that you have the ability to do controlled rollout and A-B testing because you don't

525
00:36:49,980 --> 00:36:52,780
want to just always pave over the previous model.

526
00:36:52,780 --> 00:36:56,260
So that's the most advanced customers are just getting to that point now where they're

527
00:36:56,260 --> 00:37:01,460
ready to start doing A-B testing and looking for our help to go and do that.

528
00:37:01,460 --> 00:37:02,460
Yeah.

529
00:37:02,460 --> 00:37:07,860
So, A-B, along the lines of testing, we've talked about this a little bit.

530
00:37:07,860 --> 00:37:15,060
There's both kind of the, you know, online testing of your models, freshness, but then

531
00:37:15,060 --> 00:37:23,020
also all kinds of deployment scenarios that have been developed in the context of DevOps,

532
00:37:23,020 --> 00:37:27,300
like Canary, then Red, Green, Blue kind of stuff, like all the colors.

533
00:37:27,300 --> 00:37:28,300
Yeah.

534
00:37:28,300 --> 00:37:29,300
Yeah, all the colors, right?

535
00:37:29,300 --> 00:37:32,900
Do you see all of that stuff out in the wild?

536
00:37:32,900 --> 00:37:34,580
Yes.

537
00:37:34,580 --> 00:37:39,540
The main difference we've seen with models compared to normal software being rolled out is oftentimes

538
00:37:39,540 --> 00:37:45,420
they'll develop a model and test it offline and batch for a while before using it.

539
00:37:45,420 --> 00:37:49,660
So they won't need to necessarily deploy it to receive real traffic right away.

540
00:37:49,660 --> 00:37:54,940
They'll let the model, you know, get the new model, they'll wait a week, run the model

541
00:37:54,940 --> 00:37:59,420
and batch against the past weeks worth of data, and then compare how different it is to it.

542
00:37:59,420 --> 00:38:04,300
So it's just the fact that you can test the model offline as opposed to having to do everything

543
00:38:04,300 --> 00:38:05,300
in an online fashion.

544
00:38:05,300 --> 00:38:06,300
Yeah.

545
00:38:06,300 --> 00:38:07,300
Yeah.

546
00:38:07,300 --> 00:38:10,460
That's probably the biggest Delta, but otherwise we see all the same patterns as with normal

547
00:38:10,460 --> 00:38:11,460
software.

548
00:38:11,460 --> 00:38:12,460
Right.

549
00:38:12,460 --> 00:38:13,460
Right.

550
00:38:13,460 --> 00:38:14,460
Because you're testing two things, right?

551
00:38:14,460 --> 00:38:17,460
You're testing the model's statistical ability to predict something, but then it's also

552
00:38:17,460 --> 00:38:18,460
software.

553
00:38:18,460 --> 00:38:19,460
Right.

554
00:38:19,460 --> 00:38:22,100
And you don't necessarily want to put a software, work in pieces of software out there.

555
00:38:22,100 --> 00:38:23,100
Correct.

556
00:38:23,100 --> 00:38:29,860
So it's a software with uncertain behavior, or more uncertain behavior than any normal

557
00:38:29,860 --> 00:38:31,940
software application you threw out there, yeah.

558
00:38:31,940 --> 00:38:32,940
Yeah.

559
00:38:32,940 --> 00:38:36,100
What can we look forward to in this space from your perspective?

560
00:38:36,100 --> 00:38:40,580
So as far as things to look forward to, there's lots of investments coming in improving

561
00:38:40,580 --> 00:38:46,620
our story around enterprise readiness, making it easier for customers to do, and secure

562
00:38:46,620 --> 00:38:51,660
data science and ML workloads, work to help improve collaboration and sharing across

563
00:38:51,660 --> 00:38:57,860
the enterprise, how do I figure out which other teams have been doing modeling work similar

564
00:38:57,860 --> 00:38:58,860
to mine?

565
00:38:58,860 --> 00:38:59,860
How do I take advantage of that?

566
00:38:59,860 --> 00:39:05,860
So accelerating collaboration, velocity, more work on the enterprise readiness front,

567
00:39:05,860 --> 00:39:09,820
and then a tighter-knit integration with the rest of the big data platform stuff.

568
00:39:09,820 --> 00:39:16,140
So integration with data lake, data catalog, data factory, DevOps get up, and it's all

569
00:39:16,140 --> 00:39:20,220
about helping customers get to production ML faster.

570
00:39:20,220 --> 00:39:23,020
Well Jordan, thanks so much for chatting with me.

571
00:39:23,020 --> 00:39:24,020
Thanks for having me.

572
00:39:24,020 --> 00:39:25,020
Yeah.

573
00:39:25,020 --> 00:39:26,020
Appreciate it.

574
00:39:26,020 --> 00:39:31,780
All right everyone, that's our show for today to learn more about this episode, visit

575
00:39:31,780 --> 00:39:34,100
Twomlai.com.

576
00:39:34,100 --> 00:40:02,220
As always, thanks so much for listening and catch you next time.

