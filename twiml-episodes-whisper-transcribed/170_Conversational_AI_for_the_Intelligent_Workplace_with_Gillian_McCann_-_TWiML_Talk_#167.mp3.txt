Hey everybody, Sam here. We've got some great news to share, and also a favorite
to ask. We're in the running for this year's People's Choice Podcast Awards, in both the
People's Choice and Technology Categories, and we would really appreciate your
support. To nominate us, you'll just head over to Twomlai.com slash nominate, where we've
linked to and embedded the nomination form from the award site. There, you'll need to input
your information and create a listener nomination account. Once you get to the ballot, just find
and select this week in machine learning and AI on the nomination list for both the Adam
Curry People's Choice Award and the this week in tech technology category. As you know,
we really, really appreciate each listener and would love to share in this accomplishment
with you. Remember, that url is twomlai.com slash nominate. Feel free to hit pause and take
a moment to nominate us now.
Hello and welcome to another episode of Twomletalk, the podcast why interview interesting people,
doing interesting things in machine learning and artificial intelligence. I'm your host
Sam Charington.
For those interested in our fast.ai study group, but who haven't been able to make it to our
live sessions, make sure you check out our recap videos via either the meetup page or our
YouTube channel at youtube.com slash twomlai.
In this episode, I'm joined by Jillian McCann, head of cloud engineering and AI at Workgrid
software, which offers an intelligent workplace assistant that integrates with a variety
of business tools and systems. In our discussion, which focuses on work
grid use of cloud based AI services, Jillian details some of the underlying systems that
make work grid tick, including a breakdown of its conversational interface. We also take
a look at their engineering pipeline and how they build high quality systems that depend
upon external APIs. Finally, Jillian shares her view on some of the factors that contribute
to misunderstandings and impatience on the part of users of AI based products. And now on
to the show, all right, everyone, I am on the line with Jillian McCann. Jillian is head
of cloud engineering and AI at Workgrid software. Jillian, welcome to this weekend machine learning
in AI.
Thank you. Thanks for having me.
It's great to have you on the show. Why don't we get started out by having you tell us
a little bit about your background and how you got involved in AI?
Yes, so I have been working. I'm an engineer at Harvard, really. I've been working very
much in the public cloud space last couple of years. But recently, I've started to look
at how we can use conversational AI. Specifically, I write chatbots and voice within the employee
space. So I have Workgrid software is a wholly owned company of Liberty Mutual Insurance.
And what we've done there is a lot of the great products that we've built internally
for employees that have used different aspects of conversational AI. We've decided that
it solved a lot of problems for us within the enterprise within a large global company
that there is potential there to create a product that can be used across companies worldwide.
So last year, we created that company that's separate and today, and that's now my focus.
It's cloud engineering and AI. And it's really how you can apply artificial intelligence
within employee focused products.
Can you give some examples of the types of employee focused products that you're working
on there?
Yes, so our initial product is called the Workgrid Assistant. And the aim of that is really
like building an intelligent workplace assistant. You know, not everybody is fortunate enough
to have PA, you know, a person who is able to, you know, take notes for them or create
them to do lists. So we're looking for a way to create, you know, a product that can
really help employees, you know, with productivity gains and engagement and job satisfaction.
And some of the care is that the product we have, you know, focuses on its, you know, approvals
and workflows and really automation and the aggregation of a lot of internal systems
in that one place with a really, you know, modern and intelligent intuitive interface
of which includes, you know, a chatbot. So conversation, conversational interface is
part of this as well, which is really where our initial focus of AI is.
So I mean, some of the use cases that we talk about. So as I say, this is Stan from, you
know, work with them within Liberty Mutual is that we really focus on some of our site's
central employee functions like HR, IT help desk, just general product evidence and how
we can, you know, automate and provide also like a conversation interface to many systems
so that, you know, employees can find the information they need when they need it, you
know, on the questions that are asked across offices day and day out, can be very simply
automated and give them, you know, again, that engaging experience.
And is work grid primarily building software for the use of Liberty Mutual or are you,
you know, is it more like a spin out where you're commercializing this for the broader
marketplace?
It's the commercialization, really, of a product that we built internally for ourselves.
Okay.
It's actually getting back several years ago, you know, we went through a transformation,
I think around 2014. When we looked at the marketplace, we didn't see anything there
that really would, you know, benefit us in the way that we wanted, we wanted to bring
all our underlying enterprise systems in tools that, you know, we used day and day out.
And I could give it and give a simple place for people to go and do their daily work,
you know, so instead of going into many different applications, where you've literally just
gone in success screen to click a few buttons, it just brings that one place that you can
actually achieve your daily work.
And we evolved that, you know, when last year into building into part of it as a chatbot,
which is, you know, it's a conversation like, and it's using like natural language processing
and conversation analytics to really, you know, try to find what our employees and pin
points are and also to help automate some of that.
And what's been the adoption like of the chatbot interface in particular, there's, there's
an article recently, it was in Ink magazine and it was an article with a guy who was like
founder of a startup and the headline of the article was something like chat box killed
my company or killed my startup.
And the idea was that like he built his startup around this, you know, what he now sees
as a fad and he found that people really didn't want to use chatbots.
And so I've been asking folks ever since seeing that about conversational interfaces like,
where do they think, you know, where do you think we are in terms of adoption or and
is this a way that people want to engage with software?
What have you seen?
So I would say chatbots to me are, you know, the future.
When I say that, what do I mean by a chatbot?
I think everybody has different perceptions of what a chatbot is, you know, is it a simple
bot you order flour so I'm on Facebook Messenger or is it Alexa?
It's literally, you know, the definition is like your ability to talk to and machine
to natural language.
So we do have, you know, on many levels of that, if you know what I mean.
So I say it's very much voice is the future, future interface.
So we talk about chatbots, that's what I have in my head, but it does make sense in a lot
of different contexts.
But it also, you know, it doesn't mean you have to have one sometimes as a visual interface
works better.
So it's not like everybody has to rush out and build a chatbot, but I think everybody
should consider when the conversation interface actually works better, you know, very much,
you know, like the one we have you sitting at your desk, you want to ask a simple question,
maybe about an HR policy or you want to get some information about how do we have like
reset passwords from the IT health desk, we do valor, how about two seconds chat with
a chatbot or a ten minute phone call, so people are, it's the use case that really drives
the use of the chatbot.
So we have, you know, it's still in, I would say it's like an evolving technology, but
it's not really the end of your AI piece that's evolving, it's the use case, understanding
when that context or when that conversation interface makes more sense, and also the actual
piece of the conversation design, what does the chatbot say?
If you don't design it well, the conversation doesn't flow well, and then chatbots do fail,
you know, it flows a lot more than just the AI piece it is.
I think good conversation design, good understanding of your users and understanding the benefits
it this will bring, and then also as I say, you know, the evolution of chatbots into
a voice, I definitely think, I mean, you can just, I don't know if you're seeing the
figures of, you know, hi, many Alexa's, we're bought a Chris's, you know, so there's
like, I think there's staff like one in sex, American high schools, have a smart speaker,
you know, so you think about that, those are all chatbots.
In the definition of, you know, using natural language to talk, you know, interface with
computers.
So, don't have that answer to your question.
Yeah, no, it does, I appreciate that broader definition of chatbot, and I think, you
know, when we vision kind of the future of human computer interaction, I don't think
that there's much of a question for most people that it's a natural language-based conversation
that, you know, extends beyond kind of the, you know, like the example you gave, the
Facebook messenger bot, it's not so much that, you know, that rectangular interface that
defines chatbot, but this means of interacting with the computer via natural language.
You mentioned that the first step is understanding when to apply this type of approach to a given
use case.
Do you have any guidelines that tell you when, or is it kind of, I'll know it when I see
it?
I think it's a bit of both, like really, some use cases just don't make sense.
You know, I think about things that, you know, that you're used to seeing, like, lists
of things, like, something that's very visual, if that makes sense, you know, like, we
talk about, like, if you were to ask about all the, for example, like, the Amazon days
of the day, you know, you know, the way you see everything's like cards, there's a lot
of visual input that you can take, you can ask that in a chatbot, what, how could you
get that sort of information in the same way?
You know, it just be a big list of words that it doesn't really make sense.
But, you know, we have find, like, with the employee space, it has been, like, really
listening to what employees are asked for, or some of the things are just, like, very
simple, one-off sort of actions, or, like, require, so you do find a lot of, of the use
cases within the work, basically, to be centered around a more intelligent search, you
know, actually, we know we'll have more direction into more directed questions, really, than
just, you know, type it in a search bar.
So, you know, essentially, we just have worked through them and decided, like, what makes
sense?
What doesn't make sense?
I do think there is a, especially in some of the arguments on, like, you do realize
that the user's expectations are sometimes, like, very, there will be very high, they're
very high, when you start talking about, um, any form of AI, and I don't know if you've
come across this, it's like, it's either very high or low, um, this is not a success.
So, like, from a chat board perspective, um, like, I think we've had an education ourselves,
you know, we have, like, like, the development team, and, and the guys really, like, how do
you build, like, intense and utterances and fulfilments and train the model?
What technologies do you use, do you build your own, do you use somebody else's, um, you
know, so there's a lot of that, but, like, when we're talking with the users, it's, it's
like an expectation that it's all going to magically, like, it's magic.
If you type something in, um, you know, like, what's the weather, that the chat bottle
know how to answer it, and there's a sort of explanation, well, guys, it's still cold,
we still have to write that if you want a chat box to actually know the weather, we have
to program, we have to train the model, and we have to ultimately call an API that tells
us what the weather is, but it's, it's that perception of, like, if I repeatedly ask
it what the weather is, it'll learn that we've heard that, like, it'll learn what I say,
says, well, they'll not learn until we update it.
It's not a magical box, it just learns things.
So I think there's, like, the expectations, if that makes sense, it's being very interesting
to see.
And then on the other side is, you know, people just think that they're not very good,
chat box aren't very good, the L&U is not very good, but we're sort of in a place
where it's never been any better, and it's only going to get better.
So, you know, I think what we find also is the very importance of, like, what we call
conversational analytics in that you're understanding what it's being, and you're understanding
what the box saying, or what's being said to the box, box, you know, you may not be able
to answer, but if everybody's asking for what's the weather, then potentially that's the
feature you add.
And so, you know, just that, analytics of what's being said, missed audiences, missed intents,
how well are you fulfilling the actual user intent, how well are you performing, and that's
really a driven user case as to just refine what we have, because it is a great way if
you, you know, give the box to like a pilot group, you know, you can actually through what
they're asking and how they're chatting to it, actually see maybe what they wanted to
do.
And it does surprise you in some ways, and also the realisation that, you know, different
cultures and different language of different ways of even saying hello.
So, we find all that, like, if we've ruled it out, it's still in the pilot, but in the
Liberty Mutual, you know, we see that because it's a global component, but it's been very
interesting.
And I do think chatbots, if they're designed well, they perform real actions are very powerful.
Do you feel that the, this magic that some users expect, where the chatbot is going to
learn how to understand what the user is asking, do you feel that that is just kind of organic
or does it, has it come about because we throw around terms like machine learning that people
don't really understand?
I would, yes.
That's right.
In short, yes.
And I think AI, artificial intelligence, machine learning, deep learning, you know, there's
a lot of words about need about it.
And I do think there is, you know, if we say machine learning that somehow it's going
to learn how to do these things, but like when you, you know, and I'm not an expert,
you know, but I've read some stuff, you know, the differences between supervised and unsupervised
machine learning and deep learning is different for is that when she start getting into it,
it is much more evolved, you know, the data that you have, the training data, it can only,
it can only be as good as the data you have and the historical data for supervised learning.
So I don't think everybody really, you know, they don't need to know that, but I think,
um, like once you start having an interest in it, then you do start reading these things,
you do get a better awareness of it, but I doubt I think, yeah, the terms and just everything
has got some element of AI apparently, I don't know, you know, there's a lot of products
say that.
And we need to dive into, I think a lot of it is really automation and workflow and
the real great things, but I think, you know, there's a bit of a buzz, as you know, um, and
it's applied when potentially it's not, it's not our first intelligence, something else.
So, you know, I say some great, the script product center and a lot of it, you know, it seems
more like process automation, which is very valuable to.
Yeah, basically every enterprise software product is some automation, some more slow automation.
Uh, underneath, right?
All right.
So, you've, you've got some use case, you think it could be a good fit for some type of
chatbot, then the, the next question is, how do you get there, uh, and you were speaking
at the AWS conference, so you went the cloud direction, there's also, um, you know, building
your own, how did you evaluate the, the different options and come to the conclusion that you came
to?
Yeah.
So, that's a good question.
Um, so I did look at a lot of different options, um, I think really, you know, the first
thing was, and it's written general about chatbot, I think you have to think about where
are the channels or the clients that you want to put that chatbot in, you know, who's
the customer?
Is it a, like a customer facing, or is it an internal one that, like more like the employee
stuff that I have been working on, you know, just an understanding of generally what
the chatbot, um, is going to do, but also those channels, because a lot of the, like the
channels have their built-in frameworks that really make it easier to build.
Um, I think, you know, if you're thinking about building chatbots, you have to set your
development capabilities as well, like I was like, um, I'm working from a, like an architect,
engineer, you know, I know how to write code, solutions are not going to put me off, but
if you're just in general interested by chatbots, there are a lot of great frameworks out
there that can get you up and running.
Um, I obviously then, um, some of the research we did is into, well, understands what type
of chatbot you're trying to build, so you know, and there is a framework, you can sort
of assess it against an open-demand chatbot, versus a close-demand chatbot, um, close-demand
is, um, a lot easier in that it's like a very set purpose, as I mentioned, order floors
or, um, order of pizza order attacks, so those types of chatbots are a lot simpler, because
they're only expected to do one thing, and if you started to ask a pizza bot about the
politics, you're not really expecting that to do what I mean, just expectations, um, but
what we were working in was more open-demand in that there's a wide range of questions
that an employee in a large company can ask, even within HR or IT help desk, um, use cases.
So, you know, we looked, we need to find something then that would help us build, um, a more
open-demand chatbot, um, and then also, you know, we're not really into the differences,
so the difference between, like, retrieval and generative, you know, most chatbots in
production today are retrieval-based, um, like, a generative one is, you know, we're really
arguing into, you know, um, absolute true artificial intelligence in that, you know, something
like, like, it didn't go very well, Microsoft, hey, but they've built, you know, the chatbot
itself is generating a response, first is retrieval-based, um, and retrieval-based is obviously
in a professional workplace, um, you know, it's, it's the thing that we initially, you
know, we need to deliver that, so you can't really start looking at how can I build chatbots
going to, you know, come up with the answers itself, you know, that's out the rounds of the,
uh, AI capabilities of where we are today, from what company perspective I would say, um,
but then I looked at, um, so, you know, we wanted a, um, a natural language chatbot rather
than, like, a command, sort of, you know, do this, do that, um, so we want the natural language,
looked at, um, the main cloud providers, I suppose, we say Google Microsoft Amazon, um, so,
you know, I, um, an AWS Cloud Architects, I asked lightly by this towards one, but, um, we did,
I did look at, um, Microsoft's cognitive services, a really great Google's, um, you know,
there's some great stuff there, but with Amazon, an AWS was what was chosen, because it's more than,
it takes more than an LU to build a chatbot for thousands of people. It takes, um, many different
services and capabilities, um, you know, AWS was our provider of choice at that time,
so it really made sense just to, you know, work with them and pack a technology that sort of
fitted that AWS ecosystem, and luckily enough, um, it's not last year of the year before
they announced Amazon Lex, um, as a new service, uh, in preview, um, immediately that was the
ones at the end we wanted to work with, um, and we partnered really with them to develop,
you know, they were in preview to develop the capabilities that we needed, um,
I must say it worked really well because it's more than AWS Lex, the builds it, it's,
it's a wide range of the services, um, it gives us that real scale ability, um,
on analytics and good engineering practices.
What have been some of the things that you've needed to build around it to arrive at a complete
solution? Yeah, so interesting. I thought that was really what the, um, the session, uh,
re-embed was, but like the last year was, it's more than Alexa. Um, so, so, I mean, what we had to
provide was really, um, I think the analytics was very important, you know, been able to, um,
analyze and store, um, elements of conversation so we could get insights, um, we use like APIs,
um, and storage, um, so that we actually can, you know, um, review conversations. Um, so,
we essentially built an architecture serverless architecture around Alexa. Um, so Alexa provided
the NLU, um, but then also like if we wanted to, it's initially we focused on, um,
text chatbot, but obviously then voice, as they mentioned, you know, obviously,
as a, as a, the next step. So, you know, using AWS Poly, um, to really, you know, have both
experiment with both, and then as we move into, you know, really looking at the devices in the
home. I said beside her is the Google Assistant, um, which I have started looking at too. So,
there's really powerful technology out there, um, and it's just, you know, as I said,
taking the things that worked for you, um, and ultimately AWS, um, ecosystem was very
much in place for us. So, so the, the lacks was the extra piece and once they, they brought that
out, that seemed to definitely make sense. Um, and then, you know, as I said, we're talking
at re-invent. Some of the things to talk about re-invent wasn't really, um, you know, the AI aspect
offer, it was, how do you build chatbot scale? How do you test the chatbot? How do you have
CI, CD pipelines for chatbots? Um, so it was really about good engineering practices applied to,
um, like new technology like this. And also very importantly, I'm asking the before, is the
conversation design, you know, what does it say? How does it interact with you? You know,
you want to give your users a very good experience. Um, and what you have really have to do is just
between the team is, you know, talk it out, script it out, um, potentially the bot. Um, so it's
actually quite fun. Um, it's a fun way to develop. But, um, you can definitely see that it's a scale
that, you know, we've had to build within a small team. Um, but I definitely have to say voice, um,
the centerpiece, um, you know, it's a capability that we need to grow, um, just in general, I'd say within
the engineering community. How big is your team now? So work red software, nice. So the original
living neutral team was four or five work red software. No, we have, um, see boy, year or nine,
so slightly bigger. But as I say, the conversation layout is just one piece of that. It's a larger
assistant. Um, so we're all working hard, um, to really bring this because I really, I mean, we do
honestly believe that, you know, you should value your employees, you should value their time,
um, and you'd really try and help them, um, you know, when they need to have, when they've
questions, they need answers, you can answer them quickly. But also, you know, providing that one
place to go so they can do their, you know, to do this, notifications, provokes all those things
that are a day-to-day, um, hassle sometimes, but you still have to do them. Um, you mentioned
analytics on a number of occasions. Can you talk a little bit about the analytics that you use,
and, um, what frameworks or tooling you've built to, um, support the way you want to analyze the
conversations? Yeah, so, um, so I was at the moment, I've classified them as, um, quite well,
standard analytics with the plan for more improved insights. So now let's have at the moment,
so we're using, um, things like Amazon's, Elasticsearch, and Athena, um, to give us,
you know, real time, so it's in real time that we can actually see, so the key things are in
chat, what is the, um, the conversation flow, um, they missed other answers, and missed intense,
where that terminology is, if you're not aware, um, like if you say something like book me a meeting
with Gillian McCann, and it goes, sorry, I didn't understand, and it repeatedly says that is that,
that we haven't modeled that conversation within the chat box. So we're going to say, you know,
what people are asking that doesn't match, or what we actually find more important was,
miss, we call them mismatch intense, it did match something, but it matched the wrong thing. So,
you're trying to book a meeting, and it's given you the cafe menu, so, yeah, just, so this,
so it's, it's the, to me, that's the performance of lags, the performance of the analytics itself,
how well have we created, um, the model, what sample utterances have we provided, is something
we can train, are people say things in a different way than you actually expected? So then we,
we see those, we're able to say that, and we're able to, you know, bring that back into the
lags model and immediately retrain it, um, but then we have analytics on more, I'd say, for me,
you search, you search context, sort of like, um, very high level, you know, job roles or location,
it's your specific questions are coming from a specific office, you know, that maybe you'll help
you get insights into, well, what is the problem there, I'm not here, why are they asking that? Um, so,
that's the, um, that's where we're at the moment, I also say to me, it was quite standard stuff,
but with the plan to be more involved, um, where really it gets into what I call the key thing in
any conversation is context, right? So the context, and we have different levels of context,
um, at the moment it would be things like the user, like the office, location, um, job role,
um, how does that context impact the answer or the conversation? So really, when I talk about
a personal assistant, you want it personalized to you, so it understands, you know, you,
so context from a user perspective, um, then context also could include the device that you're
on, and the capabilities that it has, um, and then getting into what I like to call, it's like
the short term and the long term memory, the conversational context, um, where we can understand,
you know, what you've previously said is in this conversation, there's nothing worse than somebody
you say, something to somebody in that two minutes later, they can completely say the same thing
and ask the same question again, they haven't remembered what you said, so it's really, you know,
how do we tree in the chat box to not be annoying like that? So look, you know, in that example,
can I book a meeting with you? I'm a can, yes, what time would you like? And then you say something,
and then jump back to, is she free at 3 p.m. tomorrow? The she has to refer to me. Do you know what
of me? We need to build that in, and that's the context, and then longer term is where I would like
to get to, is that we can start using predictive analytics, you know, we understand your patterns,
we understand that you ask for the coffee menu for Friday, um, why don't we send you that
in advance? Why don't we talk to you? Um, so it's getting, that's why being sort of, it's an
evolving path, and that's where, you know, we start looking at other areas of AI, you know,
potentially machine learning, if we've got like a history of conversations, can you predict
what the next conversation's going to be? Would that be useful even? What would that do? So I think
the base of all this is getting, um, that's what putting place is the foundations of what I say,
good quality analytics, and then we can evolve up, look for insights, and hopefully, you know,
again, employees that better experience, as it learns, as it gets better. You mentioned also that
one of the topics you covered was about the deployment, scale, CICD pipelines, all that kind of stuff.
Can you give us an overview of the kind of things that you have done there to facilitate
building out these kinds of applications on the engineering side? Yeah, um, so, um, like for any
standard, um, application, you know, we have certain practices, um, we would run unit tests,
integration tests, we would run those locally, you check your code in, the bell pipeline,
picks those up, runs more tests, um, and then pushes through the environment. So that's the general,
you know, process. Um, so what we had to do was come up with like a test framework essentially
for less itself, um, you know, a unit testing conversation. So that's what we built into the
pipeline was, um, the ability to, um, you know, we had scripted conversations that we would run
on every single build to make sure that a change, you know, we may have added new utterances or new
intents, hadn't impacted the model and the answers for all the other intents, and we did find that,
you know, a real, um, a real benefit because, you know, we were making changes, we were,
what made one thing work, made a different thing break. So we're finding it and constantly having
that, um, backup of, we're not breaking it. And that's a standard process, but it was in a new
world and a new, sorry, technology. So we had to think about how to apply those, but I would say
good practices to this new, new way of creating software. Um, so we did that. The other thing that
was very important was the building of the bought itself, um, you know, for anybody who has worked
in a large enterprise company, um, there is certain rules and standards and things have to be
put in place. And one of them is very specific, a runner usage of the public load in that we have
a sandbox environment for people to prototype POC and, um, learn about how to use new services,
but beyond that, everything has to be pushed through those accounts, um, through code. So we had
to come up with a whole build pipeline for Amazon Lacks and the bought itself, um, which we created,
um, when we were waiting on, literally, um, 19th of April, because I was waiting on the date,
when it went GA and the build SDKs were released. Um, I think that's, I think it surprised
Amazon because we've heard from them, um, after fact, that, you know, they just thought people
would do it in the console. It's like, well, that's great for POC. It's not going to get us into
production. So that's where we worked, um, on those aspects of it, which in the scheme of things
when you hear people talk about chat box, it's not the things we talk about, but those are the
practices that we'll get to into, you know, a, a chat box environment that will, you know,
scale the thousands of employees and then with work rates off, where obviously thousands of
employees across multiple countries, some companies, so you're in their multi-tenant chat box
environment. So it's really taken, you know, best practices, I think is what I would say,
and bring it into a chat box world. Um, and when they were experimenting, I said,
voice, um, how do you test the speech recognition, how do you do those things. So it's all very
interesting, very exciting, um, to be honest. Um, and I think it's probably clear, like the,
the driver or the, not the driver, the enabler for a list, to me, it's, it's public cloud.
Whichever one that you're working in is, you know, they're all comparative and competitive services.
But really, to me, it's the, the place that enablers developers just even get started,
with aspects of AI. Um, you know, if you do have an interest, it definitely is a place to experiment
with. Have you run into any downsides or limitations associated with running in the
public cloud or depending on the public cloud services? Um, I think the, I don't know if it's
done, there's no real dying science because, um, you know, as I talked about, I like to,
I, I personally feel that from an AI perspective, um, that there's few companies that would be able
to compete with some of the capabilities that are being made available in public cloud. Um,
just from the ease of use, um, the, the rollout, like the new service of the release,
the re-infair for example, just so many new stuff. Um, you know, but I think the,
the, the guy inside, it's not dying science, it's just an awareness of where your data is, you know,
um, make sure that you thoroughly think through what data you're trying to store. Um, you know,
it doesn't need to be anonymized. So it's just, you know, I think a general understanding of how
the cloud works needs to be there before you just start turning things on, um, sending data everywhere.
Um, buzzer say, like, I have that. So, you know, every service we use, um, and it is assessed
through a security process that we will have. Um, I don't think not a dying site, but just have
an awareness, um, of all the different services that you potentially use in your, um, product.
Well, Jillian, thanks so much for taking the time to chat with us. So there are any final thoughts
or things that you'd like to add? Um, I think as far as I have said it several times, I just see,
you know, the voice, um, as the future interface, um, for people who are interested, you know,
in the ability, not the, you know, it lacks, um, the developer skills SDK and the Google Assistant,
you know, if anybody's any interest, they literally can start tomorrow and create simple skills
and have a go because it really opens your eyes to the potential, um, of high, you know,
certain aspects of AI I want to change your lives. Awesome. Well, Jillian, thank you. Thank you so
much. It was great chatting with you. Thank you. All right, everyone. That's our show for today.
For more information on Jillian or any of the topics covered in this episode, head on over to
twimmelai.com slash talk slash 167. Don't forget to visit twimmelai.com slash nominate to cast
your vote for us for this year's People's Choice Podcast Awards. As always, thanks so much for
listening and catch you next time.
