WEBVTT

00:00.000 --> 00:13.400
Welcome to the Tumel AI Podcast.

00:13.400 --> 00:18.840
I'm your host Sam Charrington.

00:18.840 --> 00:23.680
Hey, what's up everyone?

00:23.680 --> 00:28.560
I want to send a huge thanks to Emily Bender and everyone who joined us for the viewing

00:28.560 --> 00:32.960
party and AMA, we held with her earlier this week.

00:32.960 --> 00:37.560
These have been a really, really fun way for us to connect with you and to connect you directly

00:37.560 --> 00:39.200
with our guests.

00:39.200 --> 00:45.400
If you miss any of them, you can check them out at twomelai.com slash viewing party.

00:45.400 --> 00:50.680
I'm also really happy to share more details on our next panel, which will focus on advancing

00:50.680 --> 00:54.240
your data science career during the pandemic.

00:54.240 --> 00:59.960
The panel will be held next Tuesday, May 26th at 12 noon Pacific time.

00:59.960 --> 01:03.960
I'll be joined by Hilary Mason, who you know from her time at Cloudera Fast Forward

01:03.960 --> 01:10.360
Labs and this podcast, Caroline Chavier, Data Science Recruiting Maven and co-founder

01:10.360 --> 01:15.760
of Paris Women in Machine Learning and Data Science, Jacqueline Nolis, co-author of Build

01:15.760 --> 01:21.800
a Career in Data Science, which is Hot Off the Presses, and Anna Maria Eshevery of IBM

01:21.800 --> 01:25.120
and the Open Data Science for All Project.

01:25.120 --> 01:29.880
With the help of your questions, this amazing panel and I will explore best practices, tips,

01:29.880 --> 01:35.360
advice and direction for those of you affected by layoffs, new to the job market, or ready

01:35.360 --> 01:37.600
to accelerate your careers.

01:37.600 --> 01:44.000
To register for this panel, visit twomelai.com slash DS Careers.

01:44.000 --> 01:47.040
And now on to the show.

01:47.040 --> 01:52.040
All right, everyone, I'm here with Alpha Lee. Alpha is a group leader in the Department

01:52.040 --> 01:58.680
of Physics at the University of Cambridge, as well as co-founder of the Startup Post-Error.

01:58.680 --> 02:01.840
Alpha, welcome to the Twomelai podcast.

02:01.840 --> 02:03.520
Thank you very much for having me.

02:03.520 --> 02:07.160
It's great to get a chance to meet and speak with you and I'm looking forward to learning

02:07.160 --> 02:13.000
a bit about what you're up to, both from a research perspective as well as with your

02:13.000 --> 02:14.000
start-up.

02:14.000 --> 02:17.160
Why don't we get started by having you share a little bit about your background and how

02:17.160 --> 02:21.960
you came to work at this intersection of chemistry and materials and machine learning?

02:21.960 --> 02:22.960
Sure.

02:22.960 --> 02:29.320
So, I started out as a chemist, and by undergraduate training, I was always fascinated

02:29.320 --> 02:35.360
about making molecules and how can you make very complicated natural products from simple

02:35.360 --> 02:36.360
selling materials.

02:36.360 --> 02:42.280
So, I started off my academic training trying to become a chemist as an undergrad and

02:42.280 --> 02:47.120
then I soon realized that being a lab is interesting and fun, but I also wanted to think

02:47.120 --> 02:53.000
a step back and understand why and how chemical reactions work and understand from a theoretical

02:53.000 --> 02:54.000
perspective.

02:54.000 --> 03:01.680
That's why I did my PhD in mathematics at Oxford, where I looked into the physics of particles

03:01.680 --> 03:03.840
in solution.

03:03.840 --> 03:09.640
And then through my postdoc at Harvard, I thought, well, mathematical theories and physical

03:09.640 --> 03:14.240
theories are very, very powerful, but there seems to be a gap between these theories and

03:14.240 --> 03:18.880
actual materials and chemistry, and that's where the data part comes in.

03:18.880 --> 03:25.600
Can we leave rich experimental data that has been done before to build better models, but

03:25.600 --> 03:28.000
obviously adding the physics in as well?

03:28.000 --> 03:33.640
A nice part, a nice bit is that actually the physics of particles and the physics of data

03:33.640 --> 03:34.880
are actually very much related.

03:34.880 --> 03:38.640
They are brought family of physics or statistical physics.

03:38.640 --> 03:43.040
And that's how I got into some machine learning through this physics chemistry angle.

03:43.040 --> 03:47.720
So since two and a half years ago, I returned back to the UK and started my own research group

03:47.720 --> 03:53.320
in Cambridge, where we now work at the intersection between a chemistry of drug discovery, physics

03:53.320 --> 03:59.640
of materials and machine learning, and so six months ago, I co-founded postera, which

03:59.640 --> 04:05.800
offers medicinal chemistry as a surface powered by machine learning, which retakes the machine

04:05.800 --> 04:10.440
learning approach that we developed once that further and deployed in the wild, so to

04:10.440 --> 04:12.640
speak, in drug discovery projects.

04:12.640 --> 04:13.640
Awesome.

04:13.640 --> 04:14.640
Awesome.

04:14.640 --> 04:16.400
You mentioned the physics of data.

04:16.400 --> 04:17.400
What does that mean to you?

04:17.400 --> 04:21.120
Is that just statistics or is there something more to it than that?

04:21.120 --> 04:27.680
I think it's both about how do we understand noise in the data sets, for example, some

04:27.680 --> 04:35.680
of my research pertains to estimating uncertainty in model predictions, both in terms of how

04:35.680 --> 04:40.680
do we estimate measurement noise and how to measure noise on certain, due to not having

04:40.680 --> 04:44.280
enough data in a certain area of chemical or material space.

04:44.280 --> 04:49.920
And that motivates a line of research on patient deep learning or patient approaches to machine

04:49.920 --> 04:52.120
learning in general.

04:52.120 --> 04:57.040
And that's actually quite important in we link it back to real life experiments, because

04:57.040 --> 05:02.640
experiments are often extremely expensive and you really want to be able to probe regions

05:02.640 --> 05:09.440
of chemical or material space, which are the most fruitful and that you really gain the

05:09.440 --> 05:12.280
most information from doing the least amount of experiments.

05:12.280 --> 05:17.200
So the physics of machine learning and the physical process of doing experiments have become

05:17.200 --> 05:21.160
pretty joined up together when you think about the whole process.

05:21.160 --> 05:26.600
Several of the conversations I've had with folks working in areas like this rely heavily

05:26.600 --> 05:30.600
on simulation for their work, do you as well?

05:30.600 --> 05:34.200
We take simulations obviously as one sort of data, and then there you're often extremely

05:34.200 --> 05:42.160
powerful in the middle of pure ML and pure experiments, which usually experiments are

05:42.160 --> 05:45.880
usually slightly slower and often more costly.

05:45.880 --> 05:52.640
But I think we are now trying to put the physics behind those simulations into the construction

05:52.640 --> 05:58.920
machine learning models or conversely interpret the architecture of the simulations as a machine

05:58.920 --> 06:00.560
learning model by itself.

06:00.560 --> 06:07.720
So for example, a lot of physics-based simulations often contains parameters, often contains equations

06:07.720 --> 06:13.320
that are empirically parameterized and a strand of my research is indeed trying to interpret

06:13.320 --> 06:17.520
these very powerful simulation engines as machine learning models themselves.

06:17.520 --> 06:23.200
And then you can think about, well, can I judiciously tune and decide these parameters based

06:23.200 --> 06:24.200
on data?

06:24.200 --> 06:28.880
Because I think the physics is so much to offer in terms of the frameworks, modeling frameworks

06:28.880 --> 06:35.960
and data is so much to offer in terms of fine tuning, the gap between predictions and observables.

06:35.960 --> 06:37.480
Can you elaborate on that a bit more?

06:37.480 --> 06:42.640
What does that mean to turn a physics simulation into a machine learning model?

06:42.640 --> 06:43.640
Right.

06:43.640 --> 06:51.280
So for example, we recently tried to use machine learning to predict the structure of complex

06:51.280 --> 06:52.280
liquids.

06:52.280 --> 06:59.760
So if you have a bunch of particles moving around in the liquid state, a question in physics

06:59.760 --> 07:05.400
as well, what will be the structure of these particles, what will be the structure of the

07:05.400 --> 07:06.400
liquid?

07:06.400 --> 07:12.040
That's one of these very basic questions in soft condense metal or condense metal physics.

07:12.040 --> 07:18.200
And a branch of physics known as liquid state theory parameterized, discussed a very elegant

07:18.200 --> 07:20.040
way to solve this question.

07:20.040 --> 07:26.080
And there's one equation called the constitutive equation or the closure relation that is unknown.

07:26.080 --> 07:31.760
And although the physical framework is there, that equation is what sort of hinders a lot

07:31.760 --> 07:32.760
of progress.

07:32.760 --> 07:38.760
And we say, hey, why not just take the framework and parameterize the equation using data.

07:38.760 --> 07:41.800
Just do a lot of simulation data and parameterize the equation using ML.

07:41.800 --> 07:44.360
And that's a lot better than throwing away the whole physical framework.

07:44.360 --> 07:47.720
Just use ML to learn everything from scratch because it's so much progress there.

07:47.720 --> 07:52.080
So that's one example where we can take a classical theory, look at the weakest link,

07:52.080 --> 07:57.080
which is usually an empirical equation and say, okay, let's tackle the empirical equation

07:57.080 --> 08:00.680
using ML, but leaving the classical theory intact.

08:00.680 --> 08:01.680
Awesome.

08:01.680 --> 08:10.480
And so ultimately you're trying to apply this to medicinal applications, drug discovery.

08:10.480 --> 08:16.960
But also, I guess I'm curious about the relationship between drug discovery and materials and material

08:16.960 --> 08:18.320
science.

08:18.320 --> 08:25.240
I often think of drugs more from the perspective of their chemical properties and materials

08:25.240 --> 08:31.520
in terms of wanting to create new kind of macro materials.

08:31.520 --> 08:37.080
I'm not sure what the question is there, but I'm trying to get at the techniques the same

08:37.080 --> 08:43.440
across drug and materials discovery, or are they very different?

08:43.440 --> 08:49.640
I think the questions that we are trying to ask, obviously, are somewhat different.

08:49.640 --> 08:55.120
Because as you alluded to from materials, you're usually interested in self-bulk properties

08:55.120 --> 08:58.560
or drugs you're interested in properties at the molecular scale.

08:58.560 --> 09:04.600
But I think in terms of the models that we construct, and in particular the philosophy

09:04.600 --> 09:08.000
that we take and construct the models, that's very similar.

09:08.000 --> 09:15.240
So for example, we use a lot of graph neural networks for chemistry because we can look

09:15.240 --> 09:21.440
at a chemical compound or chemical molecule as a graph, and you can perform operations

09:21.440 --> 09:22.440
on the graph.

09:22.440 --> 09:27.160
And we have extended this to think about Bayesian graph neural networks because uncertainty

09:27.160 --> 09:30.160
is extremely important for drug discovery.

09:30.160 --> 09:37.000
But conversely you can also think about a formula of an inorganic material, let's say a

09:37.000 --> 09:39.960
battery cathol material as a graph as well.

09:39.960 --> 09:44.000
So you can think of a formula of a material as a graph, and recently showed that we can

09:44.000 --> 09:50.600
basically featureize, let's say, a cathol material of battery as a graph, and then using

09:50.600 --> 09:57.240
this material graph to predict materials property and also estimate uncertainty and drive

09:57.240 --> 09:58.240
experiments.

09:58.240 --> 10:01.480
So in those are the methodology, I think there's a lot of synergies and similarity.

10:01.480 --> 10:06.720
And I think in terms of thinking about the whole design cycle in chemistry and drug discovery

10:06.720 --> 10:12.400
and everything about the design, make test cycles or how to design compounds, how to synthesize

10:12.400 --> 10:16.800
compounds in the lab, which usually is the rate determining step actually, and how to design

10:16.800 --> 10:19.160
experiments to test compounds.

10:19.160 --> 10:22.360
And if you think about that framework, then we can see a lot of parallels between why

10:22.360 --> 10:24.920
it's done in chemistry and why it's done in material science.

10:24.920 --> 10:28.640
Obviously, I think the key difference in material science is that experiments actually

10:28.640 --> 10:31.240
even costlier than chemistry.

10:31.240 --> 10:38.800
So in drug discovery, a lot of these measurements can be done relatively, there are protocols

10:38.800 --> 10:43.480
to power chemical assays that can do these experiments to test where the compound is

10:43.480 --> 10:47.440
actually potent against the protein relatively quickly.

10:47.440 --> 10:53.960
From materials trying to make a new superconducting material and test it can be some a PhD project

10:53.960 --> 10:54.960
in of itself.

10:54.960 --> 10:58.600
So I think the throughput is very different, which means that it's a lot more interesting

10:58.600 --> 11:03.800
ML, specifically in the low data limit that we can think about in material science.

11:03.800 --> 11:10.680
You've mentioned a few times estimating uncertainty is being one of the goals or at least a useful

11:10.680 --> 11:13.600
property to have in this type of work.

11:13.600 --> 11:18.720
What are the types of uncertainties that you're trying to estimate and how do they play

11:18.720 --> 11:20.760
out in the chemical and material realms?

11:20.760 --> 11:21.760
Right.

11:21.760 --> 11:29.040
I think the two types of uncertainty is first uncertainty due to having insufficient

11:29.040 --> 11:35.280
data in a particular chemical space or material space, and that's usually known as the epistemic

11:35.280 --> 11:36.280
uncertainty.

11:36.280 --> 11:42.960
The second type of uncertainty is the uncertainty due to the measurements themselves because

11:42.960 --> 11:49.200
a measurement inherently is not noise free and usually the noise can be an inhomogeneous

11:49.200 --> 11:53.360
function of where you are in chemical space and that is usually overlooked in a lot of

11:53.360 --> 11:54.360
approaches.

11:54.360 --> 11:57.580
If you actually do an experiment, you know that some molecules are, some materials are

11:57.580 --> 12:03.000
inherently more difficult to deal with than others.

12:03.000 --> 12:05.600
And that's called, it's about allotoric uncertainty.

12:05.600 --> 12:10.160
And we capture both within the framework of probabilistic patient deep learning and actually

12:10.160 --> 12:16.320
capturing the variation of uncertainty as a function of where you are in chemical space

12:16.320 --> 12:17.600
due to measurements.

12:17.600 --> 12:22.280
I think it's something that is super important because in a lot of material discovery, typically

12:22.280 --> 12:27.580
want to discover material that is robust rather than a material that will sometimes give

12:27.580 --> 12:30.800
you good results, but other times less so.

12:30.800 --> 12:34.200
For these different types of problems, how are you formulating your problem?

12:34.200 --> 12:35.720
What are you modeling?

12:35.720 --> 12:40.000
What are you trying to predict ultimately that you want to incorporate these different measures

12:40.000 --> 12:41.680
of uncertainty into?

12:41.680 --> 12:46.600
So we are to be trying to predict figures, figures of merit, for example, in drug discovery

12:46.600 --> 12:54.320
that would be a balance between how strongly would a molecule bind towards a protein and

12:54.320 --> 12:58.680
also how strongly would the molecule bind to other proteins which you would want to

12:58.680 --> 13:04.520
avoid because that causes toxicity effects and as well as other properties like solubility

13:04.520 --> 13:07.880
or other properties that require to make a molecule drop.

13:07.880 --> 13:13.480
In materials, it would be the property trying to optimize, for example, so the band gap of

13:13.480 --> 13:19.000
materials should try to think about photovoltaics or the strength of material, think about functional

13:19.000 --> 13:24.160
materials like alloys, but nicely about our framework is that by thinking about materials

13:24.160 --> 13:30.560
and molecules in a more abstract way, we're able to create frameworks that are generalizable

13:30.560 --> 13:33.040
across chemical material space.

13:33.040 --> 13:38.840
In the case of materials, the characteristics that you've mentioned, strength, for example,

13:38.840 --> 13:42.560
is often the target you're looking to build stronger materials.

13:42.560 --> 13:47.040
In the case of drug discovery, it's not like the things that you're trying to predict

13:47.040 --> 13:48.360
aren't so much.

13:48.360 --> 13:52.960
I think that the primary focus is efficacy against something.

13:52.960 --> 13:57.920
Is that a prior or something that you know operating and you have a laundry list of things

13:57.920 --> 14:04.000
that you want to test for these secondary characteristics or is efficacy also a part

14:04.000 --> 14:07.840
of what you're trying to use machine learning to identify?

14:07.840 --> 14:10.360
That's a great question, actually.

14:10.360 --> 14:16.240
So in drug discovery, typically, we think about in terms of three stages, the biology,

14:16.240 --> 14:19.480
the chemistry, and the medicine, I don't know why that way.

14:19.480 --> 14:24.080
So the biology part is indeed about asking what is the question.

14:24.080 --> 14:27.800
So if you want to cure a disease, which proteins you should target?

14:27.800 --> 14:28.800
Yeah.

14:28.800 --> 14:33.200
And then the chemistry part is conditioned on these proteins from the target, give me the

14:33.200 --> 14:36.800
best molecule that can hit those proteins from the others and the medicine part is

14:36.800 --> 14:42.640
so carrying forward the molecule to how do you think of drugs and impact patients.

14:42.640 --> 14:46.320
So right now, we are not really working that much on the biology part.

14:46.320 --> 14:49.720
I think it's a fascinating question, it's a very difficult question, we're not redoing

14:49.720 --> 14:50.720
that much there yet.

14:50.720 --> 14:56.320
I think a lot of very talented folks are dedicating a lot of effort in the more biological

14:56.320 --> 14:57.320
parts.

14:57.320 --> 15:00.120
So I will mostly put things to the chemistry part.

15:00.120 --> 15:03.080
So we've talked a lot about the chemistry part thus far.

15:03.080 --> 15:12.640
We also focus in your research on more theoretical questions around machine learning and algorithms

15:12.640 --> 15:17.800
and tell us a little bit more about that aspect of your research.

15:17.800 --> 15:18.800
Right.

15:18.800 --> 15:23.000
So I think in the process of trying to come up with algorithms or chemistry materials,

15:23.000 --> 15:29.920
we realize there are some interesting questions that are so general across the different algorithms

15:29.920 --> 15:32.560
that we are dealing with.

15:32.560 --> 15:39.040
For example, the question of well, we all use gradient optimizers to optimize algorithms

15:39.040 --> 15:45.120
but from an optimist parameters rather but from a physics perspective, do we know why

15:45.120 --> 15:51.240
these algorithms work and why do they not get stuck in bad solutions, high energy solutions

15:51.240 --> 15:57.720
or high loss function solutions, do we know how the loss function landscape looks like.

15:57.720 --> 16:03.120
I mean, those are questions that obviously has been considered by a lot of machine learning

16:03.120 --> 16:05.440
pioneers in the past.

16:05.440 --> 16:11.560
So we build on those works and so use techniques in statistical physics and pure mathematics

16:11.560 --> 16:16.400
to sort of analyze and sort of toy models of these neural networks and trying to get

16:16.400 --> 16:21.120
to the heart of why certain optimizers work and we in fact show that there's a very

16:21.120 --> 16:28.200
nice synergy and mapping between machine learning and a statistical physics problem of

16:28.200 --> 16:33.600
sort of free energy optimization that allows us to explain why these optimizers work and

16:33.600 --> 16:39.920
we also then move on to think about what's the landscape of the loss function in machine

16:39.920 --> 16:40.920
learning algorithm.

16:40.920 --> 16:46.080
And so to derive a set of very interesting analytical results showing why deep networks

16:46.080 --> 16:49.040
are easier to optimize than shallow networks.

16:49.040 --> 16:52.680
And obviously, the end goal of these research is to try to think about whether they are

16:52.680 --> 16:57.960
better inference algorithms and they we can obtain by taking a more principled view to

16:57.960 --> 16:59.680
why and how algorithms work.

16:59.680 --> 17:00.680
Okay.

17:00.680 --> 17:05.520
Is there a short answer to why deep networks are easier to optimize than shallow networks?

17:05.520 --> 17:12.000
So we find is that deep networks, minimized deep networks are actually closer together

17:12.000 --> 17:18.160
than shallow networks that deep networks actually of good solutions are easier to find than

17:18.160 --> 17:19.160
that solutions.

17:19.160 --> 17:20.960
I think that's something in a nutshell.

17:20.960 --> 17:26.200
So the summary of the analytical results, if you just minimize, it's just easier to find

17:26.200 --> 17:28.280
good solutions than bad ones.

17:28.280 --> 17:36.680
In our conversation thus far and in some of your work, you mentioned energy, energy landscape

17:36.680 --> 17:38.680
and free energy and things like that.

17:38.680 --> 17:44.680
Can you elaborate on how you use that concept and how it comes into play?

17:44.680 --> 17:45.680
Right.

17:45.680 --> 17:53.440
So energy landscape in physics and chemistry relates to the concept of how the potential

17:53.440 --> 17:58.400
energy of a system changes as you change the system.

17:58.400 --> 18:04.920
So for example, if I have two atoms connected by a spring, pull the atoms apart, the energy

18:04.920 --> 18:07.040
increases because atoms really want to be together.

18:07.040 --> 18:11.440
If I squash atoms too much, the energy also increases but atoms want to be sort of not overlapping

18:11.440 --> 18:12.440
each other.

18:12.440 --> 18:14.160
Therefore, you get a chemical bond.

18:14.160 --> 18:17.840
You think of this, generalizing this concept to many, many degrees of freedom, so many,

18:17.840 --> 18:22.080
many atoms and you have a whole landscape, depending on the coordinates of each atom,

18:22.080 --> 18:23.080
you get a different energy.

18:23.080 --> 18:24.080
Okay.

18:24.080 --> 18:28.200
If you think of each atom as a parameter in the machine learning model, then aha, this

18:28.200 --> 18:34.480
and you think of the energy as the loss function, then you can basically map a lot of what

18:34.480 --> 18:41.680
we have, people have fallen about in physics and chemistry into ideas in machine learning.

18:41.680 --> 18:48.520
So for example, glasses in physics correspond to the first certain interesting time of physics.

18:48.520 --> 18:49.520
Sorry.

18:49.520 --> 18:50.520
Sorry.

18:50.520 --> 18:51.520
What in physics?

18:51.520 --> 18:52.520
Glasses.

18:52.520 --> 18:54.040
The idea of glass of a glass system.

18:54.040 --> 19:03.280
So if you think about a sort of a glass, then that system is disordered, it doesn't really

19:03.280 --> 19:06.040
have an order or crystalline structure in it.

19:06.040 --> 19:11.640
It flows extremely slowly, in fact, a glass is technically not a solid because it flows

19:11.640 --> 19:13.960
extremely, extremely slowly.

19:13.960 --> 19:19.560
And the energy landscape of glasses actually shares some similarity with the energy landscape

19:19.560 --> 19:23.400
or the loss function landscape machine learning models and it says that it's highly

19:23.400 --> 19:24.400
disordered.

19:24.400 --> 19:28.800
The parameters are definitely not regular at all, like you see very bumpy energy landscapes.

19:28.800 --> 19:29.800
So that's one extreme.

19:29.800 --> 19:34.080
The other interesting example to think about is proteins, for example, the energy landscape

19:34.080 --> 19:35.800
of proteins are very fun.

19:35.800 --> 19:40.640
So go funnel like because the protein needs to reach a state very quickly as it won't

19:40.640 --> 19:41.640
vote.

19:41.640 --> 19:46.120
And if we can engineer a machine learning landscape that is funnel like, rather than glass

19:46.120 --> 19:50.200
like, and that's great because that means the machine learning model quickly finds the

19:50.200 --> 19:52.640
best parameters with minimal effort.

19:52.640 --> 19:58.000
So you can think about these physical objects and this analogies of particular models to

19:58.000 --> 20:00.920
think about how we can optimize the models.

20:00.920 --> 20:07.400
So you draw this parallel between physics and the kind of loss function optimization space

20:07.400 --> 20:08.400
of models.

20:08.400 --> 20:13.400
I guess I'm, here's what are some concrete results that you have, you know, either, you

20:13.400 --> 20:16.640
know, from physics to machine learning or the other way around.

20:16.640 --> 20:18.480
You mentioned a few things.

20:18.480 --> 20:23.200
Are those, you know, how concrete are those as opposed to ideas if we could think about

20:23.200 --> 20:28.960
the machine learning, the protein example that you use, like, have you actually implement

20:28.960 --> 20:34.280
that implemented that and use that to find concrete deep learning architectures that converge

20:34.280 --> 20:39.280
faster because they use some properties of physical protein?

20:39.280 --> 20:40.400
That's a great question.

20:40.400 --> 20:43.920
So we have had, we are still working towards that.

20:43.920 --> 20:47.880
There will be a dream of this direction that research released from my perspective is

20:47.880 --> 20:52.120
to find some math maps between the two and actually accelerate ML.

20:52.120 --> 20:53.120
Yeah.

20:53.120 --> 20:58.520
Right now we have characterized ML algorithms both numerically and analytically and

20:58.520 --> 21:05.640
created, I guess, an idea of how how ML algorithms can be mapped to physical objects, physical

21:05.640 --> 21:07.400
heuristics, right?

21:07.400 --> 21:11.200
And then an active area of research is indeed trying to go once that further and think

21:11.200 --> 21:16.480
about how to create some more optimizable ML algorithms.

21:16.480 --> 21:24.800
Do you envision that there are things that we can learn about physics by observing machine

21:24.800 --> 21:30.560
learning or is physics so much further ahead that, you know, there's probably nothing interesting

21:30.560 --> 21:31.560
there?

21:31.560 --> 21:35.920
Oh, I think physics is a lot to gain from machine learning.

21:35.920 --> 21:44.600
I think a lot of physical theories are derived based on be looking at data and say, and

21:44.600 --> 21:49.360
the physics is saying, oh, like, because I can only keep track of, well, two degrees of

21:49.360 --> 21:56.800
freedom, if I'm plotting in 2D, that I want to get a line and I only focus on two things.

21:56.800 --> 22:03.040
And that biases you towards sort of neglecting a lot of data, which could actually be interesting

22:03.040 --> 22:04.040
and useful.

22:04.040 --> 22:11.600
So in this theme, for example, we published a recent paper on a battery degradation where

22:11.600 --> 22:18.000
a lot of practitioners in the field has sort of looked at spectroscopic ideas to predict

22:18.000 --> 22:24.040
how the lifetime of battery, given non-invasive spectroscopic measurements, if you can do

22:24.040 --> 22:28.600
that, then you can tell them, so how whether battery is still alive or not, or how many

22:28.600 --> 22:33.560
cycles is left before it would die, and that's very useful for some like electric vehicles

22:33.560 --> 22:35.640
or personal consumer electronics.

22:35.640 --> 22:42.440
But the upshot is that a lot of folks look at, so very clear, observable features of

22:42.440 --> 22:46.960
these spectrum, whereas if you just train the machine learning model on the spectrum to

22:46.960 --> 22:50.640
use it to predict degradation, the machine learning model actually identifies very subtle

22:50.640 --> 22:51.640
features.

22:51.640 --> 22:57.200
So very high order correlations of features that you will never even expect to be important.

22:57.200 --> 23:02.200
And then you can go back and go to the physics and ask, well, what does this feature correspond

23:02.200 --> 23:03.200
to?

23:03.200 --> 23:07.720
And in fact, we are now thinking about are there new explanations of battery degradation

23:07.720 --> 23:12.440
based on just assigning, or this feature is actually important for degradation.

23:12.440 --> 23:16.520
That's not something you can just do looking at the data or conversion of physical theories.

23:16.520 --> 23:17.520
Interesting.

23:17.520 --> 23:27.440
Do you think it's possible that some of the well-known analytical, close analytical results

23:27.440 --> 23:33.880
that we take for granted in physics that are low-order polynomial actually have a lot

23:33.880 --> 23:40.080
of these subtle additional features that will come to discover based on this line of thinking?

23:40.080 --> 23:45.320
Or am I oversimplifying, I'm thinking like maybe it's not equals MC squared, but there's

23:45.320 --> 23:50.920
all these other, you know, there are all these other kind of higher order features that,

23:50.920 --> 23:56.640
you know, because of our bias to clean polynomial solutions, we've, you know, ended up, you

23:56.640 --> 24:01.160
know, holding these relationships up that aren't as, you know, nuanced enough.

24:01.160 --> 24:02.160
Yeah.

24:02.160 --> 24:08.280
I think I would have a distinction between, um, solve fundamental physics frameworks and

24:08.280 --> 24:10.880
how physics is being implemented and executed.

24:10.880 --> 24:15.520
I think if you think about fundamental physics frameworks, like special relativity, general

24:15.520 --> 24:18.640
relativity, quantum mechanics, gravity, I think.

24:18.640 --> 24:21.480
And I was just in a group of low-order polynomial relationships.

24:21.480 --> 24:24.280
I wasn't necessarily talking about relativity.

24:24.280 --> 24:29.840
So I'm not an expert in those areas, and I think that the frameworks they are relatively

24:29.840 --> 24:30.840
robust.

24:30.840 --> 24:37.200
Um, but I think the implementation of these frameworks into material discovery or not usually

24:37.200 --> 24:41.080
require lost simplifications, like quantum mechanics, for example, um, can we solve quantum

24:41.080 --> 24:42.080
mechanics?

24:42.080 --> 24:45.640
Um, numerically even, like super computers, like exactly numerically.

24:45.640 --> 24:50.080
So you make a lot of simplifications on the way, the same with other areas of physics.

24:50.080 --> 24:55.760
And the quality of these approximations can be massively improved with machine learning.

24:55.760 --> 24:56.760
Got it.

24:56.760 --> 25:02.160
So it's not that you're, you know, fundamental laws of thermodynamics aren't robust.

25:02.160 --> 25:03.160
They probably are.

25:03.160 --> 25:04.160
Yeah.

25:04.160 --> 25:05.160
Pretty good.

25:05.160 --> 25:10.160
But when in the real world, there are a lot of, there are a lot of other factors that aren't

25:10.160 --> 25:15.320
captured by kind of idealized models that we tend to overlook and machine learning, you

25:15.320 --> 25:19.880
know, what we learn from machine learning might give us a path to incorporating those

25:19.880 --> 25:20.880
as features.

25:20.880 --> 25:21.880
Yeah.

25:21.880 --> 25:26.360
And I think also it's one of the lot of these physical frameworks are so very easy to express

25:26.360 --> 25:27.360
mathematically.

25:27.360 --> 25:28.360
Yeah.

25:28.360 --> 25:33.840
But actually, I'm solving it for real world, for real molecule or real world problem is actually

25:33.840 --> 25:37.520
exponentially complex, the quantum mechanics, for example, is very, it's very straightforward

25:37.520 --> 25:43.080
to write down an ordinary or partial differential equation, but then to solve it for an, an

25:43.080 --> 25:47.160
electron problem is really non trivial.

25:47.160 --> 25:52.640
And so then that's why people start making a lot of these very good approximations, but

25:52.640 --> 25:57.880
so these approximations could be improved when you take a step back and say, well, why

25:57.880 --> 26:02.880
not just use existing measurements as a way to fine tune the approximations?

26:02.880 --> 26:10.000
Do you in your work look at the activity that's happening around neural ODE research and

26:10.000 --> 26:11.000
does that work?

26:11.000 --> 26:15.280
I've heard of, I've certainly read briefly the paper and I think it's a very interesting

26:15.280 --> 26:18.800
approach to how to integrate physics.

26:18.800 --> 26:24.920
So frameworks like ODE's into machine learning, I think so much could be done to physics if

26:24.920 --> 26:28.960
we start thinking about some semi empirical and empirical physical frameworks as machine

26:28.960 --> 26:29.960
learning.

26:29.960 --> 26:34.280
So people in automatic differentiation field, for example, think about differentiating

26:34.280 --> 26:36.280
through machine learning models.

26:36.280 --> 26:41.280
I've not done research in this area per se, but I am aware of a lot of amazing work coming

26:41.280 --> 26:47.040
out of this community of thinking about maybe three simulations as machine learning and

26:47.040 --> 26:51.080
do automatic differentiation through simulations, et cetera.

26:51.080 --> 26:58.000
You are currently in Santa Clara in California, a long way from either of the Cambridges.

26:58.000 --> 27:04.080
And what brought you there is the startup that you founded, Posterra, and the opportunity

27:04.080 --> 27:08.240
that you had to go through the Y-combinator accelerator.

27:08.240 --> 27:13.440
Can you talk a little bit about Posterra and what it's specifically looking to do?

27:13.440 --> 27:14.440
Yep.

27:14.440 --> 27:20.480
So Posterra is a company that tries to offer some medicinal chemistry as a surface powered

27:20.480 --> 27:22.960
by machine learning.

27:22.960 --> 27:30.440
And we realized that although a lot of excitement has gone into AIML for drug discovery, we take

27:30.440 --> 27:35.960
a step back, we realized there are two key pain points amongst the whole life cycle.

27:35.960 --> 27:42.080
So the first pain point is how do we make molecules?

27:42.080 --> 27:45.920
So the chemical synthesis of molecules, and that usually is the rate determining step

27:45.920 --> 27:49.320
when you try to make thousands of molecules.

27:49.320 --> 27:55.800
Making a molecule has been the focus for a lot of academic and industrial groups, but

27:55.800 --> 27:58.880
how to make molecules relatively less so.

27:58.880 --> 28:01.280
And so we developed a state of the art algorithm.

28:01.280 --> 28:05.400
Part of it is actually published a molecular transformer for reaction prediction and

28:05.400 --> 28:10.480
retro synthesis, which is now the state of the art for reaction prediction, predicting

28:10.480 --> 28:15.720
how to make molecules react and inverse from how to make molecules.

28:15.720 --> 28:19.880
And we know that, and we think that this can really accelerate medicinal chemistry.

28:19.880 --> 28:23.160
And the second pillar that we focus on is uncertainty.

28:23.160 --> 28:29.200
How do we use uncertainty to design experiments such that we explore the most fruitful chemical

28:29.200 --> 28:30.200
space?

28:30.200 --> 28:34.360
And I think we are the, the first really thing about how to integrate the whole design

28:34.360 --> 28:41.240
mech test cycle together in one machine learning platform and offering it to accelerate

28:41.240 --> 28:44.200
drug discovery in both farm and biotech.

28:44.200 --> 28:48.800
Okay, you mentioned molecular transformers that related to the concept of transformers

28:48.800 --> 28:51.440
that we see in like natural language processing?

28:51.440 --> 28:52.440
Yep.

28:52.440 --> 28:59.040
It is inspired by that where we take the basically reaction prediction as a machine translation

28:59.040 --> 29:00.040
problem.

29:00.040 --> 29:01.040
Okay.

29:01.040 --> 29:02.040
We're reacting reagents.

29:02.040 --> 29:06.160
So what goes into the flask is treat as a language or comes out of the flask.

29:06.160 --> 29:10.920
The product is in another language and you think about whether you can use translation

29:10.920 --> 29:17.480
to as a as a as a as a heuristic concept to think about processing these inputs and outputs.

29:17.480 --> 29:22.280
And what we have shown is that actually this perhaps very simple, simple, but direct way

29:22.280 --> 29:26.600
of thinking about chemical reaction actually outperforms a lot of the more sort of heuristic

29:26.600 --> 29:35.480
way of sort of hand crafting reaction rules or hand crafting chemistry templates from textbook.

29:35.480 --> 29:40.360
This very simple approach based on translation achieves over 90% accuracy in predicting

29:40.360 --> 29:46.160
correctly what comes out of the flask given what goes into the flask and that accuracy

29:46.160 --> 29:48.800
outperforms even trained human chemists.

29:48.800 --> 29:51.520
And so how do you go about training a model like that?

29:51.520 --> 29:58.120
Are you doing similar types of techniques where you're doing close kinds of, you know,

29:58.120 --> 30:02.960
you're trying to train in the model to predict things that are left out of your training

30:02.960 --> 30:06.320
data or this is something different.

30:06.320 --> 30:12.480
So we have over 9 million reactions reported in patents, which we have cleaned, aggressively

30:12.480 --> 30:19.120
cleaned and augmented using sort of chemistry knowledge and we train the model on those

30:19.120 --> 30:23.400
data and obviously we invalidated the model using standard training test splits.

30:23.400 --> 30:24.400
Okay.

30:24.400 --> 30:28.560
So you've got the existing relationships that you're training on, right?

30:28.560 --> 30:29.560
Okay.

30:29.560 --> 30:34.640
We use data has been published and we use patent data, which which is a data source of

30:34.640 --> 30:40.160
robust chemical reactions and that means a lot of these chemical transformations can be

30:40.160 --> 30:43.760
readily translated into industrial context.

30:43.760 --> 30:48.600
What's the granularity of the or the format of the input data?

30:48.600 --> 30:53.960
Is it kind of the chemical equations that we're used to seeing from basic, you know, chemistry

30:53.960 --> 30:55.600
class and some representation?

30:55.600 --> 31:02.680
Are you providing lower level physical information about the various molecules or atoms?

31:02.680 --> 31:10.600
Yeah, so the data format is the molecular structure of the reactants, the molecular structure

31:10.600 --> 31:18.240
of the reagents, these reactants plus reagents are what goes into the flask and the product

31:18.240 --> 31:22.960
is what you isolate from this reaction again, the molecular structure is given and then

31:22.960 --> 31:29.080
transformer takes these two inputs and outputs and performs machine translation to predict

31:29.080 --> 31:31.200
the output given inputs.

31:31.200 --> 31:37.160
And that reaction prediction step or reactions in general is actually, I think, very important

31:37.160 --> 31:40.320
part in the whole chemistry stage in drug discovery.

31:40.320 --> 31:45.160
For example, we're actually leading a COVID project, a life COVID project trying to develop

31:45.160 --> 31:50.760
new ND virus against COVID is open source and non-profit initiative that we are having

31:50.760 --> 31:52.400
to lead with academic groups.

31:52.400 --> 31:58.240
And so we launch a crowdsourcing platform, sourcing compounds based on a fragment

31:58.240 --> 32:03.840
merge, so a high throughput screen that our colleagues at Oxford ran, we got like

32:03.840 --> 32:05.800
four, a few thousand structures.

32:05.800 --> 32:09.120
If you were to just look at it one by one and decide which ones can be made easily, it

32:09.120 --> 32:13.960
would take you weeks, whereas an algorithm like molecular transformer would take a weekend

32:13.960 --> 32:18.200
to triage, like which compounds are easily make up both from purchasable starting material

32:18.200 --> 32:23.920
and is that kind of speed, which allows rapid iterations and allows you to sort of make

32:23.920 --> 32:28.520
more molecules, test more molecules and ultimately, the discovered drugs faster.

32:28.520 --> 32:29.520
Okay.

32:29.520 --> 32:33.240
And have any interesting candidates come out of that process yet for COVID?

32:33.240 --> 32:38.840
Yeah, we have got several pretty promising hits against the target, which we are now

32:38.840 --> 32:39.840
rapidly developing.

32:39.840 --> 32:42.440
Oh, that's awesome.

32:42.440 --> 32:48.840
So you started why a commonator in January, are you close to your demo day?

32:48.840 --> 32:49.840
What's your January?

32:49.840 --> 32:50.840
You just did our demo day.

32:50.840 --> 32:51.840
Oh, you just did it.

32:51.840 --> 32:52.840
I go, yep.

32:52.840 --> 32:53.840
Oh, how did it go?

32:53.840 --> 32:55.600
It went really well.

32:55.600 --> 32:58.920
We were able to close our seat ground within a week.

32:58.920 --> 32:59.920
Okay.

32:59.920 --> 33:04.760
This is really interesting stuff, and I appreciate you taking the time to chat with me.

33:04.760 --> 33:10.520
Are there, do you have any parting words or thoughts or resources that folks can follow

33:10.520 --> 33:13.360
up on if they're interested in learning more about this area?

33:13.360 --> 33:21.560
Yeah, I'm happy to chat with any folks interested in our work and so our COVID platform,

33:21.560 --> 33:26.040
the plug on that is COVID.posterror.ai.

33:26.040 --> 33:32.440
Our whole project is non-profit and completely open, so if folks are interested in tripping

33:32.440 --> 33:39.880
in ideas or commenting on the ML or the chemistry, if we do so, we have a forum, a live forum

33:39.880 --> 33:46.240
where a lot of the chemist discuss ideas, welcome people, so we're chiming in.

33:46.240 --> 33:47.240
Awesome.

33:47.240 --> 33:48.240
Awesome.

33:48.240 --> 33:49.720
Well, Alpha, thanks so much for joining us.

33:49.720 --> 33:50.720
Yeah.

33:50.720 --> 33:58.000
All right, everyone, that's our show for today.

33:58.000 --> 34:03.800
For more information on today's show, visit twomolai.com slash shows.

34:03.800 --> 34:23.920
As always, thanks so much for listening and catch you next time.

