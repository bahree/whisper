WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.640
I'm your host Sam Charrington.

00:23.640 --> 00:27.520
This week we have a very special interview for you.

00:27.520 --> 00:31.860
Those of you who've been receiving my newsletter for a while might remember that while in

00:31.860 --> 00:36.960
Switzerland last month, I had the pleasure of interviewing Yuggen Schmiedhuber in his

00:36.960 --> 00:43.000
lab Idzia, which is the Dalmahl Institute for Artificial Intelligence in Lugano, Switzerland,

00:43.000 --> 00:45.440
where he serves as scientific director.

00:45.440 --> 00:51.000
In addition to his role at Idzia, Yuggen is co-founder and chief scientist at Nacense,

00:51.000 --> 00:57.800
a company that's using AI to build large-scale neural network solutions for superhuman perception

00:57.800 --> 01:00.320
and intelligent automation.

01:00.320 --> 01:05.520
This is absolutely the furthest I've ever traveled for an interview, but I was in the

01:05.520 --> 01:09.360
neighborhood, so to speak, and boy was it worth it.

01:09.360 --> 01:14.200
Yuggen is an interesting, accomplished, and in some circles controversial figure in the

01:14.200 --> 01:20.160
AI community, and we covered a ton of really interesting ground in our discussion.

01:20.160 --> 01:24.480
So much so that I couldn't truly unpack it all until I had a chance to sit with it after

01:24.480 --> 01:26.040
the fact.

01:26.040 --> 01:32.160
We talked a bunch about his work on neural networks, especially LSTMs, or long short-term

01:32.160 --> 01:37.120
memory networks, which are a key innovation behind many of the advances we've seen in

01:37.120 --> 01:41.360
deep learning and its application over the past few years.

01:41.360 --> 01:46.000
Along the way, Yuggen walks us through a deep learning history lesson that spans 50

01:46.000 --> 01:50.840
plus years, it was like walking back in time with the three-eyed raven.

01:50.840 --> 01:56.440
I know you're really going to enjoy this one, and by the way, this is definitely a nerd

01:56.440 --> 01:57.680
alert show.

01:57.680 --> 02:01.200
A few key announcements before we jump into the show though.

02:01.200 --> 02:05.600
First off, I want to give a big thank you to our friends at Cloudera for sponsoring this

02:05.600 --> 02:07.120
episode.

02:07.120 --> 02:11.160
You probably think of Cloudera primarily as the Hadoop company, and you're not wrong

02:11.160 --> 02:15.960
for that, but did you know they also offer software for data science and deep learning?

02:15.960 --> 02:17.400
They do.

02:17.400 --> 02:19.640
The idea is pretty simple.

02:19.640 --> 02:24.320
If you work for a large enterprise, you probably already have Hadoop in place, and your Hadoop

02:24.320 --> 02:29.200
cluster is filled with lots of data that you want to use in building your models.

02:29.200 --> 02:35.720
But you still need to easily access that data, process it using the latest open source tools,

02:35.720 --> 02:39.480
and harness bursts of compute power to train your models.

02:39.480 --> 02:43.200
This is where Cloudera's data science workbench comes in.

02:43.200 --> 02:46.800
With data science workbench, Cloudera can help you get up and running with deep learning

02:46.800 --> 02:52.760
without massive new investments by implementing an on-demand self-service deep learning platform

02:52.760 --> 02:55.960
right on your existing CDH clusters.

02:55.960 --> 02:58.840
From a tech perspective, DSW is pretty cool.

02:58.840 --> 03:04.720
It uses Kubernetes to transparently scale workloads across the cluster, supporting our Python

03:04.720 --> 03:10.560
and Scala and deep learning frameworks like TensorFlow, Keras, Cafe, and Theano.

03:10.560 --> 03:17.080
And as of last month's 1.1 release, GPUs on the Hadoop cluster are fully supported.

03:17.080 --> 03:22.120
The folks at Cloudera are so confident that you're going to like what you see, that for a limited

03:22.120 --> 03:27.560
time, they're offering a drone to qualified participants simply for meeting with their

03:27.560 --> 03:32.520
sales representative for a demonstration of the data science workbench.

03:32.520 --> 03:38.080
For your demo and drone, visit twimlai.com slash Cloudera.

03:38.080 --> 03:41.720
And C-L-O-U-D-E-R-A.

03:41.720 --> 03:45.920
Next up, a reminder to register for our online meetup.

03:45.920 --> 03:50.360
If you missed the first meetup, we expect to have the recording online later today.

03:50.360 --> 03:55.960
You'll be able to view it at twimlai.com slash meetup, which is also where you can register

03:55.960 --> 04:00.560
for the next one, which will be held on Wednesday, September 13th.

04:00.560 --> 04:04.600
We're currently finalizing the topic, but if you're registered, we'll notify you as

04:04.600 --> 04:06.200
soon as that's done.

04:06.200 --> 04:11.160
Finally, if you like the podcast, please go ahead and sign up for our newsletter, which

04:11.160 --> 04:14.960
is the best way to keep up to date with me and what's going on on the show.

04:14.960 --> 04:18.760
My picks and thoughts for what's new and important in machine learning and AI for the

04:18.760 --> 04:23.800
week and exclusive promos and giveaways just for newsletter readers.

04:23.800 --> 04:28.840
To subscribe, visit twimlai.com slash newsletter.

04:28.840 --> 04:38.200
And now on to the show.

04:38.200 --> 04:39.200
All right, everyone.

04:39.200 --> 04:45.520
I have the distinct pleasure of being here with J端rgen Schmidt-Huber, who is the scientific

04:45.520 --> 04:52.360
director of the Swiss AI lab Idsia, as well as a professor of Artificial Intelligence

04:52.360 --> 04:58.400
with Usi, the University of Lugano, and Subsi, the University of Applied Science and Art

04:58.400 --> 05:04.840
of Southern Switzerland, as well as being the Chief Scientist for Nacense Company that

05:04.840 --> 05:06.680
he co-founded.

05:06.680 --> 05:14.400
J端rgen has a long history in artificial intelligence and is widely recognized as being a pioneer

05:14.400 --> 05:21.520
in the development of recurrent neural networks and in particular LSTM neural networks, which

05:21.520 --> 05:24.000
we've talked about quite a bit on this podcast.

05:24.000 --> 05:27.840
I'm super excited to be here with you, J端rgen, welcome to this week in machine learning

05:27.840 --> 05:28.840
and AI.

05:28.840 --> 05:31.440
It's my pleasure, Sam.

05:31.440 --> 05:37.280
So I like to get these conversations started by just having you introduce yourself and

05:37.280 --> 05:43.320
tell us a little bit about how you found your way into the field of artificial intelligence.

05:43.320 --> 05:46.080
I can do that.

05:46.080 --> 05:53.080
When I was a boy, I tried to figure out how can I maximize my impact and then quickly

05:53.080 --> 05:59.400
it became clear to me that I have to build something that learns to become smarter than

05:59.400 --> 06:07.400
myself, such that I can retire and such that this smarter thing is going to further self-improve

06:07.400 --> 06:11.600
and solve all the problems that I cannot solve.

06:11.600 --> 06:17.120
And that's what I've been trying to do for the past three decades or so.

06:17.120 --> 06:21.680
So you've got a lot going on, I mentioned a bit of it in the intro.

06:21.680 --> 06:25.760
Tell us a little bit about how you spend your time and some of the research efforts that

06:25.760 --> 06:27.800
keep you occupied.

06:27.800 --> 06:35.080
In many ways, I'm still doing the same thing that I've started to publish on in 1987,

06:35.080 --> 06:41.480
which is both this general purpose learning machine, which learns to better and better understand

06:41.480 --> 06:48.800
the world and learns to exploit that knowledge about the world to become a better and better

06:48.800 --> 06:53.200
and more and more general problem solver.

06:53.200 --> 07:00.280
Over the years and decades, we have made a couple of insights related to fundamental building

07:00.280 --> 07:07.000
blocks that you need to build this full AGI system, but we are not yet done.

07:07.000 --> 07:13.840
And some of the puzzle pieces still have to fall in place in a way that we are trying

07:13.840 --> 07:19.360
to push both at the Institute for Artificial Intelligence, at the Swiss AI Lab.

07:19.360 --> 07:24.680
It's here here and in my company, Nasens, where the goal is really to build a general

07:24.680 --> 07:32.880
purpose AI that learns to do not only one thing in a particular domain, but then learns

07:32.880 --> 07:37.840
on top of that to learn a new profession in another domain.

07:37.840 --> 07:43.520
And then on top of that, yet another thing, such that it becomes more and more general problems

07:43.520 --> 07:49.840
over and even learns the learning algorithm itself, such that it's not always stuck with

07:49.840 --> 07:55.520
the same initial learning algorithm, but learns to improve that method itself, such that

07:55.520 --> 08:00.480
learns to improve the way it improves itself and so on.

08:00.480 --> 08:06.400
And you can imagine that on the way to that goal, there are all kinds of little subproblems

08:06.400 --> 08:12.880
coming up and we always focus on the most promising ones.

08:12.880 --> 08:18.560
Sometimes you don't have to build a full AGI to solve interesting problems.

08:18.560 --> 08:25.600
Sometimes just better patterned recognition will do the job in certain applications and

08:25.600 --> 08:31.320
so patterned recognition, which is a tiny part of the full AI thing, that is something

08:31.320 --> 08:35.080
that has become really commercial in recent years.

08:35.080 --> 08:39.800
Some of the techniques that we have developed in the past decades are now really useful

08:39.800 --> 08:46.080
in commercial applications, such as speech recognition and machine translation and all

08:46.080 --> 08:48.760
kinds of patterned recognition.

08:48.760 --> 08:51.040
So what is the daily procedure?

08:51.040 --> 08:54.040
There is no recipe for that.

08:54.040 --> 09:01.080
You try to make progress here and there and there and sometimes you seem to be in a

09:01.080 --> 09:02.080
dead end.

09:02.080 --> 09:06.920
In fact, most of the time you are in a dead end, but then you backtrack and after 100 dead

09:06.920 --> 09:13.240
ends as and again some sort of progress and that's worth it because from there, then

09:13.240 --> 09:21.760
new search paths are opening up and new dead ends, but also new progress is coming in.

09:21.760 --> 09:29.160
I get the impression that a lot of contemporary machine learning and AI researchers are not

09:29.160 --> 09:34.920
driving towards a vision of AGI, they're solving specific point problems.

09:34.920 --> 09:42.120
Do you feel that pursuing your research in the context of trying to create a generalized

09:42.120 --> 09:47.440
AGI gives you a different perspective or heads you down a different path or gives you

09:47.440 --> 09:49.280
a different approach?

09:49.280 --> 09:55.280
The goal of AGI certainly made a difference to us.

09:55.280 --> 10:01.640
First of all, if you want to build a general purpose learning machine, then you will need

10:01.640 --> 10:09.120
a general purpose computational architecture and in our field of neural networks, that means

10:09.120 --> 10:16.240
a recurrent neural network with feedback connections, which is much more powerful than what most

10:16.240 --> 10:22.120
of my colleagues have been studying most of the time, which are so-called feed forward

10:22.120 --> 10:25.600
neural networks, which don't have feedback connections.

10:25.600 --> 10:30.280
The difference between a recurrent network and a feed forward network is a bit like the

10:30.280 --> 10:36.520
difference between a general purpose computer and a mere calculator because on a general

10:36.520 --> 10:44.400
purpose, a recurrent network, you can run arbitrary sequential parallel programs as opposed to

10:44.400 --> 10:49.480
a feed forward network where you just can shift information from an input to the next

10:49.480 --> 10:56.960
layer and then the next and finally you have a result that is determined through a very

10:56.960 --> 11:01.960
limited feed forward computation where many things that you know from traditional computer

11:01.960 --> 11:04.760
signs are not possible.

11:04.760 --> 11:10.640
So to build a general AGI within your networks, you have to start with a recurrent neural

11:10.640 --> 11:12.240
networks.

11:12.240 --> 11:19.200
And the work that we have been doing in the past decades really has focused on that.

11:19.200 --> 11:26.000
In the late 80s, I started to work on these recurrent networks, which are more challenging

11:26.000 --> 11:31.600
than the feed forward networks because you have to deal with the space of our possible

11:31.600 --> 11:35.160
programs that you can implement on a general purpose computer.

11:35.160 --> 11:43.560
So basically, you have to search in the space of all programs to find solutions to problems.

11:43.560 --> 11:50.600
This makes it hard and soon we ran into problems with that approach and the traditional recurrent

11:50.600 --> 11:56.160
networks didn't really work well and so we had to come up with a couple of improvements

11:56.160 --> 12:04.080
such that you can really use and exploit this power, the theoretical power of recurrent

12:04.080 --> 12:05.840
neural networks.

12:05.840 --> 12:11.760
Before we dive into that, I'd like to further explore this notion of a RNN as a general

12:11.760 --> 12:14.320
computer, as general compute framework.

12:14.320 --> 12:19.040
I think that's not obvious to a lot of people and as an example, I don't know if you've

12:19.040 --> 12:24.200
ever heard of this, FizzBuzz with TensorFlow, does that mean anything to you?

12:24.200 --> 12:26.880
TensorFlow means something, but FizzBuzz does not.

12:26.880 --> 12:33.920
FizzBuzz is a common programmer interview question and you tell the candidate to write

12:33.920 --> 12:41.600
a program that basically, I forget the specifics, but it's something like Prince Fizz when it

12:41.600 --> 12:45.400
runs through a loop of numbers and Prince Fizz when the number is divisible by three and

12:45.400 --> 12:49.600
buzz when the number is divisible by five and FizzBuzz when the number is divisible

12:49.600 --> 12:50.600
by 15.

12:50.600 --> 12:56.760
And so if you know how to do a loop and use mod, it's very easy to do, and someone with

12:56.760 --> 13:03.080
some experience in deep learning in TensorFlow got asked this question on an interview and

13:03.080 --> 13:07.600
they decided to, it inspired them to try to figure out how they would accomplish this

13:07.600 --> 13:09.000
with deep learning.

13:09.000 --> 13:15.600
And in fact, I think the result of their experimentation was that this thing that's

13:15.600 --> 13:23.280
extremely, extremely simple to do with common general purpose programming is very, very difficult

13:23.280 --> 13:25.240
to do with neural networks.

13:25.240 --> 13:30.120
You have to come up with a lot of data, you have to train these networks and they still,

13:30.120 --> 13:33.120
because it's probabilistic, they still get it wrong.

13:33.120 --> 13:38.640
And so with that as context, I'd love to hear you further explore this idea of RNNs

13:38.640 --> 13:41.640
as a general purpose computer.

13:41.640 --> 13:42.640
Yes.

13:42.640 --> 13:50.040
Now, first of all, how do you see that an RNN, a recurrent neural network, is a general

13:50.040 --> 13:53.720
purpose computer, as general as your laptop?

13:53.720 --> 14:02.600
Well, you can take some parts of the recurrent network and wire them up as NAND gates, not

14:02.600 --> 14:07.200
AND gates, NAND gates, very simple, very tiny little subnetworks.

14:07.200 --> 14:11.400
And then you just have to recall that the...

14:11.400 --> 14:12.400
That's what your computer is.

14:12.400 --> 14:22.720
And the CPU of your phone or of your laptop can be emulated by a network of NAND gates.

14:22.720 --> 14:30.040
It essentially is a network of NAND gates, which means you can essentially emulate your

14:30.040 --> 14:35.640
laptop and whatever program is running on it on a vicon network.

14:35.640 --> 14:43.880
In a certain sense, your microchip in your phone is just a very sparsely connected recurrent

14:43.880 --> 14:44.880
network.

14:44.880 --> 14:46.400
So that's step number one.

14:46.400 --> 14:53.840
Let me give you an even simpler example where you see how elegant and powerful recurrent

14:53.840 --> 14:58.280
networks can be in comparison to feet forward networks.

14:58.280 --> 15:01.960
Let's look at the problem of parity.

15:01.960 --> 15:05.240
Suppose somebody gives you N bits of information.

15:05.240 --> 15:13.680
0, 111, 0, and you should classify that as to whether the number of ones in that sequence

15:13.680 --> 15:15.920
is even or odd.

15:15.920 --> 15:19.040
So that's the parity problem.

15:19.040 --> 15:26.240
Now you can indeed wire up a feet forward network, which has, say, 10 different input units

15:26.240 --> 15:34.240
to implement this mapping, which maps the input string to a decision either this is an

15:34.240 --> 15:38.720
odd number of ones or an even number of ones.

15:38.720 --> 15:42.760
And it will take you a rather big network with lots of connections.

15:42.760 --> 15:48.360
And it will never generalize to 11 bits or 12 or something because the size, the input

15:48.360 --> 15:50.240
size is so limited.

15:50.240 --> 15:56.600
Now, with a recurrent network, you can solve the parity problem much more elegantly.

15:56.600 --> 16:02.120
You just feed in the bits one by one into a recurrent network that just has one input

16:02.120 --> 16:08.560
unit and one internal hidden unit and one output unit and maybe an additional bias input

16:08.560 --> 16:10.440
unit, if you know what that is.

16:10.440 --> 16:16.040
And then you have just five connections in this little network and you feed in the input

16:16.040 --> 16:23.200
string, 1 0, 111, 1 by 1 and all the network has to learn to become a flip flop because

16:23.200 --> 16:31.840
whatever, whenever a new one is appearing, then the internal unit that represents what

16:31.840 --> 16:35.480
the network has seen so far just has to flip it state.

16:35.480 --> 16:40.840
And that determines then is the current number of ones, is it odd or even.

16:40.840 --> 16:45.520
So this program for the recurrent network is so simple.

16:45.520 --> 16:52.200
It fits into five connections and you can easily guess those connections.

16:52.200 --> 16:59.000
You just do 1,000 random guesses for the five weights, something between minus 10 and

16:59.000 --> 17:06.920
plus 10 and you test the result on a tiny little validation set, maybe three patterns,

17:06.920 --> 17:09.360
three different parity problems.

17:09.360 --> 17:14.640
And if it works on that, you can be almost sure that it's going to generalize to all possible

17:14.640 --> 17:19.760
parity problems, not only those where there are 10 bits coming in, but also those where

17:19.760 --> 17:25.480
there are five bits coming in, but also those where there are 5,000 bits coming in.

17:25.480 --> 17:33.280
So the natural and elegant sequential solution to the parity problem easily fits into a simple

17:33.280 --> 17:38.680
recurrent network and it needs a really awkward, complicated feed forward network to solve

17:38.680 --> 17:43.160
a tiny part of that for just exactly 10 bits, say, or 15 bits.

17:43.160 --> 17:49.520
So this is to illustrate that the difference between a recurrent neural network and the

17:49.520 --> 17:56.720
feed forward network is like the difference between a general purpose, computational architecture

17:56.720 --> 18:00.520
and a mere calculator.

18:00.520 --> 18:07.880
Is that to say then that the referring RNNs as a general purpose computing system is way

18:07.880 --> 18:12.760
more general purpose than feed forward, but we still have a ways to go in our ability

18:12.760 --> 18:17.960
to train these things in order for them to be truly as general as our current computing

18:17.960 --> 18:19.440
architectures.

18:19.440 --> 18:23.720
Maybe another way to ask that question is, what's the fly in the ointment here?

18:23.720 --> 18:30.040
What are the limitations in the way we deal with these RNNs that prevent us from using them

18:30.040 --> 18:34.360
as general purpose computing system as you kind of assert?

18:34.360 --> 18:35.360
Yeah.

18:35.360 --> 18:41.320
So it's one thing to have a general purpose computing architecture and it's another thing

18:41.320 --> 18:48.840
to run the programs running on that architecture, which are solving your favorite programs.

18:48.840 --> 18:54.880
So to put that into an example, so we know that these RNNs are basically functions to

18:54.880 --> 18:59.920
transform inputs and outputs, and so there's some set of inputs that produces the output

18:59.920 --> 19:06.360
of your arbitrary software program, whether that's Mac OS or what have you, but learning

19:06.360 --> 19:10.840
that function based on the inputs is a whole different story.

19:10.840 --> 19:12.760
It's a whole different story.

19:12.760 --> 19:17.680
The learning of the programs running on your general purpose devices on your recurrent

19:17.680 --> 19:21.760
neural networks, that is the really interesting part.

19:21.760 --> 19:28.800
And people have tried it for a long time, but only with certain tweaks to the original

19:28.800 --> 19:35.080
concept of recurrent networks, it became feasible in practice, and it became so powerful

19:35.080 --> 19:38.840
that it's now all over the place in every smartphone.

19:38.840 --> 19:43.760
So we're going to talk about that in one second, but I'd like you to provide a little bit

19:43.760 --> 19:46.640
of historical context for RNNs.

19:46.640 --> 19:52.000
You started working on these at a time when feed-forward networks were considered the state

19:52.000 --> 19:59.080
of the art, and you've seen the way, you've seen the evolution of RNNs, tells a little

19:59.080 --> 20:01.480
bit about that history.

20:01.480 --> 20:07.960
So maybe let's start with the history of deep feed-forward networks, which go back to

20:07.960 --> 20:08.960
1965.

20:08.960 --> 20:12.880
When I was a baby, I was two years old back then.

20:12.880 --> 20:19.400
And there was this famous mathematician, Eva Knenko, in the Ukraine, and with his student

20:19.400 --> 20:25.480
Lapa, they published this method for training deep feed-forward networks.

20:25.480 --> 20:27.200
They didn't even call them neural networks.

20:27.200 --> 20:31.360
They called it the group method of data handling, but that's what this stuff was.

20:31.360 --> 20:36.680
It was deep multilayer perceptrons, and they found a way of learning internal representations

20:36.680 --> 20:43.880
in these deep multilayer perceptrons, and by 1970, they already had networks with seven

20:43.880 --> 20:47.880
eight layers, and many people built on that later.

20:47.880 --> 20:52.200
And deep in there, at that time, was on the order of ten?

20:52.200 --> 20:53.960
On the order of ten, yes.

20:53.960 --> 20:59.320
And even by the standards of the new millennium, this was deep.

20:59.320 --> 21:04.480
Because most people in the early 2000s didn't use networks that were as deep as those

21:04.480 --> 21:05.480
of Eva Knenko.

21:05.480 --> 21:07.000
That's true.

21:07.000 --> 21:09.800
And he did that 1965.

21:09.800 --> 21:17.440
That was four years before Minskia and Papad wrote a book about the limitations of shallow

21:17.440 --> 21:23.720
networks with one single layer, which is called perceptron, and which is sometimes credited

21:23.720 --> 21:30.680
for killing neural network research in America, because people thought, oh, if these eyes

21:30.680 --> 21:36.720
are so limited, then we shouldn't further progress there, or we shouldn't further work

21:36.720 --> 21:38.040
on that.

21:38.040 --> 21:43.280
And maybe it was a cold war thing over there, on the other side of the Iron Curtain, I

21:43.280 --> 21:48.120
think this was the Soviet Union, wasn't the Ukraine, and was the Soviet Union.

21:48.120 --> 21:52.480
And there were these people who had already deep learning networks back then.

21:52.480 --> 21:59.920
However, then in the 80s, the more general attempts came where you really tried to work

21:59.920 --> 22:05.480
with recur networks, with general-purpose computers, not just feed-forward networks, and you want

22:05.480 --> 22:09.480
to have these recur networks for all kinds of interesting applications, such as, for

22:09.480 --> 22:15.520
example, speech recognition or video recognition, everything where there are sequences.

22:15.520 --> 22:19.920
For example, in the video, you don't just have one single image, which you want to classify.

22:19.920 --> 22:24.000
No, you have a whole stream of images, and every few milliseconds, a new image is coming

22:24.000 --> 22:30.720
in, which means your network somehow has to memorize what it saw before in order to make

22:30.720 --> 22:36.800
sense of the current input within the temporal context of before.

22:36.800 --> 22:43.720
So it has to learn to memorize the important stuff, and to ignore the unimportant noise.

22:43.720 --> 22:50.840
And this proved to be very challenging, and people then already in the 80s realized that

22:50.840 --> 22:56.440
the first attempts at recur networks didn't really work well.

22:56.440 --> 23:00.560
And what specifically didn't work well about them?

23:00.560 --> 23:07.560
They especially failed when there were long time lags between relevant input events.

23:07.560 --> 23:14.840
For example, in speech recognition, suppose somebody says 11, and another guy says 7.

23:14.840 --> 23:17.440
And then the end of that is always 7.

23:17.440 --> 23:24.440
So to see the difference, you have to memorize that he says 50 times steps ago, because

23:24.440 --> 23:30.240
Evan by itself already consumes 50 times steps more or less, because roughly every 10

23:30.240 --> 23:33.440
milliseconds, a new input vector is coming from the microphone.

23:33.440 --> 23:36.480
So you have to look back 50 steps.

23:36.480 --> 23:41.200
And in many other applications, for example, as you are listening to me now, to make sense

23:41.200 --> 23:44.160
of what I'm saying, you have to look much further back in time.

23:44.160 --> 23:49.560
You have to think back of the beginning of our discussion to make sense of what I'm

23:49.560 --> 23:50.720
saying now.

23:50.720 --> 23:58.360
So hundreds of thousands and millions and maybe billions of steps, you have to look back

23:58.360 --> 24:01.760
to take into account the temporal context.

24:01.760 --> 24:06.080
And it turned out that these original networks are the 80s.

24:06.080 --> 24:13.320
They could look back only for five steps, six, seven, something like that.

24:13.320 --> 24:19.120
So it was completely insufficient for doing interesting things.

24:19.120 --> 24:24.200
And this is a problem that can be called the vanishing gradient problem.

24:24.200 --> 24:28.040
And then we try to figure out what is the reason for that.

24:28.040 --> 24:33.840
And my first student in 1991 was Seb Hochreiter.

24:33.840 --> 24:37.640
My first student, Evan, and his task was to figure out where's the problem?

24:37.640 --> 24:38.640
Were you here at the time?

24:38.640 --> 24:39.640
Or...

24:39.640 --> 24:41.640
And we were back then, not in Switzerland, we were in Munich.

24:41.640 --> 24:42.640
Okay.

24:42.640 --> 24:48.400
At the Tech University Munich, T. U. M端nchen.

24:48.400 --> 24:55.080
And Seb in his thesis, in his diploma thesis, showed that the problem is that these errors

24:55.080 --> 25:01.360
signals that you are getting through backpropagation in these, we can't do our networks, as you

25:01.360 --> 25:07.040
are propagating them backwards from the output layer towards the previous layer and then

25:07.040 --> 25:10.120
the layer before the previous layer and so on and so on.

25:10.120 --> 25:20.440
They get smaller and smaller in a way that is catastrophic in the sense that the shrinking

25:20.440 --> 25:23.520
happens in an exponential fashion.

25:23.520 --> 25:29.080
So as these signals which tell the system how to change its connections in order to become

25:29.080 --> 25:34.680
a better system, as these signals are being propagated back into time, so to speak, they

25:34.680 --> 25:42.880
are getting smaller and smaller in an exponential way or they explode in an exponential way.

25:42.880 --> 25:48.400
So either the gradients, for those who know what that is, they explode or they vanish and

25:48.400 --> 25:53.520
in both cases, these traditional recon networks cannot learn anything.

25:53.520 --> 25:54.520
So just...

25:54.520 --> 26:01.000
At least they cannot learn to look further back in time than say just a few time steps.

26:01.000 --> 26:02.000
Right.

26:02.000 --> 26:06.400
So just to try to paraphrase that to make sure that I'm understanding.

26:06.400 --> 26:12.160
So the way we solve these deep neural networks is to use backpropagation basically to start

26:12.160 --> 26:20.960
with the output and work our way back to the input layers and we do that using gradients

26:20.960 --> 26:29.760
which essentially, you know, let's say modulate the error that is propagated out to the output.

26:29.760 --> 26:35.360
But if you go back far enough in your network or if your network is too deep, then you're

26:35.360 --> 26:41.200
not getting enough signal back towards the beginning of your network to make the necessary

26:41.200 --> 26:42.680
corrections to improve.

26:42.680 --> 26:43.680
Yes.

26:43.680 --> 26:45.080
That basically what you found.

26:45.080 --> 26:46.080
Yes.

26:46.080 --> 26:48.840
Let's maybe quickly step back a few decades.

26:48.840 --> 26:57.360
In 1970 there was this finished guy, Seppel-Linainmar in Helsinki and he described for the first

26:57.360 --> 27:05.320
time the modern version of what is now called backpropagation, which is a method for adjusting

27:05.320 --> 27:13.160
a deep network of nodes, of computational nodes, such that you can figure out for each

27:13.160 --> 27:19.840
of these nodes and for each of these connections how much did this connection contribute to the

27:19.840 --> 27:24.080
error that you observed at the output of the network.

27:24.080 --> 27:29.680
So at the output of the network, the network is computing some result which is different

27:29.680 --> 27:34.320
from what it should have computed and the difference is called the error.

27:34.320 --> 27:39.760
And now you want to figure out for each connection in your system how much did this particular

27:39.760 --> 27:46.440
connection contribute to this error out there and you want to change then the connection

27:46.440 --> 27:49.720
such that the total error gets smaller.

27:49.720 --> 27:55.560
And Seppel-Linainmar, this guy in Finland, who was a master student back then, he wrote

27:55.560 --> 28:02.720
down this technique of automatic differentiation or today it's called backpropagation.

28:02.720 --> 28:09.880
And then people started using that in the 80s for training neural networks.

28:09.880 --> 28:15.960
And then also generalized it to the case of recon neural networks where you have feedback

28:15.960 --> 28:24.040
connections and where you still have the same basic approach as you are trying to compute

28:24.040 --> 28:29.160
for each connection in a system how much did it contribute to the error at the output side

28:29.160 --> 28:31.320
at some later point in time.

28:31.320 --> 28:38.560
You are moving backwards from the output layer to watch the time step before the computation

28:38.560 --> 28:43.120
of what you find in the output layer and then there are certain things which are called

28:43.120 --> 28:49.440
error signals are being propagated back and then from there they go one step further back

28:49.440 --> 28:55.000
and then from there one step further back and so on until they basically traverse the

28:55.000 --> 29:00.680
entire network in a way that allows you to compute for each connection in the system

29:00.680 --> 29:04.160
how much was it responsible for the final error.

29:04.160 --> 29:10.560
And then through the work of Seppel-Heiter, my first student, 1991 in his diploma thesis

29:10.560 --> 29:18.400
it became clear that the gradients are vanishing these error signals that are being propagated

29:18.400 --> 29:27.920
back they are quickly vanishing which means that although the whole idea is nice in principle

29:27.920 --> 29:32.600
it doesn't work in practice because for a quickly you don't have any signal any longer

29:32.600 --> 29:38.080
which allows you to improve your network such that it becomes a better network.

29:38.080 --> 29:44.000
Where or when did gradient descent come into play in all this as a method for solving

29:44.000 --> 29:46.200
these types of networks.

29:46.200 --> 29:54.600
Seppel-Heiter in my 1970 formulated it as a very general method for all kinds of networks

29:54.600 --> 29:58.600
consisting of differentiable nodes and that paper where he talks about backpropagation

29:58.600 --> 30:04.360
was he also applying gradient descent and did he introduce that then as the method

30:04.360 --> 30:11.400
or gradient descent is a much older technique which goes back at least to Hadamard around

30:11.400 --> 30:13.560
1900 or something like that.

30:13.560 --> 30:16.840
So these are gradient descent is a very old technique.

30:16.840 --> 30:23.200
And then the main question is how do you efficiently compute gradients in complex systems such

30:23.200 --> 30:28.240
as these complex networks where one node is computing something that depends on the

30:28.240 --> 30:33.640
activations of other nodes and then it's broadcasting it's on result to all kinds

30:33.640 --> 30:38.840
of other computational nodes which again compute something you based on these broadcasts

30:38.840 --> 30:43.920
from all the other nodes and then send their results even further and so on.

30:43.920 --> 30:50.640
So once you have a very complex system like that then how do you do the credit assignment

30:50.640 --> 30:56.800
how do you figure out how much should you change this connection there which was active

30:56.800 --> 31:02.600
maybe 100 time steps ago which nevertheless had an impact on what happened 100 time

31:02.600 --> 31:05.360
steps later at the output side.

31:05.360 --> 31:10.840
So that credit assignment problem is essentially is how we find these weights or that's the

31:10.840 --> 31:13.920
problem of finding these the weights for your network.

31:13.920 --> 31:19.960
Yes gradient descent is a very general technique a super general concept from more than a

31:19.960 --> 31:27.800
century ago which can be used to credit a sign in complex systems like these recolonial

31:27.800 --> 31:33.000
networks the individual components of the networks which are the connections and the

31:33.000 --> 31:37.360
weights on these connections so each of these connections has a little number on it which

31:37.360 --> 31:42.840
says how strongly does this neuron over here influence this neuron over here at the next

31:42.840 --> 31:52.680
time step and the numbers maybe 0.2 or minus 0.5 or minus 6.2 and it's contributing to the

31:52.680 --> 31:58.520
error at the output side which was maybe observed a thousand steps later.

31:58.520 --> 32:04.280
And now the question is how can I trace back this influence and how can I then change

32:04.280 --> 32:08.880
this connection such that becomes a better connection and that my entire network becomes

32:08.880 --> 32:13.920
a better network such that the error gets smaller.

32:13.920 --> 32:19.040
And then the efficient way of computing these gradients through automatic differentiation

32:19.040 --> 32:25.720
or backpublication as it is called today that was introduced in 1970 by Sepulina Inma but

32:25.720 --> 32:30.920
then he praised that as a very general thing and he didn't apply it to neural networks

32:30.920 --> 32:38.040
that was then done by other people in particular Paul Werbers around 1982 when he really applied

32:38.040 --> 32:40.760
it to neural networks.

32:40.760 --> 32:47.440
And then in the 80s computers became faster and faster and by 1985 they were about let

32:47.440 --> 32:54.440
you see they were about 1000 times faster than when Linna Inma wrote this down and also

32:54.440 --> 32:59.320
many academic labs for the first time in the 80s started to have really computers to play

32:59.320 --> 33:05.440
around with desktop computers today everybody has that but has a smartphone instead but

33:05.440 --> 33:09.720
back then this was exciting new stuff and then people started playing around and for the

33:09.720 --> 33:17.360
first time with very small networks they learned interesting behavior on certain little

33:17.360 --> 33:19.680
toy problems.

33:19.680 --> 33:25.880
But we couldn't move past toy problems because of this vanishing grading issue which

33:25.880 --> 33:33.160
sub identified in his research paper which led to the creation of LSTM networks tell us

33:33.160 --> 33:39.160
about LSTM and how the creation of those fell out of the findings.

33:39.160 --> 33:40.160
Yeah.

33:40.160 --> 33:47.920
So the long short term memory as it is called the LSTM falls out of this finding because

33:47.920 --> 33:55.200
it basically overcomes the problem of vanishing gradients through a very very simple trick.

33:55.200 --> 34:02.080
All we have to do is have a very stupid simple unit a particular type of neuron in there

34:02.080 --> 34:08.720
which has the simplest activation function one can think of which is the identity function

34:08.720 --> 34:14.240
which is the identity function and then if that neuron is connected to itself by a connection

34:14.240 --> 34:24.600
of 1.0 then at the next time step it will basically replace what it had before as an activation

34:24.600 --> 34:31.560
number for example 0.7 by the same number 0.7 so now you can imagine that if you run

34:31.560 --> 34:35.320
that for a thousand time steps and nothing happens for a thousand time steps and all the

34:35.320 --> 34:39.160
time 0.7 will be stored in this unit.

34:39.160 --> 34:45.000
At the same time as you are later propagating error signals back because there was a difference

34:45.000 --> 34:49.000
between what the network should have done and what it really did.

34:49.000 --> 34:54.760
Then if you know something about gradients then you know that all the time as you are propagating

34:54.760 --> 35:01.320
errors backwards you have to multiply by the derivative of the activation function which

35:01.320 --> 35:07.000
is 1.0 because it is a very stupid activation function which is just the identity function

35:07.000 --> 35:10.920
and then you multiply by 1.0 again through these recurrent connection which also has a

35:10.920 --> 35:18.280
weight of 1.0 and then now it is obviously as long as there are no external perturbations

35:18.280 --> 35:23.800
in little stupid networks like that you can propagate errors back not only five steps

35:23.800 --> 35:28.760
or ten steps but hundreds and thousands and millions and billions of steps.

35:28.760 --> 35:34.440
This is not finished yet because with that simple type of network you have all the limitations

35:34.440 --> 35:42.320
that you get through linear systems because the identity function is just a linear function

35:42.320 --> 35:47.280
and there are many things you cannot learn by just linear functions.

35:47.280 --> 35:55.200
Now that's the reason why you have to surround the little tiny linear unit, the constant error

35:55.200 --> 36:01.840
carousel as we call it, you have to surround it by a cloud of very non-linear units which

36:01.840 --> 36:08.640
we call gates and these gates then basically learn through exploitation of the error signals

36:08.640 --> 36:14.080
which are being propagated back in these simple guys in the center of these LSTM cells

36:14.080 --> 36:20.000
they learn to adjust their own connections through gradient descent such that they open up

36:20.000 --> 36:24.720
at the right moment and let new stuff into these cells into these memory cells

36:24.720 --> 36:30.560
and they close down at other good moments such that the memory in there is protected for a while

36:30.560 --> 36:35.680
until they open up again and let it out such that can influence the rest of the network

36:35.680 --> 36:43.200
and so all of this is now being learned by these long short term memory networks, the LSTM networks

36:43.200 --> 36:51.040
and they are called long short term memory networks because they are basically inspired by what

36:51.040 --> 36:57.520
the biologists know as the short term memory memories of recent events circling around in

36:57.520 --> 37:04.960
form of circulating activations in your brain but it's a short term memory that lasts for a long time

37:05.760 --> 37:11.440
and that's why it's a long short term memory because it can last not only for five or six or

37:11.440 --> 37:17.040
ten steps like in the first original Recon neural networks but it can last forever basically

37:17.040 --> 37:26.080
so that's why it's a long short term memory and LSTM yeah now you said that this was the solution

37:26.080 --> 37:30.800
to everything however it's not quite true because we also had to wait for faster computers

37:31.840 --> 37:39.040
back then already there was an old trend which held at least since 1941 when Suzy built the first

37:39.040 --> 37:44.960
program controlled computer that it really were back then Suzy could do roughly one operation per

37:44.960 --> 37:54.160
second but since then every five years computing became roughly ten times cheaper and then by the

37:54.160 --> 38:01.200
time when Lina Inma for example the test thing that was thirty years later computing was already

38:01.200 --> 38:05.840
one million times faster but it wasn't good enough and then in the beginning of the 90s that

38:05.840 --> 38:11.920
was another twenty years later was roughly ten thousand times faster than during Lina Inma's time

38:11.920 --> 38:20.080
so it was roughly ten billion times faster than during Suzy's time is that correct 40 plus 90

38:20.720 --> 38:28.800
yeah seems correct but it was still not good enough to to be commercially viable but then by

38:28.800 --> 38:39.680
2010 roughly computers were cheap enough and the factor was high enough such that all the potential

38:39.680 --> 38:45.600
in the LSTM networks really could unfold itself and I think since 2015 it's

38:46.960 --> 38:54.240
widely used in commercial applications for example for the speech recognition on two billion

38:54.240 --> 39:02.160
android phones I'll maybe ask you to talk a little bit about the use cases the applications of

39:02.160 --> 39:09.840
LSTM's do you find most interesting but before we get to that has the fundamental technology changed

39:09.840 --> 39:17.120
much between 1995 and 2015 the fundamental insights were really from the previous millennium

39:17.920 --> 39:26.000
one has to say that however there came a couple of really nice improvements around 2000 my

39:26.000 --> 39:34.240
second LSTM PhD student whose name was Felix Gehrs he was the first author on a paper which

39:34.240 --> 39:42.080
introduced something called the Forgetgate which is a particular recurrent gate unit which turns

39:42.080 --> 39:50.240
out to be really useful for many applications where the network has to also learn to forget

39:50.240 --> 39:57.680
sometimes stuff that it has observed in the past to make room for new things and to do that in a

39:57.680 --> 40:05.520
very controlled fashion and so today the Vanilla LSTM as we call it is a little bit different

40:05.520 --> 40:14.000
from the original LSTM and there are a couple of topology evolutions that came in for example in 2009

40:14.000 --> 40:20.960
just in buyer another student of mine was the first author of a paper which showed that you can

40:20.960 --> 40:28.720
evolve through artificial evolution LSTM like architectures which have the basic concepts of LSTM

40:28.720 --> 40:35.760
in there but with which sometimes change a connection here and have a different topology there

40:35.760 --> 40:41.840
and so on that it was in 2009 and there it was possible to show that in certain applications

40:41.840 --> 40:47.760
this type of LSTM topology works better and faster and learns a little bit faster than this type

40:47.760 --> 40:55.760
of LSTM architecture so there has been a couple of improvements of this kind although the basic

40:55.760 --> 41:00.800
fundamental ingredients they did date back to the previous millennium so this this last thing you

41:00.800 --> 41:07.920
were describing the evolutionary LSTM if that's the right way to describe it the core insight

41:07.920 --> 41:14.880
there is basically you've got several different types of structures for LSTMs and you can

41:15.520 --> 41:21.200
based on the you know the problem you're solving or the objectives switch between them on the

41:21.200 --> 41:30.240
flyer or is it something else this is about the basic architecture of an LSTM like network

41:30.240 --> 41:38.880
and if you look at your own brain for example it's a product of evolution and all these little

41:38.880 --> 41:45.520
special types of architectures that we find in your neurons somehow they evolved over millions

41:45.520 --> 41:50.240
of years because some of them just work better and are better at learning certain things which

41:50.240 --> 41:56.720
are important for your survival than others and so the same thing happened there in our artificial

41:56.720 --> 42:04.480
evolution of topology is where you give the system the freedom to come up with new topologies

42:04.480 --> 42:10.560
which which deviate from the original LSTM architecture a little bit because we don't have a

42:10.560 --> 42:15.840
proof that a particular LSTM topology is optimal for all kinds of things okay there is no proof

42:15.840 --> 42:23.120
like that so we try to figure out maybe we can automatically find the optimum topology in a

42:23.120 --> 42:29.360
problem specific fashion because maybe for this problem over here you need a different topology

42:29.360 --> 42:36.800
is better than for this problem over here and so we were able to show them that there are indeed

42:36.800 --> 42:44.560
applications where you can profit from this additional optimization of the topology through evolution

42:44.560 --> 42:52.160
can you elaborate a little bit on the notion of topology what makes a network fundamentally LSTM

42:52.160 --> 42:59.680
and what are the things that can vary from one LSTM topology to another yes so the vanilla LSTM cell

43:00.640 --> 43:07.680
has four little neurons in there there is the central neuron which is the stupid one that I mentioned

43:07.680 --> 43:14.400
before which has just the identity function as an activation function then it is surrounded by

43:14.400 --> 43:21.680
three gates which are multiplicative so there is this thing called the input gate which is a normal

43:21.680 --> 43:27.040
standard neuron and non-linear standard neuron and if that one is active it can completely

43:27.920 --> 43:34.960
open the access to the central unit the constant error carousel or if it is shut down if it is

43:35.440 --> 43:43.520
zero then nothing can flow into that central cell and similar for the output gate

43:43.520 --> 43:53.200
and then this is recurrent gated unit the forget gate which basically can manipulate the self

43:53.200 --> 43:59.600
connection of the stupid cell of the cell with the linear activation function to itself and can

43:59.600 --> 44:05.600
make the self forget stuff that was stored there for a while and can learn to do that on

44:05.600 --> 44:12.160
now you can come up with all kinds of variants of that topology maybe you have a little connection

44:12.160 --> 44:21.040
going out directly from the central cell to these gates and these are called peephole connections

44:21.040 --> 44:28.400
that's what Felix Ghears also called them my second LSTM student in 2001 roughly 2000 something

44:28.400 --> 44:34.240
like that you can have additional modifications of the architecture and sometimes it's useful

44:34.880 --> 44:41.360
because as I said there is no proof that a particular LSTM topology is optimal so you might want

44:41.360 --> 44:49.120
to use the principles of evolution to search for good topologies automatically and to take the

44:49.120 --> 44:53.360
existing one which is pretty good for many applications vanilla LSTM is really good for many

44:53.360 --> 45:00.400
applications but even further improve it so that is the approach behind that okay you talked a

45:00.400 --> 45:07.360
little bit about this but if we can take a step back what was the kind of intuition for all of

45:07.360 --> 45:13.360
the you know for the LSTM cell architecture right at the point that that was developed that

45:14.320 --> 45:19.840
and correct me if I'm wrong here I think of traditional RNNs as being just a lot flatter whereas

45:19.840 --> 45:24.880
LSTMs have these cells that are doing all this funky stuff is that do you think about it that way I like

45:25.520 --> 45:31.360
yeah so in it is true that the traditional become networks are very straightforward it's just a bunch

45:31.360 --> 45:37.440
of nodes with non-linear activation functions and everything is connected to everything and at every

45:37.440 --> 45:43.840
time step each of these little nodes is computing essentially the weighted sum of all the connected

45:43.840 --> 45:50.400
guys at the previous time step super basic it's very simple and basic and beautiful also because

45:50.400 --> 45:56.560
everything that simple has a tendency to be beautiful except that it was not quite it was a little

45:56.560 --> 46:03.120
bit too simple so you need a little bit of additional structure to make sure that these memory

46:03.120 --> 46:10.880
effects hold now the LSTM is more complicated but it's also very simple because you can write it

46:10.880 --> 46:17.200
write it down in five lines of code five lines of pseudo code are sufficient to explain it so

46:17.200 --> 46:22.000
maybe a traditional recon network you can write it down in one line of pseudo code and this one needs

46:22.000 --> 46:27.200
five lines of pseudo code maybe something like that and still the principles are very simple

46:27.200 --> 46:33.600
because you have a very stupid simple linear cell at the heart of each of these LSTM cells and then

46:33.600 --> 46:40.320
you surround it by a cloud of non-linear gates such that these gates can learn to open access

46:40.960 --> 46:49.200
or close access to these memories and the network itself can learn to use these gates to

46:49.200 --> 46:58.080
put important stuff into short-term memory and ignore noise and so on yeah well as Einstein said

46:58.080 --> 47:08.240
a long time ago you should make things as simple as possible but not simpler yeah very good to what

47:08.240 --> 47:17.520
degree does the LSTM you know there are some related I think concepts that come up in deep learning

47:17.520 --> 47:23.600
like attention and things like that how does how does attention for example relate to LSTMs yeah

47:24.400 --> 47:30.000
so any recon your network of course already has something like internal attention

47:30.720 --> 47:36.800
because what it can do is essentially it can learn to direct internal spotlights of attention

47:36.800 --> 47:42.480
if you will to certain parts of the network we can say let's highlight this part of myself

47:42.480 --> 47:50.240
and let's ignore this part of myself so in a way the internal computation which is based on the

47:50.240 --> 47:57.680
program that is implemented on the recon network in form of its weight matrix this attention can be

47:57.680 --> 48:06.320
learned in a problem dependent fashion and so I think my first paper on that was in 1993 where

48:06.320 --> 48:14.640
basically a recon network learn to direct its internal focus of attention is is that the

48:14.640 --> 48:23.360
correct plural the plural of focus focus is it focus okay so the focus of attention and then could

48:23.360 --> 48:31.120
could use these highlighted internal patterns in hand like fashion such that it could associate

48:31.120 --> 48:38.560
through something called fast weights these internal attention highlighted patterns such that it

48:38.560 --> 48:44.320
could do certain things that you cannot do with traditional networks because it had these

48:44.320 --> 48:49.920
extra these extra fast weights which is actually a topic that has become really popular again

48:49.920 --> 48:57.760
very recently so attention is something that was implicit in many many recon networks already

48:57.760 --> 49:05.600
in the in the past and now it is it is waking up in very interesting commercial applications

49:05.600 --> 49:11.840
again speaking of commercial applications we talked about a few of the commercial applications

49:11.840 --> 49:20.080
of LSTMs are there others that come to mind for you that exemplify its you know power flexibility

49:20.080 --> 49:28.560
interest yeah so I think it's interesting to see that not only speech recognition but also

49:29.200 --> 49:35.680
the next higher level natural language processing can be done by the same architecture so with

49:35.680 --> 49:40.480
speech for example every 10 milliseconds a new vector of numbers maybe 14 numbers or something

49:40.480 --> 49:46.400
streaming into the system from the microphone with language it's quite different there you already

49:46.400 --> 49:54.080
have letters and words and so on and now on this higher level which is basically derived from this

49:54.080 --> 50:01.920
elementary level of speech you again can use LSTMs to for example understand text for example read

50:01.920 --> 50:08.480
some document and then try to make a short summary or classify that document maybe say the

50:08.480 --> 50:15.200
the document is a CV of a person and you want to compare to another document with which is a

50:15.200 --> 50:22.320
job advertisement and then you classify the document with respect to the job ad and you say okay

50:22.320 --> 50:27.680
this guy's a match or not machine translation maybe the most visible application is now what

50:27.680 --> 50:35.520
Google is doing in since 2016 since november 2016 Google translate is not based on the old

50:35.520 --> 50:41.440
system anymore but it has LSTM at its core and it has become much better than used to be

50:41.440 --> 50:50.160
and especially the most important language pair in the world which is English to Mandarin Chinese

50:50.160 --> 50:57.760
and back that is they're the performance is much better there was a time when the Chinese they

50:57.760 --> 51:05.440
laughed at the translations and they're not laughing any longer and then the same thing can be used

51:05.440 --> 51:12.800
for example with slight modifications to segment images which again seems like a totally different

51:12.800 --> 51:21.040
problem right the same thing can be used to well that seems again closer to natural language

51:21.040 --> 51:27.680
processing can be used to train chat parts so you have lots of chats between A and B and you

51:27.680 --> 51:34.240
then just train your LSTM to imitate B whenever it answers something to A but maybe you have lots

51:34.240 --> 51:41.280
of A's and B's and so it can learn from many many different chats and so it's only even relevant

51:41.280 --> 51:49.520
for this old idea of an AI test which is called the touring test which is about chatting with some

51:49.520 --> 51:58.160
other partner and the question is is he a human or is he a machine right and so let's see where

51:58.160 --> 52:04.880
that is going to end at some point then the same LSTM also can be used for all kinds of other

52:04.880 --> 52:12.000
sequences such as music composition one more people again interested in that Doug Eck in my lab

52:12.000 --> 52:18.320
about more than 10 years ago was the first who applied LSTM to music composition where the gold

52:18.320 --> 52:25.280
was to overcome the traditional neural composition neural music composition which existed back then

52:25.280 --> 52:34.480
already which was able to learn the basic harmonies but then if you listen to compositions they

52:35.120 --> 52:43.280
sounded like music like stuff that in principle for example when you train it on certain pieces by

52:43.280 --> 52:51.040
Bach which is a favorite of many of these neural network guys then even in the 80s and 90s it was

52:51.040 --> 52:58.080
possible by Mike Moser and Pete Todd and a couple of guys like that to learn the basic harmonies

52:58.080 --> 53:05.120
that you find in this type of music and then come up with randomized version there of compositions

53:05.120 --> 53:12.480
of the network so to speak which sound loss are like kind of this type of music except no there was

53:12.480 --> 53:19.280
no overarching structure so more like elevator music versions of Bach or something and then the

53:19.280 --> 53:25.600
gold was to also learn high level structure that you find in many songs like first section eight

53:25.600 --> 53:30.400
and there's another section eight and there comes B then there comes A again and then again B

53:30.400 --> 53:37.120
something like that yeah that was Doug Eck in in the early 2000s and it was one of your students

53:37.120 --> 53:42.240
yeah yeah yeah he was now at Google he is now at Google and he's running the what is called

53:42.240 --> 53:48.960
magenta magenta I just envied him last week okay he was a postark here actually oh wow yeah okay

53:48.960 --> 53:56.560
and he was the first author on these LSTM for music papers yes 2003 I think right yes wow

53:57.200 --> 54:06.640
great great so maybe as a way to to pull the conversation back to your broader research around general

54:06.640 --> 54:10.560
artificial intelligence or artificial general intelligence depending on your preference

54:10.560 --> 54:17.360
you did an AMA on Reddit not too long ago actually it was a couple of years ago maybe yeah

54:17.360 --> 54:22.400
yeah and one of the questions that I thought was pretty interesting was what are some

54:23.520 --> 54:28.720
beliefs that you hold that are controversial in essence that was the question and one of those

54:28.720 --> 54:35.840
was that intelligence is actually simple although we think of it as rather complex can you elaborate

54:35.840 --> 54:43.040
on that a little bit yes I think I can do that so LSTM by itself is nice for patent recognition

54:43.040 --> 54:50.080
and for doing really complex applications but we should see that it's just a patent recognizer

54:50.080 --> 54:56.240
so the full AGI thing requires more it requires reinforcement learning in an unknown

54:56.240 --> 55:01.520
partially observable environment where there's no teacher who tells you what to do and where you have

55:01.520 --> 55:08.080
to maximize your future expected reward okay and so whoever can solve that problem in a general

55:08.080 --> 55:14.480
fashion has solved the the grand problem of AI and that's why almost all of our research

55:14.480 --> 55:20.160
of the past 30 years really focused on that and so to us the LSTM which has now become popular

55:20.160 --> 55:24.960
is just a side effect that's just a byproduct of this more general work you can use the LSTM

55:24.960 --> 55:30.000
to build a model of the world to predict what's going to happen if you do that and that but you

55:30.000 --> 55:35.760
still need another module which is learning which is inventing new experiments and trying to

55:36.480 --> 55:44.160
figure out which sequences of actions lead to success and which don't and that leads to this

55:44.160 --> 55:48.960
field of reinforcement learning with general purpose recurrent architectures with recurrent

55:48.960 --> 55:55.120
works basically now if you look at the LSTM it's just five lines of pseudo code so it's very simple

55:55.120 --> 56:00.240
it's not the full true AI thing yet because there you have to have the full loop through the

56:00.240 --> 56:06.160
environment act perceive act perceive act perceive maximize future reward or reward until the end

56:06.160 --> 56:12.000
of your lifetime and for that I sometimes speculate we need another five lines

56:14.560 --> 56:21.120
and why am I so optimistic because we understand in many ways how to train the separate action

56:21.120 --> 56:27.040
module and how to combine that with a model of the world which can be used through the five lines

56:27.040 --> 56:33.280
of something like the LSTM or something that is in the same ballpark at least right and I'm

56:33.280 --> 56:41.040
further motivated to believe it's very simple because here in my lab in the early 2000s at least

56:41.040 --> 56:48.960
we already have found certain theoretically optimal journal problem solvers which are also very

56:48.960 --> 56:55.760
simple so there are many computer scientists actually don't or machine learning scientists don't

56:55.760 --> 57:02.880
even know that but there is an asymptotically optimal way of solving all kinds of computational

57:02.880 --> 57:12.080
problems and this goes back to Marcus Hutter in 2002 here in my lab he was a senior researcher

57:12.080 --> 57:18.800
now he's a professor in Australia and he wrote down a very simple method which solves any

57:19.840 --> 57:28.640
well defined problem as fast as the unknown fastest meta program for solving that kind of problem

57:29.520 --> 57:38.240
save for a constant factor or a for say for an additive constant which does not depend on the

57:38.240 --> 57:44.160
size of the problem so when you have for example a traveling salesman problem which is about

57:44.160 --> 57:50.960
finding the shortest path through n cities where each city can be visited only once then as the

57:50.960 --> 57:56.080
number of cities n grows the problem becomes larger and larger but it's still just a struggling

57:56.080 --> 58:00.800
salesman problem now we do not know the best way of solving traveling salesman problems but

58:00.800 --> 58:08.560
suppose you can solve it and suppose there's an unknown method which solves it in n to the 17 steps

58:08.560 --> 58:14.240
then this method of Marcus the fastest and shortest algorithm for all value fine problems as it is

58:14.240 --> 58:24.400
called will also solve it in n to the 17 steps plus a constant number of steps and overhead

58:24.400 --> 58:32.960
which however is constant and does not depend on n so as n grows into the 17 grows even much more

58:32.960 --> 58:40.960
crazily and the constant overhead vanishes which means that almost all problems are already solved

58:40.960 --> 58:46.400
in optimal fashion because almost all problems are large problems they're just a few problems that

58:46.400 --> 58:53.120
are so small that the overhead still plays a role just so I can make sure I understand what

58:53.120 --> 59:00.560
what he did and what you're saying is the idea that if a problem can be solved that basically

59:00.560 --> 59:07.440
it can be transformed into some other kind of solution modulo a constant so what I'm saying is

59:07.440 --> 59:14.160
you have a specification of a problem and then you want to find a program that solves the problem

59:14.160 --> 59:22.160
to approach that you want to build a general problem solver which finds that program automatically

59:23.440 --> 59:30.640
now there is a general problem solver which is the fastest and shortest way of solving all

59:31.360 --> 59:38.160
well defined problems by Marcus who published in 2002 here in my lab which essentially does that

59:38.160 --> 59:46.160
and it's optimal however only in an asymptotic sense what does that mean if you're problem for

59:46.160 --> 59:52.960
example you're traveling salesman problem with n cities can be solved and into the 17 steps

59:52.960 --> 01:00:00.640
then this super algorithm of Marcus is also going to solve it and into the 17 steps plus

01:00:00.640 --> 01:00:09.600
all of one as the computer scientists say and all of one means there is a constant overhead associated

01:00:09.600 --> 01:00:15.760
with it now the constant overhead is large it turns out because there's a proof search involved

01:00:15.760 --> 01:00:22.560
and we hide all the complexity of that in a constant which can be done no matter even if the

01:00:22.560 --> 01:00:28.160
constant is large then as n is getting larger and larger as the size of the problem is getting

01:00:28.160 --> 01:00:36.720
larger and larger the constant pales in comparison sure and that's the reason why you already have

01:00:36.720 --> 01:00:44.160
since the other millennium an optimal way a mathematically optimal way asymptotically optimal

01:00:44.160 --> 01:00:50.080
way of solving all kinds of problems especially the large problems which are so large that we

01:00:50.080 --> 01:00:56.800
can ignore the overhead the constant overhead does this result just say that we can asymptotically

01:00:56.800 --> 01:01:02.000
solve them it doesn't necessarily tell us how to do that no it says it is a constructive method

01:01:02.000 --> 01:01:10.480
it really tells us how to do that why I'm not doing that all the time because the small problems

01:01:10.480 --> 01:01:17.440
that we are looking at here on this planet in our daily lives they are so small that the the

01:01:17.440 --> 01:01:23.520
constant overhead does play a role and that's why we are still doing sub optimal things such as

01:01:23.520 --> 01:01:30.080
deep learning and all kinds of things you know but at least from a mathematical perspective

01:01:30.080 --> 01:01:34.560
we already know there is a mathematically optimal way of solving all kinds of problems

01:01:35.760 --> 01:01:41.280
in the fastest possible fashion the fastest possible in a non practical way but at least

01:01:41.280 --> 01:01:46.960
an asymptotically optimal way we already have that and it's very simple it's a very short thing

01:01:46.960 --> 01:01:55.280
so in that sense you gain additional motivation to believe that in the end the whole

01:01:55.280 --> 01:02:02.240
AI thing is going to be really simple and in hindsight we will look back and we'll say we can't

01:02:02.240 --> 01:02:07.360
believe that it took so many thousands of years to understand how to understand and how to solve

01:02:07.360 --> 01:02:14.800
problems automatically so this tells you that by saying that if we can get far enough along with

01:02:14.800 --> 01:02:22.400
compute power sophistication and approaches to overcome this constant then we're there like we

01:02:22.400 --> 01:02:28.800
can solve anything is that the idea not quite because we really have to do is try to find a

01:02:28.800 --> 01:02:36.160
similarly simple thing for the small problems too and one step in that direction is the so-called

01:02:36.160 --> 01:02:44.960
girdle machine which I published in 1993 in 2003 which which you can initialize with Markus's

01:02:44.960 --> 01:02:51.440
algorithm but also with other algorithms and which essentially learns to rewrite itself in an

01:02:51.440 --> 01:02:58.960
optimal fashion once it has found a proof that the rewrite is going to improve its performance

01:02:58.960 --> 01:03:07.520
in a way that is not only asymptotically optimal but generally optimal so however also this is

01:03:08.080 --> 01:03:15.280
not yet practical at the moment however getting out of that is that the very general

01:03:16.560 --> 01:03:21.840
problem solvers may be very simple and can be written down in a few lines of code

01:03:22.800 --> 01:03:28.160
we still don't have the few lines of code that we need for a practical general problem solver in

01:03:28.160 --> 01:03:33.840
this universe but I think we are close and the puzzle pieces are starting to fall in place

01:03:34.560 --> 01:03:42.160
and I hope I will see it in my lifetime awesome maybe in the next few years awesome I've really enjoyed

01:03:42.160 --> 01:03:47.920
the opportunity to talk with you I've certainly learned a lot for folks that want to learn more

01:03:47.920 --> 01:03:55.760
about what you're up to what's the best way for them to do so it's probably to look at my sprawling

01:03:55.760 --> 01:04:02.400
website which contains more information than you ever wanted to see about us but not only has

01:04:02.400 --> 01:04:09.440
the original papers but also overview pages which try to explain in rather simple terms

01:04:09.920 --> 01:04:18.000
where the experts might want to study in detail in the original papers great and we'll include

01:04:18.000 --> 01:04:24.000
a link to that in our show notes that will be so awesome great well thank you so much Erick

01:04:24.000 --> 01:04:28.640
it was my pleasure send great

01:04:30.240 --> 01:04:36.560
all right everyone that's our show for today thanks so much for listening and for your continued

01:04:36.560 --> 01:04:42.240
feedback and support for the notes for this episode including links to you again and the many

01:04:42.240 --> 01:04:50.320
resources mentioned in the show head on over to twimmalei.com slash talk slash 44 please be

01:04:50.320 --> 01:04:55.760
sure to comment there with your feedback or questions also please note if you share your

01:04:55.760 --> 01:05:02.080
favorite quote with us via a comment or via twitter we'll send you one of our fab laptop stickers

01:05:02.880 --> 01:05:08.480
i'll be in san francisco september 18th through 20th for the artificial intelligence conference

01:05:08.480 --> 01:05:14.240
and i hope to see you there too if you've already registered send me a shout on twitter and let me

01:05:14.240 --> 01:05:19.440
know if you're still interested in coming but haven't registered yet we've got a link

01:05:19.440 --> 01:05:26.000
and discount code on the show notes page good for 20% off most conference packages the following

01:05:26.000 --> 01:05:31.920
week i'll be at strange loop a great conference held each year right here in st louis strange loop

01:05:31.920 --> 01:05:37.280
is a multi disciplinary conference that brings together the developers and thinkers building

01:05:37.280 --> 01:05:44.240
tomorrow's technology and fields such as emerging languages alternative databases concurrency

01:05:44.240 --> 01:05:50.240
distributed systems security and the web will link to the conference on the show notes page as well

01:05:50.800 --> 01:05:58.000
finally another huge thanks to this show sponsor cloudara for more information on their data

01:05:58.000 --> 01:06:05.760
science workbench for the schedule your demo and receive your drone visit twimmalei.com slash cloud

01:06:05.760 --> 01:06:17.280
dera thanks again for listening and catch you next time

