1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,280
I'm your host Sam Charrington.

4
00:00:31,280 --> 00:00:36,040
For this the final episode of our reinvent series were joined by Thorsten Joachim's professor

5
00:00:36,040 --> 00:00:39,640
in the Department of Computer Science at Cornell University.

6
00:00:39,640 --> 00:00:44,400
Thorsten participated in the conference's AI Summit, presenting his research on unbiased

7
00:00:44,400 --> 00:00:47,120
learning from biased user feedback.

8
00:00:47,120 --> 00:00:51,680
In our conversation we take a look at some of the various ways that inherent biases

9
00:00:51,680 --> 00:00:55,760
are introduced in recommender systems and how to avoid them.

10
00:00:55,760 --> 00:00:59,760
We discuss how inference techniques can be used to make learning algorithms robust to these

11
00:00:59,760 --> 00:01:05,640
types of biases and how they can be enabled with the right types of logging policies.

12
00:01:05,640 --> 00:01:09,880
Don't forget, next week I'm in Seattle at CubeCon and I'd love to connect with any

13
00:01:09,880 --> 00:01:12,920
listeners in the area or in attendance.

14
00:01:12,920 --> 00:01:19,040
Take me up on Twitter at Sam Charrington or via email or the Twimble AI website.

15
00:01:19,040 --> 00:01:24,000
And now on to the show.

16
00:01:24,000 --> 00:01:27,600
Alright everyone, I am on the line with Thorsten Joachim's.

17
00:01:27,600 --> 00:01:32,360
Thorsten is professor in the Department of Computer Science at Cornell University.

18
00:01:32,360 --> 00:01:35,800
Thorsten, welcome to this week in machine learning and AI.

19
00:01:35,800 --> 00:01:36,800
Thank you for having me.

20
00:01:36,800 --> 00:01:37,800
It's a pleasure.

21
00:01:37,800 --> 00:01:38,800
Absolutely.

22
00:01:38,800 --> 00:01:45,520
So, before we dive in, I'd love for you to share a little bit about your background

23
00:01:45,520 --> 00:01:52,640
and how you got interested and involved in machine learning and artificial intelligence.

24
00:01:52,640 --> 00:02:00,840
I was actually quite accidental how I got to machine learning and how all of the started.

25
00:02:00,840 --> 00:02:07,320
I got my degrees at the University of Dortmund in Germany and there was exactly one professor

26
00:02:07,320 --> 00:02:11,920
who did artificial intelligence and that was always something that fascinated me.

27
00:02:11,920 --> 00:02:16,400
And that was Katarina Morik and she did machine learning and so that's how I got into machine

28
00:02:16,400 --> 00:02:17,400
learning.

29
00:02:17,400 --> 00:02:23,440
But I very quickly realized that within AI, this is the, I think one of the most exciting

30
00:02:23,440 --> 00:02:32,360
areas to be in because it really enables AI in a way that we can't really foresee and

31
00:02:32,360 --> 00:02:37,200
that creates interesting new results that we just, I mean, that we haven't programmed

32
00:02:37,200 --> 00:02:38,200
in our systems.

33
00:02:38,200 --> 00:02:41,880
So, I think this idea of learning from data is just fascinating.

34
00:02:41,880 --> 00:02:42,880
Absolutely.

35
00:02:42,880 --> 00:02:43,880
Absolutely.

36
00:02:43,880 --> 00:02:48,880
I think you have many in the listening audience that will agree with you.

37
00:02:48,880 --> 00:02:53,040
And so, you were here in Las Vegas.

38
00:02:53,040 --> 00:02:54,440
I'm still in Las Vegas.

39
00:02:54,440 --> 00:02:56,280
You are back at Cornell.

40
00:02:56,280 --> 00:03:04,560
You were here in Las Vegas presenting at the AI Summit at the Reinvent Conference.

41
00:03:04,560 --> 00:03:06,400
What were you presenting on there?

42
00:03:06,400 --> 00:03:07,760
Yeah, that's right.

43
00:03:07,760 --> 00:03:15,760
So much of my work, actually both as a professor here at Cornell, as well as an Amazon

44
00:03:15,760 --> 00:03:21,920
Scholar working in Amazon Music, has to do with learning from lock data, from interaction

45
00:03:21,920 --> 00:03:22,920
locks.

46
00:03:22,920 --> 00:03:29,920
And what I mean by this is the kind of data that we observe in our systems, kind of as

47
00:03:29,920 --> 00:03:32,240
a side product of people using the system.

48
00:03:32,240 --> 00:03:37,440
So it's like the locks of a search engine where you can see what people query and where

49
00:03:37,440 --> 00:03:43,720
they click or recommender systems where, you know, we make recommendations and we see

50
00:03:43,720 --> 00:03:50,120
whether people follow this recommendation, e-commerce systems, but also now reaching

51
00:03:50,120 --> 00:03:55,960
more and more out into the physical world, like smart homes and self-driving cars.

52
00:03:55,960 --> 00:04:01,960
Any time where people use the system and we receive feedback, I think this is one of

53
00:04:01,960 --> 00:04:07,440
the most plentiful data that we have and it's certainly, I mean, it reveals the choices

54
00:04:07,440 --> 00:04:11,560
that people make and that reveals a lot of knowledge about the world.

55
00:04:11,560 --> 00:04:16,960
But at the same time, it's data that's actually pretty difficult to analyze because it's

56
00:04:16,960 --> 00:04:19,680
biased in multiple ways.

57
00:04:19,680 --> 00:04:25,320
And dealing with these biases, I think, I mean, that's what my talk was about and that's

58
00:04:25,320 --> 00:04:27,200
what much of my research is also about.

59
00:04:27,200 --> 00:04:30,320
How do we learn from this data and how do we de-bias it?

60
00:04:30,320 --> 00:04:31,320
Okay.

61
00:04:31,320 --> 00:04:38,360
I can give you a simple example of what I mean by bias here and bias has many different

62
00:04:38,360 --> 00:04:42,960
meanings and actually many of these different meanings apply, but let me be very specific

63
00:04:42,960 --> 00:04:44,800
about one of the meanings.

64
00:04:44,800 --> 00:04:50,400
So think about a movie recommendation system.

65
00:04:50,400 --> 00:04:57,640
Netflix released this big data set, now many years ago, actually, at this point.

66
00:04:57,640 --> 00:05:03,960
And if you look at that data set and look at the average star rating that people give,

67
00:05:03,960 --> 00:05:07,280
you're kind of in the four star region.

68
00:05:07,280 --> 00:05:14,240
And if you think about it, well, I mean, that can be reflective of the kind of average

69
00:05:14,240 --> 00:05:19,240
rating that people would give an Amazon, a Netflix movie, right?

70
00:05:19,240 --> 00:05:26,200
So if I actually drew a random movie from the Netflix catalog and had a random user, a

71
00:05:26,200 --> 00:05:31,080
uniformed random, rate that movie, that wouldn't be a four star movie, probably, right?

72
00:05:31,080 --> 00:05:32,800
It would be much lower.

73
00:05:32,800 --> 00:05:38,520
So if I look at the ratings that people provide, they're actually self-selected, right?

74
00:05:38,520 --> 00:05:39,520
They're biased.

75
00:05:39,520 --> 00:05:42,040
People rate the movies that they watch.

76
00:05:42,040 --> 00:05:45,480
And of course, they watch the movies that they think they will like.

77
00:05:45,480 --> 00:05:52,600
So if I look at the revealed ratings that people give, then that's a biased sample of

78
00:05:52,600 --> 00:05:55,720
all the ratings that they would probably want to give.

79
00:05:55,720 --> 00:06:03,960
Now that's a problem because if I don't account for this, I can be really far off in evaluating

80
00:06:03,960 --> 00:06:08,600
how good my movie recommendation will do, a system will do.

81
00:06:08,600 --> 00:06:14,200
And in particular, if I now think about what a recommendation system should actually do,

82
00:06:14,200 --> 00:06:20,440
is it should actually change the distribution of ratings that the user gives, right?

83
00:06:20,440 --> 00:06:26,280
What the user wants from a recommendation system is interesting new choices.

84
00:06:26,280 --> 00:06:33,360
So if I now take all data that I have and then train a new recommendation system from

85
00:06:33,360 --> 00:06:38,960
it, deploy that, the data that I'm gathering now actually will have a different distribution,

86
00:06:38,960 --> 00:06:39,960
right?

87
00:06:39,960 --> 00:06:44,440
It will be biased by the recommendations that the new system makes.

88
00:06:44,440 --> 00:06:52,520
And so what I really want is, I have to account for the fact that if I train a system and

89
00:06:52,520 --> 00:06:57,360
then field it, I'm actually intervening in the world and I'm changing the distribution

90
00:06:57,360 --> 00:06:58,360
of my data.

91
00:06:58,360 --> 00:07:04,840
I always have these kind of counterfactual problems that I need to solve of what will happen

92
00:07:04,840 --> 00:07:07,240
if I change my system.

93
00:07:07,240 --> 00:07:11,840
And that's what makes this learning from log data hard because what I observe is the behavior

94
00:07:11,840 --> 00:07:17,400
on my old system, but what I really want to optimize is the behavior or the benefit that

95
00:07:17,400 --> 00:07:19,840
people draw from my new system.

96
00:07:19,840 --> 00:07:22,200
But for that, I don't have data yet.

97
00:07:22,200 --> 00:07:31,200
So what I talked about in the talk is basically how to deal with these kind of bias and shift

98
00:07:31,200 --> 00:07:32,200
problems.

99
00:07:32,200 --> 00:07:34,200
This is a super interesting topic.

100
00:07:34,200 --> 00:07:38,760
I remember a conversation that I had with someone on the podcast.

101
00:07:38,760 --> 00:07:44,280
I don't remember how it came up or who it was, but we were talking about this issue of

102
00:07:44,280 --> 00:07:48,560
bias and user rankings and user feedback.

103
00:07:48,560 --> 00:07:54,480
And one of the comments that I remember making was that, you know, you kind of describing

104
00:07:54,480 --> 00:08:02,160
this almost bias at the level of the data set, but there's also, you know, individual bias.

105
00:08:02,160 --> 00:08:08,520
Like some, you know, I may be a, you know, my kind of average movies are three, but for

106
00:08:08,520 --> 00:08:11,760
some people it's a two and for other people it's a four.

107
00:08:11,760 --> 00:08:16,040
And I remember asking the person in this context that they've heard of kind of research

108
00:08:16,040 --> 00:08:20,840
into how to deal with all of that and we didn't come up with anything.

109
00:08:20,840 --> 00:08:24,320
So I mean, I imagine these are issues that you're very familiar with.

110
00:08:24,320 --> 00:08:25,320
Yeah.

111
00:08:25,320 --> 00:08:26,320
I mean, right.

112
00:08:26,320 --> 00:08:32,880
There's, there's, as I said, bias has many different meanings in this context.

113
00:08:32,880 --> 00:08:35,960
What I've been talking about is selection bias, really.

114
00:08:35,960 --> 00:08:41,160
There's also this kind of, you could call this, you know, that people use a rating scale

115
00:08:41,160 --> 00:08:43,080
in a different way.

116
00:08:43,080 --> 00:08:45,280
And we've actually done research on that as well.

117
00:08:45,280 --> 00:08:53,920
So in particular, that comes up when you do things like peer grading or peer reviewing.

118
00:08:53,920 --> 00:08:55,160
And so that's right.

119
00:08:55,160 --> 00:09:03,080
So one reviewer may use, you know, a scale from one to 10, like, you know, mostly give

120
00:09:03,080 --> 00:09:05,520
like high ratings or low ratings.

121
00:09:05,520 --> 00:09:09,360
Another user may, you know, kind of anchor the scale at some other place.

122
00:09:09,360 --> 00:09:10,360
Yeah.

123
00:09:10,360 --> 00:09:12,800
We've actually done work on that problem as well.

124
00:09:12,800 --> 00:09:15,960
Although that's that's somewhat different.

125
00:09:15,960 --> 00:09:24,240
And we've actually deployed this at the KDD conference in 2015 when I was one of the program

126
00:09:24,240 --> 00:09:30,000
co-chairs and trying to actually de-buy as reviewer ratings to come at kind of fairer decisions

127
00:09:30,000 --> 00:09:32,520
about which papers to accept.

128
00:09:32,520 --> 00:09:35,880
But that's, that's slightly different this problem.

129
00:09:35,880 --> 00:09:43,600
And when I'm talking about here is really more that it's about where we get data.

130
00:09:43,600 --> 00:09:48,640
And you know, that there's a selection of, you know, what ratings we observe and which

131
00:09:48,640 --> 00:09:49,640
ones we don't.

132
00:09:49,640 --> 00:09:54,400
So the extreme case, I mean, this comes up basically in all online systems.

133
00:09:54,400 --> 00:10:00,160
So and some of the extreme cases are, for example, an ad placement system.

134
00:10:00,160 --> 00:10:03,560
And there it's really very clear.

135
00:10:03,560 --> 00:10:08,880
So if I think about an ad placement system, let's say display advertising, then I have

136
00:10:08,880 --> 00:10:13,280
my current system that's displaying ads.

137
00:10:13,280 --> 00:10:17,800
And so a new user comes in to a particular page.

138
00:10:17,800 --> 00:10:21,920
My current system now selects an ad to display.

139
00:10:21,920 --> 00:10:27,120
And then for that particular ad, I get to see whether the user clicks on it or not.

140
00:10:27,120 --> 00:10:32,480
But I don't get to see what would have happened if the system had presented a different ad.

141
00:10:32,480 --> 00:10:39,000
So here I very much have a selection bias that the current system in production influences

142
00:10:39,000 --> 00:10:46,560
where I observe feedback or think about a search engine where your current ranking function,

143
00:10:46,560 --> 00:10:50,880
you know, in response to a particular query, a potential ranking.

144
00:10:50,880 --> 00:10:55,360
And that really puts a strong selection bias where I get clicks, I'm going to get clicks

145
00:10:55,360 --> 00:10:58,320
on the, you know, top few results.

146
00:10:58,320 --> 00:11:03,320
You know, it's very unlikely that anybody is going to go to position 100 and reveal

147
00:11:03,320 --> 00:11:07,720
any of the kind of, you know, whether something was relevant there.

148
00:11:07,720 --> 00:11:13,600
So again, here we have that the current system that's in production really biases where

149
00:11:13,600 --> 00:11:15,480
I get clicks.

150
00:11:15,480 --> 00:11:20,760
And now if I use or where I get feedback and now if I use this data kind of naively to

151
00:11:20,760 --> 00:11:26,680
do learning from what I'm basically just rediscovering is, you know, whatever my old system

152
00:11:26,680 --> 00:11:28,400
did.

153
00:11:28,400 --> 00:11:37,760
And so what we've been doing is to rethink how to use this data from actually from a perspective

154
00:11:37,760 --> 00:11:40,040
of causal inference.

155
00:11:40,040 --> 00:11:46,040
So let's let's take the ad placement system as the canonical example here, but it actually

156
00:11:46,040 --> 00:11:48,560
applies to all of these systems.

157
00:11:48,560 --> 00:11:54,560
Then the way that you can think about this is really not as a prediction problem, but

158
00:11:54,560 --> 00:12:00,200
much more as a problem of, you know, just like what you have in like a medical setting

159
00:12:00,200 --> 00:12:03,120
of applying a treatment.

160
00:12:03,120 --> 00:12:11,960
So think, you know, if you are, if you want to come up with a personalized treatment

161
00:12:11,960 --> 00:12:17,640
policy for a particular person, then you may have, let's say some lab measurements for

162
00:12:17,640 --> 00:12:23,440
that person, then the doctor decides, you know, to give that person, you know, drug

163
00:12:23,440 --> 00:12:28,440
A or drug B or the surgery, that's the treatment.

164
00:12:28,440 --> 00:12:32,880
And then you get to observe for that particular treatment that was chosen, whether the person

165
00:12:32,880 --> 00:12:35,440
gets better or not.

166
00:12:35,440 --> 00:12:40,120
And it's really the same setup that we have in our online systems as well, like think

167
00:12:40,120 --> 00:12:41,120
about ad placement, right?

168
00:12:41,120 --> 00:12:46,120
The user comes in, we have some idea, you know, about the user profile.

169
00:12:46,120 --> 00:12:51,000
Then we take an action, we apply a treatment, that's a particular ad that we place.

170
00:12:51,000 --> 00:12:54,760
And then we see the outcome for that particular ad that was chosen.

171
00:12:54,760 --> 00:12:58,880
But we don't get to see what would have happened if I had applied any of the other treatments,

172
00:12:58,880 --> 00:13:00,800
any of the other ads.

173
00:13:00,800 --> 00:13:05,400
And I can make the same story for almost any online system as well.

174
00:13:05,400 --> 00:13:11,960
So it's really, you know, what we've been doing is we've rethought what it means to do

175
00:13:11,960 --> 00:13:17,560
learning and online systems kind of from the perspective of learning a policy that makes

176
00:13:17,560 --> 00:13:21,880
interventions and we want these interventions to have the desired effects in the world,

177
00:13:21,880 --> 00:13:22,880
right?

178
00:13:22,880 --> 00:13:26,640
That's what I meant when I said about the recommendation system, what I really want is

179
00:13:26,640 --> 00:13:32,000
I want to make recommendations to the user and I want to change behavior in a way that

180
00:13:32,000 --> 00:13:34,400
the user appreciates it.

181
00:13:34,400 --> 00:13:39,200
So it's really learning, it's learning to intervene, it's not learning to predict.

182
00:13:39,200 --> 00:13:43,800
And that has interesting implications for, for machine learning and it kind of puts

183
00:13:43,800 --> 00:13:48,840
it into relation to problems like covariate shift and really causal inference, right?

184
00:13:48,840 --> 00:13:56,520
We want to have a policy that makes interventions that cause some desired behavior in the world.

185
00:13:56,520 --> 00:14:05,360
And so is the approach related in some way to the notion of like A, B testing or multivariate

186
00:14:05,360 --> 00:14:11,280
testing where you're displaying multiple options to the user to give you in this case

187
00:14:11,280 --> 00:14:14,840
to give you kind of more insights into what you may have missed out on?

188
00:14:14,840 --> 00:14:16,120
Yeah, that's right on.

189
00:14:16,120 --> 00:14:21,440
I mean, the kind of gold standard for doing causal inference is a controlled randomized trial,

190
00:14:21,440 --> 00:14:22,440
right?

191
00:14:22,440 --> 00:14:23,440
That's what the user meant.

192
00:14:23,440 --> 00:14:27,480
And if you want to figure out whether drug A is rather than drug B, basically what we

193
00:14:27,480 --> 00:14:34,440
do is we give some fraction of the population or of our patient's drug A, randomized, another

194
00:14:34,440 --> 00:14:41,440
fraction gets, gets the other treatment and if we do the randomization correctly, then

195
00:14:41,440 --> 00:14:46,800
that's a strong evidence of the causal effect of the treatment, right?

196
00:14:46,800 --> 00:14:49,000
And randomization is the key here.

197
00:14:49,000 --> 00:14:51,240
It's easier than it sounds.

198
00:14:51,240 --> 00:14:55,360
It is actually a part of that it sounds easy.

199
00:14:55,360 --> 00:14:56,840
It sounds easy.

200
00:14:56,840 --> 00:14:57,840
It sounds easy.

201
00:14:57,840 --> 00:15:00,800
In practice, it can be quite hard.

202
00:15:00,800 --> 00:15:06,920
And basically what we're saying, I mean, what you're doing in an AB test and an online

203
00:15:06,920 --> 00:15:10,560
system is exactly a controlled randomized trial, right?

204
00:15:10,560 --> 00:15:15,120
Some fraction of your users gets, you know, a ranking function A or a recommended function

205
00:15:15,120 --> 00:15:16,120
A.

206
00:15:16,120 --> 00:15:18,120
The other one gets a recommended function B.

207
00:15:18,120 --> 00:15:20,960
And then you can compare which one works better.

208
00:15:20,960 --> 00:15:27,040
That's really the gold standard for causal inference, but it's also incredibly expensive.

209
00:15:27,040 --> 00:15:32,760
What you need to do is you need to code up that, you know, new ranking function or policy

210
00:15:32,760 --> 00:15:34,840
more generally.

211
00:15:34,840 --> 00:15:39,600
Then you need to productionize it, you need to test it, and then you need to field it

212
00:15:39,600 --> 00:15:43,880
on your system, you know, for at least a week because otherwise, you know, you have

213
00:15:43,880 --> 00:15:48,960
circular effects from the week and probably even longer to get reliable results.

214
00:15:48,960 --> 00:15:53,880
If you have a lot of different systems that you want to evaluate this way, it's going

215
00:15:53,880 --> 00:15:57,680
to take you forever, and your machine learning development cycle is going to be incredibly

216
00:15:57,680 --> 00:15:58,680
slow.

217
00:15:58,680 --> 00:16:00,760
Or if you don't have a lot of traffic.

218
00:16:00,760 --> 00:16:04,560
If you don't have a lot of traffic, right, I mean, basically the limiting factor is also

219
00:16:04,560 --> 00:16:05,560
traffic, right?

220
00:16:05,560 --> 00:16:11,440
It's developer's time for productionizing all of these policies, and then it's traffic.

221
00:16:11,440 --> 00:16:17,720
We already have, you know, probably, you know, terabytes of old log data, existing log

222
00:16:17,720 --> 00:16:19,640
data lying around.

223
00:16:19,640 --> 00:16:24,720
And if you think about it, online AB testing is actually really wasteful.

224
00:16:24,720 --> 00:16:30,600
We, you know, we put these policies into production, we collect the data, and then we never

225
00:16:30,600 --> 00:16:31,600
reuse that data.

226
00:16:31,600 --> 00:16:38,040
We use it exactly once, and then, you know, we don't actually know what to do with it.

227
00:16:38,040 --> 00:16:45,000
So the kind of the intriguing idea that we've pursued in over the last few years is whether

228
00:16:45,000 --> 00:16:51,520
you can actually avoid online AB tests, but instead do something like counterfactual or

229
00:16:51,520 --> 00:16:53,760
offline AB tests.

230
00:16:53,760 --> 00:16:59,760
And that's basically addressing the question of, here's a lot of old log data that we already

231
00:16:59,760 --> 00:17:00,760
have.

232
00:17:00,760 --> 00:17:05,200
And I'll, we'll have to qualify that this has to be somewhat special log data.

233
00:17:05,200 --> 00:17:08,680
I'll come back to that in a bit.

234
00:17:08,680 --> 00:17:12,560
But that we have a lot of log data lying around.

235
00:17:12,560 --> 00:17:17,240
And that we can now do, answer the following counterfactual question.

236
00:17:17,240 --> 00:17:22,840
If we have a new policy that we want to evaluate, and do causal inference on, that we asked

237
00:17:22,840 --> 00:17:29,480
the question, how well would that policy have done if we had used it instead of the policy

238
00:17:29,480 --> 00:17:31,720
that was actually running at the time.

239
00:17:31,720 --> 00:17:33,840
So it's this counterfactual question.

240
00:17:33,840 --> 00:17:38,560
But essentially it gets at the same, at the heart of causal inference again is, you know,

241
00:17:38,560 --> 00:17:40,480
how good is this policy?

242
00:17:40,480 --> 00:17:46,800
It can be used the existing log data to evaluate a new policy without ever having to field that

243
00:17:46,800 --> 00:17:49,520
new policy in a new AB test.

244
00:17:49,520 --> 00:17:55,000
And it turns out in certain conditions, it's actually possible to do this.

245
00:17:55,000 --> 00:18:01,440
And if you're able to reuse old log data, you can evaluate a new policy, or let's say

246
00:18:01,440 --> 00:18:05,880
a new ranking function when you recommend a policy in seconds.

247
00:18:05,880 --> 00:18:11,200
And you can do that for many, many new policies.

248
00:18:11,200 --> 00:18:17,320
And now, you know, you're basically now speeding up your development cycle or your evaluation

249
00:18:17,320 --> 00:18:23,920
cycle from weeks that it takes to do an online AB test to seconds that it takes to do one

250
00:18:23,920 --> 00:18:27,360
of these counterfactual or offline AB tests.

251
00:18:27,360 --> 00:18:32,080
So the question of course is, when is this possible?

252
00:18:32,080 --> 00:18:40,520
And it's possible to do this offline AB testing if the policy that you use to record your

253
00:18:40,520 --> 00:18:46,360
original data, you know, there's terabytes of log data that you already had, if that

254
00:18:46,360 --> 00:18:49,920
policy was sufficiently stochastic.

255
00:18:49,920 --> 00:18:53,640
So that gets us back to controlled randomized trials, right?

256
00:18:53,640 --> 00:19:01,920
There we also exploited that the assignment of people to or of subjects to conditions

257
00:19:01,920 --> 00:19:07,680
was random, but it turns out that it doesn't have to be uniformly at random.

258
00:19:07,680 --> 00:19:13,440
It doesn't have to be, you know, you flip a coin and assign people this way.

259
00:19:13,440 --> 00:19:22,120
Any form of randomness, even if it's like, you know, 95% to 5% is sufficient to even

260
00:19:22,120 --> 00:19:29,600
in hindsight, in retrospect, compute unbiased estimates of the performance of a new policy.

261
00:19:29,600 --> 00:19:35,800
So that leads us into questions or into techniques from causal infants like inverse propensity

262
00:19:35,800 --> 00:19:36,800
waiting.

263
00:19:36,800 --> 00:19:44,600
And what this basically means is that you take your existing log data, that was collected

264
00:19:44,600 --> 00:19:51,760
according to a stochastic policy, and you basically just re-weight distributions to your new

265
00:19:51,760 --> 00:19:52,760
policy.

266
00:19:52,760 --> 00:19:57,040
And if you do that in a particular way, you can actually prove that you can get unbiased

267
00:19:57,040 --> 00:20:03,400
estimates of the performance of a new policy, basically the same number that you would get

268
00:20:03,400 --> 00:20:09,840
in an online AB test, but just by reusing existing data under the right conditions.

269
00:20:09,840 --> 00:20:12,560
And if you can do that, then you can also do learning.

270
00:20:12,560 --> 00:20:13,560
Okay.

271
00:20:13,560 --> 00:20:17,840
You've mentioned a couple times the, you know, the right conditions and the right ways

272
00:20:17,840 --> 00:20:19,080
of doing these things.

273
00:20:19,080 --> 00:20:22,600
How restrictive are these conditions?

274
00:20:22,600 --> 00:20:29,240
If you want to evaluate a new policy, you can, you have to restrict that policy to actions

275
00:20:29,240 --> 00:20:33,720
that had non-zero probability of being selected in the past.

276
00:20:33,720 --> 00:20:35,360
And that's pretty intuitive.

277
00:20:35,360 --> 00:20:42,480
If you have a new policy that picks actions that you could not have possibly ever taken

278
00:20:42,480 --> 00:20:48,080
in the past, there's just no way to know whether that action is any good.

279
00:20:48,080 --> 00:20:57,800
So the basic condition is that you can only evaluate policies on the actions that, you

280
00:20:57,800 --> 00:21:03,080
know, you're logging policy that collected the data had basically non-zero probability

281
00:21:03,080 --> 00:21:04,080
of choosing.

282
00:21:04,080 --> 00:21:09,080
It doesn't mean that it needs to have chosen all of these possible actions, typically only

283
00:21:09,080 --> 00:21:15,040
a very small subset of it, but it had to have non-zero probability to do it.

284
00:21:15,040 --> 00:21:19,200
Because this condition derived from kind of like an information theoretic type of approach,

285
00:21:19,200 --> 00:21:28,400
like you need to have this information in sufficient quantities in your log store, you

286
00:21:28,400 --> 00:21:34,840
know, to practically do anything, or is it more, you know, from a probabilistic inference

287
00:21:34,840 --> 00:21:39,360
perspective and, you know, base theorem and dividing by zero?

288
00:21:39,360 --> 00:21:44,440
It's more actually, I mean, it's more something totally different.

289
00:21:44,440 --> 00:21:49,920
There is actually, you never really get a dividing by zero problem.

290
00:21:49,920 --> 00:21:50,920
You get it.

291
00:21:50,920 --> 00:21:52,080
So there's two problems.

292
00:21:52,080 --> 00:21:57,440
If you're trying to evaluate the policy that picks actions that weren't even available

293
00:21:57,440 --> 00:22:01,280
in the past, there's just, do you have no information about them?

294
00:22:01,280 --> 00:22:08,000
And so at that point, your estimate, your offline A-B test becomes biased.

295
00:22:08,000 --> 00:22:09,800
It's no longer unbiased.

296
00:22:09,800 --> 00:22:13,480
Just because, you know, you don't know what to do for these actions, so you're going

297
00:22:13,480 --> 00:22:18,200
to impute basically a zero there, or, you know, whatever you want to impute.

298
00:22:18,200 --> 00:22:22,560
And but that's kind of drawn out of the hat, and you don't know whether that's true.

299
00:22:22,560 --> 00:22:28,920
So if you have zero probability, then you get a biased estimate, but you can avoid it

300
00:22:28,920 --> 00:22:34,640
by restricting your new policy to only those actions that didn't have zero probability.

301
00:22:34,640 --> 00:22:43,280
The second issue is that, well, maybe that action had a tiny probability under when you

302
00:22:43,280 --> 00:22:45,120
locked the data.

303
00:22:45,120 --> 00:22:46,360
And then you're dividing.

304
00:22:46,360 --> 00:22:51,520
So if you do this inverse propensity waiting, you're basically dividing by that probability.

305
00:22:51,520 --> 00:22:56,640
You're dividing your estimate by this very small number, and that drives up the variance

306
00:22:56,640 --> 00:22:58,000
of your estimate.

307
00:22:58,000 --> 00:23:03,040
You're unbiased, but you get an estimate that has large variance, which is also not good.

308
00:23:03,040 --> 00:23:09,480
So there's kind of the fundamental issue of zero probabilities, and then you become biased.

309
00:23:09,480 --> 00:23:14,720
And then there's the practical issue of if you have very small probabilities of having

310
00:23:14,720 --> 00:23:20,800
chosen a particular action in your locks, then you become, you're still unbiased, but

311
00:23:20,800 --> 00:23:27,240
you have large variance, and which is also not, you would have need a lot of data to overcome

312
00:23:27,240 --> 00:23:28,240
that.

313
00:23:28,240 --> 00:23:29,240
Right.

314
00:23:29,240 --> 00:23:30,240
Right.

315
00:23:30,240 --> 00:23:37,920
And so from the perspective of someone who wants to try to use this, how complex is

316
00:23:37,920 --> 00:23:38,920
it?

317
00:23:38,920 --> 00:23:42,720
The mathematics behind it are actually really simple.

318
00:23:42,720 --> 00:23:50,720
In terms of designing of computation during this offline AB tests is easy.

319
00:23:50,720 --> 00:23:55,440
It's more, as you said before, you know, things like making sure that the assignment is

320
00:23:55,440 --> 00:24:02,960
random and computing or logging these propensities, conceptually, that can be quite tricky.

321
00:24:02,960 --> 00:24:06,040
And then there's the question of learning, right.

322
00:24:06,040 --> 00:24:12,880
Once you can do this offline evaluation, these offline AB tests, then basically you can

323
00:24:12,880 --> 00:24:19,600
take existing learning algorithms like, you know, things like a conditional random field

324
00:24:19,600 --> 00:24:21,880
or a deep network.

325
00:24:21,880 --> 00:24:27,160
And instead of training them with kind of hand labeled data, you can train them with

326
00:24:27,160 --> 00:24:32,720
log data because basically what all of these methods are doing is empirical risk minimization.

327
00:24:32,720 --> 00:24:36,880
So they would, they're minimizing training error, right.

328
00:24:36,880 --> 00:24:42,680
And training error is nothing like an unbiased estimate of your generalization error.

329
00:24:42,680 --> 00:24:50,080
And typically we use, you know, the fraction of errors as our unbiased estimate of the generalization

330
00:24:50,080 --> 00:24:51,080
error.

331
00:24:51,080 --> 00:24:53,280
And that's what we use as training error.

332
00:24:53,280 --> 00:24:59,560
But these inverse propensity score estimators are also unbiased estimators of my generalization

333
00:24:59,560 --> 00:25:05,080
error. So I can basically just substitute my normal training error for which I need hand

334
00:25:05,080 --> 00:25:11,880
labeled data with these IPS weighted types of training errors for which I can use log data

335
00:25:11,880 --> 00:25:13,880
to train them.

336
00:25:13,880 --> 00:25:20,720
So I can basically this way, I can repurpose many existing, as I said, like deep learning

337
00:25:20,720 --> 00:25:26,040
or other learning algorithms, just by training them according to a different objective.

338
00:25:26,040 --> 00:25:31,280
And that's something that we've been developing over the past few years as well.

339
00:25:31,280 --> 00:25:38,560
So how can we train now based on log data, all of these methods, instead of having to

340
00:25:38,560 --> 00:25:43,080
use hand labeled full information data?

341
00:25:43,080 --> 00:25:50,040
And so is it the case that I'm only able to use this technique going forward in the

342
00:25:50,040 --> 00:25:57,440
sense of, you know, once I start to generate my experiment in a way that's consistent with

343
00:25:57,440 --> 00:26:03,960
the conditions we've discussed, you know, then I have access to all of this log data,

344
00:26:03,960 --> 00:26:09,160
but, you know, likely if I've not been thinking about this previously, and I try to apply

345
00:26:09,160 --> 00:26:17,120
it, everything that I have is, you know, either kind of a mess, or I don't have enough

346
00:26:17,120 --> 00:26:24,160
applicable rigor, or my approach hasn't been systemized in a way that I can do this.

347
00:26:24,160 --> 00:26:31,080
Or can you always kind of select, you know, a subset of cases for which your old log

348
00:26:31,080 --> 00:26:33,880
data is useful, and then just work with that?

349
00:26:33,880 --> 00:26:34,880
Right.

350
00:26:34,880 --> 00:26:35,880
Yeah.

351
00:26:35,880 --> 00:26:41,120
So it's certainly easiest if you're logging your data already in mind that you want it

352
00:26:41,120 --> 00:26:42,680
to be stochastic.

353
00:26:42,680 --> 00:26:49,120
So for example, a simple way of doing this is if you're currently using some deterministic

354
00:26:49,120 --> 00:26:56,200
rule that maybe, you know, scores your candidates and then picks the candidate with the maximum

355
00:26:56,200 --> 00:26:57,200
score.

356
00:26:57,200 --> 00:27:01,840
That's, you know, how many ad placement systems, for example, work.

357
00:27:01,840 --> 00:27:10,680
That would be a deterministic blogging policy, and that would be difficult to use.

358
00:27:10,680 --> 00:27:15,320
But you can easily turn this into a stochastic logging policy by changing this arc max into

359
00:27:15,320 --> 00:27:16,640
a softmax.

360
00:27:16,640 --> 00:27:21,120
And then you would be logging new data where you know exactly, you know, the probability

361
00:27:21,120 --> 00:27:25,160
of choosing each of these individual actions.

362
00:27:25,160 --> 00:27:26,480
That's certainly the easiest.

363
00:27:26,480 --> 00:27:33,960
And you refer to this as a logging policy, it's not the logging itself that you're changing.

364
00:27:33,960 --> 00:27:38,680
It's kind of the underlying thing that you're doing, you know, displaying ads or recommendations

365
00:27:38,680 --> 00:27:42,680
or what have you, and you're just logging what happens.

366
00:27:42,680 --> 00:27:44,440
Am I interpreting that correctly?

367
00:27:44,440 --> 00:27:45,440
That's right.

368
00:27:45,440 --> 00:27:50,840
What I mean by logging policy is just whatever your system was, that was in operations when

369
00:27:50,840 --> 00:27:52,000
you created the logs.

370
00:27:52,000 --> 00:27:58,480
So it's the ranking function that you've used or, you know, the ad placement function or

371
00:27:58,480 --> 00:28:03,920
the recommender that was actually in production when you recorded the logs.

372
00:28:03,920 --> 00:28:04,920
Okay.

373
00:28:04,920 --> 00:28:09,840
So yeah, it's easiest if that is explicitly stochastic and you made it stochastic in a way

374
00:28:09,840 --> 00:28:15,440
that you can easily record all of the propensities and the probabilities of choosing the respective

375
00:28:15,440 --> 00:28:16,440
actions.

376
00:28:16,440 --> 00:28:17,840
That's great.

377
00:28:17,840 --> 00:28:23,720
But in many cases, I would argue that the data that you have, the log data that you have

378
00:28:23,720 --> 00:28:25,760
is already stochastic.

379
00:28:25,760 --> 00:28:32,760
So for example, if you've been running AB tests in the past, then the assignment of users

380
00:28:32,760 --> 00:28:38,280
to different conditions of your AB test, that was a random assignment already.

381
00:28:38,280 --> 00:28:45,000
So it's already stochastic in a sense that any particular user could have gotten multiple

382
00:28:45,000 --> 00:28:47,280
different versions of the system.

383
00:28:47,280 --> 00:28:52,280
And that's stochasticity that you already have and that you can exploit.

384
00:28:52,280 --> 00:29:00,080
Actually we've shown that in ranking settings where you have, let's say, a search engine,

385
00:29:00,080 --> 00:29:05,240
there you can, even if you have a deterministic ranking function, you can exploit that your

386
00:29:05,240 --> 00:29:07,280
users are stochastic.

387
00:29:07,280 --> 00:29:12,320
So in particular, some users will go down to position three, some will go down position

388
00:29:12,320 --> 00:29:15,840
five, some will go down to position 20.

389
00:29:15,840 --> 00:29:21,800
And this stochasticity is something that you can actually exploit, even though your system

390
00:29:21,800 --> 00:29:27,480
is deterministic, the logs are still stochastic because your users are stochastic.

391
00:29:27,480 --> 00:29:35,640
To do that, you would then need to estimate something like propensity curve based on position.

392
00:29:35,640 --> 00:29:39,560
But that's something we can do and we actually have an upcoming wisdom paper for how to do

393
00:29:39,560 --> 00:29:41,560
that very efficiently.

394
00:29:41,560 --> 00:29:49,800
It strikes me that there might be some implications on the effectiveness of this relative to whatever

395
00:29:49,800 --> 00:29:51,960
your distribution is.

396
00:29:51,960 --> 00:29:56,320
You kind of alluded to this earlier with the comments about the variants.

397
00:29:56,320 --> 00:30:05,720
If you've got a distribution that has a wide variance or many choices, for example, do

398
00:30:05,720 --> 00:30:13,360
all of these things come into play and your ability to learn and converge on a solution?

399
00:30:13,360 --> 00:30:14,360
Yeah.

400
00:30:14,360 --> 00:30:20,880
I mean, you're right on in a sense that if the system that logged the data is very different

401
00:30:20,880 --> 00:30:29,840
from the system that you're now evaluating effectively, we often talk about effective

402
00:30:29,840 --> 00:30:34,360
sample size in these settings when you do these counterfactual estimates.

403
00:30:34,360 --> 00:30:39,720
If these two systems are very far apart, then it's quite intuitive that whatever is in

404
00:30:39,720 --> 00:30:47,800
your log data doesn't tell you that much about the evaluation of this new system.

405
00:30:47,800 --> 00:30:52,680
If they're very different, the overlap between the two is very small.

406
00:30:52,680 --> 00:30:59,800
This will typically show itself in kind of having large variants in your offline AB tests.

407
00:30:59,800 --> 00:31:05,120
So really there is practical limitations about what you can evaluate and how reliably

408
00:31:05,120 --> 00:31:06,480
can evaluate.

409
00:31:06,480 --> 00:31:13,080
This works best if the new system that you want to evaluate is not that far away from

410
00:31:13,080 --> 00:31:17,480
the production system or from the logging policy that logged the data.

411
00:31:17,480 --> 00:31:24,840
And there are diagnostics to kind of see how reliable you are here.

412
00:31:24,840 --> 00:31:25,840
But you're right.

413
00:31:25,840 --> 00:31:30,960
If your new system does something completely different from the old system, there's

414
00:31:30,960 --> 00:31:33,200
just not enough information in the logs.

415
00:31:33,200 --> 00:31:37,720
But it would argue in many cases, you're making incremental improvements, you're changing

416
00:31:37,720 --> 00:31:41,720
your ranking function, you're changing a little bit, you're changing your ad placement

417
00:31:41,720 --> 00:31:43,480
function a little bit.

418
00:31:43,480 --> 00:31:49,640
So I'd say many of the kind of online AB tests that you're doing today could probably

419
00:31:49,640 --> 00:31:54,640
be replaced by these types of offline kind of actual AB tests.

420
00:31:54,640 --> 00:32:00,000
But then once in a while, you have to go out, collect new data probably.

421
00:32:00,000 --> 00:32:06,360
And I would also say that before making a big launch decision, I would just double check

422
00:32:06,360 --> 00:32:13,440
in an online AB test that really everything's fine and that there's nothing unforeseen

423
00:32:13,440 --> 00:32:14,880
is happening.

424
00:32:14,880 --> 00:32:21,200
But I think these offline AB tests are really interesting for speeding up your development

425
00:32:21,200 --> 00:32:26,440
cycle that you don't have to go out, get new data for every little decision that you're

426
00:32:26,440 --> 00:32:27,440
making.

427
00:32:27,440 --> 00:32:30,960
But you just need to kind of get a reality check once in a while.

428
00:32:30,960 --> 00:32:38,200
Does this impose a requirement on folks that are using this technique that they need to

429
00:32:38,200 --> 00:32:43,000
keep track of their distributions in some way, like in their logs in a way that they're

430
00:32:43,000 --> 00:32:45,000
probably not doing today?

431
00:32:45,000 --> 00:32:46,000
Yeah.

432
00:32:46,000 --> 00:32:52,240
So the biggest thing is probably that whenever you pick an action and you're logging this,

433
00:32:52,240 --> 00:32:59,480
that you also log what's called the propensity, which is the probability of taking that action

434
00:32:59,480 --> 00:33:01,440
under the current policy.

435
00:33:01,440 --> 00:33:06,920
So the logging policy that, as I said, ideally is stochastic.

436
00:33:06,920 --> 00:33:11,120
So it will pick actions from a distribution.

437
00:33:11,120 --> 00:33:16,160
And when it makes its choice, you just record that one number of what the probability of

438
00:33:16,160 --> 00:33:17,720
that choice was.

439
00:33:17,720 --> 00:33:20,400
And that's the most important thing to log.

440
00:33:20,400 --> 00:33:24,880
And with that number, you can, you know, that's the most important number for doing this

441
00:33:24,880 --> 00:33:26,920
in first propensity waiting.

442
00:33:26,920 --> 00:33:29,360
And it's really just one additional number that you need to log.

443
00:33:29,360 --> 00:33:33,560
It's not that you need to log the whole distribution or anything complicated.

444
00:33:33,560 --> 00:33:35,200
It's just this one number.

445
00:33:35,200 --> 00:33:43,520
It sounds like a really interesting technique, does it fit into a broader array of tools kind

446
00:33:43,520 --> 00:33:47,680
of based around a similar idea that folks should be looking into?

447
00:33:47,680 --> 00:33:48,680
Yeah.

448
00:33:48,680 --> 00:33:55,520
I mean, this idea of doing this counterfactual evaluation has been something that, you know,

449
00:33:55,520 --> 00:34:01,640
especially also people at Microsoft, but now much more broadly in it has been accepted

450
00:34:01,640 --> 00:34:08,320
in industry, you know, there were very interesting papers at the last Rex's conference on both

451
00:34:08,320 --> 00:34:18,600
from YouTube, implementing this kind of counterfactual learning techniques from Spotify.

452
00:34:18,600 --> 00:34:26,600
So I mean, this is an area that is maturing and where there's actually now a lot of interest

453
00:34:26,600 --> 00:34:34,480
both in academia and also in industry on how to design better estimators, how to design

454
00:34:34,480 --> 00:34:37,920
learning algorithms that are robust in this setting.

455
00:34:37,920 --> 00:34:41,160
Because arguably, really, that's where the data is, right?

456
00:34:41,160 --> 00:34:45,160
That's where we have lots and lots of training data.

457
00:34:45,160 --> 00:34:49,640
And it's much cheaper to work with this data than to get, you know, hand labeled data.

458
00:34:49,640 --> 00:34:50,840
Very interesting, Seth.

459
00:34:50,840 --> 00:34:55,800
Were there any other things that you covered in your talk at the conference?

460
00:34:55,800 --> 00:35:00,280
One thing that I would love to have covered more, but I didn't have the time is actually

461
00:35:00,280 --> 00:35:05,040
the, you know, the other meaning of, of bias.

462
00:35:05,040 --> 00:35:08,440
And that's the meaning in terms of fairness.

463
00:35:08,440 --> 00:35:13,640
Because I think actually all of these questions are quite related.

464
00:35:13,640 --> 00:35:20,680
So once you think about, you know, what your system does not as prediction, but as a policy

465
00:35:20,680 --> 00:35:26,760
that has effects in the real world and that has desirable effects and undesirable effects

466
00:35:26,760 --> 00:35:32,280
that you may want to optimize or minimize, then really thinking about, I mean, that is

467
00:35:32,280 --> 00:35:37,520
really the right vocabulary to think about fairness of your policy as well, right?

468
00:35:37,520 --> 00:35:42,840
And we've been thinking, I mean, that's actually been very interesting for me personally

469
00:35:42,840 --> 00:35:44,560
in terms of my research.

470
00:35:44,560 --> 00:35:52,240
And it's really enabled me to talk about fairness, let's say, of a search engine in a much more

471
00:35:52,240 --> 00:35:53,400
concise way.

472
00:35:53,400 --> 00:35:59,840
So for example, you know, if you think about what a search engine does, it's really a

473
00:35:59,840 --> 00:36:07,520
system that, you know, allocates attention among the items that it ranks.

474
00:36:07,520 --> 00:36:12,760
So from that perspective, you know, it's a policy and it's a policy that has an effect

475
00:36:12,760 --> 00:36:16,400
on both the users of the system.

476
00:36:16,400 --> 00:36:19,040
Those would be the people typing in the query.

477
00:36:19,040 --> 00:36:24,280
But it's also a policy that has an effect on the items that are being ranked, right?

478
00:36:24,280 --> 00:36:29,080
And in particular, in the way that this policy allocates exposure, you know, things that

479
00:36:29,080 --> 00:36:35,600
get ranked to position one, get more exposure than things that are ranked to position 10.

480
00:36:35,600 --> 00:36:42,480
So now, if you think about it that way, what we want is we want policies that allocate

481
00:36:42,480 --> 00:36:45,960
exposure in a way that is fair.

482
00:36:45,960 --> 00:36:51,760
And what's fair, it's both, it has to be fair both to the users as well as to the items

483
00:36:51,760 --> 00:36:52,760
being ranked.

484
00:36:52,760 --> 00:36:58,760
Because, you know, if you think about the settings where we use rankings today, the items,

485
00:36:58,760 --> 00:37:03,200
you know, could be people that are candidates for a job.

486
00:37:03,200 --> 00:37:09,680
And so we want to make sure that, you know, that we don't, for example, if they're protected

487
00:37:09,680 --> 00:37:16,320
groups among our job candidates, that we're allocating exposure in a fair way and we're

488
00:37:16,320 --> 00:37:20,720
not biasing how the system allocates exposure.

489
00:37:20,720 --> 00:37:26,280
What was actually, what's interesting is an information retrieval going back to the 70s

490
00:37:26,280 --> 00:37:35,040
coming out of library science, there was this very strong focus on maximizing the utility

491
00:37:35,040 --> 00:37:38,880
of the system to the people who type in the query.

492
00:37:38,880 --> 00:37:44,240
And that was fine when we were ranking books in the library because, you know, there it's

493
00:37:44,240 --> 00:37:49,360
really more mostly about the, you know, the people coming to the library wanting to find

494
00:37:49,360 --> 00:37:50,680
books.

495
00:37:50,680 --> 00:37:54,280
And the books really didn't have many rights or needed much protection.

496
00:37:54,280 --> 00:37:58,200
It was really a tool for finding what people wanted.

497
00:37:58,200 --> 00:38:03,040
But now when we're ranking, let's say job candidates or, you know, ranking romantic,

498
00:38:03,040 --> 00:38:07,360
potential romantic partners or, you know, we're ranking anything, it's really that we

499
00:38:07,360 --> 00:38:11,720
have to rethink what it means to design a ranking system.

500
00:38:11,720 --> 00:38:19,360
And this old principle of ranking by probability of relevance is actually not necessarily fair.

501
00:38:19,360 --> 00:38:25,840
I mean, we want to rank based on marriage and married equals relevance, but we can allocate

502
00:38:25,840 --> 00:38:28,360
exposure in many different ways.

503
00:38:28,360 --> 00:38:34,480
So current retrieval system basically are a winner takes all types of system wherever

504
00:38:34,480 --> 00:38:41,440
claims the first spot is, you know, getting that by far the most attention.

505
00:38:41,440 --> 00:38:47,400
But let's say for ranking job candidates, if we have two candidates that are, let's

506
00:38:47,400 --> 00:38:55,160
say we have 10 candidates that have almost the same qualifications for the job, one person

507
00:38:55,160 --> 00:39:00,080
is absolutely better than let's say the other people.

508
00:39:00,080 --> 00:39:06,520
Is it really fair that that epsilon better person gets far more than epsilon more attention

509
00:39:06,520 --> 00:39:07,840
than all the other 10?

510
00:39:07,840 --> 00:39:09,320
I'm not sure, right?

511
00:39:09,320 --> 00:39:15,080
So we may argue for any particular situation that we actually, we still want to allocate

512
00:39:15,080 --> 00:39:20,200
exposure based on relevance, but this kind of winner takes all is not maybe not the right

513
00:39:20,200 --> 00:39:21,200
way.

514
00:39:21,200 --> 00:39:25,960
Maybe we want to actually make exposure proportional to relevance, which would mean that

515
00:39:25,960 --> 00:39:30,000
all of these 10 candidates would get almost the same exposure, just this one person gets

516
00:39:30,000 --> 00:39:34,880
like a little bit of epsilon more, but it's not a winner takes all system anymore.

517
00:39:34,880 --> 00:39:40,400
So I think many of the techniques of debiasing data that I've talked about here for selection

518
00:39:40,400 --> 00:39:45,720
biases and propensity waiting are actually extremely useful for also dealing with fairness

519
00:39:45,720 --> 00:39:46,720
in this settings.

520
00:39:46,720 --> 00:39:48,960
And so I think there's an interesting connection there.

521
00:39:48,960 --> 00:39:49,960
Okay.

522
00:39:49,960 --> 00:39:56,320
Is that later interpretation something that you've published on today?

523
00:39:56,320 --> 00:40:00,400
Yeah, that we had a KDD paper at the last KDD conference.

524
00:40:00,400 --> 00:40:05,280
It sounds like really interesting work and I'm really thankful for you for taking the

525
00:40:05,280 --> 00:40:07,200
time to share it with us.

526
00:40:07,200 --> 00:40:08,600
Yeah, it was fun.

527
00:40:08,600 --> 00:40:09,600
Thank you.

528
00:40:09,600 --> 00:40:10,600
Absolutely.

529
00:40:10,600 --> 00:40:11,600
Thanks, Thorson.

530
00:40:11,600 --> 00:40:17,280
All right, everyone, that's our show for today.

531
00:40:17,280 --> 00:40:22,760
For more information on Thorson or any of the topics covered in this show, visit twimmalei.com

532
00:40:22,760 --> 00:40:25,880
slash talk slash 207.

533
00:40:25,880 --> 00:40:47,000
As always, thanks so much for listening and catch you next time.

