All right, everyone. I'm on the line with Kelly Revoir.
Kelly is an engineering manager at Stripe,
working on machine learning infrastructure.
Kelly, welcome to this week in machine learning and AI.
Thanks for having me. I'm really excited to chat.
Same here. Same here.
We got in touch with you,
kind of occasioned by a talk you're giving at
Strata, which is actually happening as we speak.
I'm not physically in SF for it this time, but your talk,
which is going to be later today,
is on scaling model training from flexible training APIs
to resource management with Kubernetes.
And of course, machine learning infrastructure and AI
platforms is a very popular topic here on the podcast.
And so I'm looking forward to digging into the way Stripe is
platforming its machine learning processes and operations.
But before we do that,
I'd love to hear a little bit about your background
and how you got started working in this space.
Yeah, sounds great.
Maybe I'll say a little bit about what I do now
and then kind of work backward from that.
Awesome.
So right now, I'm an engineering manager at Stripe,
and I work with our data infrastructure group,
which is seven teams kind of at the lowest level things like
our production databases or things like
Elasticsearch clusters and then kind of working up through batch
and streaming platforms, core ETL data pipelines and libraries,
and also machine learning infrastructure.
I've been at Stripe for very close to six years now
from when the company was about 50 people
and have basically worked on a bunch of different things
in sort of like risk data and machine learning,
both as an engineer and engineering manager
and also initially more on kind of like the application side
and then over time moving over to the infrastructure side.
By training, I'm like a kind of research scientist person,
so I studied physics and electrical engineering in school.
Did my PhD at Stanford working on nanopatonics
and then did a short postdoc at HP labs
on nanopatonics?
Yeah, I think you had,
like I could have an oscanon recently who works on optics,
which is not too far away,
so maybe that gives you a little bit of an idea.
And then yeah, I was at HP labs for a year
so working on sort of similar things
and also some 3D imaging.
And I guess I like to call what I did,
although I don't know that anyone else calls it that,
sort of like full stack science where like you have an idea
and then you do some theory or modeling or simulation
and then you use that to design a device
and then you actually go in the clean room
and like make the device and then you actually go
in the optics lab and like shoot a bunch of lasers
at your device and measure it.
And then you sort of like process the data
and compare it to your theory and simulation.
And I was like, I found like kind of the two ends the most,
like sort of the magical moment where like, you know,
the data that you collected like matches
what you thought was gonna happen from your modeling.
And I kind of decided that I wanted to do more of that
and a little less of like fabrication
or material science and I was kind of sitting
in Silicon Valley and started looking around
and like Stripe was super exciting in terms of its mission,
like having interesting data and just like having amazing people.
Awesome, awesome, Stripe sounds really interesting
but shooting lasers at stuff also sounds really, really cool.
Yeah, people get really excited when you tell them that.
So that was fun for a while.
Nice, nice.
And so maybe tell us a little bit about Stripe's,
kind of machine learning journey
from an infrastructure perspective.
How did it, it sounds like you're doing
a bunch of interesting things
both from a training perspective,
from a data management perspective inference,
but how did it evolve?
Yeah, I think one thing that's interesting
about machine learning at Stripe,
like I think a lot of places you talk to machine learning
kind of like started out as being for some,
some kind of like offline analytics
and more like internal business questions,
like maybe like you're trying to calculate
long-term value of your users.
And we do stuff like that now,
but we actually started like our kind of core uses
have always been very much on kind of the production side,
like our kind of most business critical
and first machine learning use cases were things
like scoring transactions in the charge flow
to evaluate whether they're fraudulent or not.
We're doing kind of like internal risk management
of like making sure our users are selling things
that we can support from our terms of service
or that they're kind of like good users
that we want to support.
And so we started out from having kind of a lot
of these more like production requirements
but it needs to be this fast
and it needs to be this reliable.
And I think our machine learning platform
kind of like evolved from that side
where initially we had kind of like one machine learning team
and then even just having a couple of applications
we started seeing like here are some commonalities
like everyone needs to be able to score models
or even like having some notion of shared features
could be really valuable across just a couple of applications.
And then as we split our machine learning team,
one piece of that became machine learning infrastructure
which we've developed since then.
And it's really important for that team to work
both with the teams doing the business applications
which now include a bunch of other things
in our user-facing products like radar and billing
as well as internally and also it's important
for the machine learning infrastructure
to build on the rest of your data infrastructure
and really the rest of all of your infrastructure.
And we've worked really closely
with like our orchestration team on,
as you said in chatting about my talk
like getting training to run on Kubernetes.
Yeah man, that's maybe an interesting place to start.
You kind of alluded to the interfaces
between machine learning infrastructure as a team
and you know data infrastructure, you know,
just infrastructure.
How do they connect, you know, maybe even organizationally
and how do they tend to work with them
with one another.
For example, you know, in, you know,
training on Kubernetes, you know,
where's the line between what the ML infrastructure team
is doing and, you know, what it's requiring
of some, you know, broader technology infrastructure group?
Yeah, I think the Kubernetes case is really interesting
and it's one that's been super successful for us.
So I guess maybe like a year or two ago,
we'd initially focused on the kind of scoring,
like real-time inference part of models,
because that's the hardest.
And we'd sort of left people on their own.
It's like, well, you figure out how to train a model
and then, you know, if you manage to do that,
we'll help you score it.
And we realized that that wasn't like great, right?
So we started thinking, you know, what can we do?
And at first, we built some CLI tools
to kind of like wrap the Python people were doing,
but then we wanted to kind of do more.
So eventually we built an API
and then a big hassle had been the resource management.
And we just kind of wanted to like abstract that all the way.
And as it happened at that time, our orchestration team
had gotten like really interested in Kubernetes.
And I think they wrote a blog post like maybe a year
and a half ago, they had kind of just
moved our first application to Kubernetes,
which was some of our conjobs that we
used in our financial infrastructure.
And so we ended up collaborating.
This was kind of like a great next step
of a second application they could work on.
And we had some details, we had to work out,
we had to figure out how do we package up all of our Python code
and to some Docker file we can deploy.
And it was really useful to be able to work with them on that.
But I think we have found really good interfaces
in working with them where we wrote a client
for the community's API, but it's like anytime we need help
or anytime there's management of the Kubernetes cluster,
they take care of all of that.
So it's kind of given us this flexibility
where we can define different instance and resource types
and swap them out really easily if we need CPUs or GPUs
or we need to expand the cluster.
But we as machine learning infrastructure
kind of don't have to deal with managing Kubernetes
or updating it, we have this amazing team
of people who are totally focused on that for Stripe.
Awesome, awesome.
And then actually let's maybe stay on this topic for a moment.
So your talk at Strato was focused on this area.
What was kind of the flow of your talk?
What were the main points that you're planning
to go through with the audience there?
Yeah, great question.
So we kind of think about this in two pieces.
And maybe that's because that's how we actually did it.
So one piece was the resource management
that I talked about was getting things to run on Kubernetes.
That was actually kind of the second piece for us.
The first piece was figuring out how should the user interact
with things and where should we give them flexibility
and where should we constrain things.
And so we ended up building what we call internally
rail yard, which is a model training API.
And it goes with, there's two pieces.
There's what you put in the API request.
And then there's what we call a workflow.
And the API request is a little bit more constrained.
You have to say you're meta data for who's training
so we can track it.
You have to tell us where your data is,
how you're doing things like hold out.
Just kind of basic things that you'll always need.
But then we have this workflow piece
that people can write whatever Python they want
as long as they define a train method in it
that will hand us back the fitted model.
And we definitely have found that initially,
we were very focused on binary classifiers for things
fraud, but people have done things like word embeddings.
We have people doing time series forecasting.
We're using things like psychic learn,
actually used fast text, PyTorch profit.
So this has worked pretty well in terms of providing enough
flexibility that people can do things that we actually
didn't anticipate originally, but it's constrained enough
that we can run it and sort of track what's going on.
And give them what they need and be able to automate the things
that we need to automate.
OK, and so your interface you're describing
is this kind of Python and this train method.
Are you expecting the, well, actually that's maybe a question.
Are the users, do you think of your users
as more kind of the data science type of user
or machine learning engineer type of user,
or is there a mix of those two types of backgrounds?
Yeah, it's a mix, which has been really interesting.
And I think coming back to what I said earlier,
because we initially focused on these critical production
use cases, we started out where the team's users were really
pretty much all machine learning engineers
and very highly skilled machine learning engineers,
like people who are excellent programmers
and they know stats in ML and they're the unicorns to hire.
And over time, we've been able to broaden that
and I think having things like this tooling
has made that possible in our user survey
right after we first shipped.
Even just the API workflow piece, and we were actually
just running it on some boxes of sidecar process
we hadn't even done Kubernetes yet.
But a lot of the feedback we got was like,
oh, this new person started on my team,
and I just pointed them to the directory where the workflows are.
And I didn't have to think about how
to split all these things out because you
just pointed me in the right direction
and I could point them in the right direction.
So I think that having these common ways of doing things
has been a way to broaden our user set.
And as our data science team, which is more internally
focused has grown, they've been able to start picking up
increasingly large pieces of what we've
built for the ML engineers as well.
And we've been excited to see that and work with them.
Awesome, awesome.
And so the interface then is kind of Python code.
And our is the platform containerizing that code?
Or is the user expected to do it?
Or is it integrated into some kind of workflow
like they check it in?
And then it becomes available to the platform via a check
in or a CICD type of process.
Yeah, so we still have the experimental flow
where people can kind of try things out.
But when you're ready to productionize your workflow,
basically what you do is you get your code reviewed,
you merge it.
We ended up using Google's subpar library
because it works really well with Bazel,
which we use for a lot of our build tooling to be able to kind
of, what are those two?
Yeah, so subpar is a Google library
that helps us package Python code into like a self-contained
executable, both the source code and any dependencies
like if you're running PyTorch and you need some CUDA stuff.
And it works kind of out of the box with Bazel,
which is the open source version of Google's build system,
which we have started to use at Stripe a few years ago
and have expanded since.
It's really nice for like speed, reproducibility,
and working with multiple languages.
So this is where our ML info team kind of worked with our
orchestration team to figure out the details here,
to be able to kind of like package up all this Python code
and have it so that basically, almost like a service
deploy, you can kind of like have it turn into a Docker image
that you can deploy to like Amazon's ECR.
And then Kubernetes will kind of like know how to pull that
down and be able to run it.
So the ML engineer, the data scientist,
doesn't really have to think about any of that.
It just kind of works as part of the, you know,
you get your PR emerged and you deploy something
if you need to change the workflow.
OK, but earlier on in the process, when you're experimenting,
the currency is a, you know, some Python code.
Are you, um, does the, are you, like what kind of tooling
have you built up around experiment management
and automatically tracking various experiment parameters
or hyper parameters, hyper parameter optimization,
that kind of thing, are you doing all that
or is that all on the, the user to, to do?
Yeah, that's a really good question.
So one of the things that we added in our API for training
is we found it was really useful to have
this like custom prams field, especially
because we eventually people ended up
and, you know, we have some shared services
to support this, like sort of a retraining service
that can automate your training requests.
OK.
And so one of the things that people, from the beginning,
used the custom programs for was hyper parameter optimization.
We are kind of working toward building that out
as a first class thing.
Like, we now have like evaluation workflows
that can be integrated with all of this as well.
And that's kind of like the first step
you need for hyper parameter optimization
if you want to do it as a service is like,
what are you optimizing?
If you don't know what metrics people are looking at.
So that's something we hope to do like over the next,
you know, three to six months is to make that
like a little bit more of first class support.
And you mentioned this directory of workflows.
Elaborate on that a little bit.
Yeah.
So one of the nice things is, you know,
when you're writing your workflow,
if you put it in the right place, then our, like,
our Scala service really will know where to find it.
But one of the side benefits has also
just been that there is one place where people's workflows are.
And so that's been kind of like a nice place
for people to get started and see like,
you know, what models are other people using
or like what preprocessing or kind of what other things
are they doing or what types of parameters,
like estimator parameters, are they looking at changing
to just kind of, you know,
have that be like a little bit more available
to our users or internal users.
Mm-hmm.
And the workflow element of this is it,
uh, is it graph-based? Is it something like Airflow?
Um, how's that implemented?
Yeah. So in this case, by workflow,
all I mean is just like Python code that, you know,
you give it, like, we're actually,
Rayard, our API passes to it, um,
like, what are your features or what are your labels?
And then your Python code returns, like,
here is the fitted pipeline or model.
And, um, like, usually something like the evaluation data set
that we can pass back, um, we have had,
so we've, people have kind of built us and users,
like, interesting things on top of having a training API.
So some of our users built out, um,
actually the folks working on radar,
our fraud product built out like an auto retraining service
that we've since kind of taken over and generalized,
um, where they schedule like,
nightly retraining of all the tens and hundreds of models,
um, and, you know, that's integrated to be able to even like,
if the evaluation looks better, like potentially automatically
to play them, um, we do also have people who have put like,
training models via our service into like air flow decks.
If they have, um, you know, some,
some slightly more complicated set of things that they want to run, um,
so we're definitely seeing that as well.
Okay. And you've mentioned radar a couple of times.
Is that a, uh, uh, product that stripe,
or an internal project?
Yeah, radar is our, um, like user facing fraud product.
It, um, runs on all of our machine learning infrastructure.
And, you know, every charge that goes through stripe within,
usually a hundred milliseconds or so,
we've kind of like done a bunch of real time feature generation
and evaluated, um, like kind of all of the models that are appropriate.
And, um, in addition to sort of the machine learning piece,
there's also a product piece for it,
where users can get more visibility into what our ML has done.
They can kind of like write their own rules, um,
and like set block thresholds on them.
And there's, there's sort of like a manual review functionality.
So they're kind of some more product pieces
that are complimentary to the underlying machine learning.
Okay. Interesting.
And so the, uh,
just trying to complete the picture here,
you've got these workflows,
which are essentially Python.
They expose a train, uh, entry point.
And do you, um,
are they, you mentioned as directory of workflows,
is that like a directory like on a server somewhere,
with just like dot py files,
or is that, are they, do you require that they be versioned?
Um, and are you kind of managing those versions?
Yeah. So that, that's just like actually like,
in a code, basically.
So that's like, yeah, the workflows live together in code.
As part of, um, as part of, um, kind of our training API,
it's like, when you submit,
here's my training request, which has, you know,
here's my data, here's my metadata.
This is the workflow I want you to run.
We give you back, um, a job ID,
which then you can check the status of,
you can check the result.
The result will have things in it like,
what was the getcha, um,
and so that's like something that we can track as well.
Got it. So you're submitting the job,
with the code itself as opposed to a getcha.
Um, so I guess it depends a little bit,
which workflow you're running through,
like, um, in the case where you're running on Kubernetes,
you've merged your code to master.
Um, and then we kind of package up all this code
and, um, deploy the Docker image.
And then from there, you can kind of make requests to our service,
um, which will run the job on Kubernetes.
So at that point, your code, it's, you know,
whatever's on master for the workflow,
plus whatever you've put in the request.
Oh, got it.
Okay. Um, and so that's the, the kind of the,
the shape of the training infrastructure.
You've mentioned a couple of times that you,
it sounds like there's some degree to which,
actually, I'm not sure.
Maybe I'm, um, uh, inferring a lot here.
But, uh, let's talk about the,
where the, the data comes from for training and what kind of, uh,
uh, you know, platform support your offering folks.
Yeah, that, that's a really interesting question.
Um, kind of within the framework of like,
what do you need for a, um, like really RDPI request,
we support two different types of data sources.
Um, one is more for experimentation,
which is like, you can kind of tell us how to make
this equal to query the data warehouse.
Um, and that's kind of nice for experimentation,
but not so nice for production.
Um, what pretty much everyone uses for production is,
um, the other data source we support,
which is parquet, um, from S3.
So it's like, you tell us, you know,
where to find that and what your future names are.
And usually that's generated by, um,
our features framework that we call semblance,
which is basically, um, like a DSL that helps,
you know, gives you a lot of ways to write complex features.
Like, think, have things like counters,
be able to do things like joins,
do a lot of transformations,
and then, um, you know,
the ML infrastructure team figures out like,
how to run that code in batch,
if you are doing training or, um,
like, there's a way to run it in real time,
basically, and kind of like a Kafka consumer setup.
Um, but you only have to write your code,
future code, like once.
Okay, and so,
um, do you, are you also,
is it the user that's only writing a future code once,
or are you going after kind of sharing features
across the user based to what extent,
or are you seeing, uh, shared features?
Yeah, that's like a really excellent question.
Um, yeah, so the user writes their code once,
and like also, I think having a framework
similar to the training workflows,
where people can see what other people
have done has been really powerful.
Um, so we do have people who are, like,
definitely kind of sharing features
across applications,
and there's, there's a little bit of a trade-off,
like it's like a huge amount of leverage
if you don't have to rewrite some complicated
business logic.
Um, you do have to manage a little bit of
making sure that, um, you know,
everything is versioned,
and that you're paying attention to, like,
not deprecate someone else is using,
and that you're not, like,
just like changing a definition in place,
and that you are kind of like creating a new version
every time you are changing something.
Right.
So there's a little bit more management there,
and hopefully over time,
we can improve our tooling around that.
But I think it's, you know, even,
even since before we had a feature streamwork,
like being able to kind of share some of that stuff,
has been, like, hugely valuable for us.
And are you,
so what, uh, is the features framework,
is that, uh, is that a set of APIs,
or is that, uh, kind of a runtime,
uh, thing, like, what, what exactly is it?
Yeah, there's kind of two pieces.
So, um, which is basically sort of what you said,
like, you know, one is more like the API, um,
like, what are, what are the things we,
you know, let users express?
And one thing we've tried to do there
is actually constrain not a little bit.
So we like, you have to use events for everything,
and we don't really let you express notions of time,
so you kind of can't mess up that time machine
of, like, what was the state of the features
at some time in the past,
where you want to be training your model,
we kind of, like, take care of that for you.
Um, so that's kind of one piece,
and then, you know, we kind of compile that
into, like, an AST,
and then we use that to essentially write,
like, a compiler to be able to run it on different backends.
And then we can kind of, like, you know,
write tests and try and check,
um, at the framework level that, that things are going to be
as close as possible to the same across those different backends.
So backend could be, um,
something for training where you're going to materialize,
like, what was the value of the features
at each point in time in the past
that you want as inputs to training your model,
um, or another backend could be, like,
I mentioned, we have kind of this
cock consumer-based backend that we use,
like, for example, um,
for radar to be able to, like,
evaluate these features,
like, as a charge is happening?
Uh, and so, to what extent do you find that, um,
that, that limitation of everything being event-based
gets in the way of what folks want to do?
Yeah, that, that's a really good question to you.
Um, it's definitely, uh,
was originally a little bit of a paradigm shift for people,
because they were like, oh,
I just want to use this thing from the database,
right?
But we found that actually it's worked out pretty well,
and that, especially when you have users who are ML engineers,
like, they do really understand the value of,
like, why you want to have things be event-based,
and, like, the sort of gotchas that that helps prevent,
um, because I think everyone has their story
about how you were just looking something up
in the database, but then, you know,
the value changed, uh,
and you didn't realize it,
so it's kind of like,
you're leaking future information into your training data,
and then your model is not going to do
as well as you thought it did.
Um, uh, so, like, I think moving to a more event-based world,
and, I mean, I think in general,
shape has also kind of been doing more streaming work,
and, um, more having, like, good support.
Also, as, as, uh,
at the infrastructure level,
with Kafka has been really helpful with that.
And so does that mean that the, uh,
the models that they're building
need to be aware of kind of this streaming paradigm
during training?
Or did they get a static data set to train?
Yeah, so basically, um,
you can kind of use our future's framework to just generate,
like, per K and S3 that has materialized,
like, all of the information you want
of what was the value of each of the features
that you want at all the points in time that you want,
and then, yeah, your input to the training API is,
like, please use this per K from S3.
We could make it a little more seamless than that,
but that's worked pretty well.
Um, in part, it's just like a, uh,
serialized, like, a file format.
Yeah, it's pretty efficient.
Um, you know, I think it's used in a lot of kind of big data uses.
Um, you can also do things like predicate pushdown,
and we have, like, a way in the training API
to kind of specify some filters there,
um, to just kind of, like, save, save some effort.
Uh, use a predicate pushdown?
Yeah, so if you know you only need certain columns
or something, like, you know, you can,
you can load it a little bit more efficiently
and not have to carry around a lot of extra data.
Got it. Okay.
Um, the other interesting thing that you talked about
in the context of this event,
base framework is the whole, um, you know,
time machine is the way you said it.
Kind of alluding to the point and time correctness
of, uh, you know, a feature snapshot.
Can you elaborate a little bit on?
Um, um, did you, did you start there
or did you evolve to that?
That seems to be in my conversations kind of, uh,
I don't know, maybe you'd like one of the,
the cutting edges or bleeding edges
that people are trying to deal with as they scale up these,
um, these data management systems for features.
Yeah, for this particular project, um,
in this version, we started there.
Straight previously had kind of looked at something
a little bit related a couple years before, um,
and in a lot of ways, we kind of learned from that.
So we ended up with something that was more,
more powerful and sort of solved some of these issues
at the platform level.
Um, we did, you know, at that point,
we had been running machine learning applications
in production for a few years.
So I think everyone has their horror stories, right?
Of like all the things that can go wrong, um,
especially kind of at a correctness level,
and like everyone has their story
about like re-implementing features
in different languages,
which we, we did for a while too,
and kind of like all the things that can go wrong there.
So, um, you know, I think we,
we really tried to learn from both like,
what are all the things we'd seen go well
or go wrong in individual applications,
and then also from kind of like our previous attempts,
um, at some of this type of thing,
like what, what was good,
and you know, what could still be better?
Mm-hmm.
And, uh, out of curiosity,
what do you use for data warehouse,
and are there multiple,
or is it, is there just one?
Um, we've used a combination of Redshift and Presto,
um, over the past couple of years,
um, you know, they have a little bit of sort of like,
different abilities and strengths,
um, and those are,
those are things that people like to use
to experiment with machine learning,
although like, you know,
we generally don't use them in our production flows,
because we kind of prefer the event-based model.
Mm-hmm.
And so as the event-based model,
uh, is it kind of parallel,
or orthogonal to Redshift or Presto,
is there, or is it a front end to either of these two systems?
Yeah, I guess we have,
we actually have a front end that we've built
for Redshift and Presto, um,
you know, separately from machine learning,
that's really nice,
and lets people like, um,
you know, to the extent they have permissions to do so,
like explore tables,
or put annotations on tables.
Mm-hmm.
Um, we haven't integrated our,
in general, I would say we could do some work on our UIs
for, for our ML stuff.
We've definitely focused more on the back end,
and in front of an API side,
although we do have some things,
like our auto-retreating service has a UI,
where you can see like, um,
what's the status of my job?
Like, was it, you know,
did it finish, um,
did it produce a model that was better than the previous model?
Mm-hmm.
I think I'm just trying to wrap my head around
the, the event-based model here,
uh, you know, as an example of a question that's coming to mind,
uh, in an event-based world,
are you regenerating the features,
you know, every time,
and if you've got, you know,
some complex feature that involves a lot of transformation,
or you have the backfill of ton of data,
like, what does that even mean in an event-based world,
where I think of, like,
you have events, and they go away.
Yeah.
If there's some kind of store for all that,
that isn't Redshift or Presto?
Um, well, whenever we say event,
you know, we're publishing something to Kafka,
and then we're archiving it to S3,
uh, then that persists, like, you know,
as long as we want it to,
um, in some cases, basically, forever.
Um, and so that is available.
We do, do, end up doing, um,
a decent amount of backfilling of kind of,
like, you know, you define the transform features you want,
but then, um, you need, you know,
you need to run that back over all the data,
you'll need for your training set.
That's something that we've actually done a lot of
from the beginning, partly because of our applications,
like, when you're looking at fraud, um,
you know, the way you find out if you were right or not,
is that, like, in some time period,
usually within 90 days,
but sometimes longer than that,
the cardholder decides,
um, whether they're going to dispute something
as fragile or not, um,
and that's compared to, like, you know,
if you're doing ads or trying to get clicks,
like, you kind of get the result right away,
um, and we, you know,
so I think we've always, like,
been interested in kind of, like,
being able to backfill so that,
is, you know, you can log things forward,
but then it's like, you'll probably have to wait
a little bit of time before you have enough of the data set
that you can train on it.
Okay. Um,
cool, so we talked about the data,
uh, side of things we talked about,
training and experiments,
uh, how about inference?
Yeah, that's, that's a really great question,
and that's, that's kind of like the first thing
that we built infrastructure support for, um,
at first, a decent number of years ago,
like, I think even before things like TensorFlow
were really popular, um,
and so we have, um,
like our own Scala service that we use
to do our production real-time inference,
um, and, you know, we started out,
especially because we have, like,
mostly transactional data.
We don't have a lot of things,
like images, at least as our most critical applications,
at this point, um,
a lot of our early models,
and even still today, like most of our production models
are kind of like tree-based models,
like initially things like random forest,
and now things more like XG boost.
Um, and so, you know, we've kind of like, um,
we have the serialization for that,
built into our training workflows,
and, um, we've optimized that to run
pretty efficiently in our Scala inference service,
and then we've built some kind of nice layers
on top of that, um,
for things like model composition,
kind of what we call meta models,
where, you know, you can kind of, like,
take your machine learning model,
and, um, kind of, like, almost, like,
within the model, sort of,
compose something, like,
add a threshold to it, um,
or, like, for radar,
we train, you know,
some array of, like,
in some cases,
user-specific models,
along with, like,
maybe more of some global models,
and so, you can kind of incorporate
in the framework of a model,
doing that dispatch for your kind of,
like, if it matches these conditions
that score with these models,
otherwise score with this model,
and, like, here's how you combine it.
Um,
and then the way that interfaces with your application,
is that each application has,
uh, what we call a tag,
and basically the tag points to the model identifier,
which is kind of, like, immutable,
and then whenever you have a new model,
or you're ready to ship,
you just, like, update,
what does that tag point to?
Um, and then, you know,
in production,
you're just saying, like,
score the model for this tag.
Okay, and that
I think that's pretty similar to, like,
you know, if you read about Uber's Michelangelo
and things like that,
sometimes we're like,
oh, we all came up with the same thing.
I think that's pretty odd.
I think that's pretty good.
We had also sounds a little bit like,
uh, sorry, say that again.
Yeah, I think that, like,
a lot of people have kind of come up
with some of these,
these ways of doing things
that just kind of make sense.
Mm-hmm.
Mm-hmm.
It also sounds a little bit like,
uh, some of what,
uh,
seldom is trying to capture
in a Kubernetes environment.
Um,
uh, which I guess brings you to is the
inference running in Kubernetes
or is that a separate, um,
separate infrastructure?
It's not right now,
but I think that's mostly like a matter
of time and prioritization.
Um, like, the first thing we move to
Kubernetes was, uh,
the training piece
because the workflow management piece
was so powerful,
or sorry, the resource management piece
was so powerful,
like being able to swap out CPU,
GPU, high memory.
Mm-hmm.
Um, we've moved some of our
like, um,
sort of real-time feature evaluation
to Kubernetes,
which has, um,
been really great and made it like a lot
less toil to kind of deploy
new feature versions.
At some point,
we will probably also move
the inference service to Kubernetes.
We just kind of haven't gotten there yet
because it is still some work to do that.
Mm-hmm.
Um,
and is
the, uh,
the inferences happening on AWS as well,
and are you using kind of standard
CPU instances
or are you doing anything fancy there?
Yeah, so, um,
we ran on cloud
for pretty much everything,
and, um,
definitely use a lot of AWS.
Um,
for the real-time inference
of the most sensitive,
like production use cases,
um, we're definitely mostly using, um,
CPU,
and we've done a lot of optimization work,
um,
so that has worked pretty well for us.
Um,
I think we do have some folks
who've kind of experimented
a little bit with, like,
hourly or batch scoring, um,
using some other things.
So I think that's something
that we're definitely thinking about
as we have more people productionizing,
um,
kind of like more complex types of models
where, you know, we might want something different.
You mentioned a lot of optimization
that you've done is that, uh,
on a model by model-by-model basis
or are there, uh,
platform, uh,
things that you've done that, um,
help optimize across the various models
that you're deploying, uh,
for instance.
Yeah, definitely.
A lot of things at the platform level,
like, I think the first models
that we ever,
ever scored in our inference service,
um, were serialized with YAML,
and they were like really huge,
and, um,
they caused a lot of garbage
when we tried to load them,
and so, like,
we did some work there
for kind of tree-based models,
um,
to be able to load things
from disk to memory really quickly
and, like, not producing much garbage.
Um, so that,
that kind of thing
are things that we did,
especially, kind of,
like, in the earlier days.
Okay.
And are you,
what are you using
for querying the models?
Are you doing rest
or GRPC
or, uh,
something altogether different?
Yeah, we use rest right now, um,
I think GRPC is, like,
something that we're interested in,
um, but we haven't done yet.
Okay.
Uh,
and are you,
is all of your,
all of the inference done
via, um,
kind of via rest and, like,
a kind of microservice style,
or do you also do, um,
more,
I guess, embedded types of,
uh, inference for,
like, where you need to have super low latency requirements.
Does rest kind of meet the need
across the application portfolio?
Yeah, um,
even for our most critical applications,
like, shield things have worked pretty well.
One other thing our orchestration team has done
that's worked really well for us
is, um,
migrating a lot of things to OnVoy.
Um,
so we've seen some,
some things where, like,
we didn't understand why there was some delay,
like, in what we measured
for how long things tricks,
versus, like, what it took to the user.
There's just kind of one away,
as we move to OnVoy.
Um,
and what is OnVoy?
Uh,
OnVoy is, like,
a service service networking mesh
that was developed by Lyft,
um,
and is kind of like an open source,
open source library.
Um,
and so it handles a lot of,
it can handle a lot of things,
like service to service communication.
Okay.
Cool. Um,
and so the,
the inference,
the inference environment,
does it,
is it doing,
absent of Kubernetes,
all the things that you'd expect Kubernetes to do
in terms of, like,
auto scaling,
and, um,
you know,
load balancing,
uh,
across the different service instances,
or,
is that stuff all done,
um,
statically?
Um,
we take care of the routing,
um,
ourselves,
and we also, at this point,
have kind of, like,
charted our inference service,
so not all models are stored
on every host,
so that, you know,
we don't need hosts
with, like, infinite memory.
Um,
and so that we take care of ourselves,
um,
the scaling, we,
is not fully automated at this point.
We do, we have kind of, like,
quality of service,
so we have, like,
multiple,
kind of clusters of machines,
and we tear a little bit by, like,
you know,
how sensitive your application is
and what you need from it,
um,
so that we can be a little bit more
relaxed with people
who are developing
and want to test
and not have that,
like,
potentially have any impact
on more critical applications,
um,
but we haven't done, like,
totally automated scaling,
that's something we kind of
still look at a little bit ourselves.
Awesome. Awesome.
Um,
so if you were kind of just starting
down this journey,
uh, without having done all the,
the things that,
that you've done at Stripe,
where do you think you would start?
If you just, um,
you know,
you're, you're at an organization
that's kind of increasingly invested in
or investing in machine learning
and, you know,
needs to try to,
uh,
you know, gain some efficiencies.
Yeah, I mean, I think if you're just starting out,
like, it's good to think about,
like, what are your requirements, right?
Um,
and, you know,
if you're just trying to iterate quickly,
it's like,
do the simplest thing possible, right?
So, you know,
if you can do things in batch,
like, great, do things in batch,
um,
I think a lot of,
there are a lot of both
open-source libraries
as well as managed solutions,
um,
like, on all the different cloud providers,
so I think, you know,
I don't know, you know,
if you're only one person,
then I think that those
could make a lot of sense,
also, for people starting out,
because I think one of the interesting things
with machine learning applications
is that, um,
it takes a little bit of work,
like, usually,
there's sort of this threshold
of, like, your modeling
has to be good enough
for this to be, like,
a useful thing for you to do,
like, for fraud detection,
that's, like, if we can't catch any fraud
with our models,
then, like, you know,
we probably shouldn't have,
like, a fraud detection product,
um,
so I think it is useful to kind of have,
like, a quick iteration cycle
to find out, like,
is this a viable thing
that you even want to pursue,
and if you have an infrastructure team,
they can kind of, like,
help, um,
lower the bar for that,
but I think there are other ways to do that,
especially as, you know,
there's been, like,
this Cambrian explosion
in the ecosystem
of different open source platforms,
as well as different managed solutions.
Yeah, how do you,
how do you think
an organization knows
when they should have
an infrastructure team,
ML in particular?
Yeah, I think that's a really interesting question,
um, I guess, uh,
in our case, I think, um,
you know,
the person who originally founded the,
and she learning infrastructure team,
um, had worked in this area before at Twitter,
and kind of had a sense of, like,
this is going to be a thing
that we're really going to want to invest in,
given how important it is for our business,
and also that,
if you don't kind of, like,
dedicate some folks to it,
it's easy for them to kind of get sucked up
in other things,
like, if you just have data infrastructure,
that's undifferentiated.
Um,
so I think it's a really interesting question.
There probably is this business piece, right,
of, like,
what are your ML applications?
Like, how critical are they to your business?
And, like, how difficult are your infrastructure
requirements for them as well?
I think a lot of companies develop
their ML infrastructure,
like, starting out with things,
like, making the notebook experience really great,
because they want to support,
like, a lot of data scientists
who are doing a lot of analysis.
And so that's, like, a little bit of a different arc
from the one that we've been on.
And I think that's, like,
actually a pretty business-dependent thing.
Okay.
Awesome. Awesome.
Well, Kelly, thanks so much for taking the time
to chat with me about this really interesting
story, and I've enjoyed learning about it.
Cool. Thanks so much for chatting.
Really enjoyed it.
Awesome.
