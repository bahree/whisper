1
00:00:00,000 --> 00:00:11,040
All right, everyone. Welcome to another episode of the Twomo AI podcast. I am, of course,

2
00:00:11,040 --> 00:00:17,560
your host, Sam Charrington. Today, I'm joined by Alyosha Oshep, a postdoc at the Technical

3
00:00:17,560 --> 00:00:23,840
University of Munich and Carnegie Mellon University. Before we jump into today's conversation,

4
00:00:23,840 --> 00:00:28,200
please be sure to take a moment to head over to Apple Podcasts or your listening platform

5
00:00:28,200 --> 00:00:35,240
of choice. And if you enjoy the podcast, please leave us a 5 star rating and review. Alyosha,

6
00:00:35,240 --> 00:00:40,040
welcome to the show. I'm looking forward to digging into our conversation. We'll be talking about

7
00:00:40,040 --> 00:00:47,400
your work in robotic vision and some of the papers you'll be presenting at CVPR. Before we get there,

8
00:00:47,400 --> 00:00:52,040
though, I'd love to have you introduce yourself to our audience and share a bit about how you came

9
00:00:52,040 --> 00:00:57,240
to work in the field. Yes, sure. So first, thank you very much for inviting me. I'm very excited to

10
00:00:57,240 --> 00:01:04,200
be here today and to have this opportunity to present or work that we are going to present

11
00:01:04,200 --> 00:01:11,720
at the CVPR next week. So, I will just start with a little bit of a background about myself.

12
00:01:11,720 --> 00:01:19,560
So, yeah, my name is Alyosha Oshep and I was born in a race and born in Slovenia, quite

13
00:01:19,560 --> 00:01:25,640
small country in Central Europe. As you already mentioned, at the moment, I'm working as a

14
00:01:25,640 --> 00:01:31,800
post-log boat at the Technical University. I'm working with Laura Ljolta-Shea and I'm also still

15
00:01:31,800 --> 00:01:39,960
still working at Carnegie Mellon University. I'm working with Deva Ramana. So, you mentioned that

16
00:01:39,960 --> 00:01:45,560
you would like to hear something about how I have gotten into computer vision research. So,

17
00:01:45,560 --> 00:01:49,960
just curious, would you like to hear a shorter or the longer version?

18
00:01:49,960 --> 00:01:57,080
The medium version is maybe the best. Medium version. Is it something that you always aspire to do?

19
00:01:59,400 --> 00:02:05,720
I would not say always. I think it kind of has gotten gradually to me, but I would say that from

20
00:02:05,720 --> 00:02:11,800
a very early days, I knew that I want to... I was really interested in computer science,

21
00:02:11,800 --> 00:02:18,280
right? And in programming. So, in one way or another, I was interested in that general film.

22
00:02:18,280 --> 00:02:23,320
Okay, what was your first exposure to vision and machine learning in AI in general?

23
00:02:23,320 --> 00:02:32,200
So, I would say that my first exposure to vision, ML and AI was actually at university.

24
00:02:33,960 --> 00:02:40,040
I had some of the courses on computer vision, computer graphics, and the machine learning already

25
00:02:40,040 --> 00:02:49,000
during my bachelor's studies. But even before that, I got interested in computer programming

26
00:02:49,000 --> 00:02:54,440
as a kid. So, at the beginning, I was interested, you know, like as a kid, of course, I was playing

27
00:02:54,440 --> 00:02:59,080
computer games, right? And I was just really, really fascinated with games and all these virtual

28
00:02:59,080 --> 00:03:03,640
worlds that come together with computer games, right? So, first thing that I really want to understand

29
00:03:03,640 --> 00:03:11,800
is how... When I was still quite young, right? How this comes to be, right? What is the logic behind

30
00:03:11,800 --> 00:03:17,880
all this? How one makes computer game and virtual worlds, and you know, like all these characters

31
00:03:17,880 --> 00:03:24,120
that appear in the game and behave like in somewhat intelligent way, or at least that they appear

32
00:03:24,120 --> 00:03:29,960
that they behave in intelligent way. So, this is really how I got into more general body

33
00:03:29,960 --> 00:03:39,800
direction. So, through interesting computer games and computer graphics. But when it comes to

34
00:03:39,800 --> 00:03:44,760
computer vision and machine learning, I think that some of the first memories I have about this

35
00:03:45,960 --> 00:03:52,120
come from the time when I was doing my bachelor's studies in Slovenia. So, you know, that the one

36
00:03:52,120 --> 00:03:56,920
way you can think about graphics is it's synthesis, right? You see in these images, right? A computer

37
00:03:56,920 --> 00:04:00,520
vision is kind of opposite process, right? You get images and you have to understand what these

38
00:04:00,520 --> 00:04:06,280
images are, what in the world has generated those images, right? And I really remember, like,

39
00:04:06,280 --> 00:04:12,200
back in the days I was watching, I know this is a bit nerdy, but I really liked Star Trek, right?

40
00:04:12,200 --> 00:04:19,240
And one of the coolest things, and one of the coolest things besides traveling around space and,

41
00:04:19,240 --> 00:04:24,760
you know, like meeting alien civilization was also this holiday, right? I just find it so

42
00:04:24,760 --> 00:04:30,920
fascinating that they have like this immersive game that just generates context content and,

43
00:04:30,920 --> 00:04:37,160
you know, like you can interact with characters that computer generates and so forth. So,

44
00:04:37,160 --> 00:04:41,560
this seemed really cool to me. But I still remember there was this one episode that I find so

45
00:04:41,560 --> 00:04:46,840
fascinating where they were trying to recreate something and they have some image of, I don't know,

46
00:04:46,840 --> 00:04:50,600
I don't remember exactly what it was, but they have some image and they just fed this to this

47
00:04:50,600 --> 00:04:55,000
holodeck and holodeck kind of created 3D reconstruction of that image, right? Then they were analyzing

48
00:04:55,960 --> 00:04:59,800
what was happening with this and all that, but I just remember this idea, you know, feeding

49
00:04:59,800 --> 00:05:04,760
image in and getting 2D reconstruction out, right? So I thought this was really cool, yeah.

50
00:05:04,760 --> 00:05:10,600
I should mention that you're doing your postdoc with Laura Layout, I say, or you mentioned that,

51
00:05:10,600 --> 00:05:19,640
but I should mention that I interviewed her around this time in 2018. I'm sure it was CVPR

52
00:05:19,640 --> 00:05:26,280
related given the timing and it sounds like in some ways you're carrying on the torch, so to speak.

53
00:05:26,280 --> 00:05:30,920
In talking about your research interests, it's not just computer vision, it's robot vision,

54
00:05:31,720 --> 00:05:34,840
you know, what kinds of problems are you trying to solve?

55
00:05:34,840 --> 00:05:43,080
I would say that in broader sense, I'm really interested in 3D dynamics in understanding,

56
00:05:43,080 --> 00:05:50,120
right? Also similar to research interests of Laura, so you assume that you have some mobile

57
00:05:50,120 --> 00:05:54,920
platforms such as the robot, it's equipped with some sensors, right? It can be camera, one camera,

58
00:05:54,920 --> 00:06:01,320
two cameras or several, or and lighter and based on whatever sensors you have, you should be able to

59
00:06:01,320 --> 00:06:08,840
get as complete understanding of the world as possible, right? So you should understand the

60
00:06:08,840 --> 00:06:14,360
3D geometry of the world, right? Both static and dynamic, right? You should know where objects are,

61
00:06:14,360 --> 00:06:20,120
right? Where cars are, where pedestrians are, and so forth. In addition to that, it's also important

62
00:06:20,120 --> 00:06:24,920
that you know how these objects move over time, right? And the main reason why it's so important to

63
00:06:24,920 --> 00:06:30,040
know how objects move or how objects move in the past, which really comes to multi-object tracking,

64
00:06:30,040 --> 00:06:34,200
but the main reason why we want to know this is that we have to make predictions about what happens

65
00:06:34,200 --> 00:06:38,600
in next seconds, right? You just can imagine that if you are walking around city or if you are driving

66
00:06:38,600 --> 00:06:45,400
a car, what you end up doing most time is navigating into free space, right? So this means that you

67
00:06:45,400 --> 00:06:49,400
don't only have to understand the static scene geometry, but you also have to know where everything

68
00:06:49,400 --> 00:06:56,680
that moves will be in a few seconds, right? So at the high level, this is what interests me a lot,

69
00:06:56,680 --> 00:07:05,000
but if I get more specific, I was really fascinated with one particular question in past years,

70
00:07:05,000 --> 00:07:13,160
already during my PhD, and I'm still working on this. So you probably know that nowadays,

71
00:07:14,360 --> 00:07:21,400
nowadays, I would say that object detection, tracking, and even forecasting models work quite well,

72
00:07:21,400 --> 00:07:26,920
all right, right? We've made a huge progress in recent years, right? But there is one important

73
00:07:26,920 --> 00:07:32,920
limitation here, and this is, it works great as long as you have enough data for particular

74
00:07:32,920 --> 00:07:40,280
semantic classes that you're trying to recognize, right? So for example, if you go out record data,

75
00:07:40,280 --> 00:07:44,520
you will see lots of cars and pedestrians, right? You label them, use them to train models,

76
00:07:44,520 --> 00:07:49,960
and everything just works fantastic, right? You can recognize cars, right? But we also have this

77
00:07:49,960 --> 00:07:56,280
long-tail distribution of semantic classes, right? And lots of semantic classes appear in this long-tail,

78
00:07:56,280 --> 00:08:02,200
which means that most objects are observed very infrequently, or maybe even never when you're

79
00:08:02,200 --> 00:08:06,760
collecting data sets, right? Which just means that during your model training, you don't see

80
00:08:06,760 --> 00:08:11,080
some particular object classes, but you still have to recognize them if you really want to navigate

81
00:08:11,080 --> 00:08:16,280
around the world, right? Otherwise, otherwise, you might, otherwise, it might be really dangerous,

82
00:08:16,280 --> 00:08:21,160
right? If you didn't recognize something because it's, it looks like not like anything you have

83
00:08:21,160 --> 00:08:28,120
seen before. What kind of approaches are you exploring to address this long-tail semantic detection

84
00:08:28,120 --> 00:08:36,440
problem? Mm-hmm. Yeah, so we explored quite quite some approaches, quite some approaches in the

85
00:08:36,440 --> 00:08:43,640
past. So the way I started looking into this, into this back then was so my intuition was that

86
00:08:43,640 --> 00:08:51,160
to track any object, we can start with something that is based on a bottom-up scene understanding,

87
00:08:51,160 --> 00:08:56,280
right? So current standard object detection works more in a top-down fashion, right?

88
00:08:58,360 --> 00:09:02,920
So existing object detection models. And bottom-up approach is more that you get image and you try

89
00:09:02,920 --> 00:09:10,280
to figure out how pixels group together so that you obtain object instance, right? So which pixels

90
00:09:10,280 --> 00:09:15,880
group together to obtain, for example, pedestrian car or something gathered, you don't know quite

91
00:09:15,880 --> 00:09:20,920
what it is, but you know, based on pixel similarity, you should be able to group these pixels together

92
00:09:20,920 --> 00:09:25,320
and then possibly, you know, realize that object is there and then track these objects, right?

93
00:09:25,320 --> 00:09:33,880
And that's bottom-up. What is top-down? Top-down approach is, for example, the most well-known

94
00:09:33,880 --> 00:09:40,920
top-down approach is, for example, faster RCNN, right? You have a bunch of windows,

95
00:09:40,920 --> 00:09:47,960
or we call them region proposals, right? And then based on these region proposals, you try to

96
00:09:47,960 --> 00:09:54,040
estimate what these proposal covers, right? Is it a car, is it a person, or is, you know,

97
00:09:54,040 --> 00:10:01,080
part of a background? So you don't go from bottom-up, from pixels up, but you go from top-down.

98
00:10:01,080 --> 00:10:06,280
So you're looking at region, yeah, exactly, exactly. So we started looking into this bottom-up

99
00:10:06,840 --> 00:10:17,160
approaches, but very, very soon we actually kind of started switching back to top-down data-driven

100
00:10:17,160 --> 00:10:24,520
methods, but instead of trying to look at region proposals and classify them as one of the

101
00:10:24,520 --> 00:10:29,320
object classes that were available in data sets such as, is it a car pedestrian cyclist?

102
00:10:29,320 --> 00:10:37,960
You would just train the tactors to just tell us whether this region likely contains object

103
00:10:37,960 --> 00:10:43,080
or not, right? So basically, data-driven, we started relying on data-driven object proposals

104
00:10:43,080 --> 00:10:48,840
to initialize tracks, right? When the queue that we were also looking into at the very beginning

105
00:10:48,840 --> 00:10:57,480
was depth, depth estimates, right? So when the issue with this object proposals are, is that

106
00:10:57,480 --> 00:11:03,720
they're quite noisy, right? You will end up having like a bunch of object proposals, some of them

107
00:11:03,720 --> 00:11:09,160
will actually detect objects that you have in our scenes, but many of them will also just

108
00:11:09,160 --> 00:11:15,960
might be firing on, you know, certain background region. But the next queue you can look into

109
00:11:15,960 --> 00:11:20,920
is depth. So if, you know, if you have region proposals that is kind of supported with depth,

110
00:11:20,920 --> 00:11:25,320
estimates, this gives you a strong queue that they're actually very likely. It's an object

111
00:11:25,320 --> 00:11:32,440
that you should start to start to trajectory. Maybe kind of popping back up a level or several.

112
00:11:32,440 --> 00:11:40,120
When you think about robots, are you primarily or robotic vision? Are you primarily thinking about

113
00:11:40,120 --> 00:11:45,960
autonomous vehicle types of use cases? Or do you have, do you even have a particular

114
00:11:46,840 --> 00:11:55,000
ideal in mind in terms of the robot itself? Always when I talk about research, I have a

115
00:11:55,000 --> 00:12:00,360
wider vision in mind. So I don't only think about autonomous vehicles, I think about

116
00:12:00,360 --> 00:12:05,480
any type of robots. And I even think that, you know, that we should have perception system

117
00:12:05,480 --> 00:12:12,440
that should be applicable for robots that want to drive on highways, that want to drive in the

118
00:12:12,440 --> 00:12:18,200
intercity, inner city areas, in potentially very crowded areas, right? Then we might have

119
00:12:18,200 --> 00:12:28,600
delivery robots that might even, you know, like merge into more of a pedestrian urban areas,

120
00:12:28,600 --> 00:12:34,520
right? Or robots that might need to navigate around warehouses, airports and so forth, right?

121
00:12:34,520 --> 00:12:39,560
So I kind of think that robots, the robots we need to be everywhere in the future. And I think

122
00:12:39,560 --> 00:12:46,520
that perception systems that we develop should be general. But one thing I definitely have to admit

123
00:12:46,520 --> 00:12:53,880
is that very often when you write papers, we kind of focus on autonomous driving applications

124
00:12:53,880 --> 00:13:01,160
and scenarios. And one reason for that is that we just simply have most datasets that were captured

125
00:13:01,160 --> 00:13:05,960
for benchmarking and training algorithms for autonomous driving. I would love to have more

126
00:13:05,960 --> 00:13:10,600
datasets, actually, more general datasets. Do you have a sense for some of the ways that

127
00:13:10,600 --> 00:13:17,160
focusing on autonomous vehicles based on data set availability, you know, limits applicability

128
00:13:17,160 --> 00:13:25,800
of the work to other types of robots? I think that there definitely is certain bias if you are

129
00:13:25,800 --> 00:13:33,720
only looking at autonomous vehicle datasets, right? So for example, these datasets are obviously

130
00:13:33,720 --> 00:13:41,320
always recorded outside, right? That were inside. You might, for example, also over a lie on the

131
00:13:41,320 --> 00:13:49,160
fact that you can localize yourself outside very well with GPS, for example, right? To localize

132
00:13:49,160 --> 00:13:56,520
your poses might be impossible indoors, right? Very often is. Then very often, if you look at

133
00:13:56,520 --> 00:14:02,760
limited scenarios, such as highways, for example, your diversity of objects that you will observe

134
00:14:02,760 --> 00:14:08,600
will actually be quite limited, right? So you learn how to recognize cars, trucks, buses,

135
00:14:08,600 --> 00:14:12,360
and a few other classes, and you just think you know how to handle everything, right? But

136
00:14:12,360 --> 00:14:19,400
this is definitely not true, right? Our usual word is very richer than that. Speaking of localization,

137
00:14:19,400 --> 00:14:25,320
one of the papers that you're presenting at CVPR is called text-apause, text-to-point cloud,

138
00:14:25,320 --> 00:14:32,120
cross-modal localization. Let's dig into that a little bit. What's the kind of big picture

139
00:14:32,120 --> 00:14:37,160
problem that you're trying to solve with this paper? I would actually start a bit with motivation

140
00:14:37,160 --> 00:14:43,560
for this or before I even do that, let me just already tell you upfront that this paper is about

141
00:14:43,560 --> 00:14:51,320
to localizing position within the map, 3D map of the environment based on an actual description

142
00:14:51,320 --> 00:14:56,120
of what this is surrounding, right? You can imagine yourself that you are somewhere in the city,

143
00:14:56,120 --> 00:14:59,720
you don't know where exactly you see some things around, right? There might be church in front of you,

144
00:14:59,720 --> 00:15:05,160
some trees or something like that, and you're explaining to your friend what you see and your

145
00:15:05,160 --> 00:15:09,640
friend should realize where you are based on this description, right? If your friend of course

146
00:15:09,640 --> 00:15:16,840
knows the rough environment of the city where you're in, right? And yeah, I imagine that in the

147
00:15:16,840 --> 00:15:23,560
future robots will of course be used for many things such as, for example, food delivery, right?

148
00:15:23,560 --> 00:15:29,240
Or no like instead of Uber drivers picking us up to transport somewhere, just robots will come

149
00:15:29,240 --> 00:15:37,160
to pick us up, right? And sometimes GPS tech works great, but not always, right? And one thing that

150
00:15:37,160 --> 00:15:42,600
for example is particularly realized is particularly frustrating usually where we are somewhere at the

151
00:15:42,600 --> 00:15:47,640
campus, right? Let it be technical university of Munich or Carnegie Mellon campus, we often have

152
00:15:47,640 --> 00:15:52,440
meetings and we use food delivery services to deliver us food, right? And you would think that this

153
00:15:52,440 --> 00:15:57,320
is easy to find this, right? But it turns out it's really not. So what we end up doing, for example,

154
00:15:57,320 --> 00:16:03,640
to if you use door dash, for example, you write a detailed description on how to reach us, right?

155
00:16:03,640 --> 00:16:09,000
And even after this detailed description, where, for example, Smith Hall is, where I was working

156
00:16:09,000 --> 00:16:14,360
until recently, they would still very often get lost at the campus and call us and ask for more

157
00:16:14,360 --> 00:16:22,680
directions, right? And so it's always a bit of a hassle. So this why we also envision that

158
00:16:22,680 --> 00:16:27,480
when the robots will take over, we also need to find a way to communicate with them where they

159
00:16:27,480 --> 00:16:32,120
need to go, right? And we want to do this communication in natural language because natural

160
00:16:32,120 --> 00:16:38,600
language is what humans are used to communicate. So this is like more like overall vision, right?

161
00:16:38,600 --> 00:16:45,880
Now, now if I get more to the task, this paper is addressing. So here I have to say upfront that

162
00:16:46,680 --> 00:16:51,080
this is one of the first investigation of this problem, right? So we had to take quite some

163
00:16:51,080 --> 00:16:54,920
short cuts to making this investigation of this problem physically, right?

164
00:16:54,920 --> 00:16:58,680
Just hearing the way you describe the problem strikes me that there are, you know,

165
00:16:58,680 --> 00:17:03,400
of course, multiple ways to come at it. You know, one is, you know, the paper is called

166
00:17:03,400 --> 00:17:09,560
Text-to-Point Cloud can also envision like text to landmark somehow trying to localize not necessarily

167
00:17:11,000 --> 00:17:16,040
where you are, but the landmarks that you're describing and somehow triangulate from that.

168
00:17:16,040 --> 00:17:18,840
You know, talk a little bit more about the way you set up the problem.

169
00:17:18,840 --> 00:17:26,600
Sure, sure. So you exactly, so this comment was just spot on because this is this is actually

170
00:17:26,600 --> 00:17:32,840
part of what the what the method does, right, triangulating landmarks, right? I mean, this is

171
00:17:32,840 --> 00:17:37,160
the second stage. So we have like course localization stage and file localization stage, right?

172
00:17:38,280 --> 00:17:42,040
So what you just mentioned comes into the fine phase. I will just first touch the course

173
00:17:42,040 --> 00:17:46,840
phase and then then I'll get back to this, right? So in the course phase, we basically just

174
00:17:46,840 --> 00:17:53,160
split the 3D point cloud of the city or of some neighborhood into rectangular tiles, right?

175
00:17:53,960 --> 00:17:57,960
They're, I don't remember what exactly the size is, but you know, it's like some

176
00:17:57,960 --> 00:18:03,240
a bit larger area that you first have to find, right? So first, you want to find like this

177
00:18:03,240 --> 00:18:09,080
rectangular area in where you very likely you are located, right? And when you say tiles,

178
00:18:09,080 --> 00:18:13,160
are you thinking like open street map tiles or something along those lines?

179
00:18:13,160 --> 00:18:20,840
No, we actually have in practice a lighter point cloud. So maps that were built from lighter point

180
00:18:20,840 --> 00:18:30,280
clouds and the data set we actually use for that is the new kitty 360 data set from a research

181
00:18:30,280 --> 00:18:38,040
group of Andreas Geiger. But this this data set of course contains only this point clouds maps

182
00:18:38,040 --> 00:18:43,560
and no textual description, right? But this is this how we actually generated data set. It's

183
00:18:44,280 --> 00:18:50,760
it's a different story. I'm curious, you're, you're starting assumption is are you assuming,

184
00:18:50,760 --> 00:18:58,040
you know, no GPS or, you know, course GPS and you're using that for the first phase and then

185
00:18:58,040 --> 00:19:03,560
you're using the text base for the second phase. So this is also a great question and actually

186
00:19:03,560 --> 00:19:12,200
we assume no GPS. The only assumption we make is that we have instance segmentations available

187
00:19:12,200 --> 00:19:17,800
in our map, right? So that we have, you know, we know where the houses are, we know where the trees

188
00:19:17,800 --> 00:19:22,520
are and so forth. So we assume instance and semantic information in our maps.

189
00:19:23,240 --> 00:19:28,920
And how fine-grained are your classes? Like do you just have building building or do you have

190
00:19:28,920 --> 00:19:36,120
church super market, campus building, whatever? No, we don't have that fine-grained information.

191
00:19:36,120 --> 00:19:43,400
It's more like building tree car roads and so forth. Okay, all right, so go on and finish with

192
00:19:43,400 --> 00:19:48,120
the first part of the method. Yeah, so so first part is just course localization. But you could

193
00:19:48,120 --> 00:19:53,560
also use GPS for that. That's true, but since we didn't have GPS, we just pose this as a retrieval

194
00:19:53,560 --> 00:19:59,000
problem, right? We have textual descriptions, we have our point-code patches, and we just learned

195
00:19:59,000 --> 00:20:04,120
joint embedding space for both, right? Similar to how clipworks where they aligned images and textual

196
00:20:04,120 --> 00:20:09,800
descriptions, but you align point clouds and textual descriptions. And based on this learned

197
00:20:09,800 --> 00:20:14,040
joint embedding space, you can then based on textual description get like a list of most

198
00:20:14,040 --> 00:20:18,440
slightly sales that contain your objects. So this is the course localization step.

199
00:20:18,440 --> 00:20:23,640
It strikes me that there's like a I don't know something that I think of as like a bootstrapping

200
00:20:23,640 --> 00:20:30,120
problem like, you know, if I'm giving directions, I assume a certain amount of knowledge that

201
00:20:30,120 --> 00:20:38,840
would be hard to get from instance labels like on the wash you campus or you know, some place

202
00:20:38,840 --> 00:20:45,960
that I am at a Starbucks or something like that. Is that kind of knowledge being introduced

203
00:20:45,960 --> 00:20:52,120
somewhere? Not yet, and I think this is right now the biggest limitation of our method. So

204
00:20:52,120 --> 00:20:58,600
to incorporate such a fine-grained information, we would have to go step forward, step further,

205
00:20:58,600 --> 00:21:03,560
and align our point clouds with something like you mentioned earlier, open street map, right?

206
00:21:04,440 --> 00:21:09,480
And then you actually get access to such fine-grained information, right? What kind of building is it,

207
00:21:09,480 --> 00:21:21,160
a Starbucks sign, and so forth? But in this paper, we didn't have that. I mean, we consider this

208
00:21:21,160 --> 00:21:25,560
as a future work, but we haven't. You have to start somewhere. But it sounds like then

209
00:21:26,840 --> 00:21:33,960
conclusion would be that you're trying to localize within a relatively constrained area.

210
00:21:33,960 --> 00:21:41,480
I'm not sure what with constrains, because in principle, this should work anywhere in the city area

211
00:21:41,480 --> 00:21:48,520
for which we have a map in form of point clouds, right? But of course, there is this restriction

212
00:21:48,520 --> 00:21:53,800
that if I just tell you that I see, I don't know, traffic side in front, church on the right,

213
00:21:53,800 --> 00:21:58,920
sending us here and there, right? This description fits several locations in the city, right?

214
00:21:58,920 --> 00:22:06,200
So you have this uncertainty, right? Of where you are. And if you wanted to make this non-ambiguous,

215
00:22:06,200 --> 00:22:11,080
you would really either have to go, for example, GPS, it coercely localizes you, right?

216
00:22:11,080 --> 00:22:15,240
Then this narrow down the search space, or you would have to have

217
00:22:16,520 --> 00:22:23,000
information such as, is there a Starbucks next to me, or a street name, or something like that,

218
00:22:23,000 --> 00:22:28,600
right? Something that really is a very narrow down the initial search state, right?

219
00:22:28,600 --> 00:22:33,240
Or even a dialogue-based approach where the system can identify

220
00:22:34,920 --> 00:22:40,600
differentiating characteristics of the three places and ask you, do you see a street sign near

221
00:22:40,600 --> 00:22:49,000
you or something like that? Yeah, that's also, I think it's very important that we incorporate

222
00:22:49,000 --> 00:22:52,760
into this process in the future, because of course, this is how conversation with humans work,

223
00:22:52,760 --> 00:22:55,880
right? It's not like you just described where you are and then the other person just, you know,

224
00:22:55,880 --> 00:23:04,120
click those, right? Right, right. Okay, so that's kind of the course part of the course phase of the

225
00:23:04,120 --> 00:23:11,480
method, what's the fine phase, or the second part? When it comes to the fine phase, we do

226
00:23:11,480 --> 00:23:16,440
pretty much exactly what you said earlier, right? So based on descriptions, find the landmarks that

227
00:23:16,440 --> 00:23:23,800
you refer to, right? So, you know, if you mention a sign, then, you know, the network has to realize

228
00:23:23,800 --> 00:23:31,160
it's, there was a sign mentioned in the sentence and align this with instance of a sign within your

229
00:23:31,160 --> 00:23:39,400
rectangular area, right? So you kind of match words that refer to objects with instances,

230
00:23:39,400 --> 00:23:50,200
and once you do this matching, you, you regress offset to, to, to, to, and is success for the model

231
00:23:50,200 --> 00:24:00,040
returning a list of possible locations, or you then further trying to, you know, guess or predict

232
00:24:00,040 --> 00:24:07,480
which of the locations the user, do you apply some kind of confidence to a list, or are you

233
00:24:07,480 --> 00:24:11,320
only predicting one with a confidence level, like what's the output of the model?

234
00:24:12,200 --> 00:24:18,200
Yes, so we predict several possible locations, and the reason for that is the course

235
00:24:18,200 --> 00:24:22,920
localization step that is incurring here in certain, right? So you might be in several cells,

236
00:24:22,920 --> 00:24:31,560
right? So you get ranked list, and then evaluation is, the evaluation is also done with respect to top

237
00:24:31,560 --> 00:24:41,400
K hits. You know, so you look like, you look like a localization, sorry, you look at localization

238
00:24:41,400 --> 00:24:48,680
retrieval success with respect to top five, top 10, or top 15, uh, top 15 matches. How did you evaluate

239
00:24:48,680 --> 00:24:57,640
the method? So we actually, uh, we actually evaluated a method on, um, data set that we, uh,

240
00:24:57,640 --> 00:25:04,040
generated on top of, uh, kitty 360. Oh, right. We've now come back to the data set

241
00:25:04,040 --> 00:25:09,960
conversation. So how did you create the data set? Yeah, so this, this is actually, uh, also one of

242
00:25:09,960 --> 00:25:15,160
the main parts of the paper, right? Because in theory, this sounds like kind of difficult,

243
00:25:15,160 --> 00:25:21,080
because if you need that, uh, list of, um, annotated poses and someone writing descriptions,

244
00:25:21,080 --> 00:25:25,640
this would take lots of time and lots of researches, lots of researches that we don't have, right?

245
00:25:25,640 --> 00:25:30,440
So you kind of have to find, you have to find the heck away around this. And what we ended up

246
00:25:30,440 --> 00:25:35,640
doing was, uh, so you know, I already mentioned we had kitty 360, list of instances and semantic

247
00:25:35,640 --> 00:25:40,840
meaning. And we also have, uh, color channel available, right? Because they, they align images

248
00:25:40,840 --> 00:25:46,280
and, and library points. So what we ended up doing was we sampled a bunch of points all around

249
00:25:46,280 --> 00:25:53,320
cities. And then we looked at what, uh, what, uh, what, uh, instances are in a spatial neighborhood,

250
00:25:53,320 --> 00:25:57,800
right? For this, you really need 3D data, right? And then if you know that, you know, that there is

251
00:25:57,800 --> 00:26:02,360
a house, uh, house in front of you, then you can kind of generate based on some language template,

252
00:26:02,360 --> 00:26:10,520
uh, description, uh, that, um, uh, that mentioned these objects, right? And, uh, you can, uh,

253
00:26:10,520 --> 00:26:16,840
you can also mention something about color, for example, uh, of the object that is front of you

254
00:26:16,840 --> 00:26:22,840
based on, uh, extracted RGB color from, from instances or, or semantic class, right? You also

255
00:26:22,840 --> 00:26:27,400
have semantic classes. So in a sense, it's kind of, uh, synthetically generated dataset. You

256
00:26:27,400 --> 00:26:33,480
pick a point and then you can come up with a list of possible descriptions that refer to that point.

257
00:26:34,360 --> 00:26:43,720
Exactly. Yeah. Oh, nice, nice. Is the, the model kind of end-to-end train from natural

258
00:26:43,720 --> 00:26:53,240
language to the, the list of predicted locations? Or are you kind of tokenizing the natural language

259
00:26:53,240 --> 00:26:59,160
and trying to identify the landmarks as an intermediate step? Yeah. So, so model is actually

260
00:26:59,160 --> 00:27:06,200
trained end-to-end, uh, I mean, okay, sure, uh, the course and the final two steps are trained

261
00:27:06,200 --> 00:27:11,000
independently, but other than that end-to-end. So we have, uh, you know, we have point clouds in

262
00:27:11,000 --> 00:27:17,080
coders, point cloud encoders and, uh, and the language encoders. Are the language or point cloud

263
00:27:17,080 --> 00:27:23,880
encoders are using any off-the-shelf, you know, other models, uh, for, for those tasks? Cannot

264
00:27:23,880 --> 00:27:30,200
think of any right now. I mean, our airing, our encoders are fairly simple. Uh, so for the, for

265
00:27:30,200 --> 00:27:36,920
the text, uh, we, we are actually just using LSTMs, not really any of, uh, strong seat of the art

266
00:27:36,920 --> 00:27:45,640
encoders. And, um, for, for point clouds, use point net. You talked about the, the dataset that you

267
00:27:45,640 --> 00:27:51,720
created to train the model, you know, that suggests that maybe you, you, you didn't have any external

268
00:27:51,720 --> 00:27:57,640
benchmarks to compare your performance to, um, but maybe you're publishing this and hoping to get

269
00:27:57,640 --> 00:28:05,720
other folks kind of using the same data set to, to create, uh, um, you know, kind of compete for,

270
00:28:05,720 --> 00:28:11,640
for best performance. Is that kind of the thinking? Mm-hmm. Yeah, this, this, this is exactly what you did,

271
00:28:11,640 --> 00:28:16,680
right? So when, when we started investigating this, there was no, no really data sets or, uh,

272
00:28:16,680 --> 00:28:24,120
or, uh, you know, prior work to, to compare with. So, um, this, uh, this is, uh, this is kind of a

273
00:28:24,120 --> 00:28:31,160
first investigation into, uh, into this, uh, into this topic. Yeah. And do you think your initial

274
00:28:31,160 --> 00:28:37,640
approach did pretty well? And, uh, you nailed it. And there's not a lot of room for folks to,

275
00:28:37,640 --> 00:28:44,280
to one up you or, you know, is it kind of a rough start and there's lots more room to, to go to,

276
00:28:44,280 --> 00:28:50,120
to do better on the dataset? Uh, yeah, I would say that absolutely the latter. I think that,

277
00:28:52,920 --> 00:28:54,920
I guess that's the humility test, right?

278
00:28:58,200 --> 00:29:04,040
Yeah, I, I actually see it more that this, this investigation, uh, that it actually opens more

279
00:29:04,040 --> 00:29:10,200
questions than, uh, than answers almost, uh, I mean, in the sense that, uh, um, you know,

280
00:29:10,200 --> 00:29:15,160
it, earlier, we just talked about this course localization step, right? For example, here, uh,

281
00:29:16,680 --> 00:29:20,840
this, this was one of the outcomes that this is really the bottleneck, right? If you can, of course,

282
00:29:20,840 --> 00:29:25,960
it localize yourself correctly, then, then you, then you become pretty accurate at localization,

283
00:29:25,960 --> 00:29:30,200
but, uh, uh, this, this is definitely right now on the bottleneck. And I think that this is,

284
00:29:30,200 --> 00:29:35,880
it will be the first thing that people will have to look into how to improve. But of course,

285
00:29:35,880 --> 00:29:41,640
the question is now, I mean, of course, I'm sure that people can come up with better networks,

286
00:29:41,640 --> 00:29:47,880
right? And the given data that we have can, uh, can, uh, uh, uh, localize, can improve localization

287
00:29:47,880 --> 00:29:54,120
scores, right? But there's also here just, there's also just so far you can go because there's

288
00:29:54,120 --> 00:29:59,240
this inherent uncertainty, right? Based on one description and not like very precise information,

289
00:29:59,240 --> 00:30:05,480
you could be in any of, uh, locations if you have big area to cover, right? So I think it will be

290
00:30:05,480 --> 00:30:11,960
really important to, uh, as we talked about earlier, right? To, uh, align this with OpenStreetMaps

291
00:30:11,960 --> 00:30:19,800
and, uh, and rely on, uh, more, uh, more discriminative and unique cues for, for describing

292
00:30:19,800 --> 00:30:27,960
location. Uh, second paper that you are presenting at CVPR is focused on forecasting from LiDAR,

293
00:30:27,960 --> 00:30:33,640
via future object detection, or at least that's the title of the paper and the focus is kind of on

294
00:30:33,640 --> 00:30:39,080
joint detection and trajectory prediction. Um, talk a little bit about the motivation for that one.

295
00:30:39,960 --> 00:30:47,640
Sure. Motivation here, um, comes directly from, uh, from robot navigation, right? So a, a, um,

296
00:30:49,000 --> 00:30:55,240
I was talking already earlier about object detection and object tracking, right? Um, so, you know,

297
00:30:55,240 --> 00:31:01,320
object tracking is all about understanding how, how object moved in the past, right? But if you

298
00:31:01,320 --> 00:31:05,640
really want to navigate, it's not important to know where objects were. You have to know where

299
00:31:05,640 --> 00:31:11,400
objects will be, right? And now one way you can tackle this, and this was already tackled in

300
00:31:11,400 --> 00:31:16,680
the community, is that, you know, you're detecting objects, you're associating them over time to

301
00:31:16,680 --> 00:31:21,320
get tracks and then based on past tracks, you can, you know, like train some possibly

302
00:31:21,320 --> 00:31:26,920
outdoor aggressive model that gives you prediction, your object will be, right? Um, but, um,

303
00:31:26,920 --> 00:31:32,600
there are a bunch of problems associated with that, and one of them is that object tracking is

304
00:31:32,600 --> 00:31:40,120
very difficult by itself, right? And the question is, even do you really need, um, the really needs

305
00:31:40,840 --> 00:31:46,040
object tracks to do forecasting, or could you just, you know, encode a sequence of point cloud and

306
00:31:46,040 --> 00:31:52,920
just train network to directly, um, detect objects and predict where they, where they will be,

307
00:31:52,920 --> 00:31:56,680
right? I mean, I'm not saying that tracking is not important. I think it is, but it might be that

308
00:31:57,240 --> 00:32:03,320
model actually implicitly learns by itself what it has to know about past positions of object.

309
00:32:03,320 --> 00:32:09,080
So tracking kind of becomes implicit in, in this case. In a sense, it's, it's a similar

310
00:32:09,080 --> 00:32:16,600
uh, bottoms up versus top down type of, uh, distinction that we were talking about earlier.

311
00:32:16,600 --> 00:32:21,160
The current approaches are kind of top down and that they're trying to identify object

312
00:32:21,160 --> 00:32:25,640
instances and then they have to track them, you know, and they might be included for periods of

313
00:32:25,640 --> 00:32:30,280
time and things like that. And it gets really hard. And your hope is that, hey, just feed it a bunch

314
00:32:30,280 --> 00:32:35,960
of point data and let the network figure it out. It's, uh, I feel that this is kind of, uh,

315
00:32:35,960 --> 00:32:42,920
kind of a lesson learned over and over, right? Just, uh, feed, feed enough data and let,

316
00:32:42,920 --> 00:32:48,120
let the network figure out things by itself. Don't, uh, don't mess with that too much. And,

317
00:32:48,120 --> 00:32:51,880
I mean, I also have to say that by this, I'm not saying that, uh, track is not important.

318
00:32:51,880 --> 00:32:57,160
Right? Tracking is important for a number of, uh, other applications, but when it comes to,

319
00:32:57,160 --> 00:33:04,760
to navigation, it really might be that, uh, that what we need to look into is, uh, forecasting and

320
00:33:04,760 --> 00:33:10,120
not necessarily so much, uh, so much tracking. Although, on the other hand, it's still nice to have

321
00:33:10,120 --> 00:33:17,160
this interpretability aspect of this, right? Uh, to, uh, that, that you get with, uh, with tracking.

322
00:33:18,680 --> 00:33:23,240
And this is maybe jumping way ahead, but, you know, maybe there's some kind of multitask

323
00:33:23,240 --> 00:33:29,960
objective where you're trying to do forecasting, but have tracking be kind of a byproduct of the,

324
00:33:29,960 --> 00:33:36,440
the network that, um, you know, has some of the benefits that multitask learning can provide.

325
00:33:36,440 --> 00:33:42,120
Yeah, this, this makes perfect sense, especially if this hypothesis, uh, that I mentioned earlier,

326
00:33:42,120 --> 00:33:47,640
that if you know how to do forecasting, you somewhat have to know some idea about tracking implicitly,

327
00:33:47,640 --> 00:33:53,160
right? If this hypothesis is correct, then features that we learn to do forecasting should also

328
00:33:53,160 --> 00:33:59,800
benefit tracking, right? So, um, I don't have answer for this yet, whether this is so, but, uh, I think

329
00:33:59,800 --> 00:34:04,440
this is definitely something that we should pick into. So talk a little bit about your method.

330
00:34:05,800 --> 00:34:11,160
Yeah, so, uh, first thing I should say is that, uh, our method is not the first one that tackles

331
00:34:11,160 --> 00:34:17,080
end-to-end detection and forecasting. Uh, there, there have been very interesting paper in this topic,

332
00:34:17,080 --> 00:34:23,800
before, for example, from a group of Raquel Lourdeson from, um, uh, University of Toronto.

333
00:34:23,800 --> 00:34:31,800
And, um, one thing that is, um, quite unique, uh, with our method is that, um, that it actually

334
00:34:31,800 --> 00:34:37,000
offers multi-future interpretation, right? Forecasting is not something that, um, I mean,

335
00:34:37,000 --> 00:34:40,920
forecasting is as, just as in the paper that we were talking about earlier, right? It's inherently,

336
00:34:40,920 --> 00:34:49,400
uh, inherently, um, ambiguous, right? Uh, based on past velocity, you could make several

337
00:34:49,400 --> 00:34:55,320
guess where objects will be, right? And, uh, or, or method is actually capable of that. And

338
00:34:55,320 --> 00:35:02,040
the way that, the way that we achieve this is actually, in the end, really simple. We just

339
00:35:02,040 --> 00:35:09,400
repurpose object detectors for not just detecting objects in the current frame that we just observed,

340
00:35:09,400 --> 00:35:16,520
but also, you know, future frames that we haven't observed yet. Um, so, you know, by encoding,

341
00:35:17,160 --> 00:35:21,400
by encoding temporal sequence, you can just feed the sequence to the network and then say, hey,

342
00:35:21,400 --> 00:35:25,960
detect objects in current frame, but also in future time steps, right? So you just have,

343
00:35:25,960 --> 00:35:30,600
you will have multiple detection heads for future time steps, time steps, and you have supervision

344
00:35:30,600 --> 00:35:37,000
for that radar, you're available, right? But, um, what is nice with this is that, uh, you can then go

345
00:35:37,000 --> 00:35:42,200
instead of, you know, like doing forecasting from time t into the future, you can do something

346
00:35:42,200 --> 00:35:48,360
different and you can go into the future and backcast, right? So from the last detection,

347
00:35:48,360 --> 00:35:54,200
you can backcast vector to previous frame and from there to the previous frame and so forth,

348
00:35:54,200 --> 00:36:02,280
and then you will also have, um, uh, one to many mapping, right? Um, so it might, it might be that,

349
00:36:02,280 --> 00:36:08,680
uh, to one detection from t to t minus one, two detections might connect to this one, right?

350
00:36:08,680 --> 00:36:13,720
In this sense, you have one to many mapping, and this kind of defines this tree of possible,

351
00:36:13,720 --> 00:36:19,160
possible splitting paths. And if you can connect your backcasts all the way back to particular

352
00:36:19,160 --> 00:36:25,720
detection, then you can say, these paths are my forecasts, right? Maybe I'm reading too much into

353
00:36:25,720 --> 00:36:31,320
this, but does this kind of open up some of the graphical types of tools to you? Actually,

354
00:36:31,320 --> 00:36:37,160
we didn't use these tools, but, uh, this, this is one thing that we were definitely thinking next,

355
00:36:37,160 --> 00:36:43,160
that, uh, this is also something that, uh, that we could do even more rigorously by relying on

356
00:36:43,160 --> 00:36:47,480
the graph neural networks, right? What, what we did was actually something, something rather

357
00:36:47,480 --> 00:36:52,600
simple. So we just have multiple network heads that detect objects in future frames and then we

358
00:36:52,600 --> 00:36:57,640
just from each detection and regress a single offset vector backwards and then just based on

359
00:36:57,640 --> 00:37:04,280
a credient distance, uh, say whether some detection based on back-asset vector connects with some

360
00:37:04,280 --> 00:37:11,480
detection from the previous frame. So in, in the end, the, uh, based on a credient distance

361
00:37:11,480 --> 00:37:18,200
of the regressed offset vector. For what did you use for, for data set here? Um, I'm assuming

362
00:37:18,200 --> 00:37:26,120
based on the description that you needed to have, uh, labeled, uh, objects in the, the frames.

363
00:37:26,120 --> 00:37:31,720
Yeah, that's, that's exactly correct. So we used a new since data set. So this is, uh,

364
00:37:31,720 --> 00:37:37,800
one of the, uh, the popular automotive data sets that, uh, that we're using nowadays.

365
00:37:38,600 --> 00:37:44,280
Um, and, uh, yeah, for this, we, of course, need, uh, supervision in the form of, uh, 3D bounding

366
00:37:44,280 --> 00:37:52,440
boxes and, uh, an object tracks, right? Um, so, so we, we, we built on, we built on that.

367
00:37:52,440 --> 00:38:01,320
So in the case of forecasting from LiDAR, uh, and the object detection, uh, you mentioned earlier

368
00:38:01,320 --> 00:38:07,720
that this is a problem that has been well studied and there are existing methods that, uh, that

369
00:38:07,720 --> 00:38:14,360
have been developed to solve this problem. How did your method perform relative to the existing

370
00:38:14,360 --> 00:38:22,920
work? Um, well, I have short and long answer. So the short answer is, it works better. Uh,

371
00:38:24,760 --> 00:38:31,320
okay. The, the, the long answer is, and this was also a really big part of this paper that, um,

372
00:38:31,320 --> 00:38:38,840
metrics used in the past for this problem, for end-to-end detection forecasting, were actually not

373
00:38:38,840 --> 00:38:45,080
the, the right metrics for studying this, uh, this problem. And we actually have, uh, big, big part

374
00:38:45,080 --> 00:38:51,320
of the paper showing this that this metrics can be trivially fooled. And we also have proposed,

375
00:38:52,040 --> 00:38:59,400
new metrics that, uh, that don't have, uh, that don't have, uh, those issues. Um, so I'm not sure

376
00:38:59,400 --> 00:39:04,600
into how much details you want me to go into this because this is something that, uh, I could,

377
00:39:04,600 --> 00:39:09,800
you're actually talk a lot about, but, uh, uh, uh, I'm, I'm curious. I'd like to hear a little

378
00:39:09,800 --> 00:39:18,840
bit more. I mean, it sounds like if the metrics can be trivially fooled, then your method

379
00:39:19,800 --> 00:39:23,880
performs better on both the old metrics and the new metrics. If nothing else, you could

380
00:39:23,880 --> 00:39:30,360
trivially, trivially fooled old metrics. Yeah. Yeah. So this is, this is correct. Or, or method

381
00:39:30,360 --> 00:39:35,720
work, well, on, on both metrics, what sets of metrics, but, uh, we had a, the reason, the reason

382
00:39:35,720 --> 00:39:43,880
why we were looking into evaluation here is, so, um, previous metrics were adopted from, um,

383
00:39:44,680 --> 00:39:50,040
from more traditional forecasting setting that we're studying in the past years. And the other

384
00:39:50,040 --> 00:39:56,040
setting was that someone gives you ground truth trajectories and based on given perfect ground

385
00:39:56,040 --> 00:40:03,160
truth trajectories, just, you know, uh, predict continuation, right? But this is not the case

386
00:40:03,160 --> 00:40:08,760
in end to end forecasting, right? Because you don't have previous trajectories. Yeah. Yeah. So in

387
00:40:08,760 --> 00:40:16,040
this setting, very half past trajectories, uh, they used metrics that are called, um, uh,

388
00:40:17,720 --> 00:40:24,040
average and absolute displacement errors, right? Um, so absolute displacement errors error is quite

389
00:40:24,040 --> 00:40:29,160
simply looking at you, you, you look at your prediction, you look at the ground truth prediction,

390
00:40:29,160 --> 00:40:33,400
and if they're close enough, then, you know, you need to go to forecast, right? But the farther

391
00:40:33,400 --> 00:40:38,920
way you are, you are from the ground truth, the more you are penalized, right? Uh, and, uh,

392
00:40:38,920 --> 00:40:45,400
another, another, um, uh, metric that is usually used, uh, uh, penalizes for, uh,

393
00:40:45,400 --> 00:40:51,480
penalizes, uh, false, false forecasts. So forecasts that are nowhere close to any ground truth,

394
00:40:51,480 --> 00:40:57,720
truth forecast. And, um, when, uh, one thing that, uh, community did when they started looking

395
00:40:57,720 --> 00:41:04,200
into end to end forecasting was that, um, they, uh, evaluated separately object detection.

396
00:41:05,080 --> 00:41:14,840
And then they evaluated, um, this absolute and average, um, uh, trajectory errors with respect

397
00:41:14,840 --> 00:41:21,960
to certain detection recall, right? For example, for 60 or 90 percent recall, right? And what can

398
00:41:21,960 --> 00:41:29,320
then interiory happens is that you train object detector in a way that the detector will focus

399
00:41:29,320 --> 00:41:34,680
on detecting objects for which forecasting is easy. And there is particular type of, uh,

400
00:41:34,680 --> 00:41:39,320
forecast for that. And this, and this is namely our objects that don't move at all, right?

401
00:41:39,320 --> 00:41:45,000
The, the, are just static there. And what is even more problematic is most objects don't move,

402
00:41:45,000 --> 00:41:49,880
right? If you're driving through the city, you see parked cars everywhere, right? And we actually

403
00:41:49,880 --> 00:41:56,040
have shown that it's possible to trick these metrics by a very simple baseline. And this is,

404
00:41:56,040 --> 00:42:02,120
so called, no object moves. Exactly. Exactly. So, so we showed that this was the best approach,

405
00:42:02,120 --> 00:42:05,800
right? Better than, uh, anything proposed in the literature, which, which is of course not

406
00:42:05,800 --> 00:42:09,960
the case, right? Those methods, uh, those methods are great. It's just that the metric was wrong.

407
00:42:09,960 --> 00:42:17,880
I mean, not wrong, but, um, you put the data set or for real world. Yeah. Yeah. Exactly. Exactly.

408
00:42:17,880 --> 00:42:24,040
So our intuition was that we should, um, somehow find a way to, you know, you know, like,

409
00:42:24,040 --> 00:42:30,440
not, uh, evaluation of detection and forecasting together, right? And, um, uh, intuitively,

410
00:42:30,440 --> 00:42:34,120
you know, if you look at object detection community, they really have great evaluation tools,

411
00:42:34,120 --> 00:42:41,080
right? I mean, I mean, average precision is, you know, great metric that's why this virus has,

412
00:42:41,080 --> 00:42:46,760
you know, survived the past test of time, right? So our intuition was that, uh, we should use

413
00:42:46,760 --> 00:42:53,320
in one way or another, uh, MAP, uh, mean, average precision. And it turned out that this, uh,

414
00:42:53,320 --> 00:42:58,760
this, we can do this quite easily. Everything that we need to do is, we need to change, um,

415
00:42:58,760 --> 00:43:04,040
um, the matching criterion, right? What is considered to be true positive, false positive and false

416
00:43:04,040 --> 00:43:12,680
negative, right? And what we said was, um, if you correctly detect an object and correctly forecast

417
00:43:12,680 --> 00:43:18,360
that, right? And these two, um, so that you both have correct detection and correct forecast,

418
00:43:18,360 --> 00:43:24,040
then you have a true positive. So this, uh, this metric we call forecasting, uh, forecasting,

419
00:43:24,040 --> 00:43:30,440
uh, mean average, mean average precision. So I wanted to, uh, make sure we cover the third paper

420
00:43:30,440 --> 00:43:38,120
as well. Uh, you are a busy guy at CVPR. That one is opening up open world tracking. Um,

421
00:43:39,080 --> 00:43:44,440
yeah, tell us about that paper. What's the motivation there? Yeah, gladly. So this, this is

422
00:43:44,440 --> 00:43:52,440
actually really paper that I'm extremely excited about because, uh, uh, because the motivation

423
00:43:52,440 --> 00:43:58,840
dates way back in days when I was still doing my, uh, my PhD, right? And I already touched earlier,

424
00:43:58,840 --> 00:44:03,720
this problem of, uh, that you need to be able to track any objects, right? We can't expect that, uh,

425
00:44:03,720 --> 00:44:10,280
we have training data for, uh, for everything. And during my PhD, I was working on trackers for

426
00:44:10,280 --> 00:44:16,600
tracking any object, but once, but here we really have fundamental problem, right? You would come

427
00:44:16,600 --> 00:44:20,360
up with the model and then you have to say, how good your model is, right? But how, how you're

428
00:44:20,360 --> 00:44:25,240
going to do that, right? Back then, a few years back, we didn't have data sets or benchmarks or,

429
00:44:25,240 --> 00:44:30,440
you know, even the right metrics to talk about this. So I always had to find some

430
00:44:30,440 --> 00:44:35,960
hacky ways to, you know, make a point that's, uh, what we are proposing works, right? But, uh,

431
00:44:35,960 --> 00:44:41,880
there was never like really a right rigorous way or, you know, benchmark that would really

432
00:44:41,880 --> 00:44:46,680
help community to rigorously evaluate method study progress, compare different methods,

433
00:44:46,680 --> 00:44:53,400
apple to happen. So, um, this, the motivation for this paper is really to come off with tools

434
00:44:53,400 --> 00:44:59,640
to do this so that community can, can make progress. And this is what this paper largely is about.

435
00:44:59,640 --> 00:45:04,440
So this paper doesn't really propose a drastically new method for this. It's more,

436
00:45:05,720 --> 00:45:11,960
more than that, it, uh, rather proposes a test bet for evaluating this and kind of, uh,

437
00:45:11,960 --> 00:45:17,720
we have experimental evaluation that records holidays, different, uh, different contributions

438
00:45:17,720 --> 00:45:23,240
in tracking, but it is stealing out a good tracker, a good simple baseline for, uh, for this task.

439
00:45:23,880 --> 00:45:32,200
The tracker is a metric or a tool and environment. We actually, uh, provide, um, a benchmark for

440
00:45:32,200 --> 00:45:39,400
this and this benchmark consists of a data set, uh, evaluation metric and baselines, right? But

441
00:45:39,400 --> 00:45:45,240
the data itself, it's not something that we, uh, recorded. For, for data set itself,

442
00:45:45,240 --> 00:45:50,280
we repurposed, uh, tau data set. It's called tracking any object data set and it comes from

443
00:45:50,280 --> 00:45:54,440
our, uh, collaborators, uh, back then we were collaborators when we started this, I wasn't

444
00:45:54,440 --> 00:46:00,200
at Carnegie Mellon yet. Uh, so this comes from the group of, uh, of, uh, Deva Ramon, uh, whom I'm,

445
00:46:00,200 --> 00:46:08,600
also now, uh, working with, but, um, um, tau data set by itself was released to study tracking

446
00:46:08,600 --> 00:46:14,040
in the long tail, right? But not in the open world. And with this paper, we repurposed it for studying,

447
00:46:15,000 --> 00:46:23,480
studying object tracking in the open world. And what we did was, um, we, we split training

448
00:46:23,480 --> 00:46:29,400
and test data as follows. We, for training the models, we proposed to use

449
00:46:29,400 --> 00:46:34,760
cocoa data set that has labels for 80 object classes, right? And then in tau data set, we have labels

450
00:46:34,760 --> 00:46:41,000
for, uh, several more, uh, objects, right? For, for hundreds of, uh, object classes. And we then

451
00:46:41,000 --> 00:46:47,000
define a test bed such that, uh, you train your models with knowledge of 80 classes, then you do

452
00:46:47,000 --> 00:46:52,920
validation on additional set of semantic classes, right? Then you have additional test set in which you

453
00:46:52,920 --> 00:46:57,640
have semantic classes that don't even appear on validation set, right? So that you make sure that, uh,

454
00:46:57,640 --> 00:47:01,800
nothing is, you know, kind of leaking from one speed or of another. And here is actually one

455
00:47:01,800 --> 00:47:07,240
important distinction because usually you make sure that when you have a training test and

456
00:47:07,240 --> 00:47:12,440
validation speed that the data does no overlap, right? And you also have to make sure that, uh,

457
00:47:12,440 --> 00:47:17,640
that semantic classes don't don't overlap, right? So that in each set, you have semantic classes that

458
00:47:17,640 --> 00:47:25,080
you can say, okay, these are the unknowns. And I, uh, I didn't train anything or tune any parameters

459
00:47:25,080 --> 00:47:32,280
with knowledge of, of these classes. Got it, got it, got it. So the, when you say open world here,

460
00:47:32,280 --> 00:47:38,200
are you using that term colloquially, or does that have a specific meaning, or does it refer to

461
00:47:38,200 --> 00:47:43,960
a specific environment in this context? So, so the term itself open world is of course borrows

462
00:47:43,960 --> 00:47:49,080
from the community, right? You, you probably know about this, uh, this classic work from, uh,

463
00:47:49,080 --> 00:47:56,360
um, or Bolton, uh, Bendale, right? Where they studied open world recognition. And they're,

464
00:47:56,360 --> 00:48:01,480
they're, they study, study open world recognition in a sense that, uh, you know,

465
00:48:01,480 --> 00:48:05,640
you have some certain close set of object classes in which you train your model. And then, uh,

466
00:48:05,640 --> 00:48:09,320
during the deployment phase, you will see new objects you haven't seen before. You have to

467
00:48:09,320 --> 00:48:15,960
recognize them. Then you kind of have to ask annotators to label them, right? Um, so we borrowed

468
00:48:15,960 --> 00:48:26,360
part of this, um, part of this idea behind this, right? So we also have some, uh, closed world

469
00:48:26,360 --> 00:48:32,120
data set, right? In which we have some finite set of classes labeled. And then we study, study

470
00:48:32,120 --> 00:48:37,160
performance of those models in the open setting, in which classes that we hadn't seen during training

471
00:48:37,160 --> 00:48:43,720
also appear, right? So it's describing the setting. I think I was envisioning like a simulation

472
00:48:43,720 --> 00:48:48,520
environment that was a world that an agent would explore kind of thing, but that's not really what

473
00:48:48,520 --> 00:48:54,520
we're talking about here. Did you also, so you provide this data set, uh, and this kind of benchmarking

474
00:48:54,520 --> 00:49:02,920
approach, did you also, uh, provide, uh, kind of proof of concept models, you know, did you kind of

475
00:49:02,920 --> 00:49:09,080
bootstrap the effort? And what does that look like? Yeah, so this, this I would say it's, it's

476
00:49:09,080 --> 00:49:15,480
something that is rather simple and builds on really on contributions from multi-object tracking

477
00:49:16,520 --> 00:49:22,520
community. So, uh, the tracker itself is at the end of the day really follows this tracking by

478
00:49:22,520 --> 00:49:27,720
detection paradigm, right? Which means that you have some kind of object detector that gives you

479
00:49:27,720 --> 00:49:32,680
possible object detectors in each frame. And then by some means you connect these detections

480
00:49:32,680 --> 00:49:38,360
over time, right? Uh, but they're of course certain important differences. And first one is that, um,

481
00:49:38,360 --> 00:49:43,080
you're, uh, that you cannot really, you know, train object detector for object class for which you

482
00:49:43,080 --> 00:49:50,200
have no labels for, right? So, um, this object detector is a bit more like object proposal generator.

483
00:49:50,200 --> 00:49:56,120
I touch this briefly at, uh, at the very beginning. So we just repurposed mass carsia and then,

484
00:49:56,120 --> 00:50:02,200
so in particular, the region proposal mechanism of mass carsia and then, but, uh, but we also made

485
00:50:02,200 --> 00:50:08,920
sure that we obtain, um, object instant segmentations for, for each proposal, right? And these proposals

486
00:50:08,920 --> 00:50:13,480
are then kind of, you know, you have lots of them in the image and they are like kind of input to

487
00:50:13,480 --> 00:50:20,440
the, to the tracker and then tracker, uh, figures out over time which proposals are temporally

488
00:50:20,440 --> 00:50:25,560
stable, right? Which it can connect over time and those will be then or, or, or object tracks.

489
00:50:25,560 --> 00:50:34,040
When you describe the, the simple method, uh, kind of this, this proof of concept method that you

490
00:50:34,040 --> 00:50:39,000
ran against the benchmark that you're proposing, it sounded like what we described as kind of this

491
00:50:39,000 --> 00:50:44,760
top down, uh, traditional approach where you have these objects and you're trying to track them

492
00:50:44,760 --> 00:50:49,880
across time. Uh, the, you know, objects from bounding boxes that you're trying to, to track across

493
00:50:49,880 --> 00:50:58,680
time. When we previously spoke about the, um, forecasting from LIDAR paper, we kind of contrasted

494
00:50:58,680 --> 00:51:04,200
that top down approach with the bottoms-up approach that, you know, starts at, you know, point cloud

495
00:51:04,200 --> 00:51:12,440
or in another setting, maybe pixels. And I'm kind of wondering if the methods that you developed

496
00:51:12,440 --> 00:51:19,640
in that paper could be a future direction for someone tackling this open world tracking problem

497
00:51:19,640 --> 00:51:24,200
or are they, you know, I'm really trying to test my understanding and see how they fit together

498
00:51:24,200 --> 00:51:30,680
or are they totally unrelated and I'm off here. No, so it's, it's, it's, uh, it's not, uh, it's not

499
00:51:30,680 --> 00:51:35,720
totally unrelated. So, uh, there, there are some gaps, definitely because what I was talking about

500
00:51:35,720 --> 00:51:41,320
earlier at the beginning, it was mainly 3D computer vision and it was, uh, largely, uh, relying on

501
00:51:41,320 --> 00:51:47,720
3D sensors, right? And it's opening up open world tracking is purely image-based. So, uh, here,

502
00:51:47,720 --> 00:51:54,760
here we haven't talked about 3D at all, but there is one thing, you, uh, that, uh, if, if I

503
00:51:54,760 --> 00:52:00,920
understood correctly, um, that, um, you were asking if, um, we could, uh, instead of doing this

504
00:52:00,920 --> 00:52:05,720
separation of detection and then linking things, uh, to do something more like we did in the previous

505
00:52:05,720 --> 00:52:12,280
two papers and just do everything in end-to-end manner, right? Um, and, uh, indeed, the community,

506
00:52:12,280 --> 00:52:17,240
community is definitely moving in direction of doing multi-object tracking in end-to-end manner.

507
00:52:17,240 --> 00:52:23,720
They are, they are several very nice, uh, approaches out there to do that, uh, methods that

508
00:52:23,720 --> 00:52:30,440
base, based on graph neural networks or just, you know, regress, uh, targets or, or use, uh,

509
00:52:30,440 --> 00:52:34,200
end-to-end transformer-based detectors for tracking simulation. So, there are many of them.

510
00:52:34,760 --> 00:52:43,080
Um, but, uh, this particular dataset and challenge, uh, makes things especially difficult,

511
00:52:43,080 --> 00:52:48,280
uh, because most tracking dataset before focused on tracking pedestrians or cars and pedestrians,

512
00:52:48,280 --> 00:52:52,680
right? And you will have a few objects in the scene and, uh, shorter sequences, and this dataset

513
00:52:52,680 --> 00:53:00,200
is really huge and you have jungle of objects pretty much, right? And, uh, we just, uh, haven't

514
00:53:00,200 --> 00:53:07,640
been able to, to apply those methods, methods to this problem, uh, this problem yet. We, you very

515
00:53:07,640 --> 00:53:14,040
quickly run into issue with, uh, memory and, uh, training time and, and so forth. So,

516
00:53:14,040 --> 00:53:21,080
I think that the method that is closer, closest to being end-to-end is, uh, is tractor,

517
00:53:21,080 --> 00:53:28,920
that also comes from, uh, from Laura's group. Uh, but, uh, tractor was, uh, was, uh, um,

518
00:53:29,960 --> 00:53:34,920
was, uh, lagging behind, uh, uh, the baseline that you proposed.

519
00:53:34,920 --> 00:53:41,960
Well, Aliyasha, uh, it's been really wonderful learning a bit about your research and CVPR

520
00:53:41,960 --> 00:53:49,160
papers. Um, we'll, of course, have links to the papers on the show notes page, uh, for folks to

521
00:53:49,160 --> 00:53:55,160
check out the full details, uh, but wanted to thank you for taking the time to join us and share

522
00:53:55,160 --> 00:54:01,640
a bit about, uh, what you've been up to. Yeah, I would, I would also like to thank you for, uh,

523
00:54:01,640 --> 00:54:09,320
for inviting me here. I, uh, really had a great time, uh, discussing, uh, or, or, or recent research,

524
00:54:10,360 --> 00:54:20,520
so it was a really nice discussion and, uh, I'm, uh, um, I hope to see you at CVPR as well and, uh,

525
00:54:20,520 --> 00:54:25,640
also, also encourage audience to come to, or posters or just if you see me anywhere, just

526
00:54:25,640 --> 00:54:32,360
wave and I'm always happy to chat about research and your end of time. Awesome. Thanks so much.

527
00:54:32,360 --> 00:54:57,880
Thanks. Thank you. Bye-bye.

