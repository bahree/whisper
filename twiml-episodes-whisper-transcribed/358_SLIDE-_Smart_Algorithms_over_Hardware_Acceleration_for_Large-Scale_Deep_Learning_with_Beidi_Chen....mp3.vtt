WEBVTT

00:00.000 --> 00:05.440
All right, everyone. I am on the line with Beaty Chen. Beaty is a PhD candidate in the Department

00:05.440 --> 00:11.680
of Computer Science at Rice University. Beaty, welcome to the Twomo AI podcast. Thanks.

00:12.880 --> 00:19.120
So I had the opportunity to hear Beaty's presentation at the systems workshop.

00:19.120 --> 00:24.560
MLs always get confused. ML for system systems for ML. This was systems for ML, correct?

00:24.560 --> 00:30.160
Yeah. They changed the name recently to we submit this paper to the conference. They first

00:30.160 --> 00:35.520
have it called MLs. And then they later change it to system because the trademarks stuff.

00:35.520 --> 00:41.920
Yeah. Well, I think there are two different workshops with very similar with all the same

00:41.920 --> 00:52.480
words, but in different orders. Yes. But in any case, you presented at the workshop on your

00:52.480 --> 00:59.040
paper slide, which we will go into. But before we do that, share with us a little bit about your

00:59.040 --> 01:06.080
background, what sparked your interest in machine learning, particular on kind of this intersection

01:06.080 --> 01:14.320
of hardware and algorithms. How did this all come about? Sure. So before I study my PhD at rice,

01:14.320 --> 01:20.880
I would have my anagra in University of California Berkeley. And I was actually doing some

01:20.880 --> 01:26.800
network's research. It's kind of still computer science, but a little bit irrelevant.

01:27.760 --> 01:34.400
But that kind of yeah, that actually I did my grad my grad work and networks also like

01:34.400 --> 01:38.000
stochastic modeling and all that kind of stuff. Is that the stuff you were doing?

01:38.000 --> 01:45.200
No, not the same different kind of networks. Yeah. Okay. Yeah. So we're kind of doing like a

01:45.200 --> 01:53.280
software defined building. Yeah. So that's what's kind of close to the AI part because you want

01:53.280 --> 01:58.080
to automate it in some process and you want to predict something for people. Yeah.

01:59.360 --> 02:05.600
Yeah. Then when I come to rice, I get interested in the machine learning algorithms because I saw

02:06.560 --> 02:11.920
I always have this in mind. All the problems we're trying to attack in computer science,

02:11.920 --> 02:18.480
not all like most of it, the bottlenecks are usually the computation. Not all the problems are

02:18.480 --> 02:24.720
showing this part, but if you think about it, we usually have a brute force way of doing things.

02:24.720 --> 02:31.600
And if we do have the computation of power or we have the time to achieve it, then the problem

02:31.600 --> 02:37.600
stops. But the problem is we usually don't have that time and computation of power. So for the

02:37.600 --> 02:44.080
algorithm part, what is good is reducing from big O N square to big O N or big O N to cancel

02:44.080 --> 02:50.880
in is usually making that possible. So on the way I'm doing research is kind of I always think

02:50.880 --> 02:55.600
about what is the brute force way, what is the naive way I can chip this and can I speed it up?

02:56.480 --> 03:01.600
Either in time wise or reducing the memory, any part that is blocking this happening

03:01.600 --> 03:08.640
to produce the optimal solution and we can do approximation. And that's why I kind of doing all

03:08.640 --> 03:13.760
the randomized algorithms in my PhD because that's going to help with the memory and time.

03:14.240 --> 03:20.640
Meaning the algorithms are take advantage of randomization or you just are working on a bunch

03:20.640 --> 03:27.600
of random different projects. That is also true because you need our special weapon,

03:27.600 --> 03:32.240
this randomized the algorithm called locality sends to be hashing all kind of applications

03:32.240 --> 03:43.280
to tackle different problems. Both. Got it. Got it. So this locality hashing is kind of well

03:43.280 --> 03:50.800
obviously at the core of the paper you presented at the MSS workshop slide. What is the full name of

03:50.800 --> 03:58.400
that paper? Let me see we changed a bunch of times. I think on the first version we're trying to

03:58.400 --> 04:05.120
saying that we're defending for the smart algorithms over specialized hardwares. And then we found

04:05.120 --> 04:11.840
a kind of offensive to some people because we're not saying hardware development is not good. We're

04:11.840 --> 04:18.400
just saying that we also need more progress on the algorithm side. So we're just changing the name

04:18.400 --> 04:26.560
back to we're beating like side trying to beat GPU over like CPU stuff. So it's like the result

04:26.560 --> 04:34.400
of the paper. So nobody gets offended by the CPU GPUs beating up on CPUs except Intel, right?

04:37.440 --> 04:42.240
Yeah, we actually have three more collaborators after the first version of the paper came out.

04:42.240 --> 04:48.000
They also help us further speed it up and by using all those Intel tricks.

04:48.000 --> 04:54.400
Before we get into the kind of the core innovation of the paper, what is the motivation? You've

04:54.400 --> 05:00.800
hinted at this also. Everything takes too long but maybe talk a little bit more about the specific

05:00.800 --> 05:07.360
problem that you're trying to solve with this paper. Right. So we all know that now all the

05:07.360 --> 05:13.840
applications like vision and LP and they all kind of have the state of art by using the neural

05:13.840 --> 05:18.560
networks. But the problem one bottleneck of the neural network is the computation. That's why

05:19.840 --> 05:26.560
the inspiring GPUs were invented and then people have the computation of power to kind of do this

05:26.560 --> 05:31.680
all complicated computations. So the major bottleneck is still the matrix multiplication.

05:32.400 --> 05:38.720
And if you can do like large matrix multiplication really fast, then that's kind of solving

05:38.720 --> 05:45.520
half of the computational problem in your networks. And that's why we started from those. The problem

05:45.520 --> 05:52.160
which is like called extreme classification. So basically, for example, you have your Amazon

05:52.160 --> 06:00.000
user. So you're typing something like I want like key shoes. And then in the back end, hopefully

06:00.000 --> 06:08.960
you're going to have a black loss which can help you decide which products to show to you.

06:08.960 --> 06:13.920
Then this kind of reduced to like extreme classification because you're typing a query that's

06:13.920 --> 06:19.600
your query belong to for 1 million classes, which one is it? One class can be added as shoes.

06:19.600 --> 06:26.000
One class can be like Nike run issues like that. So for this kind of problem, the major computation

06:26.000 --> 06:31.600
is the last layer because you have 1 million classes. Then you have to do all this,

06:32.240 --> 06:39.440
the major computation over there. And then we're thinking about is GPU the only way to solve this

06:39.440 --> 06:45.200
problem. And because we realized for this kind of problem, even GPU, the speed up is not enough

06:45.200 --> 06:52.320
because like the high memory excess. And then we're thinking about all the computations necessary.

06:52.320 --> 06:58.640
Then the answer is actually no because majority of it is redundant because in the neural networks,

06:59.600 --> 07:06.320
if like the heater layers, usually you care about the high activations, which is like to

07:06.320 --> 07:13.840
inner product producing high results. And for the last layer is like even worse because for those

07:13.840 --> 07:20.400
kind of extreme classification, what you care is about Nike shoes, Adidas shoes, but you don't

07:20.400 --> 07:26.480
care about a headphone. So those irrelevance of what the computation is kind of wasted.

07:27.120 --> 07:32.960
But the problem is before you do the computation, you don't know which part are important,

07:32.960 --> 07:40.400
which part does not. And then the hatching algorithms is exactly good for these kind of situations,

07:40.400 --> 07:46.080
kind of predict which computation are going to give you like high density so that you just compute

07:46.080 --> 07:52.560
those to go through approximation of the few for the full computation. And then in this way,

07:53.360 --> 07:59.120
all the computations in the neural networks will be sparse and including the backwards.

08:00.480 --> 08:04.640
So that's how we come up with the problem and find the solution.

08:05.600 --> 08:12.160
So you mentioned hatching algorithms are useful here and telling you which of your

08:12.160 --> 08:19.600
computations are going to be useful. Why and how is that? Because usually locality sensitive

08:19.600 --> 08:26.720
hatching is very useful for the search or ranking industry. The reason why it is good for finding

08:26.720 --> 08:33.280
the nearest neighbors. And in the in the neural network space, what is near neighbors is like if two

08:33.280 --> 08:39.680
vectors attend to have the similar direction, then the inner product will tend to be higher.

08:39.680 --> 08:46.240
Let's assume that a uniform norm here. So in this way, vector A and B are neighbors

08:46.240 --> 08:52.400
if they can produce high inner product. The hatching is kind of if you know vector A,

08:52.400 --> 08:59.280
they will, they will, we can spend cash in time find all the b's which are close to a

08:59.280 --> 09:06.800
neighbors of A and producing those b's which has the higher inner products with A. And that's why

09:06.800 --> 09:13.760
I call it like a useful algorithm exactly for this kind of situation. Got it. So locality sensitive

09:13.760 --> 09:18.400
hatching, that's something that has been around. That's not something you created. Yeah. That's

09:18.400 --> 09:24.800
been like 20 years or so. Been around for 20 years. And what are some of the other applications

09:24.800 --> 09:32.640
in which it tends to come up? You mentioned search. Yeah. Usually so for locality sensitive hatching,

09:32.640 --> 09:41.520
one big application is if you have let's say 1 million images or 1 billion images, you can't

09:41.520 --> 09:49.120
and then you have a image of query on hand, for example, a cat. You cannot search linearly for

09:49.840 --> 09:57.280
the 1 billion images in a database because that's kind of cause you take a long time. Yeah.

09:57.280 --> 10:04.240
So one, the only smart way is you save the 1 million images based on some kind of structures,

10:04.240 --> 10:10.240
which we call it like neighbors goes to the same bucket so that whenever you have a query,

10:10.240 --> 10:15.520
they will quickly use the same way to compute the hash code and go to the bucket which contains

10:15.520 --> 10:21.120
all the neighbors so that your search time is bigger one. Otherwise, when you type something

10:21.120 --> 10:27.760
on Google, it's probably take you ages to search for all the information currently we have now. So

10:27.760 --> 10:33.120
we have to reduce the search time to big old lines that have big old end and that's all the

10:33.120 --> 10:42.560
hashing algorithms are useful. Producing the hash is it kind of a deterministic process or is

10:42.560 --> 10:47.520
there a formula or is it a learned process? Are you learning the hashes across the data set?

10:47.520 --> 10:54.160
So there are two categories. One is the learned one and we're actually on the other side is

10:54.160 --> 11:00.320
like data independent. But the hash functions are not deterministic is joined from some distribution

11:00.320 --> 11:07.680
and all the hash functions are all in guarantee that the collate like the for example, you have two

11:07.680 --> 11:15.680
item are exactly the same and then the probability that searching that element will be one. But if the

11:15.680 --> 11:22.000
element has some minor differences, then the probability will be different. So by calling probability

11:22.000 --> 11:28.320
is each time you generate the hash function is probably different ones. So we can only say that

11:28.320 --> 11:33.600
it's with high probability we're going to retrieve the neighbors. It seems like if the hash function

11:33.600 --> 11:40.000
is going to determine the proximity of different pieces of data it needs to know something

11:40.000 --> 11:47.280
structural about the data and I'm wondering then does that mean that you have to do a lot of

11:47.280 --> 11:56.880
work up front to create this hash function based on a given data set and then anytime you want to

11:56.880 --> 12:04.720
look at a different type of data do you have to redo that? No, but we actually provide the freedom

12:04.720 --> 12:11.920
for users to implement their own hash functions. So there are four things we're providing for

12:11.920 --> 12:18.560
different kind of similarities. I think for hash functions, there are two lines over the first

12:18.560 --> 12:23.840
one is the learned hash functions. We're currently not going to that line but people have freedom to

12:23.840 --> 12:29.840
adding those solutions to our framework as well. But the second one is we're focusing on the

12:29.840 --> 12:36.480
data independent one. By data independent, I'm saying that not only based on the data but also

12:36.480 --> 12:42.800
based on different similarities. For example, the most useful ones in our framework currently

12:42.800 --> 12:49.680
is cosine similarity. I would just talk about if you want two vectors to determine which vectors are

12:49.680 --> 12:55.440
around one query vector, then cosine similarity might be the most useful one because that's kind of

12:55.440 --> 13:01.760
determining the angle, which is the direction. And another one is like the ranking. So they don't

13:01.760 --> 13:08.160
care about the normal vector or like the sequence of the vector, but they care about the rank like

13:08.160 --> 13:13.600
for a given vector. The first one, the first dimension is larger than the second, the second is

13:13.600 --> 13:18.640
larger than the foot. Like this kind of preserved order, that's extremely useful for images.

13:18.640 --> 13:25.440
And there are a third one, which is called main hash, which is kind of preserved the

13:25.440 --> 13:31.680
jacot similarities, like for a two given set is preserved the similarity between it. So it's

13:31.680 --> 13:38.400
actually for different similarity and different kinds of data, you can use different kinds of

13:38.400 --> 13:47.120
hash functions to achieve the same guarantees. And I'm recently also getting to the learn hash

13:47.120 --> 13:53.680
function that part, but the one particular problem that is extremely trouble our framework is,

13:53.680 --> 14:00.960
our framework is not, we're going to change the hash functions or review the hash tables once in a

14:00.960 --> 14:07.040
while because we want to preserve the randomness, but the rebuilding part for those learned hash

14:07.040 --> 14:13.440
functions will be very long. Yeah, so that's a good thing about the data independent hash,

14:13.440 --> 14:22.080
because it is not time consuming. But the interest in the learned hash functions is that they

14:22.080 --> 14:28.960
might be more accurate or allow you to express different types of similarity. Yeah, that is

14:28.960 --> 14:36.320
correct. That's why we are seriously considering this case in our framework, because our frame

14:36.320 --> 14:45.200
ultimately wants to spend very less time predicting which neurons should be active, and then do

14:45.200 --> 14:52.080
all the computation on CPU. There's actually very hard tasks. You have to do it very quickly.

14:52.720 --> 14:58.400
Otherwise, that overhead is going to kill you. So that's why I'm talking about like for the

14:58.400 --> 15:05.920
learn hash function, although it's very accurate, it's going to help us like spend maybe 10 times, if the

15:05.920 --> 15:15.040
current time of computing hash functions, and that's going to make the framework maybe comparatively

15:15.040 --> 15:23.760
okay with GPU, but not obvious speed up. So the solution is doable. I think there's a time and

15:23.760 --> 15:30.800
accuracy trade-off here, but we think the independent hash functions are along the sweet spots.

15:31.360 --> 15:37.840
And so what's the relationship between the hash function and the idea of adaptive sparsity?

15:37.840 --> 15:45.120
That's a concept that you mention in the paper. Right. So just considering the random dropout

15:45.120 --> 15:50.640
is actually like each time you're just giving sparsity to the neural network randomly,

15:50.640 --> 15:58.160
so that you're is highly possible that you're going to meet some important parts if you do that

15:58.160 --> 16:06.560
randomly. So by adaptive sparsity here is that's why like for the random dropout, the sparsity can

16:06.560 --> 16:14.720
maximum goes to like 50% or so, otherwise you will decrease your accuracy. But we do another

16:14.720 --> 16:23.280
experiment and it's actually finding a lot of like previous papers to if we can compute all the

16:23.280 --> 16:32.000
computations and we only take maybe 5% or 1% of nodes as the new of the active nodes, then the

16:32.000 --> 16:38.560
accuracy will not drop. But the problem is you have to make the huge computation to know which ones

16:38.560 --> 16:45.600
we're going to produce you the top ones. So that's inspiring us like we do approximate like 5%

16:45.600 --> 16:54.160
or 10%. But so the speed is kind of close to the random dropout, but the effectiveness of it

16:54.160 --> 17:00.880
is close to the adaptive top K. So that's why we find the sweet spot in this too.

17:00.880 --> 17:07.200
Does that mean that you're applying the location sensitive hashing in two places?

17:07.200 --> 17:14.480
One at the very end for your classifier and then also more deeply in the dropout.

17:15.120 --> 17:23.040
So dropout is actually in parallel with our framework. I'm just giving an example of how the

17:23.040 --> 17:29.120
hashing is going to help. If hashing is doing a random selection of the nodes is exactly the same

17:29.120 --> 17:37.760
as the dropout. But if is computing the real top K, then that's called the the adaptive

17:37.760 --> 17:44.320
sparsity. But it's somewhere in between it's using the same time is very efficient as the random

17:44.320 --> 17:51.680
dropout because the query only take bigger one time. But it's going to retrieve you the nodes that

17:51.680 --> 17:59.120
is highly possible in that top K. So I'm just like giving an intuition of what hashing with selecting.

17:59.120 --> 18:07.280
It's nothing but just selecting adaptively the active nodes for each layer. So what is very

18:07.280 --> 18:13.920
useful in the case of extreme classification is because of the full computing like the majority

18:13.920 --> 18:19.440
of the computation is in the last layer because of the number of classes. So that's going to reduce

18:19.440 --> 18:25.440
a lot of computation. But for the usual fully connecting neural network, each layer we can

18:25.440 --> 18:32.560
build different hash tables and we can do is sparsity over each layer. So that's not restricted

18:32.560 --> 18:36.800
to the last layer. Are there other elements of this approach that we should be sure to cover?

18:36.800 --> 18:45.120
There's the application of the hashing. Do you have to do anything special in terms of your

18:45.120 --> 18:51.360
data collection or preparation or should you just be able to apply this technique?

18:53.360 --> 19:00.240
So we're actually exploring two directions of extension to this work. So this is our current

19:00.240 --> 19:07.280
framework support the fully connected neural networks. But people are more interested in when they

19:07.280 --> 19:13.760
have like the NLP or vision problem they want the CN or LSDM. Currently what we can do is at least

19:13.760 --> 19:21.360
for last layer we can apply the same technique. But the problem is for CN or LSDM in a mid like those

19:21.360 --> 19:28.960
heat layers we have to investigate there to see what advantage is the hash in the approach.

19:28.960 --> 19:35.040
And for not a line of work we're thinking about the distributed because it's well known that for

19:35.040 --> 19:42.640
the distributed setting the computational bottleneck is the communication. And our slide has

19:42.640 --> 19:50.880
is natural advantage here because of the we're only selecting for example 1% or 5% of the active

19:50.880 --> 19:58.000
nodes for each layer so that in the back propagation the gradient updates is also very sparse.

19:58.000 --> 20:03.200
So that for the distributed setting this is natural very efficient.

20:03.840 --> 20:11.840
For the same reasons that we've talked about previously intelligently using the hashing to kind

20:11.840 --> 20:18.160
of identify and strip out the sparsity. Yeah. How have you kind of measured the results and

20:18.160 --> 20:24.640
compared this approach to others and then what kind of results have you seen? Yeah for the results

20:24.640 --> 20:33.680
part that's the most impact for people care about. So for GPU comparison we're using the current

20:33.680 --> 20:42.320
state of art framework TensorFlow and also use also run the same program on a via 100 GPU which is

20:42.320 --> 20:49.680
currently kind of the state of art people use to run to perform all the experiments for those

20:49.680 --> 21:01.280
applications. And also for TensorFlow CPU we also try to see that for all those CPU tricks

21:01.280 --> 21:08.080
they're using are we beating them? And obviously you will be slower than the TensorFlow GPU

21:08.080 --> 21:15.040
but we still do some comparison there to see like there's three lines which ours is the most

21:15.040 --> 21:23.520
efficient and then TensorFlow GPU on via 100 and TensorFlow CPU on the same CPU machine we're using

21:23.520 --> 21:30.320
because we want to still compare with the CPU otherwise we can use a super powerful CPU machine

21:30.320 --> 21:36.880
and then that's then using that to be the GPU is not necessary for people because we want people

21:36.880 --> 21:44.320
to do the same thing on CPU which is not that costly as using like a via 100 or this kind of

21:45.520 --> 21:53.680
expensive GPU machines. And also there's a nice finding about how many cores we include

21:53.680 --> 22:02.960
in our CPU computation so that we can beat the TensorFlow GPU. It seems like 8 cores to 16 cores

22:02.960 --> 22:15.120
is enough to outperform the same task on GPU via 100. So although we're in the in the paper

22:15.120 --> 22:22.800
we're using like a 44 core machine but we do the obligation study to see where it intersects

22:22.800 --> 22:29.920
the performance is actually 8 to 16 cores somewhere in the middle. So with 8 to 16 cores you're

22:29.920 --> 22:39.840
able to to what outperform TensorFlow on a GPU? Yes. Okay. What's the problem that you're

22:39.840 --> 22:47.120
that you're benchmarking this on? Those are extreme classification tasks. Oh, extreme classification.

22:47.120 --> 22:54.240
And can you talk a little bit about are there established data sets and benchmarks for extreme

22:54.240 --> 23:01.280
classification? Can you talk a little bit about those? Yes. So there's a whole repo about all those

23:01.280 --> 23:08.320
for example the example that I gave earlier for Amazon data is usually like yeah you have query

23:08.320 --> 23:12.960
and you have all the products that's like extreme classification or sometimes you have

23:12.960 --> 23:18.800
a user typing query and then you have a search results for the finally the website you are

23:18.800 --> 23:25.360
clicking. That's like another case. Yeah. So all those benchmarks including one repo, all the

23:25.360 --> 23:33.920
extreme classification field like people use those as the benchmarks. For the Amazon you said

23:33.920 --> 23:42.320
how many classes are there? For the data set we're using is 670K. Okay. And then use another

23:42.320 --> 23:49.440
delicious data set that's like the one where you're predicting links. Yeah, it was like 200K.

23:50.080 --> 23:56.240
So you've got a first set of diagrams where you've got the three lines and that one is looking at

23:56.240 --> 24:02.960
the slide accuracy and then you have time on the bottom what is the time? Is that like convergence time

24:02.960 --> 24:08.880
or else? Yes, like training time. Training time? That's a lot scale. Yeah. Okay. So you've got pretty

24:08.880 --> 24:14.320
strong results here. Do you have you had anyone take this and implement it in practice?

24:15.440 --> 24:23.840
Kind of an industry or for a kind of a live application? Yeah. I receive a couple of emails requesting

24:23.840 --> 24:30.640
we have a license there and then people can implement it and Intel is extremely of course there will

24:30.640 --> 24:37.600
be super interesting. Yeah, they're working on it to include to their some kind of libraries

24:37.600 --> 24:44.640
in the future too. There's one thing that I want to share. So the all the speedouts we're getting

24:44.640 --> 24:52.000
here is not about all those CPL plans or implementation tricks. We don't even use cool glass or this kind

24:52.000 --> 24:59.040
of libraries for speeding up the computation which is use all the primitives and all the

24:59.040 --> 25:04.480
computations we're saving here because of the smart algorithms. So because we're only choosing

25:04.480 --> 25:13.600
five percent of the nose active nose so that the computation was saved. So I think this framework

25:14.640 --> 25:21.840
if viewed like we can have further speed up if those techniques tricks are applying to this.

25:21.840 --> 25:29.280
But this is just like a prototype which proves that we can do this with smart algorithms.

25:29.280 --> 25:36.080
Kind of going back to the result you found that the I found the number here the convergence time

25:36.080 --> 25:44.560
required is over two and a half times less than with TensorFlow GPU and then I was looking for your

25:45.280 --> 25:52.480
accuracy number here. How does it compare in terms of accuracy? There's no accuracy drop.

25:52.480 --> 25:57.840
Okay, so you're able to maintain accuracy but um, converge more quickly. Yes. So you've done your

25:57.840 --> 26:07.920
benchmarking for this extreme classification task which has inherent sparsity to it. Are there

26:07.920 --> 26:15.360
other types of problems that you think that this would easily lend itself to due to the because

26:15.360 --> 26:22.960
they also have high degrees of sparsity or do you see this as you know are there ways to kind of

26:22.960 --> 26:29.840
adapt this approach so that it you can apply it to use cases where they're not quite as inherently

26:29.840 --> 26:36.480
sparse. So you're talking about the approach we're having, does that work? The approach you're taking

26:36.480 --> 26:41.760
and the applicability and you know what are the requirements of the scenarios in which it would

26:41.760 --> 26:48.320
apply is you know would they have to be as sparse as the extreme classification task or can

26:48.320 --> 26:55.920
you see some ways that they can apply more broadly. You can be applied broadly as well as I

26:55.920 --> 27:02.160
just mentioned for example, we're currently working on if we can use this speeding up on the

27:02.160 --> 27:10.000
training of BERT which is very popular recently. So the difficulty of applying this to an

27:10.000 --> 27:17.120
existing framework is we have to profile where is the computation bottleneck because speeding up

27:17.120 --> 27:25.280
the non bottleneck won't help and because we're producing some overhead as well and that's not

27:25.280 --> 27:32.480
gonna suite the whole process up anyway. So we have to find the bottleneck of the training and if

27:32.480 --> 27:38.880
this technique can in this technique needs the sparsity. So if we do experiment saying that

27:39.680 --> 27:46.880
we're replacing our algorithm with like the real top K and it's decreasing the accuracy too

27:46.880 --> 27:56.960
much then it's not worth investigating. But if we're for example choose the real top 5% of the

27:56.960 --> 28:03.040
active nodes and the accuracy still maintains, then in this case there's hope for us to do this

28:03.040 --> 28:08.320
approximation because like the kind of like the ground choose works but now we just need to speed it

28:08.320 --> 28:20.080
up. So it depends on the architectures of the network. So it's not something that you would

28:20.080 --> 28:25.280
envision ever being kind of a generic tool in the toolkit that you would apply to different

28:25.280 --> 28:32.480
types of problems. You have to kind of hand tailor the application of this technique to the different

28:32.480 --> 28:39.920
to different algorithms and where they're bottleneck. Why I think this is still generic is because

28:39.920 --> 28:46.560
in all those neural networks or architectures anyway reduced to the matrix multiplication problem.

28:47.440 --> 28:55.360
Once that one is there this should be useful but it's just like we need to see which where do we

28:55.360 --> 29:03.360
where does it worth it to apply to. And we're also looking for like a GPU implementation of the same

29:03.360 --> 29:11.120
method as well because the major bottleneck for this technique currently going to GPU is because

29:11.120 --> 29:17.920
of the hashing algorithm because for those many years hashing only has the CPU implementation

29:17.920 --> 29:23.600
because it requires a lot of memory because it's remembering something right we save something

29:23.600 --> 29:32.560
in the hash table. And recently we see a step forward to the dynamic hash tables and GPUs

29:33.120 --> 29:40.560
from UC Davis folks. So for those I kind of were saying that it might be possible to implement

29:42.560 --> 29:50.560
like a locality sensitive hashing on GPU and then this can further speed up GPU and that's like

29:50.560 --> 29:57.600
also a possible future direction of the framework. Awesome awesome well maybe thanks so much

29:57.600 --> 30:27.440
for taking the time to share a little bit about what you're working on. It's very cool stuff. Thanks.

