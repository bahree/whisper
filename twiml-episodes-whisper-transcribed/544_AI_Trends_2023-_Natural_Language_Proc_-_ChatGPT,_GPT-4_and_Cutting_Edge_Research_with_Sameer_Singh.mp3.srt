1
00:00:00,000 --> 00:00:05,200
All right, everyone, welcome to our AI Trends 2023 series.

2
00:00:05,200 --> 00:00:07,140
Each year, we invite friends of the show

3
00:00:07,140 --> 00:00:10,200
to join us to recap key developments of the year

4
00:00:10,200 --> 00:00:12,080
and anticipate future advancement

5
00:00:12,080 --> 00:00:15,480
in the most interesting subfields in AI.

6
00:00:15,480 --> 00:00:17,680
And today, we're joined by Samir Singh.

7
00:00:17,680 --> 00:00:19,600
Samir is an associate professor

8
00:00:19,600 --> 00:00:22,360
in the Department of Computer Science at UC Irvine

9
00:00:22,360 --> 00:00:24,160
and a fellow at the Allen Institute

10
00:00:24,160 --> 00:00:27,000
for Artificial Intelligence for AI2.

11
00:00:27,000 --> 00:00:29,960
To talk through some of the key research developments

12
00:00:29,960 --> 00:00:32,000
in NLP.

13
00:00:32,000 --> 00:00:34,320
Of course, before we get going,

14
00:00:34,320 --> 00:00:35,960
take a moment to hit that subscribe button

15
00:00:35,960 --> 00:00:38,080
wherever you're listening to today's show.

16
00:00:38,080 --> 00:00:41,960
And you can also follow us on TikTok and Instagram

17
00:00:41,960 --> 00:00:46,640
at Twimble AI for highlights from every episode.

18
00:00:46,640 --> 00:00:48,800
All right, let's jump in, Samir.

19
00:00:48,800 --> 00:00:53,480
Welcome back to the podcast and our Trends series.

20
00:00:53,480 --> 00:00:54,800
Yeah, thank you for having me, Sam.

21
00:00:54,800 --> 00:00:56,560
It's great to be back.

22
00:00:56,560 --> 00:00:58,320
It's super excited to have you back.

23
00:00:58,320 --> 00:01:00,560
We were joking a little bit before we got rolling

24
00:01:00,560 --> 00:01:04,160
that we picked big years to have you on.

25
00:01:04,160 --> 00:01:09,160
The last one was our 2020 right in the wake of GPT-3,

26
00:01:10,560 --> 00:01:12,840
a big year, and of course,

27
00:01:12,840 --> 00:01:15,000
this has been a huge year for NLP

28
00:01:15,000 --> 00:01:18,560
with the relatively recent release of chat GPT.

29
00:01:18,560 --> 00:01:20,760
Yeah, it's always kind of crazy

30
00:01:20,760 --> 00:01:23,640
when you have these big changes happening in the year

31
00:01:23,640 --> 00:01:26,760
where there is research still going on in parallel

32
00:01:26,760 --> 00:01:28,800
and people are exploring research questions

33
00:01:28,800 --> 00:01:32,280
and a lot of them either become obsolete

34
00:01:32,280 --> 00:01:34,760
or have to be revisited and things like that

35
00:01:34,760 --> 00:01:35,680
in the middle of the year.

36
00:01:35,680 --> 00:01:37,760
And in this year, especially,

37
00:01:37,760 --> 00:01:39,760
it was much closer to the end of the year.

38
00:01:39,760 --> 00:01:41,200
So looking back at the year,

39
00:01:41,200 --> 00:01:43,760
it's always the first thing to think about the trajectory

40
00:01:43,760 --> 00:01:47,440
and what ideas will still be for a citizen, what do you want?

41
00:01:47,440 --> 00:01:50,320
Yeah, yeah, as a great point, chat GPT

42
00:01:50,320 --> 00:01:53,120
happened right at the end of the year.

43
00:01:53,120 --> 00:01:55,640
Do you think we'd have this same sense

44
00:01:55,640 --> 00:01:57,600
that this was a huge year in NLP

45
00:01:57,600 --> 00:02:02,320
if it wasn't for that late year release of chat GPT?

46
00:02:02,320 --> 00:02:03,160
Oh, I definitely.

47
00:02:03,160 --> 00:02:06,280
I think this year has been really impressive.

48
00:02:06,280 --> 00:02:10,520
I would say even bigger, even if you take out chat GPT,

49
00:02:11,520 --> 00:02:13,960
overall, this year has been really big for NLP,

50
00:02:13,960 --> 00:02:16,880
even compared to the ERGPT-3 came out.

51
00:02:16,880 --> 00:02:18,480
So yeah, I think there have been,

52
00:02:19,480 --> 00:02:22,840
I feel like it took us a while to come in terms

53
00:02:22,840 --> 00:02:25,200
with what these large language models are capable of

54
00:02:25,200 --> 00:02:28,880
or what they clearly fail at and what they are good at

55
00:02:28,880 --> 00:02:32,080
and try to build better two-wing around it,

56
00:02:32,080 --> 00:02:33,840
build better support systems around it.

57
00:02:33,840 --> 00:02:35,520
And so yeah, I think this year has been good,

58
00:02:35,520 --> 00:02:38,440
even if you don't take it out on chat GPT, yeah.

59
00:02:38,440 --> 00:02:39,280
Awesome.

60
00:02:39,280 --> 00:02:41,640
Well, we're going to dig into chat GPT

61
00:02:41,640 --> 00:02:43,360
in a fair amount of detail,

62
00:02:43,360 --> 00:02:46,040
as well as some of the other advances you just hinted at.

63
00:02:46,040 --> 00:02:48,720
But before we do, I'd love to have you take a few minutes

64
00:02:48,720 --> 00:02:52,120
to just introduce yourself to our audience

65
00:02:52,120 --> 00:02:55,680
with a focus on kind of your research focus

66
00:02:55,680 --> 00:02:57,240
and what your interests are.

67
00:02:57,240 --> 00:02:58,080
Cool, yeah.

68
00:02:58,080 --> 00:03:01,600
So I've been working in NLP for a long time now,

69
00:03:01,600 --> 00:03:04,960
but my focus has mostly been looking at

70
00:03:04,960 --> 00:03:07,080
when these language models or machine learning

71
00:03:07,080 --> 00:03:09,400
in general gets interface with real users,

72
00:03:09,400 --> 00:03:12,640
what are the needs that sort of are there.

73
00:03:12,640 --> 00:03:14,880
So a lot of my work has been in explanations

74
00:03:14,880 --> 00:03:18,280
and interpretability, but also in robustness,

75
00:03:18,280 --> 00:03:20,080
both from an adversarial perspective,

76
00:03:20,080 --> 00:03:23,840
but also from out of domain generalization perspective.

77
00:03:23,840 --> 00:03:26,360
And also in terms of evaluation,

78
00:03:26,360 --> 00:03:28,720
how do we know whether the models are doing well,

79
00:03:28,720 --> 00:03:30,160
how well are they doing?

80
00:03:30,160 --> 00:03:32,320
And in general, be able to understand

81
00:03:32,320 --> 00:03:34,320
and predict when the models would work

82
00:03:34,320 --> 00:03:36,360
and when the models would work.

83
00:03:36,360 --> 00:03:37,240
Mm-hmm.

84
00:03:37,240 --> 00:03:43,080
And I'm imagining that the advent of large language models

85
00:03:43,080 --> 00:03:45,880
and the kind of the dominance of that approach

86
00:03:45,880 --> 00:03:50,880
to NLP modeling is, well, it certainly changed the tools

87
00:03:51,240 --> 00:03:52,920
and the approach that you take.

88
00:03:52,920 --> 00:03:55,320
Has it changed kind of the fundamental way

89
00:03:55,320 --> 00:03:56,680
that you approach the problem?

90
00:03:56,680 --> 00:03:58,160
To some degree, yes and no,

91
00:03:58,160 --> 00:04:01,160
I think it has made a lot of my work obsolete

92
00:04:01,160 --> 00:04:04,440
in the sense that we were doing a really good job

93
00:04:04,440 --> 00:04:07,400
of finding fundamental faults

94
00:04:07,400 --> 00:04:08,920
in a lot of these language models

95
00:04:08,920 --> 00:04:11,000
and turned out a lot of them go away

96
00:04:11,000 --> 00:04:14,200
when you have a lot more data or a lot larger size.

97
00:04:14,200 --> 00:04:16,680
And so the specific observations and insights

98
00:04:16,680 --> 00:04:19,680
we had, not all of them have persisted.

99
00:04:19,680 --> 00:04:22,600
But the other differentiation we had in our work

100
00:04:22,600 --> 00:04:25,720
was always being somewhat model agnostic

101
00:04:25,720 --> 00:04:30,000
or try to use a black box approach to the model

102
00:04:30,000 --> 00:04:32,480
rather than looking inside what's going on.

103
00:04:32,480 --> 00:04:34,920
And that is something that you can use

104
00:04:34,920 --> 00:04:37,680
in this world of only access to API.

105
00:04:37,680 --> 00:04:40,080
A lot of those tools can still work out.

106
00:04:40,080 --> 00:04:44,520
So yeah, so it's been a mix, but it's been exciting

107
00:04:44,520 --> 00:04:46,880
to sort of continue to do that.

108
00:04:46,880 --> 00:04:51,880
Well, you've identified some themes that from your purview

109
00:04:53,000 --> 00:04:56,320
have been some of the key topic areas

110
00:04:56,320 --> 00:04:59,880
and research that have emerged in the field

111
00:04:59,880 --> 00:05:04,280
over the past year, let's start there.

112
00:05:04,280 --> 00:05:08,840
And maybe before we dive into any of the individual items,

113
00:05:08,840 --> 00:05:13,560
what's your take on 2022 broadly

114
00:05:14,560 --> 00:05:19,560
and some of the areas that you are most excited about

115
00:05:20,160 --> 00:05:21,000
in the year?

116
00:05:21,000 --> 00:05:22,640
Yeah, so I think broadly speaking

117
00:05:22,640 --> 00:05:25,720
and we'll delve deeper into a bunch of these topics.

118
00:05:25,720 --> 00:05:30,160
But broadly speaking, I think the importance of data

119
00:05:30,160 --> 00:05:33,240
and the importance of looking at what might be

120
00:05:33,240 --> 00:05:35,800
in the pre-chaining data has sort of brought up

121
00:05:35,800 --> 00:05:39,360
back into the focus in a way that I feel earlier years,

122
00:05:39,360 --> 00:05:42,080
we were a lot more agnostic of what the model was being

123
00:05:42,080 --> 00:05:46,000
trained on and just more data was better and a thing.

124
00:05:46,000 --> 00:05:49,560
This year it's been a lot more sort of thinking

125
00:05:49,560 --> 00:05:51,920
about what goes in the models.

126
00:05:51,920 --> 00:05:55,080
And also thinking of ways to use the models

127
00:05:55,080 --> 00:05:59,240
not just by simply prompting it with a simple thing,

128
00:05:59,240 --> 00:06:00,880
but trying to get it to reason,

129
00:06:00,880 --> 00:06:03,840
trying to get it to break down the problem into pieces

130
00:06:03,840 --> 00:06:07,920
and try to evaluate how much the language models can do that.

131
00:06:07,920 --> 00:06:09,840
And that I think is key when you start thinking

132
00:06:09,840 --> 00:06:14,360
about taking language models to more higher level decision

133
00:06:14,360 --> 00:06:16,240
making or higher level reason.

134
00:06:16,240 --> 00:06:17,080
Awesome, awesome.

135
00:06:17,080 --> 00:06:20,240
What's the first area you'd like to dig into?

136
00:06:20,240 --> 00:06:24,280
So let's actually start with a chain of thought prompting.

137
00:06:24,280 --> 00:06:27,240
This is work coming out of Google

138
00:06:27,240 --> 00:06:29,680
that came out earlier this year.

139
00:06:29,680 --> 00:06:33,120
And I guess the easiest way to summarize is

140
00:06:33,120 --> 00:06:36,160
to say let's think step by step.

141
00:06:36,160 --> 00:06:39,600
The idea here is to have the model not just

142
00:06:39,600 --> 00:06:42,800
generate the answer directly, but try

143
00:06:42,800 --> 00:06:45,040
to have it go through the reasoning process

144
00:06:45,040 --> 00:06:47,680
and then arrive at the answer.

145
00:06:47,680 --> 00:06:54,040
This ended up being quite a strong like a quite an effective

146
00:06:54,040 --> 00:06:57,320
method to get the model to do a lot of things, especially

147
00:06:57,320 --> 00:06:59,240
when it comes to mathematical reasoning

148
00:06:59,240 --> 00:07:01,520
and sort of where you can break down the problems

149
00:07:01,520 --> 00:07:03,320
into a bunch of different steps.

150
00:07:03,320 --> 00:07:06,800
Chain of thought prompting did extremely well

151
00:07:06,800 --> 00:07:09,280
compared to what we had before.

152
00:07:09,280 --> 00:07:11,240
And part of the difference, I guess,

153
00:07:11,240 --> 00:07:14,280
was you're not just prompting with questions and answers,

154
00:07:14,280 --> 00:07:17,200
but you're also prompting with something

155
00:07:17,200 --> 00:07:18,840
that is much more detailed.

156
00:07:18,840 --> 00:07:21,520
So the prompt itself has a bunch of examples

157
00:07:21,520 --> 00:07:23,120
of breaking the reasoning down.

158
00:07:23,120 --> 00:07:25,120
And then you have the model being

159
00:07:25,120 --> 00:07:28,000
able to walk through that reasoning and get to the answer.

160
00:07:28,000 --> 00:07:34,520
And in that work is the idea that the user of the model

161
00:07:34,520 --> 00:07:37,560
should break the prompts down into more detail

162
00:07:37,560 --> 00:07:42,400
or that the model should learn how to kind of show its work

163
00:07:42,400 --> 00:07:47,320
and give in a course screen prompts, break it, break

164
00:07:47,320 --> 00:07:48,640
the prompt down itself.

165
00:07:48,640 --> 00:07:51,840
Yeah, so I think the initial paper focused on the user

166
00:07:51,840 --> 00:07:54,760
providing a few examples of this big down.

167
00:07:54,760 --> 00:07:56,480
So if you're saying, like, you know,

168
00:07:56,480 --> 00:07:59,000
here's a mathematical word problem.

169
00:07:59,000 --> 00:08:00,440
You have two op apples.

170
00:08:00,440 --> 00:08:03,120
And then somebody gives you double of that.

171
00:08:03,120 --> 00:08:04,760
How many apples do you have?

172
00:08:04,760 --> 00:08:07,160
Breaking it down into a double means times two

173
00:08:07,160 --> 00:08:08,280
and two times two is four.

174
00:08:08,280 --> 00:08:10,160
This is a very simple example.

175
00:08:10,160 --> 00:08:12,400
But this kind of giving an example or two

176
00:08:12,400 --> 00:08:18,600
of breaking this down can be quite powerful for that.

177
00:08:18,600 --> 00:08:20,600
Especially I think one of the key insights here

178
00:08:20,600 --> 00:08:24,920
and we can talk about other papers that sort of showed

179
00:08:24,920 --> 00:08:28,400
later things, but this is a very emergent property

180
00:08:28,400 --> 00:08:31,720
that seems to exist for really large language models.

181
00:08:31,720 --> 00:08:34,120
And as if you have smaller language models,

182
00:08:34,120 --> 00:08:36,920
it's kind of difficult to get them to do this kind of piece.

183
00:08:36,920 --> 00:08:39,640
And so that's also been exciting to see.

184
00:08:39,640 --> 00:08:41,520
Did the results there?

185
00:08:41,520 --> 00:08:42,880
Did you find them surprising?

186
00:08:42,880 --> 00:08:47,480
Were they counterintuitive that that would work?

187
00:08:47,480 --> 00:08:52,000
I think how well they worked were, I think

188
00:08:52,000 --> 00:08:54,800
it surprised everyone because it's a very simple idea

189
00:08:54,800 --> 00:08:56,600
to just break it down a little bit.

190
00:08:56,600 --> 00:08:58,920
Everybody kind of assumed that the transformers

191
00:08:58,920 --> 00:09:01,080
are sort of either doing this internally

192
00:09:01,080 --> 00:09:03,400
or completely not doing this internally, right?

193
00:09:03,400 --> 00:09:07,440
And by showing you that if you actually write

194
00:09:07,440 --> 00:09:10,560
out a bunch of examples, these transformers models

195
00:09:10,560 --> 00:09:13,560
are able to do this to the extent that they are

196
00:09:13,560 --> 00:09:17,840
was quite surprising and the gains were quite impressive.

197
00:09:17,840 --> 00:09:20,040
Can you talk a little bit about the evaluation

198
00:09:22,040 --> 00:09:23,280
of that method?

199
00:09:23,280 --> 00:09:25,200
Yeah, so the evaluation was mostly focused

200
00:09:25,200 --> 00:09:27,560
on mathematical world problems.

201
00:09:27,560 --> 00:09:30,720
So there's this GSM in, GSM 8 dataset,

202
00:09:30,720 --> 00:09:35,640
and then there's this A-A-W-Q-S MOPS, I guess, dataset as well.

203
00:09:35,640 --> 00:09:38,080
These are mathematical world problems.

204
00:09:38,080 --> 00:09:41,400
And this first evaluation was mostly looking at

205
00:09:41,400 --> 00:09:45,120
how well you can do a reason through some of those.

206
00:09:45,120 --> 00:09:48,440
And yeah, it was much, much better

207
00:09:48,440 --> 00:09:51,880
than anything that we had before.

208
00:09:51,880 --> 00:09:53,960
And then there were, they had some evaluations

209
00:09:53,960 --> 00:09:55,640
on symbolic reasoning as well.

210
00:09:55,640 --> 00:10:01,120
So if you give them sort of tasks which have, like,

211
00:10:03,720 --> 00:10:06,520
you know, like, let have a bunch of,

212
00:10:07,640 --> 00:10:10,280
so like finding a character inside a long string.

213
00:10:10,280 --> 00:10:13,800
Like what is the fifth character or something like that?

214
00:10:13,800 --> 00:10:15,680
You can break it down into a bunch of steps

215
00:10:15,680 --> 00:10:17,760
and if you give it a few examples, it can do it.

216
00:10:17,760 --> 00:10:19,440
If you don't give it a few examples

217
00:10:19,440 --> 00:10:22,160
of how to break it down the models are very bad.

218
00:10:22,160 --> 00:10:25,800
And have you seen any work that looks to extend this

219
00:10:25,800 --> 00:10:30,120
beyond the kind of math and symbolic domain?

220
00:10:30,120 --> 00:10:34,840
So beyond, I'll talk a little bit about some of related ideas

221
00:10:34,840 --> 00:10:37,760
and sort of question answering a little bit later.

222
00:10:37,760 --> 00:10:42,040
But there is one work that is related that I like.

223
00:10:42,040 --> 00:10:45,440
This is called algorithmic prompting.

224
00:10:45,440 --> 00:10:49,040
And this is stuff that came out of Google brain as well.

225
00:10:49,040 --> 00:10:50,200
So, you know, a lot of the,

226
00:10:50,200 --> 00:10:51,800
this stuff is coming out of Google brain

227
00:10:51,800 --> 00:10:54,520
because you need really large language models

228
00:10:54,520 --> 00:10:58,840
to be able to work with this or even bigger than GPD3, for example.

229
00:10:58,840 --> 00:11:02,000
So in this algorithmic prompting paper,

230
00:11:02,000 --> 00:11:04,600
this was kind of interesting where they had,

231
00:11:04,600 --> 00:11:07,160
essentially the same idea as Jane of Thought,

232
00:11:07,160 --> 00:11:10,000
except that they go really detailed

233
00:11:10,000 --> 00:11:12,840
into what those reasoning steps would be.

234
00:11:12,840 --> 00:11:17,280
So they mostly focus on, you know, things that can be

235
00:11:17,280 --> 00:11:19,720
described more as an algorithm rather than us

236
00:11:19,720 --> 00:11:22,400
just breaking it into a few pieces.

237
00:11:22,400 --> 00:11:23,960
So you can say things like,

238
00:11:23,960 --> 00:11:28,880
if I had to add 12 plus 24, right?

239
00:11:28,880 --> 00:11:30,320
How would you do that?

240
00:11:30,320 --> 00:11:32,400
They literally break it down into digits.

241
00:11:32,400 --> 00:11:36,200
Oh, you take the ones place that's two in one case four

242
00:11:36,200 --> 00:11:39,000
and the other, you add them up, you get six,

243
00:11:39,000 --> 00:11:40,480
there is no carry.

244
00:11:40,480 --> 00:11:41,640
Okay, that's that's one.

245
00:11:41,640 --> 00:11:43,040
This is the second step is looking.

246
00:11:43,040 --> 00:11:45,200
Now look at the next 10th place.

247
00:11:45,200 --> 00:11:47,680
It's one and two, add them up three.

248
00:11:47,680 --> 00:11:49,680
Look at the carry or the carry is zero.

249
00:11:49,680 --> 00:11:51,880
So it's just three and then 36, right?

250
00:11:51,880 --> 00:11:54,560
So all of this, this very detailed breakdown,

251
00:11:54,560 --> 00:11:56,560
which looked like extremely detailed.

252
00:11:57,440 --> 00:12:00,200
But what was really impressive to me about that paper

253
00:12:00,200 --> 00:12:03,560
is they showed that you can give examples

254
00:12:03,560 --> 00:12:06,480
of really low digit operations.

255
00:12:06,480 --> 00:12:08,800
So like maybe two or three digit operations

256
00:12:08,800 --> 00:12:10,280
when you're talking about addition

257
00:12:10,280 --> 00:12:13,480
or multiplication, any of these things.

258
00:12:13,480 --> 00:12:15,720
But at test time, you can firstly,

259
00:12:15,720 --> 00:12:18,120
even on two or three digit stuff,

260
00:12:18,120 --> 00:12:21,400
it was much, much accurate compared to regular chain of thought.

261
00:12:22,360 --> 00:12:26,680
Like, you know, 20% going from 80% of for chain of thought

262
00:12:26,680 --> 00:12:28,240
to something that's 100%.

263
00:12:28,240 --> 00:12:29,880
I'm kind of making up numbers.

264
00:12:29,880 --> 00:12:33,720
And this is relative to asking for the model

265
00:12:33,720 --> 00:12:37,160
to solve the same problem without any intermediate steps.

266
00:12:37,160 --> 00:12:39,840
No, so without any steps is even worse, right?

267
00:12:39,840 --> 00:12:42,360
So this is asking the model to,

268
00:12:42,360 --> 00:12:44,160
so like 12 plus 24,

269
00:12:44,160 --> 00:12:46,080
I don't know exactly what the chain of thought would be,

270
00:12:46,080 --> 00:12:47,960
but it would be something that would be

271
00:12:47,960 --> 00:12:51,040
at a higher granularity, let's just say, right?

272
00:12:51,040 --> 00:12:54,840
And so when you go, when you give this detail from

273
00:12:54,840 --> 00:12:58,280
the models are more accurate, which is not so surprising.

274
00:12:58,280 --> 00:13:01,160
But what was surprising was that they kept increasing

275
00:13:01,160 --> 00:13:05,600
the size of the number at the test time.

276
00:13:05,600 --> 00:13:08,320
So started adding more and more digits

277
00:13:08,320 --> 00:13:12,480
and even up to 18 digit numbers,

278
00:13:12,480 --> 00:13:15,680
the model is able to do these operations

279
00:13:15,680 --> 00:13:17,560
much, much more accurately.

280
00:13:17,560 --> 00:13:19,240
Because even though the problems were only

281
00:13:19,240 --> 00:13:23,960
on two or three digits, sort of numbers, right?

282
00:13:23,960 --> 00:13:27,600
And so does this, does this type of work?

283
00:13:28,800 --> 00:13:31,520
Answer definitively whether, you know,

284
00:13:31,520 --> 00:13:35,480
this is already happening inside the model versus

285
00:13:35,480 --> 00:13:38,640
there's some other effects like, in a sense,

286
00:13:38,640 --> 00:13:42,360
it's really counterintuitive that it would work at all.

287
00:13:42,360 --> 00:13:45,600
Like, you know, there's no registers inside the model

288
00:13:45,600 --> 00:13:47,400
that are tracking digits, you know,

289
00:13:47,400 --> 00:13:49,160
the ones place and attend the place.

290
00:13:49,160 --> 00:13:51,040
Like, why should that work?

291
00:13:51,040 --> 00:13:53,720
Yeah, so I think there are, there are,

292
00:13:53,720 --> 00:13:55,320
people are still trying to come to terms

293
00:13:55,320 --> 00:13:57,200
with white end of thought reasoning works.

294
00:13:57,200 --> 00:13:58,840
Is there something in the pre-gaining data?

295
00:13:58,840 --> 00:14:00,200
Is there something in the model?

296
00:14:00,200 --> 00:14:02,800
And there's been some interesting work there.

297
00:14:02,800 --> 00:14:04,840
But no, I think the tricky thing here

298
00:14:04,840 --> 00:14:07,560
is you're making all of these things explicit.

299
00:14:07,560 --> 00:14:12,560
So you're not relying on the model to keep these bits

300
00:14:12,560 --> 00:14:15,280
somewhere latent in its sort of memory, right?

301
00:14:15,280 --> 00:14:16,560
Like you're making it explicit

302
00:14:16,560 --> 00:14:18,640
and of course it's attending to all of that.

303
00:14:18,640 --> 00:14:22,200
And so the chances of it sort of going away

304
00:14:22,200 --> 00:14:24,840
into a wrong place is much lower.

305
00:14:24,840 --> 00:14:28,360
So, you know, scratch pad and a bunch of other papers

306
00:14:28,360 --> 00:14:31,600
had similar ideas of like, hey, let's give some model,

307
00:14:31,600 --> 00:14:33,960
some space to think about things, right?

308
00:14:33,960 --> 00:14:38,400
So it's possible that this is just letting the model

309
00:14:38,400 --> 00:14:39,880
actually think things through.

310
00:14:39,880 --> 00:14:41,640
So it's somehow more computation

311
00:14:41,640 --> 00:14:43,600
that the model is getting.

312
00:14:43,600 --> 00:14:45,560
And then there've been some papers showing that, yeah,

313
00:14:45,560 --> 00:14:48,040
that might be the difference, the fact that you're

314
00:14:48,040 --> 00:14:50,680
generating a single number, but you're letting,

315
00:14:50,680 --> 00:14:53,080
not just asking the model to give it one shot,

316
00:14:53,080 --> 00:14:54,480
but letting it think about it.

317
00:14:56,280 --> 00:14:58,360
And it's not so much the fact that you're giving

318
00:14:58,360 --> 00:15:00,280
these examples breakdowns that helps.

319
00:15:00,280 --> 00:15:02,600
But I think, you know, as many of these things,

320
00:15:02,600 --> 00:15:04,400
I'm sure the answer is complicated

321
00:15:04,400 --> 00:15:06,680
and it's some combination of things.

322
00:15:06,680 --> 00:15:09,640
The last thing you said almost sounds like

323
00:15:09,640 --> 00:15:11,600
the kind of multitask argument.

324
00:15:11,600 --> 00:15:14,480
It's not that, you know, the specific other thing

325
00:15:14,480 --> 00:15:18,240
that you're asking the model to do matters,

326
00:15:18,240 --> 00:15:20,400
but that you're asking it to do another thing

327
00:15:20,400 --> 00:15:24,120
and that kind of, you know, on the traditional side,

328
00:15:24,120 --> 00:15:26,440
like has some kind of regularization effect

329
00:15:26,440 --> 00:15:29,400
or some kind of effect that causes your results

330
00:15:29,400 --> 00:15:31,960
to be better just by overloading the model

331
00:15:31,960 --> 00:15:32,920
a little bit.

332
00:15:32,920 --> 00:15:34,400
Yeah, yeah, exactly, right?

333
00:15:34,400 --> 00:15:36,680
So you're letting, in some sense,

334
00:15:36,680 --> 00:15:41,040
you have more activations, you have more slated states,

335
00:15:41,040 --> 00:15:44,560
you just have giving model and more things to do.

336
00:15:44,560 --> 00:15:48,880
And so it has space to explore through more reasoning.

337
00:15:48,880 --> 00:15:51,280
So maybe that's one explanation for why this kind of stuff

338
00:15:51,280 --> 00:15:53,760
works, but yeah.

339
00:15:53,760 --> 00:15:54,640
Amazing, amazing.

340
00:15:54,640 --> 00:15:57,600
And I should have mentioned earlier on,

341
00:15:57,600 --> 00:15:58,920
but I will mention it now.

342
00:15:58,920 --> 00:16:00,800
All of the papers that we're referring

343
00:16:00,800 --> 00:16:03,400
to will be available on the show notes page,

344
00:16:03,400 --> 00:16:06,800
so folks can check them out.

345
00:16:06,800 --> 00:16:08,880
So the next thing that you had on your list

346
00:16:08,880 --> 00:16:10,840
was decomposed reasoning.

347
00:16:10,840 --> 00:16:13,080
It sounds like it's in a similar vein.

348
00:16:13,080 --> 00:16:15,680
Yeah, so I think this is, that's why I kind of put them

349
00:16:15,680 --> 00:16:17,480
together, but I think fundamentally

350
00:16:17,480 --> 00:16:20,280
this is a very different approach to the same idea.

351
00:16:20,280 --> 00:16:22,680
So yes, I think terminology is something

352
00:16:22,680 --> 00:16:24,600
that the field is going to be revisiting

353
00:16:24,600 --> 00:16:26,680
and decomposed using is kind of something

354
00:16:26,680 --> 00:16:27,880
that I give up with.

355
00:16:27,880 --> 00:16:29,800
I don't even know if people use it.

356
00:16:29,800 --> 00:16:31,280
But the idea here is that there have been

357
00:16:31,280 --> 00:16:32,560
a bunch of papers here, and I'm just

358
00:16:32,560 --> 00:16:35,520
going to sort of put here on through some of them.

359
00:16:35,520 --> 00:16:38,840
But the idea here is that you shouldn't rely on the language

360
00:16:38,840 --> 00:16:41,400
model alone to do the whole task.

361
00:16:41,400 --> 00:16:44,640
So suppose I give it a mathematical word problem,

362
00:16:44,640 --> 00:16:46,920
or if I give it a question answering problem,

363
00:16:46,920 --> 00:16:49,120
that's a lot complicated.

364
00:16:49,120 --> 00:16:52,960
I shouldn't rely on the model and its parameters

365
00:16:52,960 --> 00:16:54,760
to be able to carry everything out.

366
00:16:54,760 --> 00:16:57,240
Maybe the model needs to use a calculator.

367
00:16:57,240 --> 00:17:00,560
Maybe the model needs to do a web search.

368
00:17:00,560 --> 00:17:03,720
Maybe the model needs to even write a small Python script

369
00:17:03,720 --> 00:17:07,680
and actually run it to get the answer that I want.

370
00:17:07,680 --> 00:17:12,560
And so there's this whole idea of language models

371
00:17:12,560 --> 00:17:15,280
getting what you need, but not just relying on its own

372
00:17:15,280 --> 00:17:18,040
parameters, but breaking down your problem

373
00:17:18,040 --> 00:17:20,480
and figuring out, oh, I need to call something else.

374
00:17:20,480 --> 00:17:23,360
And this is what I'm going to do to call it.

375
00:17:23,360 --> 00:17:25,800
It's an idea that sort of claim came out

376
00:17:25,800 --> 00:17:28,080
post chain of thoughts sort of middle of the year,

377
00:17:28,080 --> 00:17:30,120
but they've been a bunch of papers all the way

378
00:17:30,120 --> 00:17:35,000
to the end of the year that have been doing a lot of this.

379
00:17:35,000 --> 00:17:37,760
So yeah, it's kind of been exciting.

380
00:17:37,760 --> 00:17:41,280
A lot of them have been on the QA side of things.

381
00:17:41,280 --> 00:17:44,640
So the two I'll mention is success at prompting that came

382
00:17:44,640 --> 00:17:47,400
out of my group, but there's also decomposed prompting

383
00:17:47,400 --> 00:17:49,040
that came out of AI2.

384
00:17:49,040 --> 00:17:50,640
And the idea behind both of these

385
00:17:50,640 --> 00:17:53,280
was to take a complex question, break it down

386
00:17:53,280 --> 00:17:58,400
into simpler ones, and then have the language model

387
00:17:58,400 --> 00:18:01,320
sort of call another language model

388
00:18:01,320 --> 00:18:05,160
that is answering each of these simple questions.

389
00:18:05,160 --> 00:18:07,480
So if a simple question is a mathematical operation,

390
00:18:07,480 --> 00:18:08,880
then you would use a calculator.

391
00:18:08,880 --> 00:18:13,080
If a simple question is a very simple lookup question,

392
00:18:13,080 --> 00:18:16,360
then you would use something that is like a squad style

393
00:18:16,360 --> 00:18:18,240
question answering system, things like that, right?

394
00:18:18,240 --> 00:18:21,880
So being able to take what the user wants

395
00:18:21,880 --> 00:18:24,360
and breaking down into pieces and then

396
00:18:24,360 --> 00:18:27,560
composing the answers together to give you the actual answer,

397
00:18:27,560 --> 00:18:29,080
this is the answer.

398
00:18:29,080 --> 00:18:31,600
Can you talk a little bit in a little bit more detail

399
00:18:31,600 --> 00:18:33,760
the difference between successive prompting

400
00:18:33,760 --> 00:18:35,160
and decomposed prompting?

401
00:18:35,160 --> 00:18:37,760
How did the settings for those differ?

402
00:18:37,760 --> 00:18:40,320
They came out pretty much around the same time.

403
00:18:40,320 --> 00:18:43,840
So it's difficult to sort of, and they sort of appeared

404
00:18:43,840 --> 00:18:46,280
at the same conference as well.

405
00:18:46,280 --> 00:18:48,720
I think, yeah, so I think some of it

406
00:18:48,720 --> 00:18:51,800
depended on sort of which data set they use.

407
00:18:51,800 --> 00:18:55,600
So decomposed prompting used specifically multi-hop data

408
00:18:55,600 --> 00:18:58,720
sets and sort of trying to decompose it that way.

409
00:18:58,720 --> 00:19:01,600
Successive prompting focused a little bit more

410
00:19:01,600 --> 00:19:04,480
on calculations and symbolic operations as well.

411
00:19:04,480 --> 00:19:06,960
So yeah, I would say the difference is between them.

412
00:19:06,960 --> 00:19:09,280
But kind of same idea, different data sets,

413
00:19:09,280 --> 00:19:10,760
slightly different tooling.

414
00:19:10,760 --> 00:19:12,360
Right, right, yeah.

415
00:19:12,360 --> 00:19:15,040
And we'll see, in some of these cases,

416
00:19:15,040 --> 00:19:17,360
other pairs of papers also that are very similar

417
00:19:17,360 --> 00:19:21,440
that came out around the same time because that's where we are.

418
00:19:21,440 --> 00:19:23,400
How about tool augmented?

419
00:19:23,400 --> 00:19:24,920
So tool augmented stuff.

420
00:19:24,920 --> 00:19:29,480
So there was a paper coming out of Google, I believe,

421
00:19:29,480 --> 00:19:32,080
called tool augmented language models.

422
00:19:32,080 --> 00:19:33,560
So down is a paper.

423
00:19:33,560 --> 00:19:37,320
And this is one of the papers that was essentially showing

424
00:19:37,320 --> 00:19:41,000
that you can have, instead of just calling a calculator

425
00:19:41,000 --> 00:19:43,520
explicitly or just having a fixed set of things,

426
00:19:43,520 --> 00:19:48,080
you can create a description of API is that the language model

427
00:19:48,080 --> 00:19:49,360
has access to.

428
00:19:49,360 --> 00:19:53,080
And have the language model itself generate

429
00:19:53,080 --> 00:19:57,120
example calls to that API when it's doing an output, right?

430
00:19:57,120 --> 00:20:01,480
So if I want to say like, hey, GPT-3 or whatever,

431
00:20:01,480 --> 00:20:04,560
how hot is it going to get today, right?

432
00:20:04,560 --> 00:20:08,240
Or how hot is it going to get today in Irvine?

433
00:20:08,240 --> 00:20:09,880
The language model is going to say, okay,

434
00:20:09,880 --> 00:20:13,720
this is a question about the weather in Irvine.

435
00:20:13,720 --> 00:20:16,440
So I'm going to compose an API called

436
00:20:16,440 --> 00:20:18,720
to a weather service.

437
00:20:18,720 --> 00:20:21,720
That's going to say, what's the weather in Irvine?

438
00:20:21,720 --> 00:20:24,520
And then it'll return some JSON object that says,

439
00:20:24,520 --> 00:20:26,400
oh, the high is this, low is this,

440
00:20:26,400 --> 00:20:28,320
probability of rain is this.

441
00:20:28,320 --> 00:20:30,680
And then the language model will kick in again

442
00:20:30,680 --> 00:20:33,720
and take that output and say, oh, it's going to be

443
00:20:33,720 --> 00:20:37,360
pretty hot today, as since it is Southern California.

444
00:20:37,360 --> 00:20:39,240
And yeah, you know, something.

445
00:20:39,240 --> 00:20:41,280
Seems like this research is heading in the direction

446
00:20:41,280 --> 00:20:44,880
of how would you kind of rebuild Siri or, you know,

447
00:20:44,880 --> 00:20:47,200
Alexa or something like that with LLMs.

448
00:20:47,200 --> 00:20:48,040
Yes, yeah.

449
00:20:48,040 --> 00:20:50,720
And I think this is one of the key sort of advantages

450
00:20:50,720 --> 00:20:53,680
of these language models is not that they can do

451
00:20:53,680 --> 00:20:55,600
additions and subtractions internally.

452
00:20:55,600 --> 00:20:58,080
Like I think that's interesting from an intellectual point

453
00:20:58,080 --> 00:20:58,680
of view.

454
00:20:58,680 --> 00:21:00,280
But when you're making actual products,

455
00:21:00,280 --> 00:21:02,840
you want this language model to,

456
00:21:02,840 --> 00:21:05,520
language is a way to interface with things

457
00:21:05,520 --> 00:21:07,600
that are external to you, right?

458
00:21:07,600 --> 00:21:10,320
So the language models should take in the user queries,

459
00:21:10,320 --> 00:21:13,600
but also be the interface to other things outside

460
00:21:13,600 --> 00:21:15,600
and be able to query it.

461
00:21:15,600 --> 00:21:17,560
I think we will talk a little bit about that later.

462
00:21:17,560 --> 00:21:21,240
But one of the reasons I like this is you can also somehow

463
00:21:21,240 --> 00:21:24,120
now attribute the answer that you're getting,

464
00:21:24,120 --> 00:21:26,920
not to some internal parameter in the language model,

465
00:21:26,920 --> 00:21:29,840
but to say, look, this is the API call I made.

466
00:21:29,840 --> 00:21:31,320
And this is the answer I got.

467
00:21:31,320 --> 00:21:33,960
And now that's what that's the answer I gave you.

468
00:21:33,960 --> 00:21:36,240
So in some sense, it becomes a little bit more

469
00:21:36,240 --> 00:21:37,960
after you attribute it.

470
00:21:37,960 --> 00:21:43,200
The idea of the language model writing a program

471
00:21:43,200 --> 00:21:47,480
to figure out the answer to a question

472
00:21:47,480 --> 00:21:48,880
is a fascinating one.

473
00:21:48,880 --> 00:21:53,880
And it almost feels like if anything around LLM

474
00:21:53,880 --> 00:21:57,680
is going to be the path to AGI, it's like it's that.

475
00:21:57,680 --> 00:22:01,200
What was your reaction to that research?

476
00:22:01,200 --> 00:22:03,880
Yeah, I think it seems quite like to me

477
00:22:03,880 --> 00:22:05,280
from a practical point of view,

478
00:22:05,280 --> 00:22:09,160
it seems quite exciting.

479
00:22:09,160 --> 00:22:10,640
From a code generation point of view,

480
00:22:10,640 --> 00:22:12,600
and things like that, it's useful as well.

481
00:22:12,600 --> 00:22:15,760
But the nice thing about the code writing code

482
00:22:15,760 --> 00:22:17,600
is that it's unambiguous.

483
00:22:17,600 --> 00:22:20,600
So it's making some calls to an external database.

484
00:22:20,600 --> 00:22:24,000
If I want to update the language model or update

485
00:22:24,000 --> 00:22:27,960
this whole system, I can just update my knowledge directly.

486
00:22:27,960 --> 00:22:29,560
The knowledge is external somehow

487
00:22:29,560 --> 00:22:31,880
to the parameterization of the language model.

488
00:22:31,880 --> 00:22:34,000
That makes it super convenient to delete things,

489
00:22:34,000 --> 00:22:38,440
or to add things, or to get attributions, and all these things.

490
00:22:38,440 --> 00:22:41,960
And the interface to that data source

491
00:22:41,960 --> 00:22:45,600
is always programs, either it's like a simple API call

492
00:22:45,600 --> 00:22:47,400
or a more complex one.

493
00:22:47,400 --> 00:22:49,360
And I think I really like this idea

494
00:22:49,360 --> 00:22:52,040
because it allows the language models

495
00:22:52,040 --> 00:22:54,040
to do things that it should be doing,

496
00:22:54,040 --> 00:22:56,920
which is to understand language, or let's not call it understand,

497
00:22:56,920 --> 00:23:02,760
would be able to parse language, be able to sort of transform it.

498
00:23:02,760 --> 00:23:05,840
But it doesn't necessarily have to know

499
00:23:05,840 --> 00:23:09,400
the temperature of Irvine every day, or things like that.

500
00:23:09,400 --> 00:23:11,080
That's not something I necessarily want,

501
00:23:11,080 --> 00:23:13,400
but I mean, this is like this model.

502
00:23:13,400 --> 00:23:14,880
Yeah.

503
00:23:14,880 --> 00:23:22,560
So just very subtly in there, you kind of addressed

504
00:23:22,560 --> 00:23:25,600
another big conversation that's happening in the community

505
00:23:25,600 --> 00:23:29,680
now in this idea of do language models understand.

506
00:23:29,680 --> 00:23:31,440
You call this decomposed reasoning.

507
00:23:31,440 --> 00:23:33,880
The thing is writing programs that kind of requires

508
00:23:33,880 --> 00:23:35,040
some kind of reasoning.

509
00:23:35,040 --> 00:23:39,720
Like, what's your take on these broader questions

510
00:23:39,720 --> 00:23:46,000
about reasoning and understanding in LLMs?

511
00:23:46,000 --> 00:23:47,760
Or would you like to defer that?

512
00:23:47,760 --> 00:23:52,480
Is there a natural point later for us to talk about that?

513
00:23:52,480 --> 00:23:55,120
Come back to it a little bit later.

514
00:23:55,120 --> 00:23:57,400
Maybe even in the next section or sort of,

515
00:23:57,400 --> 00:24:00,320
we are trying to sort of question what reasoning is

516
00:24:00,320 --> 00:24:03,560
and trying to evaluate that in some sense.

517
00:24:03,560 --> 00:24:05,320
But yeah.

518
00:24:05,320 --> 00:24:07,800
The semantic argument around understanding,

519
00:24:07,800 --> 00:24:11,560
like that's not that interesting, but like how a language model

520
00:24:11,560 --> 00:24:16,280
can reason, and the extent to which it's reasoning

521
00:24:16,280 --> 00:24:22,040
versus like cutting, pasting at some level beyond

522
00:24:22,040 --> 00:24:23,840
at an impressive, in an impressive way,

523
00:24:23,840 --> 00:24:25,840
like that's kind of really interesting.

524
00:24:25,840 --> 00:24:26,720
Yeah, definitely.

525
00:24:26,720 --> 00:24:30,880
So I would say, and then even pushing it a little bit further,

526
00:24:30,880 --> 00:24:33,160
like what are the consequences of the fact

527
00:24:33,160 --> 00:24:35,960
that it is cutting, pasting versus its reasoning, right?

528
00:24:35,960 --> 00:24:39,200
So how should we calibrate what things

529
00:24:39,200 --> 00:24:41,200
these should be deployed for and what things

530
00:24:41,200 --> 00:24:43,920
this should not be deployed for based on the situations?

531
00:24:43,920 --> 00:24:46,680
Those are the kind of things that I'm really, really interested.

532
00:24:46,680 --> 00:24:48,760
Awesome, awesome, awesome.

533
00:24:48,760 --> 00:24:50,040
Is that your next section?

534
00:24:50,040 --> 00:24:52,440
Yes, and that sort of ties in very well

535
00:24:52,440 --> 00:24:56,000
with what I think is exciting next, which is,

536
00:24:56,000 --> 00:24:59,360
I'm going to call it sort of understanding

537
00:24:59,360 --> 00:25:02,360
the relationship between the data, the pre-training data,

538
00:25:02,360 --> 00:25:04,720
and the output of the model.

539
00:25:04,720 --> 00:25:08,160
And I feel like there is, again, a few different threads here,

540
00:25:08,160 --> 00:25:09,960
but there is one that came out of my group

541
00:25:09,960 --> 00:25:15,200
that I think is a simple idea that really sort of captures

542
00:25:15,200 --> 00:25:17,520
exactly what you said, the cutting, pasting versus

543
00:25:17,520 --> 00:25:19,080
a reasoning thing.

544
00:25:19,080 --> 00:25:21,400
So this paper is called impact of pre-training term

545
00:25:21,400 --> 00:25:24,440
frequencies on few short reasoning.

546
00:25:24,440 --> 00:25:27,000
And the idea here is we were looking only

547
00:25:27,000 --> 00:25:28,600
at numerical reasoning right now.

548
00:25:28,600 --> 00:25:32,440
So we started looking at all of these examples of,

549
00:25:32,440 --> 00:25:36,480
oh, a GPT-3 can do addition and multiplication

550
00:25:36,480 --> 00:25:37,720
and things like that.

551
00:25:37,720 --> 00:25:39,640
And we started looking at the instances

552
00:25:39,640 --> 00:25:43,280
and turns out that it doesn't always do it, right?

553
00:25:43,280 --> 00:25:44,680
It's not 100% at those.

554
00:25:44,680 --> 00:25:49,160
It's 80% or 90% or whatever the number is.

555
00:25:49,160 --> 00:25:53,000
So we started looking at, OK, what differentiates the one

556
00:25:53,000 --> 00:25:57,240
that gets correct and do things that it doesn't get corrected.

557
00:25:57,240 --> 00:26:01,640
So for example, we saw that if you ask it, what is 24 times

558
00:26:01,640 --> 00:26:04,200
18, the model gets it right.

559
00:26:04,200 --> 00:26:05,840
It says 432.

560
00:26:05,840 --> 00:26:11,360
If you say, what is 23 times 18, the model gets it wrong.

561
00:26:11,360 --> 00:26:13,280
So 24 times 18 is correct.

562
00:26:13,280 --> 00:26:15,560
23 times 18 is not correct.

563
00:26:15,560 --> 00:26:17,920
Is this random, like what's going on here?

564
00:26:17,920 --> 00:26:19,960
And did you just enter up there?

565
00:26:19,960 --> 00:26:24,680
Did you find that consistent across invocations?

566
00:26:24,680 --> 00:26:27,480
I've run into that kind of thing.

567
00:26:27,480 --> 00:26:28,680
We've all run into that kind of thing

568
00:26:28,680 --> 00:26:30,600
playing with chat GPT and other things.

569
00:26:30,600 --> 00:26:34,080
And sometimes it gets certain things consistently wrong.

570
00:26:34,080 --> 00:26:36,040
Other times, it gets the thing wrong.

571
00:26:36,040 --> 00:26:37,560
Sometimes and not wrong.

572
00:26:37,560 --> 00:26:39,720
Other times, I get to random seed kind of thing

573
00:26:39,720 --> 00:26:41,720
or something else going on in the model.

574
00:26:41,720 --> 00:26:43,240
Did you explore that at all?

575
00:26:43,240 --> 00:26:44,600
Yeah, we definitely saw that.

576
00:26:44,600 --> 00:26:47,720
So it's both like if you're doing few short prompting,

577
00:26:47,720 --> 00:26:50,320
which examples you put in the prompt would sometimes

578
00:26:50,320 --> 00:26:52,920
change to the output or how you phrase it.

579
00:26:52,920 --> 00:26:56,960
Like you do you say, what is 24 times 18 or what is 24x18?

580
00:26:56,960 --> 00:27:00,560
You know, things like that definitely made a difference.

581
00:27:00,560 --> 00:27:04,240
But even after averaging these things out,

582
00:27:04,240 --> 00:27:07,600
we saw that 24 times 18 was in general more accurate

583
00:27:07,600 --> 00:27:09,880
than 23 times 18.

584
00:27:09,880 --> 00:27:13,440
And even more than that, we did even further analysis.

585
00:27:13,440 --> 00:27:17,280
And it turns out that all of our instances

586
00:27:17,280 --> 00:27:21,680
that involved 24, the model was much more accurate on

587
00:27:21,680 --> 00:27:24,600
than all of the instances that involved 23.

588
00:27:24,600 --> 00:27:28,480
Well, so we decided to do this for everything

589
00:27:28,480 --> 00:27:31,600
from 0 to 100.

590
00:27:31,600 --> 00:27:33,360
So all two data numbers, essentially,

591
00:27:33,360 --> 00:27:34,960
single and two data numbers.

592
00:27:34,960 --> 00:27:36,320
And no, it's a whole spectrum.

593
00:27:36,320 --> 00:27:38,920
And we didn't see a clear reason why

594
00:27:38,920 --> 00:27:42,800
some things are low accuracy, some things are high accuracy.

595
00:27:42,800 --> 00:27:44,920
And so then what we decided to do,

596
00:27:44,920 --> 00:27:48,440
this is the part that I think I quite excited about.

597
00:27:48,440 --> 00:27:50,720
We started to count how many times

598
00:27:50,720 --> 00:27:52,640
do each number, each of these numbers

599
00:27:52,640 --> 00:27:55,320
appear in the pre-getting data.

600
00:27:55,320 --> 00:27:59,720
And turns out, and you can see the plot in the figure.

601
00:27:59,720 --> 00:28:03,320
If you plot the log of the frequency of these terms,

602
00:28:03,320 --> 00:28:05,400
and now how accurate the models are,

603
00:28:05,400 --> 00:28:09,840
it is pretty much exactly like a.

604
00:28:09,840 --> 00:28:11,560
Which is intuitive.

605
00:28:11,560 --> 00:28:14,560
The model does better on things that it sees a lot of, right?

606
00:28:14,560 --> 00:28:16,040
Yes, yeah.

607
00:28:16,040 --> 00:28:20,080
But yeah, so it's also expected yet disappointing

608
00:28:20,080 --> 00:28:23,800
because you don't want it to be such a nice strong curve,

609
00:28:23,800 --> 00:28:25,040
like you want it to do.

610
00:28:25,040 --> 00:28:27,520
Like if it's doing mathematical reasoning,

611
00:28:27,520 --> 00:28:30,000
it should know that 23 is one less than 24,

612
00:28:30,000 --> 00:28:31,440
and all of these things, right?

613
00:28:31,440 --> 00:28:34,960
So I think it's one of these things

614
00:28:34,960 --> 00:28:38,360
where it was expected that the model would be better

615
00:28:38,360 --> 00:28:40,000
on things it has seen before.

616
00:28:40,000 --> 00:28:42,000
But you also, at the same time,

617
00:28:42,000 --> 00:28:44,400
hold this thing of like, oh, it is able to reason,

618
00:28:44,400 --> 00:28:45,520
it is able to do these things,

619
00:28:45,520 --> 00:28:48,920
and it's kind of difficult to resolve both of those, right?

620
00:28:48,920 --> 00:28:50,920
So this was one example.

621
00:28:50,920 --> 00:28:53,000
I think we are barely scratching the surface,

622
00:28:53,000 --> 00:28:54,480
but this was an example of paper

623
00:28:54,480 --> 00:28:58,840
that sort of started looking at some of these pre-training

624
00:28:58,840 --> 00:29:01,520
statistics, or not just single term frequencies,

625
00:29:01,520 --> 00:29:03,760
but diagram frequencies and things like that,

626
00:29:03,760 --> 00:29:07,840
and show that the model is quite sensitive

627
00:29:07,840 --> 00:29:09,920
to what these things should be, right?

628
00:29:09,920 --> 00:29:11,920
And I don't want to sort of make a claim

629
00:29:11,920 --> 00:29:13,920
that there is cutting, wasting, going on,

630
00:29:13,920 --> 00:29:15,960
or any of these things.

631
00:29:15,960 --> 00:29:18,640
But this effect is so strong that at least

632
00:29:18,640 --> 00:29:20,240
when we think about reasoning,

633
00:29:20,240 --> 00:29:22,880
and when we are evaluating reasoning

634
00:29:22,880 --> 00:29:24,680
in these language models,

635
00:29:24,680 --> 00:29:27,760
we should be taking this effect into account.

636
00:29:28,760 --> 00:29:31,080
And this may be a side note.

637
00:29:31,080 --> 00:29:34,680
It looks like the model that you evaluated with GPTJ,

638
00:29:34,680 --> 00:29:40,000
and clearly that's a model.

639
00:29:40,000 --> 00:29:41,760
So an open source model that you had access

640
00:29:41,760 --> 00:29:45,880
to the pre-training data kind of asks questions about,

641
00:29:45,880 --> 00:29:48,960
how do you get the same kind of insight

642
00:29:48,960 --> 00:29:52,440
into these models that are behind APIs?

643
00:29:52,440 --> 00:29:54,520
Yeah, so I think the question is,

644
00:29:54,520 --> 00:29:58,720
I kind of don't mind that models are behind APIs

645
00:29:58,720 --> 00:30:00,120
to some degree that's commercially

646
00:30:00,120 --> 00:30:01,680
that that kind of makes sense.

647
00:30:01,680 --> 00:30:04,000
I feel a little bit disappointing

648
00:30:04,000 --> 00:30:07,320
that the training data also is behind

649
00:30:07,320 --> 00:30:08,440
sort of close wall, right?

650
00:30:08,440 --> 00:30:11,360
So I know that there is a lot in the training data,

651
00:30:11,360 --> 00:30:13,280
but if we want to be able to understand

652
00:30:13,280 --> 00:30:16,160
why GPTJ works, or why GPTJ works,

653
00:30:16,160 --> 00:30:19,720
or even generally, when do language models work,

654
00:30:19,720 --> 00:30:21,120
when are they safe to deploy?

655
00:30:21,120 --> 00:30:22,680
All of these get a question.

656
00:30:22,680 --> 00:30:24,920
I think it's okay if the language model

657
00:30:24,920 --> 00:30:26,920
we only have a black box access to,

658
00:30:26,920 --> 00:30:29,680
but it would be good to have access to the training data.

659
00:30:29,680 --> 00:30:31,480
It would be good to have access to a bunch

660
00:30:31,480 --> 00:30:33,400
of these other things that can help us

661
00:30:33,400 --> 00:30:35,600
sort of do simple kind of analysis like this,

662
00:30:35,600 --> 00:30:37,760
and maybe more complex ones.

663
00:30:37,760 --> 00:30:41,240
And actually be able to sort of decide

664
00:30:41,240 --> 00:30:42,520
what to do with the model, okay?

665
00:30:42,520 --> 00:30:45,880
So I think this whole direction of trying to understand

666
00:30:45,880 --> 00:30:48,320
what's in the pre-training data, I think is key

667
00:30:48,320 --> 00:30:51,200
and then something that will persist

668
00:30:51,200 --> 00:30:52,760
for the next couple of years.

669
00:30:52,760 --> 00:30:57,040
Do you think we have the right tools to do that at scale?

670
00:30:57,040 --> 00:31:00,480
I'm imagining that was not an easy task

671
00:31:00,480 --> 00:31:03,480
to do just for simple mathematical problems.

672
00:31:03,480 --> 00:31:08,480
That's true, but training GPT-3 is also not a simple problem

673
00:31:09,400 --> 00:31:10,880
and people have solved it, right?

674
00:31:10,880 --> 00:31:15,720
So I think the tooling is something that everybody right now

675
00:31:15,720 --> 00:31:19,320
is sort of excited about building tools

676
00:31:19,320 --> 00:31:21,960
that actually give information and insights

677
00:31:21,960 --> 00:31:23,000
into these language models.

678
00:31:23,000 --> 00:31:25,760
And I think even at the idea,

679
00:31:25,760 --> 00:31:28,320
we are at sort of early stages of trying to do these things

680
00:31:28,320 --> 00:31:31,680
of building some tooling that can support this kind of analysis.

681
00:31:31,680 --> 00:31:34,040
But you know, if the data set is available,

682
00:31:34,040 --> 00:31:35,800
I think people will do amazing things.

683
00:31:35,800 --> 00:31:39,240
And I thought this would be impossible

684
00:31:39,240 --> 00:31:40,720
and it seemed like crazy.

685
00:31:40,720 --> 00:31:43,360
Like, hey, this is almost a terabyte of text.

686
00:31:43,360 --> 00:31:45,760
Like how can you do anything with that?

687
00:31:45,760 --> 00:31:49,760
And it was not trivial, but it was easier than impossible.

688
00:31:50,640 --> 00:31:53,440
How do you identify,

689
00:31:54,520 --> 00:31:56,760
you know, so you identified some behavior,

690
00:31:58,360 --> 00:32:00,200
the relationship between accuracy

691
00:32:00,200 --> 00:32:04,400
and frequency in the training data.

692
00:32:07,240 --> 00:32:11,840
How do you identify what that is a consequence of?

693
00:32:11,840 --> 00:32:15,600
Meaning is it specific to the way GPT-J was trained?

694
00:32:15,600 --> 00:32:18,840
Is it all transformer-based language models?

695
00:32:18,840 --> 00:32:23,040
Is it, you know, maybe something about that particular data set?

696
00:32:23,040 --> 00:32:27,680
Like, have you, are you able to say that it is

697
00:32:27,680 --> 00:32:32,400
a broad characteristic of LLMs in general

698
00:32:32,400 --> 00:32:35,160
based on the work that you've done thus far?

699
00:32:35,160 --> 00:32:38,840
That's a little bit difficult to sort of.

700
00:32:38,840 --> 00:32:40,960
Yeah, that's a little bit difficult to measure,

701
00:32:40,960 --> 00:32:43,840
partly because we don't have data set available

702
00:32:43,840 --> 00:32:45,640
for too many models, right?

703
00:32:45,640 --> 00:32:49,440
So at least we tried the whole slew of the Luther models

704
00:32:49,440 --> 00:32:50,880
that were trained of the same data set

705
00:32:50,880 --> 00:32:56,040
and we saw similar effects on different model sizes, essentially.

706
00:32:56,040 --> 00:32:59,560
And yeah, as data sets become pre-training data sets

707
00:32:59,560 --> 00:33:03,240
become more standard, it's fairly trivial to sort of extend this stuff.

708
00:33:03,240 --> 00:33:07,280
Since this paper, we also have sort of an online demo

709
00:33:07,280 --> 00:33:09,120
where we have a bunch of more tasks

710
00:33:09,120 --> 00:33:11,880
that try to go beyond mathematical reasoning.

711
00:33:11,880 --> 00:33:14,960
It's a little bit difficult to sort of even define

712
00:33:14,960 --> 00:33:16,760
what these sort of terms are

713
00:33:16,760 --> 00:33:19,680
and what you should be computing frequency of.

714
00:33:19,680 --> 00:33:22,440
But yeah, I think we should be able to do this stuff

715
00:33:22,440 --> 00:33:24,960
for other tasks and for other models.

716
00:33:24,960 --> 00:33:29,160
And to me, I think this is somehow a consequence

717
00:33:29,160 --> 00:33:33,320
of a language modeling loss that encourages us in some sense, right?

718
00:33:33,320 --> 00:33:38,200
So yes, the model has seen more and it'll be more accurate,

719
00:33:38,200 --> 00:33:40,320
but even the ones that it has seen less,

720
00:33:40,320 --> 00:33:42,520
like it has still seen billions of times.

721
00:33:42,520 --> 00:33:45,440
So there is no reason for it to be wrong on it,

722
00:33:45,440 --> 00:33:48,800
except for the fact that the language modeling loss would sort of

723
00:33:48,800 --> 00:33:51,480
want you to be more right on the ones that have seen more.

724
00:33:51,480 --> 00:33:52,320
Mm-hmm.

725
00:33:54,400 --> 00:33:58,600
Yeah, another paper identified out of Yav Goldberg's group.

726
00:33:58,600 --> 00:33:59,440
Oh, yeah.

727
00:33:59,440 --> 00:34:02,640
So this is work led by, I think I'll quickly talk about this.

728
00:34:02,640 --> 00:34:07,280
So this had a similar sort of intuition

729
00:34:07,280 --> 00:34:09,760
for trying to look at things in the data

730
00:34:09,760 --> 00:34:14,920
and trying to figure out why the model has certain biases

731
00:34:14,920 --> 00:34:16,960
or has certain errors.

732
00:34:16,960 --> 00:34:18,840
And this was sort of a little bit more

733
00:34:18,840 --> 00:34:23,840
on trying to identify when two entities are related, right?

734
00:34:23,840 --> 00:34:27,360
So if you say where was Barack Obama born,

735
00:34:27,360 --> 00:34:32,320
the model tends to say Chicago or in some sense,

736
00:34:32,320 --> 00:34:37,240
it can say Washington and depending on how you praise it.

737
00:34:37,240 --> 00:34:39,680
And like, why does it give the wrong answer?

738
00:34:39,680 --> 00:34:41,440
It's kind of a question.

739
00:34:41,440 --> 00:34:45,440
Why does it not say Hawaii or something?

740
00:34:45,440 --> 00:34:49,600
And I think to be able to answer this question,

741
00:34:49,600 --> 00:34:51,480
you have to go back to the pre-training data

742
00:34:51,480 --> 00:34:53,880
and try to see like, okay, what did it even see?

743
00:34:53,880 --> 00:34:56,360
So what I like about this paper is it kind of tries

744
00:34:56,360 --> 00:34:59,480
to build use causality tools

745
00:34:59,480 --> 00:35:01,080
and builds a whole causal graph

746
00:35:01,080 --> 00:35:05,000
for where these kind of predictions might have come from

747
00:35:05,000 --> 00:35:07,320
and then tries to estimate all of the edges

748
00:35:07,320 --> 00:35:08,440
in those causality graphs

749
00:35:08,440 --> 00:35:10,240
and tries to do some causal inference

750
00:35:10,240 --> 00:35:13,160
to sort of attribute it to specific

751
00:35:13,160 --> 00:35:16,280
statistics of the big data.

752
00:35:16,280 --> 00:35:21,280
So in this causal graph would each individual document

753
00:35:21,760 --> 00:35:25,600
in the pre-training data be an intervention of sorts?

754
00:35:25,600 --> 00:35:29,000
So they sort of worked, they worked at the level of,

755
00:35:29,000 --> 00:35:32,120
I guess, triples or something like that, right?

756
00:35:32,120 --> 00:35:35,760
So let's say you see Obama in Chicago

757
00:35:35,760 --> 00:35:37,360
being a senator there or something, right?

758
00:35:37,360 --> 00:35:40,240
So this is kind of a triple.

759
00:35:40,240 --> 00:35:43,240
And so they work on statistics of those triples

760
00:35:43,240 --> 00:35:45,800
of the pre-training data to sort of make it tractable

761
00:35:45,800 --> 00:35:49,080
and make it sort of allow this inference to work.

762
00:35:49,080 --> 00:35:52,040
But in applying the causality machinery

763
00:35:52,040 --> 00:35:56,040
like are each of those interventions relative to

764
00:35:58,160 --> 00:36:03,160
some prior relationship between the things the triples?

765
00:36:03,360 --> 00:36:05,560
Yeah, so there is the true relationship

766
00:36:05,560 --> 00:36:06,680
between these triples

767
00:36:06,680 --> 00:36:08,800
and then there is the observed relationship

768
00:36:08,800 --> 00:36:13,000
between these triples and how many of these things,

769
00:36:13,000 --> 00:36:16,520
how many times it appeared in the pre-training data.

770
00:36:16,520 --> 00:36:19,320
And so the idea would be when you're doing it

771
00:36:19,320 --> 00:36:22,360
over many different entities and many different relations,

772
00:36:23,520 --> 00:36:28,280
do you, so those kind of become your whole data set

773
00:36:28,280 --> 00:36:29,120
in some sense.

774
00:36:29,120 --> 00:36:32,080
So Obama has appeared with Chicago,

775
00:36:32,080 --> 00:36:34,640
but Hillary Clinton has appeared elsewhere

776
00:36:34,640 --> 00:36:36,320
and on all of these things.

777
00:36:36,320 --> 00:36:41,320
And then together, which of these relations

778
00:36:41,760 --> 00:36:45,600
seem to affect a specific prediction the most?

779
00:36:45,600 --> 00:36:46,600
That counts.

780
00:36:46,600 --> 00:36:47,600
Awesome, awesome.

781
00:36:48,560 --> 00:36:52,640
Kind of continuing on in the data theme,

782
00:36:52,640 --> 00:36:57,640
there's been a ton of work looking at the need for clean data.

783
00:36:58,120 --> 00:37:00,240
I think maybe one of the most surprising things for me

784
00:37:00,240 --> 00:37:03,160
is like the return of supervision

785
00:37:03,160 --> 00:37:05,560
at the scale of LLMs.

786
00:37:05,560 --> 00:37:08,040
Talk a little bit about this category.

787
00:37:08,040 --> 00:37:10,840
Yeah, so this was somehow the most surprising category

788
00:37:10,840 --> 00:37:12,720
for me for this year.

789
00:37:12,720 --> 00:37:15,360
I will say that like after GBT's

790
00:37:15,360 --> 00:37:17,880
it came out and at the end of last year,

791
00:37:17,880 --> 00:37:21,120
everybody was kind of excited about language models,

792
00:37:21,120 --> 00:37:24,120
but the solutions for what's next always seem to be like,

793
00:37:24,120 --> 00:37:27,040
hey, let's get more data and let's get larger models

794
00:37:27,040 --> 00:37:28,720
and let's train, train longer.

795
00:37:28,720 --> 00:37:31,960
And those are still sort of useful things nobody's denying.

796
00:37:31,960 --> 00:37:33,560
But this year has shown that like,

797
00:37:33,560 --> 00:37:35,600
okay, you can actually do a lot

798
00:37:35,600 --> 00:37:39,160
if you're a little bit careful about your data, right?

799
00:37:39,160 --> 00:37:42,240
And maybe if you'll start cleaning up your data

800
00:37:42,240 --> 00:37:45,400
and try to think a little bit about where the data,

801
00:37:45,400 --> 00:37:47,080
your pre-gaining data should come from,

802
00:37:47,080 --> 00:37:49,560
your pre-gaining data itself, you know,

803
00:37:49,560 --> 00:37:51,600
that could be quite interesting.

804
00:37:51,600 --> 00:37:55,160
So when you think of like RLHF as an example,

805
00:37:55,160 --> 00:37:57,480
do you think of that as fundamentally

806
00:37:57,480 --> 00:37:59,840
just cleaning up your data, being more careful

807
00:37:59,840 --> 00:38:01,920
about your data as opposed to?

808
00:38:01,920 --> 00:38:04,920
Yeah, so no, I think I was thinking more

809
00:38:04,920 --> 00:38:07,640
what happened with the loom language model

810
00:38:07,640 --> 00:38:11,920
which was trained on sort of a lot more

811
00:38:11,920 --> 00:38:14,400
thoughtful process of gathering the data set

812
00:38:14,400 --> 00:38:16,440
because partly because they documented it

813
00:38:16,440 --> 00:38:18,840
and we know what sort of they went through.

814
00:38:18,840 --> 00:38:22,880
But now like RLHF and those kind of things,

815
00:38:22,880 --> 00:38:26,840
I think our examples of showing that the language models

816
00:38:26,840 --> 00:38:29,480
are not quite ready for use case

817
00:38:29,480 --> 00:38:32,760
just based on pre-training on sort of large data

818
00:38:32,760 --> 00:38:34,720
that has been gathered.

819
00:38:34,720 --> 00:38:37,720
You need to read, like you can call it like,

820
00:38:37,720 --> 00:38:40,440
hey, cleaning up the data, but I think of it as like

821
00:38:40,440 --> 00:38:44,480
maybe reinforcing some of the nice signals in the data

822
00:38:44,480 --> 00:38:46,160
by having these examples.

823
00:38:46,160 --> 00:38:48,360
Or in some sense, you know, people have been fine-tuning

824
00:38:48,360 --> 00:38:51,280
on these sort of supervised data as well.

825
00:38:51,280 --> 00:38:55,760
And the gains that you get from RLHF

826
00:38:55,760 --> 00:38:58,840
have become extremely evident this year, right?

827
00:38:58,840 --> 00:39:01,680
So somehow that has become the secret source

828
00:39:01,680 --> 00:39:04,720
of opening eye in all of these companies

829
00:39:04,720 --> 00:39:08,320
that want to have really strong language models

830
00:39:08,320 --> 00:39:11,960
rather than scale and just draw a pre-training.

831
00:39:13,080 --> 00:39:15,200
And for completeness, we've talked a little bit

832
00:39:15,200 --> 00:39:17,800
about RLHF on the show, but what's,

833
00:39:17,800 --> 00:39:20,160
how do you think about it as a researcher?

834
00:39:20,160 --> 00:39:21,320
I think it's quite exciting.

835
00:39:21,320 --> 00:39:23,680
I think it sort of addresses a lot of

836
00:39:23,680 --> 00:39:25,640
my concerns with language models.

837
00:39:25,640 --> 00:39:29,760
I don't think pre-training data can be trusted, right?

838
00:39:29,760 --> 00:39:31,800
And you shouldn't just train something

839
00:39:31,800 --> 00:39:34,760
and expect the model to have clean output

840
00:39:34,760 --> 00:39:38,400
or have, you know, your values

841
00:39:38,400 --> 00:39:40,040
and any of these kind of things,

842
00:39:41,040 --> 00:39:43,480
whatever that means is the context of large language models.

843
00:39:43,480 --> 00:39:46,800
But essentially, if you want real users

844
00:39:46,800 --> 00:39:49,840
to be interfacing with language models,

845
00:39:49,840 --> 00:39:54,160
you need to make sure that there is some sort of check.

846
00:39:54,160 --> 00:39:57,320
And RLHF is not a solution, like a full solution,

847
00:39:57,320 --> 00:40:00,960
but at least there is a way to sort of say, okay,

848
00:40:00,960 --> 00:40:02,480
this is the actual task.

849
00:40:02,480 --> 00:40:05,240
Your actual task is to be interfacing with humans,

850
00:40:05,240 --> 00:40:08,080
not just regurgitating what you've seen

851
00:40:08,080 --> 00:40:09,800
in the pre-training corpus, right?

852
00:40:09,800 --> 00:40:13,960
And so that intuition sort of is captured by this RLHF.

853
00:40:13,960 --> 00:40:16,840
And do you, do you remember offhand any of the,

854
00:40:16,840 --> 00:40:19,200
if they were even published the stats

855
00:40:19,200 --> 00:40:22,040
in terms of the number of prompts,

856
00:40:22,040 --> 00:40:23,960
like human generated prompts that were used

857
00:40:23,960 --> 00:40:27,640
in chat GPT or in struct GPT?

858
00:40:27,640 --> 00:40:29,600
Yeah, I don't think they were published

859
00:40:29,600 --> 00:40:30,440
as far as I know.

860
00:40:30,440 --> 00:40:33,560
Yeah, I don't remember exactly what they are.

861
00:40:33,560 --> 00:40:35,760
I think until GPT had the documentation

862
00:40:35,760 --> 00:40:37,560
of sort of how they were gathered,

863
00:40:37,560 --> 00:40:39,880
but the size was like, you know,

864
00:40:39,880 --> 00:40:42,480
how many of them were sort of generation does,

865
00:40:42,480 --> 00:40:45,360
was just classification does, things like that.

866
00:40:45,360 --> 00:40:48,960
But I don't think the exact dataset is good.

867
00:40:48,960 --> 00:40:52,400
Do you, do you have a guess

868
00:40:52,400 --> 00:40:55,640
as to like the relative cost of, you know,

869
00:40:55,640 --> 00:40:57,560
gender of collecting the human feedback

870
00:40:57,560 --> 00:41:00,280
relative to the cost of training the models?

871
00:41:00,280 --> 00:41:03,840
Oh, the rate of cost of training the more it was like,

872
00:41:03,840 --> 00:41:05,120
I think it's much cheaper.

873
00:41:05,120 --> 00:41:08,360
Order of magnitude or is it like much, much, much cheaper?

874
00:41:08,360 --> 00:41:10,040
Because we always say like, you know,

875
00:41:10,040 --> 00:41:11,680
collecting the data label data

876
00:41:11,680 --> 00:41:13,440
is the most expensive part of machine learning.

877
00:41:13,440 --> 00:41:16,720
Is that still true at the scale of LLMs?

878
00:41:17,680 --> 00:41:20,840
Or is it that RLHF is like extremely efficient

879
00:41:20,840 --> 00:41:23,520
and you just need a little bit of guidance

880
00:41:23,520 --> 00:41:27,280
on top of the, you know, the pre-training data?

881
00:41:27,280 --> 00:41:29,560
I feel the true answer is somewhere in between.

882
00:41:29,560 --> 00:41:32,080
So I don't think it's like, it's nowhere

883
00:41:32,080 --> 00:41:34,560
very little data, like I think you need a lot of data

884
00:41:34,560 --> 00:41:35,680
to be able to do it,

885
00:41:35,680 --> 00:41:37,000
but I don't think it comes close,

886
00:41:37,000 --> 00:41:38,960
at least the way these are trained right now,

887
00:41:38,960 --> 00:41:42,040
I don't think it comes close to sort of training

888
00:41:42,040 --> 00:41:42,920
the model itself, right?

889
00:41:42,920 --> 00:41:45,920
So, but like when you think about, you know,

890
00:41:45,920 --> 00:41:49,920
charge GPT, it's been released publicly

891
00:41:49,920 --> 00:41:51,720
and a lot of people are using it.

892
00:41:51,720 --> 00:41:54,120
A lot of that data is gonna go into,

893
00:41:54,120 --> 00:41:56,920
in some form, back into the model and improve it.

894
00:41:56,920 --> 00:42:00,960
So was that expensive to collect in some sense

895
00:42:00,960 --> 00:42:03,880
because they had to run charge GPT, but, you know,

896
00:42:03,880 --> 00:42:06,040
they'll probably pay some managers to clean that up,

897
00:42:06,040 --> 00:42:09,000
but I don't think that's gonna prepare to actual training.

898
00:42:09,000 --> 00:42:13,400
It's also a really interesting example

899
00:42:13,400 --> 00:42:15,720
of like bootstrapping, like there's a certain amount

900
00:42:15,720 --> 00:42:18,120
that they collected themselves, you know,

901
00:42:18,120 --> 00:42:22,200
the instruction GPT work, and then they, you know,

902
00:42:22,200 --> 00:42:23,520
created something that was good enough

903
00:42:23,520 --> 00:42:25,200
to set loose in the world,

904
00:42:25,200 --> 00:42:26,960
and now they've got this virtual cycle

905
00:42:26,960 --> 00:42:29,440
where I'm imagining it's a lot cheaper

906
00:42:29,440 --> 00:42:31,440
for some annotator to clean up,

907
00:42:31,440 --> 00:42:33,960
what, you know, millions of people are creating

908
00:42:33,960 --> 00:42:36,880
then for them to create that themselves.

909
00:42:36,880 --> 00:42:39,400
And I think like, I think this year has also shown,

910
00:42:39,400 --> 00:42:41,040
maybe even to people at OpenAI,

911
00:42:41,040 --> 00:42:43,240
that the value of these things, right?

912
00:42:43,240 --> 00:42:45,080
Like when they released GPT-3,

913
00:42:45,080 --> 00:42:47,080
they probably didn't realize how valuable it would be,

914
00:42:47,080 --> 00:42:49,960
and then they sort of collected data released in start GPT,

915
00:42:49,960 --> 00:42:51,840
and yeah, on their benchmarks, it was good,

916
00:42:51,840 --> 00:42:53,480
but when people started using it,

917
00:42:53,480 --> 00:42:55,280
you realize how much better it is.

918
00:42:55,280 --> 00:42:56,960
I think similarly with chat GPT,

919
00:42:56,960 --> 00:42:59,160
they probably knew how good it was,

920
00:42:59,160 --> 00:43:00,640
but they probably didn't realize

921
00:43:00,640 --> 00:43:03,360
how good it actually is, right?

922
00:43:03,360 --> 00:43:07,480
And I think this idea of human feedback,

923
00:43:07,480 --> 00:43:10,440
being a secret source that is proprietary,

924
00:43:10,440 --> 00:43:17,440
I think, will continue to be a bigger piece in the future.

925
00:43:17,760 --> 00:43:19,880
Talk a little bit about Roots.

926
00:43:19,880 --> 00:43:22,280
Yeah, so the Roots is this nice data set

927
00:43:22,280 --> 00:43:25,000
that was gathered by the big science group,

928
00:43:25,000 --> 00:43:27,000
and I've been following the big science group,

929
00:43:27,000 --> 00:43:31,320
and a bunch of interesting things there.

930
00:43:31,320 --> 00:43:33,960
I guess I'll jump in to refer to the interview

931
00:43:33,960 --> 00:43:35,360
that I did with Thomas Wolf,

932
00:43:35,360 --> 00:43:38,640
that I don't think Roots came up explicitly,

933
00:43:38,640 --> 00:43:41,000
but we talked about that work,

934
00:43:41,000 --> 00:43:43,040
and that eventually resulted in Bloom,

935
00:43:43,040 --> 00:43:45,080
which we'll talk about a little bit more as well.

936
00:43:45,080 --> 00:43:48,440
Yeah, so Roots, I like because I think I really like

937
00:43:48,440 --> 00:43:50,680
what Luther have done with the pilot dataset,

938
00:43:50,680 --> 00:43:52,360
by releasing the dataset that was used

939
00:43:52,360 --> 00:43:54,320
to train all the GPTJ models,

940
00:43:54,320 --> 00:43:57,040
and I think the big science group sort of took their intuition

941
00:43:57,040 --> 00:43:58,600
and sort of went further with it,

942
00:43:58,600 --> 00:44:01,200
where they have a really well-documented,

943
00:44:02,320 --> 00:44:03,880
and not just well-documented,

944
00:44:03,880 --> 00:44:06,920
I would say a very thoughtful process

945
00:44:06,920 --> 00:44:08,960
of gathering this dataset.

946
00:44:08,960 --> 00:44:12,600
It's multi-lingual over many, many different languages,

947
00:44:12,600 --> 00:44:15,560
they've been careful about sort of listing which sources

948
00:44:15,560 --> 00:44:19,240
they want to even crawl in the first place before.

949
00:44:19,240 --> 00:44:21,760
So it's not like a post hot cleanup of the data,

950
00:44:21,760 --> 00:44:24,600
it's very sort of thinking about it.

951
00:44:24,600 --> 00:44:26,680
They gathered a dataset that is huge,

952
00:44:26,680 --> 00:44:29,200
and we talked a little bit about this data,

953
00:44:29,200 --> 00:44:31,560
also the hugging phase has sort of built tools

954
00:44:31,560 --> 00:44:34,760
on top of it to be able to quickly search it,

955
00:44:34,760 --> 00:44:37,560
to see what's in it, and stuff like that.

956
00:44:37,560 --> 00:44:41,640
And I kind of like that approach to life language models.

957
00:44:41,640 --> 00:44:45,920
So I think getting the right dataset

958
00:44:45,920 --> 00:44:47,840
is crucial for these language models,

959
00:44:47,840 --> 00:44:50,760
and doing this documentation and stuff

960
00:44:50,760 --> 00:44:53,120
is good for in the long term.

961
00:44:53,120 --> 00:44:57,200
So your next category is decoding only.

962
00:44:57,200 --> 00:44:59,760
Talk a little bit about what that means.

963
00:44:59,760 --> 00:45:04,520
Yeah, so this is a theme that I like about some of the work

964
00:45:04,520 --> 00:45:06,400
that has come out here.

965
00:45:06,400 --> 00:45:09,520
And partly it's because we have these language models,

966
00:45:09,520 --> 00:45:13,120
where we have this black box of interface to them.

967
00:45:13,120 --> 00:45:15,400
And a lot of it is just prompting,

968
00:45:15,400 --> 00:45:17,240
so changing things on the input side

969
00:45:17,240 --> 00:45:19,240
to see what the model generates,

970
00:45:19,240 --> 00:45:21,840
and the only thing most people are changing

971
00:45:21,840 --> 00:45:23,240
on the output side is like, oh,

972
00:45:23,240 --> 00:45:24,960
we let's change the temperature a little bit,

973
00:45:24,960 --> 00:45:27,560
and we get part of different things.

974
00:45:27,560 --> 00:45:29,280
But there has been a bunch of work looking at,

975
00:45:29,280 --> 00:45:31,040
okay, let's not just do that.

976
00:45:31,040 --> 00:45:34,000
Let's actually think about what's happening

977
00:45:34,000 --> 00:45:37,920
in the output of the model during decoding of the text.

978
00:45:37,920 --> 00:45:40,080
And maybe we can do smart things there

979
00:45:41,160 --> 00:45:44,720
that actually sort of change the output considerably, right?

980
00:45:44,720 --> 00:45:49,160
So some of these sort of came out sort of late last year.

981
00:45:49,160 --> 00:45:52,400
So there was this work on Newtius sampling,

982
00:45:52,400 --> 00:45:53,600
that's a little bit older,

983
00:45:53,600 --> 00:45:56,720
but then there was this stuff on sort of constrained decoding

984
00:45:56,720 --> 00:46:00,400
as well, where the constrained decoding paper

985
00:46:00,400 --> 00:46:02,680
came out of semantic machines.

986
00:46:02,680 --> 00:46:06,080
They showed that you can have,

987
00:46:06,080 --> 00:46:08,560
suppose you want to want the language model

988
00:46:08,560 --> 00:46:10,240
to generate programs, right?

989
00:46:10,240 --> 00:46:13,120
So the programs come with a certain grammar, right?

990
00:46:13,120 --> 00:46:16,080
Like there is a syntax that they need to follow.

991
00:46:16,080 --> 00:46:20,480
So you could actually constrain the output of the language model

992
00:46:20,480 --> 00:46:22,720
as it's generating token by token

993
00:46:22,720 --> 00:46:27,000
to sort of adhere to that syntax in some sense, right?

994
00:46:27,920 --> 00:46:29,640
And just by doing this constraint,

995
00:46:29,640 --> 00:46:31,120
you can get, firstly, obviously,

996
00:46:31,120 --> 00:46:34,360
you will get programs that are syntactically correct,

997
00:46:34,360 --> 00:46:37,880
but you can actually get the right things out of the model.

998
00:46:37,880 --> 00:46:42,880
And so there have been a lot of sort of works looking at

999
00:46:44,440 --> 00:46:48,960
how can we decode by having some constraints on the decoding.

1000
00:46:49,760 --> 00:46:51,680
So one of the papers that came out this year,

1001
00:46:51,680 --> 00:46:56,680
that we've got the best paper award as well,

1002
00:46:56,680 --> 00:47:01,680
is called Neural Logic ASTAR, ASTAR-esque decoding.

1003
00:47:02,040 --> 00:47:05,080
And the idea here is that instead of just doing

1004
00:47:05,080 --> 00:47:07,440
left to right decoding where you're being greedy

1005
00:47:07,440 --> 00:47:09,960
or where you're being doing some kind of beam search

1006
00:47:09,960 --> 00:47:12,040
or sampling or any of these process,

1007
00:47:12,040 --> 00:47:15,840
why don't you actually use some of the computer science ideas

1008
00:47:15,840 --> 00:47:19,160
that we have like ASTAR search

1009
00:47:19,160 --> 00:47:22,240
and try to find the best possible decoding.

1010
00:47:24,160 --> 00:47:25,600
And then when you're doing this kind of thing,

1011
00:47:25,600 --> 00:47:27,360
you can also think about constraints

1012
00:47:27,360 --> 00:47:29,360
that you might want to put on the decoding.

1013
00:47:29,360 --> 00:47:32,280
So you want to say, look, I want the decoding

1014
00:47:32,280 --> 00:47:34,880
to have these three words in it, right?

1015
00:47:35,920 --> 00:47:38,680
Like, hey, you're generating a recipe,

1016
00:47:38,680 --> 00:47:41,640
make sure that it has these five ingredients, right?

1017
00:47:41,640 --> 00:47:43,440
Somewhere in the generated text.

1018
00:47:44,360 --> 00:47:47,920
You can also flip it around, hey, generate whatever text

1019
00:47:47,920 --> 00:47:51,240
you generate, make sure it doesn't have these specific words,

1020
00:47:51,240 --> 00:47:52,760
like that.

1021
00:47:52,760 --> 00:47:56,520
And this paper sort of uses ASTAR

1022
00:47:56,520 --> 00:48:00,280
during decoding to generate that text,

1023
00:48:00,280 --> 00:48:04,040
that sort of, you know, your constraints are satisfied.

1024
00:48:05,080 --> 00:48:06,240
And this paper showed that, yeah,

1025
00:48:06,240 --> 00:48:07,360
once you do that properly,

1026
00:48:07,360 --> 00:48:10,880
you can actually do a lot of the tasks much better

1027
00:48:10,880 --> 00:48:14,200
just by controlling decoding rather than changing much

1028
00:48:14,200 --> 00:48:15,400
on the inputs.

1029
00:48:15,400 --> 00:48:17,080
It seems like this is another example

1030
00:48:17,080 --> 00:48:22,400
where it's predicated on having open access

1031
00:48:22,400 --> 00:48:23,760
to the model internals,

1032
00:48:23,760 --> 00:48:27,840
and you potentially lose a lot if you don't.

1033
00:48:27,840 --> 00:48:30,240
Yeah, I think so from what I understand,

1034
00:48:30,240 --> 00:48:32,320
you can still do these kind of things

1035
00:48:32,320 --> 00:48:35,160
with GPT-3 to some degree.

1036
00:48:35,160 --> 00:48:37,400
I think what you need, you can, okay,

1037
00:48:37,400 --> 00:48:40,920
so you can do this with black box model

1038
00:48:40,920 --> 00:48:42,960
as long as you get the probabilities

1039
00:48:42,960 --> 00:48:45,680
of all of the tokens at every step, right?

1040
00:48:45,680 --> 00:48:49,080
So I don't think GPT actually does that, right?

1041
00:48:49,080 --> 00:48:51,080
But, you know, you could imagine an API

1042
00:48:51,080 --> 00:48:52,600
that says, okay, the next,

1043
00:48:52,600 --> 00:48:54,920
here's the distribution over all of the tokens.

1044
00:48:56,240 --> 00:49:00,360
And you should be still be able to do these kind of items.

1045
00:49:00,360 --> 00:49:03,600
So, you know, some of the concerns is like,

1046
00:49:03,600 --> 00:49:05,560
if you want decoding to be fast,

1047
00:49:05,560 --> 00:49:08,160
then it's difficult to use some of these ideas.

1048
00:49:08,160 --> 00:49:11,440
The A-star-1 specificities is a lot slower,

1049
00:49:12,400 --> 00:49:14,440
but it's able to satisfy your constraints.

1050
00:49:14,440 --> 00:49:18,640
So it can be where you're okay to trade off some time,

1051
00:49:18,640 --> 00:49:21,320
but let the model take more time

1052
00:49:21,320 --> 00:49:23,880
in making sure the output is seen

1053
00:49:23,880 --> 00:49:25,640
and satisfy your constraints,

1054
00:49:25,640 --> 00:49:27,200
it could be really cool.

1055
00:49:28,680 --> 00:49:30,800
And now, yeah, often you see,

1056
00:49:30,800 --> 00:49:34,760
hey, we applied one method, A-star, in this case,

1057
00:49:34,760 --> 00:49:37,320
let's go back to the computer science toolkit

1058
00:49:37,320 --> 00:49:38,520
and apply everything else.

1059
00:49:38,520 --> 00:49:40,400
Have we seen that here?

1060
00:49:40,400 --> 00:49:41,240
Not yet.

1061
00:49:41,240 --> 00:49:43,840
I think it came out late enough in the year.

1062
00:49:43,840 --> 00:49:46,280
But I guess it came out sort of early in the year,

1063
00:49:46,280 --> 00:49:48,480
but yeah, we haven't seen that much yet

1064
00:49:48,480 --> 00:49:49,800
because, but I think, yeah,

1065
00:49:49,800 --> 00:49:52,040
that's the kind of thing that will happen next.

1066
00:49:52,040 --> 00:49:54,640
It's like, okay, now this is, yeah,

1067
00:49:54,640 --> 00:49:57,040
this is attracting a whole different kind of thinking

1068
00:49:57,040 --> 00:49:59,840
where people were not thinking about decoding at all,

1069
00:49:59,840 --> 00:50:01,880
and now they will be in this light,

1070
00:50:01,880 --> 00:50:04,680
which is all this kind of good paper.

1071
00:50:04,680 --> 00:50:05,520
Awesome, awesome.

1072
00:50:05,520 --> 00:50:09,800
Well, those are great themes to kind of reflect on

1073
00:50:09,800 --> 00:50:14,760
as we think about the past year and LP research.

1074
00:50:14,760 --> 00:50:16,640
Our next category is to talk about

1075
00:50:16,640 --> 00:50:20,560
some of the new tools and open source projects

1076
00:50:20,560 --> 00:50:23,320
that we saw in the year.

1077
00:50:23,320 --> 00:50:26,400
We've already talked a little bit about data sets,

1078
00:50:26,400 --> 00:50:28,000
which is kind of related.

1079
00:50:29,440 --> 00:50:32,960
But I think the first thing you have here is OPT.

1080
00:50:32,960 --> 00:50:34,680
That's all about OPT.

1081
00:50:34,680 --> 00:50:39,200
So yeah, I think OPT came out fairly early in this year.

1082
00:50:40,000 --> 00:50:42,280
And I think it kind of surprised everyone

1083
00:50:42,280 --> 00:50:46,920
because the sort of looking back at last year,

1084
00:50:46,920 --> 00:50:49,840
there weren't that many open source reproductions

1085
00:50:49,840 --> 00:50:51,440
of large sites, right?

1086
00:50:51,440 --> 00:50:54,960
So I think Luther AI was sort of leading it.

1087
00:50:54,960 --> 00:50:57,760
GPTJ was six billion and, you know,

1088
00:50:57,760 --> 00:51:00,200
they were sort of growing it slowly and slowly

1089
00:51:00,200 --> 00:51:02,880
and they had got to 20 billion parameters.

1090
00:51:03,800 --> 00:51:06,640
And then OPT sort of came into the scene

1091
00:51:06,640 --> 00:51:09,840
and they, yeah, there were a bunch of nice things.

1092
00:51:09,840 --> 00:51:12,600
They documented a lot of their whole training process

1093
00:51:12,600 --> 00:51:15,560
in a log work with sort of all kinds of insights

1094
00:51:15,560 --> 00:51:17,360
about what training a log.

1095
00:51:18,400 --> 00:51:21,080
Yes, it was released better, right?

1096
00:51:21,080 --> 00:51:24,360
And that was also not to say too much against better,

1097
00:51:24,360 --> 00:51:27,200
but it was also surprising that reproducibility

1098
00:51:27,200 --> 00:51:30,720
and open source seemed to be key aspect of OPT as well.

1099
00:51:30,720 --> 00:51:33,360
So that was kind of nice.

1100
00:51:33,360 --> 00:51:37,560
And they also released a lot of models and, you know,

1101
00:51:37,560 --> 00:51:42,560
like all different sizes, including 175 billion,

1102
00:51:42,880 --> 00:51:46,760
which hadn't been available at all.

1103
00:51:46,760 --> 00:51:50,640
And even right now, I think it's probably the most useful model

1104
00:51:50,640 --> 00:51:53,160
if you want to do stuff with 175 billion

1105
00:51:53,160 --> 00:51:55,720
is to use the OPT model, right?

1106
00:51:55,720 --> 00:52:00,720
So I think the idea of documenting the whole training data

1107
00:52:01,360 --> 00:52:04,240
gathering process, documenting the whole training

1108
00:52:04,240 --> 00:52:05,840
of the model process,

1109
00:52:05,840 --> 00:52:10,200
and then releasing all of these models available

1110
00:52:10,200 --> 00:52:14,200
for research, I think has helped the research community a lot.

1111
00:52:14,200 --> 00:52:16,400
And I expect that if there are people

1112
00:52:16,400 --> 00:52:19,600
who want to build models and potentially find you

1113
00:52:19,600 --> 00:52:22,520
in language models and do all of these things,

1114
00:52:22,520 --> 00:52:24,720
the OPT would be a pretty big source.

1115
00:52:24,720 --> 00:52:27,520
Have you seen much in terms of benchmarking it

1116
00:52:27,520 --> 00:52:29,720
against GPT-3?

1117
00:52:29,720 --> 00:52:32,680
Yes, so I think people have been benchmarking it.

1118
00:52:32,680 --> 00:52:36,040
And I think it performs reasonably well.

1119
00:52:36,040 --> 00:52:37,960
The tricky thing is, of course,

1120
00:52:37,960 --> 00:52:41,520
there is instruct GPT, which is, you know,

1121
00:52:41,520 --> 00:52:44,160
when you call it GPT-3 on the API right now,

1122
00:52:44,160 --> 00:52:47,080
it's often defaults to the instruct one.

1123
00:52:47,080 --> 00:52:49,320
And that one is a lot more difficult to beat,

1124
00:52:49,320 --> 00:52:52,000
but for all of the purposes, I think of it as like,

1125
00:52:52,000 --> 00:52:55,440
yeah, OPT-8 is basically same as GPT-3.

1126
00:52:55,440 --> 00:53:00,440
We talked a little bit about the big science project

1127
00:53:00,440 --> 00:53:04,240
and one of its outputs and other is Bloom.

1128
00:53:04,240 --> 00:53:07,320
What did you, what was your take on Bloom?

1129
00:53:07,320 --> 00:53:12,080
Bloom was, again, a really big data model.

1130
00:53:12,080 --> 00:53:14,680
That was, I think, 180 billion parameters.

1131
00:53:14,680 --> 00:53:16,880
So similar sizes, GPT-3,

1132
00:53:16,880 --> 00:53:19,680
believes to become a daily open source.

1133
00:53:19,680 --> 00:53:22,480
It's like we talked about completely well-documented

1134
00:53:22,480 --> 00:53:25,560
data process and sort of training process.

1135
00:53:25,560 --> 00:53:29,920
Combined with the fact that this was done by a group of people

1136
00:53:29,920 --> 00:53:33,920
it's kind of just volunteering their time to do so.

1137
00:53:33,920 --> 00:53:38,160
And then being able to reproduce to a large degree,

1138
00:53:38,160 --> 00:53:41,720
what OpenAI has done was quite amazing, right?

1139
00:53:41,720 --> 00:53:44,760
And then sort of, again, like both OPTs,

1140
00:53:44,760 --> 00:53:46,960
releasing all of these things is kind of a sign

1141
00:53:46,960 --> 00:53:49,640
for other big tech companies to say like,

1142
00:53:49,640 --> 00:53:52,800
hey, you can do this because we've done this kind of thing.

1143
00:53:52,800 --> 00:53:57,640
But Bloom has shown is that a bunch of people enthusiastic

1144
00:53:57,640 --> 00:54:01,600
and excited folks that are enterprising can actually do things

1145
00:54:01,600 --> 00:54:06,480
that maybe even a year or two ago would have seemed impossible.

1146
00:54:06,480 --> 00:54:14,280
It may have been in our trends conversation

1147
00:54:14,280 --> 00:54:20,800
from last year or maybe it was prior.

1148
00:54:20,800 --> 00:54:23,520
But in these kinds of conversations,

1149
00:54:23,520 --> 00:54:26,040
there was a point in time where we were lamenting

1150
00:54:26,040 --> 00:54:31,040
the loss on the part of the individual academic researcher

1151
00:54:31,400 --> 00:54:34,800
to contribute to fundamental model research

1152
00:54:34,800 --> 00:54:37,440
because of the resources that were required.

1153
00:54:37,440 --> 00:54:41,800
And to hugging face and the big sciences team,

1154
00:54:41,800 --> 00:54:46,240
like they showed that not necessarily not so fast, right?

1155
00:54:46,240 --> 00:54:48,000
Right, right, exactly.

1156
00:54:48,000 --> 00:54:50,120
And the other thing I like about this,

1157
00:54:50,120 --> 00:54:53,000
the blue method is and the corpus that came with it,

1158
00:54:53,000 --> 00:54:56,240
they were also focused on being a lot more inclusive

1159
00:54:56,240 --> 00:54:58,960
in terms of having a global perspective.

1160
00:54:58,960 --> 00:55:02,040
So they were trying to cover many, many different languages.

1161
00:55:02,040 --> 00:55:04,920
Very principled in the way they pulled the data together.

1162
00:55:04,920 --> 00:55:06,560
Yeah, yeah.

1163
00:55:06,560 --> 00:55:07,920
And also multilingual in a way

1164
00:55:07,920 --> 00:55:10,160
that none of the existing models have been.

1165
00:55:10,160 --> 00:55:12,600
So yeah, it's quite quite exciting.

1166
00:55:12,600 --> 00:55:14,880
And so like conceptually,

1167
00:55:14,880 --> 00:55:19,680
this is a great example of how one model

1168
00:55:19,680 --> 00:55:24,560
at 175 billion primers and another model,

1169
00:55:24,560 --> 00:55:26,360
you know, the same number of primers

1170
00:55:26,360 --> 00:55:28,760
could be very different, you know,

1171
00:55:28,760 --> 00:55:30,640
at least in the data that they were trained on

1172
00:55:30,640 --> 00:55:33,920
and you would expect that to result in

1173
00:55:33,920 --> 00:55:37,120
very different results using the model.

1174
00:55:37,120 --> 00:55:40,280
To what extent have we characterized that?

1175
00:55:40,280 --> 00:55:42,240
Like at that scale of data,

1176
00:55:42,240 --> 00:55:43,440
it's still a lot of data,

1177
00:55:43,440 --> 00:55:46,800
still a lot of like raw internet data.

1178
00:55:46,800 --> 00:55:49,200
Does it all kind of fall out in the wash

1179
00:55:49,200 --> 00:55:51,360
and all their efforts at being principled,

1180
00:55:51,360 --> 00:55:54,480
you know, kind of just get lost?

1181
00:55:54,480 --> 00:55:57,760
Or do we know how to compare that?

1182
00:55:57,760 --> 00:55:59,760
Yeah, so there have been a bunch of benchmarks

1183
00:55:59,760 --> 00:56:01,320
and including in their papers,

1184
00:56:01,320 --> 00:56:02,720
but in general also.

1185
00:56:02,720 --> 00:56:05,720
And that's where sort of the, you know,

1186
00:56:05,720 --> 00:56:07,040
don't hold me to this,

1187
00:56:07,040 --> 00:56:11,040
but I would say like Bloom is not the go-to language model

1188
00:56:11,040 --> 00:56:14,280
for people if they want to do English things, right?

1189
00:56:14,280 --> 00:56:17,280
All right, so I think maybe some of the trade-offs

1190
00:56:17,280 --> 00:56:19,040
that made in collecting the data

1191
00:56:19,040 --> 00:56:21,680
or even just having all languages

1192
00:56:22,720 --> 00:56:24,840
result in the model that's definitely really good

1193
00:56:24,840 --> 00:56:26,600
for multilingual things.

1194
00:56:27,600 --> 00:56:30,840
But that's not what our benchmarks have been designed for

1195
00:56:30,840 --> 00:56:32,200
unfortunately.

1196
00:56:32,200 --> 00:56:34,240
And so if you just look at the benchmarks,

1197
00:56:34,240 --> 00:56:36,520
like, you know, which are traditionally designed

1198
00:56:36,520 --> 00:56:41,360
for English Bloom, I don't think quite is at par

1199
00:56:41,360 --> 00:56:46,360
with OPD or GPDC and definitely not with Instruct.

1200
00:56:46,360 --> 00:56:51,280
And when I mention benchmark, you know,

1201
00:56:51,280 --> 00:56:54,160
there's that aspect of kind of applying

1202
00:56:54,160 --> 00:56:56,280
the traditional performance benchmarks,

1203
00:56:56,280 --> 00:57:00,640
you know, for LLMs to bloom and comparing their results

1204
00:57:00,640 --> 00:57:02,160
to the others.

1205
00:57:02,160 --> 00:57:04,200
But I'm also curious about.

1206
00:57:06,200 --> 00:57:09,320
How we characterize like qualitative differences

1207
00:57:09,320 --> 00:57:12,640
between the way Bloom responds

1208
00:57:12,640 --> 00:57:15,800
and the way GPT responds.

1209
00:57:15,800 --> 00:57:19,800
For example, you know, in terms of,

1210
00:57:21,000 --> 00:57:23,880
you know, like the kind of fairness considerations

1211
00:57:23,880 --> 00:57:27,960
or that kind of thing,

1212
00:57:27,960 --> 00:57:30,560
or, you know, are there qualitative differences

1213
00:57:30,560 --> 00:57:34,000
in the kinds of responses that you get

1214
00:57:34,000 --> 00:57:37,400
that aren't picked up by the traditional benchmarks

1215
00:57:37,400 --> 00:57:40,880
or are the traditional benchmarks like so, you know,

1216
00:57:40,880 --> 00:57:43,720
expansive at this point, we've kind of

1217
00:57:43,720 --> 00:57:46,520
characterized a lot of that stuff explicitly.

1218
00:57:46,520 --> 00:57:48,800
Yeah, again, I think the answer is somewhere in between.

1219
00:57:48,800 --> 00:57:50,920
So I don't know if people have thoroughly compared

1220
00:57:50,920 --> 00:57:53,480
the due to see like it was the level of toxicity

1221
00:57:53,480 --> 00:57:55,840
and people like that, you know, like,

1222
00:57:55,840 --> 00:57:57,080
I think when OPD came out,

1223
00:57:57,080 --> 00:57:59,640
they did a lot of disanalysis in their paper of like,

1224
00:57:59,640 --> 00:58:03,040
hey, how toxic is their model, how safe is their model?

1225
00:58:03,040 --> 00:58:05,320
And they realized that yeah, in some things,

1226
00:58:05,320 --> 00:58:10,080
they were worse off than some of the existing models.

1227
00:58:10,080 --> 00:58:14,320
But I think with loom, specifically,

1228
00:58:14,320 --> 00:58:16,360
I don't know off the top of my head,

1229
00:58:16,360 --> 00:58:18,960
how it sort of compared in terms of these,

1230
00:58:18,960 --> 00:58:22,320
these other sort of other aspects.

1231
00:58:22,320 --> 00:58:23,320
Okay.

1232
00:58:23,320 --> 00:58:25,720
Talk about the inverse scaling competition.

1233
00:58:25,720 --> 00:58:28,600
Yeah, so this was a pretty nice thing that gave out

1234
00:58:28,600 --> 00:58:30,640
and I think I suppose it's still going on

1235
00:58:30,640 --> 00:58:32,120
even though the submissions are down.

1236
00:58:32,120 --> 00:58:33,880
So I'm kind of hoping to see

1237
00:58:33,880 --> 00:58:36,480
what the actual effect of this was.

1238
00:58:36,480 --> 00:58:38,720
But this was sort of introduced sort of

1239
00:58:38,720 --> 00:58:40,400
in the middle of the year.

1240
00:58:40,400 --> 00:58:44,560
And the idea here is the thinking of things

1241
00:58:44,560 --> 00:58:47,320
like what sort of scaling laws was showing, right?

1242
00:58:47,320 --> 00:58:50,000
Like when you scale up your models,

1243
00:58:50,000 --> 00:58:52,520
performance goes up for everything.

1244
00:58:52,520 --> 00:58:53,920
And that's kind of exciting to see.

1245
00:58:53,920 --> 00:58:57,240
But it also tells us that okay,

1246
00:58:57,240 --> 00:58:59,840
there are many, many things that just the models

1247
00:58:59,840 --> 00:59:02,280
would just get better on as time goes by

1248
00:59:02,280 --> 00:59:05,720
because they'll get bigger, they'll have more data set.

1249
00:59:05,720 --> 00:59:08,280
The inverse scaling was this intuition to see,

1250
00:59:08,280 --> 00:59:11,320
okay, what are, can we characterize the phenomenas

1251
00:59:11,320 --> 00:59:15,880
that don't have the same trend, right?

1252
00:59:15,880 --> 00:59:20,040
So other aspects, you know, you created a data set

1253
00:59:20,040 --> 00:59:21,840
which is something everybody will agree

1254
00:59:21,840 --> 00:59:23,320
is a reasonable data set.

1255
00:59:23,320 --> 00:59:26,120
But when you give them to larger models,

1256
00:59:27,400 --> 00:59:29,080
they actually get worse.

1257
00:59:30,560 --> 00:59:33,400
And so this, this price and this competition

1258
00:59:33,400 --> 00:59:37,040
is an effort to identify what those,

1259
00:59:37,040 --> 00:59:39,720
what those tasks would be.

1260
00:59:39,720 --> 00:59:43,160
And sort of the better your inverse scaling is.

1261
00:59:43,160 --> 00:59:46,960
So the worse, the bigger models are on the data set

1262
00:59:46,960 --> 00:59:49,560
that you've contributed, the more likely you are

1263
00:59:49,560 --> 00:59:51,960
to build this competition.

1264
00:59:51,960 --> 00:59:53,400
And so yeah, they've had the submissions

1265
00:59:53,400 --> 00:59:55,600
and they're kind of evaluating them as opposed

1266
00:59:55,600 --> 00:59:57,920
and they haven't quite announced it.

1267
00:59:57,920 --> 01:00:01,680
But I think a lot of the stuff on, you know,

1268
01:00:01,680 --> 01:00:06,240
a lot of the interesting things could come out of this effort.

1269
01:00:06,240 --> 01:00:09,680
So one thing I could imagine is sort of deeper levels

1270
01:00:09,680 --> 01:00:14,680
of misinformation, whether the model is relying so much

1271
01:00:15,040 --> 01:00:18,040
on what it has seen in its training data.

1272
01:00:18,040 --> 01:00:19,440
Now let's not call it misinformation,

1273
01:00:19,440 --> 01:00:23,360
just not being able to update its information in some sense,

1274
01:00:23,360 --> 01:00:24,200
right?

1275
01:00:24,200 --> 01:00:26,800
So these large language models have memorized so much

1276
01:00:26,800 --> 01:00:30,440
about the pre-telling data that they kind of reject

1277
01:00:30,440 --> 01:00:32,800
evidence against that, right?

1278
01:00:32,800 --> 01:00:35,080
Maybe if they're smaller, there's less memorization

1279
01:00:35,080 --> 01:00:37,240
and more generalization.

1280
01:00:37,240 --> 01:00:39,000
But I think it could be pretty exciting to see

1281
01:00:39,000 --> 01:00:42,200
what are those things that actually get worse with scale.

1282
01:00:42,200 --> 01:00:44,960
I think it's quite an interesting question.

1283
01:00:44,960 --> 01:00:48,360
And next up, you have the Galactica,

1284
01:00:48,360 --> 01:00:49,920
can we call it a debacle?

1285
01:00:49,920 --> 01:00:50,920
Okay.

1286
01:00:50,920 --> 01:00:54,800
So Galactica is this L alone that meta released

1287
01:00:54,800 --> 01:00:59,800
that was tuned to generate scientific and research text.

1288
01:00:59,800 --> 01:01:02,440
And research text.

1289
01:01:02,440 --> 01:01:07,240
And was it even up for three days?

1290
01:01:07,240 --> 01:01:10,080
It got pulled down pretty quickly, right?

1291
01:01:10,080 --> 01:01:12,840
Yeah, I think maybe a little bit more than that,

1292
01:01:12,840 --> 01:01:14,880
but yeah, they're about, yeah.

1293
01:01:14,880 --> 01:01:19,880
And I think to me, it's a story about how not

1294
01:01:20,960 --> 01:01:24,200
anything in terms of what the Galactica team did itself,

1295
01:01:24,200 --> 01:01:26,400
like I think the model training it,

1296
01:01:26,400 --> 01:01:29,520
everything was the right thing to be doing.

1297
01:01:30,760 --> 01:01:34,200
The tricky thing was just how it was pitched

1298
01:01:34,200 --> 01:01:39,200
and how, you know, there was just not clear caveats

1299
01:01:40,760 --> 01:01:44,240
about what this model is capable of doing

1300
01:01:44,240 --> 01:01:46,800
and what it's not capable of doing,

1301
01:01:46,800 --> 01:01:49,600
that led to such a backlash, right?

1302
01:01:49,600 --> 01:01:54,600
So I think it was a language model training

1303
01:01:54,600 --> 01:01:57,680
like language model training on a lot of science papers.

1304
01:01:57,680 --> 01:01:59,320
So it's going to produce papers

1305
01:01:59,320 --> 01:02:01,680
that look like scientific text.

1306
01:02:01,680 --> 01:02:04,280
I think that was an expected thing,

1307
01:02:05,200 --> 01:02:08,800
but again, the backlash it got and stuff like that.

1308
01:02:08,800 --> 01:02:10,160
Essentially, it tells everyone,

1309
01:02:10,160 --> 01:02:15,160
and I hope the message is not to demo language models anymore,

1310
01:02:15,680 --> 01:02:17,800
but I think the message should be how to make sure

1311
01:02:17,800 --> 01:02:21,600
that you're not hyping things up more than they should be.

1312
01:02:21,600 --> 01:02:26,320
If you reflect on chat GPT, which came not very long after

1313
01:02:26,320 --> 01:02:31,320
Galactica and the launches of those respective products,

1314
01:02:34,440 --> 01:02:36,880
are there clear, is there a clear like,

1315
01:02:36,880 --> 01:02:38,280
do don't do list?

1316
01:02:38,280 --> 01:02:42,200
So I will say that chat GPT itself was also not,

1317
01:02:42,200 --> 01:02:46,080
you know, not completely without hype attached to it,

1318
01:02:46,080 --> 01:02:48,640
even sort of how they, right?

1319
01:02:48,640 --> 01:02:51,160
Somehow they managed it, it was a lot of hype.

1320
01:02:51,160 --> 01:02:52,440
Right, right, right.

1321
01:02:52,440 --> 01:02:55,280
I will say that they were fairly clear about the fact

1322
01:02:55,280 --> 01:02:57,000
that like, hey, don't trust, you know,

1323
01:02:57,000 --> 01:02:58,600
maybe they could have been clearer,

1324
01:02:58,600 --> 01:03:01,920
but like don't trust the factual stuff and things like that.

1325
01:03:01,920 --> 01:03:04,200
Like it's not a lookup engine.

1326
01:03:04,200 --> 01:03:06,720
I think they kind of could have done a lot more of that,

1327
01:03:06,720 --> 01:03:09,880
but you know, they at least had some caveats.

1328
01:03:09,880 --> 01:03:12,880
But more than that, they part of their RLHF stuff

1329
01:03:12,880 --> 01:03:16,520
was to make sure that the model is not producing,

1330
01:03:16,520 --> 01:03:19,840
at least obviously sexist, don't say.

1331
01:03:19,840 --> 01:03:23,000
Yeah, there was a lot, and maybe we're jumping into chat GPT,

1332
01:03:23,000 --> 01:03:25,200
which actually is the next thing we're gonna talk about,

1333
01:03:25,200 --> 01:03:27,880
but there was definitely a lot of,

1334
01:03:27,880 --> 01:03:31,600
especially early on, things that it just would not a pine on.

1335
01:03:31,600 --> 01:03:34,640
Like yeah, no, you're not gonna sucker me in and go in there.

1336
01:03:34,640 --> 01:03:35,760
Right, right, right, yeah.

1337
01:03:35,760 --> 01:03:38,680
And I think when you're building something that's public

1338
01:03:38,680 --> 01:03:42,400
facing that you're selling as a tool, as a product,

1339
01:03:42,400 --> 01:03:44,360
that is necessary, right?

1340
01:03:44,360 --> 01:03:48,800
Like I don't think you should be doing otherwise.

1341
01:03:48,800 --> 01:03:52,360
Galactica should not have been a public facing tool

1342
01:03:52,360 --> 01:03:55,800
for every scientist to start using to write their papers.

1343
01:03:55,800 --> 01:03:57,600
It should be a language model, right?

1344
01:03:57,600 --> 01:04:00,680
And then what the product is or what the tool is,

1345
01:04:00,680 --> 01:04:04,120
is a gap that other people can help fill in, right?

1346
01:04:04,120 --> 01:04:06,640
So that was sort of the missing piece

1347
01:04:06,640 --> 01:04:10,080
when I think about chat GPT versus galactica.

1348
01:04:10,080 --> 01:04:13,880
It's like, yeah, chat GPT has some of the caveats

1349
01:04:13,880 --> 01:04:16,160
about what it's doing.

1350
01:04:16,160 --> 01:04:17,520
Has some of the caveats about oh,

1351
01:04:17,520 --> 01:04:20,280
it's a language model, not a product to some degree.

1352
01:04:21,280 --> 01:04:24,400
And galactica was missing it, right?

1353
01:04:24,400 --> 01:04:26,040
So, yeah.

1354
01:04:26,040 --> 01:04:27,560
Now, we're there.

1355
01:04:29,400 --> 01:04:31,480
We were talking about open source.

1356
01:04:31,480 --> 01:04:34,440
Next up is kind of commercial developments.

1357
01:04:35,400 --> 01:04:37,240
Top of that list is chat GPT.

1358
01:04:37,240 --> 01:04:40,040
Right, yeah, let's talk about it.

1359
01:04:40,040 --> 01:04:42,200
It's, you said early on that,

1360
01:04:42,200 --> 01:04:45,920
hey, even without chat GPT, you know, this was a huge year

1361
01:04:45,920 --> 01:04:47,880
that's clearly not to say that chat GPT

1362
01:04:47,880 --> 01:04:50,960
wasn't a huge contribution to the year.

1363
01:04:50,960 --> 01:04:53,680
I mean, certainly one of the things

1364
01:04:53,680 --> 01:04:58,680
that I found most interesting was the degree to which

1365
01:05:01,000 --> 01:05:04,800
it kind of broke out of the MLA echo chamber

1366
01:05:04,800 --> 01:05:07,560
to, you know, just talking to random friends

1367
01:05:07,560 --> 01:05:10,080
and are like, hey, have you tried this chat GPT thing?

1368
01:05:10,080 --> 01:05:11,160
Well, yeah, I have.

1369
01:05:11,160 --> 01:05:15,480
Yeah, so that that's been the, I guess, the most surprising

1370
01:05:15,480 --> 01:05:17,680
and in some sense, the longest,

1371
01:05:17,680 --> 01:05:20,880
longest term impact for chat GPT is going to be the fact

1372
01:05:20,880 --> 01:05:23,440
that it made it commoditize.

1373
01:05:23,440 --> 01:05:27,040
It made it mainstream in a way that nothing before it had,

1374
01:05:27,040 --> 01:05:28,040
right?

1375
01:05:28,040 --> 01:05:29,120
And whether it deserved it or not,

1376
01:05:29,120 --> 01:05:32,320
what the actual innovations are and all of these things

1377
01:05:32,320 --> 01:05:34,160
is a different question, right?

1378
01:05:34,160 --> 01:05:37,080
Like it is clearly, even for research,

1379
01:05:37,080 --> 01:05:40,680
point of view, qualitatively better than GPT's tree.

1380
01:05:40,680 --> 01:05:44,400
Whether it met some threshold for becoming the big thing

1381
01:05:44,400 --> 01:05:47,160
that it did, it's sort of difficult to sort of

1382
01:05:47,160 --> 01:05:50,080
in hindsight try to evaluate that.

1383
01:05:50,080 --> 01:05:54,400
But I think it was, yeah, it is definitely something

1384
01:05:54,400 --> 01:05:59,400
that became mainstream and MVP is talking about it.

1385
01:05:59,840 --> 01:06:02,760
There is still a question in my mind whether that's a good

1386
01:06:02,760 --> 01:06:05,760
thing or not in the long run.

1387
01:06:05,760 --> 01:06:08,360
Because, you know, we can talk about some of the problems

1388
01:06:08,360 --> 01:06:11,080
with chat GPT, the biggest one being,

1389
01:06:11,080 --> 01:06:13,240
we know it's a language model.

1390
01:06:13,240 --> 01:06:15,280
Like, to some degree, we've been figuring out

1391
01:06:15,280 --> 01:06:17,400
last couple of years what these things are capable of

1392
01:06:17,400 --> 01:06:18,760
and what these things are not.

1393
01:06:18,760 --> 01:06:20,920
And I can say that in like a couple of minutes,

1394
01:06:20,920 --> 01:06:23,800
come up with tons of examples where it would fail.

1395
01:06:25,080 --> 01:06:27,720
That's not quite the case when you sort of start putting it

1396
01:06:27,720 --> 01:06:28,800
out in the public.

1397
01:06:28,800 --> 01:06:31,760
So, most people don't know what a language model is.

1398
01:06:31,760 --> 01:06:33,280
And I have played around with,

1399
01:06:33,280 --> 01:06:35,680
you know, I've got a bunch of my family to try it

1400
01:06:35,680 --> 01:06:36,720
and things like that.

1401
01:06:36,720 --> 01:06:39,080
And the biggest thing I've had,

1402
01:06:39,080 --> 01:06:41,680
the biggest difficulty I've had conveying to them

1403
01:06:41,680 --> 01:06:45,200
is the fact that it's not looking up anything

1404
01:06:45,200 --> 01:06:46,640
when you ask it something, right?

1405
01:06:46,640 --> 01:06:51,360
Like, that is a conceptual jump that is very, very

1406
01:06:51,360 --> 01:06:53,600
difficult for people to get over.

1407
01:06:54,480 --> 01:06:55,480
Yeah.

1408
01:06:55,480 --> 01:06:57,600
And so people, like, yeah, of course,

1409
01:06:57,600 --> 01:07:00,280
it should know about these things because it happened

1410
01:07:00,280 --> 01:07:02,240
yesterday and for such a big news items.

1411
01:07:02,240 --> 01:07:03,680
Like, why would it not know?

1412
01:07:03,680 --> 01:07:06,680
And now, like, no, actually, it doesn't know anything

1413
01:07:06,680 --> 01:07:07,800
beyond a certain time.

1414
01:07:07,800 --> 01:07:11,440
And even saying it knows anything from then

1415
01:07:11,440 --> 01:07:13,240
is a little bit difficult, right?

1416
01:07:14,400 --> 01:07:18,240
So I think the best analogy that I've, you know,

1417
01:07:18,240 --> 01:07:19,600
this applies to my research as well,

1418
01:07:19,600 --> 01:07:22,400
but the best analogy I've had in trying to explain

1419
01:07:22,400 --> 01:07:26,640
people what chat GPT does is do not think of it

1420
01:07:26,640 --> 01:07:28,920
as a stochastic pattern or anything like that.

1421
01:07:28,920 --> 01:07:31,200
But if you have to think in terms of animals,

1422
01:07:31,200 --> 01:07:33,280
think of it as like a chameleon,

1423
01:07:33,280 --> 01:07:36,560
like, it's trying to sort of fit in

1424
01:07:36,560 --> 01:07:39,360
to a bunch of humans, right?

1425
01:07:39,360 --> 01:07:43,240
And it's trying to just write things that will make it pass

1426
01:07:43,240 --> 01:07:46,680
as if it sort of knows all of those things, right?

1427
01:07:46,680 --> 01:07:51,680
I was in a Twitter exchange about I'd ask chat GPT

1428
01:07:52,520 --> 01:07:57,520
to explain RLHF and it came up with this acronym

1429
01:07:57,520 --> 01:08:02,160
it came up with this acronym that was like,

1430
01:08:04,680 --> 01:08:07,000
oh, I forget it and it was really funny.

1431
01:08:07,000 --> 01:08:10,680
It was like something leaderboard, you know,

1432
01:08:10,680 --> 01:08:13,080
humans, human something.

1433
01:08:13,080 --> 01:08:16,320
It was like, it was so far off.

1434
01:08:16,320 --> 01:08:18,640
Interestingly enough, I'd asked it about,

1435
01:08:18,640 --> 01:08:21,400
I'd had conversations, you know, interactions with it

1436
01:08:21,400 --> 01:08:24,080
about RLHF and then knew what it was.

1437
01:08:24,080 --> 01:08:26,760
Like to your point, it's about kind of where it sits

1438
01:08:26,760 --> 01:08:29,160
in the context of the prompt.

1439
01:08:29,160 --> 01:08:30,720
And I just kind of posted, you know,

1440
01:08:30,720 --> 01:08:35,120
is it trolling me or is it just trying to BS me?

1441
01:08:35,120 --> 01:08:38,720
And one of the responses that I got that, you know,

1442
01:08:38,720 --> 01:08:40,360
and reflecting on it was like really insightful,

1443
01:08:40,360 --> 01:08:43,040
like it's always trying to BS you.

1444
01:08:43,040 --> 01:08:45,720
That's all it's doing is trying to BS you

1445
01:08:45,720 --> 01:08:48,960
to produce some text that you will think is reasonable.

1446
01:08:48,960 --> 01:08:53,960
And, you know, to its credit, a lot of times it's right

1447
01:08:54,080 --> 01:08:56,040
but that's all that's trying to do.

1448
01:08:56,040 --> 01:08:57,040
Right, right, yeah.

1449
01:08:57,040 --> 01:09:00,080
And especially when it comes to factual stuff

1450
01:09:00,080 --> 01:09:04,400
or even like, you know, it is a very useful bullshitter

1451
01:09:04,400 --> 01:09:05,360
in some sense, right?

1452
01:09:05,360 --> 01:09:08,560
So because when it's right or when it's partially right,

1453
01:09:08,560 --> 01:09:12,560
that's still useful because, you know, it is what it is.

1454
01:09:12,560 --> 01:09:14,880
But that when you put that label,

1455
01:09:14,880 --> 01:09:17,240
like if they had sold it as like,

1456
01:09:17,240 --> 01:09:21,360
hey, we have built a really good bullshitter, right?

1457
01:09:21,360 --> 01:09:23,160
Like and it sold that as a product,

1458
01:09:23,160 --> 01:09:24,520
then people would know, okay,

1459
01:09:24,520 --> 01:09:26,000
not to use it for a bunch of tasks

1460
01:09:26,000 --> 01:09:28,920
that they're currently thinking of using it, right?

1461
01:09:28,920 --> 01:09:30,440
And so, yeah.

1462
01:09:30,440 --> 01:09:34,000
So that's the sort of divide that in messaging

1463
01:09:34,000 --> 01:09:38,080
that somehow researchers and NLP folks know,

1464
01:09:38,080 --> 01:09:39,400
oh, yeah, language model loss.

1465
01:09:39,400 --> 01:09:43,000
Obviously all that is doing is blah, blah, blah, right?

1466
01:09:43,000 --> 01:09:45,480
And yes, RLHF can help to some degree,

1467
01:09:45,480 --> 01:09:47,800
but clearly it's not going to be able

1468
01:09:47,800 --> 01:09:51,840
to do these bunch of other things all the time.

1469
01:09:51,840 --> 01:09:56,400
And that kind of thing is missing from general public,

1470
01:09:56,400 --> 01:09:59,400
but also how a lot of people are planning to use it

1471
01:09:59,400 --> 01:10:00,560
for example, right?

1472
01:10:00,560 --> 01:10:03,320
So I think that aspect is the part

1473
01:10:03,320 --> 01:10:06,800
that we need to think a little bit more about.

1474
01:10:06,800 --> 01:10:10,560
You've got Palm and Arpa Flan down.

1475
01:10:12,040 --> 01:10:15,040
You know, tell me a little bit more about your take there

1476
01:10:15,040 --> 01:10:16,680
because I hear of them, you know,

1477
01:10:16,680 --> 01:10:20,400
in a vague research context

1478
01:10:20,400 --> 01:10:22,840
because no one really has access to these,

1479
01:10:22,840 --> 01:10:27,760
but Google as much more so than, you know,

1480
01:10:27,760 --> 01:10:30,800
something that is huge from a commercial perspective.

1481
01:10:30,800 --> 01:10:33,680
Is this a prediction or is this a reflection?

1482
01:10:33,680 --> 01:10:37,720
Yeah, so no, I think of this as a palm

1483
01:10:37,720 --> 01:10:40,800
was a huge commercial development this year.

1484
01:10:40,800 --> 01:10:44,280
Like Google built this really, really large model.

1485
01:10:44,280 --> 01:10:47,000
Now there are, obviously they haven't released it, right?

1486
01:10:47,000 --> 01:10:48,680
So what's the ideal situation?

1487
01:10:48,680 --> 01:10:50,400
They completely release it open source.

1488
01:10:50,400 --> 01:10:51,680
Everybody gets access to it.

1489
01:10:51,680 --> 01:10:53,360
That's not gonna happen.

1490
01:10:53,360 --> 01:10:55,760
Another possible thing is they put an API on it

1491
01:10:55,760 --> 01:10:58,760
and charge people from a Google perspective

1492
01:10:58,760 --> 01:11:00,320
that doesn't make sense, right?

1493
01:11:00,320 --> 01:11:02,160
So it is something that they've built.

1494
01:11:02,160 --> 01:11:03,680
It's valuable internally.

1495
01:11:03,680 --> 01:11:07,040
I'm sure it sort of has, you know,

1496
01:11:07,040 --> 01:11:09,160
there are reasons not to make it public,

1497
01:11:09,160 --> 01:11:12,960
but it also has a lot of research insights

1498
01:11:12,960 --> 01:11:16,120
because nobody else has such a big language model

1499
01:11:16,120 --> 01:11:18,040
trained in a similar way.

1500
01:11:18,040 --> 01:11:20,640
And I guess I want to give them props

1501
01:11:20,640 --> 01:11:23,640
for at least publishing and evaluating

1502
01:11:23,640 --> 01:11:26,680
and doing things like that with palm.

1503
01:11:28,160 --> 01:11:31,720
Because it is doing, it is oversized that

1504
01:11:31,720 --> 01:11:35,840
we will not see for maybe another year or two

1505
01:11:35,840 --> 01:11:38,280
to be sort of publicly available,

1506
01:11:38,280 --> 01:11:41,000
but yet we get to hear about some insights,

1507
01:11:41,000 --> 01:11:43,680
what to expect, what are the emergence behaviors

1508
01:11:45,000 --> 01:11:46,560
coming out of those language models, right?

1509
01:11:46,560 --> 01:11:49,000
So yeah, it would be ideal if we could audit it

1510
01:11:49,000 --> 01:11:51,200
and all of us and I could contribute

1511
01:11:51,200 --> 01:11:52,840
in finding out what the problems are

1512
01:11:52,840 --> 01:11:54,960
and when it works and it doesn't work.

1513
01:11:54,960 --> 01:11:58,200
But given that, I think they did a good job.

1514
01:11:58,200 --> 01:12:01,000
Specifically, what I will say is that that size

1515
01:12:01,000 --> 01:12:04,000
has brought up a bunch of capabilities

1516
01:12:04,000 --> 01:12:05,800
like the whole chain of thought thing

1517
01:12:05,800 --> 01:12:08,240
that we talked about at the beginning.

1518
01:12:08,240 --> 01:12:11,440
That somehow became possible at that size,

1519
01:12:11,440 --> 01:12:14,880
but wasn't possible at other size, right?

1520
01:12:14,880 --> 01:12:19,880
So that's why that research is also all coming out of Google

1521
01:12:20,280 --> 01:12:25,600
because it applies mostly to, you know,

1522
01:12:25,600 --> 01:12:27,880
to the language, to the LLM's of that size.

1523
01:12:27,880 --> 01:12:30,880
Palm is 540 billion parameters?

1524
01:12:30,880 --> 01:12:33,280
Something, yeah, five, four years, yeah, yeah.

1525
01:12:33,280 --> 01:12:35,840
So I think, you know, they have access to it

1526
01:12:35,840 --> 01:12:38,360
and they can produce a string of papers

1527
01:12:38,360 --> 01:12:40,600
and yes, nobody else can write those papers,

1528
01:12:40,600 --> 01:12:44,480
but from a consumer of research as well as producer, right?

1529
01:12:44,480 --> 01:12:47,920
So from my consumer side, I love to read research

1530
01:12:47,920 --> 01:12:50,560
and I'm glad that they're writing those papers

1531
01:12:50,560 --> 01:12:53,040
because there's a lot of interesting stuff

1532
01:12:53,040 --> 01:12:54,160
in all of the papers.

1533
01:12:54,160 --> 01:12:55,840
So yeah, there's a whole string of papers

1534
01:12:55,840 --> 01:12:59,160
that I would recommend and I can point you to them offline.

1535
01:12:59,160 --> 01:13:01,400
But yeah, there's stuff that, you know,

1536
01:13:01,400 --> 01:13:04,840
we'll see happen publicly next year

1537
01:13:04,840 --> 01:13:06,160
or maybe another year after that

1538
01:13:06,160 --> 01:13:08,840
when those models become so much better.

1539
01:13:08,840 --> 01:13:13,840
So yeah, no, I think that's been kind of key.

1540
01:13:13,840 --> 01:13:18,200
So I'd say for that commercial, but not commercialized.

1541
01:13:18,200 --> 01:13:21,200
Yes, right, right, right, yeah, yeah, yeah.

1542
01:13:21,200 --> 01:13:23,040
I'll soon to be commercialized, I'm sure,

1543
01:13:23,040 --> 01:13:24,600
but maybe not for I could do.

1544
01:13:24,600 --> 01:13:26,600
Yeah, awesome.

1545
01:13:26,600 --> 01:13:31,600
Next up kind of the intersection between search and LLM's.

1546
01:13:31,600 --> 01:13:32,800
What do you see in there?

1547
01:13:32,800 --> 01:13:36,040
Yeah, so I think that's been kind of an interesting,

1548
01:13:36,040 --> 01:13:38,440
it's been a commercial development again,

1549
01:13:38,440 --> 01:13:40,040
questionable to some degree,

1550
01:13:40,040 --> 01:13:42,040
but because I don't think the research is,

1551
01:13:42,040 --> 01:13:44,000
and these models are quite up to stuff,

1552
01:13:44,000 --> 01:13:48,560
but this somewhat coincided with ChatGPT.

1553
01:13:49,760 --> 01:13:52,760
Well, ChatGPT certainly raised a ton of questions about,

1554
01:13:52,760 --> 01:13:54,360
hey, is this a Google killer?

1555
01:13:54,360 --> 01:13:56,000
Right, right, right, right.

1556
01:13:56,000 --> 01:13:57,240
Yes, exactly.

1557
01:13:57,240 --> 01:13:58,360
And along the same time,

1558
01:13:58,360 --> 01:14:02,120
there were at least three search engines that I know of.

1559
01:14:02,120 --> 01:14:05,920
There was publicity.ai that I don't think existed before.

1560
01:14:07,040 --> 01:14:09,880
What the product they came up with,

1561
01:14:09,880 --> 01:14:12,720
which is a search engine, which sort of gathers

1562
01:14:12,720 --> 01:14:15,520
all of the results from a typical search engine,

1563
01:14:15,520 --> 01:14:18,680
but then uses GPT-3 like models

1564
01:14:18,680 --> 01:14:21,600
to summarize the content of those links

1565
01:14:21,600 --> 01:14:25,520
and produces a paragraph that actually answers your query.

1566
01:14:25,520 --> 01:14:28,520
U.com is again a search engine that has been around for a while,

1567
01:14:28,520 --> 01:14:32,320
but they brought this whole Chat aspect to their search

1568
01:14:32,320 --> 01:14:34,920
where you're sort of chatting

1569
01:14:34,920 --> 01:14:37,000
and trying to come up with an answer.

1570
01:14:37,000 --> 01:14:40,240
And again, it's sort of not just showing you a bunch of links,

1571
01:14:40,240 --> 01:14:43,800
but composing information into text.

1572
01:14:43,800 --> 01:14:45,600
That's Richard Social's company

1573
01:14:45,600 --> 01:14:48,960
and we'll drop a link to my interview with him

1574
01:14:48,960 --> 01:14:50,440
in the show notes as well.

1575
01:14:50,440 --> 01:14:51,840
Okay, cool, yeah, yeah.

1576
01:14:51,840 --> 01:14:56,840
And Niva is another, it's a private search company.

1577
01:14:58,080 --> 01:15:00,760
It's a startup that also has an AI agent

1578
01:15:00,760 --> 01:15:02,960
that you can talk to with and things like that, right?

1579
01:15:02,960 --> 01:15:04,840
So I haven't played around with all of them.

1580
01:15:04,840 --> 01:15:07,280
I've played around with them a little bit.

1581
01:15:07,280 --> 01:15:11,200
And again, it's very easy to find problems

1582
01:15:11,200 --> 01:15:15,600
and sort of realize that, okay, these language models are,

1583
01:15:15,600 --> 01:15:17,080
you know, this interface is great

1584
01:15:17,080 --> 01:15:19,440
and it would be great to get the right paragraph

1585
01:15:19,440 --> 01:15:20,960
if it could get there,

1586
01:15:20,960 --> 01:15:23,440
but oftentimes it don't quite work

1587
01:15:23,440 --> 01:15:25,640
because of sort of fundamental issues with language models,

1588
01:15:25,640 --> 01:15:28,240
but I think from a commercial development,

1589
01:15:28,240 --> 01:15:31,600
I'm pretty excited about what search would look like in the future

1590
01:15:31,600 --> 01:15:36,600
and where language models would fit into that for much.

1591
01:15:36,600 --> 01:15:40,720
Yeah, one of my thought experiments

1592
01:15:40,720 --> 01:15:44,240
with this in the context of chat GPT,

1593
01:15:44,240 --> 01:15:45,760
I'm not that it was particularly deep,

1594
01:15:45,760 --> 01:15:50,760
but like there was this early meme,

1595
01:15:50,760 --> 01:15:52,440
you know, along the lines of,

1596
01:15:52,440 --> 01:15:55,640
hey, Google searches crap, now it's all ads,

1597
01:15:57,000 --> 01:16:00,400
chat GPT, you know, I love this interface,

1598
01:16:00,400 --> 01:16:02,080
you know, it's gonna kill Google.

1599
01:16:02,080 --> 01:16:07,160
And so I asked chat GPT to basically build response

1600
01:16:07,160 --> 01:16:08,760
with an ad in it.

1601
01:16:08,760 --> 01:16:11,000
It works, it can do it.

1602
01:16:11,000 --> 01:16:13,080
I wouldn't be so sure that your, you know,

1603
01:16:13,080 --> 01:16:15,840
LLM based search won't have any ads.

1604
01:16:15,840 --> 01:16:17,640
Right, right, yeah.

1605
01:16:17,640 --> 01:16:18,800
Yeah, no, I think, yeah,

1606
01:16:19,880 --> 01:16:23,360
where the ads would come in and how subtle the ads

1607
01:16:23,360 --> 01:16:25,240
will be once you throw in language model into it.

1608
01:16:25,240 --> 01:16:28,280
Yeah, that's kind of interesting question.

1609
01:16:28,280 --> 01:16:31,440
And I guess next up on your list of commercial developments

1610
01:16:31,440 --> 01:16:35,320
is what I might call the LLMing of all the things.

1611
01:16:35,320 --> 01:16:36,640
Yeah, so I think, you know,

1612
01:16:36,640 --> 01:16:40,920
it's been two years or so since GPT three came out.

1613
01:16:41,800 --> 01:16:45,320
And it's the question of like, okay,

1614
01:16:45,320 --> 01:16:48,200
where is the world changing products

1615
01:16:48,200 --> 01:16:50,640
that are using GPT when it came out,

1616
01:16:50,640 --> 01:16:52,600
hey, it was gonna change everything,

1617
01:16:52,600 --> 01:16:54,080
has it changed everything?

1618
01:16:54,080 --> 01:16:57,600
And I would say like for the most part, no.

1619
01:16:57,600 --> 01:17:01,160
The products that did seem to show some promise

1620
01:17:01,160 --> 01:17:03,800
and some of these are ones that will appear in the future

1621
01:17:03,800 --> 01:17:06,360
but have been kind of semi-announced.

1622
01:17:06,360 --> 01:17:08,440
It's the notion of writing assistantships, right?

1623
01:17:08,440 --> 01:17:12,600
So I think notion, notion AI is the one I think about

1624
01:17:12,600 --> 01:17:15,160
where a lot of people, it's a mainstream product,

1625
01:17:15,160 --> 01:17:16,880
anybody can use notion.

1626
01:17:16,880 --> 01:17:19,920
And notion has this GPT three thing built in

1627
01:17:19,920 --> 01:17:22,840
where it can write to do lists for you

1628
01:17:22,840 --> 01:17:23,880
and things like that.

1629
01:17:23,880 --> 01:17:28,520
So I think that is a pretty strong first version

1630
01:17:28,520 --> 01:17:31,400
of GPT three as a commercial product

1631
01:17:31,400 --> 01:17:34,120
that anybody can use that I'm quite excited about.

1632
01:17:34,120 --> 01:17:38,160
I feel like the timing there is very chat GPT influence.

1633
01:17:38,160 --> 01:17:40,320
Obviously, they've been working on it, you know,

1634
01:17:40,320 --> 01:17:43,520
they saw it when GPT three came out

1635
01:17:43,520 --> 01:17:47,520
but I think they made it available right after chat GPT

1636
01:17:47,520 --> 01:17:52,280
and a lot of these, you know,

1637
01:17:52,280 --> 01:17:54,600
Jasper's been around for a while

1638
01:17:54,600 --> 01:17:59,200
but there's a lot of new kind of writing assistant

1639
01:18:00,280 --> 01:18:02,080
types of things and it just does seem like

1640
01:18:02,080 --> 01:18:04,440
there's a step function increase in kind of energy

1641
01:18:04,440 --> 01:18:09,440
in the space of using LLAMs since chat GPT.

1642
01:18:11,080 --> 01:18:13,160
Even though they're all based on GPT three

1643
01:18:13,160 --> 01:18:15,680
which has been around for two years, right?

1644
01:18:15,680 --> 01:18:19,720
Yeah, so I don't know exactly why that thing seemed

1645
01:18:19,720 --> 01:18:20,600
to align well, right?

1646
01:18:20,600 --> 01:18:22,680
So it's like, yeah, GPT three was announced

1647
01:18:22,680 --> 01:18:24,880
but it was a while before the API was rolled down

1648
01:18:24,880 --> 01:18:27,640
to everybody and you know, and maybe after that

1649
01:18:27,640 --> 01:18:29,920
it takes a while to make the business case for these things.

1650
01:18:29,920 --> 01:18:33,440
So yeah, maybe it's just timing of why it worked

1651
01:18:33,440 --> 01:18:36,200
or there were people like already kind of working on it

1652
01:18:36,200 --> 01:18:38,400
in a sort of on the side and they were like,

1653
01:18:38,400 --> 01:18:41,640
hey, now we gotta sort of write this wave

1654
01:18:41,640 --> 01:18:43,200
and sort of introduce things, right?

1655
01:18:43,200 --> 01:18:45,160
So I don't know exactly what that looks like

1656
01:18:45,160 --> 01:18:48,400
but yeah, no, I think the fact that it aligns

1657
01:18:48,400 --> 01:18:51,160
also gets a lot more excitement and people know,

1658
01:18:51,160 --> 01:18:54,360
okay, chat GPT is something I've played around with.

1659
01:18:54,360 --> 01:18:57,280
This is now a chat GPT that's working on something

1660
01:18:57,280 --> 01:18:59,720
that I do and there's a lot of value in that.

1661
01:18:59,720 --> 01:19:04,720
Am I detecting an underlying pessimism maybe

1662
01:19:06,000 --> 01:19:10,920
about like, you know, kind of, you know,

1663
01:19:10,920 --> 01:19:13,000
where's the flying car that I was promised

1664
01:19:13,000 --> 01:19:15,200
all I have is this GPT three thing?

1665
01:19:15,200 --> 01:19:16,640
Well, it's not so much the pessimism

1666
01:19:16,640 --> 01:19:20,120
because when I saw GPT three, it became evident to me

1667
01:19:20,120 --> 01:19:22,160
that this is a great language model

1668
01:19:22,160 --> 01:19:26,080
but it's not clear as it is how it can be made

1669
01:19:26,080 --> 01:19:27,960
into a product, right?

1670
01:19:27,960 --> 01:19:29,840
But it still came with a lot of hype

1671
01:19:29,840 --> 01:19:31,840
and yeah, it can generate a bunch of things

1672
01:19:31,840 --> 01:19:34,920
but we quite haven't quite seen

1673
01:19:34,920 --> 01:19:36,840
what the product version of those look like.

1674
01:19:36,840 --> 01:19:40,240
I think the language models are extremely powerful

1675
01:19:40,240 --> 01:19:41,920
not just as language models

1676
01:19:41,920 --> 01:19:45,120
but they can be converted into products.

1677
01:19:45,120 --> 01:19:47,920
I don't quite feel like we are at a stage

1678
01:19:47,920 --> 01:19:50,000
where it's just going to be through prompting

1679
01:19:50,000 --> 01:19:53,080
and, you know, let's just tweak it a little bit.

1680
01:19:53,080 --> 01:19:54,680
I think there are a bunch of products

1681
01:19:54,680 --> 01:19:57,120
that will come out of just by doing that

1682
01:19:57,120 --> 01:19:58,720
but there's a whole slew of product

1683
01:19:58,720 --> 01:20:01,920
where the language models need to know a lot more

1684
01:20:01,920 --> 01:20:04,920
about the context where they're gonna be

1685
01:20:05,720 --> 01:20:09,920
to be able to be effective, yeah, effective tools.

1686
01:20:10,920 --> 01:20:14,120
And you mentioned Microsoft, what do you have in mind there?

1687
01:20:14,120 --> 01:20:16,520
Yeah, this was sort of a news that came out recently

1688
01:20:16,520 --> 01:20:20,280
where they're trying to have a bigger stake in OpenAI

1689
01:20:20,280 --> 01:20:23,880
but also just generally thinking of having OpenAI

1690
01:20:23,880 --> 01:20:28,120
like tools available in Word, available in PowerPoint

1691
01:20:28,120 --> 01:20:30,520
and all of these things, they don't have it yet

1692
01:20:30,520 --> 01:20:34,000
but I think those kind of things are just some of the comments.

1693
01:20:34,000 --> 01:20:38,120
Do you think a chat GPT-based Bing is a Google killer?

1694
01:20:38,120 --> 01:20:41,360
Oh, I don't think with that branding

1695
01:20:41,360 --> 01:20:46,360
they would have to call it something else or at this point, yeah.

1696
01:20:46,680 --> 01:20:49,320
I mean, that seemed to be the suggestion, right?

1697
01:20:49,320 --> 01:20:54,520
Chat GPT comes out, they're gonna take a big stake

1698
01:20:54,520 --> 01:20:58,800
and it was mentioned, if not in the official announcement,

1699
01:20:58,800 --> 01:21:02,520
it seemed to be the conjecture that it was gonna be

1700
01:21:02,520 --> 01:21:07,520
some tie up with Bing explicitly to target search, right?

1701
01:21:07,520 --> 01:21:11,640
I think there needs to be a lot more fundamental work

1702
01:21:11,640 --> 01:21:13,480
and we can talk about this in the future predictions

1703
01:21:13,480 --> 01:21:15,640
but there needs to be a lot more fundamental work

1704
01:21:15,640 --> 01:21:20,080
before we sort of are able to kill search

1705
01:21:20,080 --> 01:21:21,760
just by putting a language model, right?

1706
01:21:21,760 --> 01:21:25,400
Like I think that gap is not as simple as replacing something

1707
01:21:25,400 --> 01:21:28,040
or just augmenting existing search.

1708
01:21:28,040 --> 01:21:30,760
I think you would have to think about what kind of things

1709
01:21:30,760 --> 01:21:33,000
can language models actually do

1710
01:21:33,000 --> 01:21:36,520
and you still want to rely on sources and things like that

1711
01:21:36,520 --> 01:21:40,880
but yeah, so I think it's going to happen at some point

1712
01:21:40,880 --> 01:21:44,240
but it's going to be like search as a,

1713
01:21:44,240 --> 01:21:46,920
it won't be replacing search because it'll be a different thing,

1714
01:21:46,920 --> 01:21:49,320
right? Like it'll be, it's not gonna be search

1715
01:21:49,320 --> 01:21:50,880
because not in the way we think about search.

1716
01:21:50,880 --> 01:21:52,560
It literally means, yeah.

1717
01:21:52,560 --> 01:21:53,560
Exactly.

1718
01:21:53,560 --> 01:21:56,240
It'll be question-answering or it'll be something else, right?

1719
01:21:56,240 --> 01:21:58,960
Like it'll be a helper or whatever,

1720
01:21:58,960 --> 01:22:01,640
but searches may not be anything.

1721
01:22:01,640 --> 01:22:04,880
Well, one quick thing before we jump into predictions,

1722
01:22:04,880 --> 01:22:09,560
you kind of reflected on your top use case for the year

1723
01:22:09,560 --> 01:22:12,920
and that was code pilot.

1724
01:22:12,920 --> 01:22:15,840
Tell me a little bit more about how you thinking about that.

1725
01:22:15,840 --> 01:22:18,480
I think co-pilot came out probably not exactly

1726
01:22:18,480 --> 01:22:21,720
in this calendar, but I feel like it got a lot more

1727
01:22:21,720 --> 01:22:25,960
adoption this year and started becoming part of the tools

1728
01:22:25,960 --> 01:22:28,680
where people are coding and firstly,

1729
01:22:28,680 --> 01:22:30,080
I started using co-pilot this year,

1730
01:22:30,080 --> 01:22:33,240
so I'm gonna put it in top use case this year.

1731
01:22:33,240 --> 01:22:36,520
And I will say before notion.ai,

1732
01:22:36,520 --> 01:22:39,280
co-pilot was probably the only use

1733
01:22:39,280 --> 01:22:43,160
of large language models that I saw anywhere.

1734
01:22:43,160 --> 01:22:45,840
So from that point of view, it was interesting

1735
01:22:45,840 --> 01:22:47,800
that you could use to came out and there's nothing,

1736
01:22:47,800 --> 01:22:50,600
nothing that can fill co-pilot.

1737
01:22:50,600 --> 01:22:52,480
But from a use case point of view,

1738
01:22:52,480 --> 01:22:54,680
it has been incredibly useful, right?

1739
01:22:54,680 --> 01:22:58,120
So I've been able to, you know,

1740
01:22:58,120 --> 01:23:02,200
do things that has made me a lot more effective as a coder,

1741
01:23:02,200 --> 01:23:04,240
not that I code much, but when I do,

1742
01:23:04,240 --> 01:23:08,520
I want to do a lot and co-pilot has let me sort of do that.

1743
01:23:08,520 --> 01:23:13,040
And that's been amazing, I feel the right combination

1744
01:23:13,040 --> 01:23:15,960
of having a nice user interface,

1745
01:23:15,960 --> 01:23:19,480
having the right data that is trained on

1746
01:23:19,480 --> 01:23:22,280
to be able to sort of really help people

1747
01:23:22,280 --> 01:23:23,480
in what they want to do.

1748
01:23:23,480 --> 01:23:25,400
Now of course, co-pilot has issues,

1749
01:23:25,400 --> 01:23:30,400
it's producing, you know, code that can be dangerous,

1750
01:23:30,400 --> 01:23:33,080
that can be buggy, and of course,

1751
01:23:33,080 --> 01:23:37,400
there are the questions of copyright and plagiarism exactly.

1752
01:23:37,400 --> 01:23:41,400
So I feel like I hope those things will get resolved,

1753
01:23:41,400 --> 01:23:44,480
but those are again, when you start using a language model,

1754
01:23:44,480 --> 01:23:47,920
these are the issues that you have to solve.

1755
01:23:47,920 --> 01:23:50,840
And then I'm glad that co-pilot is bringing all of these things

1756
01:23:50,840 --> 01:23:54,320
into the discussion by having, by it being out there.

1757
01:23:55,320 --> 01:23:57,120
Yeah, I've had the same experience with it.

1758
01:23:57,120 --> 01:24:02,120
I think I've shared this on social or in the podcast

1759
01:24:03,080 --> 01:24:04,520
in a conversation.

1760
01:24:04,520 --> 01:24:09,440
I saw all the co-pilot demos played around with it

1761
01:24:09,440 --> 01:24:11,640
with kind of the toy problem things,

1762
01:24:11,640 --> 01:24:14,920
but I don't do a lot of coding necessarily,

1763
01:24:14,920 --> 01:24:17,440
but I do tend to binge on coding everyone's in law.

1764
01:24:17,440 --> 01:24:19,920
Like, and usually like that end of year holiday thing,

1765
01:24:19,920 --> 01:24:22,920
I'll have some projects, and I did that this year

1766
01:24:22,920 --> 01:24:25,600
and used co-pilot, it was amazing.

1767
01:24:25,600 --> 01:24:29,680
Like the productivity, you can, it helps,

1768
01:24:29,680 --> 01:24:33,600
the productivity helps create for you, attacking,

1769
01:24:35,160 --> 01:24:37,280
a new problem with new tools,

1770
01:24:37,280 --> 01:24:40,120
without the context switching of going to Google

1771
01:24:40,120 --> 01:24:42,720
and Stack Overflow, like it's incredible.

1772
01:24:42,720 --> 01:24:44,600
I'm a total believer.

1773
01:24:44,600 --> 01:24:47,880
Yeah, and I think that exactly is the kind of thing

1774
01:24:47,880 --> 01:24:50,560
I expect language models to be useful for.

1775
01:24:50,560 --> 01:24:51,920
They are not going to, and you know,

1776
01:24:51,920 --> 01:24:53,640
which at GPD going back a little bit,

1777
01:24:53,640 --> 01:24:56,120
people are talking about, hey, if people are going to lose jobs

1778
01:24:56,120 --> 01:24:57,920
and it's going to change everything,

1779
01:24:57,920 --> 01:25:01,200
and you know, we'll replace XYZ with Giat GPD.

1780
01:25:01,200 --> 01:25:04,760
And I don't quite see that happening,

1781
01:25:04,760 --> 01:25:07,760
but I do expect a lot of people in many different areas

1782
01:25:07,760 --> 01:25:11,440
becoming a lot more productive because of Giat GPD.

1783
01:25:11,440 --> 01:25:13,120
And co-pilot is, you know,

1784
01:25:13,120 --> 01:25:15,000
is an example of how language models

1785
01:25:15,000 --> 01:25:19,120
can make you a lot more productive without replacing,

1786
01:25:19,120 --> 01:25:21,320
I don't think it's replacing specific programmers,

1787
01:25:21,320 --> 01:25:23,680
it's just making, allowing them to do a lot more.

1788
01:25:24,680 --> 01:25:27,440
And that I think is the best use of technology.

1789
01:25:27,440 --> 01:25:28,280
Awesome, awesome.

1790
01:25:28,280 --> 01:25:29,760
Well, let's jump into predictions.

1791
01:25:31,480 --> 01:25:35,520
What are you most excited about kind of looking

1792
01:25:35,520 --> 01:25:37,080
into your crystal wall?

1793
01:25:37,080 --> 01:25:40,840
So I think the Giat GPD is the one that sort of,

1794
01:25:40,840 --> 01:25:42,800
everybody knew language models,

1795
01:25:42,800 --> 01:25:45,520
they just trained on data and making predictions.

1796
01:25:45,520 --> 01:25:49,680
What Giat GPD really did was remind everyone, like,

1797
01:25:49,680 --> 01:25:52,600
okay, even if the language modeling part is

1798
01:25:52,600 --> 01:25:54,080
quote unquote, salt, right?

1799
01:25:54,080 --> 01:25:56,920
Even if you get a really, really large language model,

1800
01:25:56,920 --> 01:26:00,320
that doesn't mean you're done, right?

1801
01:26:00,320 --> 01:26:03,440
And I think one of the biggest aspects of that was

1802
01:26:04,400 --> 01:26:08,760
making sure that what you're generating is not just BS,

1803
01:26:08,760 --> 01:26:12,400
it's somehow valid, somehow the truth,

1804
01:26:12,400 --> 01:26:16,400
somehow something that you can cite and rely on, right?

1805
01:26:16,400 --> 01:26:20,080
They definitely signed a light on how challenging that is.

1806
01:26:20,080 --> 01:26:20,920
Right.

1807
01:26:20,920 --> 01:26:21,760
Exactly.

1808
01:26:21,760 --> 01:26:23,760
So I don't think this is going to be a prediction

1809
01:26:23,760 --> 01:26:27,600
necessarily for 2023, maybe 2023 is when we'll start

1810
01:26:27,600 --> 01:26:29,680
seeing the first attempts at this,

1811
01:26:29,680 --> 01:26:33,440
but being able to generate text that's,

1812
01:26:33,440 --> 01:26:35,040
that's not have misinformation,

1813
01:26:35,040 --> 01:26:38,320
that differentiates factual from, you know,

1814
01:26:38,320 --> 01:26:43,240
creative hallucinations that is able to cite its sources

1815
01:26:43,240 --> 01:26:44,600
and sort of point to like,

1816
01:26:44,600 --> 01:26:48,280
hello, this is the piece of paragraph that I'm based on,

1817
01:26:48,280 --> 01:26:50,280
which I'm generating a piece of text.

1818
01:26:50,280 --> 01:26:54,600
I think those things are needed and it's probably going

1819
01:26:54,600 --> 01:26:58,400
to be the next aspect of language models

1820
01:26:58,400 --> 01:27:01,680
that's going to be a big topic of research.

1821
01:27:01,680 --> 01:27:04,240
Do you have a sense for where,

1822
01:27:04,240 --> 01:27:09,240
how we get there is it kind of applying the same tools,

1823
01:27:09,240 --> 01:27:13,640
RLHF, for example, attacking this specific problem,

1824
01:27:13,640 --> 01:27:16,280
or do you think is, you know,

1825
01:27:16,280 --> 01:27:18,440
we don't have the tools and it's going to need to be

1826
01:27:18,440 --> 01:27:22,320
kind of new invention that gets us there.

1827
01:27:22,320 --> 01:27:24,920
I think it's going to have to be new inventions

1828
01:27:24,920 --> 01:27:27,520
and I want to sort of think of it as not just,

1829
01:27:27,520 --> 01:27:31,320
you know, how do we attribute it to specific pieces of text,

1830
01:27:31,320 --> 01:27:34,720
but I kind of think of it as like being able to use

1831
01:27:34,720 --> 01:27:38,320
other tools, being able to use other things

1832
01:27:38,320 --> 01:27:40,720
available to the language model

1833
01:27:40,720 --> 01:27:42,400
when it's being trained as well, right?

1834
01:27:42,400 --> 01:27:47,200
So it should not rely on memorizing facts to any degree.

1835
01:27:47,200 --> 01:27:51,160
It should just rely on using existing tools,

1836
01:27:51,160 --> 01:27:54,520
including search, including maybe calculations,

1837
01:27:54,520 --> 01:27:59,160
maybe even Python, interpreter, whatever else it needs to do,

1838
01:27:59,160 --> 01:28:02,160
but still be able to do the language modeling task, right?

1839
01:28:02,160 --> 01:28:04,800
So I think there is some combination of being able

1840
01:28:04,800 --> 01:28:08,360
to refer to external stuff and still do language modeling

1841
01:28:08,360 --> 01:28:10,840
that we quite haven't quite extracted

1842
01:28:10,840 --> 01:28:15,840
and that would be something that I think will come into picture.

1843
01:28:16,000 --> 01:28:19,840
I'll give you an example of how sort of some people

1844
01:28:19,840 --> 01:28:20,840
have been thinking about it.

1845
01:28:20,840 --> 01:28:24,880
There's this whole idea of retrieval-based language modeling

1846
01:28:24,880 --> 01:28:27,440
where you're still generating the text

1847
01:28:27,440 --> 01:28:30,480
token by token, but you're always retrieving

1848
01:28:30,480 --> 01:28:33,080
some set of documents and you're conditioning on them

1849
01:28:33,080 --> 01:28:35,440
when you're generating each token.

1850
01:28:35,440 --> 01:28:40,000
That's sort of one step towards what I'm talking about,

1851
01:28:40,000 --> 01:28:43,440
where at least you're trying to look at retrieved documents

1852
01:28:43,440 --> 01:28:46,400
when you're generating, but that doesn't guarantee

1853
01:28:46,400 --> 01:28:49,600
what you're generating is actually based on.

1854
01:28:49,600 --> 01:28:54,600
So you just spoke earlier about the decomposed reasoning.

1855
01:28:55,080 --> 01:29:00,080
Is this prediction that those ideas become more real

1856
01:29:01,080 --> 01:29:03,680
in some way in 2324?

1857
01:29:03,680 --> 01:29:10,680
Is it that what we're doing with a trained model

1858
01:29:10,280 --> 01:29:13,160
to kind of get decomposed reasoning,

1859
01:29:13,160 --> 01:29:16,640
we're gonna push even deeper into the fundamental creation

1860
01:29:16,640 --> 01:29:19,480
of the model like at train time and other things?

1861
01:29:19,480 --> 01:29:21,800
Yeah, so more of the latter, right?

1862
01:29:21,800 --> 01:29:23,720
So right now we are expecting the model

1863
01:29:23,720 --> 01:29:25,600
to be able to do decouples reasoning,

1864
01:29:25,600 --> 01:29:28,760
but we only do it at test time in some sense, right?

1865
01:29:28,760 --> 01:29:31,080
Let's actually try to start thinking,

1866
01:29:31,080 --> 01:29:33,000
putting that stuff during training, right?

1867
01:29:33,000 --> 01:29:36,160
So like, again, I don't want to make this analogy too much,

1868
01:29:36,160 --> 01:29:37,760
but when you think about,

1869
01:29:37,760 --> 01:29:41,120
when you're training a human on how to do things,

1870
01:29:41,120 --> 01:29:44,480
you don't just give it pairs of input and output,

1871
01:29:44,480 --> 01:29:47,560
you give it a little bit more of a decomposition

1872
01:29:47,560 --> 01:29:50,720
and then based on that, they're able to do what they do.

1873
01:29:50,720 --> 01:29:53,320
If you want them to use the Python interpreter,

1874
01:29:53,320 --> 01:29:56,720
you don't just expect them to finish it on their own,

1875
01:29:56,720 --> 01:29:58,960
they can use the interpreter when needed, get a thing, right?

1876
01:29:58,960 --> 01:30:01,600
So I just think of language models as,

1877
01:30:01,600 --> 01:30:03,920
yeah, maybe they're still doing the language modeling task,

1878
01:30:03,920 --> 01:30:06,480
but they have access to a bunch of other tools.

1879
01:30:07,480 --> 01:30:09,800
And maybe this is more far-fetched than 2023,

1880
01:30:09,800 --> 01:30:11,720
but I think in the long run,

1881
01:30:11,720 --> 01:30:15,040
you want a system that's able to do those things.

1882
01:30:15,040 --> 01:30:17,560
You got your next prediction is around diffusion models.

1883
01:30:17,560 --> 01:30:20,520
It's kind of surprising that that term hasn't come up yet so far.

1884
01:30:20,520 --> 01:30:22,960
Yeah, I guess it is surprising,

1885
01:30:22,960 --> 01:30:24,520
but also in NLP in general,

1886
01:30:24,520 --> 01:30:27,080
I feel like we are barely scratching the surface

1887
01:30:27,080 --> 01:30:30,480
of what diffusion models can do.

1888
01:30:30,480 --> 01:30:32,160
So, yeah, I think,

1889
01:30:32,160 --> 01:30:33,960
clearly in the image generation space,

1890
01:30:33,960 --> 01:30:37,080
we've seen a lot of progress with diffusion models

1891
01:30:37,080 --> 01:30:40,760
and we've seen some in NLP, but not enough.

1892
01:30:40,760 --> 01:30:44,160
I guess what I find attractive about diffusion model

1893
01:30:44,160 --> 01:30:47,360
is that it's trying to generate more than just

1894
01:30:47,360 --> 01:30:49,360
a single thing at a point point, right?

1895
01:30:49,360 --> 01:30:52,480
So when diffusion models are applied to text,

1896
01:30:52,480 --> 01:30:54,400
the way it would look like is not just

1897
01:30:54,400 --> 01:30:56,440
producing one token at a time,

1898
01:30:56,440 --> 01:30:58,960
it will try to produce a whole sentence

1899
01:30:58,960 --> 01:31:02,320
or whatever we decide is the right guarantee.

1900
01:31:02,320 --> 01:31:05,840
And that idea of a model that is trained

1901
01:31:05,840 --> 01:31:08,320
not to do one token at a time,

1902
01:31:08,320 --> 01:31:11,800
but to do something bigger really appeals to me

1903
01:31:11,800 --> 01:31:13,720
because I feel like a lot of the issues

1904
01:31:13,720 --> 01:31:16,200
we talk about with language models

1905
01:31:16,200 --> 01:31:18,320
fundamentally come from the fact that

1906
01:31:18,320 --> 01:31:20,800
it's trained to do one token at a time

1907
01:31:20,800 --> 01:31:24,280
and sort of, and that's kind of the loss, right?

1908
01:31:24,280 --> 01:31:29,280
So if we can have the model be trained to generate more

1909
01:31:29,360 --> 01:31:31,120
and then give it a loss,

1910
01:31:31,120 --> 01:31:33,080
I think that's fundamentally interesting

1911
01:31:33,080 --> 01:31:35,800
in the previous model, so provide one way of doing it.

1912
01:31:35,800 --> 01:31:40,800
Do you, would you kind of visualize this

1913
01:31:40,800 --> 01:31:45,880
as a model like in a first iteration,

1914
01:31:45,880 --> 01:31:48,360
spitting out bullshit and then successively

1915
01:31:48,360 --> 01:31:50,800
like iterating towards truth?

1916
01:31:50,800 --> 01:31:53,560
I guess that one way that this could play out.

1917
01:31:53,560 --> 01:31:57,920
Yes, well, I mean, probably not,

1918
01:31:57,920 --> 01:32:01,280
probably it's gonna be somewhere in the latent space.

1919
01:32:01,280 --> 01:32:04,560
But I think the way I think about it is

1920
01:32:04,560 --> 01:32:08,120
like if we were doing this token by token thing

1921
01:32:08,120 --> 01:32:10,520
for images, it just wouldn't make sense, right?

1922
01:32:10,520 --> 01:32:14,120
Like, pretty certainly would produce the images

1923
01:32:14,120 --> 01:32:18,040
that we see coming out of stable diffusion and then yeah.

1924
01:32:18,040 --> 01:32:20,040
Or even what it's going to learn

1925
01:32:20,040 --> 01:32:21,400
is going to be something different,

1926
01:32:21,400 --> 01:32:24,080
what it's going to learn is given the image

1927
01:32:24,080 --> 01:32:25,520
that I have seen so far,

1928
01:32:25,520 --> 01:32:29,040
let me predict the next pixel or the next piece, right?

1929
01:32:29,040 --> 01:32:32,360
That somehow feels like a fundamentally different task

1930
01:32:32,360 --> 01:32:34,960
than being able to generate an image fully, right?

1931
01:32:35,960 --> 01:32:40,200
And so I feel like thinking about the same idea

1932
01:32:40,200 --> 01:32:41,880
for text just kind of makes sense,

1933
01:32:41,880 --> 01:32:44,880
like you write the summary in one shot

1934
01:32:44,880 --> 01:32:48,040
and realize how wrong it is,

1935
01:32:48,040 --> 01:32:50,440
feels like there's something fundamentally different

1936
01:32:50,440 --> 01:32:53,120
than hey, you got a bunch of tokens correct,

1937
01:32:53,120 --> 01:32:54,320
but you also got a bunch.

1938
01:32:54,320 --> 01:32:56,320
And in some sense, there are some analogies

1939
01:32:56,320 --> 01:32:59,920
to our LHF and using PPO for training,

1940
01:32:59,920 --> 01:33:04,160
for example, where you try to make sure it's fluent

1941
01:33:04,160 --> 01:33:05,200
and things like that.

1942
01:33:05,200 --> 01:33:08,240
These are all losses designed on not just token

1943
01:33:08,240 --> 01:33:10,800
by token basis, but something that's longer.

1944
01:33:10,800 --> 01:33:13,440
And so we've known how useful they've been.

1945
01:33:13,440 --> 01:33:15,480
So I feel like there may be something

1946
01:33:15,480 --> 01:33:18,160
in taking that idea and applying it to appreciating

1947
01:33:18,160 --> 01:33:21,080
something that's interesting, interesting.

1948
01:33:21,080 --> 01:33:23,280
I expect a lot of people will be wanting

1949
01:33:23,280 --> 01:33:25,120
to figure out how to do that.

1950
01:33:25,120 --> 01:33:26,200
Great.

1951
01:33:26,200 --> 01:33:29,240
And online updates to models.

1952
01:33:29,240 --> 01:33:32,080
Yeah, so I think one of the problems with language models,

1953
01:33:32,080 --> 01:33:34,280
so let's keep aside the grand vision

1954
01:33:34,280 --> 01:33:36,880
of how language models will use search

1955
01:33:36,880 --> 01:33:38,120
and all of these other things.

1956
01:33:38,120 --> 01:33:40,320
But one of the fundamental problems with language models

1957
01:33:40,320 --> 01:33:43,640
is that the word changes, but they don't.

1958
01:33:44,480 --> 01:33:47,120
And this seems to be a fundamental sort of issue

1959
01:33:47,120 --> 01:33:49,480
with language models, right?

1960
01:33:49,480 --> 01:33:53,680
So I think thinking about how we can update

1961
01:33:53,680 --> 01:33:59,160
language models every month or every week or every day,

1962
01:33:59,160 --> 01:34:03,320
I think is an interesting problem to be thinking about

1963
01:34:03,320 --> 01:34:04,880
and becomes increasingly relevant,

1964
01:34:04,880 --> 01:34:08,240
where Bert doesn't know anything about COVID,

1965
01:34:08,240 --> 01:34:10,800
so it's not useful for a bunch of applications,

1966
01:34:10,800 --> 01:34:12,960
even though otherwise fundamentally,

1967
01:34:12,960 --> 01:34:15,160
there's nothing wrong with it, right?

1968
01:34:15,160 --> 01:34:19,640
That kind of stuff is just not fun.

1969
01:34:19,640 --> 01:34:22,840
And I think there would be research on trying to fix that.

1970
01:34:22,840 --> 01:34:27,720
What's the current, not necessarily state of the art,

1971
01:34:27,720 --> 01:34:32,280
but the current approach for doing this at the scale

1972
01:34:32,280 --> 01:34:35,320
of a GPT-3, is it collect more data

1973
01:34:35,320 --> 01:34:36,440
and retrain from scratch?

1974
01:34:36,440 --> 01:34:42,400
Or how did they approximate or approach

1975
01:34:42,400 --> 01:34:46,320
some kind of incremental training ability, if at all?

1976
01:34:46,320 --> 01:34:51,560
Yeah, so there hasn't been that much work on that front,

1977
01:34:51,560 --> 01:34:56,320
I would say, this is something that needs a lot more attention.

1978
01:34:56,320 --> 01:35:00,160
Yeah, but I think there'd be parameter efficient training

1979
01:35:00,160 --> 01:35:05,160
on how can we slightly improve the change the model,

1980
01:35:08,800 --> 01:35:10,120
but not completely change it.

1981
01:35:10,120 --> 01:35:12,200
Find the set of parameters that we should update,

1982
01:35:12,200 --> 01:35:14,840
so that it's not updating the whole parameters,

1983
01:35:14,840 --> 01:35:16,520
but updating a little bit of it,

1984
01:35:16,520 --> 01:35:19,000
things like that I feel are around,

1985
01:35:19,000 --> 01:35:20,920
but it needs a lot more work.

1986
01:35:20,920 --> 01:35:23,920
You kind of, one way to think about the fundamental problem

1987
01:35:23,920 --> 01:35:25,360
is with the transformer,

1988
01:35:25,360 --> 01:35:28,560
it's not like a layered architecture, like a CNN,

1989
01:35:28,560 --> 01:35:32,320
where you can just chop off the N layers and retrain

1990
01:35:32,320 --> 01:35:36,200
from that point, it's just a much more complex

1991
01:35:36,200 --> 01:35:37,320
and interconnected model,

1992
01:35:37,320 --> 01:35:42,040
so that kind of incremental updating doesn't work.

1993
01:35:42,040 --> 01:35:44,720
Not so easily, yeah, I think there's been some work

1994
01:35:44,720 --> 01:35:48,640
on sort of taking like one percent of the parameters

1995
01:35:48,640 --> 01:35:50,320
sort of spread over the transformer

1996
01:35:50,320 --> 01:35:52,800
and updating them with new text,

1997
01:35:52,800 --> 01:35:55,520
but I think solving this problem

1998
01:35:55,520 --> 01:35:59,520
is going to be something that needs to happen pretty quickly.

1999
01:36:00,560 --> 01:36:03,040
And so to be clear, taking a step back,

2000
01:36:03,040 --> 01:36:06,640
like this is all the looking forward section,

2001
01:36:06,640 --> 01:36:10,640
those three things, kind of misinformation

2002
01:36:10,640 --> 01:36:12,440
and attributable generations,

2003
01:36:12,440 --> 01:36:14,200
diffusion models, and online updates

2004
01:36:14,200 --> 01:36:16,200
for specifically in your category

2005
01:36:16,200 --> 01:36:20,640
of the greatest, most exciting opportunities in the field.

2006
01:36:21,520 --> 01:36:26,040
Areas where we're likely to see a lot of research attention

2007
01:36:27,520 --> 01:36:30,640
and possibly some really interesting results

2008
01:36:30,640 --> 01:36:32,400
coming up in the next year or two.

2009
01:36:32,400 --> 01:36:35,320
And also sort of fundamental problems

2010
01:36:35,320 --> 01:36:37,840
that need to be addressed by language models.

2011
01:36:37,840 --> 01:36:42,600
And so that brings us to your top three predictions

2012
01:36:42,600 --> 01:36:46,920
for the field's proper, what do you see there?

2013
01:36:46,920 --> 01:36:49,760
Yeah, so I think, and maybe some of it is little

2014
01:36:49,760 --> 01:36:51,800
with a disappointment as well.

2015
01:36:51,800 --> 01:36:55,520
So the first one here is multiple modalities.

2016
01:36:55,520 --> 01:36:57,000
I think there's been a lot of exciting work,

2017
01:36:57,000 --> 01:36:59,160
so I don't want to sort of think that away.

2018
01:36:59,160 --> 01:37:03,680
But to me, after GBT3 came out and then you saw a clip

2019
01:37:03,680 --> 01:37:08,680
when Dali and Whisper and now there's video models

2020
01:37:08,720 --> 01:37:11,600
and things like that, to me fundamentally,

2021
01:37:11,600 --> 01:37:16,200
I don't understand technically why they're not the same model,

2022
01:37:16,200 --> 01:37:17,840
but it's still a little bit disappointing

2023
01:37:17,840 --> 01:37:19,400
that they're not the same model.

2024
01:37:19,400 --> 01:37:22,600
It's like, why is there not the same model

2025
01:37:22,600 --> 01:37:25,240
that change over the same data?

2026
01:37:25,240 --> 01:37:29,600
GBT3 is trained on, but also on the Lyon dataset

2027
01:37:29,600 --> 01:37:33,400
that does all the images and text and audio

2028
01:37:33,400 --> 01:37:35,280
and video and stuff like that, right?

2029
01:37:35,280 --> 01:37:39,320
And I think this is a sort of near future prediction

2030
01:37:39,320 --> 01:37:42,800
is that we are going to see ways for pre-gaining models

2031
01:37:42,800 --> 01:37:45,480
that cuts across multiple modalities.

2032
01:37:45,480 --> 01:37:47,160
And I think clip was a good example,

2033
01:37:47,160 --> 01:37:49,120
sort of early example of what you can do

2034
01:37:49,120 --> 01:37:51,040
when you have a lot of text and images.

2035
01:37:51,040 --> 01:37:53,960
But I think it still didn't have access

2036
01:37:53,960 --> 01:37:57,320
to a lot of text-only data.

2037
01:37:57,320 --> 01:38:00,800
And I want a model that can do chat GBT-like things,

2038
01:38:00,800 --> 01:38:05,800
but also generate images for me and maybe read them out

2039
01:38:06,000 --> 01:38:06,760
and things like that, right?

2040
01:38:06,760 --> 01:38:11,760
So I feel like multiple modalities is an exciting sort of

2041
01:38:12,400 --> 01:38:14,520
kind of an opportunity, but definitely something

2042
01:38:14,520 --> 01:38:15,800
that's going to happen soon.

2043
01:38:17,040 --> 01:38:19,120
When I first heard you describe this,

2044
01:38:19,120 --> 01:38:22,040
I thought, well, multi-modal, that was the big thing

2045
01:38:22,040 --> 01:38:25,920
we were talking about in these trends, conversations last year,

2046
01:38:25,920 --> 01:38:27,840
but you're going a level deeper.

2047
01:38:27,840 --> 01:38:31,960
You don't want multi-modal use cases or outputs.

2048
01:38:31,960 --> 01:38:36,240
You want a single architecture to do multi-modal things.

2049
01:38:36,240 --> 01:38:37,440
That's what I want.

2050
01:38:37,440 --> 01:38:41,000
My prediction is going to be a little bit more grounded,

2051
01:38:41,000 --> 01:38:46,000
so to say, but yeah, like video, for example,

2052
01:38:46,080 --> 01:38:47,040
is a more concrete one.

2053
01:38:47,040 --> 01:38:50,120
Like text to video, we've seen some initial versions

2054
01:38:50,120 --> 01:38:54,000
of those that's probably where a lot of initial stuff would go in.

2055
01:38:54,000 --> 01:38:58,400
But I've been really excited about sort of the mind dojo

2056
01:38:58,400 --> 01:39:00,840
world of like playing with text and Minecraft

2057
01:39:00,840 --> 01:39:03,160
and having an agent that can do a bunch of things

2058
01:39:03,160 --> 01:39:04,160
in Minecraft.

2059
01:39:04,160 --> 01:39:07,640
I feel like there are things that modules can learn

2060
01:39:07,640 --> 01:39:11,280
from images, even for language modeling,

2061
01:39:11,280 --> 01:39:14,480
it would benefit to see a lot of images in some sense.

2062
01:39:14,480 --> 01:39:16,440
There are just a bunch of things in images

2063
01:39:16,440 --> 01:39:18,280
that we never talk about in text.

2064
01:39:18,280 --> 01:39:27,160
So from an AI agent, I think it's useful to think about something

2065
01:39:27,160 --> 01:39:28,280
that has access to everything.

2066
01:39:28,280 --> 01:39:31,160
But yeah, more concretely, we're just going to be pushing them

2067
01:39:31,160 --> 01:39:32,120
sort of fair-wise.

2068
01:39:32,120 --> 01:39:34,320
Like, yeah, it's going to be audio and images.

2069
01:39:34,320 --> 01:39:37,800
And there's going to be a bunch of other pairs that will happen first.

2070
01:39:37,800 --> 01:39:41,640
But eventually, I think having multi-actual multiple

2071
01:39:41,640 --> 01:39:48,240
modalities, not just greater than one, but altities would be exciting.

2072
01:39:48,240 --> 01:39:48,760
Awesome, awesome.

2073
01:39:48,760 --> 01:39:49,920
Next up.

2074
01:39:49,920 --> 01:39:54,840
Next, I'm kind of excited about better training and better

2075
01:39:54,840 --> 01:39:57,640
inference and better in the sense of being

2076
01:39:57,640 --> 01:39:59,560
more computationally efficient.

2077
01:39:59,560 --> 01:40:03,240
I think this is an exciting work that a bunch of people

2078
01:40:03,240 --> 01:40:04,120
are already doing.

2079
01:40:04,120 --> 01:40:07,080
But I think this is just going to become increasingly

2080
01:40:07,080 --> 01:40:10,080
important from a sustainability point of view,

2081
01:40:10,080 --> 01:40:14,480
but also from university surviving and doing interesting things.

2082
01:40:14,480 --> 01:40:17,800
And small companies are contributing to research.

2083
01:40:17,800 --> 01:40:20,480
I think it's important to be able to train these models,

2084
01:40:20,480 --> 01:40:23,040
to be able to run these models.

2085
01:40:23,040 --> 01:40:25,240
And there's going to be a lot of research

2086
01:40:25,240 --> 01:40:28,600
in trying to do those things.

2087
01:40:28,600 --> 01:40:32,200
And you've got a few examples that we'll link to in the show notes.

2088
01:40:32,200 --> 01:40:35,600
Any anything that you want to point out?

2089
01:40:35,600 --> 01:40:39,400
Yeah, so let me mention two that I saw recently.

2090
01:40:39,400 --> 01:40:43,280
One of them is this paper called Cramming.

2091
01:40:43,280 --> 01:40:48,840
And the idea here is to think about the scaling laws paper,

2092
01:40:48,840 --> 01:40:51,280
like, hey, what can you do when the models get larger

2093
01:40:51,280 --> 01:40:52,520
and stuff like that?

2094
01:40:52,520 --> 01:40:55,440
The Cramming paper sort of turns it on his head

2095
01:40:55,440 --> 01:41:00,000
and decides, OK, what if I have just one GPU for one day?

2096
01:41:00,000 --> 01:41:02,960
What's the most I can do with that?

2097
01:41:02,960 --> 01:41:05,120
And it's a very different question,

2098
01:41:05,120 --> 01:41:08,720
but it somehow is a lot more relevant to many more people,

2099
01:41:08,720 --> 01:41:12,640
because a lot more people have a single GPU for a single day.

2100
01:41:12,640 --> 01:41:15,600
And they show that you can get almost sort of bird level

2101
01:41:15,600 --> 01:41:17,680
performance if you make the right choices

2102
01:41:17,680 --> 01:41:21,400
and this sort of detail what those choices might look like.

2103
01:41:21,400 --> 01:41:24,680
It's a paper, but I think I like that idea of like, hey,

2104
01:41:24,680 --> 01:41:28,280
what if we were scrappy about training these models?

2105
01:41:28,280 --> 01:41:29,320
How far can we get?

2106
01:41:29,320 --> 01:41:31,480
I think that's a very interesting question

2107
01:41:31,480 --> 01:41:35,600
that Google and OpenAI is not going to be asking,

2108
01:41:35,600 --> 01:41:38,280
but might be relevant for a lot of other research.

2109
01:41:38,280 --> 01:41:40,160
So the other one I want to talk about

2110
01:41:40,160 --> 01:41:43,960
is this Petals work that came out of the big science thing.

2111
01:41:43,960 --> 01:41:45,480
I haven't read too much about this,

2112
01:41:45,480 --> 01:41:48,120
but it seems like a really interesting idea

2113
01:41:48,120 --> 01:41:52,160
of the problem of running really large language models.

2114
01:41:52,160 --> 01:41:56,120
So even if OPT releases 175 billion model,

2115
01:41:56,120 --> 01:41:57,920
how do you actually run it?

2116
01:41:57,920 --> 01:41:59,680
It doesn't really help most people,

2117
01:41:59,680 --> 01:42:03,480
even if you have a big cluster, it's kind of difficult to run it.

2118
01:42:03,480 --> 01:42:06,240
So what this Petals does is they're

2119
01:42:06,240 --> 01:42:09,320
building this framework for using the ideas

2120
01:42:09,320 --> 01:42:13,920
behind BitTorrent of sort of distributed computing

2121
01:42:13,920 --> 01:42:15,600
and bringing it to language models.

2122
01:42:15,600 --> 01:42:17,200
So like, hey, you should be able to run

2123
01:42:17,200 --> 01:42:20,600
these 100 billion size language models,

2124
01:42:20,600 --> 01:42:24,080
distribute it over a bunch of commodities

2125
01:42:24,080 --> 01:42:27,560
of consumer computers.

2126
01:42:27,560 --> 01:42:29,360
So yeah, I think this is an interesting idea.

2127
01:42:29,360 --> 01:42:33,000
I haven't played around with it to see how far you can push it.

2128
01:42:33,000 --> 01:42:37,000
There's partly, you need a bunch of people also running Petals.

2129
01:42:37,000 --> 01:42:40,360
But once we get there, I think that could be a pretty exciting way

2130
01:42:40,360 --> 01:42:42,040
to run language models.

2131
01:42:42,040 --> 01:42:44,160
Interesting, interesting.

2132
01:42:44,160 --> 01:42:50,400
So your third prediction is editing and revising models.

2133
01:42:50,400 --> 01:42:51,600
What do you mean there?

2134
01:42:51,600 --> 01:42:55,520
So these are these family of models that

2135
01:42:55,520 --> 01:42:59,480
are not so much interested in generating text,

2136
01:42:59,480 --> 01:43:03,600
but taking existing text and editing it.

2137
01:43:03,600 --> 01:43:06,400
And I think this is a very interesting idea

2138
01:43:06,400 --> 01:43:08,480
that can become increasingly important.

2139
01:43:08,480 --> 01:43:11,880
And in some sense, this could be the way

2140
01:43:11,880 --> 01:43:14,880
you fix language model output, but actually,

2141
01:43:14,880 --> 01:43:18,560
is to have another model that takes the output of the language model

2142
01:43:18,560 --> 01:43:20,920
and fixes it.

2143
01:43:20,920 --> 01:43:25,000
So some of the work here, there was a paper out of Julias.

2144
01:43:25,000 --> 01:43:30,840
A group from YouTube now that sort of looked at summarization.

2145
01:43:30,840 --> 01:43:33,400
And there are systems that generate summaries.

2146
01:43:33,400 --> 01:43:36,240
How can you take the generated summaries and edit it

2147
01:43:36,240 --> 01:43:39,720
to correct all the factual mistakes it has made?

2148
01:43:39,720 --> 01:43:43,640
And editing is somehow much, let's not say definitely

2149
01:43:43,640 --> 01:43:45,880
a simpler problem, but it's a, in some sense,

2150
01:43:45,880 --> 01:43:49,320
it could be a simpler problem than writing the whole summary

2151
01:43:49,320 --> 01:43:51,720
from scratch, especially when you do the writing,

2152
01:43:51,720 --> 01:43:53,640
you do left to right generation.

2153
01:43:53,640 --> 01:43:55,960
So you can't go back and revisit something

2154
01:43:55,960 --> 01:43:57,200
that you've done before.

2155
01:43:57,200 --> 01:44:00,680
With these editing models, they have the whole picture

2156
01:44:00,680 --> 01:44:02,680
to some degree, and all they have to do

2157
01:44:02,680 --> 01:44:06,800
is fix it so that the picture is consistent.

2158
01:44:06,800 --> 01:44:11,880
And so this idea seems like potentially simpler than generation.

2159
01:44:11,880 --> 01:44:15,000
So you could generate something, and maybe this

2160
01:44:15,000 --> 01:44:16,640
is also attached to diffusion models,

2161
01:44:16,640 --> 01:44:19,480
where you write something that's maybe not so correct,

2162
01:44:19,480 --> 01:44:21,960
but you revise it, and it becomes better.

2163
01:44:21,960 --> 01:44:24,640
So there is a bunch of work along these directions

2164
01:44:24,640 --> 01:44:27,000
that came out essentially this year,

2165
01:44:27,000 --> 01:44:29,440
and maybe second half of this year.

2166
01:44:29,440 --> 01:44:33,200
Some of it early on, that tries to gather data sets

2167
01:44:33,200 --> 01:44:36,920
where you have edits, or try to maybe even generate

2168
01:44:36,920 --> 01:44:40,080
data sets where you have edits, and create these models

2169
01:44:40,080 --> 01:44:43,680
that are able to fix those edits in some sense.

2170
01:44:43,680 --> 01:44:45,520
And so the prediction specifically

2171
01:44:45,520 --> 01:44:50,720
is that teams will build on this and produce models

2172
01:44:50,720 --> 01:44:55,520
that can actually kind of deliver on the ability

2173
01:44:55,520 --> 01:44:59,040
to do editing and revising.

2174
01:44:59,040 --> 01:45:02,320
And I think this could be, for example,

2175
01:45:02,320 --> 01:45:05,640
there'll be an editing model that can fix bias issues.

2176
01:45:05,640 --> 01:45:08,240
There'll be an editing model that fixes toxicity.

2177
01:45:08,240 --> 01:45:11,520
There'll be an editing model that fixes factuality.

2178
01:45:11,520 --> 01:45:14,800
And these editing models can make web searches

2179
01:45:14,800 --> 01:45:18,200
and sort of take that information and edit the output.

2180
01:45:18,200 --> 01:45:23,520
So I could imagine that this could be a practical way

2181
01:45:23,520 --> 01:45:26,120
of solving many of the issues.

2182
01:45:26,120 --> 01:45:29,240
It is a really interesting idea that,

2183
01:45:29,240 --> 01:45:31,120
I don't know if it's like a separation of concerns

2184
01:45:31,120 --> 01:45:34,000
or something like the language model

2185
01:45:34,000 --> 01:45:36,400
doesn't necessarily need to do everything

2186
01:45:36,400 --> 01:45:38,840
if we can compensate.

2187
01:45:38,840 --> 01:45:41,400
So in a way, it's like decomposition as well.

2188
01:45:41,400 --> 01:45:45,640
Like, let it generate, and if the way

2189
01:45:45,640 --> 01:45:49,360
to get something that's not toxic that's accurate

2190
01:45:49,360 --> 01:45:53,640
is to have another type of model support it.

2191
01:45:53,640 --> 01:45:54,160
Great.

2192
01:45:54,160 --> 01:45:56,800
Yeah, I think that's right.

2193
01:45:56,800 --> 01:45:59,680
And for at least for summarization

2194
01:45:59,680 --> 01:46:02,120
and things where it's supposed to be factual and stuff

2195
01:46:02,120 --> 01:46:05,680
like that, I could see it sort of addressing those problems.

2196
01:46:05,680 --> 01:46:08,120
Of course, if it's generating a long text

2197
01:46:08,120 --> 01:46:12,480
and there are longer range sort of consistency issues

2198
01:46:12,480 --> 01:46:14,360
and stuff like that, it might be a little bit difficult

2199
01:46:14,360 --> 01:46:17,400
for editing models to come into feature there.

2200
01:46:17,400 --> 01:46:19,400
What I like about editing is also,

2201
01:46:19,400 --> 01:46:22,160
it's something that we can imagine not only working

2202
01:46:22,160 --> 01:46:25,520
on language model output, but working on a human output

2203
01:46:25,520 --> 01:46:28,760
or a text that's been written with the writing assistant

2204
01:46:28,760 --> 01:46:29,760
and things like that, right?

2205
01:46:29,760 --> 01:46:33,000
You can still go back and do a post-processing editing step

2206
01:46:33,000 --> 01:46:35,560
to polish it up and I think that would be great as well.

2207
01:46:35,560 --> 01:46:41,440
So our last category in the NLP predictions

2208
01:46:41,440 --> 01:46:46,200
is top people's companies, organizations, teams

2209
01:46:46,200 --> 01:46:49,480
to watch in the field 2023.

2210
01:46:49,480 --> 01:46:54,480
Of course, the caveat of you're not any emissions here

2211
01:46:57,200 --> 01:47:00,040
are not just like the work of any particular team,

2212
01:47:00,040 --> 01:47:02,920
but who's got your mind share

2213
01:47:02,920 --> 01:47:07,240
and who are you expecting to see interesting things

2214
01:47:07,240 --> 01:47:10,000
from in the upcoming year?

2215
01:47:10,000 --> 01:47:12,960
Yeah, so this has been a little bit difficult

2216
01:47:12,960 --> 01:47:15,360
question I think every year, but one thing I will say

2217
01:47:15,360 --> 01:47:17,280
and this is maybe the most obvious answer

2218
01:47:17,280 --> 01:47:21,920
is to keep an eye on OpenAI and what they're up to, right?

2219
01:47:21,920 --> 01:47:24,920
I think once they do something,

2220
01:47:24,920 --> 01:47:26,280
people always come back and say,

2221
01:47:26,280 --> 01:47:28,320
look, what they've done is not so exciting,

2222
01:47:28,320 --> 01:47:31,560
oh, they only scale it up or oh, they only did this

2223
01:47:31,560 --> 01:47:32,880
and this new thing.

2224
01:47:32,880 --> 01:47:36,080
But the fact is that they are the first ones to do it,

2225
01:47:36,080 --> 01:47:38,960
that the first ones to bring it out, make it available

2226
01:47:38,960 --> 01:47:41,960
and that is, and get people excited

2227
01:47:41,960 --> 01:47:45,800
about language models in a way that they weren't before,

2228
01:47:45,800 --> 01:47:49,920
so that happened with GPT-2, GPT-3 and JGPD

2229
01:47:49,920 --> 01:47:53,320
and I'm sure GPT-4 will have the same thing.

2230
01:47:53,320 --> 01:47:56,560
I'm sure retroactively we will all talk about

2231
01:47:56,560 --> 01:47:58,440
what the problems with GPT-4 are

2232
01:47:58,440 --> 01:48:02,520
and how it's incrementally only training on more data

2233
01:48:02,520 --> 01:48:05,640
or has more parameters or whatever it is,

2234
01:48:05,640 --> 01:48:07,920
but I think qualitatively it'll bring something

2235
01:48:07,920 --> 01:48:10,680
interesting to the table and I'm really curious

2236
01:48:10,680 --> 01:48:14,480
about what that next interesting thing is going to be.

2237
01:48:15,480 --> 01:48:20,320
Do you think the general predictions

2238
01:48:20,320 --> 01:48:22,240
that are kind of floating around,

2239
01:48:22,240 --> 01:48:27,240
basically spring and 100 trillion parameters?

2240
01:48:30,360 --> 01:48:32,080
Is your money on those?

2241
01:48:32,080 --> 01:48:36,920
I mean, to sort of have a completely different perspective,

2242
01:48:36,920 --> 01:48:39,840
I think this is also a nice model

2243
01:48:39,840 --> 01:48:41,960
that came out, this nice paper that came out

2244
01:48:41,960 --> 01:48:44,800
a little earlier called the Chinchilla paper.

2245
01:48:44,800 --> 01:48:47,000
This was a paper that showed that these models

2246
01:48:47,000 --> 01:48:51,440
are extremely under-trained and they are data hungry.

2247
01:48:51,440 --> 01:48:55,800
So one version of GPT-4 could be potentially

2248
01:48:55,800 --> 01:48:57,480
not even a different architecture,

2249
01:48:57,480 --> 01:49:00,200
not even more parameters, like exactly,

2250
01:49:00,200 --> 01:49:05,200
let's keep it 175 billion and let's just somehow

2251
01:49:05,200 --> 01:49:07,760
get 10 times the data if you can potentially get

2252
01:49:07,760 --> 01:49:09,720
that spot somewhere, right?

2253
01:49:09,720 --> 01:49:10,920
I could totally use it.

2254
01:49:10,920 --> 01:49:14,920
But then everybody that shared the image

2255
01:49:14,920 --> 01:49:17,680
with the little dot and the big dot would be totally wrong.

2256
01:49:17,680 --> 01:49:21,880
Well, yeah, they'll just sort of replace that with data

2257
01:49:21,880 --> 01:49:24,160
and it might still be true, right?

2258
01:49:24,160 --> 01:49:28,880
For those not on Twitter, that is dominated LLM Twitter

2259
01:49:28,880 --> 01:49:30,920
over the past couple of days.

2260
01:49:30,920 --> 01:49:32,160
Is there even,

2261
01:49:32,160 --> 01:49:36,440
I think that when GPT-3 came out,

2262
01:49:36,440 --> 01:49:41,360
the kind of colloquial articulation

2263
01:49:41,360 --> 01:49:44,320
of what they did was like train this language model

2264
01:49:44,320 --> 01:49:45,640
on the entire internet.

2265
01:49:45,640 --> 01:49:49,120
Like, is there 10X more data to train on?

2266
01:49:49,120 --> 01:49:51,440
Yeah, I don't know how much they've trained on

2267
01:49:51,440 --> 01:49:52,440
and how much there is.

2268
01:49:52,440 --> 01:49:55,240
I mean, there's definitely 10X more data.

2269
01:49:55,240 --> 01:49:58,960
There is a lot of stuff on the proprietary, right?

2270
01:49:58,960 --> 01:50:00,080
proprietary.

2271
01:50:00,080 --> 01:50:01,760
Maybe even proprietary, right?

2272
01:50:01,760 --> 01:50:05,800
Like, transcribe a bunch of videos and audio and books

2273
01:50:05,800 --> 01:50:07,120
and I guess, yeah.

2274
01:50:07,120 --> 01:50:08,800
They do have that whisper model

2275
01:50:08,800 --> 01:50:12,840
that that's really good transcribing.

2276
01:50:12,840 --> 01:50:13,880
So they could use that.

2277
01:50:13,880 --> 01:50:15,600
They didn't create that for no reason.

2278
01:50:15,600 --> 01:50:17,000
Right, yeah.

2279
01:50:17,000 --> 01:50:19,800
They also can go into scientific papers

2280
01:50:19,800 --> 01:50:22,320
and like, I don't think the 48 million

2281
01:50:22,320 --> 01:50:24,880
that I'm papers that Galactica was trained on

2282
01:50:26,240 --> 01:50:27,920
was something GPT-3 was trained on.

2283
01:50:27,920 --> 01:50:29,880
And I think that is a pretty valuable resource.

2284
01:50:29,880 --> 01:50:31,800
That Galactica paper also showed that even

2285
01:50:31,800 --> 01:50:33,720
on mathematical reasoning and things like that,

2286
01:50:33,720 --> 01:50:35,640
they were actually better.

2287
01:50:35,640 --> 01:50:37,680
So these scientific papers may be useful

2288
01:50:37,680 --> 01:50:39,880
for a bunch of other things than that we don't realize.

2289
01:50:39,880 --> 01:50:43,080
So yeah, I think where that data comes from is unclear to me,

2290
01:50:43,080 --> 01:50:46,320
but it's clear that more data is somehow

2291
01:50:46,320 --> 01:50:49,240
maybe even more interesting than more parameters.

2292
01:50:49,240 --> 01:50:54,240
And more data could include more RLHF style things, right?

2293
01:50:54,240 --> 01:50:56,200
Like, I don't know what to open it, I guess.

2294
01:50:56,200 --> 01:50:57,200
Okay.

2295
01:50:57,200 --> 01:51:00,720
The other company too, I would say again,

2296
01:51:00,720 --> 01:51:03,680
continue taking a look at is hugging face.

2297
01:51:03,680 --> 01:51:08,320
I've been constantly sort of amazed by how much

2298
01:51:08,320 --> 01:51:09,600
they've been doing.

2299
01:51:09,600 --> 01:51:13,480
One of the sort of key insights is like E-MNB,

2300
01:51:13,480 --> 01:51:15,480
which is the stock conference in NLP

2301
01:51:15,480 --> 01:51:18,480
has this demo track where they highlight sort

2302
01:51:18,480 --> 01:51:22,760
of not research papers but products of demos

2303
01:51:22,760 --> 01:51:24,400
that are relevant for research.

2304
01:51:24,400 --> 01:51:27,040
And for the last three years, I think,

2305
01:51:27,040 --> 01:51:31,840
at E-MNB, hugging face has got the best demo paper award.

2306
01:51:31,840 --> 01:51:36,000
And that's that kind of thing sort of shows

2307
01:51:36,000 --> 01:51:38,040
how they've been doing very different things,

2308
01:51:38,040 --> 01:51:41,320
but also doing things that are impactful and interesting.

2309
01:51:41,320 --> 01:51:43,880
So the two, I want to highlight this year is,

2310
01:51:43,880 --> 01:51:45,600
again, they've done many, many things,

2311
01:51:45,600 --> 01:51:50,080
but the one I want to highlight is the evaluate system

2312
01:51:50,080 --> 01:51:53,680
where they had this whole evaluation framework

2313
01:51:53,680 --> 01:51:56,920
for reproducing evaluations and evaluating models

2314
01:51:56,920 --> 01:51:59,080
and making all of this stuff really easy.

2315
01:51:59,080 --> 01:52:00,800
So you can introduce a new metric,

2316
01:52:00,800 --> 01:52:03,840
evaluated in thousands of models, and so that.

2317
01:52:03,840 --> 01:52:05,760
Make it really easy to compare models,

2318
01:52:05,760 --> 01:52:09,360
make it really easy to reproduce papers.

2319
01:52:09,360 --> 01:52:11,960
And I think that that's a really valuable service

2320
01:52:11,960 --> 01:52:13,960
to do research.

2321
01:52:13,960 --> 01:52:16,800
And the other one that I sort of we also started with this

2322
01:52:16,800 --> 01:52:20,200
of like, hey, what's happening inside the pre-training data?

2323
01:52:20,200 --> 01:52:23,640
One of the tools they have is this roots search tool

2324
01:52:23,640 --> 01:52:26,640
that takes the roots pre-training data,

2325
01:52:26,640 --> 01:52:29,520
but allows you to search it and find all kinds of things

2326
01:52:29,520 --> 01:52:31,440
that are happening inside that pre-training data.

2327
01:52:31,440 --> 01:52:33,600
So if you have a specific prediction,

2328
01:52:33,600 --> 01:52:35,320
then you want to be like, hey, is there anything

2329
01:52:35,320 --> 01:52:38,200
in the training data that looked exactly like this?

2330
01:52:38,200 --> 01:52:41,040
You can do that search and get some results.

2331
01:52:41,040 --> 01:52:45,240
So I think they are just being pretty creative

2332
01:52:45,240 --> 01:52:48,880
and thoughtful about what is useful and building tools.

2333
01:52:48,880 --> 01:52:50,320
And that's been exciting.

2334
01:52:50,320 --> 01:52:53,520
And the last one that I'll bring up, and this is something

2335
01:52:53,520 --> 01:52:56,000
that was on top of my head this week

2336
01:52:56,000 --> 01:52:57,600
with it can change.

2337
01:52:57,600 --> 01:53:00,760
It's a group called Ott.

2338
01:53:00,760 --> 01:53:03,080
It's o-u-g-h-d.

2339
01:53:03,080 --> 01:53:07,760
I believe it's Ott.org, it's a website.

2340
01:53:07,760 --> 01:53:12,360
And this is sort of a research nonprofit.

2341
01:53:12,360 --> 01:53:18,160
And they've been doing sort of interesting things

2342
01:53:18,160 --> 01:53:20,040
related to sort of building tools.

2343
01:53:20,040 --> 01:53:22,840
So they have this tool called Primer.

2344
01:53:22,840 --> 01:53:26,440
And this is going back to decomposed reasoning.

2345
01:53:26,440 --> 01:53:29,280
This tool called Primer, sort of you can give it a question

2346
01:53:29,280 --> 01:53:31,280
and it tries to come up with an answer.

2347
01:53:31,280 --> 01:53:33,600
But in the process of coming up with an answer,

2348
01:53:33,600 --> 01:53:37,840
it can do a web search or it can write a small program

2349
01:53:37,840 --> 01:53:39,280
and it can do all of these things.

2350
01:53:39,280 --> 01:53:41,320
And they're very sort of nice tool

2351
01:53:41,320 --> 01:53:44,280
to be able to visualize what the decompositions are

2352
01:53:44,280 --> 01:53:46,480
and what sort of things are being done.

2353
01:53:46,480 --> 01:53:50,920
So it's a really interesting use case of language models.

2354
01:53:50,920 --> 01:53:55,040
And then they also have another tool called Elisit,

2355
01:53:55,040 --> 01:53:58,280
which is in some sense, it's a little bit like Galactica,

2356
01:53:58,280 --> 01:54:02,880
but it's not so much interested in generating papers for you,

2357
01:54:02,880 --> 01:54:05,840
but helping you do research for your paper.

2358
01:54:05,840 --> 01:54:08,440
So you have a specific question.

2359
01:54:08,440 --> 01:54:11,200
It's going to find a bunch of relevant papers,

2360
01:54:11,200 --> 01:54:13,480
take out snippets from those papers,

2361
01:54:13,480 --> 01:54:16,160
and be able to do that.

2362
01:54:16,160 --> 01:54:19,680
So I don't know, they've had a bunch of tools

2363
01:54:19,680 --> 01:54:22,640
that when I'm looking at decomposed reasoning,

2364
01:54:22,640 --> 01:54:25,040
it comes up and I'm looking at, OK,

2365
01:54:25,040 --> 01:54:27,280
assist, select, assist, it sort of comes up.

2366
01:54:27,280 --> 01:54:29,320
And so it's been interesting to see you

2367
01:54:29,320 --> 01:54:31,120
and I'm curious what those will be next.

2368
01:54:31,120 --> 01:54:33,040
I'm really curious about that.

2369
01:54:33,040 --> 01:54:36,760
And I'm going to look into that in more detail.

2370
01:54:36,760 --> 01:54:39,680
Awesome, awesome.

2371
01:54:39,680 --> 01:54:42,120
Well, I think we are done.

2372
01:54:42,120 --> 01:54:43,160
Like, you've been a champ.

2373
01:54:43,160 --> 01:54:46,440
This has been awesome.

2374
01:54:46,440 --> 01:54:48,600
It's been fun, yeah.

2375
01:54:48,600 --> 01:54:53,040
Yeah, now, I mean, you rose to the occasion

2376
01:54:53,040 --> 01:54:57,920
of kind of capturing an amazing year in NLP, for sure.

2377
01:54:57,920 --> 01:55:01,680
So thanks so much for joining us.

2378
01:55:01,680 --> 01:55:02,800
Yeah, thanks for inviting me.

2379
01:55:02,800 --> 01:55:05,640
I think the time sort of justifies

2380
01:55:05,640 --> 01:55:09,080
how much this year had in NLP this year.

2381
01:55:09,080 --> 01:55:12,040
And I'm really curious to see where NLP is going to go.

2382
01:55:12,040 --> 01:55:15,640
I will mention that ChatGPD came out right before,

2383
01:55:15,640 --> 01:55:17,680
or I think maybe even during Eurips.

2384
01:55:17,680 --> 01:55:19,000
So I attended Eurips.

2385
01:55:19,000 --> 01:55:21,840
And I saw the first-hand experience

2386
01:55:21,840 --> 01:55:24,080
of the whole machine learning community there.

2387
01:55:24,080 --> 01:55:27,080
Then I flew to Abu Dhabi to attend the NLP.

2388
01:55:27,080 --> 01:55:30,960
And that's where I saw the reaction of the whole NLP community.

2389
01:55:30,960 --> 01:55:34,960
And it's been interesting to see sort of how the reactions

2390
01:55:34,960 --> 01:55:38,960
have sort of spanned both optimism and excitement,

2391
01:55:38,960 --> 01:55:41,160
which is kind of where I am, like to see like,

2392
01:55:41,160 --> 01:55:44,160
hey, what can we bear with this stuff?

2393
01:55:44,160 --> 01:55:47,240
To pessimism where they're like, oh,

2394
01:55:47,240 --> 01:55:49,760
it doesn't really, yeah, it's not going to change anything.

2395
01:55:49,760 --> 01:55:52,440
It's just a bigger language model.

2396
01:55:52,440 --> 01:55:54,720
All the way to essentially, I want to say

2397
01:55:54,720 --> 01:55:57,400
some form of denial, where it's like,

2398
01:55:57,400 --> 01:56:01,680
look, it's behind a proprietary closed-off system.

2399
01:56:01,680 --> 01:56:05,280
And therefore, it doesn't matter to research.

2400
01:56:05,280 --> 01:56:08,480
And that's definitely not the day I agree with.

2401
01:56:08,480 --> 01:56:10,480
So yeah, it's been exciting.

2402
01:56:10,480 --> 01:56:13,160
And there's also a fourth, which maybe is less so.

2403
01:56:13,160 --> 01:56:15,920
And I don't know, maybe less so in the research community

2404
01:56:15,920 --> 01:56:20,440
than in the general sphere, which is fear of the implications

2405
01:56:20,440 --> 01:56:23,440
of it.

2406
01:56:23,440 --> 01:56:25,960
Did you find out less so on the research side?

2407
01:56:25,960 --> 01:56:29,480
I guess less so, definitely less so on the, yeah.

2408
01:56:29,480 --> 01:56:32,240
Because I think we've been, there is

2409
01:56:32,240 --> 01:56:35,080
a little bit of fear becoming a little bit more obvious.

2410
01:56:35,080 --> 01:56:37,960
But I think the community, because of a lot of people

2411
01:56:37,960 --> 01:56:39,440
who've been sort of pointing out problems

2412
01:56:39,440 --> 01:56:41,440
in the bit of life, like the models for a while,

2413
01:56:41,440 --> 01:56:45,320
we are kind of, we know what not to.

2414
01:56:45,320 --> 01:56:47,840
As a community, we should know what not to do.

2415
01:56:47,840 --> 01:56:49,760
But it is a little bit scary, where

2416
01:56:49,760 --> 01:56:51,320
people are using it for things that,

2417
01:56:51,320 --> 01:56:53,960
clearly, at the onset, should be like,

2418
01:56:53,960 --> 01:56:55,600
hey, why are you doing this straight?

2419
01:56:55,600 --> 01:56:57,560
Yeah, yeah, yeah.

2420
01:56:57,560 --> 01:56:58,600
Awesome.

2421
01:56:58,600 --> 01:57:01,600
Well, once again, Tamir, thanks so much.

2422
01:57:01,600 --> 01:57:03,400
Really great session and conversation.

2423
01:57:03,400 --> 01:57:06,360
And I appreciate all the work you put into prepping for it.

2424
01:57:06,360 --> 01:57:16,360
Yeah, thank you so much, it's fun.

