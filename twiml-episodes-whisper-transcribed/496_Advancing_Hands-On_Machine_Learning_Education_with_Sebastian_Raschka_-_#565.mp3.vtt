WEBVTT

00:00.000 --> 00:10.520
All right, everyone. Welcome to another episode of the Twomo AI podcast. I am your host, Sam

00:10.520 --> 00:16.560
Charrington. And today, I'm joined by Sebastian Rashka, an assistant professor of statistics

00:16.560 --> 00:23.440
at the University of Wisconsin-Madison, as well as the lead AI educator at Grid AI. Sebastian,

00:23.440 --> 00:28.000
I'm really looking forward to digging into our chat. We'll touch on your research, as

00:28.000 --> 00:33.120
well as your work in ML education. Welcome to the podcast. Yeah, thank you, Sam, for the kind

00:33.120 --> 00:39.120
invitation to be on your podcast. I'm super excited to chat about research, education, or whatever.

00:39.120 --> 00:44.640
Yeah, you feel like chatting about today. Fantastic. Well, let's start by having you share a little

00:44.640 --> 00:50.720
bit about your background and introducing yourself to our audience. How do you end up in machine learning?

00:50.720 --> 00:55.680
Yeah, that is a good question. It goes back many years now, but when I was a PhD student,

00:55.680 --> 01:02.400
I was studying computational biology. And yeah, my professor back then, she recommended me

01:02.400 --> 01:08.880
taking this introduction to statistical pattern recognition class, which was a grad level class

01:08.880 --> 01:13.680
in the computer science department, and really that got me hooked. And so that was mostly about

01:13.680 --> 01:19.760
like Bayesian statistics and yeah, Bayesian methods for pattern recognition. But yeah, somehow it

01:19.760 --> 01:25.040
clicked with me and I found it super fascinating that you can kind of like teach computers how to

01:25.040 --> 01:30.800
recognize patterns in data. And from there on, I was just, yeah, super hooked and studying machine

01:30.800 --> 01:38.000
learning. And yeah, then I ended up writing a book back then in 2015, Python machine learning,

01:38.000 --> 01:43.280
joined the University of Wisconsin Medicine in the statistics department. And yeah, now at

01:43.280 --> 01:51.120
Grid AI as a educator. So yeah, it has been quite a journey. But yeah, I'm still as much excited as

01:51.120 --> 01:56.320
I was back then about machine learning. So there's so much cool stuff coming out too. So it's

01:56.320 --> 02:03.520
an exciting field. Yeah, absolutely. And your work in particular around education has been

02:03.520 --> 02:08.560
inspirational to a lot of people. I mentioned that you are a great follow on Twitter. We'll link to

02:08.560 --> 02:19.360
your Twitter handle profile in the show notes. And when I think about your take, it kind of reminds me

02:19.360 --> 02:26.080
of a little bit of the top down approach that like a Jeremy Howard advocates, but also with

02:26.720 --> 02:31.840
that academic grounding of a bottom up approach. You sooner do a really good job of fusing those

02:31.840 --> 02:38.960
together. Tell, tell us a little bit about, you know, what got you excited about the teaching aspect

02:38.960 --> 02:46.480
of machine learning and your philosophy around that. Yeah, so yeah, it's kind of like related to what

02:46.480 --> 02:52.720
you just mentioned, like fusing the academic approaches, let's say the mathematical details.

02:52.720 --> 02:59.840
And then also the practical aspect. So personally, I must say I'm really I enjoy coding. I really like

02:59.840 --> 03:05.120
programming. It's something I do for fun on the weekends and like also contributing to open

03:05.120 --> 03:11.120
source software. But then I'm also sometimes curious about certain things how they work. So it's

03:11.120 --> 03:17.280
like maybe coming from academia, you are like taught to investigate, to make sure you understand

03:17.280 --> 03:22.400
everything you're using. And yeah, I'm trying to kind of bridge the gap between the two that it's

03:22.400 --> 03:28.320
fun where you get to code things and these types of things. But at the same time, you also develop an

03:28.320 --> 03:33.280
understanding of how these things work because they make you, let's say, more powerful in terms of,

03:33.280 --> 03:38.080
you know, what to change and what to expect. It's kind of like a deeper level of understanding. And

03:38.080 --> 03:42.240
I think it's also very satisfying to know what's going on when you change certain things or when

03:42.240 --> 03:48.640
you code something up. And yeah, for teaching, that's a good point. I try also to combine the two.

03:49.920 --> 03:54.480
It's, I mean, everyone has different preferences. But when I was taking classes, I enjoy

03:55.440 --> 04:01.120
sometimes the mathematical details. But it's something I also, I need to learn on my own time.

04:01.120 --> 04:06.240
It's for me very hard to sit in a, let's say, lecture and just see mathematical equations.

04:06.240 --> 04:11.840
It's often too fast for me, even. So in that case, I try to balance it that I don't, let's say,

04:11.840 --> 04:17.680
have slides full of equations that I kind of change it up with big picture concepts. And also,

04:17.680 --> 04:23.360
yeah, code examples, because I think code examples are at least helping me a lot like in terms of

04:23.360 --> 04:29.200
solidifying things I learned. And yeah, when I'm teaching at the university, I also noticed that

04:29.200 --> 04:35.360
students like this a lot. So yeah, it is, it is a fun part. I think it's the reward. You learn

04:35.360 --> 04:39.680
something. And then you apply it and see, well, this is actually so cool. I want to do more of it.

04:39.680 --> 04:45.040
And then I think if you do it like this way, where you show cool examples and empower people to

04:45.040 --> 04:50.800
develop cool applications, people will automatically be motivated to learn more details. But if you

04:50.800 --> 04:55.200
start the other way around, if you teach people like the nitty gritty details, I think it's easy to

04:55.200 --> 05:00.480
lose track and get bored. That happened to me like with mathematics when I was like in high school

05:00.480 --> 05:05.680
taking math classes. I wasn't really excited about that because it was like a bunch of numbers.

05:05.680 --> 05:11.280
I can move them around cool. But why is that useful? Why is that cool? And yeah, with coding,

05:11.280 --> 05:15.840
you can immediately see what you can do with it if you develop like your machine learning applications.

05:16.400 --> 05:22.640
And yet in my class, I make sure that there's a decent portion of that. So I also have these

05:22.640 --> 05:28.480
class projects I like my students to work on, where at the, yeah, I would say middle of the semester,

05:28.480 --> 05:34.800
the students submitted proposal, project proposal. It could be anything they are interested in.

05:34.800 --> 05:39.760
It just has to be related to machine learning or deep learning. And then they get to work on it

05:39.760 --> 05:44.400
for the rest of the semester, while of course taking the lectures. But I think also based on

05:45.440 --> 05:50.240
hearing from students, that was really the fun part because at the end, they work in teams of

05:50.240 --> 05:55.440
three. At the end, they have like the teamwork experience. They develop something. They have

05:55.440 --> 05:59.520
something to show for us. So we also have presentations and class where the students get to talk

05:59.520 --> 06:04.320
about their project in front of other students. And then in the end, they have, I mean, I make this

06:04.320 --> 06:08.720
voluntary because I don't want to force anyone, but they can share the project that's there on GitHub

06:09.200 --> 06:13.440
publicly. And a lot of students like that because then when they apply for jobs, they can actually show

06:14.800 --> 06:18.800
where they apply that can show, okay, I've actually done something cool here with machine learning.

06:18.800 --> 06:23.600
I know how it works. I mean, of course, you can't expect in a semester to build

06:23.600 --> 06:28.400
like this huge AI system, but it is at least something cool. I mean, there were so many cool

06:28.400 --> 06:31.760
projects in the last couple of years that I've seen. I was really impressed. And I think

06:32.400 --> 06:36.720
it's fun for me to see all of that, but also fun for the students to, yeah, um,

06:36.720 --> 06:41.200
practically work on that and develop their skills. So yeah. So that's my approach to teaching

06:41.200 --> 06:45.840
basically. I make sure, well, I want to make sure that students also get this practical experience

06:45.840 --> 06:51.920
because I think it's also very important and motivating. Do you think about it from a pedagogical

06:51.920 --> 06:56.640
perspective or is this just kind of how it all naturally came together for you? I'm not that good

06:56.640 --> 07:02.400
at, let's say, reading literature in education and figuring out what is the best approach to teaching.

07:02.400 --> 07:08.960
For me, I mostly go with gut feeling. Honestly, how I do these things is really, I'm thinking about

07:08.960 --> 07:14.320
what do I like? How do I like, let's say, consuming information? Is this too much math? Is this

07:14.320 --> 07:18.960
too much code? Is is that the right balance? Would I be excited about this? And it's basically just

07:18.960 --> 07:24.800
that like being myself motivated and excited and then just going with it. That's awesome. That's

07:24.800 --> 07:32.560
awesome. You did a workshop or participated in a workshop teaching machine learning at ECML

07:33.280 --> 07:40.560
last year and did a session deeper learning by doing integrating hands-on research projects into

07:40.560 --> 07:45.440
a machine learning course. And it sounds like that in some ways, you're sharing all the things that

07:45.440 --> 07:52.960
we just touched on your experiences teaching. Are there for folks that are putting maybe putting

07:52.960 --> 07:59.520
together courses? Are there interesting things that you've learned about how to integrate projects

07:59.520 --> 08:06.800
in? Are there things that don't or is it more about be sure to focus on and include practical

08:06.800 --> 08:13.360
hands-on projects versus not? I think this is one approach of doing it, but it's maybe not the

08:13.360 --> 08:21.600
perfect approach. But one, I would say, one downside of a project is that if you imagine students

08:21.600 --> 08:26.080
taking the first machine learning class or deep learning class, they don't know what you can do

08:26.080 --> 08:32.960
with machine learning. So the one challenge I found is, yeah, you ideally want to have more time

08:32.960 --> 08:38.240
so that students can work on the project more. I mean, let's say stretching it all out over the

08:38.240 --> 08:43.920
whole semester, but then the problem becomes that if people have not encountered machine learning

08:43.920 --> 08:48.080
or have learned about machine learning, it's hard for them to propose a project centered around

08:48.080 --> 08:53.120
a machine learning because at that point they may not know what's feasible or not. So in this

08:53.120 --> 08:59.520
proposal that I make them submit at this middle of the semester, I also give them feedback. But yeah,

08:59.520 --> 09:05.280
it is the middle of the semester where it's kind of like a little bit late. And then also one downside

09:05.280 --> 09:10.000
is that they haven't seen all the methods yet that we get to cover in class because the class

09:10.000 --> 09:14.400
still goes on. But despite that, I mean, these are little downsides. I think there are lots of

09:14.400 --> 09:19.520
upsides where people really get to, yeah, get to use the methods that you learn about. And

09:20.320 --> 09:27.200
here I find also it's helpful to have homework examples where students get like also

09:28.160 --> 09:34.080
understand the details where in the beginning, it might be harder to really develop a machine learning

09:34.080 --> 09:39.280
that's a pipeline from scratch because if you're new to it, there are so many things to think about.

09:39.280 --> 09:45.360
So what I found works well is providing template code and then leaving out let's say core parts

09:45.360 --> 09:53.280
of the code. Like, for example, here we have the framework of a model at another, let's say,

09:53.280 --> 09:58.640
activation function or something like that, something little small. And the students will think about

09:58.640 --> 10:03.280
these things like how to do that, but then also have these templates that they can reuse and

10:03.280 --> 10:09.440
let's say their class projects as a framework. So I think this is helpful to provide students with

10:10.160 --> 10:16.000
ample examples like frameworks, frameworks in terms of code that is self-contained, it works.

10:16.000 --> 10:21.360
Of course, they will adopt that for their project, but at least something to help them so that

10:21.360 --> 10:26.240
they don't have to do everything from scratch. Because I think this is one intimidating thing about

10:26.240 --> 10:29.920
machine learning. There are so many tools and so many frameworks, especially for deep learning,

10:29.920 --> 10:35.440
that it's a lot of code and it can sometimes be a little bit overwhelming. And then kind of like

10:35.440 --> 10:40.080
managing that by showing a lot of examples and yet providing templates, I think, helps.

10:40.080 --> 10:45.040
What do you find is most challenging for folks just getting started with machine learning?

10:45.040 --> 10:50.880
For people who want to use machine learning, one challenge is how do I get my data into this

10:50.880 --> 10:55.360
machine learning model? I mean, there are, of course, there's machine learning and let's say the

10:55.360 --> 11:01.120
non-neuronetwork deep learning part, where these are also two different cans of worms, but

11:01.120 --> 11:05.840
the second learning on the surface looks very simple, very simple to use. It's like a few lines

11:05.840 --> 11:11.600
of code where you can fit your classifier, but I think the main challenge is really preparing your data.

11:12.800 --> 11:17.680
Especially if you don't want to do something like iris floor classification. I think the

11:17.680 --> 11:21.920
biggest challenge is students have so many cool ideas for their class projects, but then it's

11:21.920 --> 11:27.360
always about how do we make sure this data is the right format for machine learning classifier.

11:27.360 --> 11:32.480
For example, if we are using second learn, this is usually a tabular data set, so we make sure

11:32.480 --> 11:37.680
we have class labels and then also the columns, the feature columns. And for deep learning, okay,

11:37.680 --> 11:44.960
it's a little bit more, I would say raw where you can work with image data directly or text data,

11:44.960 --> 11:50.880
but still thinking about how to organize your data that is one challenge, I think.

11:50.880 --> 11:56.960
In chatting earlier, you emphasize, you really take a lot of care to make sure you walk through

11:56.960 --> 12:02.160
kind of classical machine learning before diving into deep learning. Talk a little bit more about

12:02.160 --> 12:09.840
that and your experience is there. Yeah, that is a good point. So in my book, it's a pretty long book,

12:09.840 --> 12:18.320
so it could have been two books almost. So it is a thick book. Yeah, we'll keep you busy for a while,

12:18.320 --> 12:24.320
but it is essentially, you can think of it as two books, a machine learning book where you get to

12:24.320 --> 12:29.760
learn about the basics of machine learning, model evaluation, hyperparameter tuning, all the basics

12:29.760 --> 12:36.240
in the context of psychic learn and tabular data sets. And then the second half, I think chapter

12:36.240 --> 12:41.680
11 or 12 is the turning point where I explain how to implement a neural network from scratch using

12:41.680 --> 12:50.000
NumPy and then from that, it starts with the deep learning parts. And yeah, so I feel like, okay,

12:50.000 --> 12:55.680
this could have been two books, but nowadays, I think this deep learning is so powerful and so

12:55.680 --> 13:00.720
let's say exciting that a lot of people sometimes forget that there is also non-deep learning

13:00.720 --> 13:05.920
machine learning where I think there's still a place for that if you especially look on Kaggle,

13:05.920 --> 13:12.080
a lot of competitions are still won by XGBoost, which is a grade in boosting classifier for tabular data.

13:12.080 --> 13:18.800
And honestly, I would rather say it should be kind of like, I mean, machine learning is still

13:18.800 --> 13:23.440
like the non-deep learning part a good baseline, but also depending really on your data set,

13:23.440 --> 13:29.040
how much data do you have and what format your data is, you want to use one approach over the other.

13:29.040 --> 13:33.680
For example, for smaller data sets or tabular data sets, machine learning might be, let's say,

13:33.680 --> 13:38.080
the way to go, whereas for a large image data set, you may want to go with deep learning,

13:38.080 --> 13:43.760
but still you could apply a machine learning classifier as a baseline, like logistic regression

13:43.760 --> 13:48.240
or something like that. So in that case, with that book, having both machine learning and deep

13:48.240 --> 13:53.600
learning in there, I think it is kind of like a reminder to people, hey, it's not always about

13:53.600 --> 13:58.240
deep learning, also consider, let's say, sometimes machine learning make, making sure that people who

13:58.240 --> 14:03.120
say have their first encounter with machine learning and deep learning, that they know the big picture

14:03.120 --> 14:10.560
basically, that there is more than just new networks also. You mentioned earlier just the proliferation

14:10.560 --> 14:18.400
of frameworks in ML and DL. Do you, how do you advise folks when they're thinking about projects,

14:18.400 --> 14:22.720
you know, where to start, what frameworks and tools choose that kind of thing? Oh, yeah, there is

14:22.720 --> 14:28.960
also a big question. I think, I mean, again, there is no right or wrong, I would say there are

14:28.960 --> 14:35.840
just different tools for different tasks. Personally, I started with, actually, I implemented a lot of

14:35.840 --> 14:40.800
things from scratch because this is with learning experience, but in the real world, I advise people

14:40.800 --> 14:47.120
to use something that is well supported, well developed because it's not only about, let's say,

14:47.120 --> 14:52.800
having it back free or error free, but also about efficiency. So, and there are also, if you use

14:52.800 --> 14:57.760
something that is out there, there's also usually a wider ecosystem and a community that you can

14:57.760 --> 15:03.520
ask for questions and help, which I think is also very important because nowadays, there are

15:03.520 --> 15:09.200
millions of algorithms and it's hard to find sometimes which one is, let's say, the right one,

15:09.200 --> 15:16.000
and sometimes just asking people, chatting about it is helpful. And yeah, for regular machine learning,

15:16.000 --> 15:20.880
that is not deep learning, I would advise scikit-learn still. I think it's the, maybe most mature

15:20.880 --> 15:26.480
library out there. People in my department may disagree because my department is small, let's say,

15:26.480 --> 15:34.320
R based the language R, but I personally, I think going with Python, if you want to do machine learning

15:34.320 --> 15:38.400
or deep learning is the way to go. So, one consideration is, yeah, which programming language,

15:38.400 --> 15:45.280
I think nowadays, 95% also is done in Python. And then, scikit-learn for machine learning,

15:45.280 --> 15:50.960
and deep learning becomes a little bit trickier because we have a lot of more frameworks out there.

15:50.960 --> 15:57.920
So, back then, I remember my first book, I covered Ciano, which I think is still around the people

15:57.920 --> 16:04.160
at PMC3. I think they adopted it and still maintain it. But I don't think people use it for

16:04.160 --> 16:10.640
deep learning anymore. So, I think it was like 2000, might be 15, 16, where TensorFlow came around,

16:10.640 --> 16:16.400
which is maybe the first big breakthrough library for deep learning, where it became really,

16:16.400 --> 16:21.200
really popular. And then, yeah, there were other libraries like MXNet, Chainer, and so forth,

16:21.200 --> 16:26.720
and then eventually PyTorch in 2017. And I think nowadays, when I look at the trends on

16:26.720 --> 16:32.240
papers with code and just reading research papers and looking out there, I feel like I would say,

16:32.240 --> 16:39.440
like 80% now is a PyTorch. And I would say with PyTorch, it's a two-edged sort. It's a little bit

16:39.440 --> 16:45.280
more verbose, let's say, then carous. But at the same time, for me, it makes me more productive

16:45.280 --> 16:49.600
in my research. I wouldn't say it's necessary better for everything, but I feel like personally,

16:49.600 --> 16:55.600
it strikes a nice balance between giving you all the tools you need, but giving you also some

16:55.600 --> 17:00.160
control over what you're doing in case you need to modify something. Because, yeah, in my research

17:00.160 --> 17:06.240
projects, unless it's just an application, I sometimes want to develop my own custom layer,

17:06.240 --> 17:12.160
like a custom output layer or custom loss function. And PyTorch is very fast for that. And it is

17:12.160 --> 17:18.240
a little bit, I would say, more low level. So there are also other APIs on top of that,

17:18.240 --> 17:22.240
and there are a lot of APIs on top of that. But in general, I would say for deep learning,

17:22.240 --> 17:25.280
I would personally recommend a PyTorch. I don't know this.

17:25.280 --> 17:31.920
Yeah, it's interesting that you mentioned our, I don't think I've covered our a lot on the podcast,

17:31.920 --> 17:38.960
but we did have this really awesome panel that we did that was exploring what's the best

17:38.960 --> 17:43.840
programming language for machine learning. And we had representatives from, like, I think,

17:43.840 --> 17:51.200
eight, it was like closure and JavaScript and Swift and Scala and Julia. There's certainly a ton

17:51.200 --> 17:56.400
of different, what you can do, you can do machine learning and a lot of different languages.

17:56.400 --> 18:03.120
And starting with what you know is always a good place or not a bad place at least, but the

18:03.120 --> 18:09.280
Python ecosystem is certainly very strong. If I don't, if you don't mind, I can tell you two

18:09.280 --> 18:16.480
anecdotes. So personally, so the first thing is when I was a student back then, I also started

18:16.480 --> 18:23.840
with R before I started Python. And I was doing most of my statistics stuff and plotting in R.

18:23.840 --> 18:30.560
And then I ended up in the computational biology context. I ended up having to process some custom

18:30.560 --> 18:36.080
data. And that's where I learned Python just for the data wrangling. I learned a little, a

18:36.080 --> 18:41.840
lot of, I learned about Pearl, but Pearl was a little bit unwieldy for me. So I heard Python is

18:41.840 --> 18:48.320
the hot new thing. So I learned Python. And I was writing these really weird scripts. I had

18:48.320 --> 18:54.240
like a batch script that was calling Python for processing the data and then ingesting it into R

18:54.240 --> 18:59.280
to make the plots. So for a long time, I just used R for making plots, but the rest wasn't Python

18:59.280 --> 19:04.720
and then eventually I switched. But one thing you mentioned is about also deep learning and machine

19:04.720 --> 19:11.680
learning in R. I was recently at a seminar at our university where there was a talk on, it was

19:11.680 --> 19:18.560
in general about like a machine learning industry. And the person also presented, I don't want to say,

19:18.560 --> 19:23.280
like names or anything, but it was kind of funny. It was like more like a fun thing. The person

19:23.280 --> 19:30.240
mentioned that they were training model in TensorFlow. And they had presented the results in a

19:30.240 --> 19:36.000
conference in an R related conference. And I was like, how does, wait, you trained this model

19:36.000 --> 19:41.120
in TensorFlow, but you presented in the conference that is related to R. How did you do that? And then

19:41.120 --> 19:47.360
the person said, basically, don't tell anyone, but I actually, I used just the API. I did it in

19:47.360 --> 19:52.080
Python and just use the R API or something like that so that it can be submitted to the conference.

19:52.080 --> 19:58.480
But it was another hood basically a TensorFlow and Python and stuff like that. Now I see why you

19:58.480 --> 20:06.000
were naming names. It was kind of funny though. It's like, I mean, R is still, I mean, I think it

20:06.000 --> 20:11.200
has its place for sure. I think it's very user friendly, especially for statistics. And as soon as

20:11.200 --> 20:15.840
you need to do some, I don't know, just hypothesis testing and statistics, there is of course stats

20:15.840 --> 20:21.520
models, but it is just so many extra steps. And I feel like R comes with a lot of batteries included

20:21.520 --> 20:25.680
when it comes to statistical modeling. So I think it definitely has its place. And of course,

20:25.680 --> 20:31.760
like you mentioned, Julia, I also have a few colleagues who work in Julia and my colleagues

20:31.760 --> 20:37.280
who use Julia, they really love Julia. It's like, I think that's, it's a really nice language.

20:37.280 --> 20:41.840
It's just for deep learning. I think it hasn't caught on yet. And I think it's maybe like a

20:41.840 --> 20:46.960
chicken egg problem where you don't have the community in Julia for deep learning. And without

20:46.960 --> 20:51.600
the community, you don't have, let's say, the libraries that are working on frameworks and

20:51.600 --> 20:56.880
yeah, I think I mean, it's possible for sure. And another name I don't want to mention, but I was

20:56.880 --> 21:03.440
on a committee, a PhD committee, and the student did some research on deep learning, like developing

21:03.440 --> 21:08.240
some custom methods and was using Julia for that. And in the end, the student mentioned to me, I

21:08.240 --> 21:14.000
wish I had used PyTorch because it made certain things more elegant if you wanted to do things

21:14.000 --> 21:18.960
from scratch because you have this array type in Julia and so forth. But overall, just the leg of

21:18.960 --> 21:23.440
the framework, I think they have a library. I forgot the name. It's not blocks. I forgot the name.

21:23.440 --> 21:30.080
But it is not as mature, let's say. And then once you want to compare your methods to other

21:30.080 --> 21:36.400
methods, you bump into problems because yeah, you can either compare across languages or you have

21:36.400 --> 21:41.040
to re-implement everything. And I think that is also one important consideration you want to maybe

21:41.040 --> 21:46.240
look at what other people are using, not because you want to copy them, but you want to maybe also

21:46.240 --> 21:51.760
compare your methods, especially if you want to develop a new research method. It's good to have

21:51.760 --> 21:57.520
something out there that is also useful to other people. So if let's say other people you are

21:57.520 --> 22:02.000
using your framework, it's a higher benefit for the community if you contribute it in that

22:02.720 --> 22:06.800
language. But then also if you want to make sure you make progress in research, it's easier to

22:06.800 --> 22:12.080
compare. If you make a small change to the loss function, it is easier to compare that to the

22:12.080 --> 22:16.000
reference in the same framework compared to let's say another framework because then you don't know,

22:16.000 --> 22:21.440
is this because of numerical approximation? Is it due to something else's improvement? So yeah,

22:21.440 --> 22:25.520
I think there's a lot of benefits going with what everyone else is using, although

22:26.240 --> 22:31.520
I usually try to do something different just because it's exciting, but there are definitely

22:31.520 --> 22:37.680
benefits, yeah. Maybe a little bit more about the book, have you, did you find a way to incorporate

22:37.680 --> 22:46.320
this idea of projects into the book? That is a good question. Unfortunately, no, there are only like

22:46.320 --> 22:53.440
more like toy projects. The problem is, I think it's really hard to do. I mean, there have been some

22:53.440 --> 23:00.560
great, not necessarily machine learning books that I'm thinking of, but just computer programming

23:00.560 --> 23:05.920
books that the ones that I like are the ones that I like the most are the ones that kind of take this

23:05.920 --> 23:11.760
longitudinal project approach and it kind of flows from the beginning to the end and adds,

23:11.760 --> 23:17.440
you know, each chapter will add on to the project, but I've got to imagine that that's really,

23:17.440 --> 23:22.560
really difficult to do. Yeah, yeah. Now that you mentioned it, I remember, I was last year reading a

23:22.560 --> 23:27.520
book, Introduction to PyTorch, I think, or Introduction to Deep Learning with PyTorch by E. Life

23:27.520 --> 23:33.040
Stevens, Luca and Tiga, and Thomas Feren. And they had something I could describe. The first

23:33.040 --> 23:37.840
portion was an introduction to PyTorch, and then the second part of the book was a very long

23:37.840 --> 23:42.640
example explaining, I think it was an MRI example, like object detection and these types of things,

23:42.640 --> 23:46.960
but it was like this huge project that they walked through. And I think this is very valuable because

23:48.080 --> 23:54.720
there's not much stuff like that out there. But one thing you mentioned about involving people

23:54.720 --> 24:01.040
in the project, the problem here is really how you scale that up, because in my class, I had

24:01.040 --> 24:06.960
usually 70 students, and I was just at the limit of my capacity. When the students submitted their

24:06.960 --> 24:12.320
proposals, I spent multiple weeks reviewing them. There were only two pages each, but if you have

24:12.880 --> 24:20.080
70 students, groups of three, you have like 24 groups, approximately 23, 24 groups, and yeah,

24:20.080 --> 24:26.320
reading 23, like short papers, and then thinking about them, giving feedback, that is a lot of time,

24:26.320 --> 24:30.720
and then the same thing for the final project, which was in the format of the eight page paper,

24:30.720 --> 24:36.320
a conference paper. Yeah, this keeps you busy, especially if you want to also

24:36.320 --> 24:40.560
during the semester provide feedback. So with a book, I can see the challenge is,

24:41.280 --> 24:45.760
yeah, you can describe a project, but it is hard to give feedback if people are,

24:45.760 --> 24:50.800
it's if you give an open ended exercise or something like that. But I think maybe that's

24:51.360 --> 24:55.520
that's what Kaggle competitions are almost for, where you have also this community around it,

24:55.520 --> 24:59.920
where people work on a similar project, and then they can help each other with feedback and

24:59.920 --> 25:06.560
so forth. But yeah, I think projects are very powerful, but there's still like the issue of how to

25:07.760 --> 25:13.520
how to help people with that, like in terms of having the time and capacity for that.

25:13.520 --> 25:17.600
You mentioned PyTorch Lightning earlier. Can you talk a little bit about that, and

25:18.560 --> 25:24.480
you know, what it is, how you use it? Yeah, so PyTorch Lightning, I must say, I just started using

25:24.480 --> 25:31.120
it recently earlier this year. It is essentially, I wouldn't call it a framework. It's more like a

25:31.120 --> 25:37.760
platform. So it is in a way an API that organizes your code, but it is more than just a framework.

25:37.760 --> 25:42.880
It's like, I would call it more like a platform because it helps you integrate other technologies as

25:42.880 --> 25:48.800
well. So it kind of brings together multiple things because when we do like deep learning now,

25:48.800 --> 25:53.840
what I usually tend to do in my research projects even, I try to write everything from scratch.

25:53.840 --> 25:59.200
I was, I mean, using PyTorch, but then I had my training loop. I had like a function that iterates

25:59.200 --> 26:04.720
over the data set to compute the test accuracy because you can't load the whole data into memory

26:04.720 --> 26:09.920
because it's too large. And then also logging. So I had my own logger where I was writing to a

26:09.920 --> 26:15.120
CSV file, for example, or sometimes using weights and biases or even just a tensor board.

26:15.680 --> 26:21.200
And yeah, I was just putting everything together myself. And I think this works if you work alone,

26:21.200 --> 26:26.160
but then also I noticed like three months later coming back to the project. I had like 20 helper files

26:26.160 --> 26:31.120
that I was importing from. I wanted to change something. It was like a mess. So and also when I was

26:31.120 --> 26:35.920
collaborating with students, if you have your custom code, it makes sense to you, but it doesn't

26:35.920 --> 26:43.760
make sense to anyone else. So PyTorch Lightning, yeah, it's basically, it's not much in a way that

26:44.960 --> 26:49.600
it's not much different, let's say, than PyTorch. It's more like on top of it, but it helps you

26:49.600 --> 26:55.760
integrate different other tools without having to, let's say reinvent the wheel. So what you do

26:55.760 --> 26:59.600
is essentially you can still have your regular PyTorch model. So you don't change anything about

26:59.600 --> 27:04.640
that model. It has your regular forward method and so forth. And then you have a Lightning module.

27:05.120 --> 27:10.000
And this is like a class that wraps around the PyTorch module. But when you do that, you

27:10.000 --> 27:15.120
you have still full control. You define, okay, how does my forward step look like? So the forward

27:15.120 --> 27:21.200
step for training or testing. And you define an optimizer. And then you have a trainer class.

27:21.520 --> 27:26.320
And this is it. And what's nice about it is in a trainer class, you can specify what type of

27:26.320 --> 27:31.280
logger you want, like weights and biases, tensor board, simple CSV logger. And what's really

27:31.280 --> 27:36.640
powerful is you can specify how many GPUs you want. So I mean, you can do this in PyTorch, but it's

27:37.280 --> 27:43.200
much more extra work. And I never really got it to work myself. So right now, for my research

27:43.200 --> 27:47.680
projects, I was just using the same code I had was wrapping it around a Lightning module. And I

27:47.680 --> 27:52.640
was just training it on multiple GPUs. So in that case, I mean, it's also fully open source. And

27:52.640 --> 27:57.040
if you want, you can still access the original PyTorch model. It's really just a wrapper around

27:57.040 --> 28:01.520
it that that gives you certain things for free, which is nice. I'm curious what your sense for

28:01.520 --> 28:12.080
for PyTorch usage beyond. Is it still primarily research oriented? Do you have a sense or visibility

28:12.080 --> 28:17.840
into whether it's seeing broader adoption in industry and commercial context? I still see a lot

28:17.840 --> 28:24.640
of tensor flow-out in that context. Yeah, that is an excellent question. I honestly, because I'm

28:24.640 --> 28:30.960
coming more like from an academia background, I must say I haven't really deployed anything

28:30.960 --> 28:38.880
myself yet. But talking to colleagues, I think it really depends on the company you work on.

28:38.880 --> 28:44.240
I mean, some people similar prefer one cloud provider over the other. It's one framework over

28:44.240 --> 28:50.240
the other. But there is no, I think big limitation of using PyTorch anymore for, let's say, deployment.

28:50.240 --> 28:56.480
So I'm not super familiar with how things work under the hood. Sometimes I look at the source code

28:56.480 --> 29:03.280
and it's really scary for me, like seeing all the files and stuff. But so how I understand it now

29:03.280 --> 29:11.040
is that you have Python where you run PyTorch in primarily. And the bottleneck of using Python

29:11.040 --> 29:16.800
is just like 10%. If you would remove Python, just run the code without Python, it would be maybe

29:16.800 --> 29:22.880
10% faster. That's not much of a difference. And they have two different ways you can go from that

29:22.880 --> 29:29.040
to C code. One is, I forgot the name is actually one is essentially tracing your code where it's

29:29.040 --> 29:34.880
really a static graph from that where you, if you have a followup, it gets unrolled is this

29:34.880 --> 29:39.200
static one. Like, and then they have the other approach, which is, I think it's called Torch

29:39.200 --> 29:46.480
script, where you go from this Python API to a, I think it's called lip torch, which is like

29:46.480 --> 29:52.080
the C++ API. And that one can be just used anywhere. I mean, I think they have a lot of tools

29:52.080 --> 29:57.120
in the recent versions, also for mobile deployment and stuff like that. So I think to be honest,

29:57.120 --> 30:04.160
there is no really bottleneck, no big bottleneck anymore, like using it for serious applications.

30:04.160 --> 30:08.480
And then also you have all the quantization things to make it faster.

30:09.680 --> 30:15.680
Yeah, so I think this is all really like what they focused on last year to make it more

30:15.680 --> 30:22.880
deployment friendly. One more thing just comes to mind is Onix, like the ONNX format, where

30:22.880 --> 30:29.520
you can also export PyTouch models to ONNX. And I think you can then also use it in like the Apple

30:29.520 --> 30:36.000
framework, I think metal and core ML. But this is something to be honest, beyond my comfort zone,

30:36.000 --> 30:41.120
I'm more like let's say research education. I am not really someone who is deploying applications.

30:41.120 --> 30:45.920
So yeah, let's, let's maybe switch gears and talk a little bit about your research.

30:45.920 --> 30:54.880
You're most recently you've been focused on ordinal regression among other topics. Can you

30:54.880 --> 31:01.280
share a little bit about that field and why you find it interesting and kind of what the research

31:01.280 --> 31:07.600
frontier is there? Yeah, that is a good point. So odd another regression, it's maybe an abstract

31:07.600 --> 31:13.520
term, but how we can think about it is how do we use methods when in a supervised learning context,

31:13.520 --> 31:19.360
when the class tables have a natural order. So usually when we teach or use machine learning,

31:19.360 --> 31:23.840
we have like these two different scenarios where one scenario is classification, let's say

31:23.840 --> 31:30.400
Iris flower classification, we have Satosa, Versicolor and Reginica, but we can't really say one is,

31:30.400 --> 31:34.480
let's say, Versicolor is bigger than Reginica, there's no order, it's just independent class

31:34.480 --> 31:40.560
tables. And then the other type is regression, where we have for example, I don't know house prices

31:40.560 --> 31:47.280
or something like that, where you have numeric or continuous target. And ordinal regression

31:47.280 --> 31:52.640
sits somewhere in between where we have something that looks on the surface like classification

31:52.640 --> 31:57.200
problem, but it has an order. And so the class tables have an order, for example,

31:57.200 --> 32:02.560
on Amazon customer ratings, where we have one, two, three, four, five stars. And I mean, it could

32:02.560 --> 32:06.800
also be kind of like a regression problem, but the difference is really we don't know

32:06.800 --> 32:12.960
where we can't quantify the distance between things. And we can say, okay, one in two stars and

32:12.960 --> 32:19.920
four and five stars, it is one star difference, but yeah, it is a little bit more tricky than that,

32:19.920 --> 32:24.160
because it's hard to compare a one to a two star review to a four and a five star review. It's

32:24.160 --> 32:28.400
it's hard to quantify this distance. And the same thing is true like for let's say other things

32:28.400 --> 32:36.240
like disease, if you have a scale between no disease, medium or mild disease and severe disease,

32:36.240 --> 32:42.320
it's hard to put a label on it how different these two distances are. And this is really where

32:42.320 --> 32:47.920
you don't want, so ordinal regression is where you don't want to or can't quantify the

32:49.200 --> 32:53.920
distance between categories, but at the same time, you have ordering. You can say

32:55.520 --> 33:01.920
that, for example, no disease is less than moderate than less than severe. That's like an ordering.

33:01.920 --> 33:07.120
Would you specifically use it? I'm thinking about it in contrast to like trying to

33:07.760 --> 33:16.720
attack a regression problem where you're concerned about integer values would or no regression

33:16.720 --> 33:23.840
be used when you've got a much smaller set of labels or can use it if your set of labels is

33:23.840 --> 33:28.480
relatively unconstrained and you're just really trying to focus on integers. Yeah, that is a good

33:28.480 --> 33:35.920
point. Actually, there is no limit to the number of classes. So in our first paper, we focused on

33:35.920 --> 33:44.160
age classification where we had 70 different age labels like from one to 70 years. And also,

33:44.160 --> 33:48.160
we thought, okay, age, I mean, this could be modeled with a regression model, but it's a little

33:48.160 --> 33:53.360
bit trickier than that because if you think about a person who is, let's say, 10 years old and the

33:53.360 --> 34:00.000
person who is 15 years old, there's a lot of change that takes place when a person becomes older

34:00.000 --> 34:06.720
in that five year time frame compared to, let's say, a person who is 80 and 85, where in this

34:06.720 --> 34:12.560
age frame, maybe the texture of the skin changes mostly, whereas for a younger person, it's maybe

34:12.560 --> 34:20.400
more like the growth, like the bones change and so forth. So in that case, you can use it with any

34:20.400 --> 34:26.720
type of labels, but yeah, you're really flexible with that. So, but what I feel like was

34:27.440 --> 34:33.600
people, or let's say, when I look for tutorials or anything like that, there has not been much

34:33.600 --> 34:41.280
attention, or people were not really providing, let's say, help or tutorials or methods for how to

34:41.280 --> 34:45.440
do that with deep learning. There is the classical statistics literature where we have

34:45.440 --> 34:50.320
ordinary regression models, but nothing really for, let's say, deep neural networks. So if you have

34:50.320 --> 34:55.600
an image data set and you, let's say, want to assess the damage to a building, you can't really say,

34:56.240 --> 35:00.640
so if you have like a collapsed roof versus like a scratch or something like that, it's a very

35:00.640 --> 35:06.240
type, a different type of difference compared to, or it is hard, let's say, to quantify things

35:06.240 --> 35:11.280
sometimes. You can try to put numbers on it, but really there are a lot of problems where there are

35:11.280 --> 35:16.880
no numbers that you can put on it, but you still want to, let's say, try in a classifier to recognize

35:16.880 --> 35:22.320
more severe damage compared to moderate damage or no damage, let's say, in terms of insurances or

35:22.320 --> 35:29.120
buildings and so forth. And yeah, so we have been focused on developing new networks for that,

35:29.120 --> 35:33.600
but then also the challenge is you don't want to, let's say, develop a completely new type of

35:33.600 --> 35:38.800
neural network, because then it's really hard for people to use that and compare to other methods.

35:38.800 --> 35:45.520
So the focus was essentially what are like small changes that we can make to modify an existing

35:45.520 --> 35:51.840
classifier such that it becomes an ordinary regression model. I think this is really cool because

35:51.840 --> 35:56.880
that allows people ready to take something they already trained and then just change a few lines

35:56.880 --> 36:02.080
of code and see if it becomes better. If it doesn't become better, okay, maybe I spent five minutes

36:02.080 --> 36:06.560
making that change, no big loss, but maybe it makes things better and then I think it's a huge

36:06.560 --> 36:11.680
win when people can just improve their model without having to spend a lot of time developing

36:11.680 --> 36:16.240
something completely new. Yeah, and so there are, I'm not sure if you're interested, there are a

36:16.240 --> 36:25.040
couple of methods for that I could talk about. Sure, yes, okay. One method was from 2016.

36:25.040 --> 36:32.320
It's not by our group, but it was published in CBPR. We call it just the order regression

36:32.320 --> 36:38.400
network by new at L. Yeah, and so how they tackled that problem was by, it's something called

36:38.400 --> 36:46.160
extended binary classification. So they take a class label, let's say you have five different classes

36:46.720 --> 36:52.320
and you have the classable three. So what they would do is they would extend this integer number

36:52.320 --> 37:00.880
into five or four zeros and ones. So you turn this multi-category classification problem

37:00.880 --> 37:06.320
into multiple binary classification problems and then you are predicting, is my label greater than

37:06.320 --> 37:11.200
one? Is my label greater than two? Is my label greater than three? So you have a lot of, or you have,

37:11.200 --> 37:15.920
let's say, in this case, four different yes and no questions and then you can just,

37:17.200 --> 37:22.720
and so if the classable is on three, you answer, classable is greater than one, yes, greater than two,

37:22.720 --> 37:28.880
yes, greater than three, no, and then you can, based on that, sum up the ones, add in one to it and

37:28.880 --> 37:34.640
end up with the label. And each problem is then modeled as a binary classification task,

37:34.640 --> 37:39.840
so we can use something we are familiar with like the logistic loss function on the binary

37:39.840 --> 37:45.360
cross entropy loss and then sum up these binary cross entropy losses. And so this worked really

37:45.360 --> 37:49.920
well in that paper when they have that. Sounds a bit like an extension to one hard encoding for

37:49.920 --> 37:56.240
categorical variables. Yeah, yeah, it is kind of like that, right? So except you have, you can have

37:56.240 --> 38:00.400
multiple ones basically instead of just one one, right? Yeah, but it is kind of like an encoding,

38:00.400 --> 38:05.120
right? Right, right. You turn this problem into a multiple binary classification task.

38:05.120 --> 38:11.440
The one little problem with that was when you do that, you can have like rank inconsistencies.

38:11.440 --> 38:17.120
So what happens, let's say in an age, a prediction problem, you can predict that the person is older

38:17.120 --> 38:25.120
than 41, not older than 42, but older than 43, which is like conflict. How can a person be not older

38:25.120 --> 38:31.360
than 42, but then older than 43? So yeah, in our work, we just, yeah, it was like a little bit

38:31.360 --> 38:37.280
of math. We did there. And then we had like a small tweak to prevent this rank inconsistency.

38:37.280 --> 38:43.360
And we found that this also, yeah, improves prediction performance by a lot. And it's a really

38:43.360 --> 38:49.360
small change. And all together with this method, we call that a coral, C-O-R-A-L. And

38:49.360 --> 38:57.280
a sense for consistent rank logits. So with that, you only have to do two small changes.

38:57.760 --> 39:03.120
So you have to change the last layer. There's like a constraint. We have in the last layer a

39:03.120 --> 39:08.960
weight sharing constraint. But we, I have like a PyTouch package, you can just import the layer

39:08.960 --> 39:13.360
and just use that. And then the other one is the loss function. And this is really it. So there

39:13.360 --> 39:18.640
are like two little changes to the code. Maybe the binary extension could be also considered as a

39:18.640 --> 39:23.840
change to the class table. But it's really something that you can do in five minutes. And then you

39:23.840 --> 39:30.560
have instead of a classifier, an auto regression model. And recently, we have another method called

39:30.560 --> 39:37.120
corn, C-O-R-N, which is taking this to another level. It's a bit more flexible. It has

39:37.120 --> 39:41.360
better performance than coral. It is a little bit more, you have to be more careful because

39:41.360 --> 39:45.040
with more, let's say, power comes more responsibility. So it's easier to overfit.

39:45.040 --> 39:51.200
But yeah, so these two methods, what I like about them is really they are very easy to implement.

39:51.200 --> 39:56.240
And everyone can use them. If you're using a classifier, you can just change a few lines of code.

39:56.240 --> 40:02.720
And yeah, you have an auto regression classifier. Maybe to close things out, we'll return a

40:02.720 --> 40:11.120
little bit back to education and talk about your recent role at Grid AI and kind of what you

40:11.120 --> 40:17.760
have planned there. Oh, yeah, that is a big question. A small short question at first glance,

40:17.760 --> 40:25.200
but there is a lot of things behind it. So yeah, I have been recently joined a Grid AI,

40:25.200 --> 40:31.440
which is focused on deep learning at scale. My role there is, though, lead AI educator,

40:31.440 --> 40:38.000
where I'm developing educational materials. So I'm essentially just doing what I love doing. I'm

40:38.000 --> 40:43.760
developing material to explain machine learning and deep learning to people. And in a sense,

40:44.400 --> 40:51.040
what I felt like also is I like teaching at the university, but also, let's say, going the next step,

40:51.040 --> 40:56.960
maybe having a more like online base or like a course that is accessible to everyone,

40:57.600 --> 41:03.680
my plan is to develop a free course that people can take. There's no restriction, nothing,

41:03.680 --> 41:09.280
it's a totally free course. And also, let's say, nicely produced with where I get to focus on,

41:09.280 --> 41:14.000
let's say, making this really nice and also involving the community with feedback. So yeah,

41:14.000 --> 41:19.600
that is what I'm currently working on. It will be, firstly, yeah, my first goal is to have

41:19.600 --> 41:25.600
something out maybe later this year focused on PyTorch. Maybe also PyTorch lightning, so

41:25.600 --> 41:29.360
something around that. So I'm currently working on that. And yeah, I'm really excited because

41:29.360 --> 41:36.880
I think that's like my passion. I wrote a book recently, but I also, now that I have a book,

41:36.880 --> 41:41.520
let's say, going back to the course development, developing courses. But I really like about that,

41:41.520 --> 41:48.560
it's also thinking it through. It's like, it is something where you get creative and you think

41:48.560 --> 41:55.040
about, okay, how should I cover what? And yeah, in the past, that was always like, I think you

41:55.040 --> 42:01.280
have to do it in order to see what is like the pro and kind of introducing a topic in a certain order.

42:01.280 --> 42:07.600
And this now offers me another attempt doing that, like seeing how I can structure a course and how

42:07.600 --> 42:12.000
I can develop a course. And of course, yeah, tinkering with code. This will be also very

42:12.880 --> 42:18.000
code-focused course. And hope I can also develop good exercises because I think it's also very

42:18.000 --> 42:22.640
important. There's a lot of material out there. But three, learn things. It's important to apply

42:22.640 --> 42:27.680
these things and also checking your understanding with having good exercises. So I'm currently,

42:27.680 --> 42:32.240
yeah, working on developing all of that. And hopefully I will have something by the end of the year

42:32.240 --> 42:36.720
that I can share with you and the community. And yeah, you can tell me what you think.

42:36.720 --> 42:43.280
We'll be keeping an eye out for it. In the meantime, thanks so much for joining and sharing a

42:43.280 --> 42:47.840
bit about what you've been up to. It's been great finally meeting you. Thank you. That was a

42:47.840 --> 42:53.920
total fun episode here. And I think, yeah, I could go on forever, but yeah, it was nice

42:53.920 --> 42:59.360
chatting with you. And I enjoyed it. And yeah, whenever you feel like it, I'm always open to

42:59.360 --> 43:27.520
talk more.

