All right, everyone. Welcome to another episode of the Twimble AI podcast. I'm your host,
Sam Charrington. And today I'm joined by Doina Precup. Doina is a research team lead at Deep
Mind Montreal and a professor at McGill University. Before we get going, be sure to take a moment to
leave us a five-star rating and review wherever you're listening to today's podcast. Doina,
this conversation has been a while in the making and I'm super excited to have you on the show
and looking forward to digging into your research into reinforcement learning. Welcome.
Thank you very much. I'm really excited to be on your show and yeah, big fan of your podcast.
So looking forward to our conversation. Awesome. Awesome. Same here. Let's get started. As you know,
we always do by having you share a little bit about your background and introduce yourself to
our audience. Yeah, so I split my time between Deep Mind in the Montreal team as well as McGill
University where I've been a professor since 2000. Before that, I did my PhD at University of
Massachusetts Amherst. Working on reinforcement learning. I was actually lucky enough to take the
first version of the reinforcement learning course taught by Andy Barton and Rich Satin out of
their textbook in 1995 and they got me hooked to the field and I've been working on it ever since.
Fantastic. And what prompted your interest in RL? I really found it a good way to think about
artificial general intelligence. So agents that can learn how to do many different things in an
open-ended environment. And it's really because we have on one hand rewards that give us a way to
express the task that the agent should do. And on the other hand, we really have learning from
interaction rather than let's say learning from being told what to do or not really having a clear
goal. And so it seemed to me to be the right sort of balance between having some interesting
signal in the reward but also having this ability to interact and explore and really learn freely
in the environment. RL is increasingly a broad field. So let's dig a little bit into your research
and have you share a bit about what you're most excited about. So one of the things that I've
worked on for a long time and I'm always excited to think about it is hierarchical reinforcement
learning. This is really about learning abstract representations, especially abstraction over time
because oftentimes in reinforcement learning the problem is phrased at a very small time scale,
very fine grain time scale sort of on the order of let's say muscle twitches. But really to
self-complicated problems you need to think in terms of longer time scales and variable time scales.
So for example, if you're cooking a meal you're not thinking about all the muscle twitches involved
in stirring or putting a pot on the stove you're thinking in terms of larger steps. What ingredients do
I need? Do I need to go to the store to get these ingredients? And so we reason at many levels
of abstraction both in terms of the time scale of our actions as well as in terms of the states and
features that we use. And I would really like reinforcement learning agents to be able to do
the same to really leverage abstraction and learn these abstractions from their interaction stream.
So I've been doing a lot of work in this respect in terms of for example learning a framework
for temporal abstractions called options, learning models that look at different time scales,
understanding how to use these models in planning algorithms and also understanding when agents
can use what actions, what we call affordances. And so this is a long history and a long series
of papers that is associated with it. But one of the important open questions that I still struggle
with and think about is how do agents decide which abstractions to learn about? For us maybe it's
easy. We think about objects for example and that somehow comes naturally but for reinforcement
learning agents this is still something that I would like them to acquire automatically.
Awesome and we'll return back to hierarchical RL. You also spend a lot of time working on
reward specification for RL agents. Can you tell us a little bit about that work?
Yeah so rewards are actually really important because they determine what the agent is really
learning about and thinking about. And actually David Silver, Rit Sutton, Zitinder Singh and I put
out the paper in 2020 which received quite a bit of attention both positive and negative I think
called reward is enough. It really talks about this hypothesis that a reward signal, in fact a
simple reward signal in a complex environment could really lead an agent to develop all the
interesting attributes of intelligence that we might sort of intuitively think about. So
for example in this paper we discussed squirrels. Squirrels have a perhaps a simple reward
function. They like to eat nuts because that helps them to survive. But in that process they
actually develop a lot of interesting abilities like the ability to remember where they put nuts
before and the ability to predict the time of year when these are going to be available and
the ability to deal with other squirrels and maybe you know deceive them or make sure that they
don't get to the hidden stash of their own nuts. And so that's an interesting set of abilities
that involves memory and planning and modeling aspects of the world that can all be seen as
developed from this maximization of reward. And so we were asking ourselves in this hypothesis whether
it could also be true that general age day agents would develop these kinds of abilities also
from maximizing rewards. It's a hypothesis and so you know as any hypothesis it's interesting to
think about how might we might you know confirm or dismiss it. And so following that we did some
technical work trying to understand how we can take sort of intuitive task specifications and
translate them into rewards for reinforcement learning agents. And when is this actually possible?
So this was a paper called on the expressivity of Markov reward that was led by David Able and with
several fantastic collaborators at DeepMind and also at Brown University. And it received the best
paper award at Europe's last year so that was also very wise actually when we when we wrote the
paper we weren't sure what people would make of it. But we were quite excited to see the positive
reception. And really in the paper we aimed to make this hypothesis a bit more concrete and to
try and understand from a mathematical point of view when do Markovian rewards which are a special
class of reward functions capture intuitive notion of tasks. Got it. Let's maybe start with the general
case of rewards. The hypothesis is that you know using this mechanism of reward can lead to all sorts
of intelligence capabilities behaviors. How do you characterize that in a machine learning context?
You know what how do you draw the analogy from the squirrel to the machine learning?
So in the paper we thought of essentially a framework where you might imagine somebody who has a
task in mind let's call this Alice and then you have a learner called the learner Bob and so Alice
has has a task specification in mind and now has to translate this into a reward signal that Bob
would get. So Bob is a usual reinforcement learning agent it inhabits a Markov decision process
this means that on every time that Bob observes the state of the environment can do some action
and then we'll receive as a response an immediate numerical reward signal and there will be a transition
to an X state and there's some discount factor that the values rewards that are received too far
in the future and Alice might have something more intuitive in mind so for example Alice might
have a preference over certain ways of behaving so if Bob is an agent that's in a grid world
navigation task for example Alice might prefer that Bob gets to a designated goal location quickly
but she might also prefer that Bob doesn't step into lava and get burned and so this
imposes now a preference over the set of policies that Bob might have the policy being the mapping
from state to actions and now we can think of no the task specification in Alice's head as being
this preference over the space of policies the reward function is the usual Markovian reward
which is associated with states and actions and we want to understand can Alice efficiently compute
a reward that captures her preferences and so now we can think a little bit about you know
what kind of preferences she might have and how stringent they might be so one kind of preference
which I described is over policies and we can have that be very strict or we can relax it a
little bit we can say you know Alice certainly thinks that some actions are bad like stepping into
lava is always bad but otherwise has no strong preferences in in other situations so whether
you know Bob takes one route or some other route doesn't really matter so long as he doesn't get
burned so that that leads us to ranges of policies or sort of sets of policies being acceptable or
being superior to others and we can also think in terms of trajectories so Alice might consider
trajectories that Bob might do in the environment and just prefer one trajectory over another right
so if Bob is not trying to run into walls too much maybe that's that's a good thing
so now if we have a set of preferences like this the question is can we actually always translate
them into a reward function so can Alice compute a reward function that is Markovian
and if so what's the complexity of of doing this so the the paper has two aspects to it right
there's a negative result and there's a positive result you know the the negative result is in
some in hindsight not so unexpected but it basically says you can't always find a Markovian reward
Markovian in the state of the agent and intuitively in hindsight it's it's not so surprising because
on one hand there may be states that are unreachable where Alice might have certain preferences
but they're irrelevant and so if you have some disconnected state in some corner and Alice has
some contradictory preferences there let's say in terms of the the actions that can impact
the specification of the reward overall but it's a case that we don't really care about so that's
sort of one mode of failure which is perhaps not interesting would it be an example like if Alice
you know if the maze is you know nominally 20 steps to completion and Alice has a preference
that you get there in three steps like it just can't be done that's right so this kind of thing is
you know it's impossible and and therefore you know the math says yes yes it's impossible
there's a more interesting case which is the case of of non-Markovianness so for example
Alice might say I prefer that Bob always go in the same direction so if you're going up or you
should always go up if you're going down you should always go down Markov decisions are ID so you
can't really enforce that so you can't really enforce that if you wanted to enforce that one
possibility would be that you modify the state space right so that we keep track of you know
whether you've been going up and then you know we could we could do a Markovian reward in that but
in this paper we only consider it specifying with respect to the state space that's already there
we didn't consider this larger context of you know trying to modify the states and specify the reward
I think it would be a very interesting avenue for future work to think about how do we go from
observations to a specification that has you know perhaps a new state space and the reward
function that that goes with it but that went beyond the scope of the results that we had in this
paper however one thing that we did show in the paper which is the positive result is that
there is a procedure which is a polynomial time procedure that Alice can use to either return a
reward function that is consistent with her preferences if such a reward function exists
or to determine that no such reward function is possible and the intuition actually of the
of the algorithm is interesting it's based on a linear programming approach
and so the the basic idea is that for any policy we can compute a stationary distribution associated
with that policy stationary distribution over the states of the controlled Markov process
and then we can impose constraints between the policies that our analysis acceptable set
and the fringe policies the fringe policies are identical to those that are in the acceptable set
except at one state they would take a different action and so we can have a set of inequality
constraints the basically say the acceptable policies should be better in value than these
fringe policies and with this kind of linear program either we find a solution and the size of
the linear program is polynomial in the size of the states and actions so we can find a solution
in polynomial time or if the linear program does not have a solution that means that no Markovian
reward function is actually consistent with the original preferences that were expressed.
Is there a notion of like sometimes a linear program is underspecified you've got more degrees
of freedom than you have constraints does that come up in this analysis that's a really
interesting question no in our case we want to find a solution if any solution is is okay
we don't need to find a specific solution and so there will be for example potentially multiple
reward functions that are all consistent with some optimal policy you know for example
scaling of each other and that's that's perfectly fine so long as all of them end up
ensuring that the preferences are satisfied so the good policies are better than than the
others right or that the bad policies are inferior to the others then you know all of these
solutions are are fine and acceptable for more point of view. There's an implicit assumption
that the policy is specifiable kind of mathematically or not the policy sorry the preference
which maybe limits the yeah maybe this is referring back to what we're talking about in terms
of the Markovianness of it but it also kind of speaks to the gap between a preference that's
someone might have in a real-world scenario and then trying to implement it using this method and
how do you get to a mathematical representation of that preference like did you address that at all
on the paper? Yeah so that that's an interesting question and we don't really talk about this in
the paper but I can give you my my perspective on this okay on one hand what you do in the real world
is you have some preferences but you may not have full preferences right so you might for
example in in our grid world say well Bob should never step in lava but you know if Bob has to
go through red squares or green squares I don't really care okay but as you observe behavior
those preferences might actually change right you might start preferring you know the Bob maybe
always steps on green squares because those are soft and you know Bob is like a little toddler
and shouldn't hurt himself right so there these kinds of things may come up over time now our
framework is very much single shot so Alice has a set of preferences they're there from the
beginning she computes the reward and then Bob goes off and optimizes that reward function
I think in practice there's much more of a give and take right there is much more of you know
maybe Alice observes the behavior that might get her to have new preferences right or maybe
revise existing preferences and so I think in in applications one would have to have an interaction
loop where you know Alice observes gives new reward functions Bob continues to optimize over time
and this is you know this continues for a while in order to really get to to the root of what the
task is and you know one thing that I remember for example is when my kids were young you know we
used to reward them for putting away their toys and at some point one of them discovered this
interesting strategy of always taking all the toys out of the torches and putting them back
and taking them out and putting them back you know it's the it's the behavior that gets rewarded
right so it's the classic boat spinning in the middle of the that game yeah so basically now
this is the stage at which one would actually revise what the reward function is by
alerting the quirkiness and the behavior that that is being induced and I think naturally we do
that and our framework in this paper does not does not address that um there's one other aspect
which is also not addressed which is with how good is this reward function in order to get
Bob to learn efficiently so this is not something that we talked about in the paper at all
um we interestingly observed that in the examples that that we showcased in the paper
the inferred rewards did lead to fast learning because you know the reward function is the result
of computing an LP and so it's a dense reward function it's not sparse like what people would
specify in these kinds of tasks and so it does lead as a side effect of that to pretty fast learning
but it's sort of uh it's not clear that that would always happen so the linear program serves
to constrain the search space for the reward function which yields more efficiency yeah exactly
yeah I think my question maybe I'll comment my question a little bit differently the example
you use in the paper is grid world yeah maybe it's the case that any preference about actions
in grid world you know there's a straightforward mathematical representation of those
uh in lots of other you know tasks maybe there's not a straightforward mathematical
representation of the the preferences um you know but I'm I guess I'm supposing that there are
marcovian uh tasks for which the preferences are not easily mathematically represented that
may not be the case but um did you look at like what what tasks or environments this uh all of
the supplies to that's a great question so the paper itself is more of a definitional paper we
were trying to define the problem think about you know what it means to have a a task in mind
and separate that from this issue of how do you then specify it as a reward um there's so it does
assume that you have a marcovian process and the side effect of that is that we have states
um now in the real world we have features right we don't have states but we have some observations
we see images right those are a imperfect representation of what the state might be and that is
not a setting that we have handled in the paper although I think it's really interesting for
for future work so you know Alice might have preferences but might not have access to the state
space she might only have access to observations and Bob also might not have access to the state
space only to observations and this is where uh the framework that we have and the the specifics
of the LP would not carry through I think there is a path to have a similar kind of solution
in this case because generally speaking in reinforcement learning LP formulations have been
used as a as a way to think about the problem of reinforcement learning and there are versions
of these approximate linear LPs infinite LPs that have been proposed for other kinds of reinforcement
learning problems so I you know my gut feeling is we could take some of that methodology maybe
and apply it in the case where you don't have access to state um we do assume that Alice has
preferences and so if she doesn't or if she doesn't know what they are our work doesn't
doesn't help with that um but I think uh you know this limitation for example of not having
necessarily an MDP having features uh these are are things that are more left for future work
but could potentially be overcome it would also be really interesting to think about how preferences
might be uh obtained in practice you know like what if you actually had an agent that is working
in a look to the person the person is is stating some preferences we use this methodology to extract
rewards and then you know maybe again the person observes what's happening and and goes back and
you know is there a way for us to really do this in practical applications it's not something
that we have tried but it would be fascinating to see and part of the problem is the one you
articulated earlier is the evolution of preferences over time that you have to do that plus the
uh trying to figure out how to how to close that loop in real time exactly okay going back to
the work on hierarchical reinforcement learning is there a recent paper in that space that
that comes to mind or or maybe we can talk a little bit about or start from like the
kind of what's the research frontier there how far along are we in thinking about these kinds
of hierarchical problems a lot of the work over time has gone into the question of discovery
how does an agent discover abstractions we have a lot of good understanding about how to represent
for example temporally standard actions um by having an initiation set where where such an action
can start uh by having an internal policy and an determination condition so this is the
a framework called options in hierarchical reinforcement learning that I worked on for a very
long time and what would be an example of that imagine that you have a robot that's in an environment
and uh it has a controller and the controller says you can go forward if there's nothing in front of
you so if you're far enough from any obstacle that's your initiation set um it's almost the entire
environment except for some spaces around your obstacles um the policy is just to go forward so
that's already pre-specified and you stop whenever you're too close to an obstacle so that's the
that's the termination condition and now these policies can be stochastic as well right so
doesn't necessarily have to be that you're always going forward you might be going on any kind of path
right until you get too close to an obstacle and such uh such controllers can be thought of not
just in robotics but you know more intuitively for example in in planning right so if you're
playing let's say a puzzle game like Rubik's cube you might have certain configurations that
you want to achieve that impose sub goals and then you know the the ways to achieve those
configurations can be thought of as as these kinds of options um so we understand how to formulate
the problem the interesting question that is still quite open is how do we actually find these
these sub goals um so you know for example in a in a Rubik's cube uh I don't know if you've ever
done any of these or if you like them but you know when you learn how to do them people tell you
oh there is you know you have to complete a phase right somebody's telling you that's that's your
circle complete the phase that's the first thing you should do and then you know if you have this
kind of configuration on the side here's a here's a little sequence that you should do right
so it's told to you and yes once you know this it really greatly simplifies the problem because
you don't search through all the space of possible moves you're now executing these sequences that
you know are useful um but how do we discover this on our own right that's that's the key question
and it's one that we thought about quite a bit as a community uh it's still pretty open because
it's hard um there are some interesting answers one interesting answer that one of my PhD students
at Miguel Pierre-Luc Bakon did a few years back it's called option critic it's basically an algorithm
that tries to uh use the reward from the environment and and tries to find sub goals that are on the
path to rewarding states and it uses gradient based methods very similar to uh actor critic policy
gradient methods um and so so that works quite well on in certain environments but sometimes we
still observe things like you know the agent using abstractions for some bit and then kind of
collapsing them away like getting rid of them and it again in hindsight maybe one should expect
this because if you think about how people do things oftentimes you need these kinds of hints or
sub goals at the beginning of solving a task but if you sort of if you're only solving one task
and you get used to it you don't have to think about that anymore you'll automatize it
away right so people who do the Rubik's Cube in a few seconds don't really think about the
configurations anymore like you know it's all gotten into their muscle memory and so it's a
similar situation that we observe with our agents if they only have to solve one task they might
use options for a while to try and help them out but then they get rid of them and they obtain
some flat policy that is as close to optimal as possible so one way to uh to think about this
is the agents only have to do one thing and so they're overfitting to that in some sense
if they had a very rich very large environment complex environment with many things to do
they may need to keep these abstractions around so if you know if you're not always having to solve
Rubik's Cube but you have to solve different kinds of puzzles right let's say number puzzles
then you might have certain strategies that you learn and you do keep around about you know how to
manipulate numbers for example in these puzzles or how to do a search that's the kind of richness
that we would need for our agents to keep these abstractions around and so part of it I think
is is thinking about the good the interesting environments that we should use to send and part of
it is to think about the methods themselves and you know for example our gradients enough
should we do something else like more of a generate and test approach right where we think about
useful sub goals and then we test them out and we have some way of curating a collection of
sub goals for the agent I think that's all in substance quite quite open as a as a research direction
interesting one question that's coming up for me is trying to think about the relationship
between hierarchical RL as you've described it in curriculum learning I guess one just kind of
riffing on you know compare contrast like curriculum learning is sequential in nature whereas
hierarchical RL it could be more tree-like in the sense that you've got this or maybe even like an
ensemble like the agent has this portfolio of strategies that it can employ curriculum is
maybe more training time and hierarchical is more inference time I don't know if that's
is that correct there is interesting relationships between curriculum and hierarchical RL so
on one hand you could actually use curriculum learning in order to build a hierarchy so if an agent
for example goes through a curriculum that curriculum could be targeted towards the agent learning
options right so maybe you yeah so and that would be very helpful because it would ensure that the
agent has has a set of options and so when you go to the next stage of the curriculum it's as if
you've changed your action space in some sense right you don't have you know if you've learned
how to do multiplication you don't have to learn that again now you have it as as an option and
you can just employ it whenever it seems to be useful and so curriculum learning can definitely be
a path towards obtaining a hierarchical representation but in hierarchical RL you can do this also
in different ways so for example in a lot of the tasks that people have tried they learn
the options and they learn how to choose the options at the same time and to end so an agent
that let's say plays an Atari game is learning options for this game and using them in order to
generate behavior in order to explore in order to represent its value function now of course it's
harder when you learn many things and to end at the same time and so I think you know that slows the
agent down a little bit sometimes what does harder mean here I mean there's a whole you know as you
well know field around multitask learning that suggests that it can be more computationally
efficient to learn or produce better results to learn multiple things at once yeah so this is
multitask learning the multitask setup and generally speaking non stationary setups I think
is where hierarchical RL can be the most helpful because in such setups it's worth investing
some amount of time slow learning at the beginning so that afterwards you can do many more things
right so you know in a multitask setup you might for example learn efficient exploration strategies
and those will help you because the tasks that come later you'll still have to solve them and now
you know how to walk about your environment or you know you might need to learn to use tools
you spend some time doing that but then if you know how to use the tools then you can
it can go and be much more efficient in in in in later tasks so I think it also depends it depends
on how wide the task distribution is and how long the agent is going to live in this environment
because in some sense we would expect that the benefit of hierarchical RL manifests more
when the environment is more complicated and when the agent is going to have to live longer
in this kind of complicated environment and be able to have it. Is there an analogy between hierarchical
RL and you know thinking about the different layers in a CNN where the low levels are learning kind of
more abstract things shapes textures whatever the higher layers are learning more complex shapes
yes how far would you want to take that so the CNN is hierarchical in feature space right it's
looking at different resolutions in the in the feature space hierarchical RL is similar in spirit
but at the level of actions so lower layers would look at very fine resolution in terms of the
the action actions being taken very quickly and lasting only little bits of time higher layers
may look at actions that take a very long time to complete it's not as sort of clean as a CNN in
the sense that you know these timescales can be variable and we don't necessarily want to separate
them out in in very determined layers right and here's looping and it's not as clean at all
yeah exactly so you know if you're thinking about cooking dinner uh some things take a long time
you know like maybe you have if you're making bread you have to need the bread for a long time
and some things are very fast and sometimes you just have to react like if the cats on the table
you just have to do something immediately right and so the separation is not as fixed as you
would have in in the layers of a CNN but it's the same kind of principle at the time scale we want
to have fine timescales and longer timescales of which we make decisions and at which we also make
predictions about those decisions and is it useful at all to compare it to like an
unsombling type of approach where you've got a portfolio of submodels that you can choose from
in any given time yeah so you definitely have a portfolio of submodels and you can choose between
them um it's not like unsombling in the sense that usually so you have your portfolio make a choice
once you've made that choice let's say to use a particular kind of model you just go ahead and use
it rather than looking at let's say the the entire collection I think there are interesting
things to explore in terms of planning that would be even more like ensemble methods than the
kinds of methods that we have now and so for example right now when we think about planning with
temporarily extended models we think about big jumps over the time scale but a model still usually
predicts pretty much everything that will happen at the end of an option so at the same time we might
you know if you have two hands you might have you know one hand doing one option and the other hand
doing some other option and you might want to make sort of separate predictions and then combine them
so I think there's a lot of room actually to think about what kind of composition operators we
need right now when we use temporarily extended models we use the same kinds of composition operators
as a normal mark of decision processes so think about sequences and we think about stochastic
choice but there may be other interesting things to do like you know thinking about concurrent
execution for example and the outcomes of that got it got it another area that you are exploring
is continual reinforcement learning can you talk a little bit about that yeah definitely so
continual reinforcement learning is really the natural way to do learning right it's really
thinking about an agent that is in an environment that is much much larger than the agent it's
enormous and the agent has a lifetime and has to continue to learn and adapt throughout its
lifetime so in reinforcement learning often when we teach for example reinforcement learning classes
we talk about a mark of decision process that has a finite state space it has a finite action space
and the agent is aiming to go back to certain states understand which of these states are better
and so on but in a continual learning setting there may not be a mark of decision process and if
there is maybe the agent's lifetime is much shorter than the entire environment right so the agent
can't even hope to go to each state once in its lifetime and the side effect of that is that
there is pressure on the agent to use the data that it's seeing and to learn as much as possible
from this data and just to continue learning over time so this is the case where for example
hierarchical reinforcement learning should be very helpful because the agent can learn certain
abstractions right making coffee right going for a walk and so on that would be helpful regardless
of the situations that the agent is going to find itself and later and it's also worth investing
the computational effort in order to to learn these kinds of representations so I sometimes think
about what would it look like to to rewrite reinforcement learning without mark of an assumption
right just imagine you have an agent it's receiving observations it's emitting actions there is
a reward signal because we want the agent to have a task right so there there needs to be some
goal specification that's the reward but otherwise we don't make assumptions about
macovianness about stationarity right about there being a stationary distribution of states that
the agent is visiting and one can still have algorithms that work under these settings
um there are some really simple things that we can think about so for example doing
usual temporal difference learning but with fixed learning rates right fixed learning rates mean
that the agent is always paying more attention to the recent data and that's you know a very easy
way to think about handling the the continual learning setup it's probably not sufficient right because
only sort of encourages the agent to pay attention to the recent data but doesn't necessarily mean
that the agent is trying to build these more abstract useful representations um so I think there's
going to be a lot of interesting uh work in this area we have actually a survey on archive uh that
was called out by my PhD student Kimia Ketropal um on uh continual reinforcement learning never
ending reinforcement learning um and I'm quite excited to to explore uh this further and to think
about problem definitions the mathematical limitations um and good algorithms obviously to handle
these problems do we have a sense for computational computationally uh or you know in terms of
sample complexity like how uh you expect continual RL to play out relative to classical RL
so that's a really great question and it's an area where there is a lot of excitement and a lot of
uh work on the theory side um so the first question to ask maybe is how do we talk about sample
complexity in this case and um in theoretical RL regret has been the measure of um that that
people have thought of traditionally as as a way to uh characterize sample complexity so regret
is the difference between the value of what you're currently doing compared to the optimal value
like how how well uh could you do possibly in an environment but if you're in a continual learning
environment how what could you possibly do uh is up for interpretation right so maybe the environment
is not an MDP there's not a unique optimal policy um you know what do you compare yourself against
and uh there's been some really interesting work by Ben Van Roy's research group at Stanford
recently looking at that question and defining essentially classes of policies
uh and thinking of regret within those classes of policies in this kind of continual learning setting
I think that this is something that we're gonna have to think through like what's the right measure
of of complexity there's also uh tracking this is a different approach that people have
thought of tracking basically says if something changes in the environment how quickly can you
adapt to that change and I think that's a you know it's a little bit of a different perspective
uh resetting than some of his group at University of Alberta have looked at that in the past
what what's the idea with tracking as your environment is changing uh you want to quickly adapt
uh to those changes so for example if you uh let's say you had the grid world and you used to go on
the left path always but now that path is blocked you go and you discover that it's blocked
how quickly can you find an alternative policy um and in some cases uh agents uh can do this very
quickly right some of the agents can do this quickly um it all depends how good your exploration
strategy is but you know an alternative to that is to say the reward signal is changing it's changing
smoothly over time can you smoothly adapt your policy uh to uh to track uh what would be the
the right actions right now so there's these different kinds of changes abrupt changes and
more gradual changes and you want to uh to adapt and of course biological systems do this pretty
well and in our algorithms if we use uh in principle if we use fixed learning rates things like
value functions would also adapt continually um but exploration policies I think may need to be
rethought in this context um and you know specifically a lot of exploration has been
in this context continual or tracking in particular uh continual generally speaking
okay tracking as a special case just because a lot of the work in in our exploration has been
aimed at optimism in the face of uncertainty right so if you don't know something you should
just be optimistic and go for it which is great if you're assuming that eventually you can go
everywhere but if you have an environment that's very large and you can't go everywhere
you might need to to do smarter things so maybe um information theoretic methods are more
interesting in the setup right or explicit the uh keeping track of uncertainty or you know
learning exploration strategies uh that are that are effective in the particular environment
that the agents have so there's a lot of avenues for for future research in that setting
does the continual RL setting lend itself more directly to a time series type of problem uh where
you've got some agent that's uh making decisions that are presented to it presented to it over time
relative to you know exploring environments in any way so I think there are interesting
continual learning problems in time series prediction outside of decision making too for sure
stock market prediction to be very concrete this one such example right uh but you know uh joking
aside there is I think that the the time series prediction setting is really interesting to think
about because um it avoids the problem of exploration but it still allows us to think about how fast
an agent might adapt and to think about how do we characterize the difficulty of these problems so
it's pretty clear intuitively that in some cases an agent may not be able to do anything so if
you're at a time series where at every single time step there's some random bit that happens and
the distribution that you had on the current time step has nothing to do with the distribution
on the next time step well there's nothing the agent can do right so this is the sense that you
know the existence of impossibility results but of course in in the real world there's structure
right so I think they this set up allows us to think about what kinds of structure would allow
an agent to learn successfully even if the environment is changing so of course for example if
if there is gradual change in the environment let's say your bit is drawn from a probability
distribution whose mean is gradually shifting over time then the agent can hope to learn
how this shift is happening right and then perhaps even learn to anticipate what will happen
that's but you know smoothness is only one particular kind of structure there may be others that are
that are more interesting from practical point of view to wrap things up I'd love to have you comment
broadly on the field of RL you know over the years you know RL has it's been hard for folks to
do hard for folks to get up and running how do you see that part of it evolving
what do you think are some of the big challenges going forward beyond the many that we've already
talked about of course yeah thank you for asking about that I think RL has made tremendous progress
and it's again from my point of view it's the closest to biological learning right it's the
closest paradigm that we have in the field of machine learning to biological learning so it's
very important for us to to think about it and deep RL has in fact delivered many very surprising
successes ranging from alpha-go years back to more recently things like work by Marc Chandron
Belmar on rooting loon balloons in the stratosphere using deep RL methods recent work by my colleague
Martin Reed Miller at the mind on plasma control for fusion reactors these are very complicated
control problems and reinforcement learning algorithms can handle them and and do a really great
job so you know that makes me feel very optimistic that that we are able to scale RL and to really
deliver some interesting practical results at the same time I think yes there are challenges and
a lot of them have to do with these problems that we talked about you know discovery and
efficiency how does an RL agent uses data very efficiently how does it do efficient exploration
and how does it construct really good representations and historically reinforcement learning
has relied a lot on other machine learning technology for example in order to build representation
so we use deep nets deep nets are wonderful and they've they've led to a lot of these these great
successes but in some sense that specific set of methods was developed for supervised learning
right and supervised learning is based on a different set of assumptions than RL much more
IID and so the way that we've made progress historically has been to kind of take RL methods and
make them more supervised learning like using replay buffers is a classical example of this in
order to train let's say cue learning in with deep networks I think it would be very useful for
us to think about function approximation and optimization in the context of RL and how do we do
that efficiently one of my students Emmanuel Bengeo who just graduated in fact looked at this
problem last year and he discovered that looking at let's say atom optimizers in the context of RL
doesn't give you that great results right in some cases really does not combine with
temporal difference learning and and similar algorithm so I think one of the challenges for us
is really to think about the function approximation and optimization problem in the context of RL
and also to think about this continual learning setup because that's where we really want to go
before going to build general intelligent agents they have to learn all the time they have
to learn in these settings where you know Markovian structure doesn't hold and so that's that's
the frontier awesome awesome well doing it thanks so much for spending some time chatting with
us and sharing a bit about what you've been up to in the space very cool stuff you mentioned a bunch
of research your own and others and we will try to collect that from you and be sure to include it
on the show notes page but once again it's great chatting with you thank you for joining us
likewise thank you for having me
