Welcome to the Twimal AI podcast. I'm your host, Sam Charrington.
All right, everyone. I'm here with Fuck Lay. Fuck is a research scientist at Google.
Quack, welcome to the Twimal AI podcast. Hi, everyone. It's great to have you on the show. I've
followed your research for your work for quite some time. And I'm looking forward to digging into
some of the new things that you're working on. But before we do that, I'd love to have you share
a little bit about your background and how you got started working in machine learning.
Okay, so I was born in Vietnam and I did my undergrad in Australia.
And in my second year of my undergrad, I started a summer project, doing machine learning
with Alex Mola, back in Australia. And back then, I was a player that I would call
Toronto Methods. And then I did my PhD at Stanford on a lot of deep learning back in the day
when deep learning was very cool. And that's around 2007. And around 2011, I did a summer
internship at Google. And that was when Google Brain Project was kind of founded. So when I was there,
that was Andrew and Jack Dean and Greg Coraro was there. And I was the intern. So we started
up quite small. That's all right. Yeah. And then I did some of the, you know, scaling up
neural networks with the Google Brain Fox. And then, you know, and then after two years,
I did some work on machine translation with Ilya and Oryo Vinyal. He's now at it mine.
Ilya is now at OpenAI. And we developed some of the N2N translation methods. And
and then around 2016, I started looking into more like, you know, auto-email neural architecture
search. Yeah. And more recently, I looked into more like together with auto-email. I also
looked into semi-subvised learning and so on. Awesome. Awesome. Now you mentioned early on doing
work with Alex Mola. Was he, was this before he was at Carnegie Mellon or was he visiting in
Australia? He was a professor in Australia. Yeah. Yeah. Yeah. I went to a university in
small air in the capital city of Australia. And he was, yeah, camera and he was a professor there
doing research. So I thought, you know, I had, I have been long very interested in AI and machine learning.
And I took, before that I took a class in data mining and so on. And I thought, you know,
it's a little bit boring, but machine learning and the ability to actually learn is actually
super fascinating. So I contacted him and he, he, he, he was doing like auto methods.
Machine learning and and we, we worked together for maybe a few years. Yeah.
Before he went to, he went to America and then CMU and Amazon. Okay. Okay.
Yeah. So a lot of your recent work has been focused on this idea of, you know,
automating machine learning and neural architecture search to allow machines to find the best
deep learning architectures and like, you know, talk a little bit about how you
arrived at working in that area and what some of the motivations were for getting started
digging into that problem. Yeah. Yeah. So I've been long interested in this idea of self-improvement.
Machine should be self-improving itself, machine learning, right? And even, and when I started
doing kernel methods with Alex, I always asked him, you know, how the, the code, the kernel bandwidth
and so on or how some of the hyper parameters in kernel methods decided and, you know, apparently,
they decided by using things like cross validation and so on. And then when I work on kernel methods,
sorry, neural networks, my hope is to make the hyper parameters go away.
Right. But that's how it's the opposite. So if now if you look at a, a convolution neural networks,
it's, it has a lot of hyper parameters, right? Like how many, how many layers you want it to be
and how many channels you want it to be and what are some of the size of hyper parameters since
are like kernel widths and so on. And so, not all the training parameters. Yeah, all the, yeah,
and learning, right? And as researchers develop more and more techniques for neural networks,
there's more decisions that you have to make that feel like, you see, like a problem that
you know, can be helped by a little bit of automation. So in, so I, I, I observe a lot of my
colleagues at Google when they design neural networks and I asked them about the principles of
designing neural networks and, you know, you started out having some really solid principles.
Like you add skip connection so that gradient can flow through the network and so on.
But as you tune the network harder and harder, you no longer have the principle. It's a,
it's around, you know, trial and error, right? You, you try this a little bit and it seems a little
bit better. So you try, you try that more. So I think that that is something that may be ready
for automation. So even during my grad school, I already talked about trying this, but I thought,
you know, maybe we didn't have enough compute because training in that already takes, took me days.
So when I saw that neural, you can train neural networks in 30 minutes or something like that,
you know, from on CIFA, I thought, oh, maybe this is the right time to try this. So that's when I
started doing this neural architecture search in 2016. It's interesting that, you know, even with all
of the compute resources of Google, you had to wait until the time was compressed enough in order
to be able to tackle the problem. Yeah, yeah. Third cell that to get really good results, you want
the networks to be really big and that will take a long time to train. Yeah, and it's funny coming
from me that we have so much resources at Google, but training neural networks is still taking a long
time. Yeah. And so maybe talk about the first steps in that area, did you jump right into
the neural architecture search or was that the, you know, an end stage or a end result of this work?
Oh, well, you know, I work on some of the related ideas on and off since 2012, like Blender
with how to do better hyperparameter tuning for new networks. And none of that is really published
because I didn't have good results and, you know, I didn't have enough compute and so on. So,
so I tried it on and off.
Over the time, you know, every year I would set out some time to try this idea for a few months
and, you know, and it didn't work very well because lack of compute and so on. And then around 2016,
I met Beretsov, who is my colleague now at Google, and he's very talented. So we say, oh,
let's let's try this idea of using like a reinforcement learning to generate a network, like a
little layer in a network for, for a SIFA model. SIFA model is already at that time, you could say
that, you know, train enough a few depends on how, where you want it to be, but, you know, from
30 minutes to a few hours. And that seems like about the right amount of time to get this going.
And my prediction is that you have to train maybe either between from 1000 to 10,000 models. And
I did a backup, our envelope calculation and I thought, oh, this might be the right time to do it.
But, you know, I tried this, some of these related ideas in, you know, much before that.
So you're doing a SIFAR, which is an image recognition for object detection and images.
And so you're doing, you've got this, when you say you're doing,
when you say you're doing tens of thousands of models, is that part of the optimization process
that you're describing here? You're expecting that you need to do 10,000 in order to optimize the
hyper parameters. Yeah. So, so in this process, you have a controller, which is also a machine learning
model. And every time it makes an update, that's basically, it has to, it has to try training one
model to conversion, one SIFAR model, two conversions. And it will take the signal from the
conversions of the, the SIFAR model, you know, maybe the SIFAR model will get 70%. So that 70%
would be used as a signal to make one update for them controller, one update, right? So,
typically machine learning models take a long time to train. So it requires, you know, 10,000
of updates. So that's basically the number of models that we had to try.
Were your initial attempts at this doing, you know, how do you distinguish between like,
doing your hyper parameter optimization and kind of architecture search, because there is a varying
degree of complexity in trying to come up with these new architectures. Yeah, and your more
recent work on this is like using evolutionary algorithms and a like to do this. Can you maybe talk
through kind of the progression of complexity that you went through? Yeah, so the first project that
we did was architecture search for like a SIFAR model. And that's already was already very
expensive back when we did it. And we didn't choose hyper parameters. You only, in other words,
we didn't choose hyper parameters like learning rate or way decay or dropout, which is focused on
architectural hyper parameters, basically, you know, number players. What kind of player do you use
in what stage in the network? And that's already took us like almost a week for every time we try
this, it takes like a week on a few hundred GPUs. So after that, we moved to ImageNet. And because
ImageNet, the network is bigger, we decided to use this idea called transfer learning, which is
basically find a module that works well on SIFAR 10 and transfer to ImageNet, because searching
directly on ImageNet would be very difficult. This is very expensive. Now in parallel with that,
we also try a lot of methods in not only in reinforcement learning and but also in evolution.
And we also observe the evolution that does as well or even better than reinforcement learning.
So we slowly adopt more, sorry, evolutionary methods. And then, so some of the first
of any intuition for why evolution works better than reinforcement learning?
Oh, I see. So evolution is very flexible and very easy to implement, right? For example,
in evolution, you just need to decide mutation and cross-over mutation meaning you have a network
and then it just changes a little bit and across-over meaning take two networks and make them.
So implementing evolution is actually quite easy. Now in contrast, reinforcement learning,
because we're not experts in reinforcement learning. So we have, we try a lot of reinforcement
learning methods like we started out with reinforced and then we tried something like PPO and so on
and TIPO recently. And they're good, but they also require a fair bit about tuning to get working
very well. So on the other hand, evolution seems to be very flexible in terms of implementation.
And it's also wanting that evolution does quite well is that it's easy to diversify
the models. So you can just try to, in reinforcement learning, once it happens, it will zoom in
into a particular area, area in the surface and optimize the particular model. Whereas in evolution,
it will diversify the population over time. It's easy to control the diversification process
to get better models. So we use more evolution methods now and then feed forward.
Okay, so in the second project, we already found one network that was kind of stay-of-the-art
in computer fusion on power or slightly better on the on the stay-of-the-art of image net.
So that was super exciting because we didn't think that it was possible. And then
after that, we realized that this transfer learning has a problem that you know, you transfer the
cell. And maybe the type of things that you want on cipher and the kind of cells that you want
for image net is quite different. So we started searching on image net directly. Basically,
you search a model on image net directly, but that became super expensive. You know, our
back-up envelope calculation will take a few months for this to finish. So we realized that maybe
one idea that we can have is search on small scale. Search a smaller model on cipher. Let's say,
is that of searching the biggest model possible that you could find? Search for a small model,
like that train only like five epochs in, you know, eight hours or something like that, right?
So that's small. And after that, after we found a good model, we figure out a way to scale it up to
be a size. So basically, make it new with larger image or make it deeper or make it wider.
Is that scaling up in a learning way or scaling it up? Yeah, scaling up in a learning way.
Okay. We're scaling up in a learning way. So the second stage of scaling up, basically,
what we did was, you know, develop like a, we learned the way that we should be scaling up.
And it looks like it works very well and that became something called efficient. Now it's been
used quite a bit in various places at Google and in academia. And the smallest network that we
found turns out to be super helpful for mobile devices. So people, because the network, small network
seemed to be quite fast for mobile devices. That became something like a mobile network V3.
Yeah, at Google. And after that, we say, you know,
okay, now that we can get stay of the art on image net, but the problem is that a lot of
building blocks that we used are very much, you know, building blocks that pre-describe or
pre-describe by human experts. Let's say we have to make use of the rail law layer design,
but design by human experts or we made use of a convolutional layer design by human experts
or batch norm layer design by human experts. So we say, can we, can we design everything from
scratch, basically assume that we know a non-pile library, like non-pile is this basically just,
you know, matrix vector multiplication and bunch of non-linearity. Can you use a non-pile library
to evolve the concept of machine learning? And that became something like auto-email 0.
Which basically, yeah, that's auto-email 0. And that's basically the thought process behind it.
In auto-email 0, 0, we didn't get stay of the art yet. But what's exciting about it is,
it generate a program from just matrix vector multiplication and to do machine learning,
which is super exciting. And I hope that using this method we can discover
fundamental new fundamental new building blocks for machine learning.
Yeah, folks haven't, if anyone listening hasn't taken a look at any of the blog posts or the paper
for auto-email 0, really interesting. There's one particular diagram that I've seen a couple
different versions of it, but it kind of walks through the process that this algorithm takes to learn
a model and shows the various steps that it introduces and as well as the program that it outputs.
And it's super interesting, you know, talk a little bit about this idea of,
you know, having this model work by evolving a program. Where did that come from?
So, go ahead, sorry. I was just going to say, you know, a lot of what we're doing is all software,
it's all programs, but this in particular is like arithmetic arithmetic operations that
in a very kind of simple way define all the steps that are used to evolve these algorithms.
Yes, yes. So, if you think about what computer, sorry, machine learning experts are doing now
are they? Is that basically they look at a computer program, you know, they use TensorFlow, right?
They look at TensorFlow or PyTorch and they have a bunch of players and then they figure out
to develop to write a computer program to write a new program to do machine learning.
Let's say you want to do forecasting or something like that, right? Basically, you look at
how people use LSTM and then you put together a computer program to do forecasting.
Now, the act of writing that program is still now not learned, right? So, basically, it's basically
human expert knowledge and changing that program can affect the quality of the model greatly.
Now, stepping back a little bit is that that program, the auto and the program you just put
assume a lot of knowledge about machine learning, right? The fact that the concept of gradient, you know,
by propagation is assumed during this process, right? Because the different automated differentiation
is built in into TensorFlow and PyTorch. So, we said that maybe what we can do is
to step back a little bit from PyTorch and TensorFlow and start it from NumPy and using NumPy,
can you put together a small computer program that can do, you know, SIFA classification or something
like that? Yeah, and in the setup that we have, we have three functions. So, one function is
setup, meaning that, you know, it's like a DNA that you start with, right? And then there's a
predict function, meaning that whatever you have learned, you have to use it to force survival,
right? So, you have to make, you've given a given situation view in, you have to make some
prediction. And then the third function is the learn function. You know, it's, it has to learn
so that the predict function is better over time. So, there's only this template have only three
functions, setup, predict, and learn. And AutoML0 has to fill in the rest of the program,
the rest of the instruction within these building blocks. Right, it's starting with those
three functions being totally empty. Yeah, it started out with these three functions totally empty.
So, at the beginning, it will start it, you know, amazing, right? At the beginning, it will do
nothing. So, most of the programs will be garbage. So, you have no signal at all.
You have no signal at all. So, by some random block, right? It will find some kind of
dot product linear kind of layer that somehow does more than better than random. Just slightly a
little bit, sorry, better than random, right? That's just basically the predict function does
a little bit better than random. And then it will basically slowly put together one more,
one more layer to become like a neural net. And then it will invent the concept of gradient.
So, over time, it will come, it started from a very small program that is like do linear to
go through many, many steps to eventually do a neural network. Yeah, and I referenced this
diagram. If I'm understanding the diagram correctly here, identifying all these points where
the algorithm evolves these techniques that, you know, humans do now. Like it eventually figures out
how to do SGD. It eventually figures out like Rayloo and other things and it figures out techniques
like gradient normalization and stuff. Am I reading that correctly that this is all stuff that
has figured out? Yeah, yeah, yeah. So, basically, if you put it in the whole sequence,
you know, the first step it will find like something like linear model, it find logic clipping,
it will find learning rate, and it will find Rayloo, you know, and then normalizing the input norm,
the gradient, and then, you know, doing having malinear interaction, things like that. So, things
that, you know, like over the time in the last maybe 30 years of neural networks evolution.
Just so that I understand there's no, there's no kind of priors, there's no like
recipe book or techniques that are given to the model, the algorithm, and all it's figuring out
all of this from nothing. Yeah. So, so the caveat is that AutoML 0 has access to 64 functions
from Lampai. Okay, that's there's some bias here. So, this 64 function from Lampai,
because the way that linear algebra works is very in favor of neural nets, right? So it will
tend to develop things like neural nets, because linear algebra. But that's the only thing.
There's no product there for it to use, eventually it's going to try and use it on some stuff.
But it would be hard for it to find something like trees, because, you know,
Nampai doesn't have the functions that kind of suitable for trees.
But it's because Nampai, you can argue that Nampai is like a library that's very suitable for
neural nets, right? So, it will evolve things that eventually look like a neural net.
Now, what's surprising is that it did the whole process of developing models that started from
linear and then put in development learning rate and normalizing the gradient and things like that.
It looks very much like the evolution, our own evolution process of developing machine learning
models. And so do you have you, have you, has this allowed you to see future techniques that we
may learn to apply? Yes, but we haven't found anything extremely novel in the sense that like we
never, we haven't seen it before. But we haven't found something that we haven't looked into
more closely. So, in particular, it found this bilinear layer that normally, if you do a neural net,
you would take X, multiply by some W, and then you apply some nonlinearity. Now, what we found
during AutoML 0 is that it prefer X, WX. So, and then apply some nonlinearity. So, that basically,
what they say is some kind of malinear interaction, right? And apparently this concept has been developed
by other colleagues at Google. But we never, we didn't know that before project. So, we are in the
process of trying out this layer on larger problems. But it looks like that layer is actually quite
promising. Interesting. Yeah. So, the other thing is the concept of gradient normalization. So,
basically you take the gradient and then you normalize it before you make the update. So, this is not
not something new. So, people have done this before, but it's not very popular.
And I think maybe one thing is that we are also trying this on bigger networks now ourselves.
But if you can think about this process, it will aid the process of discovering either new idea
or discovering older ideas, but we actually overlook because we have so many ideas in
machine learning that we tend to overlook them. So, but some of the recent data is look promising
or some of the ideas that it found. Do you think there's an opportunity to use a technique similar
to what you did earlier with CFAR, where you apply AutoML 0 in a small way and then scale it up
to bigger problems or networks? Yeah, I still we still thinking about how to do it effectively,
but basically, basically you're right. So, the problems that we did in AutoML 0 is like a,
it's not even CFAR, it's a Dow scale CFAR. It's a small version of CFAR. So, and then you know,
the concept like gradient normalization or things like, you know, malinia interaction is something
that it found and then we can take some of these layers and transfer into big problems. Now,
unfortunately, the problem in CFAR, it's so downscale that you we cannot present it like an image.
So, it is only 1D. And in 1D, you cannot find things like convolution on neural networks.
So, that's a that's a limitation, but in the next step, we try to make it have like make it see an
image rather than just a 1D image. Now, that's that's one aspect, which is basically how to scale
this more effectively, which we did it before. The other thing that we did is to zoom in into a
particular aspect of the neural network and can you do better. So, related to this is the
paper that we published, you know, a couple days ago, on evolving a better activation and
normalization layer. So, basically, in a neural network, people use this layer called BASHNOM
and RELU a lot, right? This is in Dresnet. If you use Dresnet, you have BASHNOM and RELU
and then there's a skip connection, right? And we say, let's use this into a single layer and
search for a new mathematical operation to replace this BASHNOM and RELU. So, we accept the rest
of the network, but we search for a mathematical operation from to to find a new layer. And it
seems like to find a very good layer as a replacement and it works better than BASHNOM and RELU.
And is the motivation there, primarily, network performance or is it computational or
what is driving you to focus on those particular layers? So, BASHNOM and RELU, one thing the BASHNOM
RELU is a good thing about it is it allows you to train with very big batch size, right? But when
it's very small batch size, it doesn't work very well. So, it's a layer that Google would like,
but most data scientists would not like because you don't have a big computer to train with a
big batch. So, there's a replacement called Group NOM and RELU, which is very good, but it works
very well on small batch size, but on the big batch size it's still a little bit worse than BASHNOM.
So, it's still confusing to many people what layers to use, even at Google. So, developing a new layer
is that, first of all, BASHNOM RELU is if it can be a good replacement for BASHNOM and RELU,
that means the layer can be used by both internal researchers and external researchers.
Well, and the second thing is BASHNOM RELU helps training, it stabilizes the training a lot,
it speed up the training. So, we use it a lot in our work, so we really want to improve on that
aspect. It plays a huge role. There's a paper where they just say that you just train only the
BASHNOM layers and don't train the convolutional nets and you still can get a good performance.
You know, it's not great, but it's good performance. It means that these layers play a very good
important role. And is that paper? Talking about training those layers, only those layers from scratch
or fine-tuning only those layers? Training those from scratch. Yeah. And so,
is the idea that you're applying techniques like what you've done at AutoML Zero to finding
these new layers or is that a totally separate approach? Oh, it's highly related. And I would say,
you know, it's like an application of this idea of AutoML Zero. Yeah. Yeah.
Cool. So, you've also been working on semi-supervised or self-supervised
learning recently. Yes. Can you describe some of that work and how it relates to this stuff?
Yeah. Sure. So, I've been working on this automated machine learning and
automated architecture design and so on. And I gave talks about this new development and a lot
of people came to me and complained. They say, you know, you automate the design of the neural networks,
but I have more data than you. So, I bid you. So, I say, oh, that's a good point. So, I thought about,
okay, automate machine learning is cool, but can you automate the labeling process?
Can you automate labeling, right? Because most people would prefer to have more data.
Because more data is very important, right? I don't mean that architecture is not important,
but having more data is also very important. So, the question is, can you automate the process
of labeling data? So, today, you don't have anything. Today, you basically get some
unlabeled data and you give it to some human experts and then they label the data for you, annotate
the data for you. So, you get techniques like active learning that can help you
out the best labels, the best data to label, which helps. That's right. Yeah, active learning
will speed up that process by selectively choose the example to annotate the data.
Now, so, one idea that we had with the concept of pseudo labels, so fake labels,
can you take your model and generate and evaluate on the unlabeled set?
And now you have weekly label data. Our observation is this, right? Like, can you take your own model?
Generate labels on a new set of unlabeled data and then put it back,
assuming that they are correct labels and train the new model on that. Well, actually,
I tried this idea many, many years ago too and it didn't work. And the problem is that if you
take the model and generate the new label data, the new weekly label data, some of them are
accurate, you know, a three would get a three, like that label of three, but sometimes a three
would get a label of five. And this error would propagate, propagate into the new training and it would hurt
the new training and the new training would not get better result than the old training.
Right? Because it's the confirmation bias going on.
Now, you, so I did not know any way to fix that problem.
So, so I thought the concept of self labeling is a, is too good to be true. But recently,
we realized that there's a way to overcome this process is when you train the new training,
you inject a lot of noise into the new student. So that's, so you have a teacher that generate
labels on a label data. You have a combined set of true label and weekly label data
or pseudo label data and you train new students on this new combined set. When you train the
student, make sure that you insert a lot of noise. So the a lot of noise in the student will,
we still don't know this, how this happened yet. But the noise in the students seems to have
this process that will overcome the confirmation bias. Probably because it will make the student
more robust, right? Because it has, it has new, not trust the labels all that much. Exactly.
Right? So it learned not to trust the label that much because it has to cope with so much noise.
And amazingly, it actually outperformed the teacher. So the noise that we use is basically things like,
you know, drop out and drop certain parts of the model, data augmentation, and super aggressively
doing this data augmentation and noise. And eventually it would do better than the student.
And you can just iterate the process, right? Once you have a better student, you label new data
and then you put back. So we keep doing this and it seems to work very well.
And how what's your performance metric or your benchmark?
Yeah. So we worked on this data set called ImageNet. So I think when we work on this data set,
the say of the art was like 82% and then using architecture search, we've pushed it into 85%,
85.4% or something like that. And then using this auto label process, you get to 88.4. So
3% improvement. So and keep in mind that 1% improvement on ImageNet at that high range is very
difficult. And I'm talking about top one accuracy. Okay. And so is this, are you,
is the setup here that you are you starting with your standard kind of 70% training and 30%
test ratio or does that matter in this? Oh, so you're having your student
label that 30% like did these ratios come into play into this? Okay. So we just follow the
conventional ImageNet setup. So ImageNet has already had a split of maybe 1.2 million
example of training and 100,000 for validation. And for other label data, you would use a different
compass. So at Google, we have this data set called JFT that has about 300 million images.
And we are operating other data beyond ImageNet that you've labeled using a model trained on ImageNet
that you don't have any labels for. I mean, meaning that you don't have any labels for your
external data set. You're just labeling it based on ImageNet. Some of those are going to be wrong.
So you introduce the noise and it all seems to. Yeah. Yeah. So basically, yeah. So here's something
that we find really surprising. There's one experiment in the paper that people did not check.
But it's super exciting. Interesting is that we can propagate back images that so, you know,
images has a 1,000 categories, right? Like, you know, some flowers, some dogs, and some cats,
and so on. Now, we propagate back images that don't have, that don't look like any,
like any categories in 1,000 category. So it could be like some kind of very strange animal.
And the model which is just like us is the ImageNet. Yeah. It doesn't belong to any,
yeah, any categories. And it still helps. Just by saying that, you know, this image is not any of
these categories. And this put a lot of low, low percentage, low probability on a lot of these
categories from propagate back is still okay. And that consistent with the consistent maybe with
the idea that the images are even if they're not, you know, helping turn your classifier layers
at the end, they're helping the network learn textures and, you know, low level features better.
Yeah. So I think what really happened is that it just tried to learn
information about natural images, right? And the fact that, basically, you give it a label.
So the teacher give it a label. And then when you do data augmentation, you ship the image
a little bit. Right. And then you say that the label is the same. So, you know, the probability table
looks very similar. So the model has to work really hard to stay consistent in the production.
So that's, and natural image in general is just very similar in similar ways, right? So it learns
to be consistent in terms of labeling for new images. And maybe that's the reason why it's
very helpful. Even though the images might not have anything to do with your other set.
And you know, I'm curious in kind of articulating that
intuition for what's happening is that based on, you know, all of your experience working with
these kind of networks. So did you perform specific experiments to try to understand, you know,
what the effect might be and, you know, what were those experiments?
Oh, okay. So network introspection or any kinds of things. I'm just curious, you know,
one of the kinds of things you've done, you know, to deepen your understanding of why this is working.
Oh, you mean this particular experiment or in general?
This particular experiment. Okay, so we try to lower the threshold, right? So when you take
the your model and then label a new, a compass of 300 million images, we feel that we have a
chance to feel that our low confidence images, right? So things that, you know, have very low
prediction, low probability of having to be in any way of the class. So we can keep, you know,
only 10,000 images or we can keep 30, sorry, 1 million images or 30 million images or,
you know, 300 million images or anywhere in between, right? So we vary the threshold.
And it seems like actually the threshold can be quite really low, right? So it could be, you know,
in the 130 million or something like that, it's still okay. And then we visualize the image that
are actually very low accuracy and a low confidence. And then we visualize them and see what they
look like. And they don't, apparently, they don't look like anything like on image net. So that's what
we found why the fact that they're helpful is very surprising. But why they are helpful, we still
don't know. So is this a hypothesis? We don't, we don't know. Yeah. Yeah. Yeah. I, which is basically
our current work is trying to analyze why it's helpful. Okay. And what do you expect your
direction to be in analyzing that? We're probably going to look into the hidden state of the
neural network. And you see, we see, you know, with these low confidence,
where is it, is it trying to make the hidden state more consistent to each other,
which is basically the same phenomenon that a lot of people do in contrastive learning in
self-supervised learning is that they also have two images of data augmented image. And they're
trying to make sure that the prediction is the same. And if you have two images of unrelated
images, you make sure the prediction is different. Right? So I think this is what happened in here.
So we're going to visualize some of the hidden state of the neural network with and without
these low-confident example and see, you know, what happened during training. Okay.
That's that's one direction that we think that we will be doing. Okay. Okay. Yeah.
Cool. You were also an author on Mina. Yes. Were you involved in that? Can you talk a little
bit about that work? Yeah. So many years ago, I worked on something called sequence learning,
and to end neural networks through NLP. And that's still for translation. And I spent like two
years after that trying to be like a chatbot to chat with me, because I always fascinate like,
can you have an agent that can talk to me, you know, in the intelligence? I've been all your emails
and you know, there's a down mic here or were you trying to, were you trying to have a communication
with the chatbot or were you trying to replace yourself, quote unquote, with the chat by having
other people be able to talk to you? Both. Both. You know, just talking to computers is fascinating.
Yeah. Yeah. And so I failed. And then we, I, why, and then we ended up meeting this person
called at Google and a very great engineer at Google called Daniel. And he, he said, how about
let's work together to make this better. And we did a lot of work on scaling up some of the models
that we built. We collected a huge amount of social media data, you know, people talking on the
internet. And then we train a huge model, like a model that I, I, you know, maybe a hundred times
because of what I train back in the day. And it can do multi-tone conversations. It started doing
multi-tone conversation very well. And one of the magic moments that I really think that
truly magic is that it actually invented like a joke. It invented a joke. So people talk, it's, it's, it
made this pun that, you know, horses go to Harvard. So cows go to Harvard, cows go to Harvard. And horses
go to Harvard. But it's very fascinating. I remember, it's been a while since I looked at that one,
but I remember it was the, you know, when you describe it as inventing this joke, it wasn't anywhere
in the training data that you could find, right? Yeah. The only instance of mentioning
Hayward is, doesn't have any, anywhere, there's only one instance of mentioning the word Hayward in the
training data. And we look at that context, and it has nothing that looks like what we are like
horses go to Harvard at all. And so what do you think was happening there? How did that work?
I guess unlike what we see in kind of conventional language models, like even the big ones,
they're picking up stuff that they've seen before generally, right? Okay. So I can tell you my
version. My version is a following. My version is a following. We still don't know, right? That's
what I say. It's a magic moment. I think, first of all, a lot of social media jokes about puns.
Right? A lot of, you know, like we find puns kind of funny. And so, and so, and a punny. And so
in the training data, we train with my pair at coding, meaning that we take a world and we break it
down into disease. So it's, Hayward is not a world, it's right, two words, Hayward, and Harvard,
and etc. So it must have learned the concept of puns. And you wouldn't understand that, you know,
Harvard and Hayward somehow is kind of related. So it made up this pun. So two things, learning
the concept of puns and learning how to put together like a new concept. And yeah, it does a lot
of new things like it make a joke. Like it makes jokes about why chicken crossed the road,
some things like that. I don't remember the particular jokes I can find it and send it to you.
It makes all kind of new jokes that we never found in the training data. And it's truly
fascinating. And so where do you, you know, that line of work, where do you see that going?
Well, what were the key, you know, was this, was this kind of simply, you know, quote unquote,
an instance of scaling up the training or where there are some, you know, new techniques or
novel things that were developed, you know, in the model that you can see applying elsewhere.
I see. Well, first of all, it basically tells a lot of people that scaling up is very
effective. Why to be, you know, NLP models, right? Like I spent two years failing doing that
chatbot. And then suddenly someone came along and found a solution. And the solution is kind of
simple, which is basically just make the model a lot larger. We also found a lot of new interesting
insights is that during the process of building the bar, we have a lot of difficulty how to measure
the performance of the bar. So basically, we, we, one thing that we want to measure is how human
like it is, right? We want to be able to have a conversation like a human like. So we, to measure
human life, we always want to ask human to look at a conversation that we have with about because
we built many models of the box, right? Right. Many, many models of the box. And every model,
we have to look at a used human expert to look at the conversations and say, hey, how many human
like this is? So during the process, what in that we observe is that as the complexity of the model,
so, you know, as you train the model better, so the objective function is called the complexity,
is like a very logo objective. This is basically how, how well you predict the next world, right?
And we've noticed that this objective function correlates very well with human,
a judgment of human likeness of the bar. And, and then we did a real plot of perplexity,
which is the local objective function, you know, tricked me in the next world. And,
human likeness, and we saw like a, a strong correlation. So that's basically another contribution
for NLP is that actually this local objective function, which is just predicting the next work,
next work, if you do a good job at it, turns out it's also a more global objective function,
which is human likeness, how, how I can behave like a human. So local mimicking is global mimicking.
What's interesting about that is that, you know, perplexity being kind of predicting the next work,
what, what makes jokes and puns work is that the next word is a surprise compared to what you saw.
So how do you get something that's good at jokes, but also is optimizing on perplexity?
Yeah, that's, that's the reason why I say it's very simple, it's surprising that low, so
these are, let me try to say it again. So there must be a low, a global objective function that we
are optimized, we like making fun, so that we can get engagement, right, making fun, saying something
meaningful, right, that's, that's global, right, but something that's not just optimizing the next
world. But what I'm just claiming is that local objective function is that actually just predicting
the next world is highly common, global, that we're going to relate to the global. And I mean,
I guess you could argue that your what perplexity doing is doing in the case of the joke is
predicting surprise, it's not like flying to the surprise, what is predicting the surprise based
on? It's predicting the surprise. Yeah, you could say that. Yeah, yeah. Huh, interesting. Cool.
Well, thank you so much, Quark, for taking the time to share with us what you're up to and
and kind of walk us through these recent works here is really, really interesting stuff.
Oh, thank you so much. It's a pleasure to talk to you. Same, same. Awesome. Awesome. Thank you.
All right, everyone. That's our show for today. To learn more about today's guest or the topics
mentioned in this interview, visit twimmelai.com. Of course, if you like what you hear on the podcast,
please subscribe, rate, and review the show on your favorite pod catcher. Thanks so much for
listening and catch you next time.
