WEBVTT

00:00.000 --> 00:25.760
Hey everyone, hope you all had a wonderful holiday.

00:25.760 --> 00:30.520
For the next few weeks we'll be running back the clock with our second annual AI Rewind

00:30.520 --> 00:32.120
series.

00:32.120 --> 00:36.960
Join by a few friends of the show, we'll be reviewing the papers, tools, use cases,

00:36.960 --> 00:43.120
and other developments that made us splash in 2019 in key fields like machine learning,

00:43.120 --> 00:49.480
deep learning, NLP, computer vision, reinforcement learning, and ethical AI.

00:49.480 --> 00:55.680
Be sure to follow along with the series at twomolai.com slash rewind 19.

00:55.680 --> 01:00.080
As always, we'd love to hear your thoughts on this series, including anything we might

01:00.080 --> 01:01.160
have missed.

01:01.160 --> 01:06.600
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via

01:06.600 --> 01:11.560
a comment on the show notes page you can find at twomolai.com.

01:11.560 --> 01:13.920
Happy New Year, let's get into the show.

01:13.920 --> 01:19.040
Alright everyone, welcome back to our AI Rewind 2019 series.

01:19.040 --> 01:23.800
This episode will be covering NLP, and I've got the pleasure of being on the line with

01:23.800 --> 01:26.520
Nasreen Mostafazadeh.

01:26.520 --> 01:34.320
She is a senior AI research scientist at Elemental Cognition, Nasreen, welcome back to the twomolai

01:34.320 --> 01:35.320
podcast.

01:35.320 --> 01:39.600
Hi Sam, glad to be back, thanks for having me.

01:39.600 --> 01:42.120
Definitely glad to be speaking with you again.

01:42.120 --> 01:49.480
We last spoke back in August of 2018, when we spoke about contextual modeling for language

01:49.480 --> 01:53.360
and vision, some of your research.

01:53.360 --> 01:59.400
This time we'll be reviewing some of your thoughts on the most important papers and developments

01:59.400 --> 02:05.880
more broadly in the field that you work in, natural language processing in 2019.

02:05.880 --> 02:11.480
I'll have folks refer back to that previous episode for a little bit more about you and

02:11.480 --> 02:17.440
your background and what you're working on, but to get this conversation started, why

02:17.440 --> 02:22.520
don't we just start with your kind of broad take on 2019 in NLP?

02:22.520 --> 02:26.600
What was the, you know, was it a big year for NLP?

02:26.600 --> 02:33.920
Sure, so actually I think, yeah, I think 2019 was actually an exciting year for NLP,

02:33.920 --> 02:40.000
where, you know, these sort of large pre-trained neural models have been stretched widely to

02:40.000 --> 02:42.320
various different directions.

02:42.320 --> 02:47.080
And, you know, slowly, but surely as a community, we've started to think about, like, the

02:47.080 --> 02:51.280
problems they have, the weaknesses, the blindness bias they have.

02:51.280 --> 02:56.200
So I can say this is sort of paradigm shift that we are seeing in NLP, you know, sort

02:56.200 --> 03:02.960
of be art in 2020 now, kind of started, I can, you know, reflect back on the decade.

03:02.960 --> 03:08.400
This paradigm should have started, you know, back in 2015 to 2016 or so.

03:08.400 --> 03:13.960
And various NLP tasks could, you know, start to get tackled by a relatively, you know,

03:13.960 --> 03:18.880
straightforward approach that you would just encode the input text.

03:18.880 --> 03:24.040
It could be, you know, looked at as a sequence of words, sequence of characters, et cetera.

03:24.040 --> 03:32.640
Then you use, like, attention to actually basically look back into the encoded representation

03:32.640 --> 03:36.720
when you're trying to predict something for the task, which could be a sequence of

03:36.720 --> 03:37.720
output tokens.

03:37.720 --> 03:44.560
So back in the late 2017 or so, you know, Chris Manning, which is one of the paniers of

03:44.560 --> 03:50.400
our field had this, basically, there was this code and belief from him that he's, he

03:50.400 --> 03:56.440
believed in biosteums, he had a hegemony, which he believed that basically, no matter what

03:56.440 --> 04:02.340
the task is out there, an LP task, if you try a biosteum at it and use attention to

04:02.340 --> 04:10.080
attend back to the, basically input, input encoding of the input, you basically can achieve

04:10.080 --> 04:11.080
state of the art.

04:11.080 --> 04:14.800
Now, this referring back to the attention is all you need paper.

04:14.800 --> 04:18.760
So attention is that only you need paper is more recent, so that was then the transform

04:18.760 --> 04:19.760
is came to picture.

04:19.760 --> 04:22.320
This has been LSTNs where it's still a thing, right?

04:22.320 --> 04:23.320
That's amazing.

04:23.320 --> 04:29.760
That how fast the field is moving in 2017 is still the, as I said, like the consensus

04:29.760 --> 04:34.800
in an LP was that you can achieve achieved state of the art if you just throw a biosteum

04:34.800 --> 04:35.800
at it with attention.

04:35.800 --> 04:37.400
That was the recipe.

04:37.400 --> 04:41.600
And back in that time, I remember like when I was like giving talks, I would conclude

04:41.600 --> 04:46.440
that look, although that has been true for a bunch of, a host of different benchmarks,

04:46.440 --> 04:51.840
it happens that for the test, it requires vast amounts of background knowledge, reasoning

04:51.840 --> 04:58.120
and basically require establishing a long context, we can, we can not yet achieve a state

04:58.120 --> 05:02.800
of the art or near human performance using these biosteum models.

05:02.800 --> 05:10.040
So fast forward, just one year, in 2018, we had like LMO, this deep contextualized word

05:10.040 --> 05:17.360
representation that basically started sort of this one more step forward of building these

05:17.360 --> 05:22.480
large language models, which happened to be contextualized, so preaching on a very

05:22.480 --> 05:28.360
large corpus and then fine tune on downstream test, which itself started meeting lots and

05:28.360 --> 05:34.480
lots of different state of the arts and establishing, you know, brand new state of the arts.

05:34.480 --> 05:39.760
And so the test that I had in mind when I was personally criticizing the fact that, oh,

05:39.760 --> 05:46.240
look, by throwing biosteums with attention on a particular benchmark, you don't necessarily

05:46.240 --> 05:50.480
issue a state of the art, for a conist's reasoning task, which is something that I personally

05:50.480 --> 05:55.120
very passionate about and happens to be a minor line of research.

05:55.120 --> 06:00.240
So the particular task was a story closed test, which I talked with you in the last

06:00.240 --> 06:05.320
time I talked with you, specifically a story closed test, which is this task that given

06:05.320 --> 06:11.960
a sequence of four sentences, which form a coherent story, very short story, the task is

06:11.960 --> 06:16.800
to choose between two alternative endings to that story, which, you know, is designed

06:16.800 --> 06:22.200
basically to evaluate a system's commonsense reasoning capabilities.

06:22.200 --> 06:28.520
So what happened in 2017 is that in mid 2017 or so, the attention is all unique paper came

06:28.520 --> 06:33.840
out, the transformer paper that you just mentioned a minute or two ago.

06:33.840 --> 06:41.520
So that paper basically enabled a cascading effect of other very large pre-trained transformer

06:41.520 --> 06:46.880
models that could actually establish the state of the art in various commonsense reasoning

06:46.880 --> 06:47.880
tasks.

06:47.880 --> 06:55.160
And being the GPT-1 paper, so the GPT-1 paper came out around, in 2018, which was, you

06:55.160 --> 06:58.160
know, this, they called it like, generative pre-training model.

06:58.160 --> 07:03.920
This was a very large language model that open AI folks have basically trained on a very

07:03.920 --> 07:08.400
large diverse corpus and then fine tune on a small data sets.

07:08.400 --> 07:14.520
And actually, the data set that they highlighted as to the place where they've made the most,

07:14.520 --> 07:20.000
you know, amazingly, basically, progress happened to be a story close to the, you know, benchmark

07:20.000 --> 07:22.760
that I really cared about.

07:22.760 --> 07:29.040
So they had gotten, you know, notably they had gotten like around 86 or so percent accuracy

07:29.040 --> 07:34.760
in which was exceedingly better than the previous numbers that people had reported on

07:34.760 --> 07:36.000
the test set.

07:36.000 --> 07:41.640
And so that really sort of changed my personal mind about verbure going with this.

07:41.640 --> 07:47.400
I started believing in the fact that, oh, look, although these models may seem to be sort

07:47.400 --> 07:53.360
of doing pattern recognition at the scale, which may not go hand in hand with doing reasoning

07:53.360 --> 07:57.520
and connecting to that and all these sorts of things that we care about and label as commonsense

07:57.520 --> 08:02.760
reasoning, if we, you know, do them in the right way or give these models enough chance

08:02.760 --> 08:07.560
of being trained for, you know, on the right data sets, fine tune on right data sets, etc.

08:07.560 --> 08:10.400
They are actually capable of doing knowledge transfer.

08:10.400 --> 08:17.880
So I think that sort of set the ground up for us to move into 2019, verbure had more

08:17.880 --> 08:25.040
and more of these very large pre-trained models that then you could basically fine tune

08:25.040 --> 08:30.560
on various downstream tasks and establish a state of the art, no matter whether or not

08:30.560 --> 08:35.680
they are from our very core and healthy tasks, like a task such as product speech tagging

08:35.680 --> 08:40.440
or very, like, semantically oriented tasks such as the story costus itself, commonsense,

08:40.440 --> 08:41.760
reasoning, etc.

08:41.760 --> 08:47.920
So I think this has been the main exciting thing about 2019, where we could see that this

08:47.920 --> 08:53.800
wasn't just the, you know, glimpse of how this wasn't just the one time thing that these

08:53.800 --> 08:58.160
models could perform well, it continued into 2019.

08:58.160 --> 09:03.760
And I think I'm actually excited about seeing verbure go with improving these, and, you

09:03.760 --> 09:08.520
know, we will talk more about the downsides of these models, but yeah, I'm very excited

09:08.520 --> 09:12.840
to see verbure are going with this paradigm shift into 2020.

09:12.840 --> 09:20.240
Yeah, I chatted a little bit in one of my previous conversations in this series.

09:20.240 --> 09:26.480
It was a conversation with Zach Lipton, in particular, about the role that these transformer

09:26.480 --> 09:31.320
models have played in NLP, and his take was pretty interesting.

09:31.320 --> 09:37.360
It was focused on the notion that the amount of compute that went into creating these

09:37.360 --> 09:44.000
models, creates a huge barrier for, or sets a new kind of a new standard that creates

09:44.000 --> 09:50.120
a huge barrier for folks that want to do future research on the model side, the amount

09:50.120 --> 09:55.840
of compute required to develop models that can achieve state-of-the-art performance

09:55.840 --> 09:59.800
is, you know, such a high bar, are you seeing that as well?

09:59.800 --> 10:04.560
Absolutely, actually, one of the papers that I wanted to highlight, which I think I can

10:04.560 --> 10:10.400
just highlight it now, is this very amazing work, I would say, that came out of UMass, this

10:10.400 --> 10:17.040
is from Struvo et al, a CL 2019 paper called Energy and Policy Consideration for Deep Learning

10:17.040 --> 10:18.040
in NLP.

10:18.040 --> 10:23.920
So I would say that, yes, as I was saying, we've come a very long way in making advancements

10:23.920 --> 10:29.280
in NLP, and through these very large pre-trained models that we are building, but they have

10:29.280 --> 10:34.720
two major sort of policy, you know, like external implications, right?

10:34.720 --> 10:41.480
One of them is the fact that these are, these require really expensive and extensive resources,

10:41.480 --> 10:47.200
you know, millions of dollars are basically used, you know, in terms of like cloud, etc.,

10:47.200 --> 10:53.120
for basically building these models, which is very much sort of unique and makes it

10:53.120 --> 10:58.520
entitled to the top players in the field, such as like the, you know, large tech companies.

10:58.520 --> 11:05.280
And I think that this sort of implies, as if AI research would tend to get privatized

11:05.280 --> 11:11.800
and only accessible to the players in industry, with access to resources, which is of course

11:11.800 --> 11:16.960
not fair, is not fair and it will have lots of other implications for the society as

11:16.960 --> 11:22.360
whole and who will have access to these kinds of amazing outcomes of AI.

11:22.360 --> 11:27.560
So I think that's definitely a major problem that we have.

11:27.560 --> 11:31.280
And along with that, the reason I wanted to highlight this paper is that something else

11:31.280 --> 11:38.280
that we have in Manpada, as much about art, environmental implications, basically, of

11:38.280 --> 11:41.640
these large models that we are building, right?

11:41.640 --> 11:47.360
So this paper that I referenced, Energy and Policy Concereship for Deep Learning, says

11:47.360 --> 11:51.720
that although people keep talking about the fact that we are throwing, we need to throw

11:51.720 --> 11:58.640
so much money at these models, which only a handful of players are capable of doing, we

11:58.640 --> 12:04.680
also are basically increasing our carbon footprint.

12:04.680 --> 12:10.040
So the tagline was actually, which got a lot of news coverage, was that training a single

12:10.040 --> 12:15.640
AI model can emit as much carbon as five cars in their lifetime.

12:15.640 --> 12:18.960
And I think that's pretty, you know, crazy, right?

12:18.960 --> 12:23.840
I think that the number of reciting was something around like, you know, more than like half

12:23.840 --> 12:29.520
a million pounds of carbon dioxide is emitted after just basically training one of these

12:29.520 --> 12:32.160
large models that we were just talking about.

12:32.160 --> 12:37.440
So I think that that's just a major consideration that we should take into account moving forward

12:37.440 --> 12:39.240
as a field.

12:39.240 --> 12:44.640
It's definitely huge and I would refer folks interested in learning more about that to

12:44.640 --> 12:52.760
check out my interview from back in July of 2019 with MS Trubel, the author of the paper

12:52.760 --> 12:55.120
that you're referring to.

12:55.120 --> 13:01.320
So with that in mind, when you stepped back and thought about some of the more important

13:01.320 --> 13:07.520
things or more interesting things to you in 2019, you divided that into a couple of key

13:07.520 --> 13:08.520
trends.

13:08.520 --> 13:11.720
Do you want to talk about the first of those?

13:11.720 --> 13:20.080
Sure, absolutely. So the first theme that I wanted to highlight was interpretability,

13:20.080 --> 13:23.120
ethics, fairness and bias in NLP.

13:23.120 --> 13:29.000
So this also happened to be one of the traumatic paper tracks for our, you know, one of our

13:29.000 --> 13:32.280
major conferences in 2019.

13:32.280 --> 13:39.560
And I think the time is actually ripe for us as a community to start thinking about the,

13:39.560 --> 13:44.600
you know, ethical implications of our work and basically, I think in beyond just making

13:44.600 --> 13:49.520
scientific improvements, but also about what are we actually enabling.

13:49.520 --> 13:54.880
So I think it's been really great in the shape learning in AI community as a whole that

13:54.880 --> 13:58.080
in the past like three, four years or so.

13:58.080 --> 14:03.240
A lot of players in the field are talking about ethics in AI, but the truth is that I think

14:03.240 --> 14:10.240
it has been long overdue and we need to educate practitioners and scientists so much more

14:10.240 --> 14:11.240
on the topic.

14:11.240 --> 14:17.320
And I think it's been really a positive change that in our conferences at least, we've started

14:17.320 --> 14:24.240
making particular tracks, particular themes, et cetera, for highlighting these particular

14:24.240 --> 14:27.760
considerations and giving them the credit that they deserve.

14:27.760 --> 14:39.760
Yeah, it seems like not long ago, the conversation in this area in NLP was rather more simplistic

14:39.760 --> 14:41.440
than it is today.

14:41.440 --> 14:47.680
You know, we would talk a lot about the kind of the word to VEC example, you know, and

14:47.680 --> 14:50.880
several of those were popular.

14:50.880 --> 14:55.880
But now it seems like the conversation is quite a bit more nuanced.

14:55.880 --> 14:57.880
Is that something you would agree with?

14:57.880 --> 14:58.880
Yes, absolutely.

14:58.880 --> 15:04.680
The field is definitely maturing and as I said, the fact that we are establishing particular

15:04.680 --> 15:12.640
tracks for just soliciting papers and submissions for these particular considerations is definitely

15:12.640 --> 15:14.760
helping that movement.

15:14.760 --> 15:19.280
And yeah, I think I can go ahead and tell you a little bit more about the particular

15:19.280 --> 15:24.240
papers that I had in mind that I wanted to highlight.

15:24.240 --> 15:28.000
So I think I will go ahead and talk about, so there are different angles right to this

15:28.000 --> 15:29.000
problem.

15:29.000 --> 15:35.600
Sort of ethics and fairness in AI and like de-biasing, basically, AI and hands-on-up

15:35.600 --> 15:44.320
models is one end of things and then basically building explainable AI in NLP systems is the

15:44.320 --> 15:49.640
other end of a spectrum because we want to have these systems be accountable towards

15:49.640 --> 15:54.000
the predictions that they are making which goes hand in hand with sort of de-biasing them

15:54.000 --> 15:59.120
or, you know, basically better societal use cases that they could have.

15:59.120 --> 16:03.240
So I will start with the explanation one.

16:03.240 --> 16:08.680
So explanation is this really overloaded term and, you know, today probably won't be

16:08.680 --> 16:13.400
the day that you are going to cover what explanation means.

16:13.400 --> 16:19.360
But I'm going to highlight one paper through which I'm going to mention a few other papers

16:19.360 --> 16:26.440
that sort of started a debate in the field in NLP field this year about explanation.

16:26.440 --> 16:32.160
So this paper is titled Attention is Not Explanation, it's a work that came out of North

16:32.160 --> 16:38.560
Eastern University, published in NACCLE 2019, the authors were Valous and Jean.

16:38.560 --> 16:44.400
So this paper, as the title suggests, is talking about attention and not being explanation.

16:44.400 --> 16:49.960
And so what they're actually trying to highlight is the fact that you remember just a few

16:49.960 --> 16:57.880
minutes ago, I was talking about this paradigm of encoding and then attending and then decoding

16:57.880 --> 17:00.760
for doing multiple NLP tasks.

17:00.760 --> 17:08.440
So attention has been used and often presented at least implicitly as this relative importance

17:08.440 --> 17:12.040
of input kind of a measurement that we've had in the field.

17:12.040 --> 17:19.320
Basically pretty much like a common citation for summarizing this commonly helped you by

17:19.320 --> 17:26.080
Leah Alt has a 16 paper, it was that attention sort of provides an important way of explaining

17:26.080 --> 17:28.000
the inner workings of neural models.

17:28.000 --> 17:33.640
So it's like pretty much, it was pretty much established until this conversation was

17:33.640 --> 17:37.760
started by this paper, that attention is something that you can count as explanation.

17:37.760 --> 17:43.680
Again, I'm not going to argue or basically define what explanation means, but even loosely

17:43.680 --> 17:48.560
there were enough people in the community to count it's attention as explanation.

17:48.560 --> 17:52.960
And you know, for me, like as someone working in Communism's reasoning, caring about deep

17:52.960 --> 17:58.160
natural language understanding, like basically going beyond what's explicit out there, et

17:58.160 --> 18:03.880
cetera, I personally took so many issues with that leave because as you can imagine, there

18:03.880 --> 18:09.320
are so many tasks where your answer or whatever the inner workings of your reasoning engine

18:09.320 --> 18:14.000
is, of your reason, reason paradigm is it's not going to be anything explicit in the input

18:14.000 --> 18:17.840
that you can even highlight as the attention waits.

18:17.840 --> 18:22.920
So that's a major obviously shortcoming, but sitting data side even for tasks like say

18:22.920 --> 18:29.000
a squad, et cetera, where you are actually going to attend literally to parts of the input

18:29.000 --> 18:34.880
of text to provide the prediction is still like people were using that as their explanation.

18:34.880 --> 18:41.520
So this work actually was critical of that premise, basically, it was claiming that it

18:41.520 --> 18:48.600
has been unclear what is the relationship between attention waste and the model outputs.

18:48.600 --> 18:54.640
And so they argue that if attention wants to be a faithful explanation for any models

18:54.640 --> 18:59.320
prediction, it should have two particular characteristics.

18:59.320 --> 19:06.080
One is that there should be a correlation between the inputs and outputs, which means that

19:06.080 --> 19:12.880
the way that they quantify this is that attention waste should be correlated with measures

19:12.880 --> 19:15.320
of feature importance that we have.

19:15.320 --> 19:21.320
And then the second point that they make that they think for a faithful explanation should

19:21.320 --> 19:27.200
be held to is the fact that the models explanation should be exclusive, meaning that if we change

19:27.200 --> 19:32.880
the attention distribution dramatically, of course the prediction should also change.

19:32.880 --> 19:38.560
So these are the two main basically points that they made and they went ahead and presented

19:38.560 --> 19:45.360
actually various experiments for showing that for the first point that actually attention

19:45.360 --> 19:51.640
waste are not correlated with measures of feature importance like the grading based ones.

19:51.640 --> 19:56.120
And for the second one, they actually showed that even if you shuffle like randomly shuffle

19:56.120 --> 20:01.320
the distribution of the attention rates, there are many cases where the predictions are

20:01.320 --> 20:03.440
actually going to stay constant.

20:03.440 --> 20:09.640
So they conclude that the standard attention modules do not provide any meaningful and systematic

20:09.640 --> 20:15.760
explanations of basically the community should stop treating them as such.

20:15.760 --> 20:20.360
The interesting thing that happened after this paper came out, and as I said, it was

20:20.360 --> 20:25.720
accepted and published at NACCLE of an overmajor conference, was that there was a follow-up

20:25.720 --> 20:31.000
paper to it which was titled Attention is Not to Not Explanation.

20:31.000 --> 20:41.880
Yeah, so this was a work that came out of Georgia Tech, again like publishing ACL in

20:41.880 --> 20:49.760
EMNLP 2019, sorry, that was arguing that this approach that the authors of the attention

20:49.760 --> 20:53.880
is not explanation took had some problems, right, and then there was some back and forth

20:53.880 --> 20:57.920
they actually encouraged the audience that if they're interested they can go and read

20:57.920 --> 21:05.040
the blog post or respective blog post that they had basically arguing the different points

21:05.040 --> 21:06.040
that they had.

21:06.040 --> 21:11.760
But I think that the conclusion is that I think this whole thread was very healthy for the

21:11.760 --> 21:18.880
community to start thinking about such presumptions that we make before digging deeper and basically

21:18.880 --> 21:22.320
proving what we are counting as XYZ.

21:22.320 --> 21:30.000
So I think that was a very interesting example of a good scientific contribution to the

21:30.000 --> 21:38.320
community where we go back in time and look at what we assume to be true and just dig deeper.

21:38.320 --> 21:43.720
And so in line with that I actually want to mention, so there has been lots of other

21:43.720 --> 21:52.400
actually follow-up papers, I want to highlight one actually toolkit that came out of AI2

21:52.400 --> 21:54.760
which is called Alan LK Interprete.

21:54.760 --> 21:59.720
So this actually happened to get the best demo paper awarded our major, one of our major

21:59.720 --> 22:06.040
conferences as well, which is a toolkit that makes it easy for different people to apply

22:06.040 --> 22:11.360
and visualize actually such saliency maps for whatever model they're deploying.

22:11.360 --> 22:19.000
I think this whole thread, as I was saying, was very good for reminding people that you

22:19.000 --> 22:24.680
have to think about interpretability, you have to think about what you count as interpretable

22:24.680 --> 22:31.960
and for the very least you should be able to visualize what is salient and how you can

22:31.960 --> 22:34.720
do adversarial attacks towards different models.

22:34.720 --> 22:41.120
And I think the kind of open source tooling that AI2 does is really helpful for enabling

22:41.120 --> 22:48.440
individuals across like academia and industry to basically dig deeper and deliver on the

22:48.440 --> 22:51.120
premise of interpretability.

22:51.120 --> 22:57.320
Is there a quick way for you to summarize where the community ended up through this back

22:57.320 --> 23:01.960
and forth and the subsequent papers on this issue of the relationship between attention

23:01.960 --> 23:03.760
and explanation?

23:03.760 --> 23:10.480
Yes, so I would say that this is just my personal view.

23:10.480 --> 23:16.520
I told you that the fateful explanation that the authors of the original paper we're talking

23:16.520 --> 23:18.960
had this was this twofold thing.

23:18.960 --> 23:22.560
They were saying that there should be a correlation between inputs and outputs.

23:22.560 --> 23:27.800
I think that the way that they had done it wasn't rigorous enough in the eyes of the, you

23:27.800 --> 23:33.720
know, other paper, but the way that they had rebuttal actually to me again as someone

23:33.720 --> 23:35.400
reading their argument made sense.

23:35.400 --> 23:42.920
I think they did a good enough of a job with justifying why the correlation was in place.

23:42.920 --> 23:47.720
But I do agree with the authors, the Georgia Tech authors that yes, explanation is this

23:47.720 --> 23:49.760
very loosely defined term.

23:49.760 --> 23:56.360
It's not clear what the original authors meant by explanation and maybe that's title of

23:56.360 --> 24:00.000
attention is not explanation was too overloaded, right?

24:00.000 --> 24:03.400
They could have specified what they mean by explanation.

24:03.400 --> 24:10.200
I think, yes, so I would say that the community should stick to not calling attention,

24:10.200 --> 24:11.200
explanation.

24:11.200 --> 24:17.880
So it's one thing though that I actually agreed with the original authors paper.

24:17.880 --> 24:22.560
Georgia authors point was that they had said that although that they would seem that that

24:22.560 --> 24:28.400
title is overloaded, it's as saying that, you know, correlation is not causation.

24:28.400 --> 24:31.400
It doesn't mean that correlation can never be causation, right?

24:31.400 --> 24:36.040
There are, if you do your studies rigorously, et cetera, there are types of correlation

24:36.040 --> 24:38.960
which are in the causation.

24:38.960 --> 24:43.120
But you can still say that in a sense that, oh, look, be careful, don't count correlation

24:43.120 --> 24:44.120
as causation.

24:44.120 --> 24:51.440
So I think, yeah, so that's pretty much my overview of observing the back and forth.

24:51.440 --> 24:55.640
And then the next paper that you identified is more on the kind of fairness and bias

24:55.640 --> 24:58.640
and of the spectrum, which one was that?

24:58.640 --> 25:05.720
So that paper was titled, what's in a name, reducing bias and values without access to

25:05.720 --> 25:08.240
protected attributes.

25:08.240 --> 25:15.600
So that was a paper that came out in, for example, our knackle 2019 and actually won the

25:15.600 --> 25:25.080
best thematic paper award a knackle by Romanov at all, Umanas Loel, MSR, CMU, collaboration.

25:25.080 --> 25:31.520
So the reason I wanted to highlight this paper is, well, first of all, it happened to have

25:31.520 --> 25:35.720
been highlighted by the community before by getting the best paper award.

25:35.720 --> 25:41.480
But second of all, they had a pretty amazingly simple approach and yet strong results, which

25:41.480 --> 25:47.160
I think should be something that we do more and more so in our community.

25:47.160 --> 25:54.080
So basically, this paper highlights the fact that, look, we are at this day and age deploying

25:54.080 --> 25:59.200
lots and lots of AI systems that are automating decision-making in our daily lives.

25:59.200 --> 26:03.320
And some of these decision-making scenarios are high stakes.

26:03.320 --> 26:08.720
So for example, like we have applications of AI in criminal justice, we have it in overcruiting,

26:08.720 --> 26:09.720
et cetera.

26:09.720 --> 26:16.680
And having deploying bias models can basically yield very negative outcomes in people's daily

26:16.680 --> 26:17.680
lives.

26:17.680 --> 26:22.120
And we should be, like, as a community mindful, as like practitioners, again, as scientists,

26:22.120 --> 26:26.360
the term should be really mindful about the such implications of the models that you're

26:26.360 --> 26:27.360
building.

26:27.360 --> 26:34.120
So, you know, as you were saying earlier, there are, there have been, like, these representational

26:34.120 --> 26:40.440
biases, like, about award embedding and how, like, I don't know, if you do, like, the

26:40.440 --> 26:48.280
classical analogy for Wurtubek, like, X's to Y, like, as, like, Z's to what, if you do

26:48.280 --> 26:52.040
the analogy for, like, say, we are talking about recruiting, right?

26:52.040 --> 26:56.080
You want to know what kind of jobs go with what kind of people.

26:56.080 --> 27:01.680
So if you're an adjust man to computer programmer, it's, like, been cited a lot around that

27:01.680 --> 27:03.560
it would save women as to homemaker, right?

27:03.560 --> 27:08.760
And these are obviously very problematic when these kind of, these kinds of, like, representational

27:08.760 --> 27:15.680
biases can turn into, like, a really harmful, allocative biases in downstream tasks.

27:15.680 --> 27:21.800
So this paper is particularly trying to address how to mitigate these allocative harms that

27:21.800 --> 27:25.920
come out of these, you know, bias representations.

27:25.920 --> 27:31.120
Their tagline is pretty cool, actually, their tagline is five bias with bias, which is really

27:31.120 --> 27:32.120
awesome.

27:32.120 --> 27:38.920
Agline, they say that they basically want to leverage bias representations, like, word embeddings

27:38.920 --> 27:44.440
basically, to the bias of classifier, so very simple idea, strong results.

27:44.440 --> 27:45.440
So what did they do?

27:45.440 --> 27:51.800
They actually based their study on a prior data set on occupation classification.

27:51.800 --> 27:59.360
This is a data set of 400,000 or so public bias of biographies, short biographies of

27:59.360 --> 28:05.080
different individuals that are aligned with the 28 different possible occupations that

28:05.080 --> 28:06.080
they could have.

28:06.080 --> 28:10.400
So you read, like, little paragraph of, like, you know, expires, you did this and this

28:10.400 --> 28:15.880
and then there's a title of the occupation matched with.

28:15.880 --> 28:24.160
So prior work had shown that bias exists in this task, which is, so in the way that when

28:24.160 --> 28:31.000
you're trying to predict what is the occupation, it's supervised in terms of gender and race.

28:31.000 --> 28:38.160
So the way that they're sort of measuring this bias is by the classification accuracy

28:38.160 --> 28:39.560
gap that they are seeing.

28:39.560 --> 28:43.840
So this is also, like, this was a prior work that came before this work by, you know,

28:43.840 --> 28:49.720
the same sort of team of authories, very de-quantified this problem as the true positive

28:49.720 --> 28:54.640
rate that existed, the true positive rate difference that exists between genders for this

28:54.640 --> 28:56.760
particular downstream tasks.

28:56.760 --> 29:00.640
So they have this, I really enjoyed reading this paper, they have this very nice graph

29:00.640 --> 29:07.520
that they show that, for example, it's more accurate to predict the job of, I don't

29:07.520 --> 29:13.040
know, like being a model for a female, that it is to predict the job of being a doctor,

29:13.040 --> 29:15.040
physician for a female.

29:15.040 --> 29:21.680
And the fact is that because the bias in the actual true positive, the population already

29:21.680 --> 29:27.880
exists, it's sort of this compounding bias that happens at prediction time, which was also

29:27.880 --> 29:30.680
supported by earlier work.

29:30.680 --> 29:35.360
One interesting thing I want to mention is that you would think that if that maybe these

29:35.360 --> 29:40.320
models, so imagine you're just building your most vanilla classifier, right, imagine

29:40.320 --> 29:46.960
the biased model that I was saying, you just feed in the bios, you attend, et cetera,

29:46.960 --> 29:51.880
you make a prediction, 28 categories, like labels that you're generating.

29:51.880 --> 29:56.680
You would think that if you scrub the gender indicators from the bios, let's say like the,

29:56.680 --> 30:03.120
you know, the proper nouns, the, you know, gender products, et cetera, maybe you will be

30:03.120 --> 30:08.720
able to de-biased these models, meaning that you can shrink that true positive rate gap

30:08.720 --> 30:10.200
that I was mentioning.

30:10.200 --> 30:14.960
But they, this study in the prior work actually showed that scrubbing such explicit gender

30:14.960 --> 30:18.360
indicators does no good, so no difference at all.

30:18.360 --> 30:25.080
So the same accuracy, same TPR, like true positive rate gap, as with a model that uses

30:25.080 --> 30:31.600
the explicit gender indicators, which goes to showing that the bias is source from elsewhere,

30:31.600 --> 30:40.160
which is a very interesting kind of a realization that this work and the prior work have had.

30:40.160 --> 30:45.320
So in order to overcome this problem, they, they have this very simple yet super effective

30:45.320 --> 30:52.200
idea that they are saying that we want to use the embedding of names as the universal

30:52.200 --> 30:56.960
proxies for race, gender, and presumably age.

30:56.960 --> 31:03.440
So what they say is that look turns out in names of individuals, but just, you know, different

31:03.440 --> 31:07.960
kind of proper nouns, the names and family names, we are already encoding lots and lots

31:07.960 --> 31:08.960
of biases.

31:08.960 --> 31:15.040
So they even like show like they prove in the paper show sort of how the gender and race

31:15.040 --> 31:20.400
could be core highly correlated with these names that you cluster them.

31:20.400 --> 31:26.920
So they go ahead and define this very simple way of sort of debiasing the classifier that

31:26.920 --> 31:33.080
you build by discouraging the model to learn a correlation between the name embedding and

31:33.080 --> 31:35.040
the predict, predicted label.

31:35.040 --> 31:41.720
So get your any, you know, vanilla classifier, all you do is that you swap in your existing

31:41.720 --> 31:46.920
objective function with this new objective function that now penalizes, basically penalizes

31:46.920 --> 31:55.040
the model if there's a correlation between the name embedding and the predicted label.

31:55.040 --> 32:01.000
So they show very like really strong results that by doing so, they can really minimize

32:01.000 --> 32:06.880
the gap, the TPR gap that I was mentioning.

32:06.880 --> 32:11.400
So that was their conclusion that this is achievable, basically moving forward.

32:11.400 --> 32:18.000
If you are deploying map data models and industry in really high stake situation, this name

32:18.000 --> 32:22.760
embedding has happened to be a good proxy for debiasing the model.

32:22.760 --> 32:28.320
But they emphasize that the bias is not zero yet, so there is definitely further room

32:28.320 --> 32:35.640
for improvement of such high stake, this predictive models in future.

32:35.640 --> 32:43.640
So kind of when you think about the relationship between the explainability aspect of the first

32:43.640 --> 32:50.320
paper that you mentioned and the bias fairness, you know, do you, you know, there are other

32:50.320 --> 32:56.600
papers that are in different kind of points on this axis that are worth mentioning for

32:56.600 --> 32:59.240
folks that want to dig in deeper.

32:59.240 --> 33:06.240
No, nothing very particular in mind, I do think that again, these are kind of new developments

33:06.240 --> 33:08.400
in the NLP community.

33:08.400 --> 33:15.080
And I think, you know, there's this definitely strong like connection between building models

33:15.080 --> 33:21.000
that can explain themselves and hence us being able to diagnose their bias towards their

33:21.000 --> 33:22.000
predictions.

33:22.000 --> 33:25.720
Nothing else that I can think of right now, honestly.

33:25.720 --> 33:30.440
But yeah, there are actually conferences, outside of the NLP community, like the fat

33:30.440 --> 33:35.160
conference, et cetera, that have lots of amazing work coming out of them.

33:35.160 --> 33:40.400
In the, you know, it's the same area, maybe they take language as one of their tasks that

33:40.400 --> 33:44.120
every now and then they report results on.

33:44.120 --> 33:48.240
And I think that people should definitely check those conferences out.

33:48.240 --> 33:52.080
But it sounds like an area that you expect to see more of in the future, but I guess we'll

33:52.080 --> 33:55.920
get to predictions, let's not get ahead of ourselves.

33:55.920 --> 34:03.720
And before we do that, kind of the next batch of papers that you identified go back to kind

34:03.720 --> 34:12.160
of your initial take on 2019 and the role of these large pre-trained models, walk us

34:12.160 --> 34:15.000
through the papers that you had in mind there.

34:15.000 --> 34:16.000
Sure.

34:16.000 --> 34:24.320
So I think it's a needless to say that this year has been the year of transfer learning

34:24.320 --> 34:31.960
for NLP sort of continue, as I was saying, continuing on 2018, but more so maybe 2019, because

34:31.960 --> 34:36.000
we saw a real world impact through this work, basically.

34:36.000 --> 34:44.640
So I want to mainly highlight two main such models, one birth and one GPT-2, which I think

34:44.640 --> 34:51.480
people have heard enough of, but I don't think that we can really end the year without

34:51.480 --> 34:53.720
sort of mentioning them at least.

34:53.720 --> 35:00.240
So as like I'm sure probably your audiences have heard a lot, branches despite directional

35:00.240 --> 35:05.480
and encoder from transfer model by Google AI folks.

35:05.480 --> 35:11.680
It came out actually in 2018, so kind of sort of maybe not 2019 paper, but it actually

35:11.680 --> 35:17.000
got officially published in knackle 2019 and got the base paper worth there.

35:17.000 --> 35:21.160
So I think, you know, whatever we can count it 2019 paper.

35:21.160 --> 35:27.680
And this paper basically is just yet another large pre-trained language model, maybe the

35:27.680 --> 35:32.520
only main difference that is notable is the fact that their training objective was different.

35:32.520 --> 35:38.040
They had this training objective called MAST language model, but the main reason that

35:38.040 --> 35:42.960
this paper got as much attention as it did was the fact that right after it came out

35:42.960 --> 35:50.280
and throughout the ML community, it was achieving different states of the art for wide variety

35:50.280 --> 35:57.520
of an LP task, ranging from question answering to national language inference, etc.

35:57.520 --> 36:03.520
So I was just checking the end of day and to this day this paper has collected like 2,300

36:03.520 --> 36:10.040
citations and counting and it's definitely, I think, by any stretch of imagination the

36:10.040 --> 36:13.160
paper of the year in terms of the impact it has had.

36:13.160 --> 36:17.560
And I think I don't want to spend like much time talking about like all the other models

36:17.560 --> 36:22.200
that came out, like there's so many models that have, you know, been built sort of on top

36:22.200 --> 36:26.080
of birds and being inspired by birds, or birds, etc.

36:26.080 --> 36:30.840
But I think the main thing that I want to highlight is the fact that Bert got used a lot

36:30.840 --> 36:37.720
in industry, basically at Disney and I just, there's so many, like, I was personally surprised

36:37.720 --> 36:42.560
and you would see like these different startups that even their job posts, one of the requirements

36:42.560 --> 36:46.840
that they've list is like, oh, like you have to have work with Bert and I'm like, what?

36:46.840 --> 36:47.840
Like, what?

36:47.840 --> 36:53.680
I mean, living and it makes like Bert becomes like a requirement for like people to recruit.

36:53.680 --> 37:01.160
So this just goes to saying that there was this fear of missing out in the, you know, industry

37:01.160 --> 37:05.760
for people who were not actually improving whatever underlying and healthy pipelines they

37:05.760 --> 37:07.120
had through bird.

37:07.120 --> 37:13.000
And as I said, it was due to the fact that there were so many positive signals from the,

37:13.000 --> 37:19.440
you know, corresponding positive and progress in the other tests that everyone thought

37:19.440 --> 37:23.760
that, oh, whatever XYZ test they're working on should also be one of them.

37:23.760 --> 37:28.920
So anyways, that, that's one of the interesting, I would say, observations about the affected

37:28.920 --> 37:30.760
Bert had in the community.

37:30.760 --> 37:38.680
And the second is I think the main notable use case of even maybe NLP, but at least Bert

37:38.680 --> 37:44.840
in the industry was the fact that Google, itself, Google search itself, I reported that

37:44.840 --> 37:49.000
they have not incorporated Bert into their search engines.

37:49.000 --> 37:50.280
This is pretty grand, right?

37:50.280 --> 37:57.520
Like Google being one of the major tech companies search being the major of that company.

37:57.520 --> 38:04.360
I think this is just really, congratulations to the authors of the Bert paper who, you

38:04.360 --> 38:06.120
know, this is making real world impact.

38:06.120 --> 38:11.880
And I think that as research scientists, a lot of us basically dream of being able to

38:11.880 --> 38:16.440
make something in real world that actually serves a real problem.

38:16.440 --> 38:22.360
So Google actually was, you know, cited the exciting in their blog post that they, like,

38:22.360 --> 38:29.520
I don't know, like one out of 10 search queries are now improved by using Bert for both

38:29.520 --> 38:33.840
re-ranking of the hits that you retrieve and you make a query.

38:33.840 --> 38:38.800
And also generating those snippets that are, like, these little summaries of the pages

38:38.800 --> 38:43.720
basically that are retrieved, which is, you know, pretty amazing to hear, honestly, as

38:43.720 --> 38:50.480
just an LLP researcher. And I think that the main difference that they were citing was

38:50.480 --> 38:56.880
the fact that now, because this is going from keyword search, which is, like, you know,

38:56.880 --> 39:01.920
all the school, like, just search, like, information retrieval, et cetera, they are moving away

39:01.920 --> 39:05.240
from that on towards natural language understanding for search.

39:05.240 --> 39:11.040
They are capable of doing much more sophisticated query understanding, natural language understanding.

39:11.040 --> 39:16.160
So, like, for example, they highlighted the fact that they are now understanding prepositions

39:16.160 --> 39:21.600
much better than they used to. So, for example, queries such as, I don't know, like,

39:21.600 --> 39:29.600
2019 Brazil traveler to USA needs a visa. They were saying that before Bert, they didn't

39:29.600 --> 39:37.080
know that this means that, like, it should be a Brazilian traveling to US. But now with

39:37.080 --> 39:41.760
Bert, they know that, like, what that to preposition actually means, and hence retrieve

39:41.760 --> 39:48.360
better results, which is, you know, pretty, I would say, amazing outcome for the community

39:48.360 --> 39:49.840
to see such an impact.

39:49.840 --> 39:54.600
That's awesome. Yeah. I have definitely, well, I guess it goes without saying that I've

39:54.600 --> 40:03.760
seen it all over the place as well. But you also mentioned GPT-2. Did you want to chat

40:03.760 --> 40:08.480
about that as well? Absolutely. So, another thing that we cannot go

40:08.480 --> 40:15.520
without talking about is GPT-2. GPT-2 definitely in the way that the whole, you know, release

40:15.520 --> 40:21.240
of it was handled was one of the highlights of the year in AI, for sure, let alone NLP.

40:21.240 --> 40:28.840
So, just, you know, for whoever that might not be familiar with GPT-2 was this another

40:28.840 --> 40:35.680
larger scale pre-trained language model that basically came out of opening AI. The main

40:35.680 --> 40:40.920
feature of it being the fact that it was large enough so their largest model was 1.5,

40:40.920 --> 40:46.160
like, billion-providers. So, it was large enough and trained on good enough of it, you

40:46.160 --> 40:54.560
know, sort of high-quality curated web scale data set that they were able to showcase

40:54.560 --> 41:01.400
that they are generating really coherent outputs, paragraphs, and stories, you call it.

41:01.400 --> 41:06.360
They also showed that through this particular very large language model that they've built,

41:06.360 --> 41:12.400
they are able to do zero-shot generalization to other downstream tasks. So, not fine-tuning,

41:12.400 --> 41:18.480
meaning you don't have it even a small scale particular training corpus to fine-tune the

41:18.480 --> 41:24.240
model, just literally zero-shot right out of the box. You show that you can do, you know,

41:24.240 --> 41:29.960
some level of, you know, you can compete with a state-of-the-art in some, you know, much less

41:29.960 --> 41:33.920
than a state-of-the-art, but still makes some performance in machine translation, question

41:33.920 --> 41:39.160
answer, and reading comprehension, summarization, et cetera. So, this was the work, but I would

41:39.160 --> 41:46.840
say that the attention to this work, God, was not due to the performance necessarily,

41:46.840 --> 41:52.680
but due to the way that it was released. So, what happened, and I'm sure, you know,

41:52.680 --> 41:56.760
I know you guys have already covered this, so I'll just mention this quickly, that this

41:56.760 --> 42:02.480
state-release process that they had in mind, where they basically cited that given how

42:02.480 --> 42:08.760
amazing this work, this model is working, it's too basically dangerous for it to be released

42:08.760 --> 42:14.000
to the public community for the potential of misuse. So, they held back, and they did

42:14.000 --> 42:20.000
this stage-release process, they released the smallest model in February, 2019, then

42:20.000 --> 42:28.120
in November, they finally reached a full 1.5 million-parameter model, but that whole process

42:28.120 --> 42:33.440
sort of created this GPT-2 saga, of course, there were so many people that were kind of

42:33.440 --> 42:39.800
outraged by the fact that, oh my goodness, this is open, and open AI, how could you not

42:39.800 --> 42:45.520
release something that you have created, and then there were some proponents saying that

42:45.520 --> 42:50.120
I don't know, and this is a good example of the community sort of thinking about the implications

42:50.120 --> 42:55.480
of their work, et cetera. So, sitting data side, and I don't think it's the, as I said,

42:55.480 --> 43:00.800
I know you guys have already debated this issue, but I think it was an interesting moment

43:00.800 --> 43:07.280
for the NLP and AI research in general to have gone through this. But I wanted to mainly

43:07.280 --> 43:12.320
talk about the text generation aspects. So, sitting beside the PR, and like the rights

43:12.320 --> 43:18.360
or wrongs that the opening AI folks did, forward a way that they released this model. For

43:18.360 --> 43:22.520
me, as a researcher myself, like having worked in text generation and still working on

43:22.520 --> 43:29.640
it, I was really excited to get my hands on the largest spas model that they had given

43:29.640 --> 43:37.000
them examples that they had in their paper, because the problem of doing cohere in national

43:37.000 --> 43:42.800
language generation has been one of the longest-running problems in NLP community, for sure.

43:42.800 --> 43:49.360
And I think anyone would be excited to know how far we've gone and tackling that problem.

43:49.360 --> 43:56.440
So just looking back in time, again, like 2017 or so before any of these models were out,

43:56.440 --> 44:03.200
I myself, like sort of characterized very viarbit language generation, saying that,

44:03.200 --> 44:11.160
look, we have these at the time, this Verde RNN, LM, so Recurin Neural Network-based language

44:11.160 --> 44:15.880
model is not transformed based language models. I would characterize them as being locally

44:15.880 --> 44:21.200
coherent, meaning that they are very much capable of generating grammatical sentences,

44:21.200 --> 44:27.480
but then generating logically sound paragraphs are still so longer than a sentence, and like

44:27.480 --> 44:32.480
something like a story or narratives, which happens to be my area of research. They were

44:32.480 --> 44:36.920
still super lacking. So just looking at the examples that they had in their paper made

44:36.920 --> 44:42.440
me really excited to just try it out, right? So I was talking about story closed test,

44:42.440 --> 44:49.160
very given like four sentences, a particular, very simplistic story generated basically

44:49.160 --> 44:55.160
the ending, right? So I basically, after even the initial releases, the smaller models

44:55.160 --> 45:00.720
put in them, but the latest and largest model, I personally tried out this, did you

45:00.720 --> 45:07.720
see the GPT2 model for various such story closed test instances? So one, for example, that

45:07.720 --> 45:12.920
I would want to highlight was just this very simple story that I'm going to build up on

45:12.920 --> 45:18.560
top of. So the story is Lili was writing her scooter, a bike turn in front of her, she

45:18.560 --> 45:28.160
tried to break abruptly, and she fell on the ground. And so that way that GPT2 continues,

45:28.160 --> 45:35.400
as the story is, Lili fell into the lake. I dragged her out. She said that she could not

45:35.400 --> 45:40.840
go down. I was desperately searching for another slant, blah, blah, blah, and you know, it

45:40.840 --> 45:48.360
goes forever. So it goes to saying that the one of the major problems that I would characterize

45:48.360 --> 45:54.560
about, like, neural language models for a national engagement generation back in 2017 was

45:54.560 --> 46:02.360
that, here, as him is very good in hypothesizing why a model's generation is actually logically

46:02.360 --> 46:07.400
sound, because if you have this way of projecting our own sense of meaning, right, from even

46:07.400 --> 46:12.320
the most mindless generation. And I think what's happening here is just very much still in

46:12.320 --> 46:17.280
line with that as well, that sure, you can hypothesize that probably Lili then fell into the

46:17.280 --> 46:22.240
lake and then I dragged her out and she says she could not go down, et cetera. But it's

46:22.240 --> 46:26.360
clear, as you know, the farther you go, and this was just one example, as I said, like,

46:26.360 --> 46:31.200
I tried to sum so many more, the more you play with the model, the more you see that that

46:31.200 --> 46:37.160
logically sound generation globally at, like, paragraph, little story, little, et cetera,

46:37.160 --> 46:42.000
is still something that, as a community, we are lacking. And I think that there's a wide

46:42.000 --> 46:46.400
range of, you know, generation tasks that you can work on and care about. And I guess

46:46.400 --> 46:52.520
I would characterize this still to this day, I would say that generating logically sound

46:52.520 --> 46:58.560
stories, narratives that make sense, a show common sense is, you know, one of the major

46:58.560 --> 47:06.000
bottlenecks, I would say, of building an LP model that can work well, basically, effectively.

47:06.000 --> 47:11.400
How would you characterize the differences between the smaller model and the full model that

47:11.400 --> 47:17.400
was released later in the year in terms of storage generation? That's a very good question.

47:17.400 --> 47:23.240
I, I never did a quantitative analysis, right? And that goes to one of the main other problems

47:23.240 --> 47:27.760
we have in the area that I actually wanted to highlight, which is evaluation. These still

47:27.760 --> 47:33.960
don't, like, as a community, we don't have a, a good way of evaluating generation. So

47:33.960 --> 47:39.720
it's like we don't have a, they have automatically evaluating whether or not a system is generating

47:39.720 --> 47:45.080
something sound, sort of meaning AI, judging AI, that's a major problem. So because of

47:45.080 --> 47:52.520
that, it's been really hard to know, like, to sort of quantitatively measure progress.

47:52.520 --> 47:57.680
So that's just a separate problem we have, which I'm hoping some, it should be something

47:57.680 --> 48:02.280
actually that, as a community, we work harder on. And as you can imagine, right, the reason

48:02.280 --> 48:10.600
we do industry courses as a multi-choice test set was so that it's evaluable, quickly,

48:10.600 --> 48:16.320
systematically, easily, right? But it comes with a caveat of being basically gameable and

48:16.320 --> 48:21.880
all the kinds of, you know, biases that we find about the data sets. So generation is

48:21.880 --> 48:25.920
ideal, but then the flip side is we don't know how to evaluate generation. So setting

48:25.920 --> 48:31.000
that aside qualitatively, and, you know, their proxies, we can use blue, et cetera, but

48:31.000 --> 48:36.640
the fact that they don't correlate with human judgment is an issue that we just yet have

48:36.640 --> 48:41.480
an address. But yeah, quantitatively looking at the results, there wasn't honestly that

48:41.480 --> 48:48.400
much difference between the largest GPT-2 models that I've played with and the smallest,

48:48.400 --> 48:54.560
but definitely a summary that people have to do more systematic evaluation.

48:54.560 --> 48:59.800
Yeah, so one more thing I wanted to mention here, hopefully real quick, is about the fact

48:59.800 --> 49:04.640
that so in the community after GPT-2 came out, there have been lots of back and forth in

49:04.640 --> 49:08.680
different use cases that people have found. And, you know, there was even this, the interview

49:08.680 --> 49:13.800
that was done with GPT-2, so many different angles that this whole line of research basically

49:13.800 --> 49:19.960
has taken in the public eye, media coverage, et cetera. One thing that people, if you

49:19.960 --> 49:25.200
people at least have rightfully pointed out is the fact that are we actually making real

49:25.200 --> 49:29.640
progress towards national language understanding and national language generation through such

49:29.640 --> 49:34.960
pieces of work? Are these models capable of building so-called mental models of the

49:34.960 --> 49:39.960
world, right? So this is something that I'm personally extremely passionate about. And

49:39.960 --> 49:44.320
like I'm hope, you know, at Elemental Commission, one of the pieces of work that we are hoping

49:44.320 --> 49:50.240
to come out next year is exactly on this. So for the lily story that I was mentioning,

49:50.240 --> 49:56.400
for example, like for any even like a child, human child reading that story, they would

49:56.400 --> 50:02.640
know the causal chain of events that happened. They would know the emotional, you know, turbulence

50:02.640 --> 50:07.080
that the character went through like, oh, she was like writing a scooter and then the

50:07.080 --> 50:11.720
bike turn, oh, how was she feeling then? When she fell on the ground, oh, did she skin

50:11.720 --> 50:17.720
her knee? How did she feel after being injured, et cetera? And we can basically build this

50:17.720 --> 50:24.320
pretty consistent models of the world, mental models of the world, even as children read

50:24.320 --> 50:30.560
a very short story. So I think that we are very far away from building an AI system that

50:30.560 --> 50:38.320
can showcase such common implicit common sense, understanding of the world, even as a five-year-old

50:38.320 --> 50:43.440
child would do. And I think there's enough evidence that the likes of GPT-2 are not

50:43.440 --> 50:47.400
doing that, given the mistakes that they're making. And I think as a community, we should

50:47.400 --> 50:50.720
focus on tackling such problems moving forward.

50:50.720 --> 50:57.480
Yeah, I mean, this is probably a good time to note that the interview that I did with

50:57.480 --> 51:03.200
David Faruji from Elemental Cognition, the title of that one was, are we being honest

51:03.200 --> 51:09.180
about how difficult AI really is? Actually, turn out to be our number one, you know, most

51:09.180 --> 51:19.120
popular show of 2019. Oh, wow. That's awesome. Yeah, yeah. But that was one of several that,

51:19.120 --> 51:24.600
you know, spoke to kind of, you know, maybe a sobering perspective on the way we think

51:24.600 --> 51:29.280
about AI and building models and what they're really capable of, what they, you know,

51:29.280 --> 51:33.840
what we should be expecting out of them. And you're kind of echoing that same sentiment.

51:33.840 --> 51:38.920
Exactly. Yes. Yes. So what's the next paper on your list?

51:38.920 --> 51:45.480
So now that we kind of covered our bases with the main two pioneers or whatever we can

51:45.480 --> 51:54.400
call them BERT and GPT-2s of the pre-train paradigm that we were living in 2019, I think

51:54.400 --> 52:03.040
it's good to highlight one of the major advances that we could make through the likes of BERT,

52:03.040 --> 52:09.840
et cetera, on a downstream task that was held as one of the feats of the year. So this

52:09.840 --> 52:17.800
is a work that came out of AI-2 as well. It's called from F to A on the, on the New York

52:17.800 --> 52:24.840
region science exam and overview of the RISTO project. So this is basically an accomplishment

52:24.840 --> 52:30.560
that AI-2 had, which fills on top of the work that they've been doing for the past like

52:30.560 --> 52:37.240
four or five years, at least, on tackling science exams. So, you know, what Lake Paul

52:37.240 --> 52:44.240
Allen had this dream of doing, like building a digital RISTODL, and actually four years

52:44.240 --> 52:50.920
ago or so, they made a challenge for the research community to come up with an AI system that

52:50.920 --> 52:58.120
can be 10-8 greater in this standardized science test. So back then, to belief was that,

52:58.120 --> 53:04.440
you know, we've, okay, we've built like IBM Watson, which is good at jeopardy. Can

53:04.440 --> 53:08.600
we build a system that doesn't do jeopardy, but it's somewhat simpler, it just beats

53:08.600 --> 53:14.800
an 8th grader. So, as I said, that was like one of the, like, one of Paul Allen's dreams,

53:14.800 --> 53:20.520
but back in time, when they did this as a Kaggle contest on the best system that was submitted

53:20.520 --> 53:26.800
got around like 60-something percent, which was far, far away from the human performance,

53:26.800 --> 53:32.760
of course, or like a 8th grader performance to pass the test. So, fast forward, one of

53:32.760 --> 53:39.880
the main advances, I would say, in 2019, that was made was that they, through using births,

53:39.880 --> 53:46.760
so, both births and perverda, language models, they could boost their performance from

53:46.760 --> 53:55.960
63 or something, I think, percent that they had achieved in 2016 to now 90.7% in 2019,

53:55.960 --> 54:04.720
which was a passing score. So, this was pretty much of a feat in the field. For, you know,

54:04.720 --> 54:10.080
various reasons, this, you know, like the choice of science exams is something that we

54:10.080 --> 54:15.760
can debate, whether or not it's a good benchmark, but at least from the surface level, it seems

54:15.760 --> 54:22.160
like, you know, science, such science questions require national linkage understanding, having

54:22.160 --> 54:26.560
common sense knowledge, pretty broad common sense knowledge, knowing how the world works,

54:26.560 --> 54:31.520
et cetera, and then reasoning capabilities, right? And also from like a more practical

54:31.520 --> 54:36.960
standpoint, exams are accessible, measurable, right, the multi-choice exam, of course,

54:36.960 --> 54:44.160
is something that you can quickly evaluate. So, it seems like a pretty compelling, these,

54:44.160 --> 54:49.200
you know, the following seem like a pretty compelling reason to, to one account that is a good metric.

54:49.200 --> 54:56.640
But, of course, as many even teachers would argue, standards, tastes like multiple choice tests

54:56.640 --> 55:01.920
are not the best measure of intelligence. They are gameable, really like even children who

55:01.920 --> 55:06.800
get good tests scores are not necessarily the most intelligent and learn the best in their classes.

55:07.520 --> 55:15.440
So, there are those aspects, and I actually go at that AI2 folks have been pretty good with not

55:15.440 --> 55:22.800
letting this get hyped up, right, out of their scope of what they would characterize, beyond their

55:22.800 --> 55:28.560
scope of what they would characterize as their real outcome of this work. So, they even themselves

55:28.560 --> 55:35.040
did some adversarial testing of the model. They showcase that if you add various other

55:35.920 --> 55:41.120
multiple, like, what are some of their choices to this multiple choice instances that are like

55:41.120 --> 55:47.680
likely to sort of be the answer, it's just this challenging answer. The model's performance

55:47.680 --> 55:55.120
drops from 90%, 90 plus percent that it had thought into 60%. So, they have really even themselves

55:55.120 --> 56:00.320
highlighted the fact that, look, this is, this is it. It's a narrow particular test set,

56:00.320 --> 56:05.120
standard test set that this model is working well on. It doesn't mean true intelligence. Please

56:05.120 --> 56:12.880
don't title this as, now we have an AI system that can be ties, coolers, etc. So, with that caveat,

56:12.880 --> 56:18.000
aside from one more thing, by the way, they also mentioned that the real eighth graders

56:18.720 --> 56:25.360
also answered the questions that include diagrams and they don't. So, that's another, you know,

56:25.360 --> 56:31.040
point to take into account. But yeah, even all this, I think still, this is pretty amazing that

56:31.040 --> 56:36.560
why basically introducing these large language models that do implicitly include lots of

56:36.560 --> 56:42.880
word knowledge, lots of contextualized knowledge, lots of grammatical even knowledge,

56:42.880 --> 56:49.120
you are able to boost your performance on such a test. Nice, nice. What's the next paper on your list?

56:49.760 --> 56:57.280
So, the next paper is called right forward and wrong reasons. So, first of all, I love the

56:57.280 --> 57:04.720
title. I think it surpasses so many things that it has gone wrong and could go wrong

57:05.280 --> 57:12.400
in our, you know, without benchmarking and and open community. So, this this paper is sort of

57:12.400 --> 57:18.880
for me an example from a host of different papers that have come out and are trying to show us

57:18.880 --> 57:26.080
the blind spots of these models or all various ways that they are making the supposedly rights

57:26.080 --> 57:33.360
predictions, but all for the wrong reasons. This actually, this kind of paradigm, not paradigm,

57:33.360 --> 57:38.960
maybe a realization, I'd call it. And then on the community started a few years back and actually

57:38.960 --> 57:44.000
to my knowledge, at least one of the first ones was on the very story close test test that we

57:44.000 --> 57:50.880
ourselves did, very then we made it into a challenge. The top performing model actually had this

57:50.880 --> 57:58.560
observation that turns out there in these biases in the way that the endings in the story

57:58.560 --> 58:04.320
cost us are authored by our crowdsource workers. So, things like the fact that, oh, it turns out

58:04.320 --> 58:09.840
that the wrong ending is, you know, often has like negative adjectives or it's like a longer

58:09.840 --> 58:18.080
shorter, etc. So, these like synthetic sort of biases that are in the in our test sets which

58:18.080 --> 58:23.840
we don't even realize. Now, remember us talking about how difficult it was to construct these

58:23.840 --> 58:29.520
test examples without kind of various types of tells in them that, you know, would tip off

58:29.520 --> 58:37.200
the model. Exactly. Exactly. It's very hard and I think we talked back then that you're even lucky

58:37.200 --> 58:42.240
as, you know, just in the research community, we're lucky even to be catch these, right? God knows

58:42.240 --> 58:49.440
which other benchmarks that we are using on a day to day basis have other implicit on like hidden

58:49.440 --> 58:55.520
biases that we are not even aware of or is so hard to uncover. So, I think this is just, you know,

58:55.520 --> 59:02.960
you thought other papers is 2019 paper, those outcomes like the story close test that was like 2017

59:02.960 --> 59:07.280
actually. So, this is like two, three years after and still we are dealing with this problem.

59:07.280 --> 59:14.640
So, this particular paper that was co-authored by folks from Johns Hopkins and Brown appeared in

59:14.640 --> 59:22.880
ACL 2019, which to me is like just highlighting the growing movement in an LP community to move

59:22.880 --> 59:30.400
beyond interpreting the test sets, you know, leaderboards as just pure achievements. But,

59:30.400 --> 59:36.960
care more about analyzing what's actually the thing that these models are learning and hard to

59:36.960 --> 59:44.400
perform in book. So, they actually, what this particular paper observes is that for the particular

59:44.400 --> 59:51.760
task of MNLI, which is this multi-genre and natural language inference data set, they show that

59:51.760 --> 59:59.760
there are superficial syntactic properties such as, like, whether or not the words in the sentence

59:59.760 --> 01:00:05.120
that is going to be the, you know, on the prediction set overlaps with the one in the input.

01:00:05.120 --> 01:00:10.480
So, like, pretty superficial, you kind of like go and scratch your head like, oh my god,

01:00:10.480 --> 01:00:14.960
how come we are still doing this and we're having these problems after like three years of people

01:00:14.960 --> 01:00:21.280
talking about this. But this is, you know, their reality. We've been, you know, sort of evaluating

01:00:21.280 --> 01:00:26.080
our models on the benchmarks, which still have these hidden problems. And as I said, because

01:00:26.080 --> 01:00:30.560
it's really hard, as you were discussing, it's really hard to uncover social biases.

01:00:30.560 --> 01:00:36.640
So, what they did is that, and I think I can actually mention just once more, natural language

01:00:36.640 --> 01:00:44.320
inference is the task where given a particular input, sentence, the system is, and another sentence,

01:00:44.320 --> 01:00:49.600
you are supposed to classify whether or not the second sentence is an entailment or a contradiction,

01:00:49.600 --> 01:00:55.680
or in some of these benchmarks neutral, meaning that it doesn't necessarily contradict or entail.

01:00:55.680 --> 01:01:01.120
So, anyways, they did this analysis. They made this data set, sort of an adversarial data set,

01:01:01.120 --> 01:01:08.480
Kant's called Kant's data set, where they actually curate these particular test instances,

01:01:09.200 --> 01:01:15.360
which sort of uncover whether or not a particular model is using substantive heuristics.

01:01:15.360 --> 01:01:20.880
For example, they have this heuristic, called lexical overlap, just pure lexical overlap.

01:01:20.880 --> 01:01:27.760
The definition is that assume that a premise, which is the input sentence on the left hand side,

01:01:27.760 --> 01:01:32.800
entails all the hypotheses constructed from the words in the premise.

01:01:32.800 --> 01:01:38.400
So, if you, it can also, for example, the use is that, for example, if the premise is the doctor

01:01:38.400 --> 01:01:45.120
was paid by the actor, you can hypothesize that the doctor paid the actor just because it has the

01:01:45.120 --> 01:01:51.920
fully word lexical word overlap is going to be entailed by that sentence, but it is wrong, right?

01:01:53.040 --> 01:01:58.960
So, the model that basically shouldn't say that that is an entailment, but if a model is biased

01:01:58.960 --> 01:02:05.440
towards using such lexical overlap heuristics, it will wrongly classify that as correct and entailment.

01:02:06.240 --> 01:02:11.680
So, what they do is that they show, show that actually, on this Kant's data set that they create,

01:02:11.680 --> 01:02:18.240
that is actually true, that a lot of like a majority actually have the state-of-the-art models

01:02:18.240 --> 01:02:25.440
on M and L.I. Data set, they're doing this very thing, that they were actually very inaccurate in

01:02:25.440 --> 01:02:34.320
classifying such instances where the heuristic flip basically. So, they actually show that,

01:02:34.320 --> 01:02:40.240
although that's the case, they show that if they augment these models and retrain them using

01:02:40.240 --> 01:02:47.280
the Kant's data set, they can improve their performances. But the reason, as I said, the main reason

01:02:47.280 --> 01:02:52.560
I wanted to highlight this paper is that still, 2019, we are dealing with the same problem

01:02:52.560 --> 01:02:59.440
that we were dealing in 2017 of having models that are biased towards the intricacies of the

01:02:59.440 --> 01:03:03.680
test sets and train sets that they're getting trained on, and that's something that we have to

01:03:03.680 --> 01:03:10.400
keep working on moving forward. Yeah, I suspect that, you know, different versions of these problems

01:03:10.400 --> 01:03:16.320
will keep us busy for quite some time, which actually leads us quite nicely into your predictions

01:03:16.320 --> 01:03:25.120
for the field. Yes, absolutely. So, I think that, as I was just, you know, the way that we started

01:03:25.120 --> 01:03:30.480
this whole conversation, I think they've come really along, Bay, in the past couple of years,

01:03:30.480 --> 01:03:36.720
if not like the past decade, and tackling lots of low-hanging fruits in NLP using these

01:03:36.720 --> 01:03:43.600
really amazing tools that we've built. But, you know, I think the papers that I had selected

01:03:43.600 --> 01:03:49.680
kind of nicely highlight the problems we have as well, like the limitations and the kind of

01:03:50.960 --> 01:03:57.520
weaknesses that these models tend to keep showing. And I think 2020 should be the year that we

01:03:57.520 --> 01:04:04.080
start to get ambitious again, and think about how much, you know, harder kinds of problems

01:04:04.960 --> 01:04:11.520
we can tackle moving forward, now that we have covered the basis sort of. So, actually this year,

01:04:12.160 --> 01:04:18.240
2020, for the first time in the history of ACL conference, so ACL being a, you know, major

01:04:19.680 --> 01:04:25.200
computational linguistic community conference, we have a special theme that asks the community

01:04:25.200 --> 01:04:32.320
to write papers to reflect back on the progress of the field and what V as a community should be

01:04:32.320 --> 01:04:38.160
focusing on moving forward. And I think that's pretty refreshing because it indicates that there

01:04:38.160 --> 01:04:44.400
is this consensus that, look, from the outside, it feels like there's so many benchmarks that keep

01:04:44.400 --> 01:04:51.040
getting beaten every month or so through these new other tools that come out bigger, better.

01:04:51.040 --> 01:04:57.200
But where are we going with this? Are we actually defining truly what natural language

01:04:57.760 --> 01:05:05.120
understanding means? Are we truly working on systems that show common sense, you know,

01:05:05.120 --> 01:05:14.480
inferences of even a child? Are we actually building systems that can transfer the knowledge that

01:05:14.480 --> 01:05:19.520
they have, what they learn from a test to another without really needing to get, you know,

01:05:19.520 --> 01:05:26.480
retrained, et cetera? So, I think I would love for that to be how the, you know,

01:05:26.480 --> 01:05:33.920
shift their focus in 2020. I think that we should focus on the things that we cannot do yet,

01:05:33.920 --> 01:05:40.160
or have not basically have had the chance of doing because of having focused on the simpler

01:05:40.160 --> 01:05:46.240
problems. I think one major issue we have with all these new things that we've built is that

01:05:46.240 --> 01:05:52.560
still to this day in the industry, there are a lot of systems that use like old-school,

01:05:53.280 --> 01:05:57.840
you know, rule-based models, pattern recognition, and the sense of just doing

01:05:57.840 --> 01:06:04.640
reggae smashing, et cetera, because, you know, they know how they work, they know how to turn it off

01:06:04.640 --> 01:06:11.120
and think the system, whatever makes a stupid mistake. But these, you know, very accurate actually

01:06:11.120 --> 01:06:16.400
neural models, they often make stupid, stupid mistakes that we don't even know why, right? And

01:06:16.400 --> 01:06:22.160
that's, I think, something that needs to be looked into, how can we build, sort of,

01:06:22.160 --> 01:06:27.600
better controls over these highly accurate models to know where they could go wrong,

01:06:27.600 --> 01:06:35.120
can we get guarantees, et cetera? And I think the more we move into areas at high stakes,

01:06:35.120 --> 01:06:41.760
the more the need to do so. Do you think those controls look more like changes to the way

01:06:41.760 --> 01:06:47.920
these models are trained or evaluated or lost functions or things like that, or more like hybrid

01:06:47.920 --> 01:06:55.440
types of systems that incorporate elements of rules and elements of more modern NLP?

01:06:56.160 --> 01:07:04.160
I think it could be either, right? I think what was very refreshing about the way that deep learning,

01:07:04.160 --> 01:07:10.000
a revolutionized NLP in the past, a couple of years, is the fact that despite the mainstream,

01:07:10.000 --> 01:07:14.560
there were many, you know, like, even if a smaller community, but there were folks who were still

01:07:14.560 --> 01:07:19.920
doing research in the area and thinking beyond what the mainstream is dictating. And I think,

01:07:19.920 --> 01:07:25.600
in order to make tremendous progress moving forward, we do need people who think differently.

01:07:25.600 --> 01:07:30.480
We do need people who think they know, like, there's no way that deep learning is going to be the

01:07:30.480 --> 01:07:35.280
silver bullet we have to think about a hybrid system. Or people who believe that, no, there's no way

01:07:35.280 --> 01:07:41.120
that we can have, like, symbolic models incorporated into these neural models, and we have to just

01:07:41.120 --> 01:07:47.120
fix the way that we do training in order to exhibit better generalization, better transfer of knowledge,

01:07:47.120 --> 01:07:52.160
et cetera. So I think there's no way for me or anyone, honestly, to say which one is necessarily

01:07:52.160 --> 01:07:59.120
going to thrive. I think the more people we have in the community caring about the right problems,

01:07:59.120 --> 01:08:04.400
as opposed to the right approaches, that the higher chances of tackling these major remaining

01:08:04.400 --> 01:08:10.640
problems in the area. What else do you foresee? So there are a couple of other things. I think that

01:08:11.440 --> 01:08:19.120
the will start having more rigorous evaluations in place. I think that we would better know the

01:08:19.120 --> 01:08:25.040
implications of establishing state-of-the-art on various benchmarks. As I was saying, there are

01:08:25.040 --> 01:08:31.680
even environmental implications of all the sort of fact-planting that we do at this day and age.

01:08:31.680 --> 01:08:37.600
And I think more people should think about those aspects of their work. Actually, there was

01:08:37.600 --> 01:08:45.520
a work, another work called gray eye by UW people that they were encouraging the community to

01:08:46.160 --> 01:08:52.800
also report the efficiency of their resource usage, along with the other classical metrics,

01:08:52.800 --> 01:08:57.280
such as accuracy, et cetera. And they are reporting numbers. And I think those are really

01:08:57.280 --> 01:09:03.760
interesting directions that the community could take. And potentially, we may no longer count

01:09:03.760 --> 01:09:08.480
the best work of the year, the largest work of the year. Maybe we know that, oh, look,

01:09:08.480 --> 01:09:15.040
this just had this really negative implication environmentally and whatever it wasn't fair. So we

01:09:15.040 --> 01:09:23.920
can think beyond that, basically. And another thing is such a no-brainer. I think we are going

01:09:23.920 --> 01:09:29.440
to start to work more and more on explainable models and interpretable models. So it's very

01:09:29.440 --> 01:09:36.880
commonly the reason people care about explanation and interpretability is for the accountability issue,

01:09:36.880 --> 01:09:42.720
for fairness issue, et cetera, which is really major. But the reason I personally am a big advocate

01:09:42.720 --> 01:09:49.280
and I've been interested in working on this sort of past couple of years on V2. So even more broadly

01:09:49.280 --> 01:09:54.720
at elemental cognition is the fact that explanation is this inherent capability of human beings,

01:09:54.720 --> 01:10:01.280
right? Even a little child can explain the kinds of reasoning that they do. Of course,

01:10:01.280 --> 01:10:08.240
we can argue, again, what is explanation. But I think building models that can be held accountable

01:10:08.240 --> 01:10:13.600
towards the predictions they make and have ways of explaining it to an average human, which I would

01:10:13.600 --> 01:10:17.680
argue should be through national language, is going to be something that we will see more and more

01:10:17.680 --> 01:10:24.560
in 2020. And the probably, hopefully, last thing that I would mention is, of course, we have

01:10:24.560 --> 01:10:31.200
to build causal models of the world that I was mentioning. We need to build systems that show

01:10:31.200 --> 01:10:37.360
common sense, build systems that are able to basically build this causal map of the world, how

01:10:37.360 --> 01:10:44.240
the events basically follow each other, how do we know this happens versus the other thing doesn't

01:10:44.240 --> 01:10:50.320
happen. And what are the implications in terms of the emotions of characters who vary what,

01:10:50.320 --> 01:10:55.600
et cetera. So I think these are really kinds of directions that the field should be taking moving

01:10:55.600 --> 01:11:01.040
forward in the decade, not necessarily 2020. And I'm hoping, really, in the next eight,

01:11:01.040 --> 01:11:08.000
nine years or so, we are going to say that finally, we have a system that can start to at least

01:11:08.000 --> 01:11:13.840
show the basic common sense understanding of a five-year-old child. That's awesome. Awesome.

01:11:14.640 --> 01:11:20.160
Nestering, thanks so much for taking the time to review your favorite papers of 2019 with us and

01:11:20.160 --> 01:11:28.960
to talk through your predictions. At no doubt, it will be an exciting year in 2020 and NLP and

01:11:28.960 --> 01:11:34.240
looking forward to keeping in touch on it. Yes, same here. Thank you so much, Sam,

01:11:34.240 --> 01:11:36.800
looking forward to 2020. Thanks so much.

01:11:40.560 --> 01:11:46.000
All right, everyone, that's our show for today. For more information on today's guest or for

01:11:46.000 --> 01:11:52.160
links to any of the materials mentioned, check out twimmelai.com slash rewind19.

01:11:53.120 --> 01:11:57.440
Be sure to leave us a five-star rating and a glowing review after you hit that subscribe

01:11:57.440 --> 01:12:09.120
button on your favorite podcast catcher. Thanks so much for listening and catch you next time.

