WEBVTT

00:00.000 --> 00:08.240
All right, everyone. Welcome to another episode of the Twimmel AI podcast. I am your host Sam Charrington and today

00:08.240 --> 00:13.680
I'm joined by Demetrius Zermas. Demetrius is a principal scientist at Centera.

00:14.320 --> 00:21.760
Before we get into today's conversation, please take a moment and head over to Apple Podcasts, Spotify, or your listening platform of choice,

00:21.760 --> 00:28.960
and subscribe, review, and leave us a five-star rating if you enjoy the show. Demetrius, welcome to the podcast.

00:28.960 --> 00:35.760
Hey Sam, thank you for having me. I'm really looking forward to this conversation. We'll be chatting a bit about

00:35.760 --> 00:42.160
data center AI and in particular how it applies to your work in precision agriculture.

00:42.960 --> 00:48.960
But before we get there, let's start like we usually do with a little bit of background.

00:49.760 --> 00:56.480
How'd you get started in ML and AI? I'm originally from Greece and when it was growing up,

00:56.480 --> 01:07.280
I really wanted to work with robotics, and then as I managed to get accepted for a PSD program in the United States,

01:07.280 --> 01:16.880
I was really hoping to do robotics. But the lab I joined was doing both robotics and AI, and I was drawn to AI

01:16.880 --> 01:27.440
more, so then that sort of changed my plans or how I was viewing myself or my future self.

01:28.800 --> 01:35.280
And that essentially started a trajectory that led me to where I'm here today.

01:35.280 --> 01:42.320
Awesome, tell us a little bit about Centera. Centera is a software and hardware company,

01:42.320 --> 01:50.800
specializing in precision agriculture. We are building tools to assist agronomists and

01:50.800 --> 02:01.600
assist enterprises that work in the business of precision agriculture or business of agriculture,

02:01.600 --> 02:07.760
with making decisions for their crops during the growing season.

02:07.760 --> 02:14.240
Awesome. And what kind of tools in particular, what do those tools look like? Starting from the hardware,

02:15.040 --> 02:24.160
we are building cameras that can be mounted on small drones. We also built a drone that has

02:25.040 --> 02:34.800
a fixed wing airplane that you can launch by hand. And then these are paired nicely with a suite

02:34.800 --> 02:41.680
of software and analytics solutions. So we have a platform where you can upload your data and then

02:41.680 --> 02:48.480
view the results after the analysis is done. We also create all the analysis on the images we collect,

02:48.480 --> 02:55.280
and this can be counting how many plants have emerged. And that way you can, or the agronomists can

02:55.280 --> 03:04.080
decide whether the seed planted was doing well or not, whether he need to re-plant or not, as well as

03:05.120 --> 03:12.480
figuring out the general vigor and health of the crop they're growing throughout the year.

03:12.480 --> 03:19.120
Why build all of these components? Like why build a separate drone and a separate camera

03:19.120 --> 03:28.560
as opposed to the software part that's kind of directly impacting the agronomists?

03:28.560 --> 03:36.800
There is a continuous hunger for improvement of productivity in the precision agriculture and

03:36.800 --> 03:44.880
the agriculture industry in general. They want to be flying more acres or capturing data for more

03:44.880 --> 03:52.960
acres in a smaller amount of time. We figured that by combining a fixed wing drone that can fly

03:52.960 --> 04:00.800
for about an hour, we can cover agronomists and cover much more ground than whatever was available

04:00.800 --> 04:09.440
in the market. We are of course supporting some well-known drone companies. I'm not going to

04:09.440 --> 04:21.440
mention any names because as long as they capture images that are up to some specifications,

04:21.440 --> 04:27.520
then of course the algorithm can use them. How long have you been in the company? What stage

04:28.080 --> 04:32.480
was the company at with regard to using ML and AI when you joined?

04:32.480 --> 04:40.480
I was informed that I recently concluded the fourth year of joining the drones in

04:40.480 --> 04:49.200
there. Time flies for sure. When we started or when I started, the company that was barely any

04:50.480 --> 04:59.760
analytics available. We had some simple plant counting algorithm that was sort of

04:59.760 --> 05:09.920
struggling to expand and able to be used for different types of plants. That was something

05:09.920 --> 05:16.240
that the company leadership wanted to fix. We wanted to invest majorly at the analysis of

05:16.240 --> 05:22.640
the images we're capturing because that was a unique strength for the company. The ability to

05:22.640 --> 05:31.760
be able to both capture the data and process them. That was the beginning. I think the first

05:31.760 --> 05:41.120
employee of a data analytics group. Is plant counting still one of the major challenges that you're

05:41.120 --> 05:47.920
working on? More broadly, what are some of the various ways that you do use machine learning there?

05:47.920 --> 05:54.400
Plant counting is still something important and something that we're focusing on and I know

05:54.400 --> 06:02.400
means it has been completely solved. There are always new challenges, you know corner cases

06:02.400 --> 06:08.400
that people are asking us who can support this and that. Apart from plant counting, we are

06:08.400 --> 06:13.840
doing other types of counting. We're counting tassels of corn at the end of the

06:13.840 --> 06:19.600
towards the end of the growing season where counting value crops are some permanent crops like

06:19.600 --> 06:25.840
trees, value crops like lettuce for example. The way I like to separate the different problems

06:25.840 --> 06:32.560
we're addressing is we like to count things. We like to detect areas of images that show some

06:32.560 --> 06:41.360
particular textures, some particular pattern. We're trying to classify images into different

06:41.360 --> 06:51.280
classes that show some specific characteristics. We also want to be able to detect anomalies,

06:51.280 --> 07:00.000
things that should not be there like weeds for example or a plant that is of the correct species

07:00.000 --> 07:08.160
but is their own hybrid. For example, a corn seed that was left there from a previous year and

07:08.160 --> 07:17.360
didn't germinate previous years just germinated by accident when it shouldn't. So these types of

07:19.040 --> 07:26.880
problems, it's a broad spectrum of what we're seeing. Yeah, you mentioned that when you joined

07:26.880 --> 07:35.040
and early on in the companies evolution of using ML and analytics, one of these first problems was

07:35.040 --> 07:43.200
the counting problem and forget the exact words you used but it sounded like they were some

07:43.200 --> 07:52.000
barely working algorithms at first. Let's dig into that evolution. What were you using at first

07:53.840 --> 07:59.280
and what were some of the challenges that you were running into? That was the standard

07:59.280 --> 08:06.400
tale of how we are moving our computer vision technology moves from the classical computer vision

08:06.400 --> 08:13.760
to deep learning and the merits of deep learning and how it overtakes everything. The challenges as

08:14.320 --> 08:20.800
everybody who's familiar with the old computer vision can tell you that you cannot create

08:20.800 --> 08:30.320
an algorithm that can be completely autonomous in an environment that you cannot control. Lightning

08:30.320 --> 08:36.400
the light conditions change. A cloud passes over the field when you're capturing an immense

08:37.600 --> 08:42.800
different soil types and different regions of the country or different regions of the world.

08:42.800 --> 08:52.240
Everything lays their own into trying to set some parameters, some thresholds in achieving your

08:52.240 --> 09:00.640
result, your final result and that essentially forced us to start creating very complicated

09:01.840 --> 09:10.480
algorithmic pipelines. At some point it was almost unsustainable to be able to update them or

09:10.480 --> 09:17.520
verify that they're working tests and etc. Then the next four years ago deep learning was

09:18.640 --> 09:24.480
the hype. We decided to start looking some ways of utilizing deep learning to do counting

09:25.600 --> 09:34.160
to solve that problem and even from the very few initial experiments the results were

09:34.160 --> 09:41.520
substantial. We were looking at seeing much better results than computer vision and

09:42.640 --> 09:50.640
that was essentially the sign telling us that as much as you love classical computer vision

09:50.640 --> 09:58.960
because that was what we were taught. It's time to change, time to update and embrace

09:58.960 --> 10:08.480
machine learning, deep learning. That's essentially what we are doing since then.

10:09.280 --> 10:18.240
Thinking about this move into a deep learning type of an approach, just given the company's business

10:18.240 --> 10:22.800
which you described earlier, you have these drones, you have these camera, your drones can fly

10:22.800 --> 10:31.120
longer than any other drones, should be easy. You've got lots and lots of images. Is that the case?

10:34.320 --> 10:40.880
Yes and no. That's a trick question for the audience because that's what we're talking about.

10:40.880 --> 10:52.240
Yes, yes. Having embracing the deep learning approach for sure, solve some of the problems of

10:52.240 --> 11:00.480
having variability in the images we capture for sure. But then again, on the other hand,

11:00.480 --> 11:12.640
one of the main problems of the successful deep learning product is that 80% of the solution is

11:12.640 --> 11:20.080
having data, having good annotations, having a variety of annotations from different scenes.

11:20.080 --> 11:32.640
Which essentially means that you have to take a step back before you send any images to be annotated

11:32.640 --> 11:40.000
and make a decision on whether this image is going to add new information to your training,

11:40.000 --> 11:47.280
or if it's going to saturate a sensory or overtrain on things that you already know,

11:47.280 --> 11:54.240
that you already have captured in your data set. The problem of selecting which images are the ones

11:54.240 --> 12:07.520
that need to be annotated and trained, arose, and that's another significant hard rule we need to

12:07.520 --> 12:16.960
overcome. You mentioned that in one of the challenges with classical computer vision,

12:16.960 --> 12:25.040
techniques was you had to create this really complicated algorithmic pipeline to deal with all

12:25.040 --> 12:33.200
of the variability that you were seeing in your images and in the cornfields and deep learning

12:35.200 --> 12:43.680
did help with that. But there was still an aspect of variability that was problematic for you.

12:43.680 --> 12:55.680
Yes, and that had mostly to do with corner cases. After you go through the initial excitement

12:55.680 --> 13:02.000
of having something that works much better, you start actually testing and looking at corner

13:02.000 --> 13:08.400
cases and making sure that it works for you on every occasion. And what we found was that

13:08.400 --> 13:18.960
there were, of course, areas where we could improve. We always need to detect smaller plants,

13:18.960 --> 13:28.640
for example, but as well as being able to detect different and isolate different plants when they

13:28.640 --> 13:34.800
are overlapping. I don't know if you have ever visited the cornfield after the first month,

13:34.800 --> 13:43.920
month and a half, depending on the weather, it is very hard to even walk between the rows.

13:44.480 --> 13:51.520
I can imagine trying to identify which plant is wheat and separate and between the leaves.

13:53.520 --> 14:01.520
But at the same time, the ability to be able to do this separation is important as it can reveal

14:01.520 --> 14:09.520
a lot of information to both people that are growing corn in a production setting as well as

14:09.520 --> 14:16.720
the people that are growing for seed production. So the people that are growing the corn that will

14:16.720 --> 14:23.600
be planted next year. So you've got all these factors that are introducing variability into your

14:23.600 --> 14:32.720
data set. And you mentioned you've got these corner cases that you're trying to be able to account

14:32.720 --> 14:38.000
for. You've got this real world problem that you're trying to solve and you can't just solve it

14:38.000 --> 14:43.120
for the perfect case. What's the relationship between the variability and the corner cases,

14:43.120 --> 14:52.400
I guess, in your situation? The first thing that comes to mind is the size of the database.

14:52.400 --> 15:00.320
This is a weird problem in a sense that we have millions of images.

15:02.240 --> 15:11.520
So in that sense, we're lucky. But at the same time, there's an imbalance in the data.

15:12.960 --> 15:20.240
And the corner case is something that happens that does not happen frequently.

15:20.240 --> 15:27.840
So although we have a sea of data, identifying images that have good examples of the corner

15:27.840 --> 15:36.880
cases is stuff. So I think that's the challenge. How do you search through your data space,

15:38.960 --> 15:44.320
identifying corner cases, identifying images that are outliers,

15:44.320 --> 15:55.280
and then include them in a somewhat meaningful way in your main training data set.

15:57.040 --> 16:06.960
Because there's always the fear of the danger of skewing your data one direction or the other.

16:06.960 --> 16:15.680
And of course, it requires a lot of testing to figure out if you're doing the right thing.

16:16.800 --> 16:23.600
But starting, but you need to start somewhere and just trying to solve the problem of,

16:24.400 --> 16:29.040
I need somewhat a made way to tell me which images are unique or interesting.

16:29.040 --> 16:36.160
I think the first step. So you have the millions of images. These are

16:37.920 --> 16:46.640
kind of raw images, so to speak, off of the camera. And your first step in curating this

16:46.640 --> 16:52.560
data set is, well, probably first you just took a bunch of images and kind of proved to yourself

16:52.560 --> 16:59.760
that deep learning would work. But then the next step is, okay, how do we curate a data set that really

16:59.760 --> 17:11.280
works for all the cases that you need to consider? And in order to do that, you couldn't manually go

17:11.280 --> 17:17.040
through millions of images to identify. You essentially have to label all of your images to identify

17:17.040 --> 17:24.400
the ones that were useful. And that is costly and expensive. And so you did what?

17:24.960 --> 17:35.120
We tried to approach it in a couple of ways. One is the first one and more simpler in my mind is,

17:35.120 --> 17:42.480
let's try and train a network and randomly pick images from different data sets.

17:42.480 --> 17:51.280
And see what the performance is. So essentially, down sizing your

17:53.280 --> 18:01.040
your these millions of images to a few thousands of images by randomly sampling.

18:02.240 --> 18:08.080
So before we even annotate, presentation is costly. Let's do inference and see, do we get good

18:08.080 --> 18:15.600
results or not? If we don't get good results, then the network is balanced. So we should probably

18:17.040 --> 18:20.400
use these images, these challenging images in the data set.

18:20.400 --> 18:27.920
So and just to make sure I understand that you've got these different data sets that represent

18:27.920 --> 18:35.360
kind of in a discrete way some of the variability that you see in the wild. So what if we sample

18:35.360 --> 18:42.320
kind of randomly from across these data sets that in aggregate, we're still representing

18:42.320 --> 18:47.760
the variability that we see in the wild. And so let's train a network on that and see if that works.

18:47.760 --> 18:56.240
Yes, but not necessarily before you even train, let's see how the current network performs

18:56.240 --> 19:06.160
on these images across the spectrum. The sort of nice thing is that there is variability in this

19:06.160 --> 19:12.240
problem, but there is not unlimited variability. I mean, at some point you have exhausted all of the

19:12.240 --> 19:20.240
different soil types you would encounter. You have exhausted flying in the morning at the middle of

19:20.240 --> 19:30.960
the day or in the evening or later in the day. So there are a few components that create,

19:31.760 --> 19:39.120
if you combine them, create something unique, but they are finite. So it's through this sort of

19:39.120 --> 19:45.840
brute force method you can get a good sense of what you're dealing with. After doing this to

19:45.840 --> 19:52.320
the manual labor for a few images, then next year comes we're getting two times more images

19:52.320 --> 20:02.080
than the previous one. And then you start asking yourself if you have actually captured all the

20:02.080 --> 20:09.120
changes that you could actually see in the field. And if that statement holds, then you're right,

20:09.120 --> 20:16.960
we're done, but if it doesn't, and how do I know if it doesn't? So then the next step was to do

20:16.960 --> 20:25.760
something a little bit more sophisticated, let's say. And that's when I started reading about

20:25.760 --> 20:36.640
zero-shot learning and clustering without a supervision. I thought that this is probably a good

20:36.640 --> 20:44.640
area to try and implement. That would probably be an adequate solution or at least the most

20:44.640 --> 20:51.680
sophisticated solution that would ease my anxiety of whether I captured enough data or not.

20:51.680 --> 20:58.400
And so what does a zero-shot learning-based approach look like in your scenario?

20:58.400 --> 21:05.280
In my case, essentially, we're talking about finding visual similarities between images.

21:05.280 --> 21:14.160
And essentially, this happens by downsizing significantly the images we capture.

21:14.160 --> 21:26.400
Yeah, I should have mentioned this before, but one of the unique properties of the problem is

21:26.400 --> 21:31.680
that the images we capture are at least 3,000 by 4,000 pixels. So we're doing what I'm about

21:31.680 --> 21:38.800
a very high definition. Images, some of them are also a wide field of view. We're going to

21:38.800 --> 21:51.280
feed an image like that in a normal network. So downsizing significantly to something I can

21:51.280 --> 22:01.360
be handled is step number one. And then after this downsizing, we're passing it through a network that

22:01.360 --> 22:12.480
acts as sort of a not only code there. Essentially takes the image to the dimensional image and finds

22:12.480 --> 22:24.000
a representation in a higher dimensional space. And by doing so, it allows the algorithm itself to

22:24.000 --> 22:32.800
find which images are close by in that higher dimensional space and then clusters them together.

22:32.800 --> 22:44.480
And then through this by looking at this embedding space, essentially, which is this higher dimensional

22:44.480 --> 22:52.960
one, you can sort of find similarities between images. And what you will find may surprise you.

22:52.960 --> 23:05.040
Because what is what are similar images for the algorithm? For us, maybe some uniformity in the

23:05.040 --> 23:14.240
soil. The first time we tried this, the algorithm just grouped the images based on the direction

23:14.240 --> 23:23.360
of the row. Not very useful. How do you adjust for that? How do you iterate towards an algorithm

23:23.360 --> 23:35.040
that works? So this happens with augmentations. So what the algorithms do is that they do not have

23:35.040 --> 23:42.560
explicit constraints between images that should look alike. It's completely unsupervised. So what

23:42.560 --> 23:49.200
they do is that they take an image, they apply some augmentations and then they treat the augmented

23:49.200 --> 23:56.640
image and the original image as similar. So the network is being trained through this process of

23:58.640 --> 24:04.880
taking an image, augmenting it and then projecting it, projecting both of them in that embedding

24:04.880 --> 24:11.440
space. And these two should be very close in the embedding space because they come from the

24:11.440 --> 24:18.880
same image. So essentially, in order to overcome the issues of unwanted clustering or unwanted

24:18.880 --> 24:25.280
similarity, essentially, you are augmenting so that the network learns that it doesn't matter

24:25.280 --> 24:32.000
what direction the rows are with random rotations, for example. So the implementation you can

24:32.000 --> 24:38.640
use is the random rotation. This means that the network learns now that the image where the rows

24:38.640 --> 24:47.520
are perpendicular is very similar with the image where the rows are horizontal. So this misses

24:47.520 --> 24:52.560
this characteristic of the image and looks for something else. And I'm imagining that

24:53.680 --> 25:00.960
you alluded to this earlier, the only way you really figure out what is happening is you have to

25:00.960 --> 25:08.000
look at the results and spend a lot of time and just kind of intuit what the network is trying to

25:08.000 --> 25:20.800
do. By the way, this is one of the reasons why I relate algorithmic development in computer vision

25:20.800 --> 25:27.360
because you can't see the result. So you know if you're doing something wrong. Yes, it requires

25:27.360 --> 25:39.840
a lot of verification and reviewing. But then it is possible to get the images from the embedding

25:39.840 --> 25:46.960
space down into 2D or you have the connection. Essentially, you know that this image is that one

25:46.960 --> 25:55.120
in the embedding space. So you can see you can, you know, through a key means or a clustering,

25:55.120 --> 26:03.040
you can find which are the closest neighbors. And you know, through that essentially move forward

26:03.040 --> 26:07.200
with making informed decisions or whether what you're trying to do works or not.

26:08.560 --> 26:17.040
Do you start with images that, you know, some smaller set of images that you know how they relate

26:17.040 --> 26:24.320
and then look and see if they relate in the space or... Yes, definitely. I mean, you know,

26:24.320 --> 26:32.640
it's like this unit testing sort of attitude towards the data. Yeah, and again, as I mentioned,

26:32.640 --> 26:40.960
with having millions of images, it doesn't allow you to literally just start processing

26:42.320 --> 26:48.000
gigabytes of data. And since most of the processing, if not all of the processing is done on the cloud,

26:48.000 --> 26:55.600
you can imagine, you know, the hefty price will have to pay. So talk a little bit about the

26:55.600 --> 27:01.840
result you saw with the zero shot approach. How did it work out for you? So we're still the

27:02.480 --> 27:11.520
testing phase. So we're working with smaller data. Crop season started sometime in May. And

27:11.520 --> 27:19.440
that means for us, stop whatever you do and make sure that the product works. So, yeah, so

27:20.000 --> 27:28.160
through some testing, it seems to be working well. Some networks work better than others, of course.

27:29.680 --> 27:40.160
But we have been able to identify what we are looking for, which is that we can decrease the number

27:40.160 --> 27:49.200
of images that express some unique characteristics by about 40%. Which is great, because this means that,

27:49.200 --> 27:55.040
you know, from, instead of annotating 100 images, now we can annotate 60. Since we're going through

27:55.040 --> 28:00.800
a process of somewhat cumbersome and elaborate process of annotation, where 18 images seen by

28:01.600 --> 28:04.720
two people at least, and then we need to run some

28:04.720 --> 28:13.280
comparisons and figure out if there's an agreement scoring high enough or not, and then correct the

28:13.280 --> 28:22.560
annotations. That's a huge time saver. And maybe should have raised this as a point more directly

28:22.560 --> 28:29.520
earlier on, but the, you know, we, in the context of deep learning, we talk, you know, we throw

28:29.520 --> 28:34.880
around this idea, collect more data, collect more data, collect more data, but really your ultimate

28:34.880 --> 28:40.880
goal here, well, your ultimate goal is to produce a better product, but your intermediate goal

28:42.080 --> 28:50.160
was not to collect more data. It was to, you know, identify less data or the right data to annotate,

28:50.160 --> 28:59.920
so that you wouldn't get eaten alive by annotation costs. That is very true, yes. And I think that I, we

28:59.920 --> 29:09.120
probably see that this is a direction most people are heading, even in recent research that's,

29:09.920 --> 29:17.440
that's probably a narrative that people are trying to push. And I think it actually makes sense.

29:17.440 --> 29:27.440
The fortunate thing about this particular problem, the counting, the plan counting is that we are

29:28.080 --> 29:34.320
receiving the data, whether we want it or not, because people are flying, collect, collect data

29:34.320 --> 29:40.800
for their own purposes, so they're there, so why not take advantage of it? But if you think of

29:40.800 --> 29:51.360
how we, we will be approaching new products, then that's a game changer, because

29:53.840 --> 29:59.600
you know, a new customer comes in and they want to count potatoes. The first thing people are asking

29:59.600 --> 30:07.920
me is, how many, how many images do you want? And I think after asking me so many times already,

30:07.920 --> 30:14.400
we have created some common understanding where they know they know not to ask me this question.

30:16.400 --> 30:22.160
Because my answer is always, I don't want more images, I want a variety of images, so it's better if

30:22.160 --> 30:28.880
you go to 10 different fields in different states and capture two images from it than giving me

30:28.880 --> 30:37.920
1000 images from two neighboring fields, for example. And are those specific numbers to, you know, kind of

30:39.120 --> 30:44.560
arbitrary numbers to prove your point, or is that kind of the order of magnitude that you actually

30:44.560 --> 30:51.920
require? I would take the actual order, mind you, would be around 100. That's the typical, because

30:51.920 --> 30:58.000
again, since they are quite large, the images are quite large, you may imagine to train the network,

30:58.000 --> 31:07.360
you have to crop them down. So out of one image, you can generate 40, smaller ones, but we'll

31:07.360 --> 31:13.280
actually go into training. So I think it's a substantial number, and it always depends on the

31:13.280 --> 31:20.720
problem we're trying to solve. But I think it's an indicative number. Yeah, just to kind of

31:20.720 --> 31:28.400
to play back, the, you know, it's a, when we think about, you know, deep learning, like that's an

31:28.400 --> 31:35.840
astonishingly small number, what we're doing with that 100, you know, that's a 100 images across

31:35.840 --> 31:45.360
some representative number of fields, and what you're creating with that is this embedding space

31:45.360 --> 31:55.600
that you can then use to create automatically more, you know, a more balanced data set,

31:55.600 --> 32:00.640
which will probably consist of a lot more images. Is that right?

32:00.640 --> 32:06.800
The reason why this helps a lot with the creation of new products is that I can, I can quickly

32:06.800 --> 32:16.960
identify how many more images, or in which locations or what type of images we would like to

32:17.520 --> 32:27.440
collect more of. So, let me get these 100 images, and then through this zero-short learning,

32:27.440 --> 32:35.840
we'll find some, we have already, we have identified a few different clusters that we expect,

32:36.880 --> 32:41.040
you know, different light conditions, different sort of different sort of types, whether the

32:41.040 --> 32:48.560
images are the edge of the field or not. So, these are things that we expect. So, essentially,

32:49.200 --> 32:55.840
by doing this clustering, I know where I am short, what type of image I am short of.

32:55.840 --> 33:04.400
So, this is sort of the first leap iteration. I get 100 images, identify what type of images I want

33:04.400 --> 33:08.320
more, and then I send them back out to collect more of that type.

33:08.320 --> 33:15.440
Oh, okay. So, it's not even just reducing labeling costs. Here, we're talking about,

33:16.400 --> 33:22.320
you know, the cost of sending someone to a field and flying a plane or a drone, and, you know,

33:22.320 --> 33:28.720
it's the whole end-to-end data collection costs, which I'm imagining is significantly more

33:28.720 --> 33:36.000
even. Yes, it can be, especially people are busy during the growing season. So, it is, you know,

33:36.000 --> 33:42.320
I have to be very respectful of their time when I'm asking you data. And in previous years, it was more

33:42.320 --> 33:49.200
of a gut feeling. You know, we have seen so many images, we have seen what works or what and what

33:49.200 --> 33:57.440
doesn't work. But now it's more specific, more targeted. I want this, you know, fly at this time,

33:58.080 --> 34:06.000
try to get images of where, you know, the day is not cloudy. So, it gives you a lot of space

34:06.000 --> 34:12.560
to plan. So, this last example you talk through is still a counting type of a problem. Do you see the

34:12.560 --> 34:23.920
approach applying to a broader set of projects? Yes, for sure. The next place I would like to

34:23.920 --> 34:31.040
spend more time is detect the weed detection. And that is also something that is significant for

34:31.040 --> 34:41.280
the industry. And it's also a problem where the annotation is much more costly. You can imagine

34:41.280 --> 34:46.960
what, when you're counting things, you are requesting a human annotator to click on the object you

34:46.960 --> 34:54.320
want to count for weeds, especially when you have patches of weed. We're talking about drawing

34:54.320 --> 35:03.680
polygons. And the polygons are a series of clicks. And that requires, you know, more concentration

35:03.680 --> 35:07.120
from the annotator because there may be small weeds. We don't want to miss those.

35:07.120 --> 35:14.800
Polygons are generally more time consuming than just clicking and then going to the next one.

35:15.760 --> 35:20.640
So, this is, I think it will be a time saver for annotations there too.

35:20.640 --> 35:32.080
And maybe one last question. More broadly, has what you've learned in, you know, this project and

35:32.080 --> 35:40.080
this approach change how you think about deep learning problems and applying deep learning within

35:40.080 --> 35:45.920
your space? Yeah. I mean, previously I thought that 80% of the solution is data. Now I think 90%

35:45.920 --> 35:56.560
of the solution is data. This has, in use, some changes or demanded some changes in how we architect

35:56.560 --> 36:06.960
the deep learning framework we're building. These are more technical changes, essentially.

36:06.960 --> 36:28.080
So, we have a data pipeline that goes from fetching data from our main database to reviewing the

36:28.080 --> 36:33.120
selection of images we're doing, to sending them to a couple of human annotators and then

36:33.120 --> 36:39.920
compare their annotations and do some quality control. So, initially, the way we were doing the

36:40.480 --> 36:49.840
fetch data from the main database was either random or I'm going to select a few images and then say

36:49.840 --> 36:56.880
this image has weeds in it, this image has different type of soil, this image is cloudy, etc.

36:56.880 --> 37:05.920
Now things are more automated, which means that we can process, we can make this type of decision

37:05.920 --> 37:15.920
in season since it's automated. We can essentially assess the images we collect Monday to Friday,

37:16.720 --> 37:21.680
we can make the assessment over the weekend and then on Monday we can start training a new

37:21.680 --> 37:30.480
network if there are any new data we want to include. As opposed to, before that, since it was a

37:30.480 --> 37:37.680
human intense process, we had to wait until the end of the growing season where everybody was

37:37.680 --> 37:48.320
relaxed and free and then we could do this type of data discovery. So, yeah, there are some small

37:48.320 --> 37:55.440
changes, essentially, with additional features that we couldn't do before. Awesome, awesome.

37:55.440 --> 38:02.320
Well, it sounds like a very exciting project and journey and I'm super appreciative of you jumping

38:02.320 --> 38:07.120
on and sharing a bit about what you've accomplished to me, Tris. Thank you very much, Sam. Thank you

38:07.120 --> 38:18.400
for having me, for reaching out and I hope this, my experience has helped. I'll see if there goes.

38:18.400 --> 38:45.840
Awesome. Thanks so much. Bye-bye.

