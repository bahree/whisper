1
00:00:00,000 --> 00:00:17,040
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:17,040 --> 00:00:22,040
people doing interesting things in machine learning and artificial intelligence.

3
00:00:22,040 --> 00:00:25,080
I'm your host Sam Charrington.

4
00:00:25,080 --> 00:00:30,720
Thanks so much to everyone who sent in their favorite quote from last week's podcast.

5
00:00:30,720 --> 00:00:32,720
Your stickers are on the way.

6
00:00:32,720 --> 00:00:36,720
We had a bunch of fun with this contest and we've decided to continue it while our sticker

7
00:00:36,720 --> 00:00:38,320
supplies last.

8
00:00:38,320 --> 00:00:42,200
So definitely send us your favorite quote from today's show as well.

9
00:00:42,200 --> 00:00:47,880
You can do that via a comment or post on Facebook, Twitter, YouTube or SoundCloud as well as

10
00:00:47,880 --> 00:00:51,160
the show notes page for any episode of the show.

11
00:00:51,160 --> 00:00:54,520
Okay, and now about today's show.

12
00:00:54,520 --> 00:00:56,520
I've got a very special guest this week.

13
00:00:56,520 --> 00:01:01,520
His name is Evan Wright and he's principal data scientist at Anomaly.

14
00:01:01,520 --> 00:01:06,600
If that name sounds familiar, it's because Evan was the winner of our O'Reilly strata

15
00:01:06,600 --> 00:01:10,360
Hadoop World ticket giveaway earlier this month.

16
00:01:10,360 --> 00:01:14,440
Evan and I met up at the conference last week and got to chat about a number of topics

17
00:01:14,440 --> 00:01:18,240
in the realm of machine learning and cybersecurity.

18
00:01:18,240 --> 00:01:22,600
We discussed the three big problems in cybersecurity that ML can help with.

19
00:01:22,600 --> 00:01:27,480
The challenges of acquiring ground truth and cybersecurity in some ways to do it and

20
00:01:27,480 --> 00:01:33,440
the use of decision trees, generative adversarial networks and other algorithms in that field.

21
00:01:33,440 --> 00:01:36,960
I think you really enjoy this show.

22
00:01:36,960 --> 00:01:41,160
Speaking of cybersecurity, that's just one of the many topics that will be covered in

23
00:01:41,160 --> 00:01:46,880
the future of data summit event that I'm hosting May 15th and 16th at the inter-op conference

24
00:01:46,880 --> 00:01:48,760
in Las Vegas.

25
00:01:48,760 --> 00:01:54,600
At the event, Diana Kelly, a global executive security advisor with IBM, will be talking

26
00:01:54,600 --> 00:01:58,760
about the future of securing cloud, IoT and big data systems.

27
00:01:58,760 --> 00:02:04,240
We'll also be giving you a glimpse into the future of machine learning in AI, so-called

28
00:02:04,240 --> 00:02:10,720
fog or edge computing, augmented and virtual reality, blockchain, algorithmic IT operations

29
00:02:10,720 --> 00:02:12,440
and much more.

30
00:02:12,440 --> 00:02:18,040
You can learn more about the summit at twimmolai.com slash future of data.

31
00:02:18,040 --> 00:02:20,040
And now on to the show.

32
00:02:20,040 --> 00:02:29,280
All right, hey, everyone.

33
00:02:29,280 --> 00:02:33,520
I am here with Evan Wright, Principal Data Scientist with Anomaly.

34
00:02:33,520 --> 00:02:39,960
You may remember Evan's name because we announced him as the winner of our O'Reilly Stratta

35
00:02:39,960 --> 00:02:42,840
Hadoop ticket giveaway.

36
00:02:42,840 --> 00:02:46,840
Evan and I decided to get together and record a little show.

37
00:02:46,840 --> 00:02:48,640
Evan, say hi.

38
00:02:48,640 --> 00:02:49,640
Good morning, everybody.

39
00:02:49,640 --> 00:02:53,320
Good afternoon or good evening as it applies.

40
00:02:53,320 --> 00:02:54,320
Awesome.

41
00:02:54,320 --> 00:03:00,920
Evan, why don't we start by having you talk a little bit about the company you're at

42
00:03:00,920 --> 00:03:02,240
and what you do there?

43
00:03:02,240 --> 00:03:03,240
Sure.

44
00:03:03,240 --> 00:03:04,240
Sure.

45
00:03:04,240 --> 00:03:07,120
And Anomaly, we are focused on threat intelligence.

46
00:03:07,120 --> 00:03:12,680
I think there's really three big problems in cybersecurity that are really ripe for using

47
00:03:12,680 --> 00:03:15,400
machine learning to improve.

48
00:03:15,400 --> 00:03:18,680
One problem is malware detection.

49
00:03:18,680 --> 00:03:23,680
Another problem is threat intelligence, which is really the focus of our work.

50
00:03:23,680 --> 00:03:29,000
And another one is sort of stream detection of when you've got a series of events like

51
00:03:29,000 --> 00:03:33,960
for example, network traffic, trying to sift through that and find the security relevant

52
00:03:33,960 --> 00:03:36,200
concerns out of that stream.

53
00:03:36,200 --> 00:03:42,880
So we're focused on the latter two the most, especially as far as understanding threat

54
00:03:42,880 --> 00:03:44,400
intelligence better.

55
00:03:44,400 --> 00:03:51,000
So threat intelligence is this idea of imagine you're conducting an investigation like if

56
00:03:51,000 --> 00:03:56,040
you're a law enforcement, you would collect pieces of evidence and try and stitch together

57
00:03:56,040 --> 00:03:58,640
the story of what happened.

58
00:03:58,640 --> 00:04:04,000
And if possible, try and stop things, stop bad things happening before they actually

59
00:04:04,000 --> 00:04:05,000
occur.

60
00:04:05,000 --> 00:04:08,160
So something similar is the case in organizations.

61
00:04:08,160 --> 00:04:14,000
They run these security teams and security teams want to know things like of all the things

62
00:04:14,000 --> 00:04:19,920
that our network is talking out to on the internet are some of those malicious, right?

63
00:04:19,920 --> 00:04:21,400
And they want to be able to separate that.

64
00:04:21,400 --> 00:04:27,760
So threat intelligence, fundamentally, is understanding the tools and the infrastructure

65
00:04:27,760 --> 00:04:34,640
out on the internet and in your network to be able to separate those, right?

66
00:04:34,640 --> 00:04:39,880
So very pragmatically, it's something like a list of IPs or domains to start with,

67
00:04:39,880 --> 00:04:47,800
which if your network has activity going to these IPs or domains, then it would be malicious.

68
00:04:47,800 --> 00:04:52,720
So a lot of our main focus at anomaly is we create a threat intelligence platform.

69
00:04:52,720 --> 00:04:56,800
The idea is there's all these security vendors out there which kind of give us this ground

70
00:04:56,800 --> 00:04:57,800
truth.

71
00:04:57,800 --> 00:05:00,280
And in security, ground truth is so, so important.

72
00:05:00,280 --> 00:05:06,880
It is really the fundamental problem and why machine learning is hard in cybersecurity

73
00:05:06,880 --> 00:05:08,960
because ground truth is really expensive.

74
00:05:08,960 --> 00:05:16,840
We're not exactly spoiled in some sense, like having image data, like having video data.

75
00:05:16,840 --> 00:05:22,200
Because in those spaces, you have a very clear sense of what data should I collect and

76
00:05:22,200 --> 00:05:23,560
what data do I care about.

77
00:05:23,560 --> 00:05:27,760
If you look at an image, it's easy to tell if it's a cat or not to have that ground truth

78
00:05:27,760 --> 00:05:29,880
for labeling images.

79
00:05:29,880 --> 00:05:32,280
In cybersecurity, it's much, much harder.

80
00:05:32,280 --> 00:05:36,760
Instead of just being a layman who can look at an image and tell you it's a cat, in the

81
00:05:36,760 --> 00:05:41,280
case of cybersecurity, usually you need a very specialized expert to be able to tell you

82
00:05:41,280 --> 00:05:44,480
if you want really good confidence.

83
00:05:44,480 --> 00:05:52,560
They often need to be even more specialized because the best ground truth comes from analyzing

84
00:05:52,560 --> 00:05:55,920
reverse engineering pieces of software.

85
00:05:55,920 --> 00:06:01,760
So reverse engineering how it works, maybe there's, maybe the software calls out to components

86
00:06:01,760 --> 00:06:06,280
out on the internet that are malicious.

87
00:06:06,280 --> 00:06:08,520
And you need to be able to pull those out.

88
00:06:08,520 --> 00:06:13,080
So all of these indicators are essentially pieces of evidence that we try and put together

89
00:06:13,080 --> 00:06:18,920
and organizations want to alert on and stop or at least investigate if it's happening

90
00:06:18,920 --> 00:06:20,120
in their organization.

91
00:06:20,120 --> 00:06:24,640
So our company provides a stream of this threat intelligence.

92
00:06:24,640 --> 00:06:27,920
And that product is incidentally called threat stream.

93
00:06:27,920 --> 00:06:29,640
Okay.

94
00:06:29,640 --> 00:06:37,040
So it sounds analogous to maybe the kind of signatures that an antivirus type of company

95
00:06:37,040 --> 00:06:42,640
would produce, is that a fair assessment or maybe even another way of asking that.

96
00:06:42,640 --> 00:06:46,400
Like what makes the stream streamy?

97
00:06:46,400 --> 00:06:49,800
So different people can get this ground truth in different ways.

98
00:06:49,800 --> 00:06:53,680
So some of them are, in fact, antivirus companies.

99
00:06:53,680 --> 00:07:00,560
Some of them are organizations that do sort of a forensic investigation on incidents that

100
00:07:00,560 --> 00:07:03,400
happen after the fact.

101
00:07:03,400 --> 00:07:07,920
Some of them are, in fact, applying machine learning given their different data sets.

102
00:07:07,920 --> 00:07:17,480
So everyone has sort of a different strategy, both in their data collection and in how they

103
00:07:17,480 --> 00:07:18,560
validate it.

104
00:07:18,560 --> 00:07:22,800
Some of it is extremely qualitative by hiring cybersecurity experts.

105
00:07:22,800 --> 00:07:27,440
Some of it is extremely data driven and quantitative and pretty much everything you can imagine

106
00:07:27,440 --> 00:07:28,440
in between.

107
00:07:28,440 --> 00:07:32,880
And organizations really struggle from this problem of which one do I pick?

108
00:07:32,880 --> 00:07:38,000
And then if they can decide that, then they still have this issue of how reliable is that

109
00:07:38,000 --> 00:07:39,320
stream.

110
00:07:39,320 --> 00:07:45,400
So one of the things that in our platform we do, our machine learning use case, is to give

111
00:07:45,400 --> 00:07:51,400
people an assessment of how confident are we in this indicator from the stream.

112
00:07:51,400 --> 00:07:59,600
So we're able to, you buy sort of a big database of, this is what all the badness is.

113
00:07:59,600 --> 00:08:02,560
You pay a particular provider.

114
00:08:02,560 --> 00:08:12,480
Then we run our own largely regression based, like ensemble regression, on the input feeds

115
00:08:12,480 --> 00:08:14,240
that they give us.

116
00:08:14,240 --> 00:08:18,720
And then we use that to sort of augment a lot of the incoming feeds, especially when

117
00:08:18,720 --> 00:08:22,800
we go out to the internet, collect free and open source data.

118
00:08:22,800 --> 00:08:27,120
Then we pull back data and how reliable is this particular indicator?

119
00:08:27,120 --> 00:08:29,840
How reliable is this feed in general?

120
00:08:29,840 --> 00:08:35,760
So, so that's where we use a lot of our scoring algorithms to help separate, you know, how

121
00:08:35,760 --> 00:08:41,280
seriously should you take an action on this particular indicator from this feed?

122
00:08:41,280 --> 00:08:45,040
Because there's a drowning in data problem in cybersecurity.

123
00:08:45,040 --> 00:08:50,800
Yes, I'd like to get more concrete on what the specific feeds are.

124
00:08:50,800 --> 00:08:59,440
I know some examples of things that I've seen folks doing are, you could have, you know,

125
00:08:59,440 --> 00:09:03,760
blacklisted IPs, for example, you know, so that may be one, is that an example of a feed

126
00:09:03,760 --> 00:09:04,760
in this case?

127
00:09:04,760 --> 00:09:10,600
Sure, yeah, so you could have, so we scrape a bunch of open source data.

128
00:09:10,600 --> 00:09:14,160
So that's part of it that we just sort of provide with the platform.

129
00:09:14,160 --> 00:09:18,560
Because, you know, intuitively, it kind of makes sense that that needs more vetting, you

130
00:09:18,560 --> 00:09:24,800
know, and so the ML scoring is more important in that role, because if it's just an open source

131
00:09:24,800 --> 00:09:31,600
intelligence feed that comes down, having the scoring is particularly important, whereas

132
00:09:31,600 --> 00:09:37,080
if you're paying for a particular feed, then you intuitively probably have a bit more

133
00:09:37,080 --> 00:09:38,080
confidence in it.

134
00:09:38,080 --> 00:09:45,320
So certainly, you may be tipped off by honeypots, right?

135
00:09:45,320 --> 00:09:50,040
So that's what honeypots are one example of how to collect this data, and that's basically

136
00:09:50,040 --> 00:09:55,720
we put a very vulnerable looking machine on the internet and just try and advertise that

137
00:09:55,720 --> 00:09:58,760
we're vulnerable and get people to attack us.

138
00:09:58,760 --> 00:10:03,720
So we have an open source project called the Modern Honeynet, which is MHN, which is

139
00:10:03,720 --> 00:10:10,800
used to collect a lot of these, no matter what type of honeypot you have, then we can

140
00:10:10,800 --> 00:10:15,200
take this data and sort of centralize it, and it simplifies the data collection piece,

141
00:10:15,200 --> 00:10:18,360
no matter how many different types of honeypots you have.

142
00:10:18,360 --> 00:10:20,360
So that's an example of one of them.

143
00:10:20,360 --> 00:10:28,360
So we've got the kind of the IP, the black listed IPs, like what are some other data

144
00:10:28,360 --> 00:10:36,800
sources that you're collecting, cleaning, providing that feed into, you know, making ultimately

145
00:10:36,800 --> 00:10:42,120
you're trying to make a determination whether, you know, some activity is, you know, likely

146
00:10:42,120 --> 00:10:43,360
to be a threat.

147
00:10:43,360 --> 00:10:45,160
Yeah, absolutely.

148
00:10:45,160 --> 00:10:48,560
And so some organizations may choose to immediately block it.

149
00:10:48,560 --> 00:10:52,400
Other organizations may just want to monitor it and see what happened.

150
00:10:52,400 --> 00:10:57,960
Some organizations get breached and then need to do a retrospective evaluation of how

151
00:10:57,960 --> 00:10:58,960
did this get in?

152
00:10:58,960 --> 00:10:59,960
How did this happen?

153
00:10:59,960 --> 00:11:03,480
What was our whole, what was our chink and our armor?

154
00:11:03,480 --> 00:11:08,840
And so with the retrospective evaluation, we've got another, another tool, anomaly enterprise

155
00:11:08,840 --> 00:11:15,440
that's very focused on applying, basically taking the work off of your existing information

156
00:11:15,440 --> 00:11:19,360
aggregation tools, call the SIM.

157
00:11:19,360 --> 00:11:25,640
Taking the workload off that and applying some ML, applying these known pieces of evidence

158
00:11:25,640 --> 00:11:34,200
like IPs and domains and URLs and file hashes when they occur in the computer network.

159
00:11:34,200 --> 00:11:42,520
Yeah, I'm still trying to wrap my head around like concretely the specific, the specific

160
00:11:42,520 --> 00:11:49,480
types of data that you're ultimately be running machine learning algorithms over.

161
00:11:49,480 --> 00:11:50,480
Right.

162
00:11:50,480 --> 00:11:51,480
Sure.

163
00:11:51,480 --> 00:11:52,480
So there's two kind of main buckets.

164
00:11:52,480 --> 00:11:55,880
Host-based data and the other is network-based data.

165
00:11:55,880 --> 00:12:01,360
So on the host-based side, we have, the most common is something like file hashes, right?

166
00:12:01,360 --> 00:12:06,760
So unique identifier of a file because it's easy to rename a file if it's a piece of malware

167
00:12:06,760 --> 00:12:08,400
that's a really bad indicator.

168
00:12:08,400 --> 00:12:15,000
So we hash the contents of the file or in more sophisticated cases, we hash subsets of

169
00:12:15,000 --> 00:12:20,760
the file to indicate maybe that there's a piece of the malware that's, you know, getting

170
00:12:20,760 --> 00:12:23,600
passed around, for example.

171
00:12:23,600 --> 00:12:29,960
So another kind of lower volume, host-based piece of information that we track is things

172
00:12:29,960 --> 00:12:38,440
like, you know, function names and regaxes and so these types of sub pieces of a piece

173
00:12:38,440 --> 00:12:45,040
of software that could be used in a piece of malware because it's, when you start tracking

174
00:12:45,040 --> 00:12:49,280
by a file hash, so much of cybersecurity is an evolution, right?

175
00:12:49,280 --> 00:12:54,240
So we have an active adversary trying to elude us, which is part of what makes it a bit

176
00:12:54,240 --> 00:12:56,120
more of a unique space.

177
00:12:56,120 --> 00:13:02,040
And so one way this manifests is the type of data that we collect, for example, hashing

178
00:13:02,040 --> 00:13:03,520
files.

179
00:13:03,520 --> 00:13:07,880
When adversaries know that we do that to track their malware, they're going to try and

180
00:13:07,880 --> 00:13:10,440
find methods to get around it.

181
00:13:10,440 --> 00:13:21,760
So one way to get around hashing of files is to just randomly add a little bit of gibberish

182
00:13:21,760 --> 00:13:25,320
in a non-executed portion of the file.

183
00:13:25,320 --> 00:13:31,920
And you can get into, in the ecosystem, like the economic ecosystem of malware, you can

184
00:13:31,920 --> 00:13:42,160
have automatic systems to just add, pad, garbage in the end of the file, just to change the

185
00:13:42,160 --> 00:13:45,800
file hash, just to mislead you.

186
00:13:45,800 --> 00:13:49,920
And is the idea with that to just render tools like yours not useful because there's

187
00:13:49,920 --> 00:13:54,400
so much noise out there, or are you trying to replicate the hash of, like, replicate

188
00:13:54,400 --> 00:13:55,400
the known good hash?

189
00:13:55,400 --> 00:14:03,160
Well, the idea is that file hashes are a very commonly used strategy in cybersecurity

190
00:14:03,160 --> 00:14:04,240
in general.

191
00:14:04,240 --> 00:14:08,480
And so it's a very popular way to uniquely identify a file.

192
00:14:08,480 --> 00:14:13,840
And so adversaries, they're not doing to elude us, they're doing it to elude everyone.

193
00:14:13,840 --> 00:14:18,120
And so then we get into smaller parts of the file.

194
00:14:18,120 --> 00:14:27,440
So there's, in like a PE32, like an executable on Windows, you might have different sections.

195
00:14:27,440 --> 00:14:33,760
If you break down the assembly code, you might hash the import table for all the libraries

196
00:14:33,760 --> 00:14:36,040
that you import.

197
00:14:36,040 --> 00:14:41,840
So these more granular notions of hashing make it harder for the adversaries to avoid,

198
00:14:41,840 --> 00:14:48,480
but it requires a larger context of piecing together the different pieces to the puzzle.

199
00:14:48,480 --> 00:14:54,200
So you've got the host base data, you've got the network, net flow data, presumably,

200
00:14:54,200 --> 00:14:58,440
and perhaps other types of network data.

201
00:14:58,440 --> 00:15:05,840
That stuff is all coming off of ultimately routers, firewalls, network devices.

202
00:15:05,840 --> 00:15:13,320
And then you are talking about the difficulty of kind of coming to identifying ground truth.

203
00:15:13,320 --> 00:15:23,320
Like you have all this data that you've collected, how do you go about labeling or using unsupervised

204
00:15:23,320 --> 00:15:30,040
techniques machine learning wise to try to identify the bad actors?

205
00:15:30,040 --> 00:15:35,280
So we're using, most of our work is definitely in the supervised space.

206
00:15:35,280 --> 00:15:41,880
So like XG boost, random forest, that sort of a space, ensembles, we see a lot of benefit

207
00:15:41,880 --> 00:15:43,360
from.

208
00:15:43,360 --> 00:15:47,760
What's interesting in the threat and intelligence space, I've tried, I've personally looked

209
00:15:47,760 --> 00:15:52,880
at a few different problems and pretty reliably ensemble decision trees or ensemble regression

210
00:15:52,880 --> 00:15:56,520
trees end up being the best strategy.

211
00:15:56,520 --> 00:16:00,120
So maybe can you talk a little bit about some of the different types of problems that

212
00:16:00,120 --> 00:16:01,520
you've looked at?

213
00:16:01,520 --> 00:16:09,640
And how do you go about defining the problems in a way that leads you to being able to solve

214
00:16:09,640 --> 00:16:11,440
them using ML?

215
00:16:11,440 --> 00:16:12,440
Sure.

216
00:16:12,440 --> 00:16:13,440
Sure.

217
00:16:13,440 --> 00:16:17,560
So one of the important rules of machine learning, I think, is only use machine learning

218
00:16:17,560 --> 00:16:22,560
when you actually need it, because most machine learning has false positives and false negatives.

219
00:16:22,560 --> 00:16:27,640
If you can come up with maybe logical expressions that don't have those, then you should use

220
00:16:27,640 --> 00:16:29,480
other methods.

221
00:16:29,480 --> 00:16:36,680
And so when I think of the order of the tools that we use, if we can use the very first

222
00:16:36,680 --> 00:16:42,360
most naive version of a tool, a sort of a white list and black list, just big, big lists

223
00:16:42,360 --> 00:16:47,600
of exact matching, then the next level of tool is a bit more fuzzy matching, starting

224
00:16:47,600 --> 00:16:54,880
to get into space of reg X's and maybe signatures that are capturing patterns with the typical

225
00:16:54,880 --> 00:16:57,920
sort of programming language style of logical expressions, right?

226
00:16:57,920 --> 00:17:03,720
If we see these group of things, possibly followed by another group of things.

227
00:17:03,720 --> 00:17:08,840
If those two methods fail, then your problem is really ripe for machine learning.

228
00:17:08,840 --> 00:17:12,400
And that's really the space we're interested in.

229
00:17:12,400 --> 00:17:18,200
I feel like you need a little bit of intuition to believe that machine learning would work.

230
00:17:18,200 --> 00:17:23,040
So one example is domain generation algorithms.

231
00:17:23,040 --> 00:17:30,400
So for a long time, the strategy is when you have malware infecting your computer network

232
00:17:30,400 --> 00:17:36,240
and it calls out to maybe an IP or domain, then you block that and you're good.

233
00:17:36,240 --> 00:17:40,760
So first we did that with IPs and then the bad guys became aware of it.

234
00:17:40,760 --> 00:17:46,320
So then they started calling out to domains and then once those were getting blocked and

235
00:17:46,320 --> 00:17:51,480
the strategy was successful, then the bad guys evolved yet again.

236
00:17:51,480 --> 00:17:58,400
And so the innovation after that was in one sense fast flux, which is sort of moving

237
00:17:58,400 --> 00:18:04,080
around IPs, but the one that really stuck is something called domain generation algorithms.

238
00:18:04,080 --> 00:18:09,040
So what that is is in a piece of malware itself, you have a pseudo random function that

239
00:18:09,040 --> 00:18:15,760
is known to the computer that's infected with the malware and it's known to the adversary

240
00:18:15,760 --> 00:18:17,360
maybe out on the internet, right?

241
00:18:17,360 --> 00:18:20,320
The guy that's running the whole botnet.

242
00:18:20,320 --> 00:18:28,760
So both of those two points know what the pseudo random algorithm will predict for tomorrow.

243
00:18:28,760 --> 00:18:32,200
And the problem is in between those two points, no one knows.

244
00:18:32,200 --> 00:18:36,560
So the guys that are responsible to defend the network have no idea what the domains might

245
00:18:36,560 --> 00:18:39,840
be tomorrow or the next day.

246
00:18:39,840 --> 00:18:48,280
And so this asymmetry that gets created between the infected malware and the bad guy in

247
00:18:48,280 --> 00:18:52,200
the internet know what's going to happen and the good guys at the perimeter that have

248
00:18:52,200 --> 00:18:56,440
to stop it creates a pretty big disadvantage, right?

249
00:18:56,440 --> 00:19:01,720
Because there's no way they're going to know what domains need to be blocked.

250
00:19:01,720 --> 00:19:09,880
So the problem gets compounded, the asymmetry is compounded specifically because you not

251
00:19:09,880 --> 00:19:12,400
only have one domain like per day, right?

252
00:19:12,400 --> 00:19:16,960
These domains would change every day, for example, but it's not just one.

253
00:19:16,960 --> 00:19:22,600
Maybe that the malware calls out to 1,000 domains each day.

254
00:19:22,600 --> 00:19:27,640
And the bad guys only have to find one chink in your armor and the good guys have to defend

255
00:19:27,640 --> 00:19:29,760
100%.

256
00:19:29,760 --> 00:19:36,400
And so exploiting that asymmetry, the bad guys just maybe pick one or two of the domains

257
00:19:36,400 --> 00:19:38,880
that the malware will call out to tomorrow.

258
00:19:38,880 --> 00:19:44,200
They register that ahead of time and that's how you utilize this communication mechanism.

259
00:19:44,200 --> 00:19:50,080
Because all malware in order to be useful needs to have some network connectivity, right?

260
00:19:50,080 --> 00:19:51,840
There's very few exceptions.

261
00:19:51,840 --> 00:19:55,520
And so, you know, if you're stealing someone's credit card information, you need to send

262
00:19:55,520 --> 00:19:59,160
that back so it can be resold and you can monetize it, right?

263
00:19:59,160 --> 00:20:03,480
If you're doing economic espionage, you need to get the data out of the network and back

264
00:20:03,480 --> 00:20:05,920
into your organization.

265
00:20:05,920 --> 00:20:13,320
So this is a sort of fundamental rule of attacks is that nearly every type of attack is about

266
00:20:13,320 --> 00:20:17,520
some sort of taking information.

267
00:20:17,520 --> 00:20:23,000
So then since the bad guys are able to use this domain generation algorithm to get the

268
00:20:23,000 --> 00:20:30,560
information out of the network, then this is clearly a weak point in the defense of

269
00:20:30,560 --> 00:20:33,080
the good guys perimeter, right?

270
00:20:33,080 --> 00:20:40,280
And so since it's a pseudo random algorithm, the intuition is that humans can look at it

271
00:20:40,280 --> 00:20:43,240
and say, hey, that domain looks like it's gibberish, right?

272
00:20:43,240 --> 00:20:47,280
It might be XLJQBZF2, right?

273
00:20:47,280 --> 00:20:50,720
Absolutely.

274
00:20:50,720 --> 00:21:01,360
And so it's usually the case that these very random sort of high entropy domains are affiliated

275
00:21:01,360 --> 00:21:03,280
with domain generation algorithms.

276
00:21:03,280 --> 00:21:07,280
There's a few exceptions, but the vast majority is like that.

277
00:21:07,280 --> 00:21:12,840
And so what's fascinating is that human analysts, when they see it, they're like duh.

278
00:21:12,840 --> 00:21:14,640
I can pick that out really easy.

279
00:21:14,640 --> 00:21:16,920
Why is this really a problem?

280
00:21:16,920 --> 00:21:21,680
Well because these algorithms may sort of tune down how aggressively they call out, start

281
00:21:21,680 --> 00:21:23,600
blending in with other traffic.

282
00:21:23,600 --> 00:21:28,080
And if you have to look through maybe hundreds of millions of domains that your organization

283
00:21:28,080 --> 00:21:32,160
is called out to yesterday, are you going to be able to look through all of them and

284
00:21:32,160 --> 00:21:33,160
pick this out?

285
00:21:33,160 --> 00:21:34,760
The answer is no.

286
00:21:34,760 --> 00:21:36,840
We need to automate.

287
00:21:36,840 --> 00:21:42,600
Because you can't really automate this very well with logical expressions.

288
00:21:42,600 --> 00:21:49,040
And even if you did, you might be able to reconstruct maybe like a regax that describes

289
00:21:49,040 --> 00:21:51,080
this one particular algorithm.

290
00:21:51,080 --> 00:21:54,000
Oh, but by the way, there's different types of malware.

291
00:21:54,000 --> 00:21:56,240
They all use very different algorithms.

292
00:21:56,240 --> 00:22:00,400
They're often have different seeds to their randomization.

293
00:22:00,400 --> 00:22:05,760
And so you really need an effective way to be able to distinguish between these random

294
00:22:05,760 --> 00:22:11,960
looking domains and normal user traffic and activity.

295
00:22:11,960 --> 00:22:20,160
So this space is really what motivated my first project in this space of machine learning

296
00:22:20,160 --> 00:22:22,760
applied to cybersecurity.

297
00:22:22,760 --> 00:22:24,240
So we saw these things happening.

298
00:22:24,240 --> 00:22:27,520
This was in 2009 actually.

299
00:22:27,520 --> 00:22:33,400
So in 2009, I was talking to some colleagues.

300
00:22:33,400 --> 00:22:38,000
And they, we had realized this was a problem that all these domains were happening out

301
00:22:38,000 --> 00:22:39,600
there in the internet.

302
00:22:39,600 --> 00:22:46,880
And so we put together a supervised prototype and turned out it was able to detect them

303
00:22:46,880 --> 00:22:49,440
pretty effectively.

304
00:22:49,440 --> 00:22:52,360
So that was in about 2009.

305
00:22:52,360 --> 00:23:00,320
And then I think an interesting story that's kind of unique to security is that in a little

306
00:23:00,320 --> 00:23:08,600
over a year later, we, someone, about a year and a half later, someone published a very,

307
00:23:08,600 --> 00:23:13,480
very similar strategy, a little less scalable, but a very similar strategy.

308
00:23:13,480 --> 00:23:14,960
So this was focused.

309
00:23:14,960 --> 00:23:21,040
This was, I can give you the name, this was from Texas AMU.

310
00:23:21,040 --> 00:23:24,200
And they published this paper, which was some good work.

311
00:23:24,200 --> 00:23:29,000
But we intentionally chose to not publish our strategy at the time.

312
00:23:29,000 --> 00:23:32,960
Because there's this question of, if you tell an adversary, you can detect them.

313
00:23:32,960 --> 00:23:34,760
What will happen?

314
00:23:34,760 --> 00:23:39,120
So the upside to the situation is, we had been monitoring them.

315
00:23:39,120 --> 00:23:44,040
We had been monitoring the overall activity of these domain generation algorithm domains

316
00:23:44,040 --> 00:23:46,000
out on the internet.

317
00:23:46,000 --> 00:23:55,680
And so when the report was released at UsNix, it took about two to three weeks or so.

318
00:23:55,680 --> 00:24:01,120
And you can clearly see the points when the information from the conference got back

319
00:24:01,120 --> 00:24:03,520
to the adversaries.

320
00:24:03,520 --> 00:24:07,760
And we can see this, first it drops off, nearly completely.

321
00:24:07,760 --> 00:24:11,960
And that happened maybe lasted about two months or so.

322
00:24:11,960 --> 00:24:16,760
And then after that, you see a significant change in the variance of these.

323
00:24:16,760 --> 00:24:22,920
So it might have been something like, you know, maybe like the weekend activity, you

324
00:24:22,920 --> 00:24:26,920
know, was much higher, but the weekday was lower, something, something like that.

325
00:24:26,920 --> 00:24:30,280
You see a big change in the variance of day-to-day activity.

326
00:24:30,280 --> 00:24:33,920
Then before the paper was released, I have a chart of this, too.

327
00:24:33,920 --> 00:24:34,920
All right.

328
00:24:34,920 --> 00:24:45,880
So, so you did this project, tell, tell us about how, how you wanted about solving the problem.

329
00:24:45,880 --> 00:24:51,400
You know, what techniques did you use and what you learned in the process of, of deploying

330
00:24:51,400 --> 00:24:52,400
them?

331
00:24:52,400 --> 00:24:53,400
Sure.

332
00:24:53,400 --> 00:24:59,800
So, one thing that was very interesting was, I thought was interesting, was understanding

333
00:24:59,800 --> 00:25:03,880
how many, which different models were more effective.

334
00:25:03,880 --> 00:25:16,000
And so, I remember at the time I was using WECA APIs and instrumenting various type of algorithms.

335
00:25:16,000 --> 00:25:22,720
I was very interested in a set-up across fold validation and figure out which algorithms

336
00:25:22,720 --> 00:25:25,680
are going to work better.

337
00:25:25,680 --> 00:25:33,040
The one that ultimately I found worked the best was a mix of adabust with adabust on

338
00:25:33,040 --> 00:25:34,040
Ripper.

339
00:25:34,040 --> 00:25:37,320
So if you've ever heard of the Ripper algorithm was made by William Cohen.

340
00:25:37,320 --> 00:25:42,560
It's a rule-based learner, very similar to, very similar to a decision tree.

341
00:25:42,560 --> 00:25:47,360
But it's a non-boosted algorithm and then boosting it after the fact ended up being more

342
00:25:47,360 --> 00:25:51,560
effective than something like, you know, maybe rent and forest out of the box.

343
00:25:51,560 --> 00:25:53,080
Well, let's dig into all of this.

344
00:25:53,080 --> 00:25:57,840
So, WECA, I've heard of a bunch of times.

345
00:25:57,840 --> 00:26:00,400
I don't know a whole lot about it.

346
00:26:00,400 --> 00:26:06,880
I get the impression that it has declined in popularity relative to newer things, but

347
00:26:06,880 --> 00:26:09,320
maybe tell, what's your take on that?

348
00:26:09,320 --> 00:26:15,600
Let me, you know, what it is to you and the role that, you know, where would you, you know,

349
00:26:15,600 --> 00:26:17,600
what kind of situations would you turn to it?

350
00:26:17,600 --> 00:26:21,520
I think the biggest sweet spot for WECA in my opinion is people who aren't real comfortable

351
00:26:21,520 --> 00:26:23,480
in programming.

352
00:26:23,480 --> 00:26:27,680
So it has a really good point and click GUI.

353
00:26:27,680 --> 00:26:29,280
And so it does have APIs.

354
00:26:29,280 --> 00:26:30,280
It certainly does.

355
00:26:30,280 --> 00:26:34,000
They're Java APIs and it has a lot of algorithms implemented.

356
00:26:34,000 --> 00:26:40,680
So it's going to be much less than or implemented in R, but for being a machine learning framework,

357
00:26:40,680 --> 00:26:48,240
it's got a pretty good selection of APIs, includes feature selection, it includes clustering,

358
00:26:48,240 --> 00:26:51,000
it includes a bit of neural nets.

359
00:26:51,000 --> 00:26:57,800
They've got a sort of repository for prototype code, which includes a lot of bioinformatics

360
00:26:57,800 --> 00:26:59,400
work.

361
00:26:59,400 --> 00:27:04,040
But I think in my perspective, I think it's the, all the tools I've seen, I think it's

362
00:27:04,040 --> 00:27:07,400
my favorite for someone who doesn't do programming.

363
00:27:07,400 --> 00:27:09,480
If you do do programming, you can also use it.

364
00:27:09,480 --> 00:27:12,280
So there's a nice sort of stepping out, right?

365
00:27:12,280 --> 00:27:17,400
So you can go from just using it in GUI to, it's also got a command line where you can

366
00:27:17,400 --> 00:27:25,360
call, you know, like, you know, call it C45 classifier, for example, just from the command

367
00:27:25,360 --> 00:27:27,880
line with the CSV file.

368
00:27:27,880 --> 00:27:32,160
So it's got a nice sort of transition path to programming, and of course, it's also got

369
00:27:32,160 --> 00:27:34,120
Java APIs as well.

370
00:27:34,120 --> 00:27:39,120
So I feel like the transition path from, I don't program to like a program a little bit,

371
00:27:39,120 --> 00:27:42,320
I think in WECK is really strong.

372
00:27:42,320 --> 00:27:46,200
I think like, if you want to compare it to some, like, it's a little bit similar to orange.

373
00:27:46,200 --> 00:27:50,840
So I, what I liked about orange is that it has a canvas format, so you can have sort

374
00:27:50,840 --> 00:27:54,760
of a drag and drop GUI, where you can assemble a pipeline.

375
00:27:54,760 --> 00:28:01,480
The orange is primarily orange is Python based, and WECK is Java based.

376
00:28:01,480 --> 00:28:06,640
With orange, you can assemble sort of a pipeline of what operations you want to do with purely

377
00:28:06,640 --> 00:28:08,320
click and drop.

378
00:28:08,320 --> 00:28:11,120
You can also do this with RapidMiner.

379
00:28:11,120 --> 00:28:12,120
Okay.

380
00:28:12,120 --> 00:28:17,520
And all right, so that's WECK, and then you mentioned Addabust.

381
00:28:17,520 --> 00:28:18,520
Yeah.

382
00:28:18,520 --> 00:28:19,520
Yeah.

383
00:28:19,520 --> 00:28:22,000
Actually, boosted decision trees in general.

384
00:28:22,000 --> 00:28:26,720
Yes, talk through kind of those in the role that they play and soften problems like

385
00:28:26,720 --> 00:28:27,720
that.

386
00:28:27,720 --> 00:28:28,720
Yeah.

387
00:28:28,720 --> 00:28:36,120
So I think that when you're getting introduced to machine learning, I think a very sort

388
00:28:36,120 --> 00:28:42,400
of helpful progression is to start with an ID3, which is a really basic decision tree,

389
00:28:42,400 --> 00:28:47,640
and then kind of move to like a C45, which is a little bit more of a robust decision tree,

390
00:28:47,640 --> 00:28:51,960
and then to understand a bit more about ensembles.

391
00:28:51,960 --> 00:28:57,840
And so ensembles became a pretty hot topic in around the early 2000s, something like

392
00:28:57,840 --> 00:28:58,840
that.

393
00:28:58,840 --> 00:29:02,200
Well, maybe we should take a step back and talk about decision trees for folks that

394
00:29:02,200 --> 00:29:05,560
don't might not know about even a decision tree.

395
00:29:05,560 --> 00:29:06,560
Sure.

396
00:29:06,560 --> 00:29:07,560
Sure.

397
00:29:07,560 --> 00:29:13,160
So the general intuition with the decision tree is that we have a metric like information

398
00:29:13,160 --> 00:29:20,320
gain, so that might be when we, so so when we say decision trees, we actually don't

399
00:29:20,320 --> 00:29:21,320
mean decision trees.

400
00:29:21,320 --> 00:29:24,640
They're actually decision tree induction algorithms.

401
00:29:24,640 --> 00:29:30,720
So they take the data and from the data, they induce a decision tree.

402
00:29:30,720 --> 00:29:34,080
And that's what's a little bit confusing, because when you talk to maybe folks in the

403
00:29:34,080 --> 00:29:38,760
business side, you'll say a decision tree, and it's all about like action, and then

404
00:29:38,760 --> 00:29:43,920
there's an arrow for decisions, and then which way should we go?

405
00:29:43,920 --> 00:29:47,800
That is the end product of these decision tree algorithms.

406
00:29:47,800 --> 00:29:53,760
So the ability to induce those decision trees is really the key observation.

407
00:29:53,760 --> 00:29:58,040
So if you're really in the ML space and you say, oh, you know decision trees, right?

408
00:29:58,040 --> 00:30:02,320
It may mean a completely different thing than you think it means, right?

409
00:30:02,320 --> 00:30:06,400
So so there's special ways these trees are constructed.

410
00:30:06,400 --> 00:30:13,160
I mean, I think a good, a good conceptual understanding is take a metric like information

411
00:30:13,160 --> 00:30:21,440
gain, where we play around with drawing out a bunch of leaves in the tree.

412
00:30:21,440 --> 00:30:29,920
And then at the end of each leaf, we calculate how many, you know, so it's maybe like if

413
00:30:29,920 --> 00:30:32,000
feature a is above three, right?

414
00:30:32,000 --> 00:30:33,000
And that's one leaf.

415
00:30:33,000 --> 00:30:39,200
And if feature b is below two, right, and if feature three is true.

416
00:30:39,200 --> 00:30:45,920
So we build out these levels of of trees, and we assign a, you know, information gain

417
00:30:45,920 --> 00:30:50,560
the idea is if the likelihood is very high.

418
00:30:50,560 --> 00:30:59,200
So how many, how many instances do we label correctly versus inc, how many instances

419
00:30:59,200 --> 00:31:03,240
do we label correctly because of this branch?

420
00:31:03,240 --> 00:31:09,400
And when you have a high metric of usefulness with these, particularly with an individual

421
00:31:09,400 --> 00:31:13,080
branch, then it tends to sort of move up the tree.

422
00:31:13,080 --> 00:31:20,960
So to make this more concrete in the example we were discussing earlier, you may have,

423
00:31:20,960 --> 00:31:28,320
for example, you know, feature a is is IP on such, such and such blacklist feature b

424
00:31:28,320 --> 00:31:33,240
might be, you know, is, you know, hash on x file incorrect.

425
00:31:33,240 --> 00:31:38,240
And so you have some, some grouping of, of these features, and you're basically trying

426
00:31:38,240 --> 00:31:47,040
to create a tree that uses these features to determine whether a, whether the likelihood

427
00:31:47,040 --> 00:31:50,760
of a particular pattern is likely malicious, right?

428
00:31:50,760 --> 00:31:58,400
And then the information gain metric is, you know, as your, your decision tree induction

429
00:31:58,400 --> 00:32:06,320
algorithm is basically trying a bunch of permutations of these different features, and information

430
00:32:06,320 --> 00:32:11,600
gain is essentially asking the question, you know, is this tree adding anything that,

431
00:32:11,600 --> 00:32:15,240
you know, all the other ones that I've looked at, you know, didn't tell me.

432
00:32:15,240 --> 00:32:22,640
Yeah, yeah, so, so you draw out the tree, and then what's important, right, so so much

433
00:32:22,640 --> 00:32:28,080
of data science is then about how do we control overfitting, right?

434
00:32:28,080 --> 00:32:34,920
And I feel like overfitting needs, I think it would be helpful in the field if people

435
00:32:34,920 --> 00:32:40,280
would spend some thought on more precise subgroups of overfitting, because I think there

436
00:32:40,280 --> 00:32:48,400
are subcategories of how things can be overfit, and it's not quite as, not always necessarily

437
00:32:48,400 --> 00:32:51,400
such a binary observation.

438
00:32:51,400 --> 00:32:59,480
So tree pruning on the decision tree is usually one of the parameters that one would tune,

439
00:32:59,480 --> 00:33:04,840
and when you use, when you use tree pruning, the idea is you're, you're trying to appreciate

440
00:33:04,840 --> 00:33:09,360
this trade off between model fit versus model complexity, right?

441
00:33:09,360 --> 00:33:15,880
Since we have a data set that we're using to teach our model, then we could, if we could

442
00:33:15,880 --> 00:33:21,840
build out our tree infinitely, well, we could memorize the data effectively, and that's

443
00:33:21,840 --> 00:33:22,960
not what you want to do.

444
00:33:22,960 --> 00:33:29,240
It's important to generalize, so you don't overfit, and so one of the, one of the important

445
00:33:29,240 --> 00:33:34,920
measurements to tweak with the decision tree is pruning, and so essentially what you're

446
00:33:34,920 --> 00:33:41,800
doing is adding sort of a regularization or a penalty to when you grow out the tree

447
00:33:41,800 --> 00:33:42,800
too much.

448
00:33:42,800 --> 00:33:47,280
So you're trying to find the sweet spot between having a tree that can correctly classify

449
00:33:47,280 --> 00:33:53,280
everything according to the data that it saw, versus having a really complex tree that

450
00:33:53,280 --> 00:33:57,240
might have just memorized all the data.

451
00:33:57,240 --> 00:34:02,040
And so that's, that's a bit of an overview and decision trees, and then when we start

452
00:34:02,040 --> 00:34:05,840
talking about data boosting, we're talking about ensembles.

453
00:34:05,840 --> 00:34:11,320
And I think in the space of decision trees, Leo Bryman's work is really the seminal work

454
00:34:11,320 --> 00:34:12,320
here.

455
00:34:12,320 --> 00:34:15,360
He's, he's the creator of random forests.

456
00:34:15,360 --> 00:34:21,600
And so the, I like to try and explain ensembles in a more general way.

457
00:34:21,600 --> 00:34:25,840
There's, there's variations, so bagging and boosting and stacking are specific types

458
00:34:25,840 --> 00:34:27,680
of ensembles.

459
00:34:27,680 --> 00:34:32,200
But the general idea with ensembles is you have some sort of classifier.

460
00:34:32,200 --> 00:34:35,880
For example, it could be a decision tree, could be an SVM.

461
00:34:35,880 --> 00:34:41,960
Some of the recent work is actually looking at ensembles of deep learning models.

462
00:34:41,960 --> 00:34:50,560
And so, so the idea with an ensemble is that you construct multiple different trees.

463
00:34:50,560 --> 00:34:57,480
So in our decision tree case, you would construct a number of different estimators.

464
00:34:57,480 --> 00:35:05,360
And effectively, they, and you're able to combine the knowledge from the different estimators.

465
00:35:05,360 --> 00:35:08,880
For example, one strategy is to use sort of a voting, right?

466
00:35:08,880 --> 00:35:17,320
And so every different estimator gets a vote toward the ultimate prediction.

467
00:35:17,320 --> 00:35:19,200
And so we talk about ensembles of these.

468
00:35:19,200 --> 00:35:23,680
And so you might use, you know, dozens or hundreds or maybe even thousands, but usually

469
00:35:23,680 --> 00:35:29,800
like low hundreds is, you know, or a few dozen is kind of a good starting place for how

470
00:35:29,800 --> 00:35:32,200
many ensembles to use.

471
00:35:32,200 --> 00:35:36,520
And it's almost always the case that when you combine these ensembles, you get better

472
00:35:36,520 --> 00:35:40,280
performance than you would with the decision tree alone.

473
00:35:40,280 --> 00:35:43,720
And why is that?

474
00:35:43,720 --> 00:35:48,360
It's because some of them, you can imagine sort of variations of some trees might overfit

475
00:35:48,360 --> 00:35:49,680
just a little bit.

476
00:35:49,680 --> 00:35:52,080
Some might underfit just a little bit.

477
00:35:52,080 --> 00:35:57,760
And when you vote, I think it exploits more of a law of large numbers.

478
00:35:57,760 --> 00:36:00,720
And so you get more of an aggregate function, right?

479
00:36:00,720 --> 00:36:06,000
You see in society, you see these interesting things like in crowd voting, where maybe

480
00:36:06,000 --> 00:36:08,880
the intelligence, like wisdom of the crowds, right?

481
00:36:08,880 --> 00:36:16,800
The intelligence of the group is more valuable than the intelligence of any one individual.

482
00:36:16,800 --> 00:36:21,240
So I think that's some of the intuition of why ensembles are better.

483
00:36:21,240 --> 00:36:30,880
And how are the different trees in the ensemble different, are they different sets of features,

484
00:36:30,880 --> 00:36:36,720
are they different hyperparameters, are they, you know, different, I know, when I've

485
00:36:36,720 --> 00:36:42,640
seen ensembles, I've seen them in the case of different models, like you might have

486
00:36:42,640 --> 00:36:47,880
a linear regression, you know, that's good at figuring out one piece of your problem

487
00:36:47,880 --> 00:36:52,800
and, you know, some other type of model that's figuring out another subset of your problem

488
00:36:52,800 --> 00:36:58,800
and then you kind of ensemble them together or at least create a hierarchical model.

489
00:36:58,800 --> 00:37:03,480
When you're doing ensembles of decision trees, like what differentiates tree A from tree

490
00:37:03,480 --> 00:37:04,480
B?

491
00:37:04,480 --> 00:37:05,480
Sure.

492
00:37:05,480 --> 00:37:08,640
And it, of course, depends on the parameters that are set, right?

493
00:37:08,640 --> 00:37:14,120
So it could be that you send different, different data to certain trees.

494
00:37:14,120 --> 00:37:21,160
It could be that you use different initialization parameters, right?

495
00:37:21,160 --> 00:37:25,040
And so, you know, especially if you're creating your own algorithms, there's really no limit,

496
00:37:25,040 --> 00:37:30,640
you know, and some of them you could use information gain as the metric of what is good and

497
00:37:30,640 --> 00:37:35,440
others, you could use, you know, like genie entropy, for example.

498
00:37:35,440 --> 00:37:36,440
What's that?

499
00:37:36,440 --> 00:37:41,240
I don't think I really want to try and define genie entropy.

500
00:37:41,240 --> 00:37:44,800
Like, I've looked at a bunch of definitions, and I'm not surprised I can like succinctly

501
00:37:44,800 --> 00:37:52,640
and clearly describe it with, yeah, it's another metric like information gain that can be

502
00:37:52,640 --> 00:37:58,680
used to rate how good a model is, right, or contributions to a model R.

503
00:37:58,680 --> 00:37:59,680
Yeah.

504
00:37:59,680 --> 00:38:00,680
Okay.

505
00:38:00,680 --> 00:38:02,080
So, add a boost.

506
00:38:02,080 --> 00:38:07,720
You've got a bunch of trees that you've created from sending them different subsets of

507
00:38:07,720 --> 00:38:18,760
your data, different initialization parameters, what does the add a boost algorithm do, and

508
00:38:18,760 --> 00:38:23,240
how's that different from, well, you were talking about actually boosting and bagging and

509
00:38:23,240 --> 00:38:24,240
all that.

510
00:38:24,240 --> 00:38:25,240
Yeah.

511
00:38:25,240 --> 00:38:26,240
Kind of walk us through all that.

512
00:38:26,240 --> 00:38:27,240
Yeah.

513
00:38:27,240 --> 00:38:32,280
So, bagging, boosting, stacking, I think very, very practically.

514
00:38:32,280 --> 00:38:36,560
These are different ways to squeak a little bit of better performance out of your models.

515
00:38:36,560 --> 00:38:42,040
So you have these initial models that you believe are pretty good, and I think it's one

516
00:38:42,040 --> 00:38:44,840
of the single most straightforward ways.

517
00:38:44,840 --> 00:38:48,600
How do I improve my accuracy without changing my data?

518
00:38:48,600 --> 00:38:50,680
Do ensembles, right?

519
00:38:50,680 --> 00:38:58,760
So try different parameters, try boosting, try bagging, try stacking, and you're nearly

520
00:38:58,760 --> 00:39:05,480
guaranteed to improve your performance, whether you're measuring error rates or accuracy

521
00:39:05,480 --> 00:39:09,400
or MCC or anything, AUC.

522
00:39:09,400 --> 00:39:18,800
Yeah, I know, I don't remember the exact stats, but some very large portion of recent

523
00:39:18,800 --> 00:39:27,240
Kaggle contest winners are all gradient boosted decision trees of one sort, you know, ensembles

524
00:39:27,240 --> 00:39:28,680
of one sort or another.

525
00:39:28,680 --> 00:39:29,680
Yeah.

526
00:39:29,680 --> 00:39:35,200
So, XG Boost is really, I think actually the Kaggle is one of the greatest practical sources

527
00:39:35,200 --> 00:39:38,360
to go to for learning about this stuff.

528
00:39:38,360 --> 00:39:43,560
I mean, we all need multiple tools, ensembles, if you will, ensembles of information that

529
00:39:43,560 --> 00:39:45,160
we can pull in, right?

530
00:39:45,160 --> 00:39:50,920
Podcasts like yours, and I think the Kaggle forums are a really valuable space.

531
00:39:50,920 --> 00:39:52,640
That's, you know, a few years back.

532
00:39:52,640 --> 00:39:58,360
That's where, you know, I got first exposed to XG Boost and how sort of it was able to

533
00:39:58,360 --> 00:40:05,800
add a gradient boosted component to our existing strategy of an ensemble decision tree.

534
00:40:05,800 --> 00:40:12,760
And in many cases, XG Boost performs better than random forest.

535
00:40:12,760 --> 00:40:14,880
But it involves a bit more tuning, right?

536
00:40:14,880 --> 00:40:19,160
So it involves basically all the tuning you would have in random forests, you know, maybe

537
00:40:19,160 --> 00:40:24,920
you've got something like a half a dozen to a dozen parameters in random forest.

538
00:40:24,920 --> 00:40:31,640
But when you're tuning XG Boost, you know, you're really looking at one to maybe three dozen

539
00:40:31,640 --> 00:40:33,800
different parameters to tune.

540
00:40:33,800 --> 00:40:38,120
And that can really take some time if you're doing sort of a grid search to try out a bunch

541
00:40:38,120 --> 00:40:42,520
of different levels, you know, it's sort of a combinatorial problem, right?

542
00:40:42,520 --> 00:40:46,520
It's if you have three different settings for Feature One, three for two, three for three,

543
00:40:46,520 --> 00:40:49,680
then it's all multiplicative of how long it will take.

544
00:40:49,680 --> 00:40:54,640
And at the same time, you often want to do it with pretty large data sets because usually

545
00:40:54,640 --> 00:40:59,640
you'll get better performance, if you're doing something like cross validation, cross

546
00:40:59,640 --> 00:41:04,000
whole validation, you'll do, you'll probably get better performance out of it.

547
00:41:04,000 --> 00:41:08,040
So that can really take some time to compute.

548
00:41:08,040 --> 00:41:16,200
So we were kind of walking through applying this stuff to the cyber security use case.

549
00:41:16,200 --> 00:41:22,360
And in that project you were describing, you ended up using decision trees to kind of

550
00:41:22,360 --> 00:41:28,200
get to basically to e-couch your increase performance.

551
00:41:28,200 --> 00:41:37,000
Yeah, how do you even think about performance in this context like what, you know, what,

552
00:41:37,000 --> 00:41:41,560
I don't know if you can quote specific like, you know, error rates or something like that

553
00:41:41,560 --> 00:41:46,560
or what are, what are the key metrics, you know, business metrics, I guess.

554
00:41:46,560 --> 00:41:51,040
And then how do those tie to, you know, like metrics that you would be thinking about

555
00:41:51,040 --> 00:41:52,560
as a data scientist?

556
00:41:52,560 --> 00:42:01,440
Well, actually I can come back to describing that for how we score indicators and just

557
00:42:01,440 --> 00:42:05,400
pass along the confident indicators to our users.

558
00:42:05,400 --> 00:42:09,440
But I think there's an interesting story there when talking about, when talking about the

559
00:42:09,440 --> 00:42:16,000
domain generation algorithm use case because we have this situation of, you have a few

560
00:42:16,000 --> 00:42:17,320
different families of malware.

561
00:42:17,320 --> 00:42:23,080
Let's say you have three families of malware and they generate domain names, right?

562
00:42:23,080 --> 00:42:26,960
It's the domains that they'll call out to in the next day or two or whatever, right?

563
00:42:26,960 --> 00:42:32,040
And so intuitively the first thought you would have is, okay, well, I'm going to measure,

564
00:42:32,040 --> 00:42:38,040
you know, my performance metric, maybe it's, maybe it's accuracy, maybe it's a UC or

565
00:42:38,040 --> 00:42:39,040
whatever.

566
00:42:39,040 --> 00:42:43,200
I'm going to measure that, I'm going to train on some of those domain names, but then

567
00:42:43,200 --> 00:42:47,720
I'm going to test on other domain names and, you know, cross-fold validation and that's

568
00:42:47,720 --> 00:42:49,560
very straightforward.

569
00:42:49,560 --> 00:42:56,640
That's not the right metric to describe it, no matter what, whether you use AUC or Accuracy

570
00:42:56,640 --> 00:43:04,360
or MCC or whatever metric you're using, that's simply the wrong problem you're measuring.

571
00:43:04,360 --> 00:43:10,920
This is what I was meaning about kind of defining, I think there's value to us as a community

572
00:43:10,920 --> 00:43:18,480
to think about better definitions of overfitting, because if I can predict new domains from

573
00:43:18,480 --> 00:43:23,280
the same malware that I've already looked at, what is the value?

574
00:43:23,280 --> 00:43:31,040
Even if it's 100% confidence or 100% accuracy, that's great, but I'm predicting different

575
00:43:31,040 --> 00:43:33,800
domains from the same malware families.

576
00:43:33,800 --> 00:43:40,080
What you want to be doing is predicting new domains from different malware families.

577
00:43:40,080 --> 00:43:44,880
And so when the research was coming out in the space, I was sort of surprised that all

578
00:43:44,880 --> 00:43:51,680
the metrics that was being done were always on detecting new domains from the same malware

579
00:43:51,680 --> 00:43:57,120
families and people weren't measuring other malware families and the accuracy there.

580
00:43:57,120 --> 00:43:59,200
Does that point make sense?

581
00:43:59,200 --> 00:44:05,920
So it's maybe sort of that you need to look in a larger business case to observe what

582
00:44:05,920 --> 00:44:07,600
you should be measuring.

583
00:44:07,600 --> 00:44:15,120
Yeah, so what you're saying is more like, it's almost like a different approach to cross

584
00:44:15,120 --> 00:44:16,120
validation.

585
00:44:16,120 --> 00:44:21,120
You're cross validating across malware families instead of just cross validating within

586
00:44:21,120 --> 00:44:22,600
one malware family.

587
00:44:22,600 --> 00:44:27,520
Yeah, I think that's a great assessment of it, is to think about the scope of the cross

588
00:44:27,520 --> 00:44:32,560
validation and what you're ultimately trying to solve.

589
00:44:32,560 --> 00:44:39,960
Any other interesting use cases or other projects that you guys are working on in the cybersecurity

590
00:44:39,960 --> 00:44:41,440
domain?

591
00:44:41,440 --> 00:44:47,960
Well, I think one thing that's an interesting thing that we are tracking is that how

592
00:44:47,960 --> 00:44:54,640
well are adversaries sort of evolving to our methods, right?

593
00:44:54,640 --> 00:44:59,120
We could talk about this in the space of adversarial machine learning, right?

594
00:44:59,120 --> 00:45:05,600
The idea I think with adversary machine learning is the, so in the cybersecurity domain,

595
00:45:05,600 --> 00:45:12,600
it's pretty common that good guys will first find strong signals of how to detect the bad

596
00:45:12,600 --> 00:45:13,600
guys.

597
00:45:13,600 --> 00:45:19,840
Eventually, the bad guys will know that the good guys are using that particular strong

598
00:45:19,840 --> 00:45:23,120
signal and they'll try to avoid it.

599
00:45:23,120 --> 00:45:28,240
We see this case, you know, this back and forth game happening.

600
00:45:28,240 --> 00:45:34,520
And so what machine learning allows us to do is potentially pick up on a lot of weak

601
00:45:34,520 --> 00:45:39,440
signals that we didn't quite realize were there.

602
00:45:39,440 --> 00:45:43,560
And then use those weak signals to detect the adversary.

603
00:45:43,560 --> 00:45:45,240
And that's kind of the state we're at now.

604
00:45:45,240 --> 00:45:51,280
There's a lot of cyber security companies trying to employ machine learning on obviously,

605
00:45:51,280 --> 00:45:56,560
you know, weak signals to try and get good predictability to detect the bad guys, especially

606
00:45:56,560 --> 00:46:01,440
invariants, right, invariant mechanisms that the bad guys aren't changing and moving

607
00:46:01,440 --> 00:46:03,080
around.

608
00:46:03,080 --> 00:46:09,560
Now the concern is are the bad guys using machine learning, right, or will the bad guys

609
00:46:09,560 --> 00:46:13,720
use machine learning to try and blend in, right?

610
00:46:13,720 --> 00:46:17,640
The most straightforward way is, you know, maybe they would try to model how we do it, for

611
00:46:17,640 --> 00:46:19,880
example.

612
00:46:19,880 --> 00:46:27,040
If the bad guys are modeling how the good guys do it, then constructing an outlier example

613
00:46:27,040 --> 00:46:28,040
isn't too hard.

614
00:46:28,040 --> 00:46:30,320
I mean, that's just the space of generative models.

615
00:46:30,320 --> 00:46:37,360
So if you can fit a generative model to approximate the algorithm the good guy is using, then

616
00:46:37,360 --> 00:46:44,640
this is, you know, this is a great way to sort of evade what the good guys are using.

617
00:46:44,640 --> 00:46:51,080
And that's one way that the space of generative adversarial networks, I think, is a little

618
00:46:51,080 --> 00:47:01,160
bit exciting because most of the generative adversarial networks, the GANs, are used,

619
00:47:01,160 --> 00:47:05,800
so Ian Goodfellow was really a big pioneer in the space.

620
00:47:05,800 --> 00:47:13,760
And so he had some interesting observations and was able to put together a system where

621
00:47:13,760 --> 00:47:17,240
you have sort of a generator function as a generative model.

622
00:47:17,240 --> 00:47:20,960
And then you have a discriminator function.

623
00:47:20,960 --> 00:47:28,920
The discriminator function is trying to detect when, in the case of maybe classifying images.

624
00:47:28,920 --> 00:47:32,640
So you have a generative function that's trying to generate an image and the discriminative

625
00:47:32,640 --> 00:47:36,520
function is looking at the generative model.

626
00:47:36,520 --> 00:47:41,240
And then, or the images that it produces, right, exactly, yeah, it's looking at the

627
00:47:41,240 --> 00:47:44,640
images that the generative model produces, right?

628
00:47:44,640 --> 00:47:50,400
And then the discriminative function, its goal, is to determine, was this determined

629
00:47:50,400 --> 00:47:53,400
by like a human or a computer, right?

630
00:47:53,400 --> 00:47:58,560
So most of this gets used because there's really impressive results.

631
00:47:58,560 --> 00:48:03,040
You end up seeing things like, you know, you can use this in a few different ways.

632
00:48:03,040 --> 00:48:08,880
You can use it to, like, as a better mechanism to interpolate.

633
00:48:08,880 --> 00:48:15,360
So for example, if you have a gap in an image and you want to fill in the image gap, you

634
00:48:15,360 --> 00:48:19,560
could use interpolation and try and sort of guess what it would be.

635
00:48:19,560 --> 00:48:22,240
The point is you've lost some information.

636
00:48:22,240 --> 00:48:26,240
And so when you try and algorithmically fill it in, it looks like it's algorithmically

637
00:48:26,240 --> 00:48:27,240
filled in.

638
00:48:27,240 --> 00:48:28,240
Right.

639
00:48:28,240 --> 00:48:33,120
Meaning if you're averaging pixels or something like that, it looks it just doesn't look

640
00:48:33,120 --> 00:48:34,120
right.

641
00:48:34,120 --> 00:48:35,120
Exactly.

642
00:48:35,120 --> 00:48:40,760
In case you're training a generator or you have a generator that's generating, you

643
00:48:40,760 --> 00:48:44,960
know, proposed, this is a proposed way that I might fill this image in.

644
00:48:44,960 --> 00:48:48,760
And your discriminator is saying, ah, that doesn't look good, ah, that looks good.

645
00:48:48,760 --> 00:48:55,680
And so using the generative adversary on network approach works tends to produce better

646
00:48:55,680 --> 00:49:02,080
looking in fill than, you know, your algorithmic approach is your, you know, averaging and

647
00:49:02,080 --> 00:49:03,080
stuff like that.

648
00:49:03,080 --> 00:49:06,360
That's a, yeah, that's, that's a good assessment of it.

649
00:49:06,360 --> 00:49:10,520
And so a lot of times how it's used is just to try to produce something that's somewhat

650
00:49:10,520 --> 00:49:12,320
convincing.

651
00:49:12,320 --> 00:49:18,120
And the cybersecurity space, we can, we can imagine how that could be used for a type

652
00:49:18,120 --> 00:49:21,600
of like poisoning and evasion.

653
00:49:21,600 --> 00:49:30,400
So, so if we want to generate maybe network data that looks believable, but also gets

654
00:49:30,400 --> 00:49:36,000
around your model, we can use the generator, generator and the discriminator together

655
00:49:36,000 --> 00:49:43,640
to, um, to optimally have the right sort of balance of these two components.

656
00:49:43,640 --> 00:49:47,800
And so in the long term, this is definitely a concern in the security space, you know,

657
00:49:47,800 --> 00:49:53,200
as far as the maturity of the field, you know, we're really just starting to get production

658
00:49:53,200 --> 00:49:58,200
machine learning to do a lot of the, the detection for network and malware and so forth,

659
00:49:58,200 --> 00:50:03,120
you know, the past two, three, four years have been pretty hot in this space.

660
00:50:03,120 --> 00:50:08,880
And so we're really just getting, maturing our process of machine learning, deployed

661
00:50:08,880 --> 00:50:11,240
in cybersecurity.

662
00:50:11,240 --> 00:50:16,360
And so it's a little bit premature to talk about adversaries evading our model, but we

663
00:50:16,360 --> 00:50:20,520
definitely see the adversaries when I was talking about strong signals and weak signals.

664
00:50:20,520 --> 00:50:25,080
We definitely see the adversaries picking up on the strong signals that the humans are

665
00:50:25,080 --> 00:50:27,800
finding and evolving.

666
00:50:27,800 --> 00:50:33,040
And so it's just the case that the way that the adversaries are evolving is suboptimal

667
00:50:33,040 --> 00:50:37,120
because if they had, you know, if they were using machine learning, they could learn

668
00:50:37,120 --> 00:50:41,000
how, learn more optimal strategies to avoid the good guys.

669
00:50:41,000 --> 00:50:42,520
Interesting.

670
00:50:42,520 --> 00:50:47,720
So the good guys all hire these black headers to try to, you know, act like the bad guys

671
00:50:47,720 --> 00:50:55,160
and, and, you know, break them to the bad guys all hire white headers to try to, um, give

672
00:50:55,160 --> 00:50:59,320
them information about what the, the good guys might know about.

673
00:50:59,320 --> 00:51:05,960
So so far, um, as far as what we see adversaries doing to avoid good guys machine learning detection

674
00:51:05,960 --> 00:51:10,800
for the ones we actually catch, right, because we're limited to just what we can catch, um,

675
00:51:10,800 --> 00:51:16,000
we haven't really seen deep model understanding coming from the bad guys.

676
00:51:16,000 --> 00:51:19,520
It's really still at that sort of subtle signal point.

677
00:51:19,520 --> 00:51:26,600
We do in a less ML type, in a less machine learning type of way, we do see bad guys, you

678
00:51:26,600 --> 00:51:31,680
know, testing their malware code, maybe by running it against a bunch of antivirus, right,

679
00:51:31,680 --> 00:51:36,280
because that's, frankly, that's just QA to the bad guys, right?

680
00:51:36,280 --> 00:51:37,280
Right.

681
00:51:37,280 --> 00:51:38,280
Right.

682
00:51:38,280 --> 00:51:39,280
Right.

683
00:51:39,280 --> 00:51:49,000
Um, so Gans, I've only ever seen that in the context of, um, you know, images and deep

684
00:51:49,000 --> 00:51:56,640
neural nets, um, have you seen Gans in, is it, does Gans make sense, uh, in the context

685
00:51:56,640 --> 00:51:59,120
other than deep neural nets?

686
00:51:59,120 --> 00:52:06,800
Um, so yeah, so yeah, well, yeah, that's what's interesting about it is, um, so you, Ian

687
00:52:06,800 --> 00:52:12,080
Goodfeld pretty much invented this space and, uh, I think it was at, I think was at nips

688
00:52:12,080 --> 00:52:13,080
this year.

689
00:52:13,080 --> 00:52:15,480
He gave a pretty famous tutorial on it.

690
00:52:15,480 --> 00:52:20,560
And in that, he mentions, um, you can plug in any supervised algorithm.

691
00:52:20,560 --> 00:52:29,000
Um, but I, we, before there was Gans, there was work in adversarial machine learning.

692
00:52:29,000 --> 00:52:31,240
There was some folks at Berkeley that worked on it.

693
00:52:31,240 --> 00:52:37,280
There was folks, um, at university of, as it pronounced, Caligari, it's that Island

694
00:52:37,280 --> 00:52:40,960
in, uh, the middle of the Mediterranean.

695
00:52:40,960 --> 00:52:41,960
Yeah.

696
00:52:41,960 --> 00:52:48,600
So I think it's the university of Caligari, um, they created a framework for just testing,

697
00:52:48,600 --> 00:52:54,040
um, you know, uh, simple algorithms like SVMs and, you know, decision trees and these types

698
00:52:54,040 --> 00:52:55,040
of things.

699
00:52:55,040 --> 00:53:00,080
So they created a framework for testing those and creating sort of an adversarial space.

700
00:53:00,080 --> 00:53:05,400
So in, in my mind, those types of testing frameworks are examples of adversarial machine

701
00:53:05,400 --> 00:53:07,040
learning that are not Gans.

702
00:53:07,040 --> 00:53:10,160
Yeah, I wasn't aware of any of that stuff, so that's super interesting.

703
00:53:10,160 --> 00:53:21,360
Um, and so in your context, um, are you applying Gans per se or non neural network, uh, uh, models

704
00:53:21,360 --> 00:53:28,520
to, to this and, uh, are you applying neural nets elsewhere in the stuff you're doing?

705
00:53:28,520 --> 00:53:34,520
So we are watching pretty closely as far as do we need to start thinking of an adversarial

706
00:53:34,520 --> 00:53:35,520
component.

707
00:53:35,520 --> 00:53:40,000
And if we do, we'll certainly, the first indication we get, we're going to reorient

708
00:53:40,000 --> 00:53:47,040
our work, um, because it could be a month out, it could be 10 years out, right?

709
00:53:47,040 --> 00:53:52,840
So, um, at the moment, it's, there's not enough data to convince me that it's really worth

710
00:53:52,840 --> 00:53:57,040
that big investment, but it's definitely worth kind of watching and keeping on the radar.

711
00:53:57,040 --> 00:54:05,800
Um, as far as neural nets, um, they're less, uh, advantageous for us for the most part.

712
00:54:05,800 --> 00:54:12,120
Um, I really tend to value interpretable models and I, and I think model interpretability

713
00:54:12,120 --> 00:54:18,440
and cybersecurity is uniquely important because so much of cybersecurity is about, you have

714
00:54:18,440 --> 00:54:24,240
the right data, um, when you understand how your model works, it gives you insights into

715
00:54:24,240 --> 00:54:29,600
what current features are useful, you get a sense of how your model is working, which

716
00:54:29,600 --> 00:54:32,440
you don't really get with neural networks.

717
00:54:32,440 --> 00:54:37,760
And when you get that insight, then you can, um, if you've got some subject matter and

718
00:54:37,760 --> 00:54:42,080
knowledge about how networks work and how operating system work, systems work, you can

719
00:54:42,080 --> 00:54:46,640
start to have the conversation of, well, we should instrument at this level or collect

720
00:54:46,640 --> 00:54:48,880
this new type of data.

721
00:54:48,880 --> 00:54:54,160
So I think the biggest innovations in cybersecurity space are being able to interpret a model

722
00:54:54,160 --> 00:55:00,720
to go back to the data collection, make suggestions of new data to collect and then, uh, improve

723
00:55:00,720 --> 00:55:06,480
the process and start all over there because, um, you know, we've kind of seen a pretty

724
00:55:06,480 --> 00:55:11,160
long history of, for the most part, having good data generally tends to be better than

725
00:55:11,160 --> 00:55:13,080
having really good algorithms.

726
00:55:13,080 --> 00:55:18,760
Of course, you want both, if you have bad data, good algorithms, don't do that much, right?

727
00:55:18,760 --> 00:55:23,800
And even deep neural nets usually require tremendous amounts of data.

728
00:55:23,800 --> 00:55:29,440
And so one of the nice things about our, um, about our company is that, um, when we're

729
00:55:29,440 --> 00:55:34,720
rating all these different feeds that come in, um, in order to help our end users, you

730
00:55:34,720 --> 00:55:38,640
know, we can, we can learn from all these different sources of labeled malicious data,

731
00:55:38,640 --> 00:55:42,600
which includes, you know, all sorts of different techniques for, you know, how do they detect

732
00:55:42,600 --> 00:55:43,600
malware, right?

733
00:55:43,600 --> 00:55:48,920
How much of it is an automated malware of maybe executing and watching the execution patterns

734
00:55:48,920 --> 00:55:54,600
versus, um, statically analyzing a piece of code and reverse engineering and walking

735
00:55:54,600 --> 00:56:03,560
through it or, you know, um, or, or methods like setting up honeypots or, um, other types

736
00:56:03,560 --> 00:56:07,680
of like network-based measurements may be monitoring the collection outside of a tornoid

737
00:56:07,680 --> 00:56:10,080
and just generally tracking this maliciousness.

738
00:56:10,080 --> 00:56:16,000
There's so many different ways to get insight into what malicious is.

739
00:56:16,000 --> 00:56:18,760
We do actually have a nice amount of data.

740
00:56:18,760 --> 00:56:24,560
So for that reason, I think deep networks, deep neural nets might be on the horizon,

741
00:56:24,560 --> 00:56:32,040
but really understanding what data to use to judge the incoming streams of data is very,

742
00:56:32,040 --> 00:56:33,360
um, is important.

743
00:56:33,360 --> 00:56:35,640
So model interpretability is huge for us.

744
00:56:35,640 --> 00:56:36,640
Right.

745
00:56:36,640 --> 00:56:37,640
Right.

746
00:56:37,640 --> 00:56:43,160
Yeah, that comes up in so many conversations I have with folks about, um, as being, uh,

747
00:56:43,160 --> 00:56:51,360
you know, real challenge for employing neural nets right now, at least, um, although, you

748
00:56:51,360 --> 00:56:55,880
know, as those, as that technique matures, you know, there are some signs that will start

749
00:56:55,880 --> 00:57:02,880
to, you know, that it's not necessarily, you know, as I'm not necessarily, uh, antithetical

750
00:57:02,880 --> 00:57:05,040
to have interpretability with neural nets.

751
00:57:05,040 --> 00:57:07,040
It's just, we're not there yet.

752
00:57:07,040 --> 00:57:08,040
Absolutely.

753
00:57:08,040 --> 00:57:09,040
Absolutely.

754
00:57:09,040 --> 00:57:12,520
So, I mean, I think once, once we can cross that threshold of interpretability, I think

755
00:57:12,520 --> 00:57:16,960
it's, um, going to be much more productive in the security space, at least for the companies

756
00:57:16,960 --> 00:57:17,960
that have enough data.

757
00:57:17,960 --> 00:57:18,960
Yeah.

758
00:57:18,960 --> 00:57:19,960
Yeah.

759
00:57:19,960 --> 00:57:23,160
Do you want to touch on before we wrap it up?

760
00:57:23,160 --> 00:57:31,640
So how I got into machine learning, uh, was that in late 2008, I saw this, like, 60

761
00:57:31,640 --> 00:57:36,280
minutes news story, uh, it was titled Reading Your Mind.

762
00:57:36,280 --> 00:57:41,200
And it was, uh, some work happening at Carnegie Mellon, uh, with Tom Mitchell and Marcel

763
00:57:41,200 --> 00:57:51,240
Just and they were using some of the early folks to, to put people in FMRI images, FMRI

764
00:57:51,240 --> 00:57:59,080
brain imaging systems and then use machine learning algorithms to uniquely identify various

765
00:57:59,080 --> 00:58:00,080
thought patterns.

766
00:58:00,080 --> 00:58:02,440
So the FMRI measures blood flow in your brain.

767
00:58:02,440 --> 00:58:04,040
So it's your data source.

768
00:58:04,040 --> 00:58:09,120
And then the ML algorithms were able to separate, um, particular words and concepts.

769
00:58:09,120 --> 00:58:13,920
So they were able to show them a picture of a house or a hammer or something like that.

770
00:58:13,920 --> 00:58:20,200
And then FMRI brain image, the blood flow in the brain and then using the ML to predict

771
00:58:20,200 --> 00:58:22,760
what the person was thinking about.

772
00:58:22,760 --> 00:58:26,920
And so I saw that, it's a little creepy, but it's really cool.

773
00:58:26,920 --> 00:58:32,360
And I just got to thinking, wow, we should, we should be using this for cybersecurity.

774
00:58:32,360 --> 00:58:38,360
And so, um, that was a little bit earlier than a lot of the hot, um, hot, exciting times

775
00:58:38,360 --> 00:58:41,640
when people got, you know, started getting into, so I was a little, a little early to the

776
00:58:41,640 --> 00:58:47,880
game in some, in, in some extent, um, but I ended up a few years later end up publishing

777
00:58:47,880 --> 00:58:53,400
a paper with Tom Mitchell as well and, uh, and Ellen Ritter, uh, at, who's now at Ohio

778
00:58:53,400 --> 00:58:54,400
State.

779
00:58:54,400 --> 00:58:55,400
Okay.

780
00:58:55,400 --> 00:59:01,600
And what we were doing is we had, um, sort of a special, so we applied expectation regularization

781
00:59:01,600 --> 00:59:06,360
to, uh, to detect, um, detect patterns in Twitter stream.

782
00:59:06,360 --> 00:59:12,800
So there's this space of weak supervision in machine learning and the idea is doing supervised

783
00:59:12,800 --> 00:59:16,240
learning when you have very few examples.

784
00:59:16,240 --> 00:59:22,680
And so it was mostly an NLP problem and we were harvesting from data from the Twitter

785
00:59:22,680 --> 00:59:24,480
verse.

786
00:59:24,480 --> 00:59:31,640
And, um, what we were trying to do was predict security, um, pull out the security events.

787
00:59:31,640 --> 00:59:38,160
So create sort of a security news source out of just the general Twitter stream.

788
00:59:38,160 --> 00:59:43,040
And so with just a few hand labeled events, we were able to pull out, you know, events

789
00:59:43,040 --> 00:59:45,480
on like DDoS and account hijacking.

790
00:59:45,480 --> 00:59:50,920
I think we labeled like a couple dozen events and it had reasonably, um, reasonably high

791
00:59:50,920 --> 00:59:59,400
accuracy given the, given the weak supervision space, right, and very few labeled examples.

792
00:59:59,400 --> 01:00:06,400
So then we were able to sort of create these news streams of, um, these news streams of

793
01:00:06,400 --> 01:00:07,840
security events, right?

794
01:00:07,840 --> 01:00:12,520
So this, like, these are the events of people getting, uh, people's accounts getting hijacked

795
01:00:12,520 --> 01:00:13,680
today.

796
01:00:13,680 --> 01:00:16,440
And so able to track, um, news stories.

797
01:00:16,440 --> 01:00:21,560
And I just thought that might be useful in security socks, right, to know a security operation

798
01:00:21,560 --> 01:00:26,080
centers to know, uh, you know, what are the big events of the internet?

799
01:00:26,080 --> 01:00:31,960
Because in cybersecurity, you go into any fancy high-end, um, cybersecurity operation center

800
01:00:31,960 --> 01:00:34,720
and they've always got like CNN on the view screen.

801
01:00:34,720 --> 01:00:35,720
Why?

802
01:00:35,720 --> 01:00:38,680
Because if 9-11 happens, it's really going to change the network characteristics, right?

803
01:00:38,680 --> 01:00:41,800
So you just need a basic exposure to the news.

804
01:00:41,800 --> 01:00:47,240
And so the thinking with some of this work was, um, if we can pull out the security news

805
01:00:47,240 --> 01:00:51,880
from the Twitterverse, then we can track some of these cyber events that are happening

806
01:00:51,880 --> 01:00:54,640
near the cyber news from Twitter.

807
01:00:54,640 --> 01:00:55,920
Mm-hmm, right.

808
01:00:55,920 --> 01:00:58,920
And the technique that you mentioned using was what?

809
01:00:58,920 --> 01:01:01,160
Expectation, regularization.

810
01:01:01,160 --> 01:01:07,520
So we're, um, we're looking at the, the log likelihood, um, so we, we do a bunch of sort

811
01:01:07,520 --> 01:01:12,800
of n-gram analysis of looking at the tweet and the particular words that are involved.

812
01:01:12,800 --> 01:01:24,040
And, um, we're able to, um, take, take the log likelihood and then have a term for regularizing,

813
01:01:24,040 --> 01:01:28,600
um, the label, so sort of a, uh, a penalty for being wrong, right?

814
01:01:28,600 --> 01:01:34,800
Um, and then also use, um, use L2 regularization as well.

815
01:01:34,800 --> 01:01:40,560
So two types of regularization as a penalty, um, the sort of basic ideas in the same way

816
01:01:40,560 --> 01:01:45,120
we don't want to grow our decision tree really large, we want to prune it back.

817
01:01:45,120 --> 01:01:50,040
So we don't fit too tightly to the data by having a lot of strong regularization.

818
01:01:50,040 --> 01:01:54,400
The intuition is, so we'll try and pick up on, on the key patterns.

819
01:01:54,400 --> 01:01:59,400
And in this case, the key patterns were like nearby words in the tweet, um, we used hash

820
01:01:59,400 --> 01:02:01,360
tags as, as well.

821
01:02:01,360 --> 01:02:07,080
Um, and so from that, we're able to sort of create, uh, security news service of sorts.

822
01:02:07,080 --> 01:02:09,360
Cool, does that still exist?

823
01:02:09,360 --> 01:02:12,240
Um, it existed about a year ago.

824
01:02:12,240 --> 01:02:15,760
I could try and email Allen and let you know if it gets back up.

825
01:02:15,760 --> 01:02:22,240
We, we had the security tweets domain and security tweets.org and, uh, that just pointed

826
01:02:22,240 --> 01:02:27,040
to like one of his test servers at Ohio State, um, right now it's down.

827
01:02:27,040 --> 01:02:29,040
So I could ping him, ask if you could turn it back on.

828
01:02:29,040 --> 01:02:30,840
Oh, it sounds like interesting work.

829
01:02:30,840 --> 01:02:33,800
Maybe we'll be able to include a link to the paper or something like that.

830
01:02:33,800 --> 01:02:34,800
Yeah.

831
01:02:34,800 --> 01:02:40,680
So it was presented at the, uh, the World Wide Web Conference in 2015.

832
01:02:40,680 --> 01:02:41,680
Okay.

833
01:02:41,680 --> 01:02:42,680
Yeah.

834
01:02:42,680 --> 01:02:43,680
Great, great.

835
01:02:43,680 --> 01:02:51,240
Uh, well, Evan, thanks so much for, uh, for meeting with me for taking the time to join

836
01:02:51,240 --> 01:02:52,240
us here on the show.

837
01:02:52,240 --> 01:02:57,520
I think this is that this may be the first time I've had a kind of a diehard Twoma listener

838
01:02:57,520 --> 01:02:58,520
on the show.

839
01:02:58,520 --> 01:03:01,720
So it's, it's super exciting for me from that perspective.

840
01:03:01,720 --> 01:03:07,480
Um, and I'm so glad you're able to join us here at the, uh, at the strata event.

841
01:03:07,480 --> 01:03:12,320
Um, maybe before we go, uh, have you, what have you learned so far at the conference?

842
01:03:12,320 --> 01:03:13,320
Anything interesting?

843
01:03:13,320 --> 01:03:14,320
Yeah.

844
01:03:14,320 --> 01:03:15,320
So, uh, so there was two talks.

845
01:03:15,320 --> 01:03:20,280
This is pretty early in the conference, but, uh, two talks was sitting in, um, uh, some

846
01:03:20,280 --> 01:03:26,760
folks from Microsoft were talking about, um, they were talking about how they use machine

847
01:03:26,760 --> 01:03:28,800
learning and their security operations.

848
01:03:28,800 --> 01:03:33,800
Um, in the Q and A section, we had a little bit of discussion of data imputation because

849
01:03:33,800 --> 01:03:36,760
that's, that's an area that's been on my radar lately.

850
01:03:36,760 --> 01:03:41,320
So in cybersecurity, we often have to make like a bunch of external API calls because

851
01:03:41,320 --> 01:03:43,360
that's part of our feature space.

852
01:03:43,360 --> 01:03:50,200
And so for whatever reason, if those API calls fail, then you're forced to impute that data.

853
01:03:50,200 --> 01:03:54,360
And can that impute data then cause you to make significant errors?

854
01:03:54,360 --> 01:03:59,880
For example, if you were an adversary and you knew how to get me to impute my data, then

855
01:03:59,880 --> 01:04:03,160
I could misclassify because of that.

856
01:04:03,160 --> 01:04:07,960
So, um, so we ended up chatting about that a little bit in the Q and A section.

857
01:04:07,960 --> 01:04:09,360
So valuable so far?

858
01:04:09,360 --> 01:04:10,360
Yeah.

859
01:04:10,360 --> 01:04:11,360
Okay, absolutely.

860
01:04:11,360 --> 01:04:15,520
I love that there's, um, a little bit of an, uh, there's multiple sessions that are

861
01:04:15,520 --> 01:04:22,000
covering the cybersecurity use case now, um, because it used to just be, um, basically

862
01:04:22,000 --> 01:04:27,000
media studies of video, audio, or images, or NLP, right?

863
01:04:27,000 --> 01:04:31,600
And so kind of the, the minority of us that are outside of those two areas, um, I don't

864
01:04:31,600 --> 01:04:35,400
think it quite as much attention in the machine learning space.

865
01:04:35,400 --> 01:04:36,400
Awesome.

866
01:04:36,400 --> 01:04:37,400
Awesome.

867
01:04:37,400 --> 01:04:41,400
Thanks again, and, uh, thanks for being on the show.

868
01:04:41,400 --> 01:04:42,400
Great.

869
01:04:42,400 --> 01:04:43,400
Thanks for having me.

870
01:04:43,400 --> 01:04:44,400
All right.

871
01:04:44,400 --> 01:04:45,400
Bye.

872
01:04:45,400 --> 01:04:47,400
All right, everyone.

873
01:04:47,400 --> 01:04:48,880
That's our show for today.

874
01:04:48,880 --> 01:04:53,320
Once again, thanks so much for listening and for your continued support.

875
01:04:53,320 --> 01:04:59,080
Don't forget to share your favorite quotes for one of our hot, new, tumult stickers.

876
01:04:59,080 --> 01:05:03,960
You can share them via the show notes page, via Twitter, via our Facebook page, or via

877
01:05:03,960 --> 01:05:07,320
a comment on YouTube or SoundCloud.

878
01:05:07,320 --> 01:05:13,520
The notes for this week's show will be up on twomlai.com slash talk slash 16, where you'll

879
01:05:13,520 --> 01:05:18,320
find links to Evan and the various resources mentioned in the show.

880
01:05:18,320 --> 01:05:46,320
Thanks so much for listening and catch you next time.

