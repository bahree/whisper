1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,600
I'm your host Sam Charrington.

4
00:00:31,600 --> 00:00:36,760
In today's episode we're joined by Kamyar Azizad Anisheli, PhD student at the University

5
00:00:36,760 --> 00:00:42,440
of California Irvine and visiting researcher at Caltech where he works with Anima Anankamar

6
00:00:42,440 --> 00:00:46,400
who you might remember from Twimble Talk 142.

7
00:00:46,400 --> 00:00:51,200
We begin with a reinforcement learning primer of sorts, in which we review the core elements

8
00:00:51,200 --> 00:00:56,680
of RL, along with quite a few examples to help newcomers get up to speed.

9
00:00:56,680 --> 00:01:02,560
We then discuss a pair of Kamyar's recent RL-related papers, efficient exploration through Bayesian

10
00:01:02,560 --> 00:01:11,640
deep-cune networks, and sample efficient deep RL with generative adversarial tree search.

11
00:01:11,640 --> 00:01:16,000
In addition to discussing Kamyar's work, we also chat a bit about the general landscape

12
00:01:16,000 --> 00:01:19,120
of reinforcement learning research today.

13
00:01:19,120 --> 00:01:24,040
So whether you're new to the field or want to dive into cutting-edge reinforcement learning

14
00:01:24,040 --> 00:01:27,360
research with us, this podcast is here for you.

15
00:01:27,360 --> 00:01:31,680
If you'd like to jump ahead to the research discussion, the primer portion of this show

16
00:01:31,680 --> 00:01:34,080
is about 30 minutes long.

17
00:01:34,080 --> 00:01:39,120
We'll have a more specific timestamp noted in the show's description, so check your podcast

18
00:01:39,120 --> 00:01:45,680
app or the show notes at twimlai.com slash talk slash 177 for that.

19
00:01:45,680 --> 00:01:47,680
A quick note before we get into the show.

20
00:01:47,680 --> 00:01:52,000
One day is Labor Day here in the States, and we won't be publishing a show then.

21
00:01:52,000 --> 00:01:56,720
But this is an extra long episode, which will tide you over until we do publish the next

22
00:01:56,720 --> 00:01:59,760
show, which will be Thursday, September 6th.

23
00:01:59,760 --> 00:02:03,000
Alright, onto the show.

24
00:02:03,000 --> 00:02:10,200
Alright everyone, I am on the line with Kamyar Aziza Denisheli.

25
00:02:10,200 --> 00:02:16,280
Kamyar is a PhD student at the University of California Irvine, as well as a visiting

26
00:02:16,280 --> 00:02:22,080
student researcher at Caltech, where he works with Anima Anankumar, who was a guest of

27
00:02:22,080 --> 00:02:24,360
ours back in May of this year.

28
00:02:24,360 --> 00:02:27,360
Kamyar, welcome to this week in machine learning and AI.

29
00:02:27,360 --> 00:02:28,680
Yeah, thank you, Sam.

30
00:02:28,680 --> 00:02:31,360
Thank you for the introduction.

31
00:02:31,360 --> 00:02:36,000
Why don't you give us a sense of your research interests and some of the work you're doing

32
00:02:36,000 --> 00:02:39,320
at Caltech and Irvine?

33
00:02:39,320 --> 00:02:48,160
Right, my research interests is mainly in the area of a specific area of machine learning,

34
00:02:48,160 --> 00:02:51,680
which is called reinforcement learning.

35
00:02:51,680 --> 00:02:58,760
This is my main focus on my main research interests, but at the same time, I do a lot of

36
00:02:58,760 --> 00:03:03,280
other stuff in the field like TensorFlow methods.

37
00:03:03,280 --> 00:03:10,600
I do optimization, I do generative models, study of generative models.

38
00:03:10,600 --> 00:03:20,480
I do a study of safety and furnace in machine learning, because we build up many theory

39
00:03:20,480 --> 00:03:28,880
in machine learning and we build up many practical, like, evolutionary methods, revolutionary

40
00:03:28,880 --> 00:03:29,880
methods.

41
00:03:29,880 --> 00:03:35,920
The question is, can we use them in real world and the question and answer is, hey, you

42
00:03:35,920 --> 00:03:41,440
need to make sure that your algorithm is robots, your algorithm is safe, it's fair.

43
00:03:41,440 --> 00:03:46,720
There are a lot of questions you can ask when you take your machine learning algorithm

44
00:03:46,720 --> 00:03:48,920
and deployed in real world.

45
00:03:48,920 --> 00:03:54,280
Those are not a part of machine learning field that I'm interested in.

46
00:03:54,280 --> 00:04:00,320
We've talked about reinforcement learning on this podcast a number of times, both from

47
00:04:00,320 --> 00:04:12,080
a theoretical perspective, as well as applications like game playing, AlphaGo, things like that.

48
00:04:12,080 --> 00:04:19,600
For this conversation, I thought we would dig into a couple of recent papers of yours

49
00:04:19,600 --> 00:04:24,840
looking at deep reinforcement learning, but also spend some time up front to refresh

50
00:04:24,840 --> 00:04:30,920
ourselves on some of the foundational research in this area.

51
00:04:30,920 --> 00:04:39,160
Why don't we get started by having you walk us through some of the core elements of

52
00:04:39,160 --> 00:04:43,160
reinforcement learning like deep Q networks, for example.

53
00:04:43,160 --> 00:04:45,440
Great, that's an amazing thing.

54
00:04:45,440 --> 00:04:50,680
I really like to, whenever I want to talk about it and explain reinforcement learning,

55
00:04:50,680 --> 00:04:58,040
I get super excited because it's an amazing framework, an amazing setting that almost

56
00:04:58,040 --> 00:05:07,200
it has a lot of intuition from human behavior and also it has super nice theoretical analysis.

57
00:05:07,200 --> 00:05:14,440
The modeling is crazy, great, and I really love it and I really like to explain to others.

58
00:05:14,440 --> 00:05:20,400
It's like, let's assume that I can give you one example, let's assume that you have

59
00:05:20,400 --> 00:05:27,640
a vacuum cleaner in your house or in your apartment or your place and you just leave it

60
00:05:27,640 --> 00:05:34,680
alone and this vacuum cleaner should know how to clean your place.

61
00:05:34,680 --> 00:05:41,400
So what it does is like, vacuum cleaner goes around if it finds something and if it's

62
00:05:41,400 --> 00:05:45,800
like cleaning that place, when you come back home, you tell the vacuum cleaner, hey,

63
00:05:45,800 --> 00:05:46,800
you did a good job.

64
00:05:46,800 --> 00:05:50,760
And if you, the other day you should come back home and like, you see the vacuum cleaner,

65
00:05:50,760 --> 00:05:56,360
you didn't do anything, you tell the vacuum cleaner, hey, you did not do a good job.

66
00:05:56,360 --> 00:06:00,880
It's like, you punish it, I'm not like, you do not punish it, but it's like, this is

67
00:06:00,880 --> 00:06:01,880
the term we use.

68
00:06:01,880 --> 00:06:09,800
So you give some sort of reinforced feedback to the robot, the agent is like going around

69
00:06:09,800 --> 00:06:15,920
the environment or let's say, it even makes it easier in which might make more sense

70
00:06:15,920 --> 00:06:24,120
is like, a baby, a newborn baby, if the newborn baby is getting closer to like, to fire,

71
00:06:24,120 --> 00:06:30,880
the baby feels like some harm and also parents like tell the baby that, hey, do not do it

72
00:06:30,880 --> 00:06:36,080
or like, if the baby does something good, parents, they give the baby like a reward which

73
00:06:36,080 --> 00:06:40,480
is like, let's say, candy, hopefully not candy, but something like, rewarding to the baby

74
00:06:40,480 --> 00:06:47,160
and baby learns that the thing that he or she did was good things over time, the baby

75
00:06:47,160 --> 00:06:51,440
interacts with the environment, which is like the whole world and parents.

76
00:06:51,440 --> 00:06:56,960
And based on the decision the baby makes, like going toward the fire or like getting

77
00:06:56,960 --> 00:07:02,680
higher grade in the, in school, or I don't know, learning how to like, when, let's say

78
00:07:02,680 --> 00:07:08,720
my father of a baby and I'm teaching my baby like, how to ride a bike, if my baby does

79
00:07:08,720 --> 00:07:14,480
a good job, I give a lot of rewards for my baby is like, my kid is like, has this incentive

80
00:07:14,480 --> 00:07:16,480
to learn this task fast.

81
00:07:16,480 --> 00:07:22,520
So it's like, the baby here or the kid here is like the oral agent, it's interacting

82
00:07:22,520 --> 00:07:28,240
with the environment and based on this signal, this feedback system learns how to do

83
00:07:28,240 --> 00:07:32,720
a best job and find the optimal behavior.

84
00:07:32,720 --> 00:07:38,000
So it's a general framework why I really like reinforcement learning because it's like

85
00:07:38,000 --> 00:07:42,960
almost all the time we have reinforcement in problems because you're learning, human

86
00:07:42,960 --> 00:07:47,760
learns, human runs like reinforcement learning somehow is not a, it's not a exact statement

87
00:07:47,760 --> 00:07:50,360
but somehow you can think of it.

88
00:07:50,360 --> 00:07:55,520
So it's reinforcement is actually, or we, in short, we call it RL, it's actually really

89
00:07:55,520 --> 00:08:02,440
interesting framework that has a lot of root in psychology, a lot of root in neuroscience

90
00:08:02,440 --> 00:08:07,240
and also theoretically we found it from graph theory, which was quite interesting.

91
00:08:07,240 --> 00:08:13,000
So from theory, we analyzed many things and some point we realized that oops, is actually

92
00:08:13,000 --> 00:08:18,680
the way human behaves, there was like the nice intersection between like psychology

93
00:08:18,680 --> 00:08:22,760
and neuroscience and like machine learning, which was quite interesting.

94
00:08:22,760 --> 00:08:29,520
So now, this is a motivation why reinforcement is important and why we work on reinforcement

95
00:08:29,520 --> 00:08:35,120
learning because if you can understand the way human learns and the way mathematically

96
00:08:35,120 --> 00:08:41,280
we can make systems to learn, then we can design a robot which can clean a house or

97
00:08:41,280 --> 00:08:48,800
which can like, I can find, I can build a robot arm which helps me to like build something

98
00:08:48,800 --> 00:08:55,480
or like get grass or something, or if I do not have legs, it can help me to walk or

99
00:08:55,480 --> 00:09:02,520
I can come up with the robot, this can help my doctor and my robot is given a patient

100
00:09:02,520 --> 00:09:08,160
comes to my like a clinic, I ask doctor, hey, what is the best prescription you can give

101
00:09:08,160 --> 00:09:14,120
to this patient and also I ask the robot, hey, what do you think?

102
00:09:14,120 --> 00:09:20,400
And the robot has seen many other like trials over the world, so the doctor cannot see

103
00:09:20,400 --> 00:09:26,440
all the possible experiences that other doctors they have seen, but if I can have a robot

104
00:09:26,440 --> 00:09:31,640
which interacts with all the doctors around the world and I give that robot to my doctor

105
00:09:31,640 --> 00:09:37,640
and my doctor can see what my robot thinks and what my doctor is on his own or her own

106
00:09:37,640 --> 00:09:42,640
is thinking about that new patient and then combine this information and give the better

107
00:09:42,640 --> 00:09:46,080
prescription to the patient and patient gets better over time.

108
00:09:46,080 --> 00:09:51,400
So there are a lot of interesting like application and like necessary and interesting application

109
00:09:51,400 --> 00:09:57,680
of reinforcement learning in like, in real world, but the way the model is, there are

110
00:09:57,680 --> 00:10:04,880
many ways to model and the setting, I just want to talk about one of them because mostly

111
00:10:04,880 --> 00:10:11,160
using deep reinforcement learning, let's assume I am playing a game, okay, when I play

112
00:10:11,160 --> 00:10:17,000
a game, let's say, I don't know, you're familiar with, let's say GTA, okay, and then I've

113
00:10:17,000 --> 00:10:18,000
taught her.

114
00:10:18,000 --> 00:10:23,040
Yeah, granted, I thought her or yeah, this is one example that I can imagine many people

115
00:10:23,040 --> 00:10:28,520
have seen it because either they were like punishing their kids for playing this game

116
00:10:28,520 --> 00:10:36,680
like for 24 hours a day or the kids they played themselves is like, you have this agent

117
00:10:36,680 --> 00:10:43,000
going around the world, its own world and like, need to make a decision to optimize its

118
00:10:43,000 --> 00:10:45,280
final goal, okay.

119
00:10:45,280 --> 00:10:51,600
So the decision the agent makes at each point is like walking to left or right or like going

120
00:10:51,600 --> 00:10:57,240
to the barber shop, this or going to gym, this kind of stuff, these decisions at the time

121
00:10:57,240 --> 00:11:02,400
gives it like some reward, each of these decisions has like long term effect.

122
00:11:02,400 --> 00:11:12,400
If my guy in Grand Theft Auto goes to like gym, it gains more power, can run faster, okay.

123
00:11:12,400 --> 00:11:17,680
So this running faster doesn't have meaning at the time, but the thing is this running

124
00:11:17,680 --> 00:11:23,960
faster feature is useful for the agent for later in like few days later when the agent

125
00:11:23,960 --> 00:11:26,920
is going to run away from the cops, something like that.

126
00:11:26,920 --> 00:11:31,960
So the action you're making, the decision you're making now has long term effect.

127
00:11:31,960 --> 00:11:38,280
That effect of this action is like appeared like later, let's say if I am like sitting

128
00:11:38,280 --> 00:11:45,680
in my office, I'm going to grab a coffee, if I grab my wallet now, it doesn't have any

129
00:11:45,680 --> 00:11:49,640
meaning until I get to the coffee shop and I want to pay, okay.

130
00:11:49,640 --> 00:11:53,720
The action I made at the very beginning has a long term effect or if like I go, I want

131
00:11:53,720 --> 00:12:00,320
to invest in a bank, the bank tells me like, hey, if you put money now, you're going

132
00:12:00,320 --> 00:12:02,400
to get returned like over a year.

133
00:12:02,400 --> 00:12:05,680
So the action I'm going to make has like a reward and this reward is like accumulating

134
00:12:05,680 --> 00:12:08,680
over the whole future.

135
00:12:08,680 --> 00:12:15,560
So it's like this reinforcement has this temporal effect, like if I make some decision now,

136
00:12:15,560 --> 00:12:20,240
I see the effect of this decision like way later in the future, which is like makes the

137
00:12:20,240 --> 00:12:24,520
problem hard and also makes the problem interesting because the real world works like this.

138
00:12:24,520 --> 00:12:30,480
If I break my arm now, I might have serious problem like the whole life, my whole there is

139
00:12:30,480 --> 00:12:31,480
my life.

140
00:12:31,480 --> 00:12:37,040
It's not just I see something claim at that time, I see same thing like that effect for

141
00:12:37,040 --> 00:12:38,040
long.

142
00:12:38,040 --> 00:12:47,800
And so one of the big challenges and reinforcement learning is the idea that you've got this huge

143
00:12:47,800 --> 00:12:54,400
in many cases environment, like if we're thinking about this whole, the analogy to humans,

144
00:12:54,400 --> 00:12:58,120
we've got the entire world of things that you can do.

145
00:12:58,120 --> 00:13:05,800
And so how do we explore, how do we decide how we explore all of these possible actions

146
00:13:05,800 --> 00:13:12,880
and states that we could end in end up that that's actually amazing question and the challenge

147
00:13:12,880 --> 00:13:19,480
for reinforcement learning that we are trying to deal with like for many years is like, so

148
00:13:19,480 --> 00:13:26,600
now I give you the world and you need to explore this world, you don't know what each action

149
00:13:26,600 --> 00:13:27,600
is doing.

150
00:13:27,600 --> 00:13:33,440
So, you don't know if you follow some sequence of actions where you're going to end up.

151
00:13:33,440 --> 00:13:37,360
That's exactly the interesting question that you need to explore.

152
00:13:37,360 --> 00:13:41,600
Let's say you go to coffee shop and I'm bringing this example of coffee shop, I don't

153
00:13:41,600 --> 00:13:48,680
know, it makes it more interesting, you go to coffee shop and there is like latte, you

154
00:13:48,680 --> 00:13:53,800
have like let's say five options, there is latte, there is Americano, there is like

155
00:13:53,800 --> 00:13:58,720
cappuccino and brew coffee and also let's assume that in your local coffee shop they also

156
00:13:58,720 --> 00:14:03,960
sell vodka and you go to like coffee shop in eight in the morning, let's assume that

157
00:14:03,960 --> 00:14:08,520
you are not Russian or like Polish and you go eight in the morning to the coffee shop

158
00:14:08,520 --> 00:14:14,720
and you want to get coffee, you for sure not going to order like vodka, right?

159
00:14:14,720 --> 00:14:20,040
But you don't know that you like latte the most or you like like cappuccino the most.

160
00:14:20,040 --> 00:14:25,400
What you do, you need to know which one you think is the best one now and like you drink

161
00:14:25,400 --> 00:14:29,720
let's say you choose latte but tomorrow when you go you are like, I don't know what is

162
00:14:29,720 --> 00:14:34,280
the taste of like cappuccino, let's try cappuccino and for the first time you try the cappuccino

163
00:14:34,280 --> 00:14:35,280
for first time.

164
00:14:35,280 --> 00:14:41,000
Third time you go you order maybe brew coffee and the fourth time you go you are like, okay,

165
00:14:41,000 --> 00:14:47,040
I think latte was good so I'm going to go with latte today, so you get latte and the

166
00:14:47,040 --> 00:14:53,920
day after you say, well I'm not pretty sure how how like cappuccino tastes compared

167
00:14:53,920 --> 00:14:59,200
to latte, so next time you try like cappuccino, so over time you do this type of expression

168
00:14:59,200 --> 00:15:06,800
you are trying different things but you do not like try vodka, so you try some actions

169
00:15:06,800 --> 00:15:14,880
that they make most sense, right, so this is the way we want the RL agent or robot to

170
00:15:14,880 --> 00:15:20,920
do exploration, we don't want to have a RL agent to explore everything uniformly which

171
00:15:20,920 --> 00:15:27,680
is not possible, I don't want to have a GCA organic auto agent to just go left, left

172
00:15:27,680 --> 00:15:33,080
right, like without any reasoning of like what it does, I want to have the agent which

173
00:15:33,080 --> 00:15:38,760
does exploration, in such a way that it maximizes the amounts of information it gains from

174
00:15:38,760 --> 00:15:45,560
the environment while trying to, I mean by information I mean it can build better understanding

175
00:15:45,560 --> 00:15:52,520
of the world, while do not forgetting that the agent is there to collect like more like

176
00:15:52,520 --> 00:15:59,840
rewards, it's like those money in grant us also getting, so my agent wants to, my GCA agent

177
00:15:59,840 --> 00:16:07,400
wants to go around and understand how the world works, while it wants to maximize the understanding

178
00:16:07,400 --> 00:16:13,800
of the world or minimize the uncertainty around the world while trying to maximize the

179
00:16:13,800 --> 00:16:21,520
reward of the game, maximize the score of the game or like get the game to be done, if

180
00:16:21,520 --> 00:16:27,160
you have a kid playing like GTA, the kid doesn't explore the whole year and then start

181
00:16:27,160 --> 00:16:33,080
playing games, the agent while playing games like try to learn and build a model and see

182
00:16:33,080 --> 00:16:38,680
who is friend, what is good, going to gym is good, at the very beginning the kid might

183
00:16:38,680 --> 00:16:42,840
not know that going to gym is good, maybe if you go there they kill you, so something

184
00:16:42,840 --> 00:16:48,280
that, so there are a lot of things, characteristics of the environment that you need to learn,

185
00:16:48,280 --> 00:16:54,280
you need to explore, but you need to explore it carefully, so this way you do exploration

186
00:16:54,280 --> 00:16:59,000
is like actually the key factor in reinforcement learning that most of my works are based on

187
00:16:59,000 --> 00:17:06,200
like how do you do exploration efficiently, and so one of the simple concepts that comes

188
00:17:06,200 --> 00:17:11,880
up in reinforcement learning is this idea of explore, exploit and you've talked a little

189
00:17:11,880 --> 00:17:19,640
bit about that trade off via examples and then one of the algorithms that encodes that

190
00:17:19,640 --> 00:17:24,600
is this idea of epsilon greedy, what is that?

191
00:17:24,600 --> 00:17:30,360
epsilon greedy is actually super powerfully interesting like algorithm and it's super

192
00:17:30,360 --> 00:17:35,720
simple as well, what it does is that again I'm going to talk about coffee shop, you're

193
00:17:35,720 --> 00:17:41,000
at the coffee shop, you have these five options like I told you, you have lots of cappuccino,

194
00:17:41,000 --> 00:17:47,240
americano and like blue coffee and vodka, okay, so now let's assume that so far based on

195
00:17:47,240 --> 00:17:52,040
the, let's say you've seen that you have been in the coffee shop for many times and you know

196
00:17:52,040 --> 00:17:58,520
that you feel that you are a fan of latte demos, you like them all latte demos, so your greedy

197
00:17:58,520 --> 00:18:05,560
decision, which is the decision which maximizes your satisfaction is choosing latte, okay?

198
00:18:06,200 --> 00:18:11,960
So epsilon greedy exploration and exploitation strategy says hey, with probability like,

199
00:18:11,960 --> 00:18:19,160
let's assume epsilon is like 0.1, it says with probability 1 minus 0.1 which is like 1 minus epsilon,

200
00:18:19,160 --> 00:18:25,080
I'm going to choose the most greedy decision which maximizes my satisfaction which is latte,

201
00:18:25,080 --> 00:18:31,080
so with probability 90%, I'm going to choose latte and also I say hey, I'm also not

202
00:18:31,080 --> 00:18:38,600
pretty sure about other other decisions, I'm not sure, very sure about like cappuccino,

203
00:18:38,600 --> 00:18:44,520
so what it does with probability 1 minus epsilon which is like 10%, the agent randomizes

204
00:18:44,520 --> 00:18:49,720
over all these five actions, I think it says like with probability, but with probability 10%,

205
00:18:49,720 --> 00:18:53,720
it's randomizes over all the actions means that sometimes it's like choosing cappuccino,

206
00:18:53,720 --> 00:18:59,560
sometimes choosing americano and sometimes choosing latte when it does exploration and also

207
00:19:00,200 --> 00:19:05,880
it sometimes chooses vodka, so it's like the probability that you choose vodka is exactly equal

208
00:19:05,880 --> 00:19:13,000
to the probability that you choose brute coffee or like cappuccino, which is actually the part

209
00:19:13,000 --> 00:19:19,960
that epsilon greedy sales because you know that vodka is not good, why you choose it again and again,

210
00:19:19,960 --> 00:19:27,240
so this is one part that in one of my papers we address and we resolve this issue which is quite

211
00:19:27,240 --> 00:19:35,320
interesting, so epsilon greedy is like the powerful algorithm, it does choose like the best action

212
00:19:35,320 --> 00:19:40,040
with high probability, the best action I mean the best action that agent so far knows is the best,

213
00:19:40,040 --> 00:19:45,800
might not be, might not be the best, and with probability 1 minus epsilon, sorry with probability

214
00:19:45,800 --> 00:19:51,960
epsilon is gonna just uniformly choose other action, even though the agent might know that some

215
00:19:51,960 --> 00:19:58,200
actions are really bad, but it does choose them, so this is the epsilon greedy and it's been

216
00:19:58,200 --> 00:20:05,800
used in like area of deep, it was a prominent like algorithm for making tradeoff between

217
00:20:06,520 --> 00:20:13,080
exploration and exploitation in area of deep reinforcement learning, and one of the

218
00:20:13,080 --> 00:20:20,360
famous algorithm which uses this is deep q network, so I can just briefly tell like what deep

219
00:20:20,360 --> 00:20:27,800
q network does is, so let's go back again to the grandest auto game, deep q networks are one of

220
00:20:27,800 --> 00:20:36,680
the algorithms for deep reinforcement learning that really popularized the space and

221
00:20:37,560 --> 00:20:44,280
launched a lot of the efforts to solve video games and things like that, it was created at deep

222
00:20:44,280 --> 00:20:50,920
mind, is that right? Yeah, deep q network was created at deep mind and it was like one of the

223
00:20:50,920 --> 00:20:58,520
main reason why like there are many researchers are working on deep reinforcement learning because

224
00:20:58,520 --> 00:21:06,760
this first paper made it somehow possible to go beyond like a small grid work, small games

225
00:21:06,760 --> 00:21:12,680
that theoreticians we were like working on the small work and like this paper was the first paper

226
00:21:12,680 --> 00:21:19,080
that we were able to apply reinforcement learning method on the games like Atari's and like the

227
00:21:19,080 --> 00:21:24,200
games, the video games, the games that we were not even thinking that possible to solve

228
00:21:24,200 --> 00:21:30,200
using deep reinforcement learning algorithm at the time, but this paper was like a kind of

229
00:21:30,200 --> 00:21:34,840
revolutionary paper that brought a lot of attention to the field of reinforcement learning and

230
00:21:34,840 --> 00:21:41,560
people start working on this field, a lot of practitioners and like scientists they start

231
00:21:41,560 --> 00:21:48,840
working on these games and on these algorithms because this third paper made it somehow, it's not

232
00:21:48,840 --> 00:21:54,840
I'm not claiming that this was the it was like the huge gap between the previous works and this

233
00:21:54,840 --> 00:22:01,560
work, there were like many works before like deep q networks they were able to do many things on

234
00:22:01,560 --> 00:22:07,640
Atari games, but this one was the simplest one which had like simple idea and before it

235
00:22:07,640 --> 00:22:13,800
neural networks in order to solve Atari games, so it was quite interesting. And so what's the

236
00:22:13,800 --> 00:22:20,360
relationship between epsilon greedy and deep q networks? So the very deep q network works is like

237
00:22:20,360 --> 00:22:27,880
let's get back to the grand test auto, the agent walks around the city and for each action it

238
00:22:27,880 --> 00:22:33,960
knows that if the agent choose that let's say action going forward or shooting that person,

239
00:22:33,960 --> 00:22:41,080
it has the follow up return, like if it does something it might win the game or like it might get

240
00:22:41,080 --> 00:22:49,720
some reward or some money, some score, so each action has like like upcoming and like forwarding

241
00:22:49,720 --> 00:22:55,880
reward, so it's like each action has value, how good is that action? If I kill that person,

242
00:22:55,880 --> 00:23:03,400
sorry for my language, but if I go to the gym, how much value it has for me, how much I get

243
00:23:03,400 --> 00:23:12,600
full fit if I do this action now, so what deep q network does for when it plays game is like

244
00:23:12,600 --> 00:23:19,320
given a state of the game, even given the image of the game, which is like the frame of the game,

245
00:23:19,320 --> 00:23:26,120
it decides how good is each action, it's like compute the value of each action, okay?

246
00:23:26,120 --> 00:23:31,560
For the time if I play sequence, which is like a submarine is like in the sea and it's

247
00:23:31,560 --> 00:23:36,280
getting out of oxygen, it needs to go on the top of the sea and like get some oxygen,

248
00:23:36,280 --> 00:23:43,000
if I see I'm running out of oxygen, I know that going up has the most value for me,

249
00:23:43,000 --> 00:23:51,000
it's like I'm staying around the bottom of the sea has lowest value, so what deep q network does,

250
00:23:51,800 --> 00:23:58,760
deep q network finds their value associated with each action, okay? The way it constructs this

251
00:23:58,760 --> 00:24:04,120
value is like the agent needs to interact with the environment, the way the agent interacts with

252
00:24:04,120 --> 00:24:10,600
the environment collect samples and explore the environment is excellent, so what deep q network does

253
00:24:11,560 --> 00:24:21,000
builds the model, which somehow gives the value of each decision, and with probability like

254
00:24:21,880 --> 00:24:28,200
one minus epsilon, it goes with the best decision, the agent so far thing is the best,

255
00:24:28,200 --> 00:24:34,360
and with probability epsilon, it randomizes over the all the action, okay? So if I run deep q

256
00:24:34,360 --> 00:24:41,640
network on myself, when I go to the coffee shop, it probably one minus epsilon, I'm gonna choose

257
00:24:41,640 --> 00:24:46,760
latte, and with probability epsilon, I'm gonna randomize over the all the action, okay? So this is

258
00:24:46,760 --> 00:24:53,080
like the way deep q network works, it learns how good is each action, it learns the value associated

259
00:24:53,080 --> 00:24:59,720
with each action, how good is making the like action up at the current time step, how good is

260
00:24:59,720 --> 00:25:05,240
this action? So it learns this function, and when it learns this, while the way it learns this

261
00:25:05,240 --> 00:25:11,400
function is like collect samples to learn this function better and better. And so we can think of

262
00:25:11,400 --> 00:25:20,600
deep q networks as essentially an algorithm for accounting for the various values of these actions

263
00:25:20,600 --> 00:25:28,280
over a series of steps, is that fair? It's fair, it's fair that it's like counting the

264
00:25:28,280 --> 00:25:36,120
amount of reward, it's gonna receive in the future, and somehow in stuff memorizing all of them,

265
00:25:36,120 --> 00:25:43,960
it learns a function which approximates this count. And so in the best case, there are many

266
00:25:43,960 --> 00:25:51,720
theoretical analysis, we show that it doesn't do it exactly, but for this conversation, we can

267
00:25:51,720 --> 00:25:59,400
assume that it's done. Does deep q networks specify a particular type of neural network architecture,

268
00:25:59,400 --> 00:26:07,560
or can it be implemented with multiple different types of networks? Oh, deep q network is actually

269
00:26:07,560 --> 00:26:15,960
an algorithm, and it can be used for, and you can anyone can design his or her own like our

270
00:26:15,960 --> 00:26:22,680
neural network architecture. Neural network here are like the machinery used to solve this problem,

271
00:26:22,680 --> 00:26:29,800
but deep q network it's own is like a generic algorithm, doesn't care that you're using neural

272
00:26:29,800 --> 00:26:38,040
network, you're using Canon machine or using linear models, I mean, the objective is being used

273
00:26:38,040 --> 00:26:43,880
there is generic and can be applied on the on the variety of different models and can be applied

274
00:26:43,880 --> 00:26:49,320
on variety of different architectures, but the first paper used this objective function

275
00:26:51,240 --> 00:26:59,160
on using like deep neural networks, so that's why we call it deep q network. So the thing is like

276
00:26:59,160 --> 00:27:05,800
it can be applied on any architecture design for the deep network. I can design my own

277
00:27:05,800 --> 00:27:11,960
deep, I design also like for different tasks, I design different neural network architectures,

278
00:27:11,960 --> 00:27:20,040
but I still use deep q networks like machinery, and I'll go down to optimize and learn this value

279
00:27:20,040 --> 00:27:26,680
of each decision. So it's kind of generic algorithm, it's not just for a specific neural network,

280
00:27:26,680 --> 00:27:34,120
it's like it works, I mean, it's generic and it's applicable to a variety of almost all the

281
00:27:34,120 --> 00:27:41,720
neural networks. Are there specific neural networks that folks tend to use with deep q networks?

282
00:27:41,720 --> 00:27:49,560
Yeah, for Atari games, when we are like dealing with Atari games, we are using a specific neural network,

283
00:27:49,560 --> 00:27:55,480
mainly people have tried different neural networks, but mainly we use the same neural networks that

284
00:27:55,480 --> 00:28:02,280
give mine paper back in a day, not back in a few years ago, there's a lot. I mean, nowadays the

285
00:28:02,280 --> 00:28:07,640
time is, when you say back in the day, you mean like five years ago in the field of AI.

286
00:28:09,880 --> 00:28:16,760
So far, like most of the researchers, they use the architecture designed in the original deep

287
00:28:16,760 --> 00:28:25,800
q network paper, which is interesting because we use the same architecture and we design better and

288
00:28:25,800 --> 00:28:31,720
better algorithms on the top of deep q network. So we have a variety of extensions to deep q network,

289
00:28:31,720 --> 00:28:36,920
we have double deep q networks, we have my work, which is like Bayesian deep q network,

290
00:28:37,640 --> 00:28:43,240
is like we use the same architecture, but we are developing better and better algorithms.

291
00:28:43,240 --> 00:28:50,920
And once again, what makes an algorithm better and better in this context is its ability to

292
00:28:51,720 --> 00:28:58,120
make better decisions about what elements of the space to explore or what decisions to make

293
00:28:58,120 --> 00:29:04,040
in any particular juncture so that it's more sample efficient. That's a concept that comes up

294
00:29:04,040 --> 00:29:11,800
a lot in here. How efficiently are we using our time in the environment to train an algorithm?

295
00:29:11,800 --> 00:29:21,080
That's a super interesting point that you brought up is I want to learn the optimal behavior for

296
00:29:21,080 --> 00:29:26,680
let's say, playing games in minimum number of interaction with the game, right? I don't want to

297
00:29:26,680 --> 00:29:35,000
like play game for 25 billion years in order to, by game, I mean, let's say game pond or game

298
00:29:35,000 --> 00:29:40,440
and sequence. I don't want to play this game for like 25 billion years in order to be able to

299
00:29:40,440 --> 00:29:47,480
play years, I mean, like time for the for the for the entire game. So I don't want to play,

300
00:29:47,480 --> 00:29:52,440
I want to like play these games for like, I don't know, half an hour. By hour, I mean,

301
00:29:53,640 --> 00:29:58,360
by this time I'm talking is like the time that actually you need to play that game.

302
00:29:58,360 --> 00:30:06,040
It's not like the time that your RL agent is going to use is going to be the number of

303
00:30:06,040 --> 00:30:11,240
interactions with the environment you have. You don't want to have the number of times you

304
00:30:11,240 --> 00:30:15,560
play the game. You don't want to be like 20 billion in order to stop the game. You want to

305
00:30:15,560 --> 00:30:21,080
play this game 100 times and be able to learn it or you want to play this game like 200 times

306
00:30:21,080 --> 00:30:27,080
and being able to learn. So the sample complexity is really issue in reinforcement learning and

307
00:30:27,080 --> 00:30:35,080
the goal mainly is to design an algorithm which makes the optimal balance between exploration

308
00:30:35,080 --> 00:30:40,440
and exploration and minimize the sample complexity or some other notions that we call regret,

309
00:30:40,440 --> 00:30:45,320
which is like, you don't want to lose a lot before getting to the good performance.

310
00:30:47,000 --> 00:30:55,160
Another thing is that these environments, they can effectively have like local optima,

311
00:30:55,160 --> 00:31:02,840
meaning you could going back to your Starbucks coffee shop example, you can kind of get into a

312
00:31:02,840 --> 00:31:11,000
rut where you settle on the latte and that's what you choose. But you don't know that in one

313
00:31:11,000 --> 00:31:18,280
particular day, you happen to order the cappuccino, the person behind you also orders the cappuccino

314
00:31:18,280 --> 00:31:24,440
and you find out that your soulmates and like we're destined to be together. If you didn't order

315
00:31:24,440 --> 00:31:30,280
the cappuccino, you would have never have had that experience. The games that these environments are

316
00:31:30,280 --> 00:31:37,320
so dynamic that unless you're really careful about the way that you explore them, you miss opportunities.

317
00:31:38,120 --> 00:31:42,600
Yeah, it's like your tour ride, it's like these environments are super complicated,

318
00:31:42,600 --> 00:31:50,920
like the dynamic is like really complicated and that's a part that makes the whole our life

319
00:31:50,920 --> 00:31:57,320
really hard and also interesting. So like if the setting might have a lot of

320
00:31:57,320 --> 00:32:06,440
weird and complicated situation and it makes me to explore all of them. But if I'm gonna for the

321
00:32:06,440 --> 00:32:11,240
first time I go to coffee shop, I don't care that much that there is a person behind me is gonna

322
00:32:11,240 --> 00:32:16,440
be my soulmate. But if I go to coffee shop and I learn that which coffee actually I like,

323
00:32:16,440 --> 00:32:22,280
then I'm certain then I start to explore other stuff. I see who is behind me. So for example,

324
00:32:22,280 --> 00:32:26,600
first time I came to United States, I was not able to speak English very well. So when I went to

325
00:32:26,600 --> 00:32:33,560
coffee shop, I was like just focused to get my coffee and pronounce things correctly and like

326
00:32:33,560 --> 00:32:39,480
say my name correctly and be ever and pay correctly. But nowadays when I go to coffee shop,

327
00:32:39,480 --> 00:32:45,720
I just while ordering coffee, I talk to the person behind me. So over time when I get more

328
00:32:45,720 --> 00:32:50,440
confident and confident about the state of the environment, the way I need to make a decision,

329
00:32:50,440 --> 00:32:56,760
I start to do more complicated exploration. So it's like a very human does and I was like when

330
00:32:56,760 --> 00:33:02,840
the first came to United States as a person who was not speaking English before coming here.

331
00:33:02,840 --> 00:33:08,440
So I had this experience that when I start going to coffee shops, I was not even care who is behind me.

332
00:33:08,440 --> 00:33:15,560
I was like I was just focused to talk to the lady or the person, the guy there and just make my

333
00:33:15,560 --> 00:33:21,480
mission accomplished. I wanted to just order my coffee and make this that task done correctly.

334
00:33:21,480 --> 00:33:28,360
And so you've got these two different metrics for what makes a good algorithm. One is it sample

335
00:33:28,360 --> 00:33:35,720
complexity, sample efficiency. The other is the degree to which it fully explores the environment.

336
00:33:36,280 --> 00:33:42,120
There are many times but for simplicity, we all let's call it like sample complexity,

337
00:33:42,120 --> 00:33:46,680
like how many samples I need to come over the good strategy. Okay, so in other words,

338
00:33:46,680 --> 00:33:52,040
what I'm here you say is you can kind of boil all of that stuff down into sample complexity

339
00:33:52,040 --> 00:33:59,800
at the end of the day, whether it's the algorithmic, like the computational element of it,

340
00:33:59,800 --> 00:34:05,480
whether it takes a long time to converge on anything or not, whether it takes a long time to

341
00:34:05,480 --> 00:34:10,600
figure everything out or not. But all of this stuff is ultimately related to the sample complexity.

342
00:34:10,600 --> 00:34:15,720
Yeah, all of them are happening like the goodness of the sample in the sense that I get

343
00:34:15,720 --> 00:34:23,240
lower uncertainty about my world. And also how much that knowing that sample is going to help me

344
00:34:23,240 --> 00:34:29,480
to come up with a better like strategy or better decision. This combination drives me to come

345
00:34:29,480 --> 00:34:36,200
with a sample complexity. So with all that in mind, maybe you can walk us through a couple of

346
00:34:36,200 --> 00:34:41,240
your papers on this topic. You know, one of them is the one that you mentioned earlier,

347
00:34:41,240 --> 00:34:46,440
the Bayesian DeepQ Networks. What are you trying to do there? Yeah, the Bayesian DeepQ Network is

348
00:34:46,440 --> 00:34:53,960
actually quite interesting. I really like that work. It has some theoretical like guarantee

349
00:34:53,960 --> 00:35:02,120
about sample complexity, but also it has interesting behavior in like in real but on Atari game.

350
00:35:02,120 --> 00:35:08,920
Let's go back again on the coffee shop example. Epsilon really what we're saying is like it's

351
00:35:08,920 --> 00:35:13,640
going to choose latte with high quality and randomized over all the actions, all the other

352
00:35:13,640 --> 00:35:21,080
decisions uniformly. So Epsilon duty is also going to choose the vodka with the same number of times

353
00:35:21,080 --> 00:35:28,280
that it's choosing other like cappuccino or good coffee. But the thing is what it's happening

354
00:35:28,280 --> 00:35:34,840
is like it doesn't care that how confident you are about the other action. It just cares how much

355
00:35:34,840 --> 00:35:41,560
information you know about the best decision, which is like latte. So even for DeepQ Network,

356
00:35:41,560 --> 00:35:47,240
DeepQ Network is like computing the value of each action. If the action related to the vodka,

357
00:35:47,240 --> 00:35:55,160
it has a really low value, the Epsilon duty action is going to choose that. So what we do in DeepQ

358
00:35:55,160 --> 00:36:04,280
Network is we say hey, instead of just estimating the by we do in Bayesian DeepQ Network is we are

359
00:36:04,280 --> 00:36:10,440
saying that instead of estimating the value of each action, how good is each action, also

360
00:36:10,440 --> 00:36:16,360
estimate how confident how confident you are about the value of that action. Let's say I'm really

361
00:36:16,360 --> 00:36:23,640
confident that value of like the latte is like high, but let's say I'm not confident that the value

362
00:36:23,640 --> 00:36:29,480
of the cappuccino is high. So I have estimation of the value of the cappuccino. I know how cappuccino

363
00:36:29,480 --> 00:36:35,800
should be good. And I also know that I'm not that certain. So it worth trying. So if I know that

364
00:36:35,800 --> 00:36:41,640
the value of the cappuccino is like high, but it's not as high as latte, but I'm really uncertain

365
00:36:41,640 --> 00:36:49,160
about this value, like how good is the cappuccino, then I would like to try it. And also if I know the

366
00:36:49,160 --> 00:36:53,880
value of the vodka is really low, and also I know that with high confidence, I know that the value

367
00:36:53,880 --> 00:37:01,800
of vodka is low, I'm not going to ever like try it, right? So what DeepQ Network does,

368
00:37:01,800 --> 00:37:08,440
Bayesian DeepQ Network is like I'll algorithm on the top of DeepQ Network, but it says in sub

369
00:37:08,440 --> 00:37:14,840
estimating just the value of each action, also estimate how confident you are about each action.

370
00:37:14,840 --> 00:37:20,280
And then you want to make a decision, see how good is the value of that action and how much

371
00:37:20,280 --> 00:37:27,320
you're confident. And if you're not confident about the action with high value, let's try that one.

372
00:37:27,320 --> 00:37:32,760
Okay, so it's like what it does is like the exploration doesn't happen in epsilon giddy setting.

373
00:37:32,760 --> 00:37:39,880
The exploration happens in a setting that it tries to maximize a combination of the uncertainty

374
00:37:39,880 --> 00:37:46,680
and expected like a goodness of that action. If the action, the agent thinks it's good,

375
00:37:47,320 --> 00:37:52,360
but the agent is certain about that, but there's another action, you choose a slice, it's

376
00:37:52,360 --> 00:37:57,160
worse, but you're really uncertain about it, you want to try that one. Let's squeeze this one.

377
00:37:57,160 --> 00:38:02,920
You go to coffee shop, you go, you always get lottery, and you never got like cappuccino,

378
00:38:02,920 --> 00:38:08,040
but your grandma every day talks about cappuccino, okay? When you go to coffee shop,

379
00:38:08,040 --> 00:38:13,880
you have, you, you believe that the cappuccino has high value because your grandma is always

380
00:38:13,880 --> 00:38:18,760
talking about it, and but you never tried it. So you have a gigantic uncertainty about it,

381
00:38:18,760 --> 00:38:26,920
okay? But still you love lottery. So at this situation, you better to choose like the cappuccino

382
00:38:26,920 --> 00:38:34,040
because you think is a really good drink is not as good as lottery, you think, but you're not sure

383
00:38:34,040 --> 00:38:38,760
that's how good it is. Maybe it's way better than lottery, but you don't know, okay? But the

384
00:38:38,760 --> 00:38:43,640
expectation, your grandma, the way your grandma explains it to you, you think that it's a good drink,

385
00:38:43,640 --> 00:38:48,120
it's comparable to lottery, but it's, but it's still you like lottery a bit more because,

386
00:38:48,120 --> 00:38:53,560
but based on your grandma's explanation, you don't know that cappuccino is how good it is,

387
00:38:53,560 --> 00:38:58,760
but you roughly speaking, you know that it's not, you believe that it's not better than lottery,

388
00:38:58,760 --> 00:39:05,000
but you are not certain about it. So you just tried that one. So it's like kind of balanced

389
00:39:05,000 --> 00:39:14,040
between like uncertainty over the your leap about the value of each decision and makes the

390
00:39:14,040 --> 00:39:20,440
decision based on the expected value of each action and also on certain your over that

391
00:39:20,440 --> 00:39:29,800
expected value. And so are you in the epsilon greedy, you are choosing your primary, the,

392
00:39:29,800 --> 00:39:34,760
the one that you believe has the most value at a probability of one minus epsilon and then

393
00:39:35,480 --> 00:39:41,320
say you've got your five choices, the probability of you choosing one of those other ones is

394
00:39:41,320 --> 00:39:48,280
epsilon over four, right? Yeah. And epsilon over five. I randomize over the whole thing.

395
00:39:48,280 --> 00:39:53,560
Are you randomize over the whole thing? And so that was my question with, with the approach

396
00:39:53,560 --> 00:40:01,320
we're describing, we're waiting, are you, are you doing this confidence waiting over just the other

397
00:40:02,360 --> 00:40:08,360
four or over all of all of you just kind of evaluating each of them based on this confidence

398
00:40:08,360 --> 00:40:15,240
weighted metric. Exactly. So I just for each action, I have estimated value and also I have

399
00:40:15,240 --> 00:40:22,360
uncertainty. And I, what I do, I, so somehow it's like, I have a belief about each action.

400
00:40:23,160 --> 00:40:28,760
I am like, I know how good it's going to be, but this goodness is not a number. It's like

401
00:40:28,760 --> 00:40:35,800
somehow a distribution over each action. So I'm like, when I say I believe that this should be,

402
00:40:35,800 --> 00:40:41,640
okay, a means of high probability, I think it's good, right? So for each action, I have a

403
00:40:41,640 --> 00:40:47,560
distribution about how good each action it is. And the mean of this distribution is going to be

404
00:40:47,560 --> 00:40:53,960
my expected like, expected like value. And also this distribution has some variance,

405
00:40:53,960 --> 00:40:58,840
it's going to be somehow my uncertainty. Okay. So for each action, I have a belief and my

406
00:40:58,840 --> 00:41:04,360
belief is somehow a distribution over the, over the goodness of each action. Or it's going to be

407
00:41:04,360 --> 00:41:11,400
uncertainty over each action. And thinking about it in terms of distributions is where the

408
00:41:11,400 --> 00:41:17,080
concept of basing comes in the body. Exactly, exactly. So this distribution is like my belief,

409
00:41:17,080 --> 00:41:23,400
my posterior belief about each action. So if I know the posterior belief about each action,

410
00:41:23,400 --> 00:41:29,320
what I can do for each action, I can sample out of this belief. I get some, like, let's assume

411
00:41:29,320 --> 00:41:38,280
that for Lasse, I am my expected, expected value is like five. And my variance is like one,

412
00:41:38,280 --> 00:41:46,360
is like, I'm somehow uncertain that this is between four to six. Okay. So what I do, I sample

413
00:41:46,360 --> 00:41:52,920
a number between four and six. And I assume that the value of the, the Lasse is that number.

414
00:41:52,920 --> 00:42:00,280
I do the same thing for other, other actions like for Lasse, for Lasse, I know that my grandma told

415
00:42:00,280 --> 00:42:05,720
me the value of the lot, sorry, for, for Kappuccino, my grandma told me the value of the Kappuccino

416
00:42:05,720 --> 00:42:10,840
is like four. But I'm like my uncertainty is like the variance of my... She thinks it's higher

417
00:42:10,840 --> 00:42:16,360
than the Lasse. So maybe like eight. No, no, she tells me that the Kappuccino is good. But for me,

418
00:42:17,000 --> 00:42:21,880
she thinks that Kappuccino is the best. But for me, I think the Lasse is the best.

419
00:42:21,880 --> 00:42:28,040
Ah, okay. Okay. So for me, Kappuccino is like four. I never tried it. My grandma told me,

420
00:42:28,040 --> 00:42:33,000
I put four for Kappuccino. But the variance for Kappuccino is like 10. So the Kappuccino can be

421
00:42:33,000 --> 00:42:39,800
between 14 to minus like six. Got it. Got it. So if I sample, there is a high chance that the

422
00:42:39,800 --> 00:42:46,440
Kappuccino is going to get a number above like five. If the Kappuccino gets a number of five,

423
00:42:46,440 --> 00:42:52,600
I'm going to, I'm going to choose Kappuccino. So this is the way I do exploration. This is called

424
00:42:52,600 --> 00:43:01,640
Council Sampling. There was a guy back in like 1930s and he developed this idea of sampling.

425
00:43:01,640 --> 00:43:08,600
If you have a belief about the environment, you sample out of that and just do just act based on

426
00:43:08,600 --> 00:43:14,440
that sample. Also in psychology, the literature is called, they call it, they do not call it

427
00:43:14,440 --> 00:43:21,320
Council Sampling. They call it Beijing Sampling because like they are from psychology backgrounds

428
00:43:21,320 --> 00:43:26,840
so they they define their own term. But they have a cool setting that they say even human,

429
00:43:26,840 --> 00:43:33,560
that's why I bring up this coffee shop example. It's like they say in psychology that human also

430
00:43:33,560 --> 00:43:39,880
does Beijing Sampling. The human come up with a belief about the world, a belief about each

431
00:43:39,880 --> 00:43:46,760
decision and sample out of that belief and do the thing, the human thing. It's like the human

432
00:43:46,760 --> 00:43:53,240
randomizes over its behavior. It doesn't go always with treaty. We do not always get a lot

433
00:43:53,240 --> 00:43:59,240
of it. Sometimes randomize. The way we do randomization is sampling through our belief.

434
00:43:59,240 --> 00:44:03,880
This is playing in psychology. I'm not making the discreet. I'm just fitting the thing they say.

435
00:44:04,440 --> 00:44:10,440
I'm not sure how exactly to articulate this but it strikes me that there's one of the important

436
00:44:10,440 --> 00:44:17,240
assumptions in here is this idea that, I mean, I guess the whole idea of sampling, that you can

437
00:44:17,240 --> 00:44:24,120
just pick a number and even though relative to the distribution, that number could be an extreme

438
00:44:24,120 --> 00:44:29,400
outlier. We're still just going to use this number as to make our decision like how,

439
00:44:29,880 --> 00:44:33,960
what tells us that we can do that? So that's an interesting question.

440
00:44:35,160 --> 00:44:43,240
So if I draw samples and one sample is suddenly it's extremely high and like the question is should

441
00:44:43,240 --> 00:44:49,480
I go with that or not? So I guess maybe to start to answer my own question, are we just kind of

442
00:44:49,480 --> 00:44:54,600
reverting to law of large numbers here? If we do this enough, we're basically sampling around,

443
00:44:54,600 --> 00:45:00,680
you know, we'll kind of converge to our distribution. It's slightly related to that but it's not

444
00:45:00,680 --> 00:45:09,000
exactly that. Here it's about concentration of the measure mainly. It's like if I have uncertainty

445
00:45:09,000 --> 00:45:15,320
about the latte and cappuccino and others, if I sample them a lot, then I am going to be sure

446
00:45:15,320 --> 00:45:22,520
how good they are, right? If I'm, let's say in the like not realistic world, I'm able to drink

447
00:45:22,520 --> 00:45:29,480
latte 10 billion times and I'm allowed to drink cappuccino 10 billion times, then I can say

448
00:45:29,480 --> 00:45:36,920
which one is better, right? So it's like over time, my belief concentrates over its actual value

449
00:45:36,920 --> 00:45:43,880
because latte for me has a goodness. I don't know what is that goodness and I need to try it many,

450
00:45:43,880 --> 00:45:49,880
many times to understand what is that goodness and if you allow me to drink latte billions of times,

451
00:45:49,880 --> 00:45:57,800
then I can over time I can like shrink down my uncertainty about latte and at some point I say,

452
00:45:57,800 --> 00:46:03,720
hey, latte is actually a bit, even though might not be, but for other people, for me, let's say

453
00:46:03,720 --> 00:46:09,720
is like when I drink latte, a lot and cappuccino and a lot, over time I am certain about how

454
00:46:09,720 --> 00:46:17,800
which one is good. So it's not just law of large numbers because we're not just sampling a lot

455
00:46:17,800 --> 00:46:25,320
from this distribution to learn its parameters. We're also updating the distribution,

456
00:46:25,320 --> 00:46:30,920
the parameters of the distribution as we go along to reflect our increased confidence.

457
00:46:30,920 --> 00:46:36,440
Yeah, and here is like the distribution, my belief is my distribution,

458
00:46:36,440 --> 00:46:43,720
right? So this distribution at the very beginning has a like fact tail, but over time when I collect

459
00:46:43,720 --> 00:46:49,640
more samples, I'm going to make sure that, so what is this distribution? This is the distribution

460
00:46:49,640 --> 00:46:55,720
over the value of latte, right? So the value of latte in expectation is a fixed number,

461
00:46:55,720 --> 00:47:01,400
so it's five, right? So, but I don't, I'm not sure about it, so I am uncertain, so this uncertainty,

462
00:47:01,400 --> 00:47:06,760
so this distribution represents my uncertainty and over time when I collect more samples,

463
00:47:06,760 --> 00:47:12,200
I'm reducing my uncertainty and this uncertainty is going to be shrink down and over time if you

464
00:47:12,200 --> 00:47:18,920
give me, if you allow me collect more samples, I'm going to shrink down my uncertainty to zero

465
00:47:18,920 --> 00:47:24,680
and claim that I exactly know what is the value of the latte. So it's going to be like a measure

466
00:47:24,680 --> 00:47:32,760
of uncertainty, like if I, if I get more experience, if I try same thing many times, I get more,

467
00:47:32,760 --> 00:47:38,520
I get more certain about it and then I reduce somehow the variance of my belief about that,

468
00:47:39,320 --> 00:47:47,480
that action, that decision. So it's going to reduce that the uncertainty over the world for me,

469
00:47:47,480 --> 00:47:54,520
so and it's been like a study that if you use tons of sampling, you actually balance between,

470
00:47:54,520 --> 00:48:01,080
nicely balanced between exploration and exploitation and you actually get a like ordered off

471
00:48:01,080 --> 00:48:09,320
and all like sample complexity. So a part of your work then is applying this, this Thompson sampling

472
00:48:09,320 --> 00:48:18,440
algorithm to the deep Q networks and does the analytical results follow as well into the reinforcement

473
00:48:18,440 --> 00:48:24,840
learning realm? Yeah, it does. So I'm not applying Thompson sampling on deep Q networks,

474
00:48:24,840 --> 00:48:31,880
I'm extending deep Q networks to have this Bayesian property. Deep Q networks, as I said,

475
00:48:31,880 --> 00:48:39,080
they just compute the value of each action, but Bayesian deep Q networks use the same machinery as

476
00:48:39,080 --> 00:48:45,880
deep Q networks, but in self computing, the value of each action, it also, not only is

477
00:48:45,880 --> 00:48:52,360
estimated value of each action, it also estimate the uncertainty of that value. So it's like

478
00:48:52,360 --> 00:48:59,160
collecting more information. And on the other hand, for exploration, Bayesian deep Q network

479
00:48:59,160 --> 00:49:06,120
doesn't use epsilon greeting, but it used tons of sampling, which is like super interesting.

480
00:49:06,120 --> 00:49:13,800
I mean, the first time I applied this algorithm, this method on the Atari game, it was like,

481
00:49:13,800 --> 00:49:19,960
when I was showing this result to my colleagues, no one was like, like believing that was going on,

482
00:49:19,960 --> 00:49:27,800
because it was doing super great, like 1000 times better in the performance, like 100 times better

483
00:49:27,800 --> 00:49:34,680
in sample complexity. It was like crazy, really good. It can learn the game that like deep Q network

484
00:49:34,680 --> 00:49:40,440
learns in like, I don't know, 100 million times that it can learn it like less than 5 million,

485
00:49:40,440 --> 00:49:45,480
or less than 2 million for some games. It's crazy, really cool. It was like

486
00:49:46,280 --> 00:49:52,120
interestingly, interesting observation and made us to think more about exploration. If you

487
00:49:52,120 --> 00:49:58,040
like the literature in the deep Q networks, and learning almost like more than 99 percent of them,

488
00:49:58,040 --> 00:50:02,840
they use epsilon greeting. They're few of them, they are using more sophisticated exploration

489
00:50:02,840 --> 00:50:07,640
algorithm. And here we show that if you just end up doing architecture design, if you just

490
00:50:08,280 --> 00:50:12,280
come up with better exploration strategy, you're going to gain a lot.

491
00:50:13,160 --> 00:50:20,040
Has that result stood the test of time? Is this still state-of-the-art for certain games,

492
00:50:20,040 --> 00:50:27,560
or has it been extended by other folks? Where does it sit in the landscape of extensions to

493
00:50:27,560 --> 00:50:34,440
deep Q networks? Interesting, interesting question. After deep Q networks, there have been many,

494
00:50:34,440 --> 00:50:41,080
many extensions to design better cost functions, design better, like sampling, the design

495
00:50:41,080 --> 00:50:47,560
better handling of the memory. There are a lot of extensions that they advanced deep Q networks,

496
00:50:47,560 --> 00:50:56,280
but they're almost all using epsilon greeting. So we are not comparing against those advanced

497
00:50:56,280 --> 00:51:01,800
architecture. We are just saying, just take the simple deep Q network, and instead of doing epsilon

498
00:51:01,800 --> 00:51:09,160
greeting, do top of that. And we show that it does way better, and also we show that it does

499
00:51:09,160 --> 00:51:14,760
better than many, many other algorithms, many, many advanced algorithms, but we did not compare

500
00:51:14,760 --> 00:51:22,120
against those algorithms that they are advancing the architecture or advancing some other like

501
00:51:22,120 --> 00:51:28,120
v-warding functions, because the point of its work was like, hey, if we just change the exploration

502
00:51:28,120 --> 00:51:33,080
algorithm, what's going to happen? If those algorithms, those algorithms, they are advancing deep

503
00:51:33,080 --> 00:51:39,640
Q network. If they, if I applied this top of sampling on those algorithms, I'm going to get

504
00:51:39,640 --> 00:51:45,240
like better performance. Got it, got it. So you kind of left it as an exercise to the reader to

505
00:51:45,240 --> 00:51:52,520
take their favorite extension to deep Q networks, and try this as a way to get even better sample

506
00:51:52,520 --> 00:51:58,600
complexity. Yeah, it's like just this is like the way we say to do exploration and exploitation

507
00:51:58,600 --> 00:52:05,000
instead of epsilon greeting. If you have a sophisticated like deep reinforcement learning algorithm,

508
00:52:05,000 --> 00:52:10,040
you're, if you're using epsilon greeting, if you apply this one, this approach, which has a

509
00:52:10,040 --> 00:52:16,760
theoretical analysis, and a theoretical guarantee, you hope to get better performance.

510
00:52:16,760 --> 00:52:22,920
What you've done here is you've proposed an alternative to epsilon greeting based on

511
00:52:22,920 --> 00:52:30,920
Thompson sampling, so based on uncertainty waiting. Presumably, there are other ways that you could

512
00:52:30,920 --> 00:52:39,560
change the exploit mechanism further. How well explored is that space? Have a lot of people

513
00:52:39,560 --> 00:52:45,720
proposed different algorithms for dealing with exploitation? That's a super interesting question.

514
00:52:46,520 --> 00:52:53,960
Is from land of theory, like the main question we are asking on everyday is to come up with

515
00:52:53,960 --> 00:52:59,480
the better exploration and exploitation algorithm. From the theory land, we developed a lot of

516
00:53:00,120 --> 00:53:06,440
nice, amazing, and sweet, I would say algorithms, they, they balance this exploration

517
00:53:06,440 --> 00:53:14,200
for different settings. So, theoretically, this area is nicely studied. There are a lot of

518
00:53:14,200 --> 00:53:25,000
rooms to explore more and to prove and study new things and find the optimal way of exploration.

519
00:53:25,000 --> 00:53:33,640
But the thing is extending those methods. Even I have seen like a theoretical way of doing

520
00:53:33,640 --> 00:53:41,880
exploration, exploitation in the theory land, but it's not easy to extend those ideas and those

521
00:53:41,880 --> 00:53:48,920
settings to deep neural networks. So, from the theory, we have a lot of algorithms and we have

522
00:53:48,920 --> 00:53:55,800
a massive amount of studies that we know how to do exploration, exploitation to get some sort of

523
00:53:55,800 --> 00:54:02,680
order optimal, like safer complexity. But the problem is that those algorithms are not easily

524
00:54:02,680 --> 00:54:09,320
extendable to deep reinforcement learning. Is it simple to explain or give an example of an

525
00:54:09,320 --> 00:54:17,000
algorithm that isn't easy to apply or how or why they tend to break? So, one of the major

526
00:54:17,800 --> 00:54:23,720
portion of the theory land in reinforcement learning is like model-based reinforcement learning.

527
00:54:23,720 --> 00:54:34,520
What we do is like we literally learn the model dynamics. For each state, we store the

528
00:54:34,520 --> 00:54:41,160
transition from each state to another state. If I'm receiving, I'm going to coffee shop,

529
00:54:41,160 --> 00:54:48,440
I get a latte and after that, I'm going to go back. So, we need to store all these possible

530
00:54:48,440 --> 00:54:55,160
transitions, going from one situation to another situation. We need to store all these things

531
00:54:55,160 --> 00:55:05,720
and do our optimization on the gigantic space of possibility. It's not possible to store

532
00:55:06,440 --> 00:55:14,920
all these things in the memory and do your computation. It's not possible. Also,

533
00:55:14,920 --> 00:55:23,560
it's like most of the algorithms we designed for the area from the theory land,

534
00:55:24,040 --> 00:55:32,040
they are not always. Most of the time, we were really concerned about the worst case scenario.

535
00:55:32,040 --> 00:55:37,080
We want to compute, we want to come up with the algorithm, which no matter what is the model,

536
00:55:37,080 --> 00:55:43,800
is going to perform the best. So, it makes almost all the algorithms we developed so far,

537
00:55:43,800 --> 00:55:49,240
makes all of them such that they are the best for worst case scenario. Because we want to

538
00:55:49,240 --> 00:55:54,840
provide a theory. Why this is a case is a case? Because we are going to provide theoretical

539
00:55:54,840 --> 00:56:01,560
guarantee. When I say this algorithm for short works, it means that for no matter what is the world,

540
00:56:01,560 --> 00:56:06,680
what is the environment, it should work. But that doesn't mean it's practical for

541
00:56:07,240 --> 00:56:13,560
any given sense. Exactly. Those are the algorithms that they give you bounds and they give you

542
00:56:13,560 --> 00:56:20,600
theoretical proof that they are going to work. But the thing is, if you know that your environment

543
00:56:20,600 --> 00:56:27,400
is not that bad, it's not adversarily chosen or it's not the worst case scenario ever can happen

544
00:56:27,400 --> 00:56:32,360
to your algorithm, then you can come up with a better and more sensible algorithm.

545
00:56:32,360 --> 00:56:39,560
Reinforcement learning is kind of young. We need many, many more people to spend time and do

546
00:56:39,560 --> 00:56:43,400
and build up that theory to kind of stop reinforcement learning. So we are like the community of

547
00:56:43,400 --> 00:56:49,640
deep reinforcement. The community of the theoreticians in reinforcement learning is not as big as

548
00:56:51,160 --> 00:56:55,400
I cannot even say like it's like the proportion is almost getting to zero. If you look at the

549
00:56:55,400 --> 00:57:00,600
number of people they do theory of like reinforcement learning compared to the people they do

550
00:57:00,600 --> 00:57:05,080
empirical study of reinforcement learning. This ratio is like super small. You cannot even see

551
00:57:05,080 --> 00:57:11,720
theoreticians. Which is like, I'm not, I mean, it's almost nice and also is a concerning. We should

552
00:57:11,720 --> 00:57:17,880
do both. And so just to make sure I understand the first of the two impediments you mentioned,

553
00:57:18,520 --> 00:57:27,400
it is that in practically speaking with deeply reinforcement learning models like deep Q, they make

554
00:57:27,400 --> 00:57:35,400
simplifications that lend themselves to, you know, practical concerns, computational concerns.

555
00:57:35,400 --> 00:57:45,240
And so for example, they are looking at, they're kind of rolling up the value into a single

556
00:57:45,800 --> 00:57:53,400
state, if you will, or element, if you will. Whereas some of these algorithms that are, you know,

557
00:57:53,400 --> 00:57:59,560
you could argue are more optimal, you know, they may be looking at a wider, you know, wider

558
00:57:59,560 --> 00:58:05,000
set of observations of the state that you couldn't really operate on practically. Is that what

559
00:58:05,000 --> 00:58:12,040
you're kind of getting at? So the more sophisticated algorithm they design, more sophisticated

560
00:58:12,680 --> 00:58:19,480
cost function. So they are like, they are designing the advancing the objective function that

561
00:58:19,480 --> 00:58:26,040
deep Q network model is solving. So it's like, they are advancing that. And the idea behind

562
00:58:26,040 --> 00:58:31,880
deep Q network was not giving the best algorithm. They, the Q network came out to show that it is

563
00:58:31,880 --> 00:58:39,160
possible to play games. It doesn't claim that it's the best. And the aim of that work from deep

564
00:58:39,160 --> 00:58:44,840
mind was not heavier doing the best reinforcement learning algorithm. They wanted to just want to show

565
00:58:44,840 --> 00:58:51,880
that, hey, it is possible to use current algorithms to in order to beat human or like, in order to

566
00:58:51,880 --> 00:58:58,200
learn simple games. And a big part of that possibility was the computational feedback. Yeah, they didn't

567
00:58:58,200 --> 00:59:03,320
yeah, they didn't care that much about like, like, like sample complexity. They didn't want to minimize

568
00:59:03,320 --> 00:59:08,920
the number of samples that the agencies, they wanted the main goal was, hey, we want our

569
00:59:08,920 --> 00:59:15,480
else from like other algorithms and we are going to ask from other like human. That was the

570
00:59:15,480 --> 00:59:21,160
goal behind the nature paper they had was like, hey, this is the case that you can come with the

571
00:59:21,160 --> 00:59:27,480
algorithm, which does better than human in certain games. Or let's say for alpha, go like the

572
00:59:27,480 --> 00:59:34,840
goal game, you do not care about the damage about the number of samples that your like goal players,

573
00:59:34,840 --> 00:59:40,840
like your agency, you just at the end wants to like, like, win against the best goal player.

574
00:59:40,840 --> 00:59:45,880
Right. Right. So it's like, this is really important. This is really interesting to do, to be

575
00:59:45,880 --> 00:59:52,440
make it feasible for first time. But after that, you need to design the algorithm to do the thing

576
00:59:52,440 --> 00:59:57,560
that reinforcement learning actually requires. It's like reducing number of samples, like learning

577
00:59:57,560 --> 01:00:02,760
better policy. So they made it feasible. The rest is going to be like, first make it better.

578
01:00:02,760 --> 01:00:09,400
By making it better, it's like coming with better sample complexity, coming with a better value

579
01:00:09,400 --> 01:00:16,280
estimator. In the second work that I want to talk is like, you can easily show that the QN

580
01:00:16,280 --> 01:00:22,280
objective is actually biased objective function. So you better off not to do it. If you want to be

581
01:00:22,280 --> 01:00:28,760
really serious in reinforcement learning, you better like do something else than get QNF. But if

582
01:00:28,760 --> 01:00:33,960
deep QNF is the only thing you can do, we just go with it. If you can't, I mean, if you're able

583
01:00:33,960 --> 01:00:40,040
to do better, it's better off not to deep QNF because the cost function is easily biased and can

584
01:00:40,040 --> 01:00:44,360
it can screw you off. So elaborate on that. You said the second cost function.

585
01:00:45,160 --> 01:00:49,640
No, in the second work that I get chance to talk about here.

586
01:00:49,640 --> 01:00:56,280
In that second paper, walk us through the results. So you started with looking at the cost

587
01:00:56,280 --> 01:01:02,040
function for deep QNF works. So I started to looking at the cost function at deep QNF work,

588
01:01:02,040 --> 01:01:10,280
and I have many friends from theory lands. They have many amazing papers and astonishing papers

589
01:01:10,280 --> 01:01:18,120
that are analyzed back in like 10 years ago or 20 years ago that they analyze this cost

590
01:01:18,120 --> 01:01:23,080
function that one of them has been used by deep QNF work. And they analyze this constantly.

591
01:01:23,080 --> 01:01:30,760
They show that hey, they are biased. And also there is another work we show that for any function

592
01:01:30,760 --> 01:01:37,560
approximation method, there is a subset of problems that if you run this functional approximation

593
01:01:37,560 --> 01:01:44,040
method, they either diverge or are biased. So it's like, there is no hope that you can get

594
01:01:44,920 --> 01:01:52,760
algorithm, which is like, you think that it can do something reasonable almost everywhere.

595
01:01:52,760 --> 01:02:01,320
But the hope is that they do not break in the simple setting. So if you look at the deep QNF work

596
01:02:01,320 --> 01:02:08,120
objective, it's like it minimizes two terms where you just care about one of them. The second term,

597
01:02:08,120 --> 01:02:15,560
you don't want to minimize. But it's naturally minimizes. And also it has some other issues of like

598
01:02:15,560 --> 01:02:24,920
like it somehow looks at, and stuff like like averaging the errors, it just always looks at the

599
01:02:24,920 --> 01:02:31,160
maximum error and like back propagate that maximum error, which is not the thing you want. If you

600
01:02:31,160 --> 01:02:36,280
have a Gaussian random variable, you don't want, you don't care about it max, you care about

601
01:02:36,280 --> 01:02:43,000
like it's mean. But there's the thing that deep QNF work setting passes through its back

602
01:02:43,000 --> 01:02:49,560
propagation is actually the max of the sample, which is not the thing you want. So it's kind of like

603
01:02:49,560 --> 01:02:57,640
biased in many, many different senses. And the question is, if it's biased, and if we know that

604
01:02:57,640 --> 01:03:03,560
any function approximation method can be biased or diverges, can we do anything for that? That was

605
01:03:03,560 --> 01:03:08,680
the question I was asking myself when I start to work on the second work, which we call it

606
01:03:08,680 --> 01:03:15,080
Generative Adversarial 3 Search, or in short we call it Gats. Probably you have watched the movie

607
01:03:15,080 --> 01:03:22,680
Gats B. So the name has some relation to that guy as well. But yeah, the algorithm is called Gats

608
01:03:22,680 --> 01:03:29,080
and it's trying to address the fact that if I use functional approximation for deep reinforcement

609
01:03:29,080 --> 01:03:35,240
learning, and if my functional approximation is biased, can I do anything with respect to that

610
01:03:35,240 --> 01:03:42,760
or not? And presumably you found that you could do something using a generative adversarial network

611
01:03:42,760 --> 01:03:49,080
based approach. Yeah, that's an interesting idea. So there's a line of research in psychology,

612
01:03:49,960 --> 01:03:55,880
which says when human wants to make decision, it forecasts what's going to happen in the future.

613
01:03:56,600 --> 01:04:04,360
Or people say imagine I would, I would like avoid using word imagine, but I can't, I would say

614
01:04:04,360 --> 01:04:11,800
human like when I want to go to coffee shop from my office, I just imagine or forecast or

615
01:04:11,800 --> 01:04:18,760
predict what's going to happen in the future. I can think of going there talking to the coffee man,

616
01:04:19,720 --> 01:04:26,040
an order and pay and think about what I'm going to get and come back. I can imagine and think

617
01:04:26,040 --> 01:04:34,280
about it. So this is a line of research in psychology which says sometimes when human

618
01:04:34,280 --> 01:04:41,400
makes decision, human builds the model of the environment, model of the world, and in that world

619
01:04:41,400 --> 01:04:47,960
does some analysis. It's not always human does analysis based on just observation. When human

620
01:04:47,960 --> 01:04:53,400
interact with the environment with the world, it has some model of that some abstract model of

621
01:04:53,400 --> 01:05:00,840
the environment and the world in his or her brain, okay? I mean, baby, it doesn't make sense, but

622
01:05:00,840 --> 01:05:04,600
I think to me it makes sense because when I'm going to do something and when I want to plan for

623
01:05:04,600 --> 01:05:12,440
future or for my like like a trip of I want to do a road trip, I plan everything and I think of

624
01:05:12,440 --> 01:05:17,720
what's going to happen, what are the situations, all these scenarios, I check all of them in my brain,

625
01:05:17,720 --> 01:05:22,840
I do not start my road trip and then see what's going to happen, I just plan everything.

626
01:05:22,840 --> 01:05:29,960
So this is a thing that we they say, of course there are like many decisions we do not plan for,

627
01:05:29,960 --> 01:05:35,080
like let's say there's a mosquito, it just bites us, like we do not plan to just scratch that

628
01:05:35,080 --> 01:05:42,040
part of body that happens. But part of it happens through this planning, through like the model of

629
01:05:42,040 --> 01:05:48,680
the world that we have in our brain. So we were saying like let's get some idea from this part

630
01:05:48,680 --> 01:05:55,800
of psychology and also see what theory tells us. Theory tells us that functional approximation

631
01:05:55,800 --> 01:06:06,120
like is can be bias or diverse. So now if I have let's say bias like value function, okay. And if I'm

632
01:06:06,120 --> 01:06:13,640
going to use it and also if I'm able to learn the model dynamics, the dynamic, the way model works,

633
01:06:13,640 --> 01:06:20,120
I can build the model of the world right in my brain. Or somehow the agent can based on

634
01:06:20,120 --> 01:06:25,960
interaction the agent has with the world here by world, I mean let's say Atari game, given interaction

635
01:06:25,960 --> 01:06:31,240
that the agent has with Atari games going left, right up shooting left kind of things,

636
01:06:31,240 --> 01:06:38,040
it can learn that given the current situation, if it does some sort of sequence of like decisions,

637
01:06:38,040 --> 01:06:46,920
what's going to happen in the future. So in this work, we used this an interesting framework

638
01:06:46,920 --> 01:06:54,680
out there, which is called generative adversarial networks. These methods, they're like used to

639
01:06:54,680 --> 01:07:01,640
generate images out of like random noises. So they are like the kind of generative model. And we

640
01:07:01,640 --> 01:07:09,320
use this machinery to be able to make ourselves able to like come up with a model of the world.

641
01:07:09,320 --> 01:07:16,120
So now given the samples, we designed an algorithm and a neural network which is like able

642
01:07:16,120 --> 01:07:23,800
given like current frame of the Atari game is able to tell you what is going to be the next 20

643
01:07:23,800 --> 01:07:30,120
frames. If you follow up, down, down, up some sequence of action, it's able to tell you what

644
01:07:30,120 --> 01:07:35,160
actually is going to happen in the future, which is quite interesting. So if I am able to see

645
01:07:35,160 --> 01:07:41,640
like 20 steps from now, okay, then I can say, hey, whenever I'm going to make a decision, I just

646
01:07:41,640 --> 01:07:50,360
see all possible all possibilities from now in 20 steps in my brain. And then in the lift node,

647
01:07:50,360 --> 01:07:56,520
so it's going to be I'm going to construct a tree. I say, if I am at the current time step,

648
01:07:56,520 --> 01:08:03,320
if I am my agent, my like my Atari agent is at some location in the game, if it choose action

649
01:08:03,320 --> 01:08:08,040
up, very it's going to go, if it choose action down, very it's going to go, if it goes forward,

650
01:08:08,040 --> 01:08:14,280
way it's going to go. So I can ask my this generative model, because it's generative dynamic model,

651
01:08:14,280 --> 01:08:22,440
as short like GDM. I can ask GDM, hey, if I if I am at this kind of state and if this and my action

652
01:08:22,440 --> 01:08:29,000
is going up, what's going to happen? And GDM tells me you go to the new place and I ask GDM, hey,

653
01:08:29,000 --> 01:08:34,280
now if I am in just new place, if I go action down, where I go. So I can build up a tree for

654
01:08:34,280 --> 01:08:40,440
different possible actions and see where I go. It sounds like, I mean, in a regular

655
01:08:41,640 --> 01:08:49,960
DRL type of problem, they're also kind of evaluating the expected reward for each of the

656
01:08:49,960 --> 01:08:56,600
possible actions, but that the notion of expectation is like you're you've lost a lot of

657
01:08:56,600 --> 01:09:03,960
information. And so is the idea that by using GANs to project the future state instead of or

658
01:09:03,960 --> 01:09:11,960
an expectation, you retain more information? Okay, that's a great point is if I run my DRL algorithm

659
01:09:11,960 --> 01:09:19,640
to compute this value, what I'm claiming here is that value is can be really, really off.

660
01:09:19,640 --> 01:09:27,240
It means that can be arbitrary bad. Okay, so if I run DeepQ network for, let's again,

661
01:09:27,240 --> 01:09:37,000
pond for a given situation, the pond agent, if I run, if I run DeepQ network, it tells me

662
01:09:37,000 --> 01:09:45,560
going up gives you return of like value of like 10. Okay, if I run Deep, like double DeepQ

663
01:09:45,560 --> 01:09:52,120
network, which is another algorithm, it gives me 5. It's like the DeepQ network, the value

664
01:09:52,120 --> 01:09:57,400
was estimating was like off square factor of 5, by another factor, by bias of 5, at least.

665
01:09:58,200 --> 01:10:03,640
So it's like my point is like any deep RL algorithm can be arbitrary bias.

666
01:10:04,600 --> 01:10:10,440
And now the question is like if it's bias, it means that as you said, it's supposed to tell me

667
01:10:10,440 --> 01:10:17,400
what is the future reward, right? But if this estimator is like wrong, then the question is what

668
01:10:17,400 --> 01:10:24,840
we can do. So what we can do is like we train a generative model in order to be able to like

669
01:10:24,840 --> 01:10:32,040
generate future. And if I am able to generate future, if I'm like, if I'm, if I have an agent

670
01:10:32,040 --> 01:10:39,320
is playing in the playground and it's a ditch somewhere. And if I imagine, if I like, if the agent,

671
01:10:39,320 --> 01:10:44,920
if I use this generative model and see what's going to happen in the future, I see if I choose

672
01:10:44,920 --> 01:10:50,440
action up, up, up, I'm going to fall in that ditch, right? So I do not go. So it's like,

673
01:10:50,440 --> 01:10:55,640
somehow I'm seeing what's going to happen in the future. Yeah, I think maybe the question I'm

674
01:10:55,640 --> 01:11:04,440
that I'm asking is what is it about GANS maybe that allows us to be any more accurate looking

675
01:11:04,440 --> 01:11:11,800
into the future than the machinery used for, you know, for predicting a future reward?

676
01:11:11,800 --> 01:11:22,120
Okay, so it's like, so we theoretically show that it's actually exponentially can improve the error

677
01:11:22,120 --> 01:11:28,040
in the, in the Q function. Okay, so it's, theoretically, it's like exponential improvement,

678
01:11:28,680 --> 01:11:34,360
but practical, I can tell you something. So the way we compute the reward,

679
01:11:35,000 --> 01:11:41,000
the way we compute the value and return is through discount factor, right? So there's a discount

680
01:11:41,000 --> 01:11:48,120
factor. It's like the reward I get that kind of time step worth more than reward I receive in

681
01:11:48,120 --> 01:11:54,600
like next 10 times steps, right? There's a discount factor. So now, if I have my, if my Q function

682
01:11:54,600 --> 01:12:03,160
has a bias of five, okay? And if I am able to do rollout in my generative model up to depth of

683
01:12:03,880 --> 01:12:10,520
up to depth of 20, then the Q value I'm going to use at depth of 20 is going to be

684
01:12:10,520 --> 01:12:15,640
discount factor, power of 20, discount factor is less than one. So the power of 20 is going to be

685
01:12:15,640 --> 01:12:21,560
super small times that five. So the effect of the bias is not going to appear. So actually,

686
01:12:21,560 --> 01:12:26,280
this machinery has been used for AlphaGo as well. What AlphaGo does AlphaGo is like,

687
01:12:27,080 --> 01:12:35,080
runs Monte Carlo 3 search on the board game for some depth, let's say depth of age. And when it goes

688
01:12:35,080 --> 01:12:42,360
to the like leaf node of that tree, so it builds a tree and then the leaf node of that tree

689
01:12:42,360 --> 01:12:49,800
use the learn Q value, okay? If the Q value is biased, since I'm like rolling out for like depth

690
01:12:49,800 --> 01:12:56,920
of age, the effect of that bias is going to exponentially go down. I can imagine I need to draw

691
01:12:56,920 --> 01:13:05,560
three in length. No, I think it makes sense. So your the argument is that with the deep Q networks,

692
01:13:05,560 --> 01:13:15,560
you are predicting the future reward at a given point based on a given action. But by using

693
01:13:15,560 --> 01:13:23,880
GaNS to project into the future, what the board is going to look like, you can then discount

694
01:13:23,880 --> 01:13:30,600
out the errors by the cause of the you're predicting further into the into time.

695
01:13:30,600 --> 01:13:36,600
That one thing is like the first part of your statement. When I use deep reinforcement

696
01:13:36,600 --> 01:13:43,080
and algorithms to compute to see what is the cumulative reward of future at the given point

697
01:13:43,080 --> 01:13:50,680
and given action, you said, my statement here is any deep reinforcement and algorithm use

698
01:13:50,680 --> 01:13:57,320
this error error in this estimation can be arbitrary wrong. It can be arbitrary B. So the

699
01:13:57,320 --> 01:14:05,880
actually you think that you are actually learning the expected return of that state and action

700
01:14:05,880 --> 01:14:13,880
given that the specific point that a specific action. But I can prove of the show that there are

701
01:14:13,880 --> 01:14:21,080
problems that this estimator can be arbitrary wrong. So now I'm saying that if I had a perfect

702
01:14:21,080 --> 01:14:26,280
estimator, there was no need. I mean, there isn't there isn't that much of need for having a

703
01:14:26,280 --> 01:14:35,080
generative model or GaNS. But I know that this error is big for like for deep Q networks,

704
01:14:35,080 --> 01:14:41,640
these errors are gigantic that sometimes like shockingly gigantic. People have studied this

705
01:14:41,640 --> 01:14:46,760
by bias. Because the bias in the estimator, people have studied this biases and they observe

706
01:14:46,760 --> 01:14:53,640
that these biases are like huge. It's like hundreds of percentage are like bigger than the actual value.

707
01:14:54,360 --> 01:15:00,840
So it means that the estimator you get, when you compute the expected return condition

708
01:15:00,840 --> 01:15:07,160
on the specific point and the space and the action, that estimation can be arbitrary wrong.

709
01:15:07,160 --> 01:15:13,480
Okay, so if it's arbitrary wrong, you cannot do anything. But if it's like wrong but not super

710
01:15:13,480 --> 01:15:20,920
wrong, the question is can we do anything? And one of the answer is like doing this generative

711
01:15:20,920 --> 01:15:28,600
adversary or research is like if it's wrong, I can build a tree. I can look at the future and I can

712
01:15:28,600 --> 01:15:35,240
say I want to look at the future and see where I go after like 20 times steps. And then I go to

713
01:15:35,240 --> 01:15:43,160
at the 20 times steps, time step, I'm going to use the this bias value function I learned.

714
01:15:43,160 --> 01:15:49,880
Okay, but the thing is whatever the value of that 20 step is, I'm going to multiply it by

715
01:15:49,880 --> 01:15:57,000
discount factor to power of like 19 or 20 something that. So whatever is going to be the error,

716
01:15:57,000 --> 01:16:03,080
that error is going to be a squash down exponential in depth, which is the thing I really like to have.

717
01:16:03,080 --> 01:16:11,720
Are you using the GAN to project the state only at 20 or is it 2019, 18 all the way down to one

718
01:16:11,720 --> 01:16:16,680
and they're discounting each of them? I literally build the whole tree and then scan all of them.

719
01:16:16,680 --> 01:16:26,120
Okay, and so it's other word, I literally build the model of the world. I literally build the

720
01:16:26,120 --> 01:16:31,400
NDP. If there exists NDP, I literally build the Markov decision process there.

721
01:16:31,400 --> 01:16:37,560
Right. So another way to think about this is like, you're basically trying to do what AlphaGo

722
01:16:37,560 --> 01:16:44,120
did with Go, but for a game like Go, you've got like fixed positions and fixed states. And while

723
01:16:44,120 --> 01:16:51,160
there are a lot of them and we can't enumerate them efficiently, we know what they are given a

724
01:16:51,160 --> 01:16:57,160
particular state of the game now and a particular move and we can enumerate a tree.

725
01:16:57,160 --> 01:17:04,840
For an Atari game, they're more continuous. They're not like these very discrete moves and

726
01:17:04,840 --> 01:17:11,640
positions. So how do we, how might we apply this idea of getting the tree at a bunch of future

727
01:17:11,640 --> 01:17:16,600
states? Well, we can use GANs to do that. Okay, there are three points I need to make it.

728
01:17:16,600 --> 01:17:24,360
First of all, first of all is AlphaGo does this tree search on the board game, right?

729
01:17:24,360 --> 01:17:31,240
And AlphaGo agent has the board game, has the model. But for Atari, I do not have the model.

730
01:17:31,240 --> 01:17:37,800
Yep. So I need to actually learn the model. So there's one difference from like this GANs

731
01:17:37,800 --> 01:17:45,160
and AlphaGo has actually the model of the environment. But for GANs, I need to learn the model

732
01:17:45,160 --> 01:17:52,760
of the environment. Second is for Monte Carlo tree search, in order to work, you do not need to have

733
01:17:52,760 --> 01:18:00,360
discrete like state space. They also work for continuous like state space. So the GAN tea

734
01:18:00,360 --> 01:18:06,120
like the holds before the continuous state space, it doesn't need that. But the thing which makes

735
01:18:06,120 --> 01:18:13,880
it easy for Atari again is like state space, hypothetically it's like continuous, but the transition

736
01:18:13,880 --> 01:18:19,720
is deterministic somehow. It's like if I play sequence, if I choose action left, I go to left,

737
01:18:19,720 --> 01:18:25,640
I'm not jumping somewhere else. So it's like, if we assume that this is a

738
01:18:27,000 --> 01:18:31,880
hypothesis, this hypothesis is that for Atari games, the state of space is continuous,

739
01:18:32,840 --> 01:18:37,240
but still the transition is deterministic. It's like going from one state to another,

740
01:18:37,240 --> 01:18:43,800
I mean, condition of current state and current action, I know which state I'm going to end up.

741
01:18:43,800 --> 01:18:51,080
So this is, it makes the life of the GANs easier, but the GANs is also able to handle the

742
01:18:51,080 --> 01:18:57,160
stochastic state transition. If I am at the state at some frame, if I choose action one action,

743
01:18:57,160 --> 01:19:01,800
if there is distribution of going to next state, GANs is able to handle that issue as well,

744
01:19:01,800 --> 01:19:07,800
but generally Monte Carlo tree search or opera confidence bound tree search, these algorithms,

745
01:19:07,800 --> 01:19:17,480
they do not require neither deterministic state transition nor finite state space. So they

746
01:19:17,480 --> 01:19:24,280
are able to handle continuous one as well. And so one thought that occurs to me is both the

747
01:19:24,280 --> 01:19:29,880
question as well as thinking through the implications of the question. You know, this process of

748
01:19:29,880 --> 01:19:37,720
generating this tree using GANs, what are the computational implications of this? And

749
01:19:38,680 --> 01:19:45,240
you know, assuming they're or imagining that they're significant, you know, I wonder if

750
01:19:45,800 --> 01:19:51,320
maybe it makes sense to do this like to bootstrap the deep reinforcement learning algorithm,

751
01:19:51,320 --> 01:19:58,280
but then once we gain more confidence, switch to something that's more like the approach we

752
01:19:58,280 --> 01:20:02,360
talked about before, you know, the Bayesian deep q networks or something like that.

753
01:20:02,360 --> 01:20:08,840
Sweet. You brought an interesting question. It's like, how how hard is to do this

754
01:20:09,560 --> 01:20:17,640
Monte Carlo tree search? How bad is the competition cost of doing this? The competition cost is bad.

755
01:20:19,080 --> 01:20:26,840
It makes the algorithm slow. Yeah, okay. Yeah. And but if I can get order of magnetic

756
01:20:26,840 --> 01:20:33,160
to sample complexity, then if I'm going to have a self-driving car, I would more compute to be

757
01:20:33,160 --> 01:20:42,360
able to not kill anyone. Okay. So here, the this GANs paper, that GANs work adds like a lot of

758
01:20:42,360 --> 01:20:48,360
competition cost. Like I think makes it like five times more compared to normal DQN,

759
01:20:49,320 --> 01:20:56,520
normal deep q network. But the thing is it's supposed to give you first of all better

760
01:20:56,520 --> 01:21:04,360
performance, second of all, like better sample complexity. Okay. So the general thing is we use

761
01:21:04,360 --> 01:21:11,720
Atari GANs as a test bed in order to see what our algorithms are doing. We are not using Atari

762
01:21:11,720 --> 01:21:18,040
GANs, some people they do, but I personally do not use Atari GAN, Atari GANs as a competition

763
01:21:18,040 --> 01:21:24,280
to say, hey, my numbers are better than yours. Or like my agent gets better scores than

764
01:21:24,280 --> 01:21:31,560
your agent. So mine is better. I'm not using like Atari GANs as to do this type of research,

765
01:21:31,560 --> 01:21:38,520
which is like probably you might not even call it research. I use so I try to develop algorithms

766
01:21:38,520 --> 01:21:42,680
and test them on Atari GANs to see what is their behavior. Yeah, sure. Generative

767
01:21:42,680 --> 01:21:49,800
adversaries research are really bad in the sense of competition costs, but they are supposed to

768
01:21:49,800 --> 01:21:56,280
give you better sample complexity and also better estimation of the value function.

769
01:21:56,920 --> 01:22:03,000
Can you define maybe formally sample complexity for me, because for you to say that

770
01:22:03,640 --> 01:22:09,160
something takes five times as long, but has better sample complexity to me,

771
01:22:10,280 --> 01:22:15,400
it sounds like a contradiction in terms. Oh, I see. So in reinforcement learning,

772
01:22:15,400 --> 01:22:20,920
there are like many components that you might be interested. One is like how much

773
01:22:21,560 --> 01:22:30,920
like wall clock hours you need to spend to find the good policy. So in this study, you don't care

774
01:22:30,920 --> 01:22:37,480
how many times you interact with the environment. You can have form of CPUs and GPUs,

775
01:22:37,480 --> 01:22:41,720
all of them are playing games and like they just give all the feedback to you and you come

776
01:22:41,720 --> 01:22:47,080
with a better algorithm. So if you just care about wall clock time, you don't care about

777
01:22:47,080 --> 01:22:53,320
the sample complexity. You just want to get to high performance in less number of hours

778
01:22:53,320 --> 01:22:59,640
of your wall clock. This study is about just engineering study, it's just about computation.

779
01:22:59,640 --> 01:23:06,040
So how much time you want to spend per day to be able to learn a better policy. This is an

780
01:23:06,040 --> 01:23:12,120
interesting line of study, but it's not my specialty and I'm not focusing on this one. There is another

781
01:23:12,120 --> 01:23:20,200
one is, so now there's another one is like says what objective function for DRL algorithms,

782
01:23:20,200 --> 01:23:30,600
I can define such that I get the best policy and also get the best, more reasonable policy

783
01:23:30,600 --> 01:23:37,560
and also reasonable value estimation. So there's another line of study that people have done a lot

784
01:23:37,560 --> 01:23:43,080
of research on it to come up with a better cost function or objective function. TQN uses the

785
01:23:43,080 --> 01:23:48,200
most naive objective function because the goal of deep-tune network paper was like hey,

786
01:23:48,200 --> 01:23:54,360
to show that it's feasible and it's possible to ask for human or be able to run something on

787
01:23:54,360 --> 01:24:01,560
a target. So there's not a line of research tries to come with a better objective function in order to

788
01:24:01,560 --> 01:24:07,080
estimate better policy and also estimate better value function. Even in this case, you don't care

789
01:24:07,080 --> 01:24:13,480
about the number of, in this setting, you don't care about, in the extreme case of this setting,

790
01:24:13,480 --> 01:24:18,360
you don't care about computation cost and also you don't care that much about the sample complexity.

791
01:24:18,360 --> 01:24:25,080
You are like, the people run the algorithms for like billions of time steps, okay? But there's

792
01:24:25,080 --> 01:24:30,760
another third line which is the most interesting part for me is like, can I get the same performance

793
01:24:30,760 --> 01:24:36,200
in stuff in one billion times of interaction with environment in like one million time steps?

794
01:24:37,080 --> 01:24:42,600
So one million times, so this is like literally I'm gonna, so if I have two kids, they play in the

795
01:24:42,600 --> 01:24:50,200
Atari's and they are playing that, that game, I just see which one is learning faster, but learning

796
01:24:50,200 --> 01:24:56,440
faster means that in the amount of hours they put to play game. If one kid is playing that game

797
01:24:56,440 --> 01:25:04,680
like like from 18 a morning to 9 at night and then does better than the other kid which plays

798
01:25:04,680 --> 01:25:12,040
it one hour per day, then I just, the way I compare the smallest of these two kids somehow might

799
01:25:12,040 --> 01:25:18,040
be like how many hours they play exactly the game, how many hours they interact with them with

800
01:25:18,040 --> 01:25:23,160
the game, right? How many times they press up and down? This is going to be the notion of something

801
01:25:23,160 --> 01:25:31,480
type of complexity. How many times my RL agent like interacted with environment in order to get to

802
01:25:31,480 --> 01:25:39,240
the optimal policy? So in other words, in this method you may take five times as long to figure

803
01:25:39,240 --> 01:25:46,440
out each step on a wall clock, but ultimately you're able to converge to a better policy

804
01:25:46,440 --> 01:25:54,440
taking less steps. Less steps in the game. In the game, right? Because actually the main goal of

805
01:25:54,440 --> 01:25:59,880
reinforcement learning is like when I deploy reinforcement learning for like autonomous vehicles,

806
01:25:59,880 --> 01:26:09,080
I would rather to use the whole AWS like compute to do not give anyone, right? So I'm in one

807
01:26:09,080 --> 01:26:15,160
content, it's for sure a complete compute, but main content is like developing a better algorithm

808
01:26:15,160 --> 01:26:22,840
which has less number of samples required to come with the optimal behavior. So this is like

809
01:26:22,840 --> 01:26:27,800
notion of sample complexity for this setting. I'm not using the exact theoretical notion because

810
01:26:27,800 --> 01:26:32,680
the exact theoretical notion has another meaning, but here somehow it's like how many samples

811
01:26:32,680 --> 01:26:40,200
I need to, if deep QNetwork solves a game in 200 minutes samples, I mean, get some performance

812
01:26:40,200 --> 01:26:46,680
in 200 minutes samples, am I able to reach the same performance in less than one million time

813
01:26:46,680 --> 01:26:53,160
steps and less than one million samples? The answer in the Bayesian deep QNet for paper is like

814
01:26:53,160 --> 01:27:01,240
custom games yet. You can reduce the number of interaction you need to make with the environment

815
01:27:01,240 --> 01:27:07,720
100 times less in order to get to the same performance as deep QNetwork, getting after 200

816
01:27:07,720 --> 01:27:13,160
minutes samples. This is a notion of sample complexity and this is the main part of theory that

817
01:27:13,160 --> 01:27:19,960
I'm also seeing in reinforcement learning in theory land that we mainly care about to reduce

818
01:27:19,960 --> 01:27:26,440
the number of samples we need in order to get to the nice and useful and reasonable performance.

819
01:27:27,080 --> 01:27:34,680
And so along the same lines, can you directly compare the sample efficiency of, you said,

820
01:27:34,680 --> 01:27:44,120
you know, deep Q might be 2 billion, Bayesian, you might be 1 billion. If you throw in the GANs or

821
01:27:44,120 --> 01:27:51,320
GATs approach, can you compare it directly in that same kind of metric? Yeah, of course, I can

822
01:27:51,320 --> 01:27:58,360
definitely compare it in the same metric. I have, like, I have compared it in just for one game

823
01:27:58,360 --> 01:28:05,960
in the paper, but as I said, seems like the composition complexity of GATs is a bit high,

824
01:28:05,960 --> 01:28:14,680
it's a bit beyond the power of academic research labs to do massive experiments. So if I was a

825
01:28:14,680 --> 01:28:20,120
deep mind, I would definitely have all the game. But since I am, like, using my advisors,

826
01:28:21,160 --> 01:28:28,760
AWS credit, probably I rather not burn the whole, the GPU clusters we have just for this

827
01:28:28,760 --> 01:28:36,440
board. So I just try to show a few games. Before those few games, like, how did it compare to the

828
01:28:36,440 --> 01:28:43,560
other couple of methods? Compared to deep Q network, for that game we tried, it reduced the sample

829
01:28:43,560 --> 01:28:49,880
complexity by half, like it converts to the same performance is like half of the samples required

830
01:28:49,880 --> 01:28:57,000
for deep Q network. But in order to make a empirical claim, definitely we need to try more than

831
01:28:57,000 --> 01:29:03,080
for sure one game. Right. So we are trying to come up with more experiments for more games

832
01:29:03,640 --> 01:29:09,320
and to come up with better analysis, better empirical analysis. We are not going to compare against

833
01:29:09,960 --> 01:29:19,400
EDQM or Bayesian deep Q network because the gap is exploring deep Q network inside of it.

834
01:29:19,400 --> 01:29:26,200
So because I have deep Q network and I'm building something on the top of deep Q network,

835
01:29:26,200 --> 01:29:31,240
I'm going to compare against deep Q network. I could use Bayesian deep Q network and use

836
01:29:31,240 --> 01:29:37,800
Gats on the top of that which is totally like this one line change in the code. I could be in the Gats

837
01:29:37,800 --> 01:29:44,040
on the top Bayesian deep Q network and compare against Bayesian deep Q network. This comparison is

838
01:29:45,000 --> 01:29:51,800
meaningful. But if I compare Gats with deep Q network and compare it with the Bayesian one,

839
01:29:51,800 --> 01:29:56,600
it's not that reasonable. The comparison is because we are comparing to somehow orthogonal

840
01:29:56,600 --> 01:30:01,800
effects. You gave the example. You said that like for a given game, you would expect that

841
01:30:02,520 --> 01:30:09,640
the Bayesian deep Q networks is roughly half the number of samples as regular deep Q networks.

842
01:30:09,640 --> 01:30:19,080
No, no, it's like 100 times some games better. So 100 X better. And so it was this that is the Gats

843
01:30:19,080 --> 01:30:24,520
that was half. Yeah, for one game. So for one, right, for the one game that you

844
01:30:26,040 --> 01:30:34,280
but I'm just saying something that Bayesian deep Q network just we studied to come with a

845
01:30:34,280 --> 01:30:41,000
better exploration strategy for deep reinforcement learning problem. At Gats, we are coming,

846
01:30:41,000 --> 01:30:48,120
we came up with the algorithm to reduce the error and bias in the learned value function. So

847
01:30:48,120 --> 01:30:53,480
these are kind of orthogonal effects. So the thing is I'm not going to compare these two together

848
01:30:53,480 --> 01:31:00,520
because these are two studies, not two orthogonal things. And I personally don't care that much

849
01:31:00,520 --> 01:31:05,240
which one does better because both of them can become blind together and come with new algorithm.

850
01:31:06,040 --> 01:31:10,680
Yeah, these are like we are in deep reinforcement learning. We are like far from claiming that we

851
01:31:10,680 --> 01:31:16,520
can use them in real world. So what we do, we are now going to hold the like majority of the

852
01:31:16,520 --> 01:31:23,560
fields. You are trying to study and come up with better and better algorithms. And these are

853
01:31:23,560 --> 01:31:32,520
algorithms are like studying because reinforcement learning like M4 is too big. And we need to do a

854
01:31:32,520 --> 01:31:39,000
lot of study to come up with to study different effects of different components in the setting.

855
01:31:39,000 --> 01:31:44,920
So we are not competing like if like in the vision community, we have ImageNet. Everyone

856
01:31:44,920 --> 01:31:53,720
tries on ImageNet and compare the results of one algorithm to another one. But here we are not doing,

857
01:31:53,720 --> 01:32:00,440
I mean many people they do compare their numbers but I personally do not do compare my numbers to say

858
01:32:00,440 --> 01:32:09,960
this is better than the other. We are just studying the effects of different components in the

859
01:32:09,960 --> 01:32:17,160
environment. And we are in the baby stage of like reinforcement learning. We are like getting

860
01:32:17,160 --> 01:32:25,480
things to see what what what each change would do. We have very very far from to get to real

861
01:32:25,480 --> 01:32:29,960
competition to say these algorithms better than the other ones. So in other words, you know,

862
01:32:29,960 --> 01:32:37,000
it's just kind of exploring ideas about you know what levers we even have available to us to tweak

863
01:32:37,000 --> 01:32:42,360
if we cared about maximizing performance. But we're so early on there's really nothing to maximize

864
01:32:42,360 --> 01:32:48,280
performance against. There's no ImageNet or something that we're you know someone who's building

865
01:32:48,280 --> 01:32:53,000
a self-driving car, you know, they might want to take all of these different approaches and

866
01:32:53,000 --> 01:32:57,000
you know, apply them together. But that's a lot of work and that's their issue, right?

867
01:32:57,000 --> 01:33:01,880
Yeah, they're going to combine everything together. You can imagine like like coming up with

868
01:33:01,880 --> 01:33:09,240
better like exploration and strategy like console sampling. It's obvious that someone should have

869
01:33:09,240 --> 01:33:15,080
tried this one like five years ago, right? But no one has tried it because we are like very very

870
01:33:15,080 --> 01:33:19,640
like this is a fundamental component in the reinforcement learning algorithms. Like the exploration

871
01:33:19,640 --> 01:33:26,120
is like one of the most important and actually I can say even main component of the reinforcement

872
01:33:26,120 --> 01:33:31,160
learning algorithms. But no one has tried this one. Why? Because we are in the early stages. We are

873
01:33:31,160 --> 01:33:38,840
still don't know and don't get to know like what are the fundamental components of the algorithm

874
01:33:38,840 --> 01:33:45,080
is. Like for ImageNet, we know mainly that we need to have this convolution layers. We need to have

875
01:33:46,040 --> 01:33:53,560
like softmax outputs. We can use cross entropy loss. These are the common thing that everyone uses

876
01:33:53,560 --> 01:33:58,440
and it works very well. But in reinforcement learning, we don't have this all components

877
01:33:58,440 --> 01:34:04,280
all together. We don't know what is the best way of doing exploration and exploration together.

878
01:34:04,280 --> 01:34:09,400
We don't know what is the best function approximation we should use. We don't know what is the best

879
01:34:09,400 --> 01:34:15,240
memory we should use. We don't know what we don't expect. We are like really really far from

880
01:34:15,240 --> 01:34:21,560
making. So we're just reading that even before the baby stage, we are not even got born. We were not

881
01:34:21,560 --> 01:34:28,840
born. Awesome. Well, Camyor, you've been very, very generous with your time. Thanks so much for

882
01:34:28,840 --> 01:34:35,320
taking the time to chat with you about all this stuff. It was a lot of fun and I think folks will

883
01:34:35,320 --> 01:34:40,440
enjoy this and learn a lot. I really appreciate it. Awesome. Thank you. Oh, thank you. Have a great day.

884
01:34:43,960 --> 01:34:49,160
All right, everyone. That's our show for today. For more information on Camyor or any of the

885
01:34:49,160 --> 01:34:55,560
topics covered in this episode, head over to twimmolai.com slash talk slash 177.

886
01:34:56,600 --> 01:35:01,000
If you're a fan of the podcast, we'd like to encourage you to visit your Apple or Google

887
01:35:01,000 --> 01:35:07,000
podcast app and leave us a five star rating and review. Your reviews help inspire us to create

888
01:35:07,000 --> 01:35:14,040
more and better content and they help new listeners find the show. As always, thanks so much for listening

889
01:35:14,040 --> 01:35:22,280
and catch you next time.

