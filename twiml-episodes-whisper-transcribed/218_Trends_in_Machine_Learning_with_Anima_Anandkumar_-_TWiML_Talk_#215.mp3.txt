Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington to close out 2018 and open the new year we're excited
to present to you our first ever AI rewind series.
In this series I interview friends of the show for their perspectives on the key developments
of 2018 as well as a look ahead at the year to come.
We'll cover a few key categories this year, namely computer vision, natural language
processing, deep learning, machine learning and reinforcement learning.
Of course we realize that there are many more possible categories than these, that there's
a ton of overlap between these topics and that no single interview could hope to cover
everything important in any of these areas.
Nonetheless we're pleased to present these talks and invite you to share your own perspectives
by commenting on the series page at twimbleai.com slash rewind 18.
In this episode of our AI rewind series we're back with Anima Anand Kumar, brand professor
at Caltech and now director of machine learning research at NVIDIA to discuss her take on
trends in the broader machine learning field in 2018 and beyond.
In our conversation we cover not only technical breakthroughs in a field but also those around
inclusivity and diversity.
Enjoy.
All right everyone, I am on the line with Anima Anand Kumar, Anima is the brand professor
at Caltech in the CMS department and as of a couple of months ago the director of machine
learning research at NVIDIA as well as being a good friend of the show you can hear her
previous conversation at twimble talk 142, Anima welcome back to this week of machine learning
and AI.
Thanks a lot Sam, it's great to be back the past few months have been so exciting and so
happy to see you do this show reviewing the year.
It is going to be a fun conversation, I am sure, I am sure.
Before we get too far in maybe you can kind of share a little bit about what you're up
to at NVIDIA.
Yeah, it's been an exciting couple of months at NVIDIA as we have really so many new frameworks
and we have many new researchers and engineers and others join us and my role is director
of machine learning research there and that means I want to focus on core algorithmic
problems, how do we optimize neural networks in a principled way or generalize, think about
reinforcement learning, generative modeling, so a broad class of machine learning questions
but focusing on the core algorithms, analyzing them, running careful scientific studies and
at the same time connecting with all the really interesting applications that we are seeing
today.
So we're also speaking right on the tail end of NURP's, NURP's being a new name for the
neural information processing systems conference, a new name which you helped initiate.
Maybe can you maybe share a little bit about your experience at NURP's this year as well
as your thoughts on the name change?
Yes, I'm so happy to see NURP's take important stance on diversity and inclusion.
In addition to having technical advancements, this is so critical because I feel like
we are at this cusp where we can either have a democratization of AI and have diversity
and creativity from all the groups coming in or we could have it closed off to only a certain
kind of individuals and the latter would be a sad thing for the world and for AI.
So I'm happy this year that there was so much focus not only on diversity and inclusion
but also on societal impacts, AI fairness, ethics and we have people coming from those expertise
with those expertise into the conference and that just makes it such an amazing place.
I think it's changed for the better.
There is so much awareness now about the issues affecting the underrepresented communities
and more allies coming to these diversity events, right?
It's not an event that's only for those groups but it's more of understanding the issues
those groups face and how the rest of the community can help bring in people from diverse
backgrounds and to me the name change, the whole experience was part of this push for
diversity and inclusion when we saw last year there was many issues where the name was
used as a way to make fun and have jokes that followed on sexual harassment, that is
a problem and to me some people may argue it's just a name or it's just a joke, right?
But there are deeper issues that this name change helped us explore, I mean I was personally
very surprised at the amount of trolling I received when I spoke up about it and that's
why so many women cannot speak up about it because they feel threatened and so there
are broader issues of how do we make this a safe and an inclusive environment for everybody
and I'm so happy that there are so many male allies who strive to understand the issues
better, the kept asking me throughout this conference, how do we make this better for
everybody, what can I do, how do I help, right?
And that's why this has been just a unique experience, I have never experienced a conference
like this before.
Yeah, it was a really amazing conference for me as well, on a couple of levels first I
think there's been kind of a blooming of these different interest groups at the conference.
So women and machine learning has been around for quite some time and has kind of met
in conjunction with the conference but last year for the first time I attended the black
and AI workshop which was the first time for that workshop the second time this year,
this year there was added Latinx and AI as well as several others.
I would add queer in AI and that's there was also the town hall on diversity and inclusion
where all these groups came together and discussed how there were unique problems for each groups
but also common ones right and then there is the whole issue of intersectionality that
a person may not just belong to one group but several groups and how do we you know think
about this issue and for a technical community so much of this is new and we are having
experts from other areas you know really educators on these issues.
Yeah, what I thought was really interesting was the way at least in the black and AI workshop
both the technical issues as well as these intersectionality issues were all kind of weaved
together.
I don't know it just you know as an African American in technology I'm often the only person
in the room you know for much of my career and to you know be in a room where you know
there are so many not only African Americans but you know Africans and black folks from
all over the world talking about cutting edge machine learning in AI as well as the broader
issues you know surrounding not just our communities but other communities is you know I
left really inspired and then beyond that to see in the broader Nureps conference the extent
to which you know some of these issues like not just diversity inclusion but fairness
accountability transparency you know they really took a kind of a top line played a top
line role I think in this year's conference unlike last year or I didn't notice it to
that extent last year so it's very very nice to see.
I agree yes and in the way I've been meeting lawyers policy makers you know people with
expertise in ethics and you know you couldn't imagine Nureps having these kind of people
you know attend a few years ago and that's I think great for the field.
So that was certainly some an important development for 2018 and so maybe we can use that as
a jumping off point to talk about some of the things on the research side that you are
most that most culture interests in the in the past year.
Sam you know there are being a lot of developments so I'll just pick a few doesn't mean that
you know that the others are not important there is just so vast right and that's what's
exciting to be an AI researcher today because we have so many domains and so many new for
a kind of perspectives coming into AI. So some of the highlights personally for me one paper
I would pick is this video to video synthesis that appeared here at Nureps with a group of
NVIDIA researchers and what's really novel here is how to take this idea of image to image
synthesis to videos right so over the past few years synthesizing images through GANS has
been very popular and then the next step was if you already have an image can you transform it
in a certain way to output another image right like the idea of cycle GANS and other frameworks
to do that well but when you think about the videos this is much more challenging because you
have to maintain temporal coherence and you have to generate it for that whole length of time
in a nice smooth manner and so there were many new advances in this paper the main one I would
call out is the spatial temporal adversarial objectives so it's not just the space it's not
just the time but the two have to come together and this achieves good quality synthesis as well
as coherence so you need to think of these multiple objectives being achieved here and they
show that it's possible to generate two k resolution videos of street scenes up to 30 seconds long
using only the input semantic segmentation maps and these segmentation maps were obtained from
a renderer right so the idea is now you can think that you know you only get very quick simple
inputs from a renderer and then you can actually use the GAN or learned frameworks to generate
the output videos well and so there's a lot of promise in this area videos are always much harder
than images in so many ways and I'm happy to see this development. So in their approach did they
use it sounded like you're saying they used just multiple objectives as opposed to a second
adversary or critic were they like doing adversarial in multiple dimensions time and space or did
they somehow combine those two objectives with the same adversary. It was a single adversary but
it's spatio temporal objective so you combine both of them together. You mentioned something about
a renderer can you elaborate on that? So in this case the renderer was used only to get the
semantic segmentation maps right like you know what are the what is the area of the pixels that
has the car what is the area of the pixel that has the background right and then the whole filling in
was done using the GAN framework and you could think in future you could have like a mix of
renderer doing some of the rendering task and then GAN helping it and kind of having
combines free system. There's been so much interesting work happening in the that happened in
the GAN space this year I remember another one that was really interesting was the photo realistic
images I think also from Nvidia researchers that really pushed the bar very far for what these
GAN these generated images could look like. That's right that's right there have been progressive
GANs there has been the big GAN paper right like how do we really push to very high resolution
and ultra realistic images and I think these are exciting developments and applied deep learning
and can be applied in a lot of interesting areas. Do you see GANs being applied beyond the image
domain to other types of information? I think you know the idea of having a competitive
optimization right at the core of it that is the generator and discriminator but you could think
of other kinds of competition that's applicable in so many problems so many if you want robustness
you now have this competition if you have multi agent systems you know you this framework
could be applicable so I would say at the core algorithmic side yes there are a lot of connections.
So video to video synthesis what else has caught your eye this year?
Yeah so on more the theoretical front I will talk about some of the works we've been doing
looking from a different perspective right so I just said I've been excited about GANs
and GANs are great for now generating these photo realistic images but what about applications
where it's not good right so so water applications where GAN is not that easy to use and that's
for semi supervised learning and other frameworks where you need to use the
generator process to help a supervised task and in this case these are two separate systems the
GAN is separate from your convolutional neural network which does the prediction and it's very
messy to try to combine both of them instead we came up with a framework called neural rendering
model that tries to build a generator process right into your convolutional network rather than having
a separate system and its goal is not to generate good looking images but to ask what are the relevant
hidden variables and you know what is the relevant variability I need to capture in the data
into this convolutional network such that I can do semi supervised learning well and I can also
regularize my supervised learning in a nice way so intuitively what it's doing is it's taking
this convolutional network which is a feed forward system and tries to reverse it but then there
is information loss when you're going through this network so you're not able to perfectly
reverse the system instead what it's doing is it's saying what is this uncertainty in reversing
the process can I capture that well can I capture it across all the scales and that gives me
measure of variability and uncertainty of each image and we get state of the art results semi
supervised learning in this case what are the evaluation criteria or what are the experiments that
you're running and what are you comparing it to yes in semi supervised learning the task is you
have both labeled and unlabeled data right and you can do that with benchmarks like C4
ImageNet there are a few other data sets and the key is you have very little amount of label data
and a lot of unlabeled samples can you still get good accuracy and that's why this is a much harder
task on a classifier that's right on the on the held out test set right where you have to go
unlabeled that but you only have few labeled examples in your training data set
how does the performance compare between supervised and semi supervised how much of the
you know gap have we've been able to make up you know for not having the label training data
I would say this is still very much an unsolved problem you know the gap is significant
especially when you think of ImageNet scale or on a large number of categories right and
I think that's why a lot of new works will be looking into this in ultimately in many other
domains we cannot have so many labeled examples and how do we use all the unlabeled data to help
the supervised task I would say this is a hard problem to tackle and so the this kind of reverse
past that you were describing that is trying to capture the variability in these images how does
that variability lead to and lead to performance on the forward pass side so I would say a primary
challenge in machine learning is capturing uncertainty well right so if you look at the
softmax at the very end of these convolutional networks they don't do a good job and their
series of works are saying why and try to fix that right but instead we are saying you don't
try to capture the uncertainty only in the last layer you want to capture it at all layers
okay in each layer the processing is progressively losing information and at what rate is this
happening for instance you are distinguishing a cat and a dog at which layer is it like more
confusing than the others right and this is a hard problem and we are filing away to automatically
capture that across the scales all right so another interesting one what's what's next on your list so
another paper that's coming out of another paper that came out of my group at Celtic has been
on Sinus TD which looks at gradient compression so you know right now when most of the training
happens in a centralized way right like if there are devices like IoT devices they send to the
cloud all the machine learning happens there and then the data sends back but you can think in the
future there'll be more of the demand for learning on the edge especially if you have a lot of
video streams it is becoming impractical to send everything to the cloud you just don't have
enough bandwidth to do that and so in these settings in low bandwidth distributed training settings
how do we do good machine learning and in this case you need to compress the gradients so each
device does its own pool of data computes the gradients but instead of sending it in full precision
in our paper what we asked was if you only send the sign of the gradient right so you it appears
you're losing a lot of information if you send only the sign what is the impact can we study that
and what we did was to study both from the theoretical perspective as well as the empirical one
and bridge this gap and that's why I'm emphasizing this paper because we need more that bridge the
theory and practice together and the theoretical side what we asked was what is the rate of convergence
on the training data compared to the full precision SGD right first of all you know is it even
clear that taking just the sign will converge will it go to zero training error and it turns out yes
you can get zero training error and that's because intuitively when the gradient is large even just
taking the sign means that you're descending in the right direction and and so we in fact also
argue that there are certain quantities like density of gradients and smoothness and other
properties under which we can expect it to have the same rate of convergence as the full precision
SGD and then we went to empirical studies and asked okay now can we see that happen and we saw this
converge you know very fast and we also built efficient compression algorithms so that really
could increase the throughput and get the gains of compressed gradients and and what we found was on
the generalization side we were getting almost similar accuracies as the full precision SGD
right and that's why this interface between systems and machine learning is so hard to
characterize because there can be a lot of free lunches we are compressing gradients but still
maintaining accuracy and we can also now ask theoretically why you know this is the case
and I would love to see more works like this that try to bridge theory and practice and systems
and machine learning. You mentioned building compression I don't recall if you said compression
algorithm or some compression aspect of this can you elaborate on that the sign is easy to get
what what do you need to specifically do to to kind of support this model.
That's right Sam you know I wasn't talking of a sophisticated compression algorithm
but more the details of you know so if you now take the sign how do you like kind of combine across
all the parameters and use the right CUDA primitives to make this efficient right like you
need to also worry about you know when you know in what form you're keeping this and how you're
broadcasting it into the system. Okay okay so more the distributed components of it and how
everything gets pulled together. That's right I mean the system's details but it's important to
also be about that right and many papers stop at either doing the theoretical analysis or like
asking what happens to accuracy but not coming up with efficient implementations that also
get the hardware gains. Okay great so what's next? Yeah so the next paper is one that's a negative
result so this is coming again from my group at Caltech and why I want to emphasize it is
you know it's very hard to publish negative results right and people don't want to talk about
negative results and you know we tried for a long time to of course get a positive result that
wasn't our goal in the beginning to write a negative results paper and it also took me a lot of
convincing my students to say no you know it's okay we've done a lot of work and this is the
honest results let's get this out there and in this case what we were trying to do was to combine
model-based and model-free RL reinforcement learning so you know the popular AlphaGo and other
examples what you have is not just the deep Q network but you also have the Monte Carlo
tree search right so meaning you have this tree of all possible actions and you're growing this
to a large tree depth in the case of AlphaGo this tree was of depth in the hundreds and so you know
these trees grow exponentially so you can imagine this is a very at the end at the number of leaves
is very huge and this takes a lot of time more importantly this requires all these environmental
interactions right like you need to ask the environment what happens under each action and keep
growing this tree and in the real world this would be impossible like we could never roll out to
such a large tree depth and if you're trying to simulate a more realistic real world example
you could possibly do this only for short tree depths right because for the longer ones there
would be two higher error propagation so that's the other motivation to ask what we can do with
short tree depths so when we started out thinking about the paper our idea was you know we will
use a GAN-LEC framework to learn the dynamics and predict it for a few time steps and then like
two tree search and DQN combinations and argue that this will give us a good result that was our
initial idea because then that's a motivation of like oh you can now take this to a more real
ultimately a realistic example where you can simulate and instead of interacting with the real
environment you do this in simulation and we were really worried about the model mismatch between
what's being simulated and the real world right and those issues but as we went through it it
turned out to be a very different project and that's where you know I encourage other researchers
to also adapt right like you know we may go with an original idea and if that doesn't work out
there may be so many other interesting things you find as you go through the experiments and in
this case what we found was for the class of Atari games it was actually quite easy to simulate them
yes they are simple images but we were still surprised in terms of forecasting up to like 10
time steps you know they did almost perfectly like you couldn't tell the real image from the
generated one you know visually you couldn't tell them apart and so okay that was easy so we thought
the gun part would be hard but that was easy and what was now surprising was even with this perfect
simulation we were not able to get good results so in fact I'll take it back in the first game the
pong which is a simple game we got really good results we got you know we showed that with just
half the number of samples you could like beat the baseline dqn or the ddqn network and so that's
promising now we're like okay we'll scale up to more games and we will write the paper right
but as we started doing these experiments and it was performing worse than ddqn it was performing
worse than random policy and we're like okay what's going on you know like is there a bug
come in like you know what else can we do it's so many two weeks the students did and you know
saw Kamiar and Brandon who were mainly doing the experiments and still it did not work out
and so then we took a step back and asked okay why is this the case so it turns out that if you're
using these short tree depths with your tqn model it can be worse because you're not getting the
negative experiences very well right so so think of it the analogy I would give is like a helicopter
parent like you know not getting these kids to make mistakes always like hovering around
right hovering about the kids and making sure they're okay and like kind of protecting them
so these short tree depth the Monte Carlo tree search is doing the same
let's say this agent is near a fire where you know if you went into the fire you'd get killed you'd
get a very large negative reward and that would happen with the dqn because there is no protection
but it would learn okay I should avoid the fire because I'll get killed but you have the
Monte Carlo tree search it'll look out a little bit into the future and realize oh no I shouldn't
go to the fire because I will get killed so I will avoid the fire right so it takes a long time
just avoiding the fire never getting killed but not making good progress either and that's why
you need large tree depths because then you have a global view right out then you can do really good
decisions but having this myopic short tree depth actually hurts you more than helps you
are you planning to extend this based on the success that you had with the prediction part
yeah so I mean certainly on the simulation side you know how we can
simulate the dynamics in different applications you know even real applications involving control
systems that we are very interested in and also you know now the question is how can we fix this
right if it's not this classical Monte Carlo tree search with dqn what are other reinforcement
learning algorithms that could help us combine the model based with the model free reinforcement
learning cool so I think you had like seven papers we've got through four of them what else
what else have you found interesting this year yes the other two are more I would say the
applications of AI to the sciences okay this is gonna this has to be the future right they we
need to go and impact the scientific questions you know the very fundamental discoveries I think
that's really important and I'm happy to see two papers that I think have been great one of them
is called the phase link it's a deep learning approach to seismic phase association and this is
from my colleague isong you at Caltech who is also a machine learning professor and other seismic
researchers at Caltech and here the goal was to ask can we link the unknown phases from different
sensors for a common earthquake apparently like these phases are not known and this is something
you need to do when you have so many different sensors that are trying to sense the same common
earthquake and in this case it's challenging because you don't know the you know how many sources
like is it just one earthquake shutter or is it happening in different places and these events
could be happening frequently in time so it's not just one shutter but multiple ones and there's
overlap and how do we disentangle all this well right and the challenge here was that there is
little real data right so I mean I think at Caltech we have the longest recordings of seismic
activities at least in the United States goes back almost a century but this is still not enough
if you want to come up with a very good model and especially for deep learning which is data hungry
so instead what the researchers did was a simple and an effective approach where they simulated
synthetic sequences so there's something known as the p wave and the s wave these are different
kind of waves in earthquakes and so they simulated both these kinds they arrival times were generated
using just a very simple one-dimensional velocity model so no this was nothing complicated you know
we're always when we are generating synthetic data we are worried is this realistic enough
but what was surprising in this case was this simple synthetic model when tens of millions
were sequences were used combined with real data was very effective and to me the intuition is
this is encoding all the physics of the waves in an effective way right so the basic physics is
encoded well and then with some amount of real data any additional variations it's able to also
capture and you know so to me this is a good example of going into a different field understanding
the problems in that domain well through the domain experts and you know coming up with the
fairly simple machine learning solutions but that have a big practical impact I've certainly
seen in my conversations with folks on the applied side that they can accomplish a lot with
some fairly basic tools that the you know ML and AI have provided so with with deep learning
there are tons of applications that can dramatically simplify some of some of their previous workflows
as researchers that aren't particularly complex that are you know fairly off the self applications
of you know for example image classification or you know deep learning classifiers or predictors
so so I agree to the extent that the machine learning principle is simple but at the same time
this required close collaboration between the machine learning expert and the domain experts
right like it's not a plug-and-play approach it and I believe in most applications it won't be
right and just taking the standard deep learning you usually don't have enough data and you have to
see how to augment the data or how you should do semi-supervised learning well and that's why
you know this is usually requires close collaboration and not just the domain experts using
some of the shelf machine learning tools yeah that's fair and I think that you know there's often
that the the challenges come up in the the data side getting enough labeled data or
coming up with tricks to improve sample efficiency or I've also talked to folks that have to
kind of customize their objective functions to do interesting things but it's not like they're
using you know exotic things like GANs very often at this point that's right and I would always
suggest people to start with the simplest things you know I was in this mentoring session with
for reinforcement learning the topic was reinforcement learning and there were many people coming
from the applied side and asking when they should use reinforcement learning and my answer was
only when someone puts a gun to your head I mean if you run out of everything else and there is
no nothing else that works then try reinforcement learning because reinforcement learning is expensive
right so first one asks you know how much of knowledge do I already have what kind of pre-trained
models are effective and you know do I really need to explore here or do I have enough data to
begin with I mean these are all things you want to think about rather than blindly applying
reinforcement learning and you mentioned the next paper on your list is is kind of in similar vein
an application on the science side that's right and also on the system side so this is the paper
on exescal deep learning for climate analytics and this one the Gordon Bell prize this year
at the supercomputing conference and to me this is very exciting because for the first time this
broke the Excel ops barrier right so to me it's mind boggling the scale of computations we're
able to do now this had the scaling up to 27,000 GPUs and this is on the new summit supercomputer
and traditionally supercomputing has been very CPU based and now taking this to the next level
with GPUs is very exciting and the other question is now what kinds of applications can we do on
these supercomputing traditionally it has been HPC you know kind of climate modeling so there are
like you know these known climate models and how do we simulate this right but in this case the
unique thing was to combine those HPC models with deep learning architectures specifically they
applied segmentation architectures to climate datasets and they achieved state-of-the-art
weather pattern masks and they had both the FP16 course and the traditional HPC operations
right so this kind of mixed precision and mixed operations of both HPC and deep learning together
I think will be the future in the sciences it's interesting it's my understanding from conversations
that for quite a long time maybe I don't know if it's machine learning or GPU based architectures
weren't really appreciated in the supercomputing community and for example I think the anecdote
that I heard was that that was really the reason why Jensen and Nvidia created GTC because they couldn't
you know the researchers that were working on GPUs couldn't get any papers accepted at super
computing you know that that community is hard to break into so many other I can go into the
sociological factors right and that's because you know they are in the big labs the big national
labs and the people who trained on those traditional supercomputing for decades and they don't
want to move away so I still see these issues but this is where we need like especially the
anger people to say like okay no there is so many new developments that this is inevitable
right like we have to move it from those traditional supercomputing and same with machine learning
they you know demanded these high precision operations you know 64-bit operations right and that's
like no in machine learning everything is 32 bits or lower and that's because what you know in
machine learning the idea is what we are computing anyways inherently uncertain and we don't need
high precision whereas in traditional HPC the idea is no will precisely maintain the uncertainty
quantification and in some cases this is super important right there are certain cases where this
is needed but in many others the model itself they're using has a big mismatch right like if you're
using a linear model and you want this high precision you could argue is this really needed and so
what I believe is we should revisit and ask when do we need these very high precision computations
and are there other parts of the pipeline where we could relax now so maybe let's switch gears
a little bit and talk a little bit about your kind of your predictions for 2019 and more broadly
the the near future in machine learning and AI and in particular I want to start with you know
what are you most excited about when you kind of think about where we are today all that you
know we've accomplished in 2018 the hand of the struggles of 2018 that you're intimately familiar with
you know what what are you most excited about going forward or looking forward
I think 2019 will be a great year for really expanding the horizons and the way we think of
machine learning right I think there'll be hopefully less of leaderboard chasing and myopic gains
in deep learning and more serious and more of a holistic approach and integration with the other
sciences with the humanities in a stronger way you know going beyond standards supervised learning
you know we know very well that the you know scenarios where the train machine learning models
are used will differ so much from the training data can we come up with new frameworks to you
know detect these domain shifts you know especially in safety critical applications can we
come up with ways to learn online you know as we are getting this new non stationary data
right can we do lifelong learning and never ending learning and keep getting new knowledge
and when we go to these new domains can we infuse that domain knowledge for instance can we
ask what is the knowledge of physics we can already build in if especially we want to do
learning on our robots or drones you know there's so much that's already the control
so people have developed right we can't throw all that away there is no way instead there
you know the question of do we bring in learning in a principled way because for control systems for
instance you want stability you cannot throw that away right whereas neural networks by themselves
are not stable how do we enable that and we had some initial work this year on enabling a stable
landing of drones with stability guarantees so those kinds of collaborations across different
expertise and different groups that's what I hope to see more in 2019 I'm curious you mentioned
one of the papers that you highlighted was reinforcement learning and of course you your comment
about it being a tool of last resort do you expect to see that change much in 2019 how close do you
think we are to the point where RL becomes a useful kind of general purpose tool for folks
not in the research community I like that somehow you know put me at a spot because
you know yes I work in reinforcement learning to clarify you know I have a great student on the
market come here as is are they who is an amazing reinforcement learning work and we certainly
should do the principled approaches ask what would reinforcement learning do in different scenarios
what are the algorithms and how can we analyze them and when it come to the practical setting my
comment was don't blindly apply right is it what you need here is a reinforcement learning
and to me some of the most interesting work will be in the robotics side so I talked about my
Celtic project for landing of drones and autonomous flying of drones so this is where we need
the domain experts but we also need precise measurements so in this case in the cast facility
there we have a unique wind testing facility for drones where we have 1300 programmable
pans that can generate all kinds of wind conditions and we can get precise data on how the drone
would react right so now armed with this data we can ask what kinds of reinforcement learning
and machine learning we can do very well because without this you know there is no hope of
trying to improve these systems and so data collection is important and when you have less of
this real data that's when we need very physically precise simulations and this is where at NVIDIA
I believe we have this unique capabilities to do simulations extremely well so there are two
frameworks that is flex and physics physics is a framework we made it open source recently earlier
this week here at new reps which is simulating real world physical behavior very precisely
right like think about how if you have a damage to a building or you have like movement of the
cloth they have how do we make them realistic and flex is a system that is also you know high
fidelity GPU based physics simulator and it simulates these rigid body dynamics very well
and the detail fox at NVIDIA and his group has been using flex very effectively to close the gap
from simulation to real so the main idea is you may not even know what to simulate in the
simulations right you need the real world experience rather than trying to manually tune what to
simulate what they're doing now is doing a little bit of a real world rollout and based on that
experience learning what to simulate next and so going back and forth between simulation and
reality and I believe in 2019 we'll see a lot more of such simple real applications and that'll
lead to more realistic robotic impact more realistic impact in robotics do you have some specific
predictions for kind of what you expect to see in your area and in ml more broadly in the next year or
so so the main one is moving beyond just supervised learning right so semi supervised learning
asking what are the relevant generator modeling how do I capture uncertainty in a relevant way
you know the base neural networks how do I design them in the right way and through that how do I
do better active learning how do I do domain adaptation better how do I scale up the algorithms
while understanding these tradeoffs between systems and machine learning I think we'll see a lot
more of those kind of interdisciplinary questions as well as the core machine learning questions
so any parting thoughts before we finish up no I think I should say like 2018 has
been personally a challenging year because you know I have had I think the time to
introspect more on the societal impacts of machine learning and how we can improve diversity
and inclusion in machine learning and I think the year has now ending in a good note especially
at new reps as I'm seeing so much overwhelming support and hopefully we can improve that and with
that come up with new creative approaches to machine learning hopefully less of the media hype
more of serious podcasts such as what I'm doing right now and yeah and I've been very
vocal in dispelling some of the hype and the myths and I hope to continue to do that fantastic
well Anima thank you so much for all of your work in the space and for taking the time to chat
with us about it thanks a lot Sam this is a pleasure all right everyone that's our show for today
for more information on Anima or any of the topics covered in this episode visit twimmalei.com
slash talks slash 215 you can also follow along with our AI rewind 2018 series at twimmalei.com slash
rewind 18 as always thanks so much for listening and catch you next time happy holidays
