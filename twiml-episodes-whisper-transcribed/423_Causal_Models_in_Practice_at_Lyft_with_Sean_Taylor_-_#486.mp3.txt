All right, everyone. I am here with Sean Taylor. Sean is a staff data scientist at lift working on ride share labs. Sean, welcome to the 21 AI podcast. Thanks Sam. Happy to be here.
I'm super excited to chat with you. It's been a long time in the works and really looking forward to our conversation.
I like to get these interviews started by having you share a bit about your background and introduce yourself to our community. How did you get started working in data science?
Yeah, thrilled to be here. Always fun to reminisce about how you ended up where you got. Sometimes when I think about the journey to being a data scientist that goes all the way back to college and working on real estate research or the professors to work with a pen and getting into geospatial data there.
You know, there's a long process since then. And probably the most pivotal thing was working in grad school on large scale experimentation. So my grad school program I was studying was studying to be a social scientist and study how people influence their friends online.
I was really around this era of big data and people of people getting really interested in Hadoop and Hive and running large scale experiments. And I got I got very lucky and got an internship at Facebook as a 30 year old intern at Facebook. So sort of like that movie, the interns.
And I got a great great set of mentors at Facebook, Dean at goals and a time boxy were awesome. And they taught me everything I needed to know about being a data scientist at Facebook and then decided to decided to stick around and stay. So I was a data scientist at Facebook for about seven years.
And then about two years ago, switched over to Lyft where I started working on marketplace experimentation and other other kinds of stuff that left a very different set of problems on Facebook. But, you know, you can trace that journey all the way back, you know, 20, 20 years if you want to or, you know, maybe just 10, but it's still still ending up to be a lot of time at this point.
Feeling old. Nice, nice. So tell us a little bit about rideshare labs. What's the what's the mission there?
Yeah, that's a great question. You know, for a company like Lyft, there's often things are broken into products and products have a roadmap. So you have sort of like we have teams that run certain algorithms that are pricing algorithm or dispatch algorithm of ETA prediction.
And even the product itself like the driver side of the app, the writer side of the app, a product roadmap has to be very reliable. You have to sort of deliver progress on a certain time schedule so that you can meet your business requirements.
But you'll also like to try newer and more innovative things, carving out some time and space for scientists to work on ideas that might not really pan out.
And if they were sort of like if they were their big bets, but if you don't want to bet your whole company on them. So we can kind of incubate them within labs and try things that, you know, have a maybe under 50% or under 25% hit rate.
But when they do hit, we get sort of a big boost out of them. So we'd like to create that space, you know, for scientists to do that.
And we have an engineering team as well that helps us implement those ideas and get them into practice. And then the playbook is really to get to go and then take the thing that we work on and get it into production.
And then do some kind of hand off to a production team that can kind of take the thing that we built and run it in production.
So we create that space for innovation within the company.
Nice. And until recently you had more of a managerial role on that team and you kind of swap jobs with someone to get more hands on it.
I'd love to hear more about the story there, the motivation.
You know, it's not something you see a lot of. Yeah, it isn't an usual move. And people have asked me quite a bit about it. I think I was very lucky. So number one lucky to get the job in the first place.
So I wasn't sort of.
I was hired to be head of rideshare labs at lift, but I took the role last summer after a departure with the current manager of the team. And so I sort of took on this new role of trying to plan research and coordinate research for a large team of about 13 people on labs.
It's a very different job, right? So when you, you know, you're not doing a lot of hands on science work anymore. You're just sort of helping helping people get unblocked and make sure that they have what they need to be great researchers and do excellent work.
And I really enjoyed that. And I think the mentorship side of things and where you get to see people really thrive and build awesome stuff and you get to come along for the ride with what they're doing.
I had the itch to just do some more hands-on work myself. And I think it's really hard to scratch that itch from a manager role. Your time gets real. There's a, you know, the program essay, the maker schedule and the manager schedule. Sorry, very true.
You get a lot of your time just sort of like eating up by things that are really great, but they don't allow you to accumulate progress on projects. So I got very lucky that there's this guy Nick Chalmandy.
Who's a really excellent manager left. He's been there a long time and he was willing to kind of step in and take on the director role.
And he's he took on a lot of my management responsibility. And now I have a little bit of space to go on work on some of those ideas that have been kind of piling up my brain for the last few months.
That's awesome. Is there any particular experience in your background or example that kind of gave you the, I don't know, courage for lack of a better word to kind of make that that leap to well first and know exactly what it was that you wanted and second to make it happen.
Well, I think, you know, I'm a big experimentation person and I think that that's a really important part of my philosophy, both in business and in life. So trying new things is really important.
You'll never learn if you, if you like something, unless you try it. It's like same same thing with foods as it is with careers. So you have to, you have to experiment a little bit.
So I have a bias toward action and, you know, trying new things. So that was one part of it. The other part was just having like a supportive company that would help facilitate something like that. So I think in general, it's pretty tough to to shed responsibility at a big company.
It's, and it's pretty tough to find people that really want to take on stuff that you've been doing and that the unglamorous stuff so that you can go and do the fun stuff.
It's lucky to be able to fill that gap. And so yeah, those two things combined the experimental mindset and also the thing that's been coming to mind a lot lately is there's this book called flow.
I talk to people about it all the time. I think getting into a flow state is like really like something that you should, you should try to make your work facilitate.
And so I really want to get back to doing stuff where I kind of like lose track of time and are able to kind of make big progress on projects with with a little bit of time and space. So creating that space for myself became a big priority.
Yeah, at the risk of turning this into a productivity podcast, learning podcast. I'm curious if you've also read deep work and how you compare flow and deep work.
Oh, no, I haven't I haven't read deep work yet. I have another Cal Newport book. The, you know, what is it the end of email or depth of email. I've been kind of paging through and I love all these ideas.
I do think that we've we've really kind of made a very distraction filled environment for ourselves as workers. And it's particularly for the kind of work that we that we do in data science really requires kind of sustained attention.
So some problems you just you really can't make any progress on unless you can make the space both in terms of time and mental space for them. So it's it's a big it's a big goal for me personally.
It's a big goal for me as a manager when I manage people to make sure that they have that space. I even go through that there's like these preconditions for flow state.
In the, you know, in the blog book and you can kind of apply that as a management philosophy like are the people in my team able to get into a flow state. What kind of distractions are blocking them from making progress and doing that. So I think it's just like a really important part of being an effective scientist and researcher.
Nice. Nice. So you mentioned experimentation and how core that is to your philosophy. I think anyone who follows you on Twitter and folks should, you know, knows that you're very excited about stats and you have maybe more of a stats oriented,
or an additional oriental bent than other some other folks in the ML at least the ML Twitter spear. I'd love to hear you riff on kind of your on that orientation and how you think about the relationship between, you know, stats and your work and ML and AI.
That's a secret question to reflect on. I have kind of branded myself as a statistician and I like hanging out with the statisticians because there's a really like old lineage of ideas there to kind of lean on all the way back to like, you know, Fisher and, you know, his work on experimentation.
And then you have sort of like savage is one of the original statisticians and when he was thinking about statistics, he was thinking about decision problems at its core.
How do we, how do we make more effective decisions, how do we as humans make decisions optimally, how would we as a business or someone working agriculture in the case of Fisher make better decisions.
And I really like that pragmatism of statistics sort of like geared toward a particular application and having some some real world problem that you really want to solve.
And that that's sort of like the way that I think about AI and machine learning is statistics. There are tools that we use to to make something work better, you know, achieve some new capability.
And that this a long tradition and statistics of that doesn't mean that I am like a really rigorous statistician. In fact, I think I'm not really good enough at math to be one of them.
But there's a lot of great ideas to be borrowed from from that old tradition that are we're constantly reinventing and that we don't really need to. You can go back and read these old papers and they have all the same.
All the same wisdom in them that you can read about today, just maybe, maybe we call the methods different things and they're more flexible and more scalable and stuff like that.
But at the end of the day, we're really trying to solve pretty similar problems to stuff that people have been trying to solve for hundreds of years.
Nice. Nice. Love to jump into some of the things that you're working on there at Lyft and one of the things that you are involved in is the forecasting effort there.
Can you tell us a little bit about that.
I continue to be branded as a forecasting person. You know, you're right one lousy forecasting package and everybody wants you to work on forecasting forever. But it is a really interesting problem because I think forecasting is at its core is a really human centered modeling problem because they're often consumed by humans.
People look at forecasts. They have some intuition for what they should look like and they really want to use them to make better decisions coming up with a like a flexible system where that ultimately a forecast has to be a human in the loop decision making system to be effective.
Humans have to help inject a main knowledge into forecast by making them better through what they might know about what's likely to happen in the future.
So that's an interesting part to get right and then using the forecast did you know information to make a better decision and closing the loop so that and that's another sort of human in the loop piece to it.
So it left what we what we frequently need to do is plan our market management.
So there's a supply side to lift which is drivers showing up and using the app to provide driver hours to the marketplace.
And then we have the demand side which are people requesting rides and these two things can get out of balance pretty easily if we grow driver pull too quickly and we have too many drivers that could be bad they wouldn't really earn very much money per hour if there were too many drivers on the road.
And likewise like you know over demanded situations have too much demand and on drivers are pretty disastrous very opened up the lift app and you've seen like a 25 minute wait time or something like that just means that we did a did a bad job at planning company how many drivers were going to need.
So we but we have tools to address the market imbalance and sort of can spend money on incentives on the supply side or the demand side of the market.
And so the forecast become really pivotal in deciding how we how we do that. So we have to set these real number policy variables every week and actually every hour and we'd like to go to plan that in advance and make plans to do a better job of it.
And so the planning really revolves around having a good forecast of our demands by state in the future.
One of the really interesting bits about demand and supply is that we have control over those variables so they're not just pure exogenous variables that we like to forecast like the weather.
We have to forecast like not only what will happen if we don't do something but what will happen if we do do something so if we do something like raise prices and people will demand fewer lifts and so demand will go down and so we have to sort of incorporate the effects of our previous decisions into the forecast.
So there's there's really like a rich space of modeling problems just within forecasting and it's really never going to be as simple as just take this line and extrapolate it into the future.
That's what's so exciting about it at left. So we really have to we have to think about a system rather than just like any just a particular model.
I mean, you think about incorporating in the potential decisions that you could make into your forecast. How do you close that loop? Do you end up using simulation techniques or other types of techniques to do that?
Yeah, it's at the end.
Our forecasting system is designed around causal models. So it's sort of unique in that way. So we think of it as a causal model where we have certain nodes in the graph that we control.
There's just like, you know, anybody who reads Pearls books will see a dag and think, oh, those are cool. But how do I use them? Well, we do use them a lift and we use them to model our business.
And that the nodes that have no parents on them, the pure parent nodes are variables that we control. So things like price levels and how much we spend on driver incentives.
And then there are nodes that are marketplace outcomes, the things that like happen. And there are other like, you know, nodes that are pure parents that we don't control. So things like just how many people would show up organically and request right.
So we have to sort of like do this business modeling in advance in order to create these this set of models.
And they're all linked together as one big structural models. That's a pretty exciting thing to do is you kind of couple your forecasts together into a joint system. So they're all internally consistent with one another.
So you have sort of like some of those variables are your policy variables and those the forecast for those is actually a plan. So when you when you're going to set those for future values, you can't forecast it's something that you're going to do.
So you're going to fill in this, you know, this vector of values with a plan. And then you say like, OK, under this plan, what would happen to these other variables.
The really exciting thing that that's happening these days is with with differentiable programming auto grad everywhere means that we can kind of build a model that we can put a plan in and it will tell us what will happen.
And also just flip that on its head and say, well, we have an objective just find me an optimal plan. And that's what I'm really excited about these days is that we can we can take a forecasting model and say like the forecast the purpose of the forecasting model isn't to produce forecast it's to produce plans.
And those plans should help make some business objective happen. And so they really like translate directly into something actionable for the business rather than something that we have this sort of like derive what we're supposed to do based on the forecast.
Are you using a particular kind of causal model causal modeling set of tools that you build these absin or are you have you kind of built up your own framework from the ground up.
Yeah, I'm lucky to have a team that's very interested in tool building. So they they've built a lot of the technology we needed to do this, but it's it's built on top of high torch.
And so that that was a kind of interesting design decision. We had to make early on to kind of like what are we going to build the models and and we wanted something really flexible and that had autograd built in.
So we chose high torch and we had to build a lot of scaffolding on top of that. So how do the models link together in some holistic system.
So we built sort of a way to stitch together a bag of many models that's composable.
And that can admit sort of like a joint training or training of individual model models. And then on the causal side, actually the hard part there is is coming up with old like prior experimental evidence for the causal effects of things.
So how do we know what the what the effect of a price change will be on demand.
So the way to do that is actually find historical times when we've changed prices and go and try to figure out what happened in those circumstances because we can't just use the data under no price changes to estimate that. So really a lot of the hard part of the model was finding all the evidence that we needed to estimate the subs of different curves that are that are kind of like important to the counterfactual predictions that the model makes.
A lot of a lot of the task of building a model like this is building up business knowledge about what people have done past what prior experiments have been run, you know, what interventions have we done historically.
And it's looking a little bit like macro economics, you know, if you go you go and hang out with macro economists for a while they're obsessed with history because the historical data provides the natural experiments that they need to understand what's going to happen in the future.
So you think about what's going to happen in a recession what the easiest way to figure out what's going to happen is go find historical recessions and try to see what was common about them.
And so we apply kind of a similar lens to that problem.
So this question may be, you know, I may be asking a question that you just answered, but when you think about applying causal models into the types of forecasts that you're doing, I'm imagining that you have to dramatically kind of simplify the model.
You know, it can't be, you know, so robust that it's taking into effect all of the actual causal relationships in the thing that you're actually modeling.
And so I'm wondering like the interface.
I guess the thing that I'm thinking of is like leaky abstractions like you know, how does that.
How does that manifest in trying to use causal models in a real war scenario like this.
Yeah, I love that question because I think it's the it's the hard part and it's, but I've been telling my team for we've been working on this for about two years now is that we're not building a model.
We're we're building in a system for building models that's going to evolve over time to help us like, you know, make agile adjustments to the way the business runs.
Or to or as you know, as we have new information. So really, we don't our goal isn't to build like the best model in the world. Our goal is to be agile and to create sort of a modeling framework that allows us to to get good at making models better.
Which is really kind of the goal of I think most teams that are building models really are thinking a lot about this loop of like proposing new idea, testing it very quickly and then folding in the things that work well, you know, into your system quickly.
And so we really would like to get good at that. So that the core piece of that is model model checking or, you know, asians, asians call it like model checking procedures.
And validating a model and saying whether it's better or worse than your old one is really probably the hardest part of being a machine learning researcher or a statistician in general.
When do you have a model that's better than your old one.
It turns out for our models since there's so much based on business knowledge that really that's a piece where the human in the loop is very useful.
People can inspect the output of the model and say like this doesn't make sense to me and that's that's actually very useful information.
So we're really trying to think of hard about right now is what visualizations and plots and diagnostics.
Can we create very quickly from fitted models that we can show to people that maybe you don't even know how the model works or how specific that they can understand and say like this doesn't look realistic to me.
That's important not only to improve the model, but also because that building trust with those people who like ultimately are responsible for the decisions that the model makes is sort of like for a first order importance.
They don't if they don't trust the model, they don't believe it.
And they won't they'll sort of ignore it and not make decisions using it.
So one of the big pillars for our team for a long time, we call it trust and understanding.
Like do you do do people trust this model enough to start betting money on it is really where we'd like to get and to get them there you really need to show them a lot of plots is kind of kind of the take away there.
And we've had to be very agile and modify the model a lot so that sort of like become this.
It feels it can feel specific in if you think of it as like other some ultimate ultimate goal, but really at the end of the day you're trying to build a good process.
And that's that's what we've been focused on.
And theory using causal modeling techniques should provide a level of trust or understanding kind of you know built in or or you know that's what's written on the 10.
That's what a lot of people are excited about causal models for nowadays.
It sounds like you know there's still a lot of work that needs to be done though.
I think that the hardest part isn't the modeling and fitting models, it's actually like quite easy and straightforward to do that.
What you're really really limited by is how many interventions that you've had historically so this is sort of like why macro macro economics is hard.
It's like what's going to happen in a recession, but we've only had like you know three or four recessions in the last 30, 40 years.
So there's not a lot of like examples to draw on.
The sample size is very limited for interventional data inherently, especially like you know system wide interventional data.
Now when you zoom into like you know individual level policies like a user user level randomization at a experiment or we do like time split randomizations that the like those are cases where you can get very precise causal knowledge you can estimate effects very precisely.
But for these like system level estimates really are sort of limited by the available history that you have and what you've done in the past.
It's very experimental in the past and try to a lot of things maybe you have.
You know the ability to get some more traction on the problem, but but yeah we need to become better experiment designers and ultimately be more experimental to make causal models better.
And you use causal models more broadly than in forecasting there.
Oh yeah, I mean that I think every model is a causal model.
It should or maybe they should be at least it's a very strong perspective, but in a business setting, you're almost always trying to make a decision differently.
And if you don't make a decision differently, then you didn't really have any of any effect on the business.
So some models are ultimately meant to sort of drive some decision make either a very micro level decision like for us, which which.
Which driver will we dispatch to you as a rider given that your request to ride is a decision that we make.
And there are counterfactuals around that decision what we could have dispatched this other driver or this other driver we have where we could just not dispatch a driver at all because we don't have enough with them and we need to allocate them in a scarce way.
Those are all causal questions.
And so that's a very micro level decision and then I was talking about like zoomed out macro level decisions about you know spending money at like a weekly level of granularity on some incentives.
Both causal questions and they ultimately kind of like either it's an automated system that's going to do these things in an ongoing basis without a human in the loop.
Or it's a more sort of like fuzzy business process where there's some human in the loop, but either way you sort of like would like to know what would happen if you did something differently.
Awesome. So you're also involved in efforts around marketplace experimentation there.
Can you tell us a little bit about those?
Yeah, that's one of the most interesting and challenging parts of a marketplace like lift is that it's very difficult to know when you're you know when your business is functioning better because like if you look at something like revenue for this is affected by both supply and demands we can have demand shocks that make us like a ton of revenue we didn't do anything to cause that.
Or we can you know drivers we can drivers can show up in droves and we can have lots of drivers and everybody gets good experiences and we didn't we didn't have any control where that could just be like you know macro economic factors.
So but ultimately like our business is the business of matching those things coming match supply and demand effectively and make really really good micro decisions that add up to good experiences for the participants on both sides of the marketplace.
And it's very difficult to know when you're doing a better job of that because there's just all this noise and so the thing that we can do to improve the decision making there is to build better models of how the marketplace functions that allow us to kind of like partial out the noise like de noise the signal.
That's been that's being kind of transmitted so ultimately what we want to do is try new algorithms and production so try new ways of matching riders to drivers and then be able to detect if that's a better outcome for the marketplace or not.
And so the way that we try things is through what we call time split tests so we'll sort of switch algorithms other people in industry called these switch back tests where you sort of switch algorithms on and off at random intervals.
And try to see what happens on the border lines so when we switch from one to the other you get this little this nice little experiment of like the system just changes state.
And it can do better in the next hour than it's in the prior hour and our hard job of statisticians is to be good at detecting that so how can we figure out if it really was better.
And the idea of doing the switch back tests as opposed to the more traditional kind of AB tests that or sequential testing that you really the granularity of distribution shifts is so small you kind of have to do them very quickly and kind of in parallel.
So it's it's it's slightly more complicated and that we have the problem of interference. So it's it's not just the granularity of the intervention is that if we if we gave 50% of users like a big discount.
Then they would soak up all the drivers and then these other users who are in the other condition would have fewer drivers available.
And they spillovers and marketplaces that cause the treatment that you applied to some users or some drivers to spill over to the other ones. So you think about ultimately experimentation is a prediction problem in a way trying to predict what would happen in a counterfactual world where everybody, you know, in the whole marketplace was sort of living in a world with our new algorithms.
So that's what a time split test kind of acknowledges is that maybe the best way to test something is just to try it out and see.
But you need to have a rigorous experimental design in order to get detectability there. So so we are working on, you know, finer green versions of that where it's a little bit more zoomed in. Maybe we can say
that there are some time in space and give give treatments in a little bit more precise way, but it needs to be more course than a user level randomization in order to get something like more faithful to what we really care about.
And when you talk about experimental design there and the need to be rigorous there is that something that is kind of human and a loop hand done for every new experiment or have you kind of platformize some of this so that
you can do some of that in an automated way.
Yeah, these are great. These are great questions. Sam, I wish it. It's almost sounds like I wrote these.
I think like one of my big philosophies is that we should always be running more experiments than we are.
And I tell people that all the time we're not running enough experiments, we should be running more of them.
And you think about what are the bottlenecks to running experiments? It really is the human is the bottleneck.
Because we need humans to set them up and plan them and then we need humans to analyze them and decide what to do.
And you can cut the human out of the loop for both of those steps if you really want to, but it's hard.
On the planning side, it involves sort of making experiments into changes in configuration instead of code.
And the typical A, B test is like an engineer writes a new code and you have some like if statement and, you know, and the code that changes, that's something that an engineer has to set up in order for it to be something that you can test.
But if you, if you create a configuration based system, which is sort of more common at Facebook than it is it lived, but it's sort of like, you know, engineers like this, which things into configuration when they can.
And all the parameters in your configuration file are just our experiments waiting to happen, they're just numbers or, or categorical variables that you'd like to maybe try out sometime.
So it's possible to generate ideas for experiments using machine learning and pacing optimization is one approach for doing this for you sort of like, would like to try out parameters that you are most uncertain about how they'll perform in an online test.
Then to close the loop on the other side is how do you get a machine to decide whether you should launch an experiment or not.
And this is also a huge bottleneck. And I think ultimately it boils down to that it's very difficult to get people to agree on what the objective of tests are in general.
So like what, what, what, what would success look like and is there, is there just like one variable that we could use to decide whether this is successful or not.
And it turns out that the answer is often not. People have, there's usually some trade-offs involved more of one thing is good, more of another thing is good, but they sort of like, you know, more of one thing makes the other thing go down.
And at lift, we have like a very clear set of trade-offs. There's usually like a, you know, things that improve the driver's side of the market or the writer's side of the market.
There's things that improve like lift profitability, but not for our writers and drivers. And then there's sort of like a short-term and long-term set of trade-offs, like, you know, more of a greedy set of outcomes or a long-term set of outcomes.
So the getting folks to agree on like what you're, what Ronnie Gohavi, formerly at Microsoft and then Airbnb, it's like a guru of experimentation calls an overall valuation criteria.
If you have that number, you can compute it from the experiment, and you can really just then the loop is closed, and the system can propose new experiments and then launch the ones that are good.
That's really what you see with like approaches like multi-arm bandits or fully full-based optimization type of approaches. They're hard to get right, but if you do get them right, then you can run a lot of experiments.
From the sounds of it, your experimentation metrics are, at least the ones you've thrown out, sound like business metrics as opposed to model metrics.
And is, has it been easier to drive business metrics to drive model development around business metrics with the class of models that you're using with these calls and models as opposed to, you know, deep learning or some other type of technique.
Or is it just a discipline that you as a team have, you know, just committed to.
The forecasting and planning side really resists experimentation in a lot of ways. So those models are hard to evaluate offline and they're hard to evaluate online.
So, and we do have approaches for doing that, and we do things like, you know, simulated back tests and things like that to try to see if the predictive performance of the model, the statistical performance of the model translates into better decisions.
And we have ways of doing that, and I think it sort of makes the model more faithful to the goal that it was originally designed for.
I think that, you know, we would love to have much fancier models with better architectures and more bells and whistles.
I don't want to engage in like, you know, building fancier models just for fancier models. I'd like to do it in purpose of a specific task.
So until you're very, very good at translating some offline performance metric into some online performance metric, I think it's kind of dangerous to focus only on offline metrics.
So achieving that concordance between like, I know that my way of evaluating the model offline translates into, you know, better business value.
Once you have that feedback loop, like tight, then I think it's like, okay, let's do a total free for all on the modeling.
This is a little bit of a different perspective than I think a lot of people propose because they want to gravitate toward these offline metrics that can be measured very precisely.
And get really excited about improving them, but I think the burden of proof is on you as a, as a scientist to like to show that that translates into some value in a way that like other people believe.
And not just like that is consistent with your model with the marketplace experimentation.
One of your inputs or a couple of your inputs, I would imagine are the forecasts forecasted supply and demand that go into the marketplace. Are there particular challenges associated with kind of hierarchical models and that kind of environment.
Like lips data is is really fascinating in its structure and it has a lot of structure that really like really resists efficient modeling a lot of the time. So we have sort of like like a space yo temporal process. Let's take demand, for instance, and I think it's a pretty illustrative of the problems that we have.
The demand is a point process. So people pick up an app and make a request for a ride or just check the price. And so we have a sort of like latitude and a longitude and a timestamp of what that's a unit of demand.
Now, let's say we wanted to forecast that there's a lot of different ways to do it. The simplest one and the one that's most common is to, is to like aggregate it into counts in some time and space.
And then use it traditional time series model. But you might also reasonably think about that is like I would just like to be able to predict the density or like the rate of arrival of these points in time and space. And then I could like aggregate up the forecast to whatever level that I want.
So there's this like bias branch tradeoff of this, this whole spectrum of methods like, you know, bucketing, how do you choose your buckets. So if we choose time buckets or space buckets in various ways and we get different bias branch tradeoffs.
If you use a regular grid on lift data, it will work terribly. So there's a temptation to do things like image, image type models where you have like pixels. Most of those pixels are empty. And so you're just sort of like using a very wasteful representation of the data.
And then same thing for time, like there's like many hours through the middle of the night where there's just not a lot of activity in the marketplace.
So there's a lot of zeros. And so, you know, your model is not really able to do much with that.
So ideally, we'd have a forecasting system that can give consistent forecasts at all levels of granularity.
But like if I, if I forecast at very fine level and I add it up, then I would get some things the same as if I did the very high level forecast system still a very open into research problem for us. I think we're still trying to get this right.
But that, that degree of coordination would be really excellent for us because it would allow us sort of like the micro level marketplace algorithms to be making the decisions based on the same information that we're using for more macro level decisions.
And we have those two things be consistent. So I hope that we can get there someday, but it really is sort of a challenging kind of like modeling problem because it has this multi resolution quality to it in both time and space.
A moment ago, you alluded to kind of the tension between simple models and more complex models.
You are doing some experimentation with neural networks for fine grain decision making. Can you tell us a little bit about those efforts?
I think there's a really rich tradition at lift of using tools like light GBM or XG boost to solve solve problems because those are like really great hammers to hit data with.
So they're like they always, they work very well without a lot of primary tuning.
And they work really reliably in production. And so there's been a gradual sort of process of trying to figure out like could, could neural networks help us and could they do better.
And I think the answer has been that they typically don't do that much better in predictive performance than then tree based models sort of like we might get something that might be a little bit better, but maybe not worth all the extra headache of changing things.
But the neural networks have a couple of really big advantages and a big one is flexibility.
So we can change the loss function on a neural network very easily to, to be something else. So like the link, the link function at the end we can make it so it's going to predict count variables. It could be like, you know, it's very easy to swap in different loss functions.
You can do that with trees too, but it's, it's more challenging.
Another one is to predict multiple outcomes at the same time. So like a ride request could turn into many different outcomes that could turn into the rider could cancel the driver could cancel.
They can turn into it can turn to ride it could turn to a report. Maybe we want to build a model that pulls all those outcomes into a single vector value outcome. And then we can kind of, you know, build a shared representation that helps us leverage like, you know, it's like a transfer learning idea.
Some of those outcomes are sparser than others. And so by pulling them into the same model, we can do better.
That's a very natural idea of neural networks. That's actually quite difficult to implement and, and tree based models. And then the other kind of hidden advantage of neural networks is the scale ability. They actually train a lot faster and we can do much larger scale.
Then we can with trees. And so I think the hope is that we'll be able to eventually sort of like, you know, train models that for our entire marketplace and in one model, rather than having like region specific models.
That would be facilitated through the ability to kind of like put everything into the same modeling architecture all at once.
So I'm very excited about that future. And we're on our way there. We're already seeing really promising results, particularly for things like heterogeneous treatment effect models where sort of like there's just some new technology that allows us to do that.
Yeah, great question. It's sort of like, you know, this is causal inference jargon that I take for granted. So in causal inference, everybody's concerned with treatment effects is like a single binary treatment, be like, you know, you take a pill and does it work or not would be the average treatment effect would be average over the population.
heterogeneous treatment effects says, hey, maybe that pill works better for some people than for others. And we like the machine learning model to give us some idea for whom it will have stronger treatment effects.
And if you think about this as a label data problem, it doesn't work because I would like, you know, to take, you know, a vector for you and predict an effect. And I'll never observe you getting the treatment and not getting the treatments.
You can't actually estimate a per, you know, per observation treatment effects. And so you have to use a kind of interesting architecture to do that.
But then once you do, you get this very powerful model is where it uses all the available features to try to explain heterogeneity in the response to the treatments that you have.
So treatments for us might be things like coupons or discounts for writers or incentives for drivers. And so by kind of like putting those into models and letting the models tell us about the, you know,
the response to that treatment might vary. We can do a better job of figuring out like which, which people are going to benefit the most from different treatments that we can use.
Nice. It sounds in some ways analogous to the idea of getting the plan out of your forecasting models instead of the forecast itself.
I do think that that's a, that's a big theme of my last couple of years is thinking about models as not returning predictions, but as returning decisions. And it creates a sort of like end to end way of thinking about machine learning is really like the part in the middle where you have an estimate or prediction is is a nuisance to the system, right.
Automated system doesn't need to know that there was like some estimate of something. It just needs to know like what am I supposed to do in the code, who am I supposed to give this treatment to or which writers supposed to be matched with which driver doesn't care about, you know, some estimate that happened along the way.
And I don't know if we'll get there anytime soon, but really the, the layer of human interpretability within these models is a little bit of like something that we just have as like best digitally for a little while until maybe we'll end up with like some more end to end systems in the long run. Nice. Nice. I love to close us out by having you talk a little bit about the role of rideshare labs relative to kind of classical lift data science, you know, just listening to you speak.
Like a lot of the stuff that you're doing is kind of practical today work that's in the short term of the business as opposed to your traditional labs, which is, you know, pursuing these moonshots that may or may not material as how do you think about that relationship.
I love that question. It is a, these are moonshots. The problems themselves are the same as the teams themselves that are working on them. And we partner with them quite closely. So we're always working with teams that are actually doing real, real work and working on actual day to day problems.
So we have this kind of collaborative model, but the moonshot part of it is that we're not we're not sure that the methods for solving those problems are going to work yet. So we have a known working solution that we can pursue sort of like small gradient steps toward improving.
But if we want to take a jump in design space to a different solution, then we need to incubate that somehow so that like the move from trees to neural networks for these systems is something that, you know, takes months to implement.
It's not something that a product team really would would would probably ever prioritize because it would just attract from getting some more immediate business value. So I think the problems stay the same.
What we're trying to do is kind of like mine the field for new new solutions and think, you know, things like going to conferences and reading reading all the latest research and saying, hey, how does this apply to our business.
Is there an idea here that like really could be a big step function improvement and how we do things and if it could be that, then it's our job to be experimental and try those things out in a kind of like limited risk setting.
And so that's that's kind of my my big idea about what the role of a labs team would be awesome. And is there is there a result from a conference or paper or something like that that stands out as an example of that, you know, something that.
You know, it was from another area or or kind of orthogonal to what you're currently doing, but you have really interesting results trying to apply it.
Yeah, I mean that heterogeneous to your modeling idea, I didn't come up with that. We, I mean, it's like, what is it? Great artist copy for good artist copy, great artist steel.
That's the same thing with scientists. That's a Claudia she and Victor Vitch and David Bly had this paper. It's called dragon net and so you can go and read that paper and it's got a really great idea for a neural network architecture that can estimate heterogeneous treatment effects.
And I saw the paper and you can see that the diagram for the neural network architecture and we're like, wow, this is a great.
It's a great idea. They already had code available online. So we can go in like, you know, try that out on our data and that's been sort of like ongoing process of trying to figure out if we can.
If we can do better than our kind of existing methodology and when and where it works better. So, yeah, so I think we we borrow very liberally and I think that that's actually the really fun part right now.
Everybody's inventing all kinds of new stuff. So it's fun to it's fun to be someone who borrows and steals.
Well, Sean, thanks so much for taking the time to share a bit about what you're up to. It's been wonderful catching up with you.
Yeah, thanks Sam for having me. This is this is super fun. Great questions. I'm looking forward to the next time. Thank you. All right. See you.
All right, everyone. I hope you enjoyed that interview. I am here with my friend Robert Osesou and us and we're going to spend a few minutes chatting about the interview and how it relates to his causal modeling and machine learning course Robert. Welcome back.
Hey, Sam. Thanks for having me again. Hey, why don't we just get started by having you just kind of riff on the interview and how it ties into the themes that you've covered in the course we've been doing the kind of joint.
All deep that AI that's your education company and formal collaboration for about a year and a half now, what maybe four or so cohorts so far. Yeah, it's been a good collaboration, you know, and I enjoyed the interview with Sean as somebody we go always back.
I think I first met him when I was running a meetup on prophecy programming back in the Bay Area, and he came and talked about profit, which was the forecasting software he built on top of prophecy programming language called Stan.
Nice. And so did any particular themes jump out at you from the interview. I think you prepared some clips that you wanted to.
It's a snippets here. You know, it's funny. You know, I've had these it's actually tangential to some conversations have had recently.
He talks about.
So one of the approaches that we focus on in our course is a is a generative modeling approach to causal modeling and.
And some of those things came up here. And so, for example, he talks about heterogeneous treatment effects.
heterogeneous treatment effects says, hey, maybe that pill works better for some people than for others. And we like the machine learning model to give us some idea for whom it will have stronger treatment effects.
And if you think about this as a label data problem, it doesn't work because I would like to take a vector for you and predict an effect.
And I'll never observe you getting the treatment and not getting the treatments. I can't actually estimate a per observation treatment effects.
So one interesting thing if you take a generative modeling approach to causal modeling. So what you're doing is you're you're modeling the distribution of the causal query.
So say you want to know what the causal effect is of this treatment on that outcome.
You're looking at the distribution of that outcome under the intervention under the treatment.
And, and so from a generative modeling approach, since you're just directly modeling the distribution heterogeneity is built in right it's a distribution and so it's spread quantifies uncertainty quantifies the diversity of a population.
And so if you want to then take for what that allows you to do is take modern probabilistic modeling tools.
Many of which are using cutting edge deep learning auto differentiation architectures to do inference and just say like, OK, why have this distribution.
What's the probability of distribution of this treatment on this outcome? What's the probability of distribution of this treatment on this outcome if this person were over six foot five and
like the right bicycles, right? And so that's all just we can we have modern tools and probabilistic modeling that we can just apply directly to those problems.
And so that's the approach that we take in the workshop because many people actually already have some skills in using some of those tools, for example, pi torch.
So one of the things that popped out to me in his interview was kind of talking about the joy of modeling and that's something that really resonates with me.
You guys had a conversation about flow state.
I talked to people about it all the time. I think getting into a flow state is really something that you should try to make your work.
And so I really want to get back to doing stuff where I lose track of time and are able to make big progress on projects with a little bit of time and space.
And it dramatically improves just my experience at work. Say, for example, I learned a little bit of functional programming how to apply it to exploratory data analysis. I learned probabilistic programming and how you can compose smaller programs into bigger programs using some ideas from category theory to the model complex systems.
It was great. You can solve new problems and, you know, more importantly, I think for me personally, more importantly was that aside from the productivity gains and the ability to do new things, it was just funer, right?
Like I was able to get into that flow state.
There's other sources of joy, right, ecstatic bliss, fiero, I don't know. I don't care about those. I really care about just getting into a deep work type of mindset and being able to do cool things with with my data with cool tech.
And the idea that causal modeling provided some of that for you.
The approach that we're working on in this workshop, that's right. So a lot of causal modeling, if you go to the textbooks, it's just like, you know, let's construct an estimator for this identifiable estimate and there's a whole bunch of theory and math and it doesn't really, you know, it's almost as though you have to learn a new degree just to just to be able to apply these methods when
take some of these concepts from generative modeling from probabilistic programming from, you know, in some cases, functional programming and and connected to your intuition and understand how bits of a causal model can compose together like programs that actually makes it
well, frankly, much funner than other ways of of going about it also makes it easier to learn and it makes it easier to apply. So, you know, that was one of the things I got, I got to thinking about when I listened to this interview of Sean.
Nice, nice. Yeah, I've often found it interesting when folks ask you, what are the prerequisites for the course and you reply, well, some basic probability, but it's not like you need to have, you know, some deep theoretical grounding in causal causal modeling or causality or
statistics or anything like that. Yeah, and obviously those things will help you if you wanted to apply it to a specific applied problem where, you know, say you're working with some variable that's that has a lot of nuance and and the in terms of statistically modeling it, you need to understand a little bit about say, I don't know
you need to measure theory that that's possible with specific class of problems, but it's completely separate from the problem of applying a causal model to the problem and reasoning about it causally.
Did you have any other clips that you wanted to share?
And I don't know, here's this bit of you were talking guys were talking about leaky abstractions, I think I, it relates to what we were just discussing in terms of reasoning about a causal model as a program that you can
build on iteratively and you know, plan some unit testing to have an explainable model and kind of build a component of that model or one thing is working and then you can move on to another part of your causal model so that you can grow it into something that is building value for you and your organization over time.
And so I'm wondering like the interface, I guess the thing that I'm thinking of is like leaky abstractions like you know, how does that, how does that manifest in trying to use causal models in a real war scenario like this.
Yeah, I love that question. I've been telling my team for we've been working on this for about two years now is that we're not building a model, we're building a system for building models that's going to evolve over time to help us like we make agile adjustments to the way the business runs or to or as we have new information.
It resonates with me because two things, number one, a lot of tools and machine learning and statistics, they kind of encourage you to get really good at learning the tool, the block box, the black box, most of the workflow and you just kind of learn some kind of art of hyper parameter optimization.
And with these types of causal models, you need domain knowledge. And so you need to get, you need to be thinking very detailed ways about the data generating process.
And so if you can, if you can take, if you make clear composable abstractions about that process and focus on some small element of it to start with and then build it up over time and also the inference algorithm, if you can separate the model from the inference algorithm, you can perhaps use some cutting edge deep learning techniques for probabilistic modeling, say stochastic variational inference.
And you learn how to make programmable inference and build that up over time, not only do you have something that's more robust because you can make sure that each component is working on isolation before you bring them together, but also something that builds IP over time for your company.
And this is a lot different from how causal inference is usually taught because it's usually taught as though the problem that you're presented with at the beginning is the only problem that you're ever going to face. And so if there's a new problem, you have to start from scratch and build a new model.
And in a production setting, that's, that's not a good approach.
Well, your target participant for the course is someone with someone who's thinking about these problems from an engineering perspective, as opposed to a traditional statistical perspective, whether that's, you know, in the kind of sciences or social sciences or what have you, which is a lot of where the where causalities been.
Where folks are thinking about causal models and causality, is that right?
Yes. So, you know, you guys had this blurb.
And so we have to incorporate the effects of our previous decisions into the forecast.
There's really like a rich space of modeling problems just within forecasting and it's really never going to be as simple as just take this line and extrapolate it into the future.
We have to think about a system rather than just like any just a particular model. Do you think about incorporating in the potential decisions that you could make into your forecast?
How do you close that loop? Do you end up using simulation techniques or other types of techniques to do that?
Yeah, it's at the end, our forecasting system is designed around causal models.
Right. And so that at least currently is the killer app for causal modeling in production. In most settings, if you're building some kind of predictive algorithm, it's going to generate a prediction that you are then going to use to make a decision.
You're often, in fact, more than often, that decision impacts the outcome of that decision impacts the data that your algorithm is going to use to make future decisions that feedback loop can cause a significant amount of bias to the algorithm.
Yes, that is the main engineering use case. That said, we have lots of researchers in applied fields who are trying to get a global perspective on causal modeling.
Many people from public health and many people from economics, for example, who maybe have learned a few specific causal model, causal modeling or causal inference techniques that are popular in their domain.
But don't really have a global understanding for the theory.
Let's maybe switch gears and talk a little bit about the course itself. What's the structure for the course?
It is a cohort based course. We have online lectures and videos. We have assignments, programming assignments.
And we have a weekly retrospective cohort meeting where we go online and talk about that week's lectures, answer questions, maybe workshops and people's individual problems.
And then alongside, we have some reading groups with previous students who are interested in talking about adjacent topics. And then we have a project based element to the course, such as if you're trying to build a project, where you can apply these ideas will provide support with that, and you can team up with other members of the course to come up with a good project outcome.
Can you mention a couple of projects that students have taken on?
Recently, one student used a deep learning technique called normalizing flows to implement a kind of factual reasoning algorithm on images.
Some other students used, these students were fans of soccer football. And they used, they built a cause of a model that would predict the outcome of a trade.
Other students prior to COVID, this was a really cool project. They used, they downloaded a bunch of Airbnb data, downloaded a bunch of real estate data from Redfin.
They would impose them into a model that would predict how much, basically, if you wanted to buy a house with the goal of maximizing revenue from Airbnb, it showed you, you know, basically created a search engine for, you know, Airbnb optimal properties.
And then post COVID, some people did some really interesting things with epidemiological models and adjusting them so that they were, they were using causal reasoning.
Nice, nice. And all that might sound intimidating to some folks who would be perfect fits for this course in the past, you've made a point to make sure that a lot of people know that they can kind of adjust their, they kind of, you get out of it, what you put into it, and you can take it at different levels.
Not a little bit.
Yeah, so if you, you know, people are busy, a lot of people are full time employed who are taking the course. And so you, we set it up so that you can move at your own pace.
If you miss a week because of work, you can hop onto the cohort and get fairly caught up in the review section of the cohort.
Obviously, if you were going to go deeper into the videos on homeworks, you'd get more out of it, but you'll not, it's set up so that you're never in a position where if you, if life happens, you kind of get left behind the rest of the course and, you know, you don't have the time to, to continue.
You'll always be able to catch up and get as much out of the courses you can.
Awesome. Awesome. Well, there's a ton more that we can go into. One of the things that past students have raved about is the access they get to Robert and the one on one guidance that he makes, makes himself available to, to deliver.
But really, the next step is to check out some more information about the course and you can do that at twimmelai.com slash causal and thanks for tuning in and thank you, Robert, for joining us.
