1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:28,720
I'm your host Sam Charrington.

4
00:00:28,720 --> 00:00:33,040
I want to send a quick thanks to our friends at Cloud Era for their sponsorship of this

5
00:00:33,040 --> 00:00:39,760
series of podcasts from the Stratidata conference, which they present along with O'Reilly media.

6
00:00:39,760 --> 00:00:41,960
Cloud Era is long been a supporter of the podcast.

7
00:00:41,960 --> 00:00:47,760
In fact, they sponsored the very first episode of Twimble Talk back in 2016.

8
00:00:47,760 --> 00:00:51,920
Since that time, Cloud Era has continued to invest in and build out its platform, which

9
00:00:51,920 --> 00:00:57,680
already securely hosts huge volumes of enterprise data, to provide enterprise customers with

10
00:00:57,680 --> 00:01:01,920
a modern environment for machine learning and analytics that works both in the cloud

11
00:01:01,920 --> 00:01:04,240
as well as in the data center.

12
00:01:04,240 --> 00:01:09,680
In addition, Cloud Era Fast Forward Labs provides research and expert guidance that helps enterprises

13
00:01:09,680 --> 00:01:14,080
understand the realities of building with AI technologies without needing the higher

14
00:01:14,080 --> 00:01:16,440
and in-house research team.

15
00:01:16,440 --> 00:01:19,840
To learn more about what the company is up to and how they can help, visit Cloud Era's

16
00:01:19,840 --> 00:01:32,960
machine learning resource center at cloudera.com slash ml.

17
00:01:32,960 --> 00:01:33,960
All right, everyone.

18
00:01:33,960 --> 00:01:35,480
I am here with Shaolin Sam.

19
00:01:35,480 --> 00:01:39,600
Shaolin is a research engineer with Cloud Era Fast Forward Labs.

20
00:01:39,600 --> 00:01:42,840
Shaolin, welcome to this week in machine learning and AI.

21
00:01:42,840 --> 00:01:43,840
Thank you.

22
00:01:43,840 --> 00:01:44,840
Thank you for having me.

23
00:01:44,840 --> 00:01:45,840
Absolutely.

24
00:01:45,840 --> 00:01:48,440
I am really excited about our conversation.

25
00:01:48,440 --> 00:01:54,200
We are going to be diving into some of the work that you've done recently exploring learning

26
00:01:54,200 --> 00:01:56,800
with limited label data.

27
00:01:56,800 --> 00:02:00,160
Before we do that, tell us a little bit about your background.

28
00:02:00,160 --> 00:02:03,720
How did you get started working in AI?

29
00:02:03,720 --> 00:02:09,240
Yeah, so my background is actually in electrical engineering and computer science.

30
00:02:09,240 --> 00:02:10,640
Go doubly.

31
00:02:10,640 --> 00:02:11,640
Yeah.

32
00:02:11,640 --> 00:02:17,640
So after graduation, I worked in the financial industry designing quantitative trading

33
00:02:17,640 --> 00:02:27,200
strategies and then that got a little bit old because I felt like we were using data but

34
00:02:27,200 --> 00:02:31,960
not really for the right reasons or the right thing.

35
00:02:31,960 --> 00:02:39,280
Then I kind of step back and started to focus more on early stage venture investments

36
00:02:39,280 --> 00:02:46,480
where I mostly looked at women-led companies and try to essentially help them through

37
00:02:46,480 --> 00:02:51,600
the whole startup venture fundraising phase.

38
00:02:51,600 --> 00:02:57,000
And after that, I realized that I actually really, really love data and missed programming.

39
00:02:57,000 --> 00:03:05,000
I miss actually building things and that's actually how I found my way back into nitty-gritty

40
00:03:05,000 --> 00:03:07,960
work and Fast Forward Labs.

41
00:03:07,960 --> 00:03:14,600
I remember when I finished, actually I don't think this was the case for undergrad but for

42
00:03:14,600 --> 00:03:19,960
grad school, like we were just at the time when the consulting industry was like sucking

43
00:03:19,960 --> 00:03:24,600
a lot of people out and it was a transition from when consulting was taking everyone to

44
00:03:24,600 --> 00:03:26,800
when financial services was taking.

45
00:03:26,800 --> 00:03:36,320
Yeah, I think my dissertation was in Operation Sweet Shows and Operations Management.

46
00:03:36,320 --> 00:03:41,720
So a lot of us went into management consulting.

47
00:03:41,720 --> 00:03:50,720
And then the next, I guess, popular choice was finance, specifically the Quant Stuff.

48
00:03:50,720 --> 00:03:51,720
Okay.

49
00:03:51,720 --> 00:03:52,720
Yeah.

50
00:03:52,720 --> 00:03:53,720
Nice, nice.

51
00:03:53,720 --> 00:04:01,080
The OR stuff was one of my favorite classes in grad school, like the, what was the,

52
00:04:01,080 --> 00:04:05,200
I'm trying to remember the name of the software that, God, I would be so dating myself to

53
00:04:05,200 --> 00:04:09,360
say that you got the textbook and it was this floppy disk in the back of the book.

54
00:04:09,360 --> 00:04:12,200
I had like a puma on it.

55
00:04:12,200 --> 00:04:13,200
A puma.

56
00:04:13,200 --> 00:04:16,960
Yeah, it was basically a linear program solver thing.

57
00:04:16,960 --> 00:04:20,280
I forget what that thing was called.

58
00:04:20,280 --> 00:04:26,600
But that, I really enjoyed the whole dynamic and stochastic programming and linear programming

59
00:04:26,600 --> 00:04:27,600
and stuff like that.

60
00:04:27,600 --> 00:04:32,440
And every once in a while, I get to kind of geek out with folks that took operations

61
00:04:32,440 --> 00:04:35,120
research stuff.

62
00:04:35,120 --> 00:04:43,160
So you went into, you went to the dark side, the financial services path.

63
00:04:43,160 --> 00:04:47,720
But now you're back doing research and focusing on data and data science.

64
00:04:47,720 --> 00:04:52,880
What exactly do you do at Fast Forward Labs, Cloud or a Fast Forward Labs?

65
00:04:52,880 --> 00:04:54,400
Cloud or a Fast Forward Labs.

66
00:04:54,400 --> 00:05:00,120
So at Cloud or a Fast Forward Labs, our goal is to bridge the gap between research and

67
00:05:00,120 --> 00:05:02,120
industry applications.

68
00:05:02,120 --> 00:05:06,440
And one of the things that we do is publish research reports.

69
00:05:06,440 --> 00:05:10,480
And these reports are meant to highlight capabilities within machine learning that we

70
00:05:10,480 --> 00:05:14,800
think will become important for the business community within the next six months to two

71
00:05:14,800 --> 00:05:16,960
years from when they're published.

72
00:05:16,960 --> 00:05:24,160
So we spend a lot of time focusing on research, focusing on what capability we should actually

73
00:05:24,160 --> 00:05:25,560
write about.

74
00:05:25,560 --> 00:05:30,480
And we also, as part of that, consult with clients.

75
00:05:30,480 --> 00:05:35,000
We are essentially their best friends, best data nerd friends.

76
00:05:35,000 --> 00:05:41,000
So we try to help them implement this capability within the corporation, within the enterprise.

77
00:05:41,000 --> 00:05:43,760
And then they can always come to us with questions.

78
00:05:43,760 --> 00:05:46,440
And that's accomplished through advising hours.

79
00:05:46,440 --> 00:05:50,200
And I've long been a big fan of what you're doing.

80
00:05:50,200 --> 00:05:56,440
Hillary was one of my first interviews on the podcast.

81
00:05:56,440 --> 00:06:04,640
And last year's strata, I interviewed Justin Norman and we talked about the current report

82
00:06:04,640 --> 00:06:07,160
at that time, which was federated machine learning.

83
00:06:07,160 --> 00:06:12,120
I think in that interview, I referenced that one of my favorites was the thing that the

84
00:06:12,120 --> 00:06:13,600
report on summarization.

85
00:06:13,600 --> 00:06:15,680
I thought that was really cool.

86
00:06:15,680 --> 00:06:22,320
But the most recent report is one that you've worked on and that's on learning with limited

87
00:06:22,320 --> 00:06:24,480
label data.

88
00:06:24,480 --> 00:06:29,080
So what motivated that report?

89
00:06:29,080 --> 00:06:31,720
A couple of things.

90
00:06:31,720 --> 00:06:38,880
So we see a lot of enterprises sitting on a large amount of data, real-life data that

91
00:06:38,880 --> 00:06:42,680
they cannot actually leverage and turn into anything useful.

92
00:06:42,680 --> 00:06:48,640
And one of the reasons is that real-life data is very messy and unorganized and unlabeled.

93
00:06:48,640 --> 00:06:55,360
But supervised machine learning, for example, requires precise labels and a lot of data for

94
00:06:55,360 --> 00:06:57,520
you to be able to actually use it.

95
00:06:57,520 --> 00:07:03,680
And because of this, what companies end up doing currently is essentially creating labels

96
00:07:03,680 --> 00:07:07,800
manually for each one of their data.

97
00:07:07,800 --> 00:07:13,200
And that's essentially their attempt to be able to convert the data into anything useful.

98
00:07:13,200 --> 00:07:20,240
But that effort is very brute force, it's also not very effective, it's very expensive,

99
00:07:20,240 --> 00:07:22,280
and doesn't scale very well.

100
00:07:22,280 --> 00:07:24,720
So there has to be a better way to do this.

101
00:07:24,720 --> 00:07:31,400
And active learning is one way that allows you to learn with limited label data.

102
00:07:31,400 --> 00:07:37,520
With active learning, you can actually just smartly select a small set of label data,

103
00:07:37,520 --> 00:07:40,680
use that to build a machine learning model.

104
00:07:40,680 --> 00:07:45,360
And then that essentially opens up capabilities that you weren't able to do before.

105
00:07:45,360 --> 00:07:52,720
You can now build new products that your enterprise can take advantage of using already your

106
00:07:52,720 --> 00:07:54,920
existing data.

107
00:07:54,920 --> 00:08:00,320
So one of the things that's always interesting about kind of the approach you take with

108
00:08:00,320 --> 00:08:07,560
these reports is that it's very much, there's usually something that's happened in the industry

109
00:08:07,560 --> 00:08:12,880
that says, okay, now it's time to do the active learning report, which is sounds like

110
00:08:12,880 --> 00:08:18,440
this learning with limited label data is like the active learning report.

111
00:08:18,440 --> 00:08:25,880
What's the thing that's happening now that is, you know, is it new advancements and

112
00:08:25,880 --> 00:08:30,040
algorithms around active learning or something else?

113
00:08:30,040 --> 00:08:33,240
So the report is called learning with limited label data.

114
00:08:33,240 --> 00:08:36,880
And as part of that, we focus on different approaches.

115
00:08:36,880 --> 00:08:41,440
We looked at different types of approaches that would help you do that.

116
00:08:41,440 --> 00:08:45,520
Active learning was something that we ended up focusing on because it's most mature out

117
00:08:45,520 --> 00:08:47,880
of all these other capabilities.

118
00:08:47,880 --> 00:08:53,560
We also looked at, for example, weeks supervision, meta learning, that that was something that

119
00:08:53,560 --> 00:08:59,400
we mentioned in the report, but it's not something that we actually looked into very deeply.

120
00:08:59,400 --> 00:09:05,320
So the reason we decided to look to focus on active learning is because, first of all,

121
00:09:05,320 --> 00:09:13,400
it is one way that most enterprises can easily use and take advantage of it and essentially

122
00:09:13,400 --> 00:09:15,080
learn from the data.

123
00:09:15,080 --> 00:09:21,440
The second reason is that recently there's been a lot of algorithmic improvements in active

124
00:09:21,440 --> 00:09:25,760
learning that allows it to be used for deep learning.

125
00:09:25,760 --> 00:09:30,040
The classical active learning strategies don't work very well for deep learning, mostly

126
00:09:30,040 --> 00:09:37,400
because deep learning is very highly non-linear and also highly complex.

127
00:09:37,400 --> 00:09:43,640
So many of the existing strategies are hard to translate over and it's not something that

128
00:09:43,640 --> 00:09:47,960
you can just easily take and apply in the deep learning setting.

129
00:09:47,960 --> 00:09:53,240
The other reason is that deep learning trains and batches and an active learning.

130
00:09:53,240 --> 00:09:56,880
What we're doing is you start with a small set of label data.

131
00:09:56,880 --> 00:10:01,800
You build a machine learning model using that small set of label data.

132
00:10:01,800 --> 00:10:06,920
The machine learning model is then used to make predictions on your entire pool of unlabeled

133
00:10:06,920 --> 00:10:08,320
data.

134
00:10:08,320 --> 00:10:14,760
In the process of making prediction, the model also identifies points that are difficult.

135
00:10:14,760 --> 00:10:17,480
And this point is then sent to a human being.

136
00:10:17,480 --> 00:10:19,960
The human provides labels for it.

137
00:10:19,960 --> 00:10:25,640
This label is then added back to the original smaller label data set and then you iterate

138
00:10:25,640 --> 00:10:27,200
this process.

139
00:10:27,200 --> 00:10:32,840
So in active learning, you're adding small chunks of newly labeled data, but the small chunk

140
00:10:32,840 --> 00:10:38,200
of newly labeled data doesn't really make much impact in a deep learning setting.

141
00:10:38,200 --> 00:10:46,160
So recently, there's been just advancement that tackles both of those issues.

142
00:10:46,160 --> 00:10:49,240
Are there different types of active learning?

143
00:10:49,240 --> 00:10:55,960
How you describe generally how it works are there different types of algorithms?

144
00:10:55,960 --> 00:11:03,000
How broad is the field of approaches under the banner of active learning?

145
00:11:03,000 --> 00:11:08,680
Yeah, at the heart of active learning is a model that is able to identify difficult

146
00:11:08,680 --> 00:11:11,680
data points and request labels for it.

147
00:11:11,680 --> 00:11:14,120
So how does it actually do that?

148
00:11:14,120 --> 00:11:20,320
It turns out this model actually relies on strategies and this strategy selects the difficult

149
00:11:20,320 --> 00:11:21,880
data points.

150
00:11:21,880 --> 00:11:26,720
So there are classical strategies and there are strategies that there has been adapted

151
00:11:26,720 --> 00:11:28,160
for deep learning.

152
00:11:28,160 --> 00:11:33,960
So classical strategies, the easiest one is random sampling.

153
00:11:33,960 --> 00:11:37,040
In random sampling, you really not following a strategy.

154
00:11:37,040 --> 00:11:39,960
You're randomly picking data points to get labels for.

155
00:11:39,960 --> 00:11:43,240
So this is the simplest possible way.

156
00:11:43,240 --> 00:11:49,520
And something slightly more complicated, but something that actually works is uncertainty.

157
00:11:49,520 --> 00:11:56,200
So when we say models look for difficult data points, what does that mean?

158
00:11:56,200 --> 00:12:02,280
One way to think about that is the models will look for points that it's uncertain about.

159
00:12:02,280 --> 00:12:07,360
And then when we get there, then we need to actually quantify what uncertainty means.

160
00:12:07,360 --> 00:12:11,440
So there's a slew of strategies that try to quantify uncertainty.

161
00:12:11,440 --> 00:12:18,960
And one way is to look at the distance between a particular data point and its decision boundary.

162
00:12:18,960 --> 00:12:25,120
These data points that are far from the decision boundaries, you can interpret that as points

163
00:12:25,120 --> 00:12:27,440
that the model is very certain about.

164
00:12:27,440 --> 00:12:31,560
Any smog changes in the decision boundary doesn't really affect the classification of those

165
00:12:31,560 --> 00:12:32,560
points.

166
00:12:32,560 --> 00:12:38,400
But when the points are very close to the decision boundary, you can think of that as the

167
00:12:38,400 --> 00:12:43,080
model being uncertain about these points because any slight changes in the decision boundary

168
00:12:43,080 --> 00:12:48,200
due to model refinement will cause these points to be classified differently.

169
00:12:48,200 --> 00:12:53,320
So in margin sampling, for example, we are essentially looking for data points that are

170
00:12:53,320 --> 00:12:57,240
very close to the decision boundary and getting labels for those.

171
00:12:57,240 --> 00:13:03,440
And once we have those labels, they are then added back to our original label data pool

172
00:13:03,440 --> 00:13:06,640
and then used to build a better model.

173
00:13:06,640 --> 00:13:14,960
I guess when I think of active learning kind of non-technically, loosely, I think of you

174
00:13:14,960 --> 00:13:20,840
kind of run, like as you said, you change your model, you have your model make some predictions.

175
00:13:20,840 --> 00:13:28,920
And then when we're doing things like classification, we'll also often see a return value that's

176
00:13:28,920 --> 00:13:36,400
like the probability, a confidence or the probability with which your model thinks that it's

177
00:13:36,400 --> 00:13:38,720
the right classification.

178
00:13:38,720 --> 00:13:43,160
Is that probability the same as can we use that as part of active learning or?

179
00:13:43,160 --> 00:13:48,760
Yes, that's in classical active learning strategies that works.

180
00:13:48,760 --> 00:13:53,720
You can use the prediction probability as a proxy for how confident the model is, for

181
00:13:53,720 --> 00:13:56,080
example.

182
00:13:56,080 --> 00:14:07,600
But this way to measure uncertainty for deep neural networks doesn't really work because

183
00:14:07,600 --> 00:14:13,920
mostly because deep neural networks are very, very complex and there are a lot of parameters.

184
00:14:13,920 --> 00:14:20,720
So there are times when you could perturb the image using some noise and you will get

185
00:14:20,720 --> 00:14:23,640
an unexpected misclassification.

186
00:14:23,640 --> 00:14:29,040
So this is along the lines of the generative adversarial problem.

187
00:14:29,040 --> 00:14:35,720
You can perturb an image that goes into a deep neural network and have it misclassify

188
00:14:35,720 --> 00:14:37,120
unexpectedly.

189
00:14:37,120 --> 00:14:42,400
So for example, you would send an image of a panda with some noise into the deep neural

190
00:14:42,400 --> 00:14:48,480
network and it will classify as a given, but the prediction probably is actually really

191
00:14:48,480 --> 00:14:49,680
high.

192
00:14:49,680 --> 00:14:56,640
So although both images look like a panda to a human being, to the network, the one that's

193
00:14:56,640 --> 00:15:02,760
perturbed looks like a given and the deep neural network is very certain of that.

194
00:15:02,760 --> 00:15:06,920
So it will say, oh, this is not a panda, this is a given with 99%.

195
00:15:06,920 --> 00:15:13,720
So if we use that prediction probability as a gauge of uncertainty in deep neural networks,

196
00:15:13,720 --> 00:15:16,600
that doesn't really work.

197
00:15:16,600 --> 00:15:25,640
So there are many different ways to essentially look at how to judge uncertainty in deep

198
00:15:25,640 --> 00:15:30,640
neural networks and in our report, we look at three major ones.

199
00:15:30,640 --> 00:15:40,400
The first one is essentially to use the adversarial approach and to measure what you would like

200
00:15:40,400 --> 00:15:45,520
to do is to measure the distance between a data point and the decision boundary, similar

201
00:15:45,520 --> 00:15:50,880
to what we do in the classical approaches, but that's not tractable in deep neural networks

202
00:15:50,880 --> 00:15:55,320
because the decision boundary is highly non-linear and you don't really actually know what it

203
00:15:55,320 --> 00:15:57,520
looks like.

204
00:15:57,520 --> 00:16:01,920
So what ends up happening is we could estimate that distance.

205
00:16:01,920 --> 00:16:11,560
For example, we could use the distance between a data point and it's next closest, a different

206
00:16:11,560 --> 00:16:14,200
class object.

207
00:16:14,200 --> 00:16:19,920
But that one will only give you a very coarse estimate of the distance that you're looking

208
00:16:19,920 --> 00:16:20,920
for.

209
00:16:20,920 --> 00:16:25,160
A more creative approach is to use the adversarial approach.

210
00:16:25,160 --> 00:16:31,360
So what you try to do is perturb the data points and perturb it such that it's such

211
00:16:31,360 --> 00:16:34,240
classes unexpectedly.

212
00:16:34,240 --> 00:16:38,760
So when you get a misclassification unexpectedly, the particular data point has switched to a

213
00:16:38,760 --> 00:16:45,120
different class and you use the magnitude of the perturbation to estimate the distance

214
00:16:45,120 --> 00:16:48,720
between that data point and the decision boundary.

215
00:16:48,720 --> 00:16:52,760
So that is one approach that we mentioned in the report.

216
00:16:52,760 --> 00:16:57,200
We also mention a fundamentally different way to think about uncertainty for deep neural

217
00:16:57,200 --> 00:17:04,480
networks and that essentially gets into a Bayesian neural networks and how to estimate

218
00:17:04,480 --> 00:17:08,960
the posterior using dropouts.

219
00:17:08,960 --> 00:17:11,480
Can you elaborate on that a little bit more?

220
00:17:11,480 --> 00:17:14,280
You knew that question.

221
00:17:14,280 --> 00:17:20,040
So like I said before, if you just look at the prediction probability for deep neural

222
00:17:20,040 --> 00:17:25,360
networks and if you use that to estimate uncertainty, that's very misleading.

223
00:17:25,360 --> 00:17:29,640
So we need a different way to think about uncertainty.

224
00:17:29,640 --> 00:17:34,160
Now if you think about it from the Bayesian approach with Bayesian neural networks, what you can

225
00:17:34,160 --> 00:17:40,640
do is to actually get the weight distribution of the neural network given the trading data.

226
00:17:40,640 --> 00:17:46,920
Once you have the probability distribution of the weights, you can then write out mathematically

227
00:17:46,920 --> 00:17:52,400
really nicely the prediction probability of the deep neural network.

228
00:17:52,400 --> 00:18:00,800
But that mathematical expression is pretty but it's not easy to compute.

229
00:18:00,800 --> 00:18:05,720
What you can do then, let me take one step back.

230
00:18:05,720 --> 00:18:09,920
It's pretty and it's not easy to compute and in addition, you actually have to implement

231
00:18:09,920 --> 00:18:12,840
a Bayesian neural network in order to get that posterior.

232
00:18:12,840 --> 00:18:19,040
That posterior is the probably distribution of the weights.

233
00:18:19,040 --> 00:18:21,520
So we don't want to do that.

234
00:18:21,520 --> 00:18:23,560
How do you actually estimate the posterior?

235
00:18:23,560 --> 00:18:28,160
In terms of how you can use dropout as a way to estimate the posterior.

236
00:18:28,160 --> 00:18:34,480
And once you have the estimate for the posterior, the whole mathematical formula for the prediction

237
00:18:34,480 --> 00:18:39,440
probability simplifies down into something that we can easily compute.

238
00:18:39,440 --> 00:18:44,000
How does dropout give you the weight?

239
00:18:44,000 --> 00:18:50,600
So dropout, I think many of us are familiar with dropout as a regularization technique.

240
00:18:50,600 --> 00:18:57,200
When we dropout, you randomly set some weights connecting to the neurons to be zero with certain

241
00:18:57,200 --> 00:18:59,200
probability.

242
00:18:59,200 --> 00:19:03,560
And once that weight is set to zero, you have a smaller neural network.

243
00:19:03,560 --> 00:19:07,760
But you still train that neural network using the initial training data.

244
00:19:07,760 --> 00:19:13,720
So what ends up happening is you have a smaller neural network, you're forcing it to work harder.

245
00:19:13,720 --> 00:19:19,920
One way to think about that is also through the workforce analogy.

246
00:19:19,920 --> 00:19:28,240
If you have a workforce of 100 people, they perform their jobs on a daily basis.

247
00:19:28,240 --> 00:19:33,760
But now when the workforce becomes smaller, you are essentially forcing the workforce to

248
00:19:33,760 --> 00:19:35,640
do the same amount of job.

249
00:19:35,640 --> 00:19:39,520
So everyone can do different types of things.

250
00:19:39,520 --> 00:19:43,880
So that's essentially what regularization is trying to do.

251
00:19:43,880 --> 00:19:49,880
Now, when we use dropout in regular deep neural network training approaches, we train

252
00:19:49,880 --> 00:19:51,920
it with dropout turned on.

253
00:19:51,920 --> 00:19:57,840
But during inference, we turn off the dropout and then we adjust the weight accordingly.

254
00:19:57,840 --> 00:20:03,080
Now if you think about this, if you leave the dropout turned on during inference, what

255
00:20:03,080 --> 00:20:07,840
you're getting essentially as samples of the neural network.

256
00:20:07,840 --> 00:20:11,720
Every time you run inference, you get a different neural network.

257
00:20:11,720 --> 00:20:16,280
So if you run inference multiple times, you get multiple different neural networks along

258
00:20:16,280 --> 00:20:19,920
with multiple different sets of weights.

259
00:20:19,920 --> 00:20:26,280
And you also know with what probability or what likelihood each set of weight will happen.

260
00:20:26,280 --> 00:20:31,960
Why do you have different sets of weights when you're running an inference?

261
00:20:31,960 --> 00:20:36,600
I'm imagining that you go through some process of training your model, you create a model,

262
00:20:36,600 --> 00:20:41,000
it's a set of weights, you deploy that on somewhere and you're running inference against

263
00:20:41,000 --> 00:20:42,000
it.

264
00:20:42,000 --> 00:20:43,600
I don't think of those weights as changing.

265
00:20:43,600 --> 00:20:50,040
But if that's the normal way to use dropout, but if you leave it turned on during inference,

266
00:20:50,040 --> 00:20:55,840
what's going to end up happening is you're going to get some neural networks, some weights

267
00:20:55,840 --> 00:20:57,320
such as zero with probability.

268
00:20:57,320 --> 00:20:58,320
I miss that.

269
00:20:58,320 --> 00:21:00,320
We're leaving dropout turned on during inference.

270
00:21:00,320 --> 00:21:01,320
Yes.

271
00:21:01,320 --> 00:21:04,320
And when you do that, you're essentially sampling from the neural network.

272
00:21:04,320 --> 00:21:10,080
So you get different neural networks that looks different, that has different weight configurations.

273
00:21:10,080 --> 00:21:14,800
And if you do it enough times, what you're going to essentially have is all different sets

274
00:21:14,800 --> 00:21:18,600
of weights along with the probability that it will happen.

275
00:21:18,600 --> 00:21:24,920
So we're essentially trying to build the posterior, which is the probability distribution of weights

276
00:21:24,920 --> 00:21:28,600
over training, over all the training data.

277
00:21:28,600 --> 00:21:31,400
So that that is an estimate of the posterior.

278
00:21:31,400 --> 00:21:35,800
But when you do that, it makes all the computation easy.

279
00:21:35,800 --> 00:21:41,720
And then you can actually simplify the expression for prediction probability.

280
00:21:41,720 --> 00:21:45,880
We can rigorously demonstrate that this allows us to estimate the posterior.

281
00:21:45,880 --> 00:21:46,880
Yes.

282
00:21:46,880 --> 00:21:47,880
That's very cool.

283
00:21:47,880 --> 00:21:53,040
It's a very cool, this is actually a very cool paper, an outcome of a very cool paper,

284
00:21:53,040 --> 00:21:55,880
I think, from a year or two ago.

285
00:21:55,880 --> 00:21:58,160
Do you remember the name or author?

286
00:21:58,160 --> 00:21:59,160
So the paper?

287
00:21:59,160 --> 00:22:00,160
Yeah.

288
00:22:00,160 --> 00:22:09,320
L-G-A-L, first name, Yarin, Y-A-A-R-I-N, it was a pretty big deal at that time.

289
00:22:09,320 --> 00:22:18,400
And is there anything particular about the way we do drop out, meaning the drop out parameter,

290
00:22:18,400 --> 00:22:25,640
like is it constrained in any way or any particular pattern to the way we do drop out or doesn't

291
00:22:25,640 --> 00:22:26,640
matter?

292
00:22:26,640 --> 00:22:33,760
So you essentially drop out each layer of weight, and then you just get a sample of the

293
00:22:33,760 --> 00:22:36,800
neural network that way.

294
00:22:36,800 --> 00:22:43,880
So that is one way to think about uncertainty neural network, and that when you couple that

295
00:22:43,880 --> 00:22:49,960
into the active learning process, what you would do is to run drop out during inference,

296
00:22:49,960 --> 00:22:56,440
run multiple sets of inference, and then each set of inference will give you a prediction

297
00:22:56,440 --> 00:22:57,440
probability.

298
00:22:57,440 --> 00:23:03,160
And you take the average of that as your estimate of the uncertainty, and then that gets fed

299
00:23:03,160 --> 00:23:10,320
into other uncertainty methods like entropy, for example, to figure out which points should

300
00:23:10,320 --> 00:23:11,320
get labeled.

301
00:23:11,320 --> 00:23:12,320
Okay.

302
00:23:12,320 --> 00:23:18,800
So you're not taking that estimate directly, you have to then apply it to an entropy-based

303
00:23:18,800 --> 00:23:20,080
model or something like that.

304
00:23:20,080 --> 00:23:21,920
Can you explain how those work?

305
00:23:21,920 --> 00:23:29,280
Yeah, so entropy, the idea of entropy is that outcomes of uncertain events carry more

306
00:23:29,280 --> 00:23:34,280
information when compared to outcomes of events that we are very certain about.

307
00:23:34,280 --> 00:23:40,920
So if you think about coin toss, the maximum entropy for coin toss is one.

308
00:23:40,920 --> 00:23:42,840
So when does that happen?

309
00:23:42,840 --> 00:23:45,160
That happens when we have a fair coin.

310
00:23:45,160 --> 00:23:50,120
When you toss a fair coin, you don't know what you're going to get, right?

311
00:23:50,120 --> 00:23:55,080
The probability is equally likely of you getting a head or a tail.

312
00:23:55,080 --> 00:23:57,400
So that's when you get an entropy of one.

313
00:23:57,400 --> 00:24:04,280
Now when you think of a biased coin, and in the very extreme case, a double headed coin,

314
00:24:04,280 --> 00:24:09,600
that event, that coin toss event has no uncertainty because you know what you're going to get.

315
00:24:09,600 --> 00:24:15,480
So the entropy for that is zero, and the entropy for a double tail coin is also zero.

316
00:24:15,480 --> 00:24:21,560
So for using, when you use entropy and active learning as a way to quantify uncertainty,

317
00:24:21,560 --> 00:24:27,600
what you do is you compute entropy for all your unlabeled data points, and then you select

318
00:24:27,600 --> 00:24:32,160
the entropy, I'm sorry, you select a data points that has the highest entropy to get labels

319
00:24:32,160 --> 00:24:33,480
for.

320
00:24:33,480 --> 00:24:38,160
And the entropy computation depends on the prediction probability.

321
00:24:38,160 --> 00:24:42,240
So in the deep neural network setting, what you would do is you would estimate that you

322
00:24:42,240 --> 00:24:47,600
would get the prediction probability using dropouts, and you feed that probability into the

323
00:24:47,600 --> 00:24:52,240
entropy approach to get your data points to get labels for.

324
00:24:52,240 --> 00:24:55,240
We've got active learning, working for deep learning.

325
00:24:55,240 --> 00:25:04,080
We've got a set of methods that work, is this something that, you know, your typical kind

326
00:25:04,080 --> 00:25:09,720
of enterprise customer can just pull off the shelf and use or are they building it from

327
00:25:09,720 --> 00:25:10,720
scratch?

328
00:25:10,720 --> 00:25:11,720
How hard is it to do?

329
00:25:11,720 --> 00:25:13,720
You make it sound very easy.

330
00:25:13,720 --> 00:25:19,800
That'll just, you know, just do some dropouts, sprinkle some entropy over here.

331
00:25:19,800 --> 00:25:28,040
How hard or easy is it to actually do in practice, and how robust is it, is it, I mean,

332
00:25:28,040 --> 00:25:31,320
it's not actually changing the performance of your network, but the robustness would be

333
00:25:31,320 --> 00:25:36,400
in the degree to which it's, to which it's a sample efficient, right?

334
00:25:36,400 --> 00:25:44,480
Yeah, so let me talk about the, how you would do it first, and talk about the performance.

335
00:25:44,480 --> 00:25:51,520
So the active learning approach is a workflow approach.

336
00:25:51,520 --> 00:25:58,160
What I mean by that is you have to have a workflow that allows you to first take a small set

337
00:25:58,160 --> 00:26:04,560
of label data, build a model with it, use the model along with the active learning strategy

338
00:26:04,560 --> 00:26:09,520
to find the difficult data points, then get labels for those data points.

339
00:26:09,520 --> 00:26:14,440
The labels are provided by human, and then you have to be able to get those labels back,

340
00:26:14,440 --> 00:26:21,200
and then add them back to your original label data set, and then repeat.

341
00:26:21,200 --> 00:26:25,720
So there are a couple of blocks for this to work.

342
00:26:25,720 --> 00:26:29,880
Obviously, you have to be able to build a model, right?

343
00:26:29,880 --> 00:26:34,480
Building a machine learning model, I think many of our clients can do.

344
00:26:34,480 --> 00:26:39,120
You have to build a model, you also have to be able to implement the selection strategy,

345
00:26:39,120 --> 00:26:44,600
and these can just be, these are not very hard to implement.

346
00:26:44,600 --> 00:26:48,520
In real life, it's just a couple of lines of Python code, for example.

347
00:26:48,520 --> 00:26:52,480
You have to be able to compute entropy, you have to be able to, if you're using a deep

348
00:26:52,480 --> 00:26:59,880
neural network, you have to be able to use some neural deep network, appropriate techniques

349
00:26:59,880 --> 00:27:04,360
like dropout or ensemble approaches.

350
00:27:04,360 --> 00:27:10,080
Once you implement those, what you get at that point is a set of data points that need

351
00:27:10,080 --> 00:27:12,080
to be labeled.

352
00:27:12,080 --> 00:27:17,160
Now in the research setting, what usually happens is the person building the model will create

353
00:27:17,160 --> 00:27:19,800
the labels for that.

354
00:27:19,800 --> 00:27:27,320
But if you were to kind of productionize this, you need maybe more workforce to label

355
00:27:27,320 --> 00:27:32,880
these type of data, and the data that we're talking about can either be images or text,

356
00:27:32,880 --> 00:27:35,560
as humans can label those.

357
00:27:35,560 --> 00:27:41,360
So with active learning, we're kind of limited to use cases where the data comes in the form

358
00:27:41,360 --> 00:27:44,200
of text or images.

359
00:27:44,200 --> 00:27:53,120
If you're building an application for detecting, for medical diagnosis, then you actually need

360
00:27:53,120 --> 00:27:56,840
an intelligent workforce to label those images for you.

361
00:27:56,840 --> 00:28:03,600
If you're building applications for text, sometimes you can do it yourself, sometimes you

362
00:28:03,600 --> 00:28:10,040
also need to farm it out to somebody else that can do a larger scale.

363
00:28:10,040 --> 00:28:15,720
Once you have those, you need to be able to get those data point labels back and integrate

364
00:28:15,720 --> 00:28:22,600
it into your system easily and quickly that will allow you to do a second iteration.

365
00:28:22,600 --> 00:28:27,440
So there's kind of the building blocks of active learning, the key is to obviously make

366
00:28:27,440 --> 00:28:30,560
everything as streamlined as possible.

367
00:28:30,560 --> 00:28:34,880
There are a lot of platforms that will connect your data to the workforce that's providing

368
00:28:34,880 --> 00:28:37,320
the labels seamlessly.

369
00:28:37,320 --> 00:28:43,440
So that part is kind of taken care of, but you just have to be aware that this whole process

370
00:28:43,440 --> 00:28:47,640
should be as seamless as possible because you need to iterate.

371
00:28:47,640 --> 00:28:51,120
So that in practice, that's how active learning would work.

372
00:28:51,120 --> 00:28:55,560
Is there something that active learning is not that good at?

373
00:28:55,560 --> 00:28:57,400
Does that affect their performance?

374
00:28:57,400 --> 00:28:59,240
That was your second question, right?

375
00:28:59,240 --> 00:29:00,240
Essentially.

376
00:29:00,240 --> 00:29:01,240
Yeah.

377
00:29:01,240 --> 00:29:03,040
How robust is it?

378
00:29:03,040 --> 00:29:06,400
Is it reliable as a technique?

379
00:29:06,400 --> 00:29:07,400
Yeah.

380
00:29:07,400 --> 00:29:14,160
So there are a couple of things about active learning that we should always be aware of first.

381
00:29:14,160 --> 00:29:18,320
In active learning, there's a learner or a model.

382
00:29:18,320 --> 00:29:21,920
And then there's also the selection strategy.

383
00:29:21,920 --> 00:29:24,040
So the learner makes the predictions.

384
00:29:24,040 --> 00:29:28,520
The strategy steps in and picks the ones that the learner has the most difficulty with,

385
00:29:28,520 --> 00:29:32,280
and then it feeds it back, gets a label and feeds it back.

386
00:29:32,280 --> 00:29:38,400
So in general, in any machine learning problem, picking the right type of learner is difficult,

387
00:29:38,400 --> 00:29:42,880
but it's made even more difficult under the active learning setting for two reasons.

388
00:29:42,880 --> 00:29:49,000
First, you have to pick the learner when you only have a small set of label data.

389
00:29:49,000 --> 00:29:54,320
Second, the learner is not only used to make predictions, it is used in conjunction with

390
00:29:54,320 --> 00:29:56,880
the strategy to help refine it.

391
00:29:56,880 --> 00:30:00,840
So the tight feedback loop between the learner and the strategy amplifies the effect of a

392
00:30:00,840 --> 00:30:02,360
wrong learner.

393
00:30:02,360 --> 00:30:09,720
What generally happens is we advise clients to use maybe not only a single type of learner,

394
00:30:09,720 --> 00:30:16,280
use an ensemble of learners of different types, and that would essentially prevent your

395
00:30:16,280 --> 00:30:21,040
label data pool from being tied to any particular model in general.

396
00:30:21,040 --> 00:30:23,560
So that's one thing.

397
00:30:23,560 --> 00:30:27,400
A second thing is the impact of these strategies.

398
00:30:27,400 --> 00:30:35,800
Some strategies will result in a biased label data pool when you compare to other strategies.

399
00:30:35,800 --> 00:30:39,280
So we can use a very simple example in the margin sampling approach.

400
00:30:39,280 --> 00:30:43,520
What we're trying to do is to pick data points that's very close to decision boundaries

401
00:30:43,520 --> 00:30:46,560
to get labels for.

402
00:30:46,560 --> 00:30:50,600
The data points that are far from the decision boundary, sometimes are not even used

403
00:30:50,600 --> 00:30:52,680
to build the model.

404
00:30:52,680 --> 00:30:59,120
So essentially your model is built using a set of data that's maybe not very representative

405
00:30:59,120 --> 00:31:01,680
of your original data pool.

406
00:31:01,680 --> 00:31:08,560
It's not representative in that, by definition, it's data that the model has problems with.

407
00:31:08,560 --> 00:31:09,560
Yes.

408
00:31:09,560 --> 00:31:14,800
Yes, because we're only picking data points that the model finds hard and then we're kind

409
00:31:14,800 --> 00:31:18,440
of refining on that set of data points.

410
00:31:18,440 --> 00:31:21,800
And those are the data points that we end up getting labels for.

411
00:31:21,800 --> 00:31:26,320
So data points that the models we're clear about are very far from the decision boundary.

412
00:31:26,320 --> 00:31:30,720
We don't request labels for them and then we don't use them when we built the supervised

413
00:31:30,720 --> 00:31:32,760
machine learning model.

414
00:31:32,760 --> 00:31:39,480
Learning that is like a class of bias are there obvious downsides to over indexing on this

415
00:31:39,480 --> 00:31:41,240
particular class of bias?

416
00:31:41,240 --> 00:31:42,240
Yes.

417
00:31:42,240 --> 00:31:47,400
There is a recognition that active learning will result in some kind of bias.

418
00:31:47,400 --> 00:31:52,120
But there are certain types of selection strategies that will help with that.

419
00:31:52,120 --> 00:31:57,280
The example that I gave was on the margin sampling where we look for data points right around

420
00:31:57,280 --> 00:31:58,600
a boundary.

421
00:31:58,600 --> 00:32:05,080
But when you do an entropy approach, for example, you don't really get data sets as biased

422
00:32:05,080 --> 00:32:08,880
because the entropy approach doesn't just look at the distance.

423
00:32:08,880 --> 00:32:11,800
It is a more balanced approach.

424
00:32:11,800 --> 00:32:17,760
There are other approaches such as density-based approaches, for example, selection.

425
00:32:17,760 --> 00:32:25,080
For those approaches, you're only picking from regions where you have dense data.

426
00:32:25,080 --> 00:32:31,800
So essentially, you're trying to counter the bias issue because I just want data where

427
00:32:31,800 --> 00:32:38,360
I do want to pick data points from regions that I have a lot of other data points around

428
00:32:38,360 --> 00:32:39,360
me.

429
00:32:39,360 --> 00:32:45,760
I'm not focusing on weird little pockets of data.

430
00:32:45,760 --> 00:32:53,200
Our report covers the basic of active learning and obviously that is meant to be an introduction

431
00:32:53,200 --> 00:32:57,760
to active learning for our clients and during our advising hours, what we end up doing

432
00:32:57,760 --> 00:33:02,920
very often is going down different technical paths for different clients based on the problem

433
00:33:02,920 --> 00:33:03,920
and their data.

434
00:33:03,920 --> 00:33:08,800
One of the questions I get all the time is I have problem X. How much data do I need to

435
00:33:08,800 --> 00:33:10,800
collect?

436
00:33:10,800 --> 00:33:17,880
Does active learning help me answer that question?

437
00:33:17,880 --> 00:33:21,360
Unfortunately, it does not.

438
00:33:21,360 --> 00:33:23,640
What it does is...

439
00:33:23,640 --> 00:33:30,880
Does it even help me answer that question relative to not using active learning?

440
00:33:30,880 --> 00:33:31,880
Yes.

441
00:33:31,880 --> 00:33:33,880
I'll give you an example.

442
00:33:33,880 --> 00:33:40,720
For all the reports that we do, we also build prototypes that illustrates the capability.

443
00:33:40,720 --> 00:33:45,720
Our prototype for this particular report is a tool that helps you understand the process

444
00:33:45,720 --> 00:33:52,520
of active learning and how different selection strategies affect your approaches and how

445
00:33:52,520 --> 00:33:54,840
does it work on different data sets.

446
00:33:54,840 --> 00:33:58,240
So we looked at three data sets.

447
00:33:58,240 --> 00:34:01,320
We start with the very simple one, the MNIST data set.

448
00:34:01,320 --> 00:34:03,720
That is the Hello World data set of machine learning.

449
00:34:03,720 --> 00:34:08,640
It's a series of images of handwritten digits from 0 to 9.

450
00:34:08,640 --> 00:34:12,080
And then we looked at a slightly more complex data set.

451
00:34:12,080 --> 00:34:14,200
It is the quick draw data set.

452
00:34:14,200 --> 00:34:16,560
It's a series of hand doodles.

453
00:34:16,560 --> 00:34:19,480
We looked at 10 categories.

454
00:34:19,480 --> 00:34:22,400
The final data set that we looked at is the Caltech data set.

455
00:34:22,400 --> 00:34:29,120
It's a series of real life images, of bridges, of animals, for example.

456
00:34:29,120 --> 00:34:33,920
And we also picked 10 categories from those.

457
00:34:33,920 --> 00:34:40,880
And then we also looked at very different types of selection strategies starting from random

458
00:34:40,880 --> 00:34:45,680
strategy to the more complex ones like drop out an entropy.

459
00:34:45,680 --> 00:34:50,960
And we used active learning to build a model using all of these data sets.

460
00:34:50,960 --> 00:34:56,000
And so for the MNIST data set, if you're familiar with the MNIST data set, the training

461
00:34:56,000 --> 00:35:00,080
data actually has 60,000 points.

462
00:35:00,080 --> 00:35:03,280
But we only start with 5,000.

463
00:35:03,280 --> 00:35:08,600
We build a model using 5,000 data points and we look at the model performance and we

464
00:35:08,600 --> 00:35:15,160
use different selection strategies to select 1,000 points to get labels for.

465
00:35:15,160 --> 00:35:19,200
And then that gets added back to the 5,000.

466
00:35:19,200 --> 00:35:22,480
And that starts the next iteration of active learning.

467
00:35:22,480 --> 00:35:33,440
So with 5,000 data points, we're able to get accuracy somewhere in the, I think, 92%.

468
00:35:33,440 --> 00:35:38,360
And as we increment, I think we did maybe seven or eight rounds of active learning.

469
00:35:38,360 --> 00:35:43,200
And we're able to get to 98% tile of 98% accuracy.

470
00:35:43,200 --> 00:35:46,400
So that's an indirect way to answer your question.

471
00:35:46,400 --> 00:35:52,080
What I'm trying to say is you don't need to use 60,000 data points to build your model.

472
00:35:52,080 --> 00:35:59,280
You can, you only need, we ended up with is 5,000 plus maybe 12.

473
00:35:59,280 --> 00:36:05,720
I think what, what it say, what it is answering is that active learning isn't going, you

474
00:36:05,720 --> 00:36:11,440
know, producing any results that says, you know, either given my problem or independent

475
00:36:11,440 --> 00:36:18,160
of my problem, you know, if I do X kind of active learning, I'm going to require 75% less

476
00:36:18,160 --> 00:36:19,440
data or something like that.

477
00:36:19,440 --> 00:36:20,440
It's all experimental.

478
00:36:20,440 --> 00:36:21,440
It's experimental.

479
00:36:21,440 --> 00:36:23,440
Yes, it's experimental.

480
00:36:23,440 --> 00:36:28,880
But what it's saying is that we know we need a lot of data.

481
00:36:28,880 --> 00:36:30,720
So we random, we don't random.

482
00:36:30,720 --> 00:36:34,480
We just kind of brute force create labels for everything.

483
00:36:34,480 --> 00:36:39,680
It turns out a lot of these labels are not useful to the model, right?

484
00:36:39,680 --> 00:36:45,480
So we kind of, if you think about these labeling projects for autonomous vehicles, they range

485
00:36:45,480 --> 00:36:50,000
and they start at a million dollars and go up from there.

486
00:36:50,000 --> 00:36:54,240
You just label everything and then you hope that it works, right?

487
00:36:54,240 --> 00:36:58,480
But when you use active learning in that kind of setting, you realize that, hey, I don't

488
00:36:58,480 --> 00:37:03,120
have to label so many images, I only need to label the images that will be helpful for

489
00:37:03,120 --> 00:37:04,120
my model.

490
00:37:04,120 --> 00:37:10,240
So that obviously brings down the cost and sometimes it allows us to get to a model faster.

491
00:37:10,240 --> 00:37:14,080
And more importantly, the model performs almost just as well, at least in the research setting

492
00:37:14,080 --> 00:37:18,920
when you compare to a model that you would have built using a much larger data set.

493
00:37:18,920 --> 00:37:21,720
How was that in November, December?

494
00:37:21,720 --> 00:37:22,720
I forget.

495
00:37:22,720 --> 00:37:30,400
AWS re-invent, they announced a new extension to the SageMaker platform, SageMaker ground

496
00:37:30,400 --> 00:37:36,600
truth, which incorporates active learning to some extent.

497
00:37:36,600 --> 00:37:41,120
There are some other tools out there.

498
00:37:41,120 --> 00:37:49,080
The folks behind Spacey, the NLP library have a, what is it, prodigy, prodigy, right?

499
00:37:49,080 --> 00:37:53,200
Have a library called prodigy that incorporates active learning.

500
00:37:53,200 --> 00:37:58,160
Have you looked at kind of these off-to-self solutions and what have you seen there?

501
00:37:58,160 --> 00:38:02,480
Yeah, so all of our reports come with a landscape chapter.

502
00:38:02,480 --> 00:38:08,920
We look at open-source, we look at vendors that help with this particular capability, and

503
00:38:08,920 --> 00:38:12,720
this is usually very helpful for clients who are thinking of implementing any of these

504
00:38:12,720 --> 00:38:14,200
capabilities.

505
00:38:14,200 --> 00:38:23,560
So as part of that, we looked at prodigy, we also looked at a lot of labeling providers.

506
00:38:23,560 --> 00:38:29,840
And the main difference between prodigy and AWS SageMaker is that prodigy, at least

507
00:38:29,840 --> 00:38:36,320
from when we're playing with it, is more of a complete solution for active learning.

508
00:38:36,320 --> 00:38:42,640
And I say that because with prodigy, you can actually modify any of your selection strategies.

509
00:38:42,640 --> 00:38:46,880
I think it comes with uncertainty sampling using entropy out of the box.

510
00:38:46,880 --> 00:38:53,600
But you can essentially link to any selection strategies that you write yourself.

511
00:38:53,600 --> 00:38:57,480
You just have to link it in to the project framework.

512
00:38:57,480 --> 00:39:00,560
AWS doesn't have that flexibility.

513
00:39:00,560 --> 00:39:04,240
It's more of kind of a black box type of solution.

514
00:39:04,240 --> 00:39:05,240
Okay.

515
00:39:05,240 --> 00:39:07,000
So that's the main difference.

516
00:39:07,000 --> 00:39:12,080
If I wanted to play with this, what's the quickest way to get started doing it?

517
00:39:12,080 --> 00:39:14,520
You said it's only one line of Python, right?

518
00:39:14,520 --> 00:39:18,480
There's more than one line.

519
00:39:18,480 --> 00:39:24,240
If you're trying to get an intuition for active learning, I would go to our prototype.

520
00:39:24,240 --> 00:39:25,760
It's public.

521
00:39:25,760 --> 00:39:28,760
It's active learner dot fast for labs.com.

522
00:39:28,760 --> 00:39:29,760
Okay.

523
00:39:29,760 --> 00:39:31,520
And you can kind of play with that.

524
00:39:31,520 --> 00:39:36,200
It's a pretty cool visualization of the process of active learning.

525
00:39:36,200 --> 00:39:42,080
If you're trying to do it yourself, you can definitely try prodigy.

526
00:39:42,080 --> 00:39:47,600
If not, you can just start right building your own model.

527
00:39:47,600 --> 00:39:51,640
Once you have the model, all you need to do is to extract out the prediction probability

528
00:39:51,640 --> 00:39:53,840
and feed it into a selection strategy.

529
00:39:53,840 --> 00:39:59,240
You can use start with, we'll always say you should start with random as a baseline.

530
00:39:59,240 --> 00:40:05,960
And then move up to more difficult strategies and see how they impact your data because

531
00:40:05,960 --> 00:40:08,320
everyone has a different data set, right?

532
00:40:08,320 --> 00:40:10,520
It all behaves differently.

533
00:40:10,520 --> 00:40:17,400
But because active learning tend to introduce bias or can introduce bias, it's helpful to

534
00:40:17,400 --> 00:40:22,400
always start at the base, which is the random sampling and then go up to maybe a more difficult

535
00:40:22,400 --> 00:40:29,840
one, which is the least, which is the margin sampling and then go into the entropy approach.

536
00:40:29,840 --> 00:40:35,600
And kind of just look at the data that's surfaced, the data points that surfaced that's asking

537
00:40:35,600 --> 00:40:37,280
labels for.

538
00:40:37,280 --> 00:40:42,360
You can create labels for those yourself and then you can feed it back to your model and

539
00:40:42,360 --> 00:40:44,480
try iterating it that way.

540
00:40:44,480 --> 00:40:51,320
Having kind of explored this area and learned a bunch about active learning in the landscape,

541
00:40:51,320 --> 00:40:56,240
you kind of see a world where active learning becomes kind of a standard, it's a pretty

542
00:40:56,240 --> 00:40:59,520
exotic thing now, right?

543
00:40:59,520 --> 00:41:04,120
Not everyone is doing it every time because it's just a standard tool that we apply.

544
00:41:04,120 --> 00:41:09,920
Do you think that over time it becomes a standard tool that is applied in most situations

545
00:41:09,920 --> 00:41:18,040
or does the bias or other downsides exist such that you want to be more selective about

546
00:41:18,040 --> 00:41:20,600
when you use it?

547
00:41:20,600 --> 00:41:27,680
I think it should be a tool that you use every time when you're starting to build a model.

548
00:41:27,680 --> 00:41:31,720
When you're trying to get a sense of what works with your data and what doesn't, it's

549
00:41:31,720 --> 00:41:36,800
definitely something that you should use also because you might be labeled constraint

550
00:41:36,800 --> 00:41:38,800
at that point.

551
00:41:38,800 --> 00:41:41,040
So it's a good starting point.

552
00:41:41,040 --> 00:41:46,760
Whether or not it will work as a workflow for all projects, it's hard to say.

553
00:41:46,760 --> 00:41:48,840
But the idea is very simple.

554
00:41:48,840 --> 00:41:53,880
What it's trying to do is reduce this amount of labeling that needs to be done, but at

555
00:41:53,880 --> 00:41:56,840
the same time doesn't affect model performance.

556
00:41:56,840 --> 00:42:01,080
So if you just think of that, I would say that...

557
00:42:01,080 --> 00:42:10,240
It should be something that's attempted at least in the beginning, and that will also

558
00:42:10,240 --> 00:42:15,920
help you discover something about your data that you might not have realized before.

559
00:42:15,920 --> 00:42:25,440
It also will help you verify whether a modeling approach will work or not before you label

560
00:42:25,440 --> 00:42:27,120
everything.

561
00:42:27,120 --> 00:42:30,960
But if active learning works, you actually don't have to label everything.

562
00:42:30,960 --> 00:42:36,520
You just continue with the process and you just end up labeling a selection of unlabeled

563
00:42:36,520 --> 00:42:39,960
data that is the most helpful for the model.

564
00:42:39,960 --> 00:42:43,840
And there are many use cases within enterprises.

565
00:42:43,840 --> 00:42:49,560
I mentioned for autonomous vehicles, it's super helpful, which is because the labeling

566
00:42:49,560 --> 00:42:53,960
causes so high and everyone just indiscriminately labeling.

567
00:42:53,960 --> 00:43:01,440
If you think about text-based examples, companies get, for example, many inquiries from the

568
00:43:01,440 --> 00:43:02,440
customers, right?

569
00:43:02,440 --> 00:43:06,280
And these inquiries need to be routed to the right departments.

570
00:43:06,280 --> 00:43:11,120
But right now, the routing portion is always the bottleneck.

571
00:43:11,120 --> 00:43:17,760
We can build a machine learning model to do the routing, but we need labels.

572
00:43:17,760 --> 00:43:21,200
Now when you use active learning, it's not as daunting because you don't have to label

573
00:43:21,200 --> 00:43:25,000
everything in the beginning, you just start with a small set of label data and then you

574
00:43:25,000 --> 00:43:28,680
iterate on that and then you figure out which ones you have to label.

575
00:43:28,680 --> 00:43:34,840
And that will help you relatively quickly come to a model and then you can, if you want

576
00:43:34,840 --> 00:43:40,640
to productionize it, you might then have to label at a larger scale.

577
00:43:40,640 --> 00:43:41,640
Awesome.

578
00:43:41,640 --> 00:43:44,400
Well, it sounds like an exciting project.

579
00:43:44,400 --> 00:43:48,040
Sheldon, thanks so much for taking the time to share it with us.

580
00:43:48,040 --> 00:43:49,040
You're very welcome.

581
00:43:49,040 --> 00:43:50,040
Awesome.

582
00:43:50,040 --> 00:43:51,040
Thank you.

583
00:43:51,040 --> 00:43:57,680
All right, everyone, that's our show for today.

584
00:43:57,680 --> 00:44:01,760
If you like what you've heard here, please do us a favor and tell your friends about

585
00:44:01,760 --> 00:44:03,080
the show.

586
00:44:03,080 --> 00:44:07,800
And if you haven't already hit the subscribe button yourself, make sure to do so so you

587
00:44:07,800 --> 00:44:11,520
won't miss any of the great episodes we've got in store for you.

588
00:44:11,520 --> 00:44:16,520
For more information on any of the shows in our strata data series, visit twomolei.com

589
00:44:16,520 --> 00:44:19,000
slash strata sf19.

590
00:44:19,000 --> 00:44:22,360
Thanks once again to cloud error for sponsoring this series.

591
00:44:22,360 --> 00:44:26,560
Be sure to check them out at cloud error.com slash ml.

592
00:44:26,560 --> 00:44:56,520
As always, thanks so much for listening and catch you next time.

