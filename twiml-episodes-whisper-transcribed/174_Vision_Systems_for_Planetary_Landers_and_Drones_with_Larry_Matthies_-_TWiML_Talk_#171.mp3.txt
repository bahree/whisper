Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. Today we're joined by Larry Mathees, senior research scientist
and head of computer vision in the mobility and robotics division at JPL.
Larry joins us on the heels of two presentations at this year's CVPR conference, the first
on onboard stereo vision for drone pursuit or sense and avoid and another on vision systems
for planetary landers. In our conversation we touch on both of these talks, his work on
vision systems for the first iteration of Mars Rovers in 2004 and the future of planetary
landing projects. Enjoy.
Alright everyone, I am on the line with Larry Mathees. Larry is a senior research scientist
at JPL and head of the computer vision group there within the mobility and robotic system
section, as well as an adjunct professor in computer science at the University of Southern
California. Larry, welcome to this weekend machine learning in AI.
Thank you. It's a pleasure to be here.
Let's get started by having you tell us a little bit about your background and how you
got started working on vision for intelligent systems.
Sure, I actually grew up in Canada. I studied computer graphics for a master's degree and
then I went to Carnegie Mellon for a PhD. I was interested in artificial intelligence.
Back in 1981 I spent a year so as a student of some of the AI faculty there and I felt
that in those days the theoretical underpinnings weren't quite what I was satisfied by so I
ended up working on computer vision for robotics which to me had a nice combination of solid
underpinnings in math and physics and some of the appeal of AI and the kind of the
visual gratification of computer graphics. It was a nice hybrid.
How long have you been at JPL? Since 1989 so I think that's almost 29 years.
Wow. What's your research focus there at the lab?
I've been working on computer vision for autonomous navigation of unmanned vehicles. That's
been our main focus the whole time. Specific activities have shifted. I started working
on autonomous navigation of ground vehicles and pretty much the whole time I've been here.
I've spent roughly half time working on NASA related projects and half time working
on non-NASA projects. Most of our funding here comes from government sources so the non-NASA
work has been mostly places like DARPA and the Army Research Lab. I worked on ground
vehicles. Then I started trying to address landers and orbiters on the NASA side. I always
tried to compliment what we do on the NASA side with related things on the non-NASA side.
In the early 2000s I started working on vision for drones along the way we've done work
on vision for autonomous navigation. Those have been my main activities.
You recently returned from the CVPR conference in Salt Lake City where you presented as part
of a workshop on visual odometry and computer vision based on location clues. That was the
title of the workshop. Can you parse that out for us and what is visual odometry and
what's the role of the location queues and all that?
Visual odometry is a technique used to estimate the motion of a camera. Usually that camera
is attached to something else you care about. In my business that's usually a camera on
the robot. You're using the camera to track nearby features in the environment to estimate
the incremental motion of the vehicle. It's called visual odometry as an analogy to
more traditional forms of odometry based on integrating fuel encoders. It's a visual
dead reckoning. The part about location clues. Visual odometry integrates forward from
some starting point and it drifts in robotics and related applications you often want to
have an estimate of absolute position and where the system is. Visual odometry doesn't
do that. You need to have some other absolute reference and location queues could come from
basically maps to give you estimates of absolute position. In this field there's a buzzword
simultaneous localization and mapping which is all about maintaining the history of where
you've been and updating that whole history. If you should double back and see places
you've been before, that's called loop closure and then you can improve your estimate of
the whole history. Although that loop closure doesn't necessarily tell you where you are
absolutely in the world. It tells you that you're back to some place you've been before
which is often just about as important. How do visual odometry and slam relate to or
visual odometry in particular I guess? How does that relate to pose estimation?
Pose estimation refers to the six degree of freedom, position and orientation of a system
so the camera. Visual odometry is estimating the pose of the camera relative to some previous
reference frame every time you take a picture. Pose estimation is basically part of visual
odometry and so your talk was on vision systems for planetary landers. Can you walk us through
the focus of your talk? Sure. In planetary landers so by planetary we're referring to a lot
of our work at JPL has been focused on landing on Mars. We did some work in the last decade
on techniques for precision landing and landing hazard avoidance on the moon and now we're
working to extend these techniques to be applicable to other places like Jupiter's Moon, Europa,
Saturn's Moon, Titan, Comments and Asteroids. In landing you want to, kind of a progression
of capabilities that we worked on. So you want to have a good estimate of the terrain
relative velocity. So in particular you know the vertical velocity and then the horizontal
velocity those have to be within strict limits that touch down or you have a failed landing.
That doesn't tell you where you are. It tells you what your velocity is. Next step is
you'd like to know where you are. So that's the precision landing part. For that you
need, you know, we're saying earlier visual geometry doesn't tell you in absolute terms
where you are. For precision landing on planetary bodies we want to know in absolute terms
where we are. So we have to have some external reference what has been the most approachable.
Often we have orbiters that take high resolution imagery from orbit of places that we want
to land. And so we've been developing algorithms that we use a downlooking camera on the spacecraft
as it gets close to the surface to take pictures and then we register those pictures with
the orbital imagery to get position updates. And we can use visual geometry as part of
that to track features with this camera during descent to improve the velocity knowledge.
So that complements the map registration to give us position knowledge. And then for places
like Mars where we've got really, really good orbital imagery. So resolution of pixels
on the ground of 25 to 30 centimeters per pixel which is good in this business. We can
see almost all of the landing hazards from orbit before the lander ever gets there.
So we don't really have to do onboard landing hazard detection for Mars. At least not
anything we need you to do so far because we can map the hazards before we get there.
And then we just as long as we know where we are during descent, we can plan a trajectory
to a landing site that we already know is fairly safe. For other places that we want to
go that aren't as well mapped, we will need onboard landing hazard detection. So this
was germane to the focus of the workshop because localization visual geometry are important
in planetary landers and that was the theme of the workshop.
How long have we been using vision-based systems in as part of our approach to trying to
land things on other planets?
So the first use of vision in real time in landing was in the Mars exploration rover mission
so that put the rover two rovers on Mars in basically January of 2004. Those rovers
were called Spirit and Opportunity. So we had a system on that. So those rovers landed
with airbags which were deployed after a parachute phase. So we put a system on that mission
that estimated horizontal velocity in the last two kilometers of descent to the surface
so that that velocity knowledge could be used as part of Redfer Rocket firing logic to
reduce the horizontal velocity to make sure that we get inverse the airbags.
We used airbags for the Mars Pathfinder mission in 1997, very successfully without these
techniques. But the rovers by 2003, 2004 were quite a bit heavier and we learned some
things about the winds on Mars that we didn't know before that gave us some concerns.
So we added this technique to add, given added measure of safety. So that was the first
time vision was used in a planetary landing system in real time onboard.
Tell us a little bit about the general approach that you take to solving these types of problems.
From our conversation before getting started, it sounds like you tend to focus on model
based approaches as opposed to deep learning or data driven approaches.
Well, so in doing computer vision for robotics, which is what this is, and especially what
we do at JPL is enable missions. And so it's applied research. And you've always, so it's
applied research in contrast with basic research at universities. So we've always got a mission
in mind. We've all with those missions always have schedule and budget constraints and
the people who build those missions tend to be a little risk averse.
So what we always need to do is take advantage of anything that can give us advantage.
And so that inherently leads us to multi-sensor approaches. So there's been a tendency, at
least there was for a number of years in in academic computer vision research to study
how much can you do with only a camera. But in our business, you know, there's always
an inertial measurement unit that gives you estimates of velocities and accelerations
in angular rates. So you want to use that. And if you have prior knowledge, you want
to use that. So we take advantage of all of those things. And then in the space business,
space-based computing is much, much less powerful than what we can do on earth. So we're talking
three to four orders of magnitude. So you have to squeeze a lot of capability in a very
small amount of compute. And we're going to places that we've either never been before
or we don't have much, we don't have much, we have some, but not much prior data on.
So it can be hard to do learning-based approaches because you just don't have data to learn from
unless you get into transfer learning and things that are probably a little bit too cutting
edge to put in a space vision yet. So we've tended to rely on a lot of geometry because
that, you know, we've got the solid modeling foundation to do that. You know, what we need
for precision landing and landing hazard avoidance is geometry anyway. We need the geometry
of the terrain, we need the position of the spacecraft, the velocity of the spacecraft.
So that's not necessarily strictly geometry, but it's related. It's all with sensors that
are noisy. So this is fundamentally navigation. You know, the whole history of navigation
research and development has always paid attention to modeling the noise and the sensors
and propagating that into uncertainty in your state estimates using that uncertainty
when the state estimates in planning and control. So we levered all of those techniques.
And you know, where we can and where it makes sense, we're trying to use state-of-the-art
learning methods. So those in particular applied to rovers. Once we're on the ground, we want
to do terrain classification. And when we've had rovers driving in areas that we've seen
from orbit and the rovers, you know, by now on Mars, we've accumulated, you know, several
tens of kilometers of traverse. So we've got a lot of images. So we can now start to
train classifiers. So if the rovers understand whether they're on bedrock or on sand, how
much they might slip, how much they might sink. So we've kind of progressed from, you know,
model based techniques that exploit multi-sensor fusion with very limited computation and very
limited prior data to scenarios where we do have more prior data. And we can see in the
future there are things in the works that we hope would give us much more computing power
in the next decade and be able to use more sophisticated techniques on board.
Can you talk a little bit about the more traditional approach where you're incorporating this
multi-sensor fusion? What are the key research challenges there? What has been the, how would
you characterize the progression of research in that field over the past few years? Ah,
well, you know, in all of computer vision and robotics, we've been helped tremendously
by things that basically came into the field from outside. So, you know, it's cliche to
say that advances in computing power and miniaturization and, you know, more compute for less mass
power volume and cost has been hugely enabling, but it's true. Same thing goes for inertial
sensors. Same thing goes for cameras. So the whole, you know, the invention of CMOS
imagers, which was a big step in miniaturizing cameras and power for cameras. But the,
you know, revolutionary progress in mobile electronics for consumer applications has spun
off effectively, tremendously valuable sensors, processors, and communication hardware that
we can use in robotics. So that's one thing. Within our field, you know, the field
ensured a lot in the time that I've been in it. So, last 30 years, 40 years actually
including grad school, in using, you know, initially common filters and incremental batch
estimation techniques, incremental bundle adjustments. So the whole simultaneous localization
at mapping or SLAM, a literature has developed over the last 35 years or so. 3D perception
has gone from, you know, being hardly possible in the early 80s to being, you know, routine
now. So progress and algorithms for stereo vision that actually work and that are affordable
progress in other 3D sensors, LIDAR, in particular, being small and compact and affordable.
That's made a huge difference. So in what I do for planetary exploration, there's not
a lot of object recognition per se, but that's really where one of the most important research
thrusts these days is in applications on Earth. And so that's an area where people tried
more model-based methods for years and that's all becoming very learning-based. So the deep
learning revolution is having a big impact and how people approach those problems.
You mentioned stereo perception and the use of, I guess, image-based approaches to stereo
perception as opposed to point clouds like LIDAR. Can you talk a little bit about that process
and how that works? I haven't thought too much about that or come across and kind of curious
about that. Okay, so I was referring to, let me call it 3D perception. So 3D perception gives
you a point cloud. LIDARs do that. There's multiple LIDAR technology, but think of it as
kind of light-ranging. So you emit a short pulse of laser light and it reflects back to a detector,
you measure that time of light that gives you range. stereo vision gives you a point cloud
by using two cameras. And you know the relative positions of those two cameras. And if you
can find the same, the image of the same object in both of those cameras, then you can triangulate
where that object is in 3D dimension. So it's just like surveyors. Where surveyors, you know,
look at the same point in the world from different locations and then they can triangulate where
that point is. You do that in stereo vision by having two cameras and software that for every pixel
in, let's say, the left image you find in the right image, the pixel that has the projection
of the same object in the world. And so if you do that at every pixel, then you build up what we
call a depth map and you triangulate. And that gives you a point cloud that is essentially
the same kind of data you get from LIDAR. It's got different noise characteristics. There are
strengths and weaknesses for each approach, but they're both methods to give you point clouds.
Okay, okay. LIDAR is much more expensive than cameras are and yet it's being used on a lot
of autonomous vehicles. Can you maybe talk about some of the relative weaknesses of the stereo
vision based approach? Well, so stereo vision, to do this image match, you need to have some
visual texture because that's what helps you find the same point in both cameras. So if you're in
an environment where you've got a lot of surfaces with very little texture, it's hard to do stereo
vision based range measurement. So think of indoor walls that are painted with very little texture
or outdoors if you're driving on a road that has nice, clean smooth asphalt or concrete that's
not very textured. The only way you can get a range measurement to the middle of those surfaces
with stereo vision is by assuming there's a continuous surface and you basically interpolate
into there. Whereas with the LIDAR, you're admitting LIDAR directly, you need the surface to have
enough reflectivity to bounce that light back to the sensor. What you don't need any texture
per se on the surface and because the LIDAR is admitting its own energy, it actually works better
in the dark than it does during daylight. A stereo vision with typical low cost visible
spectrum cameras, it needs an illuminator to work at night, that illuminator has limited range,
so the new stereo vision system has limited range. The error as a function of distance,
the error in stereo vision based range estimates grows as the square of the distance, whereas in LIDAR,
there's tends to be an analogous error characteristic, but you can design a system to have
typically greater range than a stereo system. On the other hand, as you noted,
the stereo cameras can be quite small and cheap, and what they do for you is they give you,
you get all of your pixels at the same point in time, and you can have a field of view that's
quite large in a horizontal and vertical axis. So you can have a large field of regard,
you can get all of your data simultaneously, which matters if you're moving, because if you're
moving and each pixel comes in at a different point in time, that adds complexity of how do
you relate that all to, if you're trying to build a map of the world and your data is coming in,
it's like in a different point in time, that's a harder problem. Most LIDARs have been scanning
LIDARs, and so they have that issue of motion registration of the pixels. Their LIDARs
called flash LIDARs that give you all of your pixels in the point cloud at the same point in time,
but they typically have narrower fields of regard. So those are some of the trade-offs between the two
sensors. And you also mentioned SLAM a couple of times. How does that tend to work? Well, if you mean
does it work well or poorly, these days it works well, if you need to have to talk with the work
of the latter. Okay, these are all fundamentally least squares problems. So you're setting up
an optimization where you've got measurements, so you've got a set of unknowns. Your unknowns
are the poses for the camera from each picture and the 3D coordinates of all of your landmarks.
So this whole technique has some roots in the field of photogrammetry, which is older than
computer vision. And that was used for aerial surveying for long before computer vision had become
big. And in that community, they called this technique of setting up a large least squares
optimization for all the camera poses and all of the landmarks. They called that bundle adjustment
because they thought of it as a bundle of rays from the camera out to all of the landmarks. And so
your optimization is defined, you know, the positions of all these things that adjust this
bundle of rays to minimize some error measure. So SLAM is basically another term for that kind of
technique. And there's been a lot of progress by, you know, a number of people around the world
in finding ways to do this very computationally, efficiently, incrementally. So historically,
in photogrammetry, people would basically do this offline. They would take all of their measurements,
process them offline, all at once, and give you your map. But a lot of what we want to do. So
in photogrammetry, they came up with this term that I think they called real-time bundle adjustment
where, you know, suppose you take one new picture. Can you add that to your optimization and update
it efficiently? So you get a new map without having to redo all of that computation. So the progress
in SLAM has been finding basically linear algebra tricks to do that more and more efficiently over time.
So that we can, you know, in real-time, take new data and update this network of camera poses and
landmark positions. And it's, you know, it's used more and more sophisticated basically linear
algebra tricks to make it efficient to do this incrementally. So maybe going back to your talk
on the vision systems, your goal was to provide a review of the progress that's been made there,
and some of the challenges that the field has run into and perhaps remain. How far did we
get in this conversation in terms of what you presented there? Are there other pieces that you
can share with us or did we cover the bulk of your talk there? I can add a few tidbits.
So, you know, we talked about how the first vision system, real-time vision system in a planetary
lander was in the Mars exploration rover mission that landed in 2004. We're working on another
rover mission to Mars now that we expect will launch in 2020. And the plan is to have a
precision landing capability in that mission where we do what I was describing, where we have a
downward-looking camera that takes pictures during the sand and onboard and real-time and registers
templates from those pictures to orbital maps. So we're planning to use that in the 2020
rover mission to Mars, so that'll be the first time that's been done. And then looking beyond that,
we are about NASA's interested, very interested in two-pairs moving Europa these days. So we're
studying possibilities for how we land on Europa. So I need to be careful to say there isn't
a fund of mission to do this, but we're studying it and the reason to go there is, you know,
profound scientific questions involved. One of them is, you know, is their life elsewhere
than on Earth? And if there isn't life, are there chemical precursors, you know, kind of on a path
toward the chemistry of life? And we know that Jupiter has a liquid water ocean underneath
a water ice crust, and that kept liquid by gravitational flexing in the graph of the Europa
between the gravity field of Jupiter and some of the other larger moons further away from Jupiter
than Europa. And we can see the effects of that in the surface of Europa, which is all broken up,
basically like ice flows. So the question is, you know, in that liquid water ocean inside
of Europa, could there be chemical processes that are related to the chemical processes of life,
or maybe even microbial life? And that's all very speculative, but we know that one of Saturn's
moons and celadus, it's a fairly small moon, I think it's about 500 kilometers in diameter.
We directly detected water vapor coming out of cracks in the surface of celadus,
and we have pictures from the Hubble Space Telescope of Europa where it looks like there are
water vapor queens coming out of Europa. So that's why we want to go there. The surface of Europa
is extremely rough, much rougher than any place we've tried to land on Mars. The orbital
reconnaissance imagery that we're going to have of Europa is lower resolution than we have for Mars.
So, you know, if we're ever going to land there, we need an intelligent landing system, and so we're
working to extend the techniques that were developed for Mars so that someday they could be applicable
to Europa. You know, elsewhere in the outer solar system, we know there are other places that
have liquid water oceans inside moons, Saturn's moon, Titan is one of those, Titan is unique
in the solar system outside of Earth, in that there's a lot of organic molecules on the surface
of Titan. The surface of Titan is 94 Kelvin, so it's really, really cold, and there are lakes and
seas of liquid methane. So, between those organic molecules on the surface and the liquid water
ocean inside, if that liquid water ever came in contact with all the organics is a lot of
potential interesting chemistry that could happen, that could build up more and more complex
organic molecules, and that might teach us some things ultimately about biology. So, we're
interested in landing there, and that's in in in some respects that's even more challenging.
The terrain is not that bad, but the remote sensing data is even lower resolution than we would
have at Europa, so we have to develop techniques that can cope with that. And then someday, you know,
we had also interested in Venus, you know, there have been landers on Venus, the Soviets put landers
on Venus, but they went to some of the most benign terrain there, so to go to more challenging areas
on Venus. It's going to be hard, you know, Venus has the densest atmosphere in the solar system
by far, and the hottest temperature is on the surface by far, and it's got a very opaque cloud layer,
so a very hard navigation problem on Venus, and we don't know how to solve that yet.
From a computer vision perspective, clearly, you know, when you're looking at these different
missions, there are huge system engineering types of challenges adapting to the constraints of
a given mission. How do the algorithmic requirements change the way you approach each individual
mission, or rather, how does each individual mission change the way you approach the algorithms,
I guess? Yeah, so like I said at the start, we're mission oriented, and the way NASA works,
at least the planetary science, really I think all of all of NASA's science, is we started with
what are the scientific questions you want to answer, and then you work backwards from that,
so what technology do you need, and that's always guided by what can we afford, and what has an
acceptable level of risk. So it's kind of case by case, we look at the science we want to do at
a particular planet or moon, and then how do we do that science, and to, you know, minimize the
cost and the risk, we ask, what can we leverage that we've built before, so it minimizes the
cost of new development and minimizes the risk that it's going to work, so you have to find a
happy medium where you find an engineering solution that gives us the science within affordable cost
and acceptable risk, so this is all motherhood, but that is the process. You also did another
presentation at CVPR on onboard stereo vision for drone pursuit. Can you talk a little bit about
that one? Yeah, so, you know, as I was saying, we try to have complimentary work going on where we
have some long-term objectives for capabilities to develop for NASA, and our charter includes
applying unique expertise to problems in other domains that are complimentary, so, you know,
autonomous navigation of drones is one of those areas that's complimentary and can put an
plug where we're developing. The first ever rotograph for Mars was recently approved to be a
technology demonstration on that 2020 Mars rover mission, so, you know, the work that we were
doing in drone pursuit or sense in the void is complimentary to those things, so, and that was
funded by the Army Research Lab. So, we were inspired by, you know, if you want to have a team
of cooperating drones doing anything, you don't want them to collide with each other,
and in some applications, you might want to minimize communication, so you don't necessarily want
the drones exchanging a lot of information. You may not have access to good external localization
sensors like GPS, so you needed to all be on board, and so we thought, you know, long-term,
you'd really like the drones to be able to detect each other with their own onboard sensors,
so they know that they're present, they can estimate their position and velocity of other nearby
drones, and then, you know, do their own path planning and control with awareness of other
nearby aircraft. So, our initial motivation was cooperative teams, but these techniques are
equally applicable to situations where it's not cooperative, where you might want to be pursuing
another drone, and that's relevant to, you know, what people call counter UAS, or counter
unmanned air systems, so there's a lot of concern about small drones being used for hostile purposes.
So, drone pursuit, when we're doing our research just logistically, it turned out to be easier
to do experiments in the pursuit scenario than in the cooperative scenario, so the paper ended up
describing techniques that were relevant to both the cooperative team scenario and the pursuit
scenario, but all of our testing was in the pursuit scenario, and then the stereo vision aspect
comes back to our earlier discussion about the relative pros and cons of stereo vision versus
LiDAR, so here you've got very limited payload, weight, and power capacity on small drones,
so if we can do something with cameras, especially if those cameras can give us a very wide,
instantaneous field of regard in horizontal and vertical axes, you just can't match that with
the LiDAR these days, and these are both techniques that direct with that measure 3D range,
and through that you can estimate velocity of the other aircraft, so it's quite handy if you actually
know the 3D position and velocity of the other aircraft, so we started with techniques,
in particular stereo vision, that would give us that, those do have range limits, so we can only see
other aircraft, you know, some limited distance away, 10, 15, maybe 20 meters at the outside with
our current cameras, so this was the first paper to show that this was possible and a fully
integrated fashion outdoors without the aid of external position sensing sensors, you know,
there needs to be extensions to be able to see other drones further away, you know, stereo
vision is probably not going to cut it for that, so that probably requires techniques that can
use a single camera, those require more work to develop and test than was within the scope of our
paper. And from the algorithmic perspective is the system used here, primarily a composition of
some of the things that we've already talked about, or are there other techniques that are specific
to this system that might be interesting to explore? Well, the work in this paper was most of
the competition of techniques we've already talked about, so, you know, in some respects,
it's analogous through the constraints we have in space, because onboard computation is very
limited, you know, it's a lot more than we have in space, but it's still pretty limited, so we
were using simple techniques to do reliable 3D perception as shore rings that let us do a first
demonstration of this capability. One of the questions I was asked was, could you use deep learning
to detect the other aircraft? It's a very good question. We had not yet tried that for lack of
training data, but that's certainly conceivable. Another thing that could be used is basically
difference imaging, so, you know, a standard technique in surveillance with stationary cameras
is just to take, you know, pictures over time and subtract them and look at what's changed,
and that lets you pick up things that are moving around the camera, but that's harder to do when
the camera itself is moving, and so that was one of the complexities that was out of scope of
this work, but another approach to detects moving drones from a drone that itself is moving is
background subtraction, but then you have to do motion compensation for the motion of your own drone.
That's something to look at in the future. In the paper itself, you showed standard black and
white images, but then also these multicolored images look like some kind of spectral type of
image or something. What are those? I'm thinking you're probably referring to what we call depth maps,
so yeah, that's a false color illustration of the range at each pixel. So when we do stereo
vision, we get an estimate of range at each pixel. You know, a simple way to visualize that is
just a color code, the range, and then show that as a pixel image. Got it. And so this work that you
did, this was a full end-to-end system, not just the, it wasn't a simulation, you actually put
this on a drone and put it out in the wild, is that right? That is right. How did it do?
Yeah. Well, it did quite well. We tested it on the campus of JPL and on the campus of one of our sponsors.
This problem is a lot easier if the other drone you're looking at is silhouette against the sky.
Then it's actually a very easy problem. So it's much harder if the other drone you're looking at
is seeing against what we call background clutter. So, you know, if it's at, if it's below the horizon
essentially. So if you're seeing it against trees or buildings or ground and that changes over time,
it becomes much harder if you look toward the sun because those, you know, sun causes all sorts
of glare artifacts in the image. And we found that a key advantage of our technique is that it's
pretty robust to those issues because the fact that we're using 3D perception means as long as
the drone is within range, we're not fooled by background clutter because we can segment that 3D
object from the background using the point cloud there. And when we look toward the sun and there
are artifacts in the image, you know, unless things get really, really bad, we can often still
do 3D perception that is adequate to track the drone. And since we've, you know,
let's say before you turn directly at the sun, you had a 3D model, you had a model of the position
and velocity of the other drone from previous images. So you can do prediction for a short amount
of time, you know, if you're blinded by the sun and then probably recover track because you've
got that knowledge, model based knowledge that you can use for prediction. So it didn't work pretty
well. And how do you measure the performance of the system? Do you kind of go through after the
fact and measure maybe distance from where the drone thought the thing it was supposed to be
following was and, you know, compare that to some error function or is there some automated way
that you're able to measure performance? So, you know, that's an excellent question. You always
need some kind of ground truth to measure performance. In our case, we manually label the images.
So we, you know, we marked what bounding boxes are on the drone. Another way to do it would be if
we had, you know, an external sensor that told us where everything is. So if we had GPS, for example,
on both aircraft, you could use that to give position knowledge. And in this case, I think we relied
on manual labeling. We could have the sensor infrastructure finished to do that all with GPS.
But that would help automate things. Well, great. Larry, this was a really interesting,
really interesting conversation. I appreciate you taking the time to share with us what you're
working on. Well, thank you for the time to talk about it. I appreciate that. Thanks.
All right, everyone. That's our show for today. For more information on Larry or any of the topics
covered in this episode, head over to twimmelai.com slash talk slash 170. If you're a fan of the pod,
we'd like to encourage you to pop open your Apple or Google podcast app and leave us a five-star
rating and review. Your reviews go a long way in helping new listeners find the show.
As always, thanks so much for listening and catch you next time.
