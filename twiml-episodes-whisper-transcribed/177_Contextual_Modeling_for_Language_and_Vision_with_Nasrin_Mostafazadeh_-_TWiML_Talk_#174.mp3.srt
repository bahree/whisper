1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:35,040
I'm your host Sam Charrington. Today we're joined by Nasreen Mastifazade, senior AI research

4
00:00:35,040 --> 00:00:41,360
scientist at New York-based Elemental Cognition. Our conversation focuses on Nasreen's work

5
00:00:41,360 --> 00:00:46,800
in event-centric contextual modeling and language and vision, which she sees as a means of giving

6
00:00:46,800 --> 00:00:52,600
AI systems a bit of common sense. We discuss Nasreen's work on the story closed tests, which

7
00:00:52,600 --> 00:00:58,360
is a reasoning framework for evaluating story understanding and generation. We explore the details

8
00:00:58,360 --> 00:01:03,200
of this task, including what constitutes a story, and some of the challenges it presents and

9
00:01:03,200 --> 00:01:09,120
approaches for solving it. We also discuss how you model what a computer understands, building

10
00:01:09,120 --> 00:01:14,920
semantic representation algorithms, different ways to approach explainability, and multimodal

11
00:01:14,920 --> 00:01:19,720
extensions to her contextual modeling work. Enjoy.

12
00:01:19,720 --> 00:01:25,720
Alright everyone, I am on the line with Nasreen Mastifazade. Nasreen is a senior AI research

13
00:01:25,720 --> 00:01:31,120
scientist at Elemental Cognition in New York City. Nasreen, welcome to this week in machine

14
00:01:31,120 --> 00:01:34,800
learning and AI. Thanks Sam, thanks for having me.

15
00:01:34,800 --> 00:01:39,920
So I hear that you started working on AI in high school. Tell us a little bit about that

16
00:01:39,920 --> 00:01:48,240
story. Sure, yeah. So I was into computer science and computer engineering in general.

17
00:01:48,240 --> 00:01:53,920
Like, when I was a kid, you know, I basically, that was the toy that I had. And I always

18
00:01:53,920 --> 00:01:58,680
loved the idea of doing something meaningful, but in a sense that, okay, I'm spending

19
00:01:58,680 --> 00:02:04,320
a lot of time in front of computer doing different stuff. What is the best thing that you

20
00:02:04,320 --> 00:02:09,160
can accomplish? And somehow I was introduced to the notion of programming and the fact

21
00:02:09,160 --> 00:02:15,000
that you can build like pieces of software to do things for you. And to me, the idea of

22
00:02:15,000 --> 00:02:20,200
building a piece of software that kind of automates what you do was really intriguing,

23
00:02:20,200 --> 00:02:27,520
which is how I got into robotics initially. So, you know, I started working on robotics

24
00:02:27,520 --> 00:02:32,480
for the couple of my really great friends back in high school that, you know, we then

25
00:02:32,480 --> 00:02:39,800
ended up competing in Robocop competitions, which is this, you know, annual worldwide competitions

26
00:02:39,800 --> 00:02:47,120
among different roboticists to, you know, accomplish different tasks, basically, with robots.

27
00:02:47,120 --> 00:02:51,920
And yeah, so the story, you know, we'll go forward with like how I did that for like a

28
00:02:51,920 --> 00:02:56,960
year and a half, and it was fantastic and we accomplished a lot. And I was always under

29
00:02:56,960 --> 00:03:03,840
impression that it's actually a very complex task, right? You build these multi-agent systems

30
00:03:03,840 --> 00:03:08,160
that are capable of integrating their world model with like different communications

31
00:03:08,160 --> 00:03:13,480
that then the agents and like control aspects, like mechanics, imagined electronics, and

32
00:03:13,480 --> 00:03:19,920
all those sorts of things are really complex. And then somehow through some things that

33
00:03:19,920 --> 00:03:25,360
I'm not going to tell the story of today, I got introduced to the idea that National Language

34
00:03:25,360 --> 00:03:30,320
Understanding and National Language Processing as a field in general is much more complex

35
00:03:30,320 --> 00:03:37,280
than robotics. And that kind of blew my mind like, as I said, I thought I'm working on something

36
00:03:37,280 --> 00:03:42,720
that is, you know, super high tech, which it was, but turned out that at the time, this

37
00:03:42,720 --> 00:03:48,480
is like 12 years ago, there were no systems that could do National Language Understanding

38
00:03:48,480 --> 00:03:54,080
at the level of a four-year-old kit. So, this was an interesting challenge for me to take,

39
00:03:54,080 --> 00:03:59,280
which is how I kind of switched gears and started working on National Language Understanding

40
00:03:59,280 --> 00:04:06,400
and National Language Processing since then. And so, the focus of your work since then has been

41
00:04:06,400 --> 00:04:12,720
largely centered around this idea of event-centric contextual modeling in language and vision.

42
00:04:12,720 --> 00:04:21,360
What exactly does that mean? Yeah, so I think we can start with the events first, right? So,

43
00:04:21,360 --> 00:04:27,600
I've always been interested in looking at the world, basically, through the lens of events,

44
00:04:27,600 --> 00:04:33,920
because events are such central entities of the world through which we, you know,

45
00:04:33,920 --> 00:04:40,160
go about our daily lives, we see things that happen as a result of some stuff that happen,

46
00:04:40,160 --> 00:04:45,760
which is an event. So, an event can cause something else. And then our understanding of the

47
00:04:45,760 --> 00:04:51,840
dynamics of such events and how they shape our world on a day-to-day basis is really, you know,

48
00:04:51,840 --> 00:04:58,560
a crucial part of any kind of cognitive ability. And that makes, you know, me and a lot of other AI

49
00:04:58,560 --> 00:05:04,400
scientists very interested in modeling events. So, that's the event-centric part. And then the

50
00:05:04,400 --> 00:05:10,880
context modeling is about, okay, I have an AI system, I want to build an AI system that can get

51
00:05:10,880 --> 00:05:15,680
an input and then produce some sort of an output, right? That input could be something super complex,

52
00:05:15,680 --> 00:05:22,800
imagine an entire world being perceived by a situated robot, or it could be a piece of text,

53
00:05:22,800 --> 00:05:28,960
right? Or it could be a piece of, you know, an input, which is multimodal, so both text and an

54
00:05:28,960 --> 00:05:35,440
image. So, that's where what context means in my work, basically. So, contextual modeling means

55
00:05:35,440 --> 00:05:42,000
how would the AI system deal with, representing and understanding the input that is provided with.

56
00:05:42,000 --> 00:05:47,040
And then the language on vision part is basically where I've applied this set of, you know,

57
00:05:48,000 --> 00:05:51,280
you know, different pieces of work on event-centric contextual modeling.

58
00:05:51,280 --> 00:05:57,200
My am in nature, I'm in natural language processing researcher, but I've worked on applying

59
00:05:57,200 --> 00:06:03,840
different, you know, AI, you know, methodologies, basically, in the world of vision and language,

60
00:06:03,840 --> 00:06:08,560
which has become, you know, more hot, basically, in the past three years or so.

61
00:06:08,560 --> 00:06:14,800
So, what are some of the types of problems that you come up with and try to solve in this

62
00:06:14,800 --> 00:06:21,920
context of event-centric contextual modeling? So, there are different aspects of the world that

63
00:06:21,920 --> 00:06:28,160
we can basically see through the lens of events, right? One major piece of work that I've done

64
00:06:28,160 --> 00:06:33,760
is on understanding stories, right? As you can imagine, narratives or stories are these

65
00:06:33,760 --> 00:06:41,600
sequence of really eventful sentences that we, you know, basically have to go through on a daily

66
00:06:41,600 --> 00:06:47,280
basis because as humans, we tell stories all the time, right? When we communicate stories,

67
00:06:47,280 --> 00:06:52,480
our major part of how we communicate with each other. And so, one of the, you know, lines of work

68
00:06:52,480 --> 00:06:57,040
that I've invested on in the past couple of years has been on narrative understanding or story

69
00:06:57,040 --> 00:07:02,720
understanding where you want to build the AI systems. They can read, you know, a coherent

70
00:07:02,720 --> 00:07:08,960
sequence of sentences which are event-centric in, you know, in nature. And then it should understand

71
00:07:08,960 --> 00:07:15,120
it in a way that it can answer questions about it. And more so that it could also be able to

72
00:07:15,120 --> 00:07:21,200
generate meaningful stories. So, this work, you know, basically involves not only understanding

73
00:07:21,200 --> 00:07:27,280
a piece of text which happens to be narrative, but also be able to generate, you know, a sequence

74
00:07:27,280 --> 00:07:33,520
of sentences that go together coherently as a meaningful story. And so, how do you tend to approach

75
00:07:33,520 --> 00:07:41,680
that type of problem? Yeah, so, you know, there are different things that you should take care of,

76
00:07:41,680 --> 00:07:47,280
right? If you want to build an AI system that can, say, understand stories. First and foremost is,

77
00:07:47,280 --> 00:07:53,920
okay, what is an event in the story, right? Are we talking about, like, you know, novel by Shakespeare

78
00:07:53,920 --> 00:08:01,200
or are we talking about, like, a sequence of, I don't know, four words? There are people who call

79
00:08:01,200 --> 00:08:07,760
sequence of five words also stories. So, in my work, I particularly, you know, focused on

80
00:08:07,760 --> 00:08:14,960
understanding a sequence of five sentences stories. So, these are, we call it common-sense stories

81
00:08:14,960 --> 00:08:22,080
in a way that we are basically interested in understanding the most, like, daily things that

82
00:08:22,080 --> 00:08:28,640
happen to anyone, you know, in general. That's why we call it common-sense. And we go about

83
00:08:28,640 --> 00:08:32,080
understanding them. So, that's the first thing you have to address, right? What is an event in

84
00:08:32,080 --> 00:08:37,200
the story? After you address that, then you should think about, okay, now I want to build a system

85
00:08:37,200 --> 00:08:42,960
that can understand this, just kind of an input. And then it goes about different, you know, steps

86
00:08:42,960 --> 00:08:49,040
that you should take. So, one is, okay, how do I even represent this piece of, you know, input,

87
00:08:49,040 --> 00:08:55,040
which is a narrative? So, it goes, you know, hand in hand with notions of knowledge representation.

88
00:08:55,040 --> 00:09:00,640
So, in the AI community, from, like, you know, back decades ago, people invested a lot of time

89
00:09:00,640 --> 00:09:06,640
and effort on knowledge representation. And in the NLP community, in particular, we have a lot

90
00:09:06,640 --> 00:09:11,760
of literature on semantic representation and meaning representation, right? So, in my work,

91
00:09:11,760 --> 00:09:17,760
I'm mainly interested in extracting events, right? So, what is, in particular, important about

92
00:09:17,760 --> 00:09:23,200
this story, let's call those events and let's extract those and call it the kind of representation

93
00:09:23,200 --> 00:09:28,160
that I'm interested in. So, that will be basically the second thing you do. And then the third

94
00:09:28,160 --> 00:09:33,520
thing is, okay, now I want to, you know, basically connect the dots. How do I know what are the

95
00:09:33,520 --> 00:09:40,560
stereotypical relations that exist between these events in a story? And then that will give you

96
00:09:40,560 --> 00:09:46,640
this so-called narrative structure of the story that you can basically get in as an input to any

97
00:09:46,640 --> 00:09:52,240
other, you know, system that wants to, say, answer questions about this story. So, that's more or

98
00:09:52,240 --> 00:09:56,160
less, you know, in big picture, how you would go about a story modeling?

99
00:09:57,680 --> 00:10:03,200
So, you started out with talking about how the first thing to understand is, what is a story?

100
00:10:03,760 --> 00:10:07,920
But it strikes me that there's also this question, you know, what do we even mean by understand,

101
00:10:07,920 --> 00:10:15,680
when we're talking about a computer understanding a story? And there's more to it necessarily than,

102
00:10:15,680 --> 00:10:22,720
you know, answering questions. How do you model or assess a computer's ability to understand

103
00:10:22,720 --> 00:10:27,600
a story? Or is that even, you know, part of what you're trying to get at, or is it more performance

104
00:10:27,600 --> 00:10:33,520
on individual tasks? Yeah, that's a very good question, right? And I would say a fundamental question

105
00:10:33,520 --> 00:10:39,520
for the entire AI community. So, the way we go about defining what understanding even is and

106
00:10:39,520 --> 00:10:45,280
the field these days is through these benchmarks that we define, right? So, actually, I personally,

107
00:10:46,000 --> 00:10:50,960
you know, contributed to a benchmark for story understanding, which is a story closed test.

108
00:10:51,600 --> 00:10:58,720
So, the kind of benchmark we defined in particular was as follows. So, the AI system is supposed

109
00:10:58,720 --> 00:11:05,680
to read a sequence of four sentences, which is the context of the story, basically. And then the

110
00:11:05,680 --> 00:11:10,640
task is to predict the ending to that story. So, in the five sentences, the stories that I told you,

111
00:11:10,640 --> 00:11:15,200
you basically imagine that you dropped the last sentence, and the task for the AI system is to

112
00:11:15,200 --> 00:11:22,400
predict that. So, I personally believe that is a good kind of a proxy for modeling, whether or not

113
00:11:22,400 --> 00:11:27,920
a system is understanding the story that it has read. That's one way of putting it, but as it goes

114
00:11:27,920 --> 00:11:36,400
with many benchmarks in the AI world, whenever you have a task and you make it into a benchmark and

115
00:11:36,400 --> 00:11:42,720
then you collect a very particular narrow, in a sense, a test set for it could be hacked, right?

116
00:11:42,720 --> 00:11:49,200
And it goes to say that maybe we shouldn't just define understanding in terms of beating a

117
00:11:49,200 --> 00:11:55,440
particular benchmark, but more so deploying systems in the wild. So, I would say that there are

118
00:11:55,440 --> 00:12:01,120
different ways that you can define what true understanding even is. Having benchmarks is a good

119
00:12:01,120 --> 00:12:06,960
way of making, you know, having proxies, right? And having basically evaluating ourselves as we move

120
00:12:06,960 --> 00:12:12,560
forward, but they're not the ultimate. This should not be the ultimate end goal. I would say of

121
00:12:13,200 --> 00:12:20,400
what we call true understanding. So, I will go to, you know, to add that. So, understanding

122
00:12:20,400 --> 00:12:25,440
could be through the lens of answering questions, right? So, I ask the system, what is the ending to

123
00:12:25,440 --> 00:12:30,560
this story? And if it answers it correctly, that means it's understood, right? But as it goes,

124
00:12:31,360 --> 00:12:36,400
it could be hacked, meaning that without true understanding, maybe a system, a black box system

125
00:12:36,400 --> 00:12:42,560
can actually specify the right ending. If you build systems that can explain themselves,

126
00:12:43,280 --> 00:12:48,240
that could be a win, I would say. So, which is a focus that I have at Elemental Cognition now,

127
00:12:48,240 --> 00:12:54,240
where we are building AI systems, in particular, a story understanding systems that can explain

128
00:12:54,240 --> 00:13:01,760
the decisions they make, which is a better way of kind of knowing what's behind the scenes,

129
00:13:01,760 --> 00:13:05,360
what's, you know, under the hood for the system that is making a decision.

130
00:13:06,080 --> 00:13:10,960
Before we dive into that, I want to make sure I understand the story closed test. You

131
00:13:10,960 --> 00:13:21,680
are training your system, presumably, on some corpus of five sentence stories. And then you're giving it

132
00:13:22,320 --> 00:13:29,120
four sentences that form some new unseen story and asking it to complete the sentence.

133
00:13:29,760 --> 00:13:33,520
And it's to complete the story, yeah. To complete the story, right? So, to provide the

134
00:13:33,520 --> 00:13:43,920
exact three. The final sentence. And in doing so, you demonstrate that it can draw out entities

135
00:13:43,920 --> 00:13:53,360
and contexts from the story and present them in some way that makes sense. Is it, you know,

136
00:13:53,360 --> 00:13:59,440
does, is a human grading the, the responses? I guess the, the origin of that question

137
00:13:59,440 --> 00:14:03,680
is strikes me that generally speaking for this kind of task, you could have multiple correct

138
00:14:03,680 --> 00:14:10,720
output answers. So how does that work? Exactly. So that's actually what we did. We ended up collecting

139
00:14:10,720 --> 00:14:15,520
alternative endings and then the AI, you know, system is posed with two alternatives. One is a

140
00:14:15,520 --> 00:14:19,920
wrong ending, one is the right ending, and then the task is to choose the right one. So that's

141
00:14:19,920 --> 00:14:24,160
actually the kind of a classification task that we ended up doing. But as you can imagine,

142
00:14:24,160 --> 00:14:30,320
the task could be generation, right? The agent could be just posed with the four sentences and then

143
00:14:30,320 --> 00:14:35,040
the task is to generate the ending, basically open ended, right? As opposed to multi-choice

144
00:14:35,520 --> 00:14:41,360
question. But yeah, for the actual story closed, as we ended up collecting the right and wrong

145
00:14:42,000 --> 00:14:47,440
endings by crowdsourcing and then at the end of the day, it becomes a classification task.

146
00:14:47,440 --> 00:15:00,080
And sourcing these answers, were you targeting specific mechanisms of correctness, meaning were

147
00:15:00,080 --> 00:15:07,840
you trying to test specific aspects of the algorithm? So for example, I forget whose work this is,

148
00:15:07,840 --> 00:15:14,400
maybe I've seen it in the context of Josh Tenenbaum's work, but this, the idea that, you know, within

149
00:15:14,400 --> 00:15:20,960
this concept of context, there's so much that's unsaid. You know, the cup is on the table,

150
00:15:20,960 --> 00:15:26,160
you know, there are physical forces that, you know, keep the cup on the table, it's not going to

151
00:15:26,160 --> 00:15:31,680
fall unless some other force pushes it off. That kind of thing, like, are you, I imagine you could

152
00:15:31,680 --> 00:15:37,360
create sentences that kind of test that common sense context, or there are other things that you can

153
00:15:37,360 --> 00:15:45,040
target to test with your sentences. Right. Yeah. So there are, you know, you can, I can imagine having

154
00:15:45,040 --> 00:15:52,480
stories that will, you know, kind of evaluate such cognitive abilities of a system. But the point

155
00:15:52,480 --> 00:15:59,120
about the story closed test is that it's about generic daily life events that happen to anyone,

156
00:15:59,120 --> 00:16:03,920
which is why they're called common sense. So I can give you an example. So this is an example,

157
00:16:03,920 --> 00:16:09,280
you know, a story closed test. So the context is as follows. Karen was assigned a roommate, her

158
00:16:09,280 --> 00:16:14,800
first year of college, and then the story continues that her roommate asked her to go to a nearby

159
00:16:14,800 --> 00:16:22,000
city for a concert, and then Karen agreed happily. The show was absolutely exhilarating,

160
00:16:22,000 --> 00:16:26,640
and then there are two alternative endings to the story. One is Karen became good friends with

161
00:16:26,640 --> 00:16:31,760
her roommate, and the wrong ending, the second ending, which I gave it away is the wrong ending,

162
00:16:31,760 --> 00:16:38,640
is Karen hated her roommate. Right. So this is the level of complexity and generality, actually,

163
00:16:38,640 --> 00:16:46,480
the VR tackling for the story closed test. So it's really about a daily story that has happened,

164
00:16:46,480 --> 00:16:51,920
can happen to anyone. And in particular, we have crowd sources, as I said, by very generic prompts

165
00:16:51,920 --> 00:16:57,600
to the workers. So it's more about predicting the next event, which happens to be the,

166
00:16:57,600 --> 00:17:02,640
you know, ending to the story, as opposed to the kind of phenomena that, yeah, you describe.

167
00:17:03,200 --> 00:17:09,760
Was the algorithm trained only on similar stories, or was it trained on other

168
00:17:10,720 --> 00:17:17,600
information that wasn't in that same story format? Yeah. So he kind of set up this

169
00:17:18,320 --> 00:17:24,240
challenge being the story closed test, so that people can do whatever they can to accomplish the

170
00:17:24,240 --> 00:17:32,880
task. So we did provide a corpus of 100,000 stories called rockestories. These are full five

171
00:17:32,880 --> 00:17:39,440
sentences stories that actually people can use kind of as a positive examples. They can

172
00:17:39,440 --> 00:17:44,640
mine narrative structures from them. They can build like models that predict what happens next

173
00:17:44,640 --> 00:17:50,960
in the story and so on, but it's not directly labeled data for the task of story closed test.

174
00:17:50,960 --> 00:17:56,320
And given that, we've left people, you know, free to use whatever corporate out there,

175
00:17:56,320 --> 00:18:02,640
whatever resources, common sense knowledge bases, or modeling techniques that they have to tackle

176
00:18:02,640 --> 00:18:07,360
the task. So that's basically on our end. We wanted people to, you know, the research community

177
00:18:07,360 --> 00:18:14,560
to basically improvise and be creative and bring in their own resources. So yeah, that's about it.

178
00:18:14,560 --> 00:18:21,840
What kind of engagement or uptake have you seen on this test? Have you seen any interesting

179
00:18:21,840 --> 00:18:31,440
approaches to building out algorithms to? Yeah. Yeah. So it happens that then, so the reason I made

180
00:18:31,440 --> 00:18:37,680
story closed test in the first place is that there wasn't any, you know, systematic way of

181
00:18:37,680 --> 00:18:43,840
evaluating story understanding in the field. So, you know, to get rid of my colleagues, we made

182
00:18:43,840 --> 00:18:50,800
this benchmark so that we have a way of evaluating our progressies moving forward in the field.

183
00:18:50,800 --> 00:18:56,400
And because, as I said, there wasn't any such benchmarking place, what we made actually got a lot

184
00:18:56,400 --> 00:19:02,960
of attention, specifically because we showed that human does 100% on this task. So we actually made

185
00:19:02,960 --> 00:19:08,640
sure that the test set that we put out there is like, you know, doubly human verify so that there

186
00:19:08,640 --> 00:19:15,840
are no boundary cases before choosing the right versus the wrong ending. And also the best,

187
00:19:16,560 --> 00:19:22,160
you know, state-of-the-art model that we trade that we tried on the benchmark was getting some

188
00:19:22,160 --> 00:19:29,040
very around like 59%. And as you can imagine, random baseline would do 50% human was doing 100%,

189
00:19:29,040 --> 00:19:34,960
so there was like more than 40% gap between the best system results and the human performance.

190
00:19:34,960 --> 00:19:39,520
So, you know, these two characteristics, I would say, really contributed to the task getting a lot

191
00:19:39,520 --> 00:19:44,240
of attention. And, you know, since then, this was released about two years ago, a lot of teams,

192
00:19:44,240 --> 00:19:48,800
different teams, you know, from academia and industry, however, have, you know, tackled the task,

193
00:19:48,800 --> 00:19:53,920
there has been so many results. But I go back to the point that I made about hacking, right? So

194
00:19:53,920 --> 00:19:59,520
after we released this data set for many months, maybe, you know, eight, nine months, ten months,

195
00:19:59,520 --> 00:20:04,320
there wasn't any significant improvement. And then we made it into a share task, meaning that, you

196
00:20:04,320 --> 00:20:11,120
know, we had a challenge and this workshop and there was a prize for the winner up. And then,

197
00:20:11,120 --> 00:20:16,640
when, whenever you do that, there are different approaches that will come at you, which is actually

198
00:20:16,640 --> 00:20:21,760
very healthy for a benchmark so that you know exactly what works and what doesn't. And in that,

199
00:20:22,480 --> 00:20:28,480
you know, challenge that we ran basically, that challenge day, there was a team submission from

200
00:20:28,480 --> 00:20:34,560
UDUB that had found out that actually without reading the context, which is the whole point about

201
00:20:34,560 --> 00:20:39,680
this national language understanding framework, without reading the context, you can leverage some,

202
00:20:40,800 --> 00:20:48,160
you know, stylistic features isolated in the endings, just alone that. Yeah, to find the right

203
00:20:48,160 --> 00:20:53,840
ending. And the interesting fact about this group is that so that, you know, the guy that actually

204
00:20:53,840 --> 00:21:00,480
contributes to this model used to work on detecting a fake eout previews and like detecting

205
00:21:00,480 --> 00:21:05,760
notions such as like age or gender from, from a piece of text. And it happened that turns out

206
00:21:05,760 --> 00:21:11,840
our wrong endings actually without our knowledge. Our wrong endings had some, had similar features

207
00:21:11,840 --> 00:21:18,640
as with fake reviews. So they had made this, you know, kind of a very interesting, you know,

208
00:21:18,640 --> 00:21:22,960
observation and they had leveraged that and turned out that you could do, you know, really much

209
00:21:22,960 --> 00:21:29,600
better if you just just train a classifier that just detects the such features. So that was a very

210
00:21:29,600 --> 00:21:33,840
interesting outcome of that challenge that we ran. And it's still, you know, there was a still a

211
00:21:33,840 --> 00:21:39,120
huge gap between human performance and their performance, which was around 70, like two, I think,

212
00:21:39,120 --> 00:21:46,240
or 76%. But it still was such a good example of what can go wrong when you collect data, right?

213
00:21:46,240 --> 00:21:51,120
For a narrow test, as I said, in AI. And this isn't something that has happened only to story

214
00:21:51,120 --> 00:21:57,920
close test, like other benchmarks in the field such as natural language inference, NLI or VQA

215
00:21:57,920 --> 00:22:05,040
visual question answering have basically showcased similar patterns that when we go about collecting

216
00:22:05,040 --> 00:22:12,960
data sets for testing purposes, they could be really biased. And often those biases are not even

217
00:22:12,960 --> 00:22:17,520
revealed to us. If you're lucky, we'll catch some of them. But often they're not revealed. And then,

218
00:22:17,520 --> 00:22:21,680
you know, we can go forward without knowing them and then patting yourselves on a shoulder that

219
00:22:21,680 --> 00:22:26,320
we are doing, you know, deep language understanding and like, you know, just, you know,

220
00:22:27,040 --> 00:22:31,200
claim victory that we are surpassing, I don't know, things like human performance, like

221
00:22:31,920 --> 00:22:39,600
but the truth is that a narrow benchmarks are narrow in the sense that they don't really represent

222
00:22:39,600 --> 00:22:43,520
the world. And we should be really careful with the way that we collect our data.

223
00:22:43,520 --> 00:22:49,520
Right. And I'm sorry I wanted this that we were very careful actually with the way we collected

224
00:22:49,520 --> 00:22:54,560
the story close this and still there were biases that there was no way we would have known ahead

225
00:22:54,560 --> 00:22:59,840
of time. So again, goes about saying how much more care as a community, as a research community,

226
00:22:59,840 --> 00:23:07,280
we should give to exploring, you know, different ways of collecting data and vetting them. And then,

227
00:23:07,280 --> 00:23:14,880
as I said, more so, I believe in the in the power of testing your systems on multiple frameworks

228
00:23:14,880 --> 00:23:19,600
and making sure that they scale beyond the particular training data that they're trained on

229
00:23:19,600 --> 00:23:24,480
or overfitted on basically. You created this benchmark. Have you also

230
00:23:25,600 --> 00:23:32,800
participated on the the other side of it building out models to try to improve on the state of the

231
00:23:32,800 --> 00:23:40,400
art? Yeah. So actually, so the one, you know, I worked on this. We had many different systems,

232
00:23:40,400 --> 00:23:46,400
as I said, but they weren't none of them were doing fantastically. They were basically kept at

233
00:23:46,400 --> 00:23:54,080
60-some percent. And then actually the most recent thing that we did after realizing that with the,

234
00:23:54,080 --> 00:24:00,320
you know, new results that are particularly shared in the challenge day was to come up with

235
00:24:00,320 --> 00:24:09,040
another dataset so that these hidden biases are kind of, you know, taking care of. And so now, you

236
00:24:09,040 --> 00:24:14,560
know, I research the research team that I'm working on right now at Elemental Cognition.

237
00:24:14,560 --> 00:24:20,960
We're working on different kinds of stories that are not exactly rocket stories, but technically,

238
00:24:20,960 --> 00:24:28,800
you know, you can test and, you know, evaluate our system on rocket stories as well. But it's just a

239
00:24:28,800 --> 00:24:33,520
more challenging task that we are tackling right now than the story closed test.

240
00:24:34,240 --> 00:24:40,720
Tell me about the types of approaches that you would take for, you know, either of these types of

241
00:24:40,720 --> 00:24:45,440
tasks. I guess when I hear semantic representation, the first thing that I think of from a deep learning

242
00:24:45,440 --> 00:24:51,920
perspective is like embeddings. Are there, does that come in the play or are there other, what are

243
00:24:51,920 --> 00:24:57,120
the kinds of techniques that come in the play? Yeah, I will tell you about the best model that we had,

244
00:24:57,120 --> 00:25:04,000
which was doing, you know, 59% when we released the dataset, basically. So that was a model that was

245
00:25:04,000 --> 00:25:10,960
basically, as imagine, a sentence embedding model. All it did was that you want to build a model

246
00:25:10,960 --> 00:25:20,400
that embeds the context, which is four sentences, in the shared semantic space with the right ending.

247
00:25:20,400 --> 00:25:25,920
So basically, we trained these two parallel neural nets, one of which will embed the context,

248
00:25:25,920 --> 00:25:30,720
one of which will embed the ending. And at the end of the day, you wanted the, you know, encoding

249
00:25:30,720 --> 00:25:36,960
of the right ending to be closer to the context than the wrong ending. So this was called DSSM,

250
00:25:36,960 --> 00:25:43,520
a deep semantic structure model that did like the best within the, you know, multiple models that

251
00:25:43,520 --> 00:25:49,680
we had initially tried for this story closed test. And then actually, I will go about saying that

252
00:25:49,680 --> 00:25:57,920
the core anti-state of the art, as it is about like, you know, released data points about a

253
00:25:57,920 --> 00:26:05,360
month ago, was from OpenAI where they had, they have this nice piece of work that is, you know,

254
00:26:05,360 --> 00:26:11,200
transformers that are also pre-trained on like a large language model that are doing actually

255
00:26:11,200 --> 00:26:17,280
really good job, 86% on the story closed test. So that's, that goes about telling you that

256
00:26:17,280 --> 00:26:22,160
there's, there are a lot of regularities that you can leverage by just, you know,

257
00:26:22,160 --> 00:26:28,320
reading a lot of corporate out there and building a language model. So that's sort of not exactly

258
00:26:28,320 --> 00:26:36,000
what I would have predicted in terms of what would be the best model that can tackle story

259
00:26:36,000 --> 00:26:41,440
understanding. But the recent results show that actually a very strong language model can do a

260
00:26:41,440 --> 00:26:48,240
good job in predicting the ending. With the language model that they trained, was it trained

261
00:26:48,240 --> 00:26:54,080
specifically for this task or was it trained generally like training on embedding space generally?

262
00:26:54,080 --> 00:27:01,600
Yeah. So the interesting thing about that work actually was that which kind of makes it,

263
00:27:01,600 --> 00:27:06,240
you know, sets it apart from the other systems that have submitted on our story closed test

264
00:27:06,240 --> 00:27:11,680
benchmark was that it was actually a generic language model, was this very big language model that is pre-trained

265
00:27:11,680 --> 00:27:18,080
and then tuned for our particular story closed test. So story closed test comes with a validation

266
00:27:18,080 --> 00:27:24,640
and a test set. So validation is what people usually use for for tuning purposes. But the nice,

267
00:27:24,640 --> 00:27:29,680
as I said, the nice characteristic of OpenAI system was that it was a, you know, pre-trained

268
00:27:29,680 --> 00:27:35,280
generic language model. That is actually, I will add this that I told you that we have now,

269
00:27:35,280 --> 00:27:42,240
we are working on releasing a new story closed test version, the new version of the data set

270
00:27:42,240 --> 00:27:48,400
that kind of bypasses and helps with, you know, improving the kind of biases that people had

271
00:27:48,400 --> 00:27:55,280
found out about the data set. And we tested OpenAI system and it was the only system that was

272
00:27:55,280 --> 00:28:00,480
still having a very high performance on the new data set, which goes about saying that

273
00:28:00,480 --> 00:28:05,520
turns out the other, you know, submit assistance for actually leveraging the biases as opposed to

274
00:28:05,520 --> 00:28:14,480
doing true language understanding. Is there a way to characterize whether the OpenAI system is

275
00:28:14,480 --> 00:28:21,120
just better at leveraging biases, you know, as opposed to kind of true understanding? Yeah, that's

276
00:28:21,120 --> 00:28:26,960
a very good point. And I actually, I do think that there's no way for us that there are not

277
00:28:26,960 --> 00:28:32,720
there some hidden biases in any of the data sets that we are benchmarking our progress on.

278
00:28:32,720 --> 00:28:39,120
And until we get to a point that we can really deploy a system in the wild and see that they can

279
00:28:39,120 --> 00:28:44,880
basically model different kind of complex inputs and then generate different kinds of complex

280
00:28:44,880 --> 00:28:50,000
outposts, there's no way of guaranteeing that, right? I'm sure that our new data set that we're

281
00:28:50,000 --> 00:28:55,280
about to release also has new biases that we are not aware of, right? And there's a very good

282
00:28:55,280 --> 00:29:00,800
chance that the OpenAI system, as an example, is just doing a better job at, you know, again,

283
00:29:00,800 --> 00:29:07,600
like memorizing different sorts of irregularities that are hidden. So I think there's just really

284
00:29:07,600 --> 00:29:13,120
no way of knowing that. But I guess as long as at least we have better practices in place for,

285
00:29:14,080 --> 00:29:20,160
you know, testing and evaluating your systems on a variety of benchmarks, VR in a better shape.

286
00:29:20,160 --> 00:29:28,320
One of the points you raised earlier was the idea of systems that can explain themselves. Now,

287
00:29:28,320 --> 00:29:33,360
that can mean a lot of different things. But one of the things that it can mean is that a system

288
00:29:33,360 --> 00:29:42,160
like this OpenAI system or some other system with this capability can describe, you know, why it is

289
00:29:43,360 --> 00:29:48,880
producing the sentence that it's producing or choosing the sentence that it's choosing.

290
00:29:48,880 --> 00:29:54,400
Is that when you say explain, is that the focus of your work?

291
00:29:54,400 --> 00:30:02,560
Yeah, exactly. So as I said, the outcome of the challenge that we ran on the story close test

292
00:30:02,560 --> 00:30:08,720
was kind of an eye-opening for me personally to think beyond classification tasks. And,

293
00:30:08,720 --> 00:30:13,040
you know, with a few of my colleagues, we had a lot of back and forth. And the consensus was that

294
00:30:13,040 --> 00:30:20,160
if we build AI systems that not only choose the right, whatever their, you know, space they

295
00:30:20,160 --> 00:30:27,120
provided this, choose the right ending in the context of story close test, but also explain why

296
00:30:27,120 --> 00:30:32,160
they did that so that we can basically probe them. We can see whether or not their decision

297
00:30:32,160 --> 00:30:38,720
makes sense. Because as I said, for instance, in the case of the team, the UW team that was

298
00:30:38,720 --> 00:30:44,640
leveraging the features, the, you know, stylistic features, the system can't explain that I chose

299
00:30:44,640 --> 00:30:49,680
this because it had more number of adjectives versus adverbs or it had like, I don't know, like

300
00:30:49,680 --> 00:30:55,040
eight words as opposed to five, right? It should say something that is logically sound to a human

301
00:30:55,040 --> 00:31:01,200
reader. So that's, yeah, back to your point, that's exactly what I mean by explanation. And I do

302
00:31:01,200 --> 00:31:09,920
think that explanation is this cognitive ability that us humans we have and working on the next

303
00:31:09,920 --> 00:31:15,360
generation of AI systems that can explain themselves is not really only for the sake of say

304
00:31:15,360 --> 00:31:21,360
evaluation or for the sake of, I don't know, like they're now these regulations and say EU that

305
00:31:21,360 --> 00:31:27,440
push for AI systems not to be black boxes, but it's really about modeling this human capability

306
00:31:27,440 --> 00:31:33,200
and this cognitive ability that we have as humans because explaining is a way that we showcase

307
00:31:33,200 --> 00:31:38,240
our intelligence often and we have to have AI systems that can just portray that.

308
00:31:39,040 --> 00:31:46,880
And so there are different ways of approaching that. Some take the explanations from an intrinsic

309
00:31:46,880 --> 00:31:55,520
perspective and try to introspect on the actual model that's doing the deciding and put what

310
00:31:55,520 --> 00:32:01,680
it sees there into words and others take more of an external perspective and try to apply some

311
00:32:01,680 --> 00:32:09,680
other model to the primary model to generate explanations. How does your work in the space

312
00:32:09,680 --> 00:32:17,440
approach that distinction? Yeah, so I personally, this is my personal view. I personally,

313
00:32:17,440 --> 00:32:21,600
personally, personally, foremost, I believe that the explanation should be in national language

314
00:32:21,600 --> 00:32:26,880
form, right? So there are different pieces of work off a lot also in the vision and language

315
00:32:26,880 --> 00:32:35,920
community that they basically count your attention features as the way of explaining why a system,

316
00:32:35,920 --> 00:32:41,920
you know, say chooses a particular, you know, outcome versus another, but I count national

317
00:32:41,920 --> 00:32:48,800
language explanations as a reasonable way of interfacing with human. So back to your question

318
00:32:48,800 --> 00:32:56,240
about there being two disjoint modules that one of which will do prediction. Let's say one of

319
00:32:56,240 --> 00:33:01,760
which will do the generation. I would say that I'm not against either of those. It could be a

320
00:33:01,760 --> 00:33:08,640
disjoint model. It could be a joint model, but as long as the system can actually somehow provide

321
00:33:08,640 --> 00:33:16,160
the explanation that makes sense to the human that is judging whether or not makes sense,

322
00:33:16,160 --> 00:33:21,840
it could be still useful. So there are different ways of looking at explanation, right? Is it for

323
00:33:21,840 --> 00:33:28,800
basically trying to diagnose, diagnose like why a system makes a particular decision? Is it

324
00:33:28,800 --> 00:33:34,880
for improving the system? Is it what is the purpose, right? And I would say that at the end of the day

325
00:33:34,880 --> 00:33:40,960
as long as the system can improve, for instance, through the interaction it has with human for

326
00:33:40,960 --> 00:33:48,240
explaining themselves itself, why do we care if there are two different modules that one of

327
00:33:48,240 --> 00:33:54,480
which is generating the explanation and one of which is doing the prediction. But at the end

328
00:33:54,480 --> 00:34:02,560
of the day, as I said, I think we can take explanation as a way of knowing whether or not the system

329
00:34:02,560 --> 00:34:09,520
can tell us what's going on there to hood. And if a system chooses to have two different modules

330
00:34:09,520 --> 00:34:16,880
that's just basically an implementation decision, but I can't imagine a successful system that

331
00:34:16,880 --> 00:34:22,560
doesn't have those two modules communicated with each other. So again, but I'm not opposed to

332
00:34:22,560 --> 00:34:31,040
the idea. So thus far we've talked primarily about natural language understanding aspects of

333
00:34:31,040 --> 00:34:38,320
contextual modeling, but you recently presented at CVPR on some multimodal work incorporating both

334
00:34:38,320 --> 00:34:44,960
language and vision. Can you talk a little bit about that? Sure, absolutely. So as I said, the work

335
00:34:44,960 --> 00:34:50,880
on event-centric contextual modeling has different applications. So we talked about story understanding

336
00:34:50,880 --> 00:34:57,440
another application of it for my work has been on language and vision. So the language and vision

337
00:34:57,440 --> 00:35:03,520
community has seen a lot of interest, has received a lot of interest in the past like a couple of

338
00:35:03,520 --> 00:35:11,120
years, basically after deep learning growth, which is because we can just basically have better

339
00:35:11,120 --> 00:35:19,600
ways of encoding images, which is very nicely something that you can nicely feed into a

340
00:35:19,600 --> 00:35:25,280
recurrent model with which you can generate language. So that's more so what my work has been

341
00:35:25,280 --> 00:35:32,960
around as well. So image captioning has been the most popular application in vision and language

342
00:35:32,960 --> 00:35:38,800
since the beginning, which is about how would you build an AI system that will caption it a given

343
00:35:38,800 --> 00:35:45,040
image very literally. So how would you literally describe what you see in a photo? So my work has

344
00:35:45,040 --> 00:35:51,920
been mainly focused on going beyond literal description and getting more so towards kind of

345
00:35:51,920 --> 00:35:59,120
vision language tasks that require some degree of common sense reasoning. So to give you an example,

346
00:35:59,120 --> 00:36:06,000
the very first work I started on vision language was about this static image. Imagine I'm just

347
00:36:06,000 --> 00:36:13,440
describing it basically now. Imagine this aesthetic photo you see of two policemen with a fallen

348
00:36:13,440 --> 00:36:20,480
motorcycle on the ground. And as human beings, when we see this, right, the tree status of

349
00:36:20,480 --> 00:36:27,840
aesthetic objects, right, two policemen and a fallen motorcycle. From these three static objects,

350
00:36:27,840 --> 00:36:32,960
we go beyond, we connect the dots and we infer that, oh, there should be a notion of injury,

351
00:36:32,960 --> 00:36:38,080
or oh, there should have been a motorcyclist, right, or oh, like there should have been an accident

352
00:36:38,080 --> 00:36:43,760
here. So these kind of connections that we make are really interesting and it wasn't something that

353
00:36:43,760 --> 00:36:49,680
was already explored in the research community. So we basically defined this task called visual

354
00:36:49,680 --> 00:36:58,320
question generation that what it does is it focuses on building an AI system that given a static

355
00:36:58,320 --> 00:37:03,120
image that happens to be event-centric, meaning that something's happening in the image.

356
00:37:03,120 --> 00:37:09,200
What is the most natural first question that pops to your mind, given that image? So that's basically

357
00:37:09,200 --> 00:37:14,800
the VQG work that I did that started off a series of other vision and language projects that I

358
00:37:14,800 --> 00:37:20,000
worked on. And, you know, I will go about asking you. So what would be the most natural question

359
00:37:20,000 --> 00:37:24,960
that you would ask, given the image that I just described to you? At the highest levels,

360
00:37:24,960 --> 00:37:30,240
like what happened? Yeah, what happened exactly? So we understand that something should have

361
00:37:30,240 --> 00:37:36,160
gone wrong or should have occurred that caused that scene. Yeah, what happened is the

362
00:37:36,160 --> 00:37:41,440
motorcycle is still alive. How like serious was the injury, stuff like that are the most common

363
00:37:41,440 --> 00:37:46,560
things that people ask and it was the kind of a, you know, task that we wanted to push for to go

364
00:37:46,560 --> 00:37:54,160
beyond literal description. And so how do you go about tackling that? Yeah, so we basically built

365
00:37:54,160 --> 00:38:00,080
it for first and foremost, you need data, right? So we collected our own data set called VQG,

366
00:38:00,080 --> 00:38:06,800
which was on more, more so on event-centric images, we queried like being search engine to get

367
00:38:06,800 --> 00:38:12,800
images that have something happening in them, save fire, earthquake, injury, stuff like that

368
00:38:12,800 --> 00:38:19,520
are an accident. And then using that data, we built this model that gets as an input, the feature

369
00:38:19,520 --> 00:38:25,440
vector of an image, which, you know, could be a convolutional neural net. And then given that,

370
00:38:25,440 --> 00:38:31,440
just train a language model, basically a conditional language model that will just generate the text

371
00:38:31,440 --> 00:38:38,560
description, which just happens to be a question here. So we actually leveraged existing image

372
00:38:38,560 --> 00:38:45,120
captioning models and just basically tuned them and retrained them on our own data. And it was,

373
00:38:45,120 --> 00:38:50,400
you know, semi-successful in asking relevant questions given an eventful image.

374
00:38:50,400 --> 00:38:59,360
And so with the data set you curated, you had these images of events, things that happened,

375
00:38:59,360 --> 00:39:07,680
and then you had a single caption for each image. Our data actually comes with five questions per

376
00:39:07,680 --> 00:39:13,040
given image, but you know, it could have been anything, right? It just happens that ours was five,

377
00:39:14,160 --> 00:39:22,240
just for the training purposes. We had, we collected 15,000 of such images, each paired with five

378
00:39:22,240 --> 00:39:31,040
questions. And is the task structured in such a way that it's a classification or a generation

379
00:39:31,040 --> 00:39:36,800
of a new sentence based on it? Yeah, that's a very good question. It is generation here. And as it

380
00:39:36,800 --> 00:39:43,120
goes with any generation task, you have the problem of, okay, how would you evaluate, which is a

381
00:39:43,120 --> 00:39:50,240
major, you know, problem in the community. So here, we just, you know, try different metrics that

382
00:39:50,240 --> 00:39:54,560
are classically used in the division of the language community. They're different methods,

383
00:39:54,560 --> 00:40:00,960
you know, you can use blue, you can use media or we just use Delta Blue, which happened to correlate

384
00:40:00,960 --> 00:40:05,840
the best with the human judgment. So can you elaborate on that? What is Delta Blue? What are the

385
00:40:05,840 --> 00:40:11,920
inputs to that? Of course. So these are metrics, right? There's no like, it's not no AI, like

386
00:40:11,920 --> 00:40:19,440
nothing is being trained. It's just a metric for evaluating a generated output versus some gold

387
00:40:19,440 --> 00:40:26,080
standards, which is an inherent problem because for so many taskers, no limited set of possible,

388
00:40:26,640 --> 00:40:31,600
you know, gold answers as it goes for this, right? There's so many different questions you can

389
00:40:31,600 --> 00:40:36,960
ask given an image. But at the end of the day, you, you know, as in research community, at least,

390
00:40:36,960 --> 00:40:44,720
what you have to resort to is to define a set of predefined, you know, limited set of predefined

391
00:40:44,720 --> 00:40:51,360
questions here. Even there, there are so many ways to ask the same question. Let alone

392
00:40:51,360 --> 00:40:58,560
different types of questions that you could ask for about a situation. Exactly, which goes to

393
00:40:58,560 --> 00:41:04,240
saying that to what I just said, that it's just inherently problematic. I would say that we

394
00:41:04,240 --> 00:41:09,440
yet don't know how to evaluate language generation, which is not machine translation. So in machine

395
00:41:09,440 --> 00:41:15,680
translation, even although even a machine translation, blue, which is the metric of how would you

396
00:41:15,680 --> 00:41:21,360
evaluate how good of a job you've done at translation, there even people use blue, which,

397
00:41:22,400 --> 00:41:26,880
which, you know, there are different pieces of work that show that even that doesn't correlate

398
00:41:26,880 --> 00:41:33,520
with human judgment that is strongly, but even in machine translation, the task is much more

399
00:41:33,520 --> 00:41:39,840
defined, right? The semantic content of the source language is really close to the semantic content of

400
00:41:39,840 --> 00:41:45,680
the, you know, the language that you're going to, whereas in dialogue or in story generation,

401
00:41:45,680 --> 00:41:51,600
or in this vision of language test that I just talked about, it's not, nothing is, you know,

402
00:41:51,600 --> 00:41:58,160
set predefined. So we have this major problem, which goes, again, goes back to the story I was

403
00:41:58,160 --> 00:42:03,920
telling you for the story close test that we decided to go with the classification task because

404
00:42:03,920 --> 00:42:10,560
generation inherently is hard to evaluate and classification gives us this ability to systematically

405
00:42:10,560 --> 00:42:18,560
evaluate. So yeah, anyways, we've, you know, the language and vision work that I told you about,

406
00:42:18,560 --> 00:42:25,600
we ended up using Delta Blue, which is this metric for basically counting the number of words that

407
00:42:25,600 --> 00:42:33,920
occur in the generated output versus the gold few human authored, you know, questions that you

408
00:42:33,920 --> 00:42:42,160
have in hand. So you generate a question based on the image and you're evaluating the performance

409
00:42:42,160 --> 00:42:49,760
of, or you're evaluating that question based on Delta Blue, and presumably you're crowdsourcing

410
00:42:49,760 --> 00:42:56,960
the gold standard answers. Exactly. Yes, it just comes from the data set, right? So the 15,000

411
00:42:56,960 --> 00:43:02,160
data, you know, points to be collected, we set aside a portion of that for test, where that

412
00:43:02,160 --> 00:43:12,240
that's a blue, you know, human authored gold questions come from. And so for your implementation

413
00:43:12,240 --> 00:43:18,000
of a system to do this, what was the general approach you took? To general approach, as I said,

414
00:43:18,000 --> 00:43:23,840
was this model that encodes the image using a convolutional neural net and then generates the,

415
00:43:23,840 --> 00:43:32,880
yeah, and then generates a sequence of words using a recurrent neural net. And these are,

416
00:43:32,880 --> 00:43:38,640
you know, there's this recurrent neural net language models are really strong in generating

417
00:43:38,640 --> 00:43:45,680
grammatical outputs, meaning local coherency, but they're not really good at capturing

418
00:43:45,680 --> 00:43:51,680
intricacies and generating basically contentful sentences. As you may have seen, you know,

419
00:43:51,680 --> 00:43:57,040
if you look at the kind of language, the chat box generator, a lot of such original language,

420
00:43:58,080 --> 00:44:03,520
work pieces of work, often the generations are bland, meaning that they're, you know, they're

421
00:44:03,520 --> 00:44:09,760
usually safer on the safer side. You don't have a lot of contentful words or events and stuff

422
00:44:09,760 --> 00:44:18,560
like that, but they do a really good job in generating coherent sentences. And did your work try to

423
00:44:18,560 --> 00:44:23,040
address that? Yeah, so there are different, so we did, you know, we did try different things,

424
00:44:23,040 --> 00:44:30,080
just the, you know, the question is, okay, I have this system that generates different kinds of

425
00:44:30,080 --> 00:44:36,320
output. People usually have this end best list that they rerang. So basically, when you are

426
00:44:36,320 --> 00:44:42,160
generating at the end of the day, you can have a, you know, search, right? And then the question

427
00:44:42,160 --> 00:44:47,680
is how to do that search better, so that you hit the ones that are more contentful. So they're

428
00:44:47,680 --> 00:44:53,200
like different little tricks in the paper we had at the, for the VQG work, we use your question

429
00:44:53,200 --> 00:44:58,720
generation that I just told you about. We had this very simple heuristic that if, if in your

430
00:44:58,720 --> 00:45:05,440
end best list, you have a sentence that has verbs in it, give it a higher rank, or if you have a

431
00:45:05,440 --> 00:45:10,320
longer sentence, is most possibly a better sentence. And then you put all these different,

432
00:45:10,320 --> 00:45:14,800
yeah, features together, and then you tune your model, you use merch, but they're different,

433
00:45:14,800 --> 00:45:18,960
kind of, you know, rankers you can use out there. But, you know, it's not a solve, it's a,

434
00:45:18,960 --> 00:45:24,480
it's a serious problem we have for the, you know, see, basically these kind of recurrent

435
00:45:24,480 --> 00:45:29,920
neural net generations. And when you describe this earlier, you, I think you described it as a

436
00:45:29,920 --> 00:45:35,200
conditional language model. Yeah, I mean, yeah, so you have to have a way of

437
00:45:35,200 --> 00:45:39,760
conditioning on the input right here, the input as the image, the input could have been anything,

438
00:45:39,760 --> 00:45:45,520
right? In the case of story generation, the input could be the previous sentence. Here, we want to

439
00:45:45,520 --> 00:45:51,440
generate using the image, so we condition the generation of the language on the feature vector

440
00:45:51,440 --> 00:45:59,680
of the image. I'm trying to, to visualize what that looks like, or how that is implemented.

441
00:45:59,680 --> 00:46:03,360
You know, when I think about RNNs and time steps and all that kind of stuff, where does the,

442
00:46:03,360 --> 00:46:08,640
the feature vector of the image come into play, or is that the input at these time steps?

443
00:46:09,600 --> 00:46:18,560
Yeah, so the, the very last model we tried for the VQG task, actually, what it got as the input

444
00:46:18,560 --> 00:46:24,000
was the FC7 feature of the, you know, looking the convolutional neural net, one of the, you know,

445
00:46:24,000 --> 00:46:32,080
slices that you can get is like FC7. So this fully connected layer is what we fed into the RNN,

446
00:46:32,080 --> 00:46:37,840
as the, just for the very first step. You can actually feed that in for all the steps. That's

447
00:46:37,840 --> 00:46:43,360
something we tried. We just got worse results. We just conditioned the very first time step on

448
00:46:43,360 --> 00:46:49,280
that. But as I said, that's a, that's a decision that you can make by just, just trial and error.

449
00:46:50,240 --> 00:46:57,360
Awesome. We have covered a ton of ground. Are there any other things that you might want to

450
00:46:57,360 --> 00:47:03,760
mention about your current research areas? Yeah, I think it we covered a lot. I think that

451
00:47:04,560 --> 00:47:12,640
what I would conclude this, this, you know, today's time path is to just mention that I,

452
00:47:12,640 --> 00:47:18,720
I've chosen to work on the topics in AI that I found to be really challenging in terms of

453
00:47:19,840 --> 00:47:26,320
the amount of work that still is needed to be done to even, you know, scratch the surface.

454
00:47:26,320 --> 00:47:31,920
So commonsense reasoning happens to be one of them. There's a consensus in the field these days

455
00:47:32,480 --> 00:47:38,240
that we do yet don't have an AI system that can even have the commonsense understanding of a,

456
00:47:38,240 --> 00:47:44,000
you know, four or five-year-old kid, let alone an actual human like adult. And I think that

457
00:47:44,560 --> 00:47:49,600
the kinds of work on like story understanding, story generation, or division language tasks that

458
00:47:49,600 --> 00:47:56,720
are event-centric, go about, you know, at least going one step beyond the existing

459
00:47:57,840 --> 00:48:02,640
efforts for doing something that is a little bit more challenging. And I think it's important

460
00:48:02,640 --> 00:48:10,000
to be mindful of how far we've come, which is to tackle a lot of, you know, previously challenging,

461
00:48:10,000 --> 00:48:18,240
I would say, you know, kind of perception tasks, but we really have a long way going forward

462
00:48:18,240 --> 00:48:24,240
doing more of your reasoning and cognitive tasks, which is my, my personal research interest,

463
00:48:24,240 --> 00:48:28,480
and I think a lot more into, you know, people in the community should pay attention to it.

464
00:48:28,480 --> 00:48:32,240
Well, Neswin, thank you so much for taking the time to chat with us.

465
00:48:32,880 --> 00:48:35,360
Thank you. Thank you so much for having me.

466
00:48:39,840 --> 00:48:44,880
All right, everyone. That's our show for today. For more information on Nesreen,

467
00:48:44,880 --> 00:48:51,680
or any of the topics covered in this episode, visit twimmelai.com slash talk slash 174.

468
00:48:52,560 --> 00:48:57,360
If you're a fan of the podcast, please pop open your Apple or Google podcast app,

469
00:48:57,360 --> 00:49:02,400
and leave us a five-star rating and review. Your ratings are a great way to help new listeners

470
00:49:02,400 --> 00:49:15,120
find the show. As always, thanks so much for listening and catch you next time.

