All right, everyone. I am here with Stefano Soato.
Stefano is VP of AI Application Science at AWS,
as well as a professor of computer science at UCLA.
Stefano, welcome to the Twomla AI podcast.
Thanks for having me.
I'm really looking forward to digging into this conversation.
We'll be talking about some of your recent work on an area
you called Graceful AI, which is very interesting.
But before we do that, I'd love to have you share a little bit
about your background and how you came to work in machine learning.
Yeah, I appreciate that my background is a bit unusual.
I grew up in Italy as you can tell from my accent.
And I grew up studying class, studying classics,
like history, philosophy, Latin, Greek, and so on and so forth.
And before going to college, I entered a context
for a summer course organized by a tiny school
that has a class of letters and a class of science.
And I was there to study philosophy,
but I realized that there were problems that people were writing
on a board that I had no clue how to solve.
And these were math problems.
And it really, really bugged me.
And so I got more and more interested in that side
and I started attending those seminars.
And then I rolled in engineering and in 1989, I ran across
the work of Ernst Dickman's.
Ernst Dickman's was a pioneer of autonomous driving
back in 1989 in Germany.
He was the one that had autonomous vehicle going on the Autobahn
at speeds up to 180 kilometers per hour.
And when I joined the Caltech for my PhD,
I was in the Department of Control and the Renial Systems.
And that's where I got exposed to computer vision
because the reason we don't have domestic robot helpers
is not because we don't know how to control them,
because we don't know how to endow them
without a presentation of the surrounding world.
And so I got more and more interested in that problem.
And that was a quarter of a century ago
and I'm still very excited and interesting about that area.
And so that's how I got into AI.
Nice.
We could spend a whole different interview
talking about that.
I, one of my perennial favorite topics
is the relationship between kind of control
and machine learning and end-to-end systems
versus more modular systems that use what we've learned
about physics and control and other things.
But I think that's going to be a different conversation.
Tell us a little bit about both your role at AWS
as well as your research interests across AWS and UCLA.
Yeah.
Yeah, so at AWS, I am in charge of the science
for AI applications.
Any applications are grouped by modalities.
So we have vision, so images and video.
And then we have speech, and then we have text,
and then we have vertical.
And verticals are domain-specific applications
in the industrial space as well as operations
and time series for casting and so on support.
And we are organized as research teams,
but we are very closely coupled with engineering teams,
product teams and data teams.
And we practice what we call customer-obsessed science,
which is a different curiosity-driven science,
which is what we do at the university.
And so our work ends up in the hands of customers
in a time frame that is quite fast
for somebody who's used to academic time clock
where you expect your impact in the world to be posthumous
or if you're lucky during your lifetime.
But here, there's a very quick turnaround between ideas
being generated, services being deployed,
and customers using them and getting benefit from it.
So my interest is broadly speaking in AI.
I've always been interested in autonomous system systems
that interact intelligently with the environment,
where intelligent is to be defined, everybody has their own.
But to me, a dog is plenty intelligent.
I wish we could build something that behaves like a dog.
So that's much more difficult than doing a chess-playing program
in a fight for survival.
The dog will win over the chess-playing program any time.
So you just need to unplug the power.
And definitely, there are opportunities that are...
The time is right for research that we've been doing
for 10, 20 years to become useful now.
And for me, the game changer was in 2009,
I read a report by Cisco called Visual Network in Index,
that pointed out that back then, 2009,
peer-to-peer traffic was surpassed by video traffic
on the internet.
And they were forecasting that by 2022,
it would be 90% of wireless traffic
and almost the totality of internet traffic.
And so to me, that was a game changer.
But I thought, now we have data, we store data.
But we don't know how to extract intelligence from data.
And that will take more than my lifetime.
So I was not thinking that it would be so quick.
And then in 2014, I was involved in a project
where there were training systems
to detect anomalies in CT scans in medical imaging.
And then I realized that on a task,
where humans are not naturally evolved,
which is to interpret not natural, but medical images.
So where you have to train these physicians for many years,
you could train a deep learning model
and beat even the most experienced radiologists.
And so that's where I realized, oh my God,
this is happening in my lifetime.
And so what is the right place to be part of this
if you want to be involved?
Well, you need to be in a place that has exposure
to a variety of problem domains and has patience
and has data resources to make it happen.
And slowly, one little step at a time,
but we've been able to make a difference
in several of our products that our customers have been
benefiting from.
Yeah, I like the characterization of areas
that we're evolved to be good at versus those that we aren't.
You know, as a classifier for problems
that we should seek to apply a machine learning
for, it kind of reminds me of Andruing's
anything that takes us more than a second.
But it's got a maybe a more philosophical spin to it.
Well, both the philosophical is very easy
to underestimate how difficult some tasks are.
One anecdote I have is that with my students
are participated to the first two data program challenges.
And it was very interesting to be in the audience
and watching people who are not experts
see this card right by.
And these cards really look like they didn't know
what they were doing.
And so, you know, how can you possibly not
see that bush and drive over it
when anybody can see that bush, it's obvious.
People forget that this is roughly
the size of your brain, the size of your two hands,
and roughly half of it processes visual information.
So most of the real estate in your brain, even when
you're absorbed in the most, you know, in the most
abstracts, true thinking, most of your brain is busy trying
to make sense of the sensory data that comes through.
So it's really a cool and task.
And on the contrary, some things that
look very, very difficult to humans
are trivial to computers.
And so we've been able to make inroads in both.
Some traditionally hard problems,
like learning one from few shots.
We have made progress, thanks to also the evolution
of deep learning of the past five years,
as well as the evolution of hardware, optimization
methods, and so on.
But also, we've been able to better understand
the problem, you know, in a way which
is independent of whether it's implemented
with biological hardware or silicon hardware.
There are some characteristics of learning
problems that are absolutely fascinating
that we've just started to poke into.
And they are from the perspective of somebody
with an academic inclination is very fascinating
to expect to think about.
Yeah, yeah.
And we want to dig into one of those.
And that is the work that you and your teams have been
knowing around this idea of graceful AI.
You know, what I thought was most compelling about that
is I think we've come to appreciate that when we're
using machine learning models and production,
we need to constantly evolve them, constantly train them
to counteract the effects of data drift,
or drift in the distribution of our data.
And your paper in this work, in general,
kind of ask the question or prompts the question
that, you know, are there some negative effects associated
with constantly retraining those models, you know,
what are they, and how do we deal with them?
I'd like to have you tell us a little bit
about the broader motivation that led to this body of work.
Yeah, that's a, it's actually interesting story
because it's certain dipitres, right?
So, you know, one of the luxuries of being at AWS
is that you get exposed to a media of problems
that you don't even exist.
And you would not dream of by just sitting
in your office at the university.
And so this was a case where, you know,
the natural mode of evolution of knowledge is continuous,
like you mentioned, so we don't have this
arbitraris separation between training
and inference phases, which we do have in machine learning.
But in machine learning, we have this phase
where we train something, we make,
we build a model, we offer to customer,
people use it at some point, either a better model comes along
and certainly if you follow the academic literature,
every single conference, there's an incremental improvement
and you would like to harvest all these improvements
so that can benefit customers downstream.
But what we're gonna allow is that customers
were not updating their models
and we were not understanding why wouldn't you change the model
when this one performs better.
Now, the definition of performance
is the one that comes from academic benchmark
where we count the average number of errors
and we train to minimize the proxy of that
whether it's empirical cross-entropy or some other loss.
So, but basically the criterion that we,
as academic, had guessed is most relevant
is the average number of errors.
Turns out, what people don't care
for the average number of errors.
They have very specific requirements
for what type of chords they care about
and even specific subsets of subtypes of the data
that particularly care about.
And so, we realized that the proxies that we were training for
were not the right ones for the customers.
They were just the ones that we, by default,
have been using for years.
But the conversation that triggered that was important,
it's an example of the customer obsessed science
because we need to release the model
that was better than everything we've done before.
So, we were very excited.
And then we heard the customers were not happy.
And so, we were wondering why is that?
Well, because there is regression, what is regression?
Regression is when you have a new model
and that model, even though on average,
it performs better than the old model
but it introduces mistakes that the previous model didn't make.
Okay.
And so, even if these mistakes are fewer,
when you notice the old model makes mistake
that the new model makes mistakes the old one did not make,
your puzzle doesn't use it because how is this any better?
And so, and the conversation between a scientist
and a program manager was around the tone, well,
of course, the new model can make a mistake
that the old model didn't make.
Let me explain something about machine learning to you.
And the program manager would say,
no, let me explain something to you about customers
solving their own problem.
And I was listening and I was thinking,
well, maybe we are solving their own problem.
So, this type of interaction with customer escalations
is really a treasure trove for scientists
because every time there is an obstacle,
for an engineer, it's a frustration
because it's a yet another problem to solve
to get to the finish line for the scientist
is an opportunity is because there's something to be understood.
And here, what was there to be understood
is that we are really not pretty much in the right proxy.
And so, that's where we started thinking about,
why should the average error be the right metric?
Because some people care for having sort of even performance
across different courts of the data set
or some people care about having performance
that behaves similarly in models that are deployed at the edge
or at the cloud and some people care
to maintain compatibility with previous models
because they built pre-processing post-processing
around the train model.
And so, if you change the model suddenly you break it,
it is months of work, it's extremely expensive.
And so, we realized this whole universe of problems
that arise when you're trying to train a machine
but learning model not just to perform well
in terms of the average probability of error,
but that performs well relative to the context
of pre-processing algorithm, post-processing algorithm,
pre-existing models, models that are deployed
on different hardware and so on and so forth.
And so, it's fascinating because this is a problem
that I would have never thought about
seeing a university anywhere have not ever occurred to us
if it wasn't for a customer coming back
and say, I don't like this, fix it.
So, that's kind of the essence of customer-obsessed science.
Yeah, there's so many interesting facets to this one
that jumps out at me is just how it's a reminder
of how immature machine learning is.
On the traditional engineering side,
we've got this whole set of methodologies around testing
and one of those types of tests is regression testing.
We know how to do that.
Every engineering team worth its salt runs a series
of regression tests before they release their product
to make sure that the product isn't taking steps backwards,
but this is new in the context of machine learning.
Exactly.
And there's even some aspects of it
which are very peculiar and specific to deep learning
or more in general, over-complete models
for instance, for classification, where if you take a deep learning
model, let's say you're SNed 152 and you train it
on a large data cell, let's say, ImageNet.
If you repeat the experiment a hundred times,
retrain the same exact model on the same exact data
just from different initial conditions,
you compare it to a hundred different models,
but all of them have exactly the same average error,
let's say 87.3, whatever it is.
They all have the same average error.
What do you go and look?
The mistakes they make, they're completely different.
So it's as if they were trading mistake,
I'll get this one right, but give me this one
and I'll get it wrong.
So it was really eye-opening because it's way the second.
So they all make the same number of mistakes,
but they're different mistakes.
And this is what we realize that very often,
some of these criteria, for instance,
equal error rate across different demographics,
as well as compatibility with our models,
are conflicting with average performance.
But here we have a case we have an ISO error rate surface
where all models are equivalent in terms of error rate,
but you can move along the surface
in high dimensional space to make sure
that your model does not make,
or makes as few mistakes as possible,
that were not made at the previous model.
So you can optimize criteria that are orthogonal
and they're not conflicting with each other.
So this was the first time we realized that,
okay, this is yet another performance criteria
that does not impinge on the existing one that we know
and know how to optimize.
So there are many fascinating phenomena
that arise when you start observing the behavior
of these networks beyond just very standard
and what establish metrics.
A lot of work's been going into trying to understand
how deep learning models work
and understand their internals.
Have you or other previous researchers looked at this idea
of maybe the way the error rate or not error rate,
but the types of errors kind of cluster across this ISO error
rate or accuracy frontier?
Yes, quite a bit of work.
So first of all, it kind of be ambly.
It's fascinating how, you know, 20 years ago
we thought that with math and analysis
we would be informing research neuroscience.
And now we find ourselves doing kind of artificial neuroscience
and probing deep network the way neuroscientists
probe neural network, which is kind of interesting
twist of events.
But yes, we've been looking at that at various stages.
So one is with former postdoc of mine,
his name is Hussein Mobahi.
We were looking at universal adversarial perturbations
where we realized that if you take the data
and perturb them in a way that hits the closest decision
boundaries so that with the smallest possible perturbation
you change the class and you fool the network so to speak.
All of these perturbations are aligned,
which is very mysterious.
And that might be in what sense?
Say, again, aligned in what sense?
They're aligned in the sense that their direction
in the high dimensional space of representations
is parallel, they're parallel to each other.
So that you can find a single perturbations
that apply to all the data with high probability changes
in the class at the output.
So that says something about the structure
of the decision boundaries.
There are regions of high curvature.
And so it's very different from what we had in mind
kind of coming from stand there,
the dimensional classic fires, like SPM and so on.
There was another aspect that was really puzzling to me.
So this was when Alessandro Aquila,
who's a scientist at AWS, now,
the one who would still a student with a friend
from Harvard near science, they had this conjecture
that neural networks exhibit critical learning periods.
Now, what is critical in a period?
So in biological systems, either you learn a skill
when you're young or you don't learn.
This is why you cannot teach all-down new tricks.
And this is why if you're born with a defect like cataract
or with severe myopia, unless you correct it early,
no matter how much time you have to recoup,
you never learn, right?
So the optical defect is fixed.
So it's a result, but your brain has not learned correctly,
and then you never are correct.
So, and this is normally attributed to biology,
to biochemistry of the brain, you know,
you stop generating synapses and so you age.
But neural networks don't age.
Their connectivity is fixed at the outset, it doesn't change.
So I told you, you know, this is, you guys are crazy,
this is why would you ever expect the neural network
would have a behavior like that?
It turns out it does, and which is really puzzling
because now you want, okay, now it cannot be biochemistry
because an artificial neural network doesn't have any.
There's a very resemblance to the brain,
but really, this phenomenon must be an information phenomenon.
And then we start to dig into say, okay,
what is even meaning information in a deep network?
You know, what?
A deep network is a deterministic system.
So as zero entropy, the weights are fixed, right?
The input output map, one strain is a deterministic.
So as infinite mutual information is in the input and output.
So all of the standard concept,
information theory, not useful, and they're not useful
to probe the inside, the guts of the network.
And so we spend a lot of time defining
and measuring information quantities
in these gigantic networks,
in hundreds of millions of parameters
and now even trillions.
And what we discover, for instance,
is that I don't know if you remember the movie,
the eternal sunshine of the spotless mind.
I don't know if you remember it.
For whatever reason,
it wants to forget, experience it to a person or a partner.
And so it goes to a company called La Cuna
that under, you know, there's something,
you know, just showing some pictures of the partner,
zap the brain to erase memory of it.
So we thought, well, maybe we can do that with deep networks,
right?
We can zap the brain to forget or to erase memory
of something that you saw in your training set.
And it turns out that that is possible to do with deep networks
because once you understand how information is
defined and computed and distributed in the representation,
then you can inject noise in very specific direction
that will force you to erase a particular datum
or a class or a cohort of data,
which now is also important
because of privacy issues and so on and so forth.
So there's a lot of fascinating questions
that arise when you try to understand how these
deep networks operate and you have existence proofs
of what is possible to do thanks to biology,
the human visual system, the animal visual system.
So there is definitely a lot more back and forth
between the biological inspiration and the analysis
than there was 20 years ago when we thought, you know,
that maybe we'll solve the brain, you know, with analysis.
And somebody in the 90s told me that, you know,
if you think of science and understanding
is a process of compression, you know, observe the astra
and you could record their positions,
but once you understand the laws of physics,
you can compress them into a law.
And maybe the brain is the most compressed possible
representation of itself.
There is no easier brain of representation of brain
than the brain itself.
And if it's true for deep networks,
then how do we leverage, you know,
our reductionist scientific method
has not been successful in this particular area.
So we need a more holistic approach
to define a measure information.
And then once you do that, you realize that
despite the gigantic dimension of these spaces,
the amount of information that they store is a tiny fraction.
And the way in which they store it is fascinating.
I love it because I can claim that I'm still learning
even if I'm aging because these networks at the beginning
accrue a lot of information, sort of they memorize.
And then they start shedding information,
throwing away information.
And they do this while improving the expected error
or the test error in the data site that you have sequestered.
So in a sense, it seems like forgetting
or throwing away information is a necessary part of learning,
which when I talk to biology, they say,
oh, of course, but I've never seen a written math.
I've never seen a claim that is defensible based on data
arise from that.
So it's really, there's a lot of interplay
between understanding biological networks
and understanding artificial networks.
Yeah, it seems like there would be once you've established
that there are many, many levers that folks are using
during the training process, the batch sizes
and learning rates and cyclical learning rates,
all of these things that if they're ultimately correlated
to the rate at which the network learns
and how things are forgotten would be even more impactful
than was originally believed.
Yes, that's a great point.
One thing that we understood is the following.
So all of these inducted biases that you mentioned,
which could be in the class of functions with polling
or could be in the optimization with SGD,
with the choice of the batch size
and the learning rate and so on and so forth,
or they could be in explicit regularizer.
So all of these processes are some type of regularization,
meaning that these are added terms,
whether explicit or implicit to your training process
so that you don't just minimize the empirical error
otherwise you would overfit,
but you optimize something that hopefully
will allow you to generalize
and they can be computed using, for instance,
a spec based bound and now we know how to compute
the information terms in the spec based balance and so forth.
What we discover, however, is that normally
you think of regularization as a process
that schools or regularizes your loss function.
And this is not what's happening in deep learning.
So there is a paper that we wrote with an internet AWS
at NewRips two years ago, which is titled
Time Matters in regularizing deep networks.
And what it means is a following.
So the intern Aditya Goldakkar did the following experiment.
He took standard regularizers that people use,
for instance, WADK and data augmentation.
And then he had a few epochs of training
without regularization and then turn on regularization.
So asymptotically, the loss function is regularized.
The network would not behave at all.
Vice versa, turn on the regularizer,
let the network converge for a few epochs,
then turn off the regularizer.
So the asymptotic loss is as irregular
as if it never saw regularization.
It only saw regularization during the initial transit.
Yet generalization power is just as good
as the regularized all the way.
So it appears that regularization does not affect
the geology and the geometry of the loss function
at convergence.
So the asymptotic is really not that important.
It's all in the transient.
Regularization affects what bottlenecks in the loss landscape
you can manage to get into.
And then critical and imperial,
which is what we were talking about earlier,
tells you that once you get into one of this bottleneck
and onto a wide valley, and people talk about wide,
minimum, and so on and so forth, flat, minimum,
it's very difficult to come back from that.
So if you enter during the initial transit,
enter the wrong valley, which you could do
is if you're trained with the wrong data, for instance,
because your parents didn't realize that you were myopic
and so they didn't put your glasses on
until you were five, six years old.
At that point, you cannot get back out from that valley
and you will never learn how to see correctly, right?
And this is for a variety of skills,
for a variety of species from songbirds to walking
to deep neural networks, which is puzzling and fascinating.
The analogy that comes to mind for me,
and I may be bluttering my undergraduate material science,
but is one of a kneeling where you can have two materials
that are functionally equivalent by structure,
overall structure, but their internal structure
is different because of the way that they've been created
in a via heating and cooling cycles and things like that.
And that reminds me of a point that you made in the blog post,
which is talking about model compression,
which is I think illustrative of this entire conversation.
The point was that in model compression,
we often think of it as simply trying
to find a model with equivalent error that is smaller
or meet some other set of constraints.
But the architecture, what I'm taking
to be the structure of the models matters a lot
for the reasons that we've talked about.
Can you elaborate a bit on that?
Yeah, so we have this concept that we call information
plasticity as analogous to neuronal plasticity.
Neural plasticity is where neurons forms in absence,
form, you know, form dendrites.
And so they physically change configuration.
In deep neural network, the configuration is fixed.
However, what you notice when the network is staying
is that some parameters don't matter at all,
meaning that if you take that parameter and change it,
liberally, the input-out behavior does not change.
So you could replace that parameter with noise
or you could replace it with zero.
So this is called pruning.
And observe no input-out would changes.
Changes in the input-out would behavior.
And so it appears, however, that during the initial transient,
the amount of, so you could say also that this parameter
doesn't have any information, because you could store it
with zero bits, and you will never know the difference.
Vice versa, because if you have a parameter,
do you change it a little bit?
I know the subtle behavior changes
that has a lot of information.
So you want to store it with a large number of bits.
So, but it turns out that during the initial transient,
the information counted as number of bits per weight
moves around from the different layers, you know,
so the crosses and goes from upper layer to lower layers
until it settles.
Beyond a certain point, it cannot move.
And so beyond certain points, weights that are uninformative,
stay uninformative, even if you change the training set.
And so that's really the structure of the network.
So it appears that the effective connectivity,
which is not the physical connectivity,
because it's, you know, that's the term in the outside.
But the effective connectivity as measure
by the amount of information that that connection carries,
which we now know how to measure,
after several years of work.
So that is very important.
And if you knew that at the beginning,
okay, you could train with a smaller network.
The problem is that to get there,
you need to go through this very high bump
in information in the network,
which needs the high-dimensional space
and needs a large number of parameters.
And so yeah, so and there are,
like I was suggesting, statistical physicists
and physicists in general are very fascinating
with some of these questions,
because they have thought about high-dimensional systems,
including spin glasses and so on.
So we diverged pretty quickly
from the specifics of the graceful AI set of papers.
But I want to return to those and dig in deeper.
You talked about kind of how two models
with the same accuracy can have very different types
of errors and how from a user perspective,
that might be frustrating.
They may get used to a certain type of error.
Now the model changes and all of a sudden,
the behavior that they're used to is different.
And we want to, well, I guess in introducing this,
you focus on the idea of regression
and meaning things that were working previously
don't break.
But are there kind of broader types of continuity
that a customer might come to expect?
And have you looked at that, or to what degree
have you looked at that?
Yes, we have.
In fact, when we started this project,
we thought it was a narrow set of use cases where,
let's say you have a photo collection.
And now you update to the latest software,
and which is, of course, much better than the one before,
except now you're searching for a picture of your cousin
that you were able to find before and now they don't
show up in the search and what's going on.
So that's the regression, right?
So that you are frustrated as an individual user
because something was working before
and it's not working now, something broke.
So as soon as we came up with this,
we discovered that there's a host of other related problems.
For instance, we didn't realize that it is exactly the same problem
as a cross-model compatibility where you have a service
that runs on different platforms, let's say,
the smartphone as well as on the cloud.
And of course, on the cloud, you prioritize performance.
You really have no constraint on how big the model is.
But on your phone, you don't want to keep the battery
in no time, and so you run a smaller model.
So what do you do?
Do you run the best you can do with the hardware
you have on the phone, regardless of what the big model
on the cloud does?
Or do you try to train the model on the phone
in a way that resembles or mimics the model in the cloud
in a way that can be defined?
And if you do that, then your trade space
is not just the parameters for a children architecture.
You also can optimize over the architectures,
and it becomes a hybrid search space,
over a continuous space of weights,
as well as the discrete space of architectures.
And so that, we thought, was a different problem,
but it turns out to be exactly the same problem.
Same thing with languages, we have services that build chat
bots.
And so the train model sits at the center,
but then customers build up on top of this model,
very elaborate post-processing workflows.
And even changes that you would think are innocuous
can break this post-processing.
And you really don't know a priority,
what is and what is not doable.
And so there is a whole space of dimensions
along which you may want to impose either constraints
or optimize together with the average error.
And so for the past years, we've seen dozens of cases
across different applications at AWS,
where we see this problem arises.
It's interesting that when we first published this paper,
it was not reviewed positively, because, you know,
well, there's no comparison with anything,
but yes, because this is not a nobody
has to look at this problem before.
So it's really one of the most exciting times
when you're not just having a new solution to an old problem,
but you open up a new problem that one hand
is exciting for scientists to send their teeth in.
On the other, it can be beneficial
because it can also help democratize the use of AI.
Because right now, many customers are
sitting by the sideline as new and improved models
are coming by just because they don't want to face
the issue of having to redo all the post-processing
and so on and so forth.
Let alone the cost of repressing large galleries,
because if you have a photo collection,
when I come in with a new model,
you have to reprocess every single image in your gallery
through the new model so that you can recreate an index
to search.
And also, if you have hundreds of thousands of images,
that's OK, but if you have billions, OK,
that starts becoming a little bit more complex.
And so it turns out that both the regression problem
and the reprocessing problem are solved in the same way,
and that is by utilizing the existing model.
Can you talk more about the way you approach solving the problem?
Yeah, so the backward compatibility problem
is fairly simple to formalize and the solutions
that we have to pose are very simple to implement.
And now the backward compatibility problem, the regression.
Yes, the backward compatibility problem
is where you have an old model.
You replace it with a new model.
And you would like, as in the case, for instance,
of your photo collection, to be able to use
the old model to search old pictures
without having to reprocess them through the new model.
OK, and so one way to do that is to optimize
over a new backbone, a new model that
has bigger, larger number of parameters,
different architecture, completely different,
except that model is biased to use the classifier
that the old model used.
So it says you force it to live in the same metric space
where you can do clustering, where you can do search,
and so on and so forth.
Yeah, just to interject, is that you
might think that, OK, my typical practice
might be to start with my old model
and freeze the weights of my classifier or something
like that and just retrain and fine tune based on new data.
Is that inadequate to assure the kind of backward compatibility
and the errors?
So you can do backward compatibility
by just taking the same architecture and retraining it
with the same classifier that you're fine.
But if you follow the literature, every few months
a new architecture comes by, and you
want to harvest benefits of that.
And so in that case, these two representations
that are in fit to a classifier
live in different spaces, spaces of different dimensions.
So you cannot even compare them.
And so, but you can, however, force the network
to develop a representation that is
mathematically compatible with the old classifier.
So you can compute distances, compute angles, and so on.
So that becomes a fairly cleanly formalized problem
and that you can attack with standard methods
of machine learning.
Now, the positive congruent training,
which is the training done in a way
that minimizes the regression errors,
that's a more amorphous problem.
Because depending on what type of errors you want to avoid,
it may take different shapes.
And something that is still puzzling us
is that the current best performing model for positive congruent
training is one that does not explicitly
enforce that you make a mistake on a certain course of data.
But that is trained using ensembles, which don't know
anything about each other.
And they don't know anything about the old model.
So we call this future proofing, because training with this
ensures that later there will be fewer of what we call negative flips.
Negative flips are data points for which the decision
before it becomes wrong.
So there are definitely very open scientific questions
that need to be understood.
We just put there the first seed.
But definitely there's a lot to be understood
that we and others are working on acting.
And so what do these ensembles look like?
What are the components of the ensembles?
The ensembles could be the same architecture
training different ways, or with different breakdown
of the data, or complete different architectures.
The only problem is that the ensemble is not viable in practice,
because if at different time you want
to run an ensemble of 10 models, your cost of interest
will get multiplied by 10.
And that's something that the customers will not accept
gladly, right?
So we really need to figure out ways
to perform positive congruent training
at exactly the same cost as running one model,
not an ensemble of models.
So it says it's a paragon of performance,
because right now it's achieved the best performance,
but it's not a viable target for deployment,
because it multiplies the cost by an integer multiple,
and that's not, you know, it's a nice target.
So, you know, this is a problem.
And so when you say that the ensembles can be,
it can be, you didn't say anything,
but that, you know, there's some,
you made it sound like the ensemble was not constructed
in a particular way that had this property,
but rather the act of using ensembles
had kind of a regularization type of effect
that addressed PCT.
Correct, correct.
And one would be induced in thinking
that because ensembles work,
then these negative flips are points
that are very close to the decision boundary.
So when you train different models,
the boundary jitters around,
and so these points flip.
But in fact, we discover that many of the points,
data points that flip are actually very far
from the decision boundary.
They are very high confidence data points.
And so you're making high confidence mistakes,
which are the worst kind, right?
Absolutely, it's not knowing that you don't know.
It's a real problem.
It's that don't include an effect and all that.
And so what is the best practical approach
to addressing this issue?
Yeah, so right now we have a form of what we call
focal distillation.
Distillation is where you train a model
and then you train another model to mimic
the input of behavior of the first,
but that model could be smaller or could have
other characteristics.
And of course, you don't want to imitate the old model
because then you would inherit also the mistakes.
You want to imitate the old model only
where the old model gets it right.
And otherwise, you want to have the freedom
to optimize according to the new architecture,
better data, more balanced data and whatnot.
So is there a difference between distillation,
broadly and student teacher type of an approach?
Yeah, so it's a general umbrella of methods
that go under the name of distillation
or student teacher models.
Yes.
And this is a particular class of them
where you don't just try to mimic the behavior of the model,
but you try to mimic the behavior of the model
restricted to the course of data that interests you.
Now because typically, this course of data,
including the negative flips, is a tiny minority
of the total.
I mean, tiny could be in the order of five, six, seven percent.
So it is significant if you account for the fact
that people fight very hard for a 1% performance improvement.
And all of a sudden, you're squander 7%
because of these negative flips.
And so, but if you just do distillation
on 7% of the data, that would be lost in the general loss
function.
So you have to do distillation in a clever way.
That's what we call a focal distillation.
But you know, it's, again, it's an active area of research
and we both read about methods that are coming up
that improve on that.
And we also have internal efforts
that are aimed at improving that.
It sounds coarsely like a compositive distillation
and like active learning where you're
trying to identify the most important data
to train the model on.
Yeah, that's an important point.
We very strongly believe in active learning.
We haven't quite been able to make it work the way
we would like it to work.
But yeah, there's so many open questions.
We could spend the rest of the afternoon on this.
But these are all very good questions.
And so you started with this was a problem that came up
in the context of customer challenges.
Have you, is the solution in front of customers
or is this still at the research frontier
and working its way towards practice?
It's both so that we are still working at the frontier.
But some of our backup compatible training
are now in the hands of customers.
So typically, this is how it works at AWS.
So once you have a solution, it gets deployed to customers
very quickly.
And there are customers that are also engaged in the process
so we can ensure that once we launch something,
it has customer fighting and feedback.
One of the things that you mentioned early on in the conversation
is as well as in your blog post on the topic
is pointing out this idea of the artificial, separate
relationship between training and inference
and kind of speaking to, you know, how the brain is kind
of online learning, are you working in that area
as well?
And I'd love to kind of get your take on, you know,
where we are as a community with regard
to kind of, you know, breaking the artificial separation
between training and learning.
Yeah, that's a great question.
Yeah, that goes under the general umbrella
of lifelong learning or continual learning.
And typically in the literature,
people focus on the problem of catastrophic forgetting,
meaning that as you train with new tasks and your model,
you forget that are things, so how do you avoid that
and so on and so forth?
There are big issues of scale that goes out
because as you train more and more tasks
and more and more models, the question is,
how do you fit them into an architecture?
How do you grow organic or the architecture and so on and so forth?
So there are a lot of very interesting questions.
There's one piece of work that recently that gave us hope
in this arena, which is a work actually on language.
Language is, you know, there's always a small diatribe
between language science team and vision science team
because language is quote unquote easy.
Of course, it's not easy.
But you know, it's a domain that is very large but finite
and the type of nuisance variability to which
language that are subject is very small compared to images.
So, you know, we have a sentence, you can move the words around,
you can misspell them and so on.
But if you have an image of a cat,
there's image of different cats
and the different positive,
illumination, code color and so on and so forth.
So it's infinitely many different embodiments of that concept.
But in languages, one, both the data and the object of inference
live in the same space.
So, which is not a revision.
In vision, you want the labeled cat,
but in the image, there's no, in the physical scene,
there's no labeled cat, you know, there's a pixel.
So within this work in languages called tan,
which is basically takes all language tasks.
There's a variety we took the most common,
maybe it doesn't or so.
And you know, co-reference of the solution,
a 90-tier recognition, semantic role labeling
and so on and so forth.
And Giovanni Paulini, who's a mathematician
who joined our team a couple of years ago,
was able to translate all of these different tasks
into a single task which is to translate
between different augmented languages
where the format of the language
embodies the task, okay?
So by doing that, you have all these different tasks
which you can keep training,
but you're training one model, okay?
And then the format of the query or the format
of the training data specifies
whether you're looking for name 90-tier recognition
or some other task.
And so in language, we're able to do that
in a way that we haven't yet been able to do in vision.
And still, you know, it's only very early stages
of multi-task learning and continual learning
because we still have the pre-training phase
which is artificially separated
where you train a language model
to predict missing words and so on.
So it is very early.
I think that area of investigation will go on for a while.
And but definitely it's an artificial separation,
as you said, that we need to do away with it some point.
We're just not there yet.
And so I don't think I followed the connection
between the format of the language
and the lifelong learning aspect of
that we were originally speaking to.
Can you elaborate on that a bit?
Yeah, so right now, if you train for one task,
the type of knowledge that you acquire
is specific to that task
and you cannot transfer it to a new task
which you learn because if you now take that model
that you train on task one,
now you find you don't task two,
you've lost something with respect to task one.
Now you need to worry about not forgetting something
about task one.
So if you learn them seriously,
you'll have to face this problem
when you start forgetting.
If you learn them simultaneously,
it's also synergistic because now there is knowledge
which is shared across this task.
So if you have small data set on task one,
small data set on task two,
you can train on the union of the sets
and put it as if you have, you know,
Tolkien is an author and he lives in Moscow.
Somewhere in the model,
the information that Moscow is in Russia is there.
And so if you ask the question is Tolkien Russian,
you may be induced and say yes.
So which you wouldn't, if you had trained
for each task individually
and then you don't need to worry about
to get the stuff you're forgetting,
quite the contrary,
you can harvest synergistically information
from different tasks.
Got it, got it, got it.
Very good.
What else is your team excited about?
What else are you focusing on?
Or, you know, speaking broadly about,
speaking broadly about the field,
you know, what things that we haven't talked about
or kind of on your mind?
Yeah.
So, well, first of all, it's a very exciting time
because I don't think there's ever been a time,
certainly not in my memory,
but when you as a scientist have a chance
to do things that impact people's life, you know,
right here right now.
You know, even if you were a scientist
at the forefront of research through these waves
that went through like the semiconductor wave,
the communication waves, the control waves, the wireless wave,
you know, the time it passed between ideation
and realization was in the decades, right?
But here, you know, we have scientists to join
past of the PhD and six months later
that go this in production, you know, it's just unheard of.
So it's very exciting because of that
and there's a lot of stuff happening
so that the entropy is very high.
I think where we start seeing a lot of excitement
is where there is cross task and cross model learning
and, you know, right now we understand data
and now we finally also understand
what information is, information is in the data,
but it's not the data is more.
And now we don't quite yet what knowledge is,
but knowledge has something to do with information
and we're just beginning to shed light on that.
And hopefully at some point we'll be able to reason
not in the way in which we say we do reasoning
in artificial systems, but really,
he ways that allows us to interact naturally
with the environment and naturally with physical space
and naturally I'm with machines between different machines.
And so being of US again is a little bit of a luxury
in the sense that, you know, when we joined,
we had mechanical turf.
Mechanical turf was one of the culprits for the AI revolution.
And there's A to I, which blends, you know,
artificial systems with humans.
And so all the pieces are there and, you know,
it's up to the leadership of individuals
to go and find the connections and make things happen,
which is quite exciting.
Yeah, yeah.
Well, Stefano, thanks so much for taking the time
to join us and talk through some of what you're working on.
Very, very interesting stuff and enjoy the conversation.
My pleasure, I enjoy this one.
Thank you.
Thank you.
