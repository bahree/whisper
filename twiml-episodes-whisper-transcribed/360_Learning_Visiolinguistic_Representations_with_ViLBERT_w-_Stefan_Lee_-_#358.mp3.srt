1
00:00:00,000 --> 00:00:13,400
Welcome to the Tumel AI Podcast.

2
00:00:13,400 --> 00:00:16,240
I'm your host Sam Charrington.

3
00:00:16,240 --> 00:00:24,480
Hey, what's up everyone?

4
00:00:24,480 --> 00:00:28,820
Unless you're Jared Lido or on the cast of Big Brother Germany, you've undoubtedly seen

5
00:00:28,820 --> 00:00:32,300
your world shift dramatically over the last few weeks.

6
00:00:32,300 --> 00:00:36,060
For most of us, this is simply uncharted territory.

7
00:00:36,060 --> 00:00:41,340
And unfortunately, we aren't quite able to see the finish line just yet.

8
00:00:41,340 --> 00:00:47,220
Now more than ever before is the time for us all to lean into our various communities.

9
00:00:47,220 --> 00:00:52,740
We encourage you to take this opportunity to connect with your fellow human, albeit virtually.

10
00:00:52,740 --> 00:00:57,820
And please know that the Tumel community, which is over 3,500 strong on Slack, is here

11
00:00:57,820 --> 00:01:00,580
for you, and we welcome you to join us.

12
00:01:00,580 --> 00:01:05,020
Take this chance to join a study group, work on a Kaggle competition, share your thoughts

13
00:01:05,020 --> 00:01:06,540
on a paper you're reading.

14
00:01:06,540 --> 00:01:10,700
We'd love to hear what you're working on and offer our support.

15
00:01:10,700 --> 00:01:13,900
For those who might just need a friendly hello, I'm here for that too.

16
00:01:13,900 --> 00:01:17,980
If you're so inclined, feel free to reach out via Twitter, our Slack channel, or just

17
00:01:17,980 --> 00:01:21,580
shoot me an email to Sam at Tumel AI dot com.

18
00:01:21,580 --> 00:01:25,260
Please be safe and take care of your health and that of those around you.

19
00:01:25,260 --> 00:01:28,260
Peace and love, and on to the show.

20
00:01:28,260 --> 00:01:30,060
All right, everyone.

21
00:01:30,060 --> 00:01:34,580
I am here at NURPS 2019, and I am with Steffen Lee.

22
00:01:34,580 --> 00:01:39,460
Steffen is an assistant professor at Oregon State in the School of Electrical Engineering

23
00:01:39,460 --> 00:01:40,780
and Computer Science.

24
00:01:40,780 --> 00:01:42,700
Steffen, welcome to the Tumel AI podcast.

25
00:01:42,700 --> 00:01:44,500
Yeah, thanks for inviting me.

26
00:01:44,500 --> 00:01:45,500
Absolutely.

27
00:01:45,500 --> 00:01:50,460
Let's talk a little bit about your background before we dive into some of the many things

28
00:01:50,460 --> 00:01:52,740
that you're presenting here at the conference.

29
00:01:52,740 --> 00:01:53,740
Sure, sure.

30
00:01:53,740 --> 00:01:59,100
So I did my PhD at Indiana University, and most of my work there was sort of on the core

31
00:01:59,100 --> 00:02:00,420
computer vision side.

32
00:02:00,420 --> 00:02:05,060
So how do I use computer vision to help scientists do various tasks?

33
00:02:05,060 --> 00:02:09,060
So a lot of it was replacing what would otherwise be human labeling tasks.

34
00:02:09,060 --> 00:02:10,060
Okay.

35
00:02:10,060 --> 00:02:14,700
I get bored somewhat quickly, so in my postdoc I extended out to vision and language,

36
00:02:14,700 --> 00:02:21,060
so I'm thinking about problems where an agent has to reason about visual input and linguistic

37
00:02:21,060 --> 00:02:22,060
input.

38
00:02:22,060 --> 00:02:26,860
And not only understand the visual content, but it has to express that understanding by

39
00:02:26,860 --> 00:02:30,260
creating language or responding to language, things like that.

40
00:02:30,260 --> 00:02:33,780
And is visual question answering one of the tasks that is interesting to you?

41
00:02:33,780 --> 00:02:34,780
Yep.

42
00:02:34,780 --> 00:02:37,780
So visual question answering, I've got a number of pieces of work on that topic.

43
00:02:37,780 --> 00:02:38,780
Okay.

44
00:02:38,780 --> 00:02:42,820
There's also things like captioning or doing visual dialogues from multi-round Q&A style

45
00:02:42,820 --> 00:02:43,820
dialogues.

46
00:02:43,820 --> 00:02:44,820
Okay.

47
00:02:44,820 --> 00:02:45,820
Cool.

48
00:02:45,820 --> 00:02:48,700
And then lately I've been extending out to tasks that not only have vision language,

49
00:02:48,700 --> 00:02:50,220
but also some form of action.

50
00:02:50,220 --> 00:02:55,620
So these are vision and language tasks situated in embodied context, where an agent tests

51
00:02:55,620 --> 00:03:00,060
to see and talk and move to accomplish some sort of task.

52
00:03:00,060 --> 00:03:04,060
And are the agents that we're referring to here, are they simulated agents?

53
00:03:04,060 --> 00:03:09,020
Uh, so mostly they're simulated agents, but some recent work has sort of extended out

54
00:03:09,020 --> 00:03:12,460
of the simulator into physical platforms with some surprising success.

55
00:03:12,460 --> 00:03:13,460
Okay.

56
00:03:13,460 --> 00:03:18,500
What's an example of a platform that you're using for simulation or for the robotic lab?

57
00:03:18,500 --> 00:03:22,860
For the robotics on the robotic side, is they, uh, what's the kind of dimensionality,

58
00:03:22,860 --> 00:03:24,060
how complex are they?

59
00:03:24,060 --> 00:03:25,060
Yeah.

60
00:03:25,060 --> 00:03:29,260
So most of the work we've been working on so far has been on this pie robot platform, which

61
00:03:29,260 --> 00:03:34,700
is something that Facebook has recently released, which is this wonderful low-cost, robotic

62
00:03:34,700 --> 00:03:37,260
platform, a low co-bot, I think they call it.

63
00:03:37,260 --> 00:03:38,260
Okay.

64
00:03:38,260 --> 00:03:40,500
That has a really nice interface for machine learning practitioners.

65
00:03:40,500 --> 00:03:41,500
Oh, really?

66
00:03:41,500 --> 00:03:42,660
So I have not come across that yet.

67
00:03:42,660 --> 00:03:43,660
It's worth looking at.

68
00:03:43,660 --> 00:03:47,820
So it's, you know, you can say from pie robot, import robot, and then robot, go forward

69
00:03:47,820 --> 00:03:48,820
one meter.

70
00:03:48,820 --> 00:03:52,500
And that's sort of the level of the interface, so it's a real machine learning persons robot

71
00:03:52,500 --> 00:03:54,500
for things like navigation and grasping.

72
00:03:54,500 --> 00:04:00,220
So here at the conference, you've got a number of presentations that you're, and posters

73
00:04:00,220 --> 00:04:07,740
you're involved in, uh, we'll talk about, uh, one of them in most detailed billboard, uh,

74
00:04:07,740 --> 00:04:10,740
but what are some of the others that you've been focusing on?

75
00:04:10,740 --> 00:04:11,740
Sure.

76
00:04:11,740 --> 00:04:15,220
So this year I'm presenting Ember things that you said, uh, one of them is one of these

77
00:04:15,220 --> 00:04:16,220
embodied tasks.

78
00:04:16,220 --> 00:04:17,220
Mm-hmm.

79
00:04:17,220 --> 00:04:22,580
We work on what's called vision and language navigation, where an agent is sort of spawn

80
00:04:22,580 --> 00:04:26,740
in a never-before-seen environment, and given a natural language and navigation instruction.

81
00:04:26,740 --> 00:04:31,660
So it was something like go down the hall, turn left at the wolf head, and stop on the

82
00:04:31,660 --> 00:04:35,340
third bedroom, the one with the yellow bed spread or something like this.

83
00:04:35,340 --> 00:04:40,940
So instructions are this mix of trajectory clues, like go forward and turn left, and, and

84
00:04:40,940 --> 00:04:45,060
visual grounding landmarks, and then the goal is to have an agent that can reason about

85
00:04:45,060 --> 00:04:48,380
all this while actually following that path in a simulated, uh, world.

86
00:04:48,380 --> 00:04:49,380
So I am.

87
00:04:49,380 --> 00:04:53,700
And what are the, uh, priors that the agent's bringing into this environment?

88
00:04:53,700 --> 00:04:58,500
Does it already know what a wolf head is, or is it need to figure that out from training?

89
00:04:58,500 --> 00:05:02,820
So it's a tricky question, um, a lot of vision and language, and we'll touch on this in

90
00:05:02,820 --> 00:05:07,940
Villebert, uh, sort of starts from a set of pre-trained image features, and a set of

91
00:05:07,940 --> 00:05:12,220
pre-trained language features, uh, what it doesn't have is a sense of grounding.

92
00:05:12,220 --> 00:05:15,900
So it may be able to represent a wolf head visually, and it may understand the word

93
00:05:15,900 --> 00:05:20,940
wolf head, but the connection between sort of the visual incarnation of a wolf head and

94
00:05:20,940 --> 00:05:22,700
then the term, um, isn't there.

95
00:05:22,700 --> 00:05:27,300
And that you expect to learn during the specific task, um, though in Villebert, the point

96
00:05:27,300 --> 00:05:30,500
is that we're trying to pre-trained grounding itself.

97
00:05:30,500 --> 00:05:33,060
And so you've got the agent's paper.

98
00:05:33,060 --> 00:05:34,060
Yep.

99
00:05:34,060 --> 00:05:37,780
And then there's Villebert, and then I'm also giving a talk at a workshop on emergent

100
00:05:37,780 --> 00:05:39,100
communication.

101
00:05:39,100 --> 00:05:45,540
So if you have agents that interact to perform some task, and you want them to share

102
00:05:45,540 --> 00:05:50,300
information between each other or communicate, uh, how could you make that communication

103
00:05:50,300 --> 00:05:51,900
protocol more interpretable to humans?

104
00:05:51,900 --> 00:05:52,900
Mm-hmm.

105
00:05:52,900 --> 00:05:55,460
And one way to do that is to make them actually use sort of discrete symbols.

106
00:05:55,460 --> 00:05:59,460
So something that looks like a word, uh, more or less, that doesn't necessarily have

107
00:05:59,460 --> 00:06:00,460
that meaning.

108
00:06:00,460 --> 00:06:05,100
And then the talk is about how do we make these, uh, these communication protocols more

109
00:06:05,100 --> 00:06:06,100
interpretable?

110
00:06:06,100 --> 00:06:09,540
You know, this whole field was kind of, I don't know, if popularized as the right word

111
00:06:09,540 --> 00:06:16,940
or, uh, villainized a few years ago with the Facebook, uh, those two agents that were

112
00:06:16,940 --> 00:06:20,420
said to have developed their own coded language.

113
00:06:20,420 --> 00:06:21,420
Yeah.

114
00:06:21,420 --> 00:06:24,860
Uh, similar kind of vein of, of research here.

115
00:06:24,860 --> 00:06:28,420
Um, yeah, except for the fact that we, we want to understand what the code is.

116
00:06:28,420 --> 00:06:29,420
Right.

117
00:06:29,420 --> 00:06:30,420
Um, yeah.

118
00:06:30,420 --> 00:06:35,100
Um, yeah, that was this, uh, dealer, no, dealer paper, um, right.

119
00:06:35,100 --> 00:06:37,100
I had some similar, similar goals.

120
00:06:37,100 --> 00:06:38,100
Yeah.

121
00:06:38,100 --> 00:06:39,100
Yeah.

122
00:06:39,100 --> 00:06:45,780
Um, and so it's, in, in a sense, kind of merging the field or the desire for explainability

123
00:06:45,780 --> 00:06:48,780
into this emergent communication work.

124
00:06:48,780 --> 00:06:49,780
Yeah.

125
00:06:49,780 --> 00:06:50,780
Yeah.

126
00:06:50,780 --> 00:06:54,060
And it, um, if you're thinking about building an agent that eventually will work with

127
00:06:54,060 --> 00:06:57,180
a human, uh, communication ends up being a big part of that.

128
00:06:57,180 --> 00:06:58,180
Right.

129
00:06:58,180 --> 00:06:59,180
That's how we organize ourselves.

130
00:06:59,180 --> 00:07:01,900
So making sure that we can understand what the agent is saying and that the agent can

131
00:07:01,900 --> 00:07:04,980
understand what we're saying is sort of one of the goals of my research.

132
00:07:04,980 --> 00:07:11,580
But not constrained necessarily by English allowing the agent to come up with its own stuff.

133
00:07:11,580 --> 00:07:16,460
If we can map it back to some space that has human-like structure, that would be fine.

134
00:07:16,460 --> 00:07:17,460
Right.

135
00:07:17,460 --> 00:07:22,620
Um, so rather than having a unique, unique word for the concept of red ball would prefer

136
00:07:22,620 --> 00:07:26,500
the agent to have a word for red that modifies a word for ball.

137
00:07:26,500 --> 00:07:30,260
So even if it's not English, it's something we can map back to a way that we understand,

138
00:07:30,260 --> 00:07:32,300
uh, how language works.

139
00:07:32,300 --> 00:07:35,380
And so, Wilbert, uh, what's Wilbert all about?

140
00:07:35,380 --> 00:07:36,380
Yeah.

141
00:07:36,380 --> 00:07:42,220
So I hinted on this a little bit ago, but Wilbert's about learning the associations between

142
00:07:42,220 --> 00:07:48,580
the visual incarnation of a concept and the linguistic incarnation of a concept.

143
00:07:48,580 --> 00:07:54,660
And this is sort of tricky because most people instantly hallucinate the visual part

144
00:07:54,660 --> 00:07:58,660
of something whenever they hear the other word when it's when I said wolf head, I assume

145
00:07:58,660 --> 00:08:01,100
a lot of people immediately thought of like Game of Thrones type of things.

146
00:08:01,100 --> 00:08:04,380
Um, and there was some visual concept that got brought to mind.

147
00:08:04,380 --> 00:08:07,580
But the machines don't by did it sort of automatically have this, right?

148
00:08:07,580 --> 00:08:11,140
You can, I mean, you can learn language in just in a linguistic context.

149
00:08:11,140 --> 00:08:14,140
And in fact, that's what most of national language processing does.

150
00:08:14,140 --> 00:08:15,140
Yeah.

151
00:08:15,140 --> 00:08:17,940
Is just learn it, it's an association with other words.

152
00:08:17,940 --> 00:08:18,940
Uh-huh.

153
00:08:18,940 --> 00:08:21,500
Likewise, on the visual side, you're just sort of learning to represent some sparse set

154
00:08:21,500 --> 00:08:27,900
of classes and those classes often relate to specific nouns, but they don't have a sense

155
00:08:27,900 --> 00:08:29,220
of closeness, right?

156
00:08:29,220 --> 00:08:33,380
So there's no idea that the feature for a cat should be close to the feature for a tiger

157
00:08:33,380 --> 00:08:37,460
because they're related linguistically or in certain taxonomies.

158
00:08:37,460 --> 00:08:42,020
So the point of Villebert is to try to learn these associations between vision and language

159
00:08:42,020 --> 00:08:43,020
directly.

160
00:08:43,020 --> 00:08:46,060
And this is something we'll usually call a visual grounding of a word.

161
00:08:46,060 --> 00:08:47,060
Okay.

162
00:08:47,060 --> 00:08:53,260
And so what's the, the general approach there, presumably it involves, uh, Burt, transformer

163
00:08:53,260 --> 00:08:54,260
models.

164
00:08:54,260 --> 00:08:55,260
Yeah.

165
00:08:55,260 --> 00:08:56,260
The name is not as well.

166
00:08:56,260 --> 00:09:01,220
Um, yeah, so if you, this, if you look at the Burt models and some of the big successes

167
00:09:01,220 --> 00:09:05,020
in an LP, it's these large self-supervised tasks.

168
00:09:05,020 --> 00:09:11,380
So they take a large language corpus and they learn certain little things to build supervision

169
00:09:11,380 --> 00:09:12,700
from unlabeled data.

170
00:09:12,700 --> 00:09:16,580
So they'll, you know, they'll mask out a few words and have them reproduct it based

171
00:09:16,580 --> 00:09:20,740
on the other linguistic context or their last of a sentence follows another sentence

172
00:09:20,740 --> 00:09:21,740
in text.

173
00:09:21,740 --> 00:09:25,820
And what we do is we, we find analogs for that in the vision and language space.

174
00:09:25,820 --> 00:09:26,820
Okay.

175
00:09:26,820 --> 00:09:31,740
So there's this data set called conceptual captions that came out recently, which is this

176
00:09:31,740 --> 00:09:32,740
massive data set.

177
00:09:32,740 --> 00:09:36,180
I think it's about in the order of about three million, uh, image text pairs.

178
00:09:36,180 --> 00:09:37,180
Oh, wow.

179
00:09:37,180 --> 00:09:41,140
Where they just found images online that had alt text.

180
00:09:41,140 --> 00:09:44,500
So some human had provided some alternative text for people, usually for people with visual

181
00:09:44,500 --> 00:09:45,500
impairment.

182
00:09:45,500 --> 00:09:47,540
You might interact with this by mousing over an image.

183
00:09:47,540 --> 00:09:48,540
Sure.

184
00:09:48,540 --> 00:09:49,620
And it produces some, some text tag.

185
00:09:49,620 --> 00:09:53,660
Uh, they did some processing on top of that, but it's this sort of Webly supervised data

186
00:09:53,660 --> 00:09:55,140
where they scraped all this down.

187
00:09:55,140 --> 00:10:03,540
You know, the images filtered for kind of simple, simple kind of image net types of depictions

188
00:10:03,540 --> 00:10:06,140
or they just raw whatever's out there on the web.

189
00:10:06,140 --> 00:10:09,940
They're across the board from everything from, you know, flicker style images to pictures

190
00:10:09,940 --> 00:10:12,860
of maps and things like this.

191
00:10:12,860 --> 00:10:16,820
So it's pretty, pretty broad, but that's sort of where we start as our data source and

192
00:10:16,820 --> 00:10:22,260
we perform similar self-supervised trainings where we're masking out certain image parts

193
00:10:22,260 --> 00:10:26,420
of the image, just at random, and then asking it to reconstruct those given the rest of

194
00:10:26,420 --> 00:10:28,700
the image and the language.

195
00:10:28,700 --> 00:10:33,220
And likewise, we're asking, does this sentence match with this image or not or masking out

196
00:10:33,220 --> 00:10:36,460
parts of the language and having it reconstruct from the image and the text?

197
00:10:36,460 --> 00:10:41,820
So we're designing this sort of self-supervised multi-modal task with this large, weekly supervised

198
00:10:41,820 --> 00:10:42,820
data source.

199
00:10:42,820 --> 00:10:48,740
And what we get at the end is a model that has built some representations that bridge between

200
00:10:48,740 --> 00:10:52,940
vision and language and then we fine tune that for a wide variety of other tasks.

201
00:10:52,940 --> 00:11:00,900
Now randomly masking out parts of the image sounds like a very blunt instrument to apply.

202
00:11:00,900 --> 00:11:02,940
Yeah, it is.

203
00:11:02,940 --> 00:11:09,100
So part of it is we don't know which parts of the image are being described by the associated

204
00:11:09,100 --> 00:11:10,660
alt text.

205
00:11:10,660 --> 00:11:15,580
So it's not clear that we should mask out certain regions versus certain other ones.

206
00:11:15,580 --> 00:11:20,620
So yeah, it is a blunt instrument, but I think if we were going to be more precise, we

207
00:11:20,620 --> 00:11:22,380
would already need to know the grounding.

208
00:11:22,380 --> 00:11:23,860
And that's exactly what we're trying to learn.

209
00:11:23,860 --> 00:11:28,060
Yeah, I think the picture that came to mind was doing some kind of object detection in

210
00:11:28,060 --> 00:11:32,740
the image and then masking out one or more of the objects.

211
00:11:32,740 --> 00:11:39,500
It kind of almost predicates the image net kind of image that I initially asked about

212
00:11:39,500 --> 00:11:41,660
as opposed to more natural scenes.

213
00:11:41,660 --> 00:11:45,260
Well, not only does it predicate that, it's also a good question.

214
00:11:45,260 --> 00:11:50,940
How many objects can we detect until recently it was something the biggest data set for

215
00:11:50,940 --> 00:11:53,540
detection was something like 80 or 100 classes.

216
00:11:53,540 --> 00:11:57,180
I think open images brought it up to one or 2,000.

217
00:11:57,180 --> 00:12:01,860
But if you actually look at the richness of text, we use all sorts of visual words all

218
00:12:01,860 --> 00:12:02,860
the time.

219
00:12:02,860 --> 00:12:05,460
Even if you're looking around where we're recording right now, there's lots of objects

220
00:12:05,460 --> 00:12:08,220
that you know a word for.

221
00:12:08,220 --> 00:12:13,340
You could describe, but they would never show up in a detection challenge.

222
00:12:13,340 --> 00:12:16,420
So the question is, how do we build groundings for those things?

223
00:12:16,420 --> 00:12:19,220
Not just for things we can already detect, because if we can already detect them, we sort

224
00:12:19,220 --> 00:12:21,180
of how do you have the grounding?

225
00:12:21,180 --> 00:12:24,980
Talk a little bit more about how you've kind of adapted the process for training a

226
00:12:24,980 --> 00:12:29,260
BERT-like model, transformer model, to incorporate this additional information.

227
00:12:29,260 --> 00:12:30,420
Yeah, absolutely.

228
00:12:30,420 --> 00:12:36,020
So one of the sort of core parts of the BERT model is that it treats this sequence of

229
00:12:36,020 --> 00:12:42,100
words, a sentence, sort of as a set of independent inputs and then does a bunch of self-attention

230
00:12:42,100 --> 00:12:44,100
in between to process this.

231
00:12:44,100 --> 00:12:49,460
And the actual position of a word in that sequence is just denoted by some positional embedding.

232
00:12:49,460 --> 00:12:55,260
So it's really just a set of words with some sequentiality added as a feature representation.

233
00:12:55,260 --> 00:12:56,900
And you could think about an image the same way.

234
00:12:56,900 --> 00:13:01,940
You could think about an image as a set of bounding boxes, for instance, of just various regions

235
00:13:01,940 --> 00:13:06,980
in the image that have been pulled out and some associated visual feature and a positional

236
00:13:06,980 --> 00:13:07,980
encoding.

237
00:13:07,980 --> 00:13:12,740
And this box out from over here, so it's just a set, it's an unordered set, in fact.

238
00:13:12,740 --> 00:13:18,260
So the actual input API for BERT looks the same in our model for the visual side and the

239
00:13:18,260 --> 00:13:19,260
linguistic side.

240
00:13:19,260 --> 00:13:24,540
As we're just assuming we have a set of features with some sort of encoding put in.

241
00:13:24,540 --> 00:13:28,820
And then we're sort of directly copying over a number of the self-supervised losses that

242
00:13:28,820 --> 00:13:30,220
BERT is actually designed for.

243
00:13:30,220 --> 00:13:31,220
So masking out parts.

244
00:13:31,220 --> 00:13:35,700
The image we're asking about alignment between the text and the image.

245
00:13:35,700 --> 00:13:39,660
In BERT, it's a sentence and the next sentence, and you're predicting whether one comes

246
00:13:39,660 --> 00:13:40,660
after the other.

247
00:13:40,660 --> 00:13:43,940
But in our case, it's an image and a sentence, and we're asking, does this align or not?

248
00:13:43,940 --> 00:13:48,140
Is this actually a pair from conceptual options?

249
00:13:48,140 --> 00:13:54,220
And then sort of if you're asking about more internal mechanisms, we rely on a co-attentional

250
00:13:54,220 --> 00:13:57,780
transformer, which is something we introduce here, but it's been introduced by a number

251
00:13:57,780 --> 00:14:02,780
of people simultaneously, because it's sort of intuitive idea, that rather than doing

252
00:14:02,780 --> 00:14:08,340
self-attention, you're attending one modality based on queries from the other modality.

253
00:14:08,340 --> 00:14:12,660
So you're using the language to attend over the image, and then you're taking that sort

254
00:14:12,660 --> 00:14:17,620
of pooled image feature back into the language to inform the rest of that computation.

255
00:14:17,620 --> 00:14:22,620
And we have those sort of co-attentional layers periodically through this structure.

256
00:14:22,620 --> 00:14:30,380
The result is kind of a jointly trained model that is kind of trained on the relationship.

257
00:14:30,380 --> 00:14:33,620
It's not you've kind of trained on some visual stuff, and then some language stuff, and

258
00:14:33,620 --> 00:14:36,700
you found a way to kind of merge them together.

259
00:14:36,700 --> 00:14:40,460
Yeah, I mean, I think that's a pretty fair assessment.

260
00:14:40,460 --> 00:14:43,620
We aren't training the vision from scratch, so we are sort of starting with some frozen

261
00:14:43,620 --> 00:14:48,500
representation, but we are allowing it to learn the association between the two together.

262
00:14:48,500 --> 00:14:52,820
But it's a very dynamic back and forth process, so it's not just like a direct assignment.

263
00:14:52,820 --> 00:14:55,380
So what's the frozen representation that you're starting from?

264
00:14:55,380 --> 00:14:59,860
We're starting from a faster RCNN trained on visual genome, so it's just sort of a standard

265
00:14:59,860 --> 00:15:01,780
practice in the vision and language space.

266
00:15:01,780 --> 00:15:06,140
Okay, and so how is that incorporated into the virtual model?

267
00:15:06,140 --> 00:15:09,620
So that's the thing that actually outputs these bounding boxes.

268
00:15:09,620 --> 00:15:10,620
Okay.

269
00:15:10,620 --> 00:15:13,940
So it's sort of like your object detection idea, except for that we are not stopping at

270
00:15:13,940 --> 00:15:15,700
the classes it actually predicts.

271
00:15:15,700 --> 00:15:18,820
We're looking for the associations more broadly.

272
00:15:18,820 --> 00:15:25,660
Okay, so the bounding boxes aren't random random there driven by this.

273
00:15:25,660 --> 00:15:27,660
They're driven by the faster RCN model.

274
00:15:27,660 --> 00:15:30,060
You can see arbitrary, more so than random perhaps.

275
00:15:30,060 --> 00:15:32,300
Or not even arbitrary, but there is picking out.

276
00:15:32,300 --> 00:15:33,620
You can also sample from them.

277
00:15:33,620 --> 00:15:37,060
So the faster RCN model can produce quite a lot of bounding boxes, and you can sample

278
00:15:37,060 --> 00:15:40,460
from it if you'd like to increase the randomness there.

279
00:15:40,460 --> 00:15:44,100
If you actually look at some of the bounding boxes that come out, many of them aren't really

280
00:15:44,100 --> 00:15:49,980
object aligned, because if you look at the visual genome data, it's actually fairly noisy.

281
00:15:49,980 --> 00:15:52,860
Some of it's very clean, some of it's very noisy.

282
00:15:52,860 --> 00:15:56,220
So sometimes you'll have, for instance, an object is like a road very often or something

283
00:15:56,220 --> 00:15:57,220
like this.

284
00:15:57,220 --> 00:16:01,500
It doesn't do a great job of honing in on specific objects.

285
00:16:01,500 --> 00:16:06,980
Where does Wilbert fit into kind of the broader scope of your research?

286
00:16:06,980 --> 00:16:08,580
What do you want it to take you?

287
00:16:08,580 --> 00:16:12,780
Is it a means or an end or a little bit of both?

288
00:16:12,780 --> 00:16:15,100
Yeah, I mean, so it's both.

289
00:16:15,100 --> 00:16:16,100
Right.

290
00:16:16,100 --> 00:16:21,500
So there's a fundamental question of how do we align language and vision?

291
00:16:21,500 --> 00:16:25,940
And that's the sort of interesting research question, and then why would we want to do

292
00:16:25,940 --> 00:16:28,860
that is perhaps a useful follow-up.

293
00:16:28,860 --> 00:16:31,780
And there's a number of things that are sort of concrete applications.

294
00:16:31,780 --> 00:16:36,420
If you could align vision and text really well, you can train models to do things like

295
00:16:36,420 --> 00:16:40,740
VQA or Visual Question Answering, which can help people to visual impairment ask about

296
00:16:40,740 --> 00:16:45,060
the world around them and get answers back, or you could do image captioning to provide

297
00:16:45,060 --> 00:16:50,220
descriptions of what someone would be seeing if they didn't have the same visual impairment.

298
00:16:50,220 --> 00:16:51,220
Yeah.

299
00:16:51,220 --> 00:16:59,620
So the ideas that someone could take this Wilbert model off the shelf and kind of VALA transfer

300
00:16:59,620 --> 00:17:05,420
learning, just drop it into their application, and what would the inputs be and what are

301
00:17:05,420 --> 00:17:06,420
the outputs?

302
00:17:06,420 --> 00:17:07,420
Yeah.

303
00:17:07,420 --> 00:17:08,420
Yeah.

304
00:17:08,420 --> 00:17:12,740
So in our case, the inputs are sort of image features that they're looking at, and then

305
00:17:12,740 --> 00:17:14,700
some sort of text query.

306
00:17:14,700 --> 00:17:20,500
So we transfer image features, meaning the bounding boxes extra from some model.

307
00:17:20,500 --> 00:17:26,940
So, since the Wilbert paper, we've applied it to something on the order of 12 different

308
00:17:26,940 --> 00:17:29,620
vision and language tasks.

309
00:17:29,620 --> 00:17:36,060
So you can pre-train this and then use it as a base to perform fairly well, fairly quickly

310
00:17:36,060 --> 00:17:40,140
on a wide range of these visual and language reasoning tasks.

311
00:17:40,140 --> 00:17:43,580
So the inputs, some bounding boxes, and some text.

312
00:17:43,580 --> 00:17:48,220
So let's say I was doing question answering, and I would take my image, and I would extract

313
00:17:48,220 --> 00:17:51,740
these bounding boxes and feed it in, and then I would feed my question as the text.

314
00:17:51,740 --> 00:17:56,860
And I can train output that just predicts some set of answers, and that trains quickly

315
00:17:56,860 --> 00:18:02,020
and trains well, rather than training it from scratch on this status set.

316
00:18:02,020 --> 00:18:03,780
And so you can train it.

317
00:18:03,780 --> 00:18:08,580
You can kind of fine tune for specific task types.

318
00:18:08,580 --> 00:18:14,260
Presumably you can also fine tune it with specific types of data or kind of data language

319
00:18:14,260 --> 00:18:15,260
relationships.

320
00:18:15,260 --> 00:18:16,260
Yeah.

321
00:18:16,260 --> 00:18:17,260
How might that work?

322
00:18:17,260 --> 00:18:21,300
So this is actually what comes up often in these tasks, right?

323
00:18:21,300 --> 00:18:25,820
So Wilbert's pre-trained on this huge set from conceptual captions that covers like we

324
00:18:25,820 --> 00:18:28,020
talked about earlier, quite a few things.

325
00:18:28,020 --> 00:18:33,140
But if you go to something like VQA, which is based off Coco, there's 80 major object categories

326
00:18:33,140 --> 00:18:37,940
and the images are all things people were proud enough to put on Flickr, so that they

327
00:18:37,940 --> 00:18:39,540
could eventually get downloaded.

328
00:18:39,540 --> 00:18:42,780
So that's a much more constrained image set.

329
00:18:42,780 --> 00:18:47,500
But when you're fine-tuning, you're fine-tuning the whole of Wilbert, and you are feeding in

330
00:18:47,500 --> 00:18:49,580
those images and they search for a question.

331
00:18:49,580 --> 00:18:50,580
So it can fine-tune.

332
00:18:50,580 --> 00:18:51,580
Got it.

333
00:18:51,580 --> 00:18:52,580
So you're kind of doing both at the same time.

334
00:18:52,580 --> 00:18:59,540
You're fine-tuning the visual data set, but also the, you know, for the specific task

335
00:18:59,540 --> 00:19:00,540
that you're asking it to do.

336
00:19:00,540 --> 00:19:01,540
Yep.

337
00:19:01,540 --> 00:19:02,540
That's exactly right.

338
00:19:02,540 --> 00:19:03,540
Okay.

339
00:19:03,540 --> 00:19:04,540
Interesting.

340
00:19:04,540 --> 00:19:05,540
Interesting.

341
00:19:05,540 --> 00:19:11,060
And so kind of going back to the previous question, you know, where does this lead do

342
00:19:11,060 --> 00:19:17,900
you think from the broader perspective of kind of visual, kind of this integration between

343
00:19:17,900 --> 00:19:20,780
visual and language tasks?

344
00:19:20,780 --> 00:19:21,780
Yeah.

345
00:19:21,780 --> 00:19:28,980
So I think the grounding problem itself could use sort of more attention.

346
00:19:28,980 --> 00:19:33,620
We've been very tasked-driven as a community, and we're learning sort of myopic groundings,

347
00:19:33,620 --> 00:19:34,620
right?

348
00:19:34,620 --> 00:19:39,300
So if I train a model for VQA, it understands what Coco images look like, and it understands

349
00:19:39,300 --> 00:19:43,820
what questions are, or likewise if I do a captioning model.

350
00:19:43,820 --> 00:19:45,860
Sometimes this is even more shocking.

351
00:19:45,860 --> 00:19:50,620
You've trained a model to caption Coco images, and Coco images have a really small set

352
00:19:50,620 --> 00:19:51,620
of classes.

353
00:19:51,620 --> 00:19:55,860
For instance, one example that I like to show from a recent paper is they don't have any

354
00:19:55,860 --> 00:19:57,660
guns.

355
00:19:57,660 --> 00:20:01,860
So if you've fed this caption, we're trained on Coco with an image of a man in a red

356
00:20:01,860 --> 00:20:05,300
hat with a red shirt holding a shotgun.

357
00:20:05,300 --> 00:20:08,860
And the caption is a man in a red hat and a red shirt holding a baseball bat.

358
00:20:08,860 --> 00:20:09,860
Okay.

359
00:20:09,860 --> 00:20:13,420
Because he's wearing what looks like a baseball uniform, and he's got something in his

360
00:20:13,420 --> 00:20:14,420
hands.

361
00:20:14,420 --> 00:20:16,020
Might as well be a baseball bat.

362
00:20:16,020 --> 00:20:20,980
And if we talk back to these potential applications of helping people with visual impairment, that

363
00:20:20,980 --> 00:20:23,220
kind of mistake doesn't seem justifiable.

364
00:20:23,220 --> 00:20:24,220
Yeah.

365
00:20:24,220 --> 00:20:30,100
There was a tweet going around just yesterday of the day before where someone passed a

366
00:20:30,100 --> 00:20:38,540
chart into a captioning system, and it was like a chart with like three, you know, three

367
00:20:38,540 --> 00:20:44,220
trend lines or three graph, three data sets that were kind of diverging.

368
00:20:44,220 --> 00:20:48,340
And it was like something with two scissors.

369
00:20:48,340 --> 00:20:49,340
Yeah.

370
00:20:49,340 --> 00:20:50,340
Yeah.

371
00:20:50,340 --> 00:20:52,860
So there's that whole side of things.

372
00:20:52,860 --> 00:20:53,860
Right.

373
00:20:53,860 --> 00:20:57,500
And there's this other paper that I recently did with a co-author Peter Anderson called

374
00:20:57,500 --> 00:21:02,500
No Caps, which is a novel object captioning at scale, and it addresses these sort of problems.

375
00:21:02,500 --> 00:21:07,620
Where how do I caption objects that maybe I've never heard talked about before?

376
00:21:07,620 --> 00:21:12,020
So I don't have linguistic data about shotguns, for instance, but I might have an object

377
00:21:12,020 --> 00:21:16,740
detector that can tell me that that object exists, and then how do I incorporate that into

378
00:21:16,740 --> 00:21:18,740
language descriptions?

379
00:21:18,740 --> 00:21:20,260
Tell me more about that process.

380
00:21:20,260 --> 00:21:21,260
Sure.

381
00:21:21,260 --> 00:21:22,260
How does that work?

382
00:21:22,260 --> 00:21:23,260
How does that work?

383
00:21:23,260 --> 00:21:25,620
So that's one of the open questions.

384
00:21:25,620 --> 00:21:30,460
So we presented a data set and ran some initial baselines from techniques that had sort

385
00:21:30,460 --> 00:21:33,180
of already been floating around on this.

386
00:21:33,180 --> 00:21:37,180
Did you also propose a model that performs well on the task?

387
00:21:37,180 --> 00:21:38,180
No.

388
00:21:38,180 --> 00:21:39,180
Not in that paper.

389
00:21:39,180 --> 00:21:40,180
Okay.

390
00:21:40,180 --> 00:21:41,180
So, not yet.

391
00:21:41,180 --> 00:21:42,180
Coming soon.

392
00:21:42,180 --> 00:21:43,180
Potentially.

393
00:21:43,180 --> 00:21:52,420
But the way it typically works is the model will consider regions in the image as potential

394
00:21:52,420 --> 00:21:55,020
words that it could use in its output.

395
00:21:55,020 --> 00:21:59,180
So even if it doesn't really know what the right language for that specific part of the

396
00:21:59,180 --> 00:22:03,660
image is, it might recognize that that's an important thing to talk about and want to

397
00:22:03,660 --> 00:22:08,020
try to put it into the caption wholesale, and then you rely on the object detector to

398
00:22:08,020 --> 00:22:09,020
tell you what that's...

399
00:22:09,020 --> 00:22:13,900
Maybe an action in a red hat with some object that I can't identify that seems important.

400
00:22:13,900 --> 00:22:19,540
A man with a red hat holding a that, where that is the image region that contains the

401
00:22:19,540 --> 00:22:20,860
gun in this case.

402
00:22:20,860 --> 00:22:24,620
And then you rely on object detectors to tell you, or image classifiers, to tell you what

403
00:22:24,620 --> 00:22:29,740
that image patch actually was, just some subset of already defined kind of things.

404
00:22:29,740 --> 00:22:34,100
So basically, it's saying, if I already know how to use language to describe the world,

405
00:22:34,100 --> 00:22:40,060
but I don't know specific objects, it seems silly that I should have to go collect new

406
00:22:40,060 --> 00:22:43,420
descriptions that contain that object.

407
00:22:43,420 --> 00:22:47,060
You know how to talk, but you may not know what a widget is, but if I point you to a widget,

408
00:22:47,060 --> 00:22:50,700
you can then talk about the widget, you just need that first grounding.

409
00:22:50,700 --> 00:22:53,540
So this line of work is really about given that grounding.

410
00:22:53,540 --> 00:22:56,820
How do you incorporate it into language models or how do you actually talk?

411
00:22:56,820 --> 00:23:02,140
And it kind of, it likes what, likewise, seems silly to pick some other random object.

412
00:23:02,140 --> 00:23:07,980
You know, it probably isn't this thing and stick it in this place.

413
00:23:07,980 --> 00:23:09,980
Yeah.

414
00:23:09,980 --> 00:23:13,580
Confidence estimation is an open problem in this space.

415
00:23:13,580 --> 00:23:21,260
I mean, does the model know it's not a baseball bat, or does it really think it is?

416
00:23:21,260 --> 00:23:25,580
It's something that needs to be explored, where we're going to actually deploy these things.

417
00:23:25,580 --> 00:23:32,020
His other thoughts on kind of interesting challenges in this visual language domain or

418
00:23:32,020 --> 00:23:33,820
the grounding problem in particular.

419
00:23:33,820 --> 00:23:38,180
I mean, I'll use the question as an opportunity to talk about some other recent work that I'm

420
00:23:38,180 --> 00:23:39,180
excited about.

421
00:23:39,180 --> 00:23:40,180
Yeah.

422
00:23:40,180 --> 00:23:45,900
So, for a long time, in the community, grounding has been on static images.

423
00:23:45,900 --> 00:23:51,460
But there's actually a lot of concepts that rely on motion, that rely on interaction

424
00:23:51,460 --> 00:23:52,460
to ground.

425
00:23:52,460 --> 00:23:56,020
I don't know if I could, I could give you a photo and you could tell me it looks like

426
00:23:56,020 --> 00:24:00,660
people are talking, but they could just be sitting there quietly as well, right?

427
00:24:00,660 --> 00:24:04,420
It's interaction that actually sort of tells you what's going on.

428
00:24:04,420 --> 00:24:11,460
So lately, I've been moving, are we primarily in this or definitionally talking about kind

429
00:24:11,460 --> 00:24:18,540
of verb concepts as opposed to noun concepts or is it independent of that particular distinction?

430
00:24:18,540 --> 00:24:24,100
I think that's a fine generality, I think that's a fine way to understand it in general.

431
00:24:24,100 --> 00:24:26,820
Though some verb concepts can be learned from static images.

432
00:24:26,820 --> 00:24:27,820
Sure.

433
00:24:27,820 --> 00:24:28,820
So it's like jumping, right?

434
00:24:28,820 --> 00:24:31,900
We sort of defies gravity, so we know what they're roughly doing.

435
00:24:31,900 --> 00:24:32,900
Yeah.

436
00:24:32,900 --> 00:24:34,780
They'll actually jumping versus falling.

437
00:24:34,780 --> 00:24:38,500
That nuance doesn't really get captured very easily for a lot of images.

438
00:24:38,500 --> 00:24:39,500
Yeah.

439
00:24:39,500 --> 00:24:44,060
So, I've been interested in settings where agents have to demonstrate their visual and

440
00:24:44,060 --> 00:24:49,420
linguistic understanding in simulated environments by acting, this was sort of like the VLN stuff

441
00:24:49,420 --> 00:24:52,300
I talked about earlier, the first work.

442
00:24:52,300 --> 00:24:55,700
But there's also things like if I drop an agent in an unseen environment and I ask them

443
00:24:55,700 --> 00:25:02,020
a question, what color is the car, how does an agent navigate, see the car, know that

444
00:25:02,020 --> 00:25:06,740
it now has enough information to answer, and then offer up an answer in return.

445
00:25:06,740 --> 00:25:10,100
So these sort of problems are things that I'm increasingly interested in.

446
00:25:10,100 --> 00:25:16,780
Is the problem there as you've framed it up, passing a model of sequence of images and

447
00:25:16,780 --> 00:25:25,500
having it identify either caption or identify an action or something else, or is it more

448
00:25:25,500 --> 00:25:30,980
the lot of things that you describe seem different, like the agent navigating the environment

449
00:25:30,980 --> 00:25:31,980
etc.

450
00:25:31,980 --> 00:25:36,260
What you described sounds a lot like what image, video captioning or video question answering.

451
00:25:36,260 --> 00:25:37,260
Yeah.

452
00:25:37,260 --> 00:25:43,860
But I'm much more interested in sort of situations with this active perception, so embodiment

453
00:25:43,860 --> 00:25:44,860
of some kind.

454
00:25:44,860 --> 00:25:49,300
So when I ask what color is the car, if the agent starts in a bedroom, it needs to understand

455
00:25:49,300 --> 00:25:53,460
the cars who are outside, it needs to get into the hallway because it knows hallways tend

456
00:25:53,460 --> 00:25:54,460
to lead outside doors, right?

457
00:25:54,460 --> 00:25:58,100
It shouldn't go to the bathroom attached to the bedroom, there's no outside doors there.

458
00:25:58,100 --> 00:25:59,100
Right.

459
00:25:59,100 --> 00:26:03,940
So it has to start reasoning about structural priors in the world and navigating according

460
00:26:03,940 --> 00:26:07,740
to those to actually decide and output an answer in the end.

461
00:26:07,740 --> 00:26:13,820
So it's more about controlling the agent towards a goal that's specified in natural language.

462
00:26:13,820 --> 00:26:19,580
And do you have existing data sets that will allow you to get there or is that part of the

463
00:26:19,580 --> 00:26:21,820
challenge is coming up with new data sets?

464
00:26:21,820 --> 00:26:22,820
Yeah.

465
00:26:22,820 --> 00:26:27,900
So I think data sets are obviously a big driver of progress in machine learning a couple

466
00:26:27,900 --> 00:26:31,940
about a year ago, the team in Georgia Tech, where I was working, output this embodied

467
00:26:31,940 --> 00:26:37,660
question answering project, which revolved around this, and then there's also the vision

468
00:26:37,660 --> 00:26:42,980
and language navigation task from Peter Anderson, which relies on sort of falling instructions

469
00:26:42,980 --> 00:26:45,460
in environments again by controlling the agent.

470
00:26:45,460 --> 00:26:49,100
So it's recent, but people are producing these sort of things.

471
00:26:49,100 --> 00:26:50,380
Anything else you want to cover?

472
00:26:50,380 --> 00:26:54,500
No, I think that's fine for right now, if you have more questions, I'm happy to answer

473
00:26:54,500 --> 00:26:55,500
more.

474
00:26:55,500 --> 00:27:02,540
So we're keeping our eyes peeled, we'll be keeping our eyes peeled for your upcoming publications.

475
00:27:02,540 --> 00:27:05,220
Thanks so much, Stefan, for taking the time to chat with us.

476
00:27:05,220 --> 00:27:06,220
Yeah, thanks for having me.

477
00:27:06,220 --> 00:27:07,220
All right.

478
00:27:07,220 --> 00:27:08,220
Thanks.

479
00:27:08,220 --> 00:27:13,420
All right, everyone, that's our show for today.

480
00:27:13,420 --> 00:27:17,660
To learn more about today's guest or any of the topics mentioned in this interview, visit

481
00:27:17,660 --> 00:27:19,860
twimmelai.com.

482
00:27:19,860 --> 00:27:24,500
Of course, if you like what you hear in the podcast, please, please, please subscribe,

483
00:27:24,500 --> 00:27:28,380
rate, and review the show on your favorite pod catcher.

484
00:27:28,380 --> 00:27:56,900
Thanks so much for listening, and catch you next time.

