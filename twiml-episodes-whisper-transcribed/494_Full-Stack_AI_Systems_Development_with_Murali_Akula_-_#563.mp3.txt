If you have to spend time on designing and optimizing manually for every problem you have
and deploy it onto the device, that's not going to scale, right?
So our researchers focus not on manually figuring out how to optimize the neural network itself.
They focus on innovative ways of doing AI.
All right, everyone. Welcome to another episode of the Twomo AI podcast. I am, of course,
your host, Sam Charrington. And today I'm joined by Marali Akula, Senior Director of Software
Engineering at Qualcomm. Before we get going, be sure to take a moment to hit that subscribe button
wherever you're listening to today's show. Marali, welcome to the podcast. You began your career
with more of a traditional software engineering focused. Tell us a little bit about how you found
your way into machine learning. Thanks Sam, excited to be on the show. Thanks for having me.
Like you said, I'm a software guy and I started working on a wireless software stack. I worked in
the beginning couple of commercial products and commercial teams. Then after that, I joined
the Carpet R&D group, which is a research group. And I was part of the 4G research team.
So at Qualcomm, when you were talking of 4G research, we build the entire system.
We bring it up OTA, which is over there and test it out properly.
And make sure it works in the real world constraints. So we have this mindset,
unless you build it, it's not real. So we build things that can scale into the scalable and
deployable, for example. You start with the radio and the hardware, the firmware, the software,
all the layers, talked to bottom. And we do this before even you go to standardize or commercialize it.
So this is an entirely a pre-commercial setting. We build the entire thing. So that's what I was
doing for a while, part of the 4G research team. Then late 2000s, the first of the Android smartphones
was coming out. My boss was Jeff Gellar at that time. He came on this podcast a couple of times.
So he suggested that I look into a smartphone user experiences and he connected me to a team
which was looking into latency in wireless protocol and how we can improve the smartphone user
experience. I started looking into the smartphone. And beyond wireless, there were so many components
which you built into it. And also during that time, new applications were showing up and
the so much going on. And I was really amazed about so much technology we packed into this platform.
And I really wanted to work on the entire smartphone user experience. So that's what happened.
And it was a lot of fun. After that, I continued on machine learning projects. And we did that.
It was before the deep learning took off. And over time, it's become more and more deep learning.
So that's where I am. Doing software engineering and AI research today.
Awesome. Awesome. And so tell us a little bit about your role today and the team that you
that you work with. I have a broader organizational responsibility as the head of the Carpet R&D
software engineering team that is looking into forward-looking efforts across the AI spectrum.
My day-to-day where I spend most of my time, I lead a software team in a cross-discipline effort
for AI research. So researchers applied ML, folk, software, hardware, testers. We all sort of work
together in a cross-discipline effort. And we apply the same mentality which we I talked to
talked about earlier in other research efforts. If you unless you build it, it's not real. So we
build out the technology that comes out of our AI research and prove that it can be deployed
onto devices. So that is broadly the role we have. And on one side to make that happen,
we are working with researchers jointly. And on the other side, we are working with the commercial
teams to be always ready. So by doing this, looking at the entire system, building it out,
we are in a unique position to figure out how or when our innovations can go into our products.
So we'll get the entire full stack for that. Can you elaborate a bit on the full stack?
I know from our past conversations, that's a concept and a terminology that's important to you
and the team. But it's also something that we throw around in the industry quite a bit.
So when we talk about full stack, let me start about what happens when Qualcomm is looking into AI
technology. We have a lot of research going on to advance fundamental AI for applications that
are going to be running on Snapdragon platform, either now or in future in different products.
When you are innovating new ways of doing AI, researchers run the experiments on
servers, which run in racks in a data center. And it's all good because you can run lots of
experiments and get good results. Now the innovations which come out of that from this big compute,
how do you make them run on the resource constraint devices? The software which runs on the
device, the hardware architecture, the software, the tools which you use to deploy them onto the
device, all that is different from what you did the software which you used when you actually
ran the experiments. So that's the one which I call the full stack that is needed to deploy
your AI innovations onto the devices. So we will have to look into the entire full stack.
So research into the full stack is extremely important for us. We need to continuously improve
every layer of this full stack and build it out and also show that it is real and it can be
deployable and this keeps us ready all the time so that any innovation we have, we can actually
deploy into the real world devices. So that's what we do. We continuously look at this hardware
software full stack and figure out how to deploy it onto this resource constraint that
mobile devices. When I hear you talk about that lifestyle, I think a little bit about I guess
Docker and where the container technologies came from, you know, developers running things on
their desktops and you know, trying to get it in a prod and not working and say, hey, it worked on
my machine. And you know, in a lot of places, those same kind of container technologies have been
adopted for machine learning, you know, workloads. But there's this whole other set of complexities
when you're talking about mobile and devices, you know, not just kind of the runtime, but the
constraints that you alluded to earlier in terms of power and other things. Can you elaborate a
bit on when you think about devices in particular for machine learning, you know, what are the
constraints and maybe, you know, I think we're all kind of familiar with, you know, them as users,
like, hey, not a lot of memory, not a lot of compute, bandwidth issues, but, you know, there's some
non-obvious things or like, how do you think about that as an engineer who works on this stuff
constantly? Yeah, it's interesting. You mentioned how people build applications and deploy them and
Docker containers. There is no Docker container on the device. But there are applications,
applications which are running in real time, right, very efficiently. And with the neural network,
efficiently mapped to the hardware, there's many constraints on the device. It's not, first of all,
all these mobile devices, they're on batteries. They're mobile, right. If you take a smartphone or a
VR headset or an autonomous driving car, power efficiency is extremely important. That is,
I would say almost number one, real time latency is important. When you're running experiments on
the server or even applications, for example, you can run it in batch mode on data. But when you're
running on the device, as a signal comes from cameras, you need to be making inferences in real time.
Again, you need to know that this is not the only thing which is running on the device. There's
a lot of other things which are running through. And they all have to run in real time.
Latency is extremely important. Area is a big concern. The chip size, for example, the size of the
block accelerator which you're using, the memory sizes, they're all important because in a way,
they impact power efficiency and latency. You can add more area and get the better latency,
but you'll be trading off against power, right. So area is a big concern. The
number of sensors, the resolution, the number of applications which you're running, the tasks,
everything is just increasing. The size of the neural networks, and we need to pack more and
more compute and you can't just keep increasing the chip size, right. So area is very important.
And how you run it on the machine learning accelerator, using the on-chip memory versus going off-chip.
When you actually run the neural network, you have limited on-chip memory. You can increase,
but it's going to cost and area and power, you know, all those things come into picture.
So on-chip memory. So there's many constraints like that when you run and it becomes extremely
important that how you optimize the neural network to deploy and properly map it to your hardware
resources and the entire, you know, software hardware to look at, you know, we look at all that
and develop techniques on an ongoing basis how to optimize your neural network for deployment.
So this is the neural networks are going to continue to get complex and complex, right. And we need
to make them smaller and efficient and deploy them. You know, no from prior conversations with,
you know, folks like Jeff who you mentioned, they're Qualcomm as well as folks on the research side
that techniques like quantization and compression and related techniques are, you know, often,
often a subject of research there. Are those the main types of things that you are applying when
you're talking about getting these complex models working on mobile devices or are there other
things? How do you think about that landscape and process? Yeah, I think what you mentioned,
they are very important, those and lot more. We are looking at, when I say,
entire full stack, you know, it starts from the top. We work very closely with the researchers,
so our researchers when they design the network itself, you know, they are looking at what are the
parallelism in this design, which we can exploit and how do we design this so that it can run
efficiently on a resource constraint device, right? It starts from there. And after that,
you come up with a good design for your architecture and how do you train? How do you train so that
you can deploy it onto the device? Normally, if you train freely, it becomes pretty complex.
One example of how to train for deployment onto the device is neural architecture search,
while you are designing your neural network, you could use neural architecture search to get
the best optimal architecture for your design, right? So that is next. And once you have the
neural network, then how do you deploy it? You get the best accuracy and how do you deploy? When
you deploy, these are the couple of things which you mentioned already. We run on integer hardware.
And when you train, you are training in floating point. So quantization techniques are important.
And compilation, how do you optimally map the tensors to the memory? And how do you map the compute
to the instructions of the hardware accelerators? So those are important. So that is the next step,
right? How do you deploy? And then how do you run on the device? When you optimally compile this
onto the device, the application stages the sensor data coming from the camera, how does it get
moved into the accelerator? What are the preprocessing stages? Then you run the neural network,
what are the postpassing stages? So that entire application architecture is next and then the hardware
architecture, right? So all of it. So that's what we look at when we say, you know, looking into
full stack. So I'll also mention one other thing which is extremely important, right?
I might have alluded to this in, you know, previous efforts which I mentioned too. So it takes a
continuous research effort not just to design the neural network, but also to design tools and
techniques which can make deploying them onto the device important. So we do a continuous research
into full stack with a mindset that our research in a way is running on real devices on an ongoing
basis, okay? We sort of connect that being in the middle. We look at everything hardware software,
algorithmic layers and all that and also the we collaborate, we collaborate with everybody because
research is happening in universities, research is happening in our research teams and even in
product teams, a lot of innovations are happening. So we sort of connect the entire dot.
Can you maybe take us through an example of, you know, when you think of a complex model that's
coming out of research, you know, what that, what that looks like and how you kind of walk this
process that you identified for getting, you know, into the device? Yeah, sure. Monocular depth
estimation is an application which we recently implemented and demonstrated. I can walk you through
that. Monocular depth estimation is about creating a depth map of the scene in front of you. So
this is very important for many applications. Thinking like autonomous vehicles or as one example.
Autonomous vehicles is one example. Even mobile phones when you're, you know, pointing at something
and you need to detect features of the face. For focusing for in the camera, for example.
Yeah, a lot of applications like that and VR headsets drones, you know, drones are going at high
speed. So you need to detect obstacles very fast. So in your autonomous driving example.
How far is the car in front of you in the scene? And how far is a pedestrian or obstacle?
So all those are very important and depth estimation is creating that depth map.
We need to infer for every pixel in the scene a depth value. How far it is?
And Monocular depth estimation is about applying a neural network and inferring this depth map
from a single camera image. Monocular depth estimation is about singular single camera image.
As opposed to like you've got some controlled environments with cameras all over the place
that you can interpolate to figure out depth. Exactly. Yeah. So that is the application and we implemented
it on the device and demonstrated, you know, we took it around the time for Newrips.
So when we implemented that, then I can explain how we went through the process. So what happens
is when accuracy is extremely important in this use cases, right? Researchers can get the
best accuracy, but through those experiments you can end up creating very complex models.
Then what ends up happening is it becomes so complex, it becomes almost invisible to
deploy it on the device and there are a bunch of state of the art neural networks which can achieve
that. So what our researchers did is they working closely with us and also working on an ongoing
basis with this device mentality. Our researchers designed a neural network where at training time
they increase the complexity, but that doesn't translate to inference time. So because of that,
you can get best accuracy at training time, but you can still run it on the device. So that's
where it starts, right? As I explained, you know, I think part of what you're saying is it starts
of research like there are ways to do this kind of depth estimation, but the complexity of the
resulting models is such that they're not feasible starting places for something that you eventually
want to run on device. So you're kind of starting at the algorithm level with the thought of,
hey, this eventually needs to run on the device. That is correct. Yeah. I mean, you could just say,
oh, I can just increase add more layers, right? I'll increase the complexity and get, but we know
what happens if you do that. So our researchers have that mindset. So it's a very important innovation
to be able to increase the training time complexity, but not let that go to the device.
So that's what our researchers did. And after that, we have a neural network and we take the design
and the full stack team next took that even after that, right? Our inference per second speed was
23 FPS. So that's not going to make it real time. So the next step was to make the model smaller
and our full stack team used neural network architecture search NAS techniques to reduce the size
we have NAS tools which we use. So that is the next step. Now you make the model smaller. The next
step is to deploy it on the device. To do this, you need to quantize it. Some models quantize
well, some models don't quantize well. In this particular case, we have to apply the our
aim at techniques post training quantization techniques to make sure we don't lose the accuracy.
So that is the next step. And after that, we use our compiler tools and compile it properly so
that it can be mapped properly to the hardware which we have. And the next step is application design,
right? We design the application properly and run it on the device, measure on device performance,
and we have an integration team which touches it properly and makes sure everything is stable.
The application, the architecture, the tools, any improvements which we made.
And that is the entire process of how you take one of the use cases like monocular depth estimation
all the way to the device. Let's dig into all that. You mentioned the first step is the algorithm.
Can you share a little bit more detail about the algorithm that was developed?
It's called, again, that the technology is monocular depth estimation. Our researchers came up with
a method called x-digital. x-digital? Yeah, I'll summarize it. That's a lot of
and we'll of course link to the full paper in the show notes.
So what they did is they I earlier gave a hint that you know our researchers
increased the complexity of the network while training, right? So the way they did it is they
added semantic information. That increases the complexity of the network, but that semantic
information is not needed on the device. So you don't need to transfer that network down and
the training also happens in a self supervised manner. So this is a summary of what they ended up
doing. We use extra information, increase the complexity of the network, but that extra information
which is semantic information is not required to be transferred onto the device.
So the method is called x-digital method. So that is the first step, you know, which our
researchers were able to come up with this innovation. You mentioned the next step was
applying neural architecture search. Even before you do that, you invest all this time and research
like coming up with a more efficient algorithm. Like why do you need to go through the next step
is it you know do you have predefined performance targets that you're trying to hit and you kind
of go as far as you can with algorithm but you're not quite there or is there a you know I'm thinking
of like some kind of efficient frontier of like you know where you get all your efficiencies from
to overuse efficiency maybe. I think that's an excellent question. So think of it this way. This
thing but we already have that in our aim at toolkit. So we were able to we didn't have to get into
you know they were able to design the end-to-end system in a way where it can actually run on the
device. They're looking at the complexity of the network while they're designing. When we are
working jointly with them you know being in the same team software engineers working with the
researchers, we looked at the end-to-end architecture and we found that this entropy encoding and
entropy decoding could be a bottleneck. The reason it is is we can exploit the parallelism in
the neural network and map it to our accelerator but when you do entropy decoding,
when you need to decode a symbol you need to you need the previous symbol that means you need to
decode the previous symbol it's sort of serialized the entire thing. So you have this wonderful neural
network part which can be paralyzed but the resulting latent you end up serializing it. So we
discussed with the researchers and then they looked at the latent and the mathematical properties
of it and figured out how can we paralyze it and they figured out that latent are actually either
independent or conditional independent and they were able to like figure out how to design a parallel
entropy encoder and parallel entropy decoder. So once they did that we took that and implemented
it on the CPU. SnapDragon platform is a heterogeneous platform we have the accelerator, we have the
CPU, we have the GPU, we have DSPs. So we were able to implement the parallel entropy decoder
on the CPU and match the parallelism which is happening on the accelerator and then we were
able to get it to real time. A lot of our conversation thus far has been focused on
trying to achieve like inference latency targets and other inference oriented
objectives on devices. Is there anything that you're aware of looking at more of the
like training on devices? Is that is that a thing yet? It is a thing. So are the past
number of years the full stack on the device was more about running inference workloads
while training was mostly centralized and on the servers. So the full stack accounts for
optimizing the inference the forward pass on the device but lately past couple of years
are even more we're seeing more and more training on the device using the local data.
Primarily for personalization reasons you have some data which is very unique to you you can
personalize the model for yourself. So those kind of use cases have already been deployed and
they're showing up. So training is becoming an important part of the full stack and we are looking
at it. So we at Neurip's 21 we actually implemented a federated learning system
for distributed training completely on the devices. Nothing is happening on the server except for
getting the incremental updates from various devices and combining them. So we implemented
end-to-end federated learning training system. So that is also part of our full stack effort
and it requires a lot of software engineering. It's a distributed system. So you need to make
sure the entire architecture works well and it works for both PyTorch and TensorFlow users.
We created libraries on the device. We created connections to a coordinated running in the cloud
so that everything scales up to thousands of devices. And is the idea that like Amit and Dana and
some of these other tools that it kind of just goes into the bag of tricks and the next time you
need to build a model that could benefit from distributed training you can apply this or do you
think for in the near future it will require that manual kind of hand crafting that we've talked
a little bit about. Yeah so the goal is to avoid anything manual which is not going to scale.
So you need to build these tools. They are going to become automatic more and more. And the federated
learning framework we package it in a way that our teams who want to do federated learning can
just take it and run the experiment but also be able to deploy without having to do a lot of work
downstairs. Awesome, awesome. So thinking back to the examples that we've talked about
the monocular depth estimation, the neural video codec, we've talked about kind of this process
up from getting from research to real world kind of touch it, see it on a device. Does that mean
that they're products and that they're productized or is that a further step that you take these
products or these projects? There is a further step. So we are still part of the research team.
Whatever we create is deployable. But our focus is on creating a solid research pipeline.
A research pipeline of fundamental AI improvements and also how they can be optimally
deployed onto the device. So across the full stack. So we innovate, implement and create
this research pipeline. But while doing that, the product teams also have innovative advances
in the architecture, both hardware and software. And we work very closely with the product teams.
And by doing a full stack approach in the research team, we are always ready. When the need arises,
the product teams can immediately decide to take some of the innovations. Some go very early,
some go a little late. But immediately as a very soon, take it and put it into their commercial
roadmap, product roadmap, then after that, it's about scaling. Scaling across Qualcomm business.
That's a pretty big effort. So we focus on the research side, building a solid research pipeline
and our product teams are focusing on the product roadmap. We work very closely and collaboratively.
And the form that these products end up taking, like does that, say, monocular depth estimation,
would that be built into a silicon or would that be software that's part of a toolkit or
kind of all of the above, depending on what makes the most sense? Monocular depth estimation is one
example. We have a lot of AI technology in our platform, in Snapdragon platform. We have the entire
camera, everything which is needed to do camera, everything which is needed to do video processing.
So the answer is all of the above. All these innovations, like applications,
our product teams will take integrated into the products. Same thing with the
full stack innovations, our software team, product software team takes it and puts it into the
roadmap. But we also make sure that we figure out the best way we can put it out into the community,
into the world. Like for example, a model efficiency toolkit. We felt, you know, it's important that
that's an open source project. So anybody can use it to quantize to run an integer hardware.
We also have a model zoo project. Some of the applications will just put it out in the model zoo.
And with some of them, we just directly give it to the customer. So they can integrate. So it's
sort of all of the above. Okay. Awesome. Awesome. Well, Marali, thanks so much for taking the time to
share a bit about how you see kind of this full stack ecosystem and getting some of the research
ideas into product. Thank you. Thanks, I'm just so really grateful. Thanks.
