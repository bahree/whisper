All right, everyone. I am here with Michael Bronstein. Michael is a professor at Imperial College London and head of graph machine learning at Twitter.
Michael, welcome back to the Twomo AI podcast. Thank you, Sam. Oh, great to be here. Absolutely is great to have an opportunity to catch up with you.
You joined Twitter about a year ago, following the acquisition of Fabula, which you co-founded in April of 2018, if LinkedIn is guiding me correctly, which was just a few months after the last time we spoke, which was in December of 17 at Nureps in Long Beach.
So it's been an exciting two, two and a half years for you. Yeah, that's right. So I think when we talk, it was together with Jean-Brona, and we were talking after our tutorial that we gave on this topic at Nureps, and a lot of things have changed in this field from that date in these two years. It's been really, really quick and very fast-paced.
Awesome. Awesome. Well, why don't you give us a little bit of an update with what you've been up to, your role at Twitter and any changes in what you've been up to at Imperial College London.
Sure. So probably since we last met, I think, when at Nureps 2017, when we also presented with Jean this tutorial, it still was rather a niche or an exotic topic, basically using neural networks to do deep learning on graphs appear to be something that is quite removed from what the majority of people were doing at that time in machine learning.
Nowadays, it's very different in a matter of these two years, graph neural networks have become one of the most prominent topics, and if you look at the statistics of I clear, for example, that happened a couple of months ago, that was one of the most frequent keywords in the papers that were submitted.
So you really see graph neural networks everywhere at the machine learning conferences. This is probably one of the most frequent topics, and it is always why basically graphs are a very generic abstract mathematical models for systems of relations, interactions, you can model with graphs systems in practically every field of application from particle physics to social networks to biological sciences.
And applications from these domains, drag development, neutrino detection, different things that we've been doing in these domain of work on proteins, I'll be glad to talk about it.
So it's really a very broad, a vast spectrum of applications and problems, and that's why this has become a very prominent topic.
We should say that there has not been a revolution as probably some of us that have been working in this field for a while, something similar to what happened in computer vision with convolutional neural networks about eight years ago after the famous Alex net paper that actually completely revolutionized the field of computer vision.
We've seen really in the killer up some field where graph neural networks were to make such a dramatic impact, but it is probably more an evolution extremely fast evolution.
So in a matter of few years, basically these methods are everywhere.
And I think there are several reasons, so if you can think of what were the key drivers for success of deep learning, these are obviously large data sets that became available in the public domain.
So not only data sets, but also carefully designed benchmarks that include data and tasks and the way for evaluating them, the image net is the great example, then computing power.
And in this case, it was GPUs and also software with software libraries such as PyTorch or TensorFlow, you can very easily implement and prototype deep learning systems.
So similar things started happening in the domain of graph learning as well.
And we have the open graph benchmark that was announced just less than a year ago in the end of 2019. There are several libraries that are professionally implemented and maintained like the deep graph library or PyTorch geometric.
The hardware is still the good old GPUs, you can argue whether they're well suitable for dealing with graphs, but at least they do the job.
So basically we have these magic confluence of all the factors that made deep learning successful and that promise to make deep learning on graphs successful as well.
In terms of basically water, the key challenges that we still need to overcome, one of them is scalability, basically so far most of the research really focused on small graphs.
So small graphs like citation networks, maybe maximum five or 10,000 nodes. This is not what you really see in practice.
So if you look at the graphs that we need to do at Twitter, they have hundreds of millions of nodes, multiple billions of pages.
So this is the gap of orders of magnitude.
And until I would say very recently and with exception of a few research groups, these topics have not been addressed in the academic community.
So we only now start seeing methods that are developed for dealing with large scale graphs and being evaluated what is probably more important on large scale data sets and open graph benchmark tries to breach this gap.
So basically it's only the only the beginning of seeing these.
These methods use in settings that are closed or to real life applications in industrial settings. I'm aware of a few settings where these systems are already used in production.
Some of them are confidential information so I cannot I cannot disclose the details, but there are several companies that are already using graph neural networks and their production systems.
It starts to become real industrial systems.
And is it the several companies that you're thinking of is it, you know, that there are a few companies that are operating at the scale that you suggested or is it that in general, graph neural networks, you know, we're just starting to, you know, get to that kind of edge of production deployment.
Well, so to ask that in part because I hear about it all the time, a lot of conversations about it, I get the impression that it's being used, you know, more regularly by folks.
But you're saying that, you know, you only know of a few.
Well, at least a few officially. So there, of course, they are used whether it's used in production system or whether it's used for research. So Pinterest was probably one of the pioneers. So they used, used it.
Probably already a couple of years ago.
Li Baba last year published a paper where they showed that they use graph neural networks in some of their business applications.
Yeah, I, in thinking about the killer app for this and some of the points you made about scale, I, you know, wonder if part of the issue is that, you know, the killer app is, you know, social networks and we only have a few of those, you know, we're not, not every company is out there starting their own social network.
Do you think that social networks are kind of uniquely positioned to use graphs or are they, you know, is graph neural networks, you know, just as strong a tool for some of the non kind of obviously graphical types of use cases like we see in health care and, you know, you mentioned physics and medicine and other things.
Yeah, absolutely. So to start with the first part, obviously social networks like Twitter, maybe Facebook and Google are the first candidates that come into mind.
This is probably also the very obvious graph structure data that is that is produced by by people using these platforms.
But there are many other things recommended systems, so let's say company like Amazon for example, so there are not a social network, but they have a lot of graph structure data about how people interact with their products and they have recommended systems.
The companies like Netflix, for example, as well with their classical already Netflix challenge, one of the typical examples of the ecosystem. So there are many more than just the obvious two or three companies that would come to your mind when talking about graph structure data.
Beyond these, there are many other applications where graph structure data is a very natural way of describing the data that is generated in these applications that are collected in these applications. So you mentioned health care.
Basically, the way that we can think of our body, basically it's an interaction graph between a lot of biomolecules, whether it's proteins, whether it's drugs, whether it's metabolites.
And recently, it shows that there is a lot of benefit of thinking of it in this way. So these are very complex systems. It's probably hugely simplified way of thinking of them as a static graph. It's probably dynamic system.
There is a lot of factors and degrees of freedom that we are probably still unable to account for, but the bottom line of this is there has been a lot of interesting progress that comes from modeling biological systems.
In health care applications from the position of graphs, I would mention just one of them. So there was earlier this year, paper and cell, which is top biological journal.
I grew from MIT that showed a new class of antibiotics that was virtually screened using graph neural networks.
One of the things that I noticed in the way you talk about graph neural networks today, and maybe this is evolved over the past couple of years.
But to your point, you talk about it as being applicable to non-uclidean structure data today as opposed to what I remember from you or perhaps from others being more focused on graph structure data.
So there's obvious applications of it in the case of social networks and other things, but it sounds like now you're seeing and going for people to understand the broader applicability of these types of a graphical formulation beyond the things that are obviously naturally structured as graphs.
We do like to think of it as non-uclidean structure data, basically as opposed to what's a grid-like data as you see in images or in audio or in text.
So graphs are probably the most generic models for that, but there are many other applications of many phones, for example.
We actually started with using deep learning on geometric objects and meshes on many phones and then we moved to the more general graphs.
But these methods are if you're talking about other types of data that is maybe slightly different from graphs in computer graphics in computer vision community.
There is a lot of work nowadays of using deep learning on meshes on basically discrete representations of three dimensional shapes.
Actually a little bit ironic that computer vision has always said that you know 3d data, it's computer graphics if you're working on this.
So you're not from our community or from the secret community, but.
But nowadays actually if you look at the best paper words or the candy days for best paper words, probably half of them are somehow related to 3d geometry.
Interesting how fields evolved so bottom line a lot of interesting applications in computer vision as well in computer graphics that involve geometric deep learning.
So maybe talk a little bit about the focus of your research at the next level of detail at at Twitter and the university.
Are you you know what have you been focused on to push the research forward.
Yeah so frankly I'm working on a lot of things I would say the major focus.
The major focus is geometric deep learning graphs, many folds and non-euclidean structure data as you said.
So let me start with Twitter basically we are working on deep learning on graphs basically we my ambition at least is to make these technology broadly applicable to many problems that that we need to press using machine learning systems.
So Twitter basically one of the core data sets and data data assets is graph structure date and it comes in a lot of different forms whether it's follow graphs or whether it's different interactions engagements of users with with content with tweets like tweeting with tweeting and so on.
Also some other graphs that are not exposed to the public that allow for example to detect a platform manipulation or reviews.
So basically we we are trying to develop methods graph deep learning methods that would be able to take better advantage of this of this kind of information.
And there are several challenges as I mentioned one of them is scalability basically we're dealing with very large data sets.
So we need to make sure that these methods scale to these to these kind of data sets which obviously automatically rolls out some of the methods that exist in the literature that are even not designed to work with these scales latency efficiency and so on.
Another elastic that that is typical of social networks in Twitter in particular that our graphs are dynamic so it's not really a static graph that that I kind of know biological network the way that proteins interact with each other basically it is changing every time every second.
And it's actually a graph that is basically an asynchronous theme of events basically every interaction or a user joining the platform using following some somebody a user.
Tweeting basically it's a graph where edges or nodes are created or deleted some asynchronous time points so being able to deal with with dynamic graphs is extremely important.
There are some other aspects of that are probably more on the theoretical research side but still very important basically understanding how these systems work.
Because if you want to to develop a system that eventually will be serving the public you need to at least understand better to make sure that there are no vulnerabilities that it cannot be misused or manipulated so for example understanding how powerful graph neural networks are whether they can be attacked in an adversarial way.
So what I'm talking about is you know convolutional neural networks are are pretty sensitive to adversarial perturbations adversarial noise.
So that's some works that show that they can change single pixel in an image and it will be misclassified by convolutional neural network.
So there have been several works recently in the domain of graph learning that show that basically you can do similar things for graphs and there are several works in that domain basically adversarial attacks on graph neural networks that actually show that.
You can provide certain robustness theoretical grantees of how graph neural networks can be can define the gains such attacks.
Interesting and is this work that you and your team have worked on our others in the field.
So adversarial noise depending on works in these domain were from the group of Stefan Guniman technical University of Munich.
But you know they started all these trend of adversarial attacks on graph neural networks.
Interesting interesting with regard to the the dynamic nature of graphs you published a paper just recently on temporal graph networks talk a little bit about that formulation and what you're trying to do in that paper.
So basically we what we presented is a very general framework that allows us to do deep learning on continuous time dynamic graphs.
So basically graphs that can be considered exactly as I said before is the stream of events.
It's whether pairwise events basically edges between nodes or events that affect the nodes.
So graph is a good example of such situation and basically it's it's a framework that generalizes the standard static graph neural networks.
But also some previous approaches that were developed for continuous time graphs.
I should say that we are obviously not the first to deal with these problems but most of the works consider graphs that are just given as snapshots.
There are a few pictures of the same graph basically this is discrete time graphs, which is quite disadvantageous model if we were to apply it for graph that change continuously like like the graph.
So the key element there are basically there are several key elements one of them we we have a way of using memory.
Basically we attach to each node a state that allows us to compress basically the history of the interaction of this node this node and the event that happened to this node.
Basically we update this memory from from the neighbors from the graph and then we can do node embeddings that represent the graph at a particular point of time.
The variable solve tasks like doing predictions about the node at certain time.
So we can say for example whether this user is a bad actor in the network and should be banned for example or we can predict future links.
So I know from my past interactions with either other users or maybe some content.
I can predict what kind of content I might engage with in the future. So that's the bread and butter of incremental systems link link prediction.
And are you holding the nodes constant and assuming the edges are dynamic or is it all dynamic.
So we didn't consider for simplicity node and age delicious, but basically the model allows new nodes to be added nodes to be updated and new edges to be created with the nodes as well.
So technically speaking we are looking at a node together is a hyper graph or a multi graph that that basically where there are multiple edges between between the pair of nodes.
And you mentioned the kind of the memory that you've got associated with each node and kind of this dynamic accounting is the idea that I think you also mentioned continuous it sounds like the ideas that you can have the system.
So running and following a stream of you know additions of nodes and edges and making predictions or updating itself and and being able to make predictions on the new things that are added as well as the existing things is that the general idea.
Absolutely, yeah, so that's a good idea, but it's a system that is always keeping up to date based on the stream of events that is happening to this graph. I should also say that one of the key findings in the paper is that training strategies extremely important.
So what we show is is a more advanced training strategy that allows us to do correct patching and training neural network in a way that is significantly better than that with previous simple approaches.
Can you give us an overview of traditional training for these kinds of networks and then some of the things that you needed to do to make it work in this temporal setting.
Probably that will be a little bit too many technical details, but basically it has to do with the way that you that you do the that you don't create the patches in the training basically there are dependencies between between the nodes that you need to handle efficiently.
Okay, awesome, and then one of the other things that I've seen pop up in your research quite a bit recently is talking about expressivity and expressive power of graphs. What's that line of work focused on.
So this is very important topic basically even the very understanding of how and when craft neural networks work well is still lacking to a large extent.
So what you see in experiment in experiments for example that in some settings craft neural networks work very well and in some other settings they are more or less as some simple baseline.
The question is what makes them work and probably more importantly what makes them fail.
And this is not a trivial question even for me waiting it basically what do you mean by expressive power of graph neural network because here you're not considering it just the function approximation capability like in the traditional setting where you have a fixed domain and you just your neural network essentially represent some class of functions and we know that even very simple neural networks are universal approximators.
They can approximate any function to any desired accuracy.
Any continuous function to any desired accuracy here you need to talk about both the domain so the graph itself and the function on the graph.
So it has been a line of works recently starting from works from the group of less kids at Stanford and Hamilton in Canada. Basically they drew parallels between craft neural networks and what is called the vice failure lemon graphism or phism test, which is a classical construction.
Graph theory basically it's a heuristic that tests whether to graphs are isomorphic whether they are the same after permutation of nodes the topological equivalent.
And essentially that one of the simplest versions of this algorithm is graph color refinement.
So you color the nodes of the graph basically you attach some discrete label to the nodes and then you look at the neighbors and basically color based on the on the unique structure of the neighborhood.
It is technically speaking it's represented as a multi set set where the same element can be repeated multiple times.
I should say for historical context that's not the first time that the device for a lemon construction was used in machine learning about 10 years ago.
And the group from MPI was the used the WL test for for constructing graph kernels, but let's say they in the context of deep learning on graphs that's about a year ago, the first results were published.
And basically was shown that standard graph neural networks, the message passing neural networks were the learning the network what the network does essentially exchanges information between adjacent nodes along edges.
There are in the best case as powerful as the WL test the vice versa lemon test.
And it is quite interesting result. So first of all, it's important it gives your clear idea when such networks work and when they fail at least on some class of problems such as graph classification.
And second, it's quite disappointing because it is known that a vice for a lemon test fails on even very simple cases.
So when I say fails, meaning that there might be two non isomorphic graphs that will produce the same coloring so the vice for a lemon test is a necessary, but insufficient condition.
It says that these graphs might be isomorphic, but we cannot tell for sure it tells you for sure that they are not isomorphic if the coloring is different, but if the coloring is the same, they might be isomorphic.
And one of the for example, one of the structures that the WL test cannot detect is triangles. So just a simple very simple straightforward triangle triangle motif that might exist in a graph cannot be detected by by by graph neural networks.
And this is very disappointing because triangles are extremely important patterns in social networks in biological networks.
So if you look at works and bioinformatics and complex systems, they use sub graph structures motifs all the time and triangles appear to be very important in many applications.
So basically there they've been follow up works the WL test. It's actually not a single test. It's a hierarchy of tests. So there is a hierarchy of vice for a lemon test that they've hired the higher order.
Instead of considering just single color refinement for single nodes, you can look at at two pulls of nodes. So you can look at basically at K nodes at the same time.
So basically the complexity of such tests is significantly higher. And there were works in particular from from the group of young leap on the vice and institute in Israel. Kagaimaron was his PhD student. He is now at I think that Nvidia and I was a member of his PhD exam committee.
They show higher order graph neural networks, which are equivalent to KWL test. The problem with these methods that their memory and computational complexity is very high.
So let's say going beyond the three WL test makes no sense. Even that complexity is quadratic, which means that if you have a graph of let's say even modest size of let's say with one million nodes, the complexity will be probably prohibited for any practical application.
So basically what what we try to do is we try to go beyond the device for a lemon hierarchy. We wanted to see if we can basically help the message passing neural network by explicitly encoding the structure around the node in the form of some structure of this picture.
And the simplest way to do it is to count a graph substructions, whether it's clicks, whether it's cycles, whether it's passes. So that was the paper that you mentioned that was done with my PhD student in Imperial College, Georgos Pulizas and the colleague from theater for pizza for ask.
So what we did is very similar philosophically to position and coding. So for each node, we can provide this extra bit of information that that allows you to do different convolutional like operation or different message passing dependent on how these nodes node looks like, basically, whether it's part of certain certain structure.
This conveys the graph neural network significant more explicitly. So for example, we can show that if you use even very simple structures like four clicks, basically fully connected graphs for nodes, you can be at least not less power than three WL test or the equivalent hierarchical networks.
I'm saying that it is not less powerful than three WL test because we can find country examples, we can find special family of graphs that are called the strongly regular graphs where our network that we call the graph substruction network works and the three WL test fails.
Now, we couldn't find examples to the country where our network fails and the three WL test succeeds. It doesn't mean that they don't exist, but I should say that the GSM, the graph substruction network, it's also it's a class of networks basically depends on what kind of structures you use.
You will get different results, you can get different expressivity. What we see is that with a strong, when you when you talk about these these substructures and achieving a certain level of expressivity is the idea that you are
constraining the network in some way to these substructures and that's what gives you the expressivity or that you're identifying the substructures and kind of noting that as a property of the node and that allows you to determine its expressivity.
So basically, it's a standard message passing architecture, so it has linear complexity like standard message passing graph neural networks. It receives an extra bit of information, which is a local descriptor that is given for each node or we also have a version where it's given per edge instead of a node that are pre computed.
The pre computation, the counting of substructures, of course depends on the substructure that you're counting, it might have higher complexity. For some simple substructures, there are computationally efficient methods, but you do it only once, you do it as pre computation, then the training at the inference has linear complexity, which is obviously very appealing property.
Basically, you're as efficient as standard graph neural network, which has exactly the same architectures standard graph neural network with the addition of these of these structural descriptors.
So the on the theoretical side, there are many, I will say, exciting questions, because at least to my knowledge, there are no results in graph theory that that that tells.
So basically, what we conjecture is that there exist small structures in the sense that they are order of land, they are independent on the size of the graph that allow the graph neural networks basically to to to disambiguate large graphs.
It would be probably more powerful than KW tests with very large case. Now, the results are pretty, pretty scarce. So there is what is called the graph reconstruction conjecture.
It tells you that you can reconstruct the connectivity of the graph from substructures that contain the same graph with single node removed.
But this is obviously not very interesting because the size of these substructures as the graph itself. So the question is whether we can do it with small structures is still an open question.
You can probably find some very pathological examples where it fails, but probably with very high probability, you can say that with small substructures, you can be extremely, extremely expressive.
And that's basically that's what we conjecture, that's what we see in experience. You can also see that it's.
Maybe taking a step back, you talked about this WL heuristic is being a hierarchical set of tests.
But the tests as I understand them are relating to, you know, working or not working or isomorphic or not isomorphic.
Where does the hierarchy, how does the hierarchy relate to these more binary classifications of the graph as a whole?
Right, so well, this is obviously the premise of the entire problem, right, whether the graph isomorphism or whether the fact that two graphs are isomorphic is what you want in terms of describing the the the expressivity of your graph neural network.
It's obviously simplification because we might want to look at.
From a different perspective that what we actually want from our graph neural network, but this is a very simple formulation that is very well studied in graph theory and basically here the expressive power is whether you are able to tell that two graphs are isomorphic or not.
And that's exactly what the hierarchy of KWL test does basically the subsequent test K plus one WL is strictly more powerful than KWL basically because there are graphs that on which it succeeds and the KWL fails.
So you've got successfully more complex tests, but each one gives you a greater degree of certainty that the graphs are isomorphic.
So basically it decides it succeeds on the bigger family of graphs.
Now, I should say that this is probably not the right way of approaching the problem of graph specificity.
It's very cool and it allowed to establish interesting bridges between what's a classical theoretical computer science and graph theory and the field of deep learning.
What we probably in real applications, we don't really care about exactly isomorphic graphs because it seldom happens.
So if you think even historically the WL tests were developed for applications in chemistry where chemists wanted to see if two molecules are the same or not.
Now, in many cases, they are not exactly the same, but they are almost the same. So what you're really interested in is some kind of graph distance graph any distance or maybe from a browser of distance that allows you to compare to basically gives you a number that tells you how similar to graphs are.
So the graph isomorphism testing is a binary thing. It tells you there are isomorphic that are prevalent or they're not.
But what we are probably more interested is when there are not isomorphic how much non-isomorphic they are. And this is something that is not covered that if we had that that might be something we get incorporated into the last function or, for example, during training.
Exactly, and basically this is also interesting because we can say whether graph neural networks in principle can distinguish between isomorphic non-isomorphic graphs, but we don't tell how this extends.
Can you say, for example, that if the graphs have distance of, I don't know, epsilon between them.
Whether the distance between the embeddings that will be constructed by such such a graph neural networks will be any close to this epsilon. So this is actually a different kind of branch of mathematics where these problems are dealt with.
And this is called metric geometry. So basically you're trying to approximate some ground for distance between the graphs using let's say Euclidean distance between the graph embeddings.
And once this the new distance between graph embeddings to be as close as possible to the original ground for distance. So in ideal world, which is obviously a wishful thinking you would like this to be an isometric embedding basically an embedding that preserves distance or at least some approximation of it.
There are many ways or many forms you can make this approximation. You can say that it's for example that it scales the distances by by certain factor by the by leaves its constant of this embedding.
You can add some noise. You can say that I can allow.
Absolutely distortion that the distance can be a little bit bigger or a little bit smaller. You can also write it as in the probabilistic setting as something that holds with sufficiently high probability when it's going probably approximately correct.
So I believe that that is probably the direction where we need to go next in this field of trying to express the power of graph neural networks.
Just to be clear on that is it coming up with any formulation for the distance between two graphs or coming up with a good formulation that is able to be expressed as an embedding space and has these properties.
So it obviously the results will depend on the distance you use. So some distances might be easier to embed in Euclidean space. Some distances might be harder. So I think you would probably choose a distance that makes sense in your application.
Some distance between graphs. The graph is over phase will be a practical case, basically that will be the case when the distance equals zero.
So we do have there are kind of current ways that we can express the distance between different graphs and that it sounds like the issue is again, how well they work in this embedding sense.
And talking about the W all test, you mentioned the idea of convolutions of on the graph, what does that mean? How does that convolution that we kind of typically think of as something that's happening in a 2D space like an image, how does that translate to graph world.
So well, there are several ways of thinking of it. You can think of it from the spectral perspective, and I think two and a half years ago when we last time talked, we mostly talked about this interpretation.
So think of convolution as a kind of shared weight. So if you think of a neural network that takes a vector input into vector output, it can be fully connected.
So basically the one output dimension is combined with it's a linear combination of all the input dimensions, right? So if you think of it as a matrix, it will be just full metrics with n squared parameters.
You can think of it as a sparsly connected network. So let's say one output neuron is connected to let's say three input neurons.
This spars matrix, it will be linear order of degrees of freedom in convolutional neural networks, all the parameters are shared. So the weights that you used to combine, let's say these three inputs to form a single output.
They are used for all the output neurons. And in this case, you get a very special weight matrix that has a circle structure or a top lead structure. So that's exactly the convolution operator.
And from the spectral perspective, this operator commutes with shift. That's what we call shift neck viviance. Actually, most people call it shifting variance, but the right mathematical term in shift neck viviance.
You can first shift your signal and then apply convolution, and it will be identical to first convolving and then shifting.
So commuting matrices are jointly diagonalized. So every convolution is diagonalized by the eigenvectors of the shift operator, which happens to be the Fourier transform in the individual case on the green.
That's exactly why you can formulate conversions on graphs using this analogy. So you will use some analogy of the shift operator or the plus and operator or basically any local diffusion operator as the analogy of the free basis and you can do filters in that space.
The different perspective, the spatial perspective, it's basically this perspective of weight sharing. So basically what you have is you have a node and you have its neighbors, right? On the grid, you can number these neighbors in a canonical order.
I can say in an image, I have a neighbor, my top neighbor, my bottom neighbor, my neighbor to the left and neighbor to the right. On the graph, you usually, unless you provide some extra information, you don't have canonical ordering of your neighbors.
So the way of aggregating information from the neighbors must be permutation environment.
That's why the aggregators that were used in graph neural networks are permutation environment functions, whether it's average, whether it's some, whether it's maximum, they're all permutation environment functions.
Now, the way that you combine the information from your neighbors can also be something that is learned and that's exactly the message passing. So each node in your graph receives a message from your neighbor that depends on the feature vector in that in that neighbor and the feature vector at the node.
And they're aggregated by this protein function is again usually sound or maximum. And the same, the same mechanism is used at every node.
So you see now how it's similar to convolution that it uses exactly the same parameters at each position in the graph.
And what is different from images is that the number of neighbors can be very different, but the way you aggregate them is actually completely agnostic to the structure of the graph.
That's why if you think of the of the structure of these structures, basically we add extra information that tells you what this, what this position in the graph looks like.
And you recently published a paper on differential graph modules for these graph convolutional networks. How does that fit into into this?
Yeah, so this is interesting. So I would say this is a little bit different direction.
And in most works on graph neural networks, you assume that the graph is given like the Twitter or the Facebook Friday. So you already have the graph. You have some information on the nodes. Let's now use this information, combine the information from the nodes to do something with this graph.
Let's say to classify the nodes or classify the entire graph.
In many applications, you don't know the graph. The graph can be actually used to model the structure of your data. And in some cases, the graph itself can be more valuable than the downstream task.
So imagine that you have, for example, some kind of mental point cloud where each point represents a patient.
Let's say some features from health care data records.
And what I want to do is to use the graph to represent some similarities between these patients. So when, for example, when I see your health care record and my health care record, probably the doctors will look at them differently depending on on different.
Method data for example, if I'm a male or if I'm female, if I'm older or if I'm young, what is my maybe some genetic creations or maybe what is my disease history and so. So the way that they will treat every node will be different.
And that's why the graph can be used to model the data structure, another problem, more typical application from machine learning and what's a computer vision is a few short learning.
And you have just a few labeled examples, but you have your data space. So if you can construct some representation of geometric representation of your data space that in the simplest form can be represented as a graph.
I can tell that for example that these two nearby nodes are similar somehow. So basically I can, I can learn the structure of these of these data space.
So these methods, maybe for learning or non linear dimension, if you reduction, they have been around for at least 20 years, maybe even more.
So basically methods that try to blow the structure of some kind of mental space and then represented in the lower dimensional space.
So algorithms like iso-met for example.
So one of the classical examples is you look at images, let's say images of digits from the amnest dataset.
These images can be extremely high dimensional, right? So it's thousand dimensional for example for these digits.
But if you look at the points that these images form in these thousand dimensional space, the intrinsic dimensionality of these data sets is much slower.
So many fold many fold learning algorithms try to capture these intrinsic dimension types of the data set.
Oh, imagine the same thing combined with graph learning. So we want to build a graph that captures the structure of the of these data set and then learn something similar to convolution or diffusion operation on these on these graph.
And basically what in this paper what we show is a way of.
Basically an efficient construction where where the graph is constructed on the fly. So we build both the graph and the filters that are applied on this graph.
And we show that this way we can significantly outperform existing algorithms for different healthcare applications, automatic diagnosis for the moment we want to classify patients.
This has been done already before us actually by colleagues from from Imperial College, the group of Daniel record.
And basically they use some handcrafted construction of the graph. Here we we don't know a priori which features are useful to build the graph because it also depends on the downstream task.
So the graph is constructed in an optimal way for the downstream task.
And this work are you is it you mentioned some of the work that's the way that manifolds play into kind of compressing down the space.
Are you trying to or is it inspired by some of the ways that manifolds are applied in in neural networks or is it independent of that work.
Not really so many for this probably not an appropriate term at least from the pure differential geometric standpoint basically slow dimensional space on this low dimensional structure which represents the data in this high dimensional bedding space.
It's technically speaking. It's not a manifold it might have different dimensionality at different points.
But it's a convenient mathematical model to think of it think of it as a manifold plus maybe some noise rounded.
And for learning is is exactly this type of construction that the assumption of the reason the underlying low dimensional space that has no nuclear instruction.
Yeah, we're things have evolved quite a bit over the past two and a half years if we talk again in two and a half years what do you think you know looking into your crystal ball how do you think this will all evolve what are we you know maybe on the verge of in your opinion or you know what you most excited about.
Well, I think. So let me try to make I don't know probably making dictionaries is very in grateful and dangerous business.
But still let's say what would make me happy as somebody working till basically obviously wishing to be successful.
So first of all industrial applications basically craft neural networks being standard tool used by by companies like nowadays deep learning deep neural networks are used so craft deep craft neural networks being used as in industrial settings.
Second killer apps and if you're if you asked me about what I would bet on as a single application. I think there will be multiple applications social networks are obviously one.
But what would probably be the most remarkable revolutionary field is probably health care and biological sciences.
And this is probably something where we should be looking at groundbreaking results in the next five to 10 years if everything works as expected in the game might be sooner.
And never come and basically that tool levels or two ways of looking at it. So first of all modeling the molecules themselves as graphs is extremely powerful and extremely promising.
Basically, you can predict properties of molecules using graph neural networks. Now where where this is important.
If you think of the problem of drag discovery and drag development. Basically, this is extremely expensive and extremely long process.
It takes a couple of billion dollars and about 10 years to bring a new drag on the market. Now why it is so so expensive and so and so difficult.
If you look at the search space, it is humongously large. I think estimates right, but we will have about 10 to the 60 at least synthesizable medium size molecules. So you need to choose your candy day drag out of this humongous number.
It's more than the number of atoms in the universe.
So you obviously need somehow to restrict this space. So there is that number of molecules that you can actually test clinically in the experiment.
Probably a few hundreds or a few thousands. And then the top we have this 10 to the 60. So there is this computational funnel that you need somehow to narrow down to detect the promising candidates and this can be done by virtual screening.
And this virtual screening can be done by machine learning techniques. So graph neural networks are the one of these components you can use graph neural networks to predict properties of these molecules.
So one of the key papers in this domain was from deep mind by Justin Gilmer, the actually the paper that gave rise to the message passing neural networks to this term.
And the predicted properties of molecules as well as DFT methods discrete functional theory, which is let's say a cheaper version of doing simulations for molecules, but about four to five orders for magnitude faster.
And that was in 2017, I think. Now it is, you can do way better. So this is one side. Another side is basically thinking of biological systems.
So if you think of our body, basically we have a lot of biomolecules and keep biomolecules protein.
We have about 20,000 different proteins that are encoded in our genes, our genes basically, they before nucleotides that form about two billion letters in our genetic code, they encode amino acids that then form chains that then fold into proteins.
And these proteins, basically, they are, it's not a metaphor to say that they are molecules of life. Basically, they're everywhere from metabolic processes, from carrying, carrying oxygen, from, from signaling different hormones in our body, from defense against pathogens, antibodies, the structure of our skin collagen.
And it's all proteins. We are currently not aware of any life form that is not based on proteins. Now, when we injected drug against some disease, it is usually designed to bind to a protein or multiple proteins to disrupt certain biological pathway.
So basically, that's when you're sick, one of these interactions between the proteins goes, goes wrong, and then then the drug is aimed to fix it either enabling or disabling some of these interactions.
So predicting how a molecule will interact with proteins is of crucial importance.
Now, we can do it to develop new drugs, and this is one of the exciting collaborations I have with the lab of Bruno Correa from, from a PFL in Mozambique.
They are biologists, they design proteins, for example, proteins that bind to cancer targets in, in cancer, if you know, one of the, one of the currently promising therapies is what is called the immunotherapy.
Given the Nobel Prize in medicine two years ago, basically, it is a way of reenabling the immune system to kill malignant cells that are, that are normally killed by our immune system.
Some cancers develop proteins that signal to the immune system cells to the T cells that these are healthy cells, and that's where the tumor multiplies and grows and the person becomes sick and eventually dies.
So immunotherapy, basically, the key problem is designing a binder that will, you know, will bind to one of these proteins and will basically will disable these techniques.
And one of these potential binders can be a protein on this one.
So what we are trying to do with trying to build proteins that will bind to these proteins, and we use geometric defining for these purposes.
Now, cancer is just one example, think of the current plate that is leveraging for the human kind, the novel coronavirus, right?
So you can also design potential therapies or maybe a way to block these virus by binding to the viral proteins or spike proteins on the coronavirus or some of the protein candidates.
Now, this is about designing new drugs. One of the cheaper alternatives to designing new drugs is what's called reposition your drug repurposing.
You can take an existing drug and try to find new users for it.
So the drug is already approved by FDA, so you know that it's not toxic, that it's safe, you just find it in your application for it.
And the particular interesting way is to combine multiple drugs.
So in some cases, the effect is not linear. If you take two or three drugs at the same time, they might suddenly produce a significant stronger effect.
So there has been a work also from the from the group of less cards by Marinka Zitnik, who is now faculty member, the broad institute.
So they use graph neural networks to predict side effects of this polyformacy of basically using multiple drugs. Now they're trying to use similar, similar models to predict synergies between drugs.
And myself, I'm involved in a project that I'm doing with colleagues from the Faculty of Medicine at Imperial College, Kilo Vasilkov and collaborators. We are trying to find a similar genetic combination of drugs against COVID-19.
So that's also something that we are using craft neural networks for.
This general space of health care and disease, you know, modeling the kind of underlying biological systems as graphs under modeling the drugs as graphs and potentially modeling the diseases as graphs and, you know, each of these has their own potential, you know, upside as you've illustrated a few of those.
That's correct. I think that these are some of the more exciting future applications were graphed your little skin and shine.
Awesome, awesome. Well, Michael, thanks so much for taking the time to catch us up on your work.
This has been quite a deep dive into graphical neural networks and it's been wonderful to catch up.
Thank you very much. Thank you, Sim.
Thank you.
