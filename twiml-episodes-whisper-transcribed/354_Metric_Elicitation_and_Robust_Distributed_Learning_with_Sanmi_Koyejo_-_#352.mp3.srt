1
00:00:00,000 --> 00:00:15,440
Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.

2
00:00:15,440 --> 00:00:27,120
Alright everyone, I am on the line with Sammy Koyejo. Sammy is an assistant professor at

3
00:00:27,120 --> 00:00:33,280
the Department of Computer Science in the University of Illinois, just a couple of hours away from

4
00:00:33,280 --> 00:00:38,240
where we are here in St. Louis, Sammy. Welcome to the Tumel AI Podcast. Thank you. It's

5
00:00:38,240 --> 00:00:42,160
a pleasure to be here. It is great to have you on the show and I'm looking forward to learning

6
00:00:42,160 --> 00:00:48,640
a bit more about your research. Let's start up. Absolutely. Let's start up by having you share a bit

7
00:00:48,640 --> 00:00:57,360
about how you came to work in ML and AI. Sure. So a reasonable place to start would be

8
00:00:59,040 --> 00:01:06,000
Brown grad school where I was interested in working on physical air communication systems.

9
00:01:07,200 --> 00:01:11,520
As time went on, I got more and more interested in intelligence,

10
00:01:11,520 --> 00:01:18,480
bit onto those systems and I started working on an area known as cognitive radios. Over time,

11
00:01:18,480 --> 00:01:23,680
I spent more and more time thinking about the intelligence piece and maybe less on the communication

12
00:01:23,680 --> 00:01:29,520
piece and eventually I started working on machine learning as my main area of research. So by the

13
00:01:29,520 --> 00:01:35,920
time I finished my PhD, I switched advisors and worked in a different area and specifically,

14
00:01:35,920 --> 00:01:44,080
I was looking at probabilistic models, Bayesian inference and related topics. My thesis focused on

15
00:01:44,080 --> 00:01:49,680
what is known as constrained or regularized Bayesian inference. It's a style of probabilistic

16
00:01:49,680 --> 00:01:55,120
inference where you add part of your prior structure is built into the inference algorithm and not

17
00:01:55,120 --> 00:02:03,200
just as part of the prior specification. So it's an interesting thread. From there, close to

18
00:02:03,200 --> 00:02:10,960
my PhD, I started to work with cognitive neuroscientists and so I got interested in cognitive

19
00:02:10,960 --> 00:02:19,280
neuroscience and I ended up spending a couple of years at Stanford building machine learning

20
00:02:19,280 --> 00:02:25,760
methods and machine learning models for cognitive science applications, neuroimaging applications

21
00:02:25,760 --> 00:02:33,120
and related ideas. About four years ago I started here at Illinois and I think of my research

22
00:02:33,120 --> 00:02:41,760
broadly as adaptive and robust machine learning. My research is quite broad, so I work in a bunch

23
00:02:41,760 --> 00:02:48,080
of different areas. I still work in cognitive neuroscience and neuroimaging a bit and building

24
00:02:48,080 --> 00:02:54,000
machine learning tools for those applications. But in addition, in my core machine learning work,

25
00:02:54,000 --> 00:02:59,920
I think about scalable machine learning, fault tolerance and machine learning and a variety of

26
00:02:59,920 --> 00:03:08,240
topics related to how to construct machine learning systems that make good predictions for

27
00:03:09,280 --> 00:03:14,800
various kinds of evaluation metrics and how that interacts with human decision making.

28
00:03:14,800 --> 00:03:22,080
So sort of a variety of research threads but hopefully coming together in a coherent set of ideas.

29
00:03:22,080 --> 00:03:27,440
Nice. Now I'm so kind of intrigued by this idea of a cognitive radio. How far did you get into

30
00:03:27,440 --> 00:03:35,040
that topic? Is that an actual thing? Yeah, yeah, it's an actual thing. I haven't followed

31
00:03:35,040 --> 00:03:41,200
the development over the past few years but it came about at some point where essentially

32
00:03:42,160 --> 00:03:50,560
ran out of spectrum in the US and in many developed countries. And so in order to get more

33
00:03:50,560 --> 00:03:58,960
bandwidth, those idea to make use of spectrum that ends up being sort of dead spots. So in various

34
00:03:58,960 --> 00:04:05,200
cities, different channels say broadcast on certain radio frequencies but leave some others.

35
00:04:05,200 --> 00:04:12,560
And so one way to construct a radio that works in these systems is to actually sort of hop between

36
00:04:12,560 --> 00:04:17,200
different frequencies by detecting when a frequency is in use and when is not in use and

37
00:04:17,200 --> 00:04:22,400
you sort of hop in between and sort of communicate for a short time. And this way you can make

38
00:04:22,400 --> 00:04:29,200
better use of the spectrum and actually make use of spectrum that otherwise would you could not use

39
00:04:29,200 --> 00:04:35,440
for any other purpose. So it's an interesting idea. I believe that there are systems built out.

40
00:04:35,440 --> 00:04:42,000
I don't know the extent to which there are popular implementations of these things because

41
00:04:42,000 --> 00:04:48,960
again, I haven't followed this idea. But it was there in response to I think quite difficult

42
00:04:48,960 --> 00:04:56,880
problem in the communication space of running out of wireless spectrum. Okay, interesting. Interesting.

43
00:04:56,880 --> 00:05:04,800
So you mentioned a couple of key areas in your research. One focused on metrics and metric

44
00:05:04,800 --> 00:05:12,080
elicitation, the other focus on robust distributed learning. And let's start with the first of those.

45
00:05:12,080 --> 00:05:17,360
When you're thinking about metric elicitation, tell us a little bit more about the problem that you

46
00:05:17,360 --> 00:05:24,080
are trying to solve there. Yeah, yeah, happy to. So it's a bit of a history. So it's probably worth

47
00:05:24,080 --> 00:05:30,880
taking us half a step back and giving you some context. Sure thing. So the way I like to think about

48
00:05:30,880 --> 00:05:37,280
the problem is roughly when you build a machine learning system and you're sitting using the

49
00:05:37,280 --> 00:05:42,800
machine learning system for decision making, many real-world decision making tasks are actually

50
00:05:42,800 --> 00:05:48,640
quite complex and they involve trade-offs between different factors. And the issue is that many of

51
00:05:48,640 --> 00:05:54,320
our default machine learning metrics don't account for these trade-offs in decision making.

52
00:05:54,320 --> 00:06:02,080
And so at some point around, it's not been a few years, but I think around 2014 or so,

53
00:06:03,280 --> 00:06:09,120
I got interested with, at the time, my postdoc advisor, Pradeep Ravi Kumar,

54
00:06:10,000 --> 00:06:17,120
in thinking about how to construct machine learning models that could optimize complex metrics.

55
00:06:17,120 --> 00:06:21,760
So think of, in fact, there are lots of great sort of common examples. So

56
00:06:21,760 --> 00:06:28,560
and information retrieval, popularly, people use what is known as the F measure. So this is some

57
00:06:28,560 --> 00:06:33,680
ratio of precision and recall. It's commonly used for prediction in these settings because

58
00:06:33,680 --> 00:06:39,200
this thought to be a good measure of performance. But up until some of our early work,

59
00:06:39,920 --> 00:06:44,080
there wasn't a good understanding of how one built a machine learning system that was specifically

60
00:06:44,080 --> 00:06:50,160
good at optimizing F measure. So before that, you build your system to be good at optimizing accuracy,

61
00:06:50,160 --> 00:06:55,040
and which again, we're fairly good at doing or maybe some weighted accuracy. But there wasn't

62
00:06:55,040 --> 00:07:00,720
a good sense of what to do if I changed the measure into something more complex like F measure.

63
00:07:01,440 --> 00:07:06,560
So we started this series of papers over a few years where we got better and better understanding

64
00:07:07,040 --> 00:07:12,640
how to construct good learning models for, again, what we call complex metrics. What is sometimes

65
00:07:12,640 --> 00:07:18,560
called non-decomposable metrics. And they're called non-decomposable metrics because I can't write

66
00:07:18,560 --> 00:07:25,680
down the metric as an average. And once you break this averaging possibility, lots of standard tools

67
00:07:25,680 --> 00:07:32,560
that we like to use, like say, graded descent in a sort of straightforward way, or tools that

68
00:07:33,360 --> 00:07:39,760
we like to use in terms of analysis start to break down. And so it becomes somewhat of a more

69
00:07:39,760 --> 00:07:46,160
challenging problem to solve. And so like I said, we had a series of papers where we tried to

70
00:07:46,160 --> 00:07:50,720
understand these metrics a lot more and come up with good methods for optimizing them. And I'd give

71
00:07:50,720 --> 00:07:56,560
these talks and really excited about early work on this showing, hey, give me your really complex

72
00:07:56,560 --> 00:08:02,640
evaluation measure. I can tell you how to optimize it. And I should say that we had a characterization

73
00:08:02,640 --> 00:08:07,040
that was quite general. And so it could adapt to different notions of what good measures are.

74
00:08:07,600 --> 00:08:13,520
How complex are you able to get, often when I think about metrics in the way you're describing them,

75
00:08:13,520 --> 00:08:20,560
you've got your metrics that the data scientist or machine learning engineer is trying to build

76
00:08:20,560 --> 00:08:26,720
a system to, and then maybe there are metrics that aren't the one they're using to train their

77
00:08:26,720 --> 00:08:33,920
models, but they're still kind of in their domain like your F-score. But then on the other end

78
00:08:33,920 --> 00:08:38,320
of the spectrum, there are the actual metrics that business people care about which don't look like

79
00:08:38,320 --> 00:08:45,120
either of those, or often don't look like either of those. Do your research get to that other end

80
00:08:45,120 --> 00:08:52,960
of the spectrum? Yes and no. So I can give you some sense of the scope of where we can say some

81
00:08:52,960 --> 00:08:59,840
useful things. Okay. So the first few papers, I should take a step back and mention that for

82
00:08:59,840 --> 00:09:05,920
classification problems, the primary statistic that one uses to measure whether you have a good

83
00:09:05,920 --> 00:09:11,440
model or not is something related to what is known as the confusion matrix. So the confusion matrix

84
00:09:11,440 --> 00:09:17,600
is essentially measuring for every kind of prediction and for every ground truth label. How often

85
00:09:17,600 --> 00:09:22,240
in a setting, but a certain label do you make a certain prediction? So I look at the average times

86
00:09:22,240 --> 00:09:27,440
I say, predict one when the ground truth is one, and this gives me some number. It's a confusion

87
00:09:27,440 --> 00:09:32,400
matrix for one one, or I can do this for one two. How often do I predict two when the ground truth

88
00:09:32,400 --> 00:09:36,720
is one, or how often do I predict three when the ground truth is one, and all combinations of this

89
00:09:36,720 --> 00:09:41,760
for say a multi-class classification problem. So with K classes, end up with a K squared confusion

90
00:09:41,760 --> 00:09:50,640
matrix for every pair of ground truth and prediction. So initially we worked on linear combinations

91
00:09:50,640 --> 00:09:57,040
of the entries as a confusion matrix. So you could imagine, for instance, that predicting three

92
00:09:57,040 --> 00:10:01,280
when the ground truth is one is more expensive than predicting two when the ground truth is one.

93
00:10:01,280 --> 00:10:05,840
And ideally you want to predict one when the ground truth is one. We're kind of familiar with

94
00:10:05,840 --> 00:10:10,640
this kind of scenario when we, you know, in its simplest form, like false positive and false

95
00:10:10,640 --> 00:10:17,040
negatives and some applications. Exactly those. Yes. I went to the more general multi-class case,

96
00:10:17,040 --> 00:10:20,480
but I think the binary case is enough for illustrative purposes. So in the binary case, it's true

97
00:10:20,480 --> 00:10:26,000
positives, true negatives, false positives, false negatives. Thanks for pushing me that direction.

98
00:10:26,000 --> 00:10:31,760
So initially we worked through, say, linear weighted combinations of this. Eventually we got

99
00:10:31,760 --> 00:10:38,960
two ratios of linear things, which captures things like f measure. They can write metrics like

100
00:10:38,960 --> 00:10:44,960
that as ratios of linear things. Now we're at the point where we can pretty much do sort of any

101
00:10:44,960 --> 00:10:50,800
function of the confusion matrix and we can come up with what is known as a consistent estimator.

102
00:10:50,800 --> 00:10:57,120
So some estimator that we know will have good large sample properties. So some learning algorithm

103
00:10:57,120 --> 00:11:04,400
where I can come up with a learner or optimization process that I know will have good behavior

104
00:11:04,400 --> 00:11:09,680
in terms of optimizing for some arbitrary function of confusion matrix. So how far does this go to

105
00:11:09,680 --> 00:11:15,760
reward metrics? We think it goes reasonably far, but it's clearly once you get to real world

106
00:11:15,760 --> 00:11:20,400
settings, many of the things you care about don't always, I'm not always captured by the confusion

107
00:11:20,400 --> 00:11:25,760
matrix and or cannot be reduced in that sort of simple way. Though many are, so many things

108
00:11:25,760 --> 00:11:31,200
are just weighted forms of some weight attacks different kinds of mistakes. So as long as you

109
00:11:31,200 --> 00:11:36,480
sort of roughly in a setting where the thing you care about is the ticks of mistakes and waiting

110
00:11:36,480 --> 00:11:42,480
on those or even in survivatory functions of those, they're pretty good algorithms now that

111
00:11:42,480 --> 00:11:49,200
based on work that I and others have worked on to build to build good algorithms for optimizing

112
00:11:49,200 --> 00:11:54,880
these. So like I said, or as I was going to say, I would give these talks being excited by

113
00:11:55,680 --> 00:12:01,760
this line of algorithms and saying, hey, we can do in any metric you like and you get the feedback,

114
00:12:01,760 --> 00:12:05,600
well, now you've made it clear that there are lots of different ways to measure what performance is.

115
00:12:05,600 --> 00:12:11,120
Well, which of these should I use? So it's quite unclear. Now, once it's clear that there are many ways

116
00:12:11,120 --> 00:12:18,560
of measuring performance, it becomes trickier to think of or to pick which one is best suited to

117
00:12:18,560 --> 00:12:26,320
a certain setting. And so the idea of metricalistation, which is where we ended up with, is trying to

118
00:12:26,320 --> 00:12:33,280
turn the problem on its head. I'm trying to ask, can I elicit good metrics by interaction with

119
00:12:33,280 --> 00:12:38,720
experts or with users or with panels of experts? So are there strategies that can come up with,

120
00:12:38,720 --> 00:12:46,160
that can interact with an expert to figure out what measure is closest is a close approximation

121
00:12:46,160 --> 00:12:52,400
to how they're determining trade-offs or value of different kinds of predictions. And so the idea

122
00:12:52,400 --> 00:12:58,960
is that if we can do this well, then you can optimize for that metric. And more importantly,

123
00:12:58,960 --> 00:13:04,640
this metric is transportable. So I could change the class of models I'm optimizing. I could change the

124
00:13:04,640 --> 00:13:10,640
data distribution. I could change the setting in some ways. But as long as it's capturing the expert

125
00:13:10,640 --> 00:13:16,400
trade-offs, then this is a good way of measuring good performance. And so this is something that I can

126
00:13:16,400 --> 00:13:25,440
use as I change the settings. So the example I like to give is a simplest example I think that

127
00:13:25,440 --> 00:13:34,240
maybe illustrates the idea of elistation is in a healthcare setting where say a doctor would

128
00:13:34,240 --> 00:13:42,240
try to is interested in constructing an automated health decision-making system. And so the doctor

129
00:13:42,240 --> 00:13:48,960
is an expert. They have some notion of how expensive say for a certain set of measurements,

130
00:13:48,960 --> 00:13:58,080
how expensive it is to say misdiagnosis, so misdiagnosis when there's actually sort of some disease

131
00:13:58,080 --> 00:14:05,040
there or overdiagnose somehow. So like predict the say that someone has some disorder when they

132
00:14:05,040 --> 00:14:12,080
actually don't. And so this the actual trade-off depends on the costs of treatment and maybe

133
00:14:12,080 --> 00:14:17,840
sort of potential side effects, sort of all these other considerations. But if you think about

134
00:14:17,840 --> 00:14:22,640
this as a decision-making problem, you can imagine that you could sort of compress everything down to

135
00:14:22,640 --> 00:14:26,880
there's some cost to making a prediction in one direction, some other cost making a prediction

136
00:14:26,880 --> 00:14:35,120
in the other end. Often going from this intuition idea to a concrete trade-off function is hard.

137
00:14:35,120 --> 00:14:42,400
If it was easy then one could sort of then construct models to directly try to optimize the doctor's

138
00:14:42,400 --> 00:14:47,600
trade-off function. But it's hard to do in real settings. And so what the idea of metricalistation

139
00:14:47,600 --> 00:14:54,560
is is to come up with a strategy to interact with say the doctor is an expert. And based on this

140
00:14:54,560 --> 00:15:01,120
interaction, actually pinpoint the right trade-offs that should correspond with their preferences.

141
00:15:01,120 --> 00:15:05,280
And then you can optimize those preferences directly and if you construct things at a downstream

142
00:15:05,920 --> 00:15:15,040
models. Other examples we have considered say things like ranking models. We haven't worked on

143
00:15:15,040 --> 00:15:19,360
this. We don't have results for this yet, but in the pipeline are things like say ranking models

144
00:15:19,360 --> 00:15:25,600
if you're building recommender system. Imagine that your users have different preferences

145
00:15:26,160 --> 00:15:31,600
and the order in which they want to see things. So you could imagine then constructing a

146
00:15:31,600 --> 00:15:37,680
listation procedure that did a series of say AB tests with your users and tried to pinpoint

147
00:15:37,680 --> 00:15:44,000
for the user population what the best approximation to the best sort of ranking cost function should

148
00:15:44,000 --> 00:15:51,280
be for their setting. Is the process always akin to AB testing? You mentioned that in the

149
00:15:51,280 --> 00:15:56,560
scenario of users. It is the same thing applied to doctors. You know, what do you prefer choice,

150
00:15:56,560 --> 00:16:02,400
say choice B? So there are many ways you could imagine developing a problem like this.

151
00:16:03,200 --> 00:16:10,240
We chose to go with the pairwise preference approach. The motivation after talking to experts

152
00:16:10,240 --> 00:16:16,720
in the area was that with pairwise preferences, it's sort of much more likely that we can get

153
00:16:18,240 --> 00:16:25,600
users to easily give us answers to the comparisons than if we tried other ways of

154
00:16:25,600 --> 00:16:31,520
interacting with experts or with users. Because somehow pairwise preferences are easier

155
00:16:32,720 --> 00:16:38,880
for experts to give feedback on than other kinds of ways of querying.

156
00:16:38,880 --> 00:16:45,520
Okay. So I should mention this is work led by my student, Gauru Shirendiani,

157
00:16:46,640 --> 00:16:53,040
here at Illinois and in collaboration with colleagues, Rutameta and sort of other folks at Illinois.

158
00:16:53,040 --> 00:17:00,080
So this is work that again a few years, at least the listation piece has been a couple years

159
00:17:01,040 --> 00:17:06,960
old. And again, it's trying to answer this question of what metric should I optimize in order to

160
00:17:06,960 --> 00:17:11,200
get my machine learning system to do sort of the thing in the real world that I wanted to do.

161
00:17:12,080 --> 00:17:16,160
So again, the thing in the real world is rarely optimize accuracy. It's typically something

162
00:17:16,160 --> 00:17:22,560
much more subtle, something much more complex. And we're starting to get initial answers for

163
00:17:22,560 --> 00:17:27,360
sort of reasonable algorithms for trying to answer this question. I also should mention,

164
00:17:27,360 --> 00:17:33,360
so the pain point in trying to construct this listation procedures is how many queries

165
00:17:33,360 --> 00:17:39,840
are going to ask the expert because in principle, in theory, if you had had infinite queries,

166
00:17:39,840 --> 00:17:46,480
you could elicit anything. But yeah. So the idea is in as few queries as possible get to

167
00:17:47,280 --> 00:17:57,040
formulation of your metric that is as accurate as possible or matches as closely to what the expert

168
00:17:57,040 --> 00:18:02,560
would do you define it as kind of produces a classifier that most closely matches what the expert

169
00:18:02,560 --> 00:18:07,360
would predict. Like, how do you tie it back into the metric of the classifier?

170
00:18:08,400 --> 00:18:12,080
I think you're talking about evaluation, which is tricky in these problems.

171
00:18:14,080 --> 00:18:19,280
But the target, the conceptual target is to like get the best approximation to the

172
00:18:19,280 --> 00:18:26,000
trade-off function that the expert is using. The practical evaluation is if I sort of change the

173
00:18:26,000 --> 00:18:32,240
classification setting or change distribution in some way, I should be able to get the same sort

174
00:18:32,240 --> 00:18:38,240
of outcomes as ideal outcomes from the model as what the expert would pick as ideal outcomes

175
00:18:38,240 --> 00:18:44,560
from the model. So it should replicate the expert's predictions, but again, this idea of

176
00:18:44,560 --> 00:18:49,760
transportability. So we should be able to do this in a variety of settings. I should mention it's

177
00:18:49,760 --> 00:18:55,200
quite close. Metricalistation is not that far from ideas like inverse reinforcement learning.

178
00:18:56,000 --> 00:19:00,800
So again, there's a whole literature primarily in the reinforcement learning world,

179
00:19:00,800 --> 00:19:04,800
whereas a lot of focus on learning good reward functions, and sometimes by learning this

180
00:19:04,800 --> 00:19:10,880
good reward functions by interaction with humans. In some sense, we're solving a easier version of

181
00:19:10,880 --> 00:19:17,040
the problem than what the RL folks are trying to solve. We take advantage of a lot of additional

182
00:19:17,040 --> 00:19:22,080
structure that comes from the classification version of the problem, which is most of what we've

183
00:19:22,080 --> 00:19:27,760
focused on so far. So we can get sort of much stronger results, much better algorithms than often,

184
00:19:27,760 --> 00:19:31,200
what could get and does it much more general reinforcement learning setting, because you don't

185
00:19:31,200 --> 00:19:38,080
have to think about sequentiality of sort of in the same way that an RL setting would have to

186
00:19:38,080 --> 00:19:45,280
reason about. One issue also I should mention is an investor reinforcement learning in particular,

187
00:19:45,280 --> 00:19:50,800
there's not always a focus on actually getting the reward function right. So to your point earlier,

188
00:19:50,800 --> 00:19:55,680
often the focus is on replicating behavior, not necessarily getting your reward function right.

189
00:19:55,680 --> 00:20:01,120
So in this say driving setting, I want to be able to drive the same way that the human drove

190
00:20:02,800 --> 00:20:07,680
where the human is the expert. So get me the reward function that does this the best in the setting.

191
00:20:07,680 --> 00:20:14,640
Not necessarily in this in these settings, sometimes there is not a focus on say what happens if I

192
00:20:14,640 --> 00:20:24,320
change the environment a little bit. And so now the reward function has sort of so tuned to the

193
00:20:24,320 --> 00:20:29,040
original setting that it doesn't work as well in the new setting. So one difference is that we're

194
00:20:29,040 --> 00:20:35,440
very focused on again, this idea of transportability. So we're the focus is still on learning or

195
00:20:35,440 --> 00:20:41,200
eliciting metrics such that they are agnostic to things like data distribution and the specific

196
00:20:41,200 --> 00:20:47,680
learner they're using and sort of other kinds of important, but things that want to abstract

197
00:20:47,680 --> 00:20:52,000
the way because we want it to have these trade-off functions that you can then apply in sort of

198
00:20:52,000 --> 00:20:57,600
general settings. So learning a simple setting potentially apply in a more complex setting is

199
00:20:57,600 --> 00:21:04,000
maybe one way to think about it. And so how do you get to that level of generalizability? Is it

200
00:21:04,000 --> 00:21:09,680
in the you know your selection of data that you're training on or does it have to do with the

201
00:21:10,320 --> 00:21:15,680
questions or sequence of questions that you're asking the pairs that you present or they're like

202
00:21:15,680 --> 00:21:23,840
you know black art techniques like prop out or things like that. Now it's actually in fact for

203
00:21:24,880 --> 00:21:34,240
the binary classification setting with linear trade-offs in the confusion matrix. It essentially

204
00:21:34,240 --> 00:21:39,680
balls down to a very simple binary search. So actually many settings that it balls down to

205
00:21:39,680 --> 00:21:45,600
almost trivial sort of textbook algorithms. And all the work is in characterizing sort of how

206
00:21:45,600 --> 00:21:52,640
do I want to define the feasible metric space and how does one reason about how to search efficiently

207
00:21:52,640 --> 00:22:00,080
into space. And so often once you do that work the final step of the algorithm to elicit in many

208
00:22:00,080 --> 00:22:05,680
settings is actually much more straightforward than you might imagine. So the thing that enables

209
00:22:05,680 --> 00:22:17,360
transportability is so far we've mostly focused on settings where the metric of interest

210
00:22:17,360 --> 00:22:24,320
is some function of the confusion matrix elements. And what's interesting to note is that sort of

211
00:22:24,320 --> 00:22:31,760
trade-offs in confusion matrix elements don't depend on sort of how good you are in classifying.

212
00:22:31,760 --> 00:22:36,320
They just trade-offs between different kinds of errors. So those kinds of functions are

213
00:22:36,320 --> 00:22:41,840
agnostic really to sort of if you're able to estimate them well enough they're agnostic to

214
00:22:41,840 --> 00:22:48,160
things like data distribution function class things like that. So for instance specifically again

215
00:22:48,160 --> 00:22:54,960
in the doctor example if you're interested in the cost of misdiagnosis versus sort of overdiagnosis

216
00:22:54,960 --> 00:22:59,200
of missing versus not missing a diagnosis. If you can think about this as a binary classification

217
00:22:59,200 --> 00:23:05,520
problem but just weights between false positives and false negatives. The what matters is getting

218
00:23:05,520 --> 00:23:11,920
those weights right and the actual value of the false positive and false negative doesn't matter

219
00:23:11,920 --> 00:23:16,000
as much so you can have a learner that's much better at getting low false positive false negatives

220
00:23:16,000 --> 00:23:20,640
and not to learn that's much worse at getting low false positive false negatives. So this

221
00:23:20,640 --> 00:23:25,760
differences would be say differences in using a linear model versus say maybe using a deep learning

222
00:23:25,760 --> 00:23:30,960
model in these two settings. So they would have different confusion matrix trade-offs but as

223
00:23:30,960 --> 00:23:36,880
long as you get the trade-offs right the actual values are not that important. So again we've

224
00:23:36,880 --> 00:23:43,840
focused mostly on settings where that property is mostly true. In classification this is most

225
00:23:43,840 --> 00:23:48,880
often the case if you're focusing on functions of the confusion matrix so it comes up sort of

226
00:23:48,880 --> 00:23:53,680
naturally based on the problem definition that we're interested in. And are there any

227
00:23:53,680 --> 00:24:01,360
properties that arise that relate the you know for example the number of pairs that you have

228
00:24:01,360 --> 00:24:07,360
to present to the dimensionality of your confusion matrix or something like that? Absolutely yeah so

229
00:24:07,360 --> 00:24:15,760
it roughly grows about linearly with the size of the confusion matrix. So to get roughly the

230
00:24:17,120 --> 00:24:22,960
so the conceptual with theoretical claim is to get a certain error accuracy the number of queries

231
00:24:22,960 --> 00:24:29,440
that you need scales roughly linearly with the sort of size of the confusion matrix. At least for

232
00:24:29,440 --> 00:24:37,680
I should say that this is true for linear and ratio of linear things if you're doing more complicated

233
00:24:37,680 --> 00:24:43,840
function classes other terms that to show up. So for linear things it mostly scales linearly

234
00:24:43,840 --> 00:24:49,280
with the size of the confusion matrix which is again sort of number of classes squared.

235
00:24:49,280 --> 00:24:56,000
For ratio you sort of have an extra factor of two there but again order wise it's mostly linear.

236
00:24:56,800 --> 00:25:05,280
If you go to say polynomial functions or something more complicated then it scales roughly that

237
00:25:05,280 --> 00:25:11,520
earlier of size of confusion matrix plus or times some term that depends on order to polynomial.

238
00:25:11,520 --> 00:25:20,480
So roughly that order. So you pay some cost for more and more complex types of score functions.

239
00:25:21,360 --> 00:25:27,120
There's some other discussion which we've been trying to reason through about how how complex is

240
00:25:27,120 --> 00:25:32,320
your sort of score function space need to be to capture human preferences appropriately. I think

241
00:25:32,320 --> 00:25:39,760
that's an important question that we have an answer then and I think maybe not that many folks in

242
00:25:39,760 --> 00:25:47,360
the field have maybe thought about very carefully. In fact we've been working a bit on actually

243
00:25:47,360 --> 00:25:54,560
reducing the complexity from even linear because some of the say psychology literature suggests that

244
00:25:54,560 --> 00:26:01,520
we mostly focus on sort of a few features as opposed to say arbitrary trade-offs between things.

245
00:26:01,520 --> 00:26:08,880
And so potentially the space of metrics is even lower dimensional than say linear in some large

246
00:26:08,880 --> 00:26:16,400
confusion matrix space. Again there's some interplay between human computer interactions sort of

247
00:26:16,400 --> 00:26:23,280
psychology, a bit of algorithms, a bit of machine learning. So it's an interesting set of problems

248
00:26:23,280 --> 00:26:27,760
and an interesting space for us to work in because it's sort of quite unique within machine learning

249
00:26:27,760 --> 00:26:32,400
to have all of these problems come together. But we think it's an important set of problems

250
00:26:32,400 --> 00:26:39,120
because we think it addresses core problems particularly in practice when folks are trying

251
00:26:39,120 --> 00:26:44,960
design systems and they have either a specific rough notion of what good systems should look like

252
00:26:44,960 --> 00:26:50,240
but accuracy is not cutting it or they're interested in some downstream measure that might

253
00:26:50,240 --> 00:26:55,200
involve say interaction with users and again potentially accuracy isn't getting them the results

254
00:26:55,200 --> 00:27:01,520
that they want. One area that we started to look at that is quite exciting as an application area.

255
00:27:01,520 --> 00:27:06,400
Again it's early days but I thought I should mention this. It's thinking about elicitation

256
00:27:06,400 --> 00:27:13,840
in the fairness space. So in machine learning fairness it's very clear that different measures

257
00:27:13,840 --> 00:27:20,240
of fairness end up with different notions of trade-offs between how you treat different

258
00:27:20,240 --> 00:27:26,000
say subgroups. I'm thinking primarily about say statistical group fairness in this case but

259
00:27:26,000 --> 00:27:33,040
similar ideas hold for other notions of fairness as well. And so one could imagine and Sarah's

260
00:27:33,040 --> 00:27:38,560
sort of first steps on this and there are also a couple of papers on this idea of coming up with

261
00:27:39,360 --> 00:27:47,200
elicitation procedures that can build context-specific notions of what metrics or

262
00:27:47,200 --> 00:27:53,200
statistics you should be trying to normalize across groups in order to achieve a fairness goal

263
00:27:53,200 --> 00:27:58,480
in a certain setting. So that's an application area where thinking very carefully about exactly

264
00:27:58,480 --> 00:28:05,840
what you're measuring is interesting and potentially quite important to get the results that one

265
00:28:05,840 --> 00:28:13,680
would want. I think it's still not absolutely clear to me and either the medical or the fairness

266
00:28:13,680 --> 00:28:24,720
scenario what these pairs concretely look like. In the case of I'm even having trouble like

267
00:28:24,720 --> 00:28:29,840
formulating the question concretely in the medical case. But I can imagine that there's

268
00:28:29,840 --> 00:28:35,680
a degenerate case where you're asking the physician would you rather spend like you're taking

269
00:28:35,680 --> 00:28:43,120
pokes at the function? Would you rather do this a thousand times or this one time or something

270
00:28:43,120 --> 00:28:49,360
like that but I'm getting a sense that that's not exactly it. You can show that if your metric is

271
00:28:49,360 --> 00:28:53,840
a function of certain quantities there are only things that matter our differences in those

272
00:28:53,840 --> 00:29:01,280
quantities. So in terms of pair-right comparisons for the confusion matrix setting you might imagine

273
00:29:01,280 --> 00:29:06,080
comparing confusion matrices which is not something that's easy to do by the way. And so part of the

274
00:29:06,080 --> 00:29:13,600
work is coming up with ways to translate those comparisons and two comparisons that say a medical

275
00:29:13,600 --> 00:29:19,280
expert could do. So the variety of techniques that we've started to work with to try to solve this

276
00:29:19,280 --> 00:29:27,520
last mile task. So you can imagine for instance showing where two different classifiers that have

277
00:29:27,520 --> 00:29:33,440
different confusion matrices is sort of the outputs where they differ in terms of their predictions

278
00:29:33,440 --> 00:29:39,520
or a variety of ways of working on sort of interpreting train models. So once you have a way to

279
00:29:41,120 --> 00:29:47,440
so we have good ways by the way of translating confusion matrices back to classifiers this

280
00:29:47,440 --> 00:29:53,600
ties very closely to earlier work I mentioned an optimizing arbitrary metric. So we have very good

281
00:29:53,600 --> 00:29:59,520
understanding now of how confusion matrices relate to models. So we can sort of go back and forth

282
00:29:59,520 --> 00:30:06,800
very easily. So once you have this then you can convert comparisons of confusion matrices which

283
00:30:06,800 --> 00:30:13,360
is what matters in terms of the trade-off into comparisons of models. So sort of model that

284
00:30:13,360 --> 00:30:19,040
achieves confusion A versus model achieves confusion B. And what the expert needs to be able to do

285
00:30:19,040 --> 00:30:27,040
is tell us their preference between model A versus model B some sequence of times. And we choose

286
00:30:27,040 --> 00:30:33,520
the sequence of comparisons in such a way that after sort of after a few queries we can pinpoint

287
00:30:34,800 --> 00:30:40,640
the trade-offs that best capture how they're weighing different kinds of errors in the confusion

288
00:30:40,640 --> 00:30:49,040
matrix for instance. So is is this model comparison formulation A way of looking at this or is

289
00:30:49,040 --> 00:30:54,240
is kind of fundamental to what you've described around metric elicitation always based on this

290
00:30:54,240 --> 00:31:05,280
model comparison. The way that we have built up the approach the fundamental piece is to summarize

291
00:31:05,280 --> 00:31:10,080
is roughly being able to compare confusion matrices which will bolt down to comparing models.

292
00:31:11,040 --> 00:31:18,560
To take us that back though again it's sort of whatever you're using as the sort of the parameters

293
00:31:18,560 --> 00:31:25,680
of your cost function. So the quantities in your cost function only differences in those

294
00:31:25,680 --> 00:31:31,520
quantities will show up as differences in the measure. So for instance if in addition to

295
00:31:32,240 --> 00:31:37,440
confusion matrix entities you really care about smoothness of the function that becomes a third

296
00:31:37,440 --> 00:31:44,480
thing that you add sort of a new parameter in the set of things that you would be comparing.

297
00:31:44,480 --> 00:31:51,040
And so you'd get say two classifiers that differed in confusion matrix or confusion matrices

298
00:31:51,040 --> 00:31:57,200
that they achieved and also maybe had different smoothness. And you would then tell sort of you

299
00:31:57,200 --> 00:32:03,200
would be you would be asked to give a preference between the two. So it's comparing the fundamental

300
00:32:03,200 --> 00:32:09,040
thing is being able to compare whatever quantities determine the metric. So however you define

301
00:32:09,040 --> 00:32:13,600
a metric whatever quantities determine a metric you need some procedure that allows the expert to

302
00:32:13,600 --> 00:32:18,560
compare those two things. In the classification space which is a space that we've studied by

303
00:32:18,560 --> 00:32:25,360
for the most. The natural entities are confusion matrices. And so you need a way to compare

304
00:32:25,360 --> 00:32:30,880
confusion matrices which we do by sort of providing back to models. I should say we have

305
00:32:31,840 --> 00:32:40,000
started a new line of work thinking about how to maybe do this how to select samples intelligently

306
00:32:40,000 --> 00:32:47,040
so you can imagine instead of comparing models using say whether predictions differ the most

307
00:32:47,600 --> 00:32:55,120
you could imagine the algorithm also selects a specific sample it says if you pick this model

308
00:32:55,120 --> 00:32:59,440
make this kind of prediction if you get picked this other model make decide a kind of prediction

309
00:32:59,440 --> 00:33:07,280
and using that as a way to get feedback. It's still early days on that line so it's hard to say

310
00:33:07,280 --> 00:33:14,640
sort of very clearly what is doable and what works well. Right now I'd say the work that is

311
00:33:14,640 --> 00:33:21,200
most mature is focusing on comparing confusion matrices translating this into comparing models.

312
00:33:22,160 --> 00:33:26,640
And then using that as a way to pinpoint preferences for the expert decision maker.

313
00:33:27,600 --> 00:33:32,480
And then we're going to talk about a totally different experience. Yeah. Yeah.

314
00:33:32,480 --> 00:33:42,960
A bit of extra time on this but so another line of work which we've been making I think

315
00:33:42,960 --> 00:33:51,680
quite interesting progress on is a question of robust distributed learning. So this is work

316
00:33:52,480 --> 00:34:00,160
led by my student Song Sier and collaborator here to annoy Indy Gupta who is a professor in the

317
00:34:00,160 --> 00:34:10,880
system side at Illinois. So the setup is that we're interested I'm laughing because sort of like

318
00:34:10,880 --> 00:34:20,640
you said how different it is but so the setup of the problem is that for various reasons

319
00:34:20,640 --> 00:34:25,760
particularly so the scalability in privacy there's a lot more interest in training machine learning

320
00:34:25,760 --> 00:34:31,520
models in a distributed way. So scalability is being able to use sort of lots of machines at the

321
00:34:31,520 --> 00:34:36,720
same time and potentially just getting more throughput running through much more data per second.

322
00:34:37,600 --> 00:34:43,360
So you can imagine this in data centers where sort of each machine has some amount of computing

323
00:34:43,360 --> 00:34:49,280
power. The idea is if I run lots of these machines at the same time on a stack of data I can

324
00:34:49,280 --> 00:34:56,400
and I do things appropriately I can I can sort of train my model much faster. You could also imagine

325
00:34:57,680 --> 00:35:04,880
in fact one of the I think interesting use cases of this is in sometimes called sort of

326
00:35:04,880 --> 00:35:13,360
internet of things or edge networks where say you're interested in training machine learning

327
00:35:13,360 --> 00:35:20,560
models partially on your edge device. So good example is something like a cell phone. You want

328
00:35:20,560 --> 00:35:25,600
to do somewhat processing on your cell phone and the idea is that if I do this appropriately I

329
00:35:25,600 --> 00:35:30,560
can avoid sharing data directly with the centralized server so I don't have to transfer data.

330
00:35:31,280 --> 00:35:36,720
This might win in terms of communication and if I do some extra work and I also get a win in

331
00:35:36,720 --> 00:35:44,320
terms of privacy so I can actually protect the user's data from some easy snooping but still get

332
00:35:44,320 --> 00:35:49,760
the benefits of training a big machine learning model across lots of devices. So that's the set

333
00:35:49,760 --> 00:35:55,680
of the general setup of again distributed machine learning in general. So one unique problem that

334
00:35:55,680 --> 00:36:02,960
shows up in distributed machine learning is that once you distribute your machine learning process

335
00:36:02,960 --> 00:36:09,040
you've made the system much more vulnerable to failures of various kinds and potentially to

336
00:36:09,680 --> 00:36:16,560
explicit adversarial attacks. So failures if you have 10 computers and so any one of them could

337
00:36:16,560 --> 00:36:22,800
potentially fail at some point. You could have communication issues so just now some network

338
00:36:22,800 --> 00:36:28,400
thing fails and in between within an optimization loop or between a training loop and so because of

339
00:36:28,400 --> 00:36:34,160
that if you're modeling and your optimization process is not robust you could imagine

340
00:36:35,120 --> 00:36:40,720
potentially breaking the whole training process. The worst case version of this and this comes

341
00:36:40,720 --> 00:36:49,040
from the system literature is known as Byzantine attacks. This is the idea that you want to protect

342
00:36:49,040 --> 00:36:56,080
your overall system against the worst case setting where an attacker takes over some sub-setting

343
00:36:56,080 --> 00:37:01,760
machines and does whatever they want in those sub-set machines. So they could for instance

344
00:37:01,760 --> 00:37:06,320
try to poison data on those machines or try to send wrong information back to the rest of the

345
00:37:06,320 --> 00:37:14,320
system or whatever else. And in Byzantine machine learning or Byzantine robustness the focus

346
00:37:14,320 --> 00:37:20,480
is on typically the idea is the attacker is doing this as a way to break the system. So if they

347
00:37:20,480 --> 00:37:27,280
can send the right wrong information if you like they can get the model to converge to whatever

348
00:37:27,280 --> 00:37:35,440
they like and sort of get arbitrarily bad behavior in your system. And so what you what we're

349
00:37:35,440 --> 00:37:42,320
interested in is our strategies one for just better distributed machine learning as normally as

350
00:37:42,320 --> 00:37:47,360
our initial target and just coming up with better optimization strategies both for standard

351
00:37:47,360 --> 00:37:52,800
distributed learning and also this idea of what is generally called federated machine learning.

352
00:37:53,360 --> 00:37:58,560
This idea of again training machine learning systems distributed way without sharing

353
00:37:59,680 --> 00:38:08,320
say gradients or sort of sharing information at every setting. You said without sharing gradients or

354
00:38:08,320 --> 00:38:19,360
the idea in federated machine learning very close to what is sometimes called local SGD is that each

355
00:38:19,360 --> 00:38:25,440
device runs several steps of gradient descent on their local data. And instead of sharing

356
00:38:25,440 --> 00:38:30,720
gradients at every step as you would do in a standard distributed setting they would share model

357
00:38:30,720 --> 00:38:37,120
parameters after a certain amount of training on the device. So again if you do this plus a few

358
00:38:37,120 --> 00:38:44,400
extra steps you can get privacy you can get much lower communication overhead. If you allow for

359
00:38:44,400 --> 00:38:49,200
machines to come in and out then you get something close to say what the Google system a federated

360
00:38:49,200 --> 00:38:55,440
learning system does where they can train say next word prediction models on your cell phone without

361
00:38:55,440 --> 00:39:00,960
actually sort of transporting your text data all the way to Google server so you can get privacy

362
00:39:00,960 --> 00:39:05,840
you can get some robustness but you can still get sort of reasonable performance hopefully close to

363
00:39:05,840 --> 00:39:10,880
what you would get if all the data was in the same place. There are a variety of strategies but the

364
00:39:10,880 --> 00:39:17,600
rough ideas is again targeting this distributed optimization in a way that hopefully replicates

365
00:39:17,600 --> 00:39:24,880
something close to centralized optimization. Most of our work has focused on the setting where there

366
00:39:24,880 --> 00:39:31,680
is a server somewhere and the optimization or the learning setup is that the workers communicate

367
00:39:31,680 --> 00:39:39,680
with the server every few rounds so either again using gradients or using models if it's either

368
00:39:39,680 --> 00:39:47,760
federated as standard distributed settings. And so there's a simple strategy actually that was quite

369
00:39:47,760 --> 00:39:53,120
popular when people started getting interested in robustness in distributed learning systems.

370
00:39:53,120 --> 00:40:03,680
So the idea was well mostly federated averaging which is the standard method for

371
00:40:05,120 --> 00:40:11,840
sort of federated machine learning or even standard distributed learning. Most of the methods work

372
00:40:11,840 --> 00:40:17,120
by averaging the gradients at the centralized server so the workers do whatever they do for a few

373
00:40:17,120 --> 00:40:22,880
steps one or a case steps they send some information back to the server server averages it and that

374
00:40:22,880 --> 00:40:30,160
becomes the information it gets sent across. And so the idea initially was well we know how to do

375
00:40:30,160 --> 00:40:37,040
robust great robust averaging. So if the potential failure point is this average of lots of

376
00:40:37,040 --> 00:40:42,240
different to the model parameters across devices and there's the potential for some of these

377
00:40:43,440 --> 00:40:51,280
model parameters to be incorrect or explicit attacks then we could do robust averaging and if you

378
00:40:51,280 --> 00:40:59,120
do robust averaging then you avoid the possibility that one of these devices can lead your model in

379
00:40:59,120 --> 00:41:04,080
sort of the wrong direction so that over steps you know have this what is again known as business

380
00:41:04,080 --> 00:41:11,280
behavior so get you to arbitrarily have a bad estimate or a bad model parameter by sort of

381
00:41:11,280 --> 00:41:19,120
optimization failure. So a lot of the early work in this area focused on trying to come up with or

382
00:41:19,120 --> 00:41:26,720
use robust ways of computing averages. So they're placed say the mean with the median which is

383
00:41:26,720 --> 00:41:32,320
known to be robust to sort of lots of outliers and other more sophisticated schemes that is a

384
00:41:32,320 --> 00:41:37,840
trimmed mean approach way throw away the sort of largest and smallest elements in your average.

385
00:41:37,840 --> 00:41:43,760
A few other more sophisticated this crumb which is quite popular as a way to do this sort of

386
00:41:43,760 --> 00:41:52,800
robust average. What we showed last year is an interesting behavior which I think was not obvious

387
00:41:54,160 --> 00:41:58,480
when we're I think folks would first think about this problem. So it turns out that

388
00:42:00,000 --> 00:42:10,880
you can construct a sequence of sort of bad model updates such that the mean remains close

389
00:42:10,880 --> 00:42:19,760
but the model parameter diverges over optimization steps. And the issue is that sort of the mean being

390
00:42:19,760 --> 00:42:27,200
close is not the same as sort of the optimization direction for lack of better term going in the right

391
00:42:27,200 --> 00:42:32,400
direction. So I'll try to explain this in the sort of standard distributed learning case. I think

392
00:42:32,400 --> 00:42:38,160
this is where it's maybe clear to see. So in the standard distributed learning case all the workers

393
00:42:38,160 --> 00:42:44,000
compute gradients under local data they send the gradients to the centralized server centralized server

394
00:42:44,000 --> 00:42:49,120
computes the average of the gradients and sends this back out to the workers and this average of

395
00:42:49,120 --> 00:42:56,160
the gradients is what is used for sort of the next step of gradient descent. So again the original

396
00:42:56,160 --> 00:43:02,240
papers try to just compute this average gradient in a robust way to avoid failures. So again if some

397
00:43:02,240 --> 00:43:07,920
steps of the workers were sending wearing information as long as use the robust average the mean

398
00:43:07,920 --> 00:43:14,320
would be close to the sort of original mean even if there were a few failures. But it turns out

399
00:43:14,320 --> 00:43:19,840
that if I'm running gradient descent I construct gradient updates such that the means are close

400
00:43:20,640 --> 00:43:24,720
but the direction of the sense is actually if you like even opposite from the direction that

401
00:43:24,720 --> 00:43:29,280
it should be going. So I can get the model to do really anything I want while keeping the

402
00:43:29,280 --> 00:43:37,520
means close at every step. Is the idea that you're accumulating small distances in the same

403
00:43:37,520 --> 00:43:42,240
in a deliberate direction over time and thus you're throwing your mean off or is it more nuanced

404
00:43:42,240 --> 00:43:47,920
than that? It's close. It boils down to the difference between sort of distance and angle.

405
00:43:49,280 --> 00:43:55,600
So what really matters for good gradient descent self-management is to be going in the right direction.

406
00:43:55,600 --> 00:44:02,000
So the way you construct the attack is you keep the distance close where you get the direction to

407
00:44:02,000 --> 00:44:07,680
just be a little bit off and you do this and accumulate this sort of a little bit off direction

408
00:44:07,680 --> 00:44:12,960
over steps. And so again you can get the model to do really whatever you want in this distributed

409
00:44:12,960 --> 00:44:21,440
setting. So does paper I believe it's in UAI last year where we show this? Yes, UAI 2019 called

410
00:44:21,440 --> 00:44:27,920
fall of empires breaking Byzantine tolerant SGD by inner product manipulation where we essentially

411
00:44:27,920 --> 00:44:34,640
break all of the existing methods for try to do robust distributed learning by computing robust

412
00:44:34,640 --> 00:44:42,160
averaging. Does the paper demonstrate that in a scenario that is real worldish to some degree

413
00:44:42,160 --> 00:44:49,520
that the attacker has enough information to actually execute the attack? That's a great question.

414
00:44:49,520 --> 00:44:58,160
So the setup in a lot of security work and very definitely into Byzantine world is that you try

415
00:44:58,160 --> 00:45:03,680
to protect against the worst case with the hope that if you get the worst case then you sort of

416
00:45:03,680 --> 00:45:10,160
you get easier cases for free including for instance benign cases so things just fail and turn off.

417
00:45:10,800 --> 00:45:18,640
So the focus intentionally is not on what is easily rep upcubal in real world settings. It's on

418
00:45:18,640 --> 00:45:24,160
if the attacker had full knowledge of everything and could do whatever they want, what could they do?

419
00:45:25,040 --> 00:45:29,200
And can I come up with a procedure that's robust to the thing that they could do? And it is that

420
00:45:29,200 --> 00:45:34,400
if you're robust in a setting then you get easier settings for free and it's typically the way a

421
00:45:34,400 --> 00:45:39,040
lot of security folks think about so security design is can I be secure against the worst case

422
00:45:39,040 --> 00:45:46,960
behavior? You could argue I think reasonably that sometimes it's a bit of overkill but again

423
00:45:46,960 --> 00:45:51,840
the idea is if you get this you get easier cases for free. And luckily in machine learning

424
00:45:51,840 --> 00:45:56,880
there are distributed machine learning there's lots of interesting things we can say and actually

425
00:45:56,880 --> 00:46:01,760
importantly you don't lose that much in terms of sort of overall training performance. So we have

426
00:46:01,760 --> 00:46:08,560
both theory and lots of experiments showing that if you do reasonable things you still get reasonable

427
00:46:08,560 --> 00:46:14,240
training performance not that far from what you'd get in standard settings. In fact I'd say more

428
00:46:14,240 --> 00:46:19,600
specifically if there are no failures you get something very close to training in a standard way.

429
00:46:20,160 --> 00:46:25,200
If there are failures you can show that many of the failures would break the standard training system.

430
00:46:26,480 --> 00:46:32,480
You can still train and get good results though you converge slower than if you were in a sort

431
00:46:32,480 --> 00:46:38,400
of completely benign setting with no failures. So you pay some cost but you pay a cost that sort of

432
00:46:38,400 --> 00:46:44,240
allows you to actually get results compared to settings where again if there was an attack you

433
00:46:44,240 --> 00:46:50,160
would just be completely vulnerable. So what's the nature of the approach? So one of the approaches

434
00:46:50,160 --> 00:46:57,920
that we found very effective is a bit of a twist on the problem but I like it because I think it's

435
00:46:57,920 --> 00:47:05,280
I think it's clever. So the idea is to use a validation set. So what we do is we actually

436
00:47:05,280 --> 00:47:12,080
we assume that the centralized server has access to an additional data set that's separate from

437
00:47:12,080 --> 00:47:18,480
the data sets trained on that the workers are training on. And so what we do is in every step

438
00:47:19,520 --> 00:47:25,680
use your centralized data set to check whether the gradients that you're getting are helping you

439
00:47:25,680 --> 00:47:30,640
optimize better or not. So I think pushing the model in a direction that sort of minimizing

440
00:47:30,640 --> 00:47:36,400
validation error essentially. So it feels like cheating but maybe I can argue that it's not

441
00:47:36,400 --> 00:47:43,680
it's ever ways. So one so you're paying some cost you're doing a bit of extra work on the centralized

442
00:47:43,680 --> 00:47:48,400
server. So before the centralized server all it had to do was compute an average or maybe a sort of

443
00:47:48,400 --> 00:47:53,680
smart average. Now the centralized server is doing this extra work of checking whether you're

444
00:47:53,680 --> 00:48:01,600
making some progress or not based on the gradient that you got. We can show that you can do a

445
00:48:01,600 --> 00:48:10,720
sort of highly stochastic check. So in particular you can construct a checking procedure

446
00:48:11,360 --> 00:48:15,520
that roughly boils down to taking one sample and checking whether this one sample

447
00:48:16,080 --> 00:48:21,520
slightly improves and and loss. And if that happens that's good enough to be able to check

448
00:48:21,520 --> 00:48:26,320
whether things are making progress and this is good enough to give you the protection that you

449
00:48:26,320 --> 00:48:32,560
need. So somehow the claim is that in terms of computational cost the overhead you're going to

450
00:48:32,560 --> 00:48:40,080
pay is low. In practice the win is quite large so this kind of approach is robust to some of the

451
00:48:40,080 --> 00:48:46,080
attacks I talked about where none of the robust averaging methods are. So all the robust averaging

452
00:48:46,080 --> 00:48:51,120
methods are susceptible to again getting the distance right so making the distance close but

453
00:48:51,120 --> 00:48:56,080
getting the angle completely wrong so going in the wrong direction. Whereas this method that checks

454
00:48:56,880 --> 00:49:02,560
whether you're actually making progress stochastically. So again just using a few samples can do

455
00:49:02,560 --> 00:49:08,960
this very efficiently and is able to give you protection against this sort of kind of worst case

456
00:49:08,960 --> 00:49:18,720
attack. So this is a paper we presented at ICML last year which we call Zeno Rebus synchronous

457
00:49:18,720 --> 00:49:26,080
SGD with an arbitrary number of Byzantine workers. Another nice property of the method

458
00:49:26,880 --> 00:49:33,120
is that previous work gave you protection to up to half of the machines potentially being

459
00:49:33,120 --> 00:49:41,840
corrupted. In this setup you can show that we have protection for much much higher

460
00:49:41,840 --> 00:49:50,480
potential fraction of corrupted workers. So really you need roughly maybe one good worker in

461
00:49:50,480 --> 00:49:54,880
your system to actually make progress. Again of course if there's only one good worker you pay

462
00:49:54,880 --> 00:50:01,280
some cost so everything will be slower. But another nice thing is if there's corruption but it's

463
00:50:01,280 --> 00:50:07,200
low magnitude you can actually be okay so you can actually use corrupted gradients to still make progress.

464
00:50:07,200 --> 00:50:14,720
Again as long as you can imagine an incorrect gradient being computed that's sort of a little bit

465
00:50:14,720 --> 00:50:21,680
off from the true gradient but not completely off sort of benign failure setting. We can still make

466
00:50:21,680 --> 00:50:28,640
progress using that where sometimes the standard method can have trouble with it. So again this

467
00:50:28,640 --> 00:50:34,240
line of work I think is quite exciting because it's sort of coming up with good ways for training

468
00:50:34,240 --> 00:50:40,640
large scale distributed systems with robustness built in. Along the lines of kind of the assuming

469
00:50:40,640 --> 00:50:48,080
the worst case security scenario is there are you or anyone else working on some kind of model where

470
00:50:48,800 --> 00:50:54,560
the workers and the centralized server kind of are in cahoots to determine if either of them is

471
00:50:55,440 --> 00:51:00,000
is corrupted because the centralized server is kind of a weak point in your previous example.

472
00:51:00,000 --> 00:51:05,360
I think that's a very important point. So this is assuming sort of centralized server that can be

473
00:51:05,360 --> 00:51:12,480
trusted essentially. There is a bit of work and not fully trusting the centralized server. I'm

474
00:51:12,480 --> 00:51:17,280
familiar to have to look up to get the exact references on paper is that where there's sort of

475
00:51:17,280 --> 00:51:23,920
two layers of checks or checks in both directions. I think that that entire direction is extremely

476
00:51:23,920 --> 00:51:29,840
interesting. So there's a bit of work thinking about that. Though again it's a bit sort of somewhat

477
00:51:29,840 --> 00:51:38,240
early days in that line of research. Okay. Cool. Yeah. So we've been thinking about this like I said

478
00:51:38,240 --> 00:51:42,400
for standard distributed learning settings where you're pushing gradients around. We're thinking

479
00:51:42,400 --> 00:51:47,600
about this for federate learning settings where you're pushing model parameters around. Combining

480
00:51:47,600 --> 00:51:54,720
this with just trying to scale up distributed learning to be faster doing adaptive learning rates

481
00:51:54,720 --> 00:52:00,160
and sort of other methods to get distributed learning to convert faster. So again this whole

482
00:52:00,160 --> 00:52:08,240
echo system of scaling up distributed learning and combining this with robustness because of

483
00:52:08,800 --> 00:52:13,680
special failure modes that show up when you train distributed learning systems.

484
00:52:13,680 --> 00:52:18,880
Very interesting stuff. I'm starting to think about how we're going to combine all this into a single

485
00:52:18,880 --> 00:52:29,680
title more. Yeah. Yeah. Well usually just sticking in and I also have to do it when I write up

486
00:52:29,680 --> 00:52:37,280
documents for some reason. And I didn't get to talk about any of my sort of cognitive near imaging

487
00:52:37,280 --> 00:52:46,560
work. So if I made your job a little bit easier. Yeah. So some of the threads. I mean of course

488
00:52:46,560 --> 00:52:53,040
clearly those sort of basic tools. Go back and forth. I should say and I don't think I emphasize

489
00:52:53,040 --> 00:52:59,440
this enough. I've been really lucky to have excellent colleagues and truly amazing students

490
00:52:59,440 --> 00:53:04,480
to work with here. And a lot of those sort of great ideas come from these years and from the students.

491
00:53:04,480 --> 00:53:08,880
So I mean sometimes I say that my job is to just get out of their way and sort of get them to

492
00:53:08,880 --> 00:53:17,440
do great things. So yeah. Sometimes that leads to a bit of sort of breath and ideas. Things that

493
00:53:17,440 --> 00:53:22,720
are somewhat constant. There's some I think the core mathematical tools are roughly. There's

494
00:53:22,720 --> 00:53:29,040
some new things that come on but a lot of the basics are shared. I think there's some nice

495
00:53:29,040 --> 00:53:34,800
cross-talk between for instance I didn't mention this but because we're thinking about robustness now

496
00:53:34,800 --> 00:53:39,920
as in general we've been thinking about robustness and other settings. So in the standard learning

497
00:53:39,920 --> 00:53:46,000
settings we think about robustness and loss functions, robustness and listation. So there's again

498
00:53:46,000 --> 00:53:52,640
some cross-talk between ideas that come from this. And the other way we're thinking about distributed

499
00:53:52,640 --> 00:53:58,240
learning when you have again complex settings, complex losses, interacting, complex prediction

500
00:53:58,240 --> 00:54:04,160
problems. So how does one change a distributed learning problem to account for the complex setting?

501
00:54:05,200 --> 00:54:11,360
So I think there is some cross-talk that comes between both these. And again share tools but

502
00:54:11,360 --> 00:54:15,120
I think it's fair to say that they're quite distributed, not pun intended.

503
00:54:15,120 --> 00:54:19,200
Well Semy, thanks so much for taking some time to walk us through what you're up to.

504
00:54:19,200 --> 00:54:26,640
Yeah, it was a pleasure. Thank you for time and I was glad to have some time to finally chat

505
00:54:26,640 --> 00:54:30,000
with you. I think we tried to set this up for you sometimes. Yeah, absolutely.

506
00:54:30,000 --> 00:54:34,560
Absolutely. We had a chance to go through this. It was a pleasure. Absolutely. Same. Thank you.

507
00:54:38,560 --> 00:54:43,600
All right everyone that's our show for today. For more information on today's show

508
00:54:43,600 --> 00:54:59,520
visit twomolai.com slash shows. As always, thanks so much for listening and catch you next time.

