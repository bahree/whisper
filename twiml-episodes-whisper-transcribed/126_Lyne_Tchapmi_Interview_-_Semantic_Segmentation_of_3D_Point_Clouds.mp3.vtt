WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.440
I'm your host Sam Charrington. I'm recording this intro from San Jose, California, where

00:34.440 --> 00:40.760
I'm attending the NVIDIA GTC conference. NVIDIA CEO Jensen Huang made a bunch of interesting

00:40.760 --> 00:44.760
announcements at his keynote this morning, which I live tweeted from my Twitter account

00:44.760 --> 00:50.440
at Sam Charrington. I'll also be writing about my thoughts from GTC for an upcoming addition

00:50.440 --> 00:55.760
of my newsletter, which I'd encourage you to subscribe to by visiting twimbleai.com

00:55.760 --> 01:02.640
slash newsletter. In this episode, I'm joined by Lin Chapme, PhD student in the Stanford

01:02.640 --> 01:08.760
Computational Vision and Geometry Lab, to discuss her paper SIG Cloud, semantic segmentation

01:08.760 --> 01:15.800
of 3D point clouds. SIG Cloud is an end-to-end framework that performs 3D point level segmentation,

01:15.800 --> 01:20.680
combining the advantages of neural networks, trial linear interpolation, and fully connected

01:20.680 --> 01:26.360
conditional random fields. In our conversation, Lin and I cover the ins and outs of semantic

01:26.360 --> 01:32.080
segmentation, starting from the center data that we're trying to segment to 2D and 3D representations

01:32.080 --> 01:38.240
of that data, and how we go about automatically identifying classes. Along the way, we dig

01:38.240 --> 01:43.640
into some of the details, including how she obtained a more fine-grained labeling of points,

01:43.640 --> 01:49.000
and the transition from point clouds to voxels.

01:49.000 --> 01:54.480
Before we jump into the show, a few quick questions for you. Are you an IT technology or business

01:54.480 --> 01:59.400
leader who needs to get smart on the broad spectrum of machine learning and AI opportunities

01:59.400 --> 02:04.600
in the enterprise? Or perhaps someone in your organization could benefit from a level

02:04.600 --> 02:10.560
up in this area? Or maybe you would benefit from them leveling up? If this sounds like

02:10.560 --> 02:16.400
you or someone you know, you'll probably be interested in my upcoming AI Summit event.

02:16.400 --> 02:22.640
Think of the event as a two-day no-fluff technical MBA in machine learning and AI. You'll

02:22.640 --> 02:27.240
leave with a clear understanding of how machine learning and deep learning work, with no

02:27.240 --> 02:32.480
math required, how to identify machine learning and deep learning opportunities within your

02:32.480 --> 02:37.680
organization, how to understand and take advantage of technologies like computer vision and

02:37.680 --> 02:45.040
natural language processing, how to manage and label data to take advantage of ML&AI,

02:45.040 --> 02:51.240
and how to build an AI first culture and operationalize AI in your business. You'll have an informed

02:51.240 --> 02:56.240
perspective on what's going on across the ML&AI landscape, and you'll be able to engage

02:56.240 --> 03:00.720
confidently in discussions about machine learning and AI with your colleagues, customers,

03:00.720 --> 03:05.840
and partners. I am super excited about this event and the speakers will get a chance

03:05.840 --> 03:12.840
to learn from. For more information, visit twimlai.com slash AI Summit, and feel free to contact

03:12.840 --> 03:16.400
me with your questions. And now on to the show.

03:16.400 --> 03:28.840
All right, everyone. I am here at NIPS and I am with Lynn Chapme, who is a PhD student

03:28.840 --> 03:33.920
at Stanford in the Vision and Learning Lab. Lynn, welcome to the podcast.

03:33.920 --> 03:40.240
Thank you. How did you get involved in machine learning and artificial intelligence?

03:40.240 --> 03:47.160
So I have been passionate about science and mathematics from a very young age, and when

03:47.160 --> 03:54.360
I got to college, I started studying electrical engineering and computer science. I got involved

03:54.360 --> 03:55.360
with...

03:55.360 --> 03:56.360
And where was that?

03:56.360 --> 03:57.360
That was at MIT.

03:57.360 --> 03:58.360
Okay.

03:58.360 --> 04:07.560
So I participated in the Super-UR program there, which is an undergraduate research program.

04:07.560 --> 04:16.400
And as part of that program, I got to work on designing, building, and programming wireless

04:16.400 --> 04:22.880
health monitor that essentially keeps track of cardiovascular signals. It connects to the

04:22.880 --> 04:28.800
ear and connects that data and sends it to a smartphone. And what was that program called?

04:28.800 --> 04:29.800
Super-UR.

04:29.800 --> 04:30.800
Super-UR?

04:30.800 --> 04:33.960
Yes. That was super on the Graduate Research Opportunity program.

04:33.960 --> 04:34.960
Oh, okay.

04:34.960 --> 04:40.960
That's what I stands for. Yes. So it was a pretty exciting project, and I found out that

04:40.960 --> 04:43.960
I was really interested in both the hardware and the software side.

04:43.960 --> 04:44.960
Okay.

04:44.960 --> 04:50.560
As a result, I continued my study. Mostly on the hardware side, I took a bunch of circuit

04:50.560 --> 04:57.760
design courses, and continued doing more research. Eventually, I continued with my masters.

04:57.760 --> 05:01.760
So at MIT, doing a power electronics.

05:01.760 --> 05:02.760
Okay.

05:02.760 --> 05:10.480
So my master's thesis was on designing an efficient power converter for low power circuits that

05:10.480 --> 05:11.480
I use in mobile devices.

05:11.480 --> 05:13.960
So I just had this in small phones.

05:13.960 --> 05:14.960
Okay.

05:14.960 --> 05:21.760
So when I finished my masters, I liked research, so I decided I would apply for a PhD.

05:21.760 --> 05:27.040
I got into Stanford. And when I came in, initially, I thought that I was going to continue

05:27.040 --> 05:29.160
and work in power electronics.

05:29.160 --> 05:30.160
Okay.

05:30.160 --> 05:36.720
But Stanford has a PhD rotation program. So during your first year, you can spend all three

05:36.720 --> 05:38.800
quarters working in three different labs.

05:38.800 --> 05:39.800
Oh, really?

05:39.800 --> 05:43.480
Yes. So with three different professors, and it kind of gives students the opportunity

05:43.480 --> 05:48.480
to explore a little bit, all of the different professors, all of the different projects

05:48.480 --> 05:50.080
that are available.

05:50.080 --> 05:53.200
So I took advantage of the program.

05:53.200 --> 05:58.600
I worked with Professor Juan Rivas from the power electronics lab.

05:58.600 --> 06:03.320
Then I worked with Professor Sebastian Trond, who used to do robotics.

06:03.320 --> 06:08.360
And at the time, actually, was more focused on computer vision things.

06:08.360 --> 06:13.120
And that's when I learned a little bit about computer vision, thought I was interesting,

06:13.120 --> 06:15.360
started taking classes.

06:15.360 --> 06:20.280
And during my third rotation, I worked with Professor Sylvia Savarisa, who is currently

06:20.280 --> 06:21.280
my advisor.

06:21.280 --> 06:22.280
Okay.

06:22.280 --> 06:25.120
In his lab, that's when I learned about deep learning.

06:25.120 --> 06:30.240
I started exploring it a bit more, thought he was interesting.

06:30.240 --> 06:36.600
And I think the most fascinating aspect of it was that just the possibility of using the

06:36.600 --> 06:43.600
techniques and the tools that are available to solve various problems ranging from energy,

06:43.600 --> 06:48.200
finance, construction, medical.

06:48.200 --> 06:50.280
So I thought I was pretty exciting.

06:50.280 --> 06:51.280
Awesome.

06:51.280 --> 06:52.280
Awesome.

06:52.280 --> 06:54.880
Tell me a little bit about the current focus of your research.

06:54.880 --> 06:55.880
What are you working on?

06:55.880 --> 06:59.080
In fact, you've got a paper that you're presenting here at NEPS, right?

06:59.080 --> 07:00.080
Yes.

07:00.080 --> 07:03.520
So I focused on 3D scene understanding.

07:03.520 --> 07:04.520
Okay.

07:04.520 --> 07:13.120
So my goal is to take visual information about the 3D world and try to make sense of it.

07:13.120 --> 07:20.440
And the reason why such information is important is that if you have intelligent systems that

07:20.440 --> 07:25.040
are surrounding human beings and that are able, in order for them to be able to assist

07:25.040 --> 07:28.640
human beings, they need to be able to understand what is going on around us.

07:28.640 --> 07:35.560
And some of the applications of that are self-driving cars and just general assistance within

07:35.560 --> 07:36.560
the home.

07:36.560 --> 07:42.880
Let's say an elderly person who needs help with daily tasks or even a surgeon in the hospital

07:42.880 --> 07:47.320
that needs assistance, maybe for getting tools around and things like that.

07:47.320 --> 07:51.200
So I have been working.

07:51.200 --> 07:55.720
My recent paper is about semantic segmentation of 3D point clouds.

07:55.720 --> 08:02.800
So with sensors such as connect sensors, lighter sensors, you can acquire models of 3D environments.

08:02.800 --> 08:06.160
It could be indoor spaces or outdoor spaces.

08:06.160 --> 08:12.400
And that information is not meaningful in and of itself and using deep learning, we can

08:12.400 --> 08:14.120
make sense of it.

08:14.120 --> 08:20.680
And the goal of the project was essentially to identify all of the different elements that

08:20.680 --> 08:23.680
compose the data, the 3D data.

08:23.680 --> 08:29.560
In indoor spaces you might be interested in identifying the walls, the ceilings, the objects,

08:29.560 --> 08:32.160
the tables, chairs, and all that.

08:32.160 --> 08:39.680
And in outdoor spaces you might be interested in identifying roads, the cars, and buildings

08:39.680 --> 08:41.080
for instance.

08:41.080 --> 08:46.360
So the method that we develop is just before you go into the method to just to make sure

08:46.360 --> 08:47.360
I understand.

08:47.360 --> 08:52.240
I'm imagining we've got a connect sensor in here and it's giving us kind of this 3D point

08:52.240 --> 08:57.400
cloud of everything in this room, but it's just points.

08:57.400 --> 09:06.480
And so the points aren't distinguished from the table, the chairs, et cetera, the people.

09:06.480 --> 09:14.120
And so you're applying deep learning to on top of that raw data set, then categorize

09:14.120 --> 09:18.600
or is it a categorization problem for each of the points into some object and you don't

09:18.600 --> 09:24.560
know the objects that are in the field before you do this, right?

09:24.560 --> 09:29.640
Yes, we don't know the objects that are in the field, but we have a certain set of objects

09:29.640 --> 09:31.200
that we're interested in.

09:31.200 --> 09:39.440
So the thing with deep learning or a little supervised learning is that you need to know

09:39.440 --> 09:42.640
the classes that you are interested in ahead of time.

09:42.640 --> 09:46.920
So they're in order to first perform the annotation because the ground to this needed.

09:46.920 --> 09:51.360
So during the annotation process you essentially choose the classes that you are interested

09:51.360 --> 09:52.360
in.

09:52.360 --> 09:53.360
Okay.

09:53.360 --> 09:55.160
And that's what you use for training.

09:55.160 --> 09:59.680
So although you don't know all the classes that are available in your data set, there

09:59.680 --> 10:04.960
will be a specific set of classes that are used during training and during testing.

10:04.960 --> 10:10.000
And usually there's a default class that is called clutter or other.

10:10.000 --> 10:11.000
Okay.

10:11.000 --> 10:14.920
Which is used to identify anything else that was enabled in the data set.

10:14.920 --> 10:15.920
Okay.

10:15.920 --> 10:22.640
And so when you're identifying the classes that you expect, is this a small number of,

10:22.640 --> 10:26.720
you know, I'm expecting to see, you know, this is a home environment, you know, they're

10:26.720 --> 10:31.160
probably, you know, one of these 10 things or is it like, you know, a thousand classes

10:31.160 --> 10:33.720
all of image net or something like that?

10:33.720 --> 10:41.720
So at the level of 3D data, it's still a very small number of classes, in part because

10:41.720 --> 10:46.280
3D data is much more difficult to annotate.

10:46.280 --> 10:55.040
And so the number of classes range somewhere between 8 to 10 or 20 classes.

10:55.040 --> 10:56.880
So it's not yet in the hundreds.

10:56.880 --> 10:57.880
Okay.

10:57.880 --> 11:00.640
It's a lot more difficult to annotate 3D data.

11:00.640 --> 11:07.920
And the fact that sensors, depending on what modality you annotated as well, the sensor

11:07.920 --> 11:09.800
data could also be very noisy.

11:09.800 --> 11:13.640
So it could be difficult to identify certain classes.

11:13.640 --> 11:19.080
So and that's one of the challenges with 3D data compared to images.

11:19.080 --> 11:21.880
Images are much easier to acquire.

11:21.880 --> 11:27.360
And pretty much everyone today has a cell phone, a camera, so there's a lot of images

11:27.360 --> 11:30.960
that are being created at every second.

11:30.960 --> 11:33.720
And the same, it doesn't apply to 3D data.

11:33.720 --> 11:40.200
You know, everyone has a connect sensor or a lighter that they use on a daily basis essentially.

11:40.200 --> 11:46.200
So 3D data is much less available and then you need those specialized tools to really

11:46.200 --> 11:47.200
annotate them.

11:47.200 --> 11:49.240
So it's a bit more difficult.

11:49.240 --> 11:56.920
And when you say annotate, are you talking about using your model to annotate or are you

11:56.920 --> 12:07.800
talking about the pre-training task of having, you know, creating the training set by annotating

12:07.800 --> 12:08.800
sample data?

12:08.800 --> 12:09.800
Yeah.

12:09.800 --> 12:14.680
So I'm talking about the pre, the task before training, the task of generating the data

12:14.680 --> 12:19.320
I say itself in the annotations that are going to be used for training.

12:19.320 --> 12:25.000
So that is the most challenging part around 3D computer vision.

12:25.000 --> 12:30.400
You know, even for 2D data, right?

12:30.400 --> 12:34.640
My sense is that the folks that are doing annotation at scale have all kind of come up with

12:34.640 --> 12:36.400
their own systems for doing this.

12:36.400 --> 12:40.160
There's not like an off-the-shelf open source annotation toolkit.

12:40.160 --> 12:42.720
I think there should be.

12:42.720 --> 12:45.240
But I haven't really seen anything like that.

12:45.240 --> 12:47.320
There are various ones that are out there.

12:47.320 --> 12:48.320
Are they really?

12:48.320 --> 12:50.520
Yeah, they definitely are.

12:50.520 --> 12:57.000
I think one of the, one that I can think of of the top of my head is called LabelMe.

12:57.000 --> 13:01.240
So it's available to the research community and a lot of people use it.

13:01.240 --> 13:02.240
Oh, okay.

13:02.240 --> 13:03.240
Yeah.

13:03.240 --> 13:10.040
So there definitely are four images and also for 3D data, more recently there was an annotation

13:10.040 --> 13:14.400
tool called scannet that was released to annotate 3D data.

13:14.400 --> 13:16.800
So it makes the task a lot more easier.

13:16.800 --> 13:17.800
Okay.

13:17.800 --> 13:25.400
Essentially, the task is to paint over a bunch of segments in 3D and choose a class for

13:25.400 --> 13:27.080
the segments that I painted over.

13:27.080 --> 13:30.160
So it makes the task a lot more, a lot easier.

13:30.160 --> 13:36.640
But even then, it sounds like there's, you've got all kinds of ambiguity because you're

13:36.640 --> 13:40.360
painting in 2D over this 3D point cloud.

13:40.360 --> 13:41.360
Is that right?

13:41.360 --> 13:43.880
Or are you doing like rotations and all that kind of stuff?

13:43.880 --> 13:44.880
Yeah.

13:44.880 --> 13:46.720
Scannet actually allows you to paint in 3D.

13:46.720 --> 13:47.720
Oh, really?

13:47.720 --> 13:48.720
Okay.

13:48.720 --> 13:53.880
So that is, it's more suitable for 3D data.

13:53.880 --> 14:02.320
The challenge there is that the reconstruction that is used as the basis of the annotation needs

14:02.320 --> 14:07.600
to be a very high quality, which is not always the case because the sensors, the 3D sensors

14:07.600 --> 14:09.600
are noisy.

14:09.600 --> 14:14.720
And what happens is that you're able to annotate the bigger objects that are more visible.

14:14.720 --> 14:15.720
Okay.

14:15.720 --> 14:25.800
So you can easily paint the table, but painting a bottle of water on the table is going to be

14:25.800 --> 14:29.800
a lot harder because points will be missing or that kind of thing.

14:29.800 --> 14:30.800
Exactly.

14:30.800 --> 14:32.760
Whereas on an image, it's much easier to do that.

14:32.760 --> 14:33.760
Right.

14:33.760 --> 14:34.760
Yeah.

14:34.760 --> 14:35.760
Well, interesting.

14:35.760 --> 14:39.200
I didn't realize that, you know, I've had this conversation with a bunch of folks that

14:39.200 --> 14:42.480
are working on this and they all say, no, no, there's nothing really out there.

14:42.480 --> 14:44.240
We had to build it ourselves.

14:44.240 --> 14:50.160
There are some tools, depending on the quality, there are mostly research tools.

14:50.160 --> 14:51.160
Okay.

14:51.160 --> 14:55.800
So they may not be very thoroughly developed if you want to use it for production.

14:55.800 --> 14:57.280
Maybe it's not what you want to use.

14:57.280 --> 14:58.280
Right.

14:58.280 --> 15:01.160
But there are tools that are used by the research community.

15:01.160 --> 15:02.160
Okay.

15:02.160 --> 15:03.160
Yeah.

15:03.160 --> 15:09.760
But I imagine then that, you know, even, yeah, if you're trying to do 3D annotation,

15:09.760 --> 15:16.560
is it, I'm imagining that it's harder to, like, get a mechanical turquer to do 3D labeling,

15:16.560 --> 15:20.440
you know, A because, you know, there's probably some kind of special plug-in that has to

15:20.440 --> 15:26.080
be used if you can even do it over the web, but then B, just the complexity of, like, the

15:26.080 --> 15:29.000
instructions you give and that kind of thing.

15:29.000 --> 15:30.680
It sounds very difficult.

15:30.680 --> 15:36.280
It likely is slightly more difficult than images, but it is doable.

15:36.280 --> 15:37.280
Okay.

15:37.280 --> 15:41.680
So the tool that I mentioned scan it, that's essentially what they did.

15:41.680 --> 15:48.280
They crowdsourced both the data collection process and the annotation process.

15:48.280 --> 15:49.280
Okay.

15:49.280 --> 15:55.320
So they had people, they had turkers, I'm not sure who was turkers, but they had workers

15:55.320 --> 16:00.360
actually annotates using their tool, they gave them instructions and the annotations turned

16:00.360 --> 16:01.360
out pretty decent.

16:01.360 --> 16:02.360
Okay.

16:02.360 --> 16:03.360
Yeah.

16:03.360 --> 16:04.360
Interesting.

16:04.360 --> 16:10.320
We've got the, you know, we've got some training data assembled, you know, tell us about

16:10.320 --> 16:15.680
your process and how you went from that to point segmentation.

16:15.680 --> 16:16.680
Okay.

16:16.680 --> 16:24.240
So essentially for each point in the 3D data in the point cloud, we want to assign it label.

16:24.240 --> 16:30.840
And we wanted to leverage convolutional neural networks since they've been shown to work

16:30.840 --> 16:32.840
really well for images.

16:32.840 --> 16:39.360
The data representation for 3D point clouds is not directly suitable for convolutional

16:39.360 --> 16:40.680
neural networks.

16:40.680 --> 16:51.000
So we have it because think about an image, an image is a 2D array and it has a level

16:51.000 --> 16:54.360
of structure that is suitable for convolutional neural networks.

16:54.360 --> 17:00.520
So CNN's kind of take advantage of that structure in the image through the convolutions.

17:00.520 --> 17:02.400
So you've got an array of pixels?

17:02.400 --> 17:03.400
Yes.

17:03.400 --> 17:07.520
And kind of mapping that to a point cloud at any level of resolution, you could have multiple

17:07.520 --> 17:09.560
points in kind of a box.

17:09.560 --> 17:11.680
So how would you translate it?

17:11.680 --> 17:16.160
So the point clouds, when they actually are collected, they come in as a 2D array, but

17:16.160 --> 17:19.320
it's more, it's not structured.

17:19.320 --> 17:20.320
Okay.

17:20.320 --> 17:25.320
So it's a set of XYZ coordinates with RGB color.

17:25.320 --> 17:26.320
Okay.

17:26.320 --> 17:27.400
And so it's a bit less structured.

17:27.400 --> 17:35.000
So there is no, unless you go to the process of oxidization, which is what we did.

17:35.000 --> 17:36.000
Of what?

17:36.000 --> 17:37.000
Of oxidization.

17:37.000 --> 17:40.200
So essentially turning that representation into a grid.

17:40.200 --> 17:41.200
Okay.

17:41.200 --> 17:48.320
So, and the way to do that essentially is to look at the 3D space and divide the 3D space

17:48.320 --> 17:56.440
into a grid essentially and assign, turn your XYZ RGB representation into a different

17:56.440 --> 17:59.880
representation, which has four channels.

17:59.880 --> 18:04.560
The first channel is occupancy, which is zero or one.

18:04.560 --> 18:08.440
And if it's a one, then it means that that space is occupied.

18:08.440 --> 18:11.480
And zero is not occupied.

18:11.480 --> 18:16.880
And the last three channels are not the RGB color.

18:16.880 --> 18:17.880
The RGB.

18:17.880 --> 18:18.880
Yes.

18:18.880 --> 18:22.720
Which usually is the average of all the points that are within the unit grid cell, which

18:22.720 --> 18:23.920
are a color box.

18:23.920 --> 18:24.920
Okay.

18:24.920 --> 18:25.920
Yeah.

18:25.920 --> 18:29.480
So you have to make that transition from point cloud to voxel.

18:29.480 --> 18:33.600
And the voxel usually has a predefined size in our paper.

18:33.600 --> 18:36.560
We start out with five centimeters.

18:36.560 --> 18:43.640
And there is another difficulty with 3D data, which is the added dimensionality.

18:43.640 --> 18:47.360
So in 2D, you just have two dimensions plus the channels.

18:47.360 --> 18:51.840
In 3D, you have the added depth dimension.

18:51.840 --> 18:58.280
So that kind of scales the problem and makes it a bit more difficult to solve in terms

18:58.280 --> 18:59.280
of memory.

18:59.280 --> 19:03.880
So 3D data would take more space when you're using CNNs.

19:03.880 --> 19:08.360
And for this reason, when we're using traditional convolutional neural networks, we actually

19:08.360 --> 19:11.960
have to down sample our input volume further.

19:11.960 --> 19:17.240
So we start out with the five centimeter voxel and we down sample it through the CNN.

19:17.240 --> 19:21.880
So the CNN would down sample the volume to 20 centimeter voxel.

19:21.880 --> 19:31.080
So at the end, what the CNN outputs is a label over 20 centimeter grid unit, which is

19:31.080 --> 19:32.080
relatively coarse.

19:32.080 --> 19:33.080
Yeah.

19:33.080 --> 19:34.080
Exactly.

19:34.080 --> 19:42.280
So what our paper explored is how can we obtain still average CNN, how can we obtain a

19:42.280 --> 19:47.760
more fine green labeling of our point car, how can we label the individual points which

19:47.760 --> 19:50.600
come from the original sensor.

19:50.600 --> 19:56.600
And to do that, we design a trial and error interpolation layer that a trial and error interpolation

19:56.600 --> 19:57.600
layer.

19:57.600 --> 19:59.120
A trial and error interpolation layer.

19:59.120 --> 20:05.120
So we use it to train during training to train the CNN and to end.

20:05.120 --> 20:12.240
And basically what it does is the CNN outputs a set of scores for each voxel in space.

20:12.240 --> 20:17.640
For each point in space, we look at the eight closest voxel centers.

20:17.640 --> 20:23.480
And we compute the score of the point by weighing the combination of the ways that are coming

20:23.480 --> 20:25.920
from the eight nearest voxels.

20:25.920 --> 20:30.600
So you can think of the closest voxel will be contributing more to the final score at

20:30.600 --> 20:34.400
the point than the voxels that are a bit further away from the point.

20:34.400 --> 20:38.880
And so we use that during training and the alternative to using.

20:38.880 --> 20:44.160
And now is this the eight closest occupied voxels or just the eight closest voxels?

20:44.160 --> 20:47.680
The eight closest occupied voxels, yes.

20:47.680 --> 20:56.400
And so during training, the alternative to doing that is simply doing a nearest neighbor.

20:56.400 --> 21:00.560
So taking the closest voxel and assigning that label to the point.

21:00.560 --> 21:05.040
And during training, we find out that actually using trial and error interpolation layer

21:05.040 --> 21:08.800
performs a lot better than using a nearest neighbor.

21:08.800 --> 21:14.440
So that was the first module that we added to our CNN.

21:14.440 --> 21:20.040
And in the end, we also used a conditional random field to refine the predictions that

21:20.040 --> 21:24.400
are given to us by the CNN and the trial and error interpolation layer.

21:24.400 --> 21:32.120
And so the conditional random field essentially defines an energy function that enforces consistency

21:32.120 --> 21:34.840
between labeling intuitively.

21:34.840 --> 21:41.120
You can think, if you take two points in space, if those points are close to each other,

21:41.120 --> 21:45.440
then they should, the probability that they have the same label is high.

21:45.440 --> 21:51.760
And so the energy function of the CRF enforces that close points will have, are more likely

21:51.760 --> 21:53.960
to have similar labels.

21:53.960 --> 21:59.880
And so without energy function, we are able to refine our predictions.

21:59.880 --> 22:07.520
And we use a nice implementation of conditional random fields that was published in a 2015

22:07.520 --> 22:10.280
paper called CRF as RNN.

22:10.280 --> 22:14.720
So the CRF is implemented as a recurrent neural network.

22:14.720 --> 22:20.640
And the advantage of that implementation is that you can actually combine both the CNN,

22:20.640 --> 22:24.720
the trial and error interpolation layer and the CRF and train them into N.

22:24.720 --> 22:25.720
Okay.

22:25.720 --> 22:30.720
It's a nice module that can be used for semantic segmentation.

22:30.720 --> 22:31.720
Okay.

22:31.720 --> 22:32.720
Yes.

22:32.720 --> 22:33.720
Interesting.

22:33.720 --> 22:39.000
And so what kind of, what kind of results have you seen with that?

22:39.000 --> 22:40.240
We've seen great results.

22:40.240 --> 22:46.440
So we evaluated framework on the Stanford Indoor Space Data Set.

22:46.440 --> 22:52.080
And we performed the state of the art on that data set, which was pointed.

22:52.080 --> 23:01.160
And currently we also have, we also tested on the semantic 3D net data set, which is the

23:01.160 --> 23:05.560
largest outdoor point cloud data set that is available.

23:05.560 --> 23:09.120
And at the time of publication, we're also the state of the art, but I think there is

23:09.120 --> 23:12.520
a better method now because that's what happens.

23:12.520 --> 23:13.520
But yeah.

23:13.520 --> 23:20.520
So currently we are, I guess the second best method performing, the second best performing

23:20.520 --> 23:22.280
method on those data sets.

23:22.280 --> 23:23.280
Okay.

23:23.280 --> 23:30.480
And for these data sets, was the improvement like order of magnitude or was it like huge

23:30.480 --> 23:36.040
or was it incremental improvements relative to the prior state of the art?

23:36.040 --> 23:40.800
So depending on what we saw was that the best improvement that we got was on the largest

23:40.800 --> 23:41.800
data set.

23:41.800 --> 23:42.800
Okay.

23:42.800 --> 23:49.960
So we had about, I cannot remember the actual numbers, but it was more significant on

23:49.960 --> 23:52.160
the larger outdoor point cloud data set.

23:52.160 --> 23:53.160
Okay.

23:53.160 --> 24:01.200
There was also a pretty large gap between on the, on the, on the smaller indoor spaces data

24:01.200 --> 24:02.200
set.

24:02.200 --> 24:06.960
But the data set is also relatively small.

24:06.960 --> 24:11.280
And so the performance, the absolute performance is still a little bit low.

24:11.280 --> 24:17.640
And I think as we get more data, the numbers should get better, yeah.

24:17.640 --> 24:22.000
Now this conversation reminds me of another conversation that I've had here at NIPS.

24:22.000 --> 24:24.440
But it doesn't remind me of it.

24:24.440 --> 24:28.880
It is potentially related to another conversation that I've had here at NIPS.

24:28.880 --> 24:35.120
And the, the idea with the research they were doing is that there are, like here you've

24:35.120 --> 24:42.560
had to, you know, a lot of what your research focus was was kind of fitting or transforming

24:42.560 --> 24:49.880
this point cloud to the, the voxels and potentially losing the relationship between some of the,

24:49.880 --> 24:53.000
the individual points in the cloud.

24:53.000 --> 25:03.000
And they, this was an interview as Joanne Bruna at NYU and Michael Browstein and they are

25:03.000 --> 25:08.400
developing, or they're researching algorithms that take more like a graph approach.

25:08.400 --> 25:09.400
Okay.

25:09.400 --> 25:15.680
And the individual point would be seen as a graph as opposed to points in a Euclidean

25:15.680 --> 25:16.680
space.

25:16.680 --> 25:17.680
Okay.

25:17.680 --> 25:20.640
Have you looked at that kind of approach to doing segmentation?

25:20.640 --> 25:28.040
I have not personally, but one, I think it's a promising approach as well.

25:28.040 --> 25:33.880
We do use some amount of graphical representation in our method, not at the level of the CNN

25:33.880 --> 25:37.160
but more at the final module, which is the conditional random field.

25:37.160 --> 25:38.160
Okay.

25:38.160 --> 25:43.760
So that's back to our initial representation, but I do think that approach also is likely

25:43.760 --> 25:44.760
promising.

25:44.760 --> 25:45.760
Yeah.

25:45.760 --> 25:46.760
Interesting.

25:46.760 --> 25:50.280
I guess that's the great thing about Nips is that you get exposure to all of these different

25:50.280 --> 25:55.240
folks working on kind of related things and can, you know, try to pull together different

25:55.240 --> 25:56.240
ideas.

25:56.240 --> 25:57.240
Yeah.

25:57.240 --> 26:00.480
There's definitely a lot of work that is being done in this field.

26:00.480 --> 26:04.800
People are looking into different ways of processing the data.

26:04.800 --> 26:12.880
So one of the papers that we compared to was Poinets, which essentially proposed to process

26:12.880 --> 26:16.080
the points directly rather than doing voxalization.

26:16.080 --> 26:21.760
So they're definitely alternatives to skipping voxalization.

26:21.760 --> 26:29.120
The framework that we have in a way, it could, the conditional random field could still

26:29.120 --> 26:34.440
apply to some of those methods, but definitely the point representation is flexible and

26:34.440 --> 26:37.640
there are several ways that people could do it.

26:37.640 --> 26:42.440
There, for instance, also, are more efficient ways to do convolution.

26:42.440 --> 26:48.400
There's sparse convolutions that could be done so that could help with efficiency.

26:48.400 --> 26:53.760
The opnet is one of the papers that export that direction essentially.

26:53.760 --> 26:54.760
All of it.

26:54.760 --> 26:55.760
Opnet.

26:55.760 --> 26:56.760
Yes.

26:56.760 --> 26:57.760
Okay.

26:57.760 --> 27:03.120
So it's actually trying to perform the convolution operations on the spaces that are occupied

27:03.120 --> 27:05.360
rather than the entire volume.

27:05.360 --> 27:11.400
So they're definitely various approaches to solve this problem, so interesting.

27:11.400 --> 27:19.280
With the conditional random field and the energy function, actually, that's not an area

27:19.280 --> 27:21.280
that I'm well versed in.

27:21.280 --> 27:26.440
Maybe you can talk a little bit more about that, but I'm specifically wondering is the thing

27:26.440 --> 27:32.240
that you did that was interesting was applying conditional random fields and energy functions

27:32.240 --> 27:36.920
or coming up with a specific energy function that worked well in this case.

27:36.920 --> 27:42.400
So the energy function itself was standard except for the fact that it was applied on

27:42.400 --> 27:46.520
3D points rather than traditionally on images.

27:46.520 --> 27:54.800
So we, one of the novel things was that we applied it on the point at the point level.

27:54.800 --> 27:55.800
Okay.

27:55.800 --> 28:03.920
So the end part of our framework essentially is different in that we're not operating

28:03.920 --> 28:05.320
in voxel space.

28:05.320 --> 28:12.640
So traditionally when you're using the CNN and adding a CRF to it, usually the input does

28:12.640 --> 28:15.680
not change from start to end input space.

28:15.680 --> 28:21.880
We sort of have this interpolation there that helps us to go from a discrete space to a

28:21.880 --> 28:28.520
continuous space and we do further processing within our continuous space with the CRF as

28:28.520 --> 28:31.360
well as the tri-linear interpolation layer.

28:31.360 --> 28:40.080
In terms of the energy potential that we use, we use XYZ coordinates and RGB color as

28:40.080 --> 28:41.320
the features.

28:41.320 --> 28:47.080
So the energy function itself has two different terms, the unit potentials as well as the pair

28:47.080 --> 28:54.120
white potentials. So for the unit potentials, those come directly from the CNN true, the

28:54.120 --> 29:02.040
tri-linear interpolation layer and that sort of represents the initial belief of the system

29:02.040 --> 29:06.240
about the classes of each point.

29:06.240 --> 29:11.440
And then you have the pair white potentials that are also added to the energy function and

29:11.440 --> 29:17.320
those are the potentials that ensure the consistency between the neighborhood of each point.

29:17.320 --> 29:20.000
And is that the piece that you mentioned was kind of more graph-like?

29:20.000 --> 29:21.000
Yes, exactly.

29:21.000 --> 29:22.000
Representation.

29:22.000 --> 29:23.000
Yes, exactly.

29:23.000 --> 29:26.680
So we use a fully connected conditional random field.

29:26.680 --> 29:30.200
So it represents relationships between each point and its neighborhood.

29:30.200 --> 29:34.560
In this case, because it's fully connected, it actually has a connection between each point

29:34.560 --> 29:37.160
and every other point in the point cloud.

29:37.160 --> 29:40.440
So that's the graph representation that we're using.

29:40.440 --> 29:41.440
Yeah.

29:41.440 --> 29:44.160
So what's next for this research?

29:44.160 --> 29:52.560
So this research is actually a small subset of a bigger set of problems that I am exploring.

29:52.560 --> 29:59.760
So I am interested in, as I said earlier, I'm interested in trying to see how we can enable

29:59.760 --> 30:03.480
intelligence systems to assist human beings in their environments.

30:03.480 --> 30:06.480
So the first subset of that is seen understanding.

30:06.480 --> 30:07.480
Okay.

30:07.480 --> 30:08.480
What's on the environment?

30:08.480 --> 30:10.480
Exactly.

30:10.480 --> 30:20.800
You can imagine if you give a robot in order to go to the bedroom and get a book or get

30:20.800 --> 30:26.320
the Harry Potter book from my bookshelf, what does the robot need to do in order to accomplish

30:26.320 --> 30:27.320
that task?

30:27.320 --> 30:30.280
There's various fields that come into that.

30:30.280 --> 30:34.400
There's natural language processing, there's vision, there's robotics.

30:34.400 --> 30:40.800
And so the natural language processing to first understand the command, the visual part,

30:40.800 --> 30:47.200
the perception part to navigate through the room, get to the room, identify the object,

30:47.200 --> 30:52.240
and finally, the robotics part to actually do the interaction and pick up the objects.

30:52.240 --> 31:01.320
So I started working on the scene understanding within in terms of a more detailed scene understanding

31:01.320 --> 31:07.360
identifying objects, but there's also a higher level scene understanding that is involved

31:07.360 --> 31:14.400
in those kinds of assistance, which is how do you identify the different places within

31:14.400 --> 31:15.640
a building.

31:15.640 --> 31:18.080
So how do you know where the bedroom is?

31:18.080 --> 31:22.960
And so when we take those 3D point clouds, that information also is not available.

31:22.960 --> 31:30.000
So I am interested in looking at getting the higher level understanding of scenes as well

31:30.000 --> 31:42.080
and specifically using neural networks to obtain a navigation focused floor map of environments.

31:42.080 --> 31:48.480
So there has been work done in getting floor maps of environments mainly segmenting rooms

31:48.480 --> 31:53.080
between each other, so figuring out the separation between rooms.

31:53.080 --> 31:59.280
But in order for those to be a helpful navigation, we need more information.

31:59.280 --> 32:02.200
We need the identity of rooms.

32:02.200 --> 32:08.320
So we need to know that this is a living room, a bedroom, bathroom, and so on and so forth.

32:08.320 --> 32:14.760
And in addition, potentially, we need to know the location of entrances to each room.

32:14.760 --> 32:19.600
So once you have that kind of information, you cannot plan navigation to go to a specific

32:19.600 --> 32:20.680
room.

32:20.680 --> 32:25.840
So those are the two parts of scene understanding that I'm interested in.

32:25.840 --> 32:31.240
So getting a high level understanding of scenes as well as the more detailed object level

32:31.240 --> 32:33.240
understanding of the scenes.

32:33.240 --> 32:37.640
So for the higher level piece, there's been, as you mentioned, there's been a ton of

32:37.640 --> 32:43.720
work that's happened, a lot of it in the robotics community in terms of mapping environments

32:43.720 --> 32:54.840
and things like that is part of what you're trying to do to be able to identify, identify

32:54.840 --> 33:00.240
room, for example, based on what's in the room, as opposed to being given a specific label

33:00.240 --> 33:01.240
or something like that.

33:01.240 --> 33:05.840
Yeah, the idea is to identify the room based on what is in the room, based on the visual

33:05.840 --> 33:13.760
information, but also at the 3D level, because there is work that can identify a room based

33:13.760 --> 33:16.680
on an image of the room.

33:16.680 --> 33:24.120
But if you are in one location of a building and you want to get to the other location,

33:24.120 --> 33:26.680
that kind of information is not going to be very helpful.

33:26.680 --> 33:33.560
You need to have a map of the environment and know where each room is located.

33:33.560 --> 33:38.720
And so we're doing that more at the 3D level and at a higher perspective added in using

33:38.720 --> 33:39.720
images.

33:39.720 --> 33:40.720
Yeah.

33:40.720 --> 33:47.280
And in practice, do you envision that a system like this would be used dynamically or

33:47.280 --> 33:48.280
statically?

33:48.280 --> 33:54.080
And what I mean is, are we going to have like a mini light on the robot and as it navigates

33:54.080 --> 34:01.600
the environment, it's making determinations about what the room is, or more like in the

34:01.600 --> 34:06.040
future, all buildings will have, you know, they'll come with 3D point clouds or something

34:06.040 --> 34:08.800
on whatever the future version of a floppy disk is.

34:08.800 --> 34:09.800
Okay.

34:09.800 --> 34:15.920
I think there would be an initial map of 3D environments.

34:15.920 --> 34:22.320
And as the robot navigates around the environment, then it can update what the map looks like,

34:22.320 --> 34:24.240
at a more local level.

34:24.240 --> 34:29.360
So if you have an initial state of a room and you pass at a certain point and you see

34:29.360 --> 34:33.800
that something has changed, then you can update that in the internal map that you have.

34:33.800 --> 34:41.800
So it doesn't need to be updated in real time, well, the entire map does not need to be

34:41.800 --> 34:48.040
updated always, but there needs to be an initial starting point that can be updated locally

34:48.040 --> 34:53.360
as the, as more information is gathered over time.

34:53.360 --> 35:01.320
So the, the robot would know the kind of overall 3D structure of the environment, but might

35:01.320 --> 35:06.800
be able to infer that, well, you removed the bed and put a desk, so this is an office

35:06.800 --> 35:08.880
now as opposed to a bedroom.

35:08.880 --> 35:09.880
Exactly.

35:09.880 --> 35:16.360
So I guess for an office to change, so a bedroom there would probably be a lot more than

35:16.360 --> 35:24.000
like one single change, but even imagining that a room, the room state is still the same,

35:24.000 --> 35:27.040
but just a chair was moved to a different location.

35:27.040 --> 35:31.360
That can affect navigation planning, for instance, and that's something that is worth

35:31.360 --> 35:35.120
updating as well.

35:35.120 --> 35:42.200
And then, you know, it coming from a group that's that's largely focused on vision or is

35:42.200 --> 35:47.280
your line of research like, in a sense, swimming against the tide, like I get the sense that

35:47.280 --> 35:52.200
I get the sense that, you know, vision folks think that everything will be solved through

35:52.200 --> 35:57.480
cameras, as opposed to, you know, point clouds and things like that.

35:57.480 --> 36:04.080
Well, I would say, in a way, if you think about getting the cameras, they, in a sense,

36:04.080 --> 36:11.720
are also the cameras that give you 2D data and you can transform it into 3D point cloud.

36:11.720 --> 36:17.560
I think the sort of complementary, I think 3D can come from images.

36:17.560 --> 36:23.120
So in a way, you could say that you're using images, if you're, I guess, working mainly

36:23.120 --> 36:25.160
with RGBD data.

36:25.160 --> 36:32.320
In terms of lighter, I think, I guess there is a use for longer range sensors such as

36:32.320 --> 36:33.320
sliders.

36:33.320 --> 36:37.240
So they can allow you to detect objects that are further apart.

36:37.240 --> 36:44.960
And I, you can also, you can try to use reconstruction algorithms, but they're not always as effective

36:44.960 --> 36:50.960
as the more longer, as longer range sensors are in terms of data collection.

36:50.960 --> 36:58.000
So I think both will have a role to play in terms of solving these problems and maybe

36:58.000 --> 37:02.720
in different applications, images would be better than long-range sensors.

37:02.720 --> 37:03.720
Okay.

37:03.720 --> 37:04.720
Yeah.

37:04.720 --> 37:05.720
Okay.

37:05.720 --> 37:07.200
Paulin, thank you so much.

37:07.200 --> 37:09.120
It's been a really interesting conversation.

37:09.120 --> 37:13.480
Any final words or places that you'd like to point folks to or?

37:13.480 --> 37:19.680
I guess if you want to follow my work, go and check out the website, the project that I

37:19.680 --> 37:23.960
worked on is called setcloud, 3D semantic segmentation.

37:23.960 --> 37:24.960
Okay.

37:24.960 --> 37:25.960
Yeah.

37:25.960 --> 37:26.960
Great.

37:26.960 --> 37:28.960
And we'll include a link to that in the show notes.

37:28.960 --> 37:29.960
All right.

37:29.960 --> 37:30.960
Thank you.

37:30.960 --> 37:31.960
All right.

37:31.960 --> 37:32.960
Thank you.

37:32.960 --> 37:33.960
Thank you.

37:33.960 --> 37:34.960
All right.

37:34.960 --> 37:37.640
All right, everyone, that's our show for today.

37:37.640 --> 37:42.600
For more information on Lynn or any of the topics covered in this episode, you'll find

37:42.600 --> 37:48.200
the show notes at twomolai.com slash talk slash one, two, three.

37:48.200 --> 37:52.920
As you know, we love to hear your questions and feedback on the show, so don't hesitate

37:52.920 --> 37:54.520
to comment there.

37:54.520 --> 38:05.160
Thanks so much for listening and catch you next time.

