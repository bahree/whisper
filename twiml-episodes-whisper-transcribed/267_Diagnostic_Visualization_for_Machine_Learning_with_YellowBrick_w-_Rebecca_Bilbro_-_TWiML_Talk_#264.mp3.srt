1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,160
I'm your host Sam Charrington.

4
00:00:31,160 --> 00:00:35,280
This week on the podcast we're featuring a series of shows that highlight just a few of

5
00:00:35,280 --> 00:00:39,800
the great innovations and innovators at the intersection of three very important and

6
00:00:39,800 --> 00:00:46,280
familiar topics, data science, the Python programming language and open source software.

7
00:00:46,280 --> 00:00:49,960
To better understand our listeners' views on the importance of open source and the projects

8
00:00:49,960 --> 00:00:54,520
and players in this space, I'm conducting a survey, which I'd be very grateful if you

9
00:00:54,520 --> 00:00:57,040
took a moment to complete.

10
00:00:57,040 --> 00:01:01,760
To access the survey, visit Twimbleai.com slash Python survey.

11
00:01:01,760 --> 00:01:09,800
Please hit pause now and we'll wait for you to get back.

12
00:01:09,800 --> 00:01:15,880
That's twimbleai.com slash Python survey.

13
00:01:15,880 --> 00:01:19,840
Before we dive into the show, I'd like to send a huge thanks to our sponsor for this

14
00:01:19,840 --> 00:01:22,200
series IBM.

15
00:01:22,200 --> 00:01:26,720
Speaking of open source, IBM has a long history of engaging in and supporting open source

16
00:01:26,720 --> 00:01:32,160
projects that are important to enterprise data science, projects like Hadoop, Spark,

17
00:01:32,160 --> 00:01:35,680
Jupiter and Cubeflow to name just a few.

18
00:01:35,680 --> 00:01:41,640
IBM also hosts the IBM data science community, which is a place for enterprise data scientists

19
00:01:41,640 --> 00:01:47,320
looking to learn, share and engage with their peers and industry renowned practitioners.

20
00:01:47,320 --> 00:01:52,440
Here you'll find informative tutorials and case studies, Q&As with leaders in the field

21
00:01:52,440 --> 00:01:57,800
and a lively forum covering a variety of topics of interest to both beginning and experience

22
00:01:57,800 --> 00:01:59,480
data scientists.

23
00:01:59,480 --> 00:02:05,160
Check out and join the IBM data science community by visiting IBM.com slash community slash

24
00:02:05,160 --> 00:02:06,160
data science.

25
00:02:06,160 --> 00:02:11,800
All right, everyone, I am on the line with Rebecca Billbro.

26
00:02:11,800 --> 00:02:17,120
Rebecca is the head of data science at ICX Media and co-creator of Yellowbrick.

27
00:02:17,120 --> 00:02:20,400
Rebecca, welcome to this weekend machine learning and AI.

28
00:02:20,400 --> 00:02:21,400
Thanks for having me, Sam.

29
00:02:21,400 --> 00:02:23,400
I'm excited to be here.

30
00:02:23,400 --> 00:02:26,960
I am excited to dive into our conversation as well.

31
00:02:26,960 --> 00:02:29,240
This should be an interesting one.

32
00:02:29,240 --> 00:02:36,760
Yellowbrick is a library focused on visualization and podcasts are not the best medium for conversations

33
00:02:36,760 --> 00:02:41,600
that rely heavily on visualization, but I think we will do okay.

34
00:02:41,600 --> 00:02:44,920
Why don't we get started by having you tell us a little bit about your background and

35
00:02:44,920 --> 00:02:51,000
how you got involved in data science and what led you to create Yellowbrick?

36
00:02:51,000 --> 00:02:54,200
Well, I am a practicing data scientist.

37
00:02:54,200 --> 00:03:00,520
I specialize in applied machine learning applications and in natural language processing

38
00:03:00,520 --> 00:03:02,960
usually specifically.

39
00:03:02,960 --> 00:03:08,480
Like you said, I currently lead the data science team at a company called ICX Media.

40
00:03:08,480 --> 00:03:13,960
So ICX Media is a social video intelligence company here in Washington, DC, which is

41
00:03:13,960 --> 00:03:16,000
where I live.

42
00:03:16,000 --> 00:03:21,760
And our focus is on using machine learning and natural language processing and data science

43
00:03:21,760 --> 00:03:26,240
to understand what different audiences are currently watching and what they're talking

44
00:03:26,240 --> 00:03:27,240
about.

45
00:03:27,240 --> 00:03:32,840
And we use that to sort of spot trends and let our clients know how basically how to make

46
00:03:32,840 --> 00:03:36,440
more of the kind of stuff that people are going to be excited to watch.

47
00:03:36,440 --> 00:03:41,000
Well, we don't need natural language processing and machine learning to know what people

48
00:03:41,000 --> 00:03:43,240
are watching and talking about today.

49
00:03:43,240 --> 00:03:45,480
Well, that's right.

50
00:03:45,480 --> 00:03:51,360
So yes, currently there's a lot of dialogue about end game and game of thrones, but we're

51
00:03:51,360 --> 00:03:56,520
not going to talk about it because some people maybe are not up to speed and we don't want

52
00:03:56,520 --> 00:04:01,000
any spoilers.

53
00:04:01,000 --> 00:04:03,880
But you asked sort of how I got into data science.

54
00:04:03,880 --> 00:04:07,560
I always love, you know, I love this part of the podcast.

55
00:04:07,560 --> 00:04:11,120
It's so fun to hear kind of how everybody got here.

56
00:04:11,120 --> 00:04:17,280
There's a million different stories of how people landed in this field.

57
00:04:17,280 --> 00:04:24,120
And so for me, I've always worked in the space between I would say like natural languages

58
00:04:24,120 --> 00:04:26,720
and formal languages.

59
00:04:26,720 --> 00:04:29,680
So as an undergrad, I was a double major.

60
00:04:29,680 --> 00:04:33,240
I'm double majored in mathematics and English.

61
00:04:33,240 --> 00:04:38,800
And then when I went to graduate school, I decided to study communication and visualization

62
00:04:38,800 --> 00:04:41,480
practices in engineering.

63
00:04:41,480 --> 00:04:48,600
And so after I finished my PhD, I came to DC and that was in 2011.

64
00:04:48,600 --> 00:04:54,720
And so this was, I joined the federal service as a presidential management fellow.

65
00:04:54,720 --> 00:05:02,080
And so this was during the Obama administration, they were putting a lot of kind of focus on,

66
00:05:02,080 --> 00:05:08,320
you know, analytics and kind of more data driven strategies inside the government.

67
00:05:08,320 --> 00:05:13,920
And this was just around the time that data science had started really happening.

68
00:05:13,920 --> 00:05:18,960
And so my job was basically analyzing like very, very messy data.

69
00:05:18,960 --> 00:05:21,720
But I didn't have access to a lot of tools.

70
00:05:21,720 --> 00:05:26,600
I mean, I think a lot of public servants can appreciate that, you know, the access to

71
00:05:26,600 --> 00:05:27,920
tools is somewhat limited.

72
00:05:27,920 --> 00:05:34,080
But in particular, I had a real hard time accessing tools to allow for like wrangling and modeling

73
00:05:34,080 --> 00:05:35,320
unstructured data.

74
00:05:35,320 --> 00:05:40,400
Okay. So I was working with a lot of like logs and reports and manifests and news stories

75
00:05:40,400 --> 00:05:42,800
and speeches.

76
00:05:42,800 --> 00:05:45,080
And there really just wasn't a lot of tooling out there.

77
00:05:45,080 --> 00:05:47,960
And so I basically, I taught myself Python.

78
00:05:47,960 --> 00:05:50,160
And I started building my own tools.

79
00:05:50,160 --> 00:05:53,120
And eventually that was my full time job.

80
00:05:53,120 --> 00:05:57,280
And I realized I could kind of take that and make that my full time job.

81
00:05:57,280 --> 00:06:01,440
And I started teaching data science and teaching machine learning because I realized there were

82
00:06:01,440 --> 00:06:05,640
a lot of other people who were kind of in the same boat and had a lot of the same kind

83
00:06:05,640 --> 00:06:08,200
of needs that I that I had encountered.

84
00:06:08,200 --> 00:06:15,280
Are you able to isolate what it is that prompted you to take the additional step to start building

85
00:06:15,280 --> 00:06:16,760
your own tools?

86
00:06:16,760 --> 00:06:20,720
Like I talked to a lot of people on the podcast that learn a programming language because

87
00:06:20,720 --> 00:06:22,680
they have some problem.

88
00:06:22,680 --> 00:06:26,600
But often, you know, you learn a programming language and the tool set that it offers

89
00:06:26,600 --> 00:06:31,840
you, but then building your own tools and then further kind of publishing those and supporting

90
00:06:31,840 --> 00:06:32,840
those for others.

91
00:06:32,840 --> 00:06:35,080
Like what is it that even told you that you could do that?

92
00:06:35,080 --> 00:06:37,680
Like what got you down that path?

93
00:06:37,680 --> 00:06:41,120
Oh, that's that's a great question.

94
00:06:41,120 --> 00:06:46,040
So I guess it would be, you know, one, one, one answer that I could give is that, you

95
00:06:46,040 --> 00:06:50,760
know, I was already in public service, you know, when I started getting started doing

96
00:06:50,760 --> 00:06:51,760
this.

97
00:06:51,760 --> 00:06:58,960
I already kind of had the sense that what I was doing was for more than just me, that

98
00:06:58,960 --> 00:07:04,200
it was for some greater good, but I think that really the longer that I've been working

99
00:07:04,200 --> 00:07:09,200
in open source software and people who are, like you said, kind of building these tools

100
00:07:09,200 --> 00:07:13,800
and putting them out there into the world, I would say that open source is a lot less like

101
00:07:13,800 --> 00:07:20,680
public service and more like activism or maybe for a lot of people, it's more like art,

102
00:07:20,680 --> 00:07:25,960
you know, that people are making the change that they want to see in the world.

103
00:07:25,960 --> 00:07:37,800
And it's the sort of wild and organic space and it's just fun, you know, it's sort of,

104
00:07:37,800 --> 00:07:38,800
it's fun.

105
00:07:38,800 --> 00:07:44,280
And once you sort of start doing it, it feels like, you know, it feels like the right

106
00:07:44,280 --> 00:07:46,400
way to do software.

107
00:07:46,400 --> 00:07:47,400
Awesome.

108
00:07:47,400 --> 00:07:49,440
So what is yellow brick?

109
00:07:49,440 --> 00:07:54,400
Maybe it might help if I sort of give like a little bit of a history of like why we made

110
00:07:54,400 --> 00:07:55,400
yellow brick.

111
00:07:55,400 --> 00:08:01,840
So it was, it really has to do with actually moving to trying to do data science and moving

112
00:08:01,840 --> 00:08:04,880
to try to teach data science.

113
00:08:04,880 --> 00:08:10,280
And when I started teaching, I realized like how poorly I was able to describe why I was

114
00:08:10,280 --> 00:08:14,160
making certain decisions, particularly around the machine learning workflow.

115
00:08:14,160 --> 00:08:18,160
And you know, it's like there's that, you know, Richard Feynman quote, like if you, if

116
00:08:18,160 --> 00:08:21,600
you can't explain it to your students or to your kid, you don't really understand it

117
00:08:21,600 --> 00:08:22,600
yet.

118
00:08:22,600 --> 00:08:24,120
So that was sort of like a moment for me.

119
00:08:24,120 --> 00:08:28,960
I realized I was, I was thinking about this, let I was talking about this problem a lot

120
00:08:28,960 --> 00:08:34,360
with another one of the machine learning faculty that I teach with, who's now my creative

121
00:08:34,360 --> 00:08:37,400
partner, Benjamin Bangfort.

122
00:08:37,400 --> 00:08:42,440
And essentially what we realized is like what we needed was a way to answer our students'

123
00:08:42,440 --> 00:08:45,840
questions, the way we would answer them for ourselves.

124
00:08:45,840 --> 00:08:50,600
And the way that we were answering them for ourselves was with plots, with plotting routines.

125
00:08:50,600 --> 00:08:55,200
And so, and when I say the questions, I mean like the questions the students ask are things

126
00:08:55,200 --> 00:08:58,520
like, you know, which features do I need to build this model?

127
00:08:58,520 --> 00:09:01,080
Which ones should I remove?

128
00:09:01,080 --> 00:09:03,080
You know, how do I know which model to pick?

129
00:09:03,080 --> 00:09:05,120
Why is that the right model?

130
00:09:05,120 --> 00:09:09,840
You know, why isn't my model doing a good job at modeling this data?

131
00:09:09,840 --> 00:09:13,920
And so that's really why we decided to start working on yellow brick and so Ben and

132
00:09:13,920 --> 00:09:20,720
I essentially we started this, this project, sort of in the beginning for our students.

133
00:09:20,720 --> 00:09:25,240
But now it's sort of expanded out into being this tool that we see as being useful, not

134
00:09:25,240 --> 00:09:30,840
just for students, but for engineers and for professional data scientists, who need

135
00:09:30,840 --> 00:09:38,000
a way to diagnose and visualize how data is being fit and transformed throughout the entire

136
00:09:38,000 --> 00:09:41,160
machine learning process and to end.

137
00:09:41,160 --> 00:09:45,800
So it's a pure Python project, it's fully open source, it's licensed under the Apache

138
00:09:45,800 --> 00:09:50,000
two license, you can find all of the source on GitHub.

139
00:09:50,000 --> 00:09:51,960
But yeah, it's a, I can say more about it.

140
00:09:51,960 --> 00:09:56,200
So it's, you know, it has an object oriented API.

141
00:09:56,200 --> 00:10:02,400
If you are familiar with the scikit learn API, so scikit learn is a very popular Python

142
00:10:02,400 --> 00:10:05,800
machine learning library that's open source.

143
00:10:05,800 --> 00:10:12,120
So the scikit learn API has this notion of estimators and transformers that can either

144
00:10:12,120 --> 00:10:15,520
change data or model data.

145
00:10:15,520 --> 00:10:20,480
And so our API sort of wraps that API with common visual diagnostic plotting routines

146
00:10:20,480 --> 00:10:24,000
that leverage pure Matplotlib.

147
00:10:24,000 --> 00:10:29,040
So that is kind of basically in a nutshell what yellow brick is.

148
00:10:29,040 --> 00:10:35,800
And so you reference that you you're using Matplotlib under the covers, what is yellow brick

149
00:10:35,800 --> 00:10:41,840
offering that you can't do with Matplotlib is it kind of like pre configured templates

150
00:10:41,840 --> 00:10:46,960
or way, you know, visualizations, you also mentioned in their diagnostic visualizations

151
00:10:46,960 --> 00:10:50,040
as opposed to other types of visualizations.

152
00:10:50,040 --> 00:10:52,360
Can you maybe elaborate on on all that?

153
00:10:52,360 --> 00:10:53,360
Sure.

154
00:10:53,360 --> 00:10:59,000
So to the question about what we offer on top of Matplotlib, it really is.

155
00:10:59,000 --> 00:11:08,480
About capturing those routines that are kind of become best practices, I think, for people

156
00:11:08,480 --> 00:11:13,160
who do machine learning kind of professionally or for their research.

157
00:11:13,160 --> 00:11:18,920
So you know that you like to use residuals plots to diagnose certain kinds of error when

158
00:11:18,920 --> 00:11:19,920
you're doing regressions.

159
00:11:19,920 --> 00:11:26,880
Or you know that you like to use confusion matrices to know which of your classes you're

160
00:11:26,880 --> 00:11:30,040
kind of failing to classify.

161
00:11:30,040 --> 00:11:37,600
And so really all yellow brick does is it sort of says, okay, well, in order to do a you

162
00:11:37,600 --> 00:11:43,720
know residuals plot, you need like these 50 or 60 lines of custom Matplotlib code or in

163
00:11:43,720 --> 00:11:47,760
order to do a confusion matrix, you need these 50 or 60 custom lines of Matplotlib code.

164
00:11:47,760 --> 00:11:52,440
And what we do is we wrap those into like an object.

165
00:11:52,440 --> 00:11:59,560
And so instead of doing, you know, 50 lines of custom code each time, you just import

166
00:11:59,560 --> 00:12:05,080
the object from, you know, you import the class from yellow brick and you create, you

167
00:12:05,080 --> 00:12:10,480
instantiate an object that does, you know, it executes that work for you inside an object

168
00:12:10,480 --> 00:12:11,480
oriented interface.

169
00:12:11,480 --> 00:12:16,960
And so it learns from the data and then it tells you kind of what it learned using a plot.

170
00:12:16,960 --> 00:12:18,760
Nice.

171
00:12:18,760 --> 00:12:24,800
And I would really encourage folks to take a look at the gallery that you've got in the

172
00:12:24,800 --> 00:12:25,800
documentation.

173
00:12:25,800 --> 00:12:32,320
There are tons of different types of charts and graphs here, everything from feature

174
00:12:32,320 --> 00:12:39,200
analysis to regression visual visualizers, classification visualizers, clustering, model

175
00:12:39,200 --> 00:12:45,960
selection, text modeling, decision boundaries, and target visualizers.

176
00:12:45,960 --> 00:12:53,680
Are there any particular of these tools that get the most use or kind of the bread and

177
00:12:53,680 --> 00:12:56,320
butter for yellow brick users?

178
00:12:56,320 --> 00:12:59,080
Oh, that's a fascinating question.

179
00:12:59,080 --> 00:13:03,280
Part of me kind of wishes I knew more about how people are using it.

180
00:13:03,280 --> 00:13:08,120
You know, one of the ways that we kind of try to keep a beat on this is just by like looking

181
00:13:08,120 --> 00:13:12,040
at the blog posts that people are publishing.

182
00:13:12,040 --> 00:13:14,640
You know, what people are using in those posts.

183
00:13:14,640 --> 00:13:21,160
It does seem like the regression and classification visualizers are incredibly popular.

184
00:13:21,160 --> 00:13:26,200
I think it's because, you know, those are two very common forms of, you know, supervised

185
00:13:26,200 --> 00:13:28,120
learning is very common.

186
00:13:28,120 --> 00:13:35,680
And so using things like classification heat maps or, you know, prediction error plots

187
00:13:35,680 --> 00:13:38,840
or residuals plots are very, very popular.

188
00:13:38,840 --> 00:13:41,080
We see those a lot in blog posts.

189
00:13:41,080 --> 00:13:46,440
There was just a blog post that went out where somebody was using our stochastic neighbor

190
00:13:46,440 --> 00:13:53,440
embedding plot, which is, which I really like to use when I'm doing a document modeling,

191
00:13:53,440 --> 00:13:54,440
right?

192
00:13:54,440 --> 00:13:58,440
So when I'm trying to understand a corpus and look at the different kinds of documents

193
00:13:58,440 --> 00:14:00,240
in that corpus.

194
00:14:00,240 --> 00:14:05,600
And so there's, you know, there seems like people are using kind of the whole gamut.

195
00:14:05,600 --> 00:14:09,720
And then the other way that we sort of listen is just by looking in the issues, you know,

196
00:14:09,720 --> 00:14:16,760
for future requests and bug reports to know where people are using different things and

197
00:14:16,760 --> 00:14:21,760
how they're using them and what they're encountering as they're trying to use these different

198
00:14:21,760 --> 00:14:24,040
kinds of visualizers.

199
00:14:24,040 --> 00:14:26,560
Are there any particular ones?

200
00:14:26,560 --> 00:14:33,000
Is there like a kind of lesser known hero among the different plots that you can do

201
00:14:33,000 --> 00:14:34,000
that?

202
00:14:34,000 --> 00:14:39,840
Yeah, I would have, you know, so I mentioned that I do a lot of NLP at my work and just kind

203
00:14:39,840 --> 00:14:43,800
of in general is sort of, that's my sweet spot in terms of what I like to do.

204
00:14:43,800 --> 00:14:50,400
And so as you might expect, there are, you know, a range of interesting text visualizers

205
00:14:50,400 --> 00:14:58,000
that you might be surprised to see just because they sort of, you know, they happen because

206
00:14:58,000 --> 00:15:04,440
Benjamin and I, my, at the other core maintainer, the other co-creator and I both do a lot

207
00:15:04,440 --> 00:15:06,040
of natural language processing.

208
00:15:06,040 --> 00:15:10,760
And so we've kind of evolved a lot of these sort of interesting different ways of thinking

209
00:15:10,760 --> 00:15:16,520
about text and how what might help people to model text more easily.

210
00:15:16,520 --> 00:15:20,320
And so stochastic neighbor embedding is a common one.

211
00:15:20,320 --> 00:15:24,360
So that's, you know, it's like TSNE, TSNE plots.

212
00:15:24,360 --> 00:15:29,240
So that's one that we have, we also have ones that people will probably recognize from

213
00:15:29,240 --> 00:15:31,840
another library called NLTK.

214
00:15:31,840 --> 00:15:37,800
So the natural language toolkit is kind of one of the first really robust Python libraries

215
00:15:37,800 --> 00:15:39,440
for doing natural language processing.

216
00:15:39,440 --> 00:15:46,480
So we have a visualizer that does like frequency distributions of words or tokens.

217
00:15:46,480 --> 00:15:50,400
But we also have a few things that you might not see in other libraries.

218
00:15:50,400 --> 00:15:54,840
So we have something called a dispersion plot.

219
00:15:54,840 --> 00:15:59,600
And so what a dispersion plot lets you do is see the incidence of a certain word across

220
00:15:59,600 --> 00:16:05,400
a corpus, which can be really useful for knowing it's sort of like a way of doing feature

221
00:16:05,400 --> 00:16:11,760
analysis for when your features are words and tokens.

222
00:16:11,760 --> 00:16:15,560
There's another one called a part of speech tag visualizer.

223
00:16:15,560 --> 00:16:19,160
This is when, so I'm sort of selfishly maybe talking about this one because it's one

224
00:16:19,160 --> 00:16:22,080
that I worked on recently.

225
00:16:22,080 --> 00:16:27,120
But I think a lot about, you know, what kind of document is this and how can I guess

226
00:16:27,120 --> 00:16:31,680
what kind of document this is without having to read every single document in my corpus.

227
00:16:31,680 --> 00:16:36,040
And it turns out that one really kind of easy way to guess what kind of kind of document

228
00:16:36,040 --> 00:16:39,920
it is is to just count the number of certain parts of speech.

229
00:16:39,920 --> 00:16:43,720
So like how many adjectives are there, how many nouns are there, you know, how many

230
00:16:43,720 --> 00:16:44,800
verbs are there.

231
00:16:44,800 --> 00:16:51,440
And so our part of speech tag visualizer is a way that you can like scan over a corpus

232
00:16:51,440 --> 00:16:55,080
and look at the proportion of different types of parts of speech.

233
00:16:55,080 --> 00:16:59,880
And then you can compare different slices of the corpus or different classes from the

234
00:16:59,880 --> 00:17:05,440
corpus and look at how much it varies, right, because our practices for using certain

235
00:17:05,440 --> 00:17:09,320
kinds of words are really, really different when we're talking about blogging and when

236
00:17:09,320 --> 00:17:13,120
we're talking about writing news articles when we're giving speeches.

237
00:17:13,120 --> 00:17:17,440
And so you can see really distinct variations in the ratios of different parts of speech.

238
00:17:17,440 --> 00:17:24,360
Can you give some examples of, you know, different ratios and what they signal in terms of

239
00:17:24,360 --> 00:17:25,680
the type of document?

240
00:17:25,680 --> 00:17:27,760
Oh, that's a good question.

241
00:17:27,760 --> 00:17:34,440
Yeah, I mean, I guess there's that kind of like, you know, when you're writing the news,

242
00:17:34,440 --> 00:17:38,160
you're supposed to not use any adjectives.

243
00:17:38,160 --> 00:17:40,440
And so that's kind of an easy example, right?

244
00:17:40,440 --> 00:17:45,960
So if you're scanning over the text and you do a part of speech tag visualization, let's

245
00:17:45,960 --> 00:17:54,840
see, let's say, and your proportion of adjectives is extremely low.

246
00:17:54,840 --> 00:18:00,960
There's a actually a pretty good chance that it's a news article.

247
00:18:00,960 --> 00:18:06,640
Whereas for instance, if you have, you know, a lot of adjectives, you know, you might

248
00:18:06,640 --> 00:18:14,200
be looking at something that's more akin to like poetry or music, like a song lyric,

249
00:18:14,200 --> 00:18:17,720
because adjectives are a lot more common.

250
00:18:17,720 --> 00:18:21,760
You know, so things kind of like that, I don't know if that helps to paint the picture

251
00:18:21,760 --> 00:18:22,760
a little bit better.

252
00:18:22,760 --> 00:18:23,760
It does help.

253
00:18:23,760 --> 00:18:28,160
I'm not able to easily come up with a lot of different classes that this would easily

254
00:18:28,160 --> 00:18:29,160
allow me.

255
00:18:29,160 --> 00:18:33,880
It seems like the kind of thing that, and maybe this is, this is like everything else,

256
00:18:33,880 --> 00:18:39,440
but you'd have to see a lot of examples to get an intuition for kind of how this might

257
00:18:39,440 --> 00:18:47,560
translate to a particular class of documents, and maybe it doesn't generalize all that

258
00:18:47,560 --> 00:18:48,560
well.

259
00:18:48,560 --> 00:18:49,560
That's fair.

260
00:18:49,560 --> 00:18:50,560
I think, you know, so you did.

261
00:18:50,560 --> 00:18:51,560
And that's not a critique.

262
00:18:51,560 --> 00:18:59,960
I'm just, it's very interesting that I'm intrigued that you can learn anything from

263
00:18:59,960 --> 00:19:01,760
looking at this consistently.

264
00:19:01,760 --> 00:19:02,760
Yeah.

265
00:19:02,760 --> 00:19:04,040
An example is a great one.

266
00:19:04,040 --> 00:19:09,240
I can definitely see that, and I'm just curious about others.

267
00:19:09,240 --> 00:19:13,760
It's a very interesting analysis to apply to a corpus.

268
00:19:13,760 --> 00:19:15,080
So here's another example.

269
00:19:15,080 --> 00:19:20,280
So I sort of think of another example, which is that a lot of times with students, one

270
00:19:20,280 --> 00:19:24,080
of the things that they'll try to do is they'll try to do a regression.

271
00:19:24,080 --> 00:19:28,880
You know, so let's say, you know, they have a bunch of data where each sample is a sample

272
00:19:28,880 --> 00:19:34,440
of concrete, and what we're trying to predict is how long the concrete is going to last

273
00:19:34,440 --> 00:19:39,960
given, you know, its components, you know, how much of cement isn't it, how much ash,

274
00:19:39,960 --> 00:19:41,680
how much water, et cetera, isn't it?

275
00:19:41,680 --> 00:19:47,680
So let's say they're trying to regress this data.

276
00:19:47,680 --> 00:19:52,560
And what they find is that their regression scores are really, really bad.

277
00:19:52,560 --> 00:19:58,400
And so one of the strategies that we suggest is like, well, why don't you bend the values

278
00:19:58,400 --> 00:20:01,960
into categories and then try to do classification?

279
00:20:01,960 --> 00:20:05,440
So let's say we'll, you know, we'll look at all of the samples that, you know, lasted

280
00:20:05,440 --> 00:20:10,200
between zero and five years, all the samples, the lasted between five and ten, but a lot

281
00:20:10,200 --> 00:20:16,720
of times when you pick, when you sort of manually pick those bins, you end up with really

282
00:20:16,720 --> 00:20:18,840
bad class and balance problems.

283
00:20:18,840 --> 00:20:23,640
Class and balance is like the surefire way to end up with, you know, terrible classifiers.

284
00:20:23,640 --> 00:20:28,320
And so we actually have a visualizer called balance binning.

285
00:20:28,320 --> 00:20:34,640
And what it does is it recommends bins when you are kind of trying to turn a regression

286
00:20:34,640 --> 00:20:39,400
problem that's, that's not successful into a successful classification problem.

287
00:20:39,400 --> 00:20:44,880
Is the idea that you would use the parts of speech, visualizer in conjunction with the

288
00:20:44,880 --> 00:20:50,160
binner to allow you to classify documents based on their parts of speech distribution?

289
00:20:50,160 --> 00:20:51,160
Yeah.

290
00:20:51,160 --> 00:20:54,160
So I guess I was, so for balance binning, I was just thinking about any regression

291
00:20:54,160 --> 00:20:58,800
problem that you would want to convert into a classification problem, but I can't imagine

292
00:20:58,800 --> 00:21:06,440
like a case where let's say we're looking at Yelp reviews.

293
00:21:06,440 --> 00:21:15,560
And let's say, you know, we were, you know, trying to regress a score, like an exact score

294
00:21:15,560 --> 00:21:17,520
for review is really hard.

295
00:21:17,520 --> 00:21:25,360
So trying to predict like this is exactly a 4.32 or this is like a 2.17 restaurant is

296
00:21:25,360 --> 00:21:26,560
really hard.

297
00:21:26,560 --> 00:21:32,080
But when you bin things by stars, like, you know, this is a one star, this is a two star,

298
00:21:32,080 --> 00:21:35,480
this is a three star, you can sort of get there a lot faster.

299
00:21:35,480 --> 00:21:40,600
So if you don't already have those bins, you could kind of use balance binning to help

300
00:21:40,600 --> 00:21:41,920
you decide on the bins.

301
00:21:41,920 --> 00:21:46,880
And then you could use something like part of speech tags to see if the parts of speech

302
00:21:46,880 --> 00:21:54,120
are a good way to differentiate positive reviews from negative reviews, let's say.

303
00:21:54,120 --> 00:22:03,480
Yeah, I can envision something like fake reviews, use more adjectives, for example, than

304
00:22:03,480 --> 00:22:04,960
real reviews or something like that.

305
00:22:04,960 --> 00:22:09,320
I think that's a very good hypothesis.

306
00:22:09,320 --> 00:22:10,720
Or more exclamation marks.

307
00:22:10,720 --> 00:22:12,760
Is exclamation mark a part of speech?

308
00:22:12,760 --> 00:22:13,760
Yes.

309
00:22:13,760 --> 00:22:17,640
Yeah, punctuation is included in that.

310
00:22:17,640 --> 00:22:21,800
This was originally built as a teaching tool.

311
00:22:21,800 --> 00:22:29,800
Can you maybe talk through some of the ways that you use it or have seen it use in real

312
00:22:29,800 --> 00:22:30,800
world?

313
00:22:30,800 --> 00:22:32,680
And we've kind of talked through a bunch of those, but I'm wondering if you can walk

314
00:22:32,680 --> 00:22:40,680
through and maybe more depth, kind of a workflow or use case that you're familiar with.

315
00:22:40,680 --> 00:22:41,680
Sure.

316
00:22:41,680 --> 00:22:48,440
I think that first, I would sort of say, I think a big part of how I think about the machine

317
00:22:48,440 --> 00:22:54,040
learning process is by using this expression, the model selection triple, which comes

318
00:22:54,040 --> 00:23:02,840
from a paper by Arun Kumar, which is actually a paper on sort of next generation databases

319
00:23:02,840 --> 00:23:05,440
that anticipate machine learning from the start.

320
00:23:05,440 --> 00:23:10,720
But he talks about the model selection triple as basically, this is what the machine learning

321
00:23:10,720 --> 00:23:12,520
workflow sort of boils down to.

322
00:23:12,520 --> 00:23:18,000
You do feature analysis, you do algorithm selection, and then you do hyperparameter tuning.

323
00:23:18,000 --> 00:23:22,320
And you might have to do a triple, like many triples, to get to the optimal model, but

324
00:23:22,320 --> 00:23:25,160
it generally follows that flow.

325
00:23:25,160 --> 00:23:28,400
And so what you've got then is a search problem, right?

326
00:23:28,400 --> 00:23:33,760
So you're searching, you've got this big grid of all of the possible features, all of

327
00:23:33,760 --> 00:23:39,000
the possible algorithms, all of the possible hyperparameters, and you're trying to find

328
00:23:39,000 --> 00:23:43,040
the best place in that search.

329
00:23:43,040 --> 00:23:50,080
And so what students generally do is they just like Google examples, and they kind of copy

330
00:23:50,080 --> 00:23:51,080
and paste.

331
00:23:51,080 --> 00:23:57,240
But frequently, I think people in a production context, right?

332
00:23:57,240 --> 00:24:04,200
So if you're in a company, you maybe are just buying machine learning as a service from

333
00:24:04,200 --> 00:24:05,200
somebody else, right?

334
00:24:05,200 --> 00:24:11,080
So you're paying some company to a black box kind of company, you give them your data,

335
00:24:11,080 --> 00:24:16,360
they kind of do something magical, they pass you back a model, and maybe you try to deploy

336
00:24:16,360 --> 00:24:17,360
that.

337
00:24:17,360 --> 00:24:23,480
But I think that what I am seeing is that people are starting to become dissatisfied with

338
00:24:23,480 --> 00:24:25,400
those black box models.

339
00:24:25,400 --> 00:24:29,520
They can't tune them, they don't understand them, they don't have a lot of insight into

340
00:24:29,520 --> 00:24:31,520
how they work.

341
00:24:31,520 --> 00:24:36,120
And there's ethical concerns with how is this made?

342
00:24:36,120 --> 00:24:41,040
How can we answer these questions about how this was made from our users, let's say?

343
00:24:41,040 --> 00:24:46,080
And so what I'm seeing is that people are starting to adopt yellow brick to do steering

344
00:24:46,080 --> 00:24:52,080
of the model selection triple, and kind of like taking that process back in-house so that

345
00:24:52,080 --> 00:25:00,800
they can do it in-house rather than kind of delegating that or paying for that service

346
00:25:00,800 --> 00:25:01,800
from somebody else.

347
00:25:01,800 --> 00:25:09,200
I'm curious, are you seeing folks that are adopting yellow brick as a path towards

348
00:25:09,200 --> 00:25:15,280
more explainable models or understanding the models better, you were just saying that?

349
00:25:15,280 --> 00:25:25,480
Yes, yes, so I think that it is a very powerful tool for model interpretation and explainability.

350
00:25:25,480 --> 00:25:32,840
It is not really designed for non-technical consumption, right?

351
00:25:32,840 --> 00:25:38,920
So it's not really designed to kind of produce plots that you could just drop into a business

352
00:25:38,920 --> 00:25:44,840
presentation and give to people who aren't already familiar with the process, but they

353
00:25:44,840 --> 00:25:54,840
are a very good way to simulate kind of more directed decision making for your data scientists

354
00:25:54,840 --> 00:26:00,920
so you can, you know, if your data scientists are using yellow brick to do feature analysis

355
00:26:00,920 --> 00:26:06,840
and feature engineering model selection and hyperparameter tuning, you know that they are

356
00:26:06,840 --> 00:26:09,840
doing this, you know, it's not a random walk, right?

357
00:26:09,840 --> 00:26:16,640
They're not just picking things randomly, they're doing a directed, focused search, they're

358
00:26:16,640 --> 00:26:18,480
not just using grid search, right?

359
00:26:18,480 --> 00:26:22,800
So they're thinking about the choices they're making and because they're thinking about

360
00:26:22,800 --> 00:26:28,440
the choices they're making and they have these artifacts of those choices, you can then

361
00:26:28,440 --> 00:26:33,240
trace back through those choices and say, this is why we ended up with these features

362
00:26:33,240 --> 00:26:37,880
and this is why we ended up with these models and you can kind of, you can tell that story

363
00:26:37,880 --> 00:26:43,400
with these plots that have been generated in the course of modeling, you know, these visual

364
00:26:43,400 --> 00:26:49,640
artifacts can be used, they can be used to explain how a model is working to the internal

365
00:26:49,640 --> 00:26:55,920
engineering team and so that level of explainability is oftentimes a blocker to deploying models,

366
00:26:55,920 --> 00:26:56,920
right?

367
00:26:56,920 --> 00:27:01,640
Because the data scientists know how to make models but they don't understand maybe very

368
00:27:01,640 --> 00:27:06,760
much about software engineering or deployment and the software engineers understand deployment

369
00:27:06,760 --> 00:27:14,360
but they don't understand kind of, you know, how to, for instance, how to test non-deterministic

370
00:27:14,360 --> 00:27:21,320
functions, they don't understand, you know, how to parameterize things so that they will

371
00:27:21,320 --> 00:27:28,040
work in a production context but yeah, you can use yellow brick for logging, for visual

372
00:27:28,040 --> 00:27:35,080
logging, once you've deployed a model, you can use yellow brick to generate plots on

373
00:27:35,080 --> 00:27:41,080
a regular basis to look for things like model decay, you know, if all of a sudden the

374
00:27:41,080 --> 00:27:46,760
classifier starts to perform poorly, that will show up in the plots and the engineers

375
00:27:46,760 --> 00:27:54,000
can know what to look for and that can be very powerful for explainability and interpretability.

376
00:27:54,000 --> 00:28:00,040
And so as visual logging in this use case where you're tracking model decay, is that are

377
00:28:00,040 --> 00:28:07,160
there specific yellow brick plots designed for that or are you just using yellow brick

378
00:28:07,160 --> 00:28:15,400
to display time series data? So I, that's a good question. I don't know of other people who

379
00:28:15,400 --> 00:28:20,280
are using this in context other than Benjamin and I both use this in, you know, in our jobs.

380
00:28:21,320 --> 00:28:25,960
And so essentially, you know, what you're just doing is you're, you're outputting, you know,

381
00:28:25,960 --> 00:28:32,120
just, just the way that you would output like an an F1 score on a regular basis, let's say,

382
00:28:32,120 --> 00:28:38,120
of your, your classifier and maybe you compute an F1 score on some regular, you know,

383
00:28:38,120 --> 00:28:44,840
routine basis and you look for a change. Well, instead of outputting just the F1 score,

384
00:28:44,840 --> 00:28:51,400
you could output a confusion matrix or a prediction error plot as another one of our classifier

385
00:28:51,400 --> 00:28:57,720
evaluations. You could output a prediction error plot, which would allow you to see not just

386
00:28:57,720 --> 00:29:04,120
the F1 score, which gives you the overall, you know, the harmonic mean of precision and recall,

387
00:29:04,120 --> 00:29:10,200
but actually be able to see specifically which classes where the decay is happening. And that

388
00:29:10,200 --> 00:29:16,120
is actually really helpful because you know, oh, it's this, you know, this specific class is where

389
00:29:16,120 --> 00:29:21,960
I'm struggling to classify, you know, it's not just kind of overall performance decay, it's decay

390
00:29:21,960 --> 00:29:28,120
in a specific way that I can use that information to make decisions about how to fix things. You

391
00:29:28,120 --> 00:29:36,760
mentioned in discussing that the third element of that model selection trip, triple the hyperparameter

392
00:29:36,760 --> 00:29:44,440
tuning that one of the advantages of using something like, well, this approach in general really

393
00:29:44,440 --> 00:29:52,440
is that you're taking a more principled approach to optimizing your hyperparameter is what are some

394
00:29:52,440 --> 00:29:59,800
of the plots that yellow brick offers that are there specific plots focused on hyperparameter

395
00:30:00,680 --> 00:30:06,120
tuning or is it just, you know, tracking model performance as you're working your way through

396
00:30:06,120 --> 00:30:11,320
the hyperparameter space? We do have a couple that are specifically for hyperparameter tuning,

397
00:30:11,320 --> 00:30:18,120
and so one that is very popular is the alpha selection plot. And so, you know, I'm sure you know

398
00:30:18,120 --> 00:30:26,360
this, but when you're doing regression, generally the problem is that you need to figure out how to

399
00:30:26,360 --> 00:30:33,880
execute enough smoothing so that your model will generalize to unseen data. But there is this

400
00:30:33,880 --> 00:30:40,360
sort of question about how much smoothing do I need? And generally the strategy is to grid search

401
00:30:40,360 --> 00:30:48,520
that, right? So you say, okay, well, smooth between alpha equals, you know, 0.001 all the way to alpha

402
00:30:48,520 --> 00:30:55,320
equals 10. That's a pretty big grid search, especially if you have a lot of data. That grid search

403
00:30:55,320 --> 00:31:02,440
could be running for quite a long time. And the worst part is that you don't know maybe if you've even

404
00:31:02,440 --> 00:31:09,160
picked the right range. And if you pick the wrong range to look inside, then you never get to the

405
00:31:09,160 --> 00:31:15,720
optimal answer. And so one of the things you can do with yellow brick is use the alpha selection,

406
00:31:16,600 --> 00:31:23,240
which allows you to visualize, you know, what the, let's say, regression score is at, you know,

407
00:31:23,800 --> 00:31:30,280
some number of cuts of alpha. And what you're hoping to see is some point where your score, you know,

408
00:31:30,280 --> 00:31:37,640
goes up, but starts to plateau. And if you don't see that, if you see kind of things hopping around,

409
00:31:37,640 --> 00:31:43,720
moving around a lot, where you can, you can sometimes even see if you've picked the wrong range

410
00:31:44,760 --> 00:31:50,600
to look inside. Another example that's sort of similar, but outside of regression is for when

411
00:31:50,600 --> 00:31:56,920
you're doing a centroidal clustering. So how do you pick the right K when you're doing K means?

412
00:31:57,800 --> 00:32:07,160
Well, you guess. There are, there are heuristics, let's say, for picking the best K. But

413
00:32:07,160 --> 00:32:13,720
generally, I mean, people are mostly just guessing, right? And so in the same way as with the

414
00:32:13,720 --> 00:32:21,560
smoothing coefficient, like you don't know how many K to pick. And so you can, you can grid search that,

415
00:32:21,560 --> 00:32:29,960
you can, you know, pick all of the K from four to 400. That is a very long running grid search.

416
00:32:29,960 --> 00:32:40,600
And so instead of kind of just trying to throw grid search at it, you can use the elbow plots.

417
00:32:40,600 --> 00:32:46,680
So we have silhouette scores and elbow plots, which are two visualizers in yellow brick that are

418
00:32:46,680 --> 00:32:55,000
very useful for centroidal clustering. And what a silhouette plot does is it computes the

419
00:32:55,000 --> 00:33:00,600
silhouette score for each of the clusters. So you know, you pick, okay, K will seven, let's say,

420
00:33:00,600 --> 00:33:06,680
will cluster to seven clusters. And then it will compute for you the silhouette score for

421
00:33:06,680 --> 00:33:11,400
each of the clusters, which essentially is just a way of saying, like, how dense is this cluster?

422
00:33:11,400 --> 00:33:18,200
And how far away is it from other clusters? And you want them to be as dense and as far away

423
00:33:18,200 --> 00:33:24,040
from others as possible. Because that's what, you know, that's what centroidal clustering is sort of

424
00:33:24,040 --> 00:33:32,280
for. And so you can use that to look for a single K. But because you can use that for a single K,

425
00:33:32,280 --> 00:33:41,160
you can actually scale that up so that when you are, when you place like a range of K to look in,

426
00:33:41,160 --> 00:33:47,080
you can compute the average silhouette score across all of the clusters at each number of clusters.

427
00:33:47,080 --> 00:33:54,760
And what you hope to see in your plot is an elbow. So some point where you reach a peak, a peak

428
00:33:54,760 --> 00:34:01,720
score. And if you get to like a peak score for a certain number of clusters where your average

429
00:34:01,720 --> 00:34:08,840
silhouette score for all the clusters is really high, you know, you've got to the best K. But

430
00:34:08,840 --> 00:34:14,360
you might also see something where there's no elbow and it's kind of jumping all around. And

431
00:34:14,360 --> 00:34:19,240
usually when I see that, it tells me that centroidal clustering is not the best algorithm. It's

432
00:34:19,240 --> 00:34:24,200
not the best model family. And I need to try to pick a different clustering methods, like maybe

433
00:34:24,200 --> 00:34:30,680
something that's hierarchical, let's say. So those are two hyperparameter tuning visualizers

434
00:34:30,680 --> 00:34:39,080
that I use regularly that I think are very popular. I'm curious about the community that uses yellow

435
00:34:39,080 --> 00:34:44,600
brick. First of all, how long has yellow brick been out and what have you seen over time?

436
00:34:45,960 --> 00:34:53,480
Well, so it's been open source for I think it's it's going to be three years this month or maybe

437
00:34:53,480 --> 00:35:03,080
next month. So in that time, we have accumulated almost, I think we have 75 contributors now.

438
00:35:03,080 --> 00:35:13,080
We have about 2,000 stars on GitHub. And, you know, it looks like we have about 5,000 downloads

439
00:35:13,080 --> 00:35:21,800
a month, unique downloads a month via the Python package index pi pi. So it looks like we are kind

440
00:35:21,800 --> 00:35:26,760
of we have reached a point where it is starting to become, you know, part of just the routine

441
00:35:26,760 --> 00:35:34,520
way that people do machine learning diagnostics. You know, it's hard to tell what the breakdown is

442
00:35:34,520 --> 00:35:41,160
between, you know, professional data scientists and students. But based on kind of what we're,

443
00:35:41,160 --> 00:35:45,400
you know, the feedback that we're getting and kind of what we're seeing, it looks like this is

444
00:35:45,400 --> 00:35:52,040
something that is being used across a lot of different communities. And we recently became a

445
00:35:52,040 --> 00:35:57,560
non-focus affiliated project. So I'm not sure if you're familiar with non-focus, it's, you know,

446
00:35:57,560 --> 00:36:04,120
it's a kind of open source umbrella organization that that helps promote open source projects. And

447
00:36:04,840 --> 00:36:11,000
particularly a lot of the ones that are very popular like, SciPy and Matplot Live and Scikit

448
00:36:11,000 --> 00:36:16,920
Learn. And so we recently became affiliated with non-focus, which is really nice. It helps us sort of

449
00:36:16,920 --> 00:36:22,600
be kind of more hooked into the conversation and they help us sort of spread the word.

450
00:36:23,720 --> 00:36:29,080
We work really hard to, you know, reach out to people. We give a lot of talks. You know,

451
00:36:29,080 --> 00:36:35,160
we go to PyCon and you know, PyData events all over the country. Even, you know, we've been to

452
00:36:35,160 --> 00:36:41,960
London. We've given talks in Argentina. So we really are doing our best to try to get this out

453
00:36:41,960 --> 00:36:48,680
to as broad an audience as we can. Because I think that that like the diversity of usage

454
00:36:48,680 --> 00:36:53,800
has really made yellow brick very interesting. So, you know, it's not just things for regression.

455
00:36:54,920 --> 00:36:59,720
It's not just things for classifications, not just things for text, you know, not really because

456
00:36:59,720 --> 00:37:05,160
we have users who are engaging in this conversation, who are doing all different kinds of things.

457
00:37:05,160 --> 00:37:08,120
And it's really made the library much richer over time.

458
00:37:08,120 --> 00:37:14,600
Oh, that's awesome. That's awesome. You know, one was the first feature that, you know,

459
00:37:14,600 --> 00:37:20,360
major feature or different type of graph that went into yellow brick that you or Benjamin

460
00:37:20,360 --> 00:37:25,480
didn't personally build. Oh, wow. That's been a long time.

461
00:37:26,680 --> 00:37:37,480
It has been a long time. You know, one of, I mentioned the dispersion plots that was added

462
00:37:37,480 --> 00:37:46,120
by one of our contributors, Larry. So Larry Gray is one of our core contributors and he does a

463
00:37:46,120 --> 00:37:52,600
lot of text analysis too. And so he added the dispersion plot. We have a joint plot visualizer,

464
00:37:52,600 --> 00:38:00,280
which is for doing pairwise feature analysis that was added by Prima, who is another one of our

465
00:38:00,280 --> 00:38:09,640
contributors. We have a very kind of special approach to doing unit tests. You can kind of imagine

466
00:38:09,640 --> 00:38:15,880
that when you're trying to do unit tests for visualizations, it gets a little bit hairy because

467
00:38:15,880 --> 00:38:20,600
you have to compare, you know, different plots and see if they're close enough.

468
00:38:21,640 --> 00:38:28,280
But one of our core contributors, Nathan, you know, essentially he found kind of a clever way

469
00:38:28,280 --> 00:38:34,840
of doing this that we've adopted. That was a really important contribution. We have two other

470
00:38:34,840 --> 00:38:41,640
contributors, Kristen and Adam, who have helped a lot with our documentation and getting it

471
00:38:41,640 --> 00:38:47,560
basically to a forum where, you know, people really enjoy reading the documentation and know where

472
00:38:47,560 --> 00:38:52,840
to find things because things are kind of easy, easily searchable and make sense.

473
00:38:52,840 --> 00:39:02,680
Can you give a quick summary of the approach to unit testing? Yes. So the trick is that you have

474
00:39:02,680 --> 00:39:12,920
to find a way to do visual comparisons. And so what we do is the library inside the test directory,

475
00:39:12,920 --> 00:39:20,040
we have baseline images. And every time we add a new visualizer or a change of visualizer,

476
00:39:20,040 --> 00:39:27,800
we update those baseline images. And those baseline images tell you, you know, the tests create

477
00:39:27,800 --> 00:39:36,040
images. And the baseline images are what we compare them to. And so when the tests run either on

478
00:39:36,040 --> 00:39:43,800
your machine or in CI on GitHub, they are building these plots. They're comparing them to the

479
00:39:43,800 --> 00:39:49,880
baseline images that are part of the repository and they're looking at the diffs between

480
00:39:49,880 --> 00:39:54,600
the two. So, you know, it's really just like black and white. Like if you actually look at the

481
00:39:54,600 --> 00:39:58,760
diffs, it's just black and white and you're hoping that, you know, you won't see any diff at all.

482
00:40:00,040 --> 00:40:06,440
But what actually happens in practice is that different operating systems have slight variations

483
00:40:06,440 --> 00:40:15,160
in how they execute fonts or colors, let's say, or, you know, translucency. And so you have to

484
00:40:15,160 --> 00:40:21,880
engineer, you have to use like Pytest parameterize or, you know, other tricks with Pytest

485
00:40:22,680 --> 00:40:27,480
to say, okay, well, if it's within this range of similarity, we'll call it the same close enough.

486
00:40:28,680 --> 00:40:34,920
Interesting. So yeah, it's a little bit tricky, but it's a pretty cool trick. Actually, if anybody out

487
00:40:34,920 --> 00:40:42,280
there is doing any kind of visual library stuff and having trouble with tests, I would strongly

488
00:40:42,280 --> 00:40:45,720
suggest looking at yellow brick. I don't really think that there's anything else out there.

489
00:40:47,560 --> 00:40:51,880
Like that. So it can be a really good model for people who are having similar kinds of challenges.

490
00:40:52,680 --> 00:40:58,920
Awesome. Awesome. Is that, have you put that visual compare into yellow brick or is it

491
00:41:00,360 --> 00:41:06,440
separate thing? So it's in there in the sense that it's part of our test suite. So like if you go

492
00:41:06,440 --> 00:41:12,120
into the test directory, yeah, it's so it's all in there. And we have actually a read me inside

493
00:41:12,120 --> 00:41:17,960
the test directory that even just kind of goes pretty in depth into like what visual comparison

494
00:41:17,960 --> 00:41:23,800
means and how it works. So if you're curious about that, you can dig in there. But yeah, so that is

495
00:41:23,800 --> 00:41:30,280
part of the source code. Well, that's awesome, Rebecca. Thank you so much for taking the time to

496
00:41:30,280 --> 00:41:35,800
chat with us about what you are up to and about yellow brick. Sounds like a really awesome

497
00:41:35,800 --> 00:41:43,000
library. Thanks so much. Thanks. And yeah, everybody keep an eye out version 1.0 is going to be coming

498
00:41:43,000 --> 00:41:48,920
out probably in the next month. So there's a lot of new stuff coming out with version 1.0. I think

499
00:41:48,920 --> 00:41:53,480
people are going to be pretty excited to see it. Oh, that's a huge milestone. Congratulations.

500
00:41:54,120 --> 00:41:57,480
Thank you very much. We are very excited. Awesome. Thank you.

501
00:41:57,480 --> 00:42:07,000
All right, everyone. That's our show for today. If you like what you've heard here, please do us a

502
00:42:07,000 --> 00:42:11,960
huge favor and tell your friends about the show. And if you haven't already hit that subscribe

503
00:42:11,960 --> 00:42:16,920
button yourself, make sure you do so you don't miss any of the great episodes we've got in store for

504
00:42:16,920 --> 00:42:30,360
you. As always, thanks so much for listening and catch you next time.

