WEBVTT

00:00.000 --> 00:04.960
All right, everyone. Welcome to another episode of the Twimmil AI podcast. I am, of course,

00:04.960 --> 00:10.800
your host, Sam Charrington. Today, I'm joined by Anima Anam Kumar. Anima is

00:10.800 --> 00:16.080
Bren Professor of Computing and Mathematical Sciences at Caltech and Senior Director of AI

00:16.080 --> 00:21.280
Research at NVIDIA. Of course, before we get going, take a moment to hit that subscribe button

00:21.280 --> 00:26.000
wherever you're listening to today's show. You can also follow us on TikTok and Instagram

00:26.000 --> 00:32.160
at Twimmil AI for highlights from every episode. Anima, welcome back to the podcast. It's been a bit.

00:32.160 --> 00:38.480
Yeah, thank you, Sam. It's such a pleasure to be back and so great to see where how Twimmil

00:38.480 --> 00:45.920
has expanded its audience. It's now even on TikTok. And when we started back then in the beginning

00:45.920 --> 00:52.880
of the AI revolution to where we are now, it's so great to see Twimmil be part of the journey and

00:52.880 --> 00:58.880
helping our viewers and audience to really be up to date with the latest and greatest machine

00:58.880 --> 01:05.760
learning tools. Thanks so much. I'm really excited about catching up with you and digging into

01:05.760 --> 01:11.200
all the work you're doing around AI for science. Since it's been such a while since we last spoke,

01:11.200 --> 01:18.800
I'd love to have you just reintroduce yourself to our audience and share how you came into the field.

01:18.800 --> 01:25.440
Yeah, certainly. It's been such an exciting journey to be part of the AI revolution starting

01:25.440 --> 01:33.200
all the way back when AI was considered more a theoretical concept. People didn't think in our

01:33.200 --> 01:39.200
lifetimes it would take off like the way it has done now. But that also meant I could hone in

01:39.200 --> 01:46.160
on the theoretical foundations from statistics, signal processing, machine learning, probabilistic

01:46.160 --> 01:54.400
models, and ask questions like, how do we extract hidden or latent variables or phenomena

01:54.400 --> 02:01.200
from data without labels? What we call unsupervised learning. And I was working on tensor methods

02:01.200 --> 02:09.200
which conceptually looks at higher order moments of data when people were mostly limiting to

02:09.200 --> 02:16.560
second order moments or linear algebraic techniques. We talked about that back then,

02:16.560 --> 02:23.600
and a lot of my work was saying, no, no, don't just limit to linear models. Don't just limit to

02:23.600 --> 02:31.360
spectral methods that use matrices go beyond, think bigger. And we have the computational

02:31.360 --> 02:37.360
abilities to do that now. We can go to tensors. And that kind of non-linear transformation

02:37.360 --> 02:43.760
is what we also see with neural networks. So we were early in that journey to go from matrices

02:43.760 --> 02:49.920
to tensors. And from tensors now we have all kinds of non-linear mappings learned through

02:50.480 --> 02:59.200
all the neural networks and related tools. And so to me to see that journey and now last year

02:59.200 --> 03:05.520
was the year of Generative AI. And so to me who's been working on generative modeling since my

03:05.520 --> 03:13.440
earliest days in graduate school, it's so great to see that. Come into fruitition and see how

03:13.440 --> 03:19.840
it's having a lot of really important practical implications. That's awesome. And it's been

03:19.840 --> 03:26.240
quite a quite a year for for generative AI. And I'm sure we'll be talking a little bit about that.

03:27.280 --> 03:33.040
But one of the things we wanted to dig into a bit is the work that you've been doing around AI

03:33.040 --> 03:38.720
for science. Tell me a little bit about how you got involved in that. Yeah, certainly,

03:38.720 --> 03:45.600
you know, at Caltech here we founded AI for Science as a campus wide initiative in 2018.

03:46.160 --> 03:53.760
Way before researchers were thinking of it as a mainstream area in AI, right?

03:54.640 --> 04:00.480
Indeed, a lot of the AI development has been driven by industry, by big tech,

04:00.480 --> 04:06.560
and focuses on natural images and text, because that's where we can easily get

04:06.560 --> 04:14.640
web scale data. And they also have a lot of commercial value for building the next generation

04:14.640 --> 04:21.360
internet. You could argue. But when it comes to scientific domains, you know, there's even a

04:21.360 --> 04:25.760
hard question of where do we get started. You know, there's so many challenges that we don't

04:25.760 --> 04:34.240
see in our standard AI domains like Image and Text. I know that starts with the lack of data,

04:34.240 --> 04:40.800
right? You may not have large scale data that's really necessary for deep learning. And you

04:40.800 --> 04:47.680
would not only think about a very specific task, you most of the time required zero-shot

04:48.640 --> 04:53.760
generalization. So you want to go beyond the training domain, you know, a lot of the

04:53.760 --> 05:00.240
sciences extrapolation, right? Not only the data I've seen, what is the possibility beyond?

05:00.240 --> 05:06.080
What are new drugs? What are new molecules? How do I create those? You know, you don't have data

05:06.080 --> 05:13.360
on that because all the design and discovery is about finding something new that doesn't exist

05:13.360 --> 05:19.920
so far in your training. And so that's, to me, that extrapolation and generalization beyond

05:19.920 --> 05:26.160
the training data is such an important aspect that standard machine learning wasn't capable of

05:26.160 --> 05:34.240
doing. But that's where I think now in the era of generative AI, so much of that is becoming

05:34.240 --> 05:41.040
much more possible. You know, just like we can generate avocado chairs, can we generate better drug

05:41.040 --> 05:47.920
molecules? You know, for the viewers, what I'm referring to is like a stable diffusion or

05:47.920 --> 05:56.480
dolly, these models that once they learn text to image mapping can now create entirely new

05:56.480 --> 06:02.800
images that were not in training data, right? Entirely new scenarios, like a chair that looks like

06:02.800 --> 06:08.320
an avocado. You know, you probably didn't see it in training data, but if you learn the concept

06:08.320 --> 06:14.240
of what is a chair and what is an avocado, you can try and mix that together. Do this compositional

06:14.240 --> 06:21.840
generation. And I think these are concepts now that we are coming a full circle is aspects that

06:21.840 --> 06:28.080
can have huge implications in the sciences. And yeah, so a lot of unique challenges, but also

06:28.080 --> 06:32.560
I think opportunities that lie ahead when it comes to AI for science.

06:32.560 --> 06:41.040
I think one of the, one of the areas of AI for science that's received a lot of publicity over

06:41.040 --> 06:48.320
the past year, a couple of years has been around the various protein folding research.

06:49.040 --> 06:53.760
Can you talk a little bit about that and why that's interesting and important?

06:53.760 --> 07:01.200
Yeah, absolutely. Protein folding has already been making such a big impact in our ability

07:01.200 --> 07:07.680
to quickly generate the full three-dimensional structure of proteins, right? So how do you map

07:07.680 --> 07:14.560
from sequences to now three-dimensional structures? And this folding is so important because

07:14.560 --> 07:20.960
that's what will determine how other molecules or other proteins interact and bind.

07:20.960 --> 07:25.920
So if you now have a drug molecule, which is typically a small molecule, I mean, not all the time,

07:25.920 --> 07:31.520
but most of the time it's simpler to design a small molecule. And then the question is how does

07:31.520 --> 07:37.920
it want bind in a certain pocket of the protein? You know, what are the active areas in this protein,

07:37.920 --> 07:44.800
right? So all that's determined by this three-dimensional structure. And that's why for biologists,

07:45.760 --> 07:50.640
you know, this was really critical because you can all use all the knowledge and intuitions

07:50.640 --> 07:57.440
and say, oh, this could be a good binding candidate. But until you have the three-dimensional

07:57.440 --> 08:03.760
structure easily available, how do you determine if that's good computationally? And that's

08:03.760 --> 08:09.120
where you have to go to the lab and do those experiments, which are really expensive.

08:10.160 --> 08:18.000
And now we are going even beyond the folding, right? Even just the protein by itself,

08:18.000 --> 08:23.280
what is the structure? To now, how do the protein and ligands, which are small molecules,

08:23.280 --> 08:29.760
bind together? So in many cases, it's even more complicated, as always, biology is quite

08:29.760 --> 08:36.240
complicated, right? The protein, when it's binding to this small molecule, in fact, changes

08:36.800 --> 08:42.800
its structure. So it's not even a static process. And so that's one of the work we've been

08:42.800 --> 08:50.720
exploring using these generative AI diffusion models is not only to model the static

08:50.720 --> 08:56.640
behavior of how a protein looks in three dimensions, but how does that three-dimensional structure

08:56.640 --> 09:04.160
change when it binds to the small molecules? And so to model that process, and with that,

09:04.160 --> 09:10.880
we can predict better about the contacts. How well does it bind? And that's so crucial for

09:10.880 --> 09:15.840
coming up with much more realistic predictions. And that's just one example. You know,

09:15.840 --> 09:23.360
we're talking a lot about structure. And this is, quoting Frances Arnold, who is Nobel Prize winner

09:23.360 --> 09:30.560
here at Caltech, for directed evolution of protein. So she is quoted in New York Times on

09:30.560 --> 09:36.480
Generative AI for proteins that appeared, I think, about two weeks ago. And so she's asking,

09:36.480 --> 09:42.480
that's great. We're solving all these structure problems, right? We can speed up our prediction of

09:42.480 --> 09:48.800
structure. We can greatly increase the accuracy of our structure prediction. But what about function?

09:49.680 --> 09:55.040
Because ultimately, what we want to understand is, what is the functionalities of different proteins?

09:55.760 --> 10:02.960
And can we create like better proteins, which have functional implications? Because, you know,

10:02.960 --> 10:10.240
if you can create better targets and create drugs for it, we can cure a lot of diseases and

10:10.240 --> 10:18.080
issues that so far have not been able to do through traditional therapeutics. And I think that's

10:18.080 --> 10:24.720
where a lot of the work that, for instance, my group is doing as well as others, is to not just

10:24.720 --> 10:29.920
limit to the structure. Because if you only look at proteins and their structure and create the

10:29.920 --> 10:36.320
foundation models, you know, it's not very easy to talk about the function. But if you go to the level

10:36.320 --> 10:42.400
of the genome, you know, the DNA, right, that determines the function. If you can learn the

10:42.400 --> 10:47.840
relationship between different genes, you know, what is the relationship and how does it relate

10:47.840 --> 10:52.960
to different functionalities of the proteins? Because genes can be mapped to the proteins that

10:52.960 --> 10:59.120
generate. And by understanding relationship between the genes, can we generate better proteins

10:59.120 --> 11:04.720
that are functionally meaningful for different tasks? And that's what we've been doing now. And

11:04.720 --> 11:12.640
and it has so many applications. So in the largest biological language model that we built

11:12.640 --> 11:20.880
of about 25 billion parameters, what we showed was the ability to learn at the genome level,

11:20.880 --> 11:28.960
so long sequences of bacteria and viruses, more than 110 million such sequences. So we consumed

11:28.960 --> 11:35.440
all that into a language model. So in a way, you're understanding the language of the genome. And then

11:35.440 --> 11:42.880
from that, we can generate now new gene sequences, right. And with that, we could in fact predict

11:42.880 --> 11:50.160
new variants of coronavirus, as well as, you know, the ones we had held out, the existing variants that

11:50.160 --> 11:56.800
appeared like Delta, Omicron, right, all those we could predict, even though the model had never

11:56.800 --> 12:03.200
seen that before. What was the text that that model was trained on? Genome sequences. Got it.

12:03.200 --> 12:10.400
So think of now the language is the genome, right. So that's your now alphabet. That's your sentences.

12:11.280 --> 12:18.160
It's longer and that's a challenge. In fact, we used a combination of language model like GPT,

12:18.160 --> 12:23.760
but with the diffusion backbone in the latent space. So it's a hierarchical model that has

12:23.760 --> 12:30.400
both diffusion and GPT components, which is necessary for these long sequence modeling.

12:31.360 --> 12:37.520
And, you know, and that really helped us to understand at the genome level these relationships.

12:38.320 --> 12:44.320
And you could also see organization in the latent space of different latent genes, right. So genes

12:44.320 --> 12:52.320
that are closing that latent space are also functionally similar or related. And I think those

12:52.320 --> 12:57.440
also help us now to design better proteins. This reminds me a little bit of

12:58.320 --> 13:04.320
work that I spoke with Richard Socher about when he was, I think he did this at Salesforce

13:04.320 --> 13:10.800
of all places. Are you familiar with that at all? Yeah, certainly. That was some of the early work on

13:10.800 --> 13:19.840
understanding protein. I think sequence models, if I recall right, or like molecule models, right.

13:19.840 --> 13:25.040
So a lot of work in the last few years has gone first at the molecular level. You know,

13:25.040 --> 13:30.560
how do we generate better molecules? How do we give like condition on various properties and

13:30.560 --> 13:35.520
generate better molecules? And then we said, we can't just look at a molecule in isolation.

13:35.520 --> 13:40.160
We have to look at how it binds to the protein, right. That's the important interaction.

13:40.160 --> 13:45.280
And that's why we went into the protein structure prediction, which is the alpha-fold and

13:45.280 --> 13:51.440
the meta-ESM and all open-fold and all these tools. But then we said, we can't look at the protein

13:51.440 --> 13:57.840
in isolation. We have to look at how the protein and this molecule bind. And that's the joint

13:57.840 --> 14:03.520
prediction. And we also do that dynamically because the protein structure can change as it's

14:03.520 --> 14:09.440
finding. So that's modeling the dynamics. And now we are saying, all this is great, but this

14:09.440 --> 14:15.760
still doesn't tell me the function. You know, what are these proteins doing in our body or in

14:15.760 --> 14:21.040
organism? And that's where we need to go to the DNA level. And so we need to understand the

14:21.040 --> 14:27.440
language of the genome and not only learn those relationships between genes because they map

14:27.440 --> 14:34.000
two proteins, but also potentially generate new gene sequences and through that new proteins and

14:34.000 --> 14:40.240
new targets. And I think that's exciting because we're really going up the hierarchy and then

14:40.240 --> 14:46.640
essentially understanding the code of life, right, life on Earth. So I think it's very exciting times.

14:47.200 --> 14:54.000
Yeah. If the genome encodes the protein structure, how does that get, how does understanding

14:54.000 --> 15:00.640
that get you to function? Because it's at the genome level that you're determining a lot of

15:00.640 --> 15:05.760
functionalities of the organism, right? So it's encoding more than just protein structure.

15:06.320 --> 15:13.280
Yeah, absolutely. Because that's what evolution has endowed with, you know, all the fitness,

15:13.280 --> 15:18.960
right? Like which one survive, which ones are good proteins? You know, that's what is encoded

15:19.680 --> 15:27.200
in all our DNA as well as, you know, the DNA from all the way from viruses to all the higher organisms.

15:27.200 --> 15:34.560
And it's complicated, right? You can't just directly map one to one in higher organisms and that's

15:34.560 --> 15:42.080
why we started from virus and bacteria where it's clearer and simpler to analyze, but I think we can

15:42.640 --> 15:48.880
as these models get bigger as we create foundation models on long genome sequences and

15:49.440 --> 15:56.560
really get into understanding what in the latent space we can encode, right? And what does the

15:56.560 --> 16:02.240
generation of new sequences mean? And, you know, what kind of evolutionary gaps or other things

16:02.240 --> 16:06.400
they're filling? I think we can do a lot more than what we've done today.

16:07.840 --> 16:14.000
You mentioned diffusion a couple of times in the couple of works that you've discussed. Can you

16:14.000 --> 16:19.280
talk a little bit about the relationship between how are you using diffusion and how it's used in

16:19.280 --> 16:24.720
the context of stable diffusion? Yeah, absolutely. You know, last year has been exciting for

16:24.720 --> 16:34.480
generative AI and stable diffusion has really, you know, been a trial blazer in the sense that we

16:34.480 --> 16:42.400
have an open source model, right? Where has found a whole range of an ecosystem of applications

16:42.400 --> 16:48.320
and it can generate seamlessly new image candidates based on the text instruction.

16:48.320 --> 16:56.000
And so what the diffusion model at its core is a generative model that can sample new

16:56.720 --> 17:03.920
candidates, new realizations from a distribution it has learned. And the way it does is to go from,

17:03.920 --> 17:09.680
let's say, a Gaussian distribution, which is simple to sample from to the distribution of images,

17:10.400 --> 17:16.000
which would be hard to directly learn. But by learning this mapping, like think of it as a

17:16.000 --> 17:22.880
slow diffusion, you go from Gaussian noise and you, you know, progressively make it look more

17:22.880 --> 17:27.760
and more like the image. It'll be a noisy image and ultimately at the very end, all the noise

17:27.760 --> 17:35.760
would be filtered out and it would be the true image. So that slow process, how can we model that

17:35.760 --> 17:41.280
through learning these mappings, right? Learning this transformation from a Gaussian distribution to

17:41.280 --> 17:47.840
the image. And similarly, we can do that for scientific domains as well because it can, you know,

17:47.840 --> 17:54.240
the concept is you can learn any arbitrary distribution, probability distribution. And so we can

17:54.240 --> 18:01.360
learn now the probability distribution of how does the pros, you know, how do we create new genome

18:01.360 --> 18:06.720
sequences, right? What is the probability of them occurring, right? Because there is, you know,

18:06.720 --> 18:11.920
we know a lot of like there's high correlation between different gene sequences in a genome.

18:11.920 --> 18:17.680
You know, there's just like a code where some combinations are just not valid or they're not

18:17.680 --> 18:23.680
going to survive. They're not that, you know, relevant for the real world. And that's encoded in

18:23.680 --> 18:32.400
that data set of all known genome sequences that people have collected. And so by encoding that,

18:32.400 --> 18:40.080
we can learn now a better way to sample. And that's true for also so many other applications.

18:41.040 --> 18:48.720
Another setting we're thinking about over the last few years has been to look at all kinds of

18:49.920 --> 18:56.320
spatial temporal processes and the phenomena in science and engineering, right? Think about how

18:56.320 --> 19:04.640
fluids move, how does our weather forecasting change? How does our ability to predict earthquakes

19:04.640 --> 19:11.440
develop? And so so that's another area as well where uncertainty quantification is very important.

19:12.000 --> 19:16.480
And I think tools like this could be very effective when that's something we're working on.

19:16.480 --> 19:25.280
Are you for seeing kind of the diffusion applying diffusion to all the things? Is it a tool that we

19:25.280 --> 19:31.120
kind of stumbled upon for, you know, generating cool images, but it's going to have much broader

19:31.120 --> 19:36.960
implications over the coming years? I think it's one of the tools, right? I mean, the question of

19:36.960 --> 19:43.200
which specific architecture and which specific framework can evolve over time. It was GANS

19:43.200 --> 19:49.680
a few years ago. And we saw like challenges in optimization. And maybe one day will overcome that

19:49.680 --> 19:55.920
optimization challenges, right? And I do think that's important for the scientific applications too

19:55.920 --> 20:02.400
because in sciences, we have a lot of constraints. You know, for instance, one setting we've been

20:02.400 --> 20:08.000
extensively working on is solving partial differential equations. And so they're incorporating the

20:08.000 --> 20:13.840
physics constraints, incorporating, you know, the fact that you should satisfy this equation. So

20:13.840 --> 20:20.720
you penalize if you don't satisfy that equation, right? Our constraints. And so these kind of

20:20.720 --> 20:27.600
constrained optimization frameworks are similar to again, right? Because they're primal dual

20:27.600 --> 20:34.000
optimization. And so we still haven't solved those fundamental optimization issues. In deep learning,

20:34.000 --> 20:39.040
we've kind of swept it under the rug and said, okay, this problem is hard. So let's just develop

20:39.040 --> 20:45.040
a different methodology where we don't have to tackle that at all. And I think in the short run

20:45.040 --> 20:49.840
that gives us some gains, but at the same time in the long run, I think that comes with its own

20:49.840 --> 20:56.080
issues. You know, for instance, diffusion models are slow to sample because they have to

20:56.080 --> 21:02.960
gradually go from a Gaussian distribution to the distribution of images. And one of the frameworks

21:02.960 --> 21:09.520
we've developed to overcome that, you know, to do a decoding in parallel. So instead of sequentially

21:09.520 --> 21:16.320
sampling going from Gaussian to the image distribution, can we directly jump? And can we do it in

21:16.320 --> 21:22.960
a generalizable manner, not just overfitted one setting and then, right, it doesn't work in other

21:22.960 --> 21:30.080
settings. And the frameworks we are using in all these settings has been what we're called neural

21:30.080 --> 21:37.200
operators, which are, I think, a really important concept when it comes to scientific domain

21:37.200 --> 21:43.760
applications. They can solve differential equations like the one used in diffusion models for

21:43.760 --> 21:50.800
sampling. But they can also solve all kinds of other differential equations like in fluid dynamics,

21:50.800 --> 21:59.040
you know, the ability to model seismic waves underground to predict earthquakes to model carbon

21:59.040 --> 22:05.120
capture and storage, you know, as we mitigate climate change. How do we pump carbon dioxide

22:05.120 --> 22:10.800
deep underground? And how does it interact with water? These what we call multi-phase flow systems.

22:10.800 --> 22:18.480
And I think that to me conceptually is different from the standard neural networks. And that becomes

22:18.480 --> 22:24.480
a really important tool for scientific domains. Can you dig into that a little bit more? What does a

22:24.480 --> 22:30.720
neural operator look like and how did you arrive at that? Yeah, absolutely. I think this has been

22:31.360 --> 22:38.000
such an exciting journey because we went about asking, you know, let's say I have the problem of

22:39.040 --> 22:45.600
saying how does the fluid dynamics evolve, right? I start the fluid and I shake it a little.

22:46.320 --> 22:52.160
How does the flow change over time? You know, it sounds very similar to a video prediction problem.

22:52.160 --> 22:58.320
So why not just use all the tools we've already developed to do this prediction? So it turns out

22:58.800 --> 23:05.760
the main challenge is that here the fine scales matter a lot, right? We know the turbulent

23:05.760 --> 23:12.240
phenomena like this. You can't just like filter out or smooth out the fine scales or the high

23:12.240 --> 23:18.560
resolution information. If you do that, you will get come up with bad predictions. On the other

23:18.560 --> 23:24.400
hand, in the natural image domain, we want to all the time filter out, right? All the pixel

23:24.400 --> 23:30.160
is just too high dimensional and mostly useless information. We want to go from pixels to object

23:30.160 --> 23:38.240
localization. And so that means we all the processes to remove all this irrelevant information

23:38.240 --> 23:43.680
and filter out smooth out. And that's what like convolutional filters do because locally,

23:43.680 --> 23:49.600
you're learning these ability to smooth out and extract only the relevant features like edges.

23:50.160 --> 23:56.080
But that's different in so many of the scientific simulations like fluid dynamics

23:56.080 --> 24:03.040
because you cannot just filter out or smooth out the higher resolution information, right?

24:03.040 --> 24:11.760
You have to keep that. And the consequence of that is you cannot just learn your input to

24:11.760 --> 24:18.320
output mapping in one resolution. So think of any kind of image generation or image prediction.

24:19.120 --> 24:25.760
You always specify, pre-specify, what is the resolution of the image you want to either generate

24:25.760 --> 24:30.640
or predict on, right? It doesn't work at a different resolution. People train and the train only

24:30.640 --> 24:37.360
at that one resolution, they use the model only at that resolution. Whereas for the scientific

24:37.360 --> 24:43.520
domains, that would break down. And that's why until now, until these neural operators,

24:45.280 --> 24:50.640
we proposed until that point, people never thought about replacing the full traditional

24:51.440 --> 24:55.920
numerical solvers, right? So the whole point was, let's keep the solvers, let's

24:56.560 --> 25:00.400
get measurements from them and then try to do super resolution on top of it.

25:00.960 --> 25:06.320
Still mapping from one fixed resolution to another fixed resolution. But on the other hand,

25:06.320 --> 25:11.920
we couldn't completely get rid of the numerical methods and the traditional solvers.

25:12.640 --> 25:19.760
And so the way we went about overcoming this fundamental drawback is to come up with now

25:20.240 --> 25:27.280
framework called neural operator. So once you've trained the model using some training data

25:27.280 --> 25:33.120
at certain resolution, at testing time or at inference time, you can test it at any resolution.

25:33.120 --> 25:38.640
So you can give it an input at a different resolution, even a higher resolution than what it

25:38.640 --> 25:45.280
has seen during training. And still, it can make valid predictions. And that's the concept that

25:46.080 --> 25:52.320
makes this both, first of all, important for scientific simulations because people want

25:52.320 --> 25:57.840
the flexibility to choose different measures, different sampling techniques, right? They don't

25:57.840 --> 26:03.360
want to limit to one resolution, one grid. And the other is it also gives it the superior ability

26:03.360 --> 26:08.480
to capture the fine scales, which is so important for the simulation frameworks.

26:09.040 --> 26:19.040
And you mentioned and explaining this convolutional operator is the idea here that the

26:19.040 --> 26:30.800
the neural operator is a kind of an abstraction of that idea and the exact relationship between

26:30.800 --> 26:35.760
the thing that you apply it to and the output is learned.

26:35.760 --> 26:41.200
Absolutely. You know, at a conceptual level, it's very similar to the current neural networks,

26:41.200 --> 26:46.880
right? Could be convolutional neural network transformers. You know, it does also try to learn

26:46.880 --> 26:53.440
a mapping from the input to output. But the difference here is that it doesn't just accept your

26:53.440 --> 26:58.960
input at one resolution. It has the flexibility to accept input at different resolutions.

26:59.760 --> 27:05.760
And which is lacking in our current standard networks. You know, if you take convolutional

27:05.760 --> 27:11.440
neural network, it learns filter at one resolution. So you give it input at a different resolution,

27:11.440 --> 27:19.600
right? It completely fails. And so this fundamentally says that we can now learn

27:20.080 --> 27:27.520
an operator, which is mapping between function spaces. So we are changing our input to a fix

27:27.520 --> 27:33.520
size, right? Could be a vector or matrix like image, which is of a fix size to one that is a

27:33.520 --> 27:39.360
function. And that function, you could sample anywhere in a domain, usually the continuous domain.

27:39.360 --> 27:45.600
So if it's a fluid flow, right? And I tell you, what is the domain? What is its boundary?

27:46.800 --> 27:51.520
You know, you could make your resolution finer and finer, right? Because it's a continuous domain.

27:51.520 --> 27:58.400
So you don't just limit to one resolution. And that's the ability we provide in these neural

27:58.400 --> 28:04.240
operators. Of course, the next question is, how do I make this practical, right? I mean, sure,

28:04.240 --> 28:11.200
this is a wish list. So far, what I said is, this is what I want out of a learnt mapping

28:11.920 --> 28:18.720
that is not present in the standard networks. Now, how do I make neural operator actually possible

28:18.720 --> 28:25.920
that can handle these different resolutions? And so the way we go about doing this is through

28:25.920 --> 28:31.840
Fourier domain operations. So what we call Fourier neural operator. And you can operate the Fourier

28:31.840 --> 28:40.480
transform at any resolution, even any grid, right? So conceptually, it has the expressivity to handle

28:40.480 --> 28:45.760
different grids, different resolutions. And is this where the traditional numerical methods come in?

28:45.760 --> 28:51.600
It is inspired by traditional methods, which also use Fourier transform. But the differences,

28:51.600 --> 28:58.080
we are marrying that with non-linear transformations, like in standard neural networks. That's the

28:58.080 --> 29:04.320
power that deep learning has, right? It's not just linear. You have non-linear activations.

29:04.320 --> 29:11.760
So we are combining the principles from signal processing, signal representation theory that

29:11.760 --> 29:20.240
using Fourier transforms can represent signals at any resolution. But now, you know, if I learn,

29:20.240 --> 29:25.200
like in the frequency domain weights, and I go back to the standard domain, that's not enough,

29:25.200 --> 29:31.600
right? That's just a linear transformation. And so I now do non-linear activations and keep doing

29:31.600 --> 29:39.360
these series of operations again and again, this way I can capture scales at different frequencies.

29:39.360 --> 29:45.440
So I can capture a big spectrum, even though like each set of operations is just linear.

29:46.320 --> 29:52.640
But I have non-linear activations in between. And so in a way, it's a marriage of the old and the new,

29:52.640 --> 30:00.480
right? The old being properties that Fourier transforms have that, you know, people in numerical

30:00.480 --> 30:07.280
methods have been using that as a way to express signal more efficiently. But then they assume

30:07.280 --> 30:12.560
linearity that, you know, you should be able to capture the spectrum of your input

30:13.280 --> 30:19.360
compactly through Fourier transform. And that's usually not true, right? Many of this have high

30:19.360 --> 30:25.600
frequencies. So just doing Fourier transform once isn't very efficient. But if you combine it with

30:25.600 --> 30:34.080
non-linear transformations of multiple such blocks, it is expressive. And in fact, we show that

30:34.080 --> 30:42.480
this becomes a universal approximator for functions basis. Meaning just as theoretically a standard

30:42.480 --> 30:47.520
neural network can universally approximate any function in a fixed dimension,

30:47.520 --> 30:55.760
we can now use neural operators and approximate any operator in a function space. So it can,

30:55.760 --> 31:01.840
it has the expressivity to handle any non-linear operators, like what we see as solutions

31:02.480 --> 31:06.960
of fluid dynamics and other partial differential equations, as an example.

31:06.960 --> 31:11.520
So you've gone from wish list to something that's more tangible, but you haven't,

31:12.160 --> 31:16.960
you know, yet demonstrated practicality. Can you talk a little bit about practicality?

31:16.960 --> 31:23.360
Oh, yeah, yeah, absolutely. And that's where, you know, the last year is where we really took this off

31:23.360 --> 31:30.000
to very practical and real-world large-scale applications. So one of them has been in the

31:30.000 --> 31:36.480
realm of weather forecasting, you know, our ability to predict what's going to happen in the next

31:36.480 --> 31:42.480
week or next two weeks, right? It is so critical. And especially extreme weather events, which are

31:42.480 --> 31:48.160
increasing in their intensity and scale as climate change, you know, intensifies.

31:48.960 --> 31:57.840
And so what we showed is that these framework using these kind of concepts from neural operator,

31:57.840 --> 32:04.960
we can predict whether as good as the current numerical weather models for as long as two weeks,

32:04.960 --> 32:14.000
but be about 45,000 times faster. Wow. And so that's, you know, really exciting. And a hard

32:14.000 --> 32:21.360
benchmark, because currently numerical weather models use numerical solvers sometimes over

32:21.360 --> 32:27.760
thousands of variables of partial differential equations. And, you know, that can take our

32:27.760 --> 32:35.280
some CPU clusters, right? Whereas our model works within a second, in fact, a quarter of a second

32:35.280 --> 32:40.720
on a single GPU. And we've open sourced this model, we've made this available to the community.

32:41.360 --> 32:47.200
And I think that's very exciting to, you know, see this kind of democratization, not just in the

32:47.200 --> 32:52.640
generative AI that's so much in the news, but in these scientific domains, you know, enabling

32:52.640 --> 32:58.320
everybody to run their own weather model and build all kinds of downstream applications, whether it's

32:58.320 --> 33:05.680
for agriculture, you know, can we come up with accurate regional predictions to, you know,

33:05.680 --> 33:12.640
that interest new sensor data, new information, and that can refine the scale locally to provide better

33:12.640 --> 33:20.480
estimates. We can also ask how this information could be used in conjunction with a climate model,

33:20.480 --> 33:25.280
because not just looking at the weather of today, but what about the weather of the future?

33:26.160 --> 33:32.160
How well does it extrapolate? And these are aspects that we are right now working closely with

33:32.800 --> 33:40.480
meteorologists and climate scientists, both at NVIDIA, as well as the broader community,

33:40.480 --> 33:46.080
and that's been very exciting. And that's just one of the examples. The other one that I've mentioned

33:46.080 --> 33:53.520
before has been in carbon capture and storage, you know, so climate change is upon us, and I think

33:53.520 --> 34:00.800
a lot of scientists believe that the only way to completely, you know, tackle this is to mitigate it

34:00.800 --> 34:06.880
through frameworks like capturing carbon, right? Whether it's directly from that atmosphere,

34:06.880 --> 34:11.760
what is known as direct-air capture, you know, people are also trying that from the ocean,

34:11.760 --> 34:19.280
or it's like as it's being produced, you kind of isolate and capture it and pump it deep underground.

34:19.280 --> 34:24.880
And so they're the phenomena that we need to model is how does the carbon dioxide pressure

34:24.880 --> 34:30.880
build up deep underground? You know, as we interacted with water, it's a highly non-linear

34:30.880 --> 34:37.680
gas-plume evolution. And so how do we contain the pressure over several decades? And even there,

34:37.680 --> 34:45.360
we can see benefits as much as 700,000 times faster than numerical solvers. Because here,

34:45.360 --> 34:51.200
it's a very, again, fine-scale phenomena in this wells underground. You need to model how both

34:51.200 --> 34:55.920
carbon dioxide and water, you know, this is called multi-phase because you have both liquid and gas

34:56.480 --> 35:02.400
interact. And so modeling that non-linear phenomena. But also, it's not just in a single well,

35:02.400 --> 35:08.240
it's multiple such wells, but there's some porosity, right? There's some permeability. They're still

35:08.240 --> 35:15.200
interact. And so having a multi-grid approach and the ability to capture both the cores and the

35:15.200 --> 35:21.680
fine-skills is so important in this application. And so that's just another application of

35:22.560 --> 35:29.600
these methods. And yeah, it's been very exciting. We have now, whether forecasting, carbon capture

35:29.600 --> 35:35.040
and storage, you know, modeling deformation in materials, you know, how much can I stretch

35:35.040 --> 35:42.800
this material, right? How plastic it is is also a very non-linear phenomena. And we can, again,

35:42.800 --> 35:48.240
show hundreds of thousands of times speed up over traditional solvers. We're able to

35:49.360 --> 35:57.040
do modeling of lithography process. So how do I go from, right, a mask designed to? What is the

35:57.040 --> 36:04.240
resist to make like what is finally being shown on the silicon wafer? And we also show the inverse

36:04.240 --> 36:09.360
problem, which is really critical in many of these applications because you want to design a better

36:09.360 --> 36:17.680
mask to be able to create the wafer of like desirable properties, right? So inverse problems are

36:17.680 --> 36:23.040
especially hard with numerical methods because if you keep running this forward simulation,

36:23.040 --> 36:29.200
which is very expensive. And but again, this is not data driven. So it's very hard to

36:30.080 --> 36:35.440
make changes or explore the design space. But what we showed with our

36:36.480 --> 36:42.560
neural operator-based methods is that we can progressively self-trainer improve. You know,

36:42.560 --> 36:48.960
we can keep creating better masks and we can train on them. And with that, it can generate better

36:48.960 --> 36:54.640
candidates. And so all of this, what you see with, you know, reinforcement learning or progressive

36:54.640 --> 37:01.440
self-training, all these phenomena, in other general AI applications, we can bring all those tools

37:01.440 --> 37:08.720
here as well. And marry it with neural operators because that gives the right foundation to capture

37:08.720 --> 37:13.200
all the fine scales, which are very important in many of these scientific domains.

37:13.200 --> 37:22.560
It sounds like the various applications you've described all involved kind of research efforts

37:22.560 --> 37:30.720
to apply neural operators to a particular domain. You know, I imagine what you eventually want

37:30.720 --> 37:35.680
is a tool that you take off the shelf and, you know, assuming you know your, you know,

37:35.680 --> 37:42.480
underlying PDE models, you can more, you can readily apply this tool. Can you talk a little bit

37:42.480 --> 37:48.160
about, you know, A, is that a fair characterization, and B, kind of, how do you think you get there?

37:48.160 --> 37:54.160
Yeah, I mean, as all these efforts, right, in the beginning, it's been very important to work with

37:54.160 --> 37:59.200
domain scientists, and that's been the goal of also the AI for Science Initiative when I found

37:59.200 --> 38:05.120
it at Caltech. But now, it's even broader than just the Caltech community, right? We are working

38:05.120 --> 38:11.760
with national labs, we are working with other universities, closely working, with also,

38:11.760 --> 38:17.360
NVIDIA engineers who are enabling and helping scale up all these frameworks and open sourcing

38:17.360 --> 38:22.080
it to the community. And I think that kind of building trust in the beginning is important,

38:22.800 --> 38:29.200
because, you know, Sally Benson at Stanford is considered a leader in carbon capture and storage,

38:29.200 --> 38:37.920
right? And we are collaborating with ECMWF, the European weather agency that, you know, you,

38:37.920 --> 38:42.960
that, in fact, created the data sets for these, you know, historical weather. And so working with

38:42.960 --> 38:48.800
them and asking, what are the metrics that matter here, right? You know, it's not just the short term,

38:48.800 --> 38:54.720
what is the long term behavior? What are the uncertainties? And I think that aspect of the deep domain

38:54.720 --> 39:01.760
expertise being married with AI and fully integrated is important. But you're perfectly right,

39:01.760 --> 39:09.360
as we show that these proof of concepts are really solid, they've already, right, been making

39:09.360 --> 39:15.120
impact in these domains. The next question remains, how do we generalize? You know, how do we create

39:15.120 --> 39:22.160
foundation models for science and engineering? And yeah, and that's a mission that I'm undertaking

39:22.160 --> 39:28.480
now. And I should say it's not straightforward, it's because one is we don't have the data,

39:28.480 --> 39:35.360
like what we do for text or images, right? And even there, it required quite a bit of curation

39:35.360 --> 39:42.160
effort. And here we need to think what that is. The other aspect we've been working really

39:42.160 --> 39:46.960
well on is how to incorporate the right physics constraints. What are the meta-learning and other

39:46.960 --> 39:53.520
approaches to generalize beyond one domain? And we've been seeing really good success. So I do think

39:53.520 --> 39:59.040
it's a great time to scale up, but we need to bring all these pieces together. And that's what I

39:59.040 --> 40:07.440
hope to do this year. Yeah, how different does the application to whether look relative to the

40:07.440 --> 40:13.520
application to carving capture? Yeah, that's a great question. You know, there's some architectural

40:13.520 --> 40:18.880
details that could be different. For instance, one of the newest architectures we are

40:18.880 --> 40:25.200
are experimenting in the weather case involves spherical geometry, right? Because we know we want to

40:25.200 --> 40:30.160
predict the weather on the surface of the earth that's a sphere. So by incorporating that

40:30.160 --> 40:36.800
geometric information, how much better can you do, right? So, and that's always this balance

40:36.800 --> 40:43.120
between, you know, knowing more about the domain incorporating the inductive bias versus

40:43.120 --> 40:49.680
via a generalist model that may know less about it, but with more data could do well.

40:50.400 --> 40:56.560
But the underlying concepts are still same. And I think there is a way to still capture the aspect

40:56.560 --> 41:05.120
that, you know, the underlying, right, if you only look at say the turbulence, right, there is

41:05.120 --> 41:11.680
similarity. But then on the carbon capture and storage, that interaction with water is very

41:11.680 --> 41:16.720
important to model. Whereas in the weather, there's certainly water, we have the oceans,

41:16.720 --> 41:23.040
but that's usually considered a longer timescale evolution compared to the shorter timescale,

41:23.040 --> 41:29.680
wind surface wind and other variables. And so people conveniently kind of ignore some of the

41:29.680 --> 41:37.200
phenomena or simplify it for different domains, which I think is very important for tractability.

41:37.200 --> 41:46.080
But yeah, I do think that, you know, there is a broader range of phenomena that could be

41:46.080 --> 41:52.880
captured through a common model. And it need not be a model that's learning everything all at once,

41:52.880 --> 42:00.240
right? It could kind of provide the seed or initialization for other models to further hone in and

42:00.240 --> 42:10.480
find you and on a very specific domain. When you're referring to your results with weather,

42:10.480 --> 42:19.200
for example, and you have the significant speed up, is that operating under the same

42:20.480 --> 42:26.800
sets of variables, or are you, is the model that you're referring to simplified relative to

42:26.800 --> 42:35.120
the model that you're comparing against. And likewise, one of the big motivations here is the

42:35.120 --> 42:43.040
ability to handle, you know, different resolutions. Are you, you know, when you are comparing those

42:43.040 --> 42:49.360
models, are you looking at the same resolutions, different resolutions? Yeah, absolutely. First of all,

42:49.360 --> 42:56.480
we're even not even currently taking all the information that's available in the historical data,

42:56.480 --> 43:01.520
right, and all the variables. So, you know, in the beginning, when we started this project,

43:01.520 --> 43:06.640
we wanted to do just a rough cut. We thought, okay, let's start with a sub sample of data and

43:06.640 --> 43:13.040
a selection of variables just to see what we get. And we didn't expect it to be producing such

43:13.040 --> 43:19.760
good results with even just that subset. And that's where I think, you know, there is a potential,

43:19.760 --> 43:26.400
and we, you know, see evidence that we can even beat the current weather predictions by training

43:26.400 --> 43:32.720
bigger models on all of the data available. And the weather is a great use case because

43:32.720 --> 43:38.320
we have wealth of historical data available, you know, that's collected since the 1970s,

43:38.320 --> 43:45.280
collected hourly, right, whereas these numerical weather models hardly use any of that data. So,

43:45.280 --> 43:52.480
they're using it to just calibrate a few coefficients or parameters in their numerical methods.

43:53.120 --> 43:58.000
And so, in the sense, they're redoing these computations again and again without having the

43:59.520 --> 44:07.040
a wealth of data being understood and incorporated into the calculations. And I think that's where

44:07.040 --> 44:13.760
the, you know, impressive speed up comes, right? One of the important reasons is because these

44:13.760 --> 44:22.720
AI-based methods can learn from data and they can learn better ways to do these numerical iterations,

44:22.720 --> 44:30.480
whereas the standard PDE solvers are forced to go to a very fine grid. As I said, we cannot ignore

44:30.480 --> 44:37.840
the fine scale phenomena. So, they have to like for guarantees for convergence be operating at a

44:37.840 --> 44:44.400
very fine grid and that makes them expensive. And so, our ability to learn from data better

44:44.400 --> 44:49.840
nonlinear transformations where computations become quicker I think is a key to the speed up.

44:50.560 --> 44:55.920
And regarding your question of, you know, can we predict at higher resolutions? So,

44:55.920 --> 45:03.280
currently the data that's available we've trained on is 25 kilometers spatial resolution, right?

45:03.280 --> 45:10.960
So, it's 25 kilometers proofed across the globe. And so, we are working with regional

45:11.600 --> 45:16.320
weather agencies to see if we can get now, you know, we are in fact getting some of the higher

45:16.320 --> 45:20.960
resolution data. And then we can ask, you know, how do the predictions look? And is there even

45:20.960 --> 45:28.080
potential to further refine those? And I think that's why this kind of a foundation model

45:28.080 --> 45:35.040
is valuable because you can go from being able to predict all across the globe to regional models

45:35.040 --> 45:41.280
where especially countries that don't have a lot of computing capabilities or

45:41.280 --> 45:46.800
technical abilities, right? Could start with a reasonably good model and hone in

45:47.360 --> 45:54.160
on their region and refine in a much more cost effective manner rather than running these

45:54.160 --> 45:59.200
numerical methods which are extremely expensive as you get to the fine grids.

45:59.200 --> 46:06.640
So, just to give you an intuition of how expensive it gets as you refine the grid for these kind

46:06.640 --> 46:13.600
of calculations, my colleague, Tapio Schneider here at Caltech Hours in Climate Sciences

46:14.160 --> 46:20.160
estimates that you need about 100 billion times more computing than what we have today.

46:20.160 --> 46:27.360
If we have to go to the actual finest resolution to be able to capture all these

46:27.760 --> 46:33.280
turbulence in low-lying clouds, which for climate models is the biggest source of uncertainty.

46:34.000 --> 46:38.480
And so that, you know, requires computing at the resolution of one meter.

46:39.200 --> 46:43.920
You know, I'm talking 25 kilometers, right? We need to go all the way to one meter,

46:43.920 --> 46:49.840
spatial resolution all across the globe and one second in time temporal resolution.

46:50.800 --> 46:57.280
And so that's, you know, the more slow is no longer upon us and even all this

46:57.280 --> 47:03.120
acceleration and scaling will not get us to 100 billion times, right? And even if they say,

47:03.120 --> 47:08.720
oh, that's too much, what if we get somewhere intermediate? It's still too large and that's where

47:08.720 --> 47:14.960
I think machine learning is a necessity, otherwise there's no way we can tackle these deepest

47:14.960 --> 47:23.920
questions in scientific domains. With these models, you've referred to the relative speed up.

47:25.200 --> 47:31.280
In the case of the weather, for example, in terms of performance, predictive performance,

47:31.280 --> 47:35.440
how did they, this method compare to traditional numerical models?

47:35.440 --> 47:42.640
Yeah, absolutely. In the case of the weather we are doing, as well as the current weather models

47:42.640 --> 47:50.160
for as much as two weeks, which is considered where, right, it's very predictive. So after that

47:50.160 --> 47:55.440
and a bit longer, you kind of get to the subsisinal and seasonal scales where it's no longer

47:56.000 --> 48:03.200
predictable. So it becomes chaotic and, you know, you can kind of say, right, statistical measures

48:03.200 --> 48:09.440
on average, what it would look like, but not the actual trajectory, the actual weather, what

48:10.240 --> 48:15.600
it is because it's just not predictive. And that's another challenge too with many of these

48:15.600 --> 48:23.760
models, right? So for short term, we can predict, but for the longer term, we can only simulate and

48:23.760 --> 48:29.440
on average get to what's known as the invariant measure, which is the attractor or the right

48:29.440 --> 48:36.400
distribution, right? Because you cannot predict exactly where the trajectory evolves. And we are

48:36.400 --> 48:43.040
also working on now using these neural operators and we've shown their ability not only to capture

48:43.040 --> 48:49.440
short term phenomena, but also the long term behavior by capturing the nature of chaotic

48:50.800 --> 48:55.200
systems and hence, through that, be able to simulate them again effectively.

48:55.200 --> 49:02.160
Do you characterize the failure or error of these models and like compare that to the weight

49:02.160 --> 49:10.320
traditional error models fail, thinking about, you know, whether it's something that, you know,

49:10.320 --> 49:15.840
we, there's kind of this inherent, you know, we joke about how bad weather models are, right? And,

49:15.840 --> 49:22.320
and how, and how, you know, difficult it is to predict the weather, but then a lot of people

49:22.320 --> 49:32.080
count on those predictions in very important ways, thinking about like if we, you know, start

49:32.080 --> 49:37.840
predicting the weather in different ways and those predictions are, you know, generally better,

49:37.840 --> 49:44.400
but occasionally much worse, like how does, how does, do we communicate that to the users of

49:44.400 --> 49:51.120
the predictions? And I'm just curious, you know, how that, you know, factors into the way you think

49:51.120 --> 49:57.520
about the problem, right? And that is a big challenge, right? Like as you said, you know, we need

49:57.520 --> 50:02.320
the uncertainty quantification as well, you know, are we getting to the right, uh,

50:03.120 --> 50:09.680
probabilities? So for instance, if we slightly perturb the initial condition, how does my prediction

50:09.680 --> 50:16.000
change? And so that's an important notion of stability and, and, and also because we have some

50:16.000 --> 50:20.880
uncertainty on our input, right? Like the, what we are measuring from our satellites and how

50:20.880 --> 50:28.000
that is all assimilated has, uh, certainly errors, right? It has uncertainties. And so this kind

50:28.000 --> 50:34.000
of what we call unsombling because you're not just feeding in one fixed initial condition,

50:34.480 --> 50:40.080
but you perturb it and then you ask, even with those perturbations, what is the output now?

50:40.080 --> 50:45.200
And through that, can I get a measure of the uncertainty or probabilities? And so we can also

50:45.200 --> 50:53.280
calibrate our weather models, AI-based ones, um, similar and even better many times than the current

50:53.280 --> 50:58.560
numerical weather models because we are much cheaper, right? So these numerical weather models

50:59.200 --> 51:06.000
will be surprised to use only about 50 ensemble members. So what kind of statistical

51:06.000 --> 51:13.120
averaging do we get out of 50, right? So 50 samples. And but that's so expensive. So, you know,

51:13.120 --> 51:19.280
people are using large clusters even for one single prediction and takes a few hours.

51:19.840 --> 51:26.480
So they are bound by the cost. And that's where this is another big benefit that we are seeing from

51:27.440 --> 51:33.200
AI-based models because it is so cheap. You can now run thousands of ensemble members like we

51:33.200 --> 51:40.160
are doing now. And we're carefully testing how does it do in all kinds of scenarios, right? We

51:40.160 --> 51:46.160
show, for instance, it can nicely capture the uncertainty around hurricane evolution. So many

51:46.160 --> 51:51.840
of the famous hurricanes that didn't appear in the training data, but we are testing them on,

51:51.840 --> 51:58.080
you know, how not just like the single trajectory, right? Because people care about uncertainties.

51:58.080 --> 52:03.920
And that has all kinds of downstream implications, you know, does hurricane hit Florida or go into

52:03.920 --> 52:11.200
the ocean, right? That makes all the difference. And so being able to get to the right uncertainties

52:11.200 --> 52:18.160
is such a big aspect. And that's where AI models are already proving to be superior. And by

52:18.160 --> 52:25.040
honing in all that and giving the right ensemble level estimates very cheaply, I think will make

52:25.040 --> 52:30.480
this so valuable. And that's what we are seeing now and working with weather scientists.

52:30.480 --> 52:37.520
Now, all these applications that you've been talking about have traditionally been, we talked

52:37.520 --> 52:44.160
about kind of the traditional numerical approaches. Those of all driven kind of the development of

52:44.160 --> 52:55.280
high performance computing as a field. And I guess I'm curious what, you know, if you thought of

52:55.280 --> 53:03.680
HPC as a, you know, a pie or whatever, like how much of that pie is, you know, being eaten up by

53:03.680 --> 53:13.040
AI today and in time, you know, over time, does AI, you know, eat all of that pie or their elements,

53:13.040 --> 53:17.600
are the things that we're doing with HPC that, you know, we don't think or you don't think will be

53:18.800 --> 53:24.960
that are incompatible with AI in some way or does AI, you know, just come to be a standard tool

53:24.960 --> 53:30.720
that apply to that type of problem. Yeah, no, that's a great question. I mean, so many now

53:30.720 --> 53:38.240
national labs and other centers are rebranding us HPC AI, right, combination, which is great. And,

53:38.240 --> 53:44.960
you know, that's been our experience too, like kind of to think of that as an integrated approach

53:44.960 --> 53:52.560
to begin with, right? And that doesn't mean like keep the numerical methods as they are, keep an AI

53:52.560 --> 53:58.080
tool as a standard hammer, you know, take something that works on natural images, bring it here,

53:58.080 --> 54:05.520
right? That's not the, that to me is not a winning strategy. When I say hybrid HPC AI, it's really

54:05.520 --> 54:11.840
thinking conceptually at an algorithmic level and say what does an integrated algorithm look like?

54:11.840 --> 54:16.720
And that's how we came up with Fourier neural operator, right? So we know numerical methods

54:16.720 --> 54:22.640
compute in the spectral domain that's efficient and that's also a nice way to create a basis that

54:22.640 --> 54:32.320
works in any dimensions and any resolution because it is having the ability to learn in a function

54:32.320 --> 54:40.320
space itself. So those, you know, thinking and that intuition, we don't need to reinvent from

54:40.320 --> 54:45.520
scratch. We already have so much that's been dealt in numerical methods, right? On the other hand,

54:45.520 --> 54:51.200
we know what's also the downside of numerical methods. They can't be data driven. So you can't

54:51.200 --> 54:56.720
think that computation based on if it's an easier or a harder sample, right? And you can't use all

54:56.720 --> 55:02.800
the existing computations you've done to learn and improve. And that's what AI provides. The other

55:02.800 --> 55:10.000
is all these non-linear transformations, which is right also an effect of being data driven

55:10.000 --> 55:16.160
that we have the ability to learn these non-linear transformations. So I think to me,

55:16.160 --> 55:21.120
ultimately, they'll all come together. But I don't want to get into the debate of what was

55:21.120 --> 55:28.560
numerical method, what was AI? You know, in that I worked like baby, you shouldn't be able to tell

55:28.560 --> 55:34.640
them apart. That's kind of the whole point that they just seamlessly integrate together. That's

55:34.640 --> 55:41.600
my hope and that's what we are working towards. Are there other ways that you see the

55:42.560 --> 55:48.640
kind of advances around, you know, that are happening broadly in AI, kind of driving AI for

55:48.640 --> 55:55.280
scientific applications? Yeah, as we've been talking quite a bit last year has been the

55:55.280 --> 56:02.000
era of generative AI and foundation models, right? So these big models having the ability,

56:02.000 --> 56:07.920
not just to do now one targeted task like we were doing for the past decade, but really be general

56:07.920 --> 56:13.440
purpose. You know, they're not all the way, but if zero short, you can't get a good answer,

56:13.440 --> 56:18.160
you could do few short, right? And you can give some examples. You could even fine tune.

56:18.800 --> 56:24.880
And what we've been very excited is taking one step further and asking how we can use these

56:24.880 --> 56:30.720
foundation models for decision making, you know, using with reinforcement learning,

56:30.720 --> 56:38.080
limitation learning, all the tools, which are conceptually the right thing to adapt to new

56:38.080 --> 56:44.880
environments and make decisions, but are very, very expensive, right? We really haven't broken

56:44.880 --> 56:54.720
beyond standard game settings and still fairly low dimensional action space to one now that is

56:54.720 --> 57:00.720
open world setting. And that's where I think there's been a lot of exciting work and one of the

57:00.720 --> 57:08.320
works that we've provided as a benchmark to the community is called Mind Audio. So it's a suite of

57:09.200 --> 57:17.120
thousands of tasks in Minecraft along with internet scale information about YouTube videos of how

57:17.120 --> 57:24.080
people are playing Minecraft, how they're building structures, wiki, ready articles. So all kinds

57:24.080 --> 57:30.000
of like text, image, tabular data, all the information that you can clean. And from that,

57:30.000 --> 57:37.440
can you now solve not just one task, but thousands of tasks, right? So what we call as open-ended

57:38.240 --> 57:45.040
task solving. And so, you know, if I give the instruction in text-based prompts and say go

57:45.040 --> 57:51.520
minor diamond or go build me a castle, right? So it should figure out how to do it, but it's not

57:51.520 --> 57:57.920
doing that from scratch, like what we saw in Alpha Go, right? Or Alpha Zero, rather we said either it's

57:57.920 --> 58:05.360
some limited imitation data or it is from scratch, both of which are still way too expensive in these

58:05.360 --> 58:12.000
kind of domains. And that's where I think Minecraft is different from other games because it's not

58:12.000 --> 58:19.280
one goal, right? It's all about unleashing creativity and solving all kinds of new tasks coming up with

58:19.280 --> 58:26.000
new structures. You can create a castle, you can create a flying dragon, all kinds of new structures.

58:26.560 --> 58:32.480
But you still need to understand the environment and its complexity is huge. It's not one tool,

58:32.480 --> 58:39.440
it's not killing somebody, it's not trying to negotiate and imitate what we saw in training data,

58:39.440 --> 58:47.040
right? It is having to solve and learn something new. But all the information that's available as

58:47.040 --> 58:53.600
foundation models through videos, through text will help us towards that goal. And that's

58:53.600 --> 59:00.800
where I think that benchmark, you know, we got the outstanding paper awarded Nureps, which I'm

59:00.800 --> 59:06.880
very proud of what the team did. Congrats on that. Thank you. But it also paves the way for the

59:06.880 --> 59:13.280
community to take this as a new challenge, right? And this is in a way much, much harder than any of

59:13.280 --> 59:21.520
the game playing bots we've seen solve. But at the same time is to me very timely and relevant

59:21.520 --> 59:26.800
because all these foundation models are becoming so powerful. But it's challenging them beyond

59:26.800 --> 59:32.240
their current capabilities, right? Because it's not just learning what they've already seen,

59:32.720 --> 59:38.240
but pushing the envelope to create new worlds, new structures using all that knowledge.

59:38.240 --> 59:44.560
And is it a formal challenge in the sense that there's a competition and a leaderboard and all that?

59:44.560 --> 59:51.920
We have the website where all the information is available. We haven't launched a leaderboard

59:51.920 --> 59:57.360
because this ultimate goal is very hard, right? So people are solving there is there was a deep

59:57.360 --> 01:00:05.600
mind paper that's all only mining diamonds, for instance. And we are building to end our first

01:00:05.600 --> 01:00:13.520
work showed that you can, you know, do solve multiple tasks by using what we call a mine clip,

01:00:13.520 --> 01:00:19.920
meaning you look at Minecraft videos, you connect it with text. So you create a clip like model

01:00:20.480 --> 01:00:26.960
and use that as a way to get dense rewards, right? And so you make progress towards the goal.

01:00:26.960 --> 01:00:32.400
And that is just the first step. And so we are working on it. We have no others in the community

01:00:32.400 --> 01:00:38.640
are. So as people start solving it, it'll be easy to create a leaderboard. So it's still a bit

01:00:39.520 --> 01:00:44.880
you know, out of reach, I would say from what the community is capable. So that's why it's

01:00:45.760 --> 01:00:55.280
the challenge for the next decade. Do you expect that the models that perform, you know, best in

01:00:55.280 --> 01:01:00.720
here will be based on kind of these foundation models, language models, you know, in some way,

01:01:00.720 --> 01:01:09.040
or is that still, you know, a way to open ended a question or I mean the foundation models are a

01:01:09.040 --> 01:01:15.760
necessity to begin with, right? Because we are providing text instructions on what to do in Minecraft.

01:01:15.760 --> 01:01:21.600
So you're describing the task in text. And it could be entirely new things that, right,

01:01:21.600 --> 01:01:27.440
this agent hasn't seen before. And so you need to go to now the foundation models to understand

01:01:27.440 --> 01:01:32.720
the text, but also understand what this image means, right? And go to the wiki and see,

01:01:32.720 --> 01:01:38.320
oh, this is what a hammer is. Now, how do I go and pick up a hammer where do I find it?

01:01:38.320 --> 01:01:45.360
So even to just get started, all this is necessary, right? And so that because we are not pre-programming

01:01:45.360 --> 01:01:51.760
all of this, whereas if it's one fixed environment, you just give all this rules beforehand. You set

01:01:51.760 --> 01:01:57.760
what the objective is. You design the reward function beforehand, right? So then you let reinforcement

01:01:57.760 --> 01:02:04.160
learning do its magic, which can take very long and can be very challenging to do, but it's all

01:02:04.160 --> 01:02:11.280
preset. Whereas in mind, Ojo, none of this is given beforehand. So to even get started, you need this,

01:02:11.280 --> 01:02:17.280
but it's a great test for foundation models because what can you really learn from it? You know,

01:02:17.280 --> 01:02:24.240
I give you everybody playing Minecraft before. You know, what can you lean from it, but still we

01:02:24.240 --> 01:02:29.760
are asking for something new beyond that. You know, how well can you do problems solving? And I think

01:02:29.760 --> 01:02:35.520
it's exciting to see how that develops. Awesome. Awesome. And I feel like there's a whole interview

01:02:35.520 --> 01:02:44.080
just on on that topic. Absolutely. And in fact, with new reps, there was a workshop on foundation

01:02:44.080 --> 01:02:51.280
models for decision making, and there's just so much excitement, because I think that'll provide

01:02:51.280 --> 01:02:58.000
in my view the right starting point, the right initialization for some of really challenging

01:02:58.000 --> 01:03:03.520
the reinforcement learning problems, because I would argue that reinforcement learning has shown,

01:03:04.080 --> 01:03:09.680
you know, really fantastic gains, but in highly focused and limited domains, right? It hasn't

01:03:09.680 --> 01:03:16.320
become the generalist agent like we see with, say, generative modeling, you know, you can sample

01:03:16.320 --> 01:03:24.320
from distribution through diffusion models or GPT-like models very easily, but that's not task-oriented,

01:03:24.320 --> 01:03:30.800
that's not, you know, learning online with the reward function. And so these two have to marry

01:03:30.800 --> 01:03:37.520
together to bring that generality while being able to learn online, improve, adapt, and make better

01:03:37.520 --> 01:03:44.720
decisions. Yeah. Well, clearly, I mean, three plus years is too long between our conversations,

01:03:44.720 --> 01:03:51.360
because there's so much to cover. Certainly been an exciting time, and I'm privileged to have

01:03:51.360 --> 01:03:58.320
a great team that is enabling me to, you know, think about such a wide range of problems. So that's

01:03:58.320 --> 01:04:03.520
been very exciting. Awesome. Absolutely. Well, thanks so much for joining us and sharing a bit about

01:04:03.520 --> 01:04:09.120
what you've been up to. Thanks a lot Sam. Well, it's been a pleasure and I hope you have a great

01:04:09.120 --> 01:04:39.040
rest of 2023. Absolutely. You too. Thank you.

