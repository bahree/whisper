Hey, this is Sam Charrington, host of the Twoma AI Podcast, and today I'm coming to you live
from the Future Frequency Podcast Studio at the AWS re-invent conference in Las Vegas.
Today I'm joined by Kumar Chalapilla. Kumar is general manager for machine learning and AI
services at AWS. Before we get going, please be sure to take a moment to hit that subscribe button
wherever you're listening to today's show. And if you want to check us out here at the studio,
you can bounce over to YouTube if that's not where you're listening now. Kumar,
welcome to the podcast. Thanks for having me, Sam. I'm really excited to chat with you.
You're part of the team that announced one of the big machine learning announcements here at
re-invent this year. Support for geospatial machine learning. We'll be digging into that,
but before we do, I'd love to have you share a little bit about your background and how you
came into the field. Definitely, yeah. I've been in the machine learning field for almost 25
years now. I have a PhD in machine learning from the late 90s. Back then, I was on the research side,
worked at Microsoft Research for the first five years after that. I'm probably best known
for my work on training deep neural nets on GPUs from long before deep learning became a thing.
But more recently, I've been at AWS for two and a half years. Currently,
general manager for several machine learning and AI services, quite a few of them SageMaker related.
A few years before I came to AWS, I worked on self-driving. Last four years before I started,
worked at Uber ATG, which is now part of Aurora, and also Lift Level 5, which is now part of Toyota.
Okay, super excited to be here. So you have to stay close to the exciting spaces.
I've kind of chased, I mean, it's interesting. People say, if you want to do the best physics,
you go to the place where the largest colliders. If you want to do machine learning,
you follow the data and wherever companies have the best to latest data, they have the best problems
for applying machine learning. And my career has kind of followed that. With AWS, it's amazing.
Now, rather than building actual solutions and applications, I actually help customers build
their solutions and democratize machine learning. And SageMaker is a great way to do that.
You mentioned that you cover several services. Can you talk about the ones that you're responsible for?
Definitely. Today, we announced geospatial support in SageMaker. Super excited about that.
I also manage the human-in-the-loop services in SageMaker. For example, SageMaker Ground Truth
is one where folks who want a label data, label data is important for machine learning.
80% of the models built there are built using supervised learning techniques,
which require large amounts of label data. So AWS offers through SageMaker a data-labeling service.
I also own some of the DevOps and Code Guru services that allow AI and ML techniques to be
applied to improve developer operations, things like increasing availability of your services.
Amazon DevOps Guru is the service there. I'm also the GM for Amazon Code Guru,
which helps you use machine learning and automated reasoning to find bugs in your code,
improve your code quality, and also identify security gaps and recommend fixes and so on.
Those are some of the examples. Is there a method to the madness? It seems like a very broad scope
in terms of the diversity of services. Yeah, so I think the way machine learning works is
the applications are persona-based. So I am one of the GMs who actually I'm the GM that owns
services for software engineers and security engineers when they need machine learning and AI.
So you'll see DevOps Guru, you'll see Code Guru and services in that suite. The other one also
human in the loop. Mechanical Turk is a very well-known service, so that's part of my portfolio.
It's the legacy crowd platform for tasking, and then we also have augmented AI,
which allows you to combine a human with the AI service. And that way even if the AI is 80 or 90%
there, but your customers need 95 or 98%, you can have that synergy between the AI that does
most of the heavy lifting, but then you can back off to a human who can then treat like a labeling
task or even a human task so that you get the quality you need without having to wait for the AI
to mature to meet your customer's quality needs. So I think the human in the loop is the other group.
Do you especially something that I think mapping and geospatial is the other area that is kind
of sister areas to that? Well, let's dig into geospatial. I've been in and around that community
for a little bit, not deeply in, but I was at the company that I was at last did a lot of work
with Esri, and we would go to the geomant conferences. And there has been attempts to, you know,
we've been, that community has struggled with the amount of data that has been collected from
satellites. And as the center, as the center sensors, multiplying, get richer,
that those sensors have presented, and that data have presented lots of challenges.
So I'm curious, like why now? Why is now the time that AWS is investing in geospatial?
Yeah, so whenever we look at building an AWS service, whether it's for machine learning and AI,
you look at where is the undifferentiated heavy lifting? What is stopping the latest GIS
expert or the data scientists who want to work with geospatial data? They really want to solve the
ML or the analytics or the AI problem, but in order to get to that problem, a lot of them have to
walk through how do I manage my data? How do I process it? How do I get access to it? So on the one
side, geospatial data is now become ubiquitous. It's very easy to access high resolution,
yet even sub meter level, like 50 centimeter per pixel data is easily accessible. But if you want
to access that, there's a large friction to getting access to that data. There's months of
negotiation, getting a license, getting that data, and most of them think of it as I need to
download the data and then work with it. So with SageMaker geospatial, one of the things we want
to take away was, can you with one click in a Jupyter notebook or SageMaker studio,
can you just get access to the data? And only the data you need, like San Francisco is 50 square
miles of data, not that hard, but with a single click in an area of interest, can you get access to
all of San Francisco data? And now you can run your fancy computer vision models or your downstream
notebook preprocessing or combine it with other data point of interest data. Maybe you have
customers who have given you location data, photos with that long information that you want to
co-late and understand who your closest people are or what are the closest landmarks,
bring point of interest data. So that ability to bring that data very easily into a notebook
experience, or if you want to push something to production, you want to build a pipeline,
and you want to make that like a production pipeline on AWS or AWS SageMaker, how do you make
that easy? So timing-wise, one is unstructured data is growing a lot. It's becoming very
easy to access the data, and we just want to unlock that. And just to get a sense of the scale,
right, it's not even linear scale, this is exponential. Just two years ago, the numbers satellites
in the sky that were doing things like capturing imagery or whether it would be high
perspective or even sorry imagery of the surface of the earth was just less than a thousand.
Now the zero or early next year, it'll be 10,000, 8,000 or 9,000. In another 10 years,
it'll be at 100,000 satellites. Wow. And so, and these are their multipurpose.
Cameras are getting so good that within two years, the cameras on the satellite go obsolete.
Yeah. So companies are not even trying to keep them up there every two years, and the satellites
are like little shoe boxes. They're not these monsters that they used to send out, they cost millions
of dollars. Now they cost tens of thousands, maybe a hundred thousand dollars to get a little
solid up there. And in two years, that thing will slowly come down and when they won't even
hit the ground, it'll just burn out in the atmosphere. Yeah. Right. And so you, you have this,
this capability of sensors everywhere, right? On the internet, we talk about the IoT of
revolution, but even in space, that that capability is coming, how do you bring all that data
in a place where customers can work with it? And you won't believe the scale of data so large that
if somebody wants to work with California, data, just all of California, all of sudden it's like a
10 gigabyte to 10 terabyte image. We're like, I can work with four megabytes or five megabytes.
My iPhone pictures are a few megabytes. I can work with that. You say, oh, we have recognition
in AWS. We can run it through. You're looking at a few like 20 megapixels already state of the art.
Yeah. I'm talking about 20 giga to 20 terapixel images. And we're like, oh, but then if it's computer
vision, you have a deep learning model, you just scan it all the way across and we're like, okay,
if you can do that with a single click or single API call behind the scenes, AWS can say we'll
take care of slicing and dicing it, tiling it, running all your inferences there and then bringing
it back. Or you want to label the same way you do zoom in pan on your favorite mapping app.
The whole world is there, but as you zoom into your neighborhood, and so how do you bring that
capability in an API that makes it easy for people to, so we kind of try to hide all of that and
make it easy for data scientists and GIS experts to work with that. Awesome. Awesome. Not 10, 10, 12
years ago in the space, when you wanted satellite data, there were really two major providers that you
were engaging in these negotiations with. Has that aspect of it diversified as well? Absolutely.
I think the number of startups that are doing satellites today is enormous. I mean,
there's black sky, there's IC, there's, and they have one or two satellites they start with.
And their purpose built satellites. Previously, these things used to like Macsars,
a well-known company, their satellites are millions of dollars. Today, you need to invest,
and that satellite has to serve for 20, 30 years for you to get value out of that. And launching
that satellite is very expensive. Putting anything in geostationary orbit or even low-earth orbit
is very expensive. And so those are very hard to do. That has become easier. The number of
satellite launches, like if you look at SpaceX, just the ability to move payload into near-earth
or geostationary orbit is becoming very accessible. The other side is the sensor suites.
There are satellites built just for detecting methane. NASA is going to launch one,
methane-sat. GHG-sat is a greenhouse gas that companies name it out. GHG-sat. Their whole
purpose is I will set up sensors there for you so that at a certain resolution, I can tell you
how much methane there is. And it's like for sustainability purposes, measuring and sensing is way
better than predicting and modeling and all that. And once you have that, then you can empower a
lot of applications that want that. Hyper spectral is another very big one. The soil vegetation,
they react differently to visible light versus other spectra. So if you want to know moisture
in the soil, you don't use light because that's not that useful. You can detect water versus land,
but if you want to know whether your soil is dry or what kinds of, I mean, how much nitrogen
there is, those kinds of things you can actually detect with hyper spectral. And so their purpose
is to build satellites for ag tech purpose. Of course, clearly surveillance is another one,
people want resolutions. And so to me, even simple things like supply chain, commercial
applications where we know last two years, everybody talks about supply chain, but we're like,
okay, where are you getting information from? Is there, if I could go to the LA port and count
how many containerships are waiting to get in? And they've been sitting there for three days.
So for years now, like counting cars in Walmart to determine, you know, start, you know, try to predict stock
prices, yeah, stock prices thing like things like that. And the beauty of containers, there's only 24,000
containers in the whole world, right? And they're somewhere between East Asia and America and Europe,
they're moving around. They're huge. They're like size of a, you know, a big bus. And even at a half
a meter pixel resolution, or even like at a meter pixel resolution, they show up as nice
tens of pixels by tens of pixels in size. And then they move, but at the satellite images,
they move relatively slowly. So you can detect and track all of them. And you can count them.
Yeah. Some of the resolutions are high enough that you can read the numbers off of these.
And so you can, you don't even have to worry about tracking them over time. You can just kind of
know how many there are. So, and putting that in the hands of somebody who is a commercial,
you know, either of somebody who's predicting the economy, impact of this, or even predicting
the performance of stocks or businesses, you know, financial industries, insurance companies want
to know. Super powerful. So was there something akin to a satellite data marketplace that a
developer can just go to at the beginning of this process, identify what offerings are available,
and use that as the starting place for building models. Yeah. So SageMaker Geospatial currently
offers a catalog. So you can go in there, you can drop down. So for example, Planet Labs,
for Square, they're data sets that we currently support. We also support Sentinel 2 and Landsat
from Amazon Open Data. So Amazon Open Data is also a larger repository of data sets that are
publicly available. They're free for anybody to download and use are using within SageMaker.
So we're starting with the catalog. We want to grow that over time. SageMaker Geospatial is in
public preview right now. And so it'll go GA soon. So we'll offer a wide variety of sources.
Both commercial and open. Yes, both commercial and open. So Amazon Data Exchange has an open
data set catalog. That's pretty rich. For example, NASA first brings all of their Landsat and Sentinel
data to AWS. And we help them process it and we host it. And everybody else who wants that data
downloads it from AWS will continue to do that for more open source data sets. We have commercial
companies like For Square that offer a point of interest data here in Azure Data sets and they're
also available through Amazon Location Service APIs. For Square is a name that I have not heard in a
long time. What are they doing in this space? They are very good. They're well known for their point
of interest data. And I mean, you probably remember the check-ins. Yeah, check-ins 10 years ago.
It's still, I mean, I think of check-ins as like human curated, highly, what I would call it,
curated data sets versus, I mean, I worked with Uber. Somebody said that this is really a bar as
opposed to, you know, some. Yeah, and they check in, right? And you can even have
there's a currency. Plus, it's just one dot. So on the back end, it's so easy. Like, I worked at Uber
and I worked at Lyfton. When you build these right-sharing apps, you know that for safety and for
predicting ETAs and so on, the what the driver app and the writer app talk to the services in the
cloud all the time so that you know where the driver is and you know how far you are from your
destination, all that stuff. Now, they don't, they collect so much data, but if you ask them,
where was the person picked up? Where was the person dropped off? You have to trust whatever
they chose. If I say, I want to be dropped off in Mountain View and I want to be picked up
at the airport, you trust that that's what things happen. But you know how humans are. We're like,
oh, I've got picked up four blocks away or I asked them to drop me off a little bit earlier because,
you know, a lot. Exactly. And so, whereas here when a person says, I checked in, you don't need
that whole stretch of data. You can get really high fidelity data. That's one of the benefits.
Okay. So I think any time humans curate data, it becomes very powerful. It's almost like label data.
I mean, it's still, there's noise and humans do make mistakes. But I think that data is still
crowdsourced through check-ins. Yeah, there's, and of course, and they've kind of grown beyond that.
I believe today, they also aggregate data from multiple sources just to kind of have a complete
data set. Yeah. They're best known for their places data. They also do foot traffic data.
So they know movement of people. So I think they're their places API and the foot traffic data is
pretty good. Okay. Interesting. Interesting. So we've talked about use cases and ag,
financial services, finance, surveillance. Are there, was there any one of those that was
kind of a standout driver for developing this and offering this service?
We, I mean, there's many verticals, right? We are in public preview. So we want to start with things
that we were passionate about and that customer is really cared about. I think AgTech is one where
we've sort of started with AgTech. We also think automotive is important. For example, BMW is using
SageMaker Geospatial to decide where to put things like their charging stations. They have
information about where their drivers and their customers are driving their cars. Things like
today they're gas cars. And so you know where all the gas stations are. And they want to then
figure out if I were to understand the driving profile and the driving behavior of my customers,
I can offer an EV. And I can also decide whether where to put the EVs among the customers I'm
going after so that they don't never have to go too far to get their car charged. So automotive
is one. The third one is insurance. So a lot of our things around predicting and monitoring
forest fires, floods. So insurance companies want to know what is the likelihood of
the impact to a property or impact to an asset that they're evaluating from one or more of
these geospatial signals that you're looking for. So we're talking about folks developing
applications around this type of data. Let's dig into some of the challenges that they tend to
run into. You've talked a little bit already about this idea of taking away the heavy lifting
and you kind of flew by. Things that came back to mind to be like tiling or the rectification,
all these things that you need to do with this data. Talk a little bit about the pain points
that you're trying to solve for customers. Yeah, so the first one is I mentioned data access.
I think a lot of the data scientists really want to be in the notebook and be able to access the
data as if it was from a database or a native frame which they're used to. And so that's one.
I think the intrinsic thing we want to make very easy is create an experience where they can bring
all their data sources in one place so that they can work with the SageMaker Studio or Jupiter
notebook style like environment. The second one is joins. It is surprisingly difficult
to join geospatial data because think of something like GPS traces. You take a right share
from you get picked up at the airport, you get dropped off at home. You think that GPS sensor
on your phone is so good that it will give you all the right locations along the way. It doesn't.
There's noise there is. If you're lucky, it's less than 10 meters. Even the best GPS traces are,
you know, they're pretty noisy. So the first thing people want to do is clean up the data.
So we offer a map matching service which allows you to take a GPS and say, oh I would like you to
match it with this version of an OSM map. And you can go from several thousand data points to a
few dozen data points which are the intersections and the roads you were at. And that can be a
pre-processing engine that will make it very easy for you to later on reason about things.
So that's sort of one direction we are trying to make things easy. The ability to even post
process, people don't want to work with pixels. They start with pixels because that's where the
data is. Most of the data providers will give you pixels. And you mentioned some of the low level
single processing things like orthorectification. So further upstream. Yeah, so we want to enable
the cloud removal is one. Yeah, very simple. Everybody wants it. And you know, and the question is,
I need just need to know where I shouldn't care because I can't see the things
because of cloud cover, right? But there's deeper and deeper things. The technology for working
with geospatial data today, I come from a signal and image processing background before I got
an machine learning my masters was in signal and image processing. And the technology, I mean
computer versions come so far in terms of like phones, languages and text. But that technology
hasn't reached the geospatial space. So you can apply object detection semantic segmentation.
But it's still new. There's no image net for geospatial. Image net for for image classification
is 10 years old. Alex net is 10 years old. It's amazing. It's the 10th anniversary. And that was one
of the pivotal things that brought deep learning and computer vision to the masses. So I think the
geospatial revolution of that kind is happening now. Maybe it's already two years in, but you know,
there's still some ground. And I want to make that easy for people who are bringing the best deep learning
models and apply it to this data. So I'll give you a simple example. The traditional way of doing
geospatial data processing was more around you had vector data, which is like, let's say a road
network. And you have pixel data, which is satellite imagery. And then you have point of interest
data, you know, high-faltower, Statue of Liberty, your favorite restaurant, whatever. And you would
write traditional algorithms, computer science software, and some of them maybe AI where you
write heuristics and rules and algorithms to search and so on. And that's how you'd operate.
Very hard to create a machine, a machine that says, if you give me more data and more labels,
I can get you a better system. With things like ETA prediction, when will my Uber ride arrive,
or when I start taking the Uber ride, how many minutes will it take me to get to my destination?
All those things are predictive models, right? Clearly the distance and the locations matter,
but there's a lot of data that's coming behind it. In order to make those better and better,
you need this data flywheel. Better data, better labels, a better ML model should be iterated
in a flywheel, since get you a better and better model. That is hard to do with traditional
computer science heuristics or even single processing. So I think by bringing these data sets,
there's also a persona change. GIS is the traditional expert. That field is very deep. I have
a lot of respect for that. And there's so many human vagaries that are there on the road network.
Just complexity of that is mind-boggling. Whereas in the future, I think people will just use
large amounts of data and just like language models. They're going to score it around. You don't
need to know grammar and other things to do like breakdown sentences. You'll just train a huge
neural net that's going to train on insane amount of data and just do things for you. And so
bringing that computer vision technology to these spaces will make it very easy. So traditional
things like object detection, semantic segmentation, and the other beautiful thing I like about
geospatial, which is not the same as computer vision, is you can take pictures of people all day
for 10 years or 100 years. People are going to change. Cool. Is there going to change? The
technology will change. Surface of the earth does not change that fast, right? And there's like
planets that... For cars, if you're trying to identify vehicles, top down. There's a finite number
of them. Yeah. And the surface of the earth is finite, right? Things change, but they don't change
that fast. So with every pass of this data of the surface of the earth, you're almost like getting
higher and higher richer information. Now there's some change. One or two percent changes every
while you want to keep up to the date after date with the changes. But it's almost like watching
something that over time you're getting more and more information about. So it's going to get
very, very good. And so I think when you bring computer vision to that, I think the computer
vision models working on geospatial will be way more accurate. And they can bring a lot of value
because things don't change as much and things don't change as fast. We've talked about these
three different types of data, satellite mapping and point of interest. Does someone need to be
building an application that uses all three to be interested in the SafeMaker geospatial ML? Or
if you're just building something around mapping, for example, is it so relevant?
Yeah. You don't need to. In some cases, you can. There's nothing that limits you.
You don't need to. You don't need to, you don't need to combine all three for the application.
I'll give you an example. Let's say, simple thing, let's say you're a mapping company.
You want to find where the roads have changed. You're looking for changes, but you're not looking
for any kind of change, not where the vegetation's growing. You just want to wear the new roads paved
and maybe my map has some roads that are no longer around or with COVID, they've shot them down
or somewhere. So you need to have a deep role. You can build a deep role in that that extracts roads.
And it's just a, it's a bunch of pixels that it lights up as a mask. You can compare that
against your road network because you can always render a vector data set on an image and then you
can look for differences. And then you can have a human in the loop that just looks at the
differences and goes, oh yeah, that's a new highway. We didn't see that. Oh, that old one. Okay,
so you can update your map using that information. Now here, I gave you an example of combining this
two, but you don't have to. A lot of the, let's say you're an ag tech, you have farms. There's no
people use maps today to navigate. You may not even be close to a road. And so there's no such thing
as a road network for farms, but you still have, you have farms that are not going to ask the
question is do I have to care about the satellite imagery component for this to be useful or if
I'm just doing stuff based on mapping, is it helping me? Yes. Yeah. So let's take the example of
GPS traces and road networks. Okay. You have a data scientist who's sitting on 10 million GPS
traces from phones, people, cars, whatever your GPS. They want to clean that data and they want
to understand what is one of the most visited places. And for that, you, you can use map matching to
clean up your data and get it on a registered vector map. Okay. And then you can do post processing
there and that you can use SageMegra.juice patient for doesn't have to use pixels. Okay.
Now, you'll be surprised if you ask a data scientist today to build an ML model, do something.
The first thing they'll do is they'll render that vector graphic into an image,
pass it into a commercial and build a deep net because that technology is like the big hammer.
That's the tool on the toolbox. Yeah. If you ask them to create custom features, they're like
the hand edited hand features is still, but you can still do random forests with XG boost.
Maybe the more veteran machine learning engineer will do it, but the fresh birds are like,
how can I fit this into a deep net? Because then I don't have to think. Yeah. And it's
so most of the time just worked. It's a big hammer, right? Awesome. Awesome.
So we're talking about challenges. We talk about data access and making that easier. We talked
about your study and talk about off-the-shelf models, so helping folks with common tasks. What are
the tasks that you're currently able to help folks with? Yeah. So SageMegra is built for builders.
SageMegra.juice patient will allow a data scientist who really wants to carve out their special
geospatial ML model from scratch. We'll support that. But there are customers, if you go down to
some of the GIS type of customers or customers who are like, I just want to consume something that's
post processed. They're one of the things we're sort of releasing is vegetation indices. You want
to know, I talked about land classification today. You just want to know what parts have certain
properties for land use. It's like classifying. It's a semantic segment. It's like a productive
land. Exactly. If somebody's studying vegetation growth over time, they can then run this model and
just look at the number of pixels and the map of the pixels over time and say, oh, are the forest
shrinking? Are the forest growing? Are the farmland? What is the state of things? And they want to
operate at the output layer. So you can think of it as another image map, but it's post processed.
You don't have to go back to the raw pixels. And the raw pixels come in multispectrum. There's
multiple bands. You may not want to be at that level. So that's one that people are interested in.
So do you think of things like that as pre-processing steps or transformations? Or do you think of
them as pre-trained models that you're offering folks? I try to think of them as pre-trained models,
because in most cases, some of them will be like, I'm happy with it. I'll use it as is. Other
things are like, oh, it's kind of there, but for my data or for my use case, I want to tune it
a bit more. Or can I get a notebook or a solution that I can just then extend from there? So we're
still in that SageMaker middle layer in the AIML stack that we operate. They're builders, but they
aren't your ML engineer or even like, I'm going to spend three months building infrastructure
type of engineer. They're more like, can I quickly get to a model to see how I can build an
application and solve a problem? So a lot of analytics workloads also come in that model.
So my feeling is that the way to think about it is layered. At the very low level, you want to
give them all the power. So they want to work on pixels, we'll give them pixels and the low level
primitives. If they want to work on higher levels over time, there's some standard building blocks
that are there. And we want to make those available so that if people are okay with something
state-of-the-art or even, you know, standard in the industry, not research made me, then they
can just build on top of it. So that way, they don't need to rebuild things that they're okay or
happy with and then build on top of them. So then we went from data access to this catalog of
pre-trained models, but I think jumped over the primitives step. What are some of the primitives
that you provide to make it easier for folks that, you know, want to roll up their sleeves, but,
you know, also are lazy. Like, good engineers are. So we are, the APIs we're providing at the
really low level are closer to computer vision APIs where you can work with pixels, you can work
with vector data in like point-of-interest data or points and polygons. So think of them as
a road network would just be a polygon with renewable things. And we're also making it easy for
them to access data from Amazon location services through here or through Esri and so on. So that
way, they can build a holistic end-to-end app on AWS using both SageMegade.us spatial and Amazon
location services. So that way, anything that you can bring into the notebook or into your pipeline,
your machine learning pipeline, you can combine and mix and match those.
So two examples, maybe, to be very concrete. One is the map matching when I mentioned,
which is more of an algorithm, but we also have reverse geo-quiting. Your phone might release
a lot long for you. You're like, oh, can you tell me if it's an address, if they checked in,
or you know it's a location, what's the closest location, what's the closest intersection,
what's the closest street, those reverse geo-quiting, that's another primitive we offer.
Did you announce, you mentioned BMW as a customer, what other customer examples were announced?
I think I would go with that one. I think automotive is very good.
Is there one use case at BMW or are there multiple?
There were three use cases that we were doing. One was building driver profiles by understanding
their driving behaviors, and they were using map matching as one of the ways in which they could
understand. Things like, you know, are you an aggressive driver or are you a driver who likes
luxury, and so based on that, and their driving styles, they can recommend. Also, if you're close
to an area where EVs are becoming more in the vogue, then you may be open to that.
So that was one use case. They also wanted to figure out where, like, site selection is one,
where a lot of companies make a decision on where should, I mean, the example they give you.
Superchargers. Yeah, superchargers is now, it's sort of interesting, the site selection is exactly
a supercharger location solving problem. But also, the example they give you, where should I open
my next Starbucks? There's so many Starbucks everywhere. I need to find. But the beauty of that is,
that's a pure non-pixel, non-image type of data set where you want population statistics.
You want to know traffic statistics. Anytime you think about opening a business,
like location, location, location, right? Yeah. If you're going to be selling coffee,
you want to know that there's a lot of foot traffic around that space. You want to know what the
Evan flow is. And you want to be away from other services. If you're going to open like a luxury
store for shopping, you want to know the incomes of that zip code, that neighborhood, and the
flow of things. So opening it, so site selection has that. McDonald's does that too. They need to
know where to open the next McDonald's. And they keep doing this, the reclaim locations,
the ad-new locations, just to optimize their whole collection of stores that they have.
I basically remember a conversation with someone talking about the site selection use case,
and they were starting to explore at the time, this was several years ago, reinforcement learning.
And simulation, as a purchase, do you have you come across overlaps between what folks are
doing and what you're doing in geospatial and wanting to apply RL and simulation? So there's
on the simulation is good for, I think there's a set of services that do 3D reconstruction
of the world, the digital twins. I think there's some convergence going there. Even in computer vision,
I spoke a lot about machine learning version and computer vision, but there's also the reconstruction
part of computer vision, which is like slam algorithms to rebuild 3D models of the world. There's
computer vision algorithms and they're more geometric. They're less about this flywheel of data,
machine learning, and I don't care what's happening under the, just get me the best performance you
can. These are like generative. So you actually have people who are building just cities to the degree
or you have a assembly line and they want to rebuild the assembly line to like within centimeters
of accuracy. Once you have that, then you can do simulations. So some of the customers want to
combine the two, but a lot of the work is in how do you generate that 3D world? And then once you
generate the 3D world using some type of geospatial data processing or even geospatial machine learning,
you can, for example, one of the things that people want to do is they want to know the building
extent, like give me the bounding box for the building and how big is the building and so on. And
they want to reconstruct the building from not just satellite imagery but drone and even in some
cases, light our mapping data. But once you build it, then you're going to ask hard questions like
where should I put my next bridge or where should my next, next highway come because I'm seeing all
this traffic congestion. So then they can do simulations. And a lot of those simulations now scale
to like tens, hundreds, even millions of agents that can move. And then you can ask the kinds of
questions that are generative and then think about what problems you want to solve.
Do you see this interacting with some of the emerging work around diffusion models and generative
models, large models? Possibly. So like I said, it is still early days for geospatial machine
learning. And that's one of the things I'm super excited about, which is the tools and the technology
and the knowledge in the minds of these data scientists is pretty deep. But they've been applying
to like traditional camera from smartphones and even self-driving is more advanced.
Bringing that to geospatial will be phenomenal. And to me, that large language models,
generative models for geospatial will be pretty cool. It's still early that they're able to generate
people's faces, like this person does not exist. They're able to generate art of different kinds
because they have huge collections of those that have been labeled or semi labeled and they've
been able to train. There's a ton of geospatial data but nobody's thought about, hey, can I do
super resolution on these images? Or like I said, cloud cover. You remove the clouds. Now you've
got holes. I would love to see a generative model do that. Or if you see the evolution of a
forest over time, deforestation, you want to predict three, four years from now, how will that
look? So there's a lot of generative capabilities. Plus people just love synthetic stuff. So if you like
to visualize how this area would look if you were built out a certain way. So I think there's room
for that. It's still early. Speaking of synthetic, does synthetic data play into geospatial much at
all? I think we were talking a little bit of, there's geospatial and there's 3D reconstruction
or I think there's simulation there. Yeah, and so I mean, video games are another good example
of like the technologies that they're bringing to these 3D reconstruction, the worlds, even the
meta worse if you want to bring that in, is that you can sample from the real world and then build
it. It's much easier to just replicate something and algorithm can do a ton of that and then you
can have a human artist come in and finish it up versus synthesizing from scratch. So there's
some synergies there. Overall, people go back and forth. Matting is one area where they kind of
go back and forth where when you have sensors like LIDAR combined with aerial and drone, you can
then reconstruct a region with a lot of the computer vision and ML technology, then go back and
make it into wireframe and put it into all these fancy 3D models and then run your simulations.
Going back to the kind of target user, do you see this primarily as making the folks that are
currently doing this more effective by eliminating happy lifting or do you see it more as a
kind of democratization play by enabling a broader community of use of folks to work with
geospatial data because now they have better tools. Very good question. So the place where we're
starting is to make the lives of data scientists who want to work with geospatial data easy. I think
that's what AWS does best. That's what SageMaker is really good at and we want to bring that to the
geospatial community. Now the second one in my vision is there's geospatial experts, the GIS
experts, they're slowly moving to the cloud. That's already happening and they're picking up cloud
skills. You have the modern day data scientist who is really well versed in language vision and
those types of technologies, but they haven't spent much time with geospatial. My vision is that
that community will merge when it comes to geospatial. There's a lot of domain expertise that GIS
folks have which might or might not be needed when the future data scientists get to it, but there
are a lot of tricks in the toolbox for a data scientist that would greatly benefit the GIS
experts. And so the merging or the unification or the synergy, synergistic combination of those
two is my vision for how I want to see where we want to go. So today we're starting with how do we
make it easy to access data, which is very hard. You'll be surprised. 80% of work for data scientists
where animal engineer does is just data munging. Getting access to data, not being happy with the
quality of the data, cleaning the data, labeling the data, and then once you get the flywheel
set up, then it's still the actual amount of time you train and evaluate models, even though
that's sort of that part has been hardened quite a bit. Whereas for geospatial, the early parts
are still, there's a lot of work to do. So my sense is in the early days, I want to make it super
easy to access data and work with data in a notebook environment for a data scientist. But then I
think I see in two to five years a convergence between the GIS community and the data science community.
I would love for every data scientist to know how to work with geospatial data and I would love
for every GIS expert to learn how to use a notebook and use cloud and just scale out and bring
them the best of deep learning to their problems. Awesome. Awesome. Well, Kumar, thanks so much for
taking the time to chat. It's great learning about what you're doing with the geospatial I know.
Thanks, I really appreciate that conversation. Thanks for the opportunity.
