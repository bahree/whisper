Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Let me start by sending a huge thanks to everyone who listened to our podcast series from the
O'Reilly AI Conference in San Francisco last week.
Thanks so much for your feedback and comments.
We're glad you enjoyed the podcast.
From now through the end of the year, I'll be attending a bunch of events and we'll be releasing
a ton of great interviews, so please keep those comments coming.
Reach out to us via at Twimble AI on Twitter or Facebook or via the show notes page for any
episode. In the event you missed our tweets on Friday the 13th, we've got a very special announcement
for you. In a few weeks, we'll be back in New York for the NYU Future Labs AI Summit.
As some of you may remember, we held our very first Twimble Happy Hour in New York City
just a few months ago and it was great, which inspired us to go even bigger this time.
And that is just what we did. We're excited to present the AI Apocalypse and Killer Robots
Halloween Social, which will be held on Monday, October 30th in New York City.
This will be both a fun and informative event and I'll be doing a live podcast taping and
panel discussion on the myths and realities of extreme AI featuring Dr. Seth Baum, Executive
Director of the Global Catastrophic Institute, Shane Hobel, Founder of Mountain Scout Survival
School, and Charlie Oliver, Founder of Tech 2025. This discussion will be interactive,
so we'll get to discuss your most pressing questions about extreme AI and we couldn't
be more excited about this event. For ticket info and more details about the AI Apocalypse
and Killer Robots Halloween Social, visit twimbleai.com slash Halloween. And for 25% off of all
ticket types for the NYU Future Labs AI Summit, use code twimble25. I'd like to take a moment
to tell you about our sponsor for this episode, Nexosis, and thank them for supporting this week's
show. Nexosis is a company focused on providing easy access to machine learning. The Nexosis
Machine Learning API meets developers where they're at, regardless of their mastery of data
science, so they can start coding up predictive applications today in their preferred programming
language. It's as simple as loading your data and selecting the type of problem you want to solve.
Their automated platform trains and selects the best model fit for your data and then outputs
predictions. Get your free API key and discover how to start leveraging machine learning in
your next project at nexosis.com slash twimble. That's n-e-x-o-s-i-s dot com slash t-w-i-m-l.
Head over, check them out, and be sure to let them know who sent you.
One final reminder about the upcoming twimble online meetup. Yes, just a couple of days from now
on Wednesday, October 18th. At 3 p.m. Pacific time, we'll be discussing the paper,
Visual Attribute Transfer Through Deep Image Analogy, by Jing Li Yao and others from Microsoft Research.
The discussion will be led by Duncan Stothers. To join the meetup or to catch up on what you missed
from the first two meetups, visit twimbleai.com slash meetup. Okay, about today's show. A few
weeks ago, I sat down with James Goosa, US Chief Data Scientist at Deloitte Consulting,
to talk about human factors in machine intelligence. James was in San Francisco to give a talk
at the O'Reilly AI Conference on why AI needs human center design. James and I had an amazing
chat in which we explored the many reasons why the human element is so important in ML and AI,
along with useful ways to build algorithms and models that reflect this human element while
avoiding problems like groupthink and bias. This was a very interesting conversation. I enjoyed it
a ton and I'm sure you will too. And now on to the show.
All right, everyone. I am here at the AI conference in San Francisco and I'm here with James
Goosa. And James is the US Chief Data Scientist with Deloitte Consulting and he's going to be
speaking later today actually on a topic that you've heard me allude to here on the podcast,
the number of times, human factors in artificial intelligence. And so I'm really looking forward
to diving into this conversation with you. James, welcome to the podcast. Thank you very much.
I'm happy to be here. Absolutely. So why don't we get started by having you tell us a little
bit about your background? You want to hear my checkered past? I want to hear your checkered
past. Yeah. Well, in fact, I saw in your bio that you've got a PhD in the philosophy of science.
Yeah, I know it's a cliche. Yeah, I have a PhD in philosophy from University of Chicago. I'm a
very intellectually curious person. Actually, when I when I entered philosophy, what I thought
was going to study was artificial intelligence. Really? Yeah, I'm a very old person. So this is
back in the early 90s. And back then, artificial intelligence was, you know, talked about, you know,
a lot of people are connected to this and Jerry Fodorans and so on and so on. And I almost went to
University of Pittsburgh, which had a lot of types of Carnegie Mellon University. And there's a very
strong, there's like one of the strongest philosophy of science programs in the country is at PIP,
right down the street from CMU. And I really thought I'm going to do artificial intelligence.
Long story, I changed my mind and went to the University of Chicago and I got a PhD in philosophy,
but I focused more in philosophy of physics and especially the way statistics is used in physics.
Okay, this is all my way of saying I studied pre-unemployment. I always joke that philosophy is
the Greek word that means pre-unemployment. So, you know, I did, it was, it was wonderful. It was
some of the best years of my life. I love Chicago. I love the University of Chicago. I love
what I studied. It was fabulous. I needed a way to make a living. And this is back in the early
early 2000s, the late 90s. I was, you know, thinking through the various options. I thought,
well, I'm a humanities guy, good at law school. I'm going to do that. You know, I'm, I'm doing
kind of scientific stuff. I could go to Wall Street, right. I could do the whole options thing,
right, which is very sexy back then. I didn't think I, it would culturally work for me. I just,
I just didn't think I, I actually landed a job and went out for the interview. I just didn't
think I'd enjoy it. So, I didn't do it. So, I went, I went for the fame and fortune and glam
or becoming an actuary. Okay. And I didn't know what that meant back then. But I assumed that
actual science meant data science. And it didn't, but now it kind of does. And there's a weird
way in which actual is where the original data scientist, there's a weird way in which my first
data science program or project, which I did at the All-State Research Center in Menlo Park,
California, is a weird way in which that was actually artificial intelligence. I didn't think
of it at the time, but I basically, you know, credit scores, you know, you could think of credit
scores as sort of an early example of AI in a sense that Chris Hammond from Narrative Science
in Northwestern talks about, which is that it's not so much AI from focusing in it from a,
like, through a technical lens, but functionally it's AI because we used to have this whole
profession called bank loan officers. And it was like a lot of people. That was their job.
It was bank loan officers. And it turns out that went out the window when we used algorithms
to make loan, to make lending decisions. And I can get into this. There are a lot of reasons
why that makes a lot of sense. Some of it is on the data side. And some of it is on the human
cognition side. So a lot of reasons why what aspect of this makes a lot of sense. Well, there's a
lot of reasons why there's a lot of reasons why that was an early case where algorithms outperformed
human judgment. And you know, and the use of algorithms kind of like, you know, kind of like a
shrunk a certain part of the workforce, right? And so was this contested like that feels intuitively
obvious to me that, you know, this is a fundamentally database decision. And if you can accurately
characterize the, you know, a person situation, you know, in data, which we are able to do now,
then algorithms are going to do a pretty good job of this over, you know, golf and relationships.
And some of the things that we think of as the past lives of the loan officer. Absolutely.
And I would say yes, but I think that's okay. I think this kind of gets a bonus, but it's always
the interesting part. Absolutely. No, exactly. Exactly. No, this, and this is sort of what it kind of
gets at one aspect of what I'm going to be talking about in my talk this afternoon, which is that
unated judgment is notoriously unreliable when it comes to making judgments and decisions.
And you know, if the listeners have read things like Daniel Kahneman thinking fast and slow,
or nudge by Taylor and Sunstein, you know, or clued by Gary Marcus, the cognitive scientist,
we realized that, you know, our brains evolved. They were optimized by evolution for a certain kind
of environment, you know, outrunning predators in the sedentary, whatever it was. I'm also
thinking predictably irrational by the area. There's another popularization of this whole thing,
exactly. But you know, the real, the real pop classic, in my opinion, is thinking fast and slow
by the economy. That should, that should be in every machine learners shelf, in my opinion. This
is kind of like the compliments machine learning in my opinion. So yeah, I mean, those same sort of
mental heuristics that service well in kind of everyday life, they don't service so well when we
put on a suit and sit around a boardroom and try to decide, should I, you know, acquire this company,
should I admit the student to university? How should I treat this patient? Does this person get
the loan or not? There can be all sorts of biases creeping in. Okay. And then this is the theme
of predictably irrational and thinking fast and slow. So, you know, Kahneman talks about, he calls
the miracles and the flaws of any judgment. It's a paraphrase. The miracle is that most of the
decisions we make in day to day life are what he would call thinking fast. You know, just like,
you know, effortless, they come automatically. We kind of tell a story and the story kind of works.
But, you know, in these kind of mission critical cases where it's more like, you know,
is this person going to, you know, commit a crime or is this person going to pay back the loan
or is this person going to crash his car or does this person have the disease? Not so well,
we really need help from algorithms. But that's kind of like one, one half of the story. I think
another one of the topics that's really that we've all known about for decades, but it's really
becoming, it's coming to the fore is the need to kind of make sure we reflect societal values
in these algorithms at the same time. So, when I was, it all stayed doing this. It's kind of an
interesting story. I wasn't building a credit scoring algorithm for all say because they were
underwriting loans. It's because they're selling insurance contracts. Turns out the credit is
hugely predictive. Who's going to crash their car or have like a water, homeowners claim something
then? We can get into that. It's a very interesting story about like, why is the data so predictive
in that way? But we're also very interested in the legal doctrine of disparate impacts. So,
even if we didn't put like a protected class in the algorithm, there could be an unintended
consequence, you know, where the algorithm could have like systematically different scores
for different groups of people like income or whatever it is. Or urban rule or race or gender
or whatever it is. We don't want that, right? And it's like, you know, some, you know, and it's
very interesting because like, you know, these same early conversations are now reflected in a
larger scale in the world of AI, right? But early on, you know, I'd get into these conversations
with other quants, you know, back then there were actuaries in English. Back then we called it
machine learning to answer your question, right? They were data mining. That's what we called it back
then. KDD was around back then so we called it data mining. But, you know, the actuaries would say,
well, this is just ridiculous. I mean, we just want to come up with the actuarly fair price.
Everybody should just like be charged insurance and reflects their risk. But that might be a
limited perspective. There might be other perspectives that legitimately should constrain models.
So what is the, you know, what is the way to kind of optimize models? Is it, you know,
is it kind of a metric of we're going to like, you know, make this prediction with the greatest
out of sample accuracy? Or is it subject to this that in the other constraint? And some of those
constraints are societal in nature. Some of those constraints are what we were calling human factors
in nature. So there are some cases where maybe it's really complex risk. Maybe it's, maybe I'm
trying to underwrite a very complex loan or a very complex insurance contractor, a very complex
medical diagnosis or a judge making a parole decision. We don't necessarily want to simply turn
that over to a machine, right? We don't just want to automate it away and take humans out of the
loop. But, you know, rather what we want to do is we want to kind of take the best of both worlds
and say, well, the machines are good in one way, you know, they can weigh together 400 factors,
you know, better than we can weigh together four. And by the way, do it the same way before
lunch and after lunch versus after lunch, which we don't do. But at the same time, they don't have
common sense. Humans have common sense. They understand ethics. They understand the strategic goals
of the organization. They understand, you know, you know, societal values, legal constraints,
whatever it is. You know, public relations, whatever it is. And so figuring out how to marry
the best of both worlds. That's part of what I mean by design. You know what I mean? Yeah.
You know, it's like one analogy I've made for a long time is that, yeah, these algorithms
do replace humans to certain tasks. So I always talk about credit scores, like in an early example
of artificial intelligence. For some types of loans or risks or medical diagnosis or whatever,
you just kind of like turn the algorithm on, it'll make a smart decision. And it might be wrong
part of the time, but, you know, maybe the losses are acceptable for whatever reason. Yeah.
In other cases, you just can't do that. And so it's more like there's this art to somehow blending
the machine indication with the human judgment. And that's always fascinated me. That's been
more the late motif of my work at Deloitte since leaving Allstate. You know, at Allstate, we're doing
a lot of, you know, big data, personal insurance. Allstate is the second or third largest
insurance carrier in the country. So they have all this big data. And you can do a lot of this kind
of like, well, we'll just have a, you know, this algorithm, it'll spit out a price. Boom,
there's your price. It's automation. But when I joined Deloitte, there's a lot more work for
smaller medium-sized companies or organizations that want to use data to make more complex
decisions. And so this idea of blending algorithmic indications with human judgment became
much more of an issue. I only came to appreciate this gradually as I was working Deloitte to start.
I started off as a pure quant. I was just interested in the math and I still am. I love it.
So that's the geeky side of me. But there's also this kind of side where like I'm just
fascinated by the way it's used in organizations. I'm just interested in like you need organizational
buy-in. You need to reflect domain knowledge and institutional knowledge in the data,
in the design of the algorithm. You need to think upfront about how is the algorithm going to be
used in the organization? Who's going to be using it? Who are stay-coulders? And if you get all
those things wrong and if you don't plan for them upfront, I won't guarantee it, but I will
bet money. You'll get a negative ROI on your analytics project. So that's why I become
obsessed by this. And I think the exact same issues arise in now the room age of AI.
Yeah. Yeah. One of the things that struck me as interesting in hearing you tell the story about
the work you did at Allstate in particular was it sounded like you were very aware at that time
about you know issues that you know I think of in many ways is like only now kind of
finding contemporary voice. You're right. So you know bias and algorithmic bias. I know it's very
fascinating. Just last night, just last night Pedro Domingo tweeted true fact algorithms
cannot discriminate. And so I replied well how do we define algorithm? What are we talking
about here? Exactly. And so in that lens it's like this is the new issue like we're just
gearing up to fight this fight. Yeah. But it sounds like you were grappling with this way back what?
And not just not just thinking about it, but the impression I'm getting from the way you
described it was that the organization had a consciousness around it. Absolutely.
It's to talk more about this. Yeah, yeah. No, no, it completely is. And I'd love to talk a little
bit more about pro social uses of big data. I have kind of like a little mantra on that.
No, I mean this is one of those things where you know I mean Ben Franklin had decided doing
well by doing good. And I don't want to make any grand claims for my employees or anything
like that. But it's in organizations in light and self-interest to think in a long term.
Maybe in the short term you can like just throw out whatever model you want. But you know
they're smart enough to realize that if there are these unintended consequences it'll come back
to the bite them later on. It doesn't make any sense, right? So since all organizations are
self-interested, does that mean that some are more enlightened than others? I think some are more
enlightened. But I mean it's clear. I mean you know something else we could just be a tangent.
We could talk about a group thing. I mean they're they're you know think about all the organizations
you know or you know both private and public sector organizations that make catastrophically
bad decisions. You know when even this is another interesting thing like our organizations people
will know. There are people that comprise organizations, right? Organizations can act as if
they're rational or not or they're enlightened or not. And sometimes what happens is you'll
meet a lot of very well-meaning people in an organization but they kind of have to self-sensor.
And even if they think there's something that's not quite right. They have to kind of be
their self-sensor or maybe they just get into this habit of believing their elders or their
superiors because of drinking the cool aid. Drinking the cool aid. Yeah I mean this is called
group thing, right? You know it's like the opposite of collective intelligence which is what
the data science should be all about. No so I mean so that was a tangent but I mean I think
this recognition is it was it purely internal? Was it driven by regulatory framework? Yeah
sort of fear. Yeah I don't I don't want to be granteos. I do have a sense. I don't want to be
grandiose. Insurance is very heavily regulated. It's regulated at the state level in fact so it's
not just one agency. It's 50 agencies in the United States and it's actually it's a rare case
where it's actually more heavily regulated in the US than it is in Europe and so they absolutely
in part in fact part of the reason why my employer wanted to build a statue of limitations
applies here. I think one of the reasons they wanted to build this thing in house is that they
actually wanted to have control over the details of the model. They want to be able to make sure
that no we're getting this exactly right you know it maybe it's maybe you can't use medical
bankruptcies in this state. Well we're not see we can prove it because this is our algorithm.
Where's if we bought some black box off the shelf thing we're not sure we've reflected that
regulation in the thing and of course regulations are an attempt to reflect societal values right
so the ultimate thing is you want to reflect societal values in the algorithms and regulations
are kind of a halfway house. I'm speaking philosophically from my perspective but I think that's
what's going on here and so that's the game right I mean the companies want to make sure they're
using algorithms to you know run their their processes more efficiently in the case of all state
you know we want to you know it's the oldest game in the book you want to be able to come up with
a more accurate price for a risk you know the logic of credit scoring and insurance is you know
we all know that six-year-old male motorcycles are bad drivers right or probably risk I should
say risk you've an average drivers perhaps right risk you then perhaps a middle-aged female
station wagon driver perhaps but if you can find the six-year-old male motorcycle driver who's
also present in the chess club subscribe to Martha Stewart Living magazine and has a good credit
score he's probably good risk and if you can collect all those good credit score six-year-old male
motorcycle drivers you can kind of give them a lower rate because they are actually better risks
than might appear on the surface and that means the other companies who don't have credit score
have to charge more for the six-year-old male motorcycle drivers and it's this kind of
adverse selection spiral so that's that's the that's the kind of like economic logic for doing
and this is why insurance is a very early adopter of big data data mining analytics but that's
subject to a constraint I mean if you just did kind of crowdsourcing competition you know come
up with the best segmentation thing right there'll be you know unless you've prepared the data
yourself and unless you're very careful about auditing that algorithm you know you're not sure that
that reflects these regulatory constraints which you know our reflexes are societal values or not
so you know and other things crowdsourcing would be bad in this context I'm just saying you have
to kind of take that into account you're optimizing more than one thing not just out of separate
accuracy but these other things too and you know and I think there are a lot of companies that it's
not just regulation they just want to do the right thing you know I mean like actually Richard
Thaler who's one of my heroes he's a father of behavioral economics of the University of Chicago
Business School he tweeted about I won't I won't say which company it is but it's an airline
that kind of fixed its fees for flights out of Miami at a fairly low rate to help people
escape the storm even though they could have done surge pricing Thaler would say that that
just kind of goes against the grain of human psychology you know we have these things that Adam
Smith called moral sentiments you know which we call ethics now it's like that just doesn't feel
right so even though from a from a technical classical economics homo economic as rational profit
maximizing perspective they should charge $4,000 for a flight to Atlanta from Miami but they didn't
and failure saying it's because it's because they're thinking in the longer term
in that case it wasn't regulation it was just like we're playing the long game
and there are other companies that did jack up the prices right and that's really interesting
actually because they they may be the case I don't know I'm speculating but that may have been
unbridled algorithmic thinking it may be that like a pricing algorithm you know it's quite
possible that some of these competitors did do surge pricing because it's kind of like the
algorithm is just kind of calling the shots and that's what the algorithms usually did absolutely
right and so this is a really nice case where it's it's sort of like parallel I think to the
insurance case except it wasn't due to regulation it was just more due to like customizing value
you don't want to alienate people right people just are going to remember things like this so it's
like our customers but does the algorithm the algorithm would have to have a pretty long
life cycle to pick up on that customer lifetime value yeah that's and that's the point
that suggests that you know it's more likely than not there was human in a loop there precisely
no that that's exactly the point yeah it's like you know we can kind of speculate about
singularities we kind of speculate and have you know fun conversations about what are we going to
reach out to visual general intelligence we have like a robot that can like use common sense and
price it you know both to optimize things but also that's okay but that's not having any time
soon right and we've got these are machine learning algorithms are essentially like statistical
models you know on steroids basically deep learning models are like well just regression
models on steroids that that create their own features right so that's that's what we got
and that's great it's really really powerful but but as you're suggesting I think what it implies
is that we want to have humans that have common sense reasoning to keep the models in check
and so that what that implies is that the people that yeah and I'm going to quote an economist here
and in John Kaye the people that understand that don't do too much of that on this podcast though
well I really I'm just joking sorry
what are your economists yeah that's a big turn on no John Kaye was my favorite I think he's
retired he was my favorite columnist in the financial times he used to be an Oxford economist I
think or London Business School was something but he was asked 10 years ago to diagnose
somebody asking the question in the in the aftermath of the financial crisis why is it that all
these models built by Harvard, Cambridge, MIT, Quance failed so badly and Kaye was so direct
into the point of so elegant he said the problem was that the people that understood the math
didn't understand the world the people that understood the world didn't understand the math and
you know so I think that's yeah that's that's another kind of case of where we need to kind of like
or I think I think I can't remember who said this but I heard a very nice quote of the day that
a really good data scientist needs a kind of communication and empathy ability to be able to kind
of talk to the people that understand the world not just to reflect their knowledge in the data
but also to reflect just the kind of like strategic values the societal values the long true we don't
want to alienate our customers along to all those kinds of things you know what I mean so interesting
so this is all kind of background for your talk like how did you had you organized your talk
did you have a list of human factors that an organization needs to consider or oh no it's
nothing nothing that cutting right honestly nothing about me is like that in my background's
philosophy right I always kind of go back to first principles and I'm just kind of I'm just
really just thinking about what are algorithms good at you know why do we have algorithms what
what you know what are their limitations what are ways of overcoming those limitations and yeah
and I you know I do have I do have some ideas for you know where we can need to you know inject
sort of like extra statistical or extra machine learning or extra computer you know beyond computer
science principles into into what we're doing and so these are all examples that I've been
giving though so the way I structure the talk should we get into that now or yeah the way I
structure the talk is actually I'm quoting someone I know a little bit Chris Hammond at Northwestern
University of Narrative Science he's somebody greatly admire actually that he he and I overlapped
the University of Chicago's I was getting my PhD in philosophy when he was a computer science
professor there so he's now at Northwestern doing really innovative stuff and he's also the chief
science scientist which he's science officer of narrative science the natural language generation
I mean this is really a nice way to think about AI we shouldn't think of AI in terms of the
underlying technology we should really think about AI in terms of like what is its function what
are we trying to achieve here oh well we're trying to automate this process we're trying humans are
really bad at this they fall asleep in the wheel so let's have AI that drives for them or let's
just have AI that kind of recognizes their face when they're getting drowsy like the effective
software right like Rana Alkalubi then kind of nudges them maybe maybe turns the radio up or
something right gives them a punch just like whatever it is I'm not trying to nudge or punch
that's a goal and so some of these things can be done through robotic process automation
which is not even data driven it's just kind of like logic some can be done through deep learning
so yeah sure if you upload my photograph into Facebook it'll say that's a picture of George
Clooney which is a pretty good guess right joke so that that's automation and but there's also
the augmentation side of things which I want to talk about too so that that's that's kind of
like the large structures start off with Chris Hammond talk about the fact that when we talk about AI
it should be kind of like a functional thing not a tech first thing so it's not just about deep
learning it's not just about machine learning it's really like it's really building computer
algorithms that do things that were they to be done by humans they'd be considered intelligent
right that's sort of like that's kind of Chris Hammond channeling John McCarthy at Dartmouth in
1956 he's one of the founding fathers of AI very smart kind of like operational definition
sure and I like it because it just it's a consultant because I'm really a consultant first
I'm a consultant who happens to be a data scientist or then a data scientist who happens to
work in a consulting firm and so as someone who really believes that he's a consultant I think
that's just a really great way of thinking about it because I've seen you know the hype cycles
come and go but over and over and over again I see that the the organizations and the the leaders
who kind of take a tech first view of the stuff it tends to get a lot of attention and buzz early
on but it doesn't really produce the value downstream whereas if you start with kind of like a problem
centric view first and kind of reverse engineer from there well what do I want you're more
likely to succeed and it's likely to be a more efficient elegant and frankly cost effective
solution with less risk and in many cases a lot simpler and a lot simpler would have done if you
were just following shiny object yes no exactly in fact you know again I'm old so I'll you know
I'll I'll quote I'll do some more own quotes one of my someone who's sort of an informal mentor
of mine at the University of Chicago was a very prominent Bayesian econometrician named Arnold
Zelmer and he had a concept called sophisticated simplicity sophisticatedly simple the idea is
that you start up with a simple model and if it works done if it doesn't work you just gradually
add structure right until it does work and then you stop yeah you don't start with most complicated
things it makes you seem like most smart or impressive or macho right and I think that an
analogous comment can be made about our official intelligence and Chris was kind of making this point
of the day in his tutorial which I which I just loved you know he said if you if all you need is
robotic process automation do it what's what's the downside just do it you don't even need big data for
that you just need like you know smart consultants and programmers and you'll just save a lot of
money and there's very little downside risk let me ask you about you do a lot of you well you've
brought up RV a couple of times oh yeah it's it's part of the family of AI yeah I'm just curious
your perspective on this I jump to the question right the question is you know can you provide for
me specific proof point examples where you know people are doing RPA that's you know that suggests
that RPA is more than a rebranding of BPM you know I don't I probably shouldn't comment too much
on that because I'm not like one of our RPA experts and maybe it is it's just like this is this is an
idea that's been around for a long time it just makes eminent common sense I don't I don't really
care what you call it so much but just the idea of taking processes we're just somebody's doing
something that's just like routine and wrote boring spade work and if you can just get a macro
or a script to do that why not do it that that's kind of like analogous zone zone viscous simplicity
like a statistics thing if it's just like looking at the difference of two means is all you need
you know looking to bootstrapping do it and a business context if all you need is to automate
something that's really really wrote in simple and spade work do it you don't need machine learning
for that you know and you know in kind of going up up that kind of food chain of complexity you
just kind of want to start I guess what I was getting is you want to start with the problem
and kind of back into the either technology or the data science and the machine learning whatever
is a little solve the problem and so you mentioned automation and augmentation augmentation what
is that mean for you and how are you seeing folks skin value there I've seen I've seen folks
gain value from augmentation in my whole career I've been in Deloitte since 2001 and it's been one
of the most common themes of what I've been doing we built algorithms that will automate things
but very often you know we don't always work by augmentation are we talking about augmenting
human intelligence okay that's exactly right augmentation or some other yeah yeah
you know I'm talking about augmenting intelligence and that that vocabulary is somewhat new to me
I haven't always described what we do in those terms but I like I like the vocabulary so yeah
I mean you know we very often early on when we do our projects you know like for example suppose
we're working for again with so we're working for an insurance company but say it's a commercial
insurance company so instead of selling auto insurance they say for example they sell workers
confidence now there are fewer businesses to insurer in the world than there are cars and businesses
have fewer factors in common than cars do some are florists some are hipster coffee shops some are
hospitals right and so you have like means you have fewer rows in your database new fewer columns
and you but you're trying to do something similar to what you know my my first job was which is
you're trying to come up with like a better price for the risk or an underrated decision should I
sell this this person insurance or not should I sell your hipster coffee shop insurance or not
and that's the case where like what we found just empirically you know through our data and through
through you know blind test validation is that it would work pretty well in certain cases and
that was an empirical question and it was it was partly empirical partly strategic you know it's
like we'd have to work with the client to figure out what is the cutoff here where we're going
to like straight through processes decisions whereas these other decisions really is going to simply
give it to an underwriter maybe like rank order some things we'll try to explain the underwriter
what's going on here you know we'll try to like train the underwriter ahead of time to understand
the premises of the models and you know if and if we don't do that it's just not going to work
so it's like a very simple example of you know what we were calling human factors earlier I don't
know human factors is quite the right word but it's some some kind of like a either a human centered
or an organization centered design we you know I began to use the analogy you know Mr. underwriter
or Ms. Underwriter just you know imagine that your eyes are myopic and see you go to the doctor
and you get a pair of glasses you can see better well you know Daniel Connemon and Danny O'Reilly
and all these behavioral economists and psychologists teach us that our brains are myopic our brains
have these you know bound biased heuristics that we use to make decisions so they we have blurring
mental vision and so in these in these augmentation cases the algorithms are kind of like prostheses
they're kind of like eyeglasses for the mind's eye they just help de-biased our cognition
so kind of getting that equation right we sort of the art to our science and what fascinates me
about it is that statistics is part of it but not all of it you know so in business we've always
called this kind of change management so this kind of goes into the change management rubric
frankly right now it's in art but I but I like to think that it can become more of a science
so I call this the last mile problem you know we don't stop with an algorithmic output
we stop with the decision in the case of automation the computer makes a decision it's saying
you know I'm just going to send you this ad for these pair of shoes because I think you like these shoes
the augmentation is more like you know I'm going to tell the doctor there's this probability
this person has this rare disease but it's really the doctor's judgment call I'm just going to
and I'm going to tell the doctor why the algorithm thinks this maybe I'll use an information retrieval
system our IBM Watson to give some collateral information but I'm going to give this to the doctor
this is one area where behavioral economics comes back again is that behavioral economics teaches us
that simply giving people information doesn't always result in the optimal decision it's also the way
you present information matters that we've learned this in the last 30 or 40 years this is the
whole basis of the book nut which is only 10 years old so we've really come to appreciate this a
lot more so the so behavioral economics is absolutely a you know one way of thinking of this
quote human factors idea or human centered design idea you know so I feel like we've been sort of
like muddling through perhaps all these years and it works right it's it's it's it's it's more
when I say muddling through I mean it's more an art than a science it's it's something that we've
done for a long time we've gotten better at over time we do it with our with our clients but I
I'm intrigued with the idea that now that AI and machine learning is becoming such a
a business as a sidal trend maybe there can be a new science emerging about this idea of human
computer collaboration or human computer interaction can I give you one more example that sort of like
yes this is not absolutely not a gym example it's not a deloitte example but I find it incredibly
thought-provoking so it's more of a metaphor but it but I find it's a very thought-provoking metaphor
and it's a very nice way to think about sort of the future of work to you know people being
displaced algorithms and so on and forgive me if you've heard this if you heard the sort of
about freestyle chess no I don't think so good thank you I like when people say no I only learned
of a few years ago myself I actually read the article and I forgot it and then I reread it
so I read an article in 2011 by Gary Kasparov the chess grandmaster who's published in the New
York Review of Books in 2011 and he was talking about his own experiences being put out of work
by IBM Deep Blue so this is a prequel to Watson right you know there's like there's a magazine
cover cover called the brain's last stand you know the machine is vanquishing man right the chess
master because that's identified with human intelligence and this is way back in 97 it's like
20 years ago right I mean I thought it was like I just turned 12 I think kidding and but it turned
out the story is a lot more interesting than that Kasparov actually invented a new game after he
lost to Deep Blue called advanced chess an advanced chess would be instead of me playing you in chess
it'd be Jim equipped with a laptop playing you equipped with a laptop and it turns out that the
same skills that enabled Kasparov to be good at traditional chess he wasn't quite as good
as freestyle chess or this advanced chess concept right anyway that's fast forward to the year 2005
and I think a German website had an open game called freestyle chess which is anybody around the
world can enter it can be you know Kasparov playing another grandmaster it can be Kasparov plus
teaming up with a super computer playing another grandmaster teamed up with another super computer
can be anything okay and turns out there's an upset victory the team that one was two amateur
chess players from New Hampshire it's two young guys working with three ordinary laptops equipped
with three different chess programs they won freestyle chess they beat the grandmasters and the
supercomputers and the grandmasters working with the supercomputers supercomputers and Kasparov when he
wrote about this in the New York review of books he said this is a leader called Kasparov's law it's
a weak human plus an ordinary computer plus a better process of working together
outperforms the grandmaster or the supercomputer or both plus an inferior process
and when I'm going to present this this afternoon I'm going to circle the better process
that's what we need and when I when I read that for the second time I'm a slow study you know
when I read this all these years later I realized oh my god that better process of the chess player
working with the computer that's just like what we would do in our consulting practice when we give
like a doctor an underwriter an admissions officer a public sector case worker a list of cases
saying here this will be bias your judgment but it's ultimately up to you and we're going to help
you do this we're going to train you to do it we're going to train you to understand the algorithm
we're going to try to train you to understand you know it's premises it's assumptions the
data it's based on the variables in the model and that way if you know that the the model contains
variables one through forty but you know factors forty one forty two and forty three and if you judge
those to be really important and the algorithm doesn't know that then you you can override the algorithm
and that's okay because you're using your brain you're not just you know using kind of
thinking fast you're not using biased heuristics you're making up using metacognition and using
intelligence to say yeah the computer is saying this but I've also got this common sense of this
other I know these contextual factors I'm going to override it and do this other thing and I
could be wrong but at least it's a principal decision yeah so when I talk about freestyle chess
I'm not trying to make the claim that a human computer will always win chess that's not my point
but the point is that it's a very nice metaphor for this idea that the computer can do things
the humans aren't good at like look through this decision tree of you know you know all these
possible moves and all the implications these moves downstream better than a human can but the
human has other you know kinds of capabilities right it can kind of so it can cast for off comments
when about these two guys who won freestyle chess he said they're insight into looking deeply at
what the computers were indicating it really enabled them to kind of outperform the grand
message in a fair process so they actually had an insight into how the algorithms worked and
developed kind of like an intuitive spidey sense for when should I trust this recommendation versus
that recommendation so I just find it like a very nice metaphor for a real world professional
making a machine critical judgment under it's called judgment under uncertainty with the help
of an algorithm yeah it strikes me that the process in yeah his characterization of this is it's
kind of a lot of vocabulary that has a bunch of individual things under it right there it's like
user interface there's you know the things that we might traditionally think of as a process like
your right steps there's mentioned a bank of experiences to fall back on on how you know
how have I been able to rely on the computer's advice historically have you done or seen any kind of
work to kind of characterize this more more granularly yeah I mean you mean like exactly how do you
pull off this better process you may like what are the steps involved with the principles involved
and so I think ultimately the goal is like as a business if I can you know if I can pick apart the
pieces of what you know process means in this battle that enable you know plus computer to
you know to outperform you know expert then you know that kind of provides me a roadmap for while
first I need to make sure that my data is you know displayed in a way that is comprehensible for the
computer example you know then I need to make sure that I've got the tools available to interact
with you know the systems it's I'm just I'm curious whether you know how evolved the thinking is
there like I said I would like it to be more of a science than it is but I think what ends up
happening is that it's it's it's a series of kind of like what are Simon called satisfying we
make it you know maybe not optimal decisions but we make we kind of like you know look at the
business context maybe never will be a science maybe it's always going to be like it'll always
be like a devil in the details kind of thing you know so like you know if it's a metal if it's
a medical case then the case is where you let the computer just make an automatic decision versus a
human might be different depending on how many doctors are around you know if you're in a poor
country you know it might not be optimal to have a doctor working with a computer but you know if
the village has no doctors at all and you can just like take a picture of a wound you know do deep
learning on and upload it into the cloud and it comes back with like you don't need stitches versus
you do need stitches that's pretty good ideally we'd have a human in the loop you know what I mean
but so that it's asking me like it is modeling through kind of thing like you know what what is
the cut off you know you know in some cases like jurisprudence you know I think Daniel kind of
wrote about this actually a few years ago he said the public would be shocked to hear that like
an algorithm was making decisions without a judge I mean is it even is that even constitutional
so that might be kind of like just like ground you know unavoidable reasons why you always need
to have a human in the loop but I think that you know I think that we are kind of gradually getting
better at this stuff I mean people are coming up with better algorithms for explaining models
like so one of the I'm probably going to get his name wrong one of the speakers early in the
conference Carlos Questren from University of Austin yeah yeah he came up with the Lyme algorithm
right for kind of explaining why does a deep learning model classify what it classifies well
conceptually and I hated to say conceptually it's much more sophisticated but you know we've
always done analogous things with our work right I mean we would we would output not just a score
saying you know the answer is 42 we'd say well what does 42 mean and why does the algorithm think
it's 42 you know so you know so every single score is contextualized with a set of sort of
like English language you know language so again primitive natural language generation but still
nevertheless natural language generation so all these kinds of things we've been doing for a long
time are getting refined you know so we've got better reason algorithms we've got natural
Christmas natural language generation right we've got more advanced data visualization maybe
we're going to come up with better apps so that you know people the emotional aspect of this
important you know John Whalen was speaking yesterday about the emotional quality of this kind of
stuff and he said you know a nice comment he made was that people will choose a personal digital
assistant even if it's the less accurate if it's just more emotionally pleasing to work with
and so even just getting that right is something and that's something that my practice you know
probably could get a little bit better at yeah so these are all different aspects of the human
factors right so it's like some of it is helping us think better but there's a lot of interesting
kind of neuroscience around emotions and I think you know in the last 20 years
so another you know kind of like headline that it's kind of new to a lot of people including me is
that emotions are not kind of like the Mr. Spock versus Captain Kirk thing that we all think of
because it's not like emotions are so like the noise it closes the static rationality it's more
like emotions are sort of part and parcel of the fundamental yeah yeah it's a part and parcel
a big part of what thinking fast that's right thinking slow I'm just messed that up no thinking
fast and slow that's exactly right yeah and also like you know the the effective computing stuff that
Rana L. Kalebi was was speaking about that kind of relates I think you know that that's effective
computing well there's also effective neuroscience and like one of the findings of effective neuroscience
is that healthy emotions are important to rational decision making they're they're not separate you
know and so that's an interesting kind of lens through which to look at this too you know and
again these are these are all these are all developing now so why such an exciting time to be
working in this field so I think the general idea is that I think savvy people have always realized
that when it comes to more complex decisions you don't want to just turn over to an algorithm
sure we're surrounded by more big data now sure our algorithms are getting better sure our
computing power is is getting cheaper and cheaper so sure there will be more and more decisions
that can be automated but until we come up with this kind of singularity which you know
right whatever you know it's not separate podcasts not on that horizon anytime soon yeah we're
you know for a lot of decisions we're going to have kind of humans in the loop we're going to need to
kind of have a science of augmentation this kind of the freestyle x idea so freestyle insurance
underwriting freestyle medicine freestyle jurisprudence freestyle university admissions right it's
the algorithms helping be by us the humans but the humans kind of keeping the algorithms in check
and so getting getting that balance right is that's what I find fascinating that that's what kind
of that's fantastic yeah well I'll mention since you mentioned Carlos Gastran and Ronald
Koyubi I'll note for folks that are listening that both of them have been on the podcast before
great taste until that Carlos at the very first AI conference in New York and Rana at the
previous one in New York this is the third one and so folks can find that find those on the
website both great conversations beautiful and I really enjoy this conversation likewise thank you
so much thank you real pleasure absolutely all right everyone that's our show for today thank you
so much for listening and of course for your ongoing feedback and support for more information on
James or any of the other topics covered in this episode head on over to twomlai.com slash talk
slash 56 and please please please remember to send any questions or comments that you have
either for us or our guests via Twitter at twomlai or at sam charrington or just leave a comment
right on the show notes page for more information about the Halloween social visit twomlai.com slash
Halloween tickets are on sale right now and we do expect a sellout so get your tickets to register
for Wednesday's meetup visit twomlai.com slash meetup thanks again for listening and catch you
next time
