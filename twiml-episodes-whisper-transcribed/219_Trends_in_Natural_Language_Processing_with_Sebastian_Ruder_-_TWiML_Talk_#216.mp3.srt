1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:35,920
I'm your host Sam Charrington to close out 2018 and open the new year we're excited

4
00:00:35,920 --> 00:00:40,640
to present to you our first ever AI rewind series.

5
00:00:40,640 --> 00:00:45,040
In this series I interview friends of the show for their perspectives on the key developments

6
00:00:45,040 --> 00:00:49,720
of 2018 as well as a look ahead at the year to come.

7
00:00:49,720 --> 00:00:54,360
We'll cover a few key categories this year, namely computer vision, natural language

8
00:00:54,360 --> 00:00:59,200
processing, deep learning, machine learning and reinforcement learning.

9
00:00:59,200 --> 00:01:03,840
Of course we realize that there are many more possible categories than these, that there's

10
00:01:03,840 --> 00:01:08,840
a ton of overlap between these topics and that no single interview could hope to cover

11
00:01:08,840 --> 00:01:12,240
everything important in any of these areas.

12
00:01:12,240 --> 00:01:17,120
Nonetheless we're pleased to present these talks and invite you to share your own perspectives

13
00:01:17,120 --> 00:01:25,840
by commenting on the series page at twimbleai.com slash rewind 18.

14
00:01:25,840 --> 00:01:31,360
In this episode of our AI rewind series we've brought back recent guest Sebastian Rooter,

15
00:01:31,360 --> 00:01:37,240
PhD student at the National University of Ireland and research scientist at alien to discuss

16
00:01:37,240 --> 00:01:41,840
trends in natural language processing in 2018 and beyond.

17
00:01:41,840 --> 00:01:46,160
In our conversation Sebastian covers a bunch of interesting papers spanning topics such

18
00:01:46,160 --> 00:01:51,320
as pre-trained language models, common sense inference data sets, large document reasoning

19
00:01:51,320 --> 00:01:55,360
and more and talks through his predictions for the new year.

20
00:01:55,360 --> 00:01:56,360
Enjoy.

21
00:01:56,360 --> 00:02:01,760
Alright everyone, I am on the line with Sebastian Rooter.

22
00:02:01,760 --> 00:02:08,280
You may remember Sebastian from our recent conversation in tumultalk number 195, Sebastian's

23
00:02:08,280 --> 00:02:13,200
a PhD student in natural language processing at the National University of Ireland as well

24
00:02:13,200 --> 00:02:17,840
as a research scientist at alien, Sebastian welcome back to twimble.

25
00:02:17,840 --> 00:02:18,840
Um, thanks.

26
00:02:18,840 --> 00:02:19,840
Great to be back.

27
00:02:19,840 --> 00:02:20,840
Absolutely.

28
00:02:20,840 --> 00:02:27,320
So, as folks should know by now we're doing something special for kind of the end of

29
00:02:27,320 --> 00:02:33,360
year beginning of the year talking with mostly returning guests with some new guests about

30
00:02:33,360 --> 00:02:39,400
what they found interesting in their fields in 2018 and what they expect to see in the

31
00:02:39,400 --> 00:02:41,200
new year.

32
00:02:41,200 --> 00:02:44,440
This is kind of an interesting conversation for us to have because our last conversation

33
00:02:44,440 --> 00:02:50,520
was kind of this like long term look back in time for natural language processing.

34
00:02:50,520 --> 00:02:56,840
But now we'll kind of focus in on the past year and really looking forward to hearing

35
00:02:56,840 --> 00:03:02,600
what your kind of top, you know top developments were in the field.

36
00:03:02,600 --> 00:03:07,440
So why don't we jump in and start with some of the papers that you thought were interesting

37
00:03:07,440 --> 00:03:08,440
this year?

38
00:03:08,440 --> 00:03:09,440
Sure.

39
00:03:09,440 --> 00:03:10,440
Yeah.

40
00:03:10,440 --> 00:03:11,440
I think we can start.

41
00:03:11,440 --> 00:03:12,440
Yeah.

42
00:03:12,440 --> 00:03:18,720
Let's maybe just start with a very uncontroversial one, I think, which is probably on a lot of

43
00:03:18,720 --> 00:03:20,200
people's highlights list as well.

44
00:03:20,200 --> 00:03:25,640
So that is the paper on Elmo that won the best paper award at knackle and peak conference

45
00:03:25,640 --> 00:03:30,400
in the summer this year on deep contextualized word representations.

46
00:03:30,400 --> 00:03:36,160
And I think that particular paper is remarkable for a lot of people just because it was one

47
00:03:36,160 --> 00:03:42,760
of the first, which really demonstrated how useful these pre-trained language model representations

48
00:03:42,760 --> 00:03:47,000
can be on a very diverse array of tasks.

49
00:03:47,000 --> 00:03:51,120
And so I think for completeness we should definitely mention that paper here.

50
00:03:51,120 --> 00:03:55,800
And for me personally, I just would like to highlight some additional aspects.

51
00:03:55,800 --> 00:04:02,440
Maybe people don't really, maybe might not remember given those really superb empirical

52
00:04:02,440 --> 00:04:09,560
results, which are that I personally found that the empirical analysis and the experiments

53
00:04:09,560 --> 00:04:13,600
in the paper were really like exactly well done.

54
00:04:13,600 --> 00:04:18,800
And I really found those that particular section of the paper really a pleasant experience

55
00:04:18,800 --> 00:04:26,400
to read, particularly because I found that the authors really tried actually to really

56
00:04:26,400 --> 00:04:31,440
make a good effort at understanding what is really going on in the network and watch

57
00:04:31,440 --> 00:04:36,040
those particular layers in the model actually learning.

58
00:04:36,040 --> 00:04:41,840
And in particular, I really liked the experiments they did on word sense dissemination and part

59
00:04:41,840 --> 00:04:46,840
of speech training where they essentially took the features learned in the first and the

60
00:04:46,840 --> 00:04:50,960
second layer of this bidirectional LSTM language model.

61
00:04:50,960 --> 00:04:56,480
And just compared that to essentially really state of the art models and both those tasks.

62
00:04:56,480 --> 00:05:03,400
And we're really able to show that just by training this deep language model, a lot of data,

63
00:05:03,400 --> 00:05:09,800
you get representations that really come very close to the state of the art on, yeah,

64
00:05:09,800 --> 00:05:13,360
tasks that have been widely researched.

65
00:05:13,360 --> 00:05:17,760
And secondly, that highlights also that kind of a difference between the layers that in

66
00:05:17,760 --> 00:05:23,160
the first layer, you would kind of get better performance on this part of speech tagging.

67
00:05:23,160 --> 00:05:29,240
Also this more low level syntactic task, whereas with the second layer, the more high level

68
00:05:29,240 --> 00:05:35,160
layer, you would get better performance on this more semantic words as a dissemination task.

69
00:05:35,160 --> 00:05:40,920
And I think this, like, yeah, this as one particular finding was really exciting for me to

70
00:05:40,920 --> 00:05:41,920
read in that paper.

71
00:05:41,920 --> 00:05:47,960
And so for folks that aren't as familiar with the paper, the main thrust or contribution

72
00:05:47,960 --> 00:05:53,400
of the paper was the the architecture they proposed or was it something else?

73
00:05:53,400 --> 00:05:59,520
So the main contribution of that paper was essentially this kind of overall framework

74
00:05:59,520 --> 00:06:07,760
of using a kind of a state of the art pre-trained language model and using that as features with

75
00:06:07,760 --> 00:06:16,600
learned scalar as a using the pre-trained representations or using a convex combination

76
00:06:16,600 --> 00:06:20,560
of the pre-trained representations as an input into a downstream model.

77
00:06:20,560 --> 00:06:29,000
And observing that those, that this particular approach of this framework leads to, yeah,

78
00:06:29,000 --> 00:06:35,560
exceptionally significant improvements on the viability of tasks like reading comprehension,

79
00:06:35,560 --> 00:06:39,080
yeah, maybe anti-recommissioned sentiment analysis.

80
00:06:39,080 --> 00:06:40,080
Awesome.

81
00:06:40,080 --> 00:06:41,080
Awesome.

82
00:06:41,080 --> 00:06:45,120
And that paper was by researchers at the Allen Institute, is that right?

83
00:06:45,120 --> 00:06:46,800
Yeah, yes, exactly.

84
00:06:46,800 --> 00:06:54,440
So that was building on a paper they had last year where they used these similar language

85
00:06:54,440 --> 00:07:00,440
model representations just as an input into a sequence labeling model.

86
00:07:00,440 --> 00:07:01,440
Yeah.

87
00:07:01,440 --> 00:07:05,640
So most of the authors are from the Allen Institute of the University of Washington with the

88
00:07:05,640 --> 00:07:07,400
lead author being Matthew Peters.

89
00:07:07,400 --> 00:07:08,400
Yeah.

90
00:07:08,400 --> 00:07:11,920
All the papers we'll be talking about are relatively fresh.

91
00:07:11,920 --> 00:07:16,960
They just came out, but if you've seen people yet extending this work, there's been a lot

92
00:07:16,960 --> 00:07:19,920
of conversation about this paper in the community.

93
00:07:19,920 --> 00:07:20,920
Yeah, yeah.

94
00:07:20,920 --> 00:07:28,360
So there's been, I mean, it's really been remarkable seeing the, and the additional developments

95
00:07:28,360 --> 00:07:32,800
and really the interest in this overall direction.

96
00:07:32,800 --> 00:07:39,560
So yeah, I personally had a paper with Jeremy Howard from FASTDI where we proposed a similar

97
00:07:39,560 --> 00:07:46,520
idea, placing more emphasis on how we fine tune these language models for different tasks.

98
00:07:46,520 --> 00:07:53,560
And then I think what has captured a lot of people's attention recently was a paper called

99
00:07:53,560 --> 00:07:54,560
Birch.

100
00:07:54,560 --> 00:08:02,720
So they, from Google with a pre-trained, essentially an even bigger model, a deep transformer model

101
00:08:02,720 --> 00:08:07,720
that was previously proposed for machine translation on even a lot more data for a lot longer

102
00:08:07,720 --> 00:08:11,120
using a lot more TVs.

103
00:08:11,120 --> 00:08:17,920
And they were able just using, yeah, kind of a deeper model with some additional modifications

104
00:08:17,920 --> 00:08:23,280
to outperform these previous results by a large margin again.

105
00:08:23,280 --> 00:08:28,280
So that really shows, yeah, so this paper, I mean, we highlighted as one example in this

106
00:08:28,280 --> 00:08:32,640
overall direction, and I think we'll probably be talking more about that in the kind of predictions

107
00:08:32,640 --> 00:08:34,160
for the next year.

108
00:08:34,160 --> 00:08:38,480
But I think there's still a lot of potential that it's left untapped in this direction.

109
00:08:38,480 --> 00:08:40,960
Awesome, awesome.

110
00:08:40,960 --> 00:08:43,000
So what's next on your list?

111
00:08:43,000 --> 00:08:44,000
Cool, yeah.

112
00:08:44,000 --> 00:08:50,800
So the next, the next paper I want to mention is one in a line of essentially papers on

113
00:08:50,800 --> 00:08:54,000
antivirized cross-lingual models.

114
00:08:54,000 --> 00:09:00,480
And I think the paper that is, yeah, particularly interesting to mention here, which was also

115
00:09:00,480 --> 00:09:06,160
highlighted by the community before, so that one, one best paper award at, in the EMLP,

116
00:09:06,160 --> 00:09:11,920
a large, another large NLP conference at the end of this year is the paper, Freyspace

117
00:09:11,920 --> 00:09:19,800
and Neural Antivirized Machine translation by a group of researchers from Facebook I research

118
00:09:19,800 --> 00:09:27,320
with the Yom Lample pioneering or spearheading that.

119
00:09:27,320 --> 00:09:32,360
And that paper was, to me, particularly interesting, because there had been some earlier papers

120
00:09:32,360 --> 00:09:40,040
at I clear this year, which basically proposed first this idea of antivirized machine translation.

121
00:09:40,040 --> 00:09:47,920
So really, yeah, they proposed kind of three papers independently posed two models that

122
00:09:47,920 --> 00:09:53,120
were maybe not for the first time, and show there's been some classic approaches before,

123
00:09:53,120 --> 00:09:58,440
for the first time, with these more recent deep neural network-based approaches able

124
00:09:58,440 --> 00:10:05,960
to only give a mono-lingual copper into languages, so normally when you train a machine translation

125
00:10:05,960 --> 00:10:12,280
model, you really want to have a parallel, parallel corpus, which is a corpus that contains

126
00:10:12,280 --> 00:10:16,880
pairs of sentences in one language and their translation in the other language.

127
00:10:16,880 --> 00:10:20,680
And the models they proposed were just able to take these mono-lingual copper, which were

128
00:10:20,680 --> 00:10:28,240
not aligned, and just by having these mono-lingual copper learn in alignment and learn a translation

129
00:10:28,240 --> 00:10:35,400
model that is able to produce somewhat okay transitions.

130
00:10:35,400 --> 00:10:41,620
So in those papers, we mainly prove a concept showing that this particular framework

131
00:10:41,620 --> 00:10:49,120
of paradigm actually worked, and the results weren't still quite a bit ways below suffice

132
00:10:49,120 --> 00:10:57,680
approaches, and in this kind of extension or, yeah, building on those ideas from earlier

133
00:10:57,680 --> 00:11:05,200
this year, this new paper kind of took those ideas and made them a lot more robust and

134
00:11:05,200 --> 00:11:11,720
useful in practice, in particular by using many ideas from classical statistical and

135
00:11:11,720 --> 00:11:19,400
phrase-based machine translation models and really getting those ideas to work and really

136
00:11:19,400 --> 00:11:22,440
show very good results on non-different datasets.

137
00:11:22,440 --> 00:11:31,280
And so for this one are the corpora parallel at, for example, the document level, but

138
00:11:31,280 --> 00:11:38,960
just not aligned sentence or phrase by phrase, or are they, let's say random kind of collection

139
00:11:38,960 --> 00:11:46,480
of documents in one language and another random collection of documents in the other language.

140
00:11:46,480 --> 00:11:51,880
So the, yeah, the corpora mostly random collections of documents in both languages, so essentially

141
00:11:51,880 --> 00:11:58,720
I think they crawled corpora, like new scopra online or used unlabeled corpora that were

142
00:11:58,720 --> 00:12:02,760
previously available for machine translation research.

143
00:12:02,760 --> 00:12:07,000
So I think there's maybe some kind of topical alignment in that we have new scopra talking

144
00:12:07,000 --> 00:12:12,080
about different things and probably I would, my intuition would be that if you would train

145
00:12:12,080 --> 00:12:17,600
these models with totally, with corpora coming from totally different domains, like biomedical

146
00:12:17,600 --> 00:12:25,920
and, yeah, new scopra they likely wouldn't do as well, but yeah, still you don't really

147
00:12:25,920 --> 00:12:30,160
have any sort of alignment or the corpora itself are really easy to collect as you just

148
00:12:30,160 --> 00:12:33,080
need to scrape new scopra essentially.

149
00:12:33,080 --> 00:12:40,440
Is there any indication that this kind of approach can extend beyond machine translation

150
00:12:40,440 --> 00:12:42,560
or cross-lingual task?

151
00:12:42,560 --> 00:12:50,720
Did we learn anything in this paper that is broadly applicable to unsupervised NLP tasks?

152
00:12:50,720 --> 00:12:55,560
Yes, so what I really like was that in the paper, in the beginning they took some time

153
00:12:55,560 --> 00:13:02,680
essentially laying down the three key requirements for this sort of unsurvice machine translation,

154
00:13:02,680 --> 00:13:08,880
which are, I think, in a sense, ones that are generally useful for many other unsupervised

155
00:13:08,880 --> 00:13:16,560
tasks in NLP, and these are in particular having a good initialization, doing something

156
00:13:16,560 --> 00:13:21,520
that allows you, doing essentially language modeling, so something allows you to get representations

157
00:13:21,520 --> 00:13:29,720
of the language, and having something that allows you to model the inverse task or, so

158
00:13:29,720 --> 00:13:34,960
in their case, they use back translation, which is a technique of getting, if you don't

159
00:13:34,960 --> 00:13:41,560
have, or if you only have mono-lingual corpora available, to still get that this kind of alignment

160
00:13:41,560 --> 00:13:47,120
pair that you can train your supervised models on, by just using your current or the current

161
00:13:47,120 --> 00:13:54,240
state of your model to translate your mono-lingual data, and that's using that as kind of a weak

162
00:13:54,240 --> 00:14:02,240
supervision signal effectively, and this sort of back translation essentially can be seen

163
00:14:02,240 --> 00:14:10,080
as it allows the model to, given only those data from one of the sources really enforce

164
00:14:10,080 --> 00:14:16,600
something like a cyclical consistency, so it allows the model itself essentially to

165
00:14:16,600 --> 00:14:20,080
learn to be more consistent with itself.

166
00:14:20,080 --> 00:14:24,240
So I guess you would, yeah, kind of people in the computer vision community have seen similar

167
00:14:24,240 --> 00:14:29,880
ideas in cycle-gan, for instance, which can be seen as a similar idea.

168
00:14:29,880 --> 00:14:36,760
We have also non-parallel sets of images from different domains, and the model then learns

169
00:14:36,760 --> 00:14:45,920
to translate or to transfer one image from one of the domains to the other domain, basically,

170
00:14:45,920 --> 00:14:54,840
also having this principle of doing this inverse modeling or enforcing this cyclical consistency.

171
00:14:54,840 --> 00:15:00,400
So I think these notions are generally applicable, and then in particular as well, this notion

172
00:15:00,400 --> 00:15:06,600
of having good initialization that just allows you, because the optimization process in these

173
00:15:06,600 --> 00:15:13,320
unsurveys approaches can be really very noisy or can be very challenging optimization problem,

174
00:15:13,320 --> 00:15:18,760
so you really want to start out from as good of an initialization as possible.

175
00:15:18,760 --> 00:15:25,360
And in their case, they use, essentially, a shared word piece vocabulary, but we've also

176
00:15:25,360 --> 00:15:34,040
seen there was another paper at ACL that I particularly liked by attaching and others

177
00:15:34,040 --> 00:15:40,520
who proposes, basically, a better initialization for learning cross-linked embeddings by effectively

178
00:15:40,520 --> 00:15:47,280
incorporating some sense of his domain knowledge of the problem, that words that are similar

179
00:15:47,280 --> 00:15:53,360
to each other in other languages have similar distributions of words they are similar to

180
00:15:53,360 --> 00:15:54,360
in intern.

181
00:15:54,360 --> 00:15:57,960
I mean, we could talk more about this figure here, but I think we probably want to cover

182
00:15:57,960 --> 00:15:58,960
a few more.

183
00:15:58,960 --> 00:16:06,680
Yeah, I think we could probably do whole shows on each of the papers you're talking about,

184
00:16:06,680 --> 00:16:08,680
but let's move on to the next one.

185
00:16:08,680 --> 00:16:15,120
Yeah, of course, so the next one is, I think, yeah, it's also, I think, for me, in line

186
00:16:15,120 --> 00:16:20,000
or an example of a larger trend that I found particularly interesting this year, which

187
00:16:20,000 --> 00:16:28,320
is kind of this increasing awareness of having to incorporate common sense into our models,

188
00:16:28,320 --> 00:16:33,800
so because many people have observed in different papers or different studies that the current

189
00:16:33,800 --> 00:16:41,400
models are brittle or in many different ways, so that they don't actually encode information

190
00:16:41,400 --> 00:16:48,400
that assumes we find plausible or that makes sense to us, so that they don't really have

191
00:16:48,400 --> 00:16:54,280
this notion of like causality or of common sense, really.

192
00:16:54,280 --> 00:17:00,800
And in this line, there's been some efforts mainly on creating different datasets that allow

193
00:17:00,800 --> 00:17:11,800
us to train models that can exhibit some of these common, common sense inference mechanisms.

194
00:17:11,800 --> 00:17:16,960
And in that line, there's been, yes, and great work coming from the University of Washington

195
00:17:16,960 --> 00:17:25,480
from the Jitren Choice Group, so she had two papers on proposing two datasets, event

196
00:17:25,480 --> 00:17:36,200
two mind and swag, which, yeah, kind of in both instances, create a task that tries

197
00:17:36,200 --> 00:17:42,680
to capture some of these commonalities, so for swag, for instance, they used captions

198
00:17:42,680 --> 00:17:50,200
from images to essentially create a multiple choice task where you have a sentence and

199
00:17:50,200 --> 00:17:58,000
then four possible subsequent sentences to that, and the model then has to decide which

200
00:17:58,000 --> 00:18:04,200
of those four possibilities is the most possible one or which is the actual one that would

201
00:18:04,200 --> 00:18:09,640
plausibly follow the current one, and what I particularly liked about their paper was

202
00:18:09,640 --> 00:18:16,240
that they really tried to take some of the lessons we've seen recently in that many of

203
00:18:16,240 --> 00:18:22,520
the previously proposed datasets exhibited different sort of biases, so often the models

204
00:18:22,520 --> 00:18:29,880
trained in those were able to exploit some patterns in how the human, like how the human

205
00:18:29,880 --> 00:18:36,800
annotators generate the data, because often generally people who would enter those datasets

206
00:18:36,800 --> 00:18:38,480
are on mechanical chalk.

207
00:18:38,480 --> 00:18:44,720
They don't have a lot of time, so they often try to take shortcuts if possible to just

208
00:18:44,720 --> 00:18:49,800
generate those examples faster, and those shortcuts would lead to some patterns in the

209
00:18:49,800 --> 00:18:56,080
data that a model would be able to exploit, and that would then kind of show the spritleness

210
00:18:56,080 --> 00:19:03,880
of these models that a lot of people have criticized, and in this swag paper, a very, very

211
00:19:03,880 --> 00:19:12,880
nice acronym as well, they took some measures to project against these biases by proposing

212
00:19:12,880 --> 00:19:20,960
a new mechanism that uses some other models as well to more effectively select these possible

213
00:19:20,960 --> 00:19:27,280
negative answers in each example, and I think that's a really useful direction, and I think

214
00:19:27,280 --> 00:19:33,920
we'll probably see kind of in future dataset papers, at least some discussion of bias,

215
00:19:33,920 --> 00:19:40,120
and hopefully some measure or some means to prevent that prematurely.

216
00:19:40,120 --> 00:19:52,040
Now, this is the formulation of their problem where the prediction task comes before the

217
00:19:52,040 --> 00:19:59,880
contextual sentences, is that unique or novel, most of the time the prediction task follows

218
00:19:59,880 --> 00:20:03,840
the contextual sentences, is that right?

219
00:20:03,840 --> 00:20:11,760
So typically you would have, I mean, it's kind of rare, so you have like in most cases

220
00:20:11,760 --> 00:20:16,520
in those Q&A datasets, you have some sort of like passage that they're reading and they

221
00:20:16,520 --> 00:20:22,320
try to select some answer based on that, or you have some like challenging datasets

222
00:20:22,320 --> 00:20:27,920
in the past, like the Reno Grad schema where you have a sentence, and then you try to disambiguate

223
00:20:27,920 --> 00:20:34,120
a certain word in that sentence, like it's more kind of akin to word sense disambigation

224
00:20:34,120 --> 00:20:43,040
in context, and in contrast to those, the dataset is more really about kind of this notion

225
00:20:43,040 --> 00:20:49,000
of causality and really events that plausibly follow each other, so for instance in example

226
00:20:49,000 --> 00:20:56,480
here, there's a sentence where which describes that a woman takes seat at the piano, so given

227
00:20:56,480 --> 00:21:02,160
that real world situation, is it more likely that the woman then in the next sentence plays

228
00:21:02,160 --> 00:21:09,560
with the doll, does she just smile, are there dancers in the crowd, or does she actually

229
00:21:09,560 --> 00:21:12,120
put her fingers on the keys to play?

230
00:21:12,120 --> 00:21:17,120
So it's really early, yeah, so this dataset very much is about really understanding kind

231
00:21:17,120 --> 00:21:23,400
of these real world knowledge and understanding what kind of events follow each other in

232
00:21:23,400 --> 00:21:24,400
the real world.

233
00:21:24,400 --> 00:21:31,560
Okay, and just to be clear, are the sample sentences like the ones you described in that

234
00:21:31,560 --> 00:21:37,840
order where you get one or multiple contextual sentences, and then the multiple choice, or

235
00:21:37,840 --> 00:21:42,880
is it the other way around where you, the multiple choices, the context, and then you get

236
00:21:42,880 --> 00:21:47,240
the sentences that follow and the task is to try to guess the context.

237
00:21:47,240 --> 00:21:52,200
Yeah, so it's the first way, so you first have the sentence, and then you have multiple

238
00:21:52,200 --> 00:21:55,040
choice questions, so understand that.

239
00:21:55,040 --> 00:22:00,840
Yeah, exactly, and there have been, I mean, there's been some other datasets before as well,

240
00:22:00,840 --> 00:22:08,520
I think there is one, the Lombada dataset, and a lot of those are based on stories which

241
00:22:08,520 --> 00:22:16,200
give you some sentences or a paragraph of the story, and then ask you to kind of decide

242
00:22:16,200 --> 00:22:24,480
either what is the next sentence or whether certain, what is certain entity in the next sentence.

243
00:22:24,480 --> 00:22:28,920
So these are very kind of automatically generated based on stories, and in comparison to that,

244
00:22:28,920 --> 00:22:32,000
this one really emphasized this kind of more event-based notion.

245
00:22:32,000 --> 00:22:36,440
Cool, and so what's the next one on your list?

246
00:22:36,440 --> 00:22:45,720
Cool, yeah, so the next one was something that flew a bit under people's radar mainly,

247
00:22:45,720 --> 00:22:51,600
so I found that particularly interesting because it combines seamless-wise learning, so

248
00:22:51,600 --> 00:22:56,880
learning, essentially, so these previous approaches to transfer learning that I mentioned,

249
00:22:56,880 --> 00:23:03,160
they essentially learn from a large, unlabeled corpus to get pre-trained representations,

250
00:23:03,160 --> 00:23:08,160
and seamless-wise learning in general often kind of assumes that the unlabeled data is

251
00:23:08,160 --> 00:23:13,360
from the target task you care about, so if you have a seamless-wise learning model,

252
00:23:13,360 --> 00:23:19,120
you might apply that to sentiment task, then you have some additional reviews in that domain,

253
00:23:19,120 --> 00:23:26,560
basically, and a lot of these transfer learning approaches can also be used in that setting,

254
00:23:26,560 --> 00:23:32,480
but this particular paper proposed some ideas that are complementary to these more

255
00:23:32,480 --> 00:23:41,120
language-model-based representations in that it explicitly encourages a model to be more robust

256
00:23:41,120 --> 00:23:46,480
on unlabeled data, basically, so the paper I'm talking about is the seamless-wise sequence

257
00:23:46,480 --> 00:23:55,360
modeling with cross-view training that was presented at MLP 2018 from Clark and collaborators,

258
00:23:56,240 --> 00:24:06,320
and they take some, so you can see their paper in line of some recent seamless-wise learning

259
00:24:06,320 --> 00:24:13,760
approaches, mainly in computer vision, and in a lot of those approaches models try to enforce

260
00:24:13,760 --> 00:24:20,720
some notion of consistency within the model's prediction on unlabeled data. So in a lot of cases,

261
00:24:20,720 --> 00:24:27,120
we had, I think, in 2015, there was a paper called The Letter Networks, and that essentially

262
00:24:27,120 --> 00:24:35,200
enforced that a model's prediction under noise. So if you have your original model and the model

263
00:24:37,200 --> 00:24:42,480
where you add noise to, the predictions of those two variants should be the same on unlabeled data,

264
00:24:43,440 --> 00:24:54,160
and then recently we've had some more similar approaches in that line, which try to make the model,

265
00:24:54,160 --> 00:25:02,720
the model's predictions consistent with the its predictions in the past. So some of these

266
00:25:02,720 --> 00:25:11,120
approaches also called self-assembling approaches. So we can see that in this design of works,

267
00:25:11,680 --> 00:25:18,320
and this paper in particular proposed a very, very simple framework. In that you have your,

268
00:25:18,320 --> 00:25:24,640
say, for an NLP task, you have your bilostium encoder, you feed your sentence through that and get

269
00:25:24,640 --> 00:25:30,240
a prediction for your, for your labeled example. So you still train that as normally on your labeled

270
00:25:30,240 --> 00:25:36,400
examples, and then on unlabeled example, you would again feed your input through that, and you

271
00:25:36,400 --> 00:25:42,000
would get a prediction on your unlabeled example. So this is the prediction of your main model,

272
00:25:42,000 --> 00:25:51,360
and then you would kind of iteratively or alternately obfuscate different parts of the input.

273
00:25:51,360 --> 00:25:58,640
So say if the input sentence was, they traveled to Washington by plane in like a name anti-recognition

274
00:25:58,640 --> 00:26:05,680
task, you would, in one auxiliary model, remove the last part of the sentence. So you would have

275
00:26:05,680 --> 00:26:12,480
an auxiliary model, you would only feed in, they traveled to into your bilostium encoder and get

276
00:26:12,480 --> 00:26:19,760
another prediction. Then you would do that there for other parts where you fuscate other parts of

277
00:26:19,760 --> 00:26:25,440
the input like they traveled to Washington by plane. And for each of these auxiliary models,

278
00:26:25,440 --> 00:26:30,480
you would get a prediction based on the model only seen parts of the input, and in the end,

279
00:26:30,480 --> 00:26:36,480
you would enforce those models, those losses to be similar to the or consistent with the model

280
00:26:37,200 --> 00:26:44,720
seeing the full example. And that essentially enforces that the model is kind of less sensitive

281
00:26:44,720 --> 00:26:51,280
to some of these local features in the input, but really encourages the model to pay attention

282
00:26:51,280 --> 00:26:58,320
to, yeah, particular words like travel that actually travel highlights some highlights that

283
00:26:58,320 --> 00:27:04,240
traveling is to location and really makes model, yeah, also be able to learn from unlabeled data,

284
00:27:04,240 --> 00:27:10,480
these useful patterns effectively. It kind of reminds me of dropout in the language domain as

285
00:27:10,480 --> 00:27:16,560
opposed in the network domain. Yeah, exactly. So it's very similar to like a word dropout essentially,

286
00:27:16,560 --> 00:27:22,240
so people have tried this like dropping out individual words in input rather than dropping

287
00:27:22,240 --> 00:27:28,960
about hidden states like in record dropout. And so I think this is very similar to that. And maybe

288
00:27:28,960 --> 00:27:35,680
the main difference is that in regular or in this word dropout, you're still mostly restricted

289
00:27:36,480 --> 00:27:41,680
to learning on labeled examples. So if you can really only apply the dropout on just making

290
00:27:41,680 --> 00:27:46,000
your model more robust given the label data that you have, and with this additional objective,

291
00:27:46,000 --> 00:27:50,800
you can even, because you're not trying to predict a label, but just enforcing this sort of

292
00:27:50,800 --> 00:27:55,680
consistency, you can also apply that to unlabeled data. Yeah, I think you've got a couple more papers

293
00:27:55,680 --> 00:28:02,000
yet, right? Yeah, so I think maybe one of the last ones I'd like to mention is kind of in line

294
00:28:02,000 --> 00:28:08,240
with, I think, one of the broader or main challenges in MLP going forward, which is really

295
00:28:08,240 --> 00:28:14,080
making sense of very large documents. So we, you have that in summarization, for instance,

296
00:28:14,080 --> 00:28:19,360
where if you want to summarize a very large document, that is still something that current

297
00:28:19,360 --> 00:28:26,240
approaches struggle with. And similarly, if for, say, current question answering systems,

298
00:28:27,200 --> 00:28:34,560
most of those approaches are based on answering questions from on a paragraph. And so really

299
00:28:34,560 --> 00:28:41,200
a kind of a direction and challenge going forward is to expand this context from paragraph to

300
00:28:41,200 --> 00:28:48,800
entire articles. And then hopefully to longer, yeah, larger bodies of information, a collection of

301
00:28:48,800 --> 00:28:55,360
documents, or even whole narratives, like in books or movie scripts. And the paper in that

302
00:28:55,360 --> 00:29:02,560
line that I want to highlight here is from some researchers from DeepMind, the narrative QA

303
00:29:02,560 --> 00:29:09,440
reading comprehension challenge. That was at, yeah, was published in a tackle, the journal of

304
00:29:09,440 --> 00:29:15,280
computational linguistics. And this particular paper I like, so this paper also presented in new

305
00:29:15,280 --> 00:29:22,240
dataset for for question answering. And what I like particularly about this paper is that,

306
00:29:22,240 --> 00:29:29,680
so generally, if you, so the dilemma really is that because current models really are only

307
00:29:29,680 --> 00:29:35,120
able to do really well on, if you have a paragraph as input, if you want to propose a new

308
00:29:35,120 --> 00:29:42,800
dataset or a new challenge that requires models to answer questions based on entire books,

309
00:29:42,800 --> 00:29:48,560
that will be too difficult for current models. So you won't even be able to apply existing models

310
00:29:48,560 --> 00:29:54,320
to that just because the task is so much different to what we have. So you really want to have

311
00:29:54,320 --> 00:30:00,880
something that is both a kind of a challenge and like a more long term direction, but something that

312
00:30:00,880 --> 00:30:06,480
also allows you to apply current models to the task. And I think they did a good job of striking

313
00:30:06,480 --> 00:30:12,320
the balance between those two extremes. In that they had as part of their dataset, they proposed

314
00:30:12,320 --> 00:30:18,160
this kind of overall challenge of answering questions about books and movie scripts.

315
00:30:19,360 --> 00:30:26,400
By the questions, the way they obtained those were based on summaries of those movie scripts

316
00:30:26,400 --> 00:30:34,080
and of the books. So humans were provided with a summary of the of the book or of the movie script

317
00:30:34,080 --> 00:30:40,240
and had to generate possible question answer pairs based on that. So so now to make it

318
00:30:40,240 --> 00:30:47,600
so in the near term to make models so to make this as easier for models, they can then also be

319
00:30:47,600 --> 00:30:51,840
supplied with the summaries that were actually used to generate these question answer pairs,

320
00:30:52,800 --> 00:30:57,920
which makes it then more similarly existing datasets like squat and which allows the application

321
00:30:57,920 --> 00:31:03,520
of existing models to that. And then as we find that current models really are able to do very

322
00:31:03,520 --> 00:31:11,200
well in that, we can then essentially scale up the context, incorporating kind of the the book

323
00:31:11,200 --> 00:31:19,360
or the movie script context as well to get closer to that overall challenge. And yeah,

324
00:31:19,360 --> 00:31:24,800
particularly in the paper, they also talk about so one way that you can already try to leverage

325
00:31:24,800 --> 00:31:30,400
some of this entire content of the book is to use it in a two step way where you first apply

326
00:31:30,400 --> 00:31:36,880
information retrieval approach to that. So you first try to retrieve in like relevant sentences

327
00:31:36,880 --> 00:31:43,200
based to the question from the entire book and then use those for giving an answer. But as I say

328
00:31:43,200 --> 00:31:48,560
in the paper that task because in a book, yeah, book is not something you would typically apply

329
00:31:48,560 --> 00:31:54,320
IR to and you you'd have a lot of sentences that are very similar and particularly when the question

330
00:31:54,320 --> 00:32:01,280
is about some main characters, you have many sentences talking about those. So it's still by itself

331
00:32:01,280 --> 00:32:07,280
even a very challenging IR problem. What kind of experimental results did they present in the

332
00:32:07,280 --> 00:32:13,680
paper? Yeah, I fed all. So essentially they present results both on this kind of simplified

333
00:32:13,680 --> 00:32:20,320
setting where you answer questions based on the summary that the annotators use to generate

334
00:32:20,320 --> 00:32:27,280
the original questions. So to that setting, they just apply existing models like existing,

335
00:32:27,280 --> 00:32:33,200
say if they are reading comprehension models that have achieved good performance on some of the other

336
00:32:33,200 --> 00:32:41,520
tasks. So they show those results and then basically propose this two-step approach for that I

337
00:32:41,520 --> 00:32:50,000
just mentioned for using the entire book and present some results for those as well given

338
00:32:50,000 --> 00:32:58,960
different different base lines and different encoder methods as well as kind of looking if you

339
00:32:58,960 --> 00:33:04,400
had a perfect IR model for instance, I would be able to give the most relevant sentences given

340
00:33:04,400 --> 00:33:12,080
the answer actually how good the models would actually perform then. In addition, they also simplify

341
00:33:12,080 --> 00:33:18,320
the task a bit or try to give like another simpler option of the task rather than generating the

342
00:33:18,320 --> 00:33:26,320
answer selecting the answer from a candid set of 30 different answers. So rather than having actually

343
00:33:26,320 --> 00:33:31,280
the more challenging task of generating the answer, you can just reduce the two classification

344
00:33:31,280 --> 00:33:37,280
setting which also in the near term might make it more accessible to do this sort of QA on full

345
00:33:37,280 --> 00:33:44,160
stories. So some really interesting papers this year? As I said here, I think there's been really a

346
00:33:44,160 --> 00:33:51,200
yeah, a lot a lot of very interesting very diverse papers and a lot of very important directions as

347
00:33:51,200 --> 00:33:57,520
well. Awesome. So our next category is tools and open source projects. What what culture are here?

348
00:33:58,880 --> 00:34:05,840
Cool. Yeah, so in that regard maybe yeah kind of building on what we just this has previously

349
00:34:05,840 --> 00:34:15,120
what I found personally most exciting was just those research groups really open sourcing good

350
00:34:15,120 --> 00:34:22,640
implementations of many of the papers that I just mentioned. So we we had yeah shortly after for

351
00:34:22,640 --> 00:34:27,200
instance the the birch paper which is kind of the current set of the language modeling paper

352
00:34:28,000 --> 00:34:35,440
came out a TensorFlow version together with a collaboratively notebook demo of that which people

353
00:34:35,440 --> 00:34:42,480
then ported into a pie torch. So you can use it in in different frameworks and other people

354
00:34:42,480 --> 00:34:50,000
already using that. So yeah, and similarly to that before you had Elmo as well. So that

355
00:34:50,640 --> 00:34:57,120
being made very easily easily accessible in both an NP as well as in TensorFlow via TensorFlow Hub.

356
00:34:57,120 --> 00:35:04,880
And similarly we also open sourced our old fit model in the faster iLibrary and I think just

357
00:35:05,920 --> 00:35:12,640
the yeah like this whole trend of really researchers and people not taking a lot of time to

358
00:35:13,200 --> 00:35:18,800
share their models with the community really allowed much of this recent kind of accelerated

359
00:35:18,800 --> 00:35:24,240
progress. And I think has really kind of captured a lot of the imagination and really made it

360
00:35:24,240 --> 00:35:31,040
comparatively easy now for people to actually put that put their ideas into practice and apply them.

361
00:35:32,640 --> 00:35:39,360
And similarly, yeah, I was also really excited to see here the groups from

362
00:35:39,360 --> 00:35:44,960
Ateche and from Facebook sharing the implementations of Ansovets machine translation

363
00:35:45,600 --> 00:35:51,040
because also I think that is really very promising particularly for low use of languages.

364
00:35:51,040 --> 00:35:56,960
And yeah, I'm personally really excited to see what people how people are going to build on those

365
00:35:56,960 --> 00:36:03,760
implementation and really use them and apply them to their own applications. And yeah,

366
00:36:03,760 --> 00:36:09,280
and nothing that I just wanted to mention here. So those were just kind of more research paper

367
00:36:09,280 --> 00:36:15,680
open source implementations of those. And another trend that I particularly like this year or

368
00:36:15,680 --> 00:36:23,120
just another development I liked was the one particular tool this Google dataset search.

369
00:36:24,480 --> 00:36:32,320
So more so so I haven't used that tool much recently yet or it doesn't seem that that great

370
00:36:32,320 --> 00:36:38,160
coverage yet for many NLP data sets or many of the most commonly used ones in academia at least.

371
00:36:38,160 --> 00:36:45,840
But I think it's an instance kind of of a larger trend of really making kind of the the lifeblood

372
00:36:46,800 --> 00:36:52,800
of the current models that we have this the data sets that we need to train and evaluate the models

373
00:36:54,080 --> 00:37:00,240
really more easily accessible and placing those really at the fingertips of a lot of people.

374
00:37:00,240 --> 00:37:08,560
And I think just start having like this data set search engine as well as some other yeah,

375
00:37:08,560 --> 00:37:14,240
some other resources. So there's been some great news online on like collections of data sets

376
00:37:14,240 --> 00:37:21,920
recently and also for NLP. I together with other people we've worked on compiling a repository

377
00:37:21,920 --> 00:37:27,520
of different collection of different data sets as well as sales yards results in natural

378
00:37:27,520 --> 00:37:34,720
language processing as well. And there's been some cool extensions and more work in that direction

379
00:37:34,720 --> 00:37:40,800
recently. So I think overall yeah for me that's been really useful direction of just making those

380
00:37:40,800 --> 00:37:45,760
data sets and this knowledge of what is actually the current state of the art and the current

381
00:37:45,760 --> 00:37:50,640
model setup being used more accessible to the wider community. That's great. You mentioned that

382
00:37:50,640 --> 00:37:59,440
this Google data set search isn't very up to date on NLP results. Is it is it manually curated or

383
00:37:59,440 --> 00:38:07,440
is it you know is it kind of a search engine and what do you suppose is the causing the lag?

384
00:38:08,560 --> 00:38:15,040
Yes. So I think it's mostly I mean I'm not exactly sure how it works on the under the hood.

385
00:38:15,040 --> 00:38:21,760
I think a data set so people who host data sets online or so they can embed some Java script

386
00:38:21,760 --> 00:38:29,360
or some schema pattern into their file which would allow the data set search to pick up on that.

387
00:38:29,360 --> 00:38:37,920
But I'm happy to be corrected in that. Yeah, I'm not entirely sure where I think it's my guess

388
00:38:37,920 --> 00:38:43,360
it would be automatic but not entirely sure how it works under the hood. I've just observed.

389
00:38:43,360 --> 00:38:50,080
I mean it's as well kind of related to that not all of the data sets have public pages.

390
00:38:50,080 --> 00:38:53,520
So a lot of the data sets are still you still have to go to

391
00:38:54,880 --> 00:39:01,840
research papers to find to track down the links or actually in some case maybe even reach out

392
00:39:01,840 --> 00:39:07,840
to the authors if there's some particular copyright on the data set. So I think in many cases it's

393
00:39:07,840 --> 00:39:14,400
still not that standardized which would maybe make it easier for this search engine to pick up

394
00:39:14,400 --> 00:39:20,080
on the particular data sets but I'm hopeful that going forward it will be easy like coverage of

395
00:39:20,080 --> 00:39:26,880
a tool that will be broader or that we'd have other tools that make it easier. Jumping into the next

396
00:39:26,880 --> 00:39:37,680
category commercial developments you had a particular announcement or a new product in mind.

397
00:39:37,680 --> 00:39:43,840
Yeah so regarding the commercial development I think the most yeah the most striking development

398
00:39:43,840 --> 00:39:51,280
this year for me the other one I was really most excited about was here from Google this the demo

399
00:39:51,280 --> 00:40:00,640
they showed of Google Duplex so there which was their assistant effectively using Google Assistant

400
00:40:00,640 --> 00:40:07,200
to talk and make appointments or perform certain tasks related to making appointments with

401
00:40:07,200 --> 00:40:15,360
restaurants or the hairdresser so very kind of narrow scenarios and leading like following that

402
00:40:15,360 --> 00:40:23,440
there were some some issues around just privacy and kind of some yeah concerns about the

403
00:40:24,960 --> 00:40:32,160
just the the bot actually sharing that it is not human-operated but it is actually

404
00:40:32,160 --> 00:40:37,600
bot so I think there's still some issues to be addressed around that but more broadly I found

405
00:40:37,600 --> 00:40:48,640
that really kind of a very yeah like really great demonstration of I think yeah particular use case

406
00:40:48,640 --> 00:40:54,400
where these sort of dialect technologies can have a big impact in the new term which is really

407
00:40:54,400 --> 00:41:03,840
focusing on narrow mostly well-defined domains that you can where you can mostly

408
00:41:03,840 --> 00:41:13,280
nominate most of the potential scenarios or most of the potential interactions that you can have

409
00:41:14,160 --> 00:41:20,160
and where you can then apply these models to really make make people more productive or efficient

410
00:41:20,160 --> 00:41:27,360
in general yeah that was absolutely stunning demo and it sounds like they've rolled it out in

411
00:41:27,360 --> 00:41:34,640
at least some limited way this past month maybe yeah I think I mean yeah I've only heard in

412
00:41:34,640 --> 00:41:38,480
the yes that might take place there I think here in Europe we probably have to wait a bit longer

413
00:41:38,480 --> 00:41:45,200
for that yeah awesome awesome and then you've got a couple quick points on kind of top

414
00:41:46,160 --> 00:41:51,360
application areas for the year what caught your eye in terms of the way folks are putting all

415
00:41:51,360 --> 00:41:58,080
this all these research to work yeah so regarding so regarding application areas I think the most

416
00:41:58,080 --> 00:42:06,160
impactful for me this year in like a societal and context really was anything related to dealing

417
00:42:06,160 --> 00:42:12,320
with news and in particular combating the fake news problem that has really has yeah has received

418
00:42:12,320 --> 00:42:18,880
a lot of attention recently and in particular there's been yeah kind of new startups and initiatives

419
00:42:18,880 --> 00:42:26,320
like FAC matter for instance in London were yeah doing a really working closely with journalists

420
00:42:26,320 --> 00:42:35,280
to to to fight fight these problems there's been yeah more increased attention in the community

421
00:42:35,280 --> 00:42:40,240
of instance as part of different workshops so there was a workshop on fact extraction and

422
00:42:40,880 --> 00:42:47,120
verification which really tried to define yeah because in most of these cases really the first step

423
00:42:47,120 --> 00:42:53,360
to fighting combating that is really to build data sets first that allows to to train these models

424
00:42:53,360 --> 00:42:59,200
and sharing those with the community so workshops like that or similarly there's share tasks

425
00:42:59,200 --> 00:43:05,200
next year on hyper partisanship detection so I think initiatives like that will really go

426
00:43:05,200 --> 00:43:11,920
long way themselves and and in addition so news was also on on my personal radar just because of

427
00:43:11,920 --> 00:43:20,160
the work I've been doing here at alien on news analysis and relation extraction is also really

428
00:43:20,160 --> 00:43:26,880
something that we're more trying to focus on as well and I think yeah now fake news maybe

429
00:43:26,880 --> 00:43:33,600
besides addressing additional bias in models that are trained on MLP tasks I think it's really

430
00:43:33,600 --> 00:43:40,960
one of the directions MLP can have the most the biggest impact in society oh yeah that's

431
00:43:40,960 --> 00:43:49,280
become a clear area of opportunity for these kinds of for NLP in general so definitely agree

432
00:43:49,280 --> 00:43:56,080
with that one before we run out of time I'd love to get your take on some predictions for 2019

433
00:43:56,080 --> 00:44:03,200
where do you think the the biggest opportunities lay for NLP researchers and practitioners?

434
00:44:03,200 --> 00:44:10,560
cool yeah so so I think yeah for me personally I think the biggest opportunities are around

435
00:44:11,120 --> 00:44:16,560
transfer learning in general so really as I mentioned before these sort of pre-trained

436
00:44:16,560 --> 00:44:24,320
representations where we're getting we're having right now I think with those we're still very much

437
00:44:24,320 --> 00:44:30,320
at the start there's still yeah I think a lot of a lot of potential that we can leverage there

438
00:44:30,320 --> 00:44:34,880
and in particular I think just for researchers or kind of practitioners who want to

439
00:44:36,320 --> 00:44:41,840
do like yeah work with those who make their own have their own impact in that direction

440
00:44:41,840 --> 00:44:46,720
I think yeah people shouldn't be discouraged by I mean now you've seen with these recent

441
00:44:46,720 --> 00:44:53,440
approaches that like I think bird was trained on like 64 TPU parts on I think four or five days

442
00:44:53,440 --> 00:44:58,880
or something so I think people shouldn't be any even that model what's now the state of the art

443
00:44:58,880 --> 00:45:05,040
I think people shouldn't be discouraged by these like this data or compute requirements and I

444
00:45:05,040 --> 00:45:09,360
think there's a lot of directions that still don't require you to have that many that access to

445
00:45:09,360 --> 00:45:17,600
that much compute so for instance in just better understanding what these models actually capture

446
00:45:17,600 --> 00:45:25,200
and what they if they can yeah if they allow us to capture things like common sense or not and

447
00:45:25,200 --> 00:45:29,920
what those representations actually look like I think that that's really exciting direction and then

448
00:45:29,920 --> 00:45:37,440
just applying those models to different tasks essentially and then relate to that I think

449
00:45:38,640 --> 00:45:43,440
with understanding how like what they can can capture I think there's really a lot of room

450
00:45:43,440 --> 00:45:50,640
for just learning and creating representations that address those particular deficiencies

451
00:45:50,640 --> 00:45:56,160
so for instance there was a recent cool paper I liked from people at Facebook I researched

452
00:45:56,160 --> 00:46:04,160
where they learned dedicated representations dedicated to representations for word pairs

453
00:46:04,160 --> 00:46:11,840
yeah so word pairs just how to words relate to each other in general yeah and these representations

454
00:46:11,840 --> 00:46:18,080
are quite useful for tasks like entailment or even quest answering where you try to relate words

455
00:46:18,080 --> 00:46:23,520
in the question to the words and the paragraph words in the hypothesis towards the premise

456
00:46:25,600 --> 00:46:32,160
because your model in the end has to do some cross sentence inference essentially and I think

457
00:46:32,160 --> 00:46:39,200
really figuring out what is the task to care about and what is something that you that is

458
00:46:39,200 --> 00:46:44,720
necessary for that task to do well on this and how can you capture that and learn that with

459
00:46:44,720 --> 00:46:51,200
representations I think it's really like overall really a useful direction and something so I

460
00:46:51,200 --> 00:46:58,000
think I mean for me personally we've seen now or we have like a lot of in computer vision a lot

461
00:46:58,000 --> 00:47:02,880
of other disciplines things like model zoos where people have access to different sort of pre-trained

462
00:47:02,880 --> 00:47:07,680
models like different types of pre-trained image net models and recently we've seen people

463
00:47:07,680 --> 00:47:13,200
combining different forms of word embeddings and actually showing that different forms of word

464
00:47:13,200 --> 00:47:18,080
embeddings so word embeddings for more tobacco and cloth and other ones are have complementary

465
00:47:18,080 --> 00:47:25,440
information to each other so I think going forward one thing that we probably see is just having

466
00:47:25,440 --> 00:47:30,560
a lot more of these pre-trained representations that encode particular information relating to

467
00:47:30,560 --> 00:47:36,720
syntax, semantics and other aspects that then people can effectively mix and match and combine

468
00:47:36,720 --> 00:47:43,360
for the relative tasks. That's really interesting. We spoke quite a bit about transfer learning in

469
00:47:43,360 --> 00:47:50,960
NLP and the embedding stuff as well in the previous interview that I'd refer folks back to

470
00:47:51,680 --> 00:47:58,160
but I can definitely see us kind of continuing to make progress down this path that's been

471
00:47:58,160 --> 00:48:05,680
pretty astounding what's what we've seen this year. Yeah I totally agree. Yeah and the second

472
00:48:05,680 --> 00:48:11,040
direction that I just want to quickly highlight here is that we've seen this year just a lot more

473
00:48:11,040 --> 00:48:18,400
attention on the importance of evaluating things like on multiple languages and more people are

474
00:48:18,400 --> 00:48:24,240
working on kind of multilingual and cross-lingual applications so I think I'm really confident or

475
00:48:24,240 --> 00:48:31,280
hopeful at least that we'll see more of that next year in particularly particularly looking at

476
00:48:31,280 --> 00:48:37,840
low-resist languages and yeah and even people combining kind of learning some of these

477
00:48:37,840 --> 00:48:44,080
representations in a cross-lingual way so that you have a model like the recent kind of bird multilingual

478
00:48:44,080 --> 00:48:50,240
model that was trained on a lot of languages and that allows them application or almost

479
00:48:50,240 --> 00:48:56,560
zeros at learning for a lot of these other languages. So do you have some kind of top predictions,

480
00:48:56,560 --> 00:49:05,120
some you know specific predictions for 2019 or the near future? Well okay so in terms of specific

481
00:49:05,120 --> 00:49:12,320
predictions I mean I think so we'll see well it's kind of hard to be like particularly specific

482
00:49:12,880 --> 00:49:19,280
I mean we'll see a lot more of like I think almost every paper that is going to achieve state of

483
00:49:19,280 --> 00:49:24,480
the art on a particular NEP task will use some form of contextual or pre-trained representations

484
00:49:24,480 --> 00:49:33,680
that's pretty certain besides that I yeah I would hope that if we look at like keywords of

485
00:49:33,680 --> 00:49:39,760
language like of where it's mentioned in in different papers you would see at least an increase

486
00:49:39,760 --> 00:49:47,600
in other languages they mentioned yeah so that's that's another one I think it's yeah it's

487
00:49:47,600 --> 00:49:53,760
kind of hard to be like too yeah much more specific than that. It sounds like the thrust of your

488
00:49:53,760 --> 00:50:00,000
predictions are in these kind of two main directions more cross-language work more attention to

489
00:50:01,280 --> 00:50:08,480
using transfer learning. Yeah I mean and like I think anything that I mentioned before so also

490
00:50:08,480 --> 00:50:14,240
the work on commonsense inference reasoning with large documents I think all of those could really be

491
00:50:15,440 --> 00:50:21,200
kind of their own predictions or their own directions in the future as well so I would personally

492
00:50:21,200 --> 00:50:27,600
expect to see more work in in those related directions as well and similarly I would expect or

493
00:50:27,600 --> 00:50:33,440
hope to see more work generally on just developing kind of more general seamless learning algorithms

494
00:50:33,440 --> 00:50:40,400
for different NEP tasks. Any other thoughts on what we might expect to see in the coming year?

495
00:50:41,920 --> 00:50:48,640
Yeah so I just want to kind of call out a few just initiatives and organizations that I think

496
00:50:48,640 --> 00:50:53,360
are really worth watching in the next year as well and I'm personally really excited about

497
00:50:54,160 --> 00:51:02,000
yeah kind of local initiatives that are spearheading and evangelizing machine learning in AI

498
00:51:02,000 --> 00:51:08,800
in particularly for kind of underrepresented groups so particularly I think the deep learning

499
00:51:08,800 --> 00:51:14,320
in Daba and the team behind that which is an initiative to strengthen machine learning in

500
00:51:14,320 --> 00:51:22,080
Africa has been doing some amazing work and relates to that also there's been the the Black and

501
00:51:22,080 --> 00:51:28,320
the AI workshop at Neroops this year has really had an amazing attendance or similarly there's

502
00:51:28,320 --> 00:51:36,320
been queer and Latinx in AI so I think yeah I really find it amazing and yeah really fruitful to see

503
00:51:36,320 --> 00:51:46,240
these initiatives really getting more support and getting kind of a wider attendance and yeah

504
00:51:46,240 --> 00:51:52,320
strengthening and yeah and kind of similar to that personally being in Europe as well if

505
00:51:52,320 --> 00:51:59,120
on this exciting that there's been some local European initiatives Alice and Claire in particular

506
00:51:59,120 --> 00:52:06,240
which try to bring kind of Europeans more closely together in kind of a joint

507
00:52:08,240 --> 00:52:15,680
joint organization or joint association effectively and then I hope to see more work in

508
00:52:15,680 --> 00:52:20,320
in the coming year from yeah really like open source initiatives people

509
00:52:21,440 --> 00:52:27,040
open-sourcing more work and in particular yeah particularly things from faster AI for instance

510
00:52:27,040 --> 00:52:33,760
which has really been doing amazing work both on integrating many people as well as even publishing

511
00:52:34,320 --> 00:52:39,840
very high impact research both in computer vision as well as natural language processing.

512
00:52:40,480 --> 00:52:47,360
Well Sebastian thanks so much for taking this time to chat with me and and review this past year

513
00:52:47,360 --> 00:52:53,680
and what we should expect for 2019 definitely an exciting time in NLP.

514
00:52:53,680 --> 00:52:56,560
Cool great yeah thanks for having me was a pleasure.

515
00:52:59,840 --> 00:53:04,800
All right everyone that's our show for today for more information on Sebastian

516
00:53:04,800 --> 00:53:12,080
or any of the topics covered in this episode visit twililai.com slash 216 you can also follow along

517
00:53:12,080 --> 00:53:20,960
with our AI rewind 2018 series at twililai.com slash rewind 18 as always thanks so much for listening

518
00:53:20,960 --> 00:53:25,280
and catch you next time happy holidays

