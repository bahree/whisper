All right, everyone. Welcome to another episode of the Twimmel AI podcast. I'm your host,
Sam Charrington. And today I'm joined by Fatih Pratikli, Senior Director of Artificial
Intelligence at Qualcomm. Before we get into today's conversation, be sure to take a moment
to head over to Apple Podcasts or your listening platform of choice. And if you enjoy the show,
we'd greatly appreciate your five-star rating and review. Fatih, welcome to the podcast.
Thank you so much for hearing me, Sam. This is pleasure. I'm very excited. I'm all yours.
All righty, I'm really looking forward to digging into our conversation. We'll be talking a lot
about your research in the areas of computer vision and perception. But before we do that,
I'd love to have you share a little bit about your background and how you came to work in the field.
I am a computer scientist and electrical engineer. I was on the both sides of the fence,
the fence, dividing academia and industry several times. You know, as everyone, I started my PhD.
I did some research. I was a research assistant. I stayed at the university. Then I moved to
industry. Many years after that, working on real problems. We're challenging problems.
Maybe 13 years after that, I switched back to academia again. I was a full professor,
tenor professor for a long time. Then I found myself intentionally, of course, in industry again,
even trying to solve bigger problems, trying to create bigger impact for everyone. So now I'm with
Qualcomm. And the whole time, have you been working on computer vision or have you switched areas of
interest? Computer vision was always there. That is one of the things really excites me,
amazes me, because if we consider human brain electrical activity, maybe 70-75% of what we
actually consume in our brain dedicated to visual perception, significant portion of brain is also
goes to visual understanding. So vision is the way that we understand, makes sense of the
world life, everything around us. Actually, if we close our eyes, you know, I just close,
a loss of vision might be the most devastating disability. You know, it comes so naturally to us,
the way that we understand the 3D scene, recognize people, recognize faces, recognize objects. So
then I was almost always interested in how we can make computers, machines to, you know,
have that capability as well, this visual understanding, visual perception. So computer vision
has been always there. Before maybe, let's say, 25-30 years ago, it was more conventional,
engineered solutions, you think about, okay, what would the human perception do? How would
brain work? And how I'm going to sit down and try some mathematical description to convert it
to something a computer would understand. Now we are kind of as many of us are very familiar,
we are using AI artificial intelligence to make it natural, look into data and learn something
automatically from observing the environment around us. Awesome, awesome. Can you share a little bit
more around your areas of interest from a research perspective? Under perception, there are
several modalities. One is working with image and video data. This would be directly related to
computer vision and then there is 3D data, point cloud and 3D representations. That's also,
I will say, that computer vision as well. But perception is not only in these two modalities,
3D point cloud and image video visual data, there is also RF radio frequency signals all around us.
And they are, in a way, kind of the lights that we see, they are all around us feeling
the space. We also look into those invisible frequencies and try to understand everything about
the scene, about the world, about how everything works, objects, intrex. So these are the modalities that
I am very much interested in it. What do we do with these modalities? For instance, in images
and videos and 3D point cloud data, RF signals, any signal actually, including X-ray and ultrasound,
we detect things. For instance, whether there is a vehicle or a person on the street, we reconstruct
3D model of the world around us. That's also very interesting, very challenging, actually,
if you want to just use one single image, not two, like we do. We have two left and right eye,
so we use stress-coffee vision, but can you do it just using a single camera image? And the answer
is yes, for a while, you know, I was really impressed with that one, and recognizing activities,
labeling everything in the scene. In a way that what goes on the lower level in our brain, we
want to do all of these, accomplish all of those processes, perception, and all of them were
very interesting to me personally, and these tests, these understanding goes into many applications
from, let's say, XR, augmented reality and virtual reality to autonomous vehicles, to robotics,
IoT, you can imagine, wherever there is a human being, and if you replace or put a machine in front
of it, kind of those applications exist, all around us and a computer vision enables all of those,
and that's why I think it's very exciting to me. And some of these problems are big problems,
they are not solved problems, they are presenting a big challenge, so that's another attractiveness
for many people. So I want to dig into a few of the perception-related papers that you've got
at CVPR this year, and the first of the ones is a paper on panoptic segmentation. The full title is
Panoptic Instance and Semantic Relations, a relational context encoder to enhance
panoptic segmentation. Let's start at the beginning, what is panoptic segmentation?
Yeah, it's a long title, San, panoptic segmentation. So there are things and stuff around us,
right? Things are the comfortable things like there is one vehicle, another vehicle, another vehicle,
one glass, another glass, one person, another person, but there are also uncomfortable things like
sky, like building, like road, is not comfortable, right? So segmentation, the goal of segmentation,
take this visual information, images and video, or point cloud, and then label every pixel,
every region with the identity of that region. For instance, if it's a sky, if we see sky,
it will tell, computer will tell, okay, this is a sky pixel, that's specific pixel and the region,
if there is a person, it will tell, this is person, but it's not going to just say person,
it's going to say that this is person A, and another person B, even though they are occluded
each other, maybe half of the person B is visible, it will still distinguish. So this is a very
challenging task. You are trying to label all data pixels in this case with these corresponding
things and stuff identities. That is what penoptic segmentation does. And so from that,
in that sense, it is kind of a superset of instant segmentation, which is identifying the things
and semantic segmentation, which is more focused on the stuff. Very good, absolutely. Yes, actually,
that's the right technical description, instant segmentation and semantic segmentation together
will give, would be under the penoptic segmentation. Describe the setting for this paper,
what is the problem that you set out to the solve? So now we understand what penoptic segmentation is,
and as maybe I should point out that recently there has been significant attention and
excitement around a new technology in AI, which is called as transformers. So transformers,
let me briefly mention that when we give an image data video algorithm learns which parts of
for instance, image are related to each other. It knows to collect such supportive information,
pay right attention to the right parts. For instance, if there is a vehicle part,
a tire would, you know, put more confidence, if less everyone to detect a vehicle,
with the hood of the vehicle, door of the vehicle. But road pixels, sky pixel, they will,
even though they may look similar, attention mechanism will ignore those. So it will focus on the
better parts, supportive evidence to, you know, make such deductions. So transformer is
self-attention, advanced self-attention mechanism. It is important to relate these areas,
image areas, and it has been applied to semantic segmentation before. Now the challenging part for
penoptic segmentation, there are, you know, unknown random number of things like these instances
of the other classes, like people and vehicles in the scene. How are we going to use transformers
to make sure that these instance of, let's say, this person kind of is different. This transform
mechanism would distinguish from the other person. Otherwise, since they are, they look same,
they will make the transformer will think that they are same thing, and it may not distinguish
these two. For penoptic segmentation, we want to label them separately. So this paper explains,
first time in the world, how you can actually kind of combine this type of attention mechanism
into a segmentation framework. Got it. And so what have prior approaches tried to do
for solving penoptic segmentation? On a high level, we can consider there are a single shot or,
you know, multi-shot setting, single-shot setting. It aims to take an input image and directly
label each pixel as a different object, instance of an object, like this person and the other person.
And there are multi-shot algorithms, multi-shot meaning first, we find these regions of interest.
You can think that those are like boxes. You say, okay, this is like a box. There's a person here,
another person here, another person here, then in the second stage, we go and look, okay,
these two, this box contains a single instance of a person, and it doesn't overlap with anything,
okay, good, then I will just go, you know, do segmentation between these boxes and I will find
segmentation to person. If there is overlap, then I will kind of infer which one, which pixel
is blocked, which person there are two or multiple persons. So this is the other way of doing that.
Usually these algorithms lack an attention mechanism across different instances. We can do it
pixelized, but if I have one person here, another person behind it, another person far away,
how I'm going to learn where to focus if I want to detect all of them at the same time. So this
is what we accomplish. For a different number, varying number of instances,
CVPR paper shows that you can learn or train an algorithm that would learn to focus on the
right areas of the image, which then improves the accuracy of the segmentation. That's what we
show in the paper. Got it, got it. I think this is maybe related to the single shot versus multi-shot,
but I got the impression from the paper that one of the big things that you're doing differently
here is that previous attempts have tried to, hey, in order to solve penoptic segmentation,
let's do image segmentation or instance segmentation and then semantic segmentation
separately and kind of put the results together, whereas you're doing them as part of a consistent
coherent system. Absolutely, very good point. Now, the same network can do in an end-to-end
fashion, these two tests together. And when you do it together in an end-to-end fashion in the same
network, they support each other. They don't kind of dismiss, but for instance, semantic segmentation
will generate or instance segmentation will generate the leverage together and which generates
better improved numbers, honestly. What were some of the biggest challenges to this approach?
Transformer architecture or self-attention architecture. One challenge, I can say, that they are
computationally, you know, intensive and how to make them efficient was a challenge. And also,
you know, our people is not specific to any specific backbone. A backbone usually considered
as a pre-processing neural network takes image or video and then creates useful features for the
downstream test, like semantic penoptic or instance segmentation or many other tests.
Our algorithm, our idea actually, can apply to any backbone, any, you know, kind of a segmentation
framework, it could be plugged into improve their performance. In the paper, we tried maybe more than
15 or 20 different segmentation algorithms and every time we plugged in this type of transformer-based
instance self-attention with, you know, kind of semantic segmentation, the results were much better.
How do you have both the ability to plug in whatever segmentation model you want to use?
Was that specifically for the instance segmentation or for either of the components?
What our algorithm does, it leverages these features coming from the previous segmentation algorithm
and then it takes them and it learns, reweighting them. In a way, that's what transformer does.
The input transformer is some kind of, let's simplify it, let's say it is an image, output is
another image, let's say, but what you see, like maybe now much clear and focus on the right parts.
Maybe input image, you can't think that it's a noisy image and there are some, you know, kind of like
things not very visible in the output, now much sharper and kind of these things are clearly
distinguishable. Of course, it is not an image but goes into this network, it is a set of feature
vectors and it's called as the features for image. Each pixel has a feature descriptor,
those descriptors goes into this component and comes in a better, more trustable,
kind of more useful manner. You know, then we do that, then we make the features better,
any downstream test will benefit from that. So all these segmentation algorithms, they have
this type of feature generators either at the beginning or at the end. So this idea can plug
kind of and directly apply to those feature maps. Got it, got it. So the core of what you've done
is this transformer that takes as input these feature vectors and you don't really care how the
feature vectors are created, any of these algorithms that you've tried, it worked just fine as part of
your network. Yes, this is a strength of it, it can take any of the features and make them better,
you know, more representative of the test that we want to accomplish. However, this is also
something known in the paper, then we use this thing and we also go back to the previous
stages like feature generator and other branches to make them even more accurate. So kind of
when we do end-to-end training using PISR, the paper, the idea that we talk about in the paper,
all network becomes kind of updated and even the previous part, feature generation parts
improves. So overall kind of that further improves the accuracy. Semantic segmentation, you know,
penoptic segmentation is one of the most challenging tests in computer vision. It's very difficult
for a human to segment. By the way, if you want to do, if I, if you ask me to go, you know, label each
instance, I will do something, but if you ask another person, it will do differently, you know,
even for humans, there is significant variation in the outputs of how we, you know, do penoptic
segmentation, instance and semantic segmentation for a computer vision algorithm for a machine to
do it even more challenging. So this type of ideas really kind of pushes the state off to our
such that, you know, they are becoming more and more feasible for bigger use cases to improve
our daily lives through these applications in XR and auto and other type of use cases.
How did you measure the performance of your model? There are very recognized metrics and there
are benchmark data sets. So the huge benchmark data sets, those benchmark data sets have the
ground truth information one way or another defined. These are real also data sets, real images,
real videos. We use those metrics, for instance, mean IOU or similar metrics. It defines how well
one mask, object mask overlaps with another one. So this is very common in semantic segmentation.
For instance, segmentation, there are similar, you know, advanced versions of this thing now
considering whether kind of we are confusing identity of the instances or not. So there are
these common metrics and benchmarks that this how we evaluate the algorithm. And what kind of results
did you see? Then we submitted the paper, we look at all the existing state of the art,
existing work including the archive, things appeared on the archive, not maybe published,
but very fresh things. So just before the submission deadline, the previous week we applied
whatever we found and every time we observed improvement on this segmentation pipelines
and our results also when we submitted to different or investigated, you know, generated results
on these benchmarks were the top of the line. And in the paper, we showed that, you know,
those are the best segmentation results possible. Of course, the field is changing, maybe
next CVPR, there might be even better numbers that might be coming from us or other people,
but at the time, it was the number one on multiple datasets also, not one dataset.
Okay. And where do you see this particular line of research heading? Is it a solve,
is panaptic segmentation a solve problem now? I think when the conditions are right, it is,
this performance is, you know, almost product quality level, but there is significant variation
in the input quality. For instance, it could be a dark image, it could be a very noisy image,
it could be a blurred image, you know, you can imagine, you know, there might be many problems
in the image, things may be very small, some of the objects might be very tiny and not maybe only,
let's say, a hand of a person would be visible, you know, significantly up to the country,
our goal is actually to take that hand as well, you know, kind of even the only is the hand.
So those type of very difficult settings still require more work to make the algorithm
to be more robust, generalize those type of challenging situations. And also,
another thing, if, for instance, we use a data set and that data set collected in a specific
manner using maybe the similar type of cameras and labeling might be similar and lighting might
be similar, but now in a specific application, the environment lighting, everything would be
different, how to adapt to that such domain changes, domain variations is one thing.
Another thing, some, I mean, we don't, these things, instances could be any class,
right? I may be repeated many times, like it could be a person, it could be a vehicle,
but it could be anything, right? It could be, you know, a piece of machinery,
it could be a kind of component in an assembly line, you can imagine, you know,
it could be a bird, multiple birds, you know, for kind of, you can imagine, this is like a commodity,
this type of AI solution, people would like to take it and count the number of ends, you know,
in a lab setting, those type of things. So how to adapt automatically with minimal labeling
to such different classes, different type of things, different type of stuff for semantic
and instant segmentation, it is, I think, still a challenge and we are working on all of those
problems, which would, for instance, take our solution and automatically adapt to a very
different, completely different, you know, class, set of classes, different type of objects.
So there is still work to be done, but the quality of the segmentation results for key applications,
for instance, camera essentials for autonomous vehicles, for XR applications, for robotics,
I think it's very promising and soon such solutions, either from us or maybe, you know,
everyone in the world using our solutions will appear in products. I'm very confident about it.
Awesome. Does this approach assume prior knowledge of the classes? I was envisioning this like
an autonomous vehicle type of scenario where you, you know, you have a camera off of,
you know, a picture of a road, a camera off of the front of the vehicle and there's something
in the road and you're trying to differentiate, you know, not something in the road versus something
in the road, but that's a different problem than what we're talking about here. Oh, this may be
obstacle detection or, you know, we have a class of unrecognizable things as well. There are ways to
solve it. Yes, that is also possible, but if it's supervised solution, if the target classes
changes, it's a problem. You have to do this transfer learning. There is a specific term for that,
you know, my goal target classes now change. How I'm going to leverage on the previous network
that I trained and maybe some portion of the previous data, anything semantically related,
semantic information and now I can adapt to this type of things. Two, the day things are not even
be defined. Usually, there is a class of unidentified things. We don't know what they are,
like these are unidentified object classes. You have all, like you all see something, you know,
we can give it a name, but then maybe some other intelligent mechanism has to decide,
oh, is this something that I should worry about it? If I'm approaching that thing, is it going to
be a problem for us or not? We are, of course, that's a more higher level inference. There are
ways of doing that. Supervised learning, of course, limited. Now, at Qualcomm, also, we are looking
at self-supervised solutions or unsupervised solutions. Exactly for, this is one of the reasons,
you know, we cannot expect people to generate these data labels, ground truth, to train these
algorithms over and over again. And human beings, we don't learn that way, right? I mean, it's not
like here's an example of a cat, example of a dog, example of a tiger, and then we remember that
we know when we see an animal, that is an animal, you know, it is not like, I don't need a training
data. Even if I don't see it, I would infer, you know, I would deduce that, okay, it looks like a
tiger, so it might be some type of tiger. So we do it using this continual self and unsupervised
learning. And we are, we have solutions actually, you know, kind of applied to different tests.
In this paper, we don't really talk about that, but that is something we are very active on it as
well. Another paper that we wanted to chat about was the imposing consistency for optical
flow estimation paper. Tell us about the problem that you're trying to solve there.
Yeah, absolutely. So optical flow is finding where each pixel in the current image was in the
previous image. So in a way, it is motion. It describes pixelized motion. Why this is important,
because if I know where the pixel was in the previous frame, then, you know, first of all, I know
how things are moving in the scene. I can deduce about the camera motion, and then I can also
understand object motion. For instance, we have a headset, XR headset, and or AR glass, we are
moving our head. And this is most, but then some other people also most. So we know this type of
motion, which is important. And also, I can relate the previous frame to when I compute or make
deduction for the current frame. So motion is important. And optical flow is what you will
to obtain if you correspond this pixel with its previous location in the previous frame. So it
looks like a field, you know, last so kind of like motion vector from individual pixels to the next
to where that pixel ends up in the next frame. Of course. Yeah, it could be from current frame to
the previous frame or previous frame to the current frame or current to the future frame. We can
predict also. Absolutely, sir. Got it. And so what's the approach that you took with this paper?
So one challenge, again, in AI, in data driven learning, the dataset, what I mentioned before,
we need ground-through data for supervised training. Here is, let's say, two images like the
current frame and the previous frame, video frames. And this is the optical flow motion between them.
So we can, you know, if we synthesize those images, we can we know we control everything,
we might have a game engine, for instance, or any, you know, any software can generate this type
of different, we can, for instance, move the image, things in the image in a game, and we know how
they moved, the type of ground-through is available. But if we have real images, you know, how we are
going to find the ground-through, like how each pixel is moved is not like something measurable.
You can think that there's a much harder labeling problem than what we just talked about.
Absolutely. So such datasets, I mean, still computed datasets, there are some datasets,
smaller scale. And in AI, we want big datasets, you know, huge datasets with tens of thousands of
samples. It doesn't exist. So if we don't have the dataset, we will not have a good model.
So in this paper, but we show that, you know, you do not need such a big dataset. We will do
self-supervised learning, unsupervised learning. We will, for instance, take the previous image,
and we will do some transformations on it. We will rotate it, we will warp it, and we will
apply lots of different degradations, you know, without really destroying the image, still,
you know, it's like similar scene, and then we look at it, you know, it would maybe look a little
bit noisier or less noisier or the color is different, but maybe it's also warped. So we do all
type, this type of transformations, and we know, because we applied those, we defined those
transformations, so we know the ground truth in that way. So what about leveraging on that thing,
and if we do that, you know, kind of optical flow should be consistent with the way that we warp
transform the image, this one thing, we also look, okay, if I do forward, if I go backward, what would happen.
Let's say there's, my hand is moving, right, in front of my face, and then it moves, either it
occludes some parts or reveals some other part, and that's important. If I have two frames, it's
not like every pixel is going to be visible in both images, right. This is called as occlusion,
and occlusion map, we want our network to not, you know, handicapped by the occlusion areas. So
if we detect such occlusion areas, automatically, and if we manage them, automatically again,
in the network, maybe as a separate, you know, channel estimating those occlusion maps, it will,
overall, it will benefit during training and in the inference time also explicitly by estimating
such occlusion maps. So this paper does all of these things that I mentioned. And just to elaborate on
what you just said, it sounds like what you're trying to do is not necessarily
teach the network to predict occlusion or anything like that for its own benefit, but rather,
you're trying to teach the network to identify when there is occlusion. So it doesn't take
the ground truth that it's creating otherwise, and it does, it knows if that date is going to be
bad because it's, you can't relate the one pixel to the next. Absolutely. Occlusion masks are not
available. I mean, we synthesize them. This is a sub supervision part, and also these transformations,
we define them, and it will be, it is a large set of transformations, and we then apply all these
training, you know, improvements, enhancements, novelties to a network, which is
one of the state-of-the-art models for optical flow motion computation. It creates these cost volumes
and in different scales, and then it starts with a previous optical flow or just random, you know,
initialize optical flow, and every time it trace itself at these iterations, the optical flow,
estimates optical flow becomes more and more refined, more accurate and especially higher resolution.
So this is what we do, then we, for instance, take these ideas, training ideas, self-provised,
unsupervised training ideas, and modify the network such that now it can also do occlusion,
reasoning, and kind of train it in this manner, with the existing data sets, you know,
still simple data sets, even on those data sets, it improves the performance.
And there is a benchmark, there are multiple benchmarks, actually one is called as Kitty,
the other one is Sintal. In both those benchmarks, our solution was when we submitted,
and later also, because it's an open benchmark, it was ranking on the top of the data board,
and if I'm not mistaken, there are more than 200 solutions, 100 of them is somehow AI-based,
deep learning-based. So that was quite good news. We weren't expecting, but we were confident,
this is the right solution to do, and yeah, it went to the top of the leader board with the
solution. What's the future direction for this particular approach? Very good question, Sam.
This solution, since it requires big cost volumes, and iterations, they are computationally
expensive, and they require a lot of memory because of the cost volumes, and because of
iterations, they are slow. So what we are working on now, and you know, kind of we will have a demo
very soon, now we show that the same solution could run on a mobile phone, on a call-com platform,
in real time, for a large input size, input image size, large video size. So this has been
quite an exactment for us. We put a lot of effort to make it more efficient. Yeah, this is our
current work, and we also want to extend it to other things. You can do this type of things
for one camera, and this is one video, right, on the same camera, but you can do optical flow,
which is called a scene flow, across multiple cameras, and you will find 3D motion, not 2D motion.
So we are extending to that, and optical flow is core for many other, you know, perception
test, and higher level understanding. We are now plugging this solution into different pipelines to
see how much improvement we would get. Super solution is one of them, for instance. Yeah.
Okay. Well, there's one more paper that you have at CVPR, and we wanted to make sure to touch
on that one as well. The next one is dense vision transformers for single image inverse rendering,
and newer scenes. This particular one is focused on inverse rendering. What's that problem?
So usually, when we synthesize a scene, we know about 3D, we know about the objects, like there's a
catch, there's a chair, there's a ball, and we know the color of those. We also know their properties,
reflectance properties. We want to generate a scene, computer graphics. So we know the location
of the light. We know many things about the scene, like surface, normals, albeda, roughness,
you know, you can imagine. So it will look real time, real life. So inverse rendering, so this is
rendering, this synthesizing that I mentioned, inverse rendering starts on the other end, it takes
an image, natural image, and then it tries to find these components, for instance, the lighting,
location, lighting direction, lighting intensity, room shape, I mean, for indoors, and the properties
of the everything, objects, all the objects in the scene, their shape, their color, their
materials, whether it's leather or, you know, metal or wood or cloth, those type of things. So
inverse rendering takes an image, real image, and kind of finds these components. Each one of them
you can think that is either an image, you know, a reflectance image, a color image, albedo image,
or a 3D model, you know, or lighting location, you know, the heat maps and those type of things.
This is the inverse rendering pipeline, and these people use a transformer based idea to accomplish
that. And so how is inverse rendering previously done when you're not using transformers?
Previously, but recently also, I shall say, because deep learning based solutions for inverse
rendering are not that old either, maybe at most, a couple of years, but people did, okay,
here is the input image, and I know 3D model, because I generated that input image, and now I'm going
to switch the order, I will give input image and try to estimate the 3D shape, and I know also
the surface normals, and I know the color, you know, I know the color of the objects, I know the
locations of the objects, I know the reflectivity of the objects, I know the lighting location.
So, kind of this is done in a supervised manner, and separately. I think kind of the
keyword here is these type of things done separately, and there has not been any attempt to
learn for each test, and across multiple tests, where to focus when we are doing this type of
inverse rendering. If there is an image, if I want to, let's say, generate the lighting location
and lighting direction, which part of the scene image provides the right information that has
not been done before. And so, this particular approach in using the transformer, again,
you referenced this idea of the transformer's ability to attend to the right, and the most
important parts of the image. That's a big part of what's making this work. Yes, transformers
does that, in this case, we incorporated such self-attention or cross-attention mechanisms into
our work, into inverse rendering to improve the accuracy of each of these inverse rendering
tests, and also lighting estimation. So, when we have that, when we decompose an image into
this type of components that we factorize it, then we can, for instance, put anything in the scene,
you know, the lighting location, you know, how it reflects from other objects in the scene,
and it would look more realistic, more natural, much more natural. So,
and for instance, one application will be here is an input image, and then we put a completely
virtual objects in a way that, you know, all the shadows are correct. It's very difficult to,
you know, distinguish what we inserted, edited in the image than any other things already exist in
the image. Yeah, I'm so excited about that work. That's what I forgot, but I should have said it.
For instance, we have an image of a real scene, a house, let's say, and walls have a specific color
set. Now, I can change the wall color, or I can change, for instance, there's the catch here,
I can make the leather catch to a cloth, you know, some different fabric. So, we can really modify
the scenes in a very realistic manner. Right, in a way that preserves that realism without it
falling apart. And so, what were the, I'm imagining challenges here, again, you know, with
using transformers or the computational intensity of the approach as one of them?
In this case, we didn't focus on a computational kind of intensity. Yes, it's computation very heavy,
and we planned to kind of make it also very efficient. There's no question about that, but one of
the challenges was lighting direction estimation is not a straightforward problem, because it's
difficult to take an image, and before knowing about the location of the light, and also 3D
scene structure deduce about the lighting direction. I mean, you can imagine this lighting,
there's a window, there's a sun outside the window. By the way, this window is not visible in
the image. It is like right in front of me, there is a window, and you don't see it, right?
In this test, our goal is to find where that window is and where the sun is, you know, not this direction,
but this direction. So, this is kind of what we want to do, if we want to really put an object
in a way that, you know, shadows are correct, ambient lighting is correct. So, we are imagining
estimating the invisible things in the image. If it's visible, it's much more easier. So,
we are estimating the remaining part of the scene room, for instance, and that cannot be done,
you know, using just an input image directly going to there. You need to go step by step,
first get an idea about the room layout, 3D scene structure, you know, reflectivity,
all type of cues, then leveraging on those, and with attention mechanism transformers,
back to believe in the previous computer information, okay, there is the light now, invisible lights.
What was the direction of the light? What was the properties of lights? So, that was the challenging part.
What kind of constraints are you making on the image? For example, you have a good number of
candles behind you. Are you limiting the number of light sources that you're assuming to be in the
image? For example, it's a good question. We don't actually, that is quite restricted to
number of light sources. Type of light sources are maybe shape of the light sources. There are some
assumptions for windows, you know, there are maybe some additional assumptions. It is retained
for indoor scenes. Maybe I should mention this is specifically due to the data set that we use,
open rooms, data set, and scan it. It is for indoor scenes, but it could be any number of lights,
you know, kind of, yeah. Before we wrap up, Qualcomm has a number of other activities at CVPR. Let's
briefly have you share a little bit about those. One is a workshop on wireless AI perception.
What's that one about? Absolutely. That is the first time a wireless AI perception, wireless
itself is becoming a workshop at CVPR. CVPR is more visual data and, you know, there has been some
other models as well, but not at a degree of a workshop. And if you look at the field, we see that
people are using cameras in addition to together with, let's say, Wi-Fi or, you know, 5G or
terrorist imaging. So, for instance, there is a Wi-Fi around me right now and there is a camera
and together they accomplish more things than just using the camera or the Wi-Fi
separately. In this workshop, we bring the leaders in the field, they will, they are going to give
us several keynotes, maybe 7-6 keynotes and very exacting presentations, talks about tools
available, data sets available for this type of research. So, we bring these leaders and create a
platform so people can discuss further improvement ideas and share information, share their
observations. So, that is going to accelerate more research in this area.
When you mentioned Wi-Fi in that context, I know that Qualcomm has done some research around
using Wi-Fi signals to determine presence in a room, that kind of thing. Is that the sense in
what you're using it or more traditionally as a communication? That we showed we can do it.
It is not only sensing the person in the room, but some we actually know when person moves,
where the person is less than 10 centimeter accuracy, you know, depending on the number of
like the Wi-Fi access points, it could be even better. So, we know kind of like, and we can
track people in these Wi-Fi or 5G environments, it could be your phone, could be track, for instance,
using these access points. But in addition to that, we can also estimate the body pose of the people,
for instance, people know that they actually pull down to ground and they need help or not.
You know, those are the things that we are trying to accomplish. If there's a camera in the system,
it would make even stronger. In this workshop, we are not really presenting our work,
but our goal was to accelerate further research innovation in this area and support
everyone, academia and, you know, anyone interested in virus perception, and we will release
some data says as well. Okay, great. There's also a omnidirectional workshop. What's that one about?
So, the previous workshop initiated at Qualcomm and kind of several Qualcomm members are
in the organizing committee. They are sharing the event. The only directional
computer vision workshop is another one. We have the similar setting. It is the third time of
the third workshop, and in this edition, there are many people from all around the different
companies, autonomous vehicle companies, and academia joining this event, showing, presenting
ideas for a setting where cameras may not be just like our phone cameras, but could be
fisheye cameras or 360 cameras. The only directional kind of indicates in colors,
cameras with wider, much wider field of view. Of course, those cameras have different
geometry and different type of images. If you look at, for instance, maybe people who are
holding a stick 360 images, those images look like not like the pictures we take in our phones,
right? And because of that solutions, computer vision solutions kind of has to work in that setting
rather than, okay, I'm going to take this funny image and create kind of like regular rectangular
version of it. Let me do that, we lose information. So there's a reason why there are dedicated
solutions for omnidirectional cameras, and this workshop combines such recent, latest state of
the art research work and provides a platform for people to discuss and learn from each other,
share their experiences. One big application is autonomous vehicles. As you know, autonomous
vehicles have multiple cameras anywhere from, you know, four, five, six, seven, including the
internal one, external one. So you get a 360 feeling, a perception around the vehicle, and
how you are going to make sure all these information coming from these different cameras,
some of them are like the fisheye cameras, and there are other sensors can be combined,
in a way that, you know, all the process optimizes works better at a higher accuracy,
you know, more robust manner. So these are the things I think people will be discussing,
and these are the kind of some of the talks in the workshop as well. Awesome, awesome. And your
team also usually is showcasing some number of demos at conferences like CVPR, do you have any
demos this year? Yeah, usually we brought, we tried to bring mini demos, this time they are
only bringing two. There are much more demos at CVPR from Qualcomm. I mean, I'm just talking about
kind of my team, Qualcomm AI research, perception part. And one of the demos is called
this auxilire adaptation for semantic segmentation. The other one is 4K image super resolution. Oh, wow.
So folks who, whether you saw that at CVPR or not, there's a blog post that we'll link to in
the show notes and you might be able to catch those demos there. Well, Fatih, it was great chatting
with you and congrats on so many accepted papers at the conference and looking forward to
catching up again soon. Thank you so much, Sam. Thank you so much for having me. It was a pleasure.
I hope, you know, sometimes I state very high level, but there are many things in the papers,
you know, kind of I'm sure people will love the details we provide in the paper. Please let me know
if you have any questions also later. We'll definitely link to those papers in the show notes
and encourage folks to reach out if they have any questions.
