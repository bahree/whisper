1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,880
I'm your host Sam Charington.

4
00:00:23,880 --> 00:00:28,080
Just a couple of quick announcements today related to the Twimble Online Meetup.

5
00:00:28,080 --> 00:00:32,800
First, the video from our December meetup has been posted and it's now available on our

6
00:00:32,800 --> 00:00:36,920
YouTube channel and at twimbleai.com slash meetup.

7
00:00:36,920 --> 00:00:41,760
It was a great meetup, so if you missed it, you'll definitely want to check it out.

8
00:00:41,760 --> 00:00:46,240
But you definitely don't want to miss our next meetup either.

9
00:00:46,240 --> 00:00:52,080
On Tuesday, January 16th at 3 o'clock Pacific, we'll be joined by Microsoft Research's

10
00:00:52,080 --> 00:00:57,440
Timnett Gebru, who will be presenting her paper using deep learning and Google Street View

11
00:00:57,440 --> 00:01:03,040
to estimate the demographic makeup of neighborhoods across the United States, which has received

12
00:01:03,040 --> 00:01:06,560
national media attention for some of its findings.

13
00:01:06,560 --> 00:01:11,000
Timnett will be digging into those results as well as the pipeline she used to identify

14
00:01:11,000 --> 00:01:15,720
22 million cars and 50 million Google Street View images.

15
00:01:15,720 --> 00:01:20,920
I'm anticipating a very lively discussion segment as well to kick off the session, so make

16
00:01:20,920 --> 00:01:25,880
sure to bring your AI resolutions and predictions for 2018.

17
00:01:25,880 --> 00:01:32,480
For links to the paper or to join the meetup group, visit twimbleai.com slash meetup.

18
00:01:32,480 --> 00:01:34,320
Alright, onto today's show.

19
00:01:34,320 --> 00:01:39,520
In this episode, we hear from Kenneth Stanley, professor in the Department of Computer Science

20
00:01:39,520 --> 00:01:45,720
at the University of Central Florida and senior research scientist at Uber AI Labs.

21
00:01:45,720 --> 00:01:51,960
Kenneth studied under twimble talk number 47 guest, Risto Mikulainen at UT Austin, after

22
00:01:51,960 --> 00:01:57,520
geometric intelligence, the company he co-founded with Gary Marcus and others, was acquired

23
00:01:57,520 --> 00:01:59,520
in late 2016.

24
00:01:59,520 --> 00:02:04,360
Kenneth's research focuses Neuroevolution, which applies the idea of genetic algorithms

25
00:02:04,360 --> 00:02:08,160
to the challenge of evolving neural network architectures.

26
00:02:08,160 --> 00:02:14,240
In this conversation, we discuss the Neuroevolution of Augmenting Topologies, or Neat, paper

27
00:02:14,240 --> 00:02:20,040
that Kenneth authored along with Risto, which won the 2017 International Society for Artificial

28
00:02:20,040 --> 00:02:26,080
Life's Award for Outstanding Paper of the Decade 2002-2012.

29
00:02:26,080 --> 00:02:30,720
We also cover some of the extensions to that approach he's created since, including

30
00:02:30,720 --> 00:02:35,680
Hyper-Neat, which can efficiently evolve very large neural networks with connectivity

31
00:02:35,680 --> 00:02:41,240
patterns that look more like those of the human brain and that are generally much larger

32
00:02:41,240 --> 00:02:46,840
than what prior approaches to neural learning could produce, as well as novelty search,

33
00:02:46,840 --> 00:02:52,000
an approach that, unlike most evolutionary algorithms, has no defined objective, but

34
00:02:52,000 --> 00:02:55,240
rather simply searches for novel behaviors.

35
00:02:55,240 --> 00:03:01,760
We also cover concepts like complexification and deception, biology versus computation,

36
00:03:01,760 --> 00:03:06,520
and some of his other work, including his book, and Nero, a video game, complete with

37
00:03:06,520 --> 00:03:08,880
real-time neural evolution.

38
00:03:08,880 --> 00:03:13,320
This is a meaty, nerd alert interview that I think you'll really enjoy.

39
00:03:13,320 --> 00:03:15,880
And now on to the show.

40
00:03:15,880 --> 00:03:26,120
Alright, everyone, I am on the line with Kenneth Stanley.

41
00:03:26,120 --> 00:03:30,680
Kenneth is a professor in the Department of Computer Science at the University of Central

42
00:03:30,680 --> 00:03:36,160
Florida, as well as a senior research scientist at Uber AI Labs.

43
00:03:36,160 --> 00:03:39,120
Kenneth, welcome to this week in Machine Learning and AI.

44
00:03:39,120 --> 00:03:40,120
Thanks very much.

45
00:03:40,120 --> 00:03:41,120
Real happy to be here.

46
00:03:41,120 --> 00:03:42,120
Fantastic.

47
00:03:42,120 --> 00:03:45,800
Why don't we get started by having you tell us a little bit about your background?

48
00:03:45,800 --> 00:03:46,800
Sure.

49
00:03:46,800 --> 00:03:50,920
I've been interested in artificial intelligence since I was a little kid, maybe around

50
00:03:50,920 --> 00:03:55,920
eight years old, went on to major in computer science because of that, and carried that interest

51
00:03:55,920 --> 00:04:01,640
into graduate school, where I was at the University of Texas at Austin, where I did my PhD,

52
00:04:01,640 --> 00:04:06,760
and there I became interested in particular in neural networks, artificial neural networks,

53
00:04:06,760 --> 00:04:11,440
which are what are now the basis of deep learning, which everybody's talking about.

54
00:04:11,440 --> 00:04:17,280
And also what's called evolutionary computation, which means kind of Darwinian type of principles

55
00:04:17,280 --> 00:04:20,200
being applied inside of computer algorithms.

56
00:04:20,200 --> 00:04:24,040
And so the intersection of those two things is what's called today, neural evolution.

57
00:04:24,040 --> 00:04:28,480
So it means like evolving neural networks or like evolving brains, you could think of

58
00:04:28,480 --> 00:04:30,400
it as in a computer.

59
00:04:30,400 --> 00:04:34,960
And I guess my particular interest is just how brains evolved, you know, these amazing

60
00:04:34,960 --> 00:04:37,520
astronomically complex things that are in our heads.

61
00:04:37,520 --> 00:04:43,480
I was always fascinated by how an unguided process, seemingly an intelligent process like

62
00:04:43,480 --> 00:04:45,760
evolution could just produce something.

63
00:04:45,760 --> 00:04:50,080
So astronomically complex and amazing as our own brains.

64
00:04:50,080 --> 00:04:53,440
And so as a neural evolution researcher, I've been trying to figure out how can you actually

65
00:04:53,440 --> 00:04:58,200
make algorithms that would evolve something of similar scale and complexity?

66
00:04:58,200 --> 00:05:03,600
Was there anything in particular that you came across at the age of eight or so that

67
00:05:03,600 --> 00:05:05,440
got you interested in AI?

68
00:05:05,440 --> 00:05:06,440
Yeah, yeah.

69
00:05:06,440 --> 00:05:10,200
So at the age of eight, that's when my family bought a computer.

70
00:05:10,200 --> 00:05:12,680
It was like a Commodore 64.

71
00:05:12,680 --> 00:05:13,680
Yes.

72
00:05:13,680 --> 00:05:17,640
And it was, it was also I, my parents put me in a programming class.

73
00:05:17,640 --> 00:05:21,240
And that was on a TRS 80, which is a very old computer system.

74
00:05:21,240 --> 00:05:22,240
And.

75
00:05:22,240 --> 00:05:23,240
Flash 80.

76
00:05:23,240 --> 00:05:24,240
Yeah.

77
00:05:24,240 --> 00:05:25,240
Exactly.

78
00:05:25,240 --> 00:05:30,400
And I guess for some reason, like, as a little kid, it just really made an impression on

79
00:05:30,400 --> 00:05:33,120
me that I could tell the computer to do anything.

80
00:05:33,120 --> 00:05:36,680
Like I had this feeling like there was like infinite freedom in the things that I could

81
00:05:36,680 --> 00:05:38,840
get the computer can do to do.

82
00:05:38,840 --> 00:05:42,080
If only I could just figure out how to tell it what I wanted.

83
00:05:42,080 --> 00:05:43,080
Right.

84
00:05:43,080 --> 00:05:46,680
And I felt like if I could just tell it how to have a conversation with me, then it would

85
00:05:46,680 --> 00:05:50,800
basically be my friend or like talk to me.

86
00:05:50,800 --> 00:05:55,240
And I was really, really interested in just getting the computer to have a conversation

87
00:05:55,240 --> 00:05:59,600
with me, like a casual conversation, like how are you doing, what's your name, that kind

88
00:05:59,600 --> 00:06:00,840
of thing.

89
00:06:00,840 --> 00:06:06,000
And at first, I would write really simple programs and basic, the basic computer language

90
00:06:06,000 --> 00:06:09,000
that would have like little conversations like this, like I'd say, what's your name?

91
00:06:09,000 --> 00:06:12,480
I'd say Ken, like basically in typing and they would say, hi, Ken.

92
00:06:12,480 --> 00:06:16,120
And I was very impressed that we could have this kind of conversation and that I got it

93
00:06:16,120 --> 00:06:17,120
to do that.

94
00:06:17,120 --> 00:06:22,640
But I quickly hit a wall where I couldn't get it to like really do anything interesting,

95
00:06:22,640 --> 00:06:25,560
you know, just a very star scripted thing.

96
00:06:25,560 --> 00:06:30,320
And at the time, like around age eight, I thought there's some way to do this that I just

97
00:06:30,320 --> 00:06:33,280
need to read a book or something like there's something that would just tell me how to get

98
00:06:33,280 --> 00:06:35,920
it to have a real conversation with me.

99
00:06:35,920 --> 00:06:40,040
And I didn't realize that this is like one of the greatest problems like facing humankind,

100
00:06:40,040 --> 00:06:43,120
like how to get a computer actually being intelligent, like a real person.

101
00:06:43,120 --> 00:06:46,520
And it took me a while actually for it to strike me that this is actually like an extremely

102
00:06:46,520 --> 00:06:47,520
hard problem.

103
00:06:47,520 --> 00:06:51,720
And there's not just like some manual you can read that can get the computer to do that.

104
00:06:51,720 --> 00:06:55,560
So I probably, within a couple of years, I realized this is like a huge problem and then

105
00:06:55,560 --> 00:06:59,720
I was really interested in hooked and like, wow, this is actually hard and like there's

106
00:06:59,720 --> 00:07:01,520
got to be a way to do this.

107
00:07:01,520 --> 00:07:06,120
And I guess I would just stay captivated by that problem like forever.

108
00:07:06,120 --> 00:07:10,080
But I guess I changed the shift a bit in my interest because if you look at that and

109
00:07:10,080 --> 00:07:13,760
you look at it from the lens of like today's subfields of artificial intelligence, you

110
00:07:13,760 --> 00:07:16,840
probably call that natural language processing or something like that.

111
00:07:16,840 --> 00:07:21,840
And I kind of shifted away from that over time to more like lower level stuff, like control,

112
00:07:21,840 --> 00:07:22,840
like neural stuff.

113
00:07:22,840 --> 00:07:27,080
That was like what initially hooked me into it and got me into the AI.

114
00:07:27,080 --> 00:07:28,080
Interesting.

115
00:07:28,080 --> 00:07:30,600
And you mentioned that you studied at UT Austin.

116
00:07:30,600 --> 00:07:35,600
I did an interview with Risto, Michaeline and did you study with him there?

117
00:07:35,600 --> 00:07:36,600
Yeah.

118
00:07:36,600 --> 00:07:41,240
So I guess it's just a coincidence that Risto is my advisor or was my advisor during the

119
00:07:41,240 --> 00:07:42,240
PhD.

120
00:07:42,240 --> 00:07:44,240
I worked with him for years there.

121
00:07:44,240 --> 00:07:45,240
Yeah.

122
00:07:45,240 --> 00:07:46,240
Awesome.

123
00:07:46,240 --> 00:07:47,240
Awesome.

124
00:07:47,240 --> 00:07:50,360
Can you tell me a little bit about your primary research focus?

125
00:07:50,360 --> 00:07:51,360
Sure.

126
00:07:51,360 --> 00:07:56,880
So my primary research focus is in an area called Neuroevolution.

127
00:07:56,880 --> 00:08:02,840
And it's an area that is probably less well known in the general public like you hear tons

128
00:08:02,840 --> 00:08:08,480
of stuff about deep learning today, but you don't hear so much about neural evolution.

129
00:08:08,480 --> 00:08:14,040
It's certainly related to deep learning because both of them are about in effect neural networks.

130
00:08:14,040 --> 00:08:18,320
But Neuroevolution has this twist, which is that we're interested in neural networks,

131
00:08:18,320 --> 00:08:24,600
which are for those who don't know basically these rough abstractions of what happens in

132
00:08:24,600 --> 00:08:25,600
brains.

133
00:08:25,600 --> 00:08:28,440
Like, you know, the word neural comes from neurons and neurons are in our brain.

134
00:08:28,440 --> 00:08:34,520
So neural networks are roughly motivated or inspired by brains in nature, although they're

135
00:08:34,520 --> 00:08:37,280
not at all accurate models of them.

136
00:08:37,280 --> 00:08:42,760
But then in neural evolution, we're combining that with evolutionary principles, which really

137
00:08:42,760 --> 00:08:44,600
means kind of like breeding.

138
00:08:44,600 --> 00:08:47,720
Like if you think about it, like it's like if you had a neural network that does something

139
00:08:47,720 --> 00:08:52,120
good, like say drives a robot and makes it able to do attacks, like say, walk, like it

140
00:08:52,120 --> 00:08:56,720
gets your biped robot to walk, then like, neural evolution is kind of like you're breeding

141
00:08:56,720 --> 00:08:57,720
those brains.

142
00:08:57,720 --> 00:09:00,000
So you're saying, okay, I have a bunch of brains.

143
00:09:00,000 --> 00:09:01,000
These are artificial brains.

144
00:09:01,000 --> 00:09:03,600
We'll call them neural networks though, because artificial brains exaggerates like how

145
00:09:03,600 --> 00:09:04,600
cool they are.

146
00:09:04,600 --> 00:09:09,080
They do their artificial neural networks, and we would then look at like, well, how well

147
00:09:09,080 --> 00:09:12,960
do they get the robot to walk, like a whole bunch of them, and they call that a population.

148
00:09:12,960 --> 00:09:16,960
And then like we choose the ones that do better, some will do worse, and some will do better.

149
00:09:16,960 --> 00:09:20,440
And those that do better will have children, which basically means like new neural that

150
00:09:20,440 --> 00:09:24,880
will be born as offspring of the old ones that we chose, or we call that selection,

151
00:09:24,880 --> 00:09:26,440
we selected those.

152
00:09:26,440 --> 00:09:30,960
And our hope is that the offspring of those better ones will sometimes be even better

153
00:09:30,960 --> 00:09:31,960
than their parents.

154
00:09:31,960 --> 00:09:35,680
And we keep on playing this game, which is just breeding, so like it's not hard to understand,

155
00:09:35,680 --> 00:09:39,160
like some areas of AI are kind of complex, and are to understand at first.

156
00:09:39,160 --> 00:09:42,760
But intuitively, this is easy, because this is just like breeding horses or breeding dogs.

157
00:09:42,760 --> 00:09:47,280
They just choose the ones that are better in respect to whatever criteria you have, and

158
00:09:47,280 --> 00:09:50,400
then just breed them, and hope that things get better over time.

159
00:09:50,400 --> 00:09:53,880
And so a nerve evolution is basically about breeding these artificial things, rather than

160
00:09:53,880 --> 00:09:58,320
real organisms, which are these artificial neural networks, and thereby getting them to

161
00:09:58,320 --> 00:10:00,680
get better over generations.

162
00:10:00,680 --> 00:10:05,400
And what is interesting about it to me is that, like, well, it's like a simple concept

163
00:10:05,400 --> 00:10:09,360
in principle, at least like the initial outline that I gave is quite simple just in terms

164
00:10:09,360 --> 00:10:10,440
of breeding.

165
00:10:10,440 --> 00:10:16,000
Like under the hood, there's like real mysteries here, because this is really the process,

166
00:10:16,000 --> 00:10:20,560
you know, that produced you and me, and like the high level of intelligence that we have,

167
00:10:20,560 --> 00:10:26,240
going all the way back to single-celled organisms, and it's quite amazing to believe that, like,

168
00:10:26,240 --> 00:10:32,040
there is some kind of path through that space just through breeding that can lead to something

169
00:10:32,040 --> 00:10:35,640
like us from something so humble and simple.

170
00:10:35,640 --> 00:10:41,240
And to get algorithms to do that is an enormous challenge, and not fully understood right

171
00:10:41,240 --> 00:10:42,240
now.

172
00:10:42,240 --> 00:10:45,480
And that's where kind of the research comes in in the field.

173
00:10:45,480 --> 00:10:46,480
Interesting.

174
00:10:46,480 --> 00:10:47,480
Interesting.

175
00:10:47,480 --> 00:10:51,480
And then you're also, again, a senior research scientist at Uber AI Labs.

176
00:10:51,480 --> 00:10:55,600
What can you tell us about Uber AI labs and how that came about and what the charter

177
00:10:55,600 --> 00:10:56,600
is there?

178
00:10:56,600 --> 00:10:57,600
Right.

179
00:10:57,600 --> 00:11:02,760
So there was no Uber AI labs around nine months ago, but I was one of the co-founders of

180
00:11:02,760 --> 00:11:06,560
a startup company called Geometric Intelligence.

181
00:11:06,560 --> 00:11:12,200
My co-founders were included Gary Marcus, Zubin Garmani, and Doug Beamus.

182
00:11:12,200 --> 00:11:16,760
Some of them are really quite well known and have very respected researchers themselves.

183
00:11:16,760 --> 00:11:23,000
And we were doing in Geometric Intelligence proprietary machine learning research and

184
00:11:23,000 --> 00:11:27,480
developing new technologies and building a team that we were hoping to be a world-class

185
00:11:27,480 --> 00:11:29,320
research team.

186
00:11:29,320 --> 00:11:34,760
And what happened was that Uber acquired us nine months ago in December.

187
00:11:34,760 --> 00:11:40,720
And when Uber acquired us, they had partly one of their aspirations was to start in

188
00:11:40,720 --> 00:11:47,480
AI lab like a real research lab in industry that researchers the cutting edge of artificial

189
00:11:47,480 --> 00:11:52,960
intelligence because Uber believes and believes at the time that artificial intelligence

190
00:11:52,960 --> 00:11:59,240
is a critical competitive component of the industry where Uber needs to be staying at the

191
00:11:59,240 --> 00:12:00,560
cutting edge.

192
00:12:00,560 --> 00:12:05,800
And Uber has, and had before, a lot of competence already in machine learning.

193
00:12:05,800 --> 00:12:08,760
So it's not like there was nobody here that were plenty of people here who were very

194
00:12:08,760 --> 00:12:10,600
qualified in the field.

195
00:12:10,600 --> 00:12:14,880
But they didn't have something that was really a fundamental research lab and where they're

196
00:12:14,880 --> 00:12:19,120
sort of just really pushing on the cutting edge of AI itself as opposed to just applying

197
00:12:19,120 --> 00:12:20,120
it to internal problems.

198
00:12:20,120 --> 00:12:25,640
Like, for example, Uber has a team focused already that was focused on autonomous driving.

199
00:12:25,640 --> 00:12:30,360
And so they already had that in place, but that's an applied aspect of artificial intelligence.

200
00:12:30,360 --> 00:12:35,440
And so the AI lab that was founded off of the company that we started, which we founded,

201
00:12:35,440 --> 00:12:40,160
was really intended to be focused more in advancing the algorithms themselves.

202
00:12:40,160 --> 00:12:44,560
And so what Uber got was basically all at once, like all of these researchers who had

203
00:12:44,560 --> 00:12:48,560
this capacity to push forward the field of AI.

204
00:12:48,560 --> 00:12:53,400
And so you can kind of think about it roughly in analogy with similar types of research

205
00:12:53,400 --> 00:12:57,920
labs at big tech companies, like maybe like something like DeepMind, which was originally

206
00:12:57,920 --> 00:13:02,160
acquired by Google or something like Facebook AI research, or just also Google Brain and

207
00:13:02,160 --> 00:13:03,160
Google.

208
00:13:03,160 --> 00:13:05,800
So there's some rough analogy there between us.

209
00:13:05,800 --> 00:13:08,200
And then we're much smaller though, because we're newer.

210
00:13:08,200 --> 00:13:13,520
But we have the kind of similar mandates in terms of researching the cutting edge of AI.

211
00:13:13,520 --> 00:13:17,320
And I should say that actually we're going to, we are going to engage with the outside world

212
00:13:17,320 --> 00:13:18,320
in the academic community.

213
00:13:18,320 --> 00:13:22,680
You'll be hearing from Uber AI labs and we're going to be publishing and we understand

214
00:13:22,680 --> 00:13:28,320
that like just we cannot be a successful AI lab if we are not engaged with the outside

215
00:13:28,320 --> 00:13:29,320
world.

216
00:13:29,320 --> 00:13:34,000
So we will be publicizing and publishing some of our work so people can see what we're

217
00:13:34,000 --> 00:13:38,560
doing and so that we can communicate with other other researchers and scientists across

218
00:13:38,560 --> 00:13:39,560
the world.

219
00:13:39,560 --> 00:13:40,560
Okay.

220
00:13:40,560 --> 00:13:41,560
Great.

221
00:13:41,560 --> 00:13:42,560
Great.

222
00:13:42,560 --> 00:13:50,640
So a little bit about the intersection between your work and evolutionary AI and the kind

223
00:13:50,640 --> 00:13:54,560
of things that Uber is doing around self-driving cars.

224
00:13:54,560 --> 00:13:55,560
Yeah.

225
00:13:55,560 --> 00:13:59,560
So I can't get into specifics about what Uber is doing with their self-driving cars for

226
00:13:59,560 --> 00:14:00,560
a obvious reason.

227
00:14:00,560 --> 00:14:04,400
But I can say that Uber AI labs is diverse.

228
00:14:04,400 --> 00:14:09,920
I mean, that was one of the original inspirations behind geometric intelligence or the predecessor

229
00:14:09,920 --> 00:14:16,520
to Uber AI labs was to have a diverse group that isn't just in one particular fad which

230
00:14:16,520 --> 00:14:20,760
you might say deep learning is although it's obviously an important one that's making

231
00:14:20,760 --> 00:14:22,600
a lot of important contributions.

232
00:14:22,600 --> 00:14:27,280
But our philosophy was that, you know, we need to not have all our eggs in one basket.

233
00:14:27,280 --> 00:14:31,400
And so Uber AI labs itself is like that too and that we have a lot of diversity in terms

234
00:14:31,400 --> 00:14:34,600
of the expertise and areas that we cover.

235
00:14:34,600 --> 00:14:40,360
And so among those, we clearly are world class in neural evolution, which is the field that

236
00:14:40,360 --> 00:14:44,200
I just described where I've focused at most of my career.

237
00:14:44,200 --> 00:14:51,000
And so this is a particular direction within AI and machine learning that offers some

238
00:14:51,000 --> 00:14:56,640
unique insights and angles on certain types of problems that other areas might have a

239
00:14:56,640 --> 00:14:57,640
different take on.

240
00:14:57,640 --> 00:15:05,120
So in terms of like autonomous driving, I mean, it's clear that the idea of the evolution

241
00:15:05,120 --> 00:15:11,920
of complexity and how really high level intelligence can be evolved in terms of complex, large

242
00:15:11,920 --> 00:15:18,120
deep artificial neural networks has a connection in principle to how you could get a really

243
00:15:18,120 --> 00:15:21,880
sophisticated controller for a vehicle or something like that.

244
00:15:21,880 --> 00:15:26,760
And so the insights of the field of neural evolution, both directly, which means like using

245
00:15:26,760 --> 00:15:31,960
neural gene itself as an algorithm and indirectly in terms of insights that we gain as a side

246
00:15:31,960 --> 00:15:37,720
effect of doing experiments in that area, can impact how we would create algorithms that

247
00:15:37,720 --> 00:15:40,520
might control things like autonomous vehicles.

248
00:15:40,520 --> 00:15:45,440
But I should also note that it's not that it's not the case that the only application

249
00:15:45,440 --> 00:15:50,720
or even necessarily the main application of AI at Uber is in that area.

250
00:15:50,720 --> 00:15:54,840
I mean, Uber has AI problems across the gamut of all of their business components.

251
00:15:54,840 --> 00:15:59,160
So there's a lot of different applications that are under consideration when it comes

252
00:15:59,160 --> 00:16:01,800
to like AI labs and what AI helps does.

253
00:16:01,800 --> 00:16:02,800
Sure.

254
00:16:02,800 --> 00:16:10,000
So can you talk a little bit about how your research focus kind of compares and contrast

255
00:16:10,000 --> 00:16:13,320
with what RISTO is doing down at UT Austin?

256
00:16:13,320 --> 00:16:14,320
Yeah, sure.

257
00:16:14,320 --> 00:16:20,360
So I mean, actually, there's a lot of overlap because I mean, I'm his advisor, so I've

258
00:16:20,360 --> 00:16:26,480
taken a lot of the original teachings that he gave me as a basis of my career and obviously

259
00:16:26,480 --> 00:16:31,280
collaborated with him for years to publish some of the, in the end, it turned out to be

260
00:16:31,280 --> 00:16:34,760
some of the seminal papers in the area, both together.

261
00:16:34,760 --> 00:16:40,800
And so I think we're not actually so different in terms of like the fields that we're interested

262
00:16:40,800 --> 00:16:47,600
in where we may differ is more just in like what particular algorithms have we contributed

263
00:16:47,600 --> 00:16:53,120
to inventing sensory parted ways when I basically graduated with the PhD.

264
00:16:53,120 --> 00:17:00,200
And so he's focused on his own set of innovations and I've focused on my own and there's some

265
00:17:00,200 --> 00:17:01,200
divergence there.

266
00:17:01,200 --> 00:17:07,680
But we really ultimately tend to be very close because like when I've invented new things

267
00:17:07,680 --> 00:17:12,560
like I don't know is it, and I'm still at the University of Central Florida as a professor

268
00:17:12,560 --> 00:17:17,120
RISTO would sometimes build on those things in vice versa.

269
00:17:17,120 --> 00:17:21,440
So we're very intertwined and it's not a surprise since we started out in same area.

270
00:17:21,440 --> 00:17:22,840
Absolutely, absolutely.

271
00:17:22,840 --> 00:17:28,520
And so folks that are interested in maybe some of the background on, you know, you talked

272
00:17:28,520 --> 00:17:32,680
about the kind of breeding process that are really high level, RISTO and I spent quite

273
00:17:32,680 --> 00:17:37,880
a bit of time digging into that in more detail, you know, so folks that are interested

274
00:17:37,880 --> 00:17:44,560
in that might want to refer back to to that podcast since you've graduated and now that

275
00:17:44,560 --> 00:17:50,920
you're kind of driving your own research agenda, like what are some of the specific algorithms

276
00:17:50,920 --> 00:17:55,920
that you've published research on and, you know, how do they build on kind of that, the

277
00:17:55,920 --> 00:18:01,600
core ideas of genetic or evolutionary computing or algorithms?

278
00:18:01,600 --> 00:18:02,600
Yeah, sure.

279
00:18:02,600 --> 00:18:08,880
So, so a neural evolution, which is this idea of evolving neural networks, like one interesting

280
00:18:08,880 --> 00:18:14,520
thing is that when, what we're, at least for me, what I find really interesting is not

281
00:18:14,520 --> 00:18:19,480
just optimization, like a lot of people in machine learning think in terms of optimization,

282
00:18:19,480 --> 00:18:24,480
which means just like how do you get this structure to get better and better and better

283
00:18:24,480 --> 00:18:25,920
with respect to a task?

284
00:18:25,920 --> 00:18:29,680
But I'm also interested in what you might call complexification, which means like how

285
00:18:29,680 --> 00:18:33,960
do we get increasing complexity, like the thing that really fascinates me is like how in

286
00:18:33,960 --> 00:18:37,960
nature things got more complex, like insanely more complex.

287
00:18:37,960 --> 00:18:41,800
Not just like a little bit of incremental increases in complexity, but like from a single

288
00:18:41,800 --> 00:18:47,080
cell to organism to something that has in our brain a hundred trillion connections among

289
00:18:47,080 --> 00:18:51,360
a hundred billion cells, approximately, or a hundred billion neurons, and that's just

290
00:18:51,360 --> 00:18:56,120
amazing to me that like some kind of unguided process could build something like that.

291
00:18:56,120 --> 00:19:01,360
This is not something that was engineered and so I'm sort of always have my eye on like

292
00:19:01,360 --> 00:19:07,600
what is it that allows really high level astronomical levels of complexity to emerge from

293
00:19:07,600 --> 00:19:10,280
this kind of process, kind of automated process.

294
00:19:10,280 --> 00:19:13,600
And so the interesting thing in neural evolution is that every time it seems like we have an

295
00:19:13,600 --> 00:19:17,480
advance where we kind of figure out something about how do you get increasing complexity to

296
00:19:17,480 --> 00:19:22,760
happen inside of an algorithm, and we've made some advances, including the first thing

297
00:19:22,760 --> 00:19:28,080
that I did in grad school, which was this algorithm called neat or neural evolution of

298
00:19:28,080 --> 00:19:33,720
augmenting topologies, which I did with Risto, which was basically an algorithm about how

299
00:19:33,720 --> 00:19:39,320
can we have the neural networks that are evolving in the computer, increasing complexity over

300
00:19:39,320 --> 00:19:42,960
the course of the algorithm running in the computer.

301
00:19:42,960 --> 00:19:48,160
And it was because I had this real fascination with increasing complexity that led to us introducing

302
00:19:48,160 --> 00:19:51,800
this algorithm that increases complexity, but then what's interesting is that every time

303
00:19:51,800 --> 00:19:58,000
we make an advance like that, it sort of uncover some like deeper underlying question, because

304
00:19:58,000 --> 00:20:02,760
it turns out that like the explanation for why it was possible to get from one cell to

305
00:20:02,760 --> 00:20:08,080
trillions is really, really subtle and nuanced and complicated.

306
00:20:08,080 --> 00:20:13,720
And when you say that, are you speaking biologically or from a computational context?

307
00:20:13,720 --> 00:20:14,720
Right.

308
00:20:14,720 --> 00:20:15,720
Good question.

309
00:20:15,720 --> 00:20:16,720
Yeah.

310
00:20:16,720 --> 00:20:20,320
So actually, those things constantly get intertwined in my mind, like whether I'm speaking

311
00:20:20,320 --> 00:20:25,840
biologically or computationally, because the way I look at it is kind of like the biology

312
00:20:25,840 --> 00:20:31,320
and computation aren't really necessarily different things, like in effect, like if you read

313
00:20:31,320 --> 00:20:35,760
a biology textbook, you know, you feel like you're reading about biology, but like in effect,

314
00:20:35,760 --> 00:20:40,440
it's also about computers because, or at least algorithms, you know, because you're talking

315
00:20:40,440 --> 00:20:45,640
about a principled process that basically follows some certain kinds of rules.

316
00:20:45,640 --> 00:20:49,200
Just these analog computers that we really don't understand very well.

317
00:20:49,200 --> 00:20:52,760
Yeah, you could think of like the universe as a big analog computer, we don't really

318
00:20:52,760 --> 00:20:53,760
understand.

319
00:20:53,760 --> 00:20:58,360
And so like, I mean, but like evolution is a very algorithmic thing, you know, you're talking

320
00:20:58,360 --> 00:21:01,960
about there are individuals and those individuals reproduce.

321
00:21:01,960 --> 00:21:06,200
And then the thing that, and who gets to reproduce is based on a formula, which is, which is

322
00:21:06,200 --> 00:21:11,000
obviously complicated, but basically some, some individuals reproduce some, some don't.

323
00:21:11,000 --> 00:21:13,920
And this can be formalized as basically like a program.

324
00:21:13,920 --> 00:21:16,560
You could imagine writing the rules of the system.

325
00:21:16,560 --> 00:21:19,320
And this is what inspired the field of evolutionary computation.

326
00:21:19,320 --> 00:21:24,600
I mean, people saw the theories evolution in biology and thought like, you know what?

327
00:21:24,600 --> 00:21:29,440
This is actually not that hard to write down as a program and actually make evolution happen

328
00:21:29,440 --> 00:21:31,960
artificially inside of a computer.

329
00:21:31,960 --> 00:21:36,040
And it turned out though that like, if you just read a textbook and then, you know, learn

330
00:21:36,040 --> 00:21:40,480
these principles that sound like good explanatory principles for like how evolution works.

331
00:21:40,480 --> 00:21:43,080
Like if you read a biology, text was like, well, they know how it worked.

332
00:21:43,080 --> 00:21:44,400
That's an explanation.

333
00:21:44,400 --> 00:21:48,680
It turns out that explaining something is easier than actually implementing it, which is

334
00:21:48,680 --> 00:21:52,640
basically something that we found across the field of artificial intelligence.

335
00:21:52,640 --> 00:21:55,920
You know, you can read about, you can read a neuroscience textbook and say, this is

336
00:21:55,920 --> 00:21:56,920
how brains work.

337
00:21:56,920 --> 00:22:00,800
Of course, biology will acknowledge we don't know everything, but this is what we understand

338
00:22:00,800 --> 00:22:01,800
now.

339
00:22:01,800 --> 00:22:05,920
It's a comprehensive explanation, but it's far, far away from like telling you how to

340
00:22:05,920 --> 00:22:06,920
actually build a brain.

341
00:22:06,920 --> 00:22:10,400
I don't know how to build a brain just because we have some understanding of how brains

342
00:22:10,400 --> 00:22:11,400
work.

343
00:22:11,400 --> 00:22:12,400
It's the same with evolution.

344
00:22:12,400 --> 00:22:16,360
Like, we don't know how to build a true evolutionary system at the scale and magnitude

345
00:22:16,360 --> 00:22:21,360
of what happens on Earth, even though we know a lot of the details about what goes on.

346
00:22:21,360 --> 00:22:25,640
And the missing details, like the gap between what we understand and what we can actually

347
00:22:25,640 --> 00:22:30,400
build, that's where the research is and that's where like a lot of fascinating insights

348
00:22:30,400 --> 00:22:31,400
occur.

349
00:22:31,400 --> 00:22:37,040
Like to me, I think that to some extent, like when we make advances in artificial intelligence,

350
00:22:37,040 --> 00:22:41,280
we're actually learning something about biology in a sense because we're realizing that

351
00:22:41,280 --> 00:22:46,280
the gap in our knowledge, like what we didn't understand, are actually filled by something

352
00:22:46,280 --> 00:22:50,720
that we didn't expect or that wasn't in the textbook about how things work.

353
00:22:50,720 --> 00:22:55,560
And it's true that sometimes we may be doing things that are not actually the same as biology,

354
00:22:55,560 --> 00:23:00,040
but at least they're revealing gaps in our knowledge of biology because like if in some

355
00:23:00,040 --> 00:23:03,280
sense, if we actually knew everything about how things works, then we could just program

356
00:23:03,280 --> 00:23:05,280
it in, but we clearly don't.

357
00:23:05,280 --> 00:23:09,480
And so it's kind of like, I think AI has like a higher bar in a way than biology where

358
00:23:09,480 --> 00:23:14,120
in biology, like you can explain something or statistically analyze it, but then I actually

359
00:23:14,120 --> 00:23:16,720
actually have to build it, which is much, much harder.

360
00:23:16,720 --> 00:23:20,720
So it sort of forces us to grapple with the problems of the gaps in our knowledge and

361
00:23:20,720 --> 00:23:21,720
biology.

362
00:23:21,720 --> 00:23:25,640
Now some people in AI would just sort of like say not like that way of looking at things

363
00:23:25,640 --> 00:23:29,480
because some people in AI don't care about the biology and they just want to build intelligent

364
00:23:29,480 --> 00:23:33,880
things and they don't really care, do these things correspond or not with biology.

365
00:23:33,880 --> 00:23:34,880
That's not the goal.

366
00:23:34,880 --> 00:23:38,720
The goal is just to build intelligent things, we aren't like adhering to biology or

367
00:23:38,720 --> 00:23:39,720
not.

368
00:23:39,720 --> 00:23:44,080
I tend to be more biologically inspired, but I also agree that like I don't really,

369
00:23:44,080 --> 00:23:47,560
honestly, ultimately care whether what I build is exactly the way it works in biology

370
00:23:47,560 --> 00:23:53,080
or not, but I just find it interesting and inspiring that biology has achieved things

371
00:23:53,080 --> 00:23:58,320
that are just so amazing, I mean like human level intelligence, and I find it fascinating

372
00:23:58,320 --> 00:24:03,480
that we just don't know how, and like trying to probe those gaps in my understanding,

373
00:24:03,480 --> 00:24:09,640
I find leads to over and over again, really deep insights in artificial intelligence

374
00:24:09,640 --> 00:24:13,080
because it's like we suddenly realize, oh wait a second, actually there's an explanation

375
00:24:13,080 --> 00:24:16,800
here which is much different than what we thought it might be.

376
00:24:16,800 --> 00:24:23,480
And so after a graduate school, like there was a succession of those that I went through,

377
00:24:23,480 --> 00:24:27,880
we would realize that, you know, there's something missing still after like for example

378
00:24:27,880 --> 00:24:32,800
the need algorithm, which actually became the most used algorithm in this sort of niche

379
00:24:32,800 --> 00:24:37,280
field of neuro evolution, but we realized, you know, there's limitations on what need

380
00:24:37,280 --> 00:24:38,920
can ever do.

381
00:24:38,920 --> 00:24:42,280
And so this will, wow, can we get around those limitations?

382
00:24:42,280 --> 00:24:44,760
How did nature get around those limitations?

383
00:24:44,760 --> 00:24:50,400
So like one example is that like in need, there's this artificial DNA, which encodes the

384
00:24:50,400 --> 00:24:53,440
neural network, so we have to do evolution, so we have like an artificial DNA, which we

385
00:24:53,440 --> 00:24:58,960
call a genome, well it would have one gene per connection in this brain that's evolving.

386
00:24:58,960 --> 00:25:04,040
And like this is clearly not going to scale, even though like this, this brain can keep

387
00:25:04,040 --> 00:25:08,400
expanding, but like if you wanted to get 100 trillion connections, this is what we have

388
00:25:08,400 --> 00:25:13,080
in our brain right now in biology, we would need 100 trillion genes in need.

389
00:25:13,080 --> 00:25:15,160
And there is no way that's ever going to happen.

390
00:25:15,160 --> 00:25:19,080
100 trillion genes is just astronomically insanely large.

391
00:25:19,080 --> 00:25:24,280
And like for example, our genome in biology only has 30,000 genes, or 3 billion base pairs,

392
00:25:24,280 --> 00:25:26,320
another way of thinking about it.

393
00:25:26,320 --> 00:25:31,520
So we had to invent new algorithms, and this is after grad school and after need that

394
00:25:31,520 --> 00:25:36,000
could encode much, much larger structures, we called these indirect encodings, and this

395
00:25:36,000 --> 00:25:41,520
led to something called hyperneet eventually, which is a new kind of genetic encoding that

396
00:25:41,520 --> 00:25:44,040
is much more compact than the original need.

397
00:25:44,040 --> 00:25:49,920
And so hyperneet was something that I did after I left UT Austin, and so where I did that

398
00:25:49,920 --> 00:25:55,920
independently of Risto, and led to the ability to evolve much bigger in effect neural networks.

399
00:25:55,920 --> 00:26:01,080
And then I think one of the biggest things probably that has had a lot of impact in the

400
00:26:01,080 --> 00:26:05,960
field after that was something called novelty search, which is a result of discovering

401
00:26:05,960 --> 00:26:11,720
that in some cases the best way to get something in a search process, in evolution to kind of

402
00:26:11,720 --> 00:26:16,000
a search process, like you're searching through space of possibilities, is to not be trying

403
00:26:16,000 --> 00:26:17,000
to get it.

404
00:26:17,000 --> 00:26:22,400
And this was a really counterintuitive and paradoxical insight, but really important I think for

405
00:26:22,400 --> 00:26:24,080
realizing how things are achieved.

406
00:26:24,080 --> 00:26:29,280
So in other words, if you say that you're trying to breed for something, like say we want

407
00:26:29,280 --> 00:26:34,520
to get human level intelligence, then that actually may doom you from the start.

408
00:26:34,520 --> 00:26:38,400
Like sometimes the only way to get to something is to not be trying to get it.

409
00:26:38,400 --> 00:26:42,840
And this is a hard kind of a bitter pill to swallow, but something that, what is the mechanism

410
00:26:42,840 --> 00:26:45,840
of frying that keeps you from being able to get it?

411
00:26:45,840 --> 00:26:49,240
Yeah, so the mechanism there is something called deception.

412
00:26:49,240 --> 00:26:52,480
And actually this is something that applies way, way outside just neural evolution.

413
00:26:52,480 --> 00:26:55,560
This is a general principle for everything in life.

414
00:26:55,560 --> 00:26:56,560
Is that deception?

415
00:26:56,560 --> 00:26:58,280
It's called deception, yeah.

416
00:26:58,280 --> 00:27:04,720
It's basically the situation when if you are observing that things are getting better,

417
00:27:04,720 --> 00:27:08,200
so it's like you have some metric for what it means to be doing well, like performance

418
00:27:08,200 --> 00:27:11,600
metric, like let's say, how well are you able to walk?

419
00:27:11,600 --> 00:27:14,200
And so you have some metric that says, well, how well am I walking?

420
00:27:14,200 --> 00:27:18,720
And so normally, like if I was trying to get something to walk, I would select things,

421
00:27:18,720 --> 00:27:24,960
meaning I would breed things that are apparently better walking compared to their predecessors.

422
00:27:24,960 --> 00:27:26,320
And I would call that their fitness.

423
00:27:26,320 --> 00:27:27,800
And so that's what I mean by trying.

424
00:27:27,800 --> 00:27:31,240
I keep on intentionally picking things that seem to be better.

425
00:27:31,240 --> 00:27:36,760
And this is a very intuitive idea, like everybody for a long time felt like this is obviously

426
00:27:36,760 --> 00:27:40,200
the way to get things to evolve is to pick things that are better.

427
00:27:40,200 --> 00:27:44,640
But it turns out that if you're in a deceptive situation, which it turns out unfortunately

428
00:27:44,640 --> 00:27:50,480
you often are in, that you can be moving in the wrong direction, even though your metric

429
00:27:50,480 --> 00:27:52,520
for performance is going up.

430
00:27:52,520 --> 00:27:55,560
And that's because like the world is really, really complicated.

431
00:27:55,560 --> 00:28:00,040
So it can appear that you're improving in some way when you're actually not.

432
00:28:00,040 --> 00:28:04,360
And so for example, like when it comes to walking, like lunging forward like a maniac and

433
00:28:04,360 --> 00:28:09,560
falling down like a few feet from where you started may appear to actually be an improvement

434
00:28:09,560 --> 00:28:13,280
in your ability to travel, you know, because basically you're getting farther than your predecessors

435
00:28:13,280 --> 00:28:17,200
by throwing yourself on your head like five feet in front of you.

436
00:28:17,200 --> 00:28:21,640
But this is actually not a good stepping stone towards really good walking behavior.

437
00:28:21,640 --> 00:28:26,600
In fact, like a good stepping stone might be discovering the concept of oscillation.

438
00:28:26,600 --> 00:28:27,600
Like that's what your legs do.

439
00:28:27,600 --> 00:28:29,000
They kind of oscillate when you walk.

440
00:28:29,000 --> 00:28:32,760
Well, it could be that when you initially discover oscillation, you fall on your face.

441
00:28:32,760 --> 00:28:35,440
And so it actually looks like you're not improving.

442
00:28:35,440 --> 00:28:40,560
And so, but because your metric is basically how far did you go, it causes you to basically

443
00:28:40,560 --> 00:28:46,120
be blind to the underlying discovery that's actually essential to making the progress that

444
00:28:46,120 --> 00:28:48,320
you need to make in the long term.

445
00:28:48,320 --> 00:28:52,720
And this problem of deception is just like universal across all kinds of endeavors.

446
00:28:52,720 --> 00:28:54,440
Not just neurovolution.

447
00:28:54,440 --> 00:29:00,400
It's like, is this analogous to almost like a kind of a local maxima kind of issue?

448
00:29:00,400 --> 00:29:02,760
Yeah, I mean, it's basically the same thing.

449
00:29:02,760 --> 00:29:06,560
It's related to local maxima or local optima or premature convergence.

450
00:29:06,560 --> 00:29:09,680
Sometimes people would call it to getting stuck on a local optimum.

451
00:29:09,680 --> 00:29:14,600
But I think that the insight that we have that's different from just saying, okay, well,

452
00:29:14,600 --> 00:29:17,600
we just rediscovered local optimum because we already knew about local optima.

453
00:29:17,600 --> 00:29:18,600
Exactly.

454
00:29:18,600 --> 00:29:22,720
It's just how utterly profound the problem is.

455
00:29:22,720 --> 00:29:26,880
Then like you cannot just like, I mean, people think, well, there's ways of getting around

456
00:29:26,880 --> 00:29:27,880
local optimal.

457
00:29:27,880 --> 00:29:29,200
You know, I mean, you can do your tricks.

458
00:29:29,200 --> 00:29:30,200
We have diversity.

459
00:29:30,200 --> 00:29:31,200
We have randomness.

460
00:29:31,200 --> 00:29:32,200
Docasticity.

461
00:29:32,200 --> 00:29:35,120
There are things we can do to kind of jiggle things around a little so we don't just get

462
00:29:35,120 --> 00:29:38,480
stuck on a peak, which is what kind of we think of local Optimus like getting stuck on

463
00:29:38,480 --> 00:29:40,280
a peak in a big space.

464
00:29:40,280 --> 00:29:43,680
That like, that's just not going to cut it in certain types of problems because they

465
00:29:43,680 --> 00:29:48,600
are just so absolutely complex that almost no matter what you do, deception is going to

466
00:29:48,600 --> 00:29:49,800
kill you.

467
00:29:49,800 --> 00:29:54,240
And we showed this when we introduced this algorithm called novelty search that in some

468
00:29:54,240 --> 00:30:00,880
problems that it was like shockingly terrible with deception could do to you in these spaces.

469
00:30:00,880 --> 00:30:05,520
And what was profound was that we showed that in certain problems like this where deception

470
00:30:05,520 --> 00:30:06,760
is a really big problem.

471
00:30:06,760 --> 00:30:09,680
And I would claim that deception is a really big problem in like almost any interesting

472
00:30:09,680 --> 00:30:10,680
problem.

473
00:30:10,680 --> 00:30:13,920
And we kind of demonstrate that later if we want to get into it.

474
00:30:13,920 --> 00:30:18,400
But when it is a serious problem, then we showed that with this novelty search algorithm

475
00:30:18,400 --> 00:30:23,160
that we introduced, which was basically not trying to solve a problem, but rather it

476
00:30:23,160 --> 00:30:27,600
was just driven by selecting things that are more novel.

477
00:30:27,600 --> 00:30:31,520
So not things that are better, but just more novel, that this would actually be better

478
00:30:31,520 --> 00:30:36,720
at solving a problem that was deceptive than an algorithm that was actually explicitly

479
00:30:36,720 --> 00:30:39,520
being driven by selecting things that were better.

480
00:30:39,520 --> 00:30:44,840
So the lesson it showed is it can be better sometimes to not be trying to solve the problem

481
00:30:44,840 --> 00:30:48,680
than to actually try to solve the problem in terms of getting a better solution.

482
00:30:48,680 --> 00:30:53,800
And this obviously really counterintuitive in paradoxical and upsetting maybe even because

483
00:30:53,800 --> 00:30:58,280
it's like embarrassing in a way for anybody who's like saying, okay, I've got this really

484
00:30:58,280 --> 00:31:02,960
good optimization algorithm to lose to an algorithm doesn't even know what kind of problem

485
00:31:02,960 --> 00:31:04,280
it's trying to solve.

486
00:31:04,280 --> 00:31:05,960
And that's sort of what novelty search is.

487
00:31:05,960 --> 00:31:11,520
It's a divergent search algorithm, so basically it's just trying to find things that are different

488
00:31:11,520 --> 00:31:13,280
than what it's found before.

489
00:31:13,280 --> 00:31:18,840
It sounds a little bit like, you know, explore, exploit where you're explore is kind of optimizing

490
00:31:18,840 --> 00:31:19,840
for newness.

491
00:31:19,840 --> 00:31:26,080
Yeah, yeah, yeah, it is related to this kind of exploration, exploitation dichotomy that

492
00:31:26,080 --> 00:31:29,480
a lot of people talk about machine learning, but it's also different, I think.

493
00:31:29,480 --> 00:31:34,360
So like there's an additional element of insight here beyond that, which is really important,

494
00:31:34,360 --> 00:31:38,680
which is that when we think of exploitation versus exploration, like often we think of

495
00:31:38,680 --> 00:31:43,640
exploitation as following some gradient, which means information towards something that

496
00:31:43,640 --> 00:31:44,640
we are trying to get to.

497
00:31:44,640 --> 00:31:48,600
So in other words, we're using information to move in a direction that's intelligent.

498
00:31:48,600 --> 00:31:53,280
But interestingly, exploration we tend to think of as sort of random moves that are

499
00:31:53,280 --> 00:31:54,920
sort of ignoring the informed gradient.

500
00:31:54,920 --> 00:31:57,840
So it's like, let's just go somewhere and see what happens.

501
00:31:57,840 --> 00:31:59,280
And that's what we think of exploration.

502
00:31:59,280 --> 00:32:03,720
But what novelty search showed is that there is a principled kind of exploration that is

503
00:32:03,720 --> 00:32:09,160
not random, that actually exploration is something that's also very informed.

504
00:32:09,160 --> 00:32:12,640
And so in the novelty search case, you're informed by where you've been, because novelty

505
00:32:12,640 --> 00:32:16,320
is basically a comparison between where I am and where I've been before.

506
00:32:16,320 --> 00:32:18,200
So it's anything but random.

507
00:32:18,200 --> 00:32:20,200
It's a very informed gradient.

508
00:32:20,200 --> 00:32:24,880
It's just that it's the gradient of novelty instead of the gradient of the objective.

509
00:32:24,880 --> 00:32:28,200
And this is actually a very information rich gradient, because if you think about it, you

510
00:32:28,200 --> 00:32:29,480
know a lot about where you've been.

511
00:32:29,480 --> 00:32:33,160
In fact, you know more about where you've been than you know about where you're trying

512
00:32:33,160 --> 00:32:35,880
to go, because the whole problem with where you're trying to go is you don't know about

513
00:32:35,880 --> 00:32:36,880
it.

514
00:32:36,880 --> 00:32:38,240
Otherwise, you would just go there.

515
00:32:38,240 --> 00:32:42,920
So novelty is actually more informed, I'd say, than the objective gradient.

516
00:32:42,920 --> 00:32:47,960
And for this reason, it's an extremely interesting gradient to follow, like the gradient of novelty.

517
00:32:47,960 --> 00:32:50,800
Because you're being pushed away from where you've been before.

518
00:32:50,800 --> 00:32:54,960
And it turns out that you will be inevitably pushed towards higher complexity.

519
00:32:54,960 --> 00:32:58,120
So it's really tied into this idea of increasing complexity.

520
00:32:58,120 --> 00:33:01,520
Because if you think about it, as soon as you exhaust all the simple things you can do

521
00:33:01,520 --> 00:33:06,120
in the world, like the only choice you have if you want to continue to create novelty is

522
00:33:06,120 --> 00:33:08,200
to do something more complex.

523
00:33:08,200 --> 00:33:12,080
And so ultimately, there's an inevitability that like with novelty search that you're

524
00:33:12,080 --> 00:33:14,880
going to be pushed towards increasing complexity.

525
00:33:14,880 --> 00:33:18,960
So I think of it as almost like an information accumulator, like in order to continue to do

526
00:33:18,960 --> 00:33:23,320
novel things in the world, you have to accumulate information about the world.

527
00:33:23,320 --> 00:33:27,360
So for example, like you could imagine if you were trapped in a room and I told you

528
00:33:27,360 --> 00:33:31,840
like to just do novel stuff, like for a while you could just run around randomly and you'd

529
00:33:31,840 --> 00:33:34,720
like bump into walls and everything you do would be novel.

530
00:33:34,720 --> 00:33:37,440
But eventually you'd bump into all the walls in the room.

531
00:33:37,440 --> 00:33:39,840
And so at some point you're going to have to learn how to not bump into walls.

532
00:33:39,840 --> 00:33:42,560
And when you do that, you're going to have to learn what a wall is and how to sense

533
00:33:42,560 --> 00:33:44,520
a wall and how to navigate walls.

534
00:33:44,520 --> 00:33:46,840
And eventually you have to learn how to open a door because you have to get out of the

535
00:33:46,840 --> 00:33:48,200
room eventually to do something new.

536
00:33:48,200 --> 00:33:49,200
Right.

537
00:33:49,200 --> 00:33:51,720
And eventually you're going to have to get off planet earth and go to Mars.

538
00:33:51,720 --> 00:33:56,720
And clearly like doing that requires like learning extremely deep and complicated facets

539
00:33:56,720 --> 00:33:59,200
of how the universe works, like physics.

540
00:33:59,200 --> 00:34:03,640
And so you're going to be forced to become an expert on the domain where you find yourself

541
00:34:03,640 --> 00:34:06,760
if you're going to be pushed towards doing more and more novel things.

542
00:34:06,760 --> 00:34:10,360
And so knowledge actually is a very deep and interesting kind of a process.

543
00:34:10,360 --> 00:34:14,920
And that's why sometimes it alone will do better than actually trying to solve the problem

544
00:34:14,920 --> 00:34:16,520
you're trying to solve.

545
00:34:16,520 --> 00:34:19,440
If you think about like evolutionarily, like if you think about like how could we get

546
00:34:19,440 --> 00:34:24,560
to human intelligence from a single cell, it'd be crazy to do selection based on the

547
00:34:24,560 --> 00:34:27,080
intelligence of single celled organisms.

548
00:34:27,080 --> 00:34:30,680
Like we wouldn't start out by applying IQ tests to single celled organisms.

549
00:34:30,680 --> 00:34:32,480
That would just kill the population.

550
00:34:32,480 --> 00:34:35,480
I mean because none of them are intelligent at all.

551
00:34:35,480 --> 00:34:39,760
And so it's funny, but in a sense, the reason that we got to where we are today is because

552
00:34:39,760 --> 00:34:41,440
we were not trying to get there.

553
00:34:41,440 --> 00:34:44,680
Like if we had started out where selection was based on intelligence, then everything

554
00:34:44,680 --> 00:34:48,200
would have died or we would have gotten nowhere and we wouldn't have gotten to where we

555
00:34:48,200 --> 00:34:49,200
are today.

556
00:34:49,200 --> 00:34:51,520
So we see this issue of deception come up over and over again.

557
00:34:51,520 --> 00:34:56,960
Like it turns out that like there was a turning point long ago, eons ago, where symmetry,

558
00:34:56,960 --> 00:34:58,960
bilateral symmetry was discovered.

559
00:34:58,960 --> 00:35:00,240
These are our ancestors.

560
00:35:00,240 --> 00:35:03,400
There's these bilateral, these symmetric flatworms.

561
00:35:03,400 --> 00:35:07,080
There's no indication that they'd say anything to do with being more intelligent, but actually

562
00:35:07,080 --> 00:35:09,920
it does in some kind of like really, really long term sense.

563
00:35:09,920 --> 00:35:14,040
Like that was an important discovery that led ultimately, or stepping still in the leads

564
00:35:14,040 --> 00:35:17,640
ultimately to human level intelligence, but you wouldn't be able to predict that on

565
00:35:17,640 --> 00:35:20,080
the basis of doing an IQ test.

566
00:35:20,080 --> 00:35:22,080
And yet we needed to lock that in.

567
00:35:22,080 --> 00:35:25,600
So in some sense, we could recognize that was interesting from a novelty perspective because

568
00:35:25,600 --> 00:35:31,080
it was a very new innovation, but we cannot recognize it from a performance perspective

569
00:35:31,080 --> 00:35:36,160
because at that long, long ago point in time, it's not an indicator at all from the point

570
00:35:36,160 --> 00:35:40,120
of view of performance, like if the ultimate indicator is intelligence.

571
00:35:40,120 --> 00:35:44,520
And this is another kind of example of deception and why many things are not going to be possible

572
00:35:44,520 --> 00:35:49,120
to discover if we just set them as a goal and just select based on those things.

573
00:35:49,120 --> 00:35:52,720
And this is a principle not just for evolution, but for life too.

574
00:35:52,720 --> 00:35:57,120
You know, like there are many inventions that would not have been invented if they had

575
00:35:57,120 --> 00:36:00,880
been our goal to invent them, which is again the paradox coming up.

576
00:36:00,880 --> 00:36:04,960
Like computers, for example, were the first computer for based on vacuum tubes, but the

577
00:36:04,960 --> 00:36:08,400
people who invented vacuum tubes were not trying to invent computers.

578
00:36:08,400 --> 00:36:11,840
Like if you had gone back to the 1800s and told all the researchers working on vacuum

579
00:36:11,840 --> 00:36:17,000
tubes who were interested in electricity, that like actually there's something more interesting,

580
00:36:17,000 --> 00:36:19,400
like a computer, and maybe you should just invent that.

581
00:36:19,400 --> 00:36:21,760
Like forget this boring vacuum tube stuff.

582
00:36:21,760 --> 00:36:25,320
You would neither have vacuum tubes nor computers.

583
00:36:25,320 --> 00:36:30,720
So like once again, we needed people to be exploring very diverse ideas without having

584
00:36:30,720 --> 00:36:31,880
their eyes on the prize.

585
00:36:31,880 --> 00:36:35,840
If you think of the prize as like a computer, in order to eventually get the prize.

586
00:36:35,840 --> 00:36:37,640
And so there's a paradigm right there.

587
00:36:37,640 --> 00:36:43,200
And so this concept is so general and connected to this novelty search idea that we wrote this

588
00:36:43,200 --> 00:36:46,880
whole book about it called Why Greatness Cannot Be Plan.

589
00:36:46,880 --> 00:36:51,320
After a long time researching novelty search, and a long time for me talking in various

590
00:36:51,320 --> 00:36:56,280
forums and venues about novelty search, and I realized that like the principles are really

591
00:36:56,280 --> 00:37:01,440
general about this paradox, this is what I call the objective paradox, that like it's

592
00:37:01,440 --> 00:37:04,760
actually relevant to all society, like how we run our institutions.

593
00:37:04,760 --> 00:37:09,720
Like we give money to people based on them making progress with respect to an objective,

594
00:37:09,720 --> 00:37:12,960
like this is what granting agencies do, like in the sciences.

595
00:37:12,960 --> 00:37:15,160
And it's actually not principled in the long run.

596
00:37:15,160 --> 00:37:18,920
Like we have, there are other processes that need to be recognized and respected if we

597
00:37:18,920 --> 00:37:23,400
really want to be able to achieve really, really ambitious ends.

598
00:37:23,400 --> 00:37:28,320
And so that's why we wrote this book basically to introduce these principles of deception

599
00:37:28,320 --> 00:37:31,760
and divergent search and the objective paradox to the general public.

600
00:37:31,760 --> 00:37:36,120
We are hoping that maybe this would actually provoke a discussion of these things in a larger

601
00:37:36,120 --> 00:37:40,760
sense because of the fact that it affects many of the kind of attempts at innovation that

602
00:37:40,760 --> 00:37:43,000
we as a society are engaged in.

603
00:37:43,000 --> 00:37:47,960
So it turned out to have really broad implications across culture and society.

604
00:37:47,960 --> 00:37:48,960
Interesting.

605
00:37:48,960 --> 00:37:54,080
And then one of the papers that I noticed is one called Galactic Arms Race, is that an

606
00:37:54,080 --> 00:37:56,920
extension of this pork or is that a different direction?

607
00:37:56,920 --> 00:37:57,920
It's related.

608
00:37:57,920 --> 00:37:58,920
Yeah, it's related.

609
00:37:58,920 --> 00:38:04,840
So like we, as we started to understand this idea of, we call it sometimes divergent

610
00:38:04,840 --> 00:38:05,840
search.

611
00:38:05,840 --> 00:38:09,280
Like searches that are not aimed at a particular goal, but rather which are diverging

612
00:38:09,280 --> 00:38:11,120
through the space of what's possible.

613
00:38:11,120 --> 00:38:15,680
There are kind of searches that show you all the cool stuff that you could find, not

614
00:38:15,680 --> 00:38:16,680
just one thing.

615
00:38:16,680 --> 00:38:18,000
Evolution on Earth is kind of like that.

616
00:38:18,000 --> 00:38:21,960
It's not like one thing it's trying to do, it wasn't trying to get human level intelligence.

617
00:38:21,960 --> 00:38:25,880
It's kind of illuminated all of the possible cool stuff that's out there in nature, all

618
00:38:25,880 --> 00:38:28,000
of the diversity of nature.

619
00:38:28,000 --> 00:38:32,920
And so we started to realize these algorithms are really cool that do stuff like that, perhaps

620
00:38:32,920 --> 00:38:35,640
for applications in the real world.

621
00:38:35,640 --> 00:38:38,240
In Galactic Arms Race, the application is a video game.

622
00:38:38,240 --> 00:38:43,120
And our idea there was like maybe we could put one of these divergent search algorithms

623
00:38:43,120 --> 00:38:46,280
in a video game so it would generate the content in the game.

624
00:38:46,280 --> 00:38:49,960
And you'd get more and more cool content just like flowing into the game from nowhere.

625
00:38:49,960 --> 00:38:52,640
Like no human has to actually design or invent it.

626
00:38:52,640 --> 00:38:56,720
And in the case of Galactic Arms Race, it was the weapons of the ships that you fly.

627
00:38:56,720 --> 00:39:00,320
Like people are familiar in video games like with playing games where like you have to pick

628
00:39:00,320 --> 00:39:03,960
up new types of lasers or weapons or guns or something like that.

629
00:39:03,960 --> 00:39:04,960
Right.

630
00:39:04,960 --> 00:39:07,240
So we said let's let evolution invent the weapons.

631
00:39:07,240 --> 00:39:11,320
But with a kind of a novelty search like process where it's not like aiming for like

632
00:39:11,320 --> 00:39:15,360
the optimal weapon, it's just diverging through the space of weapons.

633
00:39:15,360 --> 00:39:18,640
But with some information about how humans are actually using them.

634
00:39:18,640 --> 00:39:22,120
So it's informed by the humans in the game and in real time inventing new weapons for

635
00:39:22,120 --> 00:39:23,840
the humans to try.

636
00:39:23,840 --> 00:39:28,000
And so there's an interaction called interactive evolution between what humans do and what

637
00:39:28,000 --> 00:39:29,480
evolution does.

638
00:39:29,480 --> 00:39:33,240
And it caused like all these cool weapons to be invented things that I don't have never

639
00:39:33,240 --> 00:39:36,680
seen in any other game that were just invented by the computer itself.

640
00:39:36,680 --> 00:39:40,920
And it's kind of I think a really nice exposition of like the potential of like divergent

641
00:39:40,920 --> 00:39:46,000
search or novelty like searches to create kind of open worlds where things are just continually

642
00:39:46,000 --> 00:39:47,000
generated.

643
00:39:47,000 --> 00:39:50,600
And sometimes we call this open-ended evolution that are interesting and hopefully without

644
00:39:50,600 --> 00:39:51,600
end.

645
00:39:51,600 --> 00:39:55,880
What's an example of a type of weapon that was invented in this game?

646
00:39:55,880 --> 00:39:56,880
Okay.

647
00:39:56,880 --> 00:39:57,880
Yeah.

648
00:39:57,880 --> 00:39:58,880
There's a couple good ones.

649
00:39:58,880 --> 00:40:01,160
So like one was I, so there's funny we started naming these things after the fact

650
00:40:01,160 --> 00:40:03,840
because they don't actually have names because they're invented by the computer.

651
00:40:03,840 --> 00:40:10,280
Like one we call the tunnel maker which would basically generate like two streams of particles,

652
00:40:10,280 --> 00:40:15,120
these are all particle weapons that would sort of like very slowly shoot on the left and

653
00:40:15,120 --> 00:40:16,440
right side of your spaceship.

654
00:40:16,440 --> 00:40:20,040
So basically it created a protective tunnel that you could fly through.

655
00:40:20,040 --> 00:40:24,160
And then in the middle of that tunnel there was another faster stream that was actually

656
00:40:24,160 --> 00:40:25,800
used for shooting things.

657
00:40:25,800 --> 00:40:29,160
So you would be creating basically like a shield that would like shoot out from your

658
00:40:29,160 --> 00:40:31,640
sides that you could then fly through.

659
00:40:31,640 --> 00:40:35,120
There was another one that we called a lasso which would just look like, it looked like

660
00:40:35,120 --> 00:40:39,120
a cowboy's lasso, you know, just like shot out and like created like this spiral around

661
00:40:39,120 --> 00:40:41,400
the enemy and then like closed in on it.

662
00:40:41,400 --> 00:40:44,440
And it was really surprising that this thing was invented and it was kind of interesting

663
00:40:44,440 --> 00:40:49,240
because I actually, it's not a great weapon in an objective sense like the lasso one because

664
00:40:49,240 --> 00:40:52,720
like I think it's much better probably just to shoot straight at something and kill it.

665
00:40:52,720 --> 00:40:55,600
But like the players loved it because of the aesthetics.

666
00:40:55,600 --> 00:40:59,520
It's just so interesting and fun like to have the lasso weapon and to kind of show off

667
00:40:59,520 --> 00:41:03,160
because it was a multiplayer game so people could see each other's lassoes that it became

668
00:41:03,160 --> 00:41:04,160
popular.

669
00:41:04,160 --> 00:41:06,800
And the game just kind of went with it, you know, the game didn't say this is objectively

670
00:41:06,800 --> 00:41:08,560
worse or objectively better.

671
00:41:08,560 --> 00:41:12,480
It just saw that people were interested in lassoes who created more lassoes and diverse

672
00:41:12,480 --> 00:41:16,840
lassoes and we had all these lasso weapons proliferate in the world because people liked

673
00:41:16,840 --> 00:41:22,600
them whether they're, you know, optimal and some objective sense or not.

674
00:41:22,600 --> 00:41:28,520
Is there an argument that says that the, you know, the, you know, issues around, you

675
00:41:28,520 --> 00:41:33,320
know that you identified in novelty search and, you know, getting led down the wrong path

676
00:41:33,320 --> 00:41:37,240
they example, I guess you gave us with, you know, a robot trying to learn how to walk

677
00:41:37,240 --> 00:41:42,640
and kind of, yeah, using a motion that kind of allows it that kind of doesn't lead it towards

678
00:41:42,640 --> 00:41:46,480
walking and eventually let's fall on its face.

679
00:41:46,480 --> 00:41:52,440
I guess the thought is are, you know, it can all of us be boiled down to just not being

680
00:41:52,440 --> 00:41:56,840
able to express enough sophistication in our objective function or not being able to

681
00:41:56,840 --> 00:42:02,240
express our objective function in the right time frame or something like that.

682
00:42:02,240 --> 00:42:03,240
Yeah.

683
00:42:03,240 --> 00:42:06,800
Actually, there's an element of truth to that view that like, yeah, like if we knew

684
00:42:06,800 --> 00:42:10,720
enough about the world, we could just write the objective function to take into account

685
00:42:10,720 --> 00:42:12,880
how the world actually works.

686
00:42:12,880 --> 00:42:18,280
But the problem is that like in practice, that's just impossible because like you ultimately

687
00:42:18,280 --> 00:42:21,520
would have to know every single thing about all the stepping stones that you would have

688
00:42:21,520 --> 00:42:24,320
to go through to write the objective function to take that into account.

689
00:42:24,320 --> 00:42:28,440
So it's like, say there's like, you know, a million steps between here and a human level

690
00:42:28,440 --> 00:42:29,440
AI.

691
00:42:29,440 --> 00:42:32,760
So it will obviously if I wrote a fitness function where your score is literally how far

692
00:42:32,760 --> 00:42:33,760
you are along that path.

693
00:42:33,760 --> 00:42:37,520
Then of course, this is like the ideal objective function is going to work out fine.

694
00:42:37,520 --> 00:42:40,600
But the whole point, the whole problem that we're facing just begs the question of how

695
00:42:40,600 --> 00:42:44,160
are we going to figure out what the stepping stones are so we're back to square one again.

696
00:42:44,160 --> 00:42:48,160
And so in practice, like you're probably not going to be able to do that in even like

697
00:42:48,160 --> 00:42:52,320
a relatively simple problem because the whole problem of searches we don't know the stepping

698
00:42:52,320 --> 00:42:56,520
stones, if we did, we wouldn't be doing search because we would just build the thing because

699
00:42:56,520 --> 00:42:59,280
we would know all the steps to get it right.

700
00:42:59,280 --> 00:43:04,040
So this paradox is basically unavoidable, you know, like if the problem's not interesting,

701
00:43:04,040 --> 00:43:06,360
then we do know the stepping stones that we don't need to do these things.

702
00:43:06,360 --> 00:43:07,760
But the problem's not interesting.

703
00:43:07,760 --> 00:43:11,440
But if the problem is interesting, it's interesting because we don't know the stepping stones.

704
00:43:11,440 --> 00:43:13,560
Like that's what makes it an interesting problem.

705
00:43:13,560 --> 00:43:17,800
And so almost any interesting problem is going to be confronting this paradox.

706
00:43:17,800 --> 00:43:20,840
Now that doesn't mean that there aren't some cases where search will work.

707
00:43:20,840 --> 00:43:24,240
Obviously it will with an objective sometimes, there's no doubt about it.

708
00:43:24,240 --> 00:43:28,320
In fact, deep learning has exposed that like in really high dimensional spaces between

709
00:43:28,320 --> 00:43:32,600
spaces of many, many parameters, like many weights in their own network, that there's

710
00:43:32,600 --> 00:43:34,520
less deception than we thought.

711
00:43:34,520 --> 00:43:36,880
Like and this has been a surprise for everybody including me.

712
00:43:36,880 --> 00:43:41,400
And so sometimes we still can just push sort of a brute force through the objective function

713
00:43:41,400 --> 00:43:45,920
because high dimensional spaces have some very odd properties and succeed at solving some

714
00:43:45,920 --> 00:43:46,920
problems.

715
00:43:46,920 --> 00:43:51,040
So we shouldn't conclude from what I'm saying that like all objectives are completely useless.

716
00:43:51,040 --> 00:43:53,160
They do work in some cases.

717
00:43:53,160 --> 00:43:57,360
But I think that it's still the case that in very, very complex problems, we are going

718
00:43:57,360 --> 00:43:59,120
to be facing deception.

719
00:43:59,120 --> 00:44:03,400
We are not going to know how to write the correct objective function to go through all

720
00:44:03,400 --> 00:44:07,760
those stepping stones, which are basically reflecting eons of progress to get to some

721
00:44:07,760 --> 00:44:09,640
of these really ambitious ends that we have.

722
00:44:09,640 --> 00:44:10,640
And so it's an element.

723
00:44:10,640 --> 00:44:13,960
It's not like everything should be done this way, but it's an ingredient that's added

724
00:44:13,960 --> 00:44:18,960
to our toolbox now, which is going to be important in concert with sometimes explicit

725
00:44:18,960 --> 00:44:19,960
objectives.

726
00:44:19,960 --> 00:44:22,320
And so it gives us kind of a powerful new tool.

727
00:44:22,320 --> 00:44:26,280
And this has actually led to a field called quality diversity where we combine quality

728
00:44:26,280 --> 00:44:30,960
measures with kind of diversity measures and try to do both at once in order to make

729
00:44:30,960 --> 00:44:35,680
a principle attempt to leverage what we know about both of those kinds of searches.

730
00:44:35,680 --> 00:44:36,680
Hmm.

731
00:44:36,680 --> 00:44:37,680
Super interesting stuff.

732
00:44:37,680 --> 00:44:42,800
Kenneth, I really appreciate you taking the time to speak with us about neural revolution

733
00:44:42,800 --> 00:44:43,800
and your research.

734
00:44:43,800 --> 00:44:46,480
Is there anything else that you'd like to leave us with?

735
00:44:46,480 --> 00:44:52,000
Well, I just, I guess just to say that take a look at neural evolution, like it's actually

736
00:44:52,000 --> 00:44:56,520
becoming now more recognized in deep learning that, you know, we have actually a lot of synergy

737
00:44:56,520 --> 00:44:59,440
with deep learning because we're also doing neural networks.

738
00:44:59,440 --> 00:45:04,040
And so both fields, I think, are realizing today that we have something to offer each other

739
00:45:04,040 --> 00:45:08,440
perhaps, you know, like a revolution can evolve architectures and deep learning can apply

740
00:45:08,440 --> 00:45:12,360
really powerful learning algorithms to those new complicated architectures for just

741
00:45:12,360 --> 00:45:13,360
as one example.

742
00:45:13,360 --> 00:45:18,280
And our evolution can contribute to reinforcement learning in new ways because of the way that

743
00:45:18,280 --> 00:45:23,200
fitness can be a different kind of driver of progress than, say, the typical gradient

744
00:45:23,200 --> 00:45:24,600
based approach.

745
00:45:24,600 --> 00:45:28,840
And so in the end, we get a possible really powerful synergy.

746
00:45:28,840 --> 00:45:33,280
And so I think it's worth looking at how these two things can possibly feed into each other

747
00:45:33,280 --> 00:45:34,280
going forward.

748
00:45:34,280 --> 00:45:35,280
Awesome.

749
00:45:35,280 --> 00:45:39,160
And what's the best way for folks to learn more about what you're doing?

750
00:45:39,160 --> 00:45:43,240
I'd point people to, I mean, I'm guessing you probably have some links associated with

751
00:45:43,240 --> 00:45:44,240
the energy.

752
00:45:44,240 --> 00:45:45,240
We can include a link.

753
00:45:45,240 --> 00:45:47,280
And I know you've got a page on the UCF site.

754
00:45:47,280 --> 00:45:48,280
Is that the best one?

755
00:45:48,280 --> 00:45:49,280
Yeah.

756
00:45:49,280 --> 00:45:53,560
I point people to my home page, my research group home page, both their UCF and also I

757
00:45:53,560 --> 00:45:58,480
can provide a link to Uber and I labs where we actually are hiring too.

758
00:45:58,480 --> 00:46:01,280
So people are just interested in jobs in general.

759
00:46:01,280 --> 00:46:02,520
That's another opportunity there.

760
00:46:02,520 --> 00:46:04,280
So I'll also point to that.

761
00:46:04,280 --> 00:46:05,280
Fantastic.

762
00:46:05,280 --> 00:46:06,280
Well, thanks so much, Kenneth.

763
00:46:06,280 --> 00:46:07,280
Yeah, thanks.

764
00:46:07,280 --> 00:46:08,280
It's been a pleasure.

765
00:46:08,280 --> 00:46:16,320
All right, everyone, that's our show for today.

766
00:46:16,320 --> 00:46:21,080
Thanks so much for listening and for your continued feedback and support.

767
00:46:21,080 --> 00:46:22,480
Thanks to your support.

768
00:46:22,480 --> 00:46:28,440
This podcast finished the year as a top 40 technology podcast on Apple podcasts.

769
00:46:28,440 --> 00:46:32,840
My producer says that one of his goals this year is to crack the top 10.

770
00:46:32,840 --> 00:46:37,280
And to do that, we need you to head over to your podcast app.

771
00:46:37,280 --> 00:46:38,280
Keep the show.

772
00:46:38,280 --> 00:46:42,800
Hopefully, we've earned your five stars and leave us a glowing review.

773
00:46:42,800 --> 00:46:48,280
And more importantly, share the podcast with your friends, family, co-workers, the Starbucks

774
00:46:48,280 --> 00:46:53,000
Barista, your Uber driver, everyone who might be interested.

775
00:46:53,000 --> 00:46:56,000
Every review, rating and share goes a long way.

776
00:46:56,000 --> 00:46:58,760
So thanks in advance.

777
00:46:58,760 --> 00:47:03,520
For more information on Kenneth or any of the topics covered in this episode, head on

778
00:47:03,520 --> 00:47:08,480
over to twimmolai.com slash talk slash 94.

779
00:47:08,480 --> 00:47:13,400
Of course, we would love to hear from you, either via a comment on the show notes page

780
00:47:13,400 --> 00:47:20,320
or via Twitter to at Sam Charrington or at Twimmolai or at Twimmolai.

781
00:47:20,320 --> 00:47:39,160
Thanks once again for listening and catch you next time.

