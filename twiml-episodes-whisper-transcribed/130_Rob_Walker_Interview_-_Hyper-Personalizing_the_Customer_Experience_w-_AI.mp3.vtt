WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.440
I'm your host Sam Charrington.

00:32.440 --> 00:37.800
In this episode I speak with Rob Walker, vice president of decision management and analytics

00:37.800 --> 00:44.600
at Pegasystems, a leading provider of software for customer engagement and operational excellence.

00:44.600 --> 00:49.160
Rob and I discussed what's required for enterprises to fully realize the vision of providing

00:49.160 --> 00:54.880
a hyper personalized customer experience and how machine learning and AI can be used

00:54.880 --> 01:00.600
to determine the next best action an organization should take to optimize sales, service, retention

01:00.600 --> 01:04.800
and risk at every step in the customer relationship.

01:04.800 --> 01:10.480
Along the way we dig into a couple of key areas, specifically some of the techniques his organization

01:10.480 --> 01:16.000
uses to allow customers to manage the trade-off between model performance and transparency,

01:16.000 --> 01:21.360
particularly in light of new laws like GDPR, and how all of this ties to an enterprise's

01:21.360 --> 01:26.160
ability to manage bias and ethical issues when deploying machine learning.

01:26.160 --> 01:30.200
We cover a lot of ground in this one and I think you'll find Rob's perspective really

01:30.200 --> 01:32.560
interesting.

01:32.560 --> 01:38.560
If you're looking for more enterprise perspectives on the latest in AI, robotics, customer engagement

01:38.560 --> 01:43.920
and digital process automation technology, you should check out Pegasystems upcoming

01:43.920 --> 01:52.160
conference Pegaworld, which will be held June 3rd to 6th at the MGM Grand in Las Vegas, Nevada.

01:52.160 --> 01:57.960
I'll be there, Rob will be there, and so will IT and experience leaders from organizations

01:57.960 --> 02:03.800
like Afflack, Anthem, Genworth Financial, JP Morgan Chase, and more.

02:03.800 --> 02:13.880
To learn more, visit Pegaworld, and as a Twymilistner, be sure to use the discount code PW18TWIML

02:13.880 --> 02:16.800
for $150 off of your registration.

02:16.800 --> 02:26.360
Thanks to Pegaworld for sponsoring this episode of the podcast, and now on to the show.

02:26.360 --> 02:29.600
All right everyone, I am on the line with Rob Walker.

02:29.600 --> 02:34.480
Rob is Vice President of Decision Management and Analytics with Pegasystems.

02:34.480 --> 02:36.600
Rob, welcome to the podcast.

02:36.600 --> 02:37.960
Thank you.

02:37.960 --> 02:43.040
It is great to have you on, as is the tradition here on the podcast, why don't we start out

02:43.040 --> 02:48.360
by having you tell us a little bit about your background and how you got involved in

02:48.360 --> 02:49.760
Analytics and AI?

02:49.760 --> 02:50.960
Yeah, thank you.

02:50.960 --> 02:51.960
Happy to do so.

02:51.960 --> 02:56.520
Yeah, I was really attracted to AI very early on.

02:56.520 --> 03:03.600
I mean, this is like, so in the 90s, when I was at the university, and I was doing my computer

03:03.600 --> 03:11.080
science studies, the specialization I chose was AI, which obviously is not quite the AI

03:11.080 --> 03:16.600
we have right now, but on the other hand, you know, much has actually remained the same,

03:16.600 --> 03:19.800
just a day die I think has changed a lot.

03:19.800 --> 03:27.000
But that was a really fun sort of specialization, and later I got my PhD in AI as well, specifically.

03:27.000 --> 03:31.040
And after that, I thought this was like, we're now talking like 2000 to approximately.

03:31.040 --> 03:35.640
I was really, you know, getting ready to do something in business.

03:35.640 --> 03:42.000
So I co-founded a company, company called KIQ, which was really specializing in what

03:42.000 --> 03:45.680
I would say is Applied AI, so that was early days.

03:45.680 --> 03:50.480
I was really like, apply predictive analytics, but at a very high scale around, you know, customer

03:50.480 --> 03:55.680
engagement specifically, but predicting things like credit, risk, or customer behavior

03:55.680 --> 04:01.480
in general, but at a very large scale, and then applying it in real time.

04:01.480 --> 04:06.840
And some facts about that was that we got the venture capital for that company, like moments

04:06.840 --> 04:09.920
before the 2000, you know, crash.

04:09.920 --> 04:13.760
So we just got the money in to start, so that was a really, really good timing.

04:13.760 --> 04:17.240
And then four years later, yeah, that was lucky.

04:17.240 --> 04:22.280
And AI wasn't as hot then as it is now, so that was double lucky.

04:22.280 --> 04:27.920
We got that going, sold it into some pretty large companies, and we're then acquired

04:27.920 --> 04:34.200
four years later by a company called Cordient, which is a Cupertino company next door

04:34.200 --> 04:40.000
neighbors with Apple, who really used that technology and AI around customer experience

04:40.000 --> 04:41.840
and personalization.

04:41.840 --> 04:45.760
And then Pegasystems acquired Cordient for that particular thing as well.

04:45.760 --> 04:49.880
So it's a long-winded thing, a long-winded way of saying that I've been in this space

04:49.880 --> 04:56.600
for a very long time, and it hasn't let many things distract me from using AI in a very

04:56.600 --> 05:00.800
practical way with sort of, you know, pretty large companies.

05:00.800 --> 05:02.800
And what's your focus at Pegas now?

05:02.800 --> 05:08.040
So at Pegas, it's really like, we have this thing called, you know, the customer brain

05:08.040 --> 05:13.200
or the customer decision hub, but it's a little bit more technical, and it's basically

05:13.200 --> 05:18.320
one of the core elements of everything we do, especially around customer engagement.

05:18.320 --> 05:25.880
So it's helping us hyper-personalize interactions in lots of different channels, very high volume

05:25.880 --> 05:31.000
kind of things, and so things digital presence on the website or mobile, but at the same

05:31.000 --> 05:38.600
time, you know, decide on next best actions for outbound communications, or if this is

05:38.600 --> 05:47.120
a bank in ATM, every single interaction we're trying to optimize, and that's not just AI,

05:47.120 --> 05:50.640
but AI is a very critical component of that.

05:50.640 --> 05:57.080
And are you optimizing it primarily from what perspective, from a financial perspective,

05:57.080 --> 06:03.560
or you mentioned customer experience earlier from a broader customer experience perspective?

06:03.560 --> 06:04.840
How do you think about that?

06:04.840 --> 06:09.200
Yeah, so the optimization function is basically critical.

06:09.200 --> 06:13.960
So I spent a lot of time, you know, talking to, especially the business people, but also

06:13.960 --> 06:19.920
with the data scientists in these large companies, so to decide on what their metric is, right?

06:19.920 --> 06:27.520
So we do next best action, and best, typically, is a combination of, like, future value, right?

06:27.520 --> 06:32.520
So it's not, we're, as best practice, we don't try to, for instance, if this is about,

06:32.520 --> 06:37.480
you know, a web interaction, it's not about, okay, what should we be selling you, or what

06:37.480 --> 06:38.480
should you be doing?

06:38.480 --> 06:44.640
It's really trying to optimize future value of the relationship, and that means that, yeah,

06:44.640 --> 06:49.720
there are products, maybe, that you're selling, but you can also try to nurture the relationship

06:49.720 --> 06:53.800
you can look at MPS scores that you're trying to optimize.

06:53.800 --> 07:00.640
So we're not specifically prescribing what the best metric is, but it's typically a proxy

07:00.640 --> 07:06.680
for future value of that relationship, that then every action is trying to optimize.

07:06.680 --> 07:10.640
Can you maybe walk us through a more concrete example of that?

07:10.640 --> 07:16.320
It doesn't have to be a specific customer, but if you've got a specific one in mind, just

07:16.320 --> 07:21.320
to put that kind of little process that you outlined for us into context?

07:21.320 --> 07:22.320
Sure.

07:22.320 --> 07:28.640
Yeah, let's take a large bank, for instance, and they've been on record, like Royal Bank

07:28.640 --> 07:35.240
of Scotland, or PNC Bank in the US, both of them, there are many other banks that are

07:35.240 --> 07:40.760
using this kind of stuff, but what they are doing is basically make sure that at every

07:40.760 --> 07:45.200
opportunity they have, when they engage in with a customer in any channel, so it can

07:45.200 --> 07:51.440
be sort of an old-fashioned email, or even more old-fashioned printed email, but typically

07:51.440 --> 07:58.280
obviously now the digital channels on the web, in the mobile, in the app everywhere, every

07:58.280 --> 08:06.040
single interaction, they're basically pinging the AI, which is then predicting all sorts

08:06.040 --> 08:15.080
of behaviors, like likelihood to turn, likelihood to, if this is a bank of getting into financial

08:15.080 --> 08:21.280
trouble, and maybe not repaying alone, obviously propensities around the whole host or product

08:21.280 --> 08:27.400
or product combinations that the bank would be selling them, and then a bunch of optimization

08:27.400 --> 08:34.240
algorithms, but also rules put in by humans to decide on, okay, if that's the case, then

08:34.240 --> 08:39.320
contextually, right now, for this interaction, and that maybe one interaction out of billions

08:39.320 --> 08:45.680
in a single day, this is what we should be doing, and that's sort of the process we're

08:45.680 --> 08:50.240
going through, and this could be, this is an offer, this is what we show on the website,

08:50.240 --> 08:56.080
this is a notification on your mobile phone, this is a prompt on the ATM terminal, any

08:56.080 --> 08:57.360
of those kind of things.

08:57.360 --> 09:07.720
I'm wondering what your take is on kind of where we are, I guess, as an industry in this

09:07.720 --> 09:14.200
process of applying AI and analytics to the customer experience, right?

09:14.200 --> 09:20.440
So calling it AI is new, but a lot of what you're describing, folks have tried to use predictive

09:20.440 --> 09:29.120
analytics for quite some time to optimize the next offer or the next step in managing

09:29.120 --> 09:30.120
a relationship.

09:30.120 --> 09:39.800
And personally, when I think about my relationships with large enterprises like banks, like telecom

09:39.800 --> 09:45.640
companies, I don't really get the sense that they're optimizing anything about that relationship.

09:45.640 --> 09:55.280
It feels very static, and I wonder what the barriers are to them realizing this kind of

09:55.280 --> 10:00.720
vision of a more personalized kind of relationship with me as their customer.

10:00.720 --> 10:06.680
Yeah, I think that, and that's obviously many, many do not, and there are, it's amazing

10:06.680 --> 10:13.160
given the benefits that I think we, but also other companies show, all of this is data

10:13.160 --> 10:14.160
driven.

10:14.160 --> 10:18.080
So it's not particularly hard to show sort of the delta on some of these, on some of

10:18.080 --> 10:19.080
these things.

10:19.080 --> 10:23.560
And the reason it's, and I have to say it's going fast, so I mean, we certainly don't complain

10:23.560 --> 10:28.720
about traction, but it's not going as fast as you would expect, because it's not how

10:28.720 --> 10:33.960
these large organizations are organized, and this is nothing to do with the technology,

10:33.960 --> 10:38.720
although I'll touch on that in a second as well, but it's also just culturally, you know,

10:38.720 --> 10:44.800
people in these big banks or big communications companies or other companies are incentivized

10:44.800 --> 10:46.720
on a specific part of the business, right?

10:46.720 --> 10:51.320
So if you're talking to a bank, in your example, you know, the people that want to sell

10:51.320 --> 10:56.400
you credit cards do their thing, typically pretty old-fashioned as well.

10:56.400 --> 10:59.800
And then the people that want to sell you mortgages do their thing.

10:59.800 --> 11:02.960
And then there are, you know, ten other lines of businesses that do their thing, and it

11:02.960 --> 11:07.920
all comes together, you know, at the point of contact, but it's not a concerted effort.

11:07.920 --> 11:11.120
It's not optimized for the bank or for you.

11:11.120 --> 11:18.600
It's basically trying to make sense of all sort of suboptimal, you know, goals.

11:18.600 --> 11:24.840
That's why it feels incoherent and inconsistent, and typically then all the different channels

11:24.840 --> 11:28.400
are not connected to this one brain in the middle either, right?

11:28.400 --> 11:35.360
So anyway, there's a lot of ways to make mistakes or to, you know, get suboptimal results

11:35.360 --> 11:37.120
as you are experiencing.

11:37.120 --> 11:44.880
So what are you seeing as the main ways that companies are trying to dismantle these silos

11:44.880 --> 11:47.920
and create better experiences?

11:47.920 --> 11:51.960
Yeah, well, obviously they're not, you know, these big companies are not charity.

11:51.960 --> 11:55.600
So they, I think the big move is, well, there's twofold.

11:55.600 --> 11:59.680
One is they actually have a lot of data, right?

11:59.680 --> 12:03.760
I mean, obviously there's a whole big data discussion, but they have a lot of it now.

12:03.760 --> 12:05.120
And it's in much better quality.

12:05.120 --> 12:09.160
So if I rewind even like ten years, you know, when everybody was building, you know,

12:09.160 --> 12:14.360
data warehouses and things like that, we're in an incomparably better state on data.

12:14.360 --> 12:20.280
And also the awareness of that, you know, that these big companies have that data is,

12:20.280 --> 12:23.240
it has grown, has grown tremendously.

12:23.240 --> 12:30.200
And then the question is, can they basically, based on what AI can, can do to optimize,

12:30.200 --> 12:32.560
you know, these, these interactions?

12:32.560 --> 12:39.120
Do they believe that, you know, there's a tangible result and we're talking about a lot

12:39.120 --> 12:40.120
of money, right?

12:40.120 --> 12:44.480
And that, I think, is the biggest reason for a lot of these big companies pivoting towards

12:44.480 --> 12:49.920
this because it's so easy to show once you get it going, what the benefits are, right?

12:49.920 --> 12:56.560
Both in customer satisfaction and in, you know, bottom line benefits that it's, it becomes

12:56.560 --> 13:01.640
pretty hard to, you know, neglect that kind of progress.

13:01.640 --> 13:07.320
Can you talk a little bit about the process that company might take to identify, you know,

13:07.320 --> 13:10.120
where it's biggest opportunities lie?

13:10.120 --> 13:16.520
I imagine for a lot of these companies, there are tons of ways that they could apply analytics

13:16.520 --> 13:24.520
and AI to create better outcomes and prioritizing them becomes a bit of a challenge in and

13:24.520 --> 13:25.520
of itself.

13:25.520 --> 13:26.520
Yeah.

13:26.520 --> 13:27.520
Not that that's true.

13:27.520 --> 13:29.200
And that becomes a huge, huge challenge.

13:29.200 --> 13:35.600
What we typically do is to, and this is much easier than it was, like not that low

13:35.600 --> 13:40.800
ago because of all the references and these tangible results.

13:40.800 --> 13:45.120
So I have a lot easier conversations than I had before.

13:45.120 --> 13:50.200
But one of the things that I think radically changes, but also really brings the need of

13:50.200 --> 13:56.200
AI to the forefront is that if you become, like, really customer centric and you have

13:56.200 --> 14:00.160
these one-to-one relationships, the whole model is changing, right?

14:00.160 --> 14:05.320
I mean, a traditional bank or large company would have marketing campaigns where obviously

14:05.320 --> 14:10.880
data scientists would create their predictive models and do all their magic and create

14:10.880 --> 14:13.560
segments and do targeting.

14:13.560 --> 14:19.360
But that's all based on sending out a message for a particular product or offer, right?

14:19.360 --> 14:25.840
If you actually put the customer at the center and now you have AI figuring out all of their

14:25.840 --> 14:30.720
behaviors that are relevant, you know, from a risk perspective, retention perspective,

14:30.720 --> 14:38.800
surface perspective, as well as sales, now you have maybe a thousand different predictive

14:38.800 --> 14:44.080
models and you need to optimize with all of those outputs as inputs.

14:44.080 --> 14:49.920
Now, that's the optimization challenge that, on the one hand, gives you a lot better

14:49.920 --> 14:55.200
customer satisfaction because it's, the relevance will go through the roof and so will

14:55.200 --> 14:57.000
bottom line benefits.

14:57.000 --> 15:01.520
But it means that a lot of companies or a lot of people in these companies now need to

15:01.520 --> 15:05.120
collaborate where they didn't have to before, right?

15:05.120 --> 15:09.160
Because the markets people would do one thing, the cars people would do a different thing.

15:09.160 --> 15:15.040
Now it's all about the customer and AI has now a big vote in what should be done or very

15:15.040 --> 15:21.080
strong opinions about, you know, recommendations that it makes about what the next best action

15:21.080 --> 15:22.080
would be.

15:22.080 --> 15:28.440
So it sounds like the first step is really at least for the companies that you're working

15:28.440 --> 15:32.200
with and the approach that you're taking with them.

15:32.200 --> 15:41.400
The first step is really centralizing, centralizing is maybe not the right word, but kind of establishing

15:41.400 --> 15:46.840
this customer-centric perspective, which is kind of strange for me to hear in some sense

15:46.840 --> 15:51.640
because like, you know, one-to-one marketing, 360 degree marketing, these kinds of, these

15:51.640 --> 15:57.560
are 20 plus year old terms and we just have tools and approaches that we can apply to

15:57.560 --> 15:58.720
them now.

15:58.720 --> 16:03.280
But it's a little surprising that it's still so hard to get companies to buy into this

16:03.280 --> 16:04.680
as an idea.

16:04.680 --> 16:05.680
Yeah.

16:05.680 --> 16:12.080
Well, partly it's still cultural, but partly also, honestly, that is because you know, one-to-one

16:12.080 --> 16:19.160
marketing or one-to-one interactions may be, you know, not the latest idea, I'm sure that,

16:19.160 --> 16:23.080
you know, we heard that maybe in the 90s or something like that.

16:23.080 --> 16:27.600
But to actually make it real, you do need AI.

16:27.600 --> 16:32.560
So I think that is really the breakthrough of the last 10 years, right, that you can have

16:32.560 --> 16:38.960
the self-learning models, adaptive models, all sorts of AI algorithms that these big companies

16:38.960 --> 16:45.000
can trust to turn through their data, come up with propensities, learn on the fly, execute

16:45.000 --> 16:48.960
contextually, and make them a lot of money.

16:48.960 --> 16:55.520
I mean, that's not a given, right, and one approach even now is to, for instance, say,

16:55.520 --> 17:01.240
let's ring fans just 1% of your customers, right, and let's see, you know, the difference

17:01.240 --> 17:06.520
of this new approach and contrast it with the control group of 99%, just to make sure

17:06.520 --> 17:12.080
that you can trust it, and you know, that's when you see the benefits, and they're not

17:12.080 --> 17:17.800
like marginal benefits, so you know, that sort of propels the project forward.

17:17.800 --> 17:25.280
So one of the other topics that I hear come up a lot in the context of enterprise applications

17:25.280 --> 17:31.080
of AI as the whole idea of explainability or transparency.

17:31.080 --> 17:39.200
This is coming to the fore, you know, a bit more of a pointed fashion in Europe in the context

17:39.200 --> 17:44.920
of GDPR, and I'm wondering if this is something that you run into with the companies that

17:44.920 --> 17:46.720
you're talking to?

17:46.720 --> 17:50.360
Yes, and that's very important.

17:50.360 --> 17:55.920
So we have GDPR sort of on the European side, which is many things, but one of the things

17:55.920 --> 18:03.280
is that, you know, you need to be able to explain any kind of AI or algorithm that is making

18:03.280 --> 18:07.840
meaningful decisions, so that's obviously a lot of wiggle space around what is meaningful,

18:07.840 --> 18:16.120
but clearly, you know, you can imagine that this is about, you know, alone or some big

18:16.120 --> 18:20.680
financial stake or risk calculations, that is a meaningful decision.

18:20.680 --> 18:28.760
So that's GDPR, that's one thing, but equally outside Europe as well, using AI as a black

18:28.760 --> 18:33.200
box for these, especially for the larger companies, these large enterprise companies is just

18:33.200 --> 18:34.200
too high a risk, right?

18:34.200 --> 18:42.400
So they have a whole compliance process around AI and explainability is very important.

18:42.400 --> 18:47.800
And for instance, one approach that we're taking really at the core of the technology

18:47.800 --> 18:51.680
is to give controls that are not naive about this, right?

18:51.680 --> 18:57.560
You can obviously say, let's insist on transparent AI, which obviously, you know, as everybody

18:57.560 --> 19:00.680
will understand clearly, will come at the cost, right?

19:00.680 --> 19:04.960
I mean, transparency because it's a cost.

19:04.960 --> 19:09.680
It means that the algorithm cannot be as effective as it could otherwise be, so you can't

19:09.680 --> 19:16.160
just switch it off and say, oh, you can only do regression models or whatever it is.

19:16.160 --> 19:22.040
So we believe that at the core of these AI systems, there needs to be a control that allows

19:22.040 --> 19:25.680
much more granular control around transparency.

19:25.680 --> 19:34.120
Say, for instance, for risk or more regulatory areas, you may really insist on transparent

19:34.120 --> 19:36.640
models and we'll define that in a moment.

19:36.640 --> 19:42.840
Whereas maybe in marketing or in image recognition, you are, you know, you can go all out with

19:42.840 --> 19:48.360
your, you know, opaque algorithms like deep learning or, you know, XG boost models or

19:48.360 --> 19:53.680
assemble models, all of these things that are, you know, become much more intractable.

19:53.680 --> 20:00.600
So that's a discussion we have a lot around AI and I think the trust in AI is really important

20:00.600 --> 20:03.080
to make this whole thing fly.

20:03.080 --> 20:05.840
And you mentioned that you would define transparency.

20:05.840 --> 20:10.160
How do you articulate that to customers?

20:10.160 --> 20:14.880
Yeah, so there are multiple ways, obviously, the naive way of this is basically tagging

20:14.880 --> 20:15.880
the algorithm.

20:15.880 --> 20:23.520
I mean, they're obviously, you know, like deep learning algorithms are notoriously opaque.

20:23.520 --> 20:30.240
But more technically, what we do is basically look at the model output and then reverse engineering

20:30.240 --> 20:35.800
it with transparent methods like, for instance, decision trees and then the complexity of

20:35.800 --> 20:41.640
a decision tree that is trying to reverse engineer the model then becomes the size of that

20:41.640 --> 20:48.200
decision tree then becomes a proxy for transparency or opacity really.

20:48.200 --> 20:50.000
Can you elaborate on that a bit?

20:50.000 --> 20:59.000
So you're training arbitrary models independent of kind of this transparency versus opacity

20:59.000 --> 21:04.200
metric and then you're like fitting a decision tree to them and depending on the complexity

21:04.200 --> 21:09.200
of the resulting decision tree, that tells you whether the models should be classified

21:09.200 --> 21:11.200
as transparent versus opaque.

21:11.200 --> 21:12.200
Did I get that right?

21:12.200 --> 21:14.600
Yeah, that's basically, that's basically correct.

21:14.600 --> 21:20.080
So obviously we don't apply to any, you know, area, but especially around customer engagement.

21:20.080 --> 21:25.000
So what we're doing, we're getting all of these predictions of behavior from neural

21:25.000 --> 21:30.640
mass or genetic algorithms or, you know, more transparent algorithms like decision trees

21:30.640 --> 21:36.800
or regression models, whatever the case may be, some may be built by data scientists,

21:36.800 --> 21:44.760
some may be built automatically by the AI, but basically once we get all of these scores

21:44.760 --> 21:50.960
or all of these propensities, we are done going to fit it, as you said, with a transparent

21:50.960 --> 21:58.040
algorithm and that in itself will give us a good view of, you know, how opaque it

21:58.040 --> 21:59.040
is.

21:59.040 --> 22:07.800
So it sounds like underneath or kind of embedded in this process is some, I don't know

22:07.800 --> 22:14.200
if you met an optimizer or some model, some auto ML, I guess, is one way to describe

22:14.200 --> 22:18.120
it that is, you've got to, you've got a problem.

22:18.120 --> 22:24.840
You're testing a bunch of different models or training a bunch of different models on

22:24.840 --> 22:30.000
this data and you coming up with parameterized models.

22:30.000 --> 22:35.160
And each of those, you know, on the one hand, you know, you've got some model that's the

22:35.160 --> 22:36.160
best model.

22:36.160 --> 22:40.920
It may be a model that's not natively transparent.

22:40.920 --> 22:47.000
And if that's the case, then you walk back from that model using this process of training

22:47.000 --> 22:53.280
a decision tree on it to get like the, you know, the closest transparent model to that

22:53.280 --> 22:54.280
model.

22:54.280 --> 22:55.280
Yeah.

22:55.280 --> 23:00.880
That's one way of, there are actually multiple approaches.

23:00.880 --> 23:02.320
So that's one way of doing it.

23:02.320 --> 23:05.600
And basically, you say, this is the closest transparent model.

23:05.600 --> 23:12.320
The other way of doing it is saying, well, whenever these two models agree on a particular

23:12.320 --> 23:16.560
prediction or whatever classification or whatever it is, if they agree, we'll just take

23:16.560 --> 23:22.960
the transparent model and then we only worry about the 5% of cases where they disagree.

23:22.960 --> 23:26.160
And then we make a choice based on the area we're in.

23:26.160 --> 23:30.720
So if we're in a regulatory area where we have to explain this, maybe GDPR, maybe it's

23:30.720 --> 23:37.960
our own compliance office as in these large companies will then actually go either all

23:37.960 --> 23:43.560
the way with the transparent model or that's where we switch to, you know, a more opaque

23:43.560 --> 23:50.040
model and take the risk that we don't completely understand how it works, but only for a small

23:50.040 --> 23:52.520
percentage of the, of the data points.

23:52.520 --> 23:58.120
There are a lot of activities, both on a research side as well as in companies that are using

23:58.120 --> 24:08.040
AI to try to create explainability out of otherwise opaque algorithms through the, kind

24:08.040 --> 24:11.760
of the architecture of those algorithms, are you involved in that as well?

24:11.760 --> 24:12.760
Yes.

24:12.760 --> 24:17.120
So we're trying to, and honestly, we're also going to see sort of where the market goes,

24:17.120 --> 24:18.120
right?

24:18.120 --> 24:24.280
But to our own technology in this case, so we currently look at like, you know, reverse

24:24.280 --> 24:33.680
engineering, these basically these scoring bins with decision tree algorithm as a good proxy.

24:33.680 --> 24:40.040
But and maybe that's good enough, but there are other algorithms that, you know, specifically

24:40.040 --> 24:45.640
are trying to reverse engineer these models and if they work better, we'll use those.

24:45.640 --> 24:52.600
So I think the point for us as sort of an enterprise AI vendor for around customer engagement

24:52.600 --> 24:56.560
is that we need to give these companies the control over this.

24:56.560 --> 25:00.960
Otherwise, they can't use it to the, to the largest extent.

25:00.960 --> 25:04.800
And that would be, you know, not a good thing for their customers and obviously for these

25:04.800 --> 25:05.800
organizations.

25:05.800 --> 25:06.800
Yeah.

25:06.800 --> 25:10.520
I guess that's an interesting topic in and of itself, kind of the, there's a, seems

25:10.520 --> 25:18.760
to be a broader debate around even asking this question, like even, you know, insisting

25:18.760 --> 25:26.520
on transparency or explainability for some of these algorithms, there seems to be at

25:26.520 --> 25:36.760
least in the Twitter echo chamber, a vocal group of folks that object to the idea of requiring

25:36.760 --> 25:45.120
transparency for algorithms, you know, in, in some sense, because, you know, I guess the

25:45.120 --> 25:52.360
argument is that it limits innovation, it provides unreasonable constraints, the example

25:52.360 --> 25:55.840
that you hear a lot is, hey, we don't know how the brain works, but we make decisions

25:55.840 --> 25:57.720
all the time and we're comfortable with that.

25:57.720 --> 26:01.280
You know, I'm wondering what your perspective is on kind of that broader argument.

26:01.280 --> 26:02.280
Yeah.

26:02.280 --> 26:07.800
So I think that's a fascinating debate and suddenly the scientist in me is, is definitely

26:07.800 --> 26:09.480
with that school.

26:09.480 --> 26:16.040
It's like, it's an, it's an, it's a constraint on AI that, you know, as a rule, if that would

26:16.040 --> 26:20.920
actually be, if we would legislate that away, you know, the opaque algorithms, I think

26:20.920 --> 26:26.160
that would be a bad thing, also not sustainable, by the way, because I think it's not just

26:26.160 --> 26:31.400
about business benefits and, and, and, and if we go, you know, maybe zoom in on this

26:31.400 --> 26:33.640
a little bit because I do think this is fascinating.

26:33.640 --> 26:40.120
I think a lot of people don't seem to realize or don't want to realize that even opaque

26:40.120 --> 26:45.760
AI, like the human brain perhaps, you know, if it makes better decisions, that's not always

26:45.760 --> 26:47.720
a good thing just for companies, right?

26:47.720 --> 26:52.240
It's also probably a good thing for, for humans, right?

26:52.240 --> 26:56.880
It may be that you are not getting the loan that you should otherwise get or it may be

26:56.880 --> 27:02.760
that you get diagnosed with something before a human can, and those are good things.

27:02.760 --> 27:10.320
So I'm also off to school that we should not, you know, rule out, opaque algorithms.

27:10.320 --> 27:14.920
I do, however, believe that certainly in the face where we're now, where, you know,

27:14.920 --> 27:21.240
the greater population needs to gain trust in these kind of things that we need to control

27:21.240 --> 27:26.760
mechanism that allows you to allow opaque algorithm where you are comfortable, and even then

27:26.760 --> 27:31.960
you need all sorts of bias tests and ethical tests at the end, but also I'll be able to

27:31.960 --> 27:38.400
restrict it with the same control algorithm or control mechanism in areas that are very

27:38.400 --> 27:39.400
heavily regulated.

27:39.400 --> 27:44.040
Otherwise, we would never see AI there.

27:44.040 --> 27:47.080
You mentioned bias and ethical test.

27:47.080 --> 27:51.000
This is something that I've been writing about a bit in my newsletter of late.

27:51.000 --> 27:55.080
I'm wondering what work you've been doing in that area.

27:55.080 --> 27:56.080
Yeah.

27:56.080 --> 28:00.720
So we have been, and I think you can tell from the emphasis on this, you know, on this

28:00.720 --> 28:06.200
called control mechanism, we really believe that for AI to be trusted and accepted and

28:06.200 --> 28:11.280
have, you know, large scale adoption, we do need these kind of control mechanisms.

28:11.280 --> 28:17.480
At the same time, even with these control mechanisms, we have the thing, so we call it revision

28:17.480 --> 28:23.240
management, which is the overall sort of police on any of these customer strategies that

28:23.240 --> 28:24.840
you put in place, right?

28:24.840 --> 28:30.160
It's not just about AI, it's also about performance, it's about all sorts of things.

28:30.160 --> 28:38.040
Because in our case, the decision layer is used at such a scale interacting with, you

28:38.040 --> 28:42.160
know, as I said, billions of interactions that you really need to get it right.

28:42.160 --> 28:47.760
So we have this sort of revision manager that makes sure that can, you know, that all of

28:47.760 --> 28:51.080
them QA is done.

28:51.080 --> 28:57.960
And at that level, we believe the ethical test and bias tests need to be formally incorporated.

28:57.960 --> 29:02.840
So you don't have just a business officer signing off on the business benefits and an IT

29:02.840 --> 29:07.000
manager signing off on the SLAs and all these kind of things.

29:07.000 --> 29:12.120
You should probably also have an ethical officer saying, listen, this is showing a bias

29:12.120 --> 29:17.640
that it goes beyond what we as a brand think is acceptable.

29:17.640 --> 29:23.320
And, you know, I think you should do that across the board, but especially if you allow

29:23.320 --> 29:28.240
some of the more opaque algorithms, which I think you should in some areas, I think without

29:28.240 --> 29:34.640
these tests, and not just as a kind of an afterthought, but part of your whole QA process,

29:34.640 --> 29:38.400
I think is critical for large companies.

29:38.400 --> 29:44.000
Because some of these new technologies and approaches get popularized, one of the things

29:44.000 --> 29:52.920
you start to see is new job titles emerge that kind of tell you where enterprises are going.

29:52.920 --> 29:58.120
So I remember when chief security officer and chief data officer came about, and now

29:58.120 --> 30:01.720
people are talking about, you know, whether there should be a chief AI officer, do you

30:01.720 --> 30:07.960
see many enterprises now that have a chief ethics officer or do you see that as something

30:07.960 --> 30:12.160
that's emerging how far away from something like that, or where does it have been lived

30:12.160 --> 30:14.920
within, you know, today's enterprise?

30:14.920 --> 30:20.200
Yeah, I think I haven't seen it tightly yet, but I think it's probably now with the

30:20.200 --> 30:25.480
compliance officer or maybe the risk officer, but probably the compliance officer.

30:25.480 --> 30:32.240
But I think we will definitely see that, especially when customers are interacting with AI

30:32.240 --> 30:33.640
more directly, right?

30:33.640 --> 30:38.920
And a lot of what I'm talking about, they don't necessarily see, right?

30:38.920 --> 30:43.160
They may see a personalized page, but you don't know the magic behind that, right?

30:43.160 --> 30:48.440
But when we see people talking to, you know, cognitive agents and chatbots, and it's

30:48.440 --> 30:53.840
very obvious that they are AI, I think people will become much more sensitive to that.

30:53.840 --> 31:01.240
And I can imagine, yeah, a chief AI officer or ethics officer is something we will see.

31:01.240 --> 31:09.720
But definitely when we do these large implementations in this revision process, an ethical sign-off

31:09.720 --> 31:11.600
is part of the process.

31:11.600 --> 31:16.760
And I can ignore it, but you know, that's what you should be putting in.

31:16.760 --> 31:26.240
And my sense is that the kind of science around making that ethical sign-off, you know,

31:26.240 --> 31:32.760
isn't very well developed at this point, how are companies managing that?

31:32.760 --> 31:35.200
How do they know when they should be signing off?

31:35.200 --> 31:43.440
What kind of process are they taking to determine whether an algorithm is, you know, ethically

31:43.440 --> 31:44.440
acceptable?

31:44.440 --> 31:48.840
Yeah, so I think that's currently, first of all, that's in its infancy, right?

31:48.840 --> 31:56.920
And what we are prescribing is basically looking at, like, you know, distributions of outcomes

31:56.920 --> 32:03.120
and then comparing it with, you know, the process, the more transparent process or the

32:03.120 --> 32:05.400
human process.

32:05.400 --> 32:10.680
But that only works, for instance, for instance, let's assume that your AI is used by a big

32:10.680 --> 32:12.920
bank and it's turning racist, right?

32:12.920 --> 32:14.520
Can easily happen.

32:14.520 --> 32:19.840
And obviously, and this is a discussion, I think, where we're now having a lot, with all

32:19.840 --> 32:25.080
the big data and also the efficiency of the AI, obviously race, you know, doesn't have

32:25.080 --> 32:31.120
to be, you know, in your data or gender or age for it, obviously, to, you know, to use

32:31.120 --> 32:32.120
these concepts.

32:32.120 --> 32:34.160
So anyway, that's, I think that's a given for this audience.

32:34.160 --> 32:40.040
But just to be precise, is that if you have an opaque algorithm and you have big data,

32:40.040 --> 32:45.760
there is no way that you can tell from the, you know, the algorithm that it is, you know,

32:45.760 --> 32:49.320
it is racist or has some other, you know, weirdness.

32:49.320 --> 32:54.800
The thing is, you can only test that if you have a test for it.

32:54.800 --> 33:00.080
And that may mean if you really don't have some, and I'm just making a brace as an example.

33:00.080 --> 33:05.040
If you don't actually carry that in your database, you cannot, you know, test the distribution,

33:05.040 --> 33:12.480
get a loan, see if it's materially or statistically significantly different from your desired policy,

33:12.480 --> 33:16.120
which should be, obviously, completely, politically correct.

33:16.120 --> 33:23.000
So that presents a little bit of a catch 22 and that I imagine organizations have strived,

33:23.000 --> 33:28.320
have strived to keep those criteria out of their databases or is that not the case?

33:28.320 --> 33:29.320
No.

33:29.320 --> 33:30.320
So yes.

33:30.320 --> 33:34.000
So for some of these things like, like, you know, age and gender, they probably have

33:34.000 --> 33:37.160
it for other purposes, so you can do it for other things.

33:37.160 --> 33:41.200
And this could be sexual orientation or political affiliation, all of these things.

33:41.200 --> 33:48.360
You probably will need a panel, you know, so you actually have a specific group of representative

33:48.360 --> 33:51.480
group of customers that you do ask for these attributes.

33:51.480 --> 33:55.560
So you can check your bias and your algorithm.

33:55.560 --> 34:00.480
But I haven't, I haven't seen that as part of a formal process yet.

34:00.480 --> 34:01.480
Okay.

34:01.480 --> 34:02.480
Okay.

34:02.480 --> 34:12.840
So you mentioned, I forgot the terms you use, but you in describing the process for kind

34:12.840 --> 34:16.800
of putting the final stamp of approval on a model.

34:16.800 --> 34:21.200
You talked about some of the testing steps and, you know, all this brings to mind the

34:21.200 --> 34:28.520
broader context of scaling and operationalizing AI within the enterprise.

34:28.520 --> 34:36.080
What are some of the things that you're seeing with regards to putting these models into

34:36.080 --> 34:42.040
production and how do they correlate to, are we learning things from, you know, other,

34:42.040 --> 34:47.480
you know, scaling and operationalizing processes like DevOps on the application development

34:47.480 --> 34:52.520
side or, you know, how how evolved and mature is this process now?

34:52.520 --> 34:53.520
Yeah.

34:53.520 --> 34:58.280
I think this is sort of more the area of like decision management, right?

34:58.280 --> 35:05.440
So it's this model execution layer that I think is getting is getting pretty mature, right?

35:05.440 --> 35:06.760
But I think that it's an important thing.

35:06.760 --> 35:12.720
If I look at the breakthroughs in AI, I'm not even thinking so much about the algorithmic

35:12.720 --> 35:13.720
sort of progression.

35:13.720 --> 35:18.720
And obviously we have seen, you know, very cool new algorithms, but, you know, a lot of

35:18.720 --> 35:20.360
it is still is still the same.

35:20.360 --> 35:25.000
I think the two things that have changed the most are the data that's available, both

35:25.000 --> 35:30.320
to learn, but also to apply the AI to in production and the speed, right?

35:30.320 --> 35:35.760
So we're not talking about taking like one predictive model into production, even if

35:35.760 --> 35:40.960
it's a complex predictive model, I'm talking about thousands of them, right?

35:40.960 --> 35:47.600
Thousands of them combined with all sorts of financial rules and cutoffs and thresholds

35:47.600 --> 35:54.720
and inventory and constraints, all of these things together are used multiple times in

35:54.720 --> 36:00.800
a single interaction to continuously predict or decide on this next best action.

36:00.800 --> 36:02.920
That's that's a big scale.

36:02.920 --> 36:09.360
If you're a company, the size of sprint or one of the one of the big banks, like Citibank

36:09.360 --> 36:16.520
or Royal Bank of Scotland, that's like such a volume of interaction to apply a thousand

36:16.520 --> 36:21.400
predictive models, thousands of rules, and then every single time something changes in

36:21.400 --> 36:26.320
the context, like a customer says, yes or no, or maybe or this is too expensive or clicks

36:26.320 --> 36:29.680
or doesn't click, and then you recalculate it.

36:29.680 --> 36:36.200
That was just completely impossible just maybe five, six years ago.

36:36.200 --> 36:43.840
And that's I think also makes AI and control over AI so important right now.

36:43.840 --> 36:51.080
And so are you are you seeing companies do specific things to try to wrangle, you know,

36:51.080 --> 36:54.920
all of these models that they have in production, is that something that your tools are doing

36:54.920 --> 36:59.040
or you're seeing them using open source tools, what do you think the landscape looks

36:59.040 --> 37:00.040
like?

37:00.040 --> 37:04.680
Now, well, what I'm seeing is like I think the open source tools still specifically tend

37:04.680 --> 37:09.560
to, you know, the mostly I think the data scientist community, right?

37:09.560 --> 37:14.880
So they're obviously getting very good at, you know, getting you all sorts of algorithms.

37:14.880 --> 37:19.600
And I think this honestly, especially on the big cloud platforms will become much more

37:19.600 --> 37:24.160
of a commodity, right, where they will just have, you know, APIs around predictive modeling

37:24.160 --> 37:26.160
and all sorts of things.

37:26.160 --> 37:34.360
I think where it gets a lot more complex and challenging is to have a company in which

37:34.360 --> 37:40.120
the business people themselves are trained enough and confident enough to build these big

37:40.120 --> 37:42.440
customer strategies looking at evidence, right?

37:42.440 --> 37:44.480
And this is a very big pivot.

37:44.480 --> 37:47.720
So this brings us all the way back to this one to one, right?

37:47.720 --> 37:52.880
If you figure out for one person and you have to execute a thousand propensity models

37:52.880 --> 37:56.440
and then all of your rules and constraints and all these kind of things to defigure

37:56.440 --> 37:58.560
out what the next best action is.

37:58.560 --> 38:03.000
And then the customer says, yeah, but I think it should be a hundred dollars less.

38:03.000 --> 38:04.440
I'm just making this up.

38:04.440 --> 38:09.520
And we recalculate if that's acceptable at that scale, when you have like 50 million

38:09.520 --> 38:15.520
customers using all of these different channels, that I think is the challenge.

38:15.520 --> 38:20.800
And I think there's no, I haven't seen any open source thing dealing with that.

38:20.800 --> 38:28.960
I think where we're looking at, for instance, is basically abstracting from whatever tool

38:28.960 --> 38:32.760
or set of tools are giving us the predictive insight because we think that will

38:32.760 --> 38:35.040
become a commodity, right?

38:35.040 --> 38:39.480
We complement it with lots of very high-scale self-learning stuff, but I think in the end

38:39.480 --> 38:44.760
that becomes a commodity and it's building decisions out of that and make that transparent

38:44.760 --> 38:49.640
and do that at the scale so that these are huge companies can use it in every single

38:49.640 --> 38:51.160
interaction channel.

38:51.160 --> 38:54.080
I think that's not easy.

38:54.080 --> 38:57.840
And I don't think open source will get us there quickly.

38:57.840 --> 39:02.720
So just to make sure I understand your perspective on that, the part that you're abstracting

39:02.720 --> 39:09.680
away at the level that you're thinking about it is the underlying, kind of modeling,

39:09.680 --> 39:13.960
model building and perhaps even the models themselves.

39:13.960 --> 39:22.560
And then the part where you mentioned the part that you're focused on is the decisioning

39:22.560 --> 39:23.560
piece.

39:23.560 --> 39:29.280
But there, I guess it strikes me that there is a middle piece which is kind of the scaling

39:29.280 --> 39:31.320
and operationalization.

39:31.320 --> 39:35.160
Does that exist today, is that something that we're still as an industry building and

39:35.160 --> 39:40.240
figuring out in your perspective or where do you see that evolving?

39:40.240 --> 39:42.360
No, I think we're there.

39:42.360 --> 39:47.280
Certainly we are doing that for the large companies, but we are consuming and this is one of

39:47.280 --> 39:48.280
the challenges, right?

39:48.280 --> 39:56.400
We are consuming these predictive models if somebody loves R or one of the modeling tools.

39:56.400 --> 39:57.400
We would consume that.

39:57.400 --> 40:02.920
There's an exchange format called PMML that you can use to sort of help standardize that.

40:02.920 --> 40:07.640
The thing is, if you want to have AI sort of at the forefront of customer engagement,

40:07.640 --> 40:12.000
you need to be able to execute these models contextually, contextually.

40:12.000 --> 40:20.160
So we're no longer, you know, in an area or a time where we would have big, bad jobs

40:20.160 --> 40:25.080
and all of these models would put scores in databases and then we'd retrieve the scores.

40:25.080 --> 40:30.480
So now we know the likelihood you're going to buy this is 0.8 and the risk is 0.2 because

40:30.480 --> 40:35.440
we need to execute them contextually like we do in a really human conversation.

40:35.440 --> 40:41.080
So the models need to be executed contextually and that, I think, is much more of the technical

40:41.080 --> 40:46.720
challenge than getting the models in the first place, although at the moment we still feel

40:46.720 --> 40:51.600
like there's a lot of model laboratories out there, right?

40:51.600 --> 40:55.480
These tools where data scientists can have 100 different algorithms to build the better

40:55.480 --> 40:58.400
they want, which is cool and we will consume that.

40:58.400 --> 41:03.520
But a model factory to actually give the business, you know, the throughputs and the volume

41:03.520 --> 41:08.640
of models that they need, probably not there yet, but I think that will become a commodity.

41:08.640 --> 41:09.640
Awesome.

41:09.640 --> 41:10.640
Awesome.

41:10.640 --> 41:16.040
Well Rob, to wrap up, do you have any final thoughts that you'd like to share with the audience?

41:16.040 --> 41:18.280
I think we touched on a lot.

41:18.280 --> 41:22.240
We've got a lot of ground.

41:22.240 --> 41:23.640
All right.

41:23.640 --> 41:24.640
Awesome.

41:24.640 --> 41:27.040
Well, it's been great getting to chat with you about this stuff.

41:27.040 --> 41:32.600
There is certainly a lot of interesting things happening as enterprises wrap their arms

41:32.600 --> 41:38.600
around AI, but also there's, you know, they've, you know, while AI is kind of the hot new

41:38.600 --> 41:42.960
term, there's, you know, they've certainly been working on analytics, decision management,

41:42.960 --> 41:47.280
predictive stuff for quite a while and it's interesting to see how these things co-evolve

41:47.280 --> 41:48.280
together.

41:48.280 --> 41:51.640
So thank you for taking some time to share your perspective with us.

41:51.640 --> 41:52.640
You're very welcome.

41:52.640 --> 42:17.000
Thank you.

48:52.640 --> 49:09.640
Thanks again to Rob and Pegasystems for sponsoring this show.

49:09.640 --> 49:14.760
And of course, make sure you head on over to pegaworld.com to learn more about the conference

49:14.760 --> 49:21.400
and be sure to use the code PW1820 for 150 off of registration.

49:21.400 --> 49:24.200
Thanks so much for listening and catch you next time.

