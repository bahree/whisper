WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.760
I'm your host Sam Charrington.

00:23.760 --> 00:28.320
This week on the podcast, we're featuring a series of conversations from the Nips conference

00:28.320 --> 00:30.560
in Long Beach, California.

00:30.560 --> 00:34.520
This was my first time at Nips and I had a great time there.

00:34.520 --> 00:37.720
I attended a bunch of talks and of course learned a ton.

00:37.720 --> 00:43.920
I organized an impromptu round table on building AI products and I met a bunch of wonderful

00:43.920 --> 00:48.040
people including some former Twimble Talk guests.

00:48.040 --> 00:52.860
I'll be sharing a bit more about my experiences at Nips via my newsletter, which you should

00:52.860 --> 00:59.540
take a second right now to subscribe to at twimblei.com slash newsletter.

00:59.540 --> 01:05.220
This week through the end of the year, we're running a special listener appreciation contest

01:05.220 --> 01:09.940
to celebrate hitting one million listens on the podcast and to thank you all for being

01:09.940 --> 01:11.900
so awesome.

01:11.900 --> 01:16.580
Tweet to us using the hashtag Twimble1Mill to enter.

01:16.580 --> 01:21.220
Everyone who enters is a winner and we're giving away a bunch of cool Twimble swag and

01:21.220 --> 01:23.220
other mystery prizes.

01:23.220 --> 01:30.020
If you're not on Twitter or want more ways to enter, visit twimblei.com slash twimble1Mill

01:30.020 --> 01:32.220
for the full rundown.

01:32.220 --> 01:36.820
Before we dive in, I'd like to thank our friends over at Intel Nirvana for their sponsorship

01:36.820 --> 01:40.020
of this podcast and our Nips series.

01:40.020 --> 01:45.060
While Intel was very active at Nips with a bunch of workshops, demonstrations and poster

01:45.060 --> 01:50.780
sessions, their big news this time was the first public viewing of the Intel Nirvana

01:50.780 --> 01:54.580
neural network processor or NNP.

01:54.580 --> 01:59.540
The goal of the NNP architecture is to provide the flexibility needed to support deep learning

01:59.540 --> 02:04.620
primitives while making the core hardware components as efficient as possible, giving

02:04.620 --> 02:10.100
neural network designers powerful tools for solving larger and more difficult problems

02:10.100 --> 02:14.740
while minimizing data movement and maximizing data reuse.

02:14.740 --> 02:20.580
To learn more about Intel's AI products group and the Intel Nirvana NNP, visit Intel

02:20.580 --> 02:22.940
Nirvana.com.

02:22.940 --> 02:28.540
In this episode, I sit down with Timnit Gebrew, postdoctoral researcher at Microsoft Research

02:28.540 --> 02:34.540
in the Fairness Accountability Transparency and Ethics in AI or Fate group.

02:34.540 --> 02:38.580
I've been following Timnit's work for a while now and was really excited to get a chance

02:38.580 --> 02:40.820
to sit down with her at the conference.

02:40.820 --> 02:46.460
We packed a ton into this conversation, especially keying in on her recently released paper,

02:46.460 --> 02:50.780
including deep learning and Google Street View to estimate the demographic makeup of the

02:50.780 --> 02:52.580
US.

02:52.580 --> 02:56.580
Timnit describes the pipeline she developed for this research and some of the challenges

02:56.580 --> 03:02.020
she faced building an end-to-end model based on Google Street View images, census data,

03:02.020 --> 03:04.260
and commercial car vendor data.

03:04.260 --> 03:08.940
We also discussed the role of social awareness in her work, including an explanation of how

03:08.940 --> 03:14.540
domain adaptation and fairness are related and her view on the major research directions

03:14.540 --> 03:16.820
in the domain of fairness.

03:16.820 --> 03:21.940
Timnit is also one of the organizers behind the Black and AI group, which held a very interesting

03:21.940 --> 03:24.740
symposium and poster session at Nips.

03:24.740 --> 03:27.740
I'll link to the group's page in the show notes.

03:27.740 --> 03:38.580
This was a really interesting conversation and one that I'm sure you'll enjoy.

03:38.580 --> 03:40.260
Timnit, welcome to the podcast.

03:40.260 --> 03:41.260
Thank you.

03:41.260 --> 03:42.260
Thanks for having me.

03:42.260 --> 03:43.260
Absolutely.

03:43.260 --> 03:45.780
What is the fate group at Microsoft Research?

03:45.780 --> 03:52.100
Fates stands for Fairness Accountability Transparency and Ethics and AI and it's a very new group

03:52.100 --> 03:57.700
started by Kate Crawford and Hannah Wallach and there's some other people there like Jen

03:57.700 --> 04:03.380
Warman, Vaughan, and some economists and some computational social scientists, so it's

04:03.380 --> 04:08.100
a combination of machine learning people and social science and economics people.

04:08.100 --> 04:12.860
Try to study the societal implications of AI and just make sure that we create algorithms

04:12.860 --> 04:16.140
that are fair, so our research is focused towards that.

04:16.140 --> 04:17.140
Oh wow.

04:17.140 --> 04:22.100
And how did you get interested in fairness and AI in particular and artificial intelligence

04:22.100 --> 04:23.620
in general?

04:23.620 --> 04:27.820
My background is in computer vision and as I was working on, there's a number of things

04:27.820 --> 04:33.020
I've always been interested in social justice and towards the end of my PhD, I saw this

04:33.020 --> 04:41.500
purplica article about a software that was being used by judges to figure out a person's

04:41.500 --> 04:46.940
likelihood of committing a crime again and judges where this was one of the inputs that

04:46.940 --> 04:52.780
they used to figure out how many years they should sentence you to prison.

04:52.780 --> 04:55.140
This being some machine learning algorithm.

04:55.140 --> 04:58.820
That's being sold by North Point and I think it's the name of this startup and that was

04:58.820 --> 05:04.380
very terrifying for me knowing because I had the background to know what kind of biases

05:04.380 --> 05:08.540
we have in the criminal justice system already and what kind of biases we have and how much

05:08.540 --> 05:12.180
discrimination there is in the data, that would be trained for it.

05:12.180 --> 05:19.500
So that was one and then while I was working on my PhD, I kind of figured that my work,

05:19.500 --> 05:24.100
my own work, could be susceptible to this kind of bias as well because my whole work was

05:24.100 --> 05:28.540
trying to show that we can do data mining using images, so large scale computer vision

05:28.540 --> 05:33.680
plus data, you know, like most people use text and social networks and other kinds of

05:33.680 --> 05:38.340
textual data to do data mining and the whole point of my PhD was to show that we could

05:38.340 --> 05:44.540
gain useful societal information using images and so if the ground truth for that that you

05:44.540 --> 05:48.820
use to train is biased, you're going to have, you know, biased conclusions.

05:48.820 --> 05:54.260
So I thought that I should be very cognizant of what kinds of issues could exist with

05:54.260 --> 05:58.260
that type of work, given that my work lies in that type of work.

05:58.260 --> 05:59.260
Okay.

05:59.260 --> 06:02.060
And now you're relatively new at Microsoft, is that right?

06:02.060 --> 06:05.380
I am, yeah, I started in July, yeah, I'm very new.

06:05.380 --> 06:06.380
Awesome.

06:06.380 --> 06:08.380
And you just published your first paper with the group?

06:08.380 --> 06:09.380
Is that right?

06:09.380 --> 06:10.380
No, no, no.

06:10.380 --> 06:13.620
So the my paper that just came out in PNAS is actually from my PhD.

06:13.620 --> 06:14.620
Oh, really?

06:14.620 --> 06:16.620
So this is a project that took four years.

06:16.620 --> 06:17.620
Wow.

06:17.620 --> 06:21.700
So I think, you know, it's more, when I give talks about it, I think people understand

06:21.700 --> 06:25.180
the level, the amount of work it went into it, but it's harder to see a thing from just

06:25.180 --> 06:28.900
from that one paper, but yeah, like that paper took a very long time and it just got

06:28.900 --> 06:29.900
published.

06:29.900 --> 06:30.900
Wow.

06:30.900 --> 06:31.900
So tell us about that paper.

06:31.900 --> 06:36.980
The paper was using Google Street View images to predict demographic characteristics.

06:36.980 --> 06:42.660
So what we did was we detected and classified cars, all the cars and 15 million Google

06:42.660 --> 06:45.820
Street View images across 200 American cities.

06:45.820 --> 06:51.700
And then we were able to use the characteristics of the cars that we detected and classified,

06:51.700 --> 06:57.300
you know, in a particular zip code or precinct and associated that with certain demographic

06:57.300 --> 07:02.940
characteristics like income or political affiliation, like an earlier paper we had even we

07:02.940 --> 07:08.740
did, like we looked at income segregation levels and even like CO2 emission rates.

07:08.740 --> 07:13.780
So we then, you know, once we detected and classified the cars, we represented each

07:13.780 --> 07:18.700
geographic region, like basically for us it would be like a zip code or a precinct,

07:18.700 --> 07:20.860
like a, you know, a voting precinct.

07:20.860 --> 07:26.860
We represented by like the type, the features that of the cars that are in that zip code.

07:26.860 --> 07:31.340
So for example, the percentage of Hondas or the percentage of each make that you have,

07:31.340 --> 07:32.340
that's one feature, right?

07:32.340 --> 07:37.820
Like percentage of triodas or Hondas or like Nisans or whatever percentage of sedans,

07:37.820 --> 07:42.860
you know, other metadata like the average miles per gallon, like efficiency of that particular

07:42.860 --> 07:44.420
zip code, et cetera.

07:44.420 --> 07:49.540
So once we had that information, then we used round truth census data or other data depending

07:49.540 --> 07:53.660
on what we're trying to predict to train another model to go from the car features to

07:53.660 --> 07:55.820
like predicting demographic characteristics.

07:55.820 --> 07:56.820
Okay.

07:56.820 --> 07:57.820
That's the project.

07:57.820 --> 07:58.820
Interesting.

07:58.820 --> 07:59.820
Interesting.

07:59.820 --> 08:03.140
So you mentioned census data as part of your training data set.

08:03.140 --> 08:08.420
When I think of the kinds of things that you're trying to predict for, like the, you know,

08:08.420 --> 08:14.100
the wealth or income of a geographic area, that's already in the census data.

08:14.100 --> 08:17.140
So how would someone use this technique?

08:17.140 --> 08:21.340
Oh, so what we did is, so we had 200 cities in our data set, right?

08:21.340 --> 08:22.340
Okay.

08:22.340 --> 08:25.500
So we used a subset of our cities for training.

08:25.500 --> 08:26.500
Okay.

08:26.500 --> 08:32.220
So we assume we have census data for like, let's say, I think is it 13 percent of, you

08:32.220 --> 08:36.380
know, or we sweep it to see, like, how much training data we need, but like, we assume

08:36.380 --> 08:42.220
we have census data for a small, very small subset of the cities that we have.

08:42.220 --> 08:47.100
And then we train a model using, and then for the rest of the cities, we don't have census

08:47.100 --> 08:48.100
data.

08:48.100 --> 08:49.100
We only have images and cars.

08:49.100 --> 08:53.980
So I guess the way we, someone would use it is if you have census data for some cities

08:53.980 --> 08:59.980
and you want to try to see, like, you know, for other cities, what the data might be, you

08:59.980 --> 09:04.260
could train our model using the census data that you have, and then the cars that are

09:04.260 --> 09:05.780
detected in the other cities.

09:05.780 --> 09:06.780
Okay.

09:06.780 --> 09:07.780
Right?

09:07.780 --> 09:08.940
So that's how we used it.

09:08.940 --> 09:13.020
And, you know, we also did some experiments and trying to do this across time.

09:13.020 --> 09:17.580
So say you have data for past census data for New York.

09:17.580 --> 09:20.460
Like we had it using Google Street View time lapse data.

09:20.460 --> 09:23.380
So for New York, and you have a bunch of images as well.

09:23.380 --> 09:27.300
And you then try to predict what's going to happen in the future, or, you know, like before

09:27.300 --> 09:28.820
you have the census data.

09:28.820 --> 09:30.660
So we did some experiments like that as well.

09:30.660 --> 09:36.060
So, you know, for me, this is kind of like, I don't want people to, like, read too much

09:36.060 --> 09:41.100
into the cars, you know, think for me, it's a proof of concept, basically, like, a new

09:41.100 --> 09:47.300
tool that you can use to do this kind of analysis, like, demography or social science applications

09:47.300 --> 09:51.380
or work, you know, it's like a new tool that is available to researchers, and we want

09:51.380 --> 09:56.100
to show, if one were, you know, say you wanted to study the, I don't know, relationship

09:56.100 --> 09:59.900
between trees or tree species and people's health or something, you know, how would you

09:59.900 --> 10:02.300
go about using images to do that?

10:02.300 --> 10:06.420
Well, now that you saw our paper, you could, like, apply a very similar pipeline to it,

10:06.420 --> 10:07.420
you know?

10:07.420 --> 10:08.420
Got it.

10:08.420 --> 10:10.860
So that's more what I want to take away to be.

10:10.860 --> 10:15.540
It's not specifically about cars, you know, projecting income based on cars.

10:15.540 --> 10:20.580
It's more about, we've got all of this visual data from cameras and sensors and things

10:20.580 --> 10:21.580
like that.

10:21.580 --> 10:25.700
How can we use that as proxy for any other thing that we might want to do?

10:25.700 --> 10:30.020
And for us, like cars, you know, there's, you know, like, there are other things you

10:30.020 --> 10:34.500
could study where the only data you could probably have is probably only visual data,

10:34.500 --> 10:35.500
right?

10:35.500 --> 10:39.140
Like, so for cars, you can argue that that's not necessarily the case you can use DMV

10:39.140 --> 10:40.140
data, right?

10:40.140 --> 10:44.620
But I guess our street view gives you a different perspective, which is you're not looking

10:44.620 --> 10:47.540
at the cars of the people who necessarily live there.

10:47.540 --> 10:52.300
You're just saying, if I were to just walk around the street, you know, what does that street

10:52.300 --> 10:53.300
look like, right?

10:53.300 --> 10:54.500
Like, what kind of cars are driving?

10:54.500 --> 10:56.260
What kind of cars are parked?

10:56.260 --> 10:59.340
That's kind of, and then what does that tell me about the people who live there?

10:59.340 --> 11:00.340
Right.

11:00.340 --> 11:05.100
For me, I'm most excited about the tool, you know, and this pipeline, I'm not, you know,

11:05.100 --> 11:10.020
and I was very, very surprised that our thing actually worked, you know, because there's

11:10.020 --> 11:11.020
a lot of stuff.

11:11.020 --> 11:12.020
I was very surprised.

11:12.020 --> 11:16.860
There's a lot of stuff that could go wrong because the pipeline, there's many, many different

11:16.860 --> 11:17.860
components of it.

11:17.860 --> 11:19.700
Well, can you walk us through the pipeline?

11:19.700 --> 11:20.700
Yeah.

11:20.700 --> 11:25.260
So the biggest thing is a lot of people in AI don't talk about this, but it's data collection

11:25.260 --> 11:26.260
is huge.

11:26.260 --> 11:31.580
So if you want to do a sort of supervised machine learning, and you want to do it in the

11:31.580 --> 11:35.540
real world, right, we're not like talking about a toy data set here.

11:35.540 --> 11:37.820
So we were saying, what is our end goal?

11:37.820 --> 11:42.540
Our end goal is to detect and classify all the cars and 50 million Google's review images

11:42.540 --> 11:45.500
and then to predict, demograph, is using that, right?

11:45.500 --> 11:50.740
So to get to that end goal, we first have to figure out, oh, okay, like how are we going

11:50.740 --> 11:52.540
to, okay, how are we going to get data?

11:52.540 --> 11:54.140
How are we going to get label data?

11:54.140 --> 11:57.780
Like how are we going to label cars and Google's review images?

11:57.780 --> 11:58.780
This is very hard.

11:58.780 --> 12:02.660
What we've been getting the data, is that easily accessible via an API, or did you have

12:02.660 --> 12:03.660
to scrape them?

12:03.660 --> 12:04.660
Google images?

12:04.660 --> 12:05.660
Yeah, they have an API.

12:05.660 --> 12:06.660
Okay.

12:06.660 --> 12:07.660
And these are publicly available images.

12:07.660 --> 12:08.660
Okay.

12:08.660 --> 12:11.980
But so then, you know, we had to be like, okay, so what are all the different types of

12:11.980 --> 12:15.580
cars around, and how we might see Google's review images?

12:15.580 --> 12:16.900
Where do we get that list?

12:16.900 --> 12:17.900
Right.

12:17.900 --> 12:22.620
So we found Edmunds.com and they have all the cars since 1990, so it's about 15,000 types

12:22.620 --> 12:23.620
of cars.

12:23.620 --> 12:24.620
But guess what?

12:24.620 --> 12:30.620
A computer vision algorithm can only, you know, we can only really kind of classify cars

12:30.620 --> 12:33.100
based on what they look on the outside.

12:33.100 --> 12:37.900
And these, you know, a subset of these 15,000 cars look the same, because they don't, they

12:37.900 --> 12:41.820
don't change them, you know, from year to year or from trim to trim or whatever.

12:41.820 --> 12:45.780
So we had to figure out how to cluster the cars that look the same.

12:45.780 --> 12:49.380
So we had a paper on this, it was a CHI paper, it was more of an HCI work.

12:49.380 --> 12:55.740
How do we get our initial subset of classes, where we bucket like cars that look the same

12:55.740 --> 12:57.380
into one class?

12:57.380 --> 13:02.740
And then that, that, that process in and of itself took a few months, by the way, because

13:02.740 --> 13:06.660
you know, you try something, it doesn't work, try something else, you know, so we did

13:06.660 --> 13:07.660
that.

13:07.660 --> 13:08.660
And so this is unsupervised.

13:08.660 --> 13:11.100
You're just clustering the cars that you're seeing in the images.

13:11.100 --> 13:12.100
Yeah.

13:12.100 --> 13:13.100
So we show.

13:13.100 --> 13:18.540
And you could say it's, yeah, so, so what we do is we show, we use Amazon Mechanical Turk

13:18.540 --> 13:20.900
and we show people, it's a graph-based algorithm.

13:20.900 --> 13:23.780
Like we show people images, two images of cars.

13:23.780 --> 13:27.100
So we have example images of cars from Edmonds.com.

13:27.100 --> 13:28.100
Okay.

13:28.100 --> 13:30.660
And so we show people, we say, are these two cars the same or different?

13:30.660 --> 13:34.420
And like we have, we have one of them is from Street View and the others from Edmonds.

13:34.420 --> 13:35.940
No, no, this is all from Edmonds.

13:35.940 --> 13:41.020
So right now, we're not even, okay, we haven't even gotten to getting labeled data right

13:41.020 --> 13:42.020
now.

13:42.020 --> 13:43.020
Right.

13:43.020 --> 13:44.020
We're trying to define what our classes are.

13:44.020 --> 13:45.020
Okay.

13:45.020 --> 13:46.540
What does class one mean?

13:46.540 --> 13:53.340
Class one means 2,500, chord 2,600, chord 2,700, you know, all of them look the same.

13:53.340 --> 13:57.460
So we haven't even started getting data, we're just starting to define what our classes

13:57.460 --> 13:58.460
are.

13:58.460 --> 13:59.460
Okay.

13:59.460 --> 14:00.460
That already takes a lot of time.

14:00.460 --> 14:01.940
That's the first thing you got to do.

14:01.940 --> 14:02.940
Right.

14:02.940 --> 14:06.500
The second one, once you define what your classes are, then for each of those classes,

14:06.500 --> 14:10.980
you have to have labeled data to train your car detector, right?

14:10.980 --> 14:14.500
So that's where we need the experts for Google Street View images.

14:14.500 --> 14:19.820
We also scrape data from like e-commerce sites, like cars.com and Craigslist.com.

14:19.820 --> 14:22.660
But you know, this is why dopamine adaptation exists.

14:22.660 --> 14:29.220
If you just, you know, train a plane like CNN or some sort of supervised machine learning

14:29.220 --> 14:34.980
algorithm on things that look like, you know, cars from Craigslist and try to test it

14:34.980 --> 14:38.820
on cars, you know, try to like detect cars in Google Street View or classify cars in

14:38.820 --> 14:40.540
Google Street View, it's not going to work.

14:40.540 --> 14:42.820
The distribution looks very, very different.

14:42.820 --> 14:49.100
So then there is a whole like, then I had an ICCV paper where we did like, we had, it's

14:49.100 --> 14:50.900
a domain adaptation based paper.

14:50.900 --> 14:51.900
Okay.

14:51.900 --> 14:55.540
So I think actually this data set is a very good domain adaptation data set.

14:55.540 --> 14:59.460
And why don't we do like an in set here on domain adaptation?

14:59.460 --> 15:01.980
What's the 32nd overview of domain adaptation?

15:01.980 --> 15:05.740
So domain adaptation is a subset of what people call transfer learning.

15:05.740 --> 15:10.300
So domain adaptation is like, you have one task, one, one exact task.

15:10.300 --> 15:13.060
And we have something that we call a source domain and a target domain.

15:13.060 --> 15:19.100
So in our example, let's say the source domain is cars from e-commerce sites, Craigslist.com.

15:19.100 --> 15:22.100
And let's say the target domain is cars from like Google Street View images.

15:22.100 --> 15:23.100
Okay.

15:23.100 --> 15:27.860
And so what you try to do in domain adaptation is you assume that you have labeled data in

15:27.860 --> 15:29.340
the source domain.

15:29.340 --> 15:33.900
But in the target domain, you know, in unsupervised adaptation, you assume you have no labeled

15:33.900 --> 15:34.900
data.

15:34.900 --> 15:39.340
So in unsupervised, we would assume that we don't have any images in Google Street View

15:39.340 --> 15:42.300
that are labeled with the types of cars they contain.

15:42.300 --> 15:43.300
That's unsupervised.

15:43.300 --> 15:48.460
In fully supervised adaptation, we would assume that in Google Street View, we have labeled

15:48.460 --> 15:50.900
images for all classes.

15:50.900 --> 15:54.540
And then in semi-supervised, we would assume that in Google Street View, which is our target

15:54.540 --> 15:59.020
domain, we have labeled, you know, data for a subset of our classes.

15:59.020 --> 16:00.020
Okay.

16:00.020 --> 16:03.820
So the idea of domain adaptation is when you're training set and your tests that have different

16:03.820 --> 16:08.900
distributions, how can you best use them, you know, within these different domains,

16:08.900 --> 16:12.380
assuming, you know, making these different assumptions that we just talked about, how can

16:12.380 --> 16:16.740
you best use data in your social domain, I guess, in conjunction with data in your target

16:16.740 --> 16:20.220
domain to maximize your accuracy on the target domain.

16:20.220 --> 16:21.220
Okay.

16:21.220 --> 16:22.220
Right?

16:22.220 --> 16:25.340
So this is a very real, in the real world, this is usually the case because you're never

16:25.340 --> 16:30.580
going to have like camera statistics are different, you know, occlusion or whatever.

16:30.580 --> 16:36.060
So like if you have a training set from like Google images or, you know, you want search

16:36.060 --> 16:37.580
images or something.

16:37.580 --> 16:42.420
And then you're like, you know, you want to apply it, you know, your model to like some

16:42.420 --> 16:43.420
other thing.

16:43.420 --> 16:44.420
Yeah.

16:44.420 --> 16:45.420
That has a different statistic.

16:45.420 --> 16:47.820
You need to know about adaptation techniques, you know.

16:47.820 --> 16:51.980
And when I think about Google Street View images, all those images have, you know, from

16:51.980 --> 16:56.580
the perspective, like they have a very specific kind of look that's different from anything

16:56.580 --> 16:59.620
you'd ever see on Edmonds or any other part side.

16:59.620 --> 17:00.620
Oh, yeah.

17:00.620 --> 17:02.020
Because Edmonds, you know, I'm trying to sell you my car.

17:02.020 --> 17:04.540
So I want to give you the best perspective.

17:04.540 --> 17:07.260
Like you have a really nice resolution.

17:07.260 --> 17:09.180
It's in one car in the middle.

17:09.180 --> 17:12.540
There's no other car, like, occluding this car.

17:12.540 --> 17:17.140
There's no trees or like, you know, I don't know, like there's, it's very, very different.

17:17.140 --> 17:20.140
You don't, you know, there's cars in Google Street View where you only see like, it's

17:20.140 --> 17:21.140
a stage of damage.

17:21.140 --> 17:22.140
Yeah.

17:22.140 --> 17:23.140
Yeah.

17:23.140 --> 17:24.140
Yeah.

17:24.140 --> 17:25.140
Yeah.

17:25.140 --> 17:26.140
There's cars in Google Street View where you only see like a couple of lights or something

17:26.140 --> 17:27.140
like that.

17:27.140 --> 17:28.540
You have this side, you know, view.

17:28.540 --> 17:33.860
So, so that's where, you know, I'm very interested in the domain adaptation problem because

17:33.860 --> 17:34.860
of that.

17:34.860 --> 17:40.020
This project helped me decide what core machine learning and computer vision areas I'm very

17:40.020 --> 17:41.420
interested in.

17:41.420 --> 17:43.580
Because of this, it's domain adaptation.

17:43.580 --> 17:49.340
The second one is data collection, efficient data, like basically what some people call efficient

17:49.340 --> 17:51.780
machine learning, a data efficient machine learning.

17:51.780 --> 17:54.540
So I would say domain adaptation is part of it.

17:54.540 --> 17:56.380
How to efficiently collect data sets.

17:56.380 --> 17:59.900
So when I think of data efficient machine learning, I think of like one shot, few shot

17:59.900 --> 18:00.900
machine learning.

18:00.900 --> 18:01.900
I kind of have anything.

18:01.900 --> 18:04.700
I haven't done too much on a few shot, one shot learning, whatever.

18:04.700 --> 18:05.700
I don't know.

18:05.700 --> 18:09.420
The semantic sometimes kind of like confused me, but you know, but just any, it basically,

18:09.420 --> 18:14.420
so because data collection does not sound fun, I don't think AI people are, you know,

18:14.420 --> 18:18.540
in computer vision, we a lot of us work on it and there's even people doing like a

18:18.540 --> 18:23.820
hybrid computer vision at HCI kind of work, especially in our labs from the very beginning,

18:23.820 --> 18:27.900
like a lot of people were always working on data collection because we don't want to

18:27.900 --> 18:28.900
work on toy problems.

18:28.900 --> 18:31.860
Like, you can't, you can only do so much with MNIST, you know, you can only, you know,

18:31.860 --> 18:36.420
really gain, you can only gain so much insight, you know, to mean into like what kind of problems

18:36.420 --> 18:37.420
we should be solving.

18:37.420 --> 18:41.700
If you're always just using like, you know, readily available data so that you can get

18:41.700 --> 18:45.380
to the next like conference deadline or something like that, you know, I'm saying you should

18:45.380 --> 18:49.220
always spend so much time collecting data, but I'm saying you should do it at least a couple

18:49.220 --> 18:55.380
of times in your life, just to see, you know, where AI is right now if we were wanted

18:55.380 --> 18:57.700
to apply it to the real world, you know.

18:57.700 --> 19:02.140
So this project really, really like, I would say that I mean, I was complaining about it

19:02.140 --> 19:05.020
the whole time I was working on it and that it's over.

19:05.020 --> 19:09.500
It really cemented like what I think is really important to do, to work on.

19:09.500 --> 19:14.420
And actually the issue of bias also came up in this project because in an earlier paper,

19:14.420 --> 19:17.340
you know, we also, we were just trying to see like, okay, we can predict this, we can

19:17.340 --> 19:18.340
predict that.

19:18.340 --> 19:20.980
And one of the things we looked into was crime, crime rates, right?

19:20.980 --> 19:24.180
So, so, and then with crime rates, as you know, the ground truth, whatever ground truth

19:24.180 --> 19:28.180
we were going to have is bias, because all we know is who got arrested for the crime,

19:28.180 --> 19:29.660
who's crime got reported.

19:29.660 --> 19:34.380
So if I say, hey, look, with images, we can, you know, with cars, we can do this, then

19:34.380 --> 19:35.660
that's already bias, right?

19:35.660 --> 19:40.020
So, so basically like, if I'm going to do this type of work, if I'm going to continue

19:40.020 --> 19:44.660
to do this type of work, I have to, I have to also be working on the bit bias and fairness

19:44.660 --> 19:46.460
and other types of issues.

19:46.460 --> 19:50.060
And what's really interesting is that domain adaptation and this whole fairness thing

19:50.060 --> 19:53.740
are very, very related, actually, I just noted.

19:53.740 --> 19:57.540
So like, so some of the techniques that you can even use, that people, some people have

19:57.540 --> 19:59.220
even already used are very related.

19:59.220 --> 20:06.100
So one of the ways in which people do domain adaptation is to say, you know, say, you

20:06.100 --> 20:11.420
know, I want a classifier, let's see your classifier has a primary task, which is to classify

20:11.420 --> 20:14.820
something, maybe the type of card my image, right?

20:14.820 --> 20:22.020
And so I want that classifier to do well, regardless of what domain my image comes from,

20:22.020 --> 20:25.860
whether or not it's Craigslist or, you know, whether it's Craigslist or Google

20:25.860 --> 20:27.060
Street View, right?

20:27.060 --> 20:31.220
So the way some people do this and there's many variations of this is they have another

20:31.220 --> 20:37.180
classifier and the other classifier all it does is it uses the features that its input

20:37.180 --> 20:39.780
is the features learned by the first classifier.

20:39.780 --> 20:45.060
And so the input is those features and the output is, you know, which domain the image

20:45.060 --> 20:48.180
came from Craigslist or Google Street View, right?

20:48.180 --> 20:53.260
So then the first classifier in its job is in addition to accurately classifying the

20:53.260 --> 20:58.460
car, its job is to also confuse the second classifier because if the second classifier

20:58.460 --> 21:04.620
can't tell based on the features learned by the first classifier, which domain the image

21:04.620 --> 21:08.140
came from, then it means you've learned features that are sort of domain invariant,

21:08.140 --> 21:09.140
correct?

21:09.140 --> 21:12.020
Now can you see how you might apply this to fairness?

21:12.020 --> 21:17.980
So say that you want to classify something like, you know, your risk score or something

21:17.980 --> 21:18.980
like this.

21:18.980 --> 21:22.980
You want to say, if my other classifier can identify some class that I don't want to

21:22.980 --> 21:23.980
identify it.

21:23.980 --> 21:24.980
Yeah.

21:24.980 --> 21:28.220
So say you want this to be like, you know, invariant to like your race or something.

21:28.220 --> 21:32.580
So then you can have another classifier that classifies the race of the person.

21:32.580 --> 21:34.780
And then you, this first one confuses that.

21:34.780 --> 21:38.780
You know, now, now, like, there's been work already that does this and, you know, it's

21:38.780 --> 21:43.660
not like a done deal, it's not solved because then there's different fairness criteria

21:43.660 --> 21:47.220
and then like, which criteria do you use, et cetera, et cetera, but but I do think that

21:47.220 --> 21:51.660
these two things, like, you know, these two fields are kind of very related.

21:51.660 --> 21:52.660
Yeah.

21:52.660 --> 21:57.020
And it's so weird to me because I didn't think about that when I got interested in both

21:57.020 --> 21:58.020
of them.

21:58.020 --> 21:59.020
I didn't think about that at all.

21:59.020 --> 22:01.980
And now I'm like, oh, wait a minute, you know, interesting.

22:01.980 --> 22:02.980
Yeah.

22:02.980 --> 22:07.300
Now this, this last thing you were describing, it sounds like it, like the kind of thing

22:07.300 --> 22:09.260
we see in adversarial networks is it?

22:09.260 --> 22:10.260
Yeah.

22:10.260 --> 22:11.260
Yeah, exactly.

22:11.260 --> 22:13.420
I mean, you people have done it with adversarial networks.

22:13.420 --> 22:16.140
So you can implement it with adversarial networks.

22:16.140 --> 22:18.300
You don't have to implement with adversarial networks.

22:18.300 --> 22:22.500
There's other works that use like a different loss function, like, there's work called,

22:22.500 --> 22:25.300
there's the gradient reversal work from a white way back.

22:25.300 --> 22:27.180
I was like 2014ers and like that.

22:27.180 --> 22:29.260
There's also domain confusion loss.

22:29.260 --> 22:35.420
So Judy, who is, well, was opposed to that in our lab, and I had a paper at ICCV where

22:35.420 --> 22:41.780
I used her loss from like a different, a prior paper of hers, which was not adversarial.

22:41.780 --> 22:45.980
But it's very, again, like, it's funny that people independently were working on this

22:45.980 --> 22:51.060
idea by the time Ian came up with his adversarial networks thing, with his GAN thing.

22:51.060 --> 22:54.220
And then once he came up with a GAN thing, they're like, oh, wait a minute.

22:54.220 --> 22:56.260
We could just use GANs for this.

22:56.260 --> 23:00.060
But that's another thing I find interesting is that like this, this idea was kind of

23:00.060 --> 23:03.100
concurrently being thought of by other people.

23:03.100 --> 23:04.100
Interesting.

23:04.100 --> 23:08.340
And so how do you take this forward in your new role at Microsoft?

23:08.340 --> 23:12.420
Oh, so at Microsoft, I'm working a whole bunch of stuff right now.

23:12.420 --> 23:20.220
So Joy Bulwamini from MIT is here, by the way, and she and I have a paper that is hopefully

23:20.220 --> 23:24.980
going to come out at this new fairness conference for this accountability, transparency, and ethics

23:24.980 --> 23:25.980
in AI conference.

23:25.980 --> 23:30.540
It's a whole conference in February, in February, it's in New York, yeah.

23:30.540 --> 23:32.860
So I'm part of this engineering community there.

23:32.860 --> 23:36.660
So we have this paper that's basically doing algorithmic audits.

23:36.660 --> 23:40.100
It's going to come out soon so you can read about it when it comes out.

23:40.100 --> 23:44.700
And then I'm also working on this idea of standardizing.

23:44.700 --> 23:49.540
So you basically want to standardize what kind of information you should put out with

23:49.540 --> 23:54.340
your data sets or pre-trained models or whatever.

23:54.340 --> 23:56.740
So I've been telling everybody about this.

23:56.740 --> 24:02.140
So I used to be a hardware engineer and hardware, we have a data sheet that comes with every

24:02.140 --> 24:03.140
component.

24:03.140 --> 24:08.380
And when you're a circuit designer, you would be very intimately familiar with the data

24:08.380 --> 24:09.380
sheet.

24:09.380 --> 24:14.100
And the designers and all this stuff, before you design something into your model, right?

24:14.100 --> 24:15.100
So that process.

24:15.100 --> 24:16.100
We'll pull down the data set.

24:16.100 --> 24:17.100
We're not doing that.

24:17.100 --> 24:19.380
We're putting in our trainer model on it without thinking about it.

24:19.380 --> 24:24.580
You have an API that someone releases an API that you have to pay for.

24:24.580 --> 24:30.220
Really, you have no information about how you really have zero information about how

24:30.220 --> 24:35.860
are you supposed to, is it supposed to work on this new data set that you're using?

24:35.860 --> 24:37.820
Are there recommended applications?

24:37.820 --> 24:40.500
What's going to happen if you use it the wrong way?

24:40.500 --> 24:42.460
And it's very dangerous.

24:42.460 --> 24:47.580
Because in hardware, at least, I think the reason things were very, it's a mature field,

24:47.580 --> 24:53.340
but things that were standardized because the failure mode is so, it's visible to most

24:53.340 --> 24:54.860
people to everyone, right?

24:54.860 --> 24:59.580
Like, your battery catches fire or something like that, whereas here could be visible to

24:59.580 --> 25:04.420
some people, you know, and might not be like, if face recognition doesn't work for you

25:04.420 --> 25:08.140
because you're black or something like that, it'll be visible, very visible to you.

25:08.140 --> 25:11.340
It won't be visible for other people because they didn't test it out or something like

25:11.340 --> 25:12.340
this.

25:12.340 --> 25:13.340
Right.

25:13.340 --> 25:16.420
Okay, so it's, you know, because probability is involved, it's, you know, might not even

25:16.420 --> 25:19.500
be, you know, formally visible in the class that's affected.

25:19.500 --> 25:20.980
Yeah, exactly.

25:20.980 --> 25:26.980
And so, and, you know, like, we, a lot of people aren't, we're even doing these tests.

25:26.980 --> 25:31.980
So like, the first thing we have to start doing is like doing these audits, audits, you

25:31.980 --> 25:32.980
know?

25:32.980 --> 25:37.700
So stuff like this that I'm working on, I'm also working on, you know, just like, I was

25:37.700 --> 25:42.500
telling you, you know, models that, that are fairer, you know, what is the, the fairness

25:42.500 --> 25:43.500
criterion.

25:43.500 --> 25:45.620
And very, you know, I've learned a lot about this field.

25:45.620 --> 25:48.940
So I, I'm putting you to this field and now I've like gotten embedded in the community

25:48.940 --> 25:50.460
which is nice.

25:50.460 --> 25:52.140
What I mean is to the fairness community.

25:52.140 --> 25:56.860
So yeah, like, so kind of working from the, from the purely machine learning perspective,

25:56.860 --> 26:01.580
you know, like, how can we have account for fairness criterion in our models?

26:01.580 --> 26:05.260
It's very broad, but like, that's one of the things I'm working on, in addition to just

26:05.260 --> 26:10.660
like my regular computer vision kind of work, like domain adaptation blah, blah.

26:10.660 --> 26:14.860
So yeah, so that's how I'm, I'm taking this to my new role at MSR.

26:14.860 --> 26:15.860
Okay.

26:15.860 --> 26:16.860
Interesting.

26:16.860 --> 26:17.860
I'm really interested in this conference.

26:17.860 --> 26:22.460
I'll need to get some info from you about it and check it out.

26:22.460 --> 26:28.100
What are some of the other kind of major research directions in the domain of fairness?

26:28.100 --> 26:32.260
So I think that uncovering bias is one.

26:32.260 --> 26:33.260
Okay.

26:33.260 --> 26:37.900
So Joy and I just did this paper that's coming out where like we were auditing, you

26:37.900 --> 26:43.420
know, commercial gender classification APIs that are sold by people that you have to

26:43.420 --> 26:48.460
pay for and looking at disparities among certain groups by skin color, by gender.

26:48.460 --> 26:51.300
And then, you know, Meg Mitchell just came out with this paper.

26:51.300 --> 26:54.300
So I'm just talking about computer vision, because actually in computer vision, it's

26:54.300 --> 26:55.300
very new.

26:55.300 --> 26:58.580
People have been doing this stuff for like, you know, there is this, I don't know, have

26:58.580 --> 26:59.580
you heard of this?

26:59.580 --> 27:04.580
Man is to programmer as woman is to a filmmaker, that's the kind of like, I've seen a few

27:04.580 --> 27:05.580
various.

27:05.580 --> 27:06.580
Yeah.

27:06.580 --> 27:09.620
So, you know, and so I think in computer vision, that's one of the reasons I wanted to get

27:09.620 --> 27:10.620
into it.

27:10.620 --> 27:12.420
I felt like computer vision people weren't thinking about this.

27:12.420 --> 27:13.420
Where's NLP?

27:13.420 --> 27:14.420
It's been a little bit worse.

27:14.420 --> 27:19.100
And I think the first time Meg is an NLP, even Hannah does some NLP and like, I mean, does

27:19.100 --> 27:20.100
NLP.

27:20.100 --> 27:25.060
And like, even, and also theory people, I feel like in terms of technical people,

27:25.060 --> 27:29.020
theory people were starting to think about it, privacy people, like, send you to work

27:29.020 --> 27:34.020
and other, you know, like, and also from the ethics side, but I felt like, okay, now

27:34.020 --> 27:35.860
deep learning people are talking about it too.

27:35.860 --> 27:40.260
And there's like papers, things like this, but like last year, I felt like, yeah, people

27:40.260 --> 27:44.940
were starting to talk about it, but it wasn't a real, you know, concern.

27:44.940 --> 27:47.980
So now, like, in computer vision, even, we're starting to see some of these work.

27:47.980 --> 27:52.460
So one is uncovering bias, like, you know, what kind of bias exists.

27:52.460 --> 27:55.700
And the second one is how do you mitigate it if you uncover it?

27:55.700 --> 27:57.100
So, you know, there's a lot of work.

27:57.100 --> 28:01.820
So with, I guess the work I just talked about, the word-to-vec work, you know, the first

28:01.820 --> 28:06.420
uncover that bias, and then they tell you some strategies of mitigating that particular

28:06.420 --> 28:07.420
bias.

28:07.420 --> 28:11.340
And then the third one, which I'm very interested in is, I'm interested in all of them,

28:11.340 --> 28:12.340
right?

28:12.340 --> 28:15.980
But the third one is also just like understanding how these things are being used.

28:15.980 --> 28:20.700
So if you have, and how to standardize them, how to have transparency, like, if you have

28:20.700 --> 28:26.460
law enforcement that's using inaccurate face recognition algorithms, where are they

28:26.460 --> 28:27.460
using it?

28:27.460 --> 28:28.460
How are they using it?

28:28.460 --> 28:30.500
Well, you know, we have no idea.

28:30.500 --> 28:35.020
And also just like, you know, there's people who are using your social network data to,

28:35.020 --> 28:39.380
like, then selling it to other people or trying to figure out, like, your credit ratings

28:39.380 --> 28:40.740
and things like their startups, right?

28:40.740 --> 28:42.740
Kathy Neal talks about it in her book.

28:42.740 --> 28:43.740
Have you read this book?

28:43.740 --> 28:44.740
Weapons of Math Destruction.

28:44.740 --> 28:45.740
Okay.

28:45.740 --> 28:46.740
Yeah.

28:46.740 --> 28:47.740
That's hard to get.

28:47.740 --> 28:48.740
And it's on my list.

28:48.740 --> 28:53.980
I mean, and so that's a very important part of the issue actually, because as AI researchers,

28:53.980 --> 28:58.580
a lot of times, I mean, me included, okay, like, I just want to sit in a corner, read my

28:58.580 --> 29:03.100
papers, you know, and I honestly write some code.

29:03.100 --> 29:08.460
That's what I love doing most, still, you know, even though I do this whole, like, social

29:08.460 --> 29:15.620
activism stuff, like, what I enjoy doing is just, just, like, reading papers, thinking

29:15.620 --> 29:17.940
about ideas, writing code, right?

29:17.940 --> 29:23.340
But we have to understand, like, what are the implications of our work are, you know?

29:23.340 --> 29:29.100
And so, like, keeping track of, I just signed this extreme vetting letter against this extreme

29:29.100 --> 29:30.100
vetting initiative.

29:30.100 --> 29:32.260
I don't know if you've heard about it by the DHS.

29:32.260 --> 29:33.260
Yeah.

29:33.260 --> 29:38.660
That was trying to, I didn't even know about it until I was, yeah, until I was, I had no

29:38.660 --> 29:40.860
idea this was going on and it's terrifying.

29:40.860 --> 29:41.860
Yeah.

29:41.860 --> 29:45.420
You know, like, so this kind of stuff, we have to keep track of and we have to make our

29:45.420 --> 29:50.500
voices heard in addition to, you know, working on uncovering bias and mitigating bias.

29:50.500 --> 29:51.500
Right.

29:51.500 --> 29:52.500
Awesome.

29:52.500 --> 29:55.340
Well, I know you've got to run off to a talk.

29:55.340 --> 29:56.340
Yes.

29:56.340 --> 30:00.260
So, let's wrap it up here, but if you have any final words or thoughts or places that

30:00.260 --> 30:04.540
folks should be, you know, looking to keep up with all this information or finding you,

30:04.540 --> 30:05.540
yeah.

30:05.540 --> 30:06.540
Now's the time.

30:06.540 --> 30:10.940
Well, I recently got on Twitter, I was told it's a good thing.

30:10.940 --> 30:11.940
Wow.

30:11.940 --> 30:15.580
Well, yeah, I'm Tim Nick, who's my, my Twitter, and I'm going to have a website.

30:15.580 --> 30:21.180
So, I will be releasing data soon for this monstrous work that I hate that we just discussed

30:21.180 --> 30:26.060
with PNAS paper and to look out for my new, yeah, paper with joy as well.

30:26.060 --> 30:27.060
Okay.

30:27.060 --> 30:28.060
And we're just going to be releasing a couple weeks.

30:28.060 --> 30:29.060
All right.

30:29.060 --> 30:32.620
Well, we'll link you on Twitter and the show notes as well as to your homepage and looking

30:32.620 --> 30:35.940
forward to following this line of research and the stuff you do at Microsoft.

30:35.940 --> 30:36.940
Thank you.

30:36.940 --> 30:44.340
All right, everyone.

30:44.340 --> 30:46.500
That's our show for today.

30:46.500 --> 30:51.660
Thanks so much for listening and for your continued feedback and support.

30:51.660 --> 30:56.780
For more information on Timnett or any of the topics covered in this episode, head on

30:56.780 --> 31:01.100
over to twimlai.com slash talk slash 88.

31:01.100 --> 31:08.020
To follow along with the nip series, visit twimlai.com slash nips 2017.

31:08.020 --> 31:14.660
To enter our Twimlai one mill contest, visit twimlai.com slash twimlai one mill.

31:14.660 --> 31:20.420
Of course, we'd be delighted to hear from you either via a comment on the show notes page

31:20.420 --> 31:25.620
or via a tweet to at twimlai or at Sam Charrington.

31:25.620 --> 31:30.900
Thanks once again to Intel Nirvana for their sponsorship of this series to learn more about

31:30.900 --> 31:37.040
the Intel Nirvana NNP and the other things Intel's been up to in the AI arena, visit intel

31:37.040 --> 31:39.100
Nirvana.com.

31:39.100 --> 31:43.860
As I mentioned a few weeks back, this will be our final series of shows for the year.

31:43.860 --> 31:49.100
So take your time and take it all in and get caught up on any of the old pods you've been

31:49.100 --> 31:50.700
saving up.

31:50.700 --> 31:53.100
Happy holidays and happy new year.

31:53.100 --> 31:55.380
See you in 2018.

31:55.380 --> 32:01.540
And of course, thanks once again for listening and catch you next time.

