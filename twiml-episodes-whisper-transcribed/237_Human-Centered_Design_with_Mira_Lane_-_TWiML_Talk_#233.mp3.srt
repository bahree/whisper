1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,440
I'm your host Sam Charrington. Today we're excited to present the final episode in our AI

4
00:00:34,440 --> 00:00:39,320
for the Benefit of Society series, in which we're joined by Mira Lane, partner director

5
00:00:39,320 --> 00:00:44,960
for Ethics and Society at Microsoft. Mira and I focus our conversation on the role of

6
00:00:44,960 --> 00:00:50,720
culture and human centered design in AI. We discuss how Mira defines human centered

7
00:00:50,720 --> 00:00:56,920
design, its connections to culture and responsible innovation and how these ideas can be scalably

8
00:00:56,920 --> 00:01:02,800
implemented across large engineering organizations.

9
00:01:02,800 --> 00:01:08,720
Before diving in, I'd like to thank Microsoft once again for their sponsorship of this series.

10
00:01:08,720 --> 00:01:13,280
Microsoft is committed to ensuring the responsible development and use of AI and is empowering

11
00:01:13,280 --> 00:01:18,600
people around the world with this intelligent technology to help solve previously intractable

12
00:01:18,600 --> 00:01:25,960
societal challenges, spanning sustainability, accessibility, and humanitarian action.

13
00:01:25,960 --> 00:01:35,480
Learn more about their plan at Microsoft.ai. Enjoy.

14
00:01:35,480 --> 00:01:40,560
Alright everyone, I am here with Mira Lane. Mira is the partner director of Ethics

15
00:01:40,560 --> 00:01:45,280
and Society at Microsoft. Mira, welcome to this weekend machine learning and AI.

16
00:01:45,280 --> 00:01:50,160
Thank you Sam, nice to meet you. Great to meet you and I'm excited to dive into this

17
00:01:50,160 --> 00:02:00,520
conversation with you. I saw that you are a video artist and technologist by background.

18
00:02:00,520 --> 00:02:03,840
How did you come to your looking away? Is that correct?

19
00:02:03,840 --> 00:02:05,360
No, that's absolutely true.

20
00:02:05,360 --> 00:02:14,120
So I noted that you're a video artist. How did you come to work at the intersection of

21
00:02:14,120 --> 00:02:17,240
Ethics and Society and AI?

22
00:02:17,240 --> 00:02:22,200
For sure. So let me say let me give you a little bit of a background on how I got to this

23
00:02:22,200 --> 00:02:29,520
point. I actually have a mathematics and computer science background from the University

24
00:02:29,520 --> 00:02:36,040
of Waterloo in Canada and so I've had an interesting journey and I've been a developer, a program

25
00:02:36,040 --> 00:02:42,840
manager and designer. And when I think about video art and artificial intelligence, I'll

26
00:02:42,840 --> 00:02:48,200
touch artificial intelligence first and then the video art. But a few years ago I had the

27
00:02:48,200 --> 00:02:54,040
opportunity to take a sabbatical and I do this every few years. I take a little break,

28
00:02:54,040 --> 00:02:58,960
reflect on what I'm doing, retool myself as well.

29
00:02:58,960 --> 00:03:03,240
So I decided to spend three months just doing art. A lot of people take a sabbatical and

30
00:03:03,240 --> 00:03:07,640
they travel but I thought I'm just going to do art for three months and it was luxurious

31
00:03:07,640 --> 00:03:13,680
and very special. But then I also thought I'm going to reflect on career at the same

32
00:03:13,680 --> 00:03:20,800
time. And I was looking at what was happening in a technology space and feeling really unsettled

33
00:03:20,800 --> 00:03:26,760
about where technology was going, how people were talking about it, the way I was seeing

34
00:03:26,760 --> 00:03:33,000
it affect our societies and I thought I want to get deeper into the AI space. And so when

35
00:03:33,000 --> 00:03:38,000
I came back to Microsoft I started poking around the company and said is there a role

36
00:03:38,000 --> 00:03:41,880
in artificial intelligence somewhere in the company and something opened up for me in

37
00:03:41,880 --> 00:03:47,960
our AI and research group where they were looking for a design manager. So I said absolutely

38
00:03:47,960 --> 00:03:56,000
I'll run one of these groups for you. But before I take the role I'm demanding that we

39
00:03:56,000 --> 00:04:01,440
have an ethics component to this work because what they were doing was they were taking

40
00:04:01,440 --> 00:04:06,240
research that was in the AI space and figuring out how do we productize this because at

41
00:04:06,240 --> 00:04:12,360
that point research was getting so close to engineering that we were developing new techniques

42
00:04:12,360 --> 00:04:16,480
and you were actually able to take those to market fairly quickly. And I thought this

43
00:04:16,480 --> 00:04:20,960
is a point where we can start thinking about responsible innovation and let's make that

44
00:04:20,960 --> 00:04:28,000
formalized practice. So me taking the role for the design manager was contingent on us

45
00:04:28,000 --> 00:04:34,960
creating a spot for ethics at the same time. And so backing up a little bit the video

46
00:04:34,960 --> 00:04:39,760
part comes in because I've traditionally been a really analog artist, been a printmaker,

47
00:04:39,760 --> 00:04:46,960
a painter and during my sabbatical I saw some more digitized, like looked at digitizing

48
00:04:46,960 --> 00:04:50,240
some of the techniques that I was playing with on the analog side. I thought well let me

49
00:04:50,240 --> 00:04:55,360
go play in the video space for a while. And so for three months I just like I said I retooled

50
00:04:55,360 --> 00:05:02,480
and I started playing around with different ways of recording, editing and teaching myself

51
00:05:02,480 --> 00:05:07,680
some of these techniques. And one of the goals I set out at the time was well can I get into

52
00:05:08,560 --> 00:05:15,200
a festival you know can I get into a music or a video festival. And so that was one of my goals

53
00:05:15,200 --> 00:05:19,600
at the end of the three months. Can I produce something interesting enough to get admitted

54
00:05:19,600 --> 00:05:24,640
into a festival. And I want a few actually. So so I was super pleased. I'm like okay well that

55
00:05:24,640 --> 00:05:29,920
means I'm I've got something there I need to continue practicing. But that for me opened up a

56
00:05:29,920 --> 00:05:37,360
whole new door. And and one of the things that I did a few years ago also was to explore art

57
00:05:37,360 --> 00:05:44,320
and with AI. And and could we create a little AI system that could mimic my artwork and become

58
00:05:44,320 --> 00:05:50,240
a little co-collaborator with myself. So we can dig into that if you want. But it was a really

59
00:05:50,240 --> 00:05:55,520
interesting journey around can AI actually complement an artist or even replace an artist.

60
00:05:56,160 --> 00:06:01,200
And and so I there's interesting learnings that came out of that experience. Okay interesting

61
00:06:01,200 --> 00:06:07,040
interesting. We're accumulating a nice list of things to to touch on here. Absolutely. Ethics

62
00:06:07,040 --> 00:06:11,360
and your views on that was at the top of my list. But before we got started you mentioned

63
00:06:11,360 --> 00:06:18,640
work that you've been doing exploring culture and the intersection between culture and AI.

64
00:06:19,440 --> 00:06:25,760
I'm curious what that means for you. It's certainly a topic that I hear brought up quite a bit

65
00:06:26,720 --> 00:06:33,360
particularly when I'm talking to folks in enterprises that are trying to adopt AI

66
00:06:33,360 --> 00:06:39,280
technologies. And here all the time oh well one of the biggest things we struggle with is culture.

67
00:06:39,280 --> 00:06:44,080
And so maybe I don't know if that's the right place to start but maybe we'll start there.

68
00:06:44,080 --> 00:06:48,800
What does that mean for you when you think about kind of culture and AI? Yeah no that's a really

69
00:06:48,800 --> 00:06:54,880
good question and I agree that one of the biggest things is culture. And the reason why I say that

70
00:06:54,880 --> 00:07:02,000
is if you look at every computer scientist that's graduating none of us have taken an ethics class

71
00:07:02,000 --> 00:07:07,600
and you look at the impact of our work it is touching the fabric of our society like it is

72
00:07:07,600 --> 00:07:12,800
touching our democracies and our freedoms our civil liberties and those are powerful tools

73
00:07:12,800 --> 00:07:19,600
that we're building yet none of us have gone through an formal ethics course. And so the discipline

74
00:07:19,600 --> 00:07:25,040
is not used to talking about this it's you know a few years ago you're just like oh I'm just

75
00:07:25,040 --> 00:07:30,640
building a tool I'm building an app I'm building a platform that people are using. And we weren't

76
00:07:30,640 --> 00:07:36,480
super introspective about that it wasn't part of the culture. And so when I think about culture

77
00:07:36,480 --> 00:07:43,280
in the AI space because we're building technologies that have scale and power and are building on

78
00:07:43,280 --> 00:07:50,000
top of large amounts of data that empower people to do pretty impressive things this whole question

79
00:07:50,000 --> 00:07:55,600
of culture and asking ourselves well what could go wrong how could this be used who is going to

80
00:07:55,600 --> 00:08:02,080
use it you know directly or indirectly and those are parts of the culture of technology that

81
00:08:02,080 --> 00:08:06,800
I don't think has been formalized it's usually here designers talking about that kind of thing

82
00:08:06,800 --> 00:08:12,720
it's part of humans and our design but even in the humans and our design space it's really about

83
00:08:12,720 --> 00:08:19,760
like what is my ideal user or my ideal customer and not thinking about well how could we exploit

84
00:08:19,760 --> 00:08:25,600
this technology in a way that we hadn't really intended and and we've talked about that from an

85
00:08:25,600 --> 00:08:30,560
engineering context the way we do you know threat modeling how could a system be attacked how do

86
00:08:30,560 --> 00:08:35,360
you think about denial of service attacks things like that but we don't talk about it from how

87
00:08:35,360 --> 00:08:40,640
could you use this to harm communities how could you use this to harm individuals or how could

88
00:08:40,640 --> 00:08:46,080
this be inadvertently harmful and so those parts of cultures are things that we're grappling right

89
00:08:46,080 --> 00:08:52,480
with right now and you know we're introducing into our engineering context so my group sits at

90
00:08:52,480 --> 00:08:58,640
an engineering level and we're trying to introduce this new framework around responsible innovation

91
00:08:58,640 --> 00:09:05,440
and there's five big components to that one is being able to anticipate look ahead anticipate

92
00:09:05,440 --> 00:09:10,320
different futures look around corners and try to see where the technology might go how someone

93
00:09:10,320 --> 00:09:16,000
could take it insert it into larger systems how you can do things at scale that are powerful

94
00:09:16,000 --> 00:09:23,920
that you may not intend to do there's a whole component around that this you know responsible

95
00:09:23,920 --> 00:09:28,560
innovation that is around reflection and looking at yourselves and saying well where do we have

96
00:09:28,560 --> 00:09:34,640
biases or where we assuming things what are our motivations can we have an honest conversation

97
00:09:34,640 --> 00:09:38,960
about our motivations why are we doing this and can we ask those questions how do we create the

98
00:09:38,960 --> 00:09:44,080
space for that we've been talking about you know diversity and inclusion like how do you bring

99
00:09:44,080 --> 00:09:49,760
diverse voices into the space especially people that would really object to what you're doing and

100
00:09:49,760 --> 00:09:55,680
how do you celebrate that versus tolerate that there's a big component around just like our

101
00:09:55,680 --> 00:10:01,040
principles and values and how do you create with intention and and how do you ensure that they

102
00:10:01,040 --> 00:10:05,760
align with the principles and they align with their values and they're still trustworthy so

103
00:10:05,760 --> 00:10:10,560
there's a whole framework around how we're thinking about innovation in the space and at the end

104
00:10:10,560 --> 00:10:14,720
of the day it comes down to what you're like the culture of the organization that you're building

105
00:10:14,720 --> 00:10:21,520
because if you can't operate at scale then you end up only having small pockets of us that are

106
00:10:21,520 --> 00:10:26,640
talking about this versus how do we get every engineer to ask what's this going to be used for

107
00:10:26,640 --> 00:10:31,920
and who's going to use it or what if this could happen and we need people to start asking those

108
00:10:31,920 --> 00:10:36,560
types of questions and then start talking about how do we architect things in a way that's

109
00:10:36,560 --> 00:10:43,360
responsible but I'd say like most engineers probably don't ask those types of questions right

110
00:10:43,360 --> 00:10:48,880
now and so we're trying to build that into the culture of how we design and develop new technologies.

111
00:10:50,080 --> 00:10:55,600
One of the things that I often find frustrating about this conversation particularly when talking

112
00:10:55,600 --> 00:11:03,440
to technology vendors is this kind of default answer while we just make the guns we don't shoot

113
00:11:03,440 --> 00:11:08,800
them right we just make the technologies you know it's they're you know they can be used for good

114
00:11:08,800 --> 00:11:16,800
they can also be used for bad but we're focused on you know the the good as aspects it sounds

115
00:11:16,800 --> 00:11:25,120
like yeah maybe you while I'm curious how do you articulate you know your responsibility

116
00:11:25,120 --> 00:11:29,200
with the tools that you're creating or Microsoft's responsibility with the tools that's creating

117
00:11:29,200 --> 00:11:35,280
do you have a well I have a very similar reaction to you when when I hear oh we're just making

118
00:11:35,280 --> 00:11:44,000
tools I think well fine that's one perspective but the responsible perspective is we're making tools

119
00:11:44,000 --> 00:11:49,120
and we understand that they can be used in these ways and we've architected them so that they

120
00:11:49,680 --> 00:11:54,640
cannot be misused and we know that there will be people that misuse them so I think

121
00:11:55,920 --> 00:12:00,480
and you're hearing a lot of this in the technology space and you know there's every year there's

122
00:12:00,480 --> 00:12:04,400
more and more of it where people are saying look we have to be responsible we have to be accountable

123
00:12:04,400 --> 00:12:10,320
and and so I think we'll hear fewer and fewer people saying what you're hearing what I'm hearing as

124
00:12:10,320 --> 00:12:17,840
well but one of the things we have to do is we have to avoid the ideal path and just talking only

125
00:12:17,840 --> 00:12:22,720
about the ideal path because it's really easy to just say here's the great ways that this technology

126
00:12:22,720 --> 00:12:27,920
was going to be used and not even talk about the other side because then again we fall into that

127
00:12:27,920 --> 00:12:32,960
pattern of well we only thought about it from this one perspective and so one of the things that

128
00:12:32,960 --> 00:12:37,760
my group is trying to do is to make it okay to talk about here's how it could go wrong

129
00:12:38,320 --> 00:12:44,640
so that it becomes part of our you know daily habit and and we do it at various levels you know we

130
00:12:44,640 --> 00:12:50,240
do it at our all hands so when people are showing our technology we have them show the dark side

131
00:12:50,240 --> 00:12:55,200
of it at the same time so that we can talk about that in an open space and it becomes okay to talk

132
00:12:55,200 --> 00:13:00,160
about it no one wants to share the bad side of technology right no one no one wants to do that

133
00:13:00,160 --> 00:13:05,280
but if we make it okay to talk about it then we can start talking about how do we prevent that

134
00:13:06,480 --> 00:13:11,440
so we do that at like you know larger forums and then you know this I know this is a podcast

135
00:13:11,440 --> 00:13:17,520
but I wanted to show you something so I'll talk about it but we created it's almost like a game

136
00:13:17,520 --> 00:13:23,920
but it's it's a way for us to look at different stakeholders and perspectives and what could happen

137
00:13:23,920 --> 00:13:30,240
and so how do we create a safe environment where you can look at one of our ethical principles

138
00:13:30,240 --> 00:13:36,240
you can look at a stakeholder that is interacting with the system and then you say well if the

139
00:13:36,240 --> 00:13:41,120
stakeholders you know for example it was a woman in a car and your system is a voice recognition

140
00:13:41,120 --> 00:13:46,400
system what would she say if she gave it a one-star review she would probably say I had to yell

141
00:13:46,400 --> 00:13:52,000
a lot and didn't recognize me because we know that most of our systems are not tuned to be diverse

142
00:13:52,000 --> 00:13:57,600
right and so we start creating this environment for us to talk about these types of things

143
00:13:57,600 --> 00:14:03,760
so that it becomes okay again really how do we create safe spaces and then as we develop our scenarios

144
00:14:03,760 --> 00:14:10,240
how do we bring those up and then track them and say well how do we fix it now we've excavated

145
00:14:10,240 --> 00:14:15,440
these issues well let's fix it and let's talk about it so that's again part of culture like how

146
00:14:15,440 --> 00:14:22,400
do we make it okay to bring up the bad parts of things right so it's not just the ideal path do

147
00:14:22,400 --> 00:14:32,160
you run into or run up against engineers or executives that say you know introspection safe spaces

148
00:14:32,160 --> 00:14:38,880
you know granola you know what about the bottom line what does this mean for you know us as a

149
00:14:38,880 --> 00:14:44,800
business how do we you know think about this from a shareholder perspective you know it's it's

150
00:14:44,800 --> 00:14:52,640
interesting I I don't actually hear a lot of that push back because I think you know internally

151
00:14:52,640 --> 00:14:57,920
at Microsoft there is this recognition of who we want to be really thoughtful and intentional

152
00:14:58,480 --> 00:15:04,080
and and I think the bigger issue that we hear is just like how do we do it it's not that we don't

153
00:15:04,080 --> 00:15:10,880
want to it's well how do we do it and how do we do it at scale and so what are the different things

154
00:15:10,880 --> 00:15:19,120
you can put in place to help people bring this into their practice and so you know there isn't a

155
00:15:19,120 --> 00:15:24,320
pushback around well this is going to like it's going to affect my bottom it's going to affect

156
00:15:24,320 --> 00:15:29,680
my bottom line but there's more of a understanding that yeah if we build things that are thoughtfully

157
00:15:29,680 --> 00:15:35,360
designed and intentional and ethical that it's better for our customers I mean our customers want

158
00:15:35,360 --> 00:15:41,920
that too but then again the question is well how do we do it and where is it manifest so there's

159
00:15:41,920 --> 00:15:47,120
things that we're doing in that space I mean when you look at AI a big part of it is data so how do

160
00:15:47,120 --> 00:15:52,160
you look at the data that's being used to power some of these systems and say is this the diverse

161
00:15:52,160 --> 00:15:58,880
data set is this well-rounded do we have gaps here what's the bias in here and so we start looking

162
00:15:58,880 --> 00:16:05,200
at certain components of our systems and helping to again architect it in a way that's that's

163
00:16:05,200 --> 00:16:11,280
better I think all of our customers would want a system that recognized all voices right and

164
00:16:12,160 --> 00:16:16,240
because again to them they wouldn't want a system that just worked for men it didn't work for

165
00:16:16,240 --> 00:16:21,360
women so again it's like better product as a result and so if we can couch it in terms of

166
00:16:21,360 --> 00:16:27,680
better product then I think it makes sense versus if it's all about us philosophizing and only

167
00:16:27,680 --> 00:16:35,200
doing that I don't know if that's the best you know only doing that is not productive right do you

168
00:16:35,200 --> 00:16:46,400
find that the uncertainty around ethical issues related to AI has been an impediment to customers

169
00:16:47,840 --> 00:16:55,440
adopting it does that get in the way do they do they need these issues to be figured out before

170
00:16:55,440 --> 00:17:04,480
they dive in I don't think it's getting in the way but I think it's what I'm hearing from customers

171
00:17:05,120 --> 00:17:12,800
is help us think about these issues and you know a lot of people a lot of customers don't

172
00:17:12,800 --> 00:17:19,040
understand AI deeply right it's it's a complex space and a lot of people are ramping up in it

173
00:17:19,040 --> 00:17:23,920
and so the question is more about well what should I be aware of what are the questions

174
00:17:23,920 --> 00:17:29,680
that I should be asking and how can we do this together we know you guys are thinking about

175
00:17:29,680 --> 00:17:35,280
this deeply we're getting just involved in it you know a customer might say and so they

176
00:17:35,280 --> 00:17:40,240
it's more about how do we educate each other and for us if we want to understand like how do you

177
00:17:40,240 --> 00:17:44,720
want to use this because sometimes we don't always know the use case for the customer so we want

178
00:17:44,720 --> 00:17:48,560
to deeply understand that to make sure that what we're building actually works for what they are

179
00:17:48,560 --> 00:17:53,040
trying to do and from their perspective they want to understand well how does this technology work

180
00:17:53,040 --> 00:17:59,280
and where will it fail and where will it not work for my customers and so the question of ethics

181
00:17:59,280 --> 00:18:05,520
is more about we don't understand the space well enough help us understand it and we are concerned

182
00:18:05,520 --> 00:18:12,960
about what it could do and can we work together on that so it's it's not preventing them from

183
00:18:12,960 --> 00:18:17,760
adopting it but there's there's definitely a lot of dialogue it comes up quite a bit around

184
00:18:17,760 --> 00:18:23,360
well we've heard this we've heard bias is an issue what does that mean right and so we and so I

185
00:18:23,360 --> 00:18:30,240
think that's an education opportunity when you think about ethics from a technology innovation

186
00:18:30,240 --> 00:18:36,880
perspective are there examples of you know things that you've seen either that Microsoft is doing

187
00:18:36,880 --> 00:18:48,080
or out in the the broader role that strike you as innovative approaches to this problem yeah you

188
00:18:48,080 --> 00:18:53,920
know I'll go back to the data side of things just briefly but there's this concept called

189
00:18:53,920 --> 00:18:59,600
data sheets which I think is super interesting yeah you're probably really familiar with that

190
00:18:59,600 --> 00:19:04,640
and I've written about some of the work that Timnick Gebru and some others with Microsoft have

191
00:19:04,640 --> 00:19:09,840
done around data sheets for data sets exactly and the the interesting part for us is well how do

192
00:19:09,840 --> 00:19:16,160
you put it into the platform how do you bake that in and and so what one of the pieces of work

193
00:19:16,160 --> 00:19:21,520
that we're doing is we're taking this notion of data sheets and we are applying it into how we

194
00:19:21,520 --> 00:19:27,760
are collecting data and how we're building out our platform and so I think that that's I don't

195
00:19:27,760 --> 00:19:31,600
know if it's super novel because it to me it's like a nutrition label for your data like you

196
00:19:31,600 --> 00:19:36,960
won't understand how is it collected what's in it how can you use it but but I think that that's

197
00:19:36,960 --> 00:19:42,080
one where now as people leave the group you know you want to make sure that there's some history

198
00:19:42,080 --> 00:19:47,600
and understanding the composition of it there's some regulation around how we manage it internally

199
00:19:47,600 --> 00:19:52,880
and how we manage data in a thoughtful way I think that's just a really interesting concept that

200
00:19:52,880 --> 00:19:58,400
we should be talking about more as an industry and then can we share data between each other in a

201
00:19:58,400 --> 00:20:04,640
way that's responsible as well yeah yeah I don't know that the the data sheet I mean I think inherent

202
00:20:04,640 --> 00:20:10,320
to the idea was the hey this isn't novel in fact look at you know electrical components and all

203
00:20:10,320 --> 00:20:16,720
these other industries that do this it's just common sense quote unquote yeah but what is a little

204
00:20:16,720 --> 00:20:25,680
novel I think is actually doing it so since that paper was published several companies have

205
00:20:25,680 --> 00:20:34,400
published kind of similar takes model cards and there there have been a handful and every time

206
00:20:34,400 --> 00:20:39,760
I hear about them I ask okay so one is this you know what are you going to be publishing these

207
00:20:39,760 --> 00:20:47,120
for your services and the data sets that you're publishing and no one's done it yet so

208
00:20:47,120 --> 00:20:55,200
it's intriguing to hear you say that you're at least starting to think in this way internally

209
00:20:56,400 --> 00:21:05,040
do you have a sense for what the you know the path to publishing these kinds of you know whether

210
00:21:05,040 --> 00:21:11,680
it's a data sheet or a card or some kind of set of parameters around and bias either in a

211
00:21:11,680 --> 00:21:18,640
data set or a model you know for a commercial public service yeah absolutely we're actually

212
00:21:18,640 --> 00:21:25,520
looking at doing this for facial recognition and we've publicly commented about that we've said

213
00:21:25,520 --> 00:21:30,320
hey we're going to be sharing for our services what they're what it's great for and what it's not

214
00:21:30,320 --> 00:21:36,960
where it's and so that stuff is actually actively being worked on right now you'll probably see

215
00:21:36,960 --> 00:21:42,480
more of this in the next few weeks but but there is public comment that's going to come out

216
00:21:43,520 --> 00:21:48,800
with more details about it and I'll say that you know on the data sheet side I think a large

217
00:21:48,800 --> 00:21:54,880
portion of it is it needs to get implemented in the engineering systems first and you need to find

218
00:21:54,880 --> 00:21:58,960
the right place to put it and so that's that's the stuff that we're working on actively right now

219
00:21:58,960 --> 00:22:08,400
um can you comment more on that I it does as you say that it does strike me a little bit as one

220
00:22:08,400 --> 00:22:15,680
of these iceberg kind of problems like it you know it you know looks very manageable kind of above

221
00:22:15,680 --> 00:22:21,280
the water line but if you think about what goes into the creation of a data set or a model there's

222
00:22:21,280 --> 00:22:26,160
a lot of complexity and certainly at you know the scale of Microsoft is working at it needs to be

223
00:22:26,160 --> 00:22:34,080
automated what are some of the challenges that have come into play and in trying to implement

224
00:22:34,080 --> 00:22:40,640
an idea like that well um let me think about this for a second so I can frame it at the right way

225
00:22:42,880 --> 00:22:50,560
the biggest challenge for us on something like that is um really thinking through

226
00:22:50,560 --> 00:22:56,640
the data collection effort first and spending a little bit time there that's where we're actually

227
00:22:56,640 --> 00:23:02,640
spending quite a bit of time as we look at um so let me back up for a second I I work in an

228
00:23:02,640 --> 00:23:08,000
engineering group that touches all the speech language vision technologies and we do an enormous

229
00:23:08,000 --> 00:23:12,560
amount of data collection to power those technologies one of the things that we're first spending

230
00:23:12,560 --> 00:23:18,560
time on is looking at exactly how we're collecting data and going through those methodologies and

231
00:23:18,560 --> 00:23:21,920
saying is this the right way that we should be doing this we want to change it in any way do we

232
00:23:21,920 --> 00:23:26,880
want to optimize it and then we want to go and apply that back in so you're right this is a big

233
00:23:26,880 --> 00:23:32,800
iceberg because there's so many pieces that are connected to it and the spec for data sheets and

234
00:23:32,800 --> 00:23:40,720
the ones we've seen are large and um and so what we've done is how do we grab the core pieces of

235
00:23:40,720 --> 00:23:46,320
this and implement and create the starting point for it and then scale over time adversioning

236
00:23:46,320 --> 00:23:50,800
being able to add your own custom schemas to it and scale over time but what is like the minimum

237
00:23:50,800 --> 00:23:55,200
piece that we can put into the system and then make sure that it's working the way we want it to

238
00:23:55,200 --> 00:24:00,320
and so it's just it's about decomposing the problem and saying which ones do we want to prioritize

239
00:24:00,320 --> 00:24:04,720
first um for us we're spending a lot of time just looking at the data collection

240
00:24:04,720 --> 00:24:09,920
methodologies first because there's so much of that going on and at the same time what is the

241
00:24:09,920 --> 00:24:14,720
minimum part of the data sheet spec that we want to go and put in and then let's start iterating

242
00:24:14,720 --> 00:24:20,560
together on that it strikes me that these will be most useful when there's kind of broad

243
00:24:20,560 --> 00:24:26,800
industry adoption or at least coalescence around some you know standard whether it's a standard

244
00:24:26,800 --> 00:24:32,880
minimum that everyone's doing and potentially growing over time or you involved in or where

245
00:24:32,880 --> 00:24:40,080
of any efforts to create something like that well I think that that's um that's one piece where

246
00:24:40,080 --> 00:24:46,320
it's important I would say also in a large corporation it's important internally as well because

247
00:24:46,320 --> 00:24:51,920
we work with so many different teams um and we're interfacing with the you know we're a platform

248
00:24:51,920 --> 00:24:56,800
where we interface with large parts of our organization and um and being able to share that

249
00:24:56,800 --> 00:25:02,880
information internally that is a really important piece to the puzzle as well I think the external

250
00:25:02,880 --> 00:25:09,040
part is as well but the internal one is not um not any less important in my eyes because that's

251
00:25:09,040 --> 00:25:14,000
where we are we want to make sure that if we have a set of data that this you know group A is

252
00:25:14,000 --> 00:25:18,080
using it in one way if group B wants to use it we want to make sure that they have the rights

253
00:25:18,080 --> 00:25:24,400
to use it they understand what it's composed of where its orientation is and um and so that if

254
00:25:24,400 --> 00:25:31,120
they pick it up they do it with full knowledge of what's in it so um for us internally it's a

255
00:25:31,120 --> 00:25:36,080
really big deal externally um I've heard pockets of this but I don't think I could really comment

256
00:25:36,080 --> 00:25:42,240
on that yet with you know like full authority I'm really curious about the intersection between

257
00:25:42,240 --> 00:25:51,680
ethics and design and um you mentioned human centered design earlier my sense is that that that

258
00:25:51,680 --> 00:25:55,680
phrase kind of captures a lot of that intersection can you elaborate on what that means for you

259
00:25:55,680 --> 00:26:02,800
yeah yeah um so when you look at traditional design functions when we talk about human centered

260
00:26:02,800 --> 00:26:07,680
design there is there's lots of different humans in our design frameworks the one I typically

261
00:26:07,680 --> 00:26:13,120
pick up is um Don Norman's you know emotional design framework where he talks about behavioral

262
00:26:13,120 --> 00:26:19,840
design reflective design and visceral design and um and so behavior is you know how is something

263
00:26:19,840 --> 00:26:25,120
functioning what is the functionality of it um reflective is how does it make you feel about

264
00:26:25,120 --> 00:26:32,240
yourself you know how does it play to your ego and your personality and um visceral is you know

265
00:26:32,240 --> 00:26:40,960
the look and feel of that that's a very um individual oriented approach to design and when I think

266
00:26:40,960 --> 00:26:47,600
about these large systems you actually need to bring in the ecosystem into that so how does this

267
00:26:47,600 --> 00:26:51,760
object you're creating or this system you're creating how does it fit into the ecosystem

268
00:26:51,760 --> 00:26:56,160
and so one of the things we've been playing around with is we've actually reached into adjacent

269
00:26:56,160 --> 00:27:00,640
areas like agriculture and explore like how do you do sustainable agriculture what are

270
00:27:00,640 --> 00:27:05,840
that some of those principles and methodologies and how do you apply that into our space so a lot

271
00:27:05,840 --> 00:27:10,560
of the conversations we're having is around ecosystems and how do you insert something into the

272
00:27:10,560 --> 00:27:16,400
ecosystem and what happens to it what is the ripple effect of that and then how do you do that in a

273
00:27:16,400 --> 00:27:21,840
way that keeps that whole thing sustainable for so it's not um it's a good solution versus one

274
00:27:21,840 --> 00:27:28,640
that's bad and um and causes other downstream effects so I think that those are changes that we

275
00:27:28,640 --> 00:27:33,840
have to have in our design methodology we're looking we're looking away from the one artifact and

276
00:27:33,840 --> 00:27:38,160
thinking about it from a you know here's how the one user is going to work with it versus how is

277
00:27:38,160 --> 00:27:42,800
the society and going to interact with it how are different communities going to interact with

278
00:27:42,800 --> 00:27:48,400
it and what does that do to that community um it's a larger problem and so there's like this shift

279
00:27:48,400 --> 00:27:54,320
in design thinking that we're trying to do with our designers so they're not just doing UI

280
00:27:54,320 --> 00:27:58,720
they're not just thinking about this one system they're thinking about it holistically um and there

281
00:27:58,720 --> 00:28:03,040
isn't a framework that we can easily pick up so we have to kind of construct one as we are going

282
00:28:03,040 --> 00:28:12,080
along yeah yeah for a while a couple of years ago maybe I was having I was in search of that framework

283
00:28:12,720 --> 00:28:22,640
and I think the the motivation was just really early experiences of seeing kind of AI shoved

284
00:28:22,640 --> 00:28:30,320
into products in ways that were frustrating or annoying like for example a nest thermostat like

285
00:28:31,120 --> 00:28:37,200
you it's intended to be very simple but it's making these decisions for you in a way that you

286
00:28:37,200 --> 00:28:42,720
can't really control and it's starting me down this path of you know what does it mean really

287
00:28:42,720 --> 00:28:53,200
build out uh you know a discipline of design that is aware of AI and intelligence I've

288
00:28:53,200 --> 00:28:58,080
jumped on the podcast before that you know I I call it intelligent design but that's overloaded

289
00:28:58,080 --> 00:29:03,840
term total is but is there a term for that now or people thinking about that how far have we

290
00:29:03,840 --> 00:29:10,160
come in building out um you know you know a discipline or a way of thinking of what it means to

291
00:29:10,160 --> 00:29:18,160
build intelligence into products yeah um we have done a lot of work around education for our designers

292
00:29:18,160 --> 00:29:24,400
because uh we found a big gap between what our engineers were doing and talking about and what

293
00:29:24,400 --> 00:29:30,160
our designers had awareness over so we actually created a deep learning for designers workshop it

294
00:29:30,160 --> 00:29:36,800
was a two-day workshop and it was really intensive so we took um you know neural nads

295
00:29:36,800 --> 00:29:41,920
convolutions like all these concepts and we introduced them to designers in a way that designers

296
00:29:41,920 --> 00:29:48,080
would understand it um we you know brought it to here's how you think about it in terms of

297
00:29:48,080 --> 00:29:51,840
Photoshop here's how you think about it in terms of the tools you're using and the words you use

298
00:29:51,840 --> 00:29:56,640
there here's how we'd apply here's a exercise where people had to get out of out of their seas and

299
00:29:56,640 --> 00:30:02,240
we created this really simple neural net with human beings and um and we had them coding as well

300
00:30:02,240 --> 00:30:09,840
and so they were coding in Python and um in notebooks and so they were uh they were exposed to it

301
00:30:09,840 --> 00:30:15,680
and and we exposed them to a lot of the techniques and terminology in a way that was concrete

302
00:30:15,680 --> 00:30:19,920
and they were able to then say oh this is what style transfer looks like oh this is what

303
00:30:19,920 --> 00:30:27,280
this is how we constructed a bot and so um first on the design side I think having the vocabulary

304
00:30:27,280 --> 00:30:31,600
to be able to say oh I know what this word means not just I know what it means but I've experienced

305
00:30:31,600 --> 00:30:35,680
it so then I can have a meaningful discussion with my engineer I think that that that was an

306
00:30:35,680 --> 00:30:41,840
important piece and then understanding how AI systems are just different from regular systems

307
00:30:41,840 --> 00:30:48,160
they are more probabilistic in nature the defaults matter they are they can be self-learning

308
00:30:48,160 --> 00:30:53,360
and so how do we think about these and starting to showcase case studies with our designers to

309
00:30:53,360 --> 00:30:58,160
understand that these types of systems are quite different from the deterministic type of systems

310
00:30:58,160 --> 00:31:03,200
that you may have designed for in the past um again I think it comes back to culture because

311
00:31:03,760 --> 00:31:08,640
it was there's a and we keep doing these workshops every quarter we'll do another one because

312
00:31:08,640 --> 00:31:13,600
we have so much demand for it and we found even engineers and PMs would come to our design workshops

313
00:31:14,160 --> 00:31:20,720
but um kind of democratizing the terminology a little bit and making it concrete to people

314
00:31:20,720 --> 00:31:25,200
was an is an important part of this it's interesting to think about

315
00:31:25,200 --> 00:31:32,720
what it does to a designer's design process to have more intimate knowledge of these concepts

316
00:31:33,760 --> 00:31:38,640
at the same time a lot of the questions that come to mind for me are you know much higher level

317
00:31:38,640 --> 00:31:46,240
concepts in the in the domain of design for example you can we talk about user experience

318
00:31:47,600 --> 00:31:54,640
to what degree should a user experience AI if that makes any sense should we be trying to make AI

319
00:31:54,640 --> 00:32:02,240
or or you know this notion of intelligence invisible to users or very visible to users um you know

320
00:32:02,240 --> 00:32:09,040
this has come up you know recently in for example I'm thinking of like Google Duplex when they announced

321
00:32:10,320 --> 00:32:14,480
that that system was going to be making phone calls to people and there was a big

322
00:32:15,440 --> 00:32:22,000
kerfuffle about whether that should be disclosed um yeah and I don't know that there's a right answer

323
00:32:22,000 --> 00:32:26,320
I get in some ways you want some of this stuff to be invisible in other ways you know time

324
00:32:26,320 --> 00:32:31,280
back to the whole ethics conversation it does make sense that you know there's some degree of

325
00:32:31,280 --> 00:32:36,960
disclosure yeah absolutely I imagine as a designer this notion of disclosure is can be a very

326
00:32:36,960 --> 00:32:43,120
nuanced thing what does that even mean yeah yeah and it's all context dependent and it's all

327
00:32:43,120 --> 00:32:50,720
norm dependent as well because if you were to look into the future and say are people more comfortable

328
00:32:50,720 --> 00:32:55,520
like I mean look at airports for example people are walking through just using face ID using the

329
00:32:55,520 --> 00:33:01,440
clear system and a few years ago I think if you ask people would you feel comfortable doing that

330
00:33:01,440 --> 00:33:07,120
most people would say no I don't feel comfortable doing that I don't want that um and so I think in

331
00:33:07,120 --> 00:33:12,080
this space because it's really fluid and new norms are being established and things are being

332
00:33:12,080 --> 00:33:18,400
tested out uh we have to be on top of how people are feeling and thinking about these technologies

333
00:33:18,400 --> 00:33:23,840
where so that we understand where some disclosure needs to happen and where things don't and um

334
00:33:23,840 --> 00:33:31,040
and in a lot of cases you almost want to assume disclosure for things that are very consequential

335
00:33:31,040 --> 00:33:37,840
in high stakes um where there is opportunity for deception in the duplex case you have to be

336
00:33:37,840 --> 00:33:44,320
thoughtful about that um and so it's this isn't one where you can say okay you should always

337
00:33:44,320 --> 00:33:50,800
disclose it just depends on the context and so we have this notion of consequential scenarios

338
00:33:50,800 --> 00:33:55,200
where things are you know if there's automated decision making if there are scenarios where there

339
00:33:55,200 --> 00:34:02,880
is um there are high stakes scenarios those ones we think about um in a we just put a little bit

340
00:34:02,880 --> 00:34:07,840
more due diligence over those and start to be more thoughtful about those and then we have you

341
00:34:07,840 --> 00:34:13,120
know other types of scenarios which are um more systems oriented and here's some things that are

342
00:34:13,120 --> 00:34:18,320
operationally oriented and they end up having different types of scenarios but we haven't been

343
00:34:18,320 --> 00:34:24,640
able to create uh here's the exact way you do every single you know you approach it in every single

344
00:34:24,640 --> 00:34:30,720
way so it is so super context dependent and expectation dependent um maybe after a while you get

345
00:34:30,720 --> 00:34:37,520
used to your nest thermostat and you're fine with the way it's operating right and so um so I

346
00:34:37,520 --> 00:34:42,080
don't know these social norms are interesting because they are someone will go and establish

347
00:34:42,080 --> 00:34:47,280
something or they'll test the waters you know google glass tested the waters and um that was

348
00:34:47,280 --> 00:34:52,320
a cultural response right people responded and said I don't want to be surveilled I don't want

349
00:34:52,320 --> 00:34:57,680
I want to be able to go to a bar and get a drink and I'll have someone recording me and um and so

350
00:34:57,680 --> 00:35:03,120
I think we have to understand where society is relative to what the technologies are that we're

351
00:35:03,120 --> 00:35:07,600
inserting into them and so again it comes back to are we listening to users are we just putting

352
00:35:07,600 --> 00:35:14,960
tech out there I think we have to really start listening to users um my group has a fairly large

353
00:35:14,960 --> 00:35:20,720
research component to it and we spend a lot of time talking to people especially in the places

354
00:35:20,720 --> 00:35:25,120
where we're going to be putting some tech and I'm understanding what it's going to do to the dynamic

355
00:35:25,120 --> 00:35:32,960
and how they're how they're reacting to it yeah it strikes me that uh and maybe it's kind of

356
00:35:32,960 --> 00:35:38,400
the engineer background in me this looking for like a framework uh you know a flow chart for how

357
00:35:38,400 --> 00:35:45,520
we can approach this problem and uh I need to embrace more of the designer that's like well every

358
00:35:45,520 --> 00:35:51,120
you know product every situation is different and it's more about a principled approach as opposed

359
00:35:51,120 --> 00:35:58,800
to a process absolutely it's more about a principled and intentional approach so what what we're

360
00:35:58,800 --> 00:36:03,920
talking about is everything that you're choosing are you intentional about that choice and are

361
00:36:03,920 --> 00:36:08,400
you very thoughtful about things like defaults because we know that people don't change them

362
00:36:08,400 --> 00:36:13,280
and so how do you think about every single design choice in being principled and in very

363
00:36:13,280 --> 00:36:18,160
intense intentional and evidence driven and so we push this on our teams and I think some of our

364
00:36:18,160 --> 00:36:22,400
teams maybe don't enjoy being with us sometimes as a result but we say look we're going to give you

365
00:36:22,400 --> 00:36:27,920
some recommendations that are going to be principled intentional and evidence driven and we want to

366
00:36:27,920 --> 00:36:33,680
hear back from you if you don't agree on your evidence and why you're saying this is a good or

367
00:36:33,680 --> 00:36:39,280
bad idea um and that's that's the way you have to operate right now because it is so context-driven

368
00:36:39,920 --> 00:36:44,480
I wonder if you can talk through some examples of how you know humans enter design and all these

369
00:36:44,480 --> 00:36:49,120
things come together in the context of kind of concrete problems that you've looked at yeah I

370
00:36:49,120 --> 00:36:53,760
was thinking about this because a lot of the work that we do is um fairly confidential but

371
00:36:53,760 --> 00:36:59,360
there's one that I can touch on which was shared at build earlier this year and that was a meeting

372
00:36:59,360 --> 00:37:03,120
room device and I don't know if you remember this but there's a meeting room device that we're

373
00:37:03,120 --> 00:37:09,520
working on that um recognizes who's in the room and um does transcription of that meeting

374
00:37:10,160 --> 00:37:16,880
and um it to me as someone who is a manager I love the idea of having a room a device in the room

375
00:37:16,880 --> 00:37:22,480
the captures action items and who was here and what was said and uh and so we started looking at

376
00:37:22,480 --> 00:37:28,240
this and we said okay well let's look at different types of meetings and people and let's look at

377
00:37:28,240 --> 00:37:34,560
categories of people that um this might affect differently and so how do you think about

378
00:37:34,560 --> 00:37:40,880
introverts in a meeting how do you think about women and minorities because there are subtle

379
00:37:40,880 --> 00:37:47,280
dynamics that are happening in meetings that um make some of these relationships they can

380
00:37:47,280 --> 00:37:53,520
um reinforce certain types of stereotypes or relationships and so we started um interviewing

381
00:37:54,080 --> 00:38:00,640
people in the context of this sort of meeting room device and um and this is research that is

382
00:38:00,640 --> 00:38:08,240
pretty well uh it's well recognized it's not um it's not novel research but but um it reinforced

383
00:38:08,240 --> 00:38:13,920
the fact that when you start putting in things that will monitor anyone that's in a room certain

384
00:38:13,920 --> 00:38:19,680
categories of people behave differently and you see larger discrepancies um and impact

385
00:38:19,680 --> 00:38:25,520
with women minorities more junior people and so we said wow this is really interesting because

386
00:38:25,520 --> 00:38:30,240
as soon as you put a recording device in a room it's going to subtly shift the dynamic where some

387
00:38:30,240 --> 00:38:35,120
people might talk less or some people might feel like they're observed or depending on if there's a

388
00:38:35,120 --> 00:38:39,200
manager in the room and there's a device in a room they're going to behave differently

389
00:38:39,200 --> 00:38:43,680
and does that result in a good meeting or a bad one we're not sure but that will affect the dynamic

390
00:38:43,680 --> 00:38:48,160
and so then we took a lot of this research and we went back to the product team and said well how do

391
00:38:48,160 --> 00:38:55,120
we now design this in such a way that we design with privacy first in mind and um make users

392
00:38:55,120 --> 00:39:00,000
feel like they're empowered to opt into it and so we've had discussions like that especially around

393
00:39:00,000 --> 00:39:06,480
these types of um devices where we've seen big impact to how people behave but it's not like a hard

394
00:39:06,480 --> 00:39:10,480
guideline or it's not really a hard set of rules around what you have to do but you know because

395
00:39:10,480 --> 00:39:15,840
all meetings are different right you've brainstorming ones that are more of a fluid ideas you don't

396
00:39:15,840 --> 00:39:20,000
really care who said what it's about getting all the ideas out you have ones where you're shipping

397
00:39:20,000 --> 00:39:24,000
something important and you want to know who said what because they're clear action items that go

398
00:39:24,000 --> 00:39:30,080
with them and so um trying to create a system that works with so many different nuanced

399
00:39:30,080 --> 00:39:36,160
conversations and um different scenarios is not an easy one so what we do is we we all run

400
00:39:36,160 --> 00:39:41,360
alongside with the product team and while they're engineering you know they're developing their work

401
00:39:41,360 --> 00:39:46,720
we will take the research where that we've gathered and we'll create alternatives for them at

402
00:39:46,720 --> 00:39:52,160
the same time so that we can run alongside with them we can say hey here's option A, B, C, D, and E

403
00:39:52,160 --> 00:39:56,080
let's play with these and maybe we come up with a version that mixes them all together

404
00:39:56,640 --> 00:40:02,160
but um but it gives them options to think about because again it comes back to well I might not

405
00:40:02,160 --> 00:40:08,240
have time to think about all of this so how do we empower people with ideas and um and concrete

406
00:40:08,240 --> 00:40:18,160
things to to look at yeah I think that examples a great example of the complexity or um maybe

407
00:40:18,160 --> 00:40:24,320
complexity is not the right word but the the idea that your initial reaction might be like the

408
00:40:24,320 --> 00:40:28,240
exact opposite of what you need to do yeah as you were saying this I was like oh just hide the

409
00:40:28,240 --> 00:40:32,400
thing so no one knows it's there it doesn't change the dynamic it's like that's exactly wrong

410
00:40:33,360 --> 00:40:38,640
you don't want to do that don't hide it right right yeah and maybe that's another piece I'm sorry

411
00:40:38,640 --> 00:40:44,800
interrupt that but but one of the things I've noticed is the our initial reaction is often wrong

412
00:40:44,800 --> 00:40:50,640
and so how do we hold it at the same time that we give ourselves a space to explore other things

413
00:40:50,640 --> 00:40:55,840
and then keep an open mind and say okay I have to adjust and change because hiding it would

414
00:40:55,840 --> 00:41:02,880
absolutely be an interesting option but then you have so many issues with red right um but again

415
00:41:02,880 --> 00:41:06,720
like it is about being able to have like an open mindset and be able to challenge yourself in

416
00:41:06,720 --> 00:41:13,920
the space do you have a sense for where you know if we kind of buy into the idea that folks that

417
00:41:13,920 --> 00:41:20,640
are working with AI need to be more thoughtful and more intentional and and maybe incorporate more

418
00:41:20,640 --> 00:41:30,000
this into more this design thinking element to their work do you have a sense for where this you

419
00:41:30,000 --> 00:41:37,520
know does or should or needs to live within a customer organization yeah I think it actually

420
00:41:37,520 --> 00:41:40,960
and this is a terrible answer I think it needs to live everywhere in some ways because

421
00:41:42,400 --> 00:41:47,440
what one thing that we're noticing is we have with corporate level things that happen we have

422
00:41:47,440 --> 00:41:53,600
an you know an ether board it's our it's an advisory board that looks at AI technologies and

423
00:41:53,600 --> 00:41:59,600
advises and that's at a corporate level that's a really interesting way of approaching it

424
00:41:59,600 --> 00:42:04,880
but it can't live alone and so the thing that we have learned is that if we pair it with groups

425
00:42:04,880 --> 00:42:10,880
like mine that sit in the engineering context that are able to translate principles concepts guidelines

426
00:42:10,880 --> 00:42:17,920
into practice that sort of partnership has been really powerful because we can take those principles

427
00:42:17,920 --> 00:42:22,080
and say well here's where it really worked and here's where it kind of didn't work and then we can

428
00:42:22,080 --> 00:42:27,920
also find issues and say well we're grappling with this issue that you guys hadn't thought about

429
00:42:27,920 --> 00:42:32,160
how do you think about this and can we create a broader principle around it so I think that there's

430
00:42:32,160 --> 00:42:36,800
this like strong cycle of feedback that happens if you have something at the corporate level

431
00:42:36,800 --> 00:42:41,520
or you establish just what your values are what are our guidelines and what are our approaches

432
00:42:41,520 --> 00:42:45,120
but then at the engineering context you have a team that can problem solve and apply

433
00:42:45,120 --> 00:42:49,440
and then you can create a really tight feedback loop between that engineering team and your

434
00:42:49,440 --> 00:42:54,880
corporate team so that you're continually reinforcing each other because the worst thing would be

435
00:42:54,880 --> 00:42:59,520
just to have a corporate level thing and just be PR speak right you don't want that right right

436
00:42:59,520 --> 00:43:02,480
and the worst thing would also be just to have it in the engineering level because then you would

437
00:43:02,480 --> 00:43:09,840
have a very distributed mechanism of doing something may not cohesibly ladder up to your

438
00:43:09,840 --> 00:43:15,120
principles and so I think you kind of need both and to have them work off each other to really

439
00:43:15,120 --> 00:43:21,280
have something effective and maybe there's other things as well but so far this has been a really

440
00:43:21,920 --> 00:43:28,000
productive and iterative experiment that we're doing. Danny pointers come to mind for folks that

441
00:43:28,000 --> 00:43:34,640
want to explore this space more deeply do you have a top three favorite resources or

442
00:43:35,680 --> 00:43:44,720
initial directions well it depends what you want to explore so I was reading the AI now report

443
00:43:44,720 --> 00:43:51,040
the other day it's you know a fairly large report 65 page report around the impact

444
00:43:51,040 --> 00:43:56,560
of AI in different systems different industries and so if you're looking at

445
00:43:56,560 --> 00:44:02,400
getting up to speed on well what areas is AI going to impact I would start with some of these

446
00:44:02,400 --> 00:44:10,480
types of groups because I found that they are super thoughtful and how they're going into each

447
00:44:10,480 --> 00:44:16,160
space and understanding each space and then bubbling up some of the scenarios so if you're

448
00:44:16,160 --> 00:44:21,840
thinking about AI from a you know how is it impacting those types of things that are really

449
00:44:21,840 --> 00:44:28,480
interesting on the engineering side I actually spent a lot of time on a few Facebook groups where

450
00:44:28,480 --> 00:44:33,360
they have there's some big AI groups in Facebook and they're always sharing here's the latest here's

451
00:44:33,360 --> 00:44:38,320
what's going on I've tried this technique and so that keeps me kind of up to speed on some of those

452
00:44:38,320 --> 00:44:44,320
that are happening and also archive just to see what research is being published the design side

453
00:44:44,320 --> 00:44:51,440
I'm sort of mixed I mean I haven't really found a strong spot yet I wish I had like something

454
00:44:51,440 --> 00:44:57,920
my back pocket where I could just refer to but the thing that maybe has been on the theory side

455
00:44:57,920 --> 00:45:05,440
that has been super interesting is to go back to a few set a few people that have made commentaries

456
00:45:05,440 --> 00:45:12,000
just around sustainable design so I refer back to Wendell Berry quite a bit the agriculturalist

457
00:45:12,000 --> 00:45:20,960
and poet actually who has really introspected how agriculture could be reframed Ursula Franklin is

458
00:45:20,960 --> 00:45:28,720
also a commentary from Canada she was she did a lot of podcast or radio broadcast a long time ago

459
00:45:28,720 --> 00:45:34,800
and she has a whole series around technology and its societal impact and if you replace a few of

460
00:45:34,800 --> 00:45:39,200
those words and put in some of our new age words it would still hold true and so I think there's

461
00:45:39,200 --> 00:45:44,320
a lot of theory out there but not a lot of like here's really great examples of what you can do

462
00:45:44,320 --> 00:45:50,640
because we're all still feeling out the space and we haven't found a perfect patterns yet that you

463
00:45:50,640 --> 00:45:56,160
can democratize and share out broadly we're very thanks so much for taking the time to chat with us

464
00:45:56,160 --> 00:46:04,880
about this stuff is a really interesting space and one that I enjoy coming back to periodically

465
00:46:04,880 --> 00:46:10,880
and I personally believe that there's you know this intersection of AI and design is one that

466
00:46:10,880 --> 00:46:20,960
is as wide open and should and will be further developed and I'm kind of looking forward to

467
00:46:20,960 --> 00:46:25,280
keeping an eye on it and I appreciate you taking the time to chat with me about it thank you so

468
00:46:25,280 --> 00:46:33,680
much Sam was wonderful talking to you thank you all right everyone that's our show for today

469
00:46:33,680 --> 00:46:40,400
for more information on Mira or any of the topics covered in this show visit twimmelai.com slash

470
00:46:40,400 --> 00:46:47,200
talk slash two three three to follow along with or catch up on our AI for the benefit of society

471
00:46:47,200 --> 00:46:55,360
series visit twimmelai.com slash AI for society as always thanks so much for listening and catch you

472
00:46:55,360 --> 00:47:10,320
next time

