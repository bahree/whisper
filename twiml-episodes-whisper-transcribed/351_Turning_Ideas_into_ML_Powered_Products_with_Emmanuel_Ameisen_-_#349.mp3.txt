Welcome to the Twimal AI Podcast.
I'm your host, Sam Charrington.
Hey, what's up everyone?
I am super excited to bring you this interview with my friend, Emmanuel Amazon, whose
new book, Building Machine Learning Powered Applications, just hit bookstores.
In our conversation, as in the book, we explore how to approach machine learning projects
systematically, from idea all the way to working product, including formulating your problem
and creating a plan, building a working pipeline and initial data set, evolving your models,
and deploying and monitoring them.
These are the same concepts covered in a new study group we're launching starting this
Saturday, February 22nd, around the AI Enterprise Workflow Specialization on Coursera.
If you're an aspiring or practicing data scientist or ML developer, and you want to level
up on the broader set of skills required to deliver models and business settings, I really
encourage you to join us.
To learn more about the study group, visit twimalai.com slash AIEW, where you can catch a recorded
session I held with Ray Lopez, the instructor for the courses.
There, you'll also find instructions for joining the study group, which is totally free,
and how you can get a free month of access to Coursera to boot.
Once again, the website is twimalai.com slash AIEW.
For more on the interview and a link to the book, visit the show notes page at twimalai.com
slash talk slash 349.
And now on to the show.
Alright everyone, I am on the line with Emanuel Amazing Emanuel is a machine learning engineer
at Stripe.
Emanuel, welcome finally, I should say, to the twimalai podcast.
How are you my friend?
Thanks for having me.
I'm great.
How are you?
I am doing well.
I'm doing well.
So Emanuel and I have known each other for at least a couple years now, maybe more.
When you met, when you were at insight, you were responsible for the insight data science
AI program.
What was your official responsibility?
Because I've also, I've interviewed Ross on the show as well with insight.
Yeah.
What was your official role there?
So I had a role very similar to Ross's, Ross was my equivalent in New York.
So I was leading the AI program there.
So we have a sort of professional education programs, fellowships in many different domains,
state science, state engineering, and we have one in AI and so I was leading that one.
And so usually we start these interviews by having folks share a little bit about their
journey.
Why don't you tell us how you got to Stripe and perhaps more importantly, how you got
to being a published author.
Congratulations.
Yeah, happy to.
So I started off, so I think like many people in this field, my fashion firm, I started
with Jeff Hinton's Coursera classes way back in the day.
Okay.
I feel like for most reports, either Jeff Hinton or Andrew Eng that got them started.
Team Andrew here, nothing against Jeff.
And then after that, I started my career, my professional career, I did data science at
the start up in the Bay Area, which got acquired by zip car later.
I spent two years there and actually after that, I joined insight.
So after having been a data scientist, I joined a role that was much more about professional
education and mentorship.
And so for a couple years at insight, I mentored in aggregate, it was over a hundred fellows
that were PhDs and engineers that wanted to transition and get a job in like the field
of machine learning.
So that was amazing and through my work there, I learned a lot about what it takes to transition
and what it takes to build successful ML projects.
A lot of that is in the book you mentioned.
After a couple of years there, I went to go back towards something more of a nice C role
for me, more of an individual contributor.
And so I went at Stripe because it sort of had the perfect blend of what I was looking
for in a role, which was it blends very heavy and challenging machine learning with sort
of very heavy engineering requirements, which I think is where the field is going in
general.
And so I wanted to do more of that.
We haven't mentioned the title of the book yet, but it is building machine learning powered
applications going from idea to product.
When did the book become available?
The book became available last week.
Nice.
Nice.
And so I have one of the first copies of it right here in my hand and it was signed by
you.
Thank you very much.
So the book has got this subtitle going from idea to product.
Is it a conceptual book, a technical book?
Yeah.
That's a good question.
The title comes from the desire scope of the book.
So the desire scope is really to give tools to aspiring engineers and data scientists
to go from sort of either a VM has an idea or you have an idea to you have something in
production that is actually being used by real people.
And it's a bit of a blend of conceptual and technical.
It is technical in a sense that there are many code examples.
There is a set of notebooks that accompany the book and there's actually an entire prototype
application that we build together throughout the book.
And at the end of the book, you can just, it has a GitHub repo.
You can go and try it out.
But it's also conceptual in a sense that a lot of these topics are more about how you
frame problems than just like copying code off of Stack Overflow.
And so there's interviews with data science leaders that have none of this sort of thing.
There's an entire section about like data ethics and how you think about shipping models
and when you shouldn't, shouldn't.
So it's a bit of a hybrid book in that sense.
Nice.
Nice.
I flipped through it and saw some of my favorite folks in here, Monica Burgatti and Rob
Monroe and I guess we know some of the same people.
Yeah.
Yeah.
Small world.
There's things that I noticed and I haven't gone through them in a lot of detail.
I mentioned to you that like I literally just got this out of the mail room here.
But the structure of the book is that you develop a sample application.
As you mentioned and the sample app is predictive text.
How did you pick that app for context?
Yeah.
That was one of, first of all, that's a really good question because that was one of the
parts that I went over the most times and just changed my mind, changed my mind very many
times about which application should be the running example.
And in fact, I had a conversation with Monica Burgatti where I pitched her on one of my
initial ideas and she told me it was a terrible idea and I should definitely not what wasn't.
I wanted to do something that was like it would listen to like politicians speeches and
then compare with how they vote and tell you whether like what they were saying in their
speeches sort of aligned with how they were actually voting.
Like a fact checker kind of thing.
Yeah.
Like some sort of automatic fact checker.
It would have been timely.
Right.
It would have been timely.
That's what I was thinking.
But it would have also been pretty hard.
The sort of reconciliation of what's true and like what's not true very quickly gets
into the realm of opinion.
And if you add to like the errors of a machine learning model to the nuanced worldview,
it's like definitely one of those examples of apps that could do more harm than good.
So Monica talked to me off the ledge on that one, but what were some of your other ideas?
Well, I consider doing initially sort of computer vision examples because does are always
the more striking to necessarily be newer folks to the field like it's, you know, an image
is like a very powerful example, but I was about 1000 or it's exactly, but I felt like
they were sort of overused and most amount tutorials are some sort of that computer vision
thing nowadays.
So I want to do something different and I want to do tabular data because that's what
I think what most people do in their day to day for most companies.
But I felt like there was less room for a standalone product there, like sort of like bring
your own tabular data has less less of a ring to it than like bring your own writing.
So I ended up settling on an LP and then I wanted to do something that was most ML products
aren't just one model.
They're not just, you know, like you have a model that solves your use case perfectly.
And then you just chip it.
They're usually a combination of sort of heuristics and rules and models and engineering work.
And so I wanted to probably then reflect that.
And when I was thinking of products that did that today, sort of writing and assisting
people to write better is a crucial example of that where you have you can check for grammar
and that's just rules or you can check for vocabulary or for a variety of things and then
you can also help them improve their style and that's more something that you can learn
with them out.
So it was a nice blend that sort of reflects what happens in the real world, I think.
So does that mean that somewhere in this book there are lots of rejects?
No.
We've chosen not to go down that path, but there could be.
The book starts with very simple, so like instead of regular expressions, it's simple
word counts.
Like, oh, how many adverbs are you using?
Are you using?
And a little too much for that sort of stuff.
And so what's the overall kind of path or structure through the book?
And kind of more importantly, what does it say about the way that you think folks need
to approach these kinds of projects?
Yeah.
The book is broadly separated to four stages.
I think generally makes sense for most ML projects.
And I think a lot of time people focus a lot on training models and then you talk to
experienced data scientists and you hear, right, oh, 95% of the job is they're looking
at the data and shipping the model, not really training the model.
And so this book purposely sort of almost ignores training models, it just assumes that
you can figure that part out with the really good courses around.
And so the four approaches are sort of, or the four parts are going from whatever your
goal is, your product, what your company's doing, what you want to do to an ML approach
into a plan for that ML approach, because I think at insight and as of course, I've seen
just many projects fail just because it's the wrong ML approach.
And if you had just chosen a slightly different approach, you'd be in a much better spot.
The second step is sort of building your MVP and that's something that was definitely
the motto at insight.
And I think that insight applied very well was sort of encouraging people to start extremely
simple and build a full project before they go diving down the rabbit hole of research.
The third part is I think one of the ones where I sat down with the most fellows over
my time at insight, which is like, how do you debug models more often like, if your model
is either not working or if it's working, but the performance isn't sufficient, how do
you know what you should do next?
How can you sort of take a deep dive into what your model is doing, what your data looks
like to actually decide what you do in your next iteration cycle?
And then the fourth step is sort of like deployment, monitoring and the concerns that come
with showing the real world to a model and a model to the real world.
Yeah, I am really appreciating all of the focus that kind of this real world ML and AI has
been getting over the, I don't know if I would say past year, past couple of years.
I mean, it's something that we've spent a lot of time focusing on and led us to produce
the TwilmoCon AI platforms conference last week to kind of talk to how folks in real
organizations are tackling these broader problems.
But even just over the weekend on Twitter, like I'm seeing tweets all the time, like, hey,
it's not just about the model, it's not just about the model anymore.
And there seems to be a kind of growing recognition that, you know, not so much recognition, but
more really appreciation that that's the case and that kind of the broader workflow that
takes you from an idea or a problem identification to getting a solution in production is, you
know, multifaceted and involves much more than just training up a model.
Right.
And to be fair, right, it used to be that just training up a model was pretty hard and
maybe you need a team of people that understood sort of the internals deeply.
But now, because the tooling has evolved so much, it actually is the case that the tooling
so good and the courses are so good to train models that that becomes relatively simpler
than the rest.
Right.
Right.
Yeah, we're actually launching a study group in just a couple of weeks now.
We do these study groups as part of our community where we'll do online courses together and
we've done a bunch of fast AI courses and Stanford courses.
But we're doing one starting with kind of an intro webinar on the 15th of February and
then continuing on after that on this AI enterprise workflow course.
This really interesting, unlike any kind of formal, this is on Coursera, unlike any formal
courses I've seen, this one touches on a bunch of the things that you cover in the book.
So, you know, how do you structure data collection?
How do you kind of visualize and analyze your data explored in an exploratory mode and
kind of test hypotheses?
How do you identify data biases in the process of collecting your data?
You mentioned using multiple models, you know, how do you use multiple models together
with heuristics in order to build a solution and then, you know, unit testing?
Like, when do you see that come up in a machine learning course, I'm both never.
And then monitoring a model in production, deploying models with microservices.
So, I'm really looking forward to that and I'll be putting a link into our show notes.
So, anyone else that wants to take this course with me can do so.
But it sounds like you are also of the belief that, you know, this is where the field needs
to go in terms of actually getting value out of machine learning, kind of thinking about
it more holistically.
Yeah, I think so.
I think researches like the class you described are really some of the most valuable classes
or lessons that everyone can learn right now because this comes from a couple of things
which is ad zip code, ads drive, and to some extent, and insight as well.
You notice that the people that are able to contribute the fastest are sort of the ones
that have the appreciation for the whole workflow.
In fact, a lot of the times you'll see companies sort of be scope out projects for people
that are like maybe interning, they're only here for a few months, and those projects
are like just training the model, right?
It's like they've done everything.
They've like thought about the product, they've decided that for this product, this is
the model we need, this is the data we have, they've prepared the data, they've decided
how they're going to serve the model, and they're like, hey, you know, you have, you're
here only for eight weeks or something, here's like a model training.
And so while that's, you know, a fun project, it turns out that if you want to have a meaningful
impact doing all of these other parts is what's going to be most helpful to your
career at a larger company, or even if you're building your own startup, right, to just
actually getting it out the door.
Yeah, I love that you're only 45 pages into this book before you're into the build your
own end-to-end or build your first end-to-end pipeline chapter.
And that chapter in the middle of that chapter is like testing.
Yeah.
So there's, at the end of the first chapter, I'll also introduce you with Monica Regatti,
and she has this great concept that she talks about, she talks about the impact bottleneck.
And sometimes, one of the examples she gives, she says like, well, sometimes your ML might
be perfect, but your product is still dead, you're still dead in the water, because you've
sort of, you know, kind of misunderstood a fundamental need of like how your users would
actually interact with it.
And this is more of maybe like a general, you know, software engineering, like first you
should talk to users before you build it, but it's also a good ML tip to say like, well,
you should, as quickly as possible, get to the point where you can show the like UI to
a friend or a user, have them try it, and then it gives out results.
Even if you don't have a good model, even if it's a heuristic, just because then you might
notice that like, oh, they're using it in a completely different way than you thought.
And that's super complicated model that you thought you needed.
You actually don't need it at all.
You need something else.
Yeah, that's kind of applying the lean startup type of approach, which I don't even
know if it deserves a net, or if it has a net, you know, we need to call it by a name
anymore.
It's kind of how we do things with MVPs, but it's kind of applying that approach to flesh
out some of the, you know, where it's just a bad idea.
Exactly.
And I think that approach is like more and more valuable as the iteration time goes up,
right?
So like, if you don't show it as someone, but it takes you, you know, 50 minutes to build,
you've lost 50 minutes.
That's okay.
So if you don't show it as someone and you need to train a model and get the data and
that whole process is going to take you two months, then you've lost a lot of time by
not doing it.
You know, what I'm curious about is there's, you know, the steps that you need to do to
kind of build this sample app.
And then there's kind of the broader, you know, things that you need to think about to
apply this methodology to your own problems.
Right.
So things like walking folks through building the end pipeline for this app or acquiring
a data set.
Like, how do you make sure it's tangible enough that, you know, you're moving, you're making
progress on your example application, but also broad enough that they can take it and
run with it.
Yeah.
I think that's a, that's a really great question.
It's really hard to think about how you balance that.
In fact, the reason, a big part of the reason for having a sample app is to hold myself
accountable so that my advice would actually be practical and the methods that I gave were
actually true.
Right.
If I, if I tell you something that's actually not going to work, then I'm going to have
a hard time demonstrating to you that it works on this toy example.
Yeah.
And so most chapters are structured in a way where I try to give broad, like, this is
how you generally look for a data set and try to explore it.
Like, for example, you generally, like, if you have a current approach right, because
you've built your plan in the previous chapter and you're saying, well, like, we're going
to, we're going to use this current approach, trying to think of problems that have sort
of the same kind of approach, even if they're in a completely different domain.
So maybe you're doing, you know, like, some, like, reaction prediction for molecules,
but maybe that looks just like text translation in a way, right?
You're predicting a sequence because in other sequence, you can try to find that form,
that first data set, the like translation data set, see if your approach works on that
and then change to your current data set.
That's sort of like a general, how you do it.
And then I illustrate it as an example with, okay, well, for this ML editor thing, this
is what, like, these are the data sets we're actually going to use.
So for most concepts, I try to do both.
The couple of examples or chapters that we've talked about, like, what are the, the broad
principles that folks need to be thinking about when they're building their first pipelines,
when they're acquiring their initial data sets?
Yeah.
So I think we skipped a little bit over the first section, but I think there's, there's
a broad principle in the first section of, like, going for a product to an ML approach
that I generally find really valuable and find that, I don't know, I think everybody
should do.
Kind of sets the tone for the rest of it, it sounds like, yeah.
Which is that for, for the same product goal, you have, again, many ways to do it, many
ways to tackle that problem using machine learning.
So an example I use is, let's say that you're, you know, a retailer and you have an online
catalog of items and you want to help people when they're, when they're typing something
in the search bar to find the category or the types of items that, you know, they, they
would want to buy.
One way to do this, and maybe the simplest way to think about it is you're like, oh, well,
somebody's writing something in the search bar, I'm going to try to, like, autocomplete
the rest of what they're going to say, right?
So if they type hands, maybe you autocomplete, like, handbag or something, right?
A slightly, maybe like, different approach is they type something and you try to identify
which words in their query are relevant to products.
So if they're like, I want, you know, like, a handbag of this brand, like, you try to
find the name of the brand.
And then a third approach that's even simpler is they type anything in that bar and all
you do is you try to like classify it and like, is it about handbags, is it about jeans,
is it about, you know, shoes or something else?
And those three approaches, all would have a slightly different UI, right?
You would build sort of the way you show results and the way you show suggestions slightly
differently.
But they all are basically each successive approach is like an order of magnitude easier
to do than the one before, especially if you don't have much data, right?
If you don't have much data, that final approach building a classifier is something where
you can label data for a few hours and you'll have a classifier that's decent.
If you want to do the sort of like, not a distraction part, that's going to take you
maybe a couple of days, but you can probably get something that's pretty good.
If you want to go do the full like language model approach of predicting the next tokens,
that's going to require a lot of data and you're also going to have a lot more variance
where like, sometimes you might predict some crazy things and unless you have the engineering
resources to add that filtering layer on top, that'll make it very hard to shift.
And so I think like a question that generally people should ask is like, what is the absolute
simplest model in that bucket of model and I give sort of like a hierarchy of models?
What is the simplest model that you could use that could solve what you're currently
trying to do?
And then if that model's too simple and it's not good enough, it's fine.
You can improve on it, but you'll have spent you know, a couple hours building it instead
of a couple months being building a model that doesn't work.
And so with that in mind, we go into building the pipeline and acquiring the data set, what
are some of the broad principles there?
Yeah, so I think there's this idea that I would say for any data project that you tackle,
any new project, you should spend like a couple hours looking at the data.
And the couple hours is definitely the minimum time you should spend looking at the day.
You could spend a lot of more time.
That's fine, but you should never spend less than a few hours looking at the data.
Looking at the data means a few things.
And I found that in my experience of insight, often people think about it in terms of aggregates,
right?
They're like, okay, well, now I'm looking at this database of like texts, what's the
average length of a sentence, you know, how many different words are there.
And that's fine, and that's something that you should do, both to find errors or things
that surprise you.
You're like, oh, I was looking at like a database of tweets and somehow like all the tweets
are one word long, just probably something wrong.
But you should also actually look at individual examples, and that's something that Rob
Monroe actually talks about a lot, where a lot of these individual examples will be the
source for which model you end up choosing, right?
So you have your product goal on one side, you've said like, okay, well, maybe like a classifier
is a single thing I can start with.
But then looking at the data will give you your initial set of like features or types
of model that you think could reasonably capture what you're trying to identify in this dataset.
And so in the chapter, I go into, well, how can you look at individual examples in a
dataset?
That's hard, right?
If you have a dataset with like 10,000 examples or 10 million, you're not going to look
at all of them.
And so there's some methods where you can basically use some NLP approaches or some
approaches from other domains to embed your data.
And then once you have this sort of like map of all your data points, you look into each
specific sub area and you're like, oh, like this sub area, you know, for me, for the
example, it was questions on Stack Overflow.
So this area of questions, like they all seem to be about, you know, the English language.
And so they use a lot of like maybe complex words and they have like longer sentences.
Oh, this area, you know, is all about like non-native speakers, that sort of stuff.
And so then it helps you like build features where it's like, okay, well, how can I identify
a good question in this area or in that area?
Do you have a sense for how you know when you have done enough looking at your data in
aggregate or as individual items that you're, you know, you're ready to move on?
Is there a feeling there or a sense or a checklist or something that beyond just, hey, I've,
you know, looking at my watch here, the two hours is up, I can move on to the fun stuff.
Like what, what should you have taken away from that experience?
Yeah, that's something that I think obviously is going to depend on your data sense.
So you're right.
Sometimes it's going to be two hours.
Sometimes it's maybe going to be two days.
Sometimes you're going to realize that something's very wrong and you, you shouldn't move
on, right?
If you realize that something's off, you should go back to the drawing board and sort of
like get another data set or look at why your data is looking all weird.
But in the case where you are ready to move on to a model, I'd say that that's when you
have a strong hypothesis about how your model will actually do its work.
So you know, we've talked about like the retailer that like you type queries and then it tells
you like, oh, is it a handbag?
Is it like something else?
Well, if you look at a bunch of examples and you realize that really there's, you know,
only two or three ways that people ever say handbag or jeans that the vocabulary is like
pretty simple.
That sentences are usually pretty short.
You can say like, okay, well, for this approach, like, you know, sort of like a simple bag
of words with like word counts will work because I'm reasonably confident that like all
I need to know is like what, which of these three words is in the sentence.
More generally, I think what I'm trying to say is when you build your first model, you
should already have the part goal, that section one.
And then a hypothesis about how your model, like, what will make your model succeed?
That's like the part two of looking at the data set because then once you've trained
your model, what you want to do is you want to check your results against your assumption.
You want to say like, okay, well, I thought that like because sentences are short, you know,
the model would be easily able to pick up on different words, but it turns out that
it's not, or it turns out that it's not performing on these.
And then you can go back to the data, right, and say like, okay, well, let's look at the
examples that my model got wrong and like, why did it get these wrong?
And then make a new hypothesis.
And that's the fastest way and generally the most, the best way to iterate because it'll
actually let you understand how your model is working, which once you're ready to deploy
it, you'd much rather be like, oh, well, I'm pretty sure that this is how my model's
making decisions, rather than like, I trained a model, I got a really high score.
I hope everything goes well.
Do you talk about explainability in the context of models?
That's something that is getting a lot of attention for folks that are, you know, particularly
working in business or enterprise types of environments.
Yeah.
This is a topic that it gets here.
A lot of attention, a lot of debate.
There's generally a few ways to look at explainability.
I say there's a lot of attention.
And I'm asking primarily from the perspective of having a sense of what your model is doing
so that you can better debug it.
Yeah.
So there's two parts to that.
One part is to have a sense and to debug it, you need to look at like the internals of
your model.
And then you can look at like its feature importance, its coefficients, that sort of stuff that
gives you some sense of explainability.
But then there's actually looking at individual data examples or even like multiple data
examples, but trends in results on examples, which I think gives you a much better sense
of explainability.
And what I mean by that is looking at what are the like 10 examples where you're, let's
say your classifier was the most confident that it was class one, but your class where
it was wrong.
What are the ones where it was most confident of the other class?
What are the ones where it was the most unsure, like looking at those examples and seeing
like, oh, you know, it seems like every time there's a question that's 16 sentences long,
the model just doesn't know what's going on, it just gives up.
That gives you one sense of explainability.
And the final sense of explainability that I think a lot of people talk about is black
box explainers.
They're like shop values or lime and we actually use those pretty heavily in the book because
you can use them to power suggestions for your users.
So again, the example of a case study is like something that's going to help you write
better.
So you give it something you wrote, a question you've wrote.
And then we have our model that predicts whether you wrote something basically good or bad
for some definition of good or bad.
And then we use lime to say like, oh, you know, we said that this question was like 60%
good, these are the features that if you were to change them, would push your questions
towards the positive class more, right?
Well, it seems like your question's much too long.
So if you cut it down, actually, like we would have said that it was much better that
sort of stuff.
So you can use explainability to power sort of user-facing suggestions.
Interesting.
Yeah.
Is that done commonly?
Do you see that a lot?
I see that in a few cases.
But I think there's no example is perfect, and so the emulator definitely has a few things
where it's like, well, how wildly applicable is that?
But out of companies that do that sort of stuff that do help people write better, actually
have an interview with Chris Harland, who is from a textio, where they do that for general
job postings and job communication.
And they do mention using sort of similar methods, at least to surface potential features,
because whenever you're doing writing recommendations, the real challenge is that you want your users
to understand them.
It's explainability becomes crucial because if I give you anything and you don't understand
why I'm recommending it to you, then you're not going to use my product.
And so for a subset of ML products, explainability isn't just a bonus.
It's what the product is.
You mentioned Lime for folks that want to learn more about that.
You can check out my seventh interview ever with Carlos Guestrin back in October of 2016.
What are some of the other things that come up from a debugging perspective?
I think from the debugging perspective, we end up going back to the iteration loop conversation
we had earlier, which is one debugging tip that actually Ross gave me, which you interviewed
earlier, and that is used widely on site and elsewhere.
I've seen it use the industry is when your model doesn't work, not just when it's, when
you're not happy with this current score, but when something is not working, you should
cut down your data set to one or two examples and then get your model to work.
Like by working, what I mean is just get your model to train and then output predictions.
They'll be completely random because you're overfitting on a couple examples.
But that alone.
But if you can't do that.
Exactly.
Exactly.
And so there's sort of like that that's helped so many fellows and honestly, like colleagues
at AdFuse companies where like if every training run takes four hours and at the end, you
have like, you know, the shape mismatch or something like it is the most infuriating process
you can go through.
So should you just start there or should you regress there when things aren't working?
I think there's nothing wrong with sort of starting with the Hail Mary of like I'm going
to write my code.
This should work.
And then if it doesn't, then I think the first step is going there.
Like if you've written your first training loop and like something's very wrong, the first
step is like, okay, you know, you go to your first line and you like take X train equals
like X train, period two, like you just take a couple examples and then you run it again
and you try to get that to work.
And in fact, in debugging, I mentioned like three steps.
The first step is that, which I call debugging the wiring, like making sure that data can
go back and forth.
And I think the first step is debugging training performance.
Once you've debugged that first aspect, what you want to see is like, can I over fit
on my data set?
And so if you take a data set of like, you know, 2000 examples, can you train a model
that becomes very good on this data set?
Once more, this model isn't going to be good in production because you've trained it
to just be good on data.
It's already seen.
But again, if you can't do that, you're probably not going to be able to train a model
that learns general things at all if it can't learn local features.
And then finally, you debug generalization.
And so these three successive steps are our steps, and honestly, I think most program
directors and insight, for example, are very used to like helping their fellows walk
through each of these steps successfully.
But I haven't seen as many resources just sort of like outline them.
So if you just follow that recipe, usually you'll debug your models much, much faster.
It's like the hierarchy of like what you should debug in three steps.
What categories of models are covered in the book?
Is it, you mentioned TF IDF, and you also mentioned that you're composing multiple models
are using both kind of traditional Python models,
scikit-learn-ish types of models, or are you doing deep learning as well?
What's the portfolio look like?
That's a, let me think, how many?
We have a bat.
We have three models that are used in the main example.
And then I'd say like probably half a dozen examples of other models as like sort of separate
from the case study.
It's like this is how you would use VGG to extract features from images, for example.
For the main application, the models we use are pretty simple.
It starts with a heuristic, that's not even a model.
So you haven't written a birth version yet?
No, I have not.
I have purposefully stayed away from birth.
I don't have anything against more recent approaches, of course, they're breakthroughs.
But I think one, they're covered enough.
I think if you talk to somebody that's either new to the field or even working in LPN,
you ask them, out of all of the blog posts and research articles on things you've read
recently, how many of them were about deep language models, and they'd probably say 85%.
So I didn't feel like I needed to sort of add my break onto that cathedral.
At the same time, a lot of the advice about building practical applications apply regardless
of the model that you use.
And so I felt that for readers, it was best if I kept with models that were as simple
as possible because I didn't want to just add complexity for the sake of adding complexity.
I think if you were to use birth or something more complicated, you could probably get just
a better performance metric on some of these models.
But to the point of this book, that's something that you do in iteration number four, number
five, number six, number seven.
The book shows you the first three iterations, which is one, you build your select first
sub at a crappy heuristic, number two, you build a simple model, number three, you build
a slightly more complicated model, and number four, actually, and I guess this is spoiler
alert.
But you look at your more complicated model and you realize it's too complicated and you
remove some of the useless features to make, so the final model we use in prototype.
So you wouldn't be following your own advice if you jump right into Burton chapter two.
Exactly.
If somebody wants to write a sequel to this book, I think there's like a lot of rooms to
try, Bert and GPT-2 and other more complicated approaches, but I think, yeah, in general,
even in what would that be called, building more complicated, machine learning pattern
applications?
Yeah, it would be called building applications the third year.
When you join a team at a company, the first year they're doing this, and then if the
team's been around for like 10 years, they're just throwing anything they can at the wall,
they're like, oh, let's try it, Bert, you know, let's try anything.
We didn't cover the feature importance of you spend quite a bit of time on feature importance.
How do you see that coming up?
Yeah, I think in this case, right, it was especially useful because we make suggestions for
users based on the features of our application.
So that application specific as opposed to a general step in the process?
Good question.
The importance is a step for debugging, where especially if you're doing a, sort of if you're
using a model that has many features either because it's a deep learning model and just
does its own feature generation, or you know, you just have like a tabular data set with
thousands of features, looking at feature importance can help you check the assumptions
that we talked about, meaning like you've made assumptions, you said like, well, because
length of a question is important, it'll be an important feature.
And then you look at your features and it's not, you know, it's like the importance is
zero, then your assumption was wrong.
And so as part of your iteration cycle, looking at feature importance is really valuable.
The specific chapter you're talking about, which is using feature importance to make
recommendations, I would say is the only chapter in the whole book that's very, very specific
to the, to the emulator, it's sort of, it was actually reviewers of the books that like
the book is great, but I feel like we should have a deep dive into the, the sort of like
emulator at some point that's like wraps things up.
And so this is, this is the books attempt at that of like, okay, well, let's take everything
together and actually get the emulator to like a, a ready product.
And then the last part of the book goes into deployment and modern, monitoring.
What are the, the key takeaways there?
Yeah.
So there's, there's like three main aspects that are covered there.
One is the, just the, the ethics of deployment and the things that you should think about
when you deploy them on models.
I, this chapter is mostly about resources that I share.
There's actually an excellent, an excellent free or highly book about data ethics that I
linked to in this, in this chapter.
And there's, there's a lot of, there's a big body of work, but I try to just give some,
some aspects that you might want to, want to think about based on recent research.
Then the other two aspects are just what is the engineering work around models, both
as you deploy them and once you've deployed them.
And so a lot of takeaways from the engineering work are for most complicated models.
So I think famously the Google smart reply.
So Google smart replies, you get an email and they suggest those three responses that you
could use.
And not the new version where you press tab and they're just all going to place the one
where I just suggest, sort of, yeah, responses you can just click on.
That model is a relatively complicated model and because of that, it fails on a non-zero
number of emails.
And so before running that model, they have what's called a filtering model, which is a
much simpler model, which the goal of that model is to say, like, should we run our model
or not?
And so I cover like tricks like these tricks, right?
For example, like, can you, when should you decide that it's worthwhile to build a first
model that's like a much simpler one that will save you compute time and save you from
showing ridiculous results to your users if, if like, this input is not suited for your
complex model?
And then the last chapter goes into sort of like CICD and monitoring for ML and like what
you can look at when you're like, maybe testing or when you're putting a new model in production
of a lot of companies that started doing what's called like shadow deployments where like
you put a model into shadow, which means that it's just like a real model except that
you don't actually use what it produces, but it's sort of like a final way to test it.
Yeah.
Nice.
So, you know, I referred back to the Twoma kind of platform's conference and some of
the writing that I've been doing on AI, ML platforms and kind of open source and commercial
products that like enable you to build out these workflows and kind of manage these workflows
for you.
I'm assuming that you're not like building all of this in one of those environments.
How are you like stringing together the pieces that you're doing?
Is it all kind of standard vanilla Python?
Are you building on vanilla Python?
Are you using any particular kind of approach to make this modular or are you worried about
that?
You know, how are you pulling this all these workflow components together?
I mean, there is a world where, you know, in like a few years, something comes out.
It's like the TensorFlow, but just for all of machine learning, maybe it'll be TensorFlow.
And this is all obsolete.
Or if you're just like, well, you could just, you know, like write one line of this to
serve new framework.
And I'm willing to take that risk.
In the current state, I feel like while there's many useful tools, it wouldn't necessarily
be what readers are looking for.
Like they're not necessarily looking to learn how to use Scoopflow or, you know, how to
use Airflow for service scheduling DAGs. So I kept things pretty lightweight where most
of it is in raw Python.
You know, there's simple unit testing, Jupyter notebooks to illustrate concepts.
And then for the serving side, built a simple flask app with some examples of like how
you cache requests.
And that I think serves the purposes of the book.
Well, whenever possible, I've added sort of links to resources where it's like, well,
if you want to know more about how you, for example, build models on device, like you might
want to check out this resource.
But for the actual code examples, again, I wanted to, because the book already has the
tall order of covering all the machine learning, I felt like I went a bit pretty focused.
Cool.
So once should we expect the, you know, volume two second edition, you're next, your
next book after a long vacation.
Yeah, I mean, depending on how well this book goes, it was really, it's actually a really
enjoyable process to write it with O'Reilly.
The process of writing a book, you know, I would recommend to no one that's horrible.
But O'Reilly made it as unhorrible as possible.
So nice.
Awesome.
Well, Emmanuel is great catching up with you.
Congratulations on getting this book published.
We've been chatting about it for a bit, at least conceptually.
You know, that it was something that you're laboring under and I'm super excited to, you
know, see it, have an opportunity to hold it in my hand and chat with you about it on
the show.
Awesome.
Thank you, Sam.
All right.
Thank you.
All right, everyone.
That's our show for today.
To learn more about Emmanuel or his new book, visit twomalai.com slash talk slash 349.
For more information on the AI Enterprise workflow study group, visit twomalai.com slash
AIEW.
Of course, if you like what you hear on the podcast, we would be very grateful if you
subscribe, rate, and review the show on your favorite pod catcher.
All right.
Thanks so much for listening and catch you next time.
