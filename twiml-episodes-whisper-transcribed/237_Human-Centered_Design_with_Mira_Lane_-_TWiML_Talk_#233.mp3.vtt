WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.440
I'm your host Sam Charrington. Today we're excited to present the final episode in our AI

00:34.440 --> 00:39.320
for the Benefit of Society series, in which we're joined by Mira Lane, partner director

00:39.320 --> 00:44.960
for Ethics and Society at Microsoft. Mira and I focus our conversation on the role of

00:44.960 --> 00:50.720
culture and human centered design in AI. We discuss how Mira defines human centered

00:50.720 --> 00:56.920
design, its connections to culture and responsible innovation and how these ideas can be scalably

00:56.920 --> 01:02.800
implemented across large engineering organizations.

01:02.800 --> 01:08.720
Before diving in, I'd like to thank Microsoft once again for their sponsorship of this series.

01:08.720 --> 01:13.280
Microsoft is committed to ensuring the responsible development and use of AI and is empowering

01:13.280 --> 01:18.600
people around the world with this intelligent technology to help solve previously intractable

01:18.600 --> 01:25.960
societal challenges, spanning sustainability, accessibility, and humanitarian action.

01:25.960 --> 01:35.480
Learn more about their plan at Microsoft.ai. Enjoy.

01:35.480 --> 01:40.560
Alright everyone, I am here with Mira Lane. Mira is the partner director of Ethics

01:40.560 --> 01:45.280
and Society at Microsoft. Mira, welcome to this weekend machine learning and AI.

01:45.280 --> 01:50.160
Thank you Sam, nice to meet you. Great to meet you and I'm excited to dive into this

01:50.160 --> 02:00.520
conversation with you. I saw that you are a video artist and technologist by background.

02:00.520 --> 02:03.840
How did you come to your looking away? Is that correct?

02:03.840 --> 02:05.360
No, that's absolutely true.

02:05.360 --> 02:14.120
So I noted that you're a video artist. How did you come to work at the intersection of

02:14.120 --> 02:17.240
Ethics and Society and AI?

02:17.240 --> 02:22.200
For sure. So let me say let me give you a little bit of a background on how I got to this

02:22.200 --> 02:29.520
point. I actually have a mathematics and computer science background from the University

02:29.520 --> 02:36.040
of Waterloo in Canada and so I've had an interesting journey and I've been a developer, a program

02:36.040 --> 02:42.840
manager and designer. And when I think about video art and artificial intelligence, I'll

02:42.840 --> 02:48.200
touch artificial intelligence first and then the video art. But a few years ago I had the

02:48.200 --> 02:54.040
opportunity to take a sabbatical and I do this every few years. I take a little break,

02:54.040 --> 02:58.960
reflect on what I'm doing, retool myself as well.

02:58.960 --> 03:03.240
So I decided to spend three months just doing art. A lot of people take a sabbatical and

03:03.240 --> 03:07.640
they travel but I thought I'm just going to do art for three months and it was luxurious

03:07.640 --> 03:13.680
and very special. But then I also thought I'm going to reflect on career at the same

03:13.680 --> 03:20.800
time. And I was looking at what was happening in a technology space and feeling really unsettled

03:20.800 --> 03:26.760
about where technology was going, how people were talking about it, the way I was seeing

03:26.760 --> 03:33.000
it affect our societies and I thought I want to get deeper into the AI space. And so when

03:33.000 --> 03:38.000
I came back to Microsoft I started poking around the company and said is there a role

03:38.000 --> 03:41.880
in artificial intelligence somewhere in the company and something opened up for me in

03:41.880 --> 03:47.960
our AI and research group where they were looking for a design manager. So I said absolutely

03:47.960 --> 03:56.000
I'll run one of these groups for you. But before I take the role I'm demanding that we

03:56.000 --> 04:01.440
have an ethics component to this work because what they were doing was they were taking

04:01.440 --> 04:06.240
research that was in the AI space and figuring out how do we productize this because at

04:06.240 --> 04:12.360
that point research was getting so close to engineering that we were developing new techniques

04:12.360 --> 04:16.480
and you were actually able to take those to market fairly quickly. And I thought this

04:16.480 --> 04:20.960
is a point where we can start thinking about responsible innovation and let's make that

04:20.960 --> 04:28.000
formalized practice. So me taking the role for the design manager was contingent on us

04:28.000 --> 04:34.960
creating a spot for ethics at the same time. And so backing up a little bit the video

04:34.960 --> 04:39.760
part comes in because I've traditionally been a really analog artist, been a printmaker,

04:39.760 --> 04:46.960
a painter and during my sabbatical I saw some more digitized, like looked at digitizing

04:46.960 --> 04:50.240
some of the techniques that I was playing with on the analog side. I thought well let me

04:50.240 --> 04:55.360
go play in the video space for a while. And so for three months I just like I said I retooled

04:55.360 --> 05:02.480
and I started playing around with different ways of recording, editing and teaching myself

05:02.480 --> 05:07.680
some of these techniques. And one of the goals I set out at the time was well can I get into

05:08.560 --> 05:15.200
a festival you know can I get into a music or a video festival. And so that was one of my goals

05:15.200 --> 05:19.600
at the end of the three months. Can I produce something interesting enough to get admitted

05:19.600 --> 05:24.640
into a festival. And I want a few actually. So so I was super pleased. I'm like okay well that

05:24.640 --> 05:29.920
means I'm I've got something there I need to continue practicing. But that for me opened up a

05:29.920 --> 05:37.360
whole new door. And and one of the things that I did a few years ago also was to explore art

05:37.360 --> 05:44.320
and with AI. And and could we create a little AI system that could mimic my artwork and become

05:44.320 --> 05:50.240
a little co-collaborator with myself. So we can dig into that if you want. But it was a really

05:50.240 --> 05:55.520
interesting journey around can AI actually complement an artist or even replace an artist.

05:56.160 --> 06:01.200
And and so I there's interesting learnings that came out of that experience. Okay interesting

06:01.200 --> 06:07.040
interesting. We're accumulating a nice list of things to to touch on here. Absolutely. Ethics

06:07.040 --> 06:11.360
and your views on that was at the top of my list. But before we got started you mentioned

06:11.360 --> 06:18.640
work that you've been doing exploring culture and the intersection between culture and AI.

06:19.440 --> 06:25.760
I'm curious what that means for you. It's certainly a topic that I hear brought up quite a bit

06:26.720 --> 06:33.360
particularly when I'm talking to folks in enterprises that are trying to adopt AI

06:33.360 --> 06:39.280
technologies. And here all the time oh well one of the biggest things we struggle with is culture.

06:39.280 --> 06:44.080
And so maybe I don't know if that's the right place to start but maybe we'll start there.

06:44.080 --> 06:48.800
What does that mean for you when you think about kind of culture and AI? Yeah no that's a really

06:48.800 --> 06:54.880
good question and I agree that one of the biggest things is culture. And the reason why I say that

06:54.880 --> 07:02.000
is if you look at every computer scientist that's graduating none of us have taken an ethics class

07:02.000 --> 07:07.600
and you look at the impact of our work it is touching the fabric of our society like it is

07:07.600 --> 07:12.800
touching our democracies and our freedoms our civil liberties and those are powerful tools

07:12.800 --> 07:19.600
that we're building yet none of us have gone through an formal ethics course. And so the discipline

07:19.600 --> 07:25.040
is not used to talking about this it's you know a few years ago you're just like oh I'm just

07:25.040 --> 07:30.640
building a tool I'm building an app I'm building a platform that people are using. And we weren't

07:30.640 --> 07:36.480
super introspective about that it wasn't part of the culture. And so when I think about culture

07:36.480 --> 07:43.280
in the AI space because we're building technologies that have scale and power and are building on

07:43.280 --> 07:50.000
top of large amounts of data that empower people to do pretty impressive things this whole question

07:50.000 --> 07:55.600
of culture and asking ourselves well what could go wrong how could this be used who is going to

07:55.600 --> 08:02.080
use it you know directly or indirectly and those are parts of the culture of technology that

08:02.080 --> 08:06.800
I don't think has been formalized it's usually here designers talking about that kind of thing

08:06.800 --> 08:12.720
it's part of humans and our design but even in the humans and our design space it's really about

08:12.720 --> 08:19.760
like what is my ideal user or my ideal customer and not thinking about well how could we exploit

08:19.760 --> 08:25.600
this technology in a way that we hadn't really intended and and we've talked about that from an

08:25.600 --> 08:30.560
engineering context the way we do you know threat modeling how could a system be attacked how do

08:30.560 --> 08:35.360
you think about denial of service attacks things like that but we don't talk about it from how

08:35.360 --> 08:40.640
could you use this to harm communities how could you use this to harm individuals or how could

08:40.640 --> 08:46.080
this be inadvertently harmful and so those parts of cultures are things that we're grappling right

08:46.080 --> 08:52.480
with right now and you know we're introducing into our engineering context so my group sits at

08:52.480 --> 08:58.640
an engineering level and we're trying to introduce this new framework around responsible innovation

08:58.640 --> 09:05.440
and there's five big components to that one is being able to anticipate look ahead anticipate

09:05.440 --> 09:10.320
different futures look around corners and try to see where the technology might go how someone

09:10.320 --> 09:16.000
could take it insert it into larger systems how you can do things at scale that are powerful

09:16.000 --> 09:23.920
that you may not intend to do there's a whole component around that this you know responsible

09:23.920 --> 09:28.560
innovation that is around reflection and looking at yourselves and saying well where do we have

09:28.560 --> 09:34.640
biases or where we assuming things what are our motivations can we have an honest conversation

09:34.640 --> 09:38.960
about our motivations why are we doing this and can we ask those questions how do we create the

09:38.960 --> 09:44.080
space for that we've been talking about you know diversity and inclusion like how do you bring

09:44.080 --> 09:49.760
diverse voices into the space especially people that would really object to what you're doing and

09:49.760 --> 09:55.680
how do you celebrate that versus tolerate that there's a big component around just like our

09:55.680 --> 10:01.040
principles and values and how do you create with intention and and how do you ensure that they

10:01.040 --> 10:05.760
align with the principles and they align with their values and they're still trustworthy so

10:05.760 --> 10:10.560
there's a whole framework around how we're thinking about innovation in the space and at the end

10:10.560 --> 10:14.720
of the day it comes down to what you're like the culture of the organization that you're building

10:14.720 --> 10:21.520
because if you can't operate at scale then you end up only having small pockets of us that are

10:21.520 --> 10:26.640
talking about this versus how do we get every engineer to ask what's this going to be used for

10:26.640 --> 10:31.920
and who's going to use it or what if this could happen and we need people to start asking those

10:31.920 --> 10:36.560
types of questions and then start talking about how do we architect things in a way that's

10:36.560 --> 10:43.360
responsible but I'd say like most engineers probably don't ask those types of questions right

10:43.360 --> 10:48.880
now and so we're trying to build that into the culture of how we design and develop new technologies.

10:50.080 --> 10:55.600
One of the things that I often find frustrating about this conversation particularly when talking

10:55.600 --> 11:03.440
to technology vendors is this kind of default answer while we just make the guns we don't shoot

11:03.440 --> 11:08.800
them right we just make the technologies you know it's they're you know they can be used for good

11:08.800 --> 11:16.800
they can also be used for bad but we're focused on you know the the good as aspects it sounds

11:16.800 --> 11:25.120
like yeah maybe you while I'm curious how do you articulate you know your responsibility

11:25.120 --> 11:29.200
with the tools that you're creating or Microsoft's responsibility with the tools that's creating

11:29.200 --> 11:35.280
do you have a well I have a very similar reaction to you when when I hear oh we're just making

11:35.280 --> 11:44.000
tools I think well fine that's one perspective but the responsible perspective is we're making tools

11:44.000 --> 11:49.120
and we understand that they can be used in these ways and we've architected them so that they

11:49.680 --> 11:54.640
cannot be misused and we know that there will be people that misuse them so I think

11:55.920 --> 12:00.480
and you're hearing a lot of this in the technology space and you know there's every year there's

12:00.480 --> 12:04.400
more and more of it where people are saying look we have to be responsible we have to be accountable

12:04.400 --> 12:10.320
and and so I think we'll hear fewer and fewer people saying what you're hearing what I'm hearing as

12:10.320 --> 12:17.840
well but one of the things we have to do is we have to avoid the ideal path and just talking only

12:17.840 --> 12:22.720
about the ideal path because it's really easy to just say here's the great ways that this technology

12:22.720 --> 12:27.920
was going to be used and not even talk about the other side because then again we fall into that

12:27.920 --> 12:32.960
pattern of well we only thought about it from this one perspective and so one of the things that

12:32.960 --> 12:37.760
my group is trying to do is to make it okay to talk about here's how it could go wrong

12:38.320 --> 12:44.640
so that it becomes part of our you know daily habit and and we do it at various levels you know we

12:44.640 --> 12:50.240
do it at our all hands so when people are showing our technology we have them show the dark side

12:50.240 --> 12:55.200
of it at the same time so that we can talk about that in an open space and it becomes okay to talk

12:55.200 --> 13:00.160
about it no one wants to share the bad side of technology right no one no one wants to do that

13:00.160 --> 13:05.280
but if we make it okay to talk about it then we can start talking about how do we prevent that

13:06.480 --> 13:11.440
so we do that at like you know larger forums and then you know this I know this is a podcast

13:11.440 --> 13:17.520
but I wanted to show you something so I'll talk about it but we created it's almost like a game

13:17.520 --> 13:23.920
but it's it's a way for us to look at different stakeholders and perspectives and what could happen

13:23.920 --> 13:30.240
and so how do we create a safe environment where you can look at one of our ethical principles

13:30.240 --> 13:36.240
you can look at a stakeholder that is interacting with the system and then you say well if the

13:36.240 --> 13:41.120
stakeholders you know for example it was a woman in a car and your system is a voice recognition

13:41.120 --> 13:46.400
system what would she say if she gave it a one-star review she would probably say I had to yell

13:46.400 --> 13:52.000
a lot and didn't recognize me because we know that most of our systems are not tuned to be diverse

13:52.000 --> 13:57.600
right and so we start creating this environment for us to talk about these types of things

13:57.600 --> 14:03.760
so that it becomes okay again really how do we create safe spaces and then as we develop our scenarios

14:03.760 --> 14:10.240
how do we bring those up and then track them and say well how do we fix it now we've excavated

14:10.240 --> 14:15.440
these issues well let's fix it and let's talk about it so that's again part of culture like how

14:15.440 --> 14:22.400
do we make it okay to bring up the bad parts of things right so it's not just the ideal path do

14:22.400 --> 14:32.160
you run into or run up against engineers or executives that say you know introspection safe spaces

14:32.160 --> 14:38.880
you know granola you know what about the bottom line what does this mean for you know us as a

14:38.880 --> 14:44.800
business how do we you know think about this from a shareholder perspective you know it's it's

14:44.800 --> 14:52.640
interesting I I don't actually hear a lot of that push back because I think you know internally

14:52.640 --> 14:57.920
at Microsoft there is this recognition of who we want to be really thoughtful and intentional

14:58.480 --> 15:04.080
and and I think the bigger issue that we hear is just like how do we do it it's not that we don't

15:04.080 --> 15:10.880
want to it's well how do we do it and how do we do it at scale and so what are the different things

15:10.880 --> 15:19.120
you can put in place to help people bring this into their practice and so you know there isn't a

15:19.120 --> 15:24.320
pushback around well this is going to like it's going to affect my bottom it's going to affect

15:24.320 --> 15:29.680
my bottom line but there's more of a understanding that yeah if we build things that are thoughtfully

15:29.680 --> 15:35.360
designed and intentional and ethical that it's better for our customers I mean our customers want

15:35.360 --> 15:41.920
that too but then again the question is well how do we do it and where is it manifest so there's

15:41.920 --> 15:47.120
things that we're doing in that space I mean when you look at AI a big part of it is data so how do

15:47.120 --> 15:52.160
you look at the data that's being used to power some of these systems and say is this the diverse

15:52.160 --> 15:58.880
data set is this well-rounded do we have gaps here what's the bias in here and so we start looking

15:58.880 --> 16:05.200
at certain components of our systems and helping to again architect it in a way that's that's

16:05.200 --> 16:11.280
better I think all of our customers would want a system that recognized all voices right and

16:12.160 --> 16:16.240
because again to them they wouldn't want a system that just worked for men it didn't work for

16:16.240 --> 16:21.360
women so again it's like better product as a result and so if we can couch it in terms of

16:21.360 --> 16:27.680
better product then I think it makes sense versus if it's all about us philosophizing and only

16:27.680 --> 16:35.200
doing that I don't know if that's the best you know only doing that is not productive right do you

16:35.200 --> 16:46.400
find that the uncertainty around ethical issues related to AI has been an impediment to customers

16:47.840 --> 16:55.440
adopting it does that get in the way do they do they need these issues to be figured out before

16:55.440 --> 17:04.480
they dive in I don't think it's getting in the way but I think it's what I'm hearing from customers

17:05.120 --> 17:12.800
is help us think about these issues and you know a lot of people a lot of customers don't

17:12.800 --> 17:19.040
understand AI deeply right it's it's a complex space and a lot of people are ramping up in it

17:19.040 --> 17:23.920
and so the question is more about well what should I be aware of what are the questions

17:23.920 --> 17:29.680
that I should be asking and how can we do this together we know you guys are thinking about

17:29.680 --> 17:35.280
this deeply we're getting just involved in it you know a customer might say and so they

17:35.280 --> 17:40.240
it's more about how do we educate each other and for us if we want to understand like how do you

17:40.240 --> 17:44.720
want to use this because sometimes we don't always know the use case for the customer so we want

17:44.720 --> 17:48.560
to deeply understand that to make sure that what we're building actually works for what they are

17:48.560 --> 17:53.040
trying to do and from their perspective they want to understand well how does this technology work

17:53.040 --> 17:59.280
and where will it fail and where will it not work for my customers and so the question of ethics

17:59.280 --> 18:05.520
is more about we don't understand the space well enough help us understand it and we are concerned

18:05.520 --> 18:12.960
about what it could do and can we work together on that so it's it's not preventing them from

18:12.960 --> 18:17.760
adopting it but there's there's definitely a lot of dialogue it comes up quite a bit around

18:17.760 --> 18:23.360
well we've heard this we've heard bias is an issue what does that mean right and so we and so I

18:23.360 --> 18:30.240
think that's an education opportunity when you think about ethics from a technology innovation

18:30.240 --> 18:36.880
perspective are there examples of you know things that you've seen either that Microsoft is doing

18:36.880 --> 18:48.080
or out in the the broader role that strike you as innovative approaches to this problem yeah you

18:48.080 --> 18:53.920
know I'll go back to the data side of things just briefly but there's this concept called

18:53.920 --> 18:59.600
data sheets which I think is super interesting yeah you're probably really familiar with that

18:59.600 --> 19:04.640
and I've written about some of the work that Timnick Gebru and some others with Microsoft have

19:04.640 --> 19:09.840
done around data sheets for data sets exactly and the the interesting part for us is well how do

19:09.840 --> 19:16.160
you put it into the platform how do you bake that in and and so what one of the pieces of work

19:16.160 --> 19:21.520
that we're doing is we're taking this notion of data sheets and we are applying it into how we

19:21.520 --> 19:27.760
are collecting data and how we're building out our platform and so I think that that's I don't

19:27.760 --> 19:31.600
know if it's super novel because it to me it's like a nutrition label for your data like you

19:31.600 --> 19:36.960
won't understand how is it collected what's in it how can you use it but but I think that that's

19:36.960 --> 19:42.080
one where now as people leave the group you know you want to make sure that there's some history

19:42.080 --> 19:47.600
and understanding the composition of it there's some regulation around how we manage it internally

19:47.600 --> 19:52.880
and how we manage data in a thoughtful way I think that's just a really interesting concept that

19:52.880 --> 19:58.400
we should be talking about more as an industry and then can we share data between each other in a

19:58.400 --> 20:04.640
way that's responsible as well yeah yeah I don't know that the the data sheet I mean I think inherent

20:04.640 --> 20:10.320
to the idea was the hey this isn't novel in fact look at you know electrical components and all

20:10.320 --> 20:16.720
these other industries that do this it's just common sense quote unquote yeah but what is a little

20:16.720 --> 20:25.680
novel I think is actually doing it so since that paper was published several companies have

20:25.680 --> 20:34.400
published kind of similar takes model cards and there there have been a handful and every time

20:34.400 --> 20:39.760
I hear about them I ask okay so one is this you know what are you going to be publishing these

20:39.760 --> 20:47.120
for your services and the data sets that you're publishing and no one's done it yet so

20:47.120 --> 20:55.200
it's intriguing to hear you say that you're at least starting to think in this way internally

20:56.400 --> 21:05.040
do you have a sense for what the you know the path to publishing these kinds of you know whether

21:05.040 --> 21:11.680
it's a data sheet or a card or some kind of set of parameters around and bias either in a

21:11.680 --> 21:18.640
data set or a model you know for a commercial public service yeah absolutely we're actually

21:18.640 --> 21:25.520
looking at doing this for facial recognition and we've publicly commented about that we've said

21:25.520 --> 21:30.320
hey we're going to be sharing for our services what they're what it's great for and what it's not

21:30.320 --> 21:36.960
where it's and so that stuff is actually actively being worked on right now you'll probably see

21:36.960 --> 21:42.480
more of this in the next few weeks but but there is public comment that's going to come out

21:43.520 --> 21:48.800
with more details about it and I'll say that you know on the data sheet side I think a large

21:48.800 --> 21:54.880
portion of it is it needs to get implemented in the engineering systems first and you need to find

21:54.880 --> 21:58.960
the right place to put it and so that's that's the stuff that we're working on actively right now

21:58.960 --> 22:08.400
um can you comment more on that I it does as you say that it does strike me a little bit as one

22:08.400 --> 22:15.680
of these iceberg kind of problems like it you know it you know looks very manageable kind of above

22:15.680 --> 22:21.280
the water line but if you think about what goes into the creation of a data set or a model there's

22:21.280 --> 22:26.160
a lot of complexity and certainly at you know the scale of Microsoft is working at it needs to be

22:26.160 --> 22:34.080
automated what are some of the challenges that have come into play and in trying to implement

22:34.080 --> 22:40.640
an idea like that well um let me think about this for a second so I can frame it at the right way

22:42.880 --> 22:50.560
the biggest challenge for us on something like that is um really thinking through

22:50.560 --> 22:56.640
the data collection effort first and spending a little bit time there that's where we're actually

22:56.640 --> 23:02.640
spending quite a bit of time as we look at um so let me back up for a second I I work in an

23:02.640 --> 23:08.000
engineering group that touches all the speech language vision technologies and we do an enormous

23:08.000 --> 23:12.560
amount of data collection to power those technologies one of the things that we're first spending

23:12.560 --> 23:18.560
time on is looking at exactly how we're collecting data and going through those methodologies and

23:18.560 --> 23:21.920
saying is this the right way that we should be doing this we want to change it in any way do we

23:21.920 --> 23:26.880
want to optimize it and then we want to go and apply that back in so you're right this is a big

23:26.880 --> 23:32.800
iceberg because there's so many pieces that are connected to it and the spec for data sheets and

23:32.800 --> 23:40.720
the ones we've seen are large and um and so what we've done is how do we grab the core pieces of

23:40.720 --> 23:46.320
this and implement and create the starting point for it and then scale over time adversioning

23:46.320 --> 23:50.800
being able to add your own custom schemas to it and scale over time but what is like the minimum

23:50.800 --> 23:55.200
piece that we can put into the system and then make sure that it's working the way we want it to

23:55.200 --> 24:00.320
and so it's just it's about decomposing the problem and saying which ones do we want to prioritize

24:00.320 --> 24:04.720
first um for us we're spending a lot of time just looking at the data collection

24:04.720 --> 24:09.920
methodologies first because there's so much of that going on and at the same time what is the

24:09.920 --> 24:14.720
minimum part of the data sheet spec that we want to go and put in and then let's start iterating

24:14.720 --> 24:20.560
together on that it strikes me that these will be most useful when there's kind of broad

24:20.560 --> 24:26.800
industry adoption or at least coalescence around some you know standard whether it's a standard

24:26.800 --> 24:32.880
minimum that everyone's doing and potentially growing over time or you involved in or where

24:32.880 --> 24:40.080
of any efforts to create something like that well I think that that's um that's one piece where

24:40.080 --> 24:46.320
it's important I would say also in a large corporation it's important internally as well because

24:46.320 --> 24:51.920
we work with so many different teams um and we're interfacing with the you know we're a platform

24:51.920 --> 24:56.800
where we interface with large parts of our organization and um and being able to share that

24:56.800 --> 25:02.880
information internally that is a really important piece to the puzzle as well I think the external

25:02.880 --> 25:09.040
part is as well but the internal one is not um not any less important in my eyes because that's

25:09.040 --> 25:14.000
where we are we want to make sure that if we have a set of data that this you know group A is

25:14.000 --> 25:18.080
using it in one way if group B wants to use it we want to make sure that they have the rights

25:18.080 --> 25:24.400
to use it they understand what it's composed of where its orientation is and um and so that if

25:24.400 --> 25:31.120
they pick it up they do it with full knowledge of what's in it so um for us internally it's a

25:31.120 --> 25:36.080
really big deal externally um I've heard pockets of this but I don't think I could really comment

25:36.080 --> 25:42.240
on that yet with you know like full authority I'm really curious about the intersection between

25:42.240 --> 25:51.680
ethics and design and um you mentioned human centered design earlier my sense is that that that

25:51.680 --> 25:55.680
phrase kind of captures a lot of that intersection can you elaborate on what that means for you

25:55.680 --> 26:02.800
yeah yeah um so when you look at traditional design functions when we talk about human centered

26:02.800 --> 26:07.680
design there is there's lots of different humans in our design frameworks the one I typically

26:07.680 --> 26:13.120
pick up is um Don Norman's you know emotional design framework where he talks about behavioral

26:13.120 --> 26:19.840
design reflective design and visceral design and um and so behavior is you know how is something

26:19.840 --> 26:25.120
functioning what is the functionality of it um reflective is how does it make you feel about

26:25.120 --> 26:32.240
yourself you know how does it play to your ego and your personality and um visceral is you know

26:32.240 --> 26:40.960
the look and feel of that that's a very um individual oriented approach to design and when I think

26:40.960 --> 26:47.600
about these large systems you actually need to bring in the ecosystem into that so how does this

26:47.600 --> 26:51.760
object you're creating or this system you're creating how does it fit into the ecosystem

26:51.760 --> 26:56.160
and so one of the things we've been playing around with is we've actually reached into adjacent

26:56.160 --> 27:00.640
areas like agriculture and explore like how do you do sustainable agriculture what are

27:00.640 --> 27:05.840
that some of those principles and methodologies and how do you apply that into our space so a lot

27:05.840 --> 27:10.560
of the conversations we're having is around ecosystems and how do you insert something into the

27:10.560 --> 27:16.400
ecosystem and what happens to it what is the ripple effect of that and then how do you do that in a

27:16.400 --> 27:21.840
way that keeps that whole thing sustainable for so it's not um it's a good solution versus one

27:21.840 --> 27:28.640
that's bad and um and causes other downstream effects so I think that those are changes that we

27:28.640 --> 27:33.840
have to have in our design methodology we're looking we're looking away from the one artifact and

27:33.840 --> 27:38.160
thinking about it from a you know here's how the one user is going to work with it versus how is

27:38.160 --> 27:42.800
the society and going to interact with it how are different communities going to interact with

27:42.800 --> 27:48.400
it and what does that do to that community um it's a larger problem and so there's like this shift

27:48.400 --> 27:54.320
in design thinking that we're trying to do with our designers so they're not just doing UI

27:54.320 --> 27:58.720
they're not just thinking about this one system they're thinking about it holistically um and there

27:58.720 --> 28:03.040
isn't a framework that we can easily pick up so we have to kind of construct one as we are going

28:03.040 --> 28:12.080
along yeah yeah for a while a couple of years ago maybe I was having I was in search of that framework

28:12.720 --> 28:22.640
and I think the the motivation was just really early experiences of seeing kind of AI shoved

28:22.640 --> 28:30.320
into products in ways that were frustrating or annoying like for example a nest thermostat like

28:31.120 --> 28:37.200
you it's intended to be very simple but it's making these decisions for you in a way that you

28:37.200 --> 28:42.720
can't really control and it's starting me down this path of you know what does it mean really

28:42.720 --> 28:53.200
build out uh you know a discipline of design that is aware of AI and intelligence I've

28:53.200 --> 28:58.080
jumped on the podcast before that you know I I call it intelligent design but that's overloaded

28:58.080 --> 29:03.840
term total is but is there a term for that now or people thinking about that how far have we

29:03.840 --> 29:10.160
come in building out um you know you know a discipline or a way of thinking of what it means to

29:10.160 --> 29:18.160
build intelligence into products yeah um we have done a lot of work around education for our designers

29:18.160 --> 29:24.400
because uh we found a big gap between what our engineers were doing and talking about and what

29:24.400 --> 29:30.160
our designers had awareness over so we actually created a deep learning for designers workshop it

29:30.160 --> 29:36.800
was a two-day workshop and it was really intensive so we took um you know neural nads

29:36.800 --> 29:41.920
convolutions like all these concepts and we introduced them to designers in a way that designers

29:41.920 --> 29:48.080
would understand it um we you know brought it to here's how you think about it in terms of

29:48.080 --> 29:51.840
Photoshop here's how you think about it in terms of the tools you're using and the words you use

29:51.840 --> 29:56.640
there here's how we'd apply here's a exercise where people had to get out of out of their seas and

29:56.640 --> 30:02.240
we created this really simple neural net with human beings and um and we had them coding as well

30:02.240 --> 30:09.840
and so they were coding in Python and um in notebooks and so they were uh they were exposed to it

30:09.840 --> 30:15.680
and and we exposed them to a lot of the techniques and terminology in a way that was concrete

30:15.680 --> 30:19.920
and they were able to then say oh this is what style transfer looks like oh this is what

30:19.920 --> 30:27.280
this is how we constructed a bot and so um first on the design side I think having the vocabulary

30:27.280 --> 30:31.600
to be able to say oh I know what this word means not just I know what it means but I've experienced

30:31.600 --> 30:35.680
it so then I can have a meaningful discussion with my engineer I think that that that was an

30:35.680 --> 30:41.840
important piece and then understanding how AI systems are just different from regular systems

30:41.840 --> 30:48.160
they are more probabilistic in nature the defaults matter they are they can be self-learning

30:48.160 --> 30:53.360
and so how do we think about these and starting to showcase case studies with our designers to

30:53.360 --> 30:58.160
understand that these types of systems are quite different from the deterministic type of systems

30:58.160 --> 31:03.200
that you may have designed for in the past um again I think it comes back to culture because

31:03.760 --> 31:08.640
it was there's a and we keep doing these workshops every quarter we'll do another one because

31:08.640 --> 31:13.600
we have so much demand for it and we found even engineers and PMs would come to our design workshops

31:14.160 --> 31:20.720
but um kind of democratizing the terminology a little bit and making it concrete to people

31:20.720 --> 31:25.200
was an is an important part of this it's interesting to think about

31:25.200 --> 31:32.720
what it does to a designer's design process to have more intimate knowledge of these concepts

31:33.760 --> 31:38.640
at the same time a lot of the questions that come to mind for me are you know much higher level

31:38.640 --> 31:46.240
concepts in the in the domain of design for example you can we talk about user experience

31:47.600 --> 31:54.640
to what degree should a user experience AI if that makes any sense should we be trying to make AI

31:54.640 --> 32:02.240
or or you know this notion of intelligence invisible to users or very visible to users um you know

32:02.240 --> 32:09.040
this has come up you know recently in for example I'm thinking of like Google Duplex when they announced

32:10.320 --> 32:14.480
that that system was going to be making phone calls to people and there was a big

32:15.440 --> 32:22.000
kerfuffle about whether that should be disclosed um yeah and I don't know that there's a right answer

32:22.000 --> 32:26.320
I get in some ways you want some of this stuff to be invisible in other ways you know time

32:26.320 --> 32:31.280
back to the whole ethics conversation it does make sense that you know there's some degree of

32:31.280 --> 32:36.960
disclosure yeah absolutely I imagine as a designer this notion of disclosure is can be a very

32:36.960 --> 32:43.120
nuanced thing what does that even mean yeah yeah and it's all context dependent and it's all

32:43.120 --> 32:50.720
norm dependent as well because if you were to look into the future and say are people more comfortable

32:50.720 --> 32:55.520
like I mean look at airports for example people are walking through just using face ID using the

32:55.520 --> 33:01.440
clear system and a few years ago I think if you ask people would you feel comfortable doing that

33:01.440 --> 33:07.120
most people would say no I don't feel comfortable doing that I don't want that um and so I think in

33:07.120 --> 33:12.080
this space because it's really fluid and new norms are being established and things are being

33:12.080 --> 33:18.400
tested out uh we have to be on top of how people are feeling and thinking about these technologies

33:18.400 --> 33:23.840
where so that we understand where some disclosure needs to happen and where things don't and um

33:23.840 --> 33:31.040
and in a lot of cases you almost want to assume disclosure for things that are very consequential

33:31.040 --> 33:37.840
in high stakes um where there is opportunity for deception in the duplex case you have to be

33:37.840 --> 33:44.320
thoughtful about that um and so it's this isn't one where you can say okay you should always

33:44.320 --> 33:50.800
disclose it just depends on the context and so we have this notion of consequential scenarios

33:50.800 --> 33:55.200
where things are you know if there's automated decision making if there are scenarios where there

33:55.200 --> 34:02.880
is um there are high stakes scenarios those ones we think about um in a we just put a little bit

34:02.880 --> 34:07.840
more due diligence over those and start to be more thoughtful about those and then we have you

34:07.840 --> 34:13.120
know other types of scenarios which are um more systems oriented and here's some things that are

34:13.120 --> 34:18.320
operationally oriented and they end up having different types of scenarios but we haven't been

34:18.320 --> 34:24.640
able to create uh here's the exact way you do every single you know you approach it in every single

34:24.640 --> 34:30.720
way so it is so super context dependent and expectation dependent um maybe after a while you get

34:30.720 --> 34:37.520
used to your nest thermostat and you're fine with the way it's operating right and so um so I

34:37.520 --> 34:42.080
don't know these social norms are interesting because they are someone will go and establish

34:42.080 --> 34:47.280
something or they'll test the waters you know google glass tested the waters and um that was

34:47.280 --> 34:52.320
a cultural response right people responded and said I don't want to be surveilled I don't want

34:52.320 --> 34:57.680
I want to be able to go to a bar and get a drink and I'll have someone recording me and um and so

34:57.680 --> 35:03.120
I think we have to understand where society is relative to what the technologies are that we're

35:03.120 --> 35:07.600
inserting into them and so again it comes back to are we listening to users are we just putting

35:07.600 --> 35:14.960
tech out there I think we have to really start listening to users um my group has a fairly large

35:14.960 --> 35:20.720
research component to it and we spend a lot of time talking to people especially in the places

35:20.720 --> 35:25.120
where we're going to be putting some tech and I'm understanding what it's going to do to the dynamic

35:25.120 --> 35:32.960
and how they're how they're reacting to it yeah it strikes me that uh and maybe it's kind of

35:32.960 --> 35:38.400
the engineer background in me this looking for like a framework uh you know a flow chart for how

35:38.400 --> 35:45.520
we can approach this problem and uh I need to embrace more of the designer that's like well every

35:45.520 --> 35:51.120
you know product every situation is different and it's more about a principled approach as opposed

35:51.120 --> 35:58.800
to a process absolutely it's more about a principled and intentional approach so what what we're

35:58.800 --> 36:03.920
talking about is everything that you're choosing are you intentional about that choice and are

36:03.920 --> 36:08.400
you very thoughtful about things like defaults because we know that people don't change them

36:08.400 --> 36:13.280
and so how do you think about every single design choice in being principled and in very

36:13.280 --> 36:18.160
intense intentional and evidence driven and so we push this on our teams and I think some of our

36:18.160 --> 36:22.400
teams maybe don't enjoy being with us sometimes as a result but we say look we're going to give you

36:22.400 --> 36:27.920
some recommendations that are going to be principled intentional and evidence driven and we want to

36:27.920 --> 36:33.680
hear back from you if you don't agree on your evidence and why you're saying this is a good or

36:33.680 --> 36:39.280
bad idea um and that's that's the way you have to operate right now because it is so context-driven

36:39.920 --> 36:44.480
I wonder if you can talk through some examples of how you know humans enter design and all these

36:44.480 --> 36:49.120
things come together in the context of kind of concrete problems that you've looked at yeah I

36:49.120 --> 36:53.760
was thinking about this because a lot of the work that we do is um fairly confidential but

36:53.760 --> 36:59.360
there's one that I can touch on which was shared at build earlier this year and that was a meeting

36:59.360 --> 37:03.120
room device and I don't know if you remember this but there's a meeting room device that we're

37:03.120 --> 37:09.520
working on that um recognizes who's in the room and um does transcription of that meeting

37:10.160 --> 37:16.880
and um it to me as someone who is a manager I love the idea of having a room a device in the room

37:16.880 --> 37:22.480
the captures action items and who was here and what was said and uh and so we started looking at

37:22.480 --> 37:28.240
this and we said okay well let's look at different types of meetings and people and let's look at

37:28.240 --> 37:34.560
categories of people that um this might affect differently and so how do you think about

37:34.560 --> 37:40.880
introverts in a meeting how do you think about women and minorities because there are subtle

37:40.880 --> 37:47.280
dynamics that are happening in meetings that um make some of these relationships they can

37:47.280 --> 37:53.520
um reinforce certain types of stereotypes or relationships and so we started um interviewing

37:54.080 --> 38:00.640
people in the context of this sort of meeting room device and um and this is research that is

38:00.640 --> 38:08.240
pretty well uh it's well recognized it's not um it's not novel research but but um it reinforced

38:08.240 --> 38:13.920
the fact that when you start putting in things that will monitor anyone that's in a room certain

38:13.920 --> 38:19.680
categories of people behave differently and you see larger discrepancies um and impact

38:19.680 --> 38:25.520
with women minorities more junior people and so we said wow this is really interesting because

38:25.520 --> 38:30.240
as soon as you put a recording device in a room it's going to subtly shift the dynamic where some

38:30.240 --> 38:35.120
people might talk less or some people might feel like they're observed or depending on if there's a

38:35.120 --> 38:39.200
manager in the room and there's a device in a room they're going to behave differently

38:39.200 --> 38:43.680
and does that result in a good meeting or a bad one we're not sure but that will affect the dynamic

38:43.680 --> 38:48.160
and so then we took a lot of this research and we went back to the product team and said well how do

38:48.160 --> 38:55.120
we now design this in such a way that we design with privacy first in mind and um make users

38:55.120 --> 39:00.000
feel like they're empowered to opt into it and so we've had discussions like that especially around

39:00.000 --> 39:06.480
these types of um devices where we've seen big impact to how people behave but it's not like a hard

39:06.480 --> 39:10.480
guideline or it's not really a hard set of rules around what you have to do but you know because

39:10.480 --> 39:15.840
all meetings are different right you've brainstorming ones that are more of a fluid ideas you don't

39:15.840 --> 39:20.000
really care who said what it's about getting all the ideas out you have ones where you're shipping

39:20.000 --> 39:24.000
something important and you want to know who said what because they're clear action items that go

39:24.000 --> 39:30.080
with them and so um trying to create a system that works with so many different nuanced

39:30.080 --> 39:36.160
conversations and um different scenarios is not an easy one so what we do is we we all run

39:36.160 --> 39:41.360
alongside with the product team and while they're engineering you know they're developing their work

39:41.360 --> 39:46.720
we will take the research where that we've gathered and we'll create alternatives for them at

39:46.720 --> 39:52.160
the same time so that we can run alongside with them we can say hey here's option A, B, C, D, and E

39:52.160 --> 39:56.080
let's play with these and maybe we come up with a version that mixes them all together

39:56.640 --> 40:02.160
but um but it gives them options to think about because again it comes back to well I might not

40:02.160 --> 40:08.240
have time to think about all of this so how do we empower people with ideas and um and concrete

40:08.240 --> 40:18.160
things to to look at yeah I think that examples a great example of the complexity or um maybe

40:18.160 --> 40:24.320
complexity is not the right word but the the idea that your initial reaction might be like the

40:24.320 --> 40:28.240
exact opposite of what you need to do yeah as you were saying this I was like oh just hide the

40:28.240 --> 40:32.400
thing so no one knows it's there it doesn't change the dynamic it's like that's exactly wrong

40:33.360 --> 40:38.640
you don't want to do that don't hide it right right yeah and maybe that's another piece I'm sorry

40:38.640 --> 40:44.800
interrupt that but but one of the things I've noticed is the our initial reaction is often wrong

40:44.800 --> 40:50.640
and so how do we hold it at the same time that we give ourselves a space to explore other things

40:50.640 --> 40:55.840
and then keep an open mind and say okay I have to adjust and change because hiding it would

40:55.840 --> 41:02.880
absolutely be an interesting option but then you have so many issues with red right um but again

41:02.880 --> 41:06.720
like it is about being able to have like an open mindset and be able to challenge yourself in

41:06.720 --> 41:13.920
the space do you have a sense for where you know if we kind of buy into the idea that folks that

41:13.920 --> 41:20.640
are working with AI need to be more thoughtful and more intentional and and maybe incorporate more

41:20.640 --> 41:30.000
this into more this design thinking element to their work do you have a sense for where this you

41:30.000 --> 41:37.520
know does or should or needs to live within a customer organization yeah I think it actually

41:37.520 --> 41:40.960
and this is a terrible answer I think it needs to live everywhere in some ways because

41:42.400 --> 41:47.440
what one thing that we're noticing is we have with corporate level things that happen we have

41:47.440 --> 41:53.600
an you know an ether board it's our it's an advisory board that looks at AI technologies and

41:53.600 --> 41:59.600
advises and that's at a corporate level that's a really interesting way of approaching it

41:59.600 --> 42:04.880
but it can't live alone and so the thing that we have learned is that if we pair it with groups

42:04.880 --> 42:10.880
like mine that sit in the engineering context that are able to translate principles concepts guidelines

42:10.880 --> 42:17.920
into practice that sort of partnership has been really powerful because we can take those principles

42:17.920 --> 42:22.080
and say well here's where it really worked and here's where it kind of didn't work and then we can

42:22.080 --> 42:27.920
also find issues and say well we're grappling with this issue that you guys hadn't thought about

42:27.920 --> 42:32.160
how do you think about this and can we create a broader principle around it so I think that there's

42:32.160 --> 42:36.800
this like strong cycle of feedback that happens if you have something at the corporate level

42:36.800 --> 42:41.520
or you establish just what your values are what are our guidelines and what are our approaches

42:41.520 --> 42:45.120
but then at the engineering context you have a team that can problem solve and apply

42:45.120 --> 42:49.440
and then you can create a really tight feedback loop between that engineering team and your

42:49.440 --> 42:54.880
corporate team so that you're continually reinforcing each other because the worst thing would be

42:54.880 --> 42:59.520
just to have a corporate level thing and just be PR speak right you don't want that right right

42:59.520 --> 43:02.480
and the worst thing would also be just to have it in the engineering level because then you would

43:02.480 --> 43:09.840
have a very distributed mechanism of doing something may not cohesibly ladder up to your

43:09.840 --> 43:15.120
principles and so I think you kind of need both and to have them work off each other to really

43:15.120 --> 43:21.280
have something effective and maybe there's other things as well but so far this has been a really

43:21.920 --> 43:28.000
productive and iterative experiment that we're doing. Danny pointers come to mind for folks that

43:28.000 --> 43:34.640
want to explore this space more deeply do you have a top three favorite resources or

43:35.680 --> 43:44.720
initial directions well it depends what you want to explore so I was reading the AI now report

43:44.720 --> 43:51.040
the other day it's you know a fairly large report 65 page report around the impact

43:51.040 --> 43:56.560
of AI in different systems different industries and so if you're looking at

43:56.560 --> 44:02.400
getting up to speed on well what areas is AI going to impact I would start with some of these

44:02.400 --> 44:10.480
types of groups because I found that they are super thoughtful and how they're going into each

44:10.480 --> 44:16.160
space and understanding each space and then bubbling up some of the scenarios so if you're

44:16.160 --> 44:21.840
thinking about AI from a you know how is it impacting those types of things that are really

44:21.840 --> 44:28.480
interesting on the engineering side I actually spent a lot of time on a few Facebook groups where

44:28.480 --> 44:33.360
they have there's some big AI groups in Facebook and they're always sharing here's the latest here's

44:33.360 --> 44:38.320
what's going on I've tried this technique and so that keeps me kind of up to speed on some of those

44:38.320 --> 44:44.320
that are happening and also archive just to see what research is being published the design side

44:44.320 --> 44:51.440
I'm sort of mixed I mean I haven't really found a strong spot yet I wish I had like something

44:51.440 --> 44:57.920
my back pocket where I could just refer to but the thing that maybe has been on the theory side

44:57.920 --> 45:05.440
that has been super interesting is to go back to a few set a few people that have made commentaries

45:05.440 --> 45:12.000
just around sustainable design so I refer back to Wendell Berry quite a bit the agriculturalist

45:12.000 --> 45:20.960
and poet actually who has really introspected how agriculture could be reframed Ursula Franklin is

45:20.960 --> 45:28.720
also a commentary from Canada she was she did a lot of podcast or radio broadcast a long time ago

45:28.720 --> 45:34.800
and she has a whole series around technology and its societal impact and if you replace a few of

45:34.800 --> 45:39.200
those words and put in some of our new age words it would still hold true and so I think there's

45:39.200 --> 45:44.320
a lot of theory out there but not a lot of like here's really great examples of what you can do

45:44.320 --> 45:50.640
because we're all still feeling out the space and we haven't found a perfect patterns yet that you

45:50.640 --> 45:56.160
can democratize and share out broadly we're very thanks so much for taking the time to chat with us

45:56.160 --> 46:04.880
about this stuff is a really interesting space and one that I enjoy coming back to periodically

46:04.880 --> 46:10.880
and I personally believe that there's you know this intersection of AI and design is one that

46:10.880 --> 46:20.960
is as wide open and should and will be further developed and I'm kind of looking forward to

46:20.960 --> 46:25.280
keeping an eye on it and I appreciate you taking the time to chat with me about it thank you so

46:25.280 --> 46:33.680
much Sam was wonderful talking to you thank you all right everyone that's our show for today

46:33.680 --> 46:40.400
for more information on Mira or any of the topics covered in this show visit twimmelai.com slash

46:40.400 --> 46:47.200
talk slash two three three to follow along with or catch up on our AI for the benefit of society

46:47.200 --> 46:55.360
series visit twimmelai.com slash AI for society as always thanks so much for listening and catch you

46:55.360 --> 47:10.320
next time

