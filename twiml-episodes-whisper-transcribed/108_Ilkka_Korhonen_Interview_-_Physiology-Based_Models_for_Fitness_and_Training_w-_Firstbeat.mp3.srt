1
00:00:00,000 --> 00:00:16,120
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:16,120 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,440
I'm your host Sam Charrington.

4
00:00:32,440 --> 00:00:33,840
Contest alert.

5
00:00:33,840 --> 00:00:38,960
This week we have a jam-packed intro, including a new contest we're launching.

6
00:00:38,960 --> 00:00:43,040
So please bear with me, you don't want to miss this one.

7
00:00:43,040 --> 00:00:46,720
First, a bit about this week's shows.

8
00:00:46,720 --> 00:00:51,440
As you may know, I spent a few days at CES earlier this month.

9
00:00:51,440 --> 00:00:56,400
While there, I spoke with a bunch of folks applying AI in the consumer electronics industry,

10
00:00:56,400 --> 00:01:01,040
and I'm including you in those conversations via this series of shows.

11
00:01:01,040 --> 00:01:05,440
Stay tuned as we explore some of the very cool ways that machine learning and AI are being

12
00:01:05,440 --> 00:01:08,600
used to enhance our everyday lives.

13
00:01:08,600 --> 00:01:13,680
This includes work being done at Anki, who built Cosmo, the cutest little computer vision-powered

14
00:01:13,680 --> 00:01:15,280
robot.

15
00:01:15,280 --> 00:01:22,120
Nighthouse, whose smart home security camera combines 3D sensing with deep learning and NLP.

16
00:01:22,120 --> 00:01:28,120
Intel, who's using the single-shot multi-box image detection algorithm to personalize video

17
00:01:28,120 --> 00:01:31,680
fees for the Ferrari Challenge North America.

18
00:01:31,680 --> 00:01:36,480
First beat, a company whose machine learning algorithms analyzed your heartbeat data to

19
00:01:36,480 --> 00:01:42,360
provide personalized insights into stress, exercise, and sleep patterns.

20
00:01:42,360 --> 00:01:48,400
3AI and Koito, who have partnered to bring machine learning-based adaptive driving beams

21
00:01:48,400 --> 00:01:52,280
or automatically adjusting high beams to the US.

22
00:01:52,280 --> 00:01:59,360
And last but not least, aerial.ai, who applies sophisticated analytics to Wi-Fi signals

23
00:01:59,360 --> 00:02:05,600
to enable some really interesting home automation and healthcare applications.

24
00:02:05,600 --> 00:02:11,000
Now, as if six amazing interviews wasn't enough, a few of these companies have been so

25
00:02:11,000 --> 00:02:15,520
kind as to provide us with products for you, the Twimmel community.

26
00:02:15,520 --> 00:02:19,360
And keeping with the theme of this series, our contest will be a little different this

27
00:02:19,360 --> 00:02:20,360
time.

28
00:02:20,360 --> 00:02:25,400
To enter, we want to hear from you about the role AI is playing in your home and personal

29
00:02:25,400 --> 00:02:28,640
life, and where you see it going.

30
00:02:28,640 --> 00:02:36,200
Just head on over to twimmelai.com slash myaicontest, fire up your webcam or smartphone camera,

31
00:02:36,200 --> 00:02:39,120
and tell us your story in two minutes or less.

32
00:02:39,120 --> 00:02:43,600
Go post the videos to YouTube, and the video with the most likes wins their choice of

33
00:02:43,600 --> 00:02:50,480
great prizes, including an Anki Cosmo, a lighthouse smart home camera, and more.

34
00:02:50,480 --> 00:02:55,040
Submissions will be taken until February 11th, and voting will remain open until February

35
00:02:55,040 --> 00:02:56,040
18th.

36
00:02:56,040 --> 00:03:02,360
Good luck.

37
00:03:02,360 --> 00:03:06,560
Before we dive into today's show, I'd like to thank our friends at Intel AI for their

38
00:03:06,560 --> 00:03:09,520
continued support of this podcast.

39
00:03:09,520 --> 00:03:14,760
Intel was extremely active at this year's CES, with a bunch of AI autonomous driving

40
00:03:14,760 --> 00:03:17,200
and VR related announcements.

41
00:03:17,200 --> 00:03:21,440
One of the more interesting partnerships they announced was a collaboration with the Ferrari

42
00:03:21,440 --> 00:03:24,720
Challenge North America race series.

43
00:03:24,720 --> 00:03:29,440
Along with the folks at Ferrari Challenge, Intel AI aspires to make the race viewing experience

44
00:03:29,440 --> 00:03:35,360
more personalized by using deep computer vision to detect and monitor individual race

45
00:03:35,360 --> 00:03:40,680
cars via camera feeds, and allow viewers to choose the specific cars feeds that they'd

46
00:03:40,680 --> 00:03:42,200
like to watch.

47
00:03:42,200 --> 00:03:46,960
Look for my conversation with Intel's Andy Keller and Emil Chindicki earlier in this series

48
00:03:46,960 --> 00:03:52,920
for an in-depth discussion about this project, and be sure to visit ai.intel.com where you'll

49
00:03:52,920 --> 00:03:56,200
find Andy's technical blog post on the topic.

50
00:03:56,200 --> 00:03:58,320
Now about today's show.

51
00:03:58,320 --> 00:04:02,640
In this episode, I'm joined by Oka Koronen, Vice President of Technology at First

52
00:04:02,640 --> 00:04:07,200
Speed, a company whose algorithms are embedded in fitness watches from companies like Garmin

53
00:04:07,200 --> 00:04:12,960
and Sunto, and which use your heartbeat data to offer personalized insights into stress,

54
00:04:12,960 --> 00:04:15,520
fitness, recovery, and sleep patterns.

55
00:04:15,520 --> 00:04:20,600
We cover a ton about first beat in the conversation, including how they transform the center readings

56
00:04:20,600 --> 00:04:26,360
into more actionable data, their use of a digital physiological model of the human body,

57
00:04:26,360 --> 00:04:31,480
how they use center data to identify and predict physiological changes within the body,

58
00:04:31,480 --> 00:04:35,440
and some of the opportunities that first beat has to further apply machine learning in

59
00:04:35,440 --> 00:04:36,680
the future.

60
00:04:36,680 --> 00:04:38,920
And now on to the show.

61
00:04:38,920 --> 00:04:46,120
Alright everyone, I am on the line with Oka Koronen, Oka is Vice President of Technology

62
00:04:46,120 --> 00:04:53,520
at First Beat, a company that I first met with at CES, in fact I met Oka at CES, and

63
00:04:53,520 --> 00:04:58,760
he was kind enough to agree to jump on the line for an interview.

64
00:04:58,760 --> 00:05:02,240
Oka, welcome to this week in machine learning in AI.

65
00:05:02,240 --> 00:05:04,120
Thank you very much.

66
00:05:04,120 --> 00:05:08,280
So why don't we get started by having you tell us a little bit about your background?

67
00:05:08,280 --> 00:05:14,080
Yeah, so my background is that I've been doing biomedical engineering for basically

68
00:05:14,080 --> 00:05:21,200
in time, my professional career I did on 90s, I was working on biomedical engineering

69
00:05:21,200 --> 00:05:27,480
and ambulatory heart rate variability monitoring, believe it or not, and activity tracking,

70
00:05:27,480 --> 00:05:31,040
but the technologies were quite different at that time.

71
00:05:31,040 --> 00:05:40,560
So gradually I moved in, I did research at that time, but then gradually moving towards

72
00:05:40,560 --> 00:05:49,040
wellness and preventative care, while finally later in 2009 I decided to take a career

73
00:05:49,040 --> 00:05:59,640
move and went to industry first for Nokia and later I spent some colleagues, a company

74
00:05:59,640 --> 00:06:05,280
called Pulse on which is an optical, one of the pioneers in optical heart rate tracking

75
00:06:05,280 --> 00:06:14,360
and later on about a year ago I quit that and now I'm with First Beat on doing the next

76
00:06:14,360 --> 00:06:21,400
level of the, in the stack, so to say the heart rate analysis rather than measurements.

77
00:06:21,400 --> 00:06:26,000
So that's in brief my background.

78
00:06:26,000 --> 00:06:30,840
And for those who aren't familiar with First Beat maybe a little bit more about what that

79
00:06:30,840 --> 00:06:38,080
company does, I happen to know a teeny bit about it because First Beat's embedded in the

80
00:06:38,080 --> 00:06:42,040
fitness watch that I use the Garmin Vivo Active 3.

81
00:06:42,040 --> 00:06:43,360
Yeah, that's correct.

82
00:06:43,360 --> 00:06:50,000
So our core business is that we are doing heart rate analytics, so we transform the heart

83
00:06:50,000 --> 00:06:54,640
rate reading into something actionable and meaningful for the user.

84
00:06:54,640 --> 00:07:01,600
We were established in 2002 as a spin of the University of Yvescola and we first looked

85
00:07:01,600 --> 00:07:12,640
at heart rate variability as a pramder or method for controlling the athletes overtraining

86
00:07:12,640 --> 00:07:18,760
but soon we realized that actually while we try to detect and prevent overtraining with

87
00:07:18,760 --> 00:07:24,800
heart rate variability based methods, those are also applicable for regular people who

88
00:07:24,800 --> 00:07:28,560
are struggling with stress and recovery.

89
00:07:28,560 --> 00:07:35,080
So that's where we started and gradually we have been moving into other domains from

90
00:07:35,080 --> 00:07:36,360
elite sports.

91
00:07:36,360 --> 00:07:46,960
We still do have this elite sport, so we are serving elite teams like NHL, NBA, etc kind

92
00:07:46,960 --> 00:07:54,080
of teams and their coaches for optimizing the training and recovery of the athletes.

93
00:07:54,080 --> 00:08:01,360
But more I think better known area where we are is that we do develop and license the

94
00:08:01,360 --> 00:08:05,360
heart rate analytics algorithms for wearables.

95
00:08:05,360 --> 00:08:13,160
So for example, Garmin, Huawei, Sunt and some other brands they are using our analytics

96
00:08:13,160 --> 00:08:19,720
to transform the sensor reading into more actionable information like fitness level,

97
00:08:19,720 --> 00:08:26,440
color consumption, stress and recovery metrics, etc.

98
00:08:26,440 --> 00:08:33,320
Now I've had a couple of conversations on the podcast about IoT and some of the challenges

99
00:08:33,320 --> 00:08:39,680
of analytics in an IoT environment and I guess when I think about this device on my

100
00:08:39,680 --> 00:08:44,920
wrist it's kind of like an IoT in the small and all of the challenges, well not all of

101
00:08:44,920 --> 00:08:52,640
them, but some of the challenges that are typical for IoT in particular, the noisiness

102
00:08:52,640 --> 00:08:56,360
of the sensor readings and things like that, I get the impression that there's a lot

103
00:08:56,360 --> 00:09:03,960
of that going on in the with regards to kind of the heart rate sensor in the wearables.

104
00:09:03,960 --> 00:09:07,720
That's very correct, so that's absolutely true.

105
00:09:07,720 --> 00:09:17,600
So the typical limitations or requirements of the IoT are that the sensor is very power

106
00:09:17,600 --> 00:09:25,480
limited or resource limited environment in that sense that you don't have the supercomputer

107
00:09:25,480 --> 00:09:32,240
power there, you have limited amount of battery you want to use as efficiently as possible

108
00:09:32,240 --> 00:09:39,120
to have the battery lifetime as long as possible, still you want to have a reach information

109
00:09:39,120 --> 00:09:40,440
from multiple of sensors.

110
00:09:40,440 --> 00:09:48,000
So for example, typical Garmin sports, what has a motion sensor and barometer in addition

111
00:09:48,000 --> 00:09:52,600
to heart rate, monitor and GPS and then of course all these radios.

112
00:09:52,600 --> 00:09:59,320
So it's quite interesting from 80s or 90s perspective, in fact it would be a supercomputer

113
00:09:59,320 --> 00:10:07,480
but it's not and things with what are being done there are quite fascinating.

114
00:10:07,480 --> 00:10:15,040
The other thing I think which is very different from for example, as I told I started with

115
00:10:15,040 --> 00:10:19,840
biomedical engineering on 90s when we were working with hospital systems and monitoring

116
00:10:19,840 --> 00:10:27,960
of who patient in a hospital, the monitoring was continuous and controlled in a way but

117
00:10:27,960 --> 00:10:35,960
here it's very liberal or there are a lot of decrease of freedom in the sensor use also

118
00:10:35,960 --> 00:10:45,240
that it's people are wearing the sensor not continuously but sometimes they are wearing

119
00:10:45,240 --> 00:10:50,720
sometimes they are not and also the way how they wear it how they attach the sensor to

120
00:10:50,720 --> 00:11:00,320
their wrist and how snug it fits to the wrist and so on they are not so well controlled

121
00:11:00,320 --> 00:11:07,240
and that causes a lot of variation in the single quality also ambient prongers like

122
00:11:07,240 --> 00:11:15,040
temperature ambient light and motion and so on they affect a lot the sensor reading.

123
00:11:15,040 --> 00:11:21,840
So it's quite challenging data what is measured by the sensors and there's quite sophisticated

124
00:11:21,840 --> 00:11:30,160
algorithms and detection technologies which are required to make sense out of that.

125
00:11:30,160 --> 00:11:39,720
And so for the devices that first beat is embedded in are you pulling data directly off

126
00:11:39,720 --> 00:11:45,520
of the sensors and kind of responsible for giving the watch kind of everything from the

127
00:11:45,520 --> 00:11:51,960
ground up the the heart rate itself plus the higher level metrics that you're providing

128
00:11:51,960 --> 00:11:59,000
or does the watch or the sensor provide some of those metrics in year doing higher level

129
00:11:59,000 --> 00:12:01,400
analytics on top of that.

130
00:12:01,400 --> 00:12:08,280
It's rather the latter so our interface to the sensor is typically the heart rate

131
00:12:08,280 --> 00:12:14,760
and there be to beat heart rate and also we use motion and GPS speed and things like

132
00:12:14,760 --> 00:12:22,760
that but we don't do the heart rate detection but we do the as I said we transform the

133
00:12:22,760 --> 00:12:28,760
heart rate and heart rate variability data into actionable information so it's a little

134
00:12:28,760 --> 00:12:38,800
bit higher level analytics based on the physiological model of how the human physiology works

135
00:12:38,800 --> 00:12:45,280
and how the autonomic nervous system controls the heart rate and heart rate variability

136
00:12:45,280 --> 00:12:55,240
and how this kind of data can be interpreted in terms of physiology and behavior.

137
00:12:55,240 --> 00:13:02,720
Okay so before we get into the company's approach to the analytics maybe take a moment

138
00:13:02,720 --> 00:13:11,080
to talk about heart rate variability and how that's calculated and why that number is

139
00:13:11,080 --> 00:13:12,760
important.

140
00:13:12,760 --> 00:13:19,280
Yeah so heart rate variability refers to beat to beat variation in the in the interval

141
00:13:19,280 --> 00:13:27,720
between the heart beats so each successive heart beat is having certain time instant

142
00:13:27,720 --> 00:13:34,200
in between them and this time difference between successive heart beats is varying as a

143
00:13:34,200 --> 00:13:42,000
function of time so and this variation is related to the functioning of the so-called

144
00:13:42,000 --> 00:13:50,840
autonomic nervous system which controls all our autonomic body functions like respiration,

145
00:13:50,840 --> 00:13:59,120
perspiration and blood pressure and heart rate among others so this autonomic nervous system

146
00:13:59,120 --> 00:14:05,640
is usually divided into two branches which are sympathetic and parasympathetic branch

147
00:14:05,640 --> 00:14:15,840
so this sympathetic branch is responsible for so-called fight-of-flight response so whenever

148
00:14:15,840 --> 00:14:23,840
our physiological resources are needed the sympathetic nervous system activates and prepares

149
00:14:23,840 --> 00:14:31,240
the body for the for the new requirements like increased energy expenditure or increased

150
00:14:31,240 --> 00:14:37,120
physical activity or increased mental activity and that is actually what happens when we

151
00:14:37,120 --> 00:14:45,800
have a body physical activity or when we have mental activity or in elevated stress for

152
00:14:45,800 --> 00:14:51,560
example it's the sympathetic nervous system which is activated which activates and that

153
00:14:51,560 --> 00:14:57,600
can be seen as an elevated heart rate and in broad terms reduced amount of heart rate

154
00:14:57,600 --> 00:15:04,960
variability the parasympathetic part is the other other nervous branch and that is responsible

155
00:15:04,960 --> 00:15:13,680
for cooling down the body after the requirements or immediate requirements of the situation are

156
00:15:13,680 --> 00:15:21,200
over so to basically recover the resources and reduce the heart rate, increase the heart rate

157
00:15:21,200 --> 00:15:31,040
variability rate and so on and I said this is in broad terms the functioning of these autonomic

158
00:15:31,040 --> 00:15:37,120
nervous system branches are seen in the heart rate and heart rate variability it sounds simple

159
00:15:37,120 --> 00:15:43,440
and the principle is quite simple but the challenge is that of course there's huge inter individual

160
00:15:43,440 --> 00:15:50,880
and also inter individual variants between in this especially heart rate variability they are affected

161
00:15:50,880 --> 00:16:00,960
by several factors like different substances and in addition to stress and physical or mental

162
00:16:00,960 --> 00:16:07,680
requirements also also our metabolism and and so on play a role there and people are different

163
00:16:07,680 --> 00:16:17,520
so their interpretation is quite challenging in fact this phenomena was identified years ago

164
00:16:17,520 --> 00:16:23,840
and there's been active research in the heart rate variability since early 70s but only

165
00:16:23,840 --> 00:16:31,920
quite recently during last last ten years so we have learned how to how to truly manage all

166
00:16:31,920 --> 00:16:39,040
these different factors which affect the phenomenon and how we can transform the information into

167
00:16:39,040 --> 00:16:46,400
something which is useful and in order to do that it requires an approach analytics that

168
00:16:47,200 --> 00:16:53,360
takes into account not just the you know not just kind of the raw data that you're pulling off

169
00:16:53,360 --> 00:17:02,400
of the sensors but also a model of how the body works is that right yeah that's right so for example

170
00:17:02,400 --> 00:17:08,560
our app wrote for the for the heart rate variability analytics is that we have constructed a digital

171
00:17:08,560 --> 00:17:18,800
physiological model of the human human body or body functions so we we have have figured out

172
00:17:18,800 --> 00:17:27,840
the physiological mechanisms for example how respiration physical activity heart rate mental activity

173
00:17:27,840 --> 00:17:34,480
how they affect each other how they are related and and and how heart rate variability reflects

174
00:17:34,480 --> 00:17:42,080
these activities and and based on this physiological model we can we can interpret the data

175
00:17:42,080 --> 00:17:49,360
we that's that's the that's the physiology part but then we use the data and data driven driven

176
00:17:49,360 --> 00:17:56,480
machine learning basically to tune the data in to be physiology or to be relevant for for its

177
00:17:56,480 --> 00:18:06,880
individual and and we have in fact a database of 250,000 individuals where where we have white

178
00:18:06,880 --> 00:18:16,480
reigns of people with different age both chambers different physical fitness and different BMI

179
00:18:16,480 --> 00:18:23,520
or weight and weight and and by using that data measured in in in daily during daily life in

180
00:18:23,520 --> 00:18:31,440
different conditions we can we can numerically adapt this model for it so that it fits for its

181
00:18:31,440 --> 00:18:44,320
individual and and produces kind of a personalized but yet standardized outputs can you talk a

182
00:18:44,320 --> 00:18:50,640
little bit about the physiological model to get us started what's the how is that expressed

183
00:18:50,640 --> 00:18:56,640
I'm imagining it's a mathematical model of some sort what's its kind of size shape level of

184
00:18:56,640 --> 00:19:04,720
complexity and is this something that ultimately lives in the device that that the and users

185
00:19:04,720 --> 00:19:13,200
wearing yeah if I start from from from the end of a question so yes yes it relies in it we have

186
00:19:13,200 --> 00:19:20,240
implemented this model into into software library and it indeed it is embedded into the

187
00:19:20,240 --> 00:19:32,160
these wearable devices these these two days super computers in a way and and and how it works is

188
00:19:32,160 --> 00:19:41,920
basically that that for each time instant we have a first first level is that we have a active

189
00:19:41,920 --> 00:19:49,680
segmentation of the data that that that we segment the data and and after that we classify

190
00:19:51,120 --> 00:19:56,880
calculate certain certain features of the data and and these physiological motivated features

191
00:19:56,880 --> 00:20:02,400
based on the physiological model like for example we transform the hard rate and hard rate

192
00:20:02,400 --> 00:20:11,840
rivalry data into into oxygen consumption or momentary oxygen consumption v02 or v02 and then we

193
00:20:11,840 --> 00:20:19,280
detect based on the hard rate rivalry we detect respiratory rate and and based on these these

194
00:20:19,920 --> 00:20:27,360
basically these two parameters described the level of of physical activity and and also

195
00:20:27,360 --> 00:20:34,960
respiratory rate is related to the also the physical activity and based on that we can classify

196
00:20:34,960 --> 00:20:43,840
whether a certain time instant is physical activity or or mental activity and after that we can also

197
00:20:43,840 --> 00:20:48,960
by looking at the hard rate variability and and whether they're very sympathetic or

198
00:20:48,960 --> 00:20:54,800
very sympathetic dominance we can classify its time instant whether that is more of physical

199
00:20:54,800 --> 00:21:03,440
mental stress or or physiological recovery and and accumulating from that we can calculate

200
00:21:03,440 --> 00:21:11,200
different kind of things like like a level of stress in during the day and amount of recovery

201
00:21:12,160 --> 00:21:21,120
from the from the v02 levels we also can calculate energy expenditure etc and when we combine

202
00:21:21,120 --> 00:21:28,880
issues like speed or or physical effort which is measured by the motion sensor we can also

203
00:21:28,880 --> 00:21:37,200
calculate what and and using the information like personal hard rate maximum and agent

204
00:21:37,200 --> 00:21:43,200
gender and so on we can we can calculate estimate the v02 max which is the fitness level of the

205
00:21:43,200 --> 00:21:49,440
individuals and things like that so it's it is there is actually we have certain white papers

206
00:21:49,440 --> 00:21:54,640
describing that but there is actually a multitude of different physiological mechanisms embedded

207
00:21:55,600 --> 00:22:01,280
I don't I think it doesn't make sense to go to details but just just to take as an example is

208
00:22:01,280 --> 00:22:06,560
that that we we have modeled that when you start a physical activity there is certain

209
00:22:07,280 --> 00:22:12,960
the hard rate increase is not immediate but gradual and it also depends on your previous

210
00:22:12,960 --> 00:22:20,880
physiological state for example in interval training we know that that the previous interval

211
00:22:20,880 --> 00:22:27,680
affects to the next one and we can have this kind of a on of kind of of mechanisms in the model

212
00:22:28,800 --> 00:22:35,520
and and those are taken into account so it's it's it's kind of a state state compartment model

213
00:22:35,520 --> 00:22:45,120
heavily heavily based on the physiology which is behind there and so you mentioned that there's

214
00:22:45,120 --> 00:22:51,840
a the physiological model and then more of a data driven model all of the metrics that you

215
00:22:52,560 --> 00:22:57,200
just mentioned are those all coming out of the physiological model before you even get to the

216
00:22:57,200 --> 00:23:06,080
data driven piece how it works it's basically that we have the principles are physiological but

217
00:23:06,080 --> 00:23:16,160
but the parameters or how it really works how you how the input data is processed and how the

218
00:23:16,160 --> 00:23:23,680
classification and and quantification of the different physiological parameters goes that's

219
00:23:23,680 --> 00:23:30,080
that's heavily data driven so we have used the big amount of data what we have to tune the

220
00:23:30,080 --> 00:23:38,080
parameters of the model and tune it also or to adapt it to its individual so that let's say that

221
00:23:38,080 --> 00:23:48,160
it data driven personalization of the model which is done there I don't know if you if that makes

222
00:23:48,160 --> 00:23:55,840
sense but well in in what way is it how personalized like is this

223
00:23:57,760 --> 00:24:05,680
machine learning type of a model that is running you know on the device itself or is it you know

224
00:24:05,680 --> 00:24:13,360
you're doing analytics using some type of machine learning that that spits out some number of

225
00:24:13,360 --> 00:24:19,040
different models and the you can choose between models on the watch how personalized are you

226
00:24:19,040 --> 00:24:27,600
able to to go with it we are using both approaches so when you take a new device into use you are

227
00:24:27,600 --> 00:24:34,560
asked to provide your background parameters like gender weight age hate and activity level and

228
00:24:34,560 --> 00:24:42,240
those are used to to put you broadly to the scale that what is typical for for an individual

229
00:24:42,240 --> 00:24:48,160
of your age and gender and fitness level but then when and that is kind of a

230
00:24:50,000 --> 00:24:56,480
machine learning or or database driven learning with what what we continuously update based on

231
00:24:56,480 --> 00:25:04,640
the data what we we accumulate from from our users but then then it's also adaptive model in the

232
00:25:04,640 --> 00:25:15,280
sense that that we use the we have algorithms which adapt the computation to your daily physiological

233
00:25:15,280 --> 00:25:24,000
or your normal physiological and and physical activity levels and and heart rate variability

234
00:25:24,000 --> 00:25:32,080
levels and so and so basically the devices get more accurate after couple of days of use when they

235
00:25:32,080 --> 00:25:38,160
have the algorithms have learned what is your typical level of heart rate variability and what is

236
00:25:38,160 --> 00:25:46,160
your your typical responses and your your real true heart rate minimums and maximums during

237
00:25:46,160 --> 00:25:53,920
different daily activities so so the model adapts to the user and that is why also these these

238
00:25:53,920 --> 00:26:00,800
devices are are typically designed for personal use if you would use for example share that

239
00:26:00,800 --> 00:26:06,960
these wearable devices with with some of your colleagues or your wife that would be suboptimal

240
00:26:06,960 --> 00:26:15,200
in the sense that that the the learning would not converge. The the learning that we're talking

241
00:26:15,200 --> 00:26:22,320
about here it's um I don't put sorry we asked this question we're not talking about like training

242
00:26:22,320 --> 00:26:27,600
in the sense of training a machine learning model that's certainly not happening on the on the

243
00:26:27,600 --> 00:26:36,720
wearable device it's more like tracking the the individuals data and adapting the model to the

244
00:26:36,720 --> 00:26:43,840
data over time. Yeah that is that's probably more accurate description of that yes so so they are

245
00:26:43,840 --> 00:26:52,800
there's in in the physiological model and which is being being broadly optimized for for your

246
00:26:52,800 --> 00:26:59,680
age and gender and weight and etc so so that is being more fine tuned to do their

247
00:27:00,480 --> 00:27:08,400
your exact characteristics based based on the adaptation there but it is correct it's more like

248
00:27:09,280 --> 00:27:15,920
adaptation. And you mentioned classifying a few times are you ending up having to do things like

249
00:27:15,920 --> 00:27:25,040
like uh like uh outlier detection and things like that where you're getting rid of noisy data

250
00:27:25,040 --> 00:27:32,320
from the sensor itself or does that happen at a lower level. We hope that most most of that happens

251
00:27:32,320 --> 00:27:39,040
in a lower level but in fact we we have a lot of that happening in the analytics layer as well

252
00:27:39,040 --> 00:27:47,280
so especially with optical sensors there is a lot of a lot of noise a lot of poor data and in fact

253
00:27:47,280 --> 00:27:56,160
we can we have we have built quite a lot of intelligence into into the separation or detection of

254
00:27:56,160 --> 00:28:05,280
poor and good appropriate quality data and what we actually do in the model is that when we detect

255
00:28:05,280 --> 00:28:14,720
that the input data is is not valid we we have a serum mechanisms to to bridge those gaps or

256
00:28:14,720 --> 00:28:24,880
or survive without without providing false outputs in in fact to an extent that for example our stress

257
00:28:24,880 --> 00:28:32,960
and recovery analysis is still still providing a quite valid outputs up to the error level of 50

258
00:28:32,960 --> 00:28:40,720
percent which is quite high so so as long as as more data which is coming in is is reflecting

259
00:28:40,720 --> 00:28:46,400
the real heart rate then we can we can provide reasonable analytics of course when when there is more

260
00:28:46,400 --> 00:28:54,160
false data than than real data then there's little one to do but right right right and so we've

261
00:28:54,160 --> 00:29:00,720
talked about primarily about the things that are happening on the device itself but you've also got

262
00:29:00,720 --> 00:29:12,240
this warehouse so to speak of data from individuals what are you you know in addition to kind of the

263
00:29:12,240 --> 00:29:18,240
work you're doing to develop the models for the device are there other things that the company is

264
00:29:18,240 --> 00:29:25,440
doing to you know with machine learning and analytics to take advantage of that data

265
00:29:25,440 --> 00:29:35,200
yeah so we have we have two other businesses and one is we provide corporate wellness service

266
00:29:35,200 --> 00:29:44,160
called a lifestyle assessment and there the idea is is that we basically sell for cooperates

267
00:29:44,160 --> 00:29:50,320
why interested to improve or promote their employer well being an assessment service where

268
00:29:50,320 --> 00:29:58,000
employers employees get a heart rate monitor special heart rate monitor for three days it's

269
00:29:58,000 --> 00:30:05,360
based on easy chisel it's very accurate to one millisecond time resolution and they do three

270
00:30:05,360 --> 00:30:13,040
day heart rate variability recording together with the diary and by using those those heart rate

271
00:30:13,040 --> 00:30:20,320
data and the diary data we construct a report and and give feedback to individuals about their

272
00:30:20,320 --> 00:30:28,000
physical activity sleep stress and recovery and fitness level and things like that and that

273
00:30:28,000 --> 00:30:34,560
those are also compared to the recommendations like physical activity recommendations sleep

274
00:30:34,560 --> 00:30:44,800
recommendations and what is normal at their age and gender and that is used very often in this

275
00:30:44,800 --> 00:30:52,640
kind of wellness campaigns where where people are are motivated to improve their lifestyles

276
00:30:53,440 --> 00:31:00,880
and improve their stress management skills so it can be used as an initial initial

277
00:31:00,880 --> 00:31:09,760
measure to to assess the starting point and identify the exact activities they should they

278
00:31:09,760 --> 00:31:16,720
should do to improve their lifestyles and and that is something as we sell as a service and that

279
00:31:16,720 --> 00:31:23,200
is actually a one source where we get a lot of data we we do we did last year about 50,000 such

280
00:31:23,200 --> 00:31:31,760
assessments of both in in Europe in Finland UK Germany Sweden now in Singapore as well not so

281
00:31:31,760 --> 00:31:38,240
much in US yet but that is actually a good source for data for us so because we we get this kind

282
00:31:38,240 --> 00:31:45,760
of a last year we got like 150,000 new datasets with with documentation and background information

283
00:31:45,760 --> 00:31:53,040
and diary and that is actually what we learn used for for the optimization and further further

284
00:31:53,040 --> 00:32:00,560
development of our our technologies which we use then for for consumer devices so yeah that is

285
00:32:00,560 --> 00:32:06,560
that is an interesting area but that that's not to say that that is just this this service is

286
00:32:06,560 --> 00:32:14,160
actually used and developed on its own it's not just for data collection but it's really just

287
00:32:14,160 --> 00:32:20,880
just to for corporate wellness business and and we we see that actually there are there's a lot

288
00:32:20,880 --> 00:32:29,840
of a lot of interest there about 95% of the people who who make the assessment are find it very

289
00:32:29,840 --> 00:32:36,640
very useful and and they recommend it for the others so so it's we looking at the at how to how

290
00:32:36,640 --> 00:32:44,320
to scale it up and how to bring make it more broadly available at the moment. How does the company

291
00:32:44,320 --> 00:32:54,080
organize to tackle analytics types of problems or challenges given the the strong domain expertise

292
00:32:54,080 --> 00:33:03,040
required but also the you know the analytical requirements to for machine learning and in

293
00:33:03,040 --> 00:33:11,680
traditional analytics. Yeah so we have a here in Finland we have a team of of data analytics

294
00:33:11,680 --> 00:33:18,640
analysis experts and and then we have a team of physiologists and in fact we have organized it

295
00:33:18,640 --> 00:33:26,560
so that these two teams are working as a single single bigger team so they work on a daily basis

296
00:33:26,560 --> 00:33:34,160
with each other so they are people who are who are from data science and and they are sports

297
00:33:34,160 --> 00:33:42,320
physiologists and and physiologists who are expert in the physiology and and and that's the big

298
00:33:42,320 --> 00:33:48,720
big thing what we are doing doing that that they are they are not they are basically never doing

299
00:33:48,720 --> 00:33:55,600
a single project that that only only one one size of that expertise would be present then of

300
00:33:55,600 --> 00:34:02,640
course we have engineers with software software background and I'm so on to transform all of that

301
00:34:02,640 --> 00:34:09,280
into into something which can be delivered to customers and or or implemented into our

302
00:34:09,280 --> 00:34:15,600
services but but the core thing there is that the maturity of that R&D or intellectual work is done

303
00:34:15,600 --> 00:34:28,160
by by the this data scientists or physiologists. I guess I'm curious about the opportunities that are

304
00:34:28,160 --> 00:34:38,560
created as you know we advance you continue to advance in our knowledge of applying machine

305
00:34:38,560 --> 00:34:43,520
learning and you know even some of the newer techniques like deep learning that you know might

306
00:34:43,520 --> 00:34:51,280
not necessarily be applicable to running on a watch but do you have any perspectives on what

307
00:34:51,280 --> 00:34:59,040
opportunities these technologies and the increased focus on these technologies creates for first

308
00:34:59,040 --> 00:35:10,000
beat. Yeah certainly I think we are we are just getting there that even though we have data from

309
00:35:10,000 --> 00:35:20,640
roughly to 250,000 individuals in our databases and so on it's still it's it's it's it's only today

310
00:35:20,640 --> 00:35:26,000
I think it starts to be in the scale where I would really truly call it as a big data. Also I

311
00:35:26,000 --> 00:35:34,240
do recognize that it's probably quite unique resource that very few few organizations would have

312
00:35:34,240 --> 00:35:40,240
or if if not if anybody would have have that amount of hard rate variability and background data

313
00:35:40,240 --> 00:35:46,320
available. So we are actually applying at the moment we're looking at things like how we could

314
00:35:46,320 --> 00:35:52,800
apply more more machine learning type of things and we have done some experimentation but

315
00:35:52,800 --> 00:35:58,640
but I have to say that to to date our perspective is very much that there are

316
00:35:58,640 --> 00:36:06,960
challenges with applying just pure machine learning without having having some kind of a domain

317
00:36:06,960 --> 00:36:15,680
knowledge to to constrain the this kind of a machine learning methods. There is a risk that you

318
00:36:15,680 --> 00:36:22,480
over over optimize to certain data sets and data on generalize and I think one of the one of the

319
00:36:22,480 --> 00:36:28,960
learnings what we have done and how why why our methods also work in the wearable environment and

320
00:36:28,960 --> 00:36:36,480
why they work in in almost all the individuals is that that they are based on this kind of a

321
00:36:36,480 --> 00:36:46,480
physiological principles and we use only only data to to kind of optimize those principles and

322
00:36:46,480 --> 00:36:53,520
make them numerically work and and in that sense we are not very heavily using using for our hard

323
00:36:53,520 --> 00:36:59,760
rate data we are not using this kind of pure machine learning like deep learning kind of methods.

324
00:36:59,760 --> 00:37:05,680
We have it and we are experimenting on that but I'm actually not the great believer that that

325
00:37:05,680 --> 00:37:12,560
would be the optimal way to go great great well okay this is a really fascinating conversation and I

326
00:37:12,560 --> 00:37:21,360
I appreciate the way that you're combining the physiological models with the the data driven

327
00:37:21,360 --> 00:37:28,400
models and I think clearly the domain knowledge is important here as is the case in in you know

328
00:37:28,400 --> 00:37:35,360
many if not all of the the use cases that folks are having success with so thanks for taking the

329
00:37:35,360 --> 00:37:44,320
time out thank you very much it was good for me as well all right everyone that's our show

330
00:37:44,320 --> 00:37:50,960
for today thanks so much for listening and for your continued feedback and support remember

331
00:37:50,960 --> 00:37:58,400
for your chance to win in our AI at home giveaway head on over to twimmolayi.com slash my AI

332
00:37:58,400 --> 00:38:04,720
contest for complete details for more information on ilka first beat or any of the topics covered

333
00:38:04,720 --> 00:38:12,160
in this episode head on over to twimmolayi.com slash talk slash 106 thanks once again to intel AI

334
00:38:12,160 --> 00:38:16,880
for their sponsorship of this series to learn more about their partnership with Ferrari North

335
00:38:16,880 --> 00:38:24,160
America Challenge and the other things they've been up to visit ai.intel.com of course we'd be

336
00:38:24,160 --> 00:38:29,840
delighted to hear from you either via a comment on the show notes page or via twitter directly to

337
00:38:29,840 --> 00:38:36,400
me at at sam sharrington or to the show at at twimmolayi. thanks once again for listening

338
00:38:36,400 --> 00:39:01,680
and catch you next time

