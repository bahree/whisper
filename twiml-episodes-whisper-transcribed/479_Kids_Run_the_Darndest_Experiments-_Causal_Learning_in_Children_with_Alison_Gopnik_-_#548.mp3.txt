All right, everyone. I am here with Allison Gopnik. Allison is a professor at the University
of California at Berkeley. Allison, welcome to the Twoma AI podcast.
I'm very happy to be here. I'm really looking forward to our conversation. We will be focusing
in on a presentation that you'll be delivering at this year's NERVS conference focused on
causal learning in children and how that relates to some of the things that we're doing in AI
and deep learning. Before we dive into that though, I'd love to have you share a little bit about
your background and you come at things from a psychology perspective. Tell us a little bit
about your background and how you came to that field. Yeah, so I actually started out my career
in philosophy and I'm still an affiliate in philosophy. So I have sort of three different
hats at Berkeley. I'm in the psychology department and affiliate in philosophy and then part of the
the Bear, the Berkeley AI research group. And the big question that I've really wanted to answer
my whole career is this, how is it that we can know so much about the world around us from so
little information? All we have is the photons in the back of our retinas and the disturbances of
air at our ears and yet people can figure out that the world is full of objects and people and
thoughts and ideas and, you know, quirks and black holes. How is that possible? And of course,
that's the big problem of epistemology and philosophy, which is where I started out. How could
you possibly do that? And it's the big problem of machine learning. It's the sort of central problem
of machine learning. How could we get representations from data? And in a way, the way that the big
problem, I mean, a way of putting this is we seem to have these very powerful abstract structured
representations of the world around us that let us make great generalizations and predictions.
And yet the data that we're getting doesn't seem to is concrete and particular and doesn't seem
to have those characteristics of being abstract and unstructured. So the question is how do we get
there here from there? Well, very early in my career, I realized, look, the people who are doing that
most effectively are actually young children. They're the ones who are going out into the world,
taking what they see in here and the actions they perform and figuring out what the world is like.
So if we want to answer that question, either as philosophers or as people in AI, the people we
should look to, the people who are really solving that problem are young children. And basically,
that's what I've been doing for my whole career is trying to figure out how is it that young children,
you know, two, three, four year olds who don't aren't, you know, don't have PhDs in computer science,
aren't philosophers. Nevertheless, seem to be solving these problems that have really stumped
the smartest minds in philosophy and AI. And about 20 years ago, I started actually collaborating
with people in both philosophy of science and in computer science to try and figure out,
could we say something computationally? What kinds of representations and algorithms could the kids
be using that let them learn as much as they do? And that's basically been the project ever since.
And I think it's interesting that there's been, just in the past few years, there's been this,
this real explosion of interest within AI in trying to look at development and look at
look at children and use that as a clue to solve some of these really tough problems. And I think
it's because, you know, so much of the new work has depended on the idea of designing systems that
learn rather than trying to build things in in the first place. And if you're interested in systems
that learn, kids are really wonderful, probably the best example we have of, of creatures that are
really, really good at learning very accurately from small amounts of data. So that's kind of the
big overarching, that's the big overarching picture about what I've done. Fantastic. It sounds like you
think about the problem of learning in children, you know, broadly at the level of kind of these
broad representations, but your talk at NURPS is focused on a particular aspect of that. And that is
the way that children and for causal relationships and structure in the world. Tell us a little bit
about your talk and your research in that area. Yeah. So if you're asking this question, you know,
how do we get these big abstract, powerful representations of the world? One of the most
important, not the only one, but certainly one of the most important kinds of ways we have of
representing and thinking about the world is thinking about causality, thinking about what makes
what happen. And the first line of work that I did with children was what's come to be called
the theory theory. And that's the idea that children are being building everyday theories of the
world that are a lot like the way that scientists build theories of the world. And that's become,
you know, one of the most prevalent theories about how children solve this problem. They do something
like build everyday theories of the world around them. But of course, then the question is, well,
how do they do that, right? If you say that they're doing something like scientists, how do scientists
build theories of the world around them? And back in the arts, there was a whole bunch of very
exciting work showing that you could build causal pictures, causal representations of the world
from data. And that's not the only thing scientists are doing. It's not the only thing involved
in theory formation, but it's a really important thing in theory formation. Because if you have a
causal model, if you have a theory, if you have a representation, then you can solve these out
of distribution generalization problems. You can say, okay, well, if we, you know, if we do this
to the vaccine, even if it's something we've never done before, we can predict what will happen
because we have a causal model of how the vaccine works. And the advantage of thinking about
causal models, so there's an example of something that lets you generalize really broadly.
And it isn't just letting you generalize about a specific area like, you know, the way that
having a representation of everyday physics or a representation of the visual system could help
you to generalize. Because that really covers everything. It covers the way that you interact with
other people. It covers the way that objects work. It covers things that you've never seen before.
So if you had a way of figuring out what makes what happen. If you had a way of figuring out
causal structure, you'd have a really, really powerful tool for going out into the world and
going out into the world and solving new problems and making new predictions.
One thing that's really important about causality, and a lot of people think of it as sort of being
the thing that makes causality different from, say, just correlation of the sort that typical
deep learning programs have done, is that causality lets you intervene, is the word that people use.
It lets you decide what to do and think about what the consequences of that are.
And causality lets you make counterfactual inferences. So it lets you, if I know that smoking
causes long cancer, for instance, if I think that that's right, then I'll know that even though
smoking is correlated with yellow fingers, washing your fingers isn't going to help change the
cancer rate, but not smoking, getting people to not smoke is going to change the cancer rate.
And it also means that I can, say, do some counterfactuals. So I can say, well, look, if we had had
not anti-smoking programs earlier, we would have saved more people from cancer.
So that ability to do interventions and counterfactuals is a very powerful aspect of causality
and causal representations. And back in the arts, people like Clark Gleymor and Peter Spirties
at CMU, and notably, probably most famously, Judea Pearl in UCLA, started developing these
formal models for how those kind of causal representations could work computationally.
So causal-based, graphical probabilistic graphical models were the kinds of representations
that they had. And back in the arts, we started trying to see our children doing something like
inferring causal-based nets from data. And amazingly, much to everyone's surprise,
it turns out that even if you're looking at two, three, and four-year-olds, they're really good at
doing that. You can give them a pattern of data of conditional probabilities, and they'll pull out
the right causal consequences from that data. So they seem to be doing something that looks like
a very effective causal inference. Going back to the theory theory, what were the alternatives
to the theory theory before that theory? And how did you demonstrate that the theory theory was
you know, was predictive and had merit? Yeah, right. So that's a great question. And in fact,
I think people in ML and AI should be very familiar with what the alternatives work,
because they're the alternatives that we have in ML2. So one alternative, again, going back to
really play-do in Aristotle, is look, it just looks as if we have all this abstract structure
and representations. Really, it's just you've looked at a whole bunch of data and you pulled out
the statistics of the data, and that's letting you make predictions. And that was one thing that
the sort of what's sometimes called the empiricist option was, okay, maybe it's not that children
have these abstract representations. Maybe they're just just following the data. And it just looks
like they have the abstract representation, because they have a whole lot of data and a whole lot of
observations. And then the other option, which again is very active in AI now, is well, look,
maybe they aren't actually learning the representations. Maybe they're just built in. So
work that's being done in AI now suggests that you might have built-in
constraints, inductive constraints that are like assumptions about how the world works,
assumptions about how physics work, assumptions about how people work. And if you just build those
in in the first place, then you can help to solve the problem. So those were the two options that
were and still are on the table. And I think those of us who actually look at young children learning,
you know, there are probably elements of both of those that are right, but it doesn't look like
either of those is what's happening, because from the time we can test, and this is where the great
methodological and experimental advances in developmental psychology kick in, even very little babies
already seem to have these abstract, powerful representations of the world. That's very different
from what a previous generation thought about about babies. But at the same time, even three
and four year olds, this is work that we did back in the eighties, are really changing what they
think about the world based on their experience. So they seem to be have abstract representations
and be learning and changing those representations from the time they're very little. And theory of
mind, for example, which is something that people are thinking about in AI as well, that ability to
understand what's going on in someone else's mind, that was the work that we did back in the eighties,
that I think really clearly showed that children have this succession of every day of every day
theories in the world. And then in the more recent work about causal inference, what we could show
is not just that they, you know, will have an abstract theory and then another one based on the
data, but we can actually say something about what those look like, what the representations look
like, and how the representations change. Does the theory theory imply that children are
taking an active role and kind of recognizing that a theory is a theory and testing the bounds
of that theory and experimentation? Yeah, exactly. So that's the thing that is the new work that we're
really doing now. So we showed over an extended period that children could could do this, they could
get statistics, they could infer causal, infer causal structure. And you might wonder how on earth
could you do this with like two year olds, right? I mean, if you asked most grownups, what does
conditional, this pattern of conditional dependency is indicate a causal chain or a common effect,
they would be not know what you were talking about. And the way we did it is we have a little
little machines, one of them is called the blanket detector that you'll see in my talk,
the little box that lights up when you put things on it plays music sometimes and doesn't,
other times. So it's a new causal system. And the kids challenge is to figure out how it works,
figure out which things are blankets. Well, blankets will make it go, which things are blankets,
and then they have to make it go themselves. So without actually asking about causal structure,
we can see what kinds of inferences they're making and we can control what kind of data we get
from. So we can give them different patterns of data and then we can see, then we can see what
they do, we can see what kinds of inferences they make. And as I said, to a remarkable degree,
the kids are making the right kinds of inferences. If they were little Bayesian hypothesis testers,
again, to get back to the theory theory, give them two hypotheses and they're picking the ones with
the best posterior probability. But the big question is, so that that's really impressive. But then
we still have this question about how are they doing that? What's happening that's letting
them solve that problem? Because of course, the big issue with Bayesian reasoning and with probabilistic
generative models in general is if they're interesting at all, the search space is enormous.
So if you think about even a Bayesnet with four or five causes, you very quickly have a very,
very big space of possibilities. And the question is, how do you limit that? How do you search through
that space? How do you solve that problem? An idea that we've had and a lot of people in AI have had
now is that active learning, to get back to your question, active learning experimentation,
that's the way that scientists solve that problem. They are not just stuck in your main frame
with data pouring over you. You can actually decide which kinds of data you want,
depending on what kind of hypothesis you're testing. And for causal work, again, because of this
intervention quality of causation, you can specify pretty clearly, here's the kind of experiment
you should do. Here's what you should do. Here's how you should wiggle X to see if Y works.
And if you think about little kids, I mean, like that's their entire life, right? I just,
I just, it's a kind of nice, it's a kind of nice convergence. I just spent a, a, a sabbatical at
Mela in Montreal, where, yeah, she was Benjiro and his colleagues are doing fantastic work just
about this question about causal inference and causal models. And at the same time, I was
visiting my one-year-old grandson. And if you watch my one-year-old grandson, basically all he
does is do experiments. I mean, you know, he, he will occasionally eat if his mom gives him some food
and, and other than that, what he's doing constantly is doing experiments. And what all the rest
of us are doing is trying to keep him from killing himself by, by doing experiments. And, you know,
it's funny, we just sort of take for granted. Oh, okay, look, I'm looking at this one-year-old and
look what he does. He takes the spoon and then he bangs it on the pot and then he turns the pot over
and then he sees if he can stick the spoon in the light socket and so on and so forth, we just take
that for granted. But why would they be doing that, right? I mean, that's a lot of, a lot of physical
energy going on to just go out and try things in the world. But if you think of them as being
these causal inference, active causal inference engines, that's exactly, that's exactly what they
should be doing. And what we're doing now is we're in collaboration with some people at Mela and
at Berkeley and actually at Google Leap Mind as well. What we've done is to set up these environments
in which you can find out about causal structure by doing experiments. We have our kind of virtual
version of our blanket detector. And what we can do is see what do kids do when you just let them
loose in this kind of environment. And how does that compare to various kinds of causal learning
algorithms you might have? And if, you know, if you think about it like a classic RL algorithm,
for example, is not going to do the kinds of things that are the best experiments because the
classic RL algorithm is just going to try and find the outcome and maximize it. And what you
need for experiments is to try lots of different things, change what you do based on what happened
before. So, but some of the things, again, like my colleagues, Deepak Prathak and Polkad
Agrawal and others coming out of Berkeley have these curiosity-based algorithms that seem to be
closer to what the kids are doing. And I think if you kind of combine the curiosity-based idea,
the idea that what you're trying to do is get a system that will make predictions and frustrate
them and explore, and the causal models idea, that could be a very powerful mechanism for solving
some of these search problems. So, what degree does your work begin to
kind of articulate, explore the structure of the or the complexity of causal relationships
that children are able to deal with at various ages? And what does that tell us about our
ML models? Yeah, so the first pass, both with Basinets and with our work, was just pretty simple
causal relationships. So, you know, here's one variable and another variable and does variable
X cause variable Y. And you can see the experiments you do to try and find that out. And then what
what Pearl and Glamour and colleagues had done was to look at more complicated things like,
is it a causal chain or is it a common effect structure or a common cause structure?
But starting in the sort of in the late arts, we started collaborating with people like the
cognitive scientist Tom Griffiths, who's at Princeton, Chris Lucas, who was a student of mine,
who was at is now in Edinburgh, to try and see if we could also make inferences about more abstract
features of causal systems. So, for instance, could I infer not just, is this blanket detector,
did this block make the detector go or not? But is the detector deterministic or stochastic? Or
does the detector work with a conjunctive logic? You need two things to make it go in combination,
or does it have a disjunctive logic? Each cause is separate. And what we've shown is that kids are
quite good at, even again three and four year olds, are quite good at making inferences about
those more abstract features of the system as well. And to get back to machine learning, you know,
the more abstract your representations are, the more powerful your generalizations are going to be.
So, the kids seem to be quite good at even making those more abstract inferences. And something
that's really interesting is that the kids are actually better at doing that than adults are. So,
well, here's the thing, and this is another point about children and childhood in general.
So, if you give the adults a structure that's really common, that they have a strong prior
for, then they're good at making inferences. But how about if it's something that's kind of weird
and unusual? So, it's an abstract feature that isn't as obvious, and you don't have a stronger
prior form. When you do that, the kids are actually better than the adults. And in a sense, the kids
lack of knowledge, lack of previous knowledge is really an advantage when you want to explore
the space, explore the space more widely. And I think that leads to another big point that I've
made that I'll be making in my talk, which is that actually just being, it gets, gets back to my
one-year-old, just being a kid, just the fact that you have this period of childhood before adulthood,
that in and of itself may be something that humans use to solve this problem. And the argument
that I've made is, again, think about that search problem. One of the reasons the search problem
is so challenging is because there are always these explore exploit trade-offs, right? So,
one of the things that we learned at the very beginning of computer science was that explore,
exploit trade-offs are a bear, and there isn't any simple or optimal way of resolving them.
But one idea that people often have is, okay, when you look at the actual algorithms that are
trying to deal with explore exploit tensions, is start out exploring. And in particular,
start out with these very wide, high temperature, bouncy, noisy kinds of searches that get through
a whole bunch of the space. And then, once you've done that, narrow in and exploit. And the idea is
that that keeps you from, you know, getting stuck in local optima, it keeps you from, it keeps you from
settling on a particular option too quickly. And what I've argued is you could think about human
life history as they call it a biology, human development, as being evolution's way of, so one of
the ideas is often a simulated annealing kind of idea. So you start out looking really widely,
and then you really, with a really high temperature, and then you cool off. And my slogan is that
childhood is evolution's way of solving explore exploit tension and doing simulated annealing.
So if you sort of say, who looks like they're bouncy and random and noisy and trying lots of
things that are not very effective versus who looks like they're narrowing in, using a lot of their
prior knowledge to do something effectively, you'll see that first, you know, that's what a one-year-old
looks like, and as opposed to what an adult looks like. So part of the idea is that if we actually
built in a developmental sequence, we sort of let AIs have be children for a while, that might,
we might also get some clues about how to learn more effectively. And this kind of trade-off that
you see where the children need a lot of care, they need a lot of people around them looking after
them, but that gives them this chance to go out and explore. That might be a trade-off that's
relevant for AIs as well. And I should say in some of the other things that I'm going to talk about,
what we've, so one thing that we've been doing is looking at how children are using
experimentation and exploration to try and solve these problems. So one thought that
that brings to mind for me is in some ways could you say that the natural machine learning
cycle kind of resembles this childhood adulthood in a sense that training is kind of like childhood
and inference once the model is kind of built and fixed and that's kind of adulthood.
Something like that I think is right. Yeah, I think the idea is that you have a period of learning
and then you have a period of actually using what you've learned to go out and make inferences.
But the kind of learning that the children are doing seems to be much more
wide-ranging than the kind of learning that a typical machine learning system is doing.
And something that I think is really interesting is when you actually look at the practicalities
and talk to people about, well, how do you solve these exploit problems? A kneeling shows up
again and again and often in multiple cycles where you'll, you know, do heat things up,
cool things down, heat things up, cool things down. But there isn't a kind of general theory
about how to do that as far as I can tell or about why that works. And again, looking at
kids might give us some clues about, you know, how does a kneeling work in the wild when you see it
in children? One of the things that I think is really cool about this idea is if you think about
exploit trade-offs, things that look like bugs from the exploit perspective might actually be
features from the explore perspective. So, for instance, having a system that's noisy that has
a lot of variability is not good if what you want is to make inferences and act effectively,
but it is good if what you want is to be able to learn as much as possible. So, many of these
things about kids that have seemed like defects, like the fact that they're curious all the time,
that they're kind of unpredictable, that they are variable, that they're noisy, those might
actually be advantages from the perspective of learning. And I think we don't have a good
theoretical account of how that all works and thinking about the kids could help.
What would you say are the closest ways that we're approximating that in the machine learning world?
People like Joshua Benjo and his colleagues and others are trying to, now, and I think this
is going to be the way for the future, is to try and design hybrid systems that have some of the
power of the, that can use some of the power of machine learning, but then can also have some of
the structure and generalization of a causal system. So, Joshua has these flow nets that are
trying to do that, so they're trying to add a layer of, a layer of further structure on top of
a kind of classic machine learning system. And RL is interesting from this perspective too,
because you can argue that, and people have argued that reinforcement learning in the psychological
sense is like the most primitive form of causal learning. As opposed to just picking out correlations,
what happens in reinforcement learning is that you do make an intervention and you see what the
outcomes are. And it's important that you're actively going out and making interventions and
seeing outcomes. But typically in RL, what you're, the outcome of that isn't a model as much as
just the fact that you're more likely to make those inferences or those policies later on.
So something like model based RL ends up looking a lot like causal inference, it ends up looking
a lot like causal structure, and that might be a relevant, that feels like that's a relevant
outcome. But I think it's important that the objective functions for a system like that
would have to be things like information gain or like knowledge or like curiosity,
rather than being things like how well you're scoring on, on some mention. And there's
really elegant work in our lives and others that show, for instance, that if you look at the kids
playing around, they seem to be acting in a way that will get them information. That information
gain seems to be sort of an objective function that describes what it is that they do.
What's an example of that? Well, there's beautiful work by my colleague Celeste Kid,
and she looked at really young babies, you know, 10-month-olds. And what they did was they showed
the babies different sequences of events that had different amounts of information in the
technical information of theoretic sense. And what they discovered was that there was this kind
of sweet spot, and they just measured how long the babies looked at each of these events,
and they discovered there was this sort of sweet spot if something was too random, too far removed
from where you were now. The babies wouldn't look, but they also wouldn't look at things that
didn't give them very much new information. There was this kind of sweet spot of just where the
information gain was going to really help you to make progress, and the babies looked the most
at those events. And we've been thinking about that, too. One of the problems with using
just using information gain, this is something that comes up in a lot of these curiosity-based
RL kind of algorithms, is what's called, you know, the TV problem. The problem is if you just
use technical information gain, then if you just put someone in front of a random TV with
a random static at it, you're getting lots of information in the information through
theoretic point, but you're not getting anything that's actually going to be very useful.
So I think one of the real interesting frontiers is this balance between
noise and structure, right? So the kids are noisy, but they're not just completely noisy. They're not
just acting like random agents. They're doing things that make sense, given the kinds of problems
they're trying to solve, the kinds of causal structures that they're trying to infer,
and how you get that balance between introducing noisiness and variability, and then also having
interventions and experiments that are relevant to the problems you're trying to solve.
That's something the kids seem to be really remarkably good at doing, and we don't quite know how to
characterize that computationally. I had cut you off to ask a question earlier, and you about to
mention some additional points from your talk over those. Yeah, so what we've been doing now,
so one thing that we've been doing is looking at how children are using active learning to
figure out the causal structure of the world in these online environments. Another thing that we're
doing is trying to see if, I mentioned that we've done work showing that children could get these
more abstract, what are some of us called, over hypotheses about causal structure, but one thing
that we're doing now is trying to see if kids can do things like actually decide what the right
causal variables are. So a big problem, so it's easy to say, okay, look, I'll tell you, you know,
here's variable X, and here's variable Y, and then you can see whether they're dependent, and see
if there's a causal relationship between them, but how do you decide which variables to look at in
the first place? And this is a big problem for ML as well, where it turns out that, you know, in the
classic adversarial examples for something like ImageNet, it turns out, well, wait a minute,
no, the system is actually not even dividing up the world in the right way. It's paying attention
to, you know, fine details of the texture instead of paying attention to the objects,
and it might look as if it's making the right kinds of inferences, but it isn't really,
because it just hasn't divided up the world in a way that makes sense from the perspective of
different variables. You know, if you're a scientist, there's all sorts of classic examples like,
you know, it turns out that if you're looking at the relationship between cholesterol and heart
disease, oh, there's actually two different kinds of cholesterol. One of them makes heart disease
more likely, one of them doesn't. So if you just didn't, if you didn't have the right measures,
if you just looked at cholesterol overall, you wouldn't see the relationship, and then it turns
out that, then it turns out that you can. And it turns out that kids are actually very good.
We have some really beautiful experiments where kids are actually good at picking out, oh, okay,
this is the variable, this is the object, or this is the property that is the one that I should
be looking at for purposes of trying to do causal inference. So they're simultaneous. Yeah,
so this is, this is my brilliant graduate student, Marielka, do. So what we do is we set up a
situation where the kids have to figure out their little Shelley, the turtle, wants to grow
cactuses. And he likes some cactuses. He likes the ones with round things on them, but not the
spiky ones. And we're trying to decide how can we help him grow his cactuses? And there's two
different things we can do. We could put the seeds in different colored flower pots, or we could
have different colored water incants that are water and seeds. And we're trying to figure out
how do we make sure that Shelley gets the cactuses that he does? So we've set up, here's these two
potential variables. It could be the water incants, it could be the pots. And what we can show is that
if we show children that the water incants make a difference and the pots don't, and now we give
them a new case, this is a new cactus, a new water incant, a new pot, they'll say, okay, the water
incant is the thing, I hope I got that right, the water incant is the thing I should be paying
attention to. I should be playing with that water incant. I should be changing it. We should
be doing things to that water incant, because I've already figured out that the pot really isn't
relevant to these differences. So in philosophy, they talk about this as causality being
difference-making. The causal thing is the thing that makes a difference. And the kids already
seem to be saying, okay, which things in my world are the things that make a difference? Which are the
things that I can control and change and make a difference? And again, if you think about an AI
system, if it was paying attention to, think about, you know, RL, part of the problem with RL is
it's just paying attention to everything in the image, right? And if it could say, oh no, these
are the things that I should be wearing. These are the things that, you know, think about like the
classic example where you see a robot being trained with RL. And it's doing all this ridiculous
stupid stuff that isn't going to make any impact at all before it kind of stumbles on the thing
that might work. If you could start out having that robot say, oh, okay, I know these are the things
that are going to make a difference and these things aren't. It would be much better all. And if it
could learn that, that would be even better. Does your research venture into the role of biases learned
by children in the process of their exploration? Yeah, one of the things that we've, one of the things
that I mentioned before was about the fact that children aren't just using these causal inferences
to make draw conclusions about, you know, blanket machines. They're using them a lot to draw
inferences about other people. And actually, something that we haven't published yet were just in
the middle of doing it. For example, suppose kids see that there's a bunch of kids who are playing
together and kids who have, you know, a particular kind of funny glasses are being welcomed into
the group and kids who have a funny hat are being shunned, for instance. So they just would be seeing
those, seeing those patterns. Are they going to conclude that there's a difference between the
people who have the glasses and the hats? Even if, I mean, that's a nice example of variable
selection, right? You know, kids start out not thinking that this difference is going to be important.
But if they see that people are being treated differently, they might very well end up
concluding. And in fact, we have some evidence that they do conclude that, that those people are,
those, those are different groups. Those are people who should be treated differently, for example.
So I think it's quite plausible that part of what's happening is that kids are paying attention
to these social differences. And then they end up having various kinds of biases as a result.
And it's an interesting question about would, you know, it seems plausible that an AI might very
well reproduce that. And we might be worried about how we could counter how we could counter that
both for the children and for the AI's. So what are the main takeaways that you want to leave
the folks that are hearing your talk at the causal inference and machine learning workshop?
So I think there's two. One of them is, this may be a little preaching to the choir,
is that causal inference is really important. It's a really powerful, it's a really powerful
technique causal representations give us lots of advantages. And, and we sort of made some progress
computationally on causal inferences and representations. And that, that's a really,
that that's exactly the, the technique we'd need to solve some of the limitations of our current
systems. But then in a way, the even more important idea is that looking at little kids,
which is not something that typically people in AI have done that feels like, oh, wait a minute,
you know, the world of people who are sitting in little chairs and playing with three
year olds is completely different from the world of computer scientists. And I think it's kind
of wonderful that that computer scientists have realized, oh, wait a minute, you know,
these little kids who we weren't paying any attention to, we thought they were just, you know, kind of,
I don't know, um, uh, mushy stuff that wasn't like what we do in AI. That those are,
that those are really, uh, you know, those are those kids might really have the clue to designing
new systems. I think that's the really big point that I want to make. And, and I think back and forth,
by looking at kids who are such great learners, we can figure out how to make more effective AI.
But also, by looking at AI, we can figure out what's going on in those kids brains that makes
them such effective learners. I think that's a really, really promising, exciting, uh,
line of research and one that we're starting to do when I hope we'll continue to do.
That's awesome. So, AI researchers go play with your kids. Exactly. Yeah, I mean, I think,
and, you know, I think this might be an example as well where,
you know, the diversity issues are really relevant, right? So, you know, I think part of the
reason to be frank, why kids haven't played a bigger role in AI is because kids are kind of girl
stuff, right? Um, uh, you know, they're, they're things that people who are off raising families are
paying a lot of, of paying a lot of attention to. And the people who were doing AI and the people
who are raising the families haven't necessarily been the same people. And I think it's a kind of
tribute to the way that we have a much wider, more diverse group of people being involved in AI,
including more women, more people who are raising families, that that's a nice example where that
actually turns out to contribute to something that's really basic to the science. Yeah, yeah.
Awesome. Awesome. Well, Allison, thanks so much for joining us and sharing a bit about your talk and
what you've been up to on the research front. That's great. Thanks for having me.
