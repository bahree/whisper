Alright everyone, welcome to another episode of the Twimble AI podcast. I am your host Sam
Charrington, and today I'm joined by Jeff Gailhar, Vice President of Technology at Qualcomm.
Jeff, welcome back to the podcast. Thank you very much. It's great to be back again.
Great to have another conversation with you, Sam, about what's going on in AI.
I am looking forward to digging into our conversation. It's always a lively one. We were talking
a little bit about what was going on in your world in our pre-call, and one of the points
that really jumped out at me that we'll be exploring in the conversation today is kind
of this divergence that's happening in the industry between model complexity, increasing
and device power in some cases decreasing, in the case of like IoT devices. But before
we do that, it's been a while since you've been on. I'd love to have you share a little bit
about your background and role with our audience.
Sure. Thank you very much again for having me. Look, so I'm VP Technology. I had Qualcomm's
AI software portfolio, and so we'll of course talk about the Qualcomm AI stack, and some
of the challenges you and I have talked about. But my role really is to provide a platform
of software solutions on top of our AI hardware that spans Qualcomm Silicon. Then
enables us to address market segments from cloud all the way to IoT, including automotive,
mobile and so on. So that's really what my team is focused on doing.
Just for context, talk a little bit about the relationship between your focus and that
of Qualcomm AI research. As you know, I'm fortunate to have the opportunity to talk to a lot
of your colleagues and research about their latest projects and papers at conferences.
How does that work relate to what you do?
Really the way to think about it is that there's sort of three prongs for success for AI
at Qualcomm. One is hardware. Of course, we're a Silicon First company in a lot of ways.
Software, which we just talked about. And then the research, which really is, you know,
drives our innovation. And so my job is to work with Qualcomm research, to work with
the hardware teams, to kind of bring a lot of those innovations forward, either in the
form of, you know, hardware software code design, where we're maybe changing hardware
and software together to address a new challenge, a new innovation in AI, say, Transformers.
And this kind of thing that's a new, relatively newer technology. Or we're bringing techniques
for optimizing networks, you know, they have a big focus on power optimization and optimality,
bringing those techniques into the software stack so that practitioners, developers and
practitioners in general can take advantage of those techniques in a way which doesn't
require them to read a paper or write their own code or whatever. That's part of my
job is to kind of bring that, you know, mass market kind of way to our end users.
One of the things that surprised me recently in, you know, having the benefit of conversations
with the folks on the research side and the folks on the, the enterprise side, the product
side is how quickly things can move from one to the next. I'm thinking in particular
of conversation that I had about using neural nets to, you know, on the research side,
using neural nets to kind of tune radio parameters that was, that was with Joseph Soriaga.
And it seemed like, you know, just a couple of months later, I was speaking with folks
about that being, you know, kind of in the wild or starting to make its way into the hardware.
Any, any perspective that you can share on that is has that kind of velocity shifted over
the years that you've been at Qualcomm or are there things that are impacting it or changing
it?
I think it's an area, thank you for the compliment because sometimes we feel like we don't move
fast enough. I think that it's an area we have focused on really kind of tightening, trying
to tighten the, the innovation cycle from research to product. And I think it's just a pleasant
product of us focusing on it and us really trying to be more strategic about this hardware
software co-design, which is kind of foundational to bringing these innovations quickly. We,
we've had this sort of virtuous three way, you know, cycle going for a couple of while
we're probably into our seventh rate generation product now. And that really bears fruit because
we have a much clearer perspective on the use cases that our customers want in driven
by the innovation and research. And then when time, you know, ripens to put one of these
innovations into practice, we have found that we're pretty well prepared to make that happen
quickly, right? So it's that whole sort of internal ecosystem we've developed and we have
focused deliberately on trying to increase the cadence by which we can take research and
bring it to the marketplace.
So let's talk a little bit about the, this idea that I mentioned earlier, kind of increasing
model complexity and kind of decreasing device power. I guess, you know, some devices in any
particular class of devices, they're getting more powerful, but the diversity of the types
of devices that you're trying to run machine learning models on is increasing, including
lower power devices. Can you just talk a little bit about what you're seeing out in the ecosystem?
Yeah. So, so I think, you know, traditionally, if you kind of rewind a little bit, you know,
a lot of focus on CNNs. And of course, in their heyday, CNNs were, you know, very demanding
workloads, right, for the hardware and the software that we had at the time. And it's not like
that demand has gone away, but as you know, the industries move towards higher complexity,
CNNs, higher resolution inputs, transformers, which are, you know, very powerful, but are
higher complexity in a lot of ways than CNNs. And the, we've gone from, from delivering AI
into mobile devices, which is kind of where Qualcomm, you know, has its heart and soul into IoT
devices, which are obviously really constrained. So think about home robot, think about a ring doorbell,
this kind of thing. And markets like XR are particularly challenging because you have high
complexity networks involving hand tracking or gestures or super resolving the displays and
stuff, right, foviated views, that kind of thing. And that's a thing, that's a device you want
to be compact, you want it to sit on your head, you can't, you know, make your face or head too hot.
So there's power constraints, right? So there's a lot of concurrency going on, which drives complexity
and their battery limits, right, and thermal limits. And so at the margins of these, you know,
let's call them simpler devices. We, you know, we find an increasing challenge to take these
complicated and, and concurrent workloads and pack them into these devices. And at the other
extreme, you can think about an automobile is having similar characteristics, tons of cameras,
tons of concurrent workloads. You think relatively a lot more power, but still there, thermal and
power is a consideration. And of course, other factors like safety, right, did you have to also factor
into your workloads? So kind of a explosion of diversity, where we're trying to build a common,
we'll talk about this, I hope a common architecture that lets the practitioners move workloads
one the other. So you think XR, but you know, in cabin camera and an automobile that's tracking your
eyes and making sure you're not texting when you're socially looking out the windshield is not in
some way so dissimilar from an sort of outside in XR workload, right? So you see kind of analogies
across these markets for different kinds of applications, right? So we want to build a common
kind of architecture that lets us do that. From the developer perspective is the common
architecture in place because you see the same types of applications across one device to the
next or, you know, one use case like XR to audio or is it not so much the same applications,
but you want the developer to be able to reuse their skills in the different environment.
It's a little bit of both. So we can talk about it. We recently announced the Qualcomm AI stack
at the kind of heart of that kind of the center of it is this Qualcomm AI engine direct layer,
which provides a API abstraction that spans across all of our chips from our cloud product all
the way to our, you know, IoT products. And like I said, all the sort of products in the middle.
And that is directly aimed at, you know, developers, practitioners. And I use that word broadly because
that could mean a hands at OEM. It could mean an application developer and so on. And that is the
contract really that we're building between the runtimes and the training frameworks to get
on to Qualcomm silicon. So we're going to provide sort of out of the box as part of that our own
Snapdragon nor processing SDK, which is our traditional runtime that we provide on our silicon.
It's built on this common block. We're of course providing like TensorFlow Lite delegate plugins
where we're going to Microsoft and Onyx runtime plugins. And that API will be open to anybody who
wants to build sort of a direct to Qualcomm silicon, you know, metal runtime of their own,
whether their runtime is just an application or whatever. Now we do also see, you know,
application providers that want to provide basically the same application on like compute devices.
So think Windows on Snapdragon, I think ARM powered Windows devices and be able to move that,
let's say to like a handset device, okay. And they want to be able to have basically the same
application, right. And so we're providing the same silicon acceleration and now we're providing
the same software platform and they can very easily move their application if you will from cloud
or from device to device onto the device. And we've demonstrated that with one of our leading
partners and and shown how the same application can be sort of cloud powered or device powered.
And then you can split the workload using the same, you know, abstraction.
Can you kind of walk us through it? You know, as you're talking to these developers,
yeah, what are kind of the real real world challenges that they run into when they're trying to
take advantage of neural networks on device? I mean, we talk about some of them, you know, power and
the kind of compute constraints of those devices, but kind of take us to the next level of detail.
Like what really, what are the pain points that the developers run into?
Yeah, I think I think one of the key ones is when we think about network optimality,
you know, there's a lot of dimensions to it. There's the size of the network.
And it can take storage. It can take time to download it. It's kind of, of course,
size is directly relevant. Let's say to the number of parameters, the number of weights and so on.
Power like you mentioned, right? You know, other factors, performance, total performance can be a
factor of model size, right? Of course. So all these factors are really, let's put them in a bucket
of like achieving network optimality, right? And we have demonstrated, in fact, we just finished
a project with one of our biggest customers where we were able to demonstrate that even their
most advanced use cases can be quantized. We can quantize the networks to 8 bits or 16 bits,
whereas their internal kind of research had shown that they needed like floating point operations
in order to achieve the accuracy that they were, you know, targeting. And through our advanced
techniques, which are all part of the, you know, the software offering that we're developing,
we were able to work with them and show them, hey, look, you can get to 8 bit on some of these
networks. You can get to 16 bit fixed point on some of these networks. The result of that,
and we're, we have a lot of research in 4 bit as well. The result of that is tremendous,
you know, improvements in power, performance, the ability to run things concurrently,
because it can run faster, right? These are all kind of, I think, the most, the biggest pain
points, because what's happened is it used to be that CNNs were sort of over-paramortized,
and so it was relatively simpler to compress them or relatively simpler to sparsify them or whatever.
And as sort of complexity has gone up, and the concurrency we talked about has gone up,
getting them all jammed into a device and getting them to work efficiently and concurrently at
whatever frame rate you need them to get it, becomes harder and harder. And so this bridge,
if you will, from ML ops, like what happens if you will in the cloud, in the laboratory,
and a data scientist invents a new network architecture or a new, whatever, and they say, hey,
now deploy it, the deployment part turns out to be really pretty hard. And now you have this sort
of impedance mismatch between that data scientist who says, hey, I've got this perfect model,
and it solves a problem perfectly. Why can't you make it run on the device? That's really,
I would say, one of the key pain points right now, and we're working very aggressively to address
that and bringing tools that can allow more easily take that model out of the data scientist's
hands and bring it to the device in a way that does not compromise the integrity, the basic design
of that of that model. And are you, do those tools apply only to kind of custom models that
ML engineer might be creating, or do they also apply to kind of the lighter weight models
that someone might download off of hugging face or something like that? Sure. So hugging faces
is a particular example where again, we can touch on Transformers. Quantizing Transformers is a
bit of a new art, which we have quite a bit of research on. And you will see that kind of flow
into our tools. But if we take the more general part of your question, no, these tools apply
to all call it run of the mill, you know, models, resnets and those kinds of things that people
are maybe familiar with, as well as these custom ones, right? And so we're working very hard to
make sure that if you've got a model in TensorFlow or PyTorch, whether it's a stock resnet or you
pull it off of GitHub or whether it's, you know, a proprietary model, that you can run it through
this workflow and you can get, you know, consistent results to bring that to, in our case, Qualcomm,
you know, Silicon and Qualcomm software. We think that's one of the challenges and I think it's
evidenced by the fact that there are, you know, new companies surfacing all the time to deal with
ML ops issues. And ML ops has its own set of complexities, managing the data, managing the
pipeline, scaling up training. I mean, there's a lot of challenges there, right? We don't want to
be one more challenge in the pipeline. We want to go, okay, you solve your ML ops problem or your
training thing. That's fine. We don't want to be the, oh, but now it doesn't work on device. We're
working really hard to make that transition, you know, as smooth as possible. So can you talk a
little bit about the workflow that you just mentioned, kind of what are the steps or tools that
you're providing that enable folks to do this? Yeah. So I think you've had some of our, some of my
other colleagues talk about like the AI model efficiency toolkit. That's kind of at the center
of this, right? So, so think about the workflow being like this. There are kind of maybe two aspects
the model efficiency toolkit. One is an aspect that that pieces of it can plug directly into your
training pipeline. So think quantization where training is a good example. So this is the process
where during the training cycle itself in your training loop, you are conditioning the network to
understand what the effects of quantization are going to be on that particular network. So it
learns the noise, if you will, that gets introduced during that quantization process instead of waiting
until you get all the way to the deployment phase and you now realize, hey, this model isn't very
resilient to quantization, right? So some models really require quantization or training and we've
got tools that plug into PyTorch and plug into TensorFlow, add a little bit of extra to your
training pipeline and then condition the model during training to be quantization aware.
Now, there's a whole category of models where you don't need that sophisticated
approach. You can do what we call post training quantization, okay? So you've trained the model in
your favorite framework. We've got tools that will read that native file format.
We run it through a series of post training techniques. Again, a lot of this we have done
quite a bit of innovation on and studying how do you do this in a way that preserves the integrity
of the model and also reduces the precision which reduces the size, you know, reduces power
consumption and so on, which are objectives. So you would run it through post training quantization.
This produces a representation of the model in this Qualcomm AI engine direct format. Think
about it like an intermediate representation. So it's a common representation. Any graph that comes
from PyTorch or TensorFlow will be quantized and then reduced into this intermediate representation.
From that intermediate representation, you can then as a practitioner, direct where you want
that model to run. Do you want it to run in our AI hexagon accelerator? Do you want it to run
on the GPU? Do you want it to run on the CPU? Once we have it in that common representation,
you can then run it wherever you want. And a lot of our customers will go, oh, I want to run this
one on the GPU and that one on the accelerator and so on. That one on the low power sub-system and
so on. That's kind of the basic workflow. And then if your deployment is, you know, a smartphone,
you can run the AI accelerator. If your deployment is a cloud device or a heavy-edged device,
it's using our cloud processor, you can take the same representation and run it there.
And that's a multi-core device. So you have a lot of performance so you can run there. So that's
where this idea of having a common representation across our silicon ties in with that sort of
developed workflow that a day of scientists might pursue. You've also been doing work in network
architecture. So how does that plan to this workflow? You know, network architecture, again,
is sort of an evolving technology, right? There was a big rush and Google was an early innovator
and we're happy to be partnered with Google on our offering. Now, the way to think about it is,
I put it in this sort of very advanced category, right? But again, it's one of the tricks when
one of the tools that we're offering to our customers to bridge that gap from what a data
scientist might want to do sort of in the laboratory, if you will, when they're designing a network
and tying the design of our network to hardware-specific characteristics. So that's kind of one
attribute. We're working with Google to make sure that Vertex NAS understands Snapdragon,
if you will, that's the way to put it. So that the network architectures that are produced
from neural architecture search are sort of inherently hardware aware. So they're inherently
optimized for Snapdragon, okay? And so by tying those two things together, again, we are providing
a way for the data scientists to produce a result that is sort of prepared to run in one of these,
say, power constrained environments. And we've had good success with some XR and automotive
workloads where we're seeing the, we're seeing NAS produce in a much quicker way than a data scientist
could iterate on their own innovative network architectures that are tuned for Snapdragon
that reduce the latency, increase the frame rate, reduce the network size, all, you know,
characteristics that we're trying to achieve when we're helping like an XR customer or an automotive
customer, you know, get a very demanding set of workloads onto, you know, our platforms.
So it's another tool there, but I think that for the listeners, the key takeaway is this linking
the speed at which you can search a lot of, you know, run a lot of experiments, if you will,
automatically, and linking it to hardware awareness so that the final output
is preconditioned to do well on our silicon. You talked about frame rate, are the primary
workloads that these are being used for vision workloads? Yeah, so I mean, I think of that because
that's the predominant workload. Of course, if this were like natural language processing,
you know, you might say like how fast can it, you know, parse a sentence or whatever,
right? So that has to do more with like, you know, word length, right? Or per length. But typically,
you know, so many of the applications which are demanding are, you know, we can go across the
whole spectrum. So of course, in mobile, it's typically, you know, low light video, let's say,
is a pretty demanding workload, super resolution, right? We've been to, we've shown some super
resolution in games, right? So I'm doing gaming, and I want to super resolve, you know, the action
scene in the game to increase the, you know, quality and the experience XR. So maybe two kinds
of workloads, not only the rendering part, right? And the sort of you're, you're doing
some kind of augmentation or whatever. And also things like hand gesture, right? Which again,
we talk about frame rate there because you want to sample it often enough that it's a smooth
experience, right? That you don't miss gestures, you don't miss motion, right? So we oftentimes use
frame rate a little loosely, meaning kind of sampling interval, right? For, for an experience.
And like we said, the car, it's looking down the road, you don't want it to look down the road,
you know, once a second, you want it to look down the road, you know, 30 times a second or 25 times
a second, right? Do you have a sense for when, when folks know that they need to start exploring
network architecture search? Like what are, what's the wall that they're bumping up against that?
Oh, if you're experiencing that, and maybe now you need to go to this advanced level.
Yeah, so that's kind of a complicated surface. You know, we didn't talk about, we didn't talk
about, for example, mixed precision, right? Is another kind of technique of like you don't have
to just do stuff in one precision, you can mix different layers and different precision,
and we support that. You know, I think there's a couple drivers, the idea that
that you can sort of algorithmically
imbue this algorithm with an awareness of the hardware is something which is hard to sort of
learn at scale. And in some cases, we don't want to tell, you know, all of our customers,
all of our secrets, right? So there's a little bit of that going on. But it's also hard to learn
a lot of the subtlety that something like a NAS system can learn on its own and can leverage.
The other thing too is just raw experimentation. I mean, if NAS can produce a thousand
candidate models in some unit time, how long would it take a data scientist to think out the same
thousand experiments, right? And so some of it's just kind of a brute force, like search
the solution space in an automated way, but it's directed, right? It's got considerations for
hardware, it's got considerations for size, and so on, kind of baked into it. So it's a nice tool to
increase the rate at which a data scientist can explore different approaches. So it sounds like
given the relative cost and complexity of using it, there's a space of high value problems where
they're close enough that they think that there's a solution, but not quite far enough that they're
at the solution that, you know, this is something to try. Yeah, you can think about it kind of maybe
in two areas. You can think about it like if you have kind of no idea how to solve a problem,
right? Your search space is very large. This might be a way to kind of get to an idea, you know,
automatically through basically, you know, wrote experimentation to solve your problem.
The experience we've done so far have focused more on, I've got a network kind of to your point.
I've got a network, and you know, if I apply this technique, how much better can I get it, right?
And sometimes you go, oh, I saved, you know, 15% in latency or something, right? Which can translate
into power or frame rate, depending on what you're trying to, you know, what effect you want.
And that could mean, oh, I could run one more network with the same unit time because this
one's going to finish sooner, right? So, you know, these projects, because they have so many
networks and there's so much complexity are really sort of a puzzle fitting experience. You can't
generally, at priori say, oh, this network absolutely has to run in such a amount of time,
or use so much power because you're looking at a collection of networks that are together
are solving a problem, right? Again, something people might, your audience might be experienced with
this is, you know, virtual reality, you know, metaverse XR, whatever term you want.
They realize, oh, it's following my hands and it's rendering the screen and they don't really
think about all the things it's doing. But if it doesn't do all those things fast enough together,
you just don't have that immersive experience. Right, right, right. You mentioned mixed precision.
Can we dig into that a little bit more? Where is mixed precision now on kind of the adoption
cycle? Is it being broadly used, do you think, or folks just starting to explore it?
So I would say that there's been a period of exploration, say, maybe the last product cycle,
where we enabled it in our products. But again, a little bit sort of to the nastery,
there weren't good tools to sort of automate it. So it was a way to address point problems.
Oh, this layer is having a problem quantizing, or we need more output resolution here. So we're
going to use a higher precision layer that produces higher resolution activations or something.
Now we're introducing automatic mixed precision. So again, bridging that the friction points
from taking a model that works great on a bench somewhere for a data scientist and getting it
onto device. Sometimes, again, thinking about post-training, quantization techniques,
this would be one of them. I'm not really retraining the model. I'm looking, I'm analyzing the model
and saying, oh, look, I can see that layer 42. That's the universal number of everything, right?
Layer 42. That's the one that's causing the trouble if it's in say 8-bit integer precision.
Let me move it to integer 16. And boom, my quantization problem goes away, right? That's the source
of my accuracy. So again, providing tools for practitioners to automate that so that it's more
push button. That's the road we're on. And well, those features are arriving now in our tool chain.
What are the features that you are expecting to see looking forward? So again, I think we're early
in NAS. I think we're still somewhat early in sort of quantization of transformers and all their
variants, right? So we're seeing kind of an emergence of vision transformers. We're seeing
transformers for like multi-modal kinds of applications, right? And so on. I would say we're,
you know, we've done a bunch of research there, but I would say that that's an emerging
area where we will see, you know, practitioners moving towards
want to apply those and facing some of the same hurdles that they're facing now on sort of
traditional architectures. And I would also say that, you know, there's a lot of smaller friction
points in moving from, you know, the data scientists part of the ecosystem to the, you know,
final deployed product. And so we can think about streamlining like the model conversion steps
better. Int 4 is a really exciting, even more exotic data format. You know, we recently announced
some discussions around FP8, which is another data format. So there's a lot of things coming with
the continued, you know, challenges as, you know, like we started off this discussion. I don't see
the challenge of increasing model complexity faced with, you know, struggles to reach optimality.
In various forms, data format is away, NAS is away there, right, the tools we're providing. These
are all roads to the same goal, which is more, you know, more complexity in, you know, packages that
will also get more capable, but there'll be limits, right, that will always face kind of a
innovation challenge. So I would say the tooling is really going to be directed at continuing to
make that easier for practitioners. And at the same time, continuing in the direction of things
like more NAS, lower precision and so on, right, which will pose our own challenges going forward.
And do you see the tools themselves getting easier to use or are they based on what I've seen
thus far? They're pretty low level tools. How did those, how did those evolve?
Yeah, so I think, so the way they're evolving is we're not ready sort of sort of fully
announced stuff, but you can think about we will also be introducing a graphically oriented tool.
So again, moving from the, I'll call it advanced practitioner who maybe is part data scientists
and part deployment engineer, right, to where, again, we want to make this simpler and simpler
and more and more mass market. And so yes, we've started with, you know, with command line tools
and sophisticated, you know, profilers and debuggers. What you will see from Qualcomm is,
I think, increasingly steady set of announcements around, oh, now we've got a, you know,
profiler or visual profiler for this. Oh, now we have a way to do, you know, network
inspection. And so the way I think for the audience, I think about it is like source code debugging,
if people, you know, still write code and they remember what that's like, you know, you want to,
you want to write your code and you do not want to see it your debugger in a semi language.
Like you don't want to see what the compiler turned your code into. You want to debug your code
in the context that you wrote it that you understand it. And so the analogy, the way I think about
it is the same here. A data scientist designed a network and they visualize it like a set of
connected nodes. They don't want any, I think, hardware or compiler company to go, oh, now you have
to debug your network that you, you know, conceptualize as a graph in a semi language. And so we're
going to do the same sort of thing. We're going to provide debuggers that put that performance and
data that the developer needs to understand how their network is performing on our silicon in
the context of the original graph as it arrived, you know, at the start of that pipeline I described,
right? And that's really the link we're trying to build is you think as a data scientist about
your network and TensorFlow, and we want to show it to you on the device. Sure, with device-specific
performance metrics and accuracy metrics, but in the context of this graph, the way you, the way
you designed it, right? Not in some low level kind of, you know, a semi language, if you will,
right? And that's what you'll see is more and more visually oriented tools that make the,
you know, ML ops part of the equation and the deployment part a lot more equitable.
When you talk about the relationship between kind of the higher level tools and programming
languages that a data scientist or developer might want to use in the lower level hardware,
it calls to mind comparisons to things like CUDA. Is that, you know, how should we think about the,
what you're offering with SDKs relative to what someone might be familiar with in the GPU world?
Yeah, so I think, I think, you know, I don't really want to do a bake off, but I think the analogy
runs like this. You could think of CUDA and CUDA is a lot of things, right? It's a parallel
programming language, it's a, you know, and so on. But you could think about our Qualcomm
Engine Direct as being sort of Qualcomm's answer to CUDA in that it provides a common way
for practitioners to either use existing runtimes, right, to deploy their solutions,
or if they're advanced, and that's what their application demands, they can sort of program
directly. So the analogy is Nvidia provides UDNN, which sits on top of CUDA, right? That's an
example of a library. You know, our stack provides very similar kinds of capabilities.
If you want to roll up your sleeves, you want to write a CUDA kernel for the GPU. Of course,
there's a way to do that. The analogy is, if you want to write a custom application or a custom
operation or layer for the Qualcomm AI stack, we provide a way to plug that into the Qualcomm
Engine Direct. We have an API for that. We provide a debugger, we provide a compiler and tools
for hexagon. We provide the same for our mobile GPU. So it's a very analogous offering. And then
one thing which a lot of people like are these visual consoles that Nvidia provides that sit
on top of their GPUs. We have offered that for like our gaming customers for a long time.
We are expanding that kind of offering to the AI portfolio and really to provide
the debug ability of Snapdragon as a whole. So you will see us working to integrate
our debug tools, not just in the AI space and there will be AI-specific tooling, but more generally
as Snapdragon is a development platform, you will see us kind of expand that offering.
So to sort of maybe put a bow on it, I think at the library level and at the sort of programming
and lower levels, we have a one-for-one corresponding kind of offering to Nvidia. And then you will
see us kind of put the top of the story onto that offering in the next period. Over the years,
we've talked a lot about the ecosystem and the way your tools support and the way you work with
other ecosystem partners. You've already mentioned things like Onyx and PyTorch TensorFlow.
Can you give us an update on where the ecosystem priorities are and any new and exciting updates?
So yeah, I can maybe tease a little bit without getting into too many details.
So you know the way I think about it is this, we continue and I think the Qualcomm Edge and
Direct is a good concrete example of where we are trying to provide an abstraction of API surface
onto which all these runtimes can plug in. So we want maximum access to our silicon and our
software. We don't just leave it to them. We've invested working with Google on TensorFlow
delegates working with Microsoft on Onyx runtime. So these are ways in which we're partnering with
the broader ecosystem. Some of the work we do, some of the work they do, we share the work.
An exciting thing to think about is the Cloud to Edge story. The idea that you could
have like a Snapdragon Windows compute device and maybe also an XR device that run very
similar applications or share and experience. These kinds of multi-device
experiences are I think only really possible when you've got this common abstraction that we're
building and you've got common runtimes. So that's why it's so important for us to work with
like the Microsoft of the world and the Google of the world so that if you were Adobe or something
and you're building Photoshop and you want a Photoshop experience on an Android device and you
want a Photoshop experience on a Snapdragon ARM powered PC device, you don't have to write
complete different pieces of software because underneath it the silicon is Qualcomm silicon.
And so we want to kind of make that easier. So that's an example of the kind of thing we have in mind.
And we're partnering with the operating system providers if you will, the main drivers of
the broader ecosystem, but also like these ISVs, right? So as our portfolio of use cases expands,
then the partners aren't just our traditional handset OEMs, they become Microsoft, Adobe,
you know, these other kind of companies that have really compelling applications that they can bring
to an assortment of Qualcomm devices. So that's kind of where I think we will see our ecosystem
efforts expand. We've touched on throughout the conversation, some of the work happening in
automotive as kind of this, you know, touchstone for, you know, a set of use cases that are kind of
pushing the, pushing the edge. Can you talk a little bit about what you're seeing in automotive
nowadays and kind of what's new and interesting in that world? Yeah, so new and interesting is
automated cars are coming, but they're not here yet, right? So, you know, we have moved from,
you know, connectivity, which is our traditional business. So this is like, you know,
your car is connected to the internet so that you can, you know, get emergency services and stuff,
to digital cockpit, right? So think about in cabin, right? And so we have very strong offerings there,
and we talked a little bit about like gaze detection, you know, cameras inside the cockpit that
are monitoring the driver as part of a safety protocol. Eight as, we recently want to
a bid with with with BMW, for example, you know, to provide BMW, you know, eight as solutions.
Think about this is, this is, you know, semi-autonomous safety oriented systems, right?
This is lane to very advanced lane departure warning, lane keeping,
automatic parking, backup. I mean, everything, everything related to making the automobile
safer and more automated experience, right? And we're on this sort of trajectory towards
sort of full automation, but before we get there, there's a lot of problems as all. So
we want the car to be surrounded by cameras. So we're talking 10 cameras, 15 cameras, right?
Completely surrounding. We're talking about immersive in cabin experiences. So
displays, right? Perseete displays, perseete audio experiences, right? We think, oh,
well, a car doesn't have to have NLP, but you could talk to your car. It could talk back to you,
right? I mean, there's just layers and layers and layers of AI in an automated vehicle,
both for the comfort of the passengers and the safety of the passengers. And so we're looking at
all of those kinds of problems. And the most challenging ones are
safety critical time critical kinds of problems. So viewing the car in the context of all the
other cars on the road, very precise lane positioning for, you know, for lane keeping and so on.
But things like, things like drive policy, like I want to tell the car, I want to go from here to
there and I want to know it needs to know where I'm going, but it needs to be aware of other cars
making lane changes. It needs to plan lane changes. It needs to do that in a safe and predictable
manner, right? These are all very intensive, basically computer vision, you know, AI power
computer vision problems that have to be completely seamless, you know, to the driver and are
very demanding from a predictability point of view, you cannot be like, oh, I'm late, you know,
with that frame, like that is could be a problem. And you have to, of course, design all that
software in a safety critical kind of way. So that's really the challenge. And to come sort of full
circle on the discussion, there's a lot of compute power that our platform brings to the automobile.
But it's proportional to the demand of the workload, right? So the challenges that we see in XR,
we see sort of analogous challenges in, in a vehicle, because the number of cameras is higher
or the resolution is higher or the modalities are different, like a mix of LiDAR and radar and
camera all working together to solve a problem. This introduces, you know, multimodal kinds of
challenges. So there's a lot of very exciting challenges. And I think you'll see Qualcomm continue
to, I know you will see Qualcomm continue to make strides on a mode of market. It's a very
important market for us. Again, we do fundamental research there that drives our innovation. And
it's a similar kind of cycle that we talked about with Qualcomm AI research. We do fundamental
research in these automotive networks. And that drives, you know, our product line.
Do you take a strong position on vision first type of automotive experience? Or do you
also support and work with kind of these with other sensors and kind of a sensor fusion type of
environment? Yeah. So we we're very agnostic. We do what our customers want, right? And so we're
looking, like I said, it kind of three, the three primary modalities camera, which I think is sort
of camera first in a lot of ways. I think pretty much for everybody. And then radar and LiDAR.
So we look at sort of all, I'll call them the three major modalities, four sensors.
Awesome. Any final thoughts, you know, as we close out, you know, what do you think we'll be talking
about next year when we get together? Well, I hope, I hope, you know, thinking about your sort of
practitioner, you know, audience, I hope, let's put this way, I am planning that we will be able to
talk and a lot more concretely about let's say the tools that we talked about, right? These sort of
visually oriented tools. We hope to be able to show them publicly by then. That's our plan.
And I would like to talk a lot more about automotive probably. We will be another year into that,
you know, evolution and hopefully, you know, of course, talk about a few more customers concretely,
but we expect to, you know, win more business and make more advances there. I say that because I
feel like it's perhaps one of the most compelling and demanding markets, you know, for AI,
generally speaking, right? I think you can't build one of these safe vehicles without AI. I don't
think there'll be a lot of debate about that statement. And I really hope we will be able to showcase
more of these multi device use cases, right? We're working pretty hard on that. And I think we will
see in the next year, the fruits of that come to bear like in a real kind of context that I can
talk about publicly, but just know that a lot of work in that direction. And we're excited about
we're excited about Windows on Snapdragon and, and, you know, putting that power of, you know,
an all day device and people's hands that can also do AI, right? Because we take for granted
background blur and these other things, but they can be even better with AI, right? And they can
be done in a way that does not compromise battery life and, you know, makes the, the experience that
we all, you know, live with every day a lot more, a lot more seamless. So I hope that these are
a forecast of some of the things we'll talk about that would be more publicly, you know,
announced in the next, you know, period. Awesome. Awesome. Well, Jeff, it's always a pleasure to
reconnect with you. Thanks so much for joining us. I appreciate it. And you have a good rest of your day.
I appreciate it. Thank you.
