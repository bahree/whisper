1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:23,040
I'm your host Sam Charrington.

3
00:00:23,040 --> 00:00:29,000
Today we close out our TwimalCon coverage with a panel on operationalizing responsible

4
00:00:29,000 --> 00:00:35,040
AI, but first a quick announcement.

5
00:00:35,040 --> 00:00:39,160
Have you been enjoying our TwimalCon coverage, but want more?

6
00:00:39,160 --> 00:00:44,480
TwimalCon video packages are now available for advanced purchase over at twimalcon.com

7
00:00:44,480 --> 00:00:46,040
slash videos.

8
00:00:46,040 --> 00:00:51,440
The package features over 13 hours of content, including all the awesome keynotes and panels

9
00:00:51,440 --> 00:00:57,640
you've heard on the podcast, all of the breakout sessions, including 9K study and 8 Tech

10
00:00:57,640 --> 00:01:03,920
track sessions, as well as the highly regarded team tear down panels with Airbnb and serving

11
00:01:03,920 --> 00:01:04,920
monkey.

12
00:01:04,920 --> 00:01:15,560
Again, visit twimalcon.com slash videos for more info and now on to the show.

13
00:01:15,560 --> 00:01:20,320
I am really excited to welcome up a panel that's going to talk through a really important

14
00:01:20,320 --> 00:01:27,320
topic, operationalizing how we do machine learning and AI ethically and to lead us through

15
00:01:27,320 --> 00:01:28,320
that.

16
00:01:28,320 --> 00:01:33,720
I'm excited to have our moderator for the panel, Kari Johnson from Benchabee, and he will

17
00:01:33,720 --> 00:01:36,520
introduce the rest of the panel.

18
00:01:36,520 --> 00:01:42,000
Kari and panel, welcome to TwimalCon.

19
00:01:42,000 --> 00:01:49,400
My name is Kari Johnson and I'm senior AI staff writer at Benchabee, and I'm really excited

20
00:01:49,400 --> 00:01:51,200
to be here with our panel.

21
00:01:51,200 --> 00:01:56,720
I could introduce everyone, but I think everyone will do a better job if they do it themselves.

22
00:01:56,720 --> 00:02:03,720
So we'll just quickly go down the row, but we can begin with a quick icebreaker, I think,

23
00:02:03,720 --> 00:02:05,120
that's all right.

24
00:02:05,120 --> 00:02:12,920
I was thinking icebreakers are, they can be corny, that's possible, but sometimes they also

25
00:02:12,920 --> 00:02:16,520
let you know a little bit about the other people too.

26
00:02:16,520 --> 00:02:18,920
When you were a kid, what did you want to be when you grew up?

27
00:02:18,920 --> 00:02:19,920
I'll go first.

28
00:02:19,920 --> 00:02:25,720
I wanted to be a firefighter, but I also asked my mom to do an op-ed on local television

29
00:02:25,720 --> 00:02:31,440
about why kids don't need to take showers, so it could be that I'm doing what I always

30
00:02:31,440 --> 00:02:32,440
wanted to be.

31
00:02:32,440 --> 00:02:37,320
I wanted to be a politician and ultimately a president.

32
00:02:37,320 --> 00:02:38,320
Yeah.

33
00:02:38,320 --> 00:02:39,320
That's nice.

34
00:02:39,320 --> 00:02:43,800
I wanted to be a writer, I actually wanted to write novels, and I write Scalicote, so

35
00:02:43,800 --> 00:02:45,520
yeah, here we are, yeah.

36
00:02:45,520 --> 00:02:51,360
At various points, I wanted to be a nurse, marine biologist, archaeologist, and writer.

37
00:02:51,360 --> 00:02:54,520
These are all good professions, I think we're doing good stuff too.

38
00:02:54,520 --> 00:03:01,600
But again, down the line, if we could start with Perry, and can you please correct me on

39
00:03:01,600 --> 00:03:05,320
the correct pronunciation of your name and tell us a little about the current work that

40
00:03:05,320 --> 00:03:06,320
you do?

41
00:03:06,320 --> 00:03:07,320
Sure.

42
00:03:07,320 --> 00:03:08,320
I'm Perry Nas.

43
00:03:08,320 --> 00:03:09,320
I'm coming from Georgian part.

44
00:03:09,320 --> 00:03:11,920
I work for Georgian partners right now.

45
00:03:11,920 --> 00:03:15,320
For those of you who are not familiar with Georgian partners, we are a venture capital

46
00:03:15,320 --> 00:03:21,840
based in Toronto, biggest private funding in Canada, investing in grossest-stage companies

47
00:03:21,840 --> 00:03:28,120
as software services, and our business plan and model is a little bit different.

48
00:03:28,120 --> 00:03:32,760
We have impact in where I'm coming from in addition to the investment team.

49
00:03:32,760 --> 00:03:37,640
We are like a applied research lab for our portfolio companies, to risking research and

50
00:03:37,640 --> 00:03:44,280
innovation for them, and accelerating the adoption of cutting edge disruptive technologies such

51
00:03:44,280 --> 00:03:50,000
as machine learning and AI, so we are a team of practitioners working on R&D projects

52
00:03:50,000 --> 00:03:52,160
with our companies.

53
00:03:52,160 --> 00:03:53,160
Awesome.

54
00:03:53,160 --> 00:03:54,920
I'm Guillaume, which is French for William.

55
00:03:54,920 --> 00:03:58,520
If my first name is too hard to pronounce, if you're coming to William, or even Bill, if

56
00:03:58,520 --> 00:04:03,080
you must, essentially, I lead computational science at LinkedIn, which is a team that's

57
00:04:03,080 --> 00:04:05,440
heavily invested in fairness.

58
00:04:05,440 --> 00:04:08,440
I assume I don't need to introduce LinkedIn, but if you don't know what it is, you should

59
00:04:08,440 --> 00:04:09,440
check it out.

60
00:04:09,440 --> 00:04:11,240
It's a pretty cool network.

61
00:04:11,240 --> 00:04:17,200
Essentially, what I've primarily worked on lately is specifically about the experimentation

62
00:04:17,200 --> 00:04:20,640
side of fairness and responsible design.

63
00:04:20,640 --> 00:04:24,680
I'm Rachel Thomas, and I'm Director of the Center for Applied Data Ethics at the University

64
00:04:24,680 --> 00:04:29,320
of San Francisco, which we just launched a few months ago, so it's still very new.

65
00:04:29,320 --> 00:04:36,520
I'm focused on harms that are happening now, so that includes unjust bias, surveillance

66
00:04:36,520 --> 00:04:41,840
and the erosion of privacy and disinformation, and at the center, we'll be working on a

67
00:04:41,840 --> 00:04:47,520
research, a mix of research, education and policy.

68
00:04:47,520 --> 00:04:52,840
Co-founder of FastAI, where we're interested in getting people from more diverse backgrounds

69
00:04:52,840 --> 00:04:55,840
into AI.

70
00:04:55,840 --> 00:05:03,320
Just to get us started, how do you get started with operationalizing AI ethics?

71
00:05:03,320 --> 00:05:07,640
What do you feel like are the first steps that an organization should be taking to set

72
00:05:07,640 --> 00:05:08,640
the table?

73
00:05:08,640 --> 00:05:09,640
That's a very good question.

74
00:05:09,640 --> 00:05:14,680
I believe you have to start with the vision, your vision of the company, and that's what

75
00:05:14,680 --> 00:05:19,360
we always value a lot when we are doing the due diligence.

76
00:05:19,360 --> 00:05:22,160
Then, of course, the culture is also very important.

77
00:05:22,160 --> 00:05:26,120
Teams, do you have the right team in place?

78
00:05:26,120 --> 00:05:32,040
You have to understand that, of course, you are building softwares, but most of these issues

79
00:05:32,040 --> 00:05:37,960
started from not having diverse teams working on these software products, so it's really

80
00:05:37,960 --> 00:05:44,560
important to start by building a diverse team, not only in terms of the gender or ethnicity,

81
00:05:44,560 --> 00:05:50,400
but maybe we need to have new roles in our machine learning or data science teams.

82
00:05:50,400 --> 00:05:55,600
Maybe we need sociologists, maybe we need somebody with legal and compliance background, so

83
00:05:55,600 --> 00:06:01,440
it's not only about, okay, let's have equal number of women and men, or from different

84
00:06:01,440 --> 00:06:07,080
ethnicity, but it's also about maybe thinking about diversity in the background of the team,

85
00:06:07,080 --> 00:06:12,760
and at the end, there is the question of what kind of processes do we need in place to

86
00:06:12,760 --> 00:06:19,000
be able and enable the team to build responsible, ethical AI products?

87
00:06:19,000 --> 00:06:20,920
Yeah, I think that makes a lot of sense.

88
00:06:20,920 --> 00:06:23,560
I think values and vision are very important.

89
00:06:23,560 --> 00:06:27,040
We found that we can't really have just one central team that writes one piece of code

90
00:06:27,040 --> 00:06:30,560
and fixes fairness, for example, for the whole company, and so it's really everyone's

91
00:06:30,560 --> 00:06:31,560
problem.

92
00:06:31,560 --> 00:06:34,760
Everyone is involved in this, and so what that means is that you really have to have

93
00:06:34,760 --> 00:06:37,280
the right alignment in terms of values and culture.

94
00:06:37,280 --> 00:06:40,440
So when you join LinkedIn, first thing they tell you before you even sit down is members

95
00:06:40,440 --> 00:06:41,440
first, right?

96
00:06:41,440 --> 00:06:45,080
So if you have a doubt that you always put the member first, and also act like an owner.

97
00:06:45,080 --> 00:06:49,280
And I think once everyone kind of feels like, okay, being fair, treating the members fairly

98
00:06:49,280 --> 00:06:52,880
and the way I would want to be treated myself, when everyone is kind of behind that, it

99
00:06:52,880 --> 00:06:53,880
makes it a lot easier.

100
00:06:53,880 --> 00:06:57,120
And of course, to get started practically, you also want to have the right tools and the

101
00:06:57,120 --> 00:07:01,520
ways to measure and the right processes, but really without culture and without the

102
00:07:01,520 --> 00:07:04,520
values, it's actually, I think, very hard to do anything, given the fact that it's really

103
00:07:04,520 --> 00:07:06,840
kind of a broad effort that everyone should take ownership of.

104
00:07:06,840 --> 00:07:07,840
Yeah.

105
00:07:07,840 --> 00:07:12,280
And for concrete resources, one resource I highly recommend is the Marcoola Center has

106
00:07:12,280 --> 00:07:18,200
a toolkit for tech ethics, and they recommend kind of a number of processes you can implement,

107
00:07:18,200 --> 00:07:21,240
but a key one is ethical risk sweeps.

108
00:07:21,240 --> 00:07:25,760
And so this is kind of periodically scheduling times to really go through kind of what could

109
00:07:25,760 --> 00:07:29,840
go wrong, and like what are the ethical risks, because I think a big, big part of ethics

110
00:07:29,840 --> 00:07:34,640
is also kind of, you know, thinking through what can go wrong before it does and having processes

111
00:07:34,640 --> 00:07:39,480
in place around what happens when there are mistakes or errors.

112
00:07:39,480 --> 00:07:44,520
Diving a deeper into that, what does the panel think about in terms of favorite frameworks

113
00:07:44,520 --> 00:07:50,760
or approaches or different tools that are available and out there now that teams can use?

114
00:07:50,760 --> 00:07:58,440
So let's start with what are under the umbrella of responsible AI, what are main concerns,

115
00:07:58,440 --> 00:08:04,200
privacy is one of them, and not only about the tools, but also about the technology, I

116
00:08:04,200 --> 00:08:10,200
really believe in the financial privacy and federated learning to build privacy preserving

117
00:08:10,200 --> 00:08:14,960
machine learning products and systems.

118
00:08:14,960 --> 00:08:23,160
In terms of explainability and communication with the end users or actually deboking, enabling

119
00:08:23,160 --> 00:08:31,240
developers to debok the systems, I really like approaches like Lime or Alime or Anchor

120
00:08:31,240 --> 00:08:32,320
Lime.

121
00:08:32,320 --> 00:08:40,160
In terms of fairness and bias, I believe it's really important to root out bias.

122
00:08:40,160 --> 00:08:48,240
So I personally use the tool developed by EPF all university and I guess Columbia University,

123
00:08:48,240 --> 00:08:55,120
its name is FairTest, discovering un-valented associations in data-driven applications

124
00:08:55,120 --> 00:09:02,920
and Google Wattif tool, they are also my favorites, and I guess that's it.

125
00:09:02,920 --> 00:09:09,760
And we recently also open sourced our TensorFlow tool for building privacy preserving machine

126
00:09:09,760 --> 00:09:11,160
learning systems.

127
00:09:11,160 --> 00:09:16,240
Yeah, so we have similar kind of areas that we care about.

128
00:09:16,240 --> 00:09:19,640
I would say there's a lot of internal tools that we're also building.

129
00:09:19,640 --> 00:09:23,080
Some of it is that there are some great tools that don't always scale, and we have a lot

130
00:09:23,080 --> 00:09:25,600
of data that we have to process.

131
00:09:25,600 --> 00:09:29,040
And we also have some tools that we're building completely our own.

132
00:09:29,040 --> 00:09:32,920
So for example, we have a lot of fairness built into experimentation.

133
00:09:32,920 --> 00:09:37,360
The idea is it's not just about the algorithm, it's not just about are you fair from an

134
00:09:37,360 --> 00:09:43,160
algorithmic perspective, once your output is put in front of humans, what kind of outcomes

135
00:09:43,160 --> 00:09:45,920
are we seeing between treatment and control, things like this.

136
00:09:45,920 --> 00:09:49,360
So that's also the kinds of, we also care a lot about that, which is try to think about

137
00:09:49,360 --> 00:09:53,040
it when you train your data, when you first put together your training data, when you train

138
00:09:53,040 --> 00:09:57,240
your model, and then even in an ongoing basis, how does it interact with people, how does

139
00:09:57,240 --> 00:09:58,240
it interact with users?

140
00:09:58,240 --> 00:10:02,480
And so we try to cover all of that, at least the whole kind of machine learning and AI

141
00:10:02,480 --> 00:10:03,480
life cycle.

142
00:10:03,480 --> 00:10:07,400
Yeah, going along with that, I think that some of the issues that arise from not thinking

143
00:10:07,400 --> 00:10:12,200
about the whole system, and so kind of how the different parts interact, I think a lot

144
00:10:12,200 --> 00:10:16,440
of tech kind of encourages us to kind of hyper specialize, and it's really important

145
00:10:16,440 --> 00:10:20,000
for people across different groups to be talking.

146
00:10:20,000 --> 00:10:25,320
One great idea I've heard from Alex Fierst is having, like having trust and safety embedded

147
00:10:25,320 --> 00:10:29,840
with engineering, product and design, because they have, you know, trust and safety is kind

148
00:10:29,840 --> 00:10:34,640
of seeing what can go wrong and what happens when it does, and that engineering, product

149
00:10:34,640 --> 00:10:39,160
and design tend to live in a bit more optimistic world, but I really think having kind of those

150
00:10:39,160 --> 00:10:43,440
communication channels open between groups, and also having kind of all the necessary

151
00:10:43,440 --> 00:10:47,880
stakeholders, like everyone's that's going to be impacted downstream involved.

152
00:10:47,880 --> 00:10:51,880
What do you feel like are some of your biggest concerns as it relates to responsible AI

153
00:10:51,880 --> 00:10:56,160
today, or to put it another way, if you could change one thing about the AI ethics debate

154
00:10:56,160 --> 00:10:57,960
today, what would it be?

155
00:10:57,960 --> 00:11:02,320
I think I might actually say, like, you know, focusing a bit more on harm and less on bias.

156
00:11:02,320 --> 00:11:04,960
I think sometimes we get focused on the role algorithms, and you're like, oh, there's

157
00:11:04,960 --> 00:11:08,640
bias, and I think there's a sense in which like, you know, every algorithm is going to

158
00:11:08,640 --> 00:11:11,520
be biased somewhat, but you might see very different types of harm.

159
00:11:11,520 --> 00:11:15,520
But so Edlington, we're about people's careers, and so we actually, very mindful of like,

160
00:11:15,520 --> 00:11:18,360
are we helping everyone, you know, kind of like get ahead in their careers?

161
00:11:18,360 --> 00:11:21,840
And so in terms of kind of shifting the debate, I think there's a sense in which it's sometimes

162
00:11:21,840 --> 00:11:25,240
really too focused on kind of the algorithm itself, and kind of the bias of the algorithm,

163
00:11:25,240 --> 00:11:28,280
versus kind of like taking a little step back and kind of talking, thinking about harm

164
00:11:28,280 --> 00:11:29,280
more globally.

165
00:11:29,280 --> 00:11:31,160
Yeah, I completely agree with that.

166
00:11:31,160 --> 00:11:34,720
Like, I think bias is an important issue, but it's just one of many, and kind of looking

167
00:11:34,720 --> 00:11:39,400
at harms, and particularly aren't harms that are already happening when there are mistakes,

168
00:11:39,400 --> 00:11:45,520
you know, we've seen mistakes of an algorithm cutting off health care incorrectly, for Medicaid

169
00:11:45,520 --> 00:11:50,360
benefits, of teachers wrongly being fired, that kind of, there are things already happening

170
00:11:50,360 --> 00:11:51,880
and understanding those.

171
00:11:51,880 --> 00:11:58,520
And I think one of the dynamics that comes into play is that algorithmic systems can make

172
00:11:58,520 --> 00:12:03,720
it harder for anybody to feel responsible, and you know, bureaucracy does this as well,

173
00:12:03,720 --> 00:12:08,200
but often algorithms are kind of extending bureaucracy, and this is an idea Dana Boyd has

174
00:12:08,200 --> 00:12:12,840
talked about, but that's something that alarms me and we need to talk more about of kind

175
00:12:12,840 --> 00:12:17,200
of how we build systems where we can feel responsible for the outcomes, because that's important.

176
00:12:17,200 --> 00:12:23,320
Yeah, I believe also I do have a few of you, but I also believe that we need to have

177
00:12:23,320 --> 00:12:30,440
machine learning specific quality assurance processes, and also we have to have like a

178
00:12:30,440 --> 00:12:38,040
guard-lay, guard-rails fault tolerance, and we have to have a plan for when things can

179
00:12:38,040 --> 00:12:43,760
go wrong, and we have to reduce the impact of models errors, especially for more critical

180
00:12:43,760 --> 00:12:46,760
decision-making or applications.

181
00:12:46,760 --> 00:12:51,520
One of the problems I had so far is like some people believe maybe that these systems

182
00:12:51,520 --> 00:12:55,960
are perfect or they can be perfect down the road, but I personally believe because they

183
00:12:55,960 --> 00:13:01,600
are probabilistic systems that we are using them for deterministic decision-making, we

184
00:13:01,600 --> 00:13:05,920
can be might never reach to 100 percent performance.

185
00:13:05,920 --> 00:13:08,760
So it's really important to have guard-rails.

186
00:13:08,760 --> 00:13:10,680
I totally agree with that.

187
00:13:10,680 --> 00:13:15,000
No doubt, one other point also is to ask about just what are the things we shouldn't be

188
00:13:15,000 --> 00:13:16,000
doing at all?

189
00:13:16,000 --> 00:13:20,000
I think sometimes people skip to this, like, oh, how do we de-biase the data?

190
00:13:20,000 --> 00:13:22,800
You know, and we're seeing this a lot with facial recognition of like, oh, you know,

191
00:13:22,800 --> 00:13:27,440
we need to de-biase it and have more faces of people of color, but we also need to ask,

192
00:13:27,440 --> 00:13:32,520
like, you know, should we be using facial recognition this much at all, and in these use cases,

193
00:13:32,520 --> 00:13:35,440
and really kind of pausing at the start, too?

194
00:13:35,440 --> 00:13:41,680
It comes to that idea of like public perception, you know, you were talking about people believing

195
00:13:41,680 --> 00:13:43,320
that it can be a perfect system.

196
00:13:43,320 --> 00:13:49,080
It seems like maybe part of the process, as well as teaching or at least having public

197
00:13:49,080 --> 00:13:53,120
education stuff out there so that people understand that these are probabilistic systems and they're

198
00:13:53,120 --> 00:13:55,600
making predictions based on data.

199
00:13:55,600 --> 00:13:56,600
Exactly.

200
00:13:56,600 --> 00:14:03,120
Education is a key for both not only for the end users, but also for the developers, product

201
00:14:03,120 --> 00:14:06,240
leads, or the executive teams in general.

202
00:14:06,240 --> 00:14:07,240
Yeah.

203
00:14:07,240 --> 00:14:08,240
Yeah.

204
00:14:08,240 --> 00:14:09,240
I agree with that.

205
00:14:09,240 --> 00:14:13,320
I would also add on that I think in some cases tech companies bear some responsibility

206
00:14:13,320 --> 00:14:17,720
for overhyping what they're selling, and then are kind of, you know, praying on that

207
00:14:17,720 --> 00:14:23,080
often people purchasing these products don't have the understanding of probability that

208
00:14:23,080 --> 00:14:30,080
they need, or have these misconceptions that AI is 100% accurate, but I think we also have

209
00:14:30,080 --> 00:14:34,360
a real responsibility to not over promise or over sell the capabilities of what we're

210
00:14:34,360 --> 00:14:35,360
doing.

211
00:14:35,360 --> 00:14:36,360
Yeah.

212
00:14:36,360 --> 00:14:41,160
There's definitely a lot of marketing injected into the AI space.

213
00:14:41,160 --> 00:14:44,840
Guillaume, I was curious if you could talk a little bit about sort of the practical challenges

214
00:14:44,840 --> 00:14:50,920
of scaling a responsible AI within a large organization, and as an aside to that, if you

215
00:14:50,920 --> 00:14:56,600
think of sort of those rules as different for the organization-wide approach as opposed

216
00:14:56,600 --> 00:14:59,080
to like specific teams within the company.

217
00:14:59,080 --> 00:15:00,080
Yeah.

218
00:15:00,080 --> 00:15:04,880
So as I briefly mentioned, it really starts with the values kind of in the culture.

219
00:15:04,880 --> 00:15:08,240
And then you know, it starts with essentially we also found that it's hard to just have

220
00:15:08,240 --> 00:15:10,080
one role that applies to everybody.

221
00:15:10,080 --> 00:15:12,840
And so what we try to do is kind of give ourselves the tools, the measurement tool, the

222
00:15:12,840 --> 00:15:16,400
analysis tool, the experimentation tools that we can then kind of like deliver to specific

223
00:15:16,400 --> 00:15:21,200
teams, and we work with them to figure out like what makes sense for their specific vertical.

224
00:15:21,200 --> 00:15:25,920
And importantly, we also care about kind of the final outpost once there's an experiment

225
00:15:25,920 --> 00:15:27,360
that people are interacting with it.

226
00:15:27,360 --> 00:15:31,720
And so you know, we really try to have like a 360 degree view of the whole process and

227
00:15:31,720 --> 00:15:33,720
make sure that kind of every part of the process is covered.

228
00:15:33,720 --> 00:15:38,000
So say you train an algorithm, for example, you know, we'd like you to be a wire like

229
00:15:38,000 --> 00:15:40,760
whether it's let's say a general representative or something like that.

230
00:15:40,760 --> 00:15:43,760
And so you know, we try to make sure that people have the information in front of their

231
00:15:43,760 --> 00:15:45,680
eyes when it's available.

232
00:15:45,680 --> 00:15:48,600
And we also have, you know, we just kind of have a lot of meetings where we just kind

233
00:15:48,600 --> 00:15:52,080
of discuss whether we this is the right thing or not the right thing.

234
00:15:52,080 --> 00:15:56,400
You know, it my sense that it has to be decentralized to an extent.

235
00:15:56,400 --> 00:15:59,920
And so this is also why, you know, when we so we analyze all the experiments and we look

236
00:15:59,920 --> 00:16:03,360
at what's happening with the fairness metrics and sometimes people have like really fairness

237
00:16:03,360 --> 00:16:05,280
and enhancing things, but they didn't know they did.

238
00:16:05,280 --> 00:16:07,320
It's just it's just a really happy accident.

239
00:16:07,320 --> 00:16:11,080
And we also want to be able to like see that and reward that and kind of learn from what

240
00:16:11,080 --> 00:16:12,080
they did that worked.

241
00:16:12,080 --> 00:16:13,080
Right.

242
00:16:13,080 --> 00:16:15,360
And so really we're trying to have this kind of like decentralized approach where everyone's

243
00:16:15,360 --> 00:16:16,360
an owner.

244
00:16:16,360 --> 00:16:20,280
What are some of the sort of major AI systems that are used at LinkedIn?

245
00:16:20,280 --> 00:16:24,280
So essentially it's, so we have like, we have the feed, we have the recommendations.

246
00:16:24,280 --> 00:16:25,280
Yeah.

247
00:16:25,280 --> 00:16:28,600
So essentially, you know, these are built into potentially they can be used by everything

248
00:16:28,600 --> 00:16:29,600
basically.

249
00:16:29,600 --> 00:16:33,800
So to give you a couple of examples, one that uses this heavily is the when we rank candidates.

250
00:16:33,800 --> 00:16:34,800
So job candidates.

251
00:16:34,800 --> 00:16:37,480
You like until LinkedIn, you see potential candidates, right.

252
00:16:37,480 --> 00:16:39,640
And so those we make sure that they're representative.

253
00:16:39,640 --> 00:16:43,080
And so there, you know, we actually make sure that when you see, let's say your first

254
00:16:43,080 --> 00:16:46,240
list of candidates, it's actually representative of the, you know, population distribution

255
00:16:46,240 --> 00:16:49,800
of the underlying talent pool and this, you know, and the bias there is like we're being

256
00:16:49,800 --> 00:16:52,320
very, very careful that you're seeing something as representative.

257
00:16:52,320 --> 00:16:53,320
So that's that's one example.

258
00:16:53,320 --> 00:16:54,320
Yeah.

259
00:16:54,320 --> 00:16:55,320
Yeah.

260
00:16:55,320 --> 00:16:59,400
And for the panel, just curious what you think about in this age of AI, how this ethos

261
00:16:59,400 --> 00:17:05,120
of move fast and break things with startups has evolved or changed, if at all?

262
00:17:05,120 --> 00:17:06,120
Yeah.

263
00:17:06,120 --> 00:17:12,960
I believe generally it's much harder for startups to proactively prioritize responsible

264
00:17:12,960 --> 00:17:14,680
and ethical use of AI.

265
00:17:14,680 --> 00:17:21,440
It's been kind of overwhelming for it and endeavor for them.

266
00:17:21,440 --> 00:17:24,680
And that's why we gave them a framework in our team.

267
00:17:24,680 --> 00:17:29,520
We provided the principles and framework for them and we, we tried to kind of educate

268
00:17:29,520 --> 00:17:35,120
them that trust and responsible AI is a, you can think of it in a two in the, in terms

269
00:17:35,120 --> 00:17:41,560
of two dimensional space where one where to, when one dimension is the value, you deliver

270
00:17:41,560 --> 00:17:46,040
to your customers and the other dimension is the level of comfort.

271
00:17:46,040 --> 00:17:51,920
So of course, the value is like the actual product you are building.

272
00:17:51,920 --> 00:17:59,000
And then level of comfort can be a, but it's mainly about data ownership and do customers

273
00:17:59,000 --> 00:18:04,840
have any opinion in your product roadmap or not privacy and security.

274
00:18:04,840 --> 00:18:10,960
One of them most important ones and explainability and transparency.

275
00:18:10,960 --> 00:18:19,000
Do your customers know how their data is used or end users and for what kind of purposes

276
00:18:19,000 --> 00:18:24,480
you are using that data and do they have based on this new compliance, for example, GDPR

277
00:18:24,480 --> 00:18:30,640
right to be, to delete their data from your existing databases or even fingerprints from

278
00:18:30,640 --> 00:18:32,160
your Michelle and X systems.

279
00:18:32,160 --> 00:18:35,720
Is this something you ran across at the Center for Applied Data?

280
00:18:35,720 --> 00:18:37,760
I think Senator, you guys are just getting started, but yeah.

281
00:18:37,760 --> 00:18:38,760
Oh, yeah.

282
00:18:38,760 --> 00:18:39,760
That's good to say.

283
00:18:39,760 --> 00:18:43,880
I think that kind of looking at like a, on a long time horizon, I think being ethical

284
00:18:43,880 --> 00:18:49,560
is a kind of like a profitable decision and I think there is this real conflict of interest

285
00:18:49,560 --> 00:18:54,040
though with, there's so many short term pressures and pressures to focus on the short term.

286
00:18:54,040 --> 00:18:58,200
And that's something that is tough, tough to crack.

287
00:18:58,200 --> 00:19:01,840
And I think that it's great to hear about like a venture firm working on that because

288
00:19:01,840 --> 00:19:07,000
I think often the incentives around venture capital and kind of just our current corporate

289
00:19:07,000 --> 00:19:12,000
ecosystem push people on short term and that it does take a longer time horizon to think

290
00:19:12,000 --> 00:19:17,680
about ethical behavior and the financial benefits of ethical behavior.

291
00:19:17,680 --> 00:19:22,360
I think that there are deeper benefits to ethical behavior in the short term as well.

292
00:19:22,360 --> 00:19:23,360
Yeah.

293
00:19:23,360 --> 00:19:27,280
And I think also like if there's also the risk that like if you're very biased, you might

294
00:19:27,280 --> 00:19:29,440
only kind of cater to one population, right?

295
00:19:29,440 --> 00:19:31,720
And so eventually that limits the growth of a user base.

296
00:19:31,720 --> 00:19:35,600
So even from a business perspective, you actually want to have everyone come on board, right?

297
00:19:35,600 --> 00:19:37,440
And so it's actually, I think it's actually, I agree with you, I think it's actually

298
00:19:37,440 --> 00:19:40,760
good business in the long run to actually be responsible and worry about this.

299
00:19:40,760 --> 00:19:43,920
I would say though, I don't think LinkedIn ever really was kind of move fast and break

300
00:19:43,920 --> 00:19:45,440
things as far as I can tell.

301
00:19:45,440 --> 00:19:48,520
It's, you know, I think it's, if you're an ML engineer, sometimes you can kind of get

302
00:19:48,520 --> 00:19:52,160
carried away and maybe you forget why you're working on, but at least when you're working

303
00:19:52,160 --> 00:19:55,600
on something that's about jobs, it kind of puts this, you know, you kind of realize,

304
00:19:55,600 --> 00:19:59,880
okay, like this is important and how would I want people to treat my resume and my own

305
00:19:59,880 --> 00:20:00,880
job?

306
00:20:00,880 --> 00:20:03,440
And so my sense is like, and of course I wasn't around when the company was created,

307
00:20:03,440 --> 00:20:07,240
but it seems like it seems like for a very long time, at least it's been about responsibility

308
00:20:07,240 --> 00:20:09,800
and kind of like putting members first.

309
00:20:09,800 --> 00:20:12,920
So I've got two more questions and then I'm going to open it up and see if there's any

310
00:20:12,920 --> 00:20:14,760
questions from the audience.

311
00:20:14,760 --> 00:20:18,400
One of them was, you know, Rachel and I were talking about this before we got started

312
00:20:18,400 --> 00:20:24,280
today and it's like, so the Pope was talking about AI ethics effectively at a tech conference

313
00:20:24,280 --> 00:20:30,880
at Envatican City a couple days ago, which is something I want to see pictures of or something

314
00:20:30,880 --> 00:20:32,880
that must be interesting.

315
00:20:32,880 --> 00:20:38,000
But I mean, and then, you know, they're essentially talking about a need for protection of the

316
00:20:38,000 --> 00:20:42,800
common, of the common good, from AI perspective as well as tech.

317
00:20:42,800 --> 00:20:49,840
And you know, other ones, examples of, yeah, ethics coming up in the context of power

318
00:20:49,840 --> 00:20:55,560
in AI, it feels like a word that's been missing from a lot of conversations around artificial

319
00:20:55,560 --> 00:21:01,960
intelligence, Alexandria, Casio, Cortez, and someone else, Joey Blumwinny having a long

320
00:21:01,960 --> 00:21:09,160
exchange about the systems working best on white men and not as well on a women of color,

321
00:21:09,160 --> 00:21:11,880
for example, it seems present in a lot of these.

322
00:21:11,880 --> 00:21:19,040
And this is a very long-witted way of asking how the panel, if there are any specific ways

323
00:21:19,040 --> 00:21:23,240
that you see this power dynamic playing out in the AI ethics conversation today.

324
00:21:23,240 --> 00:21:27,080
I think, you know, this is also one place where like diversity is like super important,

325
00:21:27,080 --> 00:21:28,080
right?

326
00:21:28,080 --> 00:21:30,280
Because essentially the people are in the room making the decision about like what should

327
00:21:30,280 --> 00:21:32,600
be fair will kind of refer to their own experience.

328
00:21:32,600 --> 00:21:35,600
And so that's why, you know, it's very important to like make sure that both in the company

329
00:21:35,600 --> 00:21:40,880
and also kind of like inside, you know, your development teams, like there's actually as

330
00:21:40,880 --> 00:21:45,720
much diversity as possible, so that the decision kind of is made in a way that's kind of like

331
00:21:45,720 --> 00:21:49,600
kind of conscious of everyone's kind of, you know, approaches.

332
00:21:49,600 --> 00:21:53,560
I think, you know, it's, and really diversity is probably one of the best way to get, at

333
00:21:53,560 --> 00:21:54,560
least get started on that.

334
00:21:54,560 --> 00:21:58,400
Yeah, I think it's a really important point you bring up, I would say one area, I think

335
00:21:58,400 --> 00:22:03,680
we see it a lot in is surveillance technology, which tends to kind of disproportionately

336
00:22:03,680 --> 00:22:07,160
be applied against, against poor people.

337
00:22:07,160 --> 00:22:12,360
We've also seen instances of, you know, Baltimore police, I, using facial recognition to identify

338
00:22:12,360 --> 00:22:17,280
protesters protesting the death of Freddie Gray, and those things definitely have this

339
00:22:17,280 --> 00:22:21,640
kind of clear power differential and, you know, even looking at history kind of how surveillance

340
00:22:21,640 --> 00:22:28,640
technology has been used to suppress dissent and moves for social, positive social change

341
00:22:28,640 --> 00:22:31,280
powers a very important dynamic.

342
00:22:31,280 --> 00:22:39,840
And then one last thing before asking the audience if you have any questions to NIA positive

343
00:22:39,840 --> 00:22:46,680
note, do you have any favorite examples of AI ethics success stories where an organization

344
00:22:46,680 --> 00:22:53,000
or team was able to resolve some, some, some challenges or confront them?

345
00:22:53,000 --> 00:22:58,640
I was working with one of our companies, turn it in, they are also based on Valley, and

346
00:22:58,640 --> 00:23:06,480
we were building a plagiarism detection tool mainly for investigators and teachers to mostly

347
00:23:06,480 --> 00:23:13,480
for universities and K-12 institutes, and so it's basically using NLP, all your kind

348
00:23:13,480 --> 00:23:20,520
of authorship style, not using NEPIO, not using any student, performance, grades, anything.

349
00:23:20,520 --> 00:23:27,120
But at the end of like development process, we get together and because I'm not a native

350
00:23:27,120 --> 00:23:35,120
speaker, I ask a question, like, is it possible that our model has, for example, more false

351
00:23:35,120 --> 00:23:39,560
positive rate against non-native speakers?

352
00:23:39,560 --> 00:23:44,520
And because, you know, like, we, historically, there might be more instances of like a

353
00:23:44,520 --> 00:23:50,400
plagiarism for non-native speakers and the model in humano-mission learning is all about

354
00:23:50,400 --> 00:23:55,280
correlation, not necessarily causation, and the model might have picked up those kinds

355
00:23:55,280 --> 00:24:01,600
of, for example, grammatical errors as the signs for, for example, plagiarism.

356
00:24:01,600 --> 00:24:06,600
And we started testing our system and we used the first test and we realized, yes, there

357
00:24:06,600 --> 00:24:13,440
is a potential problem and we actually could mitigate and address such bias in the system.

358
00:24:13,440 --> 00:24:18,120
Oh, that's, yeah, so I have two, two small examples and the, so one that I briefly mentioned

359
00:24:18,120 --> 00:24:22,320
is the representative ranker, so when, you know, when you look for candidates, you essentially

360
00:24:22,320 --> 00:24:25,480
see something else representative, the second one is something that we actually discovered

361
00:24:25,480 --> 00:24:28,720
when we analyzed the experiments, so it was kind of a surprise, but it turns out, so

362
00:24:28,720 --> 00:24:32,560
we have this feature that sends like job notifications instantly, so, you know, when

363
00:24:32,560 --> 00:24:35,720
there's a job that's good for you, you instantly get notified, and we looked at the metrics

364
00:24:35,720 --> 00:24:37,640
and we're like, wow, this is like really equalizing.

365
00:24:37,640 --> 00:24:41,040
So, you know, it seems like people who were not too engaged or like didn't have like

366
00:24:41,040 --> 00:24:44,960
that great networks, you know, were kind of categories that, you know, we thought were

367
00:24:44,960 --> 00:24:46,360
a bit disengaged.

368
00:24:46,360 --> 00:24:49,640
These people just, we see the metrics go way up and we just, we wonder why, and it turns

369
00:24:49,640 --> 00:24:53,520
out, you know, like people tend to self-sensor when they're playing to jobs, they're like,

370
00:24:53,520 --> 00:24:56,280
oh, maybe I'm not qualified enough, maybe I've done good enough, or I can see there's

371
00:24:56,280 --> 00:24:58,000
already people applied, I don't want to do it.

372
00:24:58,000 --> 00:25:01,400
And so it turns out that like when an algorithm tells you, hey, we think you should apply

373
00:25:01,400 --> 00:25:04,080
to this, you'd be good for this, that can also be empowering.

374
00:25:04,080 --> 00:25:07,120
So that's a success story there, which is, you know, and, you know, we just found out

375
00:25:07,120 --> 00:25:10,640
that like some of these algorithms actually help people and actually reduce the gap between

376
00:25:10,640 --> 00:25:12,280
the categories that we might care about.

377
00:25:12,280 --> 00:25:15,560
And so, yeah, these are two, two stories that I was, I was pretty happy about.

378
00:25:15,560 --> 00:25:16,560
Yeah.

379
00:25:16,560 --> 00:25:20,840
Yeah, a story I like is from a meetup, Evan Estola, the lead machine learning engineer

380
00:25:20,840 --> 00:25:24,880
shared that when they were building their recommendation system to recommend meetups

381
00:25:24,880 --> 00:25:29,960
to people, they realized there was the potential for a feedback loop of if you're a woman

382
00:25:29,960 --> 00:25:34,520
or interested in technical meetups, so then the algorithm might recommend even fewer

383
00:25:34,520 --> 00:25:38,920
meetups to women, tech meetups, so you would have fewer women attending and kind of create

384
00:25:38,920 --> 00:25:41,240
this feedback loop pretty quickly.

385
00:25:41,240 --> 00:25:43,680
And so they decided to short circuit that from the start.

386
00:25:43,680 --> 00:25:48,520
And I really like that story because it's also an instance of kind of not just unthinkingly

387
00:25:48,520 --> 00:25:53,480
trying to optimize a metric, but thinking about, yeah, thinking about the outcome and taking

388
00:25:53,480 --> 00:25:56,040
the responsibility around feedback loop seriously.

389
00:25:56,040 --> 00:25:58,040
Nice.

390
00:25:58,040 --> 00:26:01,080
And any questions from the audience?

391
00:26:01,080 --> 00:26:02,080
There we go.

392
00:26:02,080 --> 00:26:04,560
I use this.

393
00:26:04,560 --> 00:26:07,760
Speaking to Lola Pal.

394
00:26:07,760 --> 00:26:09,800
Alrighty.

395
00:26:09,800 --> 00:26:22,000
So with the rise of deep learning, a lot of more principled approaches have fallen to

396
00:26:22,000 --> 00:26:24,000
the wayside in favor of empiricism.

397
00:26:24,000 --> 00:26:25,000
It's going to be a long question.

398
00:26:25,000 --> 00:26:26,000
It's not very long.

399
00:26:26,000 --> 00:26:30,560
I see that a lot in natural language processing.

400
00:26:30,560 --> 00:26:37,280
But in the case of ethics and AI, how do you think principled approaches fit in?

401
00:26:37,280 --> 00:26:40,240
Can you give an example of what you mean by principled approaches?

402
00:26:40,240 --> 00:26:48,480
So I work on machine translation and we've seen of shift towards bite pair encodings using

403
00:26:48,480 --> 00:26:54,760
sub words that don't necessarily have any meaning by themselves, as opposed to using

404
00:26:54,760 --> 00:26:57,120
a large vocabulary of whole words.

405
00:26:57,120 --> 00:27:03,960
So we can't inspect why a model attributes the decision to the sub words themselves as

406
00:27:03,960 --> 00:27:06,680
intelligently as before.

407
00:27:06,680 --> 00:27:12,960
It's kind of in conflict with deep learning's empirical approach to AI.

408
00:27:12,960 --> 00:27:15,960
But I'm wondering, do you think it can come back?

409
00:27:15,960 --> 00:27:16,960
So kind of.

410
00:27:16,960 --> 00:27:21,440
What do you do when your techniques abstract away from your ability to be fair?

411
00:27:21,440 --> 00:27:22,440
In a sense?

412
00:27:22,440 --> 00:27:23,440
A little bit.

413
00:27:23,440 --> 00:27:24,440
A little bit.

414
00:27:24,440 --> 00:27:29,480
Yeah, so I would say I think the Y is generally always a hard question.

415
00:27:29,480 --> 00:27:33,720
So we try to focus kind of at first as like we're seeing some metrics about is a performance

416
00:27:33,720 --> 00:27:37,480
the same for groups that we care about, how people interact with it.

417
00:27:37,480 --> 00:27:41,920
And even with this, we even when the techniques are simple, the Y is always kind of a difficult

418
00:27:41,920 --> 00:27:42,920
question.

419
00:27:42,920 --> 00:27:44,240
Like, why is it that we're seeing this?

420
00:27:44,240 --> 00:27:45,240
What is it about the algorithm?

421
00:27:45,240 --> 00:27:47,040
And sometimes it's not the arguments about the people.

422
00:27:47,040 --> 00:27:50,680
And so when it's about people like figuring the Y is very, very complicated.

423
00:27:50,680 --> 00:27:56,880
So I think what a first step is to try and at least to an extent try to have some idea

424
00:27:56,880 --> 00:28:00,680
about the metrics that we care about or the principles that we care about and maybe

425
00:28:00,680 --> 00:28:04,680
comfortable kind of either delaying something or thinking more deeply about something until

426
00:28:04,680 --> 00:28:05,680
we've understood the Y.

427
00:28:05,680 --> 00:28:10,520
And I agree that as things get more complex or as increasingly one algorithm, the output

428
00:28:10,520 --> 00:28:12,080
of one is the input of the other one.

429
00:28:12,080 --> 00:28:15,080
The Y gets really, really hard and that's no exception there, if that makes sense.

430
00:28:15,080 --> 00:28:16,640
But that's been my experience at least.

431
00:28:16,640 --> 00:28:24,000
I also believe that it's so hard, like the root cause analysis is always hard as we use

432
00:28:24,000 --> 00:28:29,240
more complex machine learning models, it's even harder, but I'm also very encouraged

433
00:28:29,240 --> 00:28:35,520
to see if we can use the same optimization techniques because it's like like deep learning

434
00:28:35,520 --> 00:28:38,920
any of those machine learning, they are like optimization tools for us.

435
00:28:38,920 --> 00:28:44,200
Can we use the same optimization tools to reach fairness?

436
00:28:44,200 --> 00:28:48,320
And there are some papers out there in the recent conferences.

437
00:28:48,320 --> 00:28:54,960
They talk about, for example, rather than optimizing over the global population.

438
00:28:54,960 --> 00:29:00,360
For example, optimizing for micro segments of the populations and getting to, for example

439
00:29:00,360 --> 00:29:05,560
demographic parity, but of course for products like machine translation is much more complex

440
00:29:05,560 --> 00:29:11,680
because the word embeddings, they can't even be racist and we know about all these problems.

441
00:29:11,680 --> 00:29:19,720
And it's really harder to kind of modify the objective functions to reach to the fairness

442
00:29:19,720 --> 00:29:25,800
in the context of, for example, machine translation, but I'm pretty optimistic that we can use

443
00:29:25,800 --> 00:29:28,240
the same technology to get there.

444
00:29:28,240 --> 00:29:32,560
And all I would add is I think it can be useful to just even simple techniques of altering

445
00:29:32,560 --> 00:29:37,760
your input to see how that impacts the output, can give you a lot of insight into what

446
00:29:37,760 --> 00:29:43,200
might be happening, as well as to remember that with humans we're bad at knowing why

447
00:29:43,200 --> 00:29:44,600
we make the decisions we do.

448
00:29:44,600 --> 00:29:49,920
We do a lot of post-talk justification of our decisions and so the causality is often

449
00:29:49,920 --> 00:29:52,920
not well understood there either.

450
00:29:52,920 --> 00:29:54,920
Thank you to our great panel.

451
00:29:54,920 --> 00:29:56,920
Not on pause.

452
00:29:56,920 --> 00:30:05,920
All right, that's our show for today.

453
00:30:05,920 --> 00:30:13,200
To learn more about today's show or any of our panelists visit twomolai.com slash shows.

454
00:30:13,200 --> 00:30:19,200
Make sure you visit twomolcon.com slash videos to secure your access to twomolcon video content

455
00:30:19,200 --> 00:30:20,200
now.

456
00:30:20,200 --> 00:30:36,200
Peace.

