Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.
All right everyone, I am super excited to be on the line with Yashua Benjiyo. Yashua is a
professor in the Department of Computer Science and Operations Research at the University of Montreal
and the founder and scientific director of Miele. Yashua, welcome to the Twimal AI Podcast.
Hi, it's good to be here. It is super exciting for me to have you on the line. I've already said
super exciting twice, so maybe that's an indication of my enthusiasm for the opportunity to chat with
you a little bit about your work. Your name is certainly well-known among our audience and your
contributions to this field. I read somewhere that you are ranked as the most cited computer
scientist worldwide or one of. In terms of recent contributions, yes. There are people who have been
in the field longer than me and I'm among the young old people. Nice and just last year, you along
with Jeff Hinton and Jan Lecune received the ACM Touring Award for your contributions with
deep learning, deep neural networks. Why don't we start with having you tell us a little bit about
that journey and how you came to work in AI and on deep learning and of course we'll get to what
you're working on now, but let's start with some of that background. Yeah, deep learning,
neural nets, it's been my life, my professional life at least. I had started as an adolescent reading
a lot of science fiction and getting acquainted with the notion of AI and robots and so on
with the three laws of robotics and all that and movies like 2001 Space Odyssey. And then at
university, I studied first computer and electrical engineering and then computer science
in my masters and I had to choose a topic and by chance I got to read some of the early neural
net papers from people like Jeff Hinton and I realized that this was the field I wanted to work on
because it was about trying to understand the principles that would explain our own intelligence,
humans and intelligence is very central to who we are and then building machines thanks to
understanding these principles like the way that we understand how burst fly but we're not
necessarily copying the birds, we are trying to understand those principles so we can build airplanes
that also fly. A lot of your recent work is focused on this idea of consciousness. What's the
relationship between consciousness and intelligence? Very good question. So first of all, let me put
things in perspective about deep learning. So a lot of the progress we've made in deep learning
and neural nets in the last few decades has helped us to build machines that can do pretty well
at some of the things that brains are good at including perception, the ability to understand
images, sounds and so on. But there are also things that humans do with their brain, the things
that we are conscious of. So when you decide to do something and you're able to report what you're
thinking about or why you do something, that kind of cognitive ability which is associated with
being conscious of it is not something that we're good at in AI right now and some people might
even think that it's incompatible with all of the ideas that have been put forward with the
neural nets and deep learning and it's more related to some of the older ideas in AI based on
symbols and logic and expert systems. But actually your brain is a huge neural net that we need to
understand better. And the good news is that in the last two or three decades, neuroscientists,
cognitive neuroscientists have made a lot of progress in understanding what is going on in your
brain when you're doing something consciously which parts of your brain get activated in what
order and so on. And so whereas the research on consciousness was almost something taboo in the 20th
century, in this century it is something that has become an important topic of serious science.
And so my research now is at the cusp of this transformation where computer science and AI
research are starting to take stock of what has been discovered in neuroscience and try to take
some of these ideas and import them into new types of deep learning systems that would come with
the advantages of being conscious, which I can also explain. One of the ways that you describe the
relationship between the type of deep learning systems that we have today and this model that you
see as a possibility as we start to incorporate these ideas of consciousness is system one and
system two coming from Daniel, condiments book thinking fast and slow. Maybe you can elaborate a
little bit on these two different systems as he puts it and from my curiosity was that an
inspiration for the way you're thinking about this now or did it become a handy explanation
for some of what you're doing? Oh, it's definitely an inspiration. His work and the work of
psychologists and neuroscientists who have helped clarify the different types of functions that are
being that are happening in your brain has really helped me and others think about how modern AI
systems could could incorporate these things. So first of all realizing that you have these two
very different types of cognitive abilities. A system one is the kinds of things that deep learning
is good at right now. The kinds of things you can do in half a second. So you see an image,
you know that it's a cat and you don't need to think about it. It happens automatically.
In fact, you don't even need to be conscious of it. It's something that we can see in your brain
that if it's maybe happening on the side or you have not enough time, your brain will record
that it's a cat but it doesn't even get to your consciousness. So these things are intuitive
and they're hard to verbalize. Like you can't explain why you recognize that this was a cat.
Or you could try to explain it but it's not a good explanation. In fact, a lot of the earlier failures
of computer vision research was that we were trying to take our own internal explanation. So,
you know, what is it that makes a cat and take that in trying to go through all this. But it was
never, you know, very good because there's a lot of the way that our brain does it which we don't
have access to. So, everything that is intuitive that is at a, you know, subverbal's unconscious
level is roughly system one. There are some differences between conscious and conscious and system
one system two but it's a good way to first order to understand things. And system two is instead
the things you typically do consciously. The things that you can report, the things that you,
you know, you can verbalize, not everything can be verbalized but a lot of the things that are
your conscious of can be verbalized. And that you do in sequence. It's like at each step you,
you might be involving intuitive computation in your brain but you're going to sequence these things
in your mind in a way that you control and that, you know, gives you an extra power, an extra
flexibility which allows us to do things like learn to drive or learn to drive in a new city
or figure out what to do in some unusual circumstances where we have to be creative and find solutions
on the spot and it doesn't look like anything we've seen before. This kind of very powerful,
dynamically adaptive behavior that humans employ to solve new problems is a very typical of system two.
And current AI isn't very good at these things. Current AI, if you train the systems on some data
and then you deploy them in situations that are not exactly the same kind, you get a big hit
in performance. It doesn't work as well whereas humans are able to transfer their knowledge to
new domains to, you know, new environments much more easily. And very often this is when you use
your conscious strengths. So when you're doing something habitual, you don't need to think about
it. So the example I give is when you're driving on the usual path, you can talk to the person
besides you. You're conscious of the conversation but you don't need to be conscious of the details
of the road. I mean, of course you're taking chances when you're doing that but the point is a lot
is going on unconsciously because it's something familiar that you have seen a lot and you don't
need to be mindful of it. Whereas when you're doing something that's more unusual and you need to
practice a new skill or to have a new coherent way of putting together pieces of knowledge,
you're using system two and you're using conscious processing. It was interesting to me that in
your description of these two systems, you talked about unvocalize or subverbal and, you know,
things that we can vocalize and put into words. And in your descriptions of consciousness, you
use the examples of language a lot to represent these ideas. I'm curious to relationship between
language as a construct and consciousness. Yeah. This is obviously an open question. So you have
to realize that the science of consciousness is something fairly new, as I said, like a couple of
decades a bit more, and there are a lot that we need to understand. So we have to be very humble
about how much we don't know. But yeah, it seems very obvious that there's a very strong connection
between language and consciousness, but they're not equivalent. For example, you might have
things that are happening in the area of your brain that deals with language that are kind of
different from the things that have to do with being conscious of something. So, but there are
strong connections. And in fact, one of my current research tracks is to exploit hypothesis
that that connection is very tight in the sense that the high level concepts that you manipulate
consciously also correspond to things like words, things that you can verbalize. And so the
representation level, the two things are very close to each other. Yeah, there's been
a past work, nothing that I could cite or that I know intimately, but that we kind of talk about
generally that, for example, in some cultures, they don't have words for certain types of things.
And the implication being that that kind of conditions their thought in certain directions
that differs from other cultures. And it seems very much in line with this idea of
consciousness as you're defining it. Yes, absolutely. And in fact, the way that I want to use this
hypothetical connection is to help the learning of these high level concepts. So, what deep learning
is about, which I didn't have time to say earlier, is learning good representations.
And we've been very good at learning, let's say, low level and mid-level representations with deep
learning, especially on the visual input. But we don't yet have good algorithms to discover
the right variables or high level concepts that humans manipulate consciously. Of course, we can,
you can kind of cheat by telling machines, this is a cap, this is a dog, and then they kind of know
what is a cat and a dog by example. But what we don't have is an ability to discover these
high level things. And so, by connecting, say, video input with a corresponding language, in the
right way, we can force a deep learning system to learn representations, which at the top level
would have features, if you want, that correspond to words or phrases or linguistic constructs.
And that would help the representation learning system discover those high level concepts,
in the same way that when we talk to children, using words in particular context helps them
build the corresponding meaning of those words in their mind. But of course, they don't need the
words. They're building meaning absent of the words. Initially, they learn a lot of things that
they don't have words for. But it's actually difficult to disentangle these things, because even though
it might take a couple of years before they start talking, during those couple of years, they also
hear a lot of language. Now, the interesting clue is that in some cultures, parents don't talk to
their children. They talk with each other, and the children just listen. Whereas in our western
cultures, we tend to talk a lot to our babies. So you might think that they're learning from that
interaction. But it looks like even without a direct sort of naming, oh, this is a cat, this is a dog,
the babies can catch the connection between the labels, the words, and the things in the world.
So we jumped in pretty quickly and started talking about some of the implications of how you're
defining consciousness. But you really launched into this field with a paper in 2017, the consciousness
prior. That's right. What exactly is the consciousness prior? And maybe tell us the, you know,
the main points that you're trying to convey in that paper. Okay. I'm going to try to do that
in an accessible way. Yeah. So first of all, what is the word prior means? It's a term we use
in machine learning research to say that the learning system is exploiting some kind of assumption
about the world. And there's even theorems that say that you have to have assumptions, at least
minimal ones in order to be able to learn successfully. And so the brain has these kinds of assumptions
about the world that, you know, we inherit from our ancestors through that have, you know,
these assumptions have encoded somehow in our genes. And we are born with those things.
And they help us to learn faster and better in the world around us. So when I said at the
beginning that one of my research goals is to understand the principles that give rise to
intelligence, well, an important part of this is what are the kinds of assumptions that humans
exploit about the world that allow us to learn efficiently about it, to build, to understand how it
works, to learn language, to learn to model the world, to act in the world and so on. So notions,
you can think of things like, well, notions of time and space of agency that, you know, I do things
and their effects and so on, are probably things that we got from our genes and that our brain
is exploiting as some kind of assumptions about the world. And so the consciousness prior is
one such assumption which would be connected with a notion of consciousness, but they actually,
I actually have a whole list of related assumptions, but this one is very central. So what it says is
there are two kinds of knowledge about the world, which is somewhere in our brain. And that's
basically the system one knowledge and the system two knowledge, right? So the system one knowledge
is knowledge that is difficult to put in words, as we were defining system one this way essentially,
whereas system two knowledge is knowledge that is easy to put in words. Now what kind of knowledge
has the property that it can easily put in words? Well, there is a nice structural property
of the kind of knowledge we can communicate with words. And the property is that we are able to
make predictions about things that can happen, about words, given other things that we know.
So for example, if I say, if I drop my phone, it will fall on the ground,
that sentence only involves a few concepts. It involves a phone, it involves the ground,
it involves the act of dropping and the result, right? That's very few elements if you think about
it. Normally when you try to make a prediction about something in the world, you need a whole lot
of other things to make that prediction accurate. So if I'm trying to predict the next pixel that
will show up at some position in the video that I'm seeing right now, I need to know about all the
other pixels and all the pixels that I've seen in the last few seconds or something or minutes.
And that's like millions of numbers that come into that prediction. So pixels are difficult to
manipulate to explain with language. They don't have this property. But instead, if I explain the
world in terms of objects that I can name, like I did with my phone that could drop on the ground,
this way of representing information, of representing knowledge is one where we can make
statements about things that should be true and things that should not be true.
Where each of those statements only involves very few concepts. So with natural language,
we can communicate a lot of knowledge about the world, but it's decomposing to little pieces
like sentences. In each of these sentences only involves very few concepts. So in machine learning
jargon, we say we can summarize this in a single phrase. We could say that the joint distribution
of the high level concepts is sparse. Sparse here means that if you draw the connections between
all the concepts as a graph, the graph has very few edges coming out of each node. So each
node corresponds to a concept. And each concept is attached to other concepts through very few edges.
So the graph between the connections between the concepts in that sentence or the concepts in
all the things that you could say, if you think about all the things that you could say,
you decompose all the things that you could say into sentences just to make it keep it simple.
Now each sentence is like a special kind of node in the graph. And each sentence connects to
very few words. Now the words are connected to many, all of the sentences that they could appear in.
But each of these sentences only involves very few words. And so overall, the structure of that
graph is extremely sparse. Whereas if I were to draw a graph of the statistical dependencies
between pixels, it would all most need to be like fully connected. That every pixel needs to talk
to every pixel. So what happens is your brain has created these high-level variables, high-level
concepts, which allow to decouple a lot of the complicated dependencies that would otherwise
seem to exist. And in addition, so that's the consciousness probably, but there are like
side statements to this. So one of them is that those high-level variables have to do with
causality. So those dependencies, when I said that if I dropped the phone, it's going to fall on
the ground. It's also like a causal statement. It says something about if I do a particular action,
this is going to be the consequence. And there's an object which is going to be affected by the
action of dropping the phone. And so this is important because causality allows us to understand
the world in a strong sense. That goes beyond making predictions. It allows us to
imagine what could happen, for example, or what could have happened. These are called counterfactuals.
You give an example in some of your talks on this about putting on sunglasses and that simple
act which can be reduced to one bit. Change has a significant implications on all the pixels
that are firing in your retina, but it's a single bit. Yeah. So exactly. And so I was talking about
causality earlier. And one of the particularly interesting aspects of causality is that it tells us
about how the world typically changes. So it's not just about how the concepts are related to
each other, but how these relations changes over time due to agents like people or robots,
eventually, and animals doing things. And what happens is that those changes are very localized
in the sense that they involve only one or a few concepts. So typically when you come up with an
explanation for something that has happened and our brain constantly tries to come up with explaining
what is going on verbally, we end up being able to provide a very short explanation like one sentence.
Oh, he dropped the phone. But this is amazing, right? Because the world has changed like I've put
on these dark glasses. And I'm able to explain it by referring to very few things. I have put on
my dark glasses. Whereas if you didn't have that assumption, you might imagine that everything
has changed and you can't do any prediction anymore. But now the fact that is very few things
that have changed allow you to recover from those changes. And this is what humans are good at.
They're good at recovering from changes that are happening in the world. So that's this adaptive
strength skill that humans have that we would like to put in machines. So this whole research
on consciousness is not just about understanding an important part of who we are, who we are,
which is clearly consciousness is a big part of it. But it's also building those abilities in
machines to be more robust to the changes that can happen in the environment. The idea of
consciousness, then you kind of express it as this low dimensional representation of the broader
connections that are in the brain or that are in some system that we don't have yet. That's related
to the idea of attention that we've been experimenting with in neural networks. What's the connection
between those? Yes, yes. Very good question. And actually, it's a tough one because a lot of people
still don't see the connection. So let me try to explain it. So remember I said that we're exploiting
this assumption that the dependencies between concepts have this sparsity, like pieces of knowledge
each would be like a sentence. Now, if you want a machine or brain to compute over that knowledge
base that we have in our brain or in a computer, you want to take advantage of that sparsity.
And a good way to do it is to focus the computation on just the right pieces at each step,
because then you only need to consider the interactions between a few variables.
And that's much easier both in terms of computations and in terms of what we call the statistical
advantage. So if something changes then we can learn it quickly if it involves only a few variables.
So attention is a way to exploit this inherent sparsity, the assumed sparsity that I'm talking about,
so that we can do this very special kind of computation that involves very few variables at a time.
So attention selects just these few variables that come into our working memory.
And then we can utter a sentence to share what we have in our mind. But there's more to this,
the fact that we have this attention bottleneck, which is also this bottleneck is a central piece of
current theories of consciousness, forces the part of the knowledge about the world which
should go in system two to go there. So only the things that can go through this bottleneck
that involves little pieces of knowledge involving a few variables at a time are being
represented at that level. The stuff that you can't handle with consciously goes, you know,
system one, and it takes more time to learn because it about the interaction of many things together,
like extracting information from images, but the stuff that somehow has this property,
the consciousness prior property, that will be processed by system two, and now you have
advantages because you can do things at that level because you exploit that sparsity,
you can quickly reason and do all kinds of things plan that are harder to do otherwise.
Is the idea there that this system two, if we think of it as like a memory,
you know, the bottleneck says that it can't just store anything, it can only store these higher
level concepts or representations, is that? You're right. We only store the stuff that goes
through our consciousness. The things that you're unconscious of are going to stay in your mind,
in your brain, you know, very short time, and then you're going to forget them. Whereas the things
that you've been conscious of are more likely to be stored in long-term memory. But in addition,
when they go through this short-term memory where you can operate on them, you can do pretty
fancy things that is what we call thinking or reasoning or planning or discovering something in
coherent and that we do consciously. So is the idea of attention as applied to consciousness
that we might use attention as a way to train this consciousness prior? Yes. Yes. Yes. So attention
is part of the architecture of the neural net in order to enforce the prior. What's interesting
is it has already brought a kind of revolution within deep learning. So there's been amazing
progress in natural language processing. It started with the work we did in which
translation around 2014. And then since 2016, this has been put in Google Translate and then,
you know, it's become the dominant kind of technology for machine translation. But since then,
with new architectures that exploit consciousness like what's called the Transformers,
the progress in all kinds of natural language tasks has really, really progressed a lot.
And it's changing the very nature of the way we think about neural nets. So in the traditional
neural net, we think of the computation as operating one step at a time on these vectors. So like
a fixed set of numbers, which correspond to a bunch of neurons having some activity. But when
you have attention mechanisms, what it makes it possible is to operate on sets of objects rather
than on these fixed size vectors. So already, this is having a big impact in natural language
processing because language has this property that, you know, you want to just take some elements
of what I've been talking about in the last five minutes. And then reuse it in order to say
something, you know, that makes sense with respect to what I said. So it tends to select a few
elements, combine them in new ways, add some new things. But at each step, you only consider a
few things at a time. And current transformer architectures have this inherent structure.
Another thing that's going on with these attention-based systems is that in a way we are introducing
some of the old ideas from AI of indirection and naming things and having things that have a
type. So these are concepts that have been there since the beginning of programming,
but haven't really, it wasn't clear how to incorporate these ideas in your nets. And so
with attention, there's something really interesting going on, which is now you select
which neuron, for example, is going to talk to which neuron. So it's kind of like dynamically
changing the connection pattern between neurons or groups of neurons. And so when you have
this dynamic connectivity going on, you need to carry information about where is the signal coming
from. So it's not just the information I send you, but it's coming from me and you need to keep
track of that. In a lot of ways, the idea of a consciousness prior makes me think of the work
that's happening around model-based machine learning. So in reinforcement learning, for example,
we're starting to see a lot of work or we've been seeing quite a bit of work around models.
It is the idea that the consciousness prior is like the specific type of model that's analogous
to what you would call our consciousness. So the connection with model-based reinforcement
learning is the following. When you do model-free reinforcement learning, you train a policy,
in other words, you train a neural net which is going to be called whenever you have to take a
decision and it takes a decision and it's sort of automatic. So it's like when you're driving
and a habitual route and you don't need to think about it, it knows what to do and there's no need
to involve consciousness. But when you plan in your route on the fly, let's say there's some funny
construction going on on your road and suddenly you realize that you're going to have to use a
different path and you think about it. That is planning on the fly and that kind of planning on
the fly is a form of reasoning and it's something that's conscious and that allows you to deal with
as unexpected occurrences and that in principle is exactly what model-based reinforcement learning
is about. So once you have a model of how the world works, you can create a new policy on the fly,
something you've never done before. By combining the pieces of knowledge you already know, say about
pieces of roads and so on, in order to come up with a new plan and that dynamic decision-making
about the future and imagining the future in order to take decision in a very flexible way
is very, very characteristic of conscious behavior and system two. So I think that at the end of the
day we're going to have reinforcement learning that has both model-free elements and model-based
elements because they both have their strengths. So earlier I referred to this idea of conscious
as far as a memory. Is it strictly speaking memory in nature or is it more active? Does that question
make sense? Yes, it's more active. So it's more about how you process information but the connection
to memory is it's also connected to how we represent a lot of conscious knowledge. So the things
that you can access in memory are conscious pieces of information. I mean they're not conscious
until you retrieve them from memory but they can become conscious. And so there's a sense in which
a lot of your conscious knowledge is stored in your long-term memory and then it could be retrieved
as needed in order to solve problems that you're facing today. And conscious processing is dealing
with the computations that your brain is doing to do these things, to retrieve things from memory,
to interpret what you're seeing now and potentially also to visualize things that could happen
in the future. I think that the memory analogy was that it was kind of the store of the relationships
between the things. But the planning based on that information is not necessarily the same thing.
That's right. In a sense there is declarative knowledge. So all the pieces of knowledge
that are in memory and then there's the computation that you do on the fly in order to combine these
pieces of knowledge with what is going on now or what you're imagining in order to come up with
sometimes better explanations about the past or about what you'd like to do in the future.
What's kind of the current state of this line of research and how do you see it evolving?
Oh it's still in its infancy. I think how many decades it took for like Neonets and Deep Learning as
it stands now to really reach the maturity that it has. So I think this is a long-term project
and I also think that there's a huge importance in the collaboration between the brain sciences.
That includes cognitive science, neuroscience, but also philosophers of mind which have been
thinking about consciousness and mind for a long time. So all of these people who have been thinking
about the human side of the equation should be collaborating with the people in AI and Deep Learning
who are interested in understanding those principles and trying different ways of capturing
these things in computers. And that can go also back in the other direction because one of the
problems with say neuroscience or philosophy is that it's difficult to come up with good theories
that explain the many observations that we have. But machine learning can come up with interesting
theories because they are motivated from the learning theory point of view. In other words,
so my conscious prior idea is something that makes sense from a machine learning perspective
because as soon as you start making assumptions about the world, that means you could learn faster,
you could adapt faster. And so these kinds of justification can help
constrain theories that neuroscience or philosophers might be considering. So at the end of the day,
be theories that both are consistent with what we know about humans. And that makes sense from
a computational perspective, from a learning perspective. Before we got started talking about
consciousness, we were actually before we started recording. We were talking a little bit about
kind of what's going on in the world now as we record this mid-March and COVID-19. And one of the
other topics that you've been spending some time working on is how machine learning and AI
could make a difference in that setting. Can you share with us a little bit of what you're doing there?
Yes, just put a bit of context. So I've been involved, of course, in basic research in machine
learning and deep learning for many decades. But also in the last few years, I've realized the
importance of thinking about how AI is deployed, will be deployed in society. And how we can steer
our collective boat in directions that will make AI a useful force for the world and for humanity.
So this is why I got, I'm barked, for example, in the project of writing the Montreal Declaration
for Responsible Development of AI. This is why I'm involved in the project called AI Commons
to try to help developers and NGOs and philanthropy work together on applying AI to areas that
really AI for social good that may not be necessarily profitable but are important to do for
humanity. This is also why I've been involved in the last couple of years in working on how AI could
be used to fight climate change. So we wrote a very long paper that does a survey of many different
areas in which machine learning can be used to help reducing greenhouse emissions, to design new
materials, to better use the renewable energy sources that we have or to even help people understand
better what is going on with climate change. And then, of course, in the last few weeks,
like everyone else, I've been, you know, into this tornado to try to think with many of my
colleagues, not just in AI, but also in other, especially in healthcare, thinking about what AI
can do among many other disciplines that are, you know, putting their brains together. What can we
do to help fight this COVID-19 pandemics? So I'm currently involved in a number of projects.
One that is taking a lot of my time these days is the design of a tracing app. So one of the things
that we can do to really find a good balance between saving lives and allowing people to go out of
their homes is keeping track of where people go and who they meet so as to estimate their risks of
being infected and reduce those risks. And we want to do it in a way that's very mindful of privacy,
maybe unlike some of the apps that have already been put out there. This has come up quite a bit
recently. Yeah, it's very important because you just use Bluetooth and make the information available
to, you know, the benevolent government agencies. That's right. So I don't think it would pass
in North America. And I think there are good reasons for this. But the good news is there are
technical solutions to this. And we're working on that. And we're working on machine learning to
help predict your risk level based on the encounters you had before. So, you know, maybe you didn't
meet somebody, you didn't meet somebody who we knew was infected. But maybe you met somebody,
who met somebody, who met somebody who was infected. So then what the probability that you are
infected, right? And you've made 20 of these encounters. So maybe the risks accumulate. So how do you
aggregate all that information and help people know what is their risk? Another thing we're working
on is the design of new drugs. So machine learning and especially deep learning has been used
in the last couple of years. There's been a flurry of papers using these systems to propose new
candidate drugs. In particular, we're working on antivirals. In other words, drugs that you would
give to somebody who's already sick. And maybe who's very sick and we're, you know, concerned that
they might die. And so we could give them these experimental drugs because because these are
new things. And so there's a problem where machine learning can come handy is that the normal,
usual development of new drugs could take many years, sometimes a decade. But it turns out that
machine learning can greatly accelerate the search for good molecules. When you're talking to clinicians
and practitioners, folks from the healthcare, side of things, as well as, um,
virologists and epidemiologists and, uh, kind of the end users of systems like these.
What are the things that they say they need from, you know, us as a community of, um,
you know, data scientists, machine learning, uh, researchers and the like.
Well, it's not always easy to get that answer. I can tell you, I've been learning a lot,
uh, in, in the last few weeks and, uh, I'm sure a lot of people have been learning a lot because
we all need to understand what is going on. So, uh, there are many questions. On the healthcare
and clinical side, they, they would like to be able to predict just monitor what is going on.
Is, is already difficult. And, uh, predict, uh, where there might be greater need for healthcare
resources to allocate resources properly, um, predict what patient, given their history,
would be most likely to need like an ICU, uh, or a respirator, um, predict the risk level of people
as, as I was mentioning earlier, uh, help, uh, epidemiologists model what, you know, what's
going to be the likely, uh, scenarios that we're, we're going to be facing, even, even things like
logistics. So a lot, there are a lot of problems right now where we're just not organized properly to
deal with the massive number of people calling for help, right? So, uh, there's just lots and
lots of areas where machine learning can be useful. And, uh, it takes a lot of time to understand
those issues to talk to those people, uh, to get access to data as a big thing. But now the good
news is, in our societies, access to healthcare data has been a huge problem, um, because we've put
all of our, uh, weight on, uh, privacy. And, uh, it's, it has meant that it's been difficult to,
for researchers, machine learning researchers to have access to this kind of data. But, but now what
is happening with COVID is that the health authorities are seeing that, that we're missing a boat,
like, or they're, you know, quickly changing the ways that we're doing things to allow researchers
to get their hands on, on the proper data sets, or we're going to lose lives where, you know,
we could have saved those lives. So I think it's an important moment to demonstrate the need for
a more agile, uh, data infrastructure, uh, for healthcare. With regards to the app that you mentioned,
we've talked quite a bit on the podcast about differential privacy. Does that come into play,
uh, in allowing you to use this location data in a, in a private way?
We're looking at different options. Right now, I don't think we need differential privacy. So we,
we may need to blur some of some pieces of evidence that would make it too easy to reach race
people. But if you want to predict the risk level of a person, otherwise predict the probability
that you are currently infected, um, you don't need to know where I was. Um, you all need to know that,
uh, you know, maybe yesterday I, I was close to somebody who had risk level six and the day before
I was close to somebody who had risk level seven and, and so on. And you don't need to know, uh,
the kind of trace of where everyone was, that can be computed in each person's phone.
The only thing you need to share as data for training the, the risk predictor is what were the
encounters, like in the sense of what were the risk level of people you're encountered and when,
and that, from that information is very difficult to trace who met with whom because you don't,
you don't have a handle on who was where when. So, so I think that the, you can, you, we could
globally share the level of the planet, that kind of data and deal very good models of your, um,
risk level. You know, when you look across all of the things that are happening, you know,
your research and, uh, elsewhere, what are the things that, um, you see as most exciting in terms
of AI's contributions of this fight? Wow, I guess I'm very biased. So, uh, I'm, I'm most invested
right now in this tracing thing because, um, I think that, uh, medical treatments are going to take
months, probably, you know, your ish two years, for some cases to converge to something everyone
can have, especially vaccines. You have to understand that before we release a vaccine, uh,
we need to make sure this drug is really, really harmless because we're going to give it to
everybody, right? So, right, right. And that process takes a while, even if it's, even if it's
fast track, it takes a while. No, but like, if one percent of the people we give a vaccine to die
because of the vaccine, that is not good, okay? Right. But a antiviral is a drug you could give
to somebody who's already close to dying anyways. And so, we can take a bigger chance. And so,
we don't need to wait a year or two, uh, to know that it's okay. And so, I'm, I'm invested in
the, uh, project around, uh, the development of new antivirals. Um, I mean, uh, before that,
there are already been work in using machine learning to test existing drugs. This is like the
first line is there a drug that has already been approved. So, we don't need clinical trials.
Uh, that we can just use tomorrow morning. Okay. And that's what we're going on. There's a bunch
of clinical trials going on with promising leads and machine learning has already been used to
suggest candidates. But the next step, if, you know, if none of these things work is to develop, uh,
uh, a new molecule that didn't exist. Right. So, so the tracing thing is very important because it,
it's, uh, it's something we can do even before all this, right? Even before we find which,
uh, those, those drugs and, and, and do clinical trials for them, we should be able, in a matter of
days and weeks, to all have on our phone, an app that will help trace our contacts, uh, in a,
in a private sea respecting way, um, and make it possible for us to meet, for example, other people
and know that they're unlikely to be infected. And so it's okay. We can be close to each other.
We can work together. Uh, we can be in the same bus together. This is very important. A lot of
people right now don't have any transportation because if you don't have a car or if you don't drive
and you're infected, for example, you, you can't have transportation. So like, we need to know
who is likely to be infected and not or at one degree in order to organize society around this
for the next probably one or two years. Well, super important work. Thank you so much for
taking a time to chat with us about what you're up to, both the work that you've been focusing on
broadly, the consciousness work, as well as the more recent work you've been doing, uh, in this
fight against COVID. My pleasure. It's great to speak with you. Thank you. Bye.
All right, everyone. That's our show for today. For more information on today's show,
visit twomolai.com slash shows. As always, thanks so much for listening and catch you next time.
