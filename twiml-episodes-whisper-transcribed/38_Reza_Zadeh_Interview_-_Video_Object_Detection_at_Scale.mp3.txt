Hello and welcome to another episode of Swimultalk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
The show you're about to hear is part four of our five part O'Reilly AI New York series,
sponsored by Intel Nirvana.
As I've mentioned before, I am super grateful to Intel for helping make this series possible,
and I'm excited about the cool stuff they launched at the O'Reilly AI conference, including
version 2.0 of their neon framework and their new Nirvana Graph project.
Be sure to check them out at intonirvana.com if you haven't already listened to the first
show in this series, where I interview Naveen Rao, who leads Intel's AI products group
and Hanlon Tang and algorithms engineer on that team.
It's Swimultalk number 31, and you definitely want to start there.
My guess for this show is Reza Zade.
Reza is an adjunct professor of computational mathematics at Stanford University and founder
and CEO of the startup Matroid.
Our conversation focused on some of the challenges and approaches to scaling deep learning, both
in general and in the context of his company's video object detection service.
All right, on to the show.
All right, hey everyone, I am here with Reza Zade from Stanford University and Matroid.
Reza gave a presentation earlier today at the O'Reilly AI conference, and I'm excited
to catch up with him here.
Hi Reza.
Hey Sam, thanks for having me.
It's great to be here in New York.
I appreciate the time you're taking to talk about machine learning AI and the stuff
I love.
Absolutely, I love it too.
And I've been having a bunch of fun talking to folks over the past couple of days.
Why don't we start with a little bit of introduction?
Tell us a little bit about your background and how you ended up doing what you're doing
in AI.
Sure thing.
So I've been working on machine learning for around 12 years now, since I was 18, started
at Google working on machine translation, language modeling, and word alignments for machine
translation.
Back when it wasn't neural in any way, more traditional phrase-based models, transitioned
into distributed machine learning as a tool, so how do you take, say, 100 machines in
training machine learning model that is either scaled across large model, large data, or
many models?
Okay.
And that manifested itself in the machine learning library in Apache Spark, so that's
one of the projects that I've worked on heavily.
More recently, the Twitter who to follow suggestions, the recommendation algorithms on
there.
So if you go on to twitter.com, there are a lot of things on there.
And one of the things you'll notice is the recommendation pane, it says, who do you
want to follow?
And the algorithm is behind that, we're actually part of a chapter of my PhD thesis.
And then after graduating from Stanford, I got hired by my department to do a distributed
machine learning class, as well as a more data science oriented class with graph theory
in it.
Okay.
And while an adjunct professor at Stanford, I started matroid.
The computer vision has been completely taken over with machine learning.
And I've been working on machine learning my whole career.
So ever since I've had a job, it's been about machine learning.
And because the computer vision is so overtaken with it now, it was the perfect time for
me to get into it, especially since computer vision also went through a revolution.
Many problems that weren't possible before became possible.
It seemed like the perfect opportunity.
And so now, most of my time goes into matroid, the computer vision company, and that was
a talk today.
I gave a talk about matroid.
We recently released matroid.com as their website.
And I'm currently thinking hard about deep learning and computer vision on a day-to-day
basis.
Okay.
Your talks span both the distributed machine learning, scaling machine learning, and matroid
there, kind of in a role conversations, is that right?
That's right.
And matroid is a studio for creating and using detectors.
Now, these detectors are a very heavy duty in terms of computational cost.
And to be able to scale such a system to many detectors and many users and many streams,
we have to build an entire cluster of commodity machines with fancy hardware GPUs in particular,
and scale them up and down dynamically.
So as more people use our system, we need to scale up, scale down so that we're not wasting
resources.
And the way we do that is, essentially, all of it is new.
And we are particularly good at making sure it's also fault tolerant so that if, so just
a little background, as a service, we provide the ability to monitor a stream of media.
So let's say you're watching a TV channel and you want to see when there's a Coca-Cola
logo show up on this TV channel.
And then you can, of course, multiply that by all the TV channels in the world, which
is my very hard to do for an intern in a production studio.
But that's the kind of thing that's possible now when we provide as a service.
That, the fact that there are thousands of streams that could come at us and some of them
could go down and they could come up.
And that could be our fault.
We have to deal with that.
It has to be computational ability to run these models.
There has to be RAM to run these models.
These are, these models have to fit into smaller GPUs.
These GPUs have small amounts of RAM, not smaller GPUs, GPUs with small amounts of RAM.
These are all challenges that have to be solved at the same time to be able to provide a service
that we provide.
On top of that, there's a UI components.
We have our own video player, the video player lets you have a poor man's reinforcement
learning happening.
And all of that tightly integrated provided as a service is matroid.
I don't know how much you want to go into matroid itself as a product.
I'm happy to talk about either the distributed machine learning aspect of more general
distributed machine learning or the distributed machine learning aspect of matroid or machine
learning in general up to you where we go with that.
So one immediate question that I had was, and you describing what matroid is doing, it
made me think of Google's recent, in fact, I think just today they put this, their video
object detection offering into beta.
Are you doing similar things or tell me a little bit about matroid in the context of
that?
So the Google offering is primarily focused towards developers.
So the Google customer or the Microsoft or the other customers, there would be people
who can code, people who are already somewhat familiar with machine learning.
Right.
They're providing APIs.
And the reason the Google sells this is because they want people to come to Google Cloud
Platform and spread and compute.
That is something that is totally not our customer.
Our customer is someone who has a need to watch media.
And that happens where it's the production internet agency or a brand monitoring company.
That's right.
That's right.
So these people who can't code.
Right.
And there's a very different question.
So you're offering a solution as opposed to a set of APIs and yes, that's right.
Okay.
Although there is overlap in the technologies used for sure, we're using convolutional
neural networks.
For sure, we're both scaling machine learning in some way.
But the customer is actually very different.
The UI is very different.
And that's the differentiation.
We don't directly sell to developers, although we have an API that developers can use
and if they're sophisticated enough.
But we're essentially like a Photoshop for computer vision.
You just have to be determined at clicking and pointing.
You don't need to learn programming to use this.
And then the other thing you said that was interesting was poor man's reinforcement
learning.
Yes.
Tell me about that analogy and what that means in your world.
Sure thing.
So we have this whole detector creation flow within matroid.
So if you go to matroid.com, you can create a detector for whatever you want.
And in fact, during the talk today, I made some examples of whatever I want.
Right.
So in the talk today, I made a detector for cars.
Okay.
Like a live demo.
Yeah.
Absolutely.
And most I never I use like five slides and the rest of the 40 minutes was in on matroid.com.
Okay.
So it was almost entirely a live demo that you can do if you make an account on there yourself
now and do everything I did.
There's a flow there to create a detector.
And this detector once it's once it's created, it takes three or four minutes to create.
Once it's created, you can immediately see how well it's performing by running videos
through the video player.
And immediately see, okay, with the video player, you see, okay, it's working on these areas
of the video.
It's not working on these areas of the video.
And because we have our own video player, you go in and you can tag where that, where
those mistakes happen, where you want to reinforce correct attitude and where you want to guess,
give negative examples and say, no, this is incorrect.
You can do that very quickly with our with our video player.
And then that goes back into reinforcing correct attitude in the model.
So then the model can be retrained then and now you have a better model.
That's why I said it's a poor man's reinforcement learning because it's not traditional reinforcement
learning where we have the whole loop right, taking care of, you have to provide us with
examples through the video player.
But it's exactly the same spirit of reinforcement learning that an agent is essentially exploring.
So you create a model, you have some, some policy you think is right.
You run a video through, you observe how the detector is working.
If it's working well enough, you're done.
You go to town, you use that detector.
If it's not working well enough, you can you can iterate on it and reinforce correct behavior.
And the UI is essentially what allows a human to become the reinforcement learner.
And the fact that the training and the training for computer vision, strictly for computer vision,
not for general reinforcement learning.
We are not a deep learning as a service company.
We are not a machine learning as a service company.
Right.
Right.
And is it primarily video or also still images?
Of course, it's also still images.
Okay.
There's something that is in terms of the use cases and like where most of our customers
care about video, because if you remember that value proposition, the value proposition
is you're hiring someone to look at vast amounts of media, right?
You're hiring someone to look at vast amounts of media.
Sure that vast amount of media could be a very large image collection.
Yeah.
But more often than not, you would hire someone to watch very long videos, because then
you would have a need for it to hire a person.
If you have a hundred pictures, chances are you still wouldn't hire a person.
If you have a thousand pictures, chances are you still wouldn't hire a person.
If you have millions of images and photos, yeah, sure, you would hire a person at that
point.
But there are way fewer of those cases than there are people who have asked amounts of
video.
Right.
It's very easy to have a camera on 24 seven.
It's very easy to need to watch TV 24 seven.
There are multiple streams of video ever going amounts of video streams in the world now
with because cameras are cheap.
And we expect them all to have the ability to have eyes on them with with matroid or other
services.
So we talked a little bit about branding use cases and envisioning surveillance style
use cases as well.
What are the kind of the major clusters of use?
So we have we have two big industries that were focused on it.
One is TV and the other is a security.
Okay.
So those are the two.
That's it.
As a startup, we can't be too sure broad, right?
And so yes, you can actually just take your home camera and integrate it with matroid
as well.
So if you want notifications when some particular person is at home, okay, you can set that
up pretty easily to the level of person, person A versus person B versus person C or more
interesting things that that could be funny.
Like, like, maybe you want a detector that just thinks you're whenever someone with a beard
comes in.
I'm being silly here.
That's the level of customizability that we have there.
Oh, that's interesting.
So if someone with a beard walks in the room, you want a notification, it's kind of
silly, right?
But more interesting, you could do something like maybe a kid has a hand in a cookie jar.
Yeah.
That'd be pretty funny.
Yeah.
Or opening up, you know, your car and diet and you're opening up the fridge after a certain
time at night or something.
Oh, yeah.
And you pick up the Coke instead of that's that's that's an answer to take use case, but
absolutely possible.
If you put a camera in front of your fridge, yeah, we'll be able to detect whether you're
picking up a diet Coke or a regular Coke.
Oh, wow.
So it's interesting that the fact I bought a Pepsi and a Coke for the demo today, one
of the how all cards downstairs in New York, I just put them in front of it and very easily
figured out that one's a Pepsi, the other Coke.
I didn't have diet versus regular, but that's that would be a new detector.
Okay.
It's interesting that video is becoming or a computer vision in general, I guess is
becoming almost like, I don't like a lingua franca detector.
The background thought here is, there's a point in time that I wanted to do like a home
automation project and you know, the question was, how am I going to figure out, you know,
who's in the house and who's not and that kind of thing and, you know, I, this was years
ago and I got into, you know, walking around with like, you know, NFC going to work or
Bluetooth tags or things like that.
And the computer vision stuff is advanced so much that now you would just throw up a camera
and use that and it's opened up like so many different avenues.
You mentioned something, well, so another application that you're absolutely right.
Computers are getting eyes and that's incredibly exciting.
It used to be that it was a blob of numbers to the computer and now it can understand
what's going on and to the point where it becomes the ultimate sensor.
So instead of having all these weird sensors, we can just have cheap, really, really cheap
cameras, I mean, these cameras are incredibly cheap and they are incredibly powerful and
we want to give them more and more power.
That's exactly what Matroid is.
We think that the ability to sense things through your eyes is immensely powerful.
When you think about it, a computer can take in our eyes, right, their eyes taken so much
information, right?
Absolutely.
Megabytes and megabytes of information per second is going in through our eyes.
But as humans, we only have the ability to type at like a very slow speed.
So our eyes are giving us much more information than we could give a computer with clicking
and with most sensors too, actually, I would say.
So most sensors out there, they would give a computer a few bites of information per
second.
But cameras, it's not like that at all.
It's tremendous amounts of information and it's been such an overwhelming task to
sit through that until, until now, until deep learning and until CNN's.
And then it's a matter of now taking this power and being able to make it flexible, putting
it in the hands of everybody instead of just developers, going to town with it basically.
It's incredibly exciting time for computer vision.
A couple more questions, you know, I don't want to go too deep into Matroid, but a couple
more questions that came up for me one is, do you have pre-established relationships
with network supply, TV network suppliers so that like a brand agency can just click like
I want to monitor ABC, CNN and that kind of thing or did they have to go find all that
themselves?
No, that's the hard.
We actually have, we have the ability to search many TV channels.
And the way we do that is by making sure that we don't impringe upon their copyright.
So you can never watch these TV channels on our website.
You can never even see much of what's going on on there.
Other than the ability to get a notification when your detector figures out what's going
on.
And then you only get a notification and a really small blurry screenshot so that you
can verify that that actually happened and that stays within fair use.
So we are absolutely dedicated to making sure we don't step on anyone's toes here and
that's something that we figure it out.
So yes, you can essentially say, well, I want to watch these few channels for my product
showing up.
And answer questions like, well, when did this car show up next to this other car?
Like when I don't want to pick out any particular bands here, but you let your imagination
while like, when did car model number one show up next to car model number two or when
did car model number one show up?
And this would then look at all movies that have been playing, you know, movies of cars
in them all the time.
They have brands in them all the time, right?
You can answer questions like when did a particular kind of laptop show up when to kind
of brand show up and TV channels across the US across the world?
And you also have an offer like a database of movies or someone, no, not movies, not
movies.
So we have streams of media and the streams are, the streams are TV channels.
And you can have the ability to hook up your own streams.
So we integrate with many cameras.
You can search YouTube videos, you can search static videos that you own.
We have not curated a large collection of movies.
Okay.
We only have so much resource, you know, and we just, we want to make it easy for people
to search their own libraries.
So we've built in tools to allow that.
But we can't, we can't index all the videos in the world right now.
That's not what we're focused on because that's a bit of a different role for us, you
know?
And if someone has a need to monitor a stream, usually they have the stream themselves,
unless it's a very popular public stream like TV.
If it's movies, that's a different story.
We just haven't gone there yet.
Well, in more general, it sounds like the core problem is one of monitoring and less
of search, which requires kind of the broader database.
So in doing all this, one of the key challenges that you faced is how do you scale all of it?
And that was a big part of your talk today.
Walk us through some of the things that you talked about.
Sure thing.
So these models, these machine learning models that have to decide whether something is
happening in a scene or not are computationally very intensive.
To monitor a stream of video, you have to essentially dedicate around an eighth of a typical
GPU card these days, 24-7.
So you can monitor eight streams with one GPU card.
Okay.
Yeah.
That's a lot of compute.
I mean, once you would expect that you could do a lot more, if that I'd been optimized,
but that's where you can do it.
And this is inference, which is typically much lighter on the GPU than...
Oh, no, actually, we do inference on the GPU as well.
Inference is, it's not typically lighter on the GPU for video, because it's just relative
to training, I guess, is what I was.
I mean, they're almost equally as difficult when video is involved.
Because what's happening is we have a trained model, right?
Training sure.
Training takes...
We do both training and inference on GPUs.
So both of these are computationally intensive.
The reality, though, is actually inference is more computationally intensive for us, because
the training happens within a matter of a few minutes, because we have many, many pre-trained
models, and we can build up a lot of work that has happened in the background, pre-trained
models, in particular, the inference, however, you're just constantly running a CNN on
a video stream, 24-7, and that's infinitely long, whereas training, at least, is finite,
right?
Training is going to take a week, you're done, and then you have a pre-trained model,
you can use it, so inference, the way matroid deals with it, is infinitely long.
So you always, you just have to dedicate an 8th of a GPU to inference for a stream.
So that is something we have to deal with, as a company, we have to deal with that.
So first of all, that is per stream, per detector, or you can't...
Yeah, so it's per stream, per detector, per stream, per detector, yeah, so that makes it
even worse, right?
It's even more computationally intensive.
It's not like you're just, you know, you can have multiple networks kind of peering into
a stream or sharing...
Sometimes, but not often enough.
Sometimes that happens, and we do optimize for that, but it doesn't happen often enough,
because the reason you're using matroid is because you want to customize a detector.
You've made a detector for your own data, usually it's a detector that only you have,
and it's a stream that only you have access to, and so it's a detector stream.
And so we have a whole cluster dedicated for this, and the way it works is, so these models
are quite large themselves too, the models are on order of 200 megabytes, and there are
millions of them.
It's a huge amount of data.
And just storing the models, just storing the detectors, already needs a distributed
file system.
That's three.
But then some of these models are less often used than others.
And so we have this four layer cache that goes from the distributed file system to the
local hard drive of a machine that's dedicated, then has a GPU, to regular RAM that a CPU can
access, and then smaller GPU RAM, GPU memory.
The hot models, the very hot models, sit in GPU memory for a long amount of time, and
we deal with the whole cache infrastructure there.
And that infrastructure, did you have to roll your own, or is there something that is pre-existent
to manage that for you?
There are tools for general distributed computing, for example, we use Kubernetes, and Kubernetes
is at the level of resource management, resource allocation.
You say to Kubernetes, hey, look, I need this many cores and this much memory and a GPU.
And you set up Kubernetes to be able to get those resources from some cloud provider.
And ideally, it's all from the same cloud provider, but we have the ability to go through
multiple cloud providers.
And then you get that resource, and you can run your workload on it.
But the fact that Kubernetes doesn't know that we have millions of models, and that we
have this whole caching strategy, it just gives you resources when you ask them for it.
It handles some of the fault tolerance, and some of the logging, which is nice.
But like any rollout of open source software, there's a tremendous amount of work that
goes into actually productionizing it and making it good for a particular application.
And there's the interface with TensorFlow that has to work out, and the interface to our
web environment, which has to work out.
So our web app is very complicated, it's the video player, it's constantly talking to
the cluster and to other places.
These are all, it's like a big symphony orchestra that comes together.
Yeah, the specific question that I had was around the, I guess the analogy for me is
in the storage arena, there's, you know, the whole idea of, you know, hot storage, cold
storage, near line, all that kind of stuff as well established in there is tons of products
and infrastructure and stuff that's off the shelf that you can put in place to do that.
It sounds like what you're doing, shuffling data between GPU, CPU, local disk, or GPU memory,
you know, RAM, local disk, and distributed storage, not a lot pre-existent to facilitate
that, so you're kind of rolling your own there.
Yeah, absolutely, we built that from the ground up.
Using Kubernetes, I don't want to say we built the cluster management part, but the
rest of it is all, is all around, in fact, my co-founder has a PhD in distributed systems.
Okay.
And, you know, we were, part of my research was the Apache Spark library, from the machine
learning library, so if one of my papers is the ML lib library, which probably a lot
of your listeners know about, and there it's, it's more about the JVM and Apache Spark.
Well, let's definitely, let's come back to that.
Yeah, that is something we've been working on for a very long time, and we understand
those nuances very well.
We did not pick up Apache's practice time because it's not a go-to tool for, for deep learning,
exactly because it GPUs, it's access to GPUs is limited because it sits on the JVM.
So yeah, dealing with the whole stack from customized hardware to having a distributed
cluster, all of that isn't well managed by one tool right now, and so we had to roll
our own.
And a lot of it really does come to the fact that you have to be able to use customized
hardware, like being able to manage a cluster where some of the machines have a GPU and
some of them don't have a GPU, that also matters, right?
So we will use CPUs when we have no GPU machines available, but it whole thing will be slower.
Using that again is yet another corner case that we deal with in our setup.
And are you running all of this on AWS, or do you run some of it on a local cluster, or
right now everything is on AWS, but we've built everything with the mindset that we should
be able to be cloud hybrid.
So we should at the very least be able to access some custom processing power from other
cloud providers.
For example, we're looking forward to being able to use TPUs, intensive processing units
from Google when they're available.
There are multiple hardware providers who are competing on deep learning hardware and
a multitude of startups in the space, is at least five startups that I've counted in
this space.
And all of the chip manufacturers want to get into it, Intel, Qualcomm, Google is not
a traditional chip manufacturer that they will produce chips to rent on GCP.
And of course, Nvidia leads the pack here, right?
All of these folks are essentially reinventing computing from their ground up to use linear
algebra on the specialized co-processors, if you want to call them that, or intensive
processing units.
I think the name is going to become tensor processing unit, instead of central processing
unit, eventually we're just going to be using TPUs much more than CPUs.
That chipset is evolving rapidly and it's changing all of computing with it.
And that's another exciting side effect of deep learning.
For a long time, we didn't know how computing should evolve to bypass the fact that we can't
make clock speeds any faster.
And now we've realized that linear algebra operations are the way to go, because linear
algebra operations can be used in deep learning, they can be used in all of machine learning,
they can be used in so many other applications, wherever linear algebra is useful, these
operations can be made to be useful.
That's yet another area that we can spend hours on, just like, what is the right hardware
software interface between, just what is the right hardware software interface past
the traditional X86 programming model?
It doesn't make sense to have simple operations anymore.
The hardware should make linear algebra a first-class operation, a first-class citizen,
and that's been happening.
It's interesting that you are projecting that TPU becomes the general term for this.
I was talking to Navin.
That's part of the reason I actually named matroid matroid is because I expect the term
tensor to become more and more popular in computing vocabulary.
TensorFlow definitely was a step in that direction.
Even Nvidia now has the word tensor core in their products.
So if the new Nvidia Volta has many tensor cores, which do small 4x4 matrix multiplies
and accumulates.
That word is creeping around, even in Nvidia, and Nvidia has all the reason in the world
to market the term GPU, right?
But even they are using the word tensor, tensor is all over the world in deep learning.
I think that's the correct, more intuitive term for these processors.
And then so this is why we named matroid as a generalization of tensor.
It's also a domain name that sounds good and was available, so it's hard to find those
days.
And having your company named after a concept deep, deep mathematical concept is actually
very heartwarming.
And it excites a lot of my students to come help us.
Awesome.
So we were talking about scaling.
Yes.
We talked about infrastructure level concerns.
Actually, random question, I keep hearing from a lot of folks talking about Kubernetes
that it's actually kind of been a key to get up and running and working well on Amazon.
It's been reasonable for us.
It's been reasonable.
It's been okay.
So infrastructure stuff we've talked about.
What about the kind of the modeling, training, inference, architecture?
Has that evolved or it has to what degrees has that evolved specifically to enable distributed
compute and scale?
So that's a very loaded question.
So how have models in computer vision evolved is a tremendous, long question to answer
in itself?
Well, more specifically, what you guys are doing, to what degree, clearly you're thinking
about scale when you're building your systems above the layer of the infrastructure and all
of the movement of bits and compute.
For folks that are trying to build, that have, that are doing deep learning and need to
make a scale or computer vision even more specifically, like what are the things that
they need to be thinking about in the computer vision domain to enable scale?
So even with computer vision, scaling can come in three ways, right?
Scaling can come in having many models.
It can come with having very large models and it can come with having large amounts of
data, right?
So I think we already talked pretty extensively about having many models, right?
That was the caching system that I mentioned.
So that's, let's leave that aside for a second.
The other way that scaling can come into effect is having large amounts of data in training.
So assuming that the model is still small, you have a large amount of training data.
We do have that.
So when, when we train, sometimes we train a pre-trained model for weeks, by that I mean,
we train it, it starts out untrained and then it becomes one of our pre-trained models.
Now that's, that's sort of well understood in, in literature and it's one of the reasons
GPUs became really popular because AlexNet was one of these first neural network architectures
that was trained with a ton of data from the ImageNet and that, that's sort of well understood
and let's not talk about that too much right now.
The other form of scaling when it comes to models is the model itself.
So you should be able to add more and bells and whistles to the model and scale it up
to learn more things as, as you like.
And that's actually something that we focused on.
It's because as new models come out in open source, we would like to be able to suck them
in and make them part of our detector.
So if there's a new TensorFlow model out there that can detect logos really well or if
there's a new TensorFlow model out there that can detect the make and model of a car really
well, we better be able to integrate that into our system within a matter of an hours.
And that's where we are.
We have built our setup so that if there's a pre-trained model, a subnet, we call them
out there that is available to learn rapidly a large body of things, we can then hook
it up into matroid so that it becomes a new set of features to use when creating a detector.
So our detectors are essentially a cocktail of pre-trained subnets that some of which
are proprietary to us.
I have a team of three PhDs working on them non-stop building these cocktail of pre-trained
models, taking them from whenever open source provides them and using whatever we can do
to whatever our customers ask for and we don't have models for we build them ourselves.
And all of these models are combined together at the end to build that one last detector
and then detector is sort of statically available.
So yeah, the short answer is the ability to morph a model, to give it more capabilities
by adding subnets to it is something that is very valuable if done right.
And that's something that we've focused on.
And I haven't talked about too much because I guess that ties into a little bit of our
proprietary architectures and so we haven't really released much of that.
Okay.
But it does sound like if your goal is to be able to use off-the-shelf pre-trained models
then that's not our goal.
That's not my goal, right?
It's to have that be a small part of our system, to be able to essentially have a super
set of the capabilities of pre-trained models.
Okay, but be able to quickly take advantage of innovations outside of your company.
Exactly.
That's the key there.
We don't think we can take on the open source community.
It would be foolish, right?
We think that the open source community is, first of all, we contribute back.
We commit to Kubernetes and we submit bugs and we're actually running a book on TensorFlow
and so on.
There is no point in not embracing open source wholeheartedly.
So then the question is how do you build a product that can both give back and receive
from open source?
And we think this is the way is to be able to integrate what the open source community
finds and discovers as quickly as possible.
But in doing that, those are pre-trained pre-existing models.
You're not changing, changing the model architecture can't be kind of how you scale, right?
It's your, or it can't be, or to what extent can that be a part of, or is that a part
of the way you scale?
It is a core part of how we scale to detect more things.
So right now we have a Coca-Cola logo detector, say.
If I wanted to create a detector for Coca-Cola on a t-shirt detector, not Coca-Cola in general,
like not on a can, not a side of a truck, but only on t-shirts, I'd probably have to create
a custom neural network architecture for that.
Let's say I create that and I better have an easy way to add that into the matroid model
detection scheme and that's the key that we have already.
So that little subnet can go into matroid without much effort.
And if that subnet happens to be available on an open source, we would pick it out of
there and put it in.
And that's, I mean, if you want to call that scaling, or if you don't want to call that
scaling, it seems like we're having a vocabulary thing here, is that we're increasing a number
of things that can be detected by adding subnets to what is being trained.
I don't know if you want to call that scaling.
Now, I think, you know, that I think the background for this conversation is maybe an interview
I did with Shubos and Gupta at Baidu Labs.
And we were talking about the Baidu net research they did for audio and machine translation
and things like that.
The conversation was around, like, some of the same types of issues, like systems challenges
and he went through, you know, a lot of low-level things like we've talked about, but he also
talked a little bit about how, you know, the model architecture, some of the decisions
they made in architecting their neural nets were made in light of the computational limitations
that they had.
And I was just curious whether you experienced similar things and whether you adapted
model architecture in light of computational network, storage constraints, memory constraints,
things like that.
So for some of our customers who have severe memory restraints and custom hardware that
they would like to run a matroid detector on, we have compresses, yes, exactly.
Essentially, you know, one customer who wants to run detectors on their own camera.
For them, we've had to compress and remove a large number of the subnets that we use
when training a matroid detector.
But that is something that we don't do lightly.
There was a lot of engineering effort there and the client wanted this so much that they
were willing to pay us for that engineering effort.
Part of the reason that we run in the cloud is so that we can afford to have slightly larger
models.
It's a privilege there in that environment to be able to run larger models.
We have, in some cases, had to compress the models and remove the parts that weren't
so relevant for competitions and we are, in some cases, using TensorFlow Lite for some
of these.
But we can be a little bit more lazy when it comes to model compression and more focused
on model power and detecting many things as opposed to one or two things.
Once again, even in model compression, the open source community has been better than
us.
And so the compressed models that we're using for this particular camera did come from
open source.
This one, I don't want to say that we've had an innovation in putting neural networks
on cameras because we haven't.
That innovation came from open source and we're essentially commercializing it.
The things I worry about is how do we make sure that those innovations that are, first
of all, how do we contribute back to open source and make sure that it's always alive
and well?
But then also, how can we make sure to be able to bring innovations there as quickly
as possible?
And the fact that we could bring that innovation in very quickly, this model that was very
tight, very small, computationally efficient, but powerful enough for our user.
That was very promising.
It was the fact that we managed to do that.
So quickly, as a small company, one speaks to the power group in source and two speaks
to our planning.
All right.
All right.
So maybe we can take that step back to talk a little bit about Spark.
Let's do that.
Spark extensively on Spark MLlib in particular.
Yes.
And it didn't choose it as the foundation for your architecture.
Maybe talk a little bit about Spark and MLlib and some of the trade-offs that you looked
at when you were choosing to build up your system.
So we already talked about how the hardware landscape is changing.
And it's definitely completely changed as far as machine learning goes.
Its custom chips are the way to go when it comes to machine learning.
And as you know, Spark, because of legacy reasons, runs on the Java virtual machine, the
JVM.
It runs there because of Hadoop running there initially when Hadoop was created out of Yahoo,
they decided to run it on the JVM because at the time, Java was hot, there were a lot
of developers.
And so it was done.
And the thing about Java is that it makes us promise that you don't have to know what
hardware you're running on, which is an assumption that just totally goes out the window when
a hardware software interface is so volatile right now.
The hardware software interface is no longer an instruction set that is familiar to everybody.
It is a vast number of different instruction sets for different chips that have not been
standardized the way that the CPU has been.
And as a result, the JVM cannot directly and easily make use of these fancy processors.
Of course, there are ways to make native calls in Java.
But those native calls in the libraries that allow you to do those native calls.
Is that still JNI?
Yes.
Those are still usually a year or two behind the hardware coming out.
And so you're in this bad situation where some new hardware is out.
And the software that will let you use that hardware and really only still in the limited
capacity, not the full instruction set, comes out in two years after the hardware.
So you're already two years behind.
And as a researcher, then, that's unacceptable.
Well, it's a commercial user as well.
That's right.
So the hardware changes, it is absolutely unacceptable.
It's the difference between being competitive or not, especially for a company like ours,
right?
Because, like I said, we have to dedicate an eighth of a GPU for infinity to a stream.
Well, and if some new thing comes along that makes that a sixteenth, right?
We better use it.
Exactly.
Exactly.
And if our competitors do, then chances are we wouldn't be so happy and maybe even run
out of business.
So sadly, Spark lives on the JVM.
And so the question is, how do you get all those benefits in the JVM?
You can't easily.
The way the Spark community is evolving is to be able to plug in the neural network frameworks
that are actually not written in the JVM.
So what is the case now, actually, we saw at the early talk right after mine, actually,
was about how to run TensorFlow on Spark.
Is that the Yahoo, folks?
I know this.
No.
Yeah, there are many TensorFlow on Spark packages.
I think I've counted four of them.
It's a good idea, clearly, because so many people are working on it.
And this particular talk that I mentioned was a Databricks talk, but actually, yes,
Andy Fang and some folks who have also worked on putting TensorFlow on Spark in a different
way.
Yeah.
And the various approaches to it, but almost all of them involve Spark handling the data
munching and gringing, meaning ETL and just the distribution of data across many machines.
And then when it comes time to learning, they spin up a TensorFlow process and just let
the TensorFlow process go to town on the data for a long period of time, maybe 10, 20 minutes.
It does its thing.
And then eventually it gets picked out a new model, then Spark does some broadcasting
and some communication between machines and then again, hands back to TensorFlow.
So the majority of the time is spent inside a framework that is written not in the JVM,
written in more closer to hardware.
So then the question is, why do that?
Why not just stick to something that's in, that's always in close to the hardware?
Close to the hardware.
And the answer is there is an integrated answer, right?
So actually, there's this, the lab that Spark came out of at Berkeley is now moving away
from the JVM2.
So it used to be called the amp lab.
The amp lab is, for the sake of creative destruction, they wound down the amp lab and replaced
it now with the Ryze lab.
The Ryze lab on Spark now, well, no, that's not true, Apache on Spark.
But the Ryze lab has a lot of people who work on Spark, but also new projects.
And the new projects are all using C++.
Because we're back, computing has been reset essentially with the resetting of the hardware
software interface.
Computing has been reset.
And so all these chip manufacturers are also worried now because they're a little bit
closer to being competed out, because now that a large body of their moat has essentially
been removed, and that is very powerful for people who are looking to innovate in that
space.
And machine learning engineers are one of them people who work on machine learning libraries
are one of them.
And TensorFlow does this quite well in that there is a, because TensorFlow has supposed to
run on many different chipsets, it has a compiler dedicated to be able to compile TensorFlow
graphs into many different chipsets, like the Qualcomm chipset and Intel chipset and of
course, CUDA and CUDNN.
I suspect what we'll see is after a long battle, maybe over the course of five to ten
years, we'll eventually settle on a new hardware software interface that will look a lot like
linear algebra.
Okay.
There will be a chipset that at its core supports many matrix multiplies, not just many
matrix multiplies, many small matrix multiplies, but a few big matrix multiplies, so multiplying
two very big matrices together, multiplying many small matrices together, which is essentially
what the Nvidia Volta is, and then you know, matrix vector operations and vector operations
as well.
Those are actually reasonably well supported with Bloss, which has been around since the
seventies.
But the many, many small matrix multiplies is not well supported in the CPU and needs
to be done in custom chips, and that's part of what will be this new language, this
new instruction set for the CPU, for the, for the new processing unit, and I'm curious
to see what that looks like.
Once that settles down, then there's time for new JVMs to pop up.
Interesting.
So the audience didn't see me chuckling as you were describing how the Spark ecosystem,
how Spark is running TensorFlow, but it struck me as funny because Spark is like the new
yarn for TensorFlow workloads, and for some of these, in some ways, yes, workloads, which
is somewhat ironic.
At the same time, one would expect that Google expands kind of the landscape around TensorFlow
to more natively support distributed compute.
What's the, I'm not very familiar with the situation there.
I was at trying to remember the name of this, the Google event that I was at where we had
an extensive conversation around like integrating Kubernetes more natively with TensorFlow and
making more easy to do distributed TensorFlow compute.
What's the general landscape there?
So Kubernetes and TensorFlow do play very, very nicely with each other, and TensorFlow
does have a distributed mode where you can have TensorFlow running on many machines or
many cores, many GPUs on a single machine.
Both of those are supported reasonably well with TensorFlow, and it's that the reason
people don't use that is because it's often the case that when they have a lot of data,
they've already set up a Hadoop cluster or a yarn cluster, and so they don't want to
just undo all that engineering.
They just want to be able to use all that data.
And sometimes fault tolerance matters a lot more for them.
TensorFlow by default is not fault tolerant in a serious way.
So if your machines go down, you have to restart the computation, also restart the machines
and everything yourself.
Whereas with Spark, you get the fault tolerance.
But it's actually not clear whether fault tolerance is all that useful for machine learning.
Because usually if you have a training job, it runs for two or three days maybe, and
then it's done, and you may be using 10 machines or 100 machines.
If it's 100 machines for two or three days, chances are one of them will go down.
So there you will care about having fault tolerance.
But then fault tolerance in machine learning can be as easy as just restarting the machine
that died with a version of the models that the other machines had or a version of the
model that is even random.
And so it's actually not a big deal for machine learning for there to be failures.
And so fault tolerance just doesn't seem like an important deal there.
But it is an important deal for some people like us.
If we guarantee to our customers that we're monitoring a stream, we better monitor that
stream all the time.
And so fault tolerance does matter too.
So we actually have to set up our own fault tolerance mechanisms for the sake of making
sure that we always have some detector on the stream.
So the whole landscape is changing dramatically.
And everyone is very interesting and big fight.
Maybe I shouldn't call it a fight because there's not necessarily one winner.
What will probably happen is data workloads, workloads that are of the form, joins, group
buys, and selects and so on.
I think those will forever stay in Spark because Spark doesn't really well.
It doesn't matter that they're in the JVM at that point because the difference between
the JVM and the difference there is much less the CPUs, well, does joins and group buys
well enough.
It's these machine learning operations that these newer network operations that I think
are better suited to hardware, those will be run in TensorFlow.
I don't think TensorFlow will ever evolve to a point where it does joins and group buys.
So I don't think that'll ever happen.
And you always need joins and group buys to manage data.
And so Spark will always have that place.
Not always, but some tool that does joins and group buys will always have that place.
And there's no real serious competitor to Spark.
Well, there are, but you know, there's none of them are popular.
By extension, the general CPU will always have a place alongside the TPU, right?
Yes, unless the instruction sets that the TPUs provide expand and slowly over maybe
a decade take over what the CPU does to and then the CPU is obsolete.
So I don't know if that's going to happen.
Interesting.
Yeah, I don't know if that's going to, it's not, I haven't seen it happening yet.
But it might, one of these hardware manufacturers might decide, hey, I got a good lead in
these co-processor units.
Why don't I just put a little bit more semiconductor into my co-processor and make it a full-on
processor as well as a co-processor or the other way around or the other way around.
So yeah, it's going, it's definitely all of that is happening right now and we're watching
it.
And it seems as if the computing gets faster, computing gets more efficient, so I'm just
happy that it's happening.
Awesome.
Awesome.
Well, we've got to get you to a flight.
Yes.
How can folks find you, learn more about what you're up to, explore these topics more?
I am very Googleable, so matroid.com is for matroid and just Google, Reza, Zadeh and my
home page shows up very easy.
Awesome.
All right.
Thank you for having me.
Thanks so much.
It was awesome.
I enjoyed the conversation.
Learned a ton and we'll be in touch.
All right.
Thanks.
All right, everyone, that is our show.
Thanks so much for listening and for your continued support, comments and feedback.
A special thanks goes out to our series sponsor, Intel Nirvana.
If you didn't catch the first show in this series where I talked to Naveen Rao, the head
of Intel's AI product group about how they plan to leverage their leading position and
proven history in Silicon innovation to transform the world of AI, you're going to
want to check that out next.
For more information about Intel Nirvana's AI platform, visit intelnervana.com.
Remember that with this series, we've kicked off our giveaway for tickets to the AI conference.
To enter, just let us know what you think about any of the podcasts in the series or post
your favorite quote from any of them on the show notes page, on Twitter or via any of
our social media channels.
Make sure to mention at TwomoAI, at Intel AI, and at the AI come so that we know you want
to enter the contest.
Full details can be found on the series page and of course, all entrants get one of our
slick Twomo laptop stickers.
Speaking of the series page, you can find links to all of the individual show notes pages
by visiting TwomoAI.com slash O'Reilly AINY.
Thanks so much for listening and catch you next time.
