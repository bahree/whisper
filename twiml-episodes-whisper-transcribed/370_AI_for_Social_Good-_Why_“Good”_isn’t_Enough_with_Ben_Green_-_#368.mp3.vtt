WEBVTT

00:00.000 --> 00:15.520
Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.

00:22.320 --> 00:28.000
Alright everyone, I am here with Ben Green. Ben is a PhD candidate in Applied Math at Harvard

00:28.000 --> 00:32.640
and Affiliate at the Berkman Client Center for Internet and Society, also at Harvard,

00:32.640 --> 00:38.800
and a research fellow at the AINow Institute at NYU. Ben is joining me for our continued

00:38.800 --> 00:43.760
conversations coming out of the 33rd NURBS conference here in Vancouver. Ben, welcome to the

00:43.760 --> 00:47.440
Tumel AI Podcast. Thanks so much for having me. I'm excited for our conversation.

00:48.240 --> 00:54.640
I am as well. So, as I just said, your degrees are going to be very soon in Applied Math,

00:54.640 --> 01:02.240
but you have applied that application of math to largely an exploration of the intersection of

01:03.440 --> 01:09.120
technology, AI, and social good. Tell us a little bit more about your background.

01:09.120 --> 01:15.520
Yeah, so my background is primarily computer science and data science, but I always,

01:15.520 --> 01:20.240
even going back to my undergraduate years, had a really strong interest in urban policy

01:20.240 --> 01:26.560
and urban government and urban planning. And so, as I started out the PhD, a lot of my emphasis

01:26.560 --> 01:31.600
was on how can we use, you know, the tools of artificial intelligence and data science to

01:31.600 --> 01:38.240
improve society and participated in the UChicago data science for social good program and did some

01:38.240 --> 01:43.760
other work really thinking about how can I as a data scientist contribute to society? I spent

01:43.760 --> 01:49.920
a year working for the city of Boston as a data scientist in the middle of my PhD. But in the

01:49.920 --> 01:54.960
course of doing that work from originally from a more technical perspective, increasingly also

01:54.960 --> 02:03.040
came to see the broader governance political social questions that were at the heart of these

02:03.040 --> 02:10.720
technological endeavors and often were overlooked or ignored and also played a significant role in

02:11.280 --> 02:16.400
shaping the impacts of these systems in some of the projects that I worked on, whether it was

02:16.400 --> 02:21.440
building machine learning algorithms in the city of Boston or for the city of Memphis to help them

02:22.240 --> 02:29.120
prioritize various types of inspections and investments. Often what I found was that the key

02:29.680 --> 02:34.400
factor that shaped the impacts was not the technology itself, but the broader policy,

02:34.400 --> 02:39.760
government, political environment in which that technology was being embedded. And so that shaped

02:39.760 --> 02:47.040
a lot of my thinking on how to integrate technology into these broader social contexts and how to

02:47.040 --> 02:54.400
think about the ways in which what are very well-intentioned efforts to use technology for good can

02:54.400 --> 03:02.160
overlook some key factors and end up failing to achieve those those social goals. And your affiliation

03:02.160 --> 03:06.800
with the applied math department is that I imagine there are several places that you could have

03:06.800 --> 03:11.840
plugged in your research interests at a place like Harvard. Is that the selection of a particular

03:11.840 --> 03:19.600
advisor or is applied math? How does applied math fit into that broader research agenda?

03:19.600 --> 03:25.680
Yeah, so definitely the really the computer science perspective which is sort of where my

03:25.680 --> 03:32.640
more more day-to-day home is at Harvard is both very much a lot of my work is sort of with is all

03:32.640 --> 03:37.760
sort of within the realm of computer science both thinking about those tools in the past more

03:37.760 --> 03:45.280
work on building those building AI systems for various social applications, running different

03:45.280 --> 03:50.160
types of more recently. I've been running a variety of human computer interaction audits to

03:50.160 --> 03:57.200
understand how people interact with algorithms in practice. And but also my work is very much

03:57.200 --> 04:02.560
than about stepping outside of the typical modes of thinking within the field bringing in other

04:02.560 --> 04:08.400
perspectives from science and technology studies and philosophy and government and thinking about

04:08.400 --> 04:14.560
what and law and what those perspectives can do to inform our understanding of artificial

04:14.560 --> 04:21.360
intelligence and its impacts and how to develop it. So my work is definitely very multi-disciplinary

04:21.360 --> 04:25.680
and I've worked with you know even within Harvard at many different departments and with many

04:25.680 --> 04:31.360
different people but the core focus has always been on the application of data and algorithms

04:31.360 --> 04:37.520
in society. So here at NERPS you're presenting a paper called Good Isn't Good Enough

04:37.520 --> 04:43.760
at the AI for Social Good Workshop. Tell us a little bit about the paper and what your objectives

04:43.760 --> 04:50.400
are there. Yeah so this paper is so it's just a it's a short workshop paper so definitely not

04:50.400 --> 04:56.400
a fully in-depth discussion of these topics but it really emerged out of my own experiences

04:56.400 --> 05:01.680
and some of the other broader examples I was seeing of these efforts to do social good

05:02.320 --> 05:07.680
that were well intentioned but often sort of not thinking of the full of the full picture of

05:07.680 --> 05:14.800
what it actually means to do good. And recognizing in particular that efforts to do any sort of

05:14.800 --> 05:22.320
technology for social good are about somehow shaping society somehow changing society for the better

05:22.320 --> 05:28.000
and that's of course an incredibly complex topic and the two things that I really point out

05:28.000 --> 05:34.640
that I have seen missing in the majority of efforts to use AI for social good is first what I

05:34.640 --> 05:40.480
would think of as a normative theory a sort of grounded definition of what good actually means

05:40.480 --> 05:47.520
typically most groups will talk about AI for social good and the social good part is sort of

05:47.520 --> 05:52.880
taken for granted what that might mean but of course as you can sort of if you step out of the AI

05:52.880 --> 05:57.840
space and just think about our broader social political world there are many different definitions

05:57.840 --> 06:03.280
of what's good and many nuances within that type of debate so there was often a lack of sort of

06:03.280 --> 06:09.920
a normative discussion about what are we trying to accomplish and then the second part was a lack of

06:09.920 --> 06:16.080
what I would call a theory of change a theory or sort of a grounded justification for how the

06:16.080 --> 06:22.880
particular technological approach being taken is an effective means to getting to the social good

06:22.880 --> 06:31.040
and whatever that end may be and so a lot of the time even in cases where perhaps the

06:31.680 --> 06:37.360
an important problem is recognize the particular mode in which technologists go about trying to

06:37.360 --> 06:44.000
solve that problem may not be the most effective way of achieving that end if you take the social good

06:44.000 --> 06:50.000
or the social impact as the as the ultimate goal here and think about the technology as a means

06:50.000 --> 06:57.360
to achieving that goal and so both of those things are I think pretty significant challenges

06:57.360 --> 07:02.480
certainly one not ones that cannot be overcome but the types of things that really need to be

07:02.480 --> 07:08.560
incorporated into these into these discussions I like to think about it and in some sense this is

07:08.560 --> 07:12.880
really what the goal of the paper is to do is I like to think about it in terms of rigor

07:12.880 --> 07:19.040
right that when we talk about AI for social good what we're actually doing is really expanding

07:19.040 --> 07:23.760
what we're trying to accomplish with an AI system right we're not simply saying we want to build

07:23.760 --> 07:30.160
a tool that can efficiently predict this or you know efficiently analyze this data but we're

07:30.160 --> 07:37.360
trying to build a tool so that it can achieve or advance this social outcome and what I'm trying

07:37.360 --> 07:43.760
to bring as a sense of here are things that we're overlooking failing in many cases to think about

07:44.480 --> 07:49.520
and trying to frame that as a lack of rigor in these efforts that we're actually not thinking

07:49.520 --> 07:56.480
about factors that are incredibly important in shaping those outcomes and that to the same extent

07:56.480 --> 08:04.400
that we would never accept a system that hadn't done an analysis on some sort of hold out test

08:04.400 --> 08:11.680
dataset we also shouldn't be accepting systems for integration into societal contacts that also

08:11.680 --> 08:16.640
hasn't done some sort of analysis of well what will the impacts of this system be in practice how

08:16.640 --> 08:23.040
is it actually going to affect the system that we're trying to impact here and bringing in more of

08:23.040 --> 08:30.160
those types of sociotechnical analyses into what it means to build and evaluate these types of

08:30.160 --> 08:36.480
systems. The way you've framed those two components establishing kind of a normative definition

08:36.480 --> 08:46.880
for good and theoretical framework for understanding the implications of a particular technology

08:46.880 --> 08:56.960
sound like ideals. Are you holding these up as hurdles that should be overcome in order to

08:58.400 --> 09:04.560
before folks embark on their AI for social good projects or more like conversations that we

09:04.560 --> 09:11.600
need to be having? Yeah, I think it's a bit of both. I wouldn't view those as being in

09:11.600 --> 09:17.440
contrast with one and other. It's definitely I would say centrally about conversations. There's

09:17.440 --> 09:24.000
certainly not a single answer to any of these things both within the field and without the field

09:25.280 --> 09:30.240
philosophers and activists have been debating questions about what is good and how do you get there

09:30.240 --> 09:37.600
for forever essentially? Are there examples from other fields where good or even

09:37.600 --> 09:43.600
something you know as broad as good has been successfully defined and used as a foundation for

09:43.600 --> 09:50.080
kind of further work? What would this be? Ground that we're breaking in in AI? So it's definitely not

09:50.080 --> 09:55.920
ground that's being broken in AI. I would say again the point is not that there are other fields.

09:55.920 --> 10:01.680
It's less about there are other fields that have come up with the definition of good, but there are

10:01.680 --> 10:08.640
other fields that have figured out how to incorporate these sorts of conversations into what it means

10:08.640 --> 10:14.320
to do this practice. So one of the places that I've been looking and this is a paper that I just

10:14.320 --> 10:20.480
finish and will be published at the 2020 Fat Star Conference Fairness Accountability and Transparency,

10:21.120 --> 10:27.120
my co-author and I we look to the law and we look to debates in the law in the 20th century

10:27.120 --> 10:33.040
about how they moved from a system that was very formalistic that said the law has these rules.

10:33.040 --> 10:38.240
We can just deduce from these rules how do we get to the right outcome? And there were a number

10:38.240 --> 10:43.920
of challenges to the law a shift from what was known as legal formalism to what was known as

10:43.920 --> 10:51.200
legal realism that said legal practitioners whether lawyers or judges or scholars need to figure out

10:51.200 --> 10:56.400
how to reason about the law in terms of the impacts that it's actually having in real people's

10:56.400 --> 11:02.080
lives. It's not just about did you apply the principle of liberty correctly, but how will

11:02.800 --> 11:08.000
this application of the law affect people's liberty in their real lives? That's what the

11:08.000 --> 11:14.640
realism is doing here. And so this paper is about shifting from what we might call algorithmic

11:14.640 --> 11:20.720
formalism to a mode of algorithmic realism, recognizing that the law is a place while typically

11:20.720 --> 11:26.560
viewed as being in conflict with technology. And there certainly are conflicts in many ways. The

11:26.560 --> 11:33.760
law is another mechanism for having a structure to a mechanism to structure our decision making

11:33.760 --> 11:39.600
and to structure the rules by which we allocate decisions and resources. And in many ways that's

11:39.600 --> 11:46.480
what AI want applied to city governments or healthcare systems or any number of different

11:46.480 --> 11:52.960
contexts are doing right. It's providing a way to distribute to make decisions to manage

11:52.960 --> 11:57.920
discretion. And so I think that by taking for example some of those lessons of the law which

11:57.920 --> 12:02.640
have always been thinking about how do we understand or not always but have for many years been

12:02.640 --> 12:12.080
thinking about how do we emphasize the the quality of illegal analysis not in terms of it's sort of

12:12.080 --> 12:17.040
whether it not it fits the ideal theory or the principles within ideal theory but how does

12:17.040 --> 12:23.440
it affect society. And I think in many ways that's where we should be moving to in these contexts

12:23.440 --> 12:28.880
of building AI for society right. The goal is not to just build these systems for these systems

12:28.880 --> 12:34.800
own sake but because we're trying to advance some sort of social benefit or social outcome.

12:34.800 --> 12:42.880
And so is the paper is it prescriptive in the sense that it tells a practitioner who buys into

12:42.880 --> 12:48.720
this vision what to do next? Yeah absolutely I mean we're certainly not prescriptive as saying

12:48.720 --> 12:55.280
you know here's all of the answers it's a broad a broad shift but yeah so we we provide and I

12:55.280 --> 13:00.480
think you know and these sort of things that orient we call them orientations that we're putting

13:00.480 --> 13:05.600
forward are very much also aligned with what I recommend at the end of this AI for social good paper

13:05.600 --> 13:12.560
here in the rips but you know one is really thinking about what are those normative commitments

13:12.560 --> 13:18.320
that you want to embody within your application what might you be taking for granted as things that

13:18.320 --> 13:26.720
are good or not good if you're not interrogating those things. How do you bring in a more interdisciplinary

13:26.720 --> 13:32.880
approach recognizing the variety of different perspectives that might come to bear on that

13:32.880 --> 13:40.480
system or the ways in which the interactions outside of what we would think of as the algorithm

13:40.480 --> 13:45.840
itself can affect the impacts of that system so expanding beyond what are the technical

13:45.840 --> 13:51.200
specifications in terms of accuracy and efficiency but what happens when you put this into the middle

13:51.200 --> 13:55.840
of a healthcare system and you need doctors to be working with it in practice. How does you know

13:55.840 --> 14:02.560
what what happens there and that's just as central and then really engaging with the context of

14:02.560 --> 14:09.680
the problem and ultimately from what what we call an approach of agnosticism right recognizing

14:09.680 --> 14:16.720
that the AI can be built in many different ways and that the AI is just one way of approaching

14:16.720 --> 14:23.200
a social problem that if the goal ultimately is not about building the system the goal is about

14:23.200 --> 14:30.080
advancing some social outcome that requires a different sort of stance of understanding okay well

14:30.960 --> 14:36.240
what type of system should I build what is the best contribution that I as an engineer can make

14:36.240 --> 14:42.960
into this problem and how might that be different from building the sophisticated system that

14:42.960 --> 14:49.040
sounds really cool from an engineering perspective but may not actually be the best data analytic

14:49.040 --> 14:56.480
tool for pushing the ball forward on this problem so you know there's a variety of questions

14:56.480 --> 15:00.880
and prompts and sort of things like that to start start thinking about and then there are

15:00.880 --> 15:05.760
you know getting at what that looks like in practice at every single one of those decision points

15:05.760 --> 15:11.040
is certainly an area for further work and one that other people also are thinking about a lot.

15:11.040 --> 15:18.640
A big part of your work is kind of thinking about and providing a framework for thinking about

15:18.640 --> 15:24.400
kind of unintended consequences and the application of technology to social good

15:25.280 --> 15:30.880
are there you know specific examples of these that are most salient for you.

15:32.320 --> 15:38.480
Yeah so you know I think there are a couple I would say the one that I always come back to because

15:38.480 --> 15:44.960
this is the focus of my dissertation work are algorithms in the criminal justice system things

15:44.960 --> 15:53.440
like predictive policing and risk assessments and these are systems that often are deployed

15:53.440 --> 15:57.920
under the idea that these are effective ways of achieving some form some type of criminal

15:57.920 --> 16:03.600
justice reform right of taking this system that is broken in a number of ways and helping to

16:03.600 --> 16:13.120
improve it and I think that that's an example of where thinking about what the typical sort of

16:13.120 --> 16:18.400
we can build an algorithm for this problem approach where that can go wrong because I think

16:18.400 --> 16:23.760
that ultimately what we're doing with these systems is not just improving a particular

16:24.720 --> 16:29.840
aspect of the system with these algorithms but actually affecting the broader landscape of what

16:29.840 --> 16:36.000
it means to do reform what sorts of shifts that we're taking. So the example that comes to mind

16:36.000 --> 16:44.080
I'm sure for many who follow this base is like the compass system that was written about

16:44.080 --> 16:48.960
extensively in a public overport maybe a couple of years ago now you're in 2016 anything.

16:48.960 --> 16:55.120
Oh, 2016, it's been a while. Yeah but that's a system that is being you know developed and

16:55.120 --> 17:05.280
promoted by a you know commercial entity and it's not at all clear that their goals are you know

17:05.280 --> 17:10.000
reform or social good for that matter you know is that the example that comes to mind for you

17:10.000 --> 17:16.400
or are there others and you know certainly there are folks that are you know whose primary goal is

17:16.400 --> 17:23.600
you know social good you know that is not necessarily one of them. Yeah I mean it definitely

17:23.600 --> 17:28.720
varies the the landscape is sort of complex and they're definitely just companies trying to

17:28.720 --> 17:34.960
make money out of this and then I think also definitely computer scientists and other folks who

17:34.960 --> 17:42.720
are genuinely excited about these tools as as a way of improving the system. You know so two two

17:42.720 --> 17:46.720
different very different types of unintended consequences that play out there that I have looked

17:46.720 --> 17:52.080
at in my research. One being again going back to this human computer interaction component

17:52.080 --> 17:57.360
where a lot of my work and other folks have done similar work is looking at okay well you have

17:57.360 --> 18:02.080
this system that seems like it's able to make these decisions what happens when you actually put

18:02.080 --> 18:09.520
it into the context are people using these predictions in the way that you would expect and what

18:09.520 --> 18:16.720
what I and others have found is that people can in can inject their own different types of biases

18:16.720 --> 18:23.120
in terms of how they respond to these systems or they might ignore the recommendations or only

18:23.120 --> 18:29.920
use the recommendations in an unexpected and sort of particular way that has you know that

18:29.920 --> 18:34.960
benefits one group versus another so that's a particular type of other specific examples of that

18:34.960 --> 18:41.600
that come to mind. Yeah so so in my work I've looked at how this is lay people not judges but how

18:41.600 --> 18:47.040
lay people can be more likely to be influenced by higher recommendations of risk from an algorithm

18:47.040 --> 18:51.440
when that defendant is black compared to when that defendant is white so they might be more

18:51.440 --> 18:56.880
susceptible to using it in certain directions in certain cases so they'll have a higher they'll

18:56.880 --> 19:01.200
be pulled in a higher risk prediction for black defendants in a lower risk prediction for white

19:01.200 --> 19:08.480
defendants and then others like me being the the groups that you studied or the research that

19:08.480 --> 19:15.120
you're referring to demonstrates that that folks are more willing to defer to the authority of

19:15.120 --> 19:19.840
an algorithm that they don't understand in the case of a black defendant than a white defendant

19:19.840 --> 19:26.480
is that the essence of it? Yeah so it's that they they differ in different ways depending on

19:26.480 --> 19:33.040
the defendants right so if essentially if the algorithm is telling you to increase your score

19:33.040 --> 19:37.440
you're more likely to follow that recommendation for black if the defendant is black and if the

19:37.440 --> 19:42.320
algorithm is telling you to decrease your score you're more likely to follow that if the defendant is

19:42.320 --> 19:50.640
white so what you know through the point and sort of the you know how this this is this clear

19:50.640 --> 19:56.800
example of how we might think about the system we analyze we can audit the system on its own terms

19:56.800 --> 20:03.360
a great deal but what we're actually doing is incorporating this AI system into a very complex

20:03.360 --> 20:10.720
social political government context and so how it actually gets used in that context may lead it to

20:10.720 --> 20:16.320
have very different impacts even just in terms of the decisions that are made then we might expect

20:16.320 --> 20:24.160
from doing a analysis of the algorithms recommendations on their own in terms of accuracy and false

20:24.160 --> 20:30.160
positives and those sorts of things so you know this is adding even just more dimensions to something

20:30.160 --> 20:36.000
like what the pro-publica article was pointing out in terms of disparate false positive predictions

20:36.000 --> 20:43.040
and saying e above and beyond those types of analyses we can also study how these algorithms will

20:43.040 --> 20:49.520
affect decision-making when you put them closer or in their in their real context also makes me think

20:49.520 --> 21:00.400
of the use of facial recognition technology by law enforcement groups Amazon has been central to

21:00.400 --> 21:06.160
many aspects of this conversation and when I talk to them they defer to you know documenting

21:06.160 --> 21:14.240
proper use of these systems you're suggesting that improper use of of these systems is in fact

21:14.240 --> 21:20.880
like systematic and so maybe it's not enough to just point to the documentation is that a conclusion

21:20.880 --> 21:25.760
that you're making or I would say yeah I think I think you know that's a useful framework for

21:25.760 --> 21:30.960
thinking about some of these systems and facial recognition is a good example here there are questions

21:30.960 --> 21:40.080
of both is the is the actual use proper right so do we get proper improper use in practice so that's

21:40.080 --> 21:46.880
one way of breakdown is that what the we get improper use in practice the other would be that

21:46.880 --> 21:53.040
proper use would itself be troubling right and I think that many of the systems I would I think

21:53.040 --> 21:58.720
there are a lot of systems facial recognition being one of them that proper use is itself troubling

21:58.720 --> 22:04.320
right so and similarly with with risk assessments I would make the same argument that there are

22:04.320 --> 22:09.360
definitely breakdowns that can happen in practice things like the types of bias human biases I was

22:09.360 --> 22:16.000
talking about a minute ago but then there's also the question of you know what happens if the system

22:16.000 --> 22:22.720
is perfect right so with facial recognition there's been great work showing that these systems

22:22.720 --> 22:30.240
are flawed and biased in a variety of ways but the answer there is not to say well we just need to

22:30.240 --> 22:35.440
have proper use and proper systems that are perfectly accurate right because the way that facial

22:35.440 --> 22:42.480
recognition systems are primarily being used is you know law enforcement and corporate surveillance

22:42.480 --> 22:50.320
right so having a system that just works better and is better able to document people is not necessarily

22:50.320 --> 22:56.800
the outcome that we actually would want and that's a place where the proper use is itself a problem

22:56.800 --> 23:04.320
and because why because their perfection and successful use promotes further surveillance and

23:04.320 --> 23:11.600
that and of itself is kind of taken as negative or because there are specific outcomes that are

23:12.960 --> 23:19.360
that you predict or see based on their their use yeah it's definitely that it's you know it's

23:19.360 --> 23:25.280
about thinking again not it's about taking the the system like facial recognition in the broader

23:25.280 --> 23:31.280
context and how it gets used right so if the primary use of these tools are you know think about

23:31.280 --> 23:38.160
law enforcement being able to have better records of where everyone is going in the city more

23:38.160 --> 23:46.880
ability to track people and that to me and to many others as an incredibly dangerous prospect the

23:46.880 --> 23:55.040
idea that you are fully tracked just by stepping foot into you know downtown Vancouver where we are

23:55.040 --> 24:02.080
right now right and of course that type of surveillance typically has the the most significant harms

24:02.080 --> 24:09.280
for minorities in the lower and lower classes and so the idea that we would just want to make those

24:09.280 --> 24:16.000
systems more accurate and more technically sound is only enhancing the ability for those broader

24:16.000 --> 24:22.480
types of systems to be developed so I think that thinking through I think it's really important

24:22.480 --> 24:29.040
for folks who are trying to think about the social consequences of AI to sort of think about it

24:29.040 --> 24:35.120
in this just in this way of you know where problems occurring because of improper use and where

24:35.120 --> 24:40.640
would proper use itself be bad and I think that's a really helpful heuristic for thinking through

24:40.640 --> 24:46.640
what types of challenges we want to make to these systems so you know in my in my work on risk

24:46.640 --> 24:52.880
assessments there are all sorts of improper use critiques to make or sort of improper engineering

24:52.880 --> 24:58.160
critiques to make about flaws in the training data or flaws in the human interactions but then

24:58.160 --> 25:05.040
there are also critiques we made of even a perfect system what itself be bad right and we need to

25:05.040 --> 25:09.920
we need to be able to think about that relationship between those different types of harms and

25:09.920 --> 25:15.920
different types of breakdowns and I think that doing that can inform a lot of both our understanding

25:15.920 --> 25:21.920
of these systems but also our work as computer scientists to understand these systems and actually

25:21.920 --> 25:29.600
understand what are the types of flaws that we should be actively working to fix and what are the

25:29.600 --> 25:35.600
types of flaws that can prompt us to do a larger process of reimagining what types of systems

25:35.600 --> 25:41.920
we're building reimagining what problems we're choosing to work on in the first place and so

25:41.920 --> 25:46.800
this is you know and this comes back to you know something I talk about that comes up a lot

25:46.800 --> 25:54.560
in my conversations I am often sort of advocating for a position of what I would call a political

25:54.560 --> 25:59.360
orientation among computer scientists about thinking about these types of questions that's

25:59.360 --> 26:05.680
obviously an uncomfortable thing as a scientist and an engineer who has been sort of taught

26:05.680 --> 26:12.880
that you are to work from an objective remove and a position of neutrality and I think one of the

26:12.880 --> 26:19.120
key things in sort of thinking about how to do that is to understand what we mean when we talk

26:19.120 --> 26:27.120
about objectivity and to think about how there are many types of decisions that sort of fly

26:27.120 --> 26:31.840
under the radar of what we would consider objective and that we can play with those right so when I

26:31.840 --> 26:39.840
say you know political the point is not to make up data or to make up results that would advance

26:39.840 --> 26:46.000
an empirical finding that you would like to show but to think about what types of problems are you

26:46.000 --> 26:51.280
choosing to work on in the first place who are you choosing as your domain experts who are the

26:51.280 --> 26:58.160
partners that you're trying to work with and build systems for those are all ultimately incredibly

26:58.880 --> 27:04.480
what I would call political questions right that choosing choosing a problem to work on

27:04.480 --> 27:11.040
is in many cases you know that's projecting a vision of a good society or a vision of where

27:11.040 --> 27:16.560
society has gone wrong and then you know building systems within that doesn't necessarily

27:16.560 --> 27:23.280
require rejecting the typical modes that we would use when building and evaluating AI systems

27:23.280 --> 27:29.280
but that's one way that we can think about stepping back from our typical process and recognizing

27:30.080 --> 27:34.320
the decisions that we're making often without realizing it about what we're going to work on

27:34.320 --> 27:40.880
and who we're going to work with but those can be incredibly consequential and simply choosing a

27:40.880 --> 27:47.040
slightly different problem or a slightly different partner can have really big consequences and

27:47.040 --> 27:51.920
can very much shape what the type of system you're building is or the types of impacts that you're

27:51.920 --> 28:01.440
going to have so we've maybe straight a little bit from kind of establishing a basis for what we

28:01.440 --> 28:10.000
mean by good or social good and a theory of the relationship between technology and social good

28:10.000 --> 28:15.520
I think we've kind of spoke to talk a little bit about the first part of that all of this is

28:15.520 --> 28:22.480
definitely very central to exactly those those questions right yeah right right but the you know

28:22.480 --> 28:29.440
the latter of those two points kind of a theory for you know the relationship between technology

28:29.440 --> 28:35.840
and social impact you know what does that what does that look like yeah yeah you know in some

28:35.840 --> 28:41.600
ways it's like what we've been talking about like what you know you do you know you do x that has

28:41.600 --> 28:46.640
implications a and b and kind of just thinking through that but when you talk about it as kind of

28:46.640 --> 28:52.640
a theory and you you may have said formalism or I'm like kind of implying that like you know how

28:52.640 --> 28:59.440
and you said rigor like how how rigorous can we get yeah yeah yeah um yeah so that you know I

28:59.440 --> 29:05.040
I like to talk about it as in terms of a theory of change right and this is something that definitely

29:05.040 --> 29:12.080
comes out of more I guess political activist circles right that you're thinking about okay how do we

29:13.600 --> 29:22.480
tie our actions now to the outcomes that we want to get to in the medium or long term right how do

29:22.480 --> 29:29.120
we think of any particular intervention and making the impacts that we want to make and that's

29:29.120 --> 29:37.280
you know that's an incredibly hard question but one that many different areas of uh you know people

29:37.280 --> 29:43.200
in government political activists and social scientists and others have been thinking about a lot

29:43.200 --> 29:50.240
and in sort of a broader sense and then there's also a great deal of work in STS and critical

29:50.240 --> 29:56.480
algorithm studies and philosophy of science and other fields that are looking at how our

29:56.480 --> 30:03.840
technologies that are built and put into an applied social context actually affecting the world

30:04.640 --> 30:12.960
and I think that by starting to pull all of that together we can start to have some some better

30:12.960 --> 30:20.560
ways of thinking about you know okay if I'm trying to achieve you know x goal how can I

30:20.560 --> 30:27.120
think about what type of system I can build to achieve that goal or how can I think about the ways

30:27.120 --> 30:33.680
that the system that I built that seems to advance that goal could end up sort of splintering

30:33.680 --> 30:39.680
off in any different number of ways and a lot of you know great work in law for example has been

30:39.680 --> 30:48.640
looking at how laws meant to protect against discrimination or protect civil rights can end up

30:48.640 --> 30:56.160
getting wielded and used in unexpected ways that actually have impacts totally counter to what

30:56.160 --> 31:02.640
the developers uh or the creators of that law were intending so you know it's an incredibly

31:03.840 --> 31:10.240
it's a complex area but I think that starting to you know look to some of those areas uh you know

31:10.240 --> 31:17.280
we can look at for example okay how have particular types of anti-discrimination laws and efforts

31:17.280 --> 31:23.520
been effective and been ineffective and how can we uh I think a lot of the work on fairness for

31:23.520 --> 31:33.120
example today uh mirrors a lot of the typical modes of anti-discrimination work in the past

31:33.120 --> 31:39.840
that have not necessarily been as effective as they were intended to be and I've written some

31:39.840 --> 31:44.640
about that and Anna Lauren Hoffman at the University of Washington has a great paper on that

31:44.640 --> 31:49.920
so that's an example of how we can look to some of these other fields and other areas and historical

31:49.920 --> 31:57.920
contexts uh both for coral areas of how certain types of efforts have uh gone wrong or been

31:57.920 --> 32:06.000
effective and also the lessons learned in terms of what it means to uh advance a particular

32:06.000 --> 32:13.440
social project in a way that is is robust but but then to get to get more granular here uh you know

32:13.440 --> 32:17.840
I think a couple of the things that that computer scientists can do you know I think that thinking

32:17.840 --> 32:26.080
about the socio-technical context is super important here uh one of the ways that these unintended

32:26.080 --> 32:31.280
impacts go wrong as we've been discussing has been you know you put it into context and it doesn't

32:31.280 --> 32:40.000
actually get used in uh the ways you might expect and I think that part of this is if we can if

32:40.000 --> 32:47.680
we think about the work that we're doing from this perspective of uh impact and theories of change

32:47.680 --> 32:53.120
those types of things can shift from unintended impacts to things that we've thought about ahead of

32:53.120 --> 33:02.560
time so taking those you know human interaction audit studies and making that an integral part

33:02.560 --> 33:10.640
of building a system that's going to be used in an applied context or thinking about what is the

33:10.640 --> 33:16.640
what is the policy domain that I'm working in and how have other efforts in the past and today

33:16.640 --> 33:22.480
been successful at trying to challenge some of the issues that I'm challenging or been effective

33:22.480 --> 33:29.920
at moving the ball in a certain direction so there are I think sort of there's this larger

33:29.920 --> 33:35.440
conversation to be had and a lot of that is then you know talking to domain experts and talking

33:35.440 --> 33:40.800
to people outside the field but then also I think there are just ways of having a more socio-technical

33:40.800 --> 33:47.440
orientation that can take what today would be called unintended consequences and bringing them

33:47.440 --> 33:54.240
into the design process earlier in terms of okay how do we prepare to test if that consequence

33:54.240 --> 34:00.640
is going to happen and how do we make that central to what it means to evaluate this system rigorously

34:00.640 --> 34:06.480
to bring that back into the conversation a lot of what I'm hearing here is going back to

34:07.600 --> 34:14.640
as practitioners we should be thinking more about these issues you know I wonder you know for

34:14.640 --> 34:19.760
practitioners you know do we know how to think about these issues and do we need more

34:19.760 --> 34:25.600
or yeah obviously we should be thinking more about these issues but do we need more you know structure

34:25.600 --> 34:31.280
frameworks rigor you know do we all have to go back and get degrees in STS or other things in order

34:31.280 --> 34:40.080
to kind of reason through these issues it seems like at some point you know it's it's very

34:40.080 --> 34:47.200
reductionist but like you know we need a checklist or you know some kind of map or some kind of

34:47.200 --> 34:54.160
you know or at least you know a list of you know required reading or something like that to kind

34:54.160 --> 34:58.560
of narrow the scope yeah do you agree with that or do you think that you know we're just being

34:58.560 --> 35:03.840
lazy and we should just think no no no you know the you know and I think one thing that's really

35:03.840 --> 35:11.760
important to to to say on that last point is like the goal of this is not to say you computer

35:11.760 --> 35:19.760
scientists are evil or malevolent is right but to say that the at its foundations this is not

35:19.760 --> 35:25.760
the type of thing that the field is trying to think about right so you know even going back to

35:25.760 --> 35:32.240
the types of things that would be taught in an algorithms 101 class or like your canonical AI

35:32.240 --> 35:36.800
or other CS textbooks and these sorts of things don't come up so I think it really does come back

35:36.800 --> 35:44.000
to this question of education and training and sort of the broader culture and norms within the

35:44.000 --> 35:51.040
field so these are you know these are certainly things that are hard really I would say long-term

35:51.040 --> 35:56.960
shifts for the field to be to be building but I think that starting to think about how to

35:56.960 --> 36:04.000
incorporate these other other disciplines into CS education that that or not just incorporating

36:04.000 --> 36:08.320
but also expanding the boundaries of what it means to study computer science right that studying

36:08.320 --> 36:14.880
computer science means not just studying you know algorithms and their efficiency and what's

36:14.880 --> 36:21.680
an MP hard problem but also studying STS to understand how algorithms are affecting society

36:21.680 --> 36:28.320
and studying at least some form of you know the policy or social domain where you're maybe

36:28.320 --> 36:32.480
interested in applying right like if you're interested in the criminal justice system taking

36:32.480 --> 36:40.000
classes in those areas or finding ways to have rich aspects of those fields or courses brought

36:40.000 --> 36:47.680
into a computer science training world so you know and there are a lot of efforts to to do that

36:47.680 --> 36:54.560
there is you know efforts to incorporate ethics training into courses there's an effort at Harvard

36:54.560 --> 36:58.720
to do that led by Barbara Gross there are efforts at a lot of different universities now there

36:58.720 --> 37:05.040
these sort of AI ethics or variants on that type of class and so and that's this you know that's

37:05.040 --> 37:10.480
sort of the right start to be moving on is how do we expand the training that you're receiving

37:10.480 --> 37:16.480
as a computer scientist but also in maybe a broader sense how do we change what it means to be a

37:16.480 --> 37:23.520
computer scientist through that training so that these types of questions are not external to your

37:23.520 --> 37:29.520
professional responsibility or to the types of things that might come up in peer review if you're

37:29.520 --> 37:35.680
a researcher but are the among the fundamental questions that are going to be missed so I think

37:36.240 --> 37:41.600
there's you know a lot of work to be done at both the scholarship level thinking about what is this

37:41.600 --> 37:46.800
even look like and then at the education level thinking about how do we take these broader lessons

37:46.800 --> 37:54.000
and incorporate that into my undergraduate and graduate curricula for what it means to study

37:54.000 --> 37:58.080
machine learning in a different way than we would have studied machine learning even five years ago

37:58.080 --> 38:02.480
right and also referring back to an earlier part of the conversation implications for

38:03.360 --> 38:09.920
those people that are activists or trying to affect change within not just education but

38:09.920 --> 38:18.640
but practice a way to kind of constructively look back at you know at social change across

38:18.640 --> 38:24.400
you know other fields within and out and without technology and identify you know what's working

38:24.400 --> 38:31.440
what's not working and use that to rethink the way you know we engage in conversations around

38:31.440 --> 38:37.760
AI for social good mm-hmm yeah and you know again like none of this is to say that we can't

38:37.760 --> 38:43.040
you know that that the types of tools or the types of methods that we have that are you know

38:43.040 --> 38:47.760
what I would call them more sort of algorithmic formalist methods are bad or should be completely

38:47.760 --> 38:53.360
thrown away or that every single person needs to do every single one of these things but I think

38:53.360 --> 39:00.080
particularly as the field becomes more oriented around questions of social good and social impact

39:00.080 --> 39:07.600
and thinking about these applied domains that only becomes more and more central so you know it's

39:07.600 --> 39:15.120
one thing to think about building if you're a systems architect and not thinking about the social

39:15.120 --> 39:19.120
impact I mean there are always social impacts and always ways to be thinking about that every

39:19.120 --> 39:24.640
about ethics and architecture that everyone should be having but I think particularly where this

39:24.640 --> 39:32.480
really comes to the fore is in the shift in the field towards applied work towards social good

39:32.480 --> 39:39.920
which is both an incredibly exciting move for the field to be making but also a move that

39:39.920 --> 39:46.960
requires you know a sense of humility and responsibility for what that means and a recognition

39:46.960 --> 39:54.080
that that means incorporating new types of knowledge production and expertise and opening up

39:54.080 --> 40:00.080
the bounds of what expertise even means or who is an expert yeah into the field also now also

40:00.080 --> 40:04.560
well Ben thanks so much for sharing a bit of what you're up to yeah thank you so much I really

40:04.560 --> 40:12.480
enjoyed this thank you same all right everyone that's our show for today to learn more about today's

40:12.480 --> 40:18.880
guest or the topics mentioned in this interview visit twimmel ai.com of course if you like what you

40:18.880 --> 40:25.360
hear on the podcast please subscribe rate and review the show on your favorite pod catcher thanks

40:25.360 --> 40:32.800
so much for listening and catch you next time

