WEBVTT

00:00.000 --> 00:18.320
All right, everyone. I'm here with Sandra Vockler. Sandra is a professor at Oxford. Sandra,

00:18.320 --> 00:24.880
welcome to the Twomol AI podcast. Thank you so much for the invitation. I am looking forward to

00:24.880 --> 00:30.720
digging into our chat. Of course, to get things started, I'd love to have you share a little bit

00:30.720 --> 00:38.800
about your background and how you came to work in the field of artificial intelligence as a lawyer.

00:38.800 --> 00:43.680
I guess it started with the fact that I always was very, very excited about technology and I grew

00:43.680 --> 00:50.400
up with an understanding that technology could actually help us bring the world closer together.

00:50.400 --> 00:55.760
I'm just started off in the field of health tech. Actually, that was the first area that I looked

00:55.760 --> 01:02.800
at because medical devices that makes immediate sense that people understand that this can be helpful

01:02.800 --> 01:10.160
for a lot of people in our society. And then over time, and especially in my PhD, I branched out

01:10.160 --> 01:15.440
much broader and got interested in all types of technologies and how they can be used for

01:15.440 --> 01:21.920
good or bad and got interested in the question of how it affects laws, how it affects society,

01:21.920 --> 01:26.400
and what it is that we can do to reap the benefits and mitigate the risks.

01:26.960 --> 01:33.280
Nice. Were you ever been a practicing lawyer? Did you go through law school and all that?

01:33.280 --> 01:42.560
Yes, I did go for law school. I have a special degree in medical law. I do have a PhD in law.

01:42.560 --> 01:49.680
So, yes, full-fledged lawyer in dead regard, yes, in state and academia.

01:49.680 --> 02:00.960
Nice. Nice. What's your take on the... Actually, before we jump into that,

02:00.960 --> 02:08.400
tell us about your research. I was going to ask what's your take on the state of AI and the

02:08.400 --> 02:17.680
law, but that is a very broad topic. But it's not one that I've talked about extensively here

02:17.680 --> 02:25.920
on the podcast. So, there's lots to dig in there, but maybe we'll be a bit more focused and I'll

02:25.920 --> 02:32.560
ask you to talk a little bit about your research interests, which you spend a lot of time thinking

02:32.560 --> 02:41.120
about specific areas that AI intersects law. What are those areas? Yes. So, in general, I focus on

02:41.120 --> 02:48.480
the legal and ethical aspects of emerging technologies, and at the moment, I have free, very

02:50.960 --> 02:56.000
distinct research interests that have to do with AI. There is much more, so I don't want to say

02:56.000 --> 03:00.480
those are the only three people have to care about, but those are the only three I have time to

03:00.480 --> 03:11.280
care about at the moment. And there are free, free aspects that I think we all need to think about

03:11.280 --> 03:18.480
whenever we use algorithms for decision making. One definitely has to do with the Blackpops problem,

03:18.480 --> 03:26.080
the other one has to do with the protection issues, and the third one has to do with bias. So,

03:26.080 --> 03:32.560
those three areas, regardless of where you deploy an algorithm, whether this is in the US,

03:32.560 --> 03:38.640
whether this is in Europe, whether this is in New Zealand, and whether this is in banking or

03:38.640 --> 03:48.160
education, or in the health sector, those three areas will be important, because you always

03:48.160 --> 03:54.240
going to want to understand how a decision was made, how did the Blackpops come to the conclusion

03:54.240 --> 03:57.680
that you shouldn't get insurance, or that you have to go to prison, for example,

03:57.680 --> 04:02.000
always going to be a Blackpops problem. There is always going to be a data problem,

04:02.000 --> 04:07.120
because an algorithm is useless without data. So, whenever you talk about algorithms,

04:07.120 --> 04:11.520
you have to talk about data, and whenever you talk about data, you have to talk about algorithms,

04:11.520 --> 04:18.480
because data is only worth anything if you can analyze it in a way. And the genius thing about

04:18.480 --> 04:26.160
inferential analytics and AI is that you can learn so much about people, but the scary thing about

04:26.160 --> 04:33.040
the AI is that it can learn so much about people. So, the question is how can we navigate through

04:33.040 --> 04:38.320
that, that I have a powerful tool that is able to learn a lot of things about me that can be helpful,

04:38.320 --> 04:44.160
right, and be good, can help me diagnose cancer, but can also infer whether I'm gay or not,

04:44.160 --> 04:52.560
or whether I've voted in the last election, or whether I'm a woman, or whether what my

04:52.560 --> 04:58.240
sexual orientation is, right. So, the question is what needs to be done from a data protection issue,

04:58.240 --> 05:03.680
and then the last one I have to deal with both bias and discrimination, and again,

05:03.680 --> 05:09.680
this is prevalent regardless of country and regardless of sector where I deployed,

05:09.680 --> 05:16.960
because the data that we collect is, unfortunately, most instances not fair. Reason being because

05:16.960 --> 05:23.440
the world is not fair. It's not fair in any part of the world, fully at least, and it's usually not

05:23.440 --> 05:29.760
fair, fully fair in any of the sectors where AI is being used. So, the bias will be inherited

05:31.200 --> 05:36.000
when you train the algorithms on it, and just think about all the different types of

05:36.000 --> 05:42.240
sectors where we use them, right. We use them in education, we use them in traditional sector,

05:42.240 --> 05:49.200
we use them in employment, those are all areas where we know that biases exist, so it is no

05:49.200 --> 05:54.800
surprise that we're just going to transport human biases into algorithmic biases. So, regardless

05:54.800 --> 06:00.320
of where you are in the world, regardless of what you're using the technology for, you need to

06:00.320 --> 06:06.480
think about those things, and those are the three areas that I am trying to make an effort to come

06:06.480 --> 06:12.000
up with a contribution at least. Nice, nice. Well, let's maybe explore those in turn

06:13.280 --> 06:22.160
in talking about the black box challenge. We talked about that quite a bit on the podcast,

06:22.160 --> 06:30.080
the need for transparency to some degree, or explainability to some degree, often

06:30.080 --> 06:38.080
from the perspective of, yeah, I'm a business person, I'm relying on this algorithm to help me

06:38.080 --> 06:46.000
make a decision. I kind of, in order to develop a level of trust, I kind of want to know why it's

06:46.000 --> 06:55.280
recommending the thing that it's recommending. When you bring in the element of law and maybe

06:55.280 --> 07:01.600
more broadly regulation, how do the ways I need to think about that problem change?

07:02.480 --> 07:09.440
Right, so, I mean, you can do it with a brute force instrument and just say,

07:09.440 --> 07:15.680
I, as a regulator, I'm only going to allow algorithms that are explainable, period,

07:15.680 --> 07:20.960
exclamation point, and just live with those consequences, and that is something that has been

07:20.960 --> 07:27.440
discussed. And whenever this discussion comes up, then people will immediately say, hold on,

07:27.440 --> 07:32.640
hold on, hold on, you can do this because you cannot explain algorithms. So if you do that,

07:32.640 --> 07:40.560
then you let out banning them. That was the research, you know, there was a wisdom of the day when

07:40.560 --> 07:46.160
I entered the stage to get interested in this topic that it's just impossible. So when I thought

07:46.160 --> 07:53.280
about this a bit more, I came to the conclusion was, yes, there are probably two reasons why people

07:53.280 --> 07:58.800
don't want to give you an explanation. Right, the first is because they can't, and the second is

07:58.800 --> 08:05.760
because they don't want to. And those are two different topics. One is definitely because they

08:05.760 --> 08:11.760
don't want you, one to, and that has to do with trade secrets that they could tell you how

08:11.760 --> 08:18.480
an algorithm works because it's not so complex that you wouldn't know. So you could actually tell

08:18.480 --> 08:23.280
somebody why somebody has to go to prison because the algorithm is not that complex. So that's

08:23.280 --> 08:28.400
something where the law can come in and say, well, you know, just open up that black box

08:29.120 --> 08:35.200
because we know the answer. It's just a question of trying to balance the interests of business

08:35.200 --> 08:40.400
and the interests of the wider community in our society. So there is something that the law could do.

08:40.400 --> 08:46.720
And I recommend that for example, to have like a trusted third party that could have access to

08:46.720 --> 08:51.520
that doesn't have to be spread everywhere. The more challenging, I don't want to give you an

08:51.520 --> 08:57.760
explanation part is there because I can't part, which is not even the person writing a code

08:57.760 --> 09:02.880
allegedly does fully understand what's going on. So even if they want it to, they can't. And

09:02.880 --> 09:07.840
that's the more challenging problem because the law doesn't really have an answer there yet

09:07.840 --> 09:16.880
because some things are unfortunately not explainable, not even to an full extent to the people

09:16.880 --> 09:26.960
writing a code. However, I don't usually take no for an answer. So I figured I tried to

09:26.960 --> 09:32.000
explore if that's actually fully true. And I teamed up with two other people. One of them is

09:32.000 --> 09:37.360
Brent Middlestedt was an ethicist and Chris Russell works in machine learning and we wrote a paper

09:37.360 --> 09:43.760
that is called counterfactual explanations without opening the black box. So what we try to do there

09:43.760 --> 09:49.680
is try to take all those concerns on board and say, okay, is there a way to understand what's

09:49.680 --> 09:58.320
going on inside of a black box without fully understanding the black box, right? And we came

09:58.320 --> 10:05.120
up with counterfactual explanations as a way to do that because we saw it from the view of the

10:05.120 --> 10:09.920
person who wants to have an explanation, right? I have to go to prison and I didn't get the job,

10:09.920 --> 10:18.480
I wasn't promoted. The thing that I want is not a full-fledged code explanation. What I want to

10:18.480 --> 10:23.040
know is why the hell didn't I get the promotion and what do I need to do different to get the

10:23.040 --> 10:28.720
promotion? That's the thing I'm actually after, right? If you fire me and you give me a piece of

10:28.720 --> 10:34.000
code in my hand, I'm going to be very angry at you actually and feel like you haven't listened to me.

10:34.000 --> 10:40.000
And that's exactly how explanation usually works in human settings. I want to know the criteria,

10:40.000 --> 10:45.600
the reasons why I didn't happen and what I need to do differently. And luckily, that type of

10:45.600 --> 10:51.600
reasoning is something that you can code. So we call it counterfactual explanations in human

10:51.600 --> 10:58.240
settings, but in code, you can also generate a counterfactual. So where you just have a very

10:58.240 --> 11:04.640
complex system, one that you might not be fully to understand why, but I can tell you why in this

11:04.640 --> 11:10.720
particular case the decision was made in a certain way. So if you apply for a loan, for example,

11:10.720 --> 11:17.120
a counterfactual will tell you you were denied the loan because your income was 30,000 pounds.

11:17.120 --> 11:22.720
If it had been 35,000 pounds would have given you the loan, right? So you get the most important

11:22.720 --> 11:28.400
criteria and it tells you something what needs to be done to change the result. It gives you

11:28.400 --> 11:35.040
grounds to contestation, all that good stuff, right? Without having to understand the complexity of

11:35.040 --> 11:41.120
the full code. And that was a way to find a middle ground there where we can do something

11:41.120 --> 11:47.440
in a way. And that's done something that the law could require you to do. And that was actually

11:47.440 --> 11:52.880
quite exciting. So we wrote that paper a couple of years back. And Google came across our work

11:53.760 --> 12:00.000
interestingly enough. And they implemented an intensive flow and later on they implemented it

12:00.000 --> 12:06.000
in Google Cloud as well. And then many other companies have followed suits such as

12:06.000 --> 12:12.880
Vodafone for example, which is amazing to see. So the things that we cooked up in our ivory tower

12:12.880 --> 12:20.400
basically were actually something that had a positive meaning for people working on a ground

12:20.400 --> 12:26.000
which is exciting. But anybody interested in the topic can can look at our paper and the code

12:26.000 --> 12:31.760
is free available. Everybody can can use the type of explanation if they wanted to. So it's

12:31.760 --> 12:36.960
freely accessible. But yeah, that is definitely a way to think about opening the black box

12:36.960 --> 12:42.560
in a meaningful way. That's awesome. Can you give us an overview of how those counterfactual

12:42.560 --> 12:53.520
explanations are created or generated? Yes. What you do is you try to find the closest

12:54.560 --> 13:00.400
and minimal changes to a current decision model that need to be taken in order to get the

13:00.400 --> 13:05.760
thing that you want. So you're not actually just asking how does the rationale of the algorithm

13:05.760 --> 13:11.120
work? What you're doing is, okay, what are the smallest possible changes that I need to do

13:11.120 --> 13:16.960
to get from point one to be. And the interesting thing is that I could give you multiple

13:16.960 --> 13:24.000
counterfactuals, right? I could tell you you didn't get admitted to law school because your reference

13:24.000 --> 13:30.480
letters were bad and or because your grades were too low or because you had typos in whatever,

13:30.480 --> 13:36.000
right? And then give you a ranking of that and then you can figure out what would be the most helpful

13:36.000 --> 13:41.120
for you because for some of us it's easier to change the spelling on the covered letter

13:41.120 --> 13:46.480
for others it would be easier to find better reference letters, right? So to give them whatever

13:46.480 --> 13:53.680
set of possible grounds to improve your current situation is a very, very good benefit of counterfactuals.

13:54.880 --> 14:00.560
So you're you have some data point that you want to

14:02.400 --> 14:08.080
generate these explanations for and so then you permute it in different dimensions to try to

14:08.080 --> 14:15.040
understand how the what different decisions the model might take and then you use that to create

14:15.040 --> 14:24.080
the explanations. Yes, exactly. Awesome, awesome. And then the second focus area for you is around

14:24.080 --> 14:31.600
data protection and it sounds like your work is exploring a fundamental question of, you know,

14:31.600 --> 14:42.000
is privacy still viable in this fully connected fully connected world with, you know,

14:42.000 --> 14:50.000
machines making predictions and accessing consumer data and the degree to which, you know,

14:50.000 --> 14:57.120
what our expectations should be and how we can protect data. And tell us a little bit more about

14:57.120 --> 15:05.280
that area and some of your work there. Yes, certainly. That is a topic very close to my heart.

15:05.280 --> 15:12.000
And when we when why do we need to care about I said it already a little bit, but just to give

15:12.480 --> 15:18.880
people a little bit of a scary landscape of what's actually out there is that I don't think we

15:18.880 --> 15:26.880
really fully understand how good those algorithms are to protect very, very sensitive and intimate

15:26.880 --> 15:32.720
information about us that we might not want to share with anybody at all. And they can do that

15:32.720 --> 15:40.320
from very seemingly neutral data. So if I use my search engine, for example, Bing,

15:40.320 --> 15:47.760
they are able to infer based on how I move my mouse what I have Alzheimer's disease. Are you aware

15:47.760 --> 15:53.200
of that? Are you aware that you are giving that health information away to the outside world?

15:53.200 --> 15:59.680
Are you aware that Twitter can infer whether you have depression just based on what you treat online?

15:59.680 --> 16:08.080
Are you aware that to be clear when you give these examples, are you posing them as hypotheticals

16:08.080 --> 16:14.160
Bing could? No, no, they have published a paper that they can do that. So that's what I'm trying

16:14.160 --> 16:18.000
to say. This is not a hypothetical something that I cook up in the ivory tower. That's

16:18.640 --> 16:28.960
fact they can do that. One of the most problematic ones is definitely what happened with Facebook

16:28.960 --> 16:34.640
where they're able to. And again, there's research that shows that able to infer

16:35.360 --> 16:45.680
sexual orientation, ethnicity, gender, ability, without any of their users identifying with any

16:45.680 --> 16:53.280
of those classes and groups just based on what they click on, what they like, what they post,

16:53.280 --> 17:00.160
and who their friends are. They can have that information and then use it in a way where they

17:00.160 --> 17:05.200
for example allow advertisers to exclude them from seeing certain products. And this is something

17:05.200 --> 17:12.400
that they did for example. So they would in the US infer that somebody was for example black

17:12.400 --> 17:20.000
and would allow advertisers to exclude them from seeing job offers and ads for housing

17:20.000 --> 17:27.040
and financial services. So it's not just a theoretical problem that you should not know those

17:27.040 --> 17:33.840
things about me unless I say it's okay, but it also actually has negative consequences

17:34.480 --> 17:39.680
because it can be used against you in a way where you don't know about it. So we've all

17:39.680 --> 17:44.560
of that. And again, so it's just three examples that show how powerful those algorithms are.

17:44.560 --> 17:48.960
The question is, well, I mean, we have the protection law. Why not just apply data protection

17:48.960 --> 17:56.080
law to all of those problems. And the issue here is at least the one that I, you know, I've

17:56.080 --> 18:00.960
been currently working on in a, or I've been working on a research project in a paper that I wrote,

18:00.960 --> 18:09.120
which is called a right to reasonable inferences, is that I show that the current data protection law

18:09.120 --> 18:19.840
was designed in a way without fully anticipating the power of AI. And therefore it was designed

18:19.840 --> 18:27.760
in a way in a very almost 20 century way of thinking about privacy. In the sense that, you know,

18:27.760 --> 18:34.320
20 century privacy is I'm in my home and I don't like my neighbor. He's very nosy and he keeps,

18:34.320 --> 18:38.640
you know, coming over to the fence and he's looking at me and he says, oh, Sandra is again

18:38.640 --> 18:48.000
eating ice cream before noon. Oh, my God, right. She has no self control. Yeah. So she has no

18:48.000 --> 18:54.480
self control whatsoever, which is true. But that's the idea. So what is it that the law wants to

18:54.480 --> 19:01.280
do is that I want to prevent the noisy, the nosy neighbor from collecting information about me.

19:01.280 --> 19:07.760
So I give all the power to me. So you have to ask me first, right? And that is how the protection law

19:07.760 --> 19:13.680
works is a lot of like it's based on consent or transparency where somebody needs to tell you,

19:13.680 --> 19:19.040
be aware, be aware, I'm taking this data from you. Because in a human setting, that's a dangerous

19:19.040 --> 19:23.680
part, you know, they are seeing something and I can anticipate what they know, right? I'm sitting

19:23.680 --> 19:29.040
in my yard eating ice cream. That's the information that the neighbor now has about me, right?

19:29.760 --> 19:36.160
But this is why all of the data protection law focuses on. With algorithm, collecting information

19:36.160 --> 19:42.880
about me being ice cream is the first step, right? That's not the thing that they actually after.

19:42.880 --> 19:48.720
They're interested in what they can infer based on my eating habits, right? And that's the question.

19:48.720 --> 19:53.520
That's something that my neighbor couldn't do. An algorithm now might be able to infer that I

19:53.520 --> 19:58.160
might have a higher chance of getting diabetes at some point, which is something my neighbor couldn't

19:58.160 --> 20:03.920
do, right? But an algorithm can. So the interesting or more interesting, more dangerous being actually

20:03.920 --> 20:09.840
happens after data is being collected. So everything that is being inferred, but the law doesn't really

20:09.840 --> 20:16.080
care about that so much because the law still fought the data collection part is the most dangerous

20:16.080 --> 20:23.520
thing. So I've wrote a very, very long paper. I don't know, 150 pages just to show that that's

20:23.520 --> 20:29.440
a problem because the data protection law focuses too much on the input side of things, collecting

20:29.440 --> 20:35.040
the data, taking data from you and not so much on the output stage, which is what can I learn about

20:35.040 --> 20:40.320
you and that new laws actually need to govern the outputs rather than just the inputs.

20:41.440 --> 20:49.040
I'm curious about the length of the paper. It sounds obvious and clear when you,

20:49.920 --> 20:55.360
you know, you made a very simple and clear argument for this and, you know, I'm totally

20:55.360 --> 21:02.640
bought in. Why do you need 150 papers to, who are you trying to convince and what is it that they

21:02.640 --> 21:12.160
needed that you had to build out this 150 page argument? Yes, because it's really, I don't think

21:12.160 --> 21:21.680
people have looked in complete detail how it's actually being regulated. And I just went through

21:21.680 --> 21:29.600
with a magnifying glass to show that inferential analytics, unfortunately, or inferential data

21:30.160 --> 21:36.000
has almost to no protection. And I showed that in the case law as well, which is something that

21:36.000 --> 21:40.800
wasn't really looked at. Also, I could do with the fact that I was talking about the generative

21:40.800 --> 21:49.520
protection regulation, which at that point was really new too, right? So there, you know, I,

21:49.520 --> 21:54.480
and your framework came out and explaining this new thing that no one understood.

21:54.480 --> 22:00.640
Yes, yes, right. So that was important because it was very, I think,

22:02.720 --> 22:07.120
important to get the message across how really, really

22:08.000 --> 22:12.400
propriomatic that is, which I think up until this point wasn't really clear.

22:14.160 --> 22:18.480
So yes, it's, and I just want to, that's what I usually do when I see there's a problem

22:18.480 --> 22:22.960
that I'm not going to give you just one example. I'm giving you like all the examples. So you

22:22.960 --> 22:27.440
with me and, and can understand that you really need to care at this point and something needs to

22:27.440 --> 22:35.680
be done. So I know GDPR expert, but my sense is that GDPR didn't really, this wasn't really one of

22:35.680 --> 22:42.000
the issues that GDPR was addressing or trying to address. Is that right? Yes, that's definitely

22:42.000 --> 22:47.520
also one of the problems. And this is why data protection law was so much designed in the way to

22:47.520 --> 22:57.520
just keep the noisy, the noisy neighbor out, right? The, the, what actually was capable of doing

22:58.080 --> 23:06.080
happened so much later, right? If we're very honest, the new GDPR that we have is to 80, 90 percent

23:06.080 --> 23:12.640
the same stuff that we had in the data protection directive and that framework is from the 90s,

23:12.640 --> 23:20.960
right? So there are updates there. Yes, but the core mechanisms and the core assumptions and

23:20.960 --> 23:25.280
the thing that it's supposed to be doing hasn't really changed. This is not to say that it should

23:25.280 --> 23:30.320
necessarily regulate AI, but it's just to say that it was never designed to regulate AI

23:31.360 --> 23:36.640
in a way. And therefore it's failing. And I think a lot of people had hope that this new framework

23:36.640 --> 23:42.000
will be visible to all the problems. And I just wanted to say, no, I think there's still a lot of work

23:42.000 --> 23:47.360
to be done and we need to be very, very careful to do this right because there's so much on the line.

23:49.360 --> 23:54.080
You mentioned regulating AI. It's not even regulating AI as much as regulating data in the

23:54.080 --> 23:59.520
age of AI. Yes, definitely, definitely. Yes. So even, even for it to remove from that.

23:59.520 --> 24:09.360
Yeah. Is there a, is there a locale or a regulatory framework that you think

24:09.360 --> 24:20.000
does a good job with the issues that you've described? Are we there yet? No, I don't think we're there yet

24:21.120 --> 24:28.720
because I think it touches so many things at the same time that if you want to come up with one

24:28.720 --> 24:34.720
framework, it will need to do a lot of work. I think what probably would be more helpful is that

24:34.720 --> 24:40.640
to rethink our regulation should work in the future anyway, that you just don't

24:40.640 --> 24:46.960
silo data protection in one corner without thinking about competition law and without thinking

24:46.960 --> 24:51.600
about non-discrimination law and without thinking about protection law because all those things

24:51.600 --> 24:58.560
are very much interconnected. So one law that governs everything, I'm not sure if that's useful

24:58.560 --> 25:05.600
or necessary, I think what's very useful and very necessary is that different types of regulators

25:06.800 --> 25:13.760
start to collaborate more closely because AI kind of puts them in the same room anyway,

25:13.760 --> 25:19.040
right? And the data flow does that anyway. So I think that's a better way of thinking about

25:19.040 --> 25:26.240
regulation is that almost every regulatory aspect or every sectoral law that we have will be

25:26.240 --> 25:33.760
touched by some type of technology at some point. So regulation is only one approach to

25:34.880 --> 25:40.880
ensuring good behavior. There's also kind of self-regulation or industry consortia

25:41.520 --> 25:56.160
or the like. Are there any examples of folks that you can point to that have taken a responsible

25:56.160 --> 26:03.360
approach to these issues or are particularly transparent? I'm just wondering if there's a good

26:03.360 --> 26:17.520
example or reference model that has emerged for say, I guess I'm envisioning an enhanced

26:19.120 --> 26:24.960
data use statement or something like that that talks not only about these are the organizations

26:24.960 --> 26:29.360
with which we share data but these are the derivative products that are created and this is how

26:29.360 --> 26:41.600
we use and or share that information. Is anyone doing that? Yes, I think everyone is doing that

26:41.600 --> 26:46.240
and I think that's a little bit also after the problem that everybody is doing that.

26:47.600 --> 26:51.360
Doing the creating of the derivative products and sharing them or

26:51.360 --> 27:00.720
not creating codes of conduct and guidelines and best practices and standards and all of that

27:00.720 --> 27:07.520
is being created everywhere. It's created by governments, by industry, by NGOs everywhere and

27:07.520 --> 27:18.080
anywhere. My colleague, Brent Smith, said he wrote a paper in nature I think which was called

27:18.080 --> 27:23.040
white principles alone and not enough for something along those lines and I think at the point

27:23.040 --> 27:26.960
when he wrote that piece and that's also I think for probably two years old at this point,

27:26.960 --> 27:34.640
he said that there are 150 different guidelines best practices and standards out there

27:35.680 --> 27:42.720
and they're all roughly the same and he goes through for them all and just points out how

27:42.720 --> 27:50.080
they are still lacking the thing which is being applied in practice with having a good feedback

27:50.080 --> 27:59.360
look how well they actually work in practice. I'm all for responsible innovation and research

27:59.360 --> 28:05.440
and trying to come up with best practices and I think that's absolutely needed but I think that

28:05.440 --> 28:11.520
part is now over. I think the interesting part is now to figure out is anybody actually deploying

28:11.520 --> 28:16.960
them in practice, how good are they, what kind of oversight mechanisms are there. It's great to

28:16.960 --> 28:22.480
have five wonderful ethical principles if you don't tell me how you actually operationalizing them

28:22.480 --> 28:32.800
then I don't necessarily think the job's done yet. From the perspective of concerned parties,

28:32.800 --> 28:41.120
academia, what does the landscape look like in trying to address these issues? There's certainly

28:41.120 --> 28:45.600
papers like yours where you identify the issue and show that it's not being addressed,

28:46.320 --> 28:54.640
what are the steps that we can build upon that to get to something that is a better place?

28:54.640 --> 29:03.440
Yes, I think that I mean that's obviously a biased view here but I think that academia actually

29:03.440 --> 29:10.560
does have a very big role to play and I'm very happy to say that academia has played a very vocal

29:10.560 --> 29:17.520
and important role of the last years. I think if academia hadn't been as loud and persistent,

29:17.520 --> 29:21.440
a lot of things would have not have changed. So the questions around a river of my

29:21.440 --> 29:28.640
accountability and privacy protections have been front and center on a lot of agenda of people

29:28.640 --> 29:33.680
in the field and have they have not been so persistently. I don't think that so much would change

29:33.680 --> 29:40.080
at the moment so definitely that. And in that I think one of the reasons again is why

29:40.080 --> 29:47.360
many of them have been so powerful and influential is because they very often work with people

29:47.360 --> 29:53.360
that are not from the same discipline than they are and I think that's the key thing here because

29:54.400 --> 29:57.200
I can just speak for myself. I think none of my papers

30:00.320 --> 30:06.160
I could have not done it by myself and I was very important to have an emphasis in a machine

30:06.160 --> 30:14.880
learning person on there to teach me and have them also endure my teachings and I think that

30:14.880 --> 30:20.000
made the whole work stronger and I think that's really what if you are thinking about how to

30:20.000 --> 30:26.880
govern things I think you owe it that you try to look at the issue from as many as perspective as

30:26.880 --> 30:34.320
possible. And then the third pillar of your research is focused on bias fairness and discrimination

30:34.320 --> 30:40.080
with regard to the law. It sounds like this is your most recent work and what you're most excited

30:40.080 --> 30:47.600
about right now. Tell us a little bit more about that area. Yes definitely definitely extremely

30:47.600 --> 30:55.200
excited about that. I think everybody will know as I said that you know bias and unfairness is

30:55.200 --> 31:00.320
always an issue when we think about data and algorithms because unless you collecting them in

31:00.320 --> 31:07.440
utopia she said fair fair chance that the data would bias in some way and that's the reality that

31:07.440 --> 31:14.880
we have to deal with. I think this being about kind of the fundamental premise of machine learning

31:14.880 --> 31:20.640
that you know we're training based on you know information we've collected about the past and

31:20.640 --> 31:26.480
this decisions that were made in the past and the mechanism kind of fundamentally carries

31:26.480 --> 31:32.640
forth biases from the past and to the future if there's not extreme care taken. Yes absolutely

31:32.640 --> 31:38.080
I should have said that yes I mean that's as you said that's basically how all machine learning

31:38.080 --> 31:43.360
works is looking at the past trying to predict the future right you feed the algorithm with a bunch

31:43.360 --> 31:49.840
of historical data for example who has been hired who has gotten insurance who was sent to prison

31:49.840 --> 31:58.320
who did reoffend who did get sick right and you train the model based on that because you think

31:58.320 --> 32:02.560
that you have some ground truth because you have historical data you know if somebody actually

32:02.560 --> 32:07.200
got sick right you know if somebody did well in law school you know if they're reoffended and if

32:07.200 --> 32:15.040
somebody that looks similar like the person right is now applying for the job is now applying to

32:15.040 --> 32:22.800
be left out on parole is now wanting to be promoted if they look like similar people that you know

32:22.800 --> 32:28.720
reoffended or dig well on the job then you give them the same chance because you assume

32:28.720 --> 32:36.320
that similar patterns will emerge right and that's all great again if we tend to make good decisions

32:36.320 --> 32:44.400
fair and just decisions that are accurate but if you're very honest very often that is not the case

32:44.400 --> 32:51.200
so unless you're very very careful you will just reinforce the biases and injustices that we had

32:51.200 --> 32:59.120
in human decision making but at much greater speed and much less detectable and so I got interested

32:59.120 --> 33:04.480
in this topic as well and the first question that I always ask myself is like well is the law

33:04.480 --> 33:10.960
sleeping why not just use the law to solve that problem and I got interested in the question of

33:10.960 --> 33:16.960
honest remediation law because that's the closest the most sensible law to look at and

33:18.160 --> 33:24.480
give you wrote in two papers in the past one is called why fairness cannot be automated which

33:24.480 --> 33:32.480
whole what he tells you how I felt about whether this is possible and and what we what we did

33:32.480 --> 33:38.560
there was quite similar to to previous work where we showed that the law just wasn't designed in

33:38.560 --> 33:44.880
a way to govern algorithms it was designed to govern people right so if you think about a

33:44.880 --> 33:50.560
discrimination setting a discrimination setting a traditional one is where you know somebody

33:51.280 --> 33:57.520
indirectly directly is not giving you the job because you're a woman or they harass you because

33:57.520 --> 34:03.520
of your religious beliefs or you are in a hostile environment where other things are going on that

34:03.520 --> 34:08.320
prevent you from succeeding the basic point is you know that something's off right so you bring

34:08.320 --> 34:14.160
a complaint and the discrimination law will will help you with that so with algorithms again it's

34:14.160 --> 34:23.040
different because they discriminate behind your back without you actually being aware so a complaint

34:23.040 --> 34:29.040
based system such as non discrimination law is completely powerless if the person doesn't know

34:29.040 --> 34:34.400
that they have been wronged and again that's not a failing of the law per se and sort of failing

34:34.400 --> 34:40.320
of the algorithm it's just a very unhappy mismatch of the two because again the law was designed for

34:40.320 --> 34:48.560
people not for for algorithms so but discrimination still occurs so what is it that I need to do

34:50.320 --> 34:55.840
which means we have to test and test and test because if I don't know somebody has to know

34:56.480 --> 35:02.960
and the problem is you can only know if you test for it because and that's the second problem very

35:02.960 --> 35:10.880
often you might not even know that the data that you have collected is biased or unfair towards certain

35:10.880 --> 35:16.880
people because again here the intuition kind of rakes down and on discrimination law was very much

35:16.880 --> 35:22.720
based on intuition a judge looks at the case and says oh what you bent head scars from the workplace

35:22.720 --> 35:28.160
that's a problem if we're religion you don't need much data to make that point because there is

35:28.160 --> 35:34.640
a clear understanding of the social reality or the social symbolism of head scarves and religion

35:34.640 --> 35:42.560
that's not much you need to do that right what but what if I you know got fired because I don't have

35:42.560 --> 35:52.080
a dog right that sounds maybe odd but do I know that that correlates with ethnicity or gender

35:52.080 --> 35:58.000
sexual orientation versus belief I really don't know anymore right so how am I supposed to bring a

35:58.000 --> 36:06.000
case in court if you're using data where my social gut doesn't ring alarm bells anymore right

36:07.440 --> 36:13.120
so those two things that I might not know about it and that even if I know about it that

36:14.720 --> 36:20.160
I have almost no way of proving it means that fairness cannot be optimal to my defensive

36:20.160 --> 36:25.760
favorite title so we thought okay then you need to test you need to test test somebody needs to

36:25.760 --> 36:35.440
test because otherwise you wouldn't know so we we came up with with a biased test that lets you

36:36.880 --> 36:44.160
do that so the test is called um demographic conditional demographic disparity CDD

36:44.160 --> 36:54.880
and we chose that test because it aligns the most with European nondiscrimination law in a way

36:54.880 --> 37:09.040
that other tests do not and yeah this year in January of February Amazon has came across our work

37:09.040 --> 37:15.600
um and that bias test and they found interesting and they have decided to implement it in their own

37:16.400 --> 37:26.080
bias toolkit um so say to make a clarify so now customers of Amazon can use that test as well

37:26.080 --> 37:31.600
but again as with all of our research it's publicly available so if anybody's interested in that

37:31.600 --> 37:36.560
having a closer look or in the court or whatever it's free and publicly available as well

37:36.560 --> 37:43.920
and so can you talk a little bit about what differentiates this conditional demographic disparity CDD

37:44.560 --> 37:53.200
tests with the tens or potentially hundreds of other statistical tests that have been used as

37:53.200 --> 38:00.640
metrics of bias in data sets yes certainly so um that's actually quite exciting and that's

38:00.640 --> 38:07.920
the the second paper that complements the the first one uh we also wrote a paper which is called

38:07.920 --> 38:14.080
bias preservation and machine learning and the legality of fairness metrics nondiscrimination law

38:14.080 --> 38:20.960
so exactly uh the question that you uh just asked me um so what we did there is we looked at

38:21.840 --> 38:27.360
uh 20 different fairness tests and um we came up with the classification system

38:27.360 --> 38:36.560
on how they how they make decisions um the one category is um called bias preserving bias tests

38:36.560 --> 38:43.600
and the other one is called bias preserving uh bias transforming fairness tests so what we looked

38:43.600 --> 38:54.400
at is that um the majority of them so 13 out of 20 uh bias preserving so what they do is they look

38:54.400 --> 39:00.320
at the error rates to measure fairness so they want to make sure that um whatever type of decision

39:00.320 --> 39:08.240
has been made and is now being made has the same uh base rate for errors the other ones the other

39:08.240 --> 39:15.200
seven are more looking at decision rates when they're looking at how the outcome is distributed

39:15.200 --> 39:24.400
across certain groups right so that's that is the main contribution coming up with that distinction

39:24.400 --> 39:32.160
and trying to tease out the underlying assumption of this right one bucket says um as long

39:32.160 --> 39:40.000
as we're not making things worse than they used to be i give my fairness check and it's okay

39:40.000 --> 39:47.920
that bias transforming metrics that look at the decision rates say i'm only happy if equal outcomes

39:48.560 --> 39:54.800
are happening across groups right so this is the underlying assumption this is fine

39:55.440 --> 40:03.840
unless you look at what european on the scrimination law wants to do non-scrimination law in europe

40:03.840 --> 40:11.440
is not just about formal equality as in do not actively treat somebody differently because

40:11.440 --> 40:17.440
of their race or gender or sexual orientation right which is more like a negative form as

40:17.440 --> 40:25.280
in passive form of discrimination law um non-scrimination line europe is much more about

40:25.280 --> 40:34.640
substantive equality which is more about actively dismantling inequality keeping things as they are

40:34.640 --> 40:40.720
is not good enough in europe so you're supposed to take an active role as much as you can both the

40:40.720 --> 40:48.640
pride of the public sector to actually make the world a fairer place right and the majority of

40:48.640 --> 40:56.400
those fairness tests don't do that because they condition on an unequal status quo and they freeze

40:56.400 --> 41:02.640
that and that is the main problem and with our fairness test and with other data bias transforming

41:02.640 --> 41:08.880
you could counterbeam that at least would give the opportunity to actually uh make it better

41:08.880 --> 41:22.160
got it got it and so at least for those whose problem is uh specifically covered under the regime

41:22.160 --> 41:28.080
of the european on discrimination tests this cdds the only test the only fairness test that

41:28.880 --> 41:36.400
uh is adequate not not the only one like anything that is bias transforming and there are seven

41:36.400 --> 41:43.200
others that are also uh bias transforming um that you can use you could use those definitely

41:43.200 --> 41:48.000
those are absolutely fine to use and we pointed out in the papers and we listed exactly so

41:48.560 --> 41:56.000
enough other tests that you could use and also it is not to say that you cannot use bias preserving

41:56.000 --> 42:04.400
metrics completely in europe right it just depends on what the context is if you're using bias

42:04.400 --> 42:11.280
if you're making decisions life changing decisions about people in europe in a sector that is

42:11.280 --> 42:19.680
a protected and that is be known to exhibit bias then you should use a bias transforming metrics

42:19.680 --> 42:25.680
right um because it at least gives you the ability to make something better than it used to be

42:25.680 --> 42:31.280
it doesn't have to be you don't have to make it better but you need to at least justify it in a way

42:31.280 --> 42:38.880
however you could still use a bias preserving metrics um and either justify that as well but I think

42:38.880 --> 42:44.000
it's difficult but there are legitimate areas where you can use it but you don't have a problem so

42:44.000 --> 42:51.360
if you use it for research only where it doesn't affect people absolutely fine to use bias preserving

42:52.080 --> 42:59.760
in europe as well if you are unsure of what a good outcome would actually look like it's much

42:59.760 --> 43:05.040
better to keep things as they are than making them worse right if you don't have a normative idea

43:05.040 --> 43:12.640
of how things ought to be then you can use them as well you can also use them if there are situation

43:12.640 --> 43:20.000
where we have you know justified bias or even desired bias if we wanted that no one to preserve

43:20.000 --> 43:26.160
that that is fine to use as well and then areas where you do actually have ground truth where there

43:26.160 --> 43:32.000
is no bias in the data set you can also use bias preserving metrics so there's a whole range

43:32.000 --> 43:39.600
where you can justify we use it the only problem is that if you're making life changing decisions

43:39.600 --> 43:46.320
about people in a protected sector and that sector is known to exhibit certain biases

43:47.280 --> 43:53.760
then the preference would be it be easier for you from a legal perspective to use bias transforming

43:53.760 --> 43:59.760
ones because at least they offer the possibility of making things better they used to be

44:00.880 --> 44:07.440
so the the first contribution of this paper is this drawing the distinction between bias preserving

44:07.440 --> 44:19.280
and bias transforming and correlating those two classes of fairness metrics or bias metrics to

44:19.280 --> 44:27.120
the European non discrimination law but then you're also proposing this new test how does

44:27.120 --> 44:34.560
how is that new test advantage relative to the class of bias transforming metrics that you know

44:34.560 --> 44:40.800
can work under the European law what you really need to do and what a bias test should help you do

44:40.800 --> 44:48.880
is reflect on what you are able to do in order to make a positive contribution the bias test is

44:48.880 --> 44:56.160
not supposed to tell you what's right or wrong it just tell it should just tell you that something

44:56.160 --> 45:01.840
might be a problem it's supposed to act as an alarm system so what you should be doing is that

45:01.840 --> 45:08.880
you run let's say you you you thinking about loan decisions that you run your algorithm that

45:08.880 --> 45:16.080
is distributing loan decisions and it will tell you or did you know that your current deployment

45:16.080 --> 45:22.960
doesn't give black people any loans right and then it will tell you what kind of conditions

45:23.520 --> 45:31.120
criteria and variables were conditioned on and what that helps to do is to have an informed

45:31.120 --> 45:37.440
discussion of whether or not certain biases are justified or not because again in Europe on

45:37.440 --> 45:44.400
the non discrimination law not everything needs to be fair what needs to happen is if something is

45:44.400 --> 45:51.440
unfair you need to tell me why yeah and that is the thing that the test helps you to do because

45:51.440 --> 45:57.280
let's go back to the bank example you could say for example what we use income to make decisions

45:57.840 --> 46:02.320
on what does somebody should get a loan which makes intuitive little sense right and you could

46:03.120 --> 46:07.840
put those criteria conditioning on input in there and it will tell you oh

46:08.880 --> 46:14.000
hardly any women are getting loans is that on purpose right and then you could ask the question

46:14.000 --> 46:19.200
okay I'm conditioning on on salary and it has a negative effect on gender is that justified

46:19.200 --> 46:24.480
because you could say well how can we even use income since we know about the race and gender

46:24.480 --> 46:29.600
pay gap right that's a horrible criteria to use in the first place right and say oh we're not

46:29.600 --> 46:35.840
gonna use that we're not gonna do that or you could say well yes we know about those inequalities

46:35.840 --> 46:41.440
but it's a very effective and defendable proxy um but are not somebody will be repairing

46:41.440 --> 46:48.480
the loan and actually putting people in a situation where they have to default will default

46:48.480 --> 46:53.680
and in depth a minute further it's also irresponsible so maybe we should be using income right

46:53.680 --> 47:01.360
or you could say well um okay income isn't great but how about those other criteria additional

47:01.360 --> 47:08.320
criteria that are also very good at predicting something but are not as disadvantaged

47:08.320 --> 47:14.320
um that's for example income that's the dialogue you need to have right and the test can help

47:14.320 --> 47:21.760
you to do that because it would lay open how your current decision system is affecting or is

47:21.760 --> 47:27.600
making decisions across groups where you could see does it actually make equal decisions across

47:27.600 --> 47:35.040
you know gender lines or lines with ethnicity or age or whatever and it tells you what kind of

47:35.040 --> 47:40.560
criteria we're conditioned on and then you can justify to yourself and ideally actually

47:40.560 --> 47:47.600
to the general public as to why those criteria are acceptable to use um even if they

47:48.640 --> 47:55.760
maybe end up in an unequal distribution if it is the only way to go about it right because

47:55.760 --> 48:05.200
that's the very inconvenient and unhappy discussion we need to have is what kind of disparity is

48:05.200 --> 48:12.480
acceptable and which is not and that biased test will let you do that. It sounds like

48:13.120 --> 48:21.520
and this is maybe a parent from the name that the the key thing that you the key innovation

48:21.520 --> 48:26.880
or contribution of this new test is that it makes explicit this conditioning on other factors

48:27.840 --> 48:33.200
something that's maybe you don't have the same degree of flexibility or the same mechanism to do

48:33.200 --> 48:42.320
that conditioning with some of the other tests. Yes exactly. Got it got it um and then uh you know

48:42.320 --> 48:51.360
so you've you've now got this test you've got this um you've identified the you know the relationship

48:51.360 --> 48:57.520
between these tests and and this particular set of regulations in Europe you know what are the

48:57.520 --> 49:07.280
next steps for you in pursuing this researcher or more broadly your interest in bias fairness

49:07.280 --> 49:14.240
and discrimination and law. Yes yeah fantastic question um I I'm definitely going to maintain

49:15.440 --> 49:22.560
an interest in in all those free areas going forward um I think one of the next topics that I

49:22.560 --> 49:30.640
will be um diving a bit more into is the new regulation that is looming at the horizon here in

49:30.640 --> 49:37.360
Europe because um there's a new draft that came out by the European Commission um the AI Act that

49:37.360 --> 49:46.080
is the first ever first attempt at a regulatory comprehensive regulatory framework to to govern that

49:46.080 --> 49:52.320
so there will be um I think a lot of work that not just me I think a lot of people will try to dive

49:52.320 --> 49:58.880
their teeth into to figure out um you know whether whether this is a good attempt and and what can be

49:58.880 --> 50:07.360
done there so I think that will remain a very active research area and the other ones that I'm

50:07.360 --> 50:17.760
definitely going to are areas of of health that I'm particularly interested in as well as um

50:17.760 --> 50:27.200
education and financial services. And those um those latter two areas health and well three health

50:27.200 --> 50:34.560
education and financial services uh still looking at the intersections of those with AI and the law

50:34.560 --> 50:43.120
or uh I think that I think those free areas will accompany definitely but um I will probably look

50:43.120 --> 50:49.680
at it more from like a power perspective and then and a and a broader regulatory landscape and

50:49.680 --> 50:55.840
oversight um landscape of that makes sense. Got it got it got it. Well Sandra thanks so much for

50:55.840 --> 51:03.280
taking the time to share a bit about what you are working on very interesting stuff and uh

51:03.280 --> 51:08.080
certainly enjoyed the conversation. Thank you so much it was great to be here thank you for the

51:08.080 --> 51:34.800
invitation.

