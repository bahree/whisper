WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimal AI Podcast.

00:13.400 --> 00:16.400
I'm your host, Sam Charrington.

00:16.400 --> 00:24.360
Hey, what's up everyone?

00:24.360 --> 00:28.640
Before we move on to today's show, I want to give you a heads up that we are back with

00:28.640 --> 00:34.560
another interactive podcast viewing party on Monday, May 18th.

00:34.560 --> 00:38.960
This time, I'll be joined by Emily Bender, Professor of Linguistics at the University

00:38.960 --> 00:40.680
of Washington.

00:40.680 --> 00:44.840
Our recent interview explored the question of whether linguistics has been missing from

00:44.840 --> 00:46.680
NLP research.

00:46.680 --> 00:52.240
We dig into questions like, would we be making more progress on more solid foundations if

00:52.240 --> 00:57.560
more linguists were involved in modern NLP or is the progress we're making, such as

00:57.560 --> 01:01.880
with deep learning models like Transformers, just fine.

01:01.880 --> 01:07.040
We also explore issues like fairness bias and ethics in NLP and more.

01:07.040 --> 01:12.040
Emily and I will be in the chat answering all of your questions about the interview.

01:12.040 --> 01:16.680
We'll begin the viewing party at 3 o'clock Pacific on Monday.

01:16.680 --> 01:22.720
You can head over to twimalai.com slash 376 viewing for more details or to add it

01:22.720 --> 01:24.520
to your calendar.

01:24.520 --> 01:27.880
And now on to the show.

01:27.880 --> 01:29.480
Alright, everyone.

01:29.480 --> 01:31.720
I am here with Nathaniel Ruiz.

01:31.720 --> 01:38.720
Nathaniel is a PhD student in image and video computing group at Boston University.

01:38.720 --> 01:41.520
Nathaniel, welcome to the Twimal AI podcast.

01:41.520 --> 01:42.920
Thank you very much.

01:42.920 --> 01:45.440
It is great to have a chance to chat with you.

01:45.440 --> 01:50.720
I'm looking forward to digging into our topic, which is some work you've recently done

01:50.720 --> 01:53.520
on disrupting deepfakes.

01:53.520 --> 01:58.760
But before we dive into that, tell us a little bit about your background and how you got

01:58.760 --> 02:01.840
started working in ML and AI.

02:01.840 --> 02:08.040
Yeah, so the first kind of project that I did in AI was a computer vision project when

02:08.040 --> 02:10.480
I was doing an internship here at MIT.

02:10.480 --> 02:15.760
And it was about basically detecting diseases in cassava plants and we were going to deploy

02:15.760 --> 02:17.760
the application in Uganda.

02:17.760 --> 02:21.920
So that was kind of like my first introduction to deep neural networks and image processing

02:21.920 --> 02:23.240
type of stuff.

02:23.240 --> 02:26.960
And I thought that was incredibly interesting, just like the potential of these types of

02:26.960 --> 02:30.800
applications and this new technology, right, or this technology that had been advancing

02:30.800 --> 02:32.440
like very recently.

02:32.440 --> 02:34.920
So I got really interested in it.

02:34.920 --> 02:40.320
Then I did my master's at Georgia Tech, so that was right after that internship.

02:40.320 --> 02:47.360
And then so at Georgia Tech, I met a mentor and my professor there who was my advisor

02:47.360 --> 02:49.920
for my master's, Jim Ray.

02:49.920 --> 02:52.680
And I joined this group and did a lot of stuff.

02:52.680 --> 02:57.240
So I got interested, I guess, and they do a lot of work on autism and behavioral imaging

02:57.240 --> 03:04.280
for autism, trying to like diagnose or model behavior and attention in toddlers or kids.

03:04.280 --> 03:09.400
So I thought that was an example of a model that they built.

03:09.400 --> 03:12.760
So we have several works on.

03:12.760 --> 03:19.360
So they were already doing several works on gaze estimation or attention estimation.

03:19.360 --> 03:22.920
And when I joined that group, I worked with Angie Chong, one of my collaborators, and

03:22.920 --> 03:25.920
she just graduated actually from that group.

03:25.920 --> 03:30.360
And we did behavior modeling and attention modeling and scenes.

03:30.360 --> 03:36.040
So now just more in general, like if you're looking at a person from a third person point

03:36.040 --> 03:41.200
of view, like an image or a video, someone, you can actually detect where they're attending

03:41.200 --> 03:42.640
in the scene.

03:42.640 --> 03:47.720
And this work kind of generalizes and you can basically detect where the person is attending

03:47.720 --> 03:51.440
two in the scene, and it could even be like if they turned their head, this could pick

03:51.440 --> 03:55.800
up that type of attention towards the back of the room, for example, or if they're looking

03:55.800 --> 03:59.600
at the camera, they're not looking at any object inside of the scene, they're looking

03:59.600 --> 04:04.840
at something, you know, beyond the frame, and it could also detect that type of attention.

04:04.840 --> 04:10.920
So I got really basically interested in faces, mostly, just in human beings and in faces

04:10.920 --> 04:12.560
in an image and video.

04:12.560 --> 04:18.200
So I think a lot of my work kind of like goes from that and then, yeah, that's how I got,

04:18.200 --> 04:22.320
you know, to recent kind of projects that I've been doing.

04:22.320 --> 04:24.160
And how long have you been at BU?

04:24.160 --> 04:31.240
This is my second year, so about a year and a half ago, so I had my PhD with my advisor

04:31.240 --> 04:33.080
Stan Schlerof.

04:33.080 --> 04:38.000
What got you started working on or looking at deepfakes?

04:38.000 --> 04:45.560
So actually, in the past year, I was always very interested in deepfakes, especially like

04:45.560 --> 04:51.400
there's a video where there's like deepfakes of Obama using a lot of computer graphics,

04:51.400 --> 04:52.400
right?

04:52.400 --> 04:56.640
They used deep neural networks and computer graphics, and this was like about like two to three

04:56.640 --> 05:01.080
years ago, there were like some big advancements, and you can see very realistic basically

05:01.080 --> 05:04.040
reanimations of Obama's face.

05:04.040 --> 05:07.840
That already, you know, got me very interested when I was at Georgia Tech, but I think it

05:07.840 --> 05:11.880
didn't have like a technical expertise like really tackled at that point.

05:11.880 --> 05:17.360
And more recently, I think the things that have been most impressive are the deepfake

05:17.360 --> 05:22.720
applications where you only grab using only one image of a person, you can basically create

05:22.720 --> 05:28.400
more images or video of that person with different expressions and moving their head.

05:28.400 --> 05:29.560
And that's been like really amazing.

05:29.560 --> 05:33.400
I think there's a work by Samsung, a neural talking heads or something.

05:33.400 --> 05:38.880
I don't actually remember the full title, but it's just amazing how just with one image,

05:38.880 --> 05:41.280
they can create these types of things.

05:41.280 --> 05:46.080
So I had been working on generative models for faces recently, and while I was working

05:46.080 --> 05:49.800
on that, I had a conversation with Stan, my advisor.

05:49.800 --> 05:52.000
There's always a privacy issue, right?

05:52.000 --> 05:56.160
Like when you're going to try to release a paper, you have all these issues that whose

05:56.160 --> 06:02.440
faces can I actually use for this work, and also kind of the reaction of the public.

06:02.440 --> 06:05.360
And I think there's a lot of good that could come out of these applications, but definitely

06:05.360 --> 06:07.920
a lot of bad that can come out of them, right?

06:07.920 --> 06:12.840
The good part is you could have actors and movies, you could basically edit their faces

06:12.840 --> 06:18.360
in real time and not ask them to shoot a scene again, maybe if they failed, in their

06:18.360 --> 06:21.320
expression wasn't perfectly what you wanted at that point.

06:21.320 --> 06:28.440
And there's a lot of different cool applications that you can do with UI-UX and sending videos

06:28.440 --> 06:33.960
of yourself through an iPhone or something, but there's also really bad stuff that we've

06:33.960 --> 06:34.960
seen, actually.

06:34.960 --> 06:40.920
Like the first thing that started happening is they've been using deepfakes in pornography,

06:40.920 --> 06:41.920
right?

06:41.920 --> 06:46.480
Just switching faces into pornographic scenes, and it's just completely immoral and the

06:46.480 --> 06:48.480
potential for damage is so big.

06:48.480 --> 06:53.280
So basically, there's always that effect if you're publishing something on this topic

06:53.280 --> 06:58.720
that you could be helping people that want to do that, you know?

06:58.720 --> 07:03.720
So that whole thing was in my head all that time, and during a conversation, we're talking

07:03.720 --> 07:05.560
about the privacy issue.

07:05.560 --> 07:10.040
So these networks are called image translation networks.

07:10.040 --> 07:16.040
So this work by Samsung is an image translation network that goes like from one image to, you

07:16.040 --> 07:18.880
know, a new image with a different expression imposed.

07:18.880 --> 07:21.240
They also can fine tune it with like several images.

07:21.240 --> 07:26.200
But in general, that's kind of the framework that a lot of these new works are applying.

07:26.200 --> 07:33.320
So StarGAN, also StyleGAN, all of these organimation also was a, I think, 2017 ECCV paper.

07:33.320 --> 07:39.080
I think got best paper, basically putting new expressions on a person's face, so changing

07:39.080 --> 07:42.040
my expression from an image to be like smiling.

07:42.040 --> 07:46.560
And you've seen like applications of this in apps like FaceApp, you know, that's become

07:46.560 --> 07:48.120
very famous.

07:48.120 --> 07:55.280
So this, you know, image translation network is kind of simple as like neural network that

07:55.280 --> 07:59.560
goes from an image to another image, and you kind of specify what the output you want

07:59.560 --> 08:01.200
it to be, basically.

08:01.200 --> 08:06.360
So, and there's this, all this other work on adversarial attacks, right?

08:06.360 --> 08:09.960
That everyone has been hearing because, you know, you don't want to have an adversarial

08:09.960 --> 08:16.120
attack is basically an imperceptible perturbation on an image that a human being doesn't notice,

08:16.120 --> 08:18.840
but that can completely fool a neural network.

08:18.840 --> 08:23.840
So you know, this thing has been explored since 2013 for classifiers.

08:23.840 --> 08:27.960
So if you had a neural network that tells you, hey, there's a pedestrian in the scene,

08:27.960 --> 08:31.160
you know, or not, then you can fool this classifier.

08:31.160 --> 08:35.240
So everyone is obviously freaked out about the possibility of this, you know, becoming

08:35.240 --> 08:40.560
a reality to attack, you know, neural networks in the wild with this type of thing.

08:40.560 --> 08:44.600
So both of these like kind of fashionable ideas in my head, I think, you know, it just popped

08:44.600 --> 08:45.600
out of nowhere.

08:45.600 --> 08:52.400
So maybe during that conversation, maybe if we attack, you know, an image translation

08:52.400 --> 08:59.200
network, you'll be able to protect your images from being converted into deepfix.

08:59.200 --> 09:07.120
The premise of this work is, you know, it sounds like just like in the adversarial attacks,

09:07.120 --> 09:17.080
you're injecting noise to an image or you're injecting noise against an image and then disrupting

09:17.080 --> 09:23.040
the classifier here, you're trying to inject noise on an image and disrupt the ability

09:23.040 --> 09:28.600
of some generative model to do whatever it's trying to do to manipulate that image.

09:28.600 --> 09:30.320
Is that the general idea?

09:30.320 --> 09:31.320
Yeah, exactly.

09:31.320 --> 09:36.480
So in the classifier scenario, you have, you know, an image that goes into the class,

09:36.480 --> 09:39.680
into the deep neural network, and then you want a class that comes out of it.

09:39.680 --> 09:43.440
So if it's a dog picture, you want it to classify it as a dog, right?

09:43.440 --> 09:49.480
And an attack in that situation would be to make it classify it as, you know, a cat,

09:49.480 --> 09:55.080
for example, so a wrong class or you could be a targeted class like you wanted to actually

09:55.080 --> 09:56.600
always classify it as a cat.

09:56.600 --> 10:01.840
You could do a targeted attack or an untargeted attack to like drive it away from the class

10:01.840 --> 10:02.840
to dog class.

10:02.840 --> 10:06.000
So it could be any other class like, you know, lizard, the one that's closest.

10:06.000 --> 10:08.880
So the closest boundary, basically.

10:08.880 --> 10:16.640
In this case, it's kind of like a more general, it's a harder thing to quantify, right?

10:16.640 --> 10:21.440
If you have like an image translation network that goes from an image of a person, you know,

10:21.440 --> 10:25.520
my face, you know, like with a serious face, and then someone wants me, imagine just like

10:25.520 --> 10:29.280
basically this is one of the types of applications for this is, imagine there's a picture of

10:29.280 --> 10:32.440
me in like a serious situation, right?

10:32.440 --> 10:35.240
And I'm like a political figure or something, right?

10:35.240 --> 10:39.160
And then someone grabs this image, uses animation, which is, you know, it's working right

10:39.160 --> 10:40.160
now.

10:40.160 --> 10:43.200
You could actually use this or or start again or anything or cycle again.

10:43.200 --> 10:46.920
And you and changes my expression into like a smiling expression, right?

10:46.920 --> 10:52.800
That's already directly an image to image deep fake that can have a lot of impact, basically.

10:52.800 --> 10:59.280
And the idea is to, you know, make this type of transformation impossible or basically

10:59.280 --> 11:04.680
the idea of artwork is to make it either obvious or to completely disrupt the output such

11:04.680 --> 11:09.520
that it's too corrupted, it's so easy to notice that it's been corrupted, basically.

11:09.520 --> 11:16.040
So the human observer can be like, can either doubt the source of the image or, you know,

11:16.040 --> 11:20.840
that the image has been manipulated or it can, you know, or it's unusable, basically.

11:20.840 --> 11:26.760
It's like gibberish or black, you know, that's basically the idea.

11:26.760 --> 11:31.720
And so you make it sound so simple, but I'm sure, you know, flipping through the paper,

11:31.720 --> 11:37.680
there's a lot of work that went into this where, you know, what were the challenging parts

11:37.680 --> 11:41.880
and how did you take this from kind of idea to a working model?

11:41.880 --> 11:49.360
Yeah, so I think actually the idea, so what I loved about the idea is that it was so simple

11:49.360 --> 11:51.440
and it was so obviously useful.

11:51.440 --> 11:54.480
That's what really got me excited at first, right?

11:54.480 --> 11:58.440
And it's actually, you know, the first couple of weeks of implementing all of this was actually

11:58.440 --> 12:04.480
not very hard because the main idea of trying to destroy an output, using an adversarial

12:04.480 --> 12:08.000
attack in an image translation network, it's actually a lot of these image translation

12:08.000 --> 12:11.200
networks are very susceptible to attack.

12:11.200 --> 12:15.400
And some of them are a little bit more protected and maybe some of our future work is going

12:15.400 --> 12:20.480
to be on that type of thing, like which architectures have more protection than others?

12:20.480 --> 12:25.520
Why are some difficult to attack and why are some easier to attack, right?

12:25.520 --> 12:29.600
But you know, the paper, so I think a lot of it went on, you know, a lot of the work on

12:29.600 --> 12:34.880
the paper went on showing that this is possible with a lot of different types of architectures,

12:34.880 --> 12:39.240
showing a lot of very good examples that it's like a solid technique basically.

12:39.240 --> 12:45.320
And you know, there are some, so basically this paper is kind of like the first or one

12:45.320 --> 12:51.360
of the first steps into this kind of domain because in general, so for this type of, you

12:51.360 --> 12:55.880
know, attack, it's called a white box attack, where you need to know all of the parameters

12:55.880 --> 13:00.080
of the neural network and you need to know the neural network that they're using, right?

13:00.080 --> 13:05.320
So that's a big, you know, kind of if in the real world this, but definitely this could

13:05.320 --> 13:10.280
work at this moment, you know, because a lot of attacks or a lot of deep fakes, I'm

13:10.280 --> 13:13.040
sorry, are very low effort.

13:13.040 --> 13:18.280
So a lot of things happen where someone puts an architecture online on GitHub, right?

13:18.280 --> 13:23.680
And then promotes it on Reddit, for example, and then a lot of people go and use this architecture

13:23.680 --> 13:25.560
with this like pre-trained model.

13:25.560 --> 13:28.840
And then, you know, create defects with this architecture, right?

13:28.840 --> 13:33.880
So that's kind of low effort that can already be preempted by using this technology, because

13:33.880 --> 13:37.800
you know, the way that script treaties, I don't know if anyone uses that term anymore.

13:37.800 --> 13:42.960
I think we should make a new one because it's the idea that like, you know, with hacking

13:42.960 --> 13:46.920
folks would just, you know, download some Pearl script or whatever and run it against

13:46.920 --> 13:49.880
some site to find, you know, vulnerabilities.

13:49.880 --> 13:50.880
Yeah, exactly.

13:50.880 --> 13:55.720
And you still had, it's funny because, you know, I guess SQL injections were, you know,

13:55.720 --> 14:01.400
were so easy to exploit in the early days, but even until like, you know, like 10, 10

14:01.400 --> 14:06.760
years ago or something, like people were still finding SQL injection vulnerabilities.

14:06.760 --> 14:12.360
And so script kitties kind of like were still able to like work in the real world.

14:12.360 --> 14:17.080
So I think this kind of, you know, we should make a new term maybe like DL kitties or something,

14:17.080 --> 14:18.080
right?

14:18.080 --> 14:19.080
It's just people.

14:19.080 --> 14:21.800
You're here first.

14:21.800 --> 14:26.200
People trying to, trying to use like out of the box kind of stuff on, transfer learning

14:26.200 --> 14:27.200
kitties.

14:27.200 --> 14:28.200
Yeah.

14:28.200 --> 14:29.200
That's a good thing.

14:29.200 --> 14:30.200
Pre-trained model kitties.

14:30.200 --> 14:31.200
There's something in there somewhere.

14:31.200 --> 14:37.360
But at the same time, you know, I always, like, I respect anyone that tries to use these

14:37.360 --> 14:38.360
things out of the box.

14:38.360 --> 14:39.440
That's the first step, right?

14:39.440 --> 14:43.960
Out of the box from, you know, wherever you can find, put your hands on any of this technology

14:43.960 --> 14:46.320
and then start using it and try to learn from it.

14:46.320 --> 14:49.480
If it's not for doing something bad, right?

14:49.480 --> 14:52.560
If it's just to learn, I encourage everyone, right?

14:52.560 --> 14:57.200
To just go on on GitHub, try a bunch of stuff and then see if they can modify it.

14:57.200 --> 14:59.240
Like that's the way forward, right?

14:59.240 --> 15:05.200
But yeah, if you are doing deep fakes for an immoral purpose, that's definitely not good.

15:05.200 --> 15:10.880
Yeah, but this technology, so just for this specific thing, is already usable because

15:10.880 --> 15:16.520
we're able to, you know, find you're probably going to have the access to the, to the weights

15:16.520 --> 15:18.360
and to the architecture.

15:18.360 --> 15:22.720
But this is a very like constrained kind of attack in some sense.

15:22.720 --> 15:26.000
The more advanced types of attacks are called black box attacks.

15:26.000 --> 15:29.760
You could be like gray box where you know the architecture, but you don't know the weights

15:29.760 --> 15:33.120
of the architecture or like different types, there's different types of settings for

15:33.120 --> 15:34.120
that.

15:34.120 --> 15:38.480
But the black box, which is the most powerful, is just you just have a black box where

15:38.480 --> 15:42.360
you send an image and then you get an image back where you get, you send an image and

15:42.360 --> 15:45.280
you get like a class back dog or something.

15:45.280 --> 15:49.400
And then you're trying to kind of attack it without knowing almost anything about it.

15:49.400 --> 15:53.760
I think that's really interesting and that's like where the promise lies in this because

15:53.760 --> 15:59.360
if someone can find very good black box attacks against a lot of these methods, then you'll

15:59.360 --> 16:02.560
definitely have something that that can work in the real world.

16:02.560 --> 16:10.640
Yeah, I mean, you almost envision a scenario where, you know, a Facebook or Twitter, Google,

16:10.640 --> 16:16.600
like when you upload a photo, it's applying this method to all of your photos so that there

16:16.600 --> 16:21.160
are not susceptible to, you know, being used for in a various purposes.

16:21.160 --> 16:22.160
100%.

16:22.160 --> 16:23.160
Yeah, yeah.

16:23.160 --> 16:26.360
And that's like the first kind of application that I had in mind.

16:26.360 --> 16:31.360
The first application that I had in mind was actually for celebrities where, you know,

16:31.360 --> 16:36.640
their likeness is, you know, so, you know, it actually has a big value, right?

16:36.640 --> 16:41.320
And a lot of people are targeting their pictures to try to modify them.

16:41.320 --> 16:46.360
And that type of scenario where you can, you know, maybe, you know, they can buy some

16:46.360 --> 16:48.640
type of service that protects their images.

16:48.640 --> 16:52.720
And then, yeah, if you can generalize it, then you can have any type of platform online

16:52.720 --> 16:54.040
that does this.

16:54.040 --> 16:57.080
Yeah, there's just so many interesting things, right?

16:57.080 --> 17:02.520
You could give permission to someone to use your picture and then to, like, modify your

17:02.520 --> 17:04.120
picture, right?

17:04.120 --> 17:08.360
And then there are some, you know, technologies that we've been thinking of in that direction

17:08.360 --> 17:09.360
as well.

17:09.360 --> 17:11.920
Lots of blockchain opportunities in there.

17:11.920 --> 17:17.840
I feel like, yeah, I think a lot of people kind of jump to blockchain when they're thinking

17:17.840 --> 17:24.280
about protecting images of people or protecting media, but that's one way.

17:24.280 --> 17:25.280
This is another way.

17:25.280 --> 17:30.880
And blockchain, when I want to make a joke about overhyped technologies, yeah, I mean,

17:30.880 --> 17:33.880
we all fall into overhyped sometimes.

17:33.880 --> 17:34.880
That's definitely true.

17:34.880 --> 17:40.080
But I also think, I don't know, sometimes it brings attention that maybe will fade, but

17:40.080 --> 17:44.160
it's still good that these kinds of domains are having a lot of attention because they

17:44.160 --> 17:46.280
have a lot of promise.

17:46.280 --> 17:50.640
And the people that stick with it are the ones that are going to make things happen.

17:50.640 --> 17:52.320
And then the people that leave, right?

17:52.320 --> 17:57.840
Okay, well, they left, and if they're not here to, like, make something happen, then

17:57.840 --> 18:00.760
that's, you know, their issue, basically.

18:00.760 --> 18:03.720
So is the noise that you're injecting?

18:03.720 --> 18:10.240
Is it parameterized with a single, like a single epsilon value or their characteristics

18:10.240 --> 18:15.920
to the noise multiple, you know, characteristics that you're manipulating to make it work with

18:15.920 --> 18:18.160
a particular model?

18:18.160 --> 18:19.160
Yeah.

18:19.160 --> 18:26.860
So the main attacks that we propose, and the paper is not just attacking image translation

18:26.860 --> 18:28.160
systems for deepfakes.

18:28.160 --> 18:29.160
Okay.

18:29.160 --> 18:31.280
It's, it has a lot of other stuff.

18:31.280 --> 18:34.800
For example, we have, you know, you have the image translation case, but you also have

18:34.800 --> 18:38.080
the conditional case where I don't want to distract of your question, but I just want

18:38.080 --> 18:43.720
to say that we have basically several contributions that make it a little bit more than just, oh,

18:43.720 --> 18:46.920
this is, you know, a neat, neat little idea, right?

18:46.920 --> 18:53.280
Because I think the first kind of idea is, is cool, but also the, all the contributions

18:53.280 --> 18:58.160
that we do make this, like, a, kind of, like, bigger kind of type of work.

18:58.160 --> 19:02.800
Because the main idea of attacking these image translation systems, it's actually not

19:02.800 --> 19:08.600
that complicated to adapt some of the techniques that we already had, like FGSM, FastGrading

19:08.600 --> 19:15.640
Sign Method, iterative FGSM, basically FGSM is just kind of taking a step in the direction

19:15.640 --> 19:21.000
of the, of the gradient for, for your image and then modifying your image so that you

19:21.000 --> 19:24.560
go away from the class that you want to classify this thing as.

19:24.560 --> 19:26.360
That's for the classifier case, right?

19:26.360 --> 19:32.880
In our case, you have the, this loss, which is, you can have, like, your main metric is,

19:32.880 --> 19:37.160
what happens if you translate the image without any attack, you have kind of, like, a ground

19:37.160 --> 19:38.160
truth output, right?

19:38.160 --> 19:42.040
Like, this is what the picture is going to look without any attack.

19:42.040 --> 19:47.080
And then you have a translation of your image, so this is the thing that we can modify,

19:47.080 --> 19:50.800
because the translation of your image within attack, right?

19:50.800 --> 19:52.320
That's the thing that we can modify.

19:52.320 --> 19:56.640
And now you want the difference between this ground truth output and this attacked output

19:56.640 --> 19:59.880
to be as big as possible, and with some kind of metric, right?

19:59.880 --> 20:04.200
And we use L2 in the formulation of our attacks, but you could use, you know, a lot of different

20:04.200 --> 20:05.200
type of metrics.

20:05.200 --> 20:11.440
And we use, like, an image, image level metric, which goes, like, pixel pixel to pixel differences

20:11.440 --> 20:14.400
in the, using L2, right?

20:14.400 --> 20:15.760
And then you want to maximize this, right?

20:15.760 --> 20:20.440
You want your output, the attacked output to be as different as possible as the ground

20:20.440 --> 20:22.520
truth output would have been.

20:22.520 --> 20:23.800
So you can actually just do this.

20:23.800 --> 20:26.440
It's just an optimization, you know, problem.

20:26.440 --> 20:33.680
Yeah, I guess one of the things that jumps to mind is as opposed to, like, in L2, distance,

20:33.680 --> 20:38.280
maximizing the ability to fool some kind of discriminator network.

20:38.280 --> 20:45.320
So we don't actually have to do that at all, actually, because, so to train these types

20:45.320 --> 20:49.360
of architectures, yeah, you have a generator that does the image translation, right?

20:49.360 --> 20:53.760
And then you have a discriminator and this type of game between the discriminator that's

20:53.760 --> 20:59.040
creating the translation and the, the generators already and the discriminator that's trying

20:59.040 --> 21:05.240
to detect whether it's a real or fake image is actually what makes the output so good

21:05.240 --> 21:10.720
and makes it, you know, approximate these distributions so well without, without too

21:10.720 --> 21:11.720
much blurriness, et cetera.

21:11.720 --> 21:15.360
You know, that's the power of Gans, but we don't even need, you know, we kind of like

21:15.360 --> 21:20.000
throw away the discriminator in this process, just use the generator and it just becomes

21:20.000 --> 21:22.880
like an optimization problem.

21:22.880 --> 21:33.640
Maybe the layer behind my question was, is, you know, the, your L2 distance is really

21:33.640 --> 21:40.520
a proxy for perceived difference from the actual face and I'm wondering like how good

21:40.520 --> 21:47.120
L2 is for, you know, really, you know, making the generated image far away from the face

21:47.120 --> 21:52.480
or, you know, perceived, the level of perceived distortion.

21:52.480 --> 21:56.640
And so that's where I thought maybe like some kind of discriminator train discriminator

21:56.640 --> 22:00.320
thing could be better than L2, but that's a really good question, actually.

22:00.320 --> 22:04.480
I think, you know, you could think of, of, like an L2 distance, you could, you could

22:04.480 --> 22:08.880
think of, you know, the attack making just the image just a little bit brighter and then

22:08.880 --> 22:11.520
the L2 distance would go up, right?

22:11.520 --> 22:17.000
But, but, so you would have a higher L2 distance between these two, but a human being would

22:17.000 --> 22:20.800
be like, you know, it's a little bit brighter, but it's kind of still the same picture.

22:20.800 --> 22:24.000
So yeah, definitely, it's not the perfect thing in the paper.

22:24.000 --> 22:31.040
We explain why, and on average, you know, it's, it's a good metric to use and we show examples.

22:31.040 --> 22:35.320
So we didn't really just go through the L1, L2 metric and be like, yeah, it's high.

22:35.320 --> 22:37.400
So it's, it's working, right?

22:37.400 --> 22:41.960
We looked at a lot of qualitative examples, but the second, you know, the second step

22:41.960 --> 22:46.600
that you can take after this is definitely, and I, you know, encourage anyone that wants

22:46.600 --> 22:50.800
to try this out, actually, to try it out because I'm trying other things.

22:50.800 --> 22:58.640
But if anyone wants to use like a perceptual metric, so, you know, we have like VGG, for

22:58.640 --> 23:04.800
example, VGG trained on faces is a pretty good kind of proxy, I mean, it's kind of weird

23:04.800 --> 23:09.760
to say, but it's kind of a proxy of perceptual, of a perceptual metric.

23:09.760 --> 23:16.120
So two faces that are similar in this like VGG 16 feature space are, you know, you

23:16.120 --> 23:20.840
can actually kind of cluster then better in this, in this type of sense, instead of using

23:20.840 --> 23:21.840
L2 metrics.

23:21.840 --> 23:25.680
So, yeah, having a higher distance with a VGG, which is kind of like a discriminator

23:25.680 --> 23:28.000
in some sense, it's a neural network, right?

23:28.000 --> 23:32.760
Or you could do what you're saying is you could say, is this image, you know, you could

23:32.760 --> 23:36.680
have like discriminator, I just had this idea right now, actually, you could have a discriminator

23:36.680 --> 23:40.880
that tells you, oh, this image is more or less distorted, right?

23:40.880 --> 23:45.320
And you want, you train your discriminator to detect distorted images and you train your

23:45.320 --> 23:47.360
generator to distort the images.

23:47.360 --> 23:51.800
And then you could have like a game in that way, maybe with a third or second discriminator

23:51.800 --> 23:52.800
as well.

23:52.800 --> 23:57.320
But definitely like that's one of the big questions of systematizing this is it's easier

23:57.320 --> 24:02.120
to systematize with classifiers because you know when the class is wrong, there's just

24:02.120 --> 24:03.120
a number, right?

24:03.120 --> 24:06.280
The class is four, but we wanted five, so it's wrong, right?

24:06.280 --> 24:11.480
In this case, you have something and that's the problem that I've been like bumping into

24:11.480 --> 24:17.600
all over this kind of area is you're talking about human perception and trying to kind

24:17.600 --> 24:23.040
of like model a reaction of a human being, right, or what the human being perceives in

24:23.040 --> 24:24.040
an image.

24:24.040 --> 24:29.320
And that's, you know, much more complex for generating faces and for disrupting defects,

24:29.320 --> 24:30.320
right?

24:30.320 --> 24:39.400
So it started a couple of questions ago asking about like the how the noise is parameterized

24:39.400 --> 24:48.280
and I think the question behind that question was, you know, is there, you know, some way

24:48.280 --> 24:53.240
to just crank the noise up as much as you can before the image starts looking distorted

24:53.240 --> 25:00.680
and use that as a way, you know, does that get you closer to a gray box or a black box

25:00.680 --> 25:07.000
type of scenario or do you really have to, are there, you know, how nuanced is the, does

25:07.000 --> 25:11.480
the noise have to be to defeat a particular system?

25:11.480 --> 25:13.600
Yeah, that's actually a great question.

25:13.600 --> 25:17.160
One thing that I really wanted to say that I didn't get a chance to say before we started

25:17.160 --> 25:19.960
is this is not just my work, right?

25:19.960 --> 25:25.120
Like this is work with really amazing collaborators at BU.

25:25.120 --> 25:31.080
So a recent, a research assistant professor, Sarah Del Bargel, who helped me so much with

25:31.080 --> 25:35.480
this project and then my advisor Stan Sclyroff, right, and all of these kind of ideas have

25:35.480 --> 25:40.000
been like discussed with them and, you know, we've all like put so much, you know, work

25:40.000 --> 25:41.560
into this, right?

25:41.560 --> 25:46.960
So, okay, moving on from that, basically, yeah, this is kind of like, that question is really

25:46.960 --> 25:52.200
good because one of the things that we did was try it on, so first of all, just try to

25:52.200 --> 25:56.520
see how sensitive any of these architectures is to just random noise, right?

25:56.520 --> 26:01.160
Like I'll say no to something and some of them are super sensitive.

26:01.160 --> 26:06.120
If you, I don't really want to say which ones, right, because I don't want to just like

26:06.120 --> 26:09.000
single out any any, and it's not their fault, right?

26:09.000 --> 26:12.760
But some of some architectures, if you inject just a little bit of random noise, then you

26:12.760 --> 26:16.280
can have very big perturbations in the output image.

26:16.280 --> 26:20.200
That was the first step and some of them are very resistant to noise.

26:20.200 --> 26:24.040
And then this finding holds to the adversarial attack case.

26:24.040 --> 26:31.960
So an adversarial attack is just a sort of structured noise that is structured using,

26:31.960 --> 26:39.280
you know, basically you use the gradient of the network to get the biggest type of disruption

26:39.280 --> 26:41.760
in the output possible, right?

26:41.760 --> 26:48.600
So that doing an adversarial attack of same magnitude would be way more effective than

26:48.600 --> 26:51.720
just being random noise of that same magnitude.

26:51.720 --> 26:55.000
But some architectures are just really sensitive to noise.

26:55.000 --> 26:57.520
That's another lesson of this, I think.

26:57.520 --> 27:02.960
Your initial response to that question was talking about the broader contributions to this

27:02.960 --> 27:08.920
paper beyond the kind of the simple, deep fake disruption mechanism.

27:08.920 --> 27:14.040
If that's the right way to characterize it, walk us through the, you know, what you think

27:14.040 --> 27:17.600
are the biggest contributions here.

27:17.600 --> 27:21.200
This idea of attacking image translation systems is pretty natural.

27:21.200 --> 27:25.960
But then there's some specific, you know, specificity of deep fake kind of image translation

27:25.960 --> 27:26.960
networks.

27:26.960 --> 27:32.560
And one of them is that you have, you're always, you almost always have a class or their,

27:32.560 --> 27:34.760
you know, conditional image translation networks.

27:34.760 --> 27:38.920
And your condition could be or your class could be, for example, in animation, you have

27:38.920 --> 27:43.280
the action units, which are, you know, small movements of the phase that could like correspond

27:43.280 --> 27:46.480
to like smiling or moving your lips upward, et cetera, right?

27:46.480 --> 27:49.760
And then by combining them, you create expressions.

27:49.760 --> 27:53.240
So these, these networks are conditional networks.

27:53.240 --> 27:57.080
And you want to kind of attack them irrespective of the class.

27:57.080 --> 28:01.200
So my attack doesn't have to, if, you know, let's just say that you don't want it to

28:01.200 --> 28:03.600
only work on the smiling faces or something.

28:03.600 --> 28:04.600
Exactly.

28:04.600 --> 28:10.080
Like, yeah, maybe this person is going to make everyone smile and pictures, but, you

28:10.080 --> 28:14.240
know, you don't even know what the other person's going to do, maybe close your eyes, right?

28:14.240 --> 28:18.440
So yeah, for example, if you target an attack towards, and in some of these architectures

28:18.440 --> 28:23.400
you're tired to attack towards one class, then it doesn't transfer to other classes as

28:23.400 --> 28:24.400
well.

28:24.400 --> 28:27.800
And in some of them, it's just an attack for one class, just transfers completely to the

28:27.800 --> 28:28.800
others.

28:28.800 --> 28:33.360
And I think actually these two kind of properties, like the fragility of an architecture

28:33.360 --> 28:37.240
to noise and this type of transfer are actually related.

28:37.240 --> 28:44.320
That sounds kind of interesting and that is it kind of saying that the fragility isn't

28:44.320 --> 28:52.000
necessarily an architectural trait, but more specific, like a trait of the weights of an

28:52.000 --> 28:53.000
architecture.

28:53.000 --> 28:54.000
Yeah.

28:54.000 --> 28:55.000
I think so.

28:55.000 --> 29:00.280
I think the fragility of an architecture to attack is actually, that's what I'm kind

29:00.280 --> 29:06.120
of discovering now, is tied not only to the type of architecture, but to the weights.

29:06.120 --> 29:09.720
And if you think about it, the weights is just a function of the training data and the

29:09.720 --> 29:12.880
training kind of process, right?

29:12.880 --> 29:16.360
So the optimization process and the training data.

29:16.360 --> 29:20.160
So these two things are pretty important, I would say.

29:20.160 --> 29:25.280
And it depends how important they are for each kind of architecture.

29:25.280 --> 29:28.960
And it's still like, this is, you know, complete.

29:28.960 --> 29:33.600
This is kind of like an intuition a little bit from what I've seen in my experiments, but

29:33.600 --> 29:38.200
I think there's a lot of like super interesting work to be done here, because I think adversarial

29:38.200 --> 29:43.760
attacks actually are very exciting to me, and I've just gotten into them a little bit later.

29:43.760 --> 29:50.480
But you know, I've always like kind of wondered like, how exactly do deep models work in some

29:50.480 --> 29:51.480
sense?

29:51.480 --> 29:54.040
Like, why do they fail in certain cases and they don't fail in others?

29:54.040 --> 29:58.840
You know, this type of kind of explanatory process of the failures of a network or how

29:58.840 --> 30:02.760
to make it better, you know, or, you know, intuitions and how to make it better.

30:02.760 --> 30:07.880
So I think adversarial attacks are actually like a great window into fragility of an

30:07.880 --> 30:10.600
explanation of these neural networks.

30:10.600 --> 30:14.640
So that's one cool thing about this is by attacking image translation systems, I can

30:14.640 --> 30:19.440
actually see in their output, like when I attack them, what they are doing.

30:19.440 --> 30:20.440
A lot of them.

30:20.440 --> 30:26.800
So, for example, you know, this is something good for a pretty specific, I think, but for

30:26.800 --> 30:33.560
StarGAN, if you attack it, then you have the whole image that changes all at once, and

30:33.560 --> 30:38.400
you have other architecture such as animation that are very targeted towards certain parts

30:38.400 --> 30:40.480
of the image that they're modifying.

30:40.480 --> 30:45.360
So one architecture has learned how to kind of like change the whole frame at once, and

30:45.360 --> 30:50.840
then one architecture has learned to do more like fine-grained types of changes inside

30:50.840 --> 30:51.840
of an image.

30:51.840 --> 30:54.880
And one is, you know, animations more robust than StarGAN.

30:54.880 --> 31:01.360
So it's almost along the lines of work like lime and other things where you're perturbing

31:01.360 --> 31:09.200
inputs or features and seeing how the network responds to the aim of understanding explainability

31:09.200 --> 31:11.040
or producing an explanation.

31:11.040 --> 31:17.920
You know, this is, you're kind of almost at, you know, trying to explain or understand

31:17.920 --> 31:22.320
these networks through the disruptions that you're injecting.

31:22.320 --> 31:23.800
Yeah, I think so.

31:23.800 --> 31:24.800
I don't know.

31:24.800 --> 31:29.440
Actually, this work, you know, I actually don't know about that work that you just mentioned,

31:29.440 --> 31:31.720
you know, going on from your explanation.

31:31.720 --> 31:35.160
I don't know if that has been done, maybe it's been done for like classifiers, but I don't

31:35.160 --> 31:37.320
know if it's been done for image translation networks.

31:37.320 --> 31:42.520
And I think that's like a huge frontier that this kind of opens is to try to understand

31:42.520 --> 31:43.520
what's going on.

31:43.520 --> 31:48.000
And then if you know, so for example, just, you know, example of the bad, if you know

31:48.000 --> 31:54.200
that your image translation network is changing the whole frame when you actually just need

31:54.200 --> 31:59.200
to change the hair color of the person, then you could think of this as a weakness,

31:59.200 --> 32:00.200
right?

32:00.200 --> 32:02.800
And then you could think of how do I change the architecture of the training procedure

32:02.800 --> 32:04.440
to correct this, right?

32:04.440 --> 32:07.480
And there are techniques to do it, which are, which are pretty cool, I think.

32:07.480 --> 32:10.760
So yeah, definitely it's delving into this type of explainability.

32:10.760 --> 32:16.360
I guess that's also fashionable right now, but I think it's a huge thing.

32:16.360 --> 32:20.680
My lab does a lot of, of, of, of explainability and deep networks.

32:20.680 --> 32:21.680
Okay.

32:21.680 --> 32:22.680
Okay.

32:22.680 --> 32:29.320
And so this kind of broader understanding of these architectures is another contribution

32:29.320 --> 32:30.320
of the paper.

32:30.320 --> 32:31.320
What else?

32:31.320 --> 32:32.320
Yeah.

32:32.320 --> 32:37.520
So yeah, yeah, we were on that question kind of easy to get lost.

32:37.520 --> 32:41.880
So that was, that was one of one of our contributions is conditional image translation

32:41.880 --> 32:46.320
networks and, and doing, building attacks that generalize to all of the different types

32:46.320 --> 32:48.960
of, of, of classes.

32:48.960 --> 32:52.120
So yeah, it doesn't just work if someone's trying to put a smile on your face.

32:52.120 --> 32:56.000
It also works when you're, when someone's trying to close your eyes in an image, right?

32:56.000 --> 33:01.360
Or it doesn't just work if someone's trying to make you, you know, blonde or something.

33:01.360 --> 33:06.640
It works also when someone tries to make your hair darker or something.

33:06.640 --> 33:11.000
So that's, that's one thing that the other thing is, so kind of like, okay, now we have

33:11.000 --> 33:14.600
an attack typical in this kind of area is you have an attack.

33:14.600 --> 33:16.040
What are the defenses, right?

33:16.040 --> 33:21.840
Like what is someone that has like an actual like beneficial image translation network?

33:21.840 --> 33:24.240
What can they do to defend against this type of attack?

33:24.240 --> 33:28.520
Because you could also think about the scenario where, you know, in this scenario, you're

33:28.520 --> 33:30.200
trying to, you know, obstruct deepfake.

33:30.200 --> 33:33.920
So you're trying to obstruct something that is done without permission of the users,

33:33.920 --> 33:34.920
and that can be malicious.

33:34.920 --> 33:39.720
But you can imagine a scenario where someone attacks, let's say, I don't know, just off

33:39.720 --> 33:46.280
the top of my head, like, let's say you have like an X-ray and, and you have like an image

33:46.280 --> 33:51.000
translation network that makes some zones more visible to a surgeon or whatever, right?

33:51.000 --> 33:52.920
Like it's hard to come up with an example right now.

33:52.920 --> 33:57.720
But going with that example, you can imagine a malicious actor introducing one of our attacks

33:57.720 --> 34:03.040
or something like this to this X-ray to make the output not work, right?

34:03.040 --> 34:07.320
So what can a person do to defend against this?

34:07.320 --> 34:14.640
And one of the defenses that holds up to all the scrutiny is adversarial training.

34:14.640 --> 34:19.720
I think it done by Madrid all and 2017 that I think that's the paper.

34:19.720 --> 34:24.080
So basically the idea is just it's a very, it's a very simple, very powerful idea.

34:24.080 --> 34:28.920
You have PGD, which is a very strong attack projected gradient descent, and you just

34:28.920 --> 34:33.400
augment your data set with a lot of images that have been trained, that have been attacked

34:33.400 --> 34:34.960
using PGD.

34:34.960 --> 34:42.040
And then you train your neural network with those images, and it gets, it becomes a, you

34:42.040 --> 34:46.880
know, more robust to these types of adversarial attacks in this type of sense that I've been

34:46.880 --> 34:50.040
explaining.

34:50.040 --> 34:55.800
And this also is, so a lot of these defense mechanisms or, you know, defenses against

34:55.800 --> 35:01.520
adversarial attacks are so hard to make because in security, your defense has to be valid

35:01.520 --> 35:05.800
even when an attacker knows the defense you're going to use.

35:05.800 --> 35:10.960
So a lot of defenses fall apart in this, in the scenario, and this is one that that doesn't

35:10.960 --> 35:12.600
or hasn't at this point.

35:12.600 --> 35:15.440
So one of the things is it's not foolproof.

35:15.440 --> 35:16.880
You can still attack the network.

35:16.880 --> 35:21.280
So we're able to do this adversarial training for GANs, and we have like these formulations

35:21.280 --> 35:26.760
because you can, you can, you can train the, the generator with adversarial noise.

35:26.760 --> 35:29.680
But you can also train the generator and the discriminator.

35:29.680 --> 35:34.640
So you attack both the real image, but you attack the real and the fake image.

35:34.640 --> 35:36.920
So, so there's like different ways of doing it.

35:36.920 --> 35:41.280
The most powerful way of doing it is, is doing the generator plus the discriminator adversarial

35:41.280 --> 35:45.600
training, and it does defend against some certain types of attacks.

35:45.600 --> 35:50.600
But so it makes it more robust, but in the end, if we have a very strong attack, it's,

35:50.600 --> 35:52.400
it's still pretty successful.

35:52.400 --> 35:54.360
That's one thing that we learned.

35:54.360 --> 35:59.240
And in the future, more investigation in this is kind of needed to see exactly, you know,

35:59.240 --> 36:03.720
how much robustness it does bring, actually.

36:03.720 --> 36:07.080
Oh, yeah, and the last one.

36:07.080 --> 36:08.080
Yes.

36:08.080 --> 36:13.160
Oh my god, I forgot.

36:13.160 --> 36:18.600
I think all of these are pretty interesting and, you know, I would like to work on all of

36:18.600 --> 36:19.600
them.

36:19.600 --> 36:20.600
It's impossible, right?

36:20.600 --> 36:26.800
So that this one is, the last one is in a certain scenario where you can blur the,

36:26.800 --> 36:30.360
so my advisor just told me, like, oh, okay, you can attack these images.

36:30.360 --> 36:31.360
Great.

36:31.360 --> 36:35.160
But what happens in the, in the real scenario, if you're like a malicious actor, you run

36:35.160 --> 36:39.200
into one of these images and you're like, I suspect that this one's attacked, right?

36:39.200 --> 36:44.200
So I'll just blur it a little bit with, you know, a Gaussian blur or an average blur

36:44.200 --> 36:45.200
or something.

36:45.200 --> 36:49.920
And then maybe since the attack is high frequency structure noise on top of the image,

36:49.920 --> 36:52.680
then maybe this will destroy the attack.

36:52.680 --> 36:56.160
And so that's one thing in this, in our scenario, it's, it's different than the classifier

36:56.160 --> 36:57.160
scenario.

36:57.160 --> 37:02.960
In our scenario, we have to kind of also be careful of this kind of like gray box scenario

37:02.960 --> 37:06.480
where we don't, we know maybe the architecture that they're using and the, and the weights

37:06.480 --> 37:10.560
that they're using, but we don't know what pre-processing they're using.

37:10.560 --> 37:15.280
So there are some ideas in this domain like expectation over transformation where you

37:15.280 --> 37:20.720
just grab kind of like the expectation of all of the losses through, with all of the

37:20.720 --> 37:22.560
transformations that you think of.

37:22.560 --> 37:26.280
And in that paper, they did a cropping and rotating, but they didn't do blurring.

37:26.280 --> 37:31.680
So I think our paper is one of the, you know, to the best of my knowledge, it's one that

37:31.680 --> 37:37.760
is, one that addresses blurring in this type of scenario of transferability across blurring.

37:37.760 --> 37:42.120
But also, yeah, because we noticed that blurring is actually really effective.

37:42.120 --> 37:47.880
You blur and naive attack just a little bit, then you can definitely translate the image.

37:47.880 --> 37:51.880
And there's almost no downside because the output image looks as good.

37:51.880 --> 37:58.840
And when we, when you say attack here, are you speaking in the sense of kind of traditional

37:58.840 --> 38:09.080
adversarial attack or, and or, you know, your work where you're trying to prevent manipulation,

38:09.080 --> 38:12.080
which the manipulation is kind of an attack in this sense.

38:12.080 --> 38:13.080
Yeah.

38:13.080 --> 38:19.880
The terminology is actually what we tried to do is, is deep fake and deep faker, right?

38:19.880 --> 38:24.000
That's, or manipulate, you know, and that's the person that's trying to create the deep

38:24.000 --> 38:30.600
fake and then attacker, an attack and disruption, right, is kind of the, the person that's trying

38:30.600 --> 38:35.200
to, yeah, do an adversarial attack on the image to prevent the manipulation of it.

38:35.200 --> 38:39.760
So in some sense, the attacker is defending against something done onto them in this scenario,

38:39.760 --> 38:40.760
right?

38:40.760 --> 38:44.000
That's kind of hard to keep the terms, yeah.

38:44.000 --> 38:47.840
And then the funny thing is like defense is actually the deep faker that's trying to

38:47.840 --> 38:50.800
defend against, you know, the attacker, right?

38:50.800 --> 38:51.800
Right.

38:51.800 --> 38:56.640
And in this case, actually, so in the, in the paper for the blurring thing, we propose

38:56.640 --> 39:01.160
kind of like a different kind of iterative heuristic method, which is faster than expectation

39:01.160 --> 39:07.480
over transformation, but as effective, for at least for our scenario and the, and that

39:07.480 --> 39:11.120
experiment that we did, expectation over transformation is great work and all of these,

39:11.120 --> 39:16.040
you know, honestly, all of the works that I saw in that in, in this paper are just, are

39:16.040 --> 39:20.440
just really great and just like big steps in, in this kind of field.

39:20.440 --> 39:23.920
And I respect all those people like immensely.

39:23.920 --> 39:29.080
So it's just hard to list all of the work that has kind of influenced what we do.

39:29.080 --> 39:35.440
So the paper kind of explores these areas and what didn't we cover yet?

39:35.440 --> 39:37.520
I mean, I think, I think we covered everything.

39:37.520 --> 39:41.960
One of the things is that maybe neglected to say is that it's, this is another really

39:41.960 --> 39:43.840
interesting kind of thing about the papers.

39:43.840 --> 39:48.640
So if you have these blurring things, so if you have different types of blur, you can

39:48.640 --> 39:50.920
have like different magnitudes of the blur, right?

39:50.920 --> 39:55.280
And every time you're attacking a different kind of blur type, you're attacking at a different

39:55.280 --> 40:01.920
type of kind of like scale, or if you think about it, you're adding like higher, higher

40:01.920 --> 40:05.880
frequency noise and then you're adding lower and lower frequency noise.

40:05.880 --> 40:08.960
So that's why we call, so our attack is called spread spectrum attack.

40:08.960 --> 40:13.800
It kind of is inspired in a spread spectrum water marking where you could put a water

40:13.800 --> 40:19.480
mark in a lot of different in the frequency domain, a lot of different kind of in the frequency

40:19.480 --> 40:25.960
band, basically, not just not just in one, but just in in a spread, you know, spread spectrum

40:25.960 --> 40:26.960
manner.

40:26.960 --> 40:31.320
This is kind of the idea of of that defense that we, though we present in the paper.

40:31.320 --> 40:32.320
Yeah.

40:32.320 --> 40:36.080
And just, I don't know, just for future work, there's so much interesting stuff there.

40:36.080 --> 40:40.800
Are you continuing work on this or are you working on other projects now?

40:40.800 --> 40:46.280
Yeah, definitely. So I tried to like double my efforts in this because I feel like this

40:46.280 --> 40:53.000
is one of the project that has really given me, you know, a lot of ideas and different,

40:53.000 --> 40:54.760
and so many different directions.

40:54.760 --> 40:57.880
So it's actually a little bit stressful because there's maybe a little, a lot of ground

40:57.880 --> 40:58.880
to cover.

40:58.880 --> 41:04.080
And if anyone is listening to this and wants to collaborate, yeah, just send me an email.

41:04.080 --> 41:08.600
You can find it on the, on the paper and then and we can set up a collaboration because

41:08.600 --> 41:13.040
I think, you know, there's at least like three to four directions that are very different,

41:13.040 --> 41:14.320
but also super interesting.

41:14.320 --> 41:19.400
Well, in a 10 year, thanks so much for taking the time to share a bit about what you're

41:19.400 --> 41:20.400
working on.

41:20.400 --> 41:21.400
Yeah.

41:21.400 --> 41:22.400
Thanks so much.

41:22.400 --> 41:26.920
This is a great opportunity and very, you know, best of luck with all of your next shows

41:26.920 --> 41:29.640
and with all of this craziness that's happening right now, right?

41:29.640 --> 41:30.640
Yeah.

41:30.640 --> 41:31.640
Yeah.

41:31.640 --> 41:32.640
Yeah.

41:32.640 --> 41:33.640
Yeah.

41:33.640 --> 41:34.640
How are things for you?

41:34.640 --> 41:35.640
You're staying, staying inside, staying safe.

41:35.640 --> 41:36.640
All that.

41:36.640 --> 41:42.240
Absolutely, like everyone should stay inside as much as possible and just go outside to

41:42.240 --> 41:45.240
shop or, or, you know, as much as you can, right?

41:45.240 --> 41:48.760
As much as, like, allows again, yeah.

41:48.760 --> 41:52.720
But definitely here in Boston, there's nothing going on, everything's closed and BU has

41:52.720 --> 41:54.320
been closed, you know, I don't know.

41:54.320 --> 41:58.440
Maybe we'll be like this for, uh, for around, you know, four months, three months.

41:58.440 --> 41:59.840
Yeah, who knows.

41:59.840 --> 42:00.840
Awesome.

42:00.840 --> 42:03.480
Well, thanks so much and, uh, again, take care.

42:03.480 --> 42:04.480
All right.

42:04.480 --> 42:05.480
Have a good one.

42:05.480 --> 42:06.480
Yeah.

42:06.480 --> 42:07.480
Yeah.

42:07.480 --> 42:08.480
Yeah.

42:08.480 --> 42:09.480
All right, everyone.

42:09.480 --> 42:11.400
That's our show for today.

42:11.400 --> 42:17.200
For more information on today's show, visit twomolai.com slash shows.

42:17.200 --> 42:40.120
As always, thanks so much for listening and catch you next time.

