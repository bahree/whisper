WEBVTT

00:00.000 --> 00:20.480
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:20.480 --> 00:25.560
people doing interesting things in machine learning and artificial intelligence.

00:25.560 --> 00:27.840
I'm your host Sam Charrington.

00:27.840 --> 00:31.760
If you listened to last week's show you know that today Friday May 3rd is the last day

00:31.760 --> 00:36.760
to register for our O'Reilly Strata Hadoop World Conference giveaway.

00:36.760 --> 00:41.760
You've got until midnight Pacific time to register, and you can do that via our new Facebook

00:41.760 --> 00:42.840
page.

00:42.840 --> 00:46.960
Whether or not you're interested in attending Strata Hadoop, we'd really appreciate you

00:46.960 --> 00:52.560
taking a moment to like our Facebook page, as well as subscribe to our new YouTube channel.

00:52.560 --> 00:55.840
We'll link to both of these in the show notes.

00:55.840 --> 01:00.240
Last week I mentioned that I'm working on an event called the Future of Data Summit,

01:00.240 --> 01:04.080
and I'm excited to share some of the details of that event with you now.

01:04.080 --> 01:09.480
The event is part of a larger IT industry conference called Interop ITX, and I've worked

01:09.480 --> 01:14.680
with the team at UBM that organizes the event for several years now.

01:14.680 --> 01:18.920
At last year's conference, I presented a workshop called the IT Leaders Guide to Machine

01:18.920 --> 01:23.520
Learning, and based on the strong response to that session, they asked me to work with

01:23.520 --> 01:26.360
them to do something bigger this time around.

01:26.360 --> 01:32.400
The result is a two-day future of data summit that will bring together noted experts and

01:32.400 --> 01:39.360
practitioners to discuss the future of enterprise data from a variety of technology perspectives.

01:39.360 --> 01:44.360
We'll be exploring the innovation and opportunity being offered in areas such as, of course,

01:44.360 --> 01:50.080
machine learning in AI and cognitive services, but also IoT and edge computing, augmented

01:50.080 --> 01:57.080
and virtual reality, blockchain, algorithmic IT operations, data security and privacy,

01:57.080 --> 01:58.080
and more.

01:58.080 --> 02:02.640
I've handpicked the speakers to both inspire summit attendees with a view into what's

02:02.640 --> 02:07.840
possible, as well as to provide practical insights into how to get there.

02:07.840 --> 02:13.040
To give you a taste of what I've got planned, here are just three of the 16 great speakers

02:13.040 --> 02:14.920
on our agenda for the summit.

02:14.920 --> 02:19.840
Well, first off, you remember Josh Bloom, a former guest on the podcast who start up

02:19.840 --> 02:23.760
wise.io, was recently acquired by GE.

02:23.760 --> 02:30.440
Well, Josh will be joining us to speak about building AI products from idea to production.

02:30.440 --> 02:35.880
Intel's Asaf Araki will give us a view into the next five plus years of compute, storage

02:35.880 --> 02:40.800
and network innovation in his talk titled, How the Future of Hardware Enables the Future

02:40.800 --> 02:42.680
of Data.

02:42.680 --> 02:48.300
And Diana Kelly, global executive security advisor at IBM, will be discussing the future

02:48.300 --> 02:54.200
of threat landscape and how to protect cloud, IoT and big data systems.

02:54.200 --> 02:59.160
I've got more information about the event, as well as a preliminary agenda posted at

02:59.160 --> 03:02.700
twimmolai.com slash future of data.

03:02.700 --> 03:06.760
On that page, you'll also find details for registering for the conference and a code

03:06.760 --> 03:10.000
offering a special discount for twimmolisteners.

03:10.000 --> 03:13.760
To give you a bit of a sample of the type of content you'll get at the event, our guest

03:13.760 --> 03:19.560
on the show today is James McCaffrey, who's a research engineer at Microsoft Research.

03:19.560 --> 03:24.200
James will be speaking at the summit on understanding deep neural networks, and that's the focus

03:24.200 --> 03:27.240
of our conversation on the podcast as well.

03:27.240 --> 03:31.280
We had a good time with this conversation, and even if you know your way around a DNN,

03:31.280 --> 03:34.200
I think you'll pick up some interesting tidbits.

03:34.200 --> 03:39.880
Enjoy the show and check out the event page at twimmolai.com slash future of data, or

03:39.880 --> 03:46.600
the show notes page at twimmolai.com slash talk slash 13, for more information on James

03:46.600 --> 03:48.440
or the summit.

03:48.440 --> 03:51.080
And now on to the show.

03:51.080 --> 03:56.720
Hey, everyone, I am here with James McCaffrey.

03:56.720 --> 04:02.760
James is with Microsoft Research, and we've got an exciting show for you this time, and

04:02.760 --> 04:07.040
we're going to be spending some time digging into deep neural nets.

04:07.040 --> 04:10.120
James, why don't you introduce yourself?

04:10.120 --> 04:11.120
Hi, Sam.

04:11.120 --> 04:13.440
Thanks for having me today.

04:13.440 --> 04:14.600
My name is James McCaffrey.

04:14.600 --> 04:17.560
I work at Microsoft Research.

04:17.560 --> 04:21.960
Before working at research in the research division of Microsoft, I worked in the product

04:21.960 --> 04:27.200
groups, so I have some experience sort of on the pragmatic side of things.

04:27.200 --> 04:34.080
And before joining Microsoft, I was a university professor in mathematics and computer science.

04:34.080 --> 04:39.760
So I made that transition from academia to industry.

04:39.760 --> 04:48.800
At Microsoft, my area of expertise is machine learning, and in particular, neural networks.

04:48.800 --> 04:54.400
One of the things I do here at Microsoft Researcher, my role, is somewhat of a hybrid.

04:54.400 --> 05:01.120
At Microsoft Research, we have, I'm going to guess maybe in the neighborhood of 300 serious

05:01.120 --> 05:08.960
researchers, world class guys, that have very specific domain knowledge.

05:08.960 --> 05:18.040
And my role is because I have enough mathematical knowledge to understand these guys.

05:18.040 --> 05:23.960
And also my software engineering background, I act as a interface between the engineering

05:23.960 --> 05:24.960
groups here.

05:24.960 --> 05:25.960
Microsoft and the research groups.

05:25.960 --> 05:29.120
And I do, of course, some research on my own.

05:29.120 --> 05:30.120
Interesting.

05:30.120 --> 05:36.640
So the way we got connected was, in fact, you're going to be speaking at an event that I'm

05:36.640 --> 05:44.400
organizing as part of the NROP ITX conference in May, and that event is called the Future

05:44.400 --> 05:46.040
of Data.

05:46.040 --> 05:52.880
And you're going to be speaking there about understanding deep neural nets.

05:52.880 --> 05:56.920
That's a topic that you've been spending quite a bit of time on of late, isn't it?

05:56.920 --> 06:01.080
But sort of interesting, yes and no.

06:01.080 --> 06:07.520
I'd say I've been looking at neural networks for many, many, many years.

06:07.520 --> 06:13.640
But we're sort of in this area of the third wave of artificial intelligence, and in particular

06:13.640 --> 06:15.840
deep neural networks.

06:15.840 --> 06:25.720
And there's no clear consensus on exactly why deep neural networks, which is a sort of

06:25.720 --> 06:31.120
I'd call it a subset of artificial intelligence or a tool that enables artificial intelligence.

06:31.120 --> 06:34.200
Why they're making this giant come back again?

06:34.200 --> 06:40.520
Maybe some of your listeners can remember back in the 80s, the first wave of neural networks

06:40.520 --> 06:47.920
that held great promise, at least theoretically, but they tended to over-promise and under-deliver.

06:47.920 --> 06:53.520
And then for a long time, artificial intelligence, the phrase, wasn't really used because it

06:53.520 --> 06:57.760
had sort of gotten a stigma attached to it.

06:57.760 --> 07:02.320
But then here's the analogy I always use for people.

07:02.320 --> 07:08.720
I think many of your listeners might remember some of the speech recognition software that

07:08.720 --> 07:14.160
was popular in the 90s, Dragon was very well known.

07:14.160 --> 07:16.920
In fact, there still really are well known too.

07:16.920 --> 07:23.480
But then all of a sudden, about two years ago, two and a half years ago, seemingly

07:23.480 --> 07:35.560
out of nowhere, we had Siri and Cortana and Alexa from Apple, Microsoft and Amazon of course.

07:35.560 --> 07:42.120
And the speech recognition just seemed to take this gigantic quantum jump in improvement.

07:42.120 --> 07:48.160
And I'm fortunate to be working directly with those guys that created that quantum jump.

07:48.160 --> 07:49.960
In fact, it really was.

07:49.960 --> 07:53.280
And it was all due to deep neural networks.

07:53.280 --> 08:00.560
And now, you mentioned speech recognition that's clearly one of the big application areas.

08:00.560 --> 08:06.760
There was some news recently, I think, within the past three months or so about a group

08:06.760 --> 08:15.600
that I guess just hit a new kind of past, a new bar in terms of speech recognition accuracy.

08:15.600 --> 08:20.840
I think it was 95 or high 90s percent.

08:20.840 --> 08:22.840
Was that at Microsoft?

08:22.840 --> 08:28.000
It's interesting because there's several different benchmarks.

08:28.000 --> 08:33.160
And right now, there's tremendous amounts of excitement.

08:33.160 --> 08:36.120
I'm sort of beating around the bushes.

08:36.120 --> 08:41.000
My bottom line answer in a second will be, I'm not sure it could well have been.

08:41.000 --> 08:44.320
But there's literally breakthroughs.

08:44.320 --> 08:50.960
I was sitting in a talk just a few weeks ago where there was sort of like an arms race

08:50.960 --> 08:54.160
on some of these benchmark problems.

08:54.160 --> 09:03.800
And therefore speech recognition, text recognition, basically any kind of input type things.

09:03.800 --> 09:12.360
And literally, one research group after another is improving and jumping over the others

09:12.360 --> 09:16.240
on a week-by-week basis on it.

09:16.240 --> 09:23.920
Or reminds me of the early days of jet aircraft in the 1950s when there seemed to be a new

09:23.920 --> 09:28.360
speed record set every few months or a few weeks or months.

09:28.360 --> 09:34.520
Well, we're sort of in that same area of frantic activity.

09:34.520 --> 09:35.520
That doesn't sound quite right.

09:35.520 --> 09:43.720
It's not so much frantic activity, but significant advances are happening weekly now in several

09:43.720 --> 09:50.680
areas of AI, and most of them are directly related to deep neural networks.

09:50.680 --> 09:53.360
So maybe let's take a step back.

09:53.360 --> 10:01.880
I'm curious, how do you define and describe deep neural nets to people?

10:01.880 --> 10:03.880
This is very interesting.

10:03.880 --> 10:12.200
I have a way, I get asked this question so much, so excuse me, even among my colleagues

10:12.200 --> 10:15.280
who have PhDs in all different kinds of fields.

10:15.280 --> 10:22.000
And my peer engineers, who are some of the best engineers in the world, it's very, very

10:22.000 --> 10:28.080
difficult to explain what deep neural networks are without a picture of some sort.

10:28.080 --> 10:36.440
I found that the only way I can describe completely to the satisfaction of anyone is to use

10:36.440 --> 10:42.760
a diagram, and that's what I do in many of my talks, including the one I'll be doing

10:42.760 --> 10:44.080
for you.

10:44.080 --> 10:50.160
So using vocabulary, it's not, but the main differences are, I'll try to express as best

10:50.160 --> 10:51.400
I can.

10:51.400 --> 10:56.000
Whenever I try to explain what a deep neural network is, I start and say, and it kind of

10:56.000 --> 11:00.960
makes sense, you have to have an absolute solid understanding of what a so-called regular

11:00.960 --> 11:08.680
neural network is, and because the distinction until recently, when you said neural network,

11:08.680 --> 11:14.360
you meant what is now called a single hidden layer neural network.

11:14.360 --> 11:17.280
They're the simplest forms of neural network.

11:17.280 --> 11:21.720
And deep neural networks can actually have several different meanings.

11:21.720 --> 11:27.360
At the basic level, a deep neural network is simply, I mean, it's really simple.

11:27.360 --> 11:34.120
It's just a more complicated basic neural network with multiple hidden layers.

11:34.120 --> 11:39.800
If I can interrupt you and go back to this single hidden layer neural network, we're talking

11:39.800 --> 11:48.400
about a neural network that will have an input layer and then this hidden layer and an output

11:48.400 --> 11:49.400
layer.

11:49.400 --> 11:58.040
Basically, each of those layers has a set of weights assigned to them and using some math

11:58.040 --> 12:05.920
and algorithms, backpropagation, for example, you're able to, based on throwing a bunch

12:05.920 --> 12:11.120
of training data at these neural networks, come up with a, quote unquote, optimal set of

12:11.120 --> 12:14.600
weights, which really is what defines the neural network.

12:14.600 --> 12:18.680
Is that like, is that a good way to describe what this single...

12:18.680 --> 12:21.280
That's absolutely correct.

12:21.280 --> 12:22.960
Put another way.

12:22.960 --> 12:28.600
In the end, a neural network is just a very complex mathematical equation that can be used

12:28.600 --> 12:30.160
to make predictions.

12:30.160 --> 12:34.200
The number of inputs is determined by your data.

12:34.200 --> 12:39.920
Suppose you're trying to predict the political party affiliation of a person and that could

12:39.920 --> 12:44.040
be democratic, republican, or other.

12:44.040 --> 12:49.640
So that's what you're trying to predict, and your features, which means the variables

12:49.640 --> 12:54.160
you use to make the prediction, suppose that could be four things.

12:54.160 --> 13:00.560
The person's age, their annual income, their level of education, and some other metrics.

13:00.560 --> 13:01.560
So four.

13:01.560 --> 13:06.400
Therefore, your neural network would have four input nodes and it would have three output

13:06.400 --> 13:07.400
nodes.

13:07.400 --> 13:13.120
But the hidden layer processing nodes, this hidden layer is where the processing, most

13:13.120 --> 13:14.960
of the processing is done.

13:14.960 --> 13:18.920
The number of those nodes has to be determined by trial and error.

13:18.920 --> 13:21.760
But in a regular neural network, there is one such layer.

13:21.760 --> 13:25.080
So it might be maybe ten hidden nodes.

13:25.080 --> 13:27.920
But with a deep neural network, you just add multiple layers.

13:27.920 --> 13:34.800
You might have three hidden layers of ten processing nodes, twenty processing nodes,

13:34.800 --> 13:37.080
and then ten processing nodes.

13:37.080 --> 13:42.120
And in fact, neural networks have been getting much, much deeper than three layers of

13:42.120 --> 13:43.120
late.

13:43.120 --> 13:44.120
Is that right?

13:44.120 --> 13:45.120
Quite right.

13:45.120 --> 13:52.120
Until, relatively recently, there's been two things have been occurring that have led

13:52.120 --> 13:58.720
to these dramatic increases across multiple areas of artificial intelligence.

13:58.720 --> 14:05.760
One of them is that we're just getting more raw horsepower to process these things.

14:05.760 --> 14:13.520
It turns out that they get exponentially more complex and so it turns out that we're

14:13.520 --> 14:16.520
just getting more and more processing power.

14:16.520 --> 14:23.000
But the second thing is combined at the same time is that we're getting very clever with

14:23.000 --> 14:24.280
architecture.

14:24.280 --> 14:30.320
And that is combining these different hidden layers in very clever ways instead of doing

14:30.320 --> 14:32.440
it naively.

14:32.440 --> 14:38.880
The analogy in this case reminds me of the advances in computer chess programs where computer

14:38.880 --> 14:45.680
chess programs all of a sudden got very, very good, better than any human being, someone

14:45.680 --> 14:47.160
unexpectedly.

14:47.160 --> 14:53.600
And it was it was not due merely to more processing power and it wasn't due simply to better

14:53.600 --> 14:54.600
algorithms.

14:54.600 --> 14:56.720
It was a combination of the two.

14:56.720 --> 15:01.520
So we're getting quickly to an area that I find really interesting.

15:01.520 --> 15:07.560
And that is the architecture of deep neural nets.

15:07.560 --> 15:14.560
I have a ton of questions about this so I'm excited that we get a chance to chat about

15:14.560 --> 15:15.560
it.

15:15.560 --> 15:16.560
I know.

15:16.560 --> 15:25.520
I guess as a as a as a preface to this, I know that Microsoft research has been one of

15:25.520 --> 15:31.520
many research organizations that's been kind of pushing the front to here.

15:31.520 --> 15:39.640
And in fact, in 2015, they authored a paper on what's called deep residual learning that

15:39.640 --> 15:46.000
won the image net competition that year.

15:46.000 --> 15:52.080
And so, you know, I guess what I want to talk about is like what is deep neural net architecture

15:52.080 --> 15:59.120
and, you know, what is deep residual learning and what are convolutional layers like, you

15:59.120 --> 16:06.800
know, so take us from this description of deep neural net and layers through how those

16:06.800 --> 16:12.040
the architecture of those networks has evolved and, you know, what are what are how do we

16:12.040 --> 16:13.520
think about all that right now?

16:13.520 --> 16:16.920
OK, I'll do my best to describe these again.

16:16.920 --> 16:21.000
When I do describe these, I almost always have to use a diagram because I'm going to

16:21.000 --> 16:26.680
talk about architecture, it's sort of going to be a bunch of nodes and how they're connected.

16:26.680 --> 16:35.120
So let me sort of talk about all of these things that you've mentioned are closely related.

16:35.120 --> 16:37.840
They're somewhat cousins to each other.

16:37.840 --> 16:44.400
Let's let's take the residual neural network that you just described.

16:44.400 --> 16:55.040
Now this is more of an exotic variety and in my mind, at least, the residual neural network

16:55.040 --> 17:01.160
is very, very close to a close cousin to a type of neural network called a recurrent

17:01.160 --> 17:02.760
neural network.

17:02.760 --> 17:04.880
They're usually abbreviated RNNs.

17:04.880 --> 17:05.880
Right.

17:05.880 --> 17:12.240
Now, what makes it recurrent neural network special and by the way, there's a ton of research

17:12.240 --> 17:16.200
activity on all of these things that we're talking about now.

17:16.200 --> 17:22.440
But a standard neural network does not maintain state.

17:22.440 --> 17:26.640
You feed it some inputs and it produces some outputs.

17:26.640 --> 17:29.880
Then, the next set of inputs come along.

17:29.880 --> 17:31.720
The neural network is essentially wiped clean.

17:31.720 --> 17:36.680
It doesn't maintain state from that previous set of inputs and outputs.

17:36.680 --> 17:44.880
A recurrent neural network has memory and internal memory, so to speak, and that manifests itself

17:44.880 --> 17:47.320
with just some extra nodes.

17:47.320 --> 17:54.160
If you can imagine a regular neural network with a hidden layer of nodes, say, 10 nodes

17:54.160 --> 18:01.680
in there, there's going to be a recurrent neural network has a second group of 10 nodes

18:01.680 --> 18:06.520
that maintain the memory of the previous input.

18:06.520 --> 18:12.440
This allows this, just intuitively, you can tell that this makes the neural network much

18:12.440 --> 18:20.680
more powerful and smart because it has, in an English word, it has context.

18:20.680 --> 18:28.040
This means, for instance, suppose you're trying to predict, you're coming along and your

18:28.040 --> 18:36.160
inputs are words in a sentence and you're trying to predict what the next word might be.

18:36.160 --> 18:41.280
This is something that you might see on like a smartphone when you're typing a message

18:41.280 --> 18:47.040
and it tries to predict what your next word might be, although there's a pretty rudimentary

18:47.040 --> 18:48.040
right now.

18:48.040 --> 18:55.920
If you just used a regular neural network to do that, input is separate and you wouldn't

18:55.920 --> 19:01.520
have any context, but a recurrent neural network would have a shadow of the memory of the

19:01.520 --> 19:07.440
previous inputs and it would be able to make a better guess at what the next word is because

19:07.440 --> 19:13.680
the next word in a sentence is clearly going to depend on what the first words were.

19:13.680 --> 19:20.040
These recurrent neural networks sometimes are categorized into short termy recurrent

19:20.040 --> 19:27.160
neural networks where they only have a limited ability to remember quite recent inputs or

19:27.160 --> 19:32.760
they can be one of the most exciting areas of research right now is these long term recurrent

19:32.760 --> 19:38.720
neural networks and they just maintain more memory and these things have the potential

19:38.720 --> 19:45.320
to be super powerful and to get to sort of close the circle here.

19:45.320 --> 19:55.240
In my mind, I view the Microsoft residual neural networks as one of these long term recurrent

19:55.240 --> 20:01.920
neural networks with some special architecture features thrown in sort of customer.

20:01.920 --> 20:10.160
So you mentioned long-term memory and short-term memory and in fact on this show, I've

20:10.160 --> 20:18.880
talked quite a bit about applications using LSTM RNN which is long short-term memory.

20:18.880 --> 20:23.760
How does that relate to long-term and short-term?

20:23.760 --> 20:33.720
Well, to tell you the truth, the vocabulary is not very standardized and they all from

20:33.720 --> 20:34.720
a...

20:34.720 --> 20:38.560
So maybe these long short-term memories are what you're referring to as long-term and

20:38.560 --> 20:39.560
that's also...

20:39.560 --> 20:40.560
Yeah, okay.

20:40.560 --> 20:41.560
Thank you.

20:41.560 --> 20:42.560
Okay.

20:42.560 --> 20:43.560
You precisely said what I was trying to say.

20:43.560 --> 20:44.560
Got it.

20:44.560 --> 20:45.560
Got it.

20:45.560 --> 20:46.560
Okay.

20:46.560 --> 20:47.560
So, yeah.

20:47.560 --> 20:55.400
We have been hearing tons about different applications of these LSTM networks, you know, often

20:55.400 --> 21:02.560
relating to the example that you use which is you're predicting...

21:02.560 --> 21:07.920
Trying to predict words or things like that from a sentence.

21:07.920 --> 21:15.360
Which kind of brings us to maybe the difference between predictive networks and generative

21:15.360 --> 21:16.360
networks?

21:16.360 --> 21:17.360
Oh.

21:17.360 --> 21:18.360
Okay.

21:18.360 --> 21:19.360
Very good.

21:19.360 --> 21:20.360
This is...

21:20.360 --> 21:26.860
If I had to pick one area where there's more excitement, intellectual excitement in the

21:26.860 --> 21:32.600
research community than any other, it's exactly said these generative neural networks.

21:32.600 --> 21:34.880
They're called GAN.

21:34.880 --> 21:41.560
One of the most popular forms of these is called GAN, a generative adversarial network.

21:41.560 --> 21:48.400
Sure, it's really conceptually a little bit difficult to grasp, and here's how I think

21:48.400 --> 21:50.440
about it.

21:50.440 --> 21:57.920
A generative neural network does just what you might expect from its description is it

21:57.920 --> 22:01.600
doesn't try to make a prediction based on input.

22:01.600 --> 22:09.040
It more or less tries to create new inputs in some sense, which is a little bit hard

22:09.040 --> 22:10.040
to grasp.

22:10.040 --> 22:15.120
Now, I'll be the first to say that I don't fully understand these things.

22:15.120 --> 22:19.400
Like everybody else, they've only been around...

22:19.400 --> 22:24.800
Really, the biggest name is a guy named Ian Goodfellow, who is the best-known name in

22:24.800 --> 22:25.800
this area.

22:25.800 --> 22:29.680
And these things have only really been around for a matter of months now.

22:29.680 --> 22:33.680
By that, I mean, maybe a year and a half to two years or so.

22:33.680 --> 22:36.640
So a lot of us are still trying to figure it out.

22:36.640 --> 22:45.040
The classic example, at least, that I used on my blog post, is that you can feed a neural

22:45.040 --> 22:53.040
network a bunch of Van Gogh paintings, and then that generative neural network will be

22:53.040 --> 23:00.920
able to generate and create paintings based on the style of Van Gogh.

23:00.920 --> 23:04.520
In short, what it's doing is it's sort of separating out.

23:04.520 --> 23:07.360
It's learning to separate style from content.

23:07.360 --> 23:12.600
Well, this is all very difficult for me to get my head around.

23:12.600 --> 23:18.920
And I'll say that people who are much, much smarter than me figure that this is something

23:18.920 --> 23:24.000
that could lead to tremendous breakthroughs in the future.

23:24.000 --> 23:32.480
And for folks that want to dig into that last use case, I believe the paper is called

23:32.480 --> 23:35.280
Neural Artistic Style Transfer.

23:35.280 --> 23:41.080
Or at the very least, if you Google that or Bing that, you'll be able to find lots of

23:41.080 --> 23:44.200
information about that application.

23:44.200 --> 23:45.200
Right.

23:45.200 --> 23:46.200
Exactly right.

23:46.200 --> 23:50.040
But yeah, so there's generative networks and GANs.

23:50.040 --> 23:56.000
In fact, I just had an opportunity to hear in Goodfellow talk about this last week.

23:56.000 --> 24:05.600
I was at an event, a deep learning summit, rework deep learning summit in San Francisco.

24:05.600 --> 24:13.680
And the basic idea there, as I understand it, is you've got, as you mentioned, a network

24:13.680 --> 24:20.880
that is kind of trained to produce or approximate inputs.

24:20.880 --> 24:27.200
And then you feed the stuff that it spits out to another network that is, I think, called

24:27.200 --> 24:32.400
a discriminator network that's trained to basically measure how close those inputs are

24:32.400 --> 24:37.840
to the real life thing, the thing that you're trying to approximate.

24:37.840 --> 24:41.640
And then you basically have a feedback loop between these two.

24:41.640 --> 24:43.960
That's correct.

24:43.960 --> 24:49.280
They're called adversarial because really under the covers, there's two neural networks

24:49.280 --> 24:50.280
going on.

24:50.280 --> 24:59.200
Number one is trying to generate information and fake out the other neural networks.

24:59.200 --> 25:00.200
So they're adversarial.

25:00.200 --> 25:02.040
They're working against each other.

25:02.040 --> 25:09.880
And this relates more to the architecture, the engineering architecture.

25:09.880 --> 25:13.440
But as you said, the real goal is to generate information.

25:13.440 --> 25:20.680
And the idea being there, that if a neural network is smart enough to generate information,

25:20.680 --> 25:28.400
then it's also smart enough to understand and discriminate information.

25:28.400 --> 25:32.600
So we talked about RNNs.

25:32.600 --> 25:35.640
What about convolutional neural nets?

25:35.640 --> 25:41.600
How are those different from RNNs and other types of deep neural nets?

25:41.600 --> 25:49.600
It's funny that talking about convolutional networks, they're usually abbreviated CNNs,

25:49.600 --> 25:55.680
that now they seem like they're just sort of old news.

25:55.680 --> 26:00.800
But in fact, they're quite new still.

26:00.800 --> 26:07.880
The main problem with deep neural networks, as I described, a basic deep neural network,

26:07.880 --> 26:14.880
which is a simple architecture, but with just lots of nodes in multiple layers.

26:14.880 --> 26:17.520
The problem there is the training.

26:17.520 --> 26:23.640
The number of weights and biases that you have to compute, or using your optimization

26:23.640 --> 26:41.240
algorithm, just becomes intractable.

26:41.240 --> 26:49.920
The number of weights you have to do is 5 times 6, plus 6 times 3, plus 6, plus 3.

26:49.920 --> 26:58.720
As you expand the number of nodes, in English, it increases exponentially, that's not mathematically

26:58.720 --> 26:59.720
correct.

26:59.720 --> 27:03.720
Let's just say it gets really big, really fast, it gets intractable.

27:03.720 --> 27:09.960
It doesn't drive you crazy when people do say that, and it's not actually exponential.

27:09.960 --> 27:15.160
It depends on how much I've been drinking.

27:15.160 --> 27:22.160
You know, I try to see, because here at Microsoft, I speak to different audiences, I'll speak

27:22.160 --> 27:27.720
to business leaders, I'll speak to engineers, and I'll speak to mathematicians.

27:27.720 --> 27:33.640
When you're speaking to anybody but the mathematicians, if you try to phrase yourself too carefully

27:33.640 --> 27:40.800
and be correct, you mess up your argument, but when I say, for instance, that the output

27:40.800 --> 27:47.160
of a neural network, a classifier, are probabilities, oh, my math colleagues will go nuts and go,

27:47.160 --> 27:48.160
no, they're not.

27:48.160 --> 27:49.160
No, they're not.

27:49.160 --> 27:50.440
I go, okay, yeah, I know they're not.

27:50.440 --> 27:59.480
But we're back to convolutional neural networks, because a straightforward approach just

27:59.480 --> 28:02.560
isn't intractable computationally.

28:02.560 --> 28:14.160
The idea, and let's see, this was, I always have trouble, remember, is Yan Lee-Kun?

28:14.160 --> 28:15.680
Is the big name here?

28:15.680 --> 28:22.520
He created an architecture where the main idea of this architecture was to make these

28:22.520 --> 28:24.480
things tractable.

28:24.480 --> 28:30.040
CNNs are used almost exclusively for image processing.

28:30.040 --> 28:36.920
This is an area that I'm not too familiar with, I mean, I'm from Earth, math of it all.

28:36.920 --> 28:43.680
But imagine you have an image, or a set of images, and you want to classify them.

28:43.680 --> 28:49.640
The classic example is called the M-ness database, where there's a data set of umpteen thousand

28:49.640 --> 28:59.200
handwritten digit characters that were called from IRS tax returns and digitized.

28:59.200 --> 29:02.880
And so suppose you want to classify, you know, what is this?

29:02.880 --> 29:03.880
Is it a digit one?

29:03.880 --> 29:05.640
Is it a digit two or so forth?

29:05.640 --> 29:11.080
Well, even a very small image is going to have thousands of pixels, and each pixel is

29:11.080 --> 29:13.320
going to be one input.

29:13.320 --> 29:20.080
Now if you get, go up to like a seriously large picture, or even something that a smartphone

29:20.080 --> 29:23.920
can take, you've got millions of inputs.

29:23.920 --> 29:29.960
And millions of inputs, you just can't deal with that in a basic way.

29:29.960 --> 29:37.040
So the, the, the brilliancy of convolutional neural networks is to simplify.

29:37.040 --> 29:42.680
It still uses the same basic ideas of neural networks, but it uses them in very clever

29:42.680 --> 29:49.720
ways by slicing and dicing the image up and sharing weights instead of having to calculate

29:49.720 --> 29:53.520
a million times a million, which is whatever that is, weights.

29:53.520 --> 30:00.640
You can break it up, and there's a part of the secret sauce is shared weights where weights

30:00.640 --> 30:06.200
in a particular area of inputs, meaning a particular area of the image are shared.

30:06.200 --> 30:08.600
And there's a lot more to it than that.

30:08.600 --> 30:14.320
Convolutional neural networks are really a remarkable achievement of architectural design,

30:14.320 --> 30:17.960
and they're now considered more or less standard.

30:17.960 --> 30:25.400
Many of the tools that you can find in particular Google's tool, whose name I can never remember

30:25.400 --> 30:31.280
because I don't use it, it runs strictly on Linux, do you know which one I'm talking

30:31.280 --> 30:32.280
about?

30:32.280 --> 30:33.280
Say I'm with it.

30:33.280 --> 30:34.280
Google's TensorFlow.

30:34.280 --> 30:35.280
TensorFlow.

30:35.280 --> 30:36.280
Oh, thank you, thank you.

30:36.280 --> 30:37.280
Yes, sir.

30:37.280 --> 30:45.120
Anyway, so Google's TensorFlow can do CNNs, and Microsoft has a recently released, basically

30:45.120 --> 30:53.440
a same idea called CNTK, not a real, it doesn't slide off the tongue really easily there.

30:53.440 --> 31:00.280
But these things are now well known, but I always like to point out that it took a lot

31:00.280 --> 31:02.360
of researchers a lot of years.

31:02.360 --> 31:09.760
In fact, the CNN version that's in common use now is called CNN version five or something

31:09.760 --> 31:14.960
like that, which means there were many major iterations and tons of work that went

31:14.960 --> 31:15.960
on.

31:15.960 --> 31:20.040
So, in short, to summarize, you know, these CNNs to the best of my knowledge are used almost

31:20.040 --> 31:24.480
exclusively for image processing, but they are the state of their art.

31:24.480 --> 31:32.280
However, they have some really interesting problems that a lot of, there's a lot of thought

31:32.280 --> 31:35.440
about some of the limitations of CNNs.

31:35.440 --> 31:37.440
Can you speak a bit to those?

31:37.440 --> 31:43.600
Yeah, I sure there was a very interesting, fascinating paper that came out of Google

31:43.600 --> 31:46.760
Research, what was it called?

31:46.760 --> 31:54.360
It was called the intriguing properties of neural networks, something like this.

31:54.360 --> 32:02.240
And the key takeaway is, and I like to use this example of which I don't think was in

32:02.240 --> 32:04.720
the paper, but other people followed up on it.

32:04.720 --> 32:13.560
You can, suppose you train a CNN to recognize images, you can feed it a picture of a school

32:13.560 --> 32:19.200
bus, and it's clearly a school bus, and the CNN will recognize it.

32:19.200 --> 32:28.840
But by cleverly messing up just a few of the pixels, the image is completely unchanged

32:28.840 --> 32:30.120
to the human eye.

32:30.120 --> 32:36.440
However, this exact same classifier now sees the school bus as an ostrich, so it's the

32:36.440 --> 32:38.160
bus to ostrich effect.

32:38.160 --> 32:42.280
Well, this is very troubling in a lot of ways.

32:42.280 --> 32:47.520
It raises, by the way, you can't just throw, you can't just randomly mess up the picture,

32:47.520 --> 32:52.040
you have to do it in a very clever way, but it raises some important issues.

32:52.040 --> 32:56.000
One of them is, at least the whole question of comprehension.

32:56.000 --> 33:02.040
Does a CNN really understand things if you can hoax it this way?

33:02.040 --> 33:08.160
It also raises questions of, if people are going to, and they are, using these CNNs for

33:08.160 --> 33:17.600
things, which have security implications, or imagine medical imaging, where it has implications

33:17.600 --> 33:19.200
for health and safety.

33:19.200 --> 33:22.120
Are law enforcement exactly?

33:22.120 --> 33:29.080
If these things have this inherent weakness, maybe there's something wrong with CNNs.

33:29.080 --> 33:34.320
This is all just the speculation that's going to, and no one really knows, but at least

33:34.320 --> 33:40.360
just some very interesting questions, and the research goes on at just giving more

33:40.360 --> 33:47.280
interest in research, in particular, some of my colleagues are working on trying to go

33:47.280 --> 33:55.920
back to the very, very early days, where instead of just using raw math and raw processing,

33:55.920 --> 34:01.240
we're going to try to do some symbolic and some sort of a deeper level of understanding.

34:01.240 --> 34:04.840
Can you elaborate on that?

34:04.840 --> 34:11.920
What does that mean in this context, and how will we apply symbolic, symbolic here?

34:11.920 --> 34:22.960
Because we just hired the person who's considered the leading guy in this area, and he only started

34:22.960 --> 34:32.480
here, here is Paul, and I'll spell his last name, S-M-O-L-E-N-S-K-Y, Paul Smolensky.

34:32.480 --> 34:39.520
We just hired him out of Johns Hopkins University, and he's been, what many people,

34:39.520 --> 34:43.520
claiming he considered the leading researcher in this area of symbolic reasoning and machine

34:43.520 --> 34:44.520
learning.

34:44.520 --> 34:52.360
I got his book, and I'm a fairly bright guy, I have a PhD, but this was a complicated

34:52.360 --> 34:53.360
book.

34:53.360 --> 34:59.440
He's thinking at a different level, and he's trying to, I had an interesting chat with

34:59.440 --> 35:02.400
him in the hallway the other day.

35:02.400 --> 35:11.280
He sits right behind me, and the analogy goes like this, when I was an undergraduate, my

35:11.280 --> 35:17.600
very first degree was in cognitive psychology, which, through various things, that led to

35:17.600 --> 35:19.120
math and that led to computer.

35:19.120 --> 35:25.240
Anyway, when I was in my cognitive psychology days, I worked with a brilliant researcher

35:25.240 --> 35:35.000
Art Duncan-Luce, and his goal was to create a complete mathematical framework and description

35:35.000 --> 35:38.120
of certain areas of psychology in the human mind.

35:38.120 --> 35:40.640
In other words, try to map cognition.

35:40.640 --> 35:42.880
How do people think?

35:42.880 --> 35:46.440
Because still, we still don't know how people think.

35:46.440 --> 35:55.600
To map that call is attempting, in some ways, to create a meta framework for symbolic

35:55.600 --> 35:58.880
reasoning and logic.

35:58.880 --> 36:06.080
This is, right now, deep neural networks have been remarkably effective in doing what

36:06.080 --> 36:12.160
I call the sort of sensory aspect of artificial intelligence.

36:12.160 --> 36:15.520
Imagine the five senses that we have.

36:15.520 --> 36:20.520
Even vision and pattern and image recognition, they're really good at speech recognition.

36:20.520 --> 36:24.960
They're really good at, even the robotics manipulation, they're really good at, but the

36:24.960 --> 36:33.440
one thing that they just were not even close right now is the reasoning aspects of it.

36:33.440 --> 36:40.120
That's what the symbolic type of process is designed to do, or one area.

36:40.120 --> 36:43.440
It's one attack on this.

36:43.440 --> 36:48.800
I know that was a little bit vague and fishy, but maybe you can get Paul in a future one

36:48.800 --> 36:50.560
of your podcasts to talk about.

36:50.560 --> 36:52.760
I'd love to hear what he has to say.

36:52.760 --> 36:53.760
Okay.

36:53.760 --> 36:54.760
Awesome.

36:54.760 --> 36:55.760
Yeah.

36:55.760 --> 36:57.080
That would be great.

36:57.080 --> 37:11.440
We've got CNNs, RNNs, and I still want to probe around the idea of network architecture

37:11.440 --> 37:14.640
and residual learning.

37:14.640 --> 37:29.160
There was a blog post by a guy named Steven Merity who's at Salesforce now.

37:29.160 --> 37:34.240
He came in via one of their recent acquisitions.

37:34.240 --> 37:38.040
If I remember correctly, he wrote this blog post and the title was something along the lines

37:38.040 --> 37:44.120
of network architecture is the new feature engineering, meaning in traditional machine

37:44.120 --> 37:55.200
learning, a big part of the job was trying to figure out how to massage your data and

37:55.200 --> 38:08.000
how to create whether natural or man-made features that express the underlying properties

38:08.000 --> 38:12.360
of your data in a way that your machine learning algorithms can easily train on those

38:12.360 --> 38:15.040
and produce accurate results.

38:15.040 --> 38:24.440
This new world, defining the network architecture of your deep neural nets is the moral equivalent

38:24.440 --> 38:25.440
if you will.

38:25.440 --> 38:32.840
It's the new thing that we need to do to massage our data and our solutions to produce accurate

38:32.840 --> 38:33.840
results.

38:33.840 --> 38:41.400
I'm trying to, I'm wondering if you can help us wrap our heads around what that process

38:41.400 --> 38:51.000
looks like and what are the things that researchers or engineers are thinking about as they start

38:51.000 --> 38:56.120
with a problem and say, I've got this data set and I think deep neural net is the way

38:56.120 --> 38:59.920
to solve this problem.

38:59.920 --> 39:05.880
How do they then get to, oh, well, the optimal answer is something that I'm going to call

39:05.880 --> 39:13.920
the deep residual network that has 150 layers and these convolutional layers and every fifth

39:13.920 --> 39:18.200
layer is a residual layer and that whole process.

39:18.200 --> 39:21.200
Is that something you can speak to?

39:21.200 --> 39:27.880
Well, yeah, I'll talk about this because sadly the bottom line is there's no good answer

39:27.880 --> 39:31.880
to this.

39:31.880 --> 39:37.840
If sort of the phrase that everybody has heard a million times is that machine learning

39:37.840 --> 39:45.960
and AI and deep learning and all this is still as much art as it is science and that has

39:45.960 --> 39:54.400
been true and it still is true, there are some incredibly bright people who work in this

39:54.400 --> 39:55.400
field.

39:55.400 --> 40:00.620
I'm fortunate enough to work with some of the greatest minds, I mean, they're world

40:00.620 --> 40:02.560
famous and leaders.

40:02.560 --> 40:09.120
But when we sit around drinking coffee and chatting about this, there's so much unknown.

40:09.120 --> 40:18.760
And the brightest guys in the world are learning daily and new stuff and for instance, another

40:18.760 --> 40:26.760
related thing here is that another hot area is reinforcement learning, which is how

40:26.760 --> 40:28.520
does that fit in?

40:28.520 --> 40:35.960
Even among my colleagues, we're talking about, you know, we're knowledge junkies.

40:35.960 --> 40:41.640
We're just constantly trying to soak this information up, but things are happening so fast

40:41.640 --> 40:45.080
and there's so much unknown.

40:45.080 --> 40:53.000
The area you're talking about, network architecture, that's one way, I mean, that would be a good

40:53.000 --> 40:56.040
surrogate term for exactly what's going on in all of research.

40:56.040 --> 41:01.520
Now, it's almost all related directly or indirectly to the architecture.

41:01.520 --> 41:08.600
Now, I'm a pretty, you know, I believe in simplicity and for me, network architecture,

41:08.600 --> 41:16.200
deep architecture is really simple in the one hand where it's just how you combine your

41:16.200 --> 41:22.280
processing nodes and not so much input output in different ways.

41:22.280 --> 41:27.040
And it boils down to, think about the human brain, it's been some interesting work done

41:27.040 --> 41:34.120
by all things DARPA, the defense agency in conjunction with IBM, where one of the

41:34.120 --> 41:41.320
projects they have and Microsoft has a similar project that I don't think I can talk about

41:41.320 --> 41:42.320
now.

41:42.320 --> 41:46.680
It's name, it's still under wraps, but I can give you a rough idea of what we're doing

41:46.680 --> 41:52.960
by talking about the IBM and the department defense thing, where the idea here is that

41:52.960 --> 41:59.720
instead of, it's almost too simple, instead of using the approach we're using right now,

41:59.720 --> 42:05.720
which is to get very clever with very specific types of architecture, very, you know, just

42:05.720 --> 42:07.520
think of a blueprint.

42:07.520 --> 42:16.200
Instead, take the approach that the human brain may have and that is just make your architecture

42:16.200 --> 42:23.040
a bunch of, a bunch of nodes totally connected, in other words, like the human brain.

42:23.040 --> 42:29.600
And then instead of using supervised learning, where you have to have labeled data, you

42:29.600 --> 42:36.360
have to have known correct outputs with your, you know, inputs, use unsupervised learning.

42:36.360 --> 42:42.680
And I'm sort of tossing out a schmorker's board of terms here, but unsupervised learning

42:42.680 --> 42:49.600
is another incredibly hot area of research right now, where we realize that methods that

42:49.600 --> 42:56.000
require labeled training data, which is just the way to say data, where you tag what the

42:56.000 --> 42:58.920
correct output is, that can only take you so far.

42:58.920 --> 43:04.600
It's just not going to scale to the, you know, the kinds of things that we want to do.

43:04.600 --> 43:09.920
Anyway, back to the DARPA IBM thing, they're creating this thing, where their goal is

43:09.920 --> 43:17.040
to create a processor in hard work, because, you know, IBM is known for that, that kind

43:17.040 --> 43:22.080
of work, that is, you know, skills to biological levels.

43:22.080 --> 43:27.640
And as far as I can recall from last time I read that, an article on it, they believe

43:27.640 --> 43:37.000
that they have successfully created in-hardware a neural network, and they're not calling

43:37.000 --> 43:42.840
it that, that roughly simulates the complexity of the brain of a honeybee.

43:42.840 --> 43:47.120
And then, okay, the question here is not, okay, so how does it learn?

43:47.120 --> 43:49.800
And you know, that wraps around back to symbolic thing.

43:49.800 --> 43:54.680
So, I'm sorry, that was kind of a rambling answer here, but I agree that you're right,

43:54.680 --> 44:02.320
that it's right now, if you wanted to summarize all of the, or most of the areas, including

44:02.320 --> 44:08.680
these generative adversarial networks, the long short-term memory networks, the residual

44:08.680 --> 44:13.400
networks, it's all about the architecture.

44:13.400 --> 44:21.680
So to, to maybe further summarize, I guess the way, the way I kind of take away, what

44:21.680 --> 44:27.800
I would take away from what you are saying is, you know, maybe on the one hand, you know,

44:27.800 --> 44:37.360
to ask, you know, how do we create new network architectures for a given problem?

44:37.360 --> 44:44.640
We're just too early to, to, right now, we're in the stage where the fact that we come

44:44.640 --> 44:50.120
up with a new architecture for a problem that is, that works and is useful, like that's

44:50.120 --> 44:55.880
a big deal, and, and we're going to have to do a lot of that before we can say, oh, this

44:55.880 --> 45:01.320
is the process for creating new architectures for our given problems.

45:01.320 --> 45:07.560
Let me interrupt by saying, sorry for interrupting you, but you just recalled to my mind a very

45:07.560 --> 45:13.240
well-known paper, it's actually not even a paper, it's basically a blog post, but it's

45:13.240 --> 45:19.240
extremely well-known in the, you know, in our field, it's called the unreasonable effectiveness

45:19.240 --> 45:26.480
of recurrent neural networks. With the idea being that, you know, there's no obvious

45:26.480 --> 45:30.520
connection, you know, for the person who, I tried to look up, I could not find too much

45:30.520 --> 45:35.120
history on recurrent neural networks. There was some indication, but it's not exactly

45:35.120 --> 45:41.640
clear who thought of them first, but it's not at all obvious, you know, you have these recurrent

45:41.640 --> 45:48.640
architecture that has worked unexpectedly well, so unreasonably well. So the point is,

45:48.640 --> 45:52.760
I know the stuff is obvious, and we're still in the very, very early stages of figuring

45:52.760 --> 45:55.480
all this out exactly what you said.

45:55.480 --> 46:04.040
And then, I guess along those lines, if I am a listener and I'm building, I want to,

46:04.040 --> 46:10.920
I want to create a solution, you know, does it stand a reason that, you know, what, where

46:10.920 --> 46:17.960
would you start if you were building something? Like, would you even, would you even try to

46:17.960 --> 46:22.600
build your own deep neural net, or would you use some off-the-shelf implementation? Would

46:22.600 --> 46:27.880
you, you know, use a service? Would you, if you thought that you, like, how would you

46:27.880 --> 46:31.880
know if you needed to build your own thing?

46:31.880 --> 46:39.440
That's an interesting question, and there's a, there's, I want to say controversy, but

46:39.440 --> 46:49.960
there are differences of opinion here. My personal opinion is that whenever I'm going to tackle

46:49.960 --> 46:55.280
a problem, for instance, one of the problems that I'm fascinated by, and I've worked on

46:55.280 --> 47:03.400
for many years, is predicting America and National NFL football scores. And I like that as

47:03.400 --> 47:09.000
an interesting problem, because it's concrete, it's practical, and you can determine your,

47:09.000 --> 47:15.800
you know, how good you are right away. And I originally started using sort of standard

47:15.800 --> 47:21.160
canned approaches. I started with sort of regression techniques, and then I started

47:21.160 --> 47:28.680
using regular sort of neural networks from a tool, like WCA, like TensorFlow, and things

47:28.680 --> 47:35.480
like that. And I got up to a certain level of accuracy or goodness, and I just couldn't

47:35.480 --> 47:41.640
get better. No matter what I did, I couldn't get better. Until I threw it all away and created

47:41.640 --> 47:49.000
my own neural architecture from scratch, where it was custom designed for this problem.

47:49.000 --> 47:56.640
In much the same way that convolutional networks are absolutely custom designed for image

47:56.640 --> 48:06.120
recognition with, you know, they're optimized because an image has pixel values and the RGB

48:06.120 --> 48:14.280
or the red, green, blue values. Anyway, to cut to the chaser to reiterate, I totally believe

48:14.280 --> 48:23.560
that at least now with the tools that we have, you get the best results by far by creating

48:23.560 --> 48:31.360
your own custom version. And I do. Now, the problem here is that I write code every day.

48:31.360 --> 48:36.960
And these things are not easy, right, even for extremely advanced developers. And it's

48:36.960 --> 48:42.080
very time consuming and very difficult. So there aren't, you know, I'm not trying to

48:42.080 --> 48:47.080
sound boastful, but there aren't many people like me who can spin up a custom design neural

48:47.080 --> 48:56.280
network in, you know, two days or a week. So I think it's going to boil down to eventually

48:56.280 --> 49:02.120
boil down to problems. One of the things that we talk about a lot at Microsoft is the

49:02.120 --> 49:13.280
democratization of AI or machine learning. And the analogy here is maybe you and some

49:13.280 --> 49:18.640
of your listeners can remember the days, the very early days when spreadsheets, Lotus

49:18.640 --> 49:25.800
1, 2, 3, we're just becoming popular. And a lot of people were saying, what, why, why

49:25.800 --> 49:31.880
are companies, including Microsoft, making these spreadsheets? Why would people, why would

49:31.880 --> 49:37.720
normal people ever want to use a spreadsheet? This is just something for accounts. So,

49:37.720 --> 49:44.360
but then Lotus 1, 2, 3, and later Excel and the others democratized numeric processing

49:44.360 --> 49:49.400
with spreadsheets. And then all of a sudden, all kinds of interesting good things happened

49:49.400 --> 49:57.600
from that. In much the same way, the goal to democratize machine learning is the idea

49:57.600 --> 50:07.280
that if you give some basic machine learning tools and knowledge to millions of people,

50:07.280 --> 50:10.680
they're going to find interesting ways to use it and solve problems that we haven't

50:10.680 --> 50:16.640
even thought of. That said, though, I still believe that just like you can only do so

50:16.640 --> 50:23.480
much with Excel and numeric processing, you'll only ever be able to do so much with a

50:23.480 --> 50:27.840
canned program, or no matter how powerful the tool is, and that there's always going

50:27.840 --> 50:34.840
to be the need for machine learning artisans. I don't know if I said that word, right?

50:34.840 --> 50:42.400
Go in and create custom models and custom prediction models for particular problems.

50:42.400 --> 50:48.960
Your description made me prompting me to ask myself, what is the VBA for deep neural

50:48.960 --> 50:51.400
nets? Exactly.

50:51.400 --> 51:04.800
And let's skip that right over a second. Before we leave, I want to talk to you about applications,

51:04.800 --> 51:13.120
including the stuff that you do around NFL scores. But before we leave that, there are

51:13.120 --> 51:19.360
a couple of areas that I wanted to dig into around. And these are all things that I noted

51:19.360 --> 51:26.160
that you wrote blog posts around. One of them is around, I made this comment about network

51:26.160 --> 51:32.040
architecture being the new feature engineering. But in fact, it sounds like there is some of

51:32.040 --> 51:38.520
the old feature engineering that's still important and that needs to be done around data encoding

51:38.520 --> 51:43.840
and normalization when dealing with neural nets and deep neural nets. And I was wondering

51:43.840 --> 51:50.840
if you could speak to that. And then I wanted to ask you about drop out and cross entropy

51:50.840 --> 51:52.840
error as well.

51:52.840 --> 51:59.160
Okay, so I didn't quite follow the first part of what you're asking, exactly.

51:59.160 --> 52:08.360
Oh, you wrote this blog post about data encoding and normalization. And I didn't dig into

52:08.360 --> 52:18.000
that post in a lot of detail, but I was wondering if there are specific techniques in those areas

52:18.000 --> 52:24.760
related to neural nets and deep neural nets beyond the kind of things that you do in traditional

52:24.760 --> 52:25.760
machine learning.

52:25.760 --> 52:31.400
Very, very interesting. This is something that I'll answer in direct as usual. It seems

52:31.400 --> 52:40.880
like I always do. The bottom line is, okay, neural networks, no matter how you slice and

52:40.880 --> 52:47.320
dice it currently, they only understand numbers. They're number crunchers, now very, very

52:47.320 --> 52:53.040
interesting complex number crunchers. So all of your input data eventually has to be,

52:53.040 --> 52:57.640
or not eventually has to be right away turned into some kind of numeric form, so it can

52:57.640 --> 53:04.240
be understood by the neural network. And people who are new to the field, this is often

53:04.240 --> 53:12.960
one of the most discouraging parts of learning machine learning is that it seems that there's

53:12.960 --> 53:20.840
an endless number of data transformation techniques and just all this data massaging before

53:20.840 --> 53:25.360
you can ever get to the really interesting part. And it can get quite depressing for

53:25.360 --> 53:30.520
new people, but I always tell my audiences when I'm doing training and things like that.

53:30.520 --> 53:35.880
That fortunately, there's only a discrete number of these things. You have to learn,

53:35.880 --> 53:43.440
for instance, there are four real ways, I mean, four major ways to normalize your data,

53:43.440 --> 53:48.280
so it's all scaled to roughly in the same range or so. Now, once you know those four,

53:48.280 --> 53:52.040
and once you understand when they're used and when they're not used to have a few examples,

53:52.040 --> 53:55.080
then you got it. But at first, you know, when you're first trying to learn it, it seems

53:55.080 --> 53:58.520
like, hopeless, oh, man, I've got to worry about data normalization. I've got to worry

53:58.520 --> 54:09.600
about data encoding in the same way that there is only a few ways for data encoding. Now,

54:09.600 --> 54:16.760
so the answer is that, and then, and none of those things have changed with deep neural

54:16.760 --> 54:23.360
networks. Got it. However, I'm always cautious to say because that's sort of the accepted,

54:23.360 --> 54:29.360
generally accepted truth. But I love to, you know, take, whenever I hear something like

54:29.360 --> 54:33.080
that, in fact, I hadn't really thought about it until you asked this question. I always

54:33.080 --> 54:39.720
like to go back and look and go, you know what? Is this really true? Just because everyone

54:39.720 --> 54:44.880
says it's true, it just sort of like creates like this viral thing. Here's an example

54:44.880 --> 54:50.440
where I argue all the time with my colleagues. It's something very basic. Suppose you're

54:50.440 --> 54:55.080
trying to use a neural network, this is going to be a little bit technical, but you're supposed

54:55.080 --> 54:59.320
you're trying to use a neural network to predict something that can only take one of two

54:59.320 --> 55:05.760
values. For instance, you're trying to predict whether a person is male or female based

55:05.760 --> 55:10.880
on a voting behavior, based on age, based on, based on all these other things. So another,

55:10.880 --> 55:18.120
it's a binary classification problem. Now, these standard and totally accepted by everybody,

55:18.120 --> 55:26.200
except me, technique is to create a neural network that has only a single output node.

55:26.200 --> 55:32.200
And that single output node is going to be a number between zero and one where values

55:32.200 --> 55:40.280
less than 0.5 are going to indicate one of the two outcomes, male, say, and values greater

55:40.280 --> 55:48.720
than 0.5 are going to indicate the other female. So, and that is mathematically efficient

55:48.720 --> 55:56.920
as opposed to the alternative of having a neural network that has two output nodes explicitly,

55:56.920 --> 56:05.320
where they sum to one. So, you still get the same result. In other words, if your listeners

56:05.320 --> 56:10.760
know about a multi-class classifier, you just use the exact same architecture, but with

56:10.760 --> 56:14.560
two output nodes. In other words, when you're doing classification, you will never ever

56:14.560 --> 56:20.040
see a two output node, neural network classifier, because the idea being that if you're trying

56:20.040 --> 56:24.560
to predict one of two things, just make it a single node. Well, I tell everybody, I go,

56:24.560 --> 56:29.520
you know, okay, yeah, it makes sense, but I haven't, you know, explain that to me, you

56:29.520 --> 56:36.760
know, prove that to me that one node is exactly equivalent to two nodes. And so, but anyway,

56:36.760 --> 56:44.840
my point, I'm getting fired up because I'm, like I'm, I'm passionate about questioning

56:44.840 --> 56:49.520
common knowledge. So, back to your thing. So, it's common knowledge now that the data

56:49.520 --> 56:55.920
encoding and normalization techniques, that were commonly used and are being used for

56:55.920 --> 56:59.800
standard neural networks. We don't need anything new for deep neural networks. I'm not so

56:59.800 --> 57:00.800
sure.

57:00.800 --> 57:10.520
Well, before we leave the specific example, are you, are you excited about questioning the

57:10.520 --> 57:19.880
fact that a two node network in this example is inferior to a one node network, or have

57:19.880 --> 57:26.560
you demonstrated that there are some cases that a two node network is superior to a single

57:26.560 --> 57:33.320
node network, or for some external reasons, by external reasons, I mean, like, you know,

57:33.320 --> 57:37.720
maybe they're the same in terms of accuracy, but implementation wise, one is better than

57:37.720 --> 57:44.480
any other. What's the source of your excitement around this question?

57:44.480 --> 57:50.520
There's two, I say a couple, or at least two reasons. Okay, one, and primarily, I think

57:50.520 --> 57:54.200
it's hard to sometimes to be, you know, self-evaluate. I think it's probably psychological

57:54.200 --> 58:02.600
in my part, where in my world, knowledge is power. Knowing more than someone else is

58:02.600 --> 58:09.160
considered, you know, our mark of success. Yeah, it take, I work with a lot of guys who

58:09.160 --> 58:16.680
work in some form of sales. And for them, you know, I mean, everybody's competitive,

58:16.680 --> 58:21.640
but for them, a measure of success for them is how much money they make, because that's

58:21.640 --> 58:29.440
the external kind of manifestation of their goodness in some way. So in, you know, research

58:29.440 --> 58:35.440
and stuff, your measure is no one something, or coming, understanding something, publishing

58:35.440 --> 58:39.280
something first, they're the people that, so I think that there's a psychology there,

58:39.280 --> 58:48.800
where, if most people like me, you know, were competitive in some sense of the definition,

58:48.800 --> 58:53.600
where if everybody is saying this, and I'm somehow able to prove, everybody else was

58:53.600 --> 58:58.600
wrong, I'd get great satisfaction out of that. So that does, I mean, it sounds kind

58:58.600 --> 59:04.640
of terrible, but I think that's part of it. Now, the other part is, from an implementation

59:04.640 --> 59:11.040
point of view, I know that working on the code end of things, every implementation that

59:11.040 --> 59:20.240
I've seen has a completely has sort of two different code bases for neural network classifiers,

59:20.240 --> 59:26.680
one for binary classification, and one for all other cases. But if, when you're classifying,

59:26.680 --> 59:31.720
doing a binary classification, and you have two output nodes, then you only have one

59:31.720 --> 59:35.680
code base. In other words, the neural network is the neural network, where the number of

59:35.680 --> 59:41.480
output nodes is the number of classes that you're trying to predict. So from that engineering

59:41.480 --> 59:44.640
point of view, it's very appealing.

59:44.640 --> 59:51.760
There's an elegance to have the same solutions at the same code base for independent of

59:51.760 --> 59:57.640
the specifics of the problem, or to not have the exception of the single class or the

59:57.640 --> 59:59.480
two class prediction.

59:59.480 --> 01:00:11.320
And then regarding training deep neural nets, what's the state of the art there? And I

01:00:11.320 --> 01:00:17.520
think my sense is that that training techniques, or tell me if this is true or not, that training

01:00:17.520 --> 01:00:23.360
techniques are tied very closely to architecture. At this point, meaning the research papers

01:00:23.360 --> 01:00:30.280
that talk about new architectures are also talking about specific training techniques for

01:00:30.280 --> 01:00:37.080
those architectures, or is that not the case, and then talk about dropout, which is, I

01:00:37.080 --> 01:00:40.280
think that was Jeff Hinton's group in 2014.

01:00:40.280 --> 01:00:41.280
Go ahead.

01:00:41.280 --> 01:00:43.280
If that's enough to get going with.

01:00:43.280 --> 01:00:48.640
Well, first of all, I agree with you for the first part of your question, and that in

01:00:48.640 --> 01:00:55.960
general, there are a few exceptions, but in general, if you create a custom network architecture,

01:00:55.960 --> 01:01:03.320
then you'll have to use a custom training algorithm optimization. Now, there's, I'll make

01:01:03.320 --> 01:01:10.840
a parenthetical remark that an area that I believe has great promise. And again, I'm

01:01:10.840 --> 01:01:18.160
in a very much of a minority view here, is that there are certain optimization algorithms

01:01:18.160 --> 01:01:24.600
and techniques that can be applied to any network architecture. And in general, they're called

01:01:24.600 --> 01:01:30.320
swarm intelligence optimization algorithms, particle swarm optimization, and so forth.

01:01:30.320 --> 01:01:37.160
There's some others. And basically, they just use absolute brute force, whereas most

01:01:37.160 --> 01:01:42.560
opt, they're not based, the swarm techniques are not based on calculus and gradients and

01:01:42.560 --> 01:01:49.000
things. So that's, you know, most optimization algorithms are based on calculus, and you

01:01:49.000 --> 01:01:54.360
have to calculate derivatives, and the derivatives depend on the architecture. So that's why you

01:01:54.360 --> 01:02:00.520
got to basically, in most cases, create a custom training algorithm, if you have a custom

01:02:00.520 --> 01:02:07.440
training custom neural architecture. And then, as I mentioned, I'm intrigued by the

01:02:07.440 --> 01:02:12.400
idea of applying the swarm optimization to these things. I've made a few stabs at it,

01:02:12.400 --> 01:02:17.760
but like anything else, there's just not enough time. Going back, I remember, in the

01:02:17.760 --> 01:02:22.120
previous discussion we were talking about, you know, my two node versus one node binary

01:02:22.120 --> 01:02:26.880
cluster, I just haven't had time to look at it. It would take, you know, I'd have to

01:02:26.880 --> 01:02:33.600
dedicate a week or two to that. And it's like all of us, you know, I mean, I've got more

01:02:33.600 --> 01:02:40.640
things that I have to do, than things, than time to do. So in short, I agree with you

01:02:40.640 --> 01:02:49.320
that custom training algorithms are needed with the possible exception of swarm optimization,

01:02:49.320 --> 01:02:53.000
which in my few stabs, I haven't been entirely successful, but I'm not ready to give up

01:02:53.000 --> 01:02:59.480
on them. Now, with regards to drop out as interest, there's a whole bunch of, not a whole

01:02:59.480 --> 01:03:05.680
bunch of techniques, but quite a few techniques and drop out is one. Now, I remember drop out

01:03:05.680 --> 01:03:14.160
training, which is closely related to jittering input jittering and so on, are all designed,

01:03:14.160 --> 01:03:19.840
are mostly designed to prevent overfitting during training. That's sort of their motivation,

01:03:19.840 --> 01:03:27.120
in most cases. And drop out training was everywhere, I'd say two to three years ago. It was a very

01:03:27.120 --> 01:03:35.040
hot area of research, a lot of excitement around it, and that sort of faded out for reasons

01:03:35.040 --> 01:03:44.480
which aren't clear to me. I have this in nagging suspicion a lot of times that trends in research,

01:03:44.480 --> 01:03:51.760
and you know, very high end mathematical research, are subject to trends and fashions, just

01:03:51.760 --> 01:03:56.720
like a lot of things are, and sometimes things fall out of favor for no apparent reason.

01:03:56.720 --> 01:04:03.640
An example of this that I like to point out is that there's a neural network training

01:04:03.640 --> 01:04:12.400
algorithm called resilient back propagation. It's a form of obviously a variation of back

01:04:12.400 --> 01:04:19.840
propagation. I did some experimentation on it where I generated artificial data sets, very

01:04:19.840 --> 01:04:28.320
large artificial data sets, and the resilient back propagation algorithm, I mean clearly outperformed

01:04:29.840 --> 01:04:36.320
normal back propagation. Now, I have to say that with an asterisk. The problem with, it's almost

01:04:36.320 --> 01:04:42.160
impossible to compare training algorithms because they all have so many hyperparameters

01:04:42.160 --> 01:04:49.760
typically the learning rate, momentum rate, regularization, you know, L1 regularity. There's

01:04:49.760 --> 01:04:56.800
just too many parameters. You're not completely comparing apples to oranges, but you're comparing

01:04:56.800 --> 01:05:02.400
two different kinds of apples, perhaps. So it's very difficult to tell. So anyway, drop out

01:05:02.400 --> 01:05:12.640
training is something that just seems to not be in fashion, but is there and I'm a believer

01:05:12.640 --> 01:05:18.400
in dropout training, but you know, it's kind of funny and now that you asked this question,

01:05:18.400 --> 01:05:26.480
I'm thinking back to recent neural networks that I've done. And I haven't been using dropout to

01:05:26.480 --> 01:05:35.280
tell you the truth because it is, it's in my world, you know, I spin up custom on neural networks

01:05:35.280 --> 01:05:43.680
myself and they're quite difficult to implement. It creates a lot of extra work. And so I take the

01:05:43.680 --> 01:05:52.080
often take the easy way out. And what is that easy way out? Is it, I mean besides I'm not using dropout,

01:05:52.080 --> 01:05:57.760
are there other things that you're doing with your data or there are other algorithms that have

01:05:57.760 --> 01:06:05.280
the same effect of avoiding overfitting? Or is it, you know, your standard, you know, data

01:06:05.280 --> 01:06:10.880
segmenting validation sets, that kind of thing? Yeah, you know, I got to be honest with you,

01:06:10.880 --> 01:06:16.160
I don't really have a good answer to that question, you know, I'm not sure to be perfectly honest.

01:06:16.160 --> 01:06:27.920
For one, you know, here, I was talking to the chief architect of Microsoft's CNTK tool,

01:06:27.920 --> 01:06:32.400
which is our, or it's released to the public, you can find it on GitHub. It's our version of

01:06:32.400 --> 01:06:39.600
TensorFlow. Deep neural networks, including convolutional neural networks and recurrent neural networks

01:06:39.600 --> 01:06:46.400
and stuff. And I was talking to him one time because he's not only a, he's a great, the main architect,

01:06:46.400 --> 01:06:55.680
very bright guy and named Frank. And we were talking and I asked him a question. I saw some really

01:06:55.680 --> 01:07:01.440
weird behavior that I didn't understand. I don't remember what the weird behavior was. And so I saw him

01:07:02.320 --> 01:07:07.600
in the hallway and I said, hey, Frank, you know, and then I described the phenomenon. And you go,

01:07:07.600 --> 01:07:14.800
I said, can you think of anything that would cause that to happen? By the way, later turned out,

01:07:14.800 --> 01:07:21.520
it was just a weird, it was just weird random randomness. But Frank thought about it before I

01:07:21.520 --> 01:07:25.360
goes, you know, the only thing I can think of is that you've got a bug in your code.

01:07:26.560 --> 01:07:32.960
And that was, I mean, he was, and I tell you, you know, when I saw that, when I saw the behavior

01:07:32.960 --> 01:07:37.520
that I've described, that's what, that was my first question. Oh, man, I must have a serious bug

01:07:37.520 --> 01:07:43.040
in my code somewhere. Well, it turns out that it wasn't a bug at all. I was just sort of bizarre

01:07:43.040 --> 01:07:48.800
behavior. And, but the conversation led us to talk about. And then, you know, we're a conversation

01:07:48.800 --> 01:07:55.680
sort of meandered. And I said, yeah, I remember. I told him the story how I spun up a neural network

01:07:55.680 --> 01:08:02.240
as a few years ago. And I was using it for, you know, work. It was actually using it. It was

01:08:02.240 --> 01:08:08.480
performing very well until one day I was looking at the code, dusted off the code, and realized,

01:08:08.480 --> 01:08:17.680
I'd missed a, I'd completely missed updating one of the bias values. In other words,

01:08:18.480 --> 01:08:25.440
I completely was ignoring one of the constants in the equation. And yet, the neural network was

01:08:25.440 --> 01:08:30.480
performing well. So the moral of the story. And so I mentioned that to Frank, he goes, yeah, I've

01:08:30.480 --> 01:08:36.320
done this many times myself. So what's happening here is when you create a neural network, it's

01:08:36.320 --> 01:08:42.320
really, really hard to tell it if it's good or not, because you can get, you can get good results

01:08:42.320 --> 01:08:48.720
and have a seriously flawed implementation. So in the same way by coming back to this dropout

01:08:48.720 --> 01:08:55.600
thing where adding dropout or moving dropout, you'd think it'd be a relatively easy to tell

01:08:55.600 --> 01:09:02.640
is this helping me or hurting me? It's not at all easy to determine. And in total, there are

01:09:02.640 --> 01:09:07.520
a few things that have been really interesting for me about this conversation. But one of the most

01:09:08.400 --> 01:09:18.560
is the, like the hard definitive stand you took on the need to craft your own networks. And

01:09:18.560 --> 01:09:28.880
I think how that relates to here is, I don't know, I guess the idea that deep neural nets are,

01:09:30.080 --> 01:09:36.640
you know, they're kind of magic black boxes, right? And they're particularly magic,

01:09:36.640 --> 01:09:41.600
they're magic black boxes, even if you built them from scratch. And they're particularly,

01:09:41.600 --> 01:09:45.520
they're going to be even worse if you are using something out of the box that you don't fully

01:09:45.520 --> 01:09:54.800
understand. Absolutely. And I think this also relates to, you know, there's always this question

01:09:54.800 --> 01:10:01.920
around, you know, using these out of the box tools. And, you know, for many types of problems,

01:10:01.920 --> 01:10:07.040
you're trying to get from zero to 80 percent. And, you know, the researchers are trying to get

01:10:07.040 --> 01:10:14.640
from 95.2 percent to 95.7 percent. And so that's kind of an argument for, well, you know, just use the tool.

01:10:16.880 --> 01:10:21.520
You know, but if you, yeah, even if you're just trying to get to 80 percent, if you really need to

01:10:21.520 --> 01:10:29.120
understand what's happening, or you need to be able to understand what's happening in the case where

01:10:30.320 --> 01:10:35.840
it generally works great, but for whatever some variant in your input data produces

01:10:35.840 --> 01:10:41.760
outlandishly wrong results, like you have to know what's going on under the covers.

01:10:42.560 --> 01:10:45.920
Quite, I mean, I think you, I think you phrased that really, really well.

01:10:47.600 --> 01:10:53.120
I guess, how are you doing on time? Are you still, you have, I have a meeting that started

01:10:53.600 --> 01:10:59.600
right now. Okay. So we'll have to wrap this up. I'm afraid. Okay. So we'll wrap this up.

01:10:59.600 --> 01:11:06.000
Maybe I can just ask you to quickly tell us about you, you mentioned you do your own research, you've

01:11:06.000 --> 01:11:11.920
done some projects like NFL scores, like what's your, what's the project that you're most excited

01:11:11.920 --> 01:11:16.480
about? And maybe give us a quick overview of that. And if it's something that's public where we can

01:11:16.480 --> 01:11:23.840
learn more. Well, the project I'm working on right now, interestingly enough, is that Microsoft

01:11:23.840 --> 01:11:31.360
recently launched what's called the AI school. Okay. This is a big deal. Microsoft is a large

01:11:31.360 --> 01:11:40.080
organization and we create products and services. Microsoft has made a massive investment, both

01:11:40.080 --> 01:11:50.880
money wise and sort of culture wise, where our senior leadership believes that putting intelligence,

01:11:50.880 --> 01:11:57.120
real intelligence into every product and service that we do is critically important.

01:11:59.120 --> 01:12:06.480
So some of our senior leaders I've seen say something where they believe that this wave of

01:12:06.480 --> 01:12:11.760
adding artificial intelligence and machine learning intelligence into our products is every

01:12:11.760 --> 01:12:19.840
bit as important as, you know, the internet came to pass. So towards that, Microsoft created what's

01:12:19.840 --> 01:12:30.800
called the AI school. And I was hired from my previous to help run the AI school because I had

01:12:30.800 --> 01:12:36.640
a background in education. And I'd say that I'm pretty relative to most of my peers. I have a

01:12:36.640 --> 01:12:43.440
pretty broad knowledge of many areas of machine learning AI. I'm not nearly as deep as they are,

01:12:43.440 --> 01:12:49.920
of course. So that's what I'm working on right now. I'm trying to spin up, trying to determine how

01:12:49.920 --> 01:12:56.480
to transfer knowledge of all these things that we just talked about and place that knowledge into

01:12:56.480 --> 01:13:03.280
the hands of the software developers that we have, the project managers that we have, the business

01:13:03.280 --> 01:13:08.800
decision makers that we have, the salespeople who sell our products and generate the revenue that

01:13:08.800 --> 01:13:15.360
you know, keeps me employed because everybody I was surprised we we sent out an announcement to

01:13:15.360 --> 01:13:22.240
this like over creating the AI school. And we thought we'd get a, you know, maybe a couple hundred

01:13:22.800 --> 01:13:31.520
messages of interest exclusively from engineers and developers. But we got thousands and thousands

01:13:31.520 --> 01:13:37.200
literally we're I mean, we were overwhelmed by the response and not only just from engineers.

01:13:37.200 --> 01:13:46.800
I think engineers see pretty clearly that machine learning and artificial intelligent skills are

01:13:46.800 --> 01:13:53.840
quickly becoming must have skills for them. In other words, they're going to have to know how to

01:13:53.840 --> 01:13:57.520
put logistic regression in or they're going to have to know the difference between this kind of

01:13:57.520 --> 01:14:02.400
classifier and that kind of classifier. But so that made sense, but we were surprised by the number

01:14:02.400 --> 01:14:10.320
of people, designers, UI people, literally across the organization people, exactly. So that's

01:14:10.320 --> 01:14:14.880
what I'm working on now. And I'm very, you know, passionate about this and very interested in it.

01:14:14.880 --> 01:14:21.200
And trying to deliver this knowledge while at the same time, in fact, I remember the the

01:14:21.200 --> 01:14:28.320
researcher hired me to run this and he's actually in charge of this one of the most famous names

01:14:28.320 --> 01:14:36.080
in speech recognition. He basically created the technology behind Cortana, which is the same as

01:14:37.040 --> 01:14:42.480
technology behind here. I mean, extremely famous guy, but and he told me when I was interviewing

01:14:42.480 --> 01:14:48.400
for this position from my my old position just upstairs, by the way, he told me, you know, how

01:14:48.400 --> 01:14:56.720
you going to manage or how are you going to balance doing, you know, your job of creating training

01:14:56.720 --> 01:15:04.160
classes and delivering classes and doing videos and stuff with the need to stay up to date because

01:15:04.160 --> 01:15:09.760
things are rolling out on a, you know, weekly, monthly basis. Right. And I said, well, you know,

01:15:09.760 --> 01:15:14.000
that's that's the challenge. So, you know, I'll conclude by saying, you know, I'm really excited

01:15:14.000 --> 01:15:19.520
about working on the Microsoft AI school, but also really excited about all the all the things that

01:15:19.520 --> 01:15:26.400
are going on, generative adaptive neural networks and and all these other things. And now you've got,

01:15:26.400 --> 01:15:30.240
you certainly got me excited about this AI school and probably a lot of listeners as well as

01:15:30.240 --> 01:15:36.880
this primarily an internal resource or it will it be a public resource that my Microsoft is

01:15:36.880 --> 01:15:41.280
promoting. Yeah, it's a good good good. Something that we've talked about and we're really not quite

01:15:41.280 --> 01:15:47.920
sure. You know, our our mandate, of course, initially at least is to provide this internally.

01:15:47.920 --> 01:15:53.520
No, it's not a secret or anything, but we don't have any externally facing kind of information

01:15:53.520 --> 01:15:59.920
very much. But on the other hand, a lot of people are saying, Hey, you know, I mean, we want to the

01:15:59.920 --> 01:16:05.680
the content that we develop could be useful to everybody. Right. The deal here is that there's

01:16:05.680 --> 01:16:11.440
a lot of content out there already. What we're trying to do is find our sweet spot where

01:16:12.000 --> 01:16:17.920
how can we use our particular areas of expertise? We don't want to just rehash and redo,

01:16:17.920 --> 01:16:24.480
say, for instance, most of your listeners probably know about Andrew Eng out of Stanford, his

01:16:24.480 --> 01:16:29.920
excellent online course at Switch, which I think are probably pretty much state of the art.

01:16:29.920 --> 01:16:33.600
We don't want to just try to replicate that for a couple of reasons. We'd be wasting our time

01:16:33.600 --> 01:16:40.080
and we probably wouldn't do as good a job. So we're trying to find areas where we have our internal

01:16:40.080 --> 01:16:46.480
expertise and certainly not only the knowledge, but the method of delivery. And once we figure that

01:16:46.480 --> 01:16:51.680
out, then I fully believe that we'll be able to share that with everybody. Great. Great. With that

01:16:51.680 --> 01:16:58.000
James, you've been very gracious with your time. Thank you so much. And look forward to

01:16:58.000 --> 01:17:02.400
keeping in touch and to our, you know, when we meet in person at the the future of data summit.

01:17:03.280 --> 01:17:05.920
Thanks, Sam. It was a pleasure chatting with you and thanks for your time.

01:17:09.920 --> 01:17:15.280
All right, everyone. That's our show for today. Once again, thank you so much for listening and

01:17:15.280 --> 01:17:21.440
for your continued support. Please remember that we want to hear from you. You can comment on

01:17:21.440 --> 01:17:28.160
the show via the show notes page via the at Twomo AI Twitter handle or my own at Sam Charrington

01:17:28.160 --> 01:17:36.400
handle via our new Facebook and YouTube pages or just via good old fashioned email to Sam at Twomo AI.com.

01:17:37.600 --> 01:17:42.240
Please do show some love to our new Facebook and YouTube pages though. Your likes and

01:17:42.240 --> 01:17:47.280
subscribers there will really help support the show. And remember, if you're catching this

01:17:47.280 --> 01:17:53.200
podcast on Friday, you've still got time to register for our Strata Hadoop giveaway. The winner

01:17:53.200 --> 01:17:58.720
will be announced on next week's show. The notes for this show and all the links I've mentioned

01:17:58.720 --> 01:18:14.640
will be posted at Twomo AI.com slash talk slash 13. Thanks again for listening and catch you next time.

