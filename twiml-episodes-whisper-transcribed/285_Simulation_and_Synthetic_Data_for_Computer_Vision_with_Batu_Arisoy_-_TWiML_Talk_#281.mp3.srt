1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,840
I'm your host Sam Charrington.

4
00:00:32,840 --> 00:00:39,000
I'd like to start out by thanking Siemens for sponsoring today's show.

5
00:00:39,000 --> 00:00:43,720
You probably know Siemens as a global leader in the area of industrial electrification,

6
00:00:43,720 --> 00:00:49,920
automation and digitalization, or as a supplier of systems for power generation and transmission,

7
00:00:49,920 --> 00:00:52,520
as well as medical diagnosis.

8
00:00:52,520 --> 00:00:56,720
I recently had an opportunity to attend the company's spotlight on innovation event down

9
00:00:56,720 --> 00:01:00,460
in Orlando, Florida and learn about some of the interesting work happening within the

10
00:01:00,460 --> 00:01:02,560
R&D branch of the company.

11
00:01:02,560 --> 00:01:06,520
I'm happy to be able to share a bit of that with you via today's conversation with Batu,

12
00:01:06,520 --> 00:01:09,320
a computer-visant research manager there.

13
00:01:09,320 --> 00:01:15,320
To find out more about what Siemens is up to in areas like AI, digital twin, cybersecurity,

14
00:01:15,320 --> 00:01:25,320
smart city infrastructure and more, visit twimbleai.com slash Siemens.

15
00:01:25,320 --> 00:01:28,440
Alright everyone, I've got Batu, our soy on the show.

16
00:01:28,440 --> 00:01:34,280
Batu is a research manager for vision technologies and solutions at Siemens Corporate Technology.

17
00:01:34,280 --> 00:01:37,440
Batu, welcome to this week of machine learning and AI.

18
00:01:37,440 --> 00:01:39,720
Sure, it's great to be here.

19
00:01:39,720 --> 00:01:44,440
It's great to have you on the show and I'm looking forward to this conversation.

20
00:01:44,440 --> 00:01:49,360
How do we start with having you share a little bit about your background and how you came

21
00:01:49,360 --> 00:01:53,280
to work in computer vision and machine learning?

22
00:01:53,280 --> 00:01:54,280
Sure.

23
00:01:54,280 --> 00:02:00,560
So, actually I started my education and career in the field of electrical and computer

24
00:02:00,560 --> 00:02:05,480
engineering with the focus on computer vision topics in Turkey.

25
00:02:05,480 --> 00:02:12,360
And after finishing up my Bachelor of Science there, I joined Siemens and started working

26
00:02:12,360 --> 00:02:17,520
on compensation geometric problems such as 3D reconstruction from point class and geometrical

27
00:02:17,520 --> 00:02:18,920
shape matching.

28
00:02:18,920 --> 00:02:25,320
And at that point deep learning was taking off every day and at that point we were mostly

29
00:02:25,320 --> 00:02:30,120
using the traditional machine learning algorithms and that's how my interest started.

30
00:02:30,120 --> 00:02:34,880
At that time we're always using okay, can we extract some hand carved features in order

31
00:02:34,880 --> 00:02:39,200
to represent the geometries, but then we realized that there might be a better way of doing

32
00:02:39,200 --> 00:02:41,920
this and that's how I started this machine learning journey.

33
00:02:41,920 --> 00:02:47,560
So, after I received my PhD in computational geometry at CMU, I joined Siemens corporate

34
00:02:47,560 --> 00:02:52,520
technology and I started integrating artificial intelligence and computational geometry together.

35
00:02:52,520 --> 00:02:56,720
And right now I'm technology manager for vision, technologies and solutions team in Siemens

36
00:02:56,720 --> 00:03:02,520
corporate technology which is actually R&D hub for Siemens and we are providing high quality

37
00:03:02,520 --> 00:03:07,400
research development and consulting services for our business units within the Siemens.

38
00:03:07,400 --> 00:03:14,000
Okay, you mentioned that a big part of your focus is kind of merging, did I interpret

39
00:03:14,000 --> 00:03:19,120
this right kind of merging the traditional computational geometry approaches and deep

40
00:03:19,120 --> 00:03:20,120
learning?

41
00:03:20,120 --> 00:03:23,480
Is that how you're approaching the various problems you're working on?

42
00:03:23,480 --> 00:03:29,920
That's correct, that's one facade of our approach, but basically our team is known

43
00:03:29,920 --> 00:03:33,360
as the computer vision with limited data problem.

44
00:03:33,360 --> 00:03:39,680
Okay, so our team is working on how to solve computer vision problems by leveraging some

45
00:03:39,680 --> 00:03:46,200
simulation methods to generate synthetic data or by developing intelligent AI algorithms

46
00:03:46,200 --> 00:03:50,960
to eliminate the need of large amount of training data set collection.

47
00:03:50,960 --> 00:03:55,640
Because you might be seeing in most of the domains that this is one of the obstacles

48
00:03:55,640 --> 00:03:58,760
to actually deploy AI systems in industry.

49
00:03:58,760 --> 00:04:04,480
We have to collect a large amount of training data sets for a specific custom task for

50
00:04:04,480 --> 00:04:09,520
your interest, but at the same time after collecting this data, you also have to annotate

51
00:04:09,520 --> 00:04:10,840
this data.

52
00:04:10,840 --> 00:04:15,840
So we are working closely with academia and researchers within Siemens to tackle this

53
00:04:15,840 --> 00:04:21,680
problem from multiple angles, how to solve learning or computer vision with limited

54
00:04:21,680 --> 00:04:22,680
data problem.

55
00:04:22,680 --> 00:04:29,720
Is your use of simulation primarily focused on synthetic data or are you using simulation

56
00:04:29,720 --> 00:04:31,760
in other ways as well?

57
00:04:31,760 --> 00:04:34,800
So we use simulation in two different ways.

58
00:04:34,800 --> 00:04:40,400
The first way of we use the simulation is actually to generate synthetic data.

59
00:04:40,400 --> 00:04:46,640
And one great example use case that we had all have developed in the past is about spare

60
00:04:46,640 --> 00:04:48,440
part recognition.

61
00:04:48,440 --> 00:04:53,400
This is a problem if you have a mechanical object that you deployed in the field and

62
00:04:53,400 --> 00:04:59,120
you need to perform maintenance and service operations on this mechanical functional object

63
00:04:59,120 --> 00:05:00,120
over time.

64
00:05:00,120 --> 00:05:06,400
In order to solve this problem, what we are working on is using simulation to synthetically

65
00:05:06,400 --> 00:05:13,880
generate a training data set for object recognition for large amount of entities.

66
00:05:13,880 --> 00:05:19,840
In other words, we synthetically generate images as if these images are collected in real

67
00:05:19,840 --> 00:05:23,800
word from an expert and they are annotated from an expert.

68
00:05:23,800 --> 00:05:28,240
And this actually comes for free using the simulation.

69
00:05:28,240 --> 00:05:33,600
We have developed a system together with our Siemens colleagues where we deployed this

70
00:05:33,600 --> 00:05:37,440
for the maintenance applications of the trains.

71
00:05:37,440 --> 00:05:41,400
And the main goal is a service engineer goes to the field.

72
00:05:41,400 --> 00:05:46,320
He takes his tablet, he takes a picture, then he draws a rectangle box.

73
00:05:46,320 --> 00:05:50,480
And the system automatically identifies what is the object of interest that the service

74
00:05:50,480 --> 00:05:52,640
engineer would like to replace.

75
00:05:52,640 --> 00:05:57,520
And in order to make the system reliable, we have to take into consideration different

76
00:05:57,520 --> 00:06:04,600
lightning conditions, texture, colors, or whatever these parts can look like in a real world

77
00:06:04,600 --> 00:06:05,600
environment.

78
00:06:05,600 --> 00:06:13,600
Okay, so you've got some ground truth data, which is the parts itself, you know, from,

79
00:06:13,600 --> 00:06:18,960
you know, these could be imagining you have them both from a, like, maybe catalog image

80
00:06:18,960 --> 00:06:24,640
as perspective, but for some of them, you probably have down to a CAD description of these

81
00:06:24,640 --> 00:06:25,640
parts.

82
00:06:25,640 --> 00:06:32,320
And you want the service engineer to be able to take a picture of the parts in place,

83
00:06:32,320 --> 00:06:36,000
not having, you know, taking the part out and putting it on a white background or something

84
00:06:36,000 --> 00:06:40,760
like that, but just in place and then draw a bounding box of some sort around it.

85
00:06:40,760 --> 00:06:46,200
And your system should be able to identify what the part in question is.

86
00:06:46,200 --> 00:06:47,360
That's exactly the case.

87
00:06:47,360 --> 00:06:49,760
So we are interested for the built-in case.

88
00:06:49,760 --> 00:06:55,040
So you like to eliminate the need of this assembling and mechanical object in order to identify

89
00:06:55,040 --> 00:06:59,960
the object of interest, because sometimes people might have already engraved some QR codes

90
00:06:59,960 --> 00:07:00,960
on these parts.

91
00:07:00,960 --> 00:07:06,800
However, we are targeting the application scenario where this object will take so much time

92
00:07:06,800 --> 00:07:13,440
to disassemble or the QR code is not visible or due to some customer preferences, we cannot

93
00:07:13,440 --> 00:07:16,960
print any QR code and ID tags on this part.

94
00:07:16,960 --> 00:07:21,600
Our system is designed to be able to work on such scenarios.

95
00:07:21,600 --> 00:07:29,360
I can see how generating synthetic data, particularly your kind of typical data augmentation

96
00:07:29,360 --> 00:07:35,920
types of synthetic data where you're doing shifts and changing the lighting conditions

97
00:07:35,920 --> 00:07:39,960
and things like that would help a model solve this problem.

98
00:07:39,960 --> 00:07:48,240
But you also have, I would imagine the issue of kind of occlusion of your part is a big

99
00:07:48,240 --> 00:07:50,040
challenge to overcome.

100
00:07:50,040 --> 00:07:55,080
Is that dealt with via synthetic data or other methods?

101
00:07:55,080 --> 00:07:56,920
So that's a great question actually.

102
00:07:56,920 --> 00:08:01,720
Yes, so that was actually the first step that we took was data augmentation methods to

103
00:08:01,720 --> 00:08:07,480
be able to actually use some previous catalog images or CAD models to generate synthetic

104
00:08:07,480 --> 00:08:08,480
images.

105
00:08:08,480 --> 00:08:13,120
However, the real challenge is modeling also the sensor structure.

106
00:08:13,120 --> 00:08:17,960
So what are these sensor noise that you are going to get when you are in the field?

107
00:08:17,960 --> 00:08:22,720
Because if you synthetically generate images, you don't consider the noise that comes from

108
00:08:22,720 --> 00:08:26,880
your acquisition device or you don't consider the distortion that comes from your acquisition

109
00:08:26,880 --> 00:08:30,560
device or some other electromagnetic field effect.

110
00:08:30,560 --> 00:08:37,120
And this is where we published a paper in 2016 that's called depth synth and where we

111
00:08:37,120 --> 00:08:44,200
modeled with very high detail how a depth sensor works, such that are simulator generates

112
00:08:44,200 --> 00:08:48,680
very high detailed depth images from the CAD model renderings.

113
00:08:48,680 --> 00:08:51,240
So that's what it makes it really successful.

114
00:08:51,240 --> 00:08:57,280
We are not only generating or augmenting data by creating different scales, different

115
00:08:57,280 --> 00:09:03,600
rotations, but even we take into account the parameters that affects the image acquisition.

116
00:09:03,600 --> 00:09:09,640
And that way we were able to achieve success to deploy such a system in real world applications.

117
00:09:09,640 --> 00:09:17,480
And so does the depth characteristic address the occlusion of your part by other parts

118
00:09:17,480 --> 00:09:18,480
in assembly?

119
00:09:18,480 --> 00:09:23,280
Or how do those two factors tie together?

120
00:09:23,280 --> 00:09:24,280
Sure.

121
00:09:24,280 --> 00:09:25,280
Sure.

122
00:09:25,280 --> 00:09:30,920
Actually, when we synthetically render these images, we render the entire assembly so that

123
00:09:30,920 --> 00:09:37,920
even though we see the objects itself, we synthetically simulate as if there are occlusions

124
00:09:37,920 --> 00:09:39,720
from the neighboring parts.

125
00:09:39,720 --> 00:09:43,600
So our synthetic database does not only consist of clean images.

126
00:09:43,600 --> 00:09:49,160
We also take into account all these cases where you synthetically simulate this corner

127
00:09:49,160 --> 00:09:50,480
cases as well.

128
00:09:50,480 --> 00:09:54,800
So this is all baked into the simulation platform that we have.

129
00:09:54,800 --> 00:10:00,960
I'm curious with depth synth paper, what the general approach was.

130
00:10:00,960 --> 00:10:07,080
How did you incorporate the characteristics of the sensor into the training of the models

131
00:10:07,080 --> 00:10:09,080
to get better results?

132
00:10:09,080 --> 00:10:10,080
Sure.

133
00:10:10,080 --> 00:10:16,280
For example, in our paper, we talk about the representation of the depth synth pipeline

134
00:10:16,280 --> 00:10:22,040
which consists of the projector modeling, camera modeling, object modeling, and reconstruction

135
00:10:22,040 --> 00:10:24,080
and post-processing models.

136
00:10:24,080 --> 00:10:29,800
And each of these models takes as input a cat model and synthetically generates a rendering

137
00:10:29,800 --> 00:10:33,120
of a depth image corresponding to this cat model.

138
00:10:33,120 --> 00:10:38,480
And during the projector modeling, for example, we simulate the patterns motion between exposures,

139
00:10:38,480 --> 00:10:40,120
vector and lens effects.

140
00:10:40,120 --> 00:10:46,760
In the camera modeling, we take into account distortion, motion blur and other noises that

141
00:10:46,760 --> 00:10:48,320
are coming from the environment.

142
00:10:48,320 --> 00:10:52,360
In the object modeling, we take into account motion control, illumination, material properties

143
00:10:52,360 --> 00:10:56,360
and also even the surface microgeometer modeling properties.

144
00:10:56,360 --> 00:11:00,880
And at the end of the day, we even apply how this geometry or how this image will look

145
00:11:00,880 --> 00:11:06,600
like if our device is applying some post-processing steps like the smoothing or hole filling.

146
00:11:06,600 --> 00:11:10,480
And that's how we actually get depth images that looks really realistic.

147
00:11:10,480 --> 00:11:18,400
And in our paper, we show the comparison our simulation results with the real data acquisition.

148
00:11:18,400 --> 00:11:24,360
And we even simulate what if somebody is using a hand-held device and he walks within

149
00:11:24,360 --> 00:11:29,920
the building, we take into account the vibration that comes from walking within the environment

150
00:11:29,920 --> 00:11:33,120
or the motion between the exposures as well.

151
00:11:33,120 --> 00:11:36,040
And that's make it really strong.

152
00:11:36,040 --> 00:11:42,920
And so are the modules that you described, are these modules steps in a pipeline, or are

153
00:11:42,920 --> 00:11:48,400
they more like layers in a deep learning model?

154
00:11:48,400 --> 00:11:53,320
So for this one, this is not based on the deep learning model.

155
00:11:53,320 --> 00:11:57,200
But right now, this is still into a pipeline.

156
00:11:57,200 --> 00:12:03,320
But during that time, we actually modeled this mathematically, these behaviors.

157
00:12:03,320 --> 00:12:04,320
Cool.

158
00:12:04,320 --> 00:12:08,840
We dove right into a pretty detailed use case.

159
00:12:08,840 --> 00:12:15,400
But maybe before we talk about more use cases, I can have you take a step back and talk

160
00:12:15,400 --> 00:12:20,600
a little bit more broadly about what your group at Siemens does.

161
00:12:20,600 --> 00:12:21,600
Sure.

162
00:12:21,600 --> 00:12:22,600
Sure.

163
00:12:22,600 --> 00:12:28,920
In our group, actually, every day, we have three different type of interactions or activities.

164
00:12:28,920 --> 00:12:33,640
And in our group, as I mentioned before, we are targeting the computer vision with limited

165
00:12:33,640 --> 00:12:34,640
data problem.

166
00:12:34,640 --> 00:12:40,400
However, what are the different venues where we actually execute this work?

167
00:12:40,400 --> 00:12:45,160
And this can be grouped in under the three different project categories or activities.

168
00:12:45,160 --> 00:12:50,720
Our first activity involves day-to-day interaction with our business units, Siemens business units

169
00:12:50,720 --> 00:12:56,240
to understand their needs and try to identify which computer vision research topics can make

170
00:12:56,240 --> 00:12:58,960
the largest business impact for them.

171
00:12:58,960 --> 00:13:05,120
After identifying such opportunities, we formulate proof-of-concept projects with them to improve

172
00:13:05,120 --> 00:13:06,560
their competitive advantage.

173
00:13:06,560 --> 00:13:11,520
But this is one type of the interaction that we have within the company for our scientists.

174
00:13:11,520 --> 00:13:16,960
The second type of interaction that also excites many of our scientists here is our interaction

175
00:13:16,960 --> 00:13:19,360
for more fundamental research topics.

176
00:13:19,360 --> 00:13:25,000
We are very active and engaged with subsidies and grants in USA with screen potential

177
00:13:25,000 --> 00:13:30,520
call for proposal issued by U.S. government entities that align with our long-term vision.

178
00:13:30,520 --> 00:13:34,280
And if there is anything relevant for us, we collaborate with multiple universities and

179
00:13:34,280 --> 00:13:37,720
Dinosaur partners to drive wide-papers and full proposals.

180
00:13:37,720 --> 00:13:43,400
And a great example of such activity was, right now, we are one of the performance within

181
00:13:43,400 --> 00:13:48,680
DARPA Physics of AI Project, together with Cornell University.

182
00:13:48,680 --> 00:13:55,560
And we are also part of the DARPA Automatic Scientology Extraction Project program as well.

183
00:13:55,560 --> 00:13:58,760
This actually concludes our second type of activities.

184
00:13:58,760 --> 00:14:04,800
And the third type of activity is more about how can our researchers and scientists work

185
00:14:04,800 --> 00:14:10,840
on the topics that are going to create a business impact in the next two, three years for Siemens?

186
00:14:10,840 --> 00:14:15,120
For these topics, our teams are collaborating with universities and other industrial partners

187
00:14:15,120 --> 00:14:19,000
to innovate and publish research results in top conferences.

188
00:14:19,000 --> 00:14:24,800
For example, the last year our team had three CVPR-1 ECCV publication and presentation.

189
00:14:24,800 --> 00:14:30,040
And this year, we had one CVPR publication and we also organized a computer vision with

190
00:14:30,040 --> 00:14:36,080
by a state of workshop led by our principal scientist Dr. Jan Ernst and with our team member

191
00:14:36,080 --> 00:14:40,880
Dr. Kondron Pank in Long Beach, California last week.

192
00:14:40,880 --> 00:14:41,880
Okay.

193
00:14:41,880 --> 00:14:45,720
What was your CVPR paper this time?

194
00:14:45,720 --> 00:14:46,720
Sure.

195
00:14:46,720 --> 00:14:51,160
This time, it was actually on a very, very interesting topic and what we called learning

196
00:14:51,160 --> 00:14:53,160
without memorizing.

197
00:14:53,160 --> 00:15:00,440
So this project is also an important, incremental learning is an important task aimed at

198
00:15:00,440 --> 00:15:06,680
increasing the capability of a trained model in terms of the number of classes recognizable

199
00:15:06,680 --> 00:15:08,160
by the model.

200
00:15:08,160 --> 00:15:13,760
So what it means is the key problem is the requirement of storing the training data associated

201
00:15:13,760 --> 00:15:19,360
with existing classes while teaching the classifier to learn new classes.

202
00:15:19,360 --> 00:15:24,440
And this is a work jointly done by Rajat Vikram Singh and Kondron Pank.

203
00:15:24,440 --> 00:15:31,560
And what they actually invented is they find that they find that a way of emulating the

204
00:15:31,560 --> 00:15:37,440
relationship between a student and the teacher, how it works in real world.

205
00:15:37,440 --> 00:15:40,080
And they apply that to artificial intelligence.

206
00:15:40,080 --> 00:15:47,040
So the way the system works is there is one teacher model which tries to explain what it

207
00:15:47,040 --> 00:15:52,400
learned so far, but it doesn't want the student to just learn everything by heart.

208
00:15:52,400 --> 00:15:57,600
It wants the student to understand why he makes this decisions.

209
00:15:57,600 --> 00:16:03,520
And that way, if you actually keep able of teaching the neural network, how a neural network

210
00:16:03,520 --> 00:16:08,880
makes decision, then you can actually pass the inherited information from the previous

211
00:16:08,880 --> 00:16:14,120
experiences we do not require access to the previous data sets.

212
00:16:14,120 --> 00:16:19,760
Are there maybe correlations between the student teacher type of an approach and something

213
00:16:19,760 --> 00:16:28,480
like an auto encoder where you're kind of trying to map what the primary model learns

214
00:16:28,480 --> 00:16:34,800
into some lower dimensional space and then kind of build that back out?

215
00:16:34,800 --> 00:16:41,360
Yes, it's similar because actually this work is built upon one of our previous publications

216
00:16:41,360 --> 00:16:47,080
which was trying to do attention modeling and attention modeling is actually trying to

217
00:16:47,080 --> 00:16:53,960
understand where does our neural network pay most of the attention when it makes a decision.

218
00:16:53,960 --> 00:16:59,880
And we are capable of highlighting, oh, okay, these are the set of the pixels that contributes

219
00:16:59,880 --> 00:17:01,480
to the most of the decision.

220
00:17:01,480 --> 00:17:06,920
And this is the paper that we published last year, it's called the Tanme Ver to look.

221
00:17:06,920 --> 00:17:15,360
And in this paper, by leveraging this powerful explanation mechanism, we are able to encapsulate

222
00:17:15,360 --> 00:17:21,840
knowledge that's very unique from the training data set in a lower dimensional representation

223
00:17:21,840 --> 00:17:23,640
step.

224
00:17:23,640 --> 00:17:27,080
And that's how it works actually, that's why it's so relevant to the auto encoder what

225
00:17:27,080 --> 00:17:32,040
we are using another mechanism to actually encapsulate the knowledge coming from the training

226
00:17:32,040 --> 00:17:33,040
data sets.

227
00:17:33,040 --> 00:17:34,040
Okay.

228
00:17:34,040 --> 00:17:44,200
And so how does that mechanism work in the learning without memorizing paper?

229
00:17:44,200 --> 00:17:45,200
Sure.

230
00:17:45,200 --> 00:17:51,560
The way this work works is actually, first of all, what's the real use case?

231
00:17:51,560 --> 00:17:57,440
The real use case is, you might be familiar with all these edge devices where we are capable

232
00:17:57,440 --> 00:18:01,280
of performing real-time computer vision tasks on them.

233
00:18:01,280 --> 00:18:07,800
And if you'd like to improve your model, for example, you'd like to train or teach this

234
00:18:07,800 --> 00:18:13,480
model classification or recognition of the new objects, you don't want to retrain your

235
00:18:13,480 --> 00:18:20,000
system using the previous training data set first, it takes too much time again.

236
00:18:20,000 --> 00:18:25,400
And you might be not have a connectivity or you might not have access to this data.

237
00:18:25,400 --> 00:18:30,320
And there's also memory limitations on your edge device, which prevents you to actually

238
00:18:30,320 --> 00:18:35,080
store the data on your device for further training purposes.

239
00:18:35,080 --> 00:18:42,040
So the main idea is, can we encapsulate the knowledge that was already inherited from

240
00:18:42,040 --> 00:18:47,040
a previous training stage and pass it to the next stage of the training?

241
00:18:47,040 --> 00:18:53,240
And the way the algorithm works is, it tries to memorize, actually we call it learning

242
00:18:53,240 --> 00:19:00,960
without memorization, but it tries to always consistently pay attention to the same regions

243
00:19:00,960 --> 00:19:04,280
of the image pixels to make the decision.

244
00:19:04,280 --> 00:19:11,360
So it doesn't allow its attention mechanism to divert too much by looking at the new observed

245
00:19:11,360 --> 00:19:12,360
data.

246
00:19:12,360 --> 00:19:18,880
So that way by keeping our attention consistent, we are preventing our attention to be heard

247
00:19:18,880 --> 00:19:20,480
or disturbed.

248
00:19:20,480 --> 00:19:26,120
And by preventing that, we are capable of actually maintaining the data as much as possible.

249
00:19:26,120 --> 00:19:31,760
Of course, this is something at research stage and we are working on it further for more

250
00:19:31,760 --> 00:19:34,360
industrial use cases.

251
00:19:34,360 --> 00:19:42,320
Going back to the, tell me where to look paper, what's the key innovation there?

252
00:19:42,320 --> 00:19:47,560
So the key innovation there was, it was a very interesting paper because, I don't know

253
00:19:47,560 --> 00:19:51,440
if you are familiar with the attention modeling frameworks, but what they are trying to do

254
00:19:51,440 --> 00:19:57,240
is, the famous example is you'd like to recognize the ship.

255
00:19:57,240 --> 00:20:01,160
And in your training dataset, you always have the ships that are on the sea.

256
00:20:01,160 --> 00:20:05,360
Then you'd like to see, okay, let me show me the, what are the pixels that my neural network

257
00:20:05,360 --> 00:20:10,000
is paying attention and you realize that all my neural network is paying attention to

258
00:20:10,000 --> 00:20:14,200
the sea because it's actually learning the patterns of the sea instead of the geometrical

259
00:20:14,200 --> 00:20:15,840
features of a ship.

260
00:20:15,840 --> 00:20:23,480
So existing algorithms until our development, the attention models were not extracting

261
00:20:23,480 --> 00:20:30,760
the boundaries of the object of interest, they were more noisy compared to our recent

262
00:20:30,760 --> 00:20:39,200
development, but we did that work was we demonstrated that we can actually mask or hide some

263
00:20:39,200 --> 00:20:43,720
pixels of the objects or some pixels of the image.

264
00:20:43,720 --> 00:20:48,160
And then we sent back this information, this mask or altered image back to the class

265
00:20:48,160 --> 00:20:53,160
fire until it's not capable of recognizing it any of the correct classes.

266
00:20:53,160 --> 00:20:58,120
That way in an iterative fashion, we remember more pixels from the images until our class

267
00:20:58,120 --> 00:21:01,840
fire starts failing to making the right decision.

268
00:21:01,840 --> 00:21:07,640
And at that moment said, okay, we are successfully recovered all the set of pixels, which impacts

269
00:21:07,640 --> 00:21:11,880
the decision of making, this is a cat, this is a ship, or etc.

270
00:21:11,880 --> 00:21:14,840
This is the one branch of our rock.

271
00:21:14,840 --> 00:21:21,240
The second branch of the box, okay, the masking is done as part of the process, it's

272
00:21:21,240 --> 00:21:26,600
not like a labeling type of a step, is that correct?

273
00:21:26,600 --> 00:21:27,600
That's correct.

274
00:21:27,600 --> 00:21:35,960
It was completely trained and where the system optimizes jointly trying to identify where

275
00:21:35,960 --> 00:21:38,240
to pay attention pixels.

276
00:21:38,240 --> 00:21:43,520
And at the same time, I removed some pixels to see if it needs to grow these pixels over

277
00:21:43,520 --> 00:21:44,520
time.

278
00:21:44,520 --> 00:21:48,880
And this way, it actually corrects and improves its attention mechanism, and we realized

279
00:21:48,880 --> 00:21:52,880
that we were able to get much more accurate boundaries of the attention.

280
00:21:52,880 --> 00:21:58,000
And they were much more clustered together, such that they are more compact representation

281
00:21:58,000 --> 00:21:59,000
of the knowledge.

282
00:21:59,000 --> 00:22:02,000
And it also provides explainability to aspect.

283
00:22:02,000 --> 00:22:10,360
And I think at least some folks in our audience will be familiar with the line paper, which

284
00:22:10,360 --> 00:22:15,600
is Carlos Gastron and his folks from the University of Washington.

285
00:22:15,600 --> 00:22:21,320
And they do something similar where they kind of perturb aspects of the inputs to determine

286
00:22:21,320 --> 00:22:28,120
which are the pixels that are most relevant to the classifier decision.

287
00:22:28,120 --> 00:22:33,120
And you relate this work to that?

288
00:22:33,120 --> 00:22:34,120
Sure.

289
00:22:34,120 --> 00:22:39,960
From a higher level, we are trying to solve a similar problem.

290
00:22:39,960 --> 00:22:44,240
But I don't know the exact details of that paper, so that's why I won't be comfortable

291
00:22:44,240 --> 00:22:46,720
to directly compare them to here.

292
00:22:46,720 --> 00:22:47,720
Cool.

293
00:22:47,720 --> 00:22:54,160
And then you mentioned a computer vision with bias data workshop that you did at CVPR this

294
00:22:54,160 --> 00:22:55,160
year.

295
00:22:55,160 --> 00:22:57,120
Can you talk a little bit about that?

296
00:22:57,120 --> 00:22:58,120
Yes.

297
00:22:58,120 --> 00:23:05,120
And actually, this was the second series of this workshop organized by our principal scientist

298
00:23:05,120 --> 00:23:06,120
Jan Ernst.

299
00:23:06,120 --> 00:23:12,120
And actually, the main objective of this workshop was bringing together people from industry

300
00:23:12,120 --> 00:23:13,120
academia.

301
00:23:13,120 --> 00:23:21,720
And also, students as well to identify what are the big problems right now in the computer

302
00:23:21,720 --> 00:23:24,400
vision with limited and biased data.

303
00:23:24,400 --> 00:23:30,920
And one famous example is like, if you have a training database, you always have a class

304
00:23:30,920 --> 00:23:31,920
imbalance problem.

305
00:23:31,920 --> 00:23:36,560
You always have so many data points of the nice results.

306
00:23:36,560 --> 00:23:42,080
But you don't have enough number of samples of the negative samples or the problematic cases

307
00:23:42,080 --> 00:23:46,560
or out of distribution cases or the unique rare cases.

308
00:23:46,560 --> 00:23:50,120
So we had the list of speakers.

309
00:23:50,120 --> 00:23:58,840
You can also find it online, the list of speakers from the website, CVPR 2019 workshop website.

310
00:23:58,840 --> 00:24:05,080
And we discussed together what are the some common problems that the academia and industry

311
00:24:05,080 --> 00:24:06,640
is facing right now.

312
00:24:06,640 --> 00:24:13,840
And what are the some approaches that academia and industry tried so far to tackle this problem?

313
00:24:13,840 --> 00:24:16,480
That was the main objective of the workshop.

314
00:24:16,480 --> 00:24:21,680
Now these kind of class imbalance problems that you've described.

315
00:24:21,680 --> 00:24:27,800
They come up all over the place, but they are particularly present in what I imagine are

316
00:24:27,800 --> 00:24:34,000
pretty common use cases for you at Siemens, like industrial inspection where you're trying

317
00:24:34,000 --> 00:24:41,440
to identify damaged parts or, you know, damaged products coming off of an inspection line

318
00:24:41,440 --> 00:24:49,680
or maybe looking for damage on wind turbine rotors, things like that, like you've got

319
00:24:49,680 --> 00:24:56,360
so many pictures of the, you know, the good parts and very few pictures of the different

320
00:24:56,360 --> 00:25:01,400
types of damaged parts.

321
00:25:01,400 --> 00:25:06,000
What are some of the key ways that you're able to deal with this?

322
00:25:06,000 --> 00:25:11,760
Sure, actually, some people call this problem out of distribution learning.

323
00:25:11,760 --> 00:25:15,840
Some people call it novel to detection or animal detection as well.

324
00:25:15,840 --> 00:25:20,480
And yes, you're right, because it's always easy to say, okay, this is something normal

325
00:25:20,480 --> 00:25:26,040
looking versus this is something defective, but there might be a hundred different ways

326
00:25:26,040 --> 00:25:28,120
this defect might look like, right?

327
00:25:28,120 --> 00:25:34,440
So and if you'd like to try to annotate all this together, it might take too much time

328
00:25:34,440 --> 00:25:39,360
and you might not even able to recover entire problem space.

329
00:25:39,360 --> 00:25:46,840
So for this one, we are working some animal detection algorithms in order to learn representations

330
00:25:46,840 --> 00:25:52,720
similar approaches, what you mentioned previously using auto encoders, such that you learn

331
00:25:52,720 --> 00:25:58,000
visual trends and then being able to identify, okay, if this is how the normal scene looks

332
00:25:58,000 --> 00:26:04,840
like, how can we identify this sample does not fit to this distribution and the data that

333
00:26:04,840 --> 00:26:07,960
we learned from and that's labeled as the normal.

334
00:26:07,960 --> 00:26:14,880
And this is a very, very powerful technology if you can scale it different use cases as

335
00:26:14,880 --> 00:26:15,880
well.

336
00:26:15,880 --> 00:26:20,040
So what are some of the most interesting, you know,

337
00:26:20,040 --> 00:26:25,640
use cases or case studies that you've had a chance to work on at Siemens, you know,

338
00:26:25,640 --> 00:26:30,720
what's most exciting of the various applications of computer vision there?

339
00:26:30,720 --> 00:26:31,720
Sure.

340
00:26:31,720 --> 00:26:38,880
And actually, this is a very hot area and the computer vision and we always try to do research

341
00:26:38,880 --> 00:26:43,480
that's grounded to the industrial needs and business needs.

342
00:26:43,480 --> 00:26:50,080
And we usually take approaches together with government as well and we are one of the

343
00:26:50,080 --> 00:26:57,080
performance in one of the programs by Office of Naval Research and this is called the

344
00:26:57,080 --> 00:27:06,320
Activity Recognition Project where we are trying to develop a system, the user will interact

345
00:27:06,320 --> 00:27:12,640
with an AI system where the user will tell, okay, I'm looking for an object and the user

346
00:27:12,640 --> 00:27:17,760
will be able to define this object, okay, it's going to have red short, it's going to

347
00:27:17,760 --> 00:27:23,920
have blonde hair or it's going to say short, tall, these attributes or I'm looking for

348
00:27:23,920 --> 00:27:26,120
a white car or etc.

349
00:27:26,120 --> 00:27:32,080
Then the system automatically filters our or distals, millions of video images or millions

350
00:27:32,080 --> 00:27:38,880
of the videos in order to identify and locate your object of interest that you defined.

351
00:27:38,880 --> 00:27:43,800
But the nice thing, an interesting thing is this is a problem that's actually two way

352
00:27:43,800 --> 00:27:49,640
where we believe and this is very exciting, we call this human in the loop idea.

353
00:27:49,640 --> 00:27:57,000
So what we believe is you or the user can provide all this information to the system but

354
00:27:57,000 --> 00:28:00,480
we don't want AI system to just return, okay, these are the results.

355
00:28:00,480 --> 00:28:06,760
We want AI system to come back and counter back saying that did you mean by this and such

356
00:28:06,760 --> 00:28:11,040
that they are going to have a conversation between the user and the AI system until they

357
00:28:11,040 --> 00:28:16,000
actually filter out and locate the object of interest because sometimes we also observe

358
00:28:16,000 --> 00:28:20,680
that people are not good at it by describing what they are looking for and the system might

359
00:28:20,680 --> 00:28:25,760
be able to correct it saying that did you mean by that test and we are trying to actually

360
00:28:25,760 --> 00:28:32,160
map human input to the visual search queries and having a conversation between these two

361
00:28:32,160 --> 00:28:33,160
agents.

362
00:28:33,160 --> 00:28:39,960
Yeah, I'm envisioning a lot of moving parts here between the natural language elements

363
00:28:39,960 --> 00:28:46,240
of it and the visual querying and choosing the right examples to maximize information

364
00:28:46,240 --> 00:28:49,440
transfer between the humans and the computer.

365
00:28:49,440 --> 00:28:52,760
What are the different elements of solving a problem like that?

366
00:28:52,760 --> 00:28:58,720
Sure, and actually that's what makes CMAS really unique because CMAS corporate technology

367
00:28:58,720 --> 00:29:04,400
is a research R&D hub for CMAS and we are actually investing in research and development

368
00:29:04,400 --> 00:29:08,160
in the domain of artificial intelligence and there are multiple teams contributing all

369
00:29:08,160 --> 00:29:13,240
over the world with diverse backgrounds and as you have seen here, this problem requires

370
00:29:13,240 --> 00:29:15,280
a natural language processing background.

371
00:29:15,280 --> 00:29:17,240
This requires computer vision background.

372
00:29:17,240 --> 00:29:25,480
This problem requires also the person identification or this kind of application backgrounds.

373
00:29:25,480 --> 00:29:32,920
So that's why the main team consists of multiple subgroups and one of the team is focusing

374
00:29:32,920 --> 00:29:40,920
on identifying attributes from multi-camera streams so that actually you need to still train

375
00:29:40,920 --> 00:29:46,920
these systems on some training datasets such that you can learn what does mean really

376
00:29:46,920 --> 00:29:50,840
having a red car, what does mean really having a blue car.

377
00:29:50,840 --> 00:29:56,880
So the first part of the computer vision element is being able to identify the objects

378
00:29:56,880 --> 00:30:01,240
and then being able to identify the features of these objects.

379
00:30:01,240 --> 00:30:07,120
Other words, after you identify this and you are capable of actually now processing the

380
00:30:07,120 --> 00:30:12,680
human input where the user says I'm looking for, after you see that sentence, you look

381
00:30:12,680 --> 00:30:18,680
for the verbs and the nouns and our natural language processing experts are working on

382
00:30:18,680 --> 00:30:25,120
how to relate the user queries to the mathematical queries in order to actually find the objects

383
00:30:25,120 --> 00:30:29,640
of interest that are found by this computer vision modules.

384
00:30:29,640 --> 00:30:33,200
Other words, of course, there is another team which is the user interaction team which

385
00:30:33,200 --> 00:30:39,280
is focusing on, okay, what is the information exchange efficiency?

386
00:30:39,280 --> 00:30:44,800
So are we actually going to be able to find a solution if you go down to this route or

387
00:30:44,800 --> 00:30:50,520
should the system come back and ask the user, did you mean by this or such that it actually

388
00:30:50,520 --> 00:30:53,720
improve the quality of the search results?

389
00:30:53,720 --> 00:31:00,880
And along the lines of government projects, you mentioned earlier DARPA Physics of AI

390
00:31:00,880 --> 00:31:05,280
project and I wanted to ask you a little bit about that.

391
00:31:05,280 --> 00:31:07,720
What's that project about?

392
00:31:07,720 --> 00:31:15,880
So DARPA Physics of AI is a program where multiple university partners are contributing

393
00:31:15,880 --> 00:31:22,960
and we have our team member, Dr. Tichun Chang is the PI for that project and this project

394
00:31:22,960 --> 00:31:29,080
is actually trying to incorporate physics into the artificial intelligence systems in order

395
00:31:29,080 --> 00:31:32,880
to eliminate the deed of large training datasets.

396
00:31:32,880 --> 00:31:38,800
And as I explained at the beginning of my discussion, so this is really what excites us learning

397
00:31:38,800 --> 00:31:43,640
with limited data and whenever there is such a program, we try our best to be part of

398
00:31:43,640 --> 00:31:50,280
it and here we are working with the Cornell University and actually this project is trying

399
00:31:50,280 --> 00:31:59,680
to reconstruct 3D images from a sub sampled resolution sensor data where they know some

400
00:31:59,680 --> 00:32:04,120
physics constraints but they don't know they don't have the full visibility to the data

401
00:32:04,120 --> 00:32:09,920
for reconstruction and they are focusing on how to incorporate the physics in order to

402
00:32:09,920 --> 00:32:15,920
be able to reconstruct 3D by knowing the relationship within the elements in this data

403
00:32:15,920 --> 00:32:16,920
and etc.

404
00:32:16,920 --> 00:32:27,000
Okay, I find this general direction of research to be really interesting in that we were

405
00:32:27,000 --> 00:32:36,680
kind of coming from a history of building systems based on very robust models of the physical

406
00:32:36,680 --> 00:32:43,680
world and then we, the pendulum kind of swung to let's throw away all the physics and

407
00:32:43,680 --> 00:32:50,040
just use statistical models, you know, from, you know, just building up from data.

408
00:32:50,040 --> 00:32:54,560
And I find that there's a lot of interesting work happening in the middle now kind of

409
00:32:54,560 --> 00:32:58,760
the pendulum is kind of swung into the middle where we're trying to integrate these two

410
00:32:58,760 --> 00:33:06,800
and attach the physical knowledge of the world or the way whatever system that we're modeling

411
00:33:06,800 --> 00:33:12,520
works with the statistical information or statistical modeling techniques.

412
00:33:12,520 --> 00:33:15,320
There are a bunch of really interesting applications there.

413
00:33:15,320 --> 00:33:23,560
Yes, and actually one everyday example that I use is like, you don't need to observe

414
00:33:23,560 --> 00:33:29,200
1000 times a car is driving on the road in order to understand a regular car does not

415
00:33:29,200 --> 00:33:30,200
fly.

416
00:33:30,200 --> 00:33:34,520
So if you can integrate some, if you can integrate some knowledge about, okay, there's

417
00:33:34,520 --> 00:33:38,720
gravity, there are roads and etc.

418
00:33:38,720 --> 00:33:44,480
You won't need that much data, that's something that excites us and we definitely interested

419
00:33:44,480 --> 00:33:46,320
in this time of topics.

420
00:33:46,320 --> 00:33:54,480
You know, I was at the Siemens spotlight on innovation event not too long ago and one

421
00:33:54,480 --> 00:34:01,360
of the use cases that our application areas that I saw Siemens talking quite a bit about

422
00:34:01,360 --> 00:34:10,320
was smart cities and a lot of kind of a broad much broader array of applications and I usually

423
00:34:10,320 --> 00:34:13,120
see when smart cities comes up.

424
00:34:13,120 --> 00:34:17,560
Is that an area that your group works in?

425
00:34:17,560 --> 00:34:22,920
Not at the moment, but we believe the technology that we develop can be applied there as well.

426
00:34:22,920 --> 00:34:27,480
You also mentioned a project automating scientific knowledge extraction.

427
00:34:27,480 --> 00:34:29,040
What's that one about?

428
00:34:29,040 --> 00:34:37,240
Sure, that's a project that we are collaborating with our sister group here in Siemens CT and

429
00:34:37,240 --> 00:34:44,840
actually, that's the PR is the Dr Janis and actually what we are trying to in that project

430
00:34:44,840 --> 00:34:52,000
is another DARPA effort and the objective is if you have list of papers, how can you

431
00:34:52,000 --> 00:34:56,960
make sure that the knowledge that extracted from this papers corresponds to the code that

432
00:34:56,960 --> 00:35:03,480
you actually get delivered at the end of the day or how can you make sure that you can

433
00:35:03,480 --> 00:35:10,800
automatically understand what is actually explained in this paper by looking at the images,

434
00:35:10,800 --> 00:35:15,680
the text and making its relationship to the software.

435
00:35:15,680 --> 00:35:21,640
And our team is contributing to the recognition of and the composition of the neural network

436
00:35:21,640 --> 00:35:24,400
images on these papers.

437
00:35:24,400 --> 00:35:31,200
And these papers are mostly scientific papers where you have the multiple layers and actually

438
00:35:31,200 --> 00:35:36,240
we are working on developing a system that's capable of taking as input such a scientific

439
00:35:36,240 --> 00:35:43,480
paper and then process the images on these papers and tell you okay, this paper is leveraging

440
00:35:43,480 --> 00:35:49,200
and neural network architecture X with this many layers and they are using this dropout

441
00:35:49,200 --> 00:35:52,000
and this pooling layer and etc.

442
00:35:52,000 --> 00:35:57,680
So we are working on that component to be able to automatically screen such scientific

443
00:35:57,680 --> 00:36:02,360
knowledge papers and try to summarize the content and knowledge in them.

444
00:36:02,360 --> 00:36:07,480
My sense is that a lot of people who are listening to this podcast will probably be really

445
00:36:07,480 --> 00:36:15,320
interested in a system like that, particularly with all of the papers that come up in our

446
00:36:15,320 --> 00:36:23,440
space and that are published to archive, it's super difficult to keep up with even a subset

447
00:36:23,440 --> 00:36:24,840
of those papers.

448
00:36:24,840 --> 00:36:31,880
Yes, that's true, but of course now we are at the resource stage of this algorithm.

449
00:36:31,880 --> 00:36:39,240
In the future we are going to see hopefully more and more examples of such work because

450
00:36:39,240 --> 00:36:46,920
right now if I remember correctly in CVPR there were 9,300 attendees and over 1000 different

451
00:36:46,920 --> 00:36:52,200
papers and right now if you spend three or four days it really takes too much time to

452
00:36:52,200 --> 00:36:56,160
even figure out what are the papers that you like to attend and visit.

453
00:36:56,160 --> 00:37:02,560
So such a solution would be even good if it can actually screen what are the titles and

454
00:37:02,560 --> 00:37:06,320
then does really fit my interest and then I can go there.

455
00:37:06,320 --> 00:37:11,680
It is kind of towards the basic research side of the spectrum, you know at least relative

456
00:37:11,680 --> 00:37:14,720
to what some other corporate research arms are doing.

457
00:37:14,720 --> 00:37:22,120
What is the, how do you ensure that this stuff is tied into and aligned with the actual

458
00:37:22,120 --> 00:37:29,640
stuff that Siemens is doing and for that matter how important you've mentioned that that's

459
00:37:29,640 --> 00:37:36,720
important to you but you know how do you strike a balance between contributing in these broader

460
00:37:36,720 --> 00:37:42,760
efforts and collaborations and doing things that ultimately has an impact on the kind of

461
00:37:42,760 --> 00:37:45,200
stuff that Siemens cares about.

462
00:37:45,200 --> 00:37:52,000
Sure, and actually with government and also other research projects we are more tackling

463
00:37:52,000 --> 00:37:57,280
the lower tier out projects which means the lower technology readiness levels where we

464
00:37:57,280 --> 00:38:01,240
develop some proof of concept and we publish some papers.

465
00:38:01,240 --> 00:38:06,520
Of course we secure the IP and that's the one of the important metrics for us.

466
00:38:06,520 --> 00:38:14,920
How many new ideas or IPs can be actually contributed to our Siemens business units to increase

467
00:38:14,920 --> 00:38:17,360
their competitive advantages.

468
00:38:17,360 --> 00:38:23,840
From a grounding perspective any topic that we invest we always first think about does

469
00:38:23,840 --> 00:38:30,360
it really relate to the problem that we have in-house or does it really have some potential

470
00:38:30,360 --> 00:38:35,440
to be converted into a product in the future and that's always our grounding mechanism.

471
00:38:35,440 --> 00:38:40,960
Our team is working all these papers that we publish even though they sound on their

472
00:38:40,960 --> 00:38:48,120
cool research projects they all get into the real world as one component of a product

473
00:38:48,120 --> 00:38:50,640
that you experience in the world.

474
00:38:50,640 --> 00:38:56,000
And actually the spare part recognition algorithm that I discussed at the beginning of our

475
00:38:56,000 --> 00:39:00,240
interview that's actually called is spare idea.

476
00:39:00,240 --> 00:39:04,720
If you go online and Google it you can see that's actually a product that our business

477
00:39:04,720 --> 00:39:09,320
units right now actually leveraging and providing the service.

478
00:39:09,320 --> 00:39:12,640
So that's how the technology transition happens first.

479
00:39:12,640 --> 00:39:19,800
We actually perform the research internally and in collaboration with the academicians.

480
00:39:19,800 --> 00:39:25,160
And then our job is transferring this research to relevant products but we always keep

481
00:39:25,160 --> 00:39:30,600
in mind that whatever we do needs to provide some impact hopefully in the next two years

482
00:39:30,600 --> 00:39:32,600
for Siemens overall.

483
00:39:32,600 --> 00:39:38,400
When you're taking an idea like this at the you know later stages of this process like

484
00:39:38,400 --> 00:39:45,360
the Easy Spares are there unique challenges that you run into kind of in those final steps

485
00:39:45,360 --> 00:39:48,960
of commercialization always.

486
00:39:48,960 --> 00:39:55,040
And that's why we always have support from other experts from our business units who

487
00:39:55,040 --> 00:39:59,800
are really know how to productize something.

488
00:39:59,800 --> 00:40:06,320
And we definitely experienced some challenges like one of the challenges is with the neural

489
00:40:06,320 --> 00:40:10,440
networks, the memory or the processing times.

490
00:40:10,440 --> 00:40:16,200
So we experienced such challenges during the end of the proof of concept phase before

491
00:40:16,200 --> 00:40:20,040
demonstrating that to our business units.

492
00:40:20,040 --> 00:40:24,880
Well I want to thank you so much for taking the time to share a bit of what you're up

493
00:40:24,880 --> 00:40:25,880
to.

494
00:40:25,880 --> 00:40:30,680
Sounds like a lot of really interesting stuff and I'm imagining some folks will be interested

495
00:40:30,680 --> 00:40:36,640
in digging into some of the recent CVPR papers that you'll be sharing with us.

496
00:40:36,640 --> 00:40:40,280
Yes, thank you very much for your time as well.

497
00:40:40,280 --> 00:40:45,320
And I would like to also point out that in case if there are any other additional questions

498
00:40:45,320 --> 00:40:50,480
your audience is free to reach me out and I think you can also post my email your website

499
00:40:50,480 --> 00:40:53,640
so that I would be happy to connect with them afterwards.

500
00:40:53,640 --> 00:40:56,160
We'll include it in the show notes.

501
00:40:56,160 --> 00:40:57,160
Thanks about to.

502
00:40:57,160 --> 00:40:58,160
Thank you.

503
00:40:58,160 --> 00:41:07,360
Alright everyone that's our show for today.

504
00:41:07,360 --> 00:41:13,600
For more information on today's guests visit twomolai.com slash shows.

505
00:41:13,600 --> 00:41:16,880
Thanks again to Siemens for sponsoring today's episode.

506
00:41:16,880 --> 00:41:21,640
Be sure to check them out at twomolai.com slash Siemens.

507
00:41:21,640 --> 00:41:50,160
As always thanks so much for listening and catch you next time.

