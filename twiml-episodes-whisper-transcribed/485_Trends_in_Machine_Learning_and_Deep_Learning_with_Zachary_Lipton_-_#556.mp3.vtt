WEBVTT

00:00.000 --> 00:16.760
All right, everyone. Welcome to another episode of AI Rewind 2021. Today, we are joined by Zachary

00:16.760 --> 00:22.400
Lipton, an assistant professor in the machine learning department and operations group at Carnegie

00:22.400 --> 00:28.400
Mellon University to talk through all things machine learning and deep learning. Zach last joined

00:28.400 --> 00:35.040
us on the show for the 2019 edition of Rewind and I'm super excited to have him back once again,

00:35.040 --> 00:40.320
Zach. Welcome back to the Twimal AI podcast. Well, thanks for having me, Sam. Great to see you again.

00:41.040 --> 00:47.680
It is great to see you again. I think the last time we physically had the opportunity to hang out

00:47.680 --> 00:55.680
was also 2019 in Vancouver. I think that's probably a story shared by a lot of folks in our field,

00:55.680 --> 01:01.920
like that was the last opportunity that folks had to hang out in person. Right. How have the last

01:01.920 --> 01:11.440
couple of years been for you? Oh, man, it's been a good full. You know, I'm not going to pretend it's

01:11.440 --> 01:17.520
all been smooth, but I mean, some things are nice. Like, my students are great and I think how

01:18.640 --> 01:24.480
been, I think it's not very easy for everyone like some people, some people got sick, some people

01:24.480 --> 01:30.960
lost, some people didn't get to see their family for a couple of years. I feel like it's a

01:30.960 --> 01:35.680
weird thing where people managed to be startlingly productive or at least, you know, maybe I don't

01:35.680 --> 01:39.840
want to shame anyone who doesn't feel productive. I feel like from just my feeling in my life,

01:39.840 --> 01:44.000
I feel like people are, I maybe it's just kind of like CMU culture design. I feel people have been

01:44.000 --> 01:51.120
really, yeah, like locked in and researched, but I think there's a kind of like emotional

01:51.120 --> 01:56.320
wear and tear of just not seeing anyone, like, especially like some folks are like living by

01:56.320 --> 02:00.000
themselves, you know, and then like when they're quarantined, it's not seen in another human for

02:00.000 --> 02:04.320
six months. And I and for others of us just like catching up now. So it's been, it's been a little

02:04.320 --> 02:11.520
bit wild, but you know, interesting on the research side and you know, we got a puppy. So like

02:11.520 --> 02:20.720
interesting personally. Nice, nice. So as I mentioned in the lead up, we are here to review the

02:20.720 --> 02:30.880
year in ML and deep learning. This is the third rewind that we'll publish. The first couple were

02:30.880 --> 02:37.120
an NLP in computer vision or computer vision and NLP in the order that they were published. And

02:37.120 --> 02:44.720
so far a couple of key themes have emerged. One, which was common in those first couple of

02:44.720 --> 02:56.000
episodes is this idea that as John Bahan and put it, NLP eating machine learning, kind of like

02:56.000 --> 03:03.040
in the same way we would say, you know, AI eating the software or what have you. The idea that

03:03.600 --> 03:10.720
computer vision is adopting transformers and things like that. You have echoed one of the other

03:10.720 --> 03:19.440
observations that John made in that NLP conversation and it is that particular point is kind of a

03:19.440 --> 03:27.520
slowing down of the field and a little bit of a respite from that kind of breakneck pace of

03:29.280 --> 03:36.560
of change that we were experiencing for a while. So maybe that is a place for you to jump in and

03:36.560 --> 03:45.600
riff for a bit. Yeah, I'm happy to riff. You know, like just you know, maybe I may be like

03:45.600 --> 03:50.080
choking a contrarian or something. I'll start by, you know, pushing back. I want to think it's

03:50.080 --> 03:56.800
kind of interesting of like, you know, that phrase of like NLP eating ML is kind of cute because

03:57.600 --> 04:06.080
it's sort of, well, among other things, right? Like in some sense, there's like the

04:06.080 --> 04:12.560
deline for for the long as time for the last seven years. It's sort of been machine learning

04:12.560 --> 04:21.120
eating NLP and that like if you look at the seven people going into like sort of NLP oriented

04:21.120 --> 04:25.120
like Radpar, there's a point where like NLP sat really close to like, you know, there were like

04:25.120 --> 04:30.960
NLP and computational linguistics like sort of two sides of a coin and they sat not so far from

04:30.960 --> 04:36.400
their like philosophers of linguistics, whatever. And now you have a moment for the last however many

04:36.400 --> 04:42.320
years where the the median person in NLP knows absolutely nothing about language. It's nothing

04:42.320 --> 04:47.760
interesting to say about language that couldn't just as easily and it's hard to say nobody or that

04:47.760 --> 04:52.080
there isn't anyone with something interest, interesting observations or interesting experiments

04:52.080 --> 04:55.920
that are that are kind of hitting off both sides, but say that like the center of gravity of the

04:55.920 --> 05:03.200
field has moved to this way that almost there there's almost no L in NLP, you know, it's just like

05:04.240 --> 05:10.640
it's just sort of like, you know, a set of tools where if if the like commercial demand was more

05:10.640 --> 05:15.680
on music than on NLP, you would use almost, you know, conceivably like the same set of mosques,

05:15.680 --> 05:20.640
all they care is just like a sequence of tokens, like a very generic sort of approach. And so in

05:20.640 --> 05:25.120
some sense, it's sort of just been like deep learning eating NLP has been the story for a while,

05:25.120 --> 05:30.880
and I think that like this version of NLP eating ML is, well, I guess one, they don't really mean

05:30.880 --> 05:37.200
NLP, but really more it's like other application areas. I think, you know, they don't really mean

05:37.200 --> 05:42.080
NLP as much as the thing that eight NLP, which is Transformers.

05:42.080 --> 05:47.600
Right, whatever is like that like new organism that displaced NLP is now coming across,

05:47.600 --> 05:52.800
but right, it's it's more like, you know, there was like a discipline of computer vision where

05:52.800 --> 05:57.600
you had people that like the typical person who was in there knew something about like the physics

05:57.600 --> 06:02.960
of light and optics and like was sort of like doing this sort of like, you know, that that was the

06:02.960 --> 06:07.760
angle. They were like a real expert on like the the modality of vision and the person NLP knew

06:07.760 --> 06:12.960
something about language. And I think they both got eight by deep learning in such a way that,

06:12.960 --> 06:17.760
you know, over the last seven years, ideas that would hit on one side could very easily

06:17.760 --> 06:25.120
poured across to the other. And from most of that history, I think it's hard to say precisely why

06:25.120 --> 06:29.920
if there's some reason or if it's just sort of the the order in which things happen, like because

06:29.920 --> 06:33.600
like the those breakthrough image net results are really caught people's attention where we're

06:33.600 --> 06:37.840
envisioned first. But I think from most of that history, it's been very one dimensional,

06:37.840 --> 06:46.000
like very one directional. I think things going from, you know, mostly in the direction of

06:46.000 --> 06:51.440
computer vision to NLP. And I think if anything, this is not really NLP eating vision, but it's just

06:51.440 --> 06:56.880
notable that maybe one of the bigger things happening in vision is crossing in the other direction,

06:56.880 --> 07:04.080
like contrary to the pattern. But yeah, you know, broadly on the like things slowing down pattern,

07:04.080 --> 07:08.400
I think I've been I've been noting this and writing about this for I don't know, maybe like four

07:08.400 --> 07:16.640
years now, but I think there's those definitely a moment where like if we were to look through

07:16.640 --> 07:24.800
the history and say like 2012 image net 2014, like sequence to sequence models, 2015, Alpha Go,

07:26.480 --> 07:37.520
2016, 2017 big advances and like the kind of like perceptual quality of like generative models,

07:37.520 --> 07:48.800
2017, transformers, 2018, Bert. There was a kind of change that, you know, I don't think these

07:48.800 --> 07:54.720
are necessarily all profound in a sense of like some big intellectual move, but they are like

07:54.720 --> 07:59.440
qualitative changes in capabilities. They it's like there's a big move in a sense of like what

07:59.440 --> 08:03.840
set of problems do I think are best tackled with these tools and what sort of performance can I

08:03.840 --> 08:09.760
expect from them? And a big difference in the sense of if I'm a practitioner in the field and

08:09.760 --> 08:15.520
somebody hits me with a typical like industry problem, what's my go-to tool? And if we look at like

08:15.520 --> 08:20.480
2021 and now we're saying, well, if someone hits you with a classification task, what are you

08:20.480 --> 08:24.720
going to do? If I'm just going to use a resonant from 2014 and 2015, you know, someone hits you

08:24.720 --> 08:31.840
with an LP task, like basically fine tuning Bert or a Bert like, you know, very Roberta Alberto,

08:31.840 --> 08:36.000
you know, whatever it's like, you know, right, like there's this moment where, you know,

08:37.360 --> 08:45.040
like I think that in some sense, I think it's okay in that like research is not now need to like

08:46.000 --> 08:49.440
start looking somewhere else other than just like what if I tweaked the architect here a little

08:49.440 --> 08:54.000
bit? Like there's a and as I was telling you like when we were ripping before, is that I think

08:54.000 --> 08:59.040
a research is there is some aspect that people like grow up it around in the dark looking for,

08:59.040 --> 09:03.520
looking for a way in, it's almost like they're like swinging out a pinnata with a blindfold on

09:03.520 --> 09:08.160
and trying to find like where's where's where's there an angle that like where's there something big?

09:08.800 --> 09:14.640
And I think you want to have a lot of researchers in that mindset of like I'm looking for the blind

09:14.640 --> 09:20.000
spot, like I'm looking for the big prize that other people aren't looking for. And look,

09:20.000 --> 09:24.880
every now and then someone gets, every now and then someone really lands a mark and the pinnata

09:24.880 --> 09:29.280
rips open and a bunch of candy falls on the floor. And then everybody rushes on and there's some

09:29.280 --> 09:33.200
period of time where nobody's worried about like nobody even knows where the bat is, everybody's

09:33.200 --> 09:40.000
just picking candy off the floor. And you know, I think where we saw that period of like people

09:40.000 --> 09:45.120
finding all this, you know, every, it wasn't like you didn't need a big intellectual breakthrough

09:45.120 --> 09:52.000
to have a big, to have an impactful breakthrough in all those years. And I think we're getting to

09:52.000 --> 09:58.160
the point where like, you know, there is some amount of stagnation because like most of the good

09:58.160 --> 10:03.760
candies can take up and people are like looking at the old grimy like moldy stuff that's like,

10:03.760 --> 10:08.160
you know, maybe there's still something in there. It's like it's like the like the sloppy seconds

10:08.160 --> 10:17.280
on the the research, you know, pinnata. With that analogy in mind, you know, where should research

10:17.280 --> 10:23.280
be swinging the bat? Do you have, is that a, is that it, you know, it's a crystal ball kind of question,

10:23.280 --> 10:29.040
but what is your intuition tell you where opportunities might be?

10:29.920 --> 10:33.920
Well, I always look for, you know, what, what is it that we actually care about? When people

10:33.920 --> 10:40.400
are selling a story, an aspirational story about, you know, I'm not like a mathematician first,

10:40.400 --> 10:44.640
I'm not just like, what's a hard problem? Like, which is all for that reason, you know, I got into it

10:44.640 --> 10:49.680
too late, you know, I kind of back into it from like, what's the, like, what's the dream?

10:49.680 --> 10:52.960
And if you look at like the dream people are selling people, even people with like existing

10:52.960 --> 10:57.040
companies right now, the claim they're making, like if you looked at like IBM Watson, which

10:57.040 --> 11:00.880
went up in flames, if you look at the claims they were making, like, what are we going to do for you?

11:00.880 --> 11:04.000
It's like, we're going to, you're going to make better decisions, you're going to provide

11:04.000 --> 11:08.560
personalized health care, you're going to help people to, you know, have better health outcomes

11:08.560 --> 11:13.840
and otherwise would have without our AI. If you look at this kind of stuff, you know,

11:13.840 --> 11:18.000
what are people selling? What are people hoping to actually achieve? And then you look at like,

11:18.000 --> 11:22.320
what are people actually doing? And if I like, I always kind of look back and forth between those

11:22.320 --> 11:27.040
and say, like, what's, what's like the missing part? Like, if you actually want to realize the dream

11:27.040 --> 11:31.600
of what, you know, people have to seem to want, what's, what's gone? Like, what's, what's not even

11:31.600 --> 11:37.040
being addressed in a, in a mature way? And so I think one, one thing that sort of jumps out

11:37.040 --> 11:41.680
is that everybody's sort of premise, you know, everyone says like these things are good,

11:41.680 --> 11:46.720
it's always based on some notion of like accuracy or the return of an RL system as evaluated on

11:46.720 --> 11:50.880
some fixed static environment. And, and then you look at what people actually do and it's like,

11:50.880 --> 11:55.840
they're taking some model, trained in some context on some set of data and deploying a crop in

11:55.840 --> 12:02.160
some different environment, which is changing in unpredictable ways. And where the whole environment

12:02.160 --> 12:08.240
is not just changing like in a kind of benign or passive way, often it's changing in direct

12:08.240 --> 12:12.720
response. So like, you know, like think about Google search, right? You deploy Google every single

12:12.720 --> 12:16.400
time they tweak their algorithm. What's the first thing that happens? And it's like all the message

12:16.400 --> 12:22.080
boards light up and all the SEO goons are like, oh, like SEO change the algorithm. Now you need to add

12:22.080 --> 12:27.360
this keyword. You need to do this. And I think that ML doesn't address that kind of stuff. I might

12:27.360 --> 12:32.640
say ML doesn't. I don't mean nothing that we aspire to in a hell, but I mean like the, the main

12:32.640 --> 12:38.000
thing, you know, the main thing that practitioners do, the toolkit, the mature one, the like, you

12:38.000 --> 12:43.200
know, I know how to use PyTorch and train, you know, ResNet and ResNet and whatever, whatever.

12:44.080 --> 12:50.160
That world, it's like completely set in an environment of like, I train a model, evaluate on a,

12:50.160 --> 12:55.120
so like, I ID holdout set. Or even if you evaluate on some kind of challenge set, it's not like

12:55.120 --> 12:59.360
with any coherent principle for why you should expect this model to do well on that challenge set,

12:59.360 --> 13:02.640
or why you should think the performance on that challenge that's representative of what you should

13:02.640 --> 13:09.040
encounter in the world. So I think performing in a dynamic world, making decisions and not just

13:09.040 --> 13:12.080
predictions, right? Because everybody's sort of saying, ultimately, if you think you're going to

13:12.080 --> 13:15.840
make money off of this, or you think you're going to affect some kind of like, societally

13:15.840 --> 13:21.360
beneficial outcome by using AI, if you think you're going to do anything, then ultimately it's like

13:21.360 --> 13:26.800
what you're, the claim at some some point, what you're hoping to do is guide some kind of decision

13:26.800 --> 13:30.320
or automate some kind of decision, right? You're actually hoping to have an outcome, not just

13:30.320 --> 13:35.360
to like be a passive observer to the world and make accurate predictions about what would happen,

13:35.360 --> 13:40.400
were you not to take any action at all? And, and this kind of setting of like actually providing

13:41.040 --> 13:46.720
guidance for what you should do in the world is, is we are, you know, it's, it's a thing like,

13:46.720 --> 13:50.480
yeah, there are people working on causal inference. There are people who are trying to bring

13:50.480 --> 13:55.360
reinforcement learning closer to the real world by, you know, maybe incorporating some ideas

13:55.360 --> 14:00.160
and causal inference like to think about confounding that might exist in the data to be able to

14:00.160 --> 14:04.400
build models and off policy kind of way, you know, so that you're not just saying I'm just going

14:04.400 --> 14:10.480
to deploy some randomly acting system in an important application and have it stuck for two

14:10.480 --> 14:16.800
million years until it learns. But they're relatively mature and then they get relatively

14:16.800 --> 14:20.160
little attention. You're going to look at like, you know, what are people, you know, what is,

14:20.160 --> 14:23.280
you know, I'm not going to beat up on our buddies in the press, we say, what is like, you know,

14:23.280 --> 14:27.920
Cade going to write a big article about in the New York Times, it's not typically like the,

14:27.920 --> 14:32.720
the, the, the, the slog of like scientific advances and making robust machine learning or,

14:33.040 --> 14:39.440
um, you know, off policy RL or something like this, it's, it's, uh, you know, there's a big

14:39.440 --> 14:43.520
neural network that, you know, is nine trillion parameters and a billion dollar investment from

14:43.520 --> 14:50.560
Microsoft and this kind of, and, and so I think right, um, decision making, uh, robustness and

14:50.560 --> 14:56.800
dynamic environments and, and actually addressing certain societal desert or audit, you know,

14:56.800 --> 15:04.000
people have sort of noticed the problems that arise in terms of the, um, ways sort of,

15:04.000 --> 15:09.760
like AI system can affect whether it's like, um, on ethical sort of outcomes if you plug them

15:09.760 --> 15:12.880
and night evenly into certain decision making systems, but the sort of like,

15:14.560 --> 15:19.280
field of actually like developing systems that could, in some coherent way,

15:19.280 --> 15:23.440
align with societal desert is quite primitive, right? So there's like a recognition that there's

15:23.440 --> 15:30.800
a problem, but we're, we're very early stages on getting towards solutions. So I think that,

15:30.800 --> 15:37.200
to me, like, these are the areas I think are, are more interesting, um, you know, in that like,

15:38.000 --> 15:43.840
like, if you can get, if you can squeeze half a point out on like, you know, all the NLP benchmarks by

15:43.840 --> 15:48.240
like making a slight variation on birth, you'll get a lot of citations that everyone will use it,

15:48.240 --> 15:54.320
and they should, and it is useful, but it's, I feel like not, it's, it's like a direct, it's a,

15:54.320 --> 15:59.680
a slight change in degree, it's not a change in kind. And so I think, you know, when you,

15:59.680 --> 16:04.160
when I look at it the field, I think the, the luxury of being an academia, the reason of being an

16:04.160 --> 16:09.680
academia is to think that like, I don't have to just think, how do I do epsilon better than someone

16:09.680 --> 16:15.200
at the same crop we're all doing tomorrow? But like, what, what actually is something that,

16:15.200 --> 16:21.600
you know, addresses, you know, some problem that nobody's even engaging with intellectually right

16:21.600 --> 16:29.840
now? It sounds like your answer then to the where to swing the bat is, um, and getting closer to,

16:29.840 --> 16:37.760
you know, real world problems that that folks are having. And you mentioned a lot of different

16:37.760 --> 16:45.600
elements, um, you know, I heard some aspects of domain generalization than there. I heard, um,

16:45.600 --> 16:50.800
you know, aspects of even like user interface, like how you're presenting the information,

16:50.800 --> 16:56.960
heard aspects of fairness in there, but broadly, it sounds like you're kind of also calling

16:56.960 --> 17:02.240
into the question kind of the simplification that often happens in research of problems that

17:02.240 --> 17:08.880
removes them from, you know, all of the constraints and fuzziness of the real world. Yeah, and look,

17:08.880 --> 17:15.840
it's tricky, right? Because everybody's, everybody thinks they're doing that? Well, it's more like

17:15.840 --> 17:21.360
everybody's got to choose the focus on something and, and to focus on the thing they want to focus

17:21.360 --> 17:26.800
on, they got to compromise on something else. Um, and, and it's not that like one thing is right

17:26.800 --> 17:30.560
or wrong. Like, I don't think it's wrong if there's people out there building bigger language

17:30.560 --> 17:37.280
model. I don't think that's fundamentally wrong. Um, I think you got to be like, um, you know,

17:37.280 --> 17:42.240
maybe like you got to use a brain to think about how, how, how, what kind of claims I can make

17:42.240 --> 17:47.360
about these things or, or how should they be used in the real world? But I think like, look,

17:47.360 --> 17:51.280
it's interesting to turn that knob and say, what if I make this big or what, you know, what

17:52.000 --> 17:59.200
happens? Um, there, there, there's plenty of work to be done. Um, you know, like, like one

17:59.200 --> 18:05.600
cutting on like trade off that you often have is that, um, you know, you have like, if I want to

18:05.600 --> 18:10.320
get close to what real data looks like, you know, and I want to get close to things I can actually

18:10.320 --> 18:17.680
do in a bunch of domain, often like all that's available is, you know, uh, data is, um, like,

18:17.680 --> 18:22.480
there's, there's a way that like predictive modeling and like the status quo, you know, is closer

18:22.480 --> 18:28.480
to the real world and that it, it touches real data and it gets within its like narrow aspiration

18:28.480 --> 18:34.560
of like, just predict well on like, IID data, like under a naive assumption of how the world doesn't

18:34.560 --> 18:41.280
change, it's able to do that on really complex, high dimensional real world data. Um, on the other

18:41.280 --> 18:47.920
hand, what you give up when you focus only on just that problem is any kind of consideration of,

18:47.920 --> 18:52.240
you know, I think people just only think about how do I get better predict, you know, building

18:52.240 --> 18:58.640
predictive models, um, is, you know, like, they're getting close to like the dealing with real data,

18:58.640 --> 19:02.480
but they're asking a very narrow set of questions about it, which is like, how do I get higher accuracy?

19:03.440 --> 19:10.480
Then on the other side, you know, you have folks say, for example, trying to get at like, um, fundamental

19:10.480 --> 19:17.440
questions about, um, what sort of like causal query is I can make. And very often in order to flesh

19:17.440 --> 19:24.080
out those questions, now they're taking on like a more ambitious set of like, kinds of queries

19:24.080 --> 19:28.160
that I can ask, but in order to make progress, like understanding the fundamental form of those

19:28.160 --> 19:33.360
questions is often starts with, well, I got to analyze like fundamentally one of these questions

19:33.360 --> 19:36.960
even answerable from the data sets that I have. And in order to like maybe get some of that

19:36.960 --> 19:40.560
analysis to go through, I have to make some simplifying assumptions about the form of the data,

19:40.560 --> 19:44.960
like, I assume that the whole world is linear and it's not high dimensional and it's not whatever.

19:44.960 --> 19:50.400
So, you know, you have plenty of people who are, who are doing, you know, work that's like really

19:50.400 --> 19:54.880
more like ambitious and expansive on the front of like the kinds of questions I can ask,

19:55.600 --> 20:01.680
but they're making really simplifying assumptions, uh, in terms of like the source of data I have

20:01.680 --> 20:06.960
and the number of variables I have and, uh, not worried about that, but I mean, that's to compromise,

20:06.960 --> 20:13.520
they make it as other folks building predictive models and trying to get close to like, do something

20:13.520 --> 20:18.800
that works on real data, but be naive about the kind of questions you can ask and, you know,

20:18.800 --> 20:22.880
not worry too much about just how limited is like what you could do with those predictions or

20:22.880 --> 20:27.600
their power to guide decisions in the real world. And, and then I think once you have that kind of

20:27.600 --> 20:31.760
tension of like, okay, everybody's looking at something and not looking at something else,

20:33.040 --> 20:37.840
you know, I think the question you have is like a research community is like, are you overleveraged

20:37.840 --> 20:44.000
somewhere? You know, I think oftentimes people, there's like a naive form of a criticism,

20:44.000 --> 20:48.880
which is like, oh, this thing sucks and this thing is good, but like there's a more mature version

20:48.880 --> 20:56.240
of it, which is like, we're way overleveraged on this thing and paying way too much attention

20:56.240 --> 21:01.600
to this thing and neglecting these other things. So it's like, you know, like more matter of like

21:01.600 --> 21:05.840
moving the needle. It's not that like nobody should be building a bit of bigger language model,

21:05.840 --> 21:12.240
or or tinkering with architectures, but it's sort of like, okay, like we're at a point where

21:12.880 --> 21:20.800
we're not getting nearly as much juice per squeeze doing that. Why do we have 99% of the community

21:21.680 --> 21:27.360
engaged in this? Why do we have so many papers that are being submitted that most of which know,

21:27.360 --> 21:32.880
you know, are not actually contributing anything either as an idea or as a result? And so,

21:32.880 --> 21:39.680
yeah, I think we're sitting in like some funny terrain like that. So were there notable

21:40.320 --> 21:47.680
papers or research advances that you think kind of poked at some of these issues that you're

21:47.680 --> 21:53.760
raising or you think are swinging the bat in the right direction? Yeah, I think there's

21:54.560 --> 21:58.160
this very weird climate now, which is like, I think for a lot of these questions, you have sort of

21:58.160 --> 22:08.800
like a growing recognition that there are problems, but then you have like a subset of people that

22:08.800 --> 22:15.200
are just kind of like taking advantage of the way in which the like peer review system is like

22:15.200 --> 22:21.520
overtaxed and scattered and like sort of just using the language of those problems, but not

22:21.520 --> 22:24.720
actually addressing them. And I think you see this in all of these and you see this in the

22:24.720 --> 22:28.800
fairness literature. I think you see it in the robustness literature. I mean, you see in the

22:28.800 --> 22:32.880
causality, like in that people submitting papers that sound like they're addressing causal problems,

22:32.880 --> 22:37.280
they're not actually people just saying this model's robust in a way where it's like, by the way,

22:37.280 --> 22:44.240
you can never just say a model is robust. Like if you state nothing about the ways in which the

22:44.240 --> 22:51.280
environment is allowed to change and, you know, there's no such thing as like general robustness,

22:51.280 --> 22:57.440
right? Because I can pose two different assumptions about the world where in one of them when the

22:57.440 --> 23:01.040
environment changes, you know, this is what I should be doing. And the other one I want to change

23:01.040 --> 23:05.600
is that's what I'm should be doing. I have no way of discerning which world I'm in, right? Like

23:05.600 --> 23:08.960
a class would be like, do I live in the labelship assumption? Like if I make that assumption,

23:08.960 --> 23:14.000
that like that distribution of categories is changing, but the class conditional literature,

23:14.000 --> 23:18.720
what does it, what does, you know, COVID versus not COVID look like is what's not changing,

23:18.720 --> 23:23.040
but the prevalence is changing. First of all, I assume that like the covariate distribution is

23:23.040 --> 23:27.520
changing, but that the label, the conditional, the probability of the label given the input is

23:27.520 --> 23:33.040
different. I might have no way of discerning whether I'm in world A or world B, but, you know,

23:33.040 --> 23:37.920
one thing is the right, like the robust model in this setting at, you know, should do this,

23:37.920 --> 23:43.040
and the robust model in that setting should do that. So you have like a set of people that are

23:43.040 --> 23:47.200
just kind of doing the deep learning thing, which, you know, like prediction lets you get away

23:47.200 --> 23:51.280
with it, like let me throw spaghetti at the wall, because I get to evaluate on the whole

23:51.280 --> 23:55.760
doubt data, and like how well I'm doing is identified. So I don't have to be able to state in

23:55.760 --> 23:59.440
terms of any principle. If you get to like a causal effect, you don't get to observe the causal

23:59.440 --> 24:02.800
effect. So it's like, if what you're doing doesn't actually identify the causal effect, you could

24:02.800 --> 24:06.400
call it a causal, you could, you could, you could just like use the language of causality,

24:06.400 --> 24:11.520
and it'd be quite in favor of not actually like addressing causality in any kind of sound way,

24:11.520 --> 24:16.240
and fool a reviewer, but not necessarily be doing it. And so, you know, I think the thing that,

24:16.240 --> 24:20.080
like a lot of these other problems are kind of more foundational, like they're not problems where,

24:20.080 --> 24:23.600
like we know how to evaluate systems, let's have people try stuff and whatever their problems were.

24:24.800 --> 24:31.360
So I'll give you an example, like in, you know, the distribution shift world, I mean,

24:31.360 --> 24:35.280
there's, I think a handful of things people are doing that are a little bit more interesting

24:35.280 --> 24:43.120
or sound or actually giving a path forward. There, there's a group at Stanford, some of Percy's

24:43.120 --> 24:51.280
students, Shiori and Pongway among down, put together this really expansive benchmark called

24:51.280 --> 24:58.400
Wilds, and it's a, like a collection across a whole lot of different application domains of a whole

24:58.400 --> 25:02.720
lot of different settings where you have some kind of subpopulation shift or some other kind of

25:02.720 --> 25:09.680
distribution shift, and it provides like a sort of unified resource for a whole bunch of settings.

25:09.680 --> 25:14.960
Again, like you still need to have some kind of, you know, you can't just like use the data set

25:14.960 --> 25:18.800
and say, oh, I tried this thing in this one domain and it generalize well to these two others,

25:18.800 --> 25:25.760
therefore it's robust, but at least it gives you some, like unified resource for asking a question,

25:25.760 --> 25:29.200
you know, like if you compare to the world where basically people are just saying, like I have

25:29.200 --> 25:35.360
pictures of eminus images and then eminus images on funky backgrounds, I think it's like a

25:35.360 --> 25:40.480
big advance towards like a nice sanity check and putting people in touch with the sorts of

25:40.480 --> 25:46.400
problems that are arising in the real world. There's one formulation of these domain

25:46.400 --> 25:55.040
adaptation settings is that there's a version called domain generalization and here it's like

25:55.040 --> 26:00.000
sort of saying I have like a bunch of different environments that I collected data from and now I

26:00.000 --> 26:06.080
have, you know, I want to generalize well to target environments, possibly using the fact that like

26:06.080 --> 26:09.760
I can look at the different source environments that I've had and they're actually marked out of

26:09.760 --> 26:14.160
different environments. I could try to see something like what's stable versus unstable across

26:14.160 --> 26:21.280
environments. And there've been some interesting papers. So by the way before we met, I ping some

26:21.280 --> 26:24.960
of my students be like, what do you think are some of the interesting papers so I want to give some

26:24.960 --> 26:30.880
credit to my students who are now like the extension of my memory. So my students are appointed

26:30.880 --> 26:34.480
out. There's a lot of interesting work where you have a whole lot of methods that are proposed

26:35.440 --> 26:43.200
but it turns out that if you set up a really rigorous baseline and listen papers, some from CMU

26:43.200 --> 26:53.440
from my friend Alon Rosenfeld and is advisor under SESCI, some from David Lopez Paz at Fair,

26:53.440 --> 26:58.320
but where they've shown things that like for a lot of these setups, it's really, really hard

26:58.320 --> 27:02.960
to be like really stupid baselines, like just dump the data together and just do ERM on it,

27:02.960 --> 27:08.240
like just train on all the data together and don't use the environmental labels in any sophisticated

27:08.240 --> 27:14.240
way. You know, in our own lab, my students are, I've been making a lot of progress on these

27:14.240 --> 27:19.360
distribution shift problems and we have some results that we've been excited about, like among

27:19.360 --> 27:26.000
other things, working out when you're presented with, you know, you've trained on data from,

27:26.640 --> 27:30.480
you've seen, you have some classes you've seen before and then suddenly at test time, you have

27:30.480 --> 27:36.080
some data that shows up from some additional class that like you never saw before. Can you actually

27:36.080 --> 27:41.840
on the fly, look at, you know, this previously seen data from from some classes and now additional

27:41.840 --> 27:47.600
data from some from some unknown class and identify like, oh, I can I say exactly precisely what

27:47.600 --> 27:52.560
fraction of the new data is from some previously unseen class and even develop a classifier that

27:52.560 --> 27:57.200
can now start predicting it so to say, oh, I think these samples have this probability of

27:57.200 --> 28:02.320
belonging to that class. So you'd imagine that like in the context of like a model monitoring pipeline,

28:02.320 --> 28:06.880
you'd eventually like live in that world where if the world changes in some way, the model could

28:06.880 --> 28:11.840
go back to you and say, hey, I think with high probability, like, you know, at least 20% of your

28:11.840 --> 28:16.320
new data actually belongs to some new class that you've never seen before and here are some examples

28:16.320 --> 28:21.280
that I think belong to that class and then you could sort of, you know, take some kind of corrective

28:21.280 --> 28:31.760
action if you think that the model's wrong. So that's on the robustness side. On the causality side

28:31.760 --> 28:37.120
and I got some of these tips from my student, Shun Senua, some of the work that we've been talking

28:37.120 --> 28:46.560
about and going over, I think there's so causality research is really exciting because it actually

28:46.560 --> 28:50.960
gets to the question we care about, which is like, what would happen if I did this versus what

28:50.960 --> 28:55.360
would have happened if I were just a passive observer watching the decisions get made as they

28:55.360 --> 29:00.560
always are and and causality gives like a philosophically coherent way for answering those kinds of

29:00.560 --> 29:07.040
questions. But the danger is that those answers are almost always predicated on some pretty strong

29:07.040 --> 29:14.320
assumptions that like our, you know, I can learn this like the parameters of my causal model,

29:14.320 --> 29:20.560
but the structure of the causal model is given like a priori and I know it exactly and there's

29:20.560 --> 29:26.000
no one observed confounding that, you know, can make all my results invalid. And so there's a lot

29:26.000 --> 29:33.120
of interesting things happening among them. There's some folks like Carlos Chinelli who just started

29:33.120 --> 29:37.680
a faculty job in the physics department at UW have been doing a lot of interesting work on

29:37.680 --> 29:44.000
sensitivity analysis. So if there's measurement error or if there's some, some omitted variable bias

29:44.000 --> 29:50.800
or something, just how, you know, like frameworks for being able to say just how much would there

29:50.800 --> 29:55.520
have to be for me to change my causal conclusion, right? So getting towards like, I'm not just saying,

29:55.520 --> 30:01.600
oh, if I'm nailing all these ridiculously precise assumptions about how the world is,

30:01.600 --> 30:06.480
then this is the answer to your public query, but saying like, you know, this is how far off those

30:06.480 --> 30:12.240
assumptions would have to be for like me to like have to totally change my mind. Eric Chuchin Chuchin

30:12.240 --> 30:18.640
is a researcher at Wharton's statistician who does a lot of exciting work in this area and

30:18.640 --> 30:22.960
Johnson to hit me to a paper that he's doing which addresses a specific problem of, you know,

30:22.960 --> 30:29.680
people often make this assumption that there's no unobserved confounding. And, you know, that is

30:29.680 --> 30:33.360
such a strong assumption because it's like, even if you have the right confounder, if you just

30:33.360 --> 30:38.400
measure it in a slightly noisy way, then there's unobserved confounding. And so he's gotten to

30:39.360 --> 30:43.360
this formulation we call proximal causal learning. And it's like, you can allow that, okay, I have

30:43.360 --> 30:49.840
some proxies for the underlying confounders, but they're not perfect proxies. And what can I do

30:49.840 --> 30:55.600
in that situation? And finally, one thing on the causal inference side that I've been

30:55.600 --> 31:02.480
really excited about is, you know, a whole lot of machine learning just sort of takes

31:02.480 --> 31:09.920
this stance, which is like, I've got this set of variables and I've got a collection of examples.

31:09.920 --> 31:13.360
It's like something like, you know, my data looks something like a table. Now it could be kind

31:13.360 --> 31:17.440
of complicated because if it's like texts, the different documents could be different length,

31:17.440 --> 31:22.640
but it's sort of the typical formulation that people work with don't usually allow for

31:22.640 --> 31:27.440
the setting where it's like, oh, I have a collection of a bunch of different data sets and I observe

31:27.440 --> 31:30.720
this thing in this data set and this other thing in that data set. But I feel like a lot of real

31:30.720 --> 31:37.200
world decision making is actually governed by that kind of process. I think we've all gotten a

31:37.200 --> 31:41.600
little bit of a crash course in this from just watching like the like COVID response, like

31:41.600 --> 31:46.480
unfold in the public eye. And it's like, oh, I've got this data from the CDC, but it has these

31:46.480 --> 31:49.520
features, but it doesn't, you know, it tells you how many reported cases, but it doesn't tell you

31:49.520 --> 31:54.720
how many tests are run, you know, but oh, I have this other data from the manufacturers of diagnostic

31:54.720 --> 31:59.440
equipment and that data actually tells me what fraction of tests are positive, not just what number

31:59.440 --> 32:04.720
of tests are positive. And I have this other data from, you know, the local municipalities.

32:04.720 --> 32:09.360
And so you get to these questions where it's like, if I have some question where, you know,

32:09.360 --> 32:13.440
I think very often we have queries, especially in economics, this comes up to call like data fusion

32:13.440 --> 32:20.320
type problems, but where like the answer can't come like directly, I have no one data set that

32:20.320 --> 32:24.080
can necessarily answer my query, but I have a whole bunch of different data sets and it's possible

32:24.080 --> 32:28.400
that if I combine them intelligently, I could sort of like try and relate to the answer to the

32:28.400 --> 32:34.320
question that I have. We talk about that on the infrastructure side as well. Is there kind of

32:34.320 --> 32:38.960
terminology evolving on the machine learning side for thinking about problems like this?

32:38.960 --> 32:46.160
For some reason, it also calls to my graphical kinds of things in that you want, you'd imagine

32:46.160 --> 32:50.480
some kind of connectedness in the data and the way they represented to one another.

32:50.480 --> 32:55.200
Well, you know, in the econ world, they call these like data fusion problems and

32:56.560 --> 33:01.360
someone who's done some really interesting work on that from like the AIML side is a researcher

33:01.360 --> 33:07.760
named Elias Ferenboim. So he's a professor at Columbia and he was a Udapuril grad student and

33:07.760 --> 33:13.040
now he's a prophet, he's doing a bunch of, I think a lot of the, you know, super exciting work

33:13.040 --> 33:19.280
in this area. And, you know, he's gotten to these sort of questions where, like is this paper from,

33:19.280 --> 33:24.320
I don't know if it was technically 2020, but I read it in 21, so we can call it 2021.

33:25.920 --> 33:31.280
But on an algorithm, what he calls like the general, general identifiability problem.

33:31.280 --> 33:36.720
So it's not just saying like, oh, I've got this one data set, is this thing, you know, can I answer

33:36.720 --> 33:40.960
my causal query? But it's like, oh, I've got this collection of data sets. And in this data set,

33:40.960 --> 33:46.080
these are variables are observed. And this other set, this other variables observed. And maybe this

33:46.080 --> 33:51.120
data set was collected by someone doing a particular kind of experiment on one of the variables.

33:51.120 --> 33:54.960
And this other, you know, it's like, I might have different data sets from different experiments.

33:54.960 --> 33:59.440
They're not even necessarily just different views of like the same data. One of them, someone was

33:59.440 --> 34:03.520
intervening in some way. But it's, if you kind of have this collection and data sets and some

34:03.520 --> 34:09.520
underlying causal structure, now can you tell me precisely, how can I combine all these data sets

34:09.520 --> 34:13.680
to answer the question that you have? Or, or, or I guess like, you know, with causal questions,

34:13.680 --> 34:19.600
always the first step is, is it possible to identify, you know, the, the answer to the question

34:19.600 --> 34:24.320
that you have based on the data that's available? And then, you know, if you can, well, give me,

34:24.320 --> 34:28.400
give me the formula such that if I plug in the data, different data sets, I could, you know,

34:28.400 --> 34:33.600
it would give me that, that estimate. So it's like, is it estimable? And if so, like, how do I

34:33.600 --> 34:40.960
produce such an estimate? Yeah, so there's, you know, I, these are general, exciting areas. There's

34:40.960 --> 34:48.000
also a lot of work happening now in causal discovery. So this is a really ambitious problem.

34:49.280 --> 34:53.200
Because, so in causal inference, you basically say, I know the structure of the causal

34:53.200 --> 34:57.280
graph. I know which variables potentially cause which other variables. But I just don't know the

34:57.280 --> 35:03.120
functions that determine, you know, so it's like x, you know, x and y together influence z.

35:03.120 --> 35:08.480
I don't know what is the function by which the values of x and y determine z. But I know that like

35:08.480 --> 35:13.280
z listens to x and y. Like if I were to intervene on x that could potentially change the value of z,

35:13.280 --> 35:17.840
whereas if I were to intervene on z, it wouldn't change the value of x, right? And so if you have this

35:17.840 --> 35:22.480
kind of structure, causal inference says, well, how do I like figure out, you know, basically,

35:22.480 --> 35:28.000
what are those functions that I can then answer a causal query? But there's like a sort of,

35:29.040 --> 35:33.920
like in that by itself, it's super hard. And, you know, we always never agree on causal effects

35:33.920 --> 35:38.080
because it's like, well, if you assume the graph looks like this, and it's slightly different,

35:38.080 --> 35:43.280
or, you know, than all bets are off, causal discovery basically says, what if I don't even know

35:43.280 --> 35:47.760
the graph that for you, or I have some partial knowledge of the graph, but I don't actually know

35:47.760 --> 35:54.080
fully which arrows, you know, go from which variables to which other variables. So in that case,

35:54.080 --> 35:59.280
you know, you, you ask this question of like, well, what, when is it possible to recover the graph?

35:59.280 --> 36:03.520
And in general, you can only recover the graph up to like something called an equivalence class.

36:04.640 --> 36:08.080
But now there's a whole bunch of other papers and I don't have all the links that I can send them

36:08.080 --> 36:12.960
to you offline, but things really start asking questions that says, okay, like, well, if I'm able to

36:12.960 --> 36:17.520
use causal discovery to get the graph up to like equivalence, well, now I can ask

36:17.520 --> 36:22.480
question like, what sort of experiments should I run in what order to as efficiently as possible,

36:22.480 --> 36:27.680
resolve any lingering ambiguities? So like, right, just the observational data might at least tell

36:27.680 --> 36:31.600
me something like certain variables aren't connected to other variables, and I'm able to

36:31.600 --> 36:35.680
orient for some edges in the graph, like which direction do they point, but others I can't,

36:36.320 --> 36:41.280
you know, but then, you know, the hope of causal discovery is to be able to additionally do that.

36:41.280 --> 36:45.600
So that's one, you know, exciting thing. And my student Charlton who's been working a lot in that

36:45.600 --> 36:51.040
area, yes, we have a paper that gets at this question of if you get to make these decisions about

36:51.040 --> 36:55.040
kind of like I was telling you before, if you have different data sets and by combining them,

36:55.040 --> 36:59.440
you can answer a question, but not necessarily by using one of them alone, or even if you can

36:59.440 --> 37:03.600
combine them to answer the question, there's still an unresolved question of how much data should

37:03.600 --> 37:08.080
I collect from this source versus that source in order to like as efficiently as possible pin down

37:08.080 --> 37:12.320
the causal effect? We've been working on that problem of basically, how do I like, you know,

37:12.320 --> 37:16.160
imagine you're working at a company and you have some third-party data provider that charges you

37:16.160 --> 37:22.320
for, you know, I participate this much per thousand examples, right? Like, you know, how would I make

37:22.320 --> 37:27.200
the decision sequentially of like, okay, based on what I know now, which data source should I query

37:27.200 --> 37:32.080
next and for how many samples? And okay, now I update my beliefs, I make a subsequent decision,

37:32.080 --> 37:36.800
which I think this decision process is always going on in the background, right? Like if you're

37:36.800 --> 37:43.280
a company that's buying data from people or going out and actively, you know, doing some kind of

37:43.280 --> 37:48.560
monitoring effort data collections, you're making decisions on the fly about, oh, I want to collect

37:48.560 --> 37:53.120
data from here. Oh, now I know something I didn't know before. This is going to guide my decision

37:53.120 --> 37:58.160
of what to collect next, but we don't usually formally model that process. We usually sort of assume

37:58.160 --> 38:02.960
the data is already there and then focus on how do you estimate, you know, something given that the

38:02.960 --> 38:10.240
data is there. So, yeah, those are, you know, some, some are so excited about that direction.

38:10.240 --> 38:16.240
And I think on the fairness side, I think one way that things are maturing is that people have been

38:17.840 --> 38:22.080
posing these questions in ways that are maybe, I don't know if you're familiar with this, a

38:22.080 --> 38:27.840
philosopher Charles Mills who passed away recently. Yeah, so he's, he's, this is great, like,

38:27.840 --> 38:38.160
moral and political philosopher. And he writes about, sort of like ideal approach to theorizing

38:38.160 --> 38:45.760
about questions of justice. And in, and I think it, you know, and my student, I had a post-doc

38:45.760 --> 38:49.680
in graduate, Cena Fuzzle, who was now a professor at Northeastern, but we wrote a paper recent,

38:49.680 --> 38:54.240
like a couple of years ago, just, just making a connection between what's going on in ML and,

38:54.240 --> 38:58.400
and this sort of framing of ideal versus non-ideal theorizing about justice that comes from,

38:59.760 --> 39:05.120
among other folks, Charles Mills. But, you know, he has this point that, you know, when you start

39:05.120 --> 39:10.720
posing like a question about equity or question about justice as a sort of, like, technical problem,

39:10.720 --> 39:17.200
and you, you make up a toy model, there's this danger that, you know, you, you sort of highlight

39:17.200 --> 39:24.240
as, like, salient and relevant, those parts of the problem that are captured by, like, your toy model.

39:24.800 --> 39:31.280
And you relegate as, like, not even of, you know, academic consideration, everything that doesn't

39:31.280 --> 39:37.200
show up in your model, right? So I think that, and the danger here is that, like, if the things that

39:37.200 --> 39:42.000
you're just, like, completely forgetting about are actually, like, the, everything that really

39:42.000 --> 39:46.800
matters, that you wind up in a situation where you could do a lot of academic tinkering and you could

39:46.800 --> 39:52.800
even develop, like, elegant mathematical theories, but they have almost nothing to say about the

39:52.800 --> 39:56.800
underlying question of justice that you care about. And I think this is sort of the situation that

39:56.800 --> 40:01.440
we've been in to some degree, and it's not to implicate everyone, but it's, say, like, the, the main

40:01.440 --> 40:05.920
thing, right? Is that we've been posing these questions of equity in the form of, like, say,

40:05.920 --> 40:12.240
I have a data set, say, I have a particular feature, let me just sort of start enumerating different

40:12.240 --> 40:17.200
things that should be equal. And I'm saying, oh, it's not possible to make them all equal simultaneously.

40:17.200 --> 40:23.440
So let's either just, like, naively pick one and then flesh out an algorithm for it, or just kind

40:23.440 --> 40:30.080
of, like, pying about how fairness is impossible. And I think, like, what gets lost in that whole

40:31.600 --> 40:35.280
kind of discussion is that it's almost all of it. It's, like, picking for granted, just,

40:36.400 --> 40:40.160
I've got a data set. There's a bunch of anonymous features. I don't really say anything about

40:40.160 --> 40:46.720
what they actually mean, or what real world processes they correspond to, or how disparities arise,

40:46.720 --> 40:56.800
and how that consideration really bears on what is sort of the appropriate response from a

40:56.800 --> 41:02.400
standpoint of, like, affecting justice. Like, when is it, you know, we don't look at every single,

41:02.400 --> 41:10.960
like, we don't look at, I don't know, you don't look at, like, the major league baseball, for example,

41:10.960 --> 41:15.200
and say, like, oh, I said, I noticed there were more players from some country that, you know,

41:15.200 --> 41:18.800
than from some other countries, you know, relatives that are, you know, and say, okay, let me just

41:18.800 --> 41:23.920
equalize it instead of quote a system. But it's also because I don't, you don't believe that, like,

41:23.920 --> 41:27.120
I don't know, say some country that, like, you know, excels in basal, like Puerto Rico,

41:27.120 --> 41:30.720
so you don't think they've been giving an unfair advantage in getting to the major leagues

41:30.720 --> 41:34.480
or something. And so, like, this whole backstory of, like, what actually are the,

41:37.200 --> 41:41.520
what actually do these variables mean, and to the extent that there are disparities reflected

41:41.520 --> 41:48.960
in the data, like, where do they come from, and how do they correspond to some political,

41:50.080 --> 41:57.360
some coherent political stance or theory that sort of makes the straight line from that,

41:57.360 --> 42:01.440
to who has responsible to remediate, who has a responsibility to remediate it. These are,

42:01.440 --> 42:04.960
like, fundamentally, the concerns that we sort of always have when we speak, I think,

42:05.600 --> 42:11.600
in the law or in a broader sense about questions of justice. That's some, for some reason,

42:11.600 --> 42:15.440
I think there's something I think maybe just about the fact that it's a new field or something

42:15.440 --> 42:19.200
like that, but that for some reason, I've been just kind of completely sidelined or completely

42:19.200 --> 42:24.400
as a strong word, but by, like, the main branch of fairness research. And I think there is a

42:24.400 --> 42:31.360
number of people who are, who are doing interesting work here to try to actually ask the critical

42:31.360 --> 42:38.400
questions. And I think, like, Lily, who, and, um, is to call their houseman, uh, or two people who,

42:38.400 --> 42:45.280
I think, just have been, like, kind of, asking the right kinds of questions, um, for, for a while,

42:45.280 --> 42:51.520
and, and kind of framing that critique in a way that I think is what's so rare. It's like,

42:51.520 --> 42:55.840
both really understands what's happening in, like, the sort of fair ML world, and also really

42:55.840 --> 43:01.840
understands the sort of context and, um, like, the people actually understand ethics and actually

43:01.840 --> 43:08.160
understand, like, legal principles of justice and our, um, I think, able to speak from some

43:08.160 --> 43:12.480
degree of authority to sort of what's missing in a way we're opposing those questions and tackling

43:12.480 --> 43:21.280
them. Before, before you jump into their work, the things that come to mind for me are this idea of, um,

43:22.800 --> 43:27.520
you know, techno-solutionism being part of the problem, like, we're trying to, you know, throw

43:27.520 --> 43:33.600
technology at the problems that technology is, is creating for us. Yeah, we talked about that

43:33.600 --> 43:43.200
a couple of years ago. We did. We did. Uh, and also there's kind of a nod in, in the way you talked

43:43.200 --> 43:50.560
about the problem of fairness to causality, and, you know, when we all got really excited about

43:50.560 --> 43:58.000
causality a couple of years ago, you know, and, uh, I think it was that same NURPS, uh, it was,

43:58.000 --> 44:03.040
you know, where everyone left excited about causality. Like, it was supposed to be the savior of

44:03.040 --> 44:09.520
fairness, and it was, you know, applying causal modeling to machine learning more broadly was going

44:09.520 --> 44:16.800
the, you know, give us, you know, transparency, give us fairness, give us, uh, interpretability,

44:16.800 --> 44:23.120
and, you know, break open all of the black boxes and all of that. Like, where is that?

44:24.240 --> 44:28.400
You know, and I think I just, I couldn't more highly recommend, um,

44:28.400 --> 44:34.240
issa and lilies work here that I think, you know, there's a, there's a handful worse, you know,

44:34.240 --> 44:40.000
there's some work by Elias Barron-Boyam and by Ilya Spitzer that has, and, and before some earlier

44:40.000 --> 44:46.960
work by like Matt Kuzner that sort of posed different, um, notions or of that, that are, like,

44:46.960 --> 44:51.760
within a causal framing kind of like coming out of like pearls, causal modeling. So some notions of

44:51.760 --> 44:58.320
like, you know, the earliest versions of that say something like, well, there's a lot of different

44:58.320 --> 45:01.760
versions. If someone wants to say something like, okay, it's not just a question of is race or,

45:01.760 --> 45:06.400
or gender or whatever is considered as protected attribute. Does it turn out to be correlated with

45:06.400 --> 45:10.240
some outcome? We want to ask some question of like, is it what causes the outcome?

45:11.600 --> 45:16.800
Um, and there, there's a way that like these questions have been paused, and it's actually,

45:16.800 --> 45:22.080
you know, the causal framing is not unique to machine learning. It's actually something that, um,

45:23.360 --> 45:28.720
I think like the legal scholarship itself often expresses things in causal terms, right?

45:30.400 --> 45:37.200
You know, um, and before them, you know, I think like economists, for example, like, you know,

45:37.200 --> 45:41.280
there's a famous experiments, right, where people say, well, recent, like, the resume experiment,

45:41.280 --> 45:45.680
that I think, Sindel Millenathan and some others ran, or they, they, they randomized names to be,

45:45.680 --> 45:53.200
uh, tip more, more likely sort of, um, uh, you know, like black Americans sounding as more likely

45:53.200 --> 45:57.360
to be white Americans sounding as, you know, and then they, they, they send the resumes to people

45:57.360 --> 46:01.520
and they measure the response. And it's, it's an interesting experiment. It's certainly like,

46:02.400 --> 46:08.160
uh, uh, valuable research. And the fact that there is, in certain contexts, a difference in

46:08.160 --> 46:16.720
the response race, like, you know, does jump out as, um, you know, problematic, right? On the other

46:16.720 --> 46:23.760
hand, if you sent them out and there was no difference in response race, um, should be conclude in

46:23.760 --> 46:28.640
the other direction, like, oh, there's, there's nothing wrong, right? And, and I think the answer is,

46:29.360 --> 46:32.560
you know, obviously, different people will have different opinions and, and, and the answer might

46:32.560 --> 46:37.120
be answered different in different contexts. But I think that there's a lot of context so much,

46:37.120 --> 46:41.360
I think many of us or most of us would say that that's not necessarily the case, right?

46:41.920 --> 46:48.480
Um, for example, if, uh, right, like if you, if you, if you, if you, like, if you're like, oh,

46:48.480 --> 46:51.920
you know, like, because, you know, it took, what does it mean to just change the name, right?

46:51.920 --> 46:56.240
Like, I could change your, I could change your name, but, um, and that one make a difference,

46:56.240 --> 47:04.000
but if I change what college you went to from, say, like, HBCU to a, um, you know, say some,

47:04.000 --> 47:08.720
some, some other school, and that made a difference, even if your name by itself conditioned on

47:08.720 --> 47:12.640
everything else didn't make a difference. So, so there's this notion that's baked into a lot

47:12.640 --> 47:18.960
of literature that tries to pose questions about, um, discrimination through a causal lens that sort

47:18.960 --> 47:25.840
of tends to adopt a rather like narrow notion of what could constitute discrimination as like the

47:25.840 --> 47:31.280
direct effect of some attribute, like the direct effect of gender, the direct effect of race on a

47:31.280 --> 47:37.840
decision. And the problem is that, well, what about like all of these sort of potentially

47:37.840 --> 47:41.280
indirect effects that could still be, you know, if I were to make, if someone were to make a

47:41.280 --> 47:48.880
decision based on some factor that is super correlated with race and also irrelevant to the decision

47:48.880 --> 47:55.200
otherwise, well, like, would you say that that's not discrimination? Um, and so there's this,

47:55.200 --> 47:58.960
you know, and then there's some work by Elias Barron-Moham and Ily Schbitzer, which is,

47:58.960 --> 48:04.080
I think a sort of step at least conceptually in a, in a more interesting direction, whereas

48:04.080 --> 48:10.480
what they try to do is sort of, you know, if you have like a causal model over all the variables,

48:11.440 --> 48:20.560
you could say something like, well, let me disentangle the, like, the, how the effect of some attribute

48:20.560 --> 48:26.080
of interest, whether it's race or gender, comes to influence some outcome of interest along all the

48:26.080 --> 48:29.840
different sort of plausible causal paths that it might take. And I could sort of attribute,

48:29.840 --> 48:34.640
to what extent does this, you know, influencing the outcome via that variable versus via this,

48:34.640 --> 48:40.560
via this path versus via this other path. Um, now keep in mind though, like, that sort of,

48:40.560 --> 48:45.360
I think what's cool about it is it's like a thinking tool. Like in practice, do we actually expect

48:45.360 --> 48:49.840
that we would have a causal model that captures all the variables of interest and actually says

48:49.840 --> 48:55.680
exactly, we would know precisely which variables influence, like every variable that goes from

48:56.400 --> 49:02.960
somebody's gender, you know, to whether or not they got hired, you know, we'd be able to,

49:02.960 --> 49:06.960
like, we're going to trace like over what scale, like over the scope of someone's entire life,

49:06.960 --> 49:12.960
we're going to, you know, build into our graph, every opportunity, right, every, every, every,

49:12.960 --> 49:16.880
every decision, every opportunity that someone was given or not given on that account,

49:16.880 --> 49:20.960
like, that we're going to have a graph that is so rich as to capture all of that, you know,

49:20.960 --> 49:26.320
it seems unlikely, but at least it gives you maybe like a thinking tool as like, okay, it's it,

49:26.320 --> 49:30.240
you know, at least I can conceptualize and step back and think about the fact that there are these.

49:31.600 --> 49:35.520
But among other things, it outsources the normative work, right? At the end of the day,

49:36.800 --> 49:40.720
presumably that the reason to disambiguate these different pathways is to say

49:41.440 --> 49:45.440
someone believes that, you know, they usually put as like in the terms of like some paths

49:45.440 --> 49:50.880
are permissible, like maybe they run through like unambiguous qualifications for the

49:50.880 --> 49:56.800
position being hired for, versus other paths are sort of like impermissible paths because all

49:56.800 --> 50:01.760
they're really doing is telegraphing information, but they're not actually influencing,

50:02.800 --> 50:08.080
you know, they're not actually, say, relevant to the job qualifications or whatever the context is,

50:08.080 --> 50:13.440
but there's still outsourcing the normative work of someone, well, someone has to go and say

50:13.440 --> 50:18.000
which paths are permissible and which paths are impermissible, and Lily has a really sharp

50:18.000 --> 50:24.400
critique. She also has a nice set of blog posts that are called like disparate causes, I believe,

50:24.400 --> 50:31.200
on this blog phenomenal world, but it goes into this problem kind of critically, and among other

50:31.200 --> 50:36.240
things, you know, getting at this question of what we call like a direct effect or an indirect

50:36.240 --> 50:44.480
effect is partly an artifact of the representation that we have, and there are some causal questions

50:44.480 --> 50:51.520
where for any kind of process that we describe, there's multiple different valid causal representations

50:51.520 --> 50:57.520
conceivably, right, because you can always like zoom into a via this variable in this variable

50:57.520 --> 51:02.080
in an edge between, you know, I can always zoom into it and say, oh, like it's not just that like,

51:02.080 --> 51:07.600
you know, someone's college influences their internship, it's actually that college influences

51:07.600 --> 51:13.360
this subtle decision that's made by some recruiter, which influences this, which influences that,

51:13.360 --> 51:20.720
and so you can always zoom into it and sort of bring more into focus on whether or not someone

51:20.720 --> 51:24.080
would like look, you know, now like a very generic question, like what is the average

51:24.080 --> 51:29.520
treatment effect? Maybe as long as you had, whether you had a very sort of granular or very sort of

51:29.520 --> 51:36.800
um, you know, coarse representation of some process, if they're both valid, you'll have the same

51:36.800 --> 51:42.080
answer for a question like that, but this question about like what are the pathways taken

51:42.720 --> 51:49.040
is sort of like, um, and are they permissible or not? Is sort of an artifact partly of

51:49.040 --> 51:54.240
at what resolution do you zoom into this process and do you capture it? And something might look

51:54.240 --> 51:59.440
okay, you know, if you zoom way out and you subsume a whole lot of mediators into just like an arrow,

51:59.440 --> 52:04.000
but if you zoom in closely and you knew more about how that process took place, then maybe you

52:04.000 --> 52:11.760
would say, oh, this isn't kosher. Um, so I think, you know, at a high level, um, I think that like

52:11.760 --> 52:17.920
causality gives us like a set of thinking tools for thinking critically about some of these

52:17.920 --> 52:24.400
problems, and they are maybe in some way like a partial step in the right direction.

52:25.200 --> 52:32.320
Um, but at the same time, you know, I don't think it's like a magic bullet that like sort of addresses

52:32.320 --> 52:38.960
all questions of, of, of fairness or justice or discrimination, and I think that often,

52:40.160 --> 52:45.360
you know, they're, they're, that the, the sort of like model that was sufficiently rich to be able

52:45.360 --> 52:49.520
to, you know, even if you believe that they were, like you couldn't, you wouldn't actually have,

52:50.400 --> 52:54.880
you know, uh, you wouldn't actually be able to like produce the causal models that you could

52:54.880 --> 52:59.360
fully resolve those questions. And I think what, one nice point that Lily makes, um, and I think

52:59.360 --> 53:06.080
I might have been, uh, um, in joint work with, with Issa, but, but I remember one of the points is

53:06.080 --> 53:11.280
is that, you know, I think that there's a lot of times like a danger is that it could be a little

53:11.280 --> 53:15.920
bit of a distraction, you know, like you said, there's like heroic amount of, you know, I have to know

53:15.920 --> 53:20.400
every single variable and every single thing and estimate every single relation. Uh, and before I

53:20.400 --> 53:25.760
can make any kind of conclusion about whether there's discrimination, um, that there might not

53:25.760 --> 53:29.360
actually be necessary and it's not in general what we do. I think there are situations where we can

53:29.360 --> 53:36.080
size up, um, at, at a bird's eye view that there's, there's some fundamental like inequity in

53:36.080 --> 53:40.960
society and, and, and conclude that we all, that we, we have some responsibility to do something about

53:40.960 --> 53:46.640
it and that that doesn't need to be contingent upon saying that like I've exactly estimated every

53:46.640 --> 53:54.160
single possible, you know, uh, causal functional on the pathway of every single factor that, you

53:54.160 --> 53:59.920
know, plays any role in, uh, you know, on the path to some decision that's made about someone in

53:59.920 --> 54:06.240
their life that that might set like a, um, at the end of the day, like too high a bar that, you know,

54:06.240 --> 54:13.600
you kind of, um, that, that I think that I think we're, we're able to recognize cases of discrimination

54:13.600 --> 54:20.480
and plenty of cases where, where we're not able to do this kind of like, you know, um, hurtfully and

54:20.480 --> 54:27.600
like numerical feed. Let's maybe shift gears and talk a little bit about, uh, use cases or

54:27.600 --> 54:36.640
application areas that have made notable progress in 2021. Um, anything come to mind there?

54:38.960 --> 54:45.280
You know, well, look, um, one, one obvious one and, you know, as much as I might, you know,

54:45.280 --> 54:51.280
be kind of called on as a contrarian, but like, uh, well, one more, I think I'd give some credit is

54:51.280 --> 55:00.080
I think, um, alpha folds from deep mind, um, like I'm not a protein folding expert, but I know

55:00.080 --> 55:05.600
some people that are not just like gullible deep learning boosters, um, who do work in the area

55:05.600 --> 55:14.320
and as far as I can tell, it's like actually a pretty significant, um, leap forward, um, that,

55:14.320 --> 55:18.480
you know, it's work that, you know, could very well have, you know, one like a significant,

55:18.480 --> 55:25.280
you know, science prize, you know, uh, like that level of accomplishment, um, and, you know,

55:25.280 --> 55:29.760
that's a little bit, you know, hearsay and that like, I'm not, I'm not, uh, an expert in,

55:29.760 --> 55:34.880
in protein folding, but as far as I understand it, it really is, uh, a legitimate significant

55:34.880 --> 55:39.120
contribution. And I think an area where, you know, maybe deep learning wasn't quite as,

55:39.120 --> 55:46.240
you know, inlined as a essential tool. So that that's certainly, um, a use case. I think you're

55:46.240 --> 55:55.840
starting to see, um, a lot of the use cases that were maybe obvious ones, um, but not necessarily,

55:56.800 --> 56:01.840
um, you know, for example, like radiology, it's sort of like an obvious initial thing because

56:02.880 --> 56:06.000
and harkening back to like our earlier conversation about the difference between prediction and

56:06.000 --> 56:12.080
decision, part of why radiology is like people see it as like this big target is that, um, you

56:12.080 --> 56:16.000
know, there are certain roles of the radiologists where they really are involved in decision

56:16.000 --> 56:20.720
making and, uh, recognition is part of a weird, it's like an interventional radiology,

56:20.720 --> 56:25.280
but there are also our lots of people who literally are looking at images and making classifications

56:25.280 --> 56:31.040
and medical imaging is a case like a diagnostic kind of imaging, right? And I think that's a case where

56:32.160 --> 56:37.840
we've known since the moment, you know, the kind of big image recognition results started

56:37.840 --> 56:43.280
hitting and say 2012, 2013 that radiology was a potential target and he had some maybe

56:43.280 --> 56:49.440
overly, uh, optimistic statements from like Jeff had been like, if you're, you know, if I were,

56:49.440 --> 56:54.640
if you're a medical school now, do not specialize in radiology, it hasn't quite gotten to that point,

56:54.640 --> 56:58.640
it hasn't taken the radiologist out of the loop, but I've been, I've been chatting with a lot of

56:58.640 --> 57:04.400
radiologists recently and I've been surprised to find, well, sort of two things, one, on one side,

57:05.120 --> 57:11.040
that some of the systems really are quite good and you actually have some systems being

57:11.040 --> 57:17.120
deployed already, actually, like, actually piping information into patient records.

57:18.400 --> 57:22.320
And at the same time that I think some of the problems that we discussed earlier about what can

57:22.320 --> 57:27.200
go wrong are happening on the ground and you do have a situation where, for example,

57:28.640 --> 57:35.600
systems that work well on one set of equipment are not performing well on, you know, some new scanner,

57:35.600 --> 57:41.040
which, you know, you know, it's efficiently similar to all the other scanners that a human

57:41.040 --> 57:45.680
radiologist would have no problem. And these are not adversarial examples, nobody's out there

57:45.680 --> 57:49.520
designing a scanner, there's not like, there's like a radiologist on the, I'm going to, I'm going

57:49.520 --> 57:54.320
to build a scanner that just fucks up all the previous, fucks all the deep learnings, so that way

57:54.320 --> 58:01.920
we can like keep our dogs. So I think you sort of have, you know, both, uh, a moment of the

58:01.920 --> 58:09.360
technology actually kind of like making landfall. But I think you also have some, some moment of

58:09.360 --> 58:17.440
like the rubber hitting the road and people, people sort of seeing up front some, you know, up close,

58:17.440 --> 58:24.000
some, some of the ways in which the technology is brittle and dangerous. Yeah. And I think that

58:24.000 --> 58:28.400
largely, if this might take an unsexy story because it's like what's like the big sexy application,

58:28.400 --> 58:34.880
I think largely this story now. And I think that overall the biggest like, if I were to like,

58:34.880 --> 58:39.760
take a, like a bird's eye view of the economy and just like, I'm just watching like, what is AI

58:39.760 --> 58:45.280
doing? You know, I think, right, the story of like 2014, 2015 is like these new use cases,

58:45.280 --> 58:50.320
like fundamental new things popping up, like things we weren't doing with deep learning suddenly,

58:50.320 --> 58:54.720
like machine translation, people swapping out the old guts and sticking into deep learning

58:54.720 --> 59:00.560
systems. And, um, suddenly like every single mobile phone having the capacity to run some kind

59:00.560 --> 59:06.240
of small deep learning model because it's being used for recognizing objects in the cameras and

59:06.240 --> 59:10.560
doing the face recognition that unlocks your phone and all of that. I think the bigger story of

59:10.560 --> 59:18.000
the last couple of years has been more on the side of, um, more on the side of like, deployment

59:18.000 --> 59:25.040
and like diffusion and like maturity of the, like, uh, like operations around ML. Like, I

59:25.040 --> 59:28.640
notice more and more companies that like their pain point isn't that they need someone who could

59:28.640 --> 59:32.640
train a model. Their pain point is they need an ML ops president. They need someone who,

59:32.640 --> 59:38.800
who can actually keep the crap running day in, day out that someone who knows, you know,

59:38.800 --> 59:44.480
it's like, there's some specialized, it's not, it's like a pure like ML researcher, like someone

59:44.480 --> 59:49.840
like me even, like, I don't have this skill set. I haven't spent my life in like, you know,

59:49.840 --> 59:54.640
there's, there's a real serious discipline and like keeping software working day in, day out.

59:54.640 --> 59:58.640
Like the people, it's, it's amazing what we could do when Eastern companies that like have a

59:58.640 --> 01:00:03.920
software that, you know, product that like 400 million people use every day and they like go

01:00:03.920 --> 01:00:09.760
seven years without a single, you know, hour of downtime. You know, it's absolutely bonkers how

01:00:09.760 --> 01:00:14.960
difficult that is. And machine learning has rose in a whole, you know, I think researchers don't

01:00:14.960 --> 01:00:18.800
have that. But machine learning grows like a weird set of complications because all kinds of ways

01:00:18.800 --> 01:00:23.200
that things go wrong, even if there aren't software bugs. And so they need to understand something

01:00:23.200 --> 01:00:28.240
about enough about statistics. They have some sense of what could go wrong and ways that you need

01:00:28.240 --> 01:00:33.120
to model things that aren't software glitches. They're like, the world changing glitches.

01:00:33.120 --> 01:00:37.680
There's like the world is the bug. Even if, even if everything's coded precisely and need to be

01:00:37.680 --> 01:00:43.680
able to interface back and forth between like software developers, ML engineers and researchers.

01:00:43.680 --> 01:00:51.280
So I think like the maturity of ML ops and also just broadly like the use of ML not just in,

01:00:51.280 --> 01:00:57.120
you know, I think there was a moment right when there was Google, Amazon, Facebook, Microsoft.

01:00:58.240 --> 01:01:05.280
And I don't know if you ever read this, you know, but like I wrote this like satire that just

01:01:05.280 --> 01:01:09.440
because like when everyone was making a big deal about like, oh, whatever professor left to go to

01:01:09.440 --> 01:01:13.760
whatever company and just their salary and they were kind of writing about it, like, you know,

01:01:13.760 --> 01:01:18.800
almost like like football players getting traded or something. And so I wrote this stupid post

01:01:18.800 --> 01:01:23.040
that was just like announcing that I had been hired as like the intergalactic head of the

01:01:23.040 --> 01:01:27.520
machine learning by Johnson and Johnson or something. And I'd be, you know, for some, you know,

01:01:27.520 --> 01:01:33.520
astronomical sum. And it was just a stupid joke. But the final is like a year later,

01:01:33.520 --> 01:01:38.160
I forget where I was and I met someone and they worked at like Johnson and Johnson AI research.

01:01:38.160 --> 01:01:44.800
Right. And I think that this is part of like what's going on now is that like there was a moment in

01:01:44.800 --> 01:01:49.920
time. I'm sure like I'm making this up. I researched it before though, you know, but you know,

01:01:49.920 --> 01:01:54.160
this is what you come here to speak from academic authority to make up crap on your podcast.

01:01:55.360 --> 01:02:00.720
I'm sure there was a moment in time where like there were only a small number of like elite tech

01:02:00.720 --> 01:02:06.000
firms that were using like modern SQL databases. You know, when it was fresh, I think it was at IBM,

01:02:06.000 --> 01:02:08.560
when it was developed, but there's probably a moment. It was just like a really hot,

01:02:09.120 --> 01:02:13.600
fundamentally new technology that really changed its business operations at places. And there were

01:02:13.600 --> 01:02:18.160
like a handful of like super technical firms that knew how to do it. And now it's like the most

01:02:18.160 --> 01:02:24.480
boring technical firm in the world uses SQL. Right. And I think that this is a huge part of what's

01:02:24.480 --> 01:02:29.120
happening in AI if you were to size up like commercial environment. So I think there are exciting

01:02:29.120 --> 01:02:34.320
startups that are using this technology in new ways. There are, you know, interesting things going

01:02:34.320 --> 01:02:39.520
on at like the sexy tech companies. But I think there's a lot of like, you know, there's no company

01:02:39.520 --> 01:02:45.360
that you go to, you know, whether it's like, you know, I'm sure if you went to like a waste management

01:02:45.360 --> 01:02:50.800
company, like they're finding, you know, they're using AI for something or forecasting demand or

01:02:50.800 --> 01:02:56.000
trying to figure out how to route their trucks or something. And I think that like this sort of

01:02:56.000 --> 01:03:05.680
just general like progression of AI from like a luxury good to a commodity is like an essential

01:03:05.680 --> 01:03:12.400
part of what's going on. And like the fact that like, yeah, like every company has, you know,

01:03:12.400 --> 01:03:19.440
this is becoming their concern. And I think I think part and parcel of that is the way that the

01:03:19.440 --> 01:03:25.040
tooling is getting better and better and better. A whole lot of companies, you know, like what are

01:03:25.040 --> 01:03:30.320
they offering? It's like things that make it that like the stuff that everyone's already been doing

01:03:30.320 --> 01:03:37.200
for a while that anyone could do it, right? And it's easy to track and, you know, it's easy to

01:03:37.200 --> 01:03:42.400
organize. Like, you know, I think this movement of like AI from a concern of like what's the new

01:03:42.400 --> 01:03:48.640
model to like what is like a stable workflow that we can adopt such that a company that can't

01:03:48.640 --> 01:03:52.800
spend half a million dollars for engineer can still use this technology like successfully and

01:03:52.800 --> 01:03:58.080
profitably. I think that's a major part of the story of like the commercial application of AI

01:03:58.080 --> 01:04:03.200
right now. And it's kind of like, it's a pretty unsexy story. Maybe I've just like, oh, this is

01:04:04.000 --> 01:04:08.640
becoming right. But I think this is what happens to everything, right? Like, I don't know,

01:04:08.640 --> 01:04:13.040
I'm like, sorry, if you're a AI researcher, but if you're an MLOPS, it's pretty cool.

01:04:13.760 --> 01:04:19.040
Oh, yeah, absolutely. I don't really cool stuff happening in that field. And there's like a lot more

01:04:19.040 --> 01:04:25.680
jobs at every company in the world together than there is at like whatever it is, like Apple

01:04:25.680 --> 01:04:31.840
Microsoft, Amazon, Facebook, whatever. So I think that moved. Before we run out of time,

01:04:32.960 --> 01:04:38.080
I'd love to have you kind of dust off the crystal ball a little bit more and kind of share some

01:04:38.080 --> 01:04:45.280
of your predictions for the upcoming year, years. We've talked a little bit about where you'd

01:04:45.280 --> 01:04:55.520
swing the bat from a research perspective, but how do you think 2022? With the backdrop of

01:04:55.520 --> 01:05:02.800
kind of the, I don't know if you'd call it a cooling or slowing or boring a vacation or whatever

01:05:02.800 --> 01:05:12.640
you'd want to call it. Are there innovations that are, you know, you kind of see the silhouette

01:05:12.640 --> 01:05:20.880
emerging from the shadows and you think something's there? It's not. The funny thing about that

01:05:20.880 --> 01:05:26.880
is it's not like it's all cooling or it's all heating up. It's like the coolest or weirdest

01:05:26.880 --> 01:05:31.440
interesting thing about it is that whenever you sum up something like a complex

01:05:32.080 --> 01:05:38.720
phenomena with a single number, you lose a lot of information. I think it's like more like

01:05:38.720 --> 01:05:44.160
follow the Roman Empire, right? It's like like the like Rome's all right. Like Rome is still partying

01:05:44.160 --> 01:05:49.680
and like the borders are still expanding, but you also have like, you know, like you have like cities

01:05:49.680 --> 01:05:54.080
being lost and whole country is going off the map and it's like, I think that's happening,

01:05:54.080 --> 01:05:59.680
right? Like you have like like Uber AI shutting down. I research you have like hiring

01:05:59.680 --> 01:06:04.880
freezes at major companies. The big leader is like having major hiring freezes not offering

01:06:04.880 --> 01:06:10.080
the kinds of salaries in 2022 that they were offering in 2018 to like, it's kind of like well-known

01:06:10.080 --> 01:06:14.800
researchers. And at the same time, you have like whole companies where like the shockwave hasn't

01:06:14.800 --> 01:06:19.440
even hit them yet. And they're like first getting into the like like major health systems starting

01:06:19.440 --> 01:06:24.800
to adopt like, you know, deep learning and I think that yeah, there's that going on. If I had to

01:06:24.800 --> 01:06:35.040
predict what's going to happen, I'm going to double down on decision-making. So, you know, I think

01:06:35.040 --> 01:06:40.960
it's like that I'm already seeing a lot of is, you know, like, you know, you could think of like

01:06:41.600 --> 01:06:48.160
like two things came, a few things came together that made AI so hot, which is like one was suddenly

01:06:48.160 --> 01:06:55.600
the fact that like the existence of easily queriable, well-organized curated data at every single firm

01:06:55.600 --> 01:06:59.760
in the world, you know, the fact that like health companies are using electronic health records,

01:06:59.760 --> 01:07:04.480
every company being basically an internet company, everyone having a digital trace of all their

01:07:04.480 --> 01:07:08.320
customer interactions. Now, you know, we can get to a separate like normative point about whether

01:07:08.320 --> 01:07:12.480
we want to live in that world or like whether we're irked by the surveillance state or from a

01:07:12.480 --> 01:07:18.400
standpoint, like an economic standpoint, that happened together with like advances in both the

01:07:18.400 --> 01:07:22.400
tooling and algorithms around statistical modeling. And so the question became we have this data,

01:07:22.400 --> 01:07:28.400
we have statistical tools, how do we do this like analytics on the data, right? But there's

01:07:28.400 --> 01:07:32.400
another side which is like, how do we guide, how do we use the data to guide actions? And I think

01:07:32.400 --> 01:07:39.840
people, I think one one thing that is underutilized by most firms and I think only a small number of

01:07:39.840 --> 01:07:46.080
people are really sophisticated about it is, is really focusing on this like the decision problem.

01:07:46.080 --> 01:07:50.800
And part of that is, part of that is, you know, offline causal inference, which is some of the

01:07:50.800 --> 01:07:55.280
stuff we were talking about, like how can I use some some causal background knowledge together with

01:07:55.280 --> 01:08:00.960
the data that I have to infer a causal effect and use that to guide decisions. But a huge part of

01:08:00.960 --> 01:08:07.200
that is experimentation. And I think that this is a huge thing that not enough companies do that

01:08:07.200 --> 01:08:12.880
you're going to start seeing, you know, become, you know, obviously like Amazon, you know, has,

01:08:12.880 --> 01:08:17.360
has the what they call like web labs, you know, where the, you know, Google, like, you know, does

01:08:17.360 --> 01:08:22.400
randomized control trials for, you know, which shade of green the G should be in Google or something.

01:08:23.280 --> 01:08:27.680
But I think most companies grossly underutilized experimentation, like really methodical

01:08:27.680 --> 01:08:32.880
experiments, because, you know, that plays into the data fixer. Online experiments in particular?

01:08:32.880 --> 01:08:38.960
Well, online, not necessarily in the sense, I mean, I think online is part of, you know, in a sense of

01:08:38.960 --> 01:08:42.720
like, you know, doing like reinforcement learning or having a policy that's adaptive as you're getting

01:08:42.720 --> 01:08:48.720
the results, but even experimenting at all, right? Like just like we've been guiding, if you look

01:08:48.720 --> 01:08:54.560
at how we guide personalized decisions, it's often in the context of I just take passively collected

01:08:54.560 --> 01:08:58.800
traces of people's data. I do some kind of latent factor analysis or whatever to build a

01:08:58.800 --> 01:09:04.480
recommended system versus actually I'm going to randomize choices and try to estimate the sort of

01:09:04.480 --> 01:09:08.880
like, you know, potentially like heterogeneous treatment effects of how different people will

01:09:08.880 --> 01:09:14.560
respond differently to different things, but actually to estimate the effects on, you know,

01:09:14.560 --> 01:09:18.400
whether it's people's behavior or whatever, you know, I don't mean this is sort of like a sound like

01:09:18.400 --> 01:09:23.920
I'm, I'm advising that we like really nearly experiment on people without thinking about the

01:09:23.920 --> 01:09:29.120
considerations or which decisions or which experiments are potentially like of ethical import,

01:09:29.120 --> 01:09:34.160
and obviously there's a lot that needs to, there's a lot of considerations that need to go into

01:09:34.160 --> 01:09:43.440
how you do that and doing it right. But yeah, I think that, you know, the reckoning we're seeing,

01:09:43.440 --> 01:09:48.720
I think is over and over again, it's like people claiming the AI is going to, you know,

01:09:48.720 --> 01:09:53.760
going to personalize this, personalize that, it's going to lead you to make all these different decisions

01:09:53.760 --> 01:09:59.040
in better ways, and then people find like, oh, I just now newly trained a predictive model came up

01:09:59.040 --> 01:10:04.800
with some heuristic for how to operationalize that its decision, and something didn't go as planned.

01:10:04.800 --> 01:10:10.800
And I think that people actually getting more into this world of both using offline, you know,

01:10:10.800 --> 01:10:15.760
kind of causal inference on observational data, but also actually experimenting in the real world,

01:10:15.760 --> 01:10:21.600
and, you know, developing more mature processes for saying, how do I, how do I test hypotheses?

01:10:21.600 --> 01:10:27.280
How do I see what the impacts are of, you know, different actions that I have? I think that that's

01:10:27.280 --> 01:10:33.200
going to become more and more and more important, and you're going to start seeing the like hiring

01:10:33.200 --> 01:10:42.480
focus, you know, and just like where teams start moving, you know, towards those kinds of

01:10:42.480 --> 01:10:47.600
problems, and again, I don't think this is like overnight, you're going to go from people like

01:10:47.600 --> 01:10:53.040
hiring 90% which, you know, deep learning, you know, like, like pie towards jockey to like 90%

01:10:53.040 --> 01:10:59.280
hiring, you know, experts in like, you know, bounded algorithms and causal inference,

01:10:59.280 --> 01:11:05.280
but I do think that there is a, there's a shift here, I'm seeing it at every level, I'm seeing it

01:11:05.280 --> 01:11:10.480
in what, what looks, you know, interesting among new students, what looks interesting among

01:11:10.480 --> 01:11:17.280
our folks hitting the hiring market, I think that this sort of intersection of like, you know,

01:11:17.280 --> 01:11:25.200
CS, operations, research, economics, and bringing to bear, like, you know, tools of predictive

01:11:25.200 --> 01:11:33.920
modeling that we've gotten, but also more sophisticated processes of experimentation and

01:11:33.920 --> 01:11:40.720
estimating causal effects, and principles of just guiding, you know, intelligent decision-making.

01:11:40.720 --> 01:11:47.360
I see there's like a, I think there's like a growing up process happening there, and you know,

01:11:47.360 --> 01:11:55.440
I think the other thing though is, well, one thing I add on, and this is not like a specific

01:11:55.440 --> 01:12:03.040
prediction, but a meta prediction, is, you know, like the internet, like web 2.0, web 3.0,

01:12:03.040 --> 01:12:08.400
whatever the hell we're doing, like very little, there's a lot of like new, there's a lot of new

01:12:08.400 --> 01:12:12.640
stuff that we're seeing, and like the way companies are behaving in the way they're interacting with

01:12:12.640 --> 01:12:18.720
people, that isn't technologically new, right? There's a lot of stuff that like you could have done

01:12:18.720 --> 01:12:24.640
from the late 1990s, the tooling wasn't in there, which restricted how many people could develop it,

01:12:26.000 --> 01:12:30.000
but it was something else, it was something about like, you know, there was a capability that came,

01:12:30.000 --> 01:12:34.000
and a few people have figured out some very like, some early players that figured out like, you know,

01:12:34.000 --> 01:12:37.840
how to conquer e-commerce, like Amazon, whatever, but it took a long time before you got to Uber,

01:12:38.480 --> 01:12:43.840
right? And so there are certain innovations there that were like, you know, it's like there were

01:12:43.840 --> 01:12:48.240
a bunch of pieces that need to fit together, like a certain understanding of like markets,

01:12:48.240 --> 01:12:52.880
or a certain understanding of like usage patterns of phones with the technological capability that

01:12:52.880 --> 01:13:00.240
had been there all along, and I think that like, there's a kind of like innovation in deployment

01:13:00.240 --> 01:13:04.720
that doesn't actually correspond, like I think when people have been stoked about ML recently,

01:13:04.720 --> 01:13:10.960
right? It's been like, oh, BERT is like good at classifying texts, or you know, like, you know,

01:13:10.960 --> 01:13:16.080
seek to seek, you know, LSTMs, and then Transformers are good at, you know, this one thing.

01:13:16.080 --> 01:13:20.400
There's like single-purpose models, but like, I'll give you an example, so I'm an advisor for

01:13:20.400 --> 01:13:27.760
a company, so it's full COI. I'm an advisor for a company called a bridge AI, and a bridge is

01:13:27.760 --> 01:13:34.640
a company that like is sitting between like doctors and the patients, and sitting in this like

01:13:34.640 --> 01:13:42.240
interaction where patients, it turns out patients are recording their visits on their cell phones,

01:13:42.240 --> 01:13:45.760
and they're doing this already. Sometimes sort of viciously, sometimes the doctors can

01:13:45.760 --> 01:13:51.120
send, and may or may not be wired up, and depending upon like what your particular state, it's like,

01:13:51.120 --> 01:13:57.040
you know, two-party sends, so their idea is like, let's inline this as a normal part of the

01:13:57.040 --> 01:14:01.520
doctor-patient interaction. Like, let's have permission, let's have both parties get in,

01:14:01.520 --> 01:14:06.000
they'll agree to record the conversation, they'll pull out a bridge, and then they record it, and

01:14:06.000 --> 01:14:10.320
there's all kinds of different things you could do, right? Like, you can help the doctor to draft

01:14:10.320 --> 01:14:16.240
the summary of the visit, you can help the patient to understand, like, oh, like, you know, don't

01:14:16.240 --> 01:14:21.200
forget, like you mentioned that you would be starting this new medication, have you picked it up,

01:14:21.200 --> 01:14:25.600
or have you, you know, called in that prescription, or did you schedule out this follow-up?

01:14:25.600 --> 01:14:31.360
So there's like a million different places to plug in models, and any one of them by itself may

01:14:31.360 --> 01:14:38.320
or may not be like, you know, a single purpose, like, major innovation, but the ways that you can

01:14:38.320 --> 01:14:42.160
mix and match these, like, okay, I've got the conversation, I've got to send it to an ASR model,

01:14:42.160 --> 01:14:46.160
like, in fact, the text of the text, I need to flag out, like, well, what are the interest,

01:14:46.160 --> 01:14:51.680
what are the relevant or salient parts of the conversation? How do I then take that, turn it into,

01:14:51.680 --> 01:14:57.600
like, an interface feature that, you know, provide some value or a mixing's useful to the patient?

01:14:57.600 --> 01:15:01.040
And I think that, like, a lot of things, like, when Alexa works really well, right?

01:15:01.760 --> 01:15:05.600
You know, or Google Home, or any of these things, why don't work really well? It's usually not

01:15:05.600 --> 01:15:11.680
because there's one model that's, like, magnificent. It's, like, the magic is in the clever way that they

01:15:11.680 --> 01:15:18.320
stitch together, you know, some astute observations about what are the common interaction patterns

01:15:18.320 --> 01:15:22.880
together with, like, what are the right little places you can patch in machine learning,

01:15:22.880 --> 01:15:28.240
and the right ways that you can, like, patch in some intelligent heuristics and rules around it,

01:15:28.240 --> 01:15:35.280
such that, like, you know, you have, like, an end-to-end product that feels like it's magic, right?

01:15:35.280 --> 01:15:39.040
Um, you know, Shazam even is a little bit like that, but when these things are like, there's a few,

01:15:39.040 --> 01:15:43.520
like, like, a little heuristics, where if you start thinking, how do I decompose this into, like,

01:15:43.520 --> 01:15:46.800
something that works? It's like, you can make a pipeline where every single step of it's kind of

01:15:46.800 --> 01:15:51.520
simple, but, like, the end of the end result is something where it feels a little bit magical, right?

01:15:51.520 --> 01:15:56.320
And I think that, like, this is going to be a, I think, a major part of this is, like, I think

01:15:56.320 --> 01:16:00.000
we've been looking at, like, people who are really good at building single-purpose models,

01:16:00.000 --> 01:16:04.960
then turning that into a big start-up, and, or trying to turn, parlay that into a start-up,

01:16:04.960 --> 01:16:08.880
and I think there will be some amount of, like, the single-purpose models are mature,

01:16:08.880 --> 01:16:14.400
and they'll get a little bit better, but what's maybe under-explored a bit are ways that you mix

01:16:14.400 --> 01:16:20.320
and match models together with cool interaction patterns, and, you know, some clever understanding

01:16:20.320 --> 01:16:26.000
of, like, what people want, and, you know, what data's available, et cetera, to build, like, kind of,

01:16:26.000 --> 01:16:32.000
user experiences that maybe under the hood are invoking, like, seven different models, and

01:16:32.000 --> 01:16:36.560
seven different contact, and, but it's, like, kind of hidden from the user in a clever way, where it

01:16:36.560 --> 01:16:41.600
just feels like you're having, like, it adds up to a new capability that no one model,

01:16:42.480 --> 01:16:47.360
or piece of software, like, by itself would provide. And so, I do think that there is some

01:16:47.360 --> 01:16:53.200
element of this, like, you know, like, we've built a bunch of cool Legos, and we haven't given

01:16:53.200 --> 01:16:57.520
people that many years, so, like, you know, instead of, like, you know, some innovation comes

01:16:57.520 --> 01:17:01.120
from, like, I designed a new Lego piece, but, like, I think a lot of innovation will come from

01:17:01.840 --> 01:17:08.800
people that are, you know, you know, don't have to be, like, have, like, off-the-chart skills at

01:17:08.800 --> 01:17:14.400
building Legos, but they're really, they have a kind of design sentence for, you know, what are

01:17:14.400 --> 01:17:21.200
cool ways to put them together? I think that's a, it's a natural consequence of the, the broader

01:17:21.200 --> 01:17:26.240
maturity conversation we've had, we've been having, right? The Lego pieces are, you know, that

01:17:26.240 --> 01:17:30.560
that we've come up with every Lego piece that's ever going to be created, and there aren't some

01:17:30.560 --> 01:17:36.080
cool ones to come, but all the basic pieces required to build really cool stuff is in place, and

01:17:36.080 --> 01:17:41.360
now it's all about, how do you put them together? Yeah, and even more so than the pieces themselves,

01:17:41.360 --> 01:17:46.800
the tools to easily put them in place, you've got your hugging faces, you've got your ML ops tools,

01:17:47.600 --> 01:17:52.960
like, it's a great time to be a builder. Right, yeah, it also, like, it takes some of that work

01:17:52.960 --> 01:17:58.720
away that it allows you to focus, right? I think music's like that a little bit, right? Like,

01:17:58.720 --> 01:18:01.840
you know, there's this way, like, when you're learning an instrument, and you're like, I got to

01:18:01.840 --> 01:18:05.760
practice articulation, I got to practice rudiment, I got to practice scales, I got to do that, and you,

01:18:05.760 --> 01:18:08.640
you know, you're sitting here, you're going to do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do,

01:18:08.640 --> 01:18:12.400
and then you're playing this kind of shit over and over and over again when you're like 10,

01:18:12.400 --> 01:18:17.120
11, 12, 13, 14 years old, but you get to some point where like, maybe you still practice like that

01:18:17.120 --> 01:18:22.960
one hour a day, but when you go to play, you're not even thinking at that level at all, like,

01:18:22.960 --> 01:18:27.280
it's not, and I think there is some element of that, of like, people using machine learning recently

01:18:27.280 --> 01:18:30.560
have been thinking like, just like, how do I get the data and train a single model? I think,

01:18:30.560 --> 01:18:33.760
once you have a lot of these contexts where maybe you don't even need to train a model,

01:18:33.760 --> 01:18:36.880
maybe there's an off-the-shelf model that's sufficiently good at this task and it works better

01:18:36.880 --> 01:18:40.880
than anything you could train, even if you're applying it sort of on slightly different

01:18:40.880 --> 01:18:44.320
domain-shifted data, right? Then you start getting at this point where,

01:18:45.920 --> 01:18:52.480
right? Like the difference between like a great artist and like a super boring artist isn't

01:18:52.480 --> 01:18:58.400
that like the great artist is better at scales, you know? It's not, it's at a point, you know,

01:18:59.600 --> 01:19:03.280
right? So it's not like, oh, like, like, mild Davis played like better, you know,

01:19:03.280 --> 01:19:07.200
played like cleaner scales than, you know, like, I don't know.

01:19:07.200 --> 01:19:10.640
He's hated the artist staying on key or something like that. It's not like that.

01:19:12.000 --> 01:19:19.760
Yeah. So, I think there is, you know, a lot of, a lot of iteration that did be had on that side.

01:19:21.120 --> 01:19:28.080
Yeah. Awesome. Awesome. Well, Zach, it has been wonderful catching up. Let's make sure it's not

01:19:28.080 --> 01:19:34.160
two years, until the next time. Yeah. Right. Who knows what pandemic will be in full swing

01:19:34.160 --> 01:19:44.160
by that time. Awesome. Well, thanks so much for helping us reflect on 2021 and the ML&D

01:19:44.160 --> 01:20:00.080
LDM and catch you next time. Yeah, thanks for having me sound. Great to see you.

