WEBVTT

00:00.000 --> 00:15.280
All right, everyone. I am here with Rishabh Agarwal. Rishabh is a research scientist at Google Brain.

00:15.280 --> 00:20.640
Rishabh, welcome to the Twimal AI podcast. I'm looking forward to digging into our conversation. We'll

00:20.640 --> 00:28.560
be talking about your work broadly, as well as your NURBS 2021 outstanding paper, deep reinforcement

00:28.560 --> 00:35.280
learning at the edge of these statistical precipice. Great title. But before we dive into that,

00:35.280 --> 00:39.440
I'd love to have you share a little bit about your background and how you came to work in RL.

00:39.440 --> 00:45.200
Back in undergrad, I think deep mind, we released their Atari pencils. And as an undergrad,

00:45.200 --> 00:50.080
it was really cool to see like, oh, we can play these games without any sort of hard coding and you

00:50.080 --> 00:55.280
can just learn how to play these games. So it combined like two of the interests I had back then,

00:55.280 --> 01:01.360
which is playing games and being able to do that automatically. So like I decided to take like

01:01.360 --> 01:06.320
ML courses back then in undergrad and then started to get interested in more and more in research.

01:06.320 --> 01:13.520
And I tried a bunch of different things before coming to RL. And I think it really happened. So my

01:13.520 --> 01:19.520
bachelor thesis was on learning how to play Scrabble. And basically Scrabble, I think the best

01:19.520 --> 01:24.640
agent in Scrabble is something from MIT, which uses some sort of heuristics and Monte Carlo research.

01:24.640 --> 01:28.960
And we were thinking, well, can we replace these heuristics with a learned agent? Because think about

01:28.960 --> 01:34.320
like it's basically hard coding some sort of strategies to do when you are in a certain position.

01:34.320 --> 01:39.120
And we're saying, it can we learn these sort of things? Because we can collect data from multiple

01:39.120 --> 01:44.160
self-play games and whatnot. So in some sense, what we were trying was to try and do something like

01:44.160 --> 01:49.200
AlphaGo, but on Scrabble. And this was like around the same time. It's just that we didn't have

01:49.200 --> 01:54.640
the same amount of compute. So we had to resort to like imitation learning and other kind of approaches.

01:54.640 --> 02:01.920
But this is how I started doing RL. It didn't really Google. But after that, I applied to a bunch of

02:01.920 --> 02:08.960
these residency programs at these different companies and got into Jeffenton's team. And I think

02:08.960 --> 02:13.520
one thing with Jeffenton said me, told me it was very like interesting. He said, well, I've seen

02:13.520 --> 02:18.320
a lot of people with a lot of people in solidarity before they start research. Your case was interesting

02:18.320 --> 02:22.560
because you have done a lot of research and you have failed. So you know how research can be.

02:22.560 --> 02:27.280
And this is something I think you would need in a researcher. That research can be quite hard.

02:27.280 --> 02:32.240
A lot of the times. And it's better to know or seek failures early rather than later on.

02:32.240 --> 02:36.160
Because if you don't see failures early on, then you'll think that research is easy and you just

02:36.160 --> 02:41.200
have to like do some things in published papers. But it can be quite tricky. And he was like,

02:41.200 --> 02:45.840
he was like, this is the kind of quality I was looking for. At least in researchers trying to

02:45.840 --> 02:51.920
find out. That was good to know that. Well, someone to appreciate failures too. So I worked there

02:51.920 --> 02:59.360
as a resident in the brain team for a year in Toronto. And there I worked on Meliado, I think.

02:59.360 --> 03:05.200
And after that, I moved to Montreal and I'm doing art research still because I like doing art.

03:05.200 --> 03:11.520
I think I pride moving away, but art is just too exciting too. So I just keep coming back to

03:11.520 --> 03:18.000
all of you. You mentioned that you tried a few areas of research besides RL before settling

03:18.000 --> 03:24.320
into it. What were some of those? So I think the first research project I had done was something

03:24.320 --> 03:30.720
about disambiguation, which is think about like a lot of these things are already solved by large

03:30.720 --> 03:35.360
language models these days. But think about let's say I say I'm going to the bank. Now this could

03:35.360 --> 03:40.560
be a river bank or it could be a bank where you collect money, right? And without actually additional

03:40.560 --> 03:45.200
information, you can't really say which bank were you talking about. Most likely the bank

03:45.200 --> 03:49.280
where you go to the bit money, but it might be the river bank too. And we're thinking about,

03:49.280 --> 03:54.720
well, how do you actually figure out or how do you come up with word embeddings that can give you

03:54.720 --> 04:00.720
some sort of probabilistic sort of embeddings, which are like depending on the future sentences or

04:00.720 --> 04:06.800
future words you see, you will change your embedding in some sense. So it's not a static embedding.

04:06.800 --> 04:11.840
It depends on where this word is. So this is the first thing. Then I think dabbled a little bit

04:11.840 --> 04:19.360
in meta learning or how do we classify I think thanks quickly because back I think it was 2016

04:19.360 --> 04:24.800
and back then meta learning was starting to get popular. And it was interesting to see if we can

04:24.800 --> 04:30.080
generalize like one shot or two shots. So things like I think if I give you images of docs and

04:30.080 --> 04:35.920
cats and now any generalize one shot to let's say different breeds of docs, then what is in your

04:35.920 --> 04:42.320
dataset given like a few examples. So these are the sort of things. But I think the aspect of

04:42.320 --> 04:47.600
RL which was much more interesting was that the agent can learn from its own mistakes rather than a

04:47.600 --> 04:52.960
static data. And I think that was something which is quite cool because this is something we as

04:52.960 --> 04:57.440
humans also do that if we don't know how to solve the problem, we try to figure it out, collect

04:57.440 --> 05:04.160
some data maybe and then just solve it or like make attempts at doing so. But this is not something

05:04.160 --> 05:09.680
which happened in happens in supervised learning. So I think that was the aspect which made me think

05:09.680 --> 05:15.840
about plus we used we play usually games in RL. So that was another interesting thing. And within

05:15.840 --> 05:22.720
the realm of RL is there a particular area that you like to focus on nowadays? I have in the past

05:22.720 --> 05:28.560
focused on offline reinforcement learning a lot. But more recently what I've been trying to think

05:28.560 --> 05:35.360
about is that a lot of the problems we tackle are sort of we have let's say a sort of problem,

05:35.360 --> 05:39.920
we learn an agent and forget about it and we try to improve and come up with another agent.

05:39.920 --> 05:45.520
But a real world problem is not like that. A real world problem remains in existence for a few years

05:45.520 --> 05:50.080
and it's almost always the case that you don't forget about your previous agents. You always try to

05:50.080 --> 05:54.880
build up on top of that. So think about let's say you have a few recommendations system and

05:54.880 --> 05:59.520
the algorithm recommend you videos. Now let's say you want to you have another algorithm

05:59.520 --> 06:03.600
which can learn like better recommendations. You wouldn't really just throw away your existing

06:03.600 --> 06:08.160
policy or agent or whatever behavior you have learned and try to learn everything from scratch.

06:08.160 --> 06:13.440
What you really do is you try to build on top of whatever existing policy or agent you have.

06:13.440 --> 06:18.160
Now this is good and this is mostly the case for applications but in research we don't really

06:18.160 --> 06:22.880
follow this sort of protocol. Like really we have we have a benchmark we try to come up with an

06:22.880 --> 06:27.440
agent and resolve this and we try to improve on that benchmark and we're saying well why don't we

06:27.440 --> 06:31.440
really do this in research. So this is something I've been trying to focus on that how do we come up

06:31.440 --> 06:36.160
with sort of problem settings and methods which can build on existing things and these

06:36.160 --> 06:41.200
existing systems does not have to be like human demonstration. They might be just like your

06:41.200 --> 06:46.000
existing agents which not so think of like an example which I think which I quite like is think

06:46.000 --> 06:51.760
of Minecraft and think of some agent which has some basic capabilities to do build basic structures

06:51.760 --> 06:56.320
in this Minecraft let's say build a house or something like that. Now if you take that agent

06:56.320 --> 07:01.040
can it expand quite a lot and can it do like much more than just building a basic house.

07:01.040 --> 07:06.320
Things like take existing agents and improve them significantly as opposed to learning new agents

07:06.320 --> 07:13.600
entirely from scratch and I think this is valuable if your problem either takes a long amount of time

07:13.600 --> 07:18.080
or it's just not feasible to do things again and again. So if I think if I don't know if you know

07:18.080 --> 07:24.960
so opening I had this project on Dota where they basically spent 10 months of training to get

07:24.960 --> 07:30.560
the agent to beat the canvas just there. Now the thing was in their paper they have like an

07:30.560 --> 07:34.320
interesting figure where they say we never really started from scratch because if we did

07:34.320 --> 07:39.920
they would have taken us like five to six times more like months like apparently more than 40

07:39.920 --> 07:45.520
months to just get this agent and as I was really expensive to do so. So what they always did was

07:45.520 --> 07:49.680
whenever they had to and the reason they had to do this was because their environments kept

07:49.680 --> 07:55.520
in changing so their observation space or things like whatever their input is it kept in changing

07:55.520 --> 08:00.800
and they had to make sure that their model still works for these changes so and they could not

08:00.800 --> 08:04.960
really afford to like just throw away the model. So what they kept on doing was they kept adding

08:04.960 --> 08:10.480
more components to their existing model so that it can reuse whatever they have learned

08:10.480 --> 08:14.800
but also handle the existing change or the new changes which come into their pipeline.

08:14.800 --> 08:20.160
So that was like an interesting use case of this sort of setup but this is more of a one-off

08:20.160 --> 08:24.880
example of research I think because they have to tackle practical problem they had to do this

08:24.880 --> 08:29.360
but if your benchmarks will just take like I don't know a day to tackle probably people

08:29.360 --> 08:34.320
are not interested in but I still think there is value in this sort of research paradigm of

08:34.320 --> 08:39.120
building on top of existing agents or this is something I think I've been excited about for a while

08:39.120 --> 08:45.200
now. Nice nice and you're speaking about it broadly have you thought through kind of a taxonomy

08:45.200 --> 08:52.480
of approaches like it has echoes of transfer to it it has echoes of hierarchy like open AI

08:52.480 --> 08:58.560
has echoes of like initialization even you know taken simply. Yeah so this is a pretty broad topic

08:58.560 --> 09:02.880
because what do you transfer really like it depends on for example what do you transfer do you have

09:02.880 --> 09:06.880
a policy do you have a value function do you have a model do you have an exploration strategy

09:06.880 --> 09:11.680
also depends on how you transfer it and what is given to you and what not so there are a lot of

09:11.680 --> 09:16.400
these things really it's like a much bigger paradigm it's just that what we're saying is typically

09:16.400 --> 09:22.000
in research what we do most of the times is we start tabular asa but that's something we don't

09:22.000 --> 09:27.440
have to do like at least for a lot of the problems we don't do tabular asa at least in practice

09:27.440 --> 09:32.880
so why are we doing that in research and we should be moving away from this so I think in our head

09:32.880 --> 09:38.000
it's really broad but we probably start from focusing on a narrow like problem and then hopefully

09:38.000 --> 09:42.720
the community thinks this is valuable and maybe the pick it up from there got it sounds like

09:42.720 --> 09:48.640
you're early and you haven't specifically or narrowly formulated a problem statement just yet

09:48.640 --> 09:54.480
we do have something but I think it is so it's related to let's say I guess we had some desiderata

09:54.480 --> 10:01.280
which was let's say give you a policy or an agent and I have some and I have to use enough policy

10:01.280 --> 10:05.760
agent or something which can exploit existing data because that's something which is useful now

10:05.760 --> 10:11.360
can I quickly recover this for policies performance or agents performance while actually continuing

10:11.360 --> 10:17.280
learning and I think the interesting aspect here is this policy is not optimal in any sense it's

10:17.280 --> 10:22.880
just some sort of good policy it can get you some good rewards of behavior but really you want to

10:22.880 --> 10:29.040
improve on it so the dependence of your agent on this policy or age or this existing let's call

10:29.040 --> 10:33.600
a teacher should be limited like at some point you know that you should have to learn on your own

10:33.600 --> 10:38.320
like kind of like how we take learn anything we take lessons and then we learn out of on

10:38.320 --> 10:41.360
we just don't keep taking lessons forever similarly here

10:45.920 --> 10:49.760
yeah so think of it like your instructor right like your swimming instructor teaches you

10:49.760 --> 10:54.480
swimming but let's say you went on to become like a gold medalist or something so I think you

10:54.480 --> 10:58.400
definitely learned a lot on your own rather than just relying on your instructor all the time

10:58.400 --> 11:03.920
so something like that you have to let go of your instructor so that you keep learning on your own

11:03.920 --> 11:08.640
because you know that maybe you can do better than your instructor and so that's the sort of

11:08.640 --> 11:13.360
I think concrete setup you have in but yeah I still in the works and still thinking about what to do

11:13.360 --> 11:21.360
with this the paper that I mentioned earlier DRL at the edge of the statistical precipice as I

11:21.360 --> 11:27.520
mentioned at one and outstanding paper award at this past noreps but it wasn't necessarily the

11:27.520 --> 11:32.320
paper that you were trying to write when you started out on your research program how did that

11:32.320 --> 11:37.120
paper come to be yeah there's an interesting backstory so I think what happened was as you

11:37.120 --> 11:41.920
find a use this benchmark called Atari 100k which is an interesting benchmark where they say that

11:41.920 --> 11:47.120
oh we'll train an agent for 100k interactions which is approximately two to three hours of human

11:47.120 --> 11:51.920
gameplay and this is the amount of time which the human agent was given or the professional human

11:51.920 --> 11:56.560
gameplay was given to get like some sort of score on all these games so this is a baseline

11:56.560 --> 12:02.560
they use when we compare the respective humans on these games and the benchmark said okay let's

12:02.560 --> 12:07.520
do this let's actually train our own agents to this this much amount of time and see how well can

12:07.520 --> 12:13.440
we compare against human or how well we do compare to humans so so this is good this is like a good

12:13.440 --> 12:18.720
benchmark because it gives you like an estimate of how sample efficient the agent can be or like

12:18.720 --> 12:23.200
how fast can your agent learn so this is all good and I was coming up with like maybe a better

12:23.200 --> 12:29.280
approach to do this sort of thing and my agent I like get some sort of variation I thought maybe

12:29.280 --> 12:33.280
I should just increase the number of random seats so that I see less variation in my results I

12:33.280 --> 12:37.760
double the seats I still see a lot of variation I kept doubling the seats and at some point I think

12:37.760 --> 12:43.040
I was at 30 seats and I was still like the variation is huge and it's all over the place and

12:43.680 --> 12:48.000
but then I looked at the existing literature and I saw everyone who used three random seats and

12:48.000 --> 12:53.120
they were comparing the results and claims soda and whatnot and in my own results what I was seeing

12:53.120 --> 12:57.360
was that there were seats which led to really worse performance and their seats which were

12:57.360 --> 13:03.360
beating state of the art are depending on what seats I end up using right and this was concerning

13:03.360 --> 13:08.560
so for a moment I thought maybe the implication is that the the published results that you were

13:08.560 --> 13:14.400
referring to are kind of cherry picking runs particularly well rather than that I was more like

13:14.400 --> 13:20.080
it's unclear if the published results actually hold if you ran a lot more evaluations so it's like

13:20.080 --> 13:25.120
it's unclear either they underperformed their actual reported results or overperformed them so that

13:25.120 --> 13:30.560
was something which is not clear but so initially I thought maybe it's so I think the interesting artifact

13:30.560 --> 13:36.080
which happened in my own work was that if I kept adding more seats my numbers or results kept going

13:36.080 --> 13:41.040
up and I'll tell you the story about why that happened to but it was just like interesting to see

13:41.040 --> 13:46.160
that I went from 3 to 5 to 10 to 20 to 30 am I just I'll skip improving and I was like I have

13:46.160 --> 13:52.240
not done any like change here really the only changes evaluation what is happening so initially

13:52.240 --> 13:56.640
I thought maybe it's just with the method I'm evaluating so there were two parts here one was just

13:56.640 --> 14:01.440
to skip this benchmark and go to some better benchmark and think about it and the other was

14:02.800 --> 14:06.720
maybe there is something wrong going on here and people have been using this benchmark and

14:06.720 --> 14:12.320
probably they will because it is an interesting one so why not try to fix it so at this

14:12.320 --> 14:16.960
crossword we decided let's just stop the existing project we are doing and think about this other

14:16.960 --> 14:22.160
problem because it seems more fundamental and this is I think something in research which we should

14:22.160 --> 14:26.160
learn that we should always keep an eye out for interesting opportunities if you're really trying

14:26.160 --> 14:31.440
to go somewhere it's okay to take like a turn or something along the way if that looks more

14:31.440 --> 14:36.160
interesting to you like it's totally okay to give up on that so we decided let's let's focus

14:36.160 --> 14:41.200
on this other thing so what I did was I had a bunch of baselines already coded up for the

14:42.000 --> 14:46.240
like the existing published results so I thought maybe I don't trust my own baseline let me just

14:46.240 --> 14:50.240
ask the code from the authors themselves or whatever their open source code was and let's just

14:50.240 --> 14:55.920
try to evaluate this code and funny enough the same sort of things happen that the uncertainty

14:55.920 --> 15:01.760
results was pretty huge and for some of these methods I again so basically the more run number

15:01.760 --> 15:08.160
of random seats I used the better the results were and what was this issue and then I think

15:08.720 --> 15:14.000
this hit me basically what are we doing is we have an algorithm we were alluding it on a bunch of

15:14.000 --> 15:19.760
task and aggregating the results to do something and in Atari at least what we use is something called

15:19.760 --> 15:25.280
median score so if you have let's say I don't know 20 tasks what you use is the performance on the

15:25.280 --> 15:30.880
median tasks in some sense and this is like a simple statistical fact that if you take median

15:30.880 --> 15:36.480
off expectations or median of averages it's not going to come out to be the same as average of

15:36.480 --> 15:42.320
medians in some sense so basically there's a difference between if you evaluate let's say a few

15:42.320 --> 15:47.440
seats calculate the average score on each of the task and compute the median this is one quantity

15:47.440 --> 15:51.840
the other quantity is you evaluate infinite number of seats you really get the true average

15:51.840 --> 15:55.600
score you get on each of the task and then take the median this is another quantity so the funny

15:55.600 --> 15:59.680
thing is if you repeat the first process again and again which is you have few seats you

15:59.680 --> 16:04.080
evaluate the aggregate performance and then just keep repeating this process to take an average

16:04.080 --> 16:09.760
in some sense expectation this quantity is not equivalent to the the true quantity you get about

16:09.760 --> 16:14.640
because this is a bias estimator now this is I think well known statistical fact but the funny

16:14.640 --> 16:20.960
thing was the size of the bias can be as large as 30% of your performance itself so it's possible

16:20.960 --> 16:26.320
that some of the results were just like reported just cause of the bias or you really didn't know

16:26.320 --> 16:32.160
the what's happening and this is actually what we found what was it in the the problem that you

16:32.160 --> 16:36.880
were solving that made that bias always an underestimate of the actual performance yeah so this

16:36.880 --> 16:42.000
was actually an artifact of the algorithm so what happened was really like I really didn't tune

16:42.000 --> 16:47.600
my hyperparameters tackle so it hadn't overpitted and because of that it was actually like under

16:47.600 --> 16:51.920
performing when I use few seats and if I run more seats it seemed to be a bit of performing but

16:51.920 --> 16:56.080
for some other algorithms what we found that the direction of the bias was in the other direction

16:56.080 --> 17:02.000
which is it could be positive or negative depending on the algorithm so for some algorithms if I

17:02.000 --> 17:06.880
add more seats the performance went down as opposed to improving so that this made it even worse

17:06.880 --> 17:10.640
because if it was in the same direction at least you can say in algorithms we didn't prove

17:10.640 --> 17:15.440
with more seats now it was oh some algorithms actually become worse and some algorithms

17:15.440 --> 17:22.240
become better and the thing is the bias is actually reasonably large just to not say what's

17:22.240 --> 17:27.440
happening so this is one of the issues but it was not actually the big issue the biggest issue was

17:27.440 --> 17:33.920
the uncertainty in these results was pretty high so what actually happened here was like I have

17:33.920 --> 17:38.320
as I've seen like my results were changing quite a lot and sometimes the difference in the results

17:38.320 --> 17:42.320
from three seats and let's say something like 20 seats was as large as the difference between

17:42.320 --> 17:48.800
the previous soda and the baseline so it was a huge gap to see like maybe this gap can make

17:48.800 --> 17:56.480
your conclusions sort of yeah like it's unclear if your conclusions are published method actually

17:56.480 --> 18:02.720
was better or wasn't due to just statistical fluctuations and that was an issue so we thought well

18:02.720 --> 18:08.960
this is not so expensive benchmark and I am at Google so we do have a reasonable model compute

18:08.960 --> 18:15.040
let's just try to use that compute to see how much fluctuation there is and that led to like the

18:15.040 --> 18:20.320
first finding of the paper that there is an enormous fluctuation and most of the existing methods

18:20.320 --> 18:26.240
actually over reported their results were like overestimated in some sense except one method

18:26.240 --> 18:32.080
for funnily enough which actually underestimated so there's also one method which was like if I

18:32.080 --> 18:38.080
evaluated on more seats it results would have been much better it says that they didn't do so but the

18:38.080 --> 18:42.160
interesting bit was that most of the results were actually quite overlapping at least in terms of

18:42.160 --> 18:48.160
these meetings scores and it was unclear if there is a improvement and all of these were

18:48.160 --> 18:52.960
published papers in the last couple of years so people said ICML is running on and some of them

18:52.960 --> 18:58.240
were actually at near its 2012 but were there implementations published or did you have to

18:59.360 --> 19:05.440
engineer their implementations so most of them were actually published except one for which we

19:05.440 --> 19:09.120
had implemented so this was really not so we're trying to like remove all sorts of

19:09.120 --> 19:14.000
confounding factors that this is our issues it was really we just took their open source code

19:14.000 --> 19:18.960
and just ran it on a lot of seats and we said oh well there is actually a lot of overlap and what

19:18.960 --> 19:24.400
not and so this was the problem part of things and I think it was sort of at least a lot of people

19:24.400 --> 19:29.680
in researches and other low of these problems that there are problems when we compare our methods

19:29.680 --> 19:34.640
and there's a lot of fluctuations in what not so this was the first part but then I think we thought

19:34.640 --> 19:39.680
about it and realized that well we were at Google and we could have we launched these 100 seats

19:39.680 --> 19:44.560
for all these methods but this is not computationally tractable for anyone outside Google and even

19:44.560 --> 19:49.120
at Google it was pretty expensive to do so each time let's say you have to compare a method you

19:49.120 --> 19:55.440
wouldn't be launching I don't know 100 seats and it also comes like and this was like a simple

19:55.440 --> 20:00.560
benchmark or at least and computationally relatively inexpensive benchmark if you had let's say

20:00.560 --> 20:06.320
harder benchmark some let me give you some estimates so for Atari games like the Atari 57 benchmark

20:06.320 --> 20:13.680
it takes about 1000 GPUs for computing like a single result aggregated across all the tasks

20:13.680 --> 20:19.360
and so this is about if you launch all the like runs in parallel or one of all the games it will

20:19.360 --> 20:25.120
take you about three four days with like existing open source libraries so that is a lot of time

20:25.120 --> 20:30.720
just get like one result and that one regret you're talking about training an agent for a particular

20:30.720 --> 20:36.160
game in the Atari suite so no it's like training an agent for the standard 200 million frames

20:36.800 --> 20:41.520
for all on all the games and doing like let's say something like five seats so let's say this is

20:41.520 --> 20:46.880
the minimum you're going for this will take about three four days assuming you have done

20:46.880 --> 20:52.480
parallelization also now you can't really expect researchers to run 10 seats at 20 seats a lot

20:52.480 --> 20:58.240
more seats because it quickly becomes a lot more expensive but the thing is at least like in

20:58.240 --> 21:03.680
reviews we often see this like we often get requests from like reviewers saying well we've only

21:03.680 --> 21:09.600
validated in five seats why don't you run five more seats and it's very like hard to like convince

21:09.600 --> 21:14.560
them otherwise but it is really expensive and I think this was the problem I also had at the back

21:14.560 --> 21:18.640
of my mind because I have gotten reviews like these and I couldn't just say hey it's really

21:18.640 --> 21:24.000
expensive and it'll take like five days just to get five more seats and so how do I convince

21:24.000 --> 21:28.960
that these results are statistically robust while not running a lot more seats because that's not

21:28.960 --> 21:34.240
really the solution and I think this problem will become worse as we move on to harder benchmarks

21:34.240 --> 21:38.800
because these are like bench sort of games are actually 20 year old I think a 40 year old I don't

21:38.800 --> 21:43.360
exactly remember but they are pretty old and now if you think about let's say more recent and

21:43.360 --> 21:48.080
harder problems let's say you think of something like StarCraft then suddenly evaluating even just a

21:48.080 --> 21:54.080
handful of seats is pretty hard so so I think the problem of few seats and like really expensive

21:54.080 --> 21:59.920
benchmarks is going to stay at least more and more researchers would end up using fewer seats

21:59.920 --> 22:06.400
and more difficult problems curious appears a correlation between the the number of seats the

22:06.400 --> 22:12.640
reviewers asking for and the size of their company like the Google review were asking for more seats

22:12.640 --> 22:18.880
if that is possible although we did have one flood in the paper where we showed the correlation between

22:18.880 --> 22:24.240
as time passed and the number of seats we have used while our benchmarks have become more complicated

22:24.240 --> 22:28.000
so I think back in the 80s we are using something like 30 seats because our benchmarks were really

22:28.000 --> 22:33.040
simple maybe mountain card card like really simple simulated problems and now I think as we

22:33.040 --> 22:38.800
introduced the satari benchmark we started using like fewer and fewer seats and more recently

22:38.800 --> 22:43.040
I think you've been using just three to five seats that's the sort of standard idea that it's out

22:43.040 --> 22:47.920
of necessity as opposed to negligence so yeah it's not really yeah it's not really people are

22:47.920 --> 22:53.360
going to hide it's just that it's it is expensive to evaluate these models and like get like these

22:53.360 --> 22:59.920
results so people run whatever like the prior work follows so okay so we had this mindset that oh

22:59.920 --> 23:05.760
people are going to evaluate few seats and they still want to compare across these algorithms

23:05.760 --> 23:11.600
now how can we do this robustly and this is the question we had the solution we went on to

23:11.600 --> 23:16.880
something which we already doing computer science but not in machine learning which is they just

23:16.880 --> 23:23.920
plot the distributions of the score you get across all the tasks and all the seats so think

23:23.920 --> 23:27.920
about it for again so what are they doing is so this is commonly done for comparing optimization

23:27.920 --> 23:32.800
software which is how fast they're able to solve the problem so what they do is you have a bunch

23:32.800 --> 23:37.600
of problems they run their software or algorithm on all these problems they get times for each

23:37.600 --> 23:43.280
of them and now they just plot the distribution of how fast they were in this like to solve the

23:43.280 --> 23:48.000
problem and this is distribution meaning something like the CDF of all the times they got across

23:48.000 --> 23:53.600
all the problems and all the runs and this is school for multiple reasons but I'll cover that

23:53.600 --> 23:58.960
but we thought oh why don't we do this in RL also that basically the nice thing at least in

23:58.960 --> 24:05.520
RL typically is that when we have a bunch of tasks we usually have some notion of score comparisons

24:05.520 --> 24:10.720
across tasks which is at least for Atari for example we use human score human normalized scores

24:10.720 --> 24:16.320
which is all the scores are relatively humans so we often compute that's why we are able to compute

24:16.320 --> 24:21.440
aggregate scores also which is what does it mean to aggregate scores across two different games

24:21.440 --> 24:26.320
which might have really different score ranges so think about I don't know pinball versus something

24:26.320 --> 24:31.840
like pong the score in pong goes up to 21 while in pinball it can be a million or something like that

24:31.840 --> 24:36.720
now you really just can't compare their scores but if you normalize the scores that's a relative

24:36.720 --> 24:42.480
to humans now it makes sense to actually compare scores and pong in the pinball you can say how bad

24:42.480 --> 24:46.960
I'm doing with respect to humans on both of these games now this is a cool property so we thought

24:46.960 --> 24:52.400
well why don't we just mix all the scores we get and just compute like the distribution

24:52.400 --> 24:57.200
and this gives you something called performance profiles now there are a bunch of cool things about

24:57.200 --> 25:02.160
these profiles these profiles are nothing fancy these are really just the CDFs of the distribution

25:02.160 --> 25:07.200
you would get if you were to mix all the scores now the nice thing about these things is that the

25:07.200 --> 25:12.640
area under these profiles would be actually the mean score you get across all the games

25:12.640 --> 25:19.440
similarly if you compute let's say a draw line at y equal to 0.5 then you'll get the median

25:19.440 --> 25:24.240
the point where these curves into second and you can also similarly get the any sort of percentile

25:24.240 --> 25:28.640
so because it is just giving you the entire distribution in a single picture and the other

25:28.640 --> 25:35.120
nice thing about these profiles is that you don't really have to like write out a big table with

25:35.120 --> 25:40.560
like a bunch of methods and let's say Atari has 57 tasks so what your table would look like is 57

25:41.280 --> 25:47.440
sort of rows and then for each row you have multiple columns and oftentimes what happens is if

25:47.440 --> 25:52.320
you have more than 4.5 methods you will just not report standard deviation or variance because

25:52.320 --> 25:57.360
space mistakes and whatnot and then it's even harder to compare because some methods are doing

25:57.360 --> 26:02.320
better in some class and not on others so these tables end up going in the appendix most of the time

26:02.320 --> 26:06.960
and like really what you end up reporting is in the mean risk mean paper is some sort of aggregate

26:06.960 --> 26:12.400
measures but the aggregation heights a lot of the things so we said well why don't we report these

26:12.400 --> 26:19.600
sort of distributions and this was the first sort of solution the the solution to the problem of

26:19.600 --> 26:24.560
how do we do better but the actual solution was more or less something like we should report

26:24.560 --> 26:30.880
the statistical uncertainties we have when we run these algorithms so think about this whenever

26:30.880 --> 26:35.600
you're running an algorithm on a bunch of tasks what you're trying to evaluate is a random variable

26:35.600 --> 26:40.800
because the performance really depends on the randomness of whatever thing you evaluated now each

26:40.800 --> 26:44.400
time you change your random seed your comments would changes so really what you have is a random

26:44.400 --> 26:50.000
variable and you're trying to evaluate it using a finite number of samples or seeds in this case

26:50.000 --> 26:55.040
right so what you should be really reporting is also what is the uncertainty in this variable

26:55.040 --> 26:59.600
which is if you were to repeat your experiment that's a with a different random seed

27:00.400 --> 27:04.640
what what is the result you expect and now the thing is some people say well maybe we'll just

27:04.640 --> 27:09.840
fix the random seeds and that's okay and I'm saying well it's not really okay because let's say I

27:09.840 --> 27:14.960
go to a different hardware then suddenly just fixing random seeds put in really fix the randomness

27:14.960 --> 27:19.840
and also I think the bigger reason against just fixing random seed being the solution is

27:19.840 --> 27:26.160
why should I prefer seed 1 2 3 over seed I don't know 42 or something like that so these are the

27:26.160 --> 27:32.320
problems we said well let's report uncertainties and now the nice thing again by mixing all the tasks

27:32.320 --> 27:37.120
and the runs you still get some a decent sample size which is let's say I have a single task

27:37.120 --> 27:41.840
and five seeds really reporting uncertainty five seeds you don't really do much because

27:41.840 --> 27:46.320
you just have five samples to sort of report some sort of measure of what is the variation

27:46.320 --> 27:52.240
but let's say I have five seeds and 10 tasks I have 50 samples now and now you can actually think

27:52.240 --> 27:58.160
about doing something with these 50 samples and reporting like uncertainty now starts to make sense

27:58.160 --> 28:03.760
and for that we went on to something called statistical bootstrapping which is the I think the

28:03.760 --> 28:09.440
first time I use bootstrapping in a different context then and then the way we typically use

28:09.440 --> 28:15.200
another but it was basically I guess without going into details of what it is doing but the main

28:15.200 --> 28:20.880
question we are trying to answer was we have finite data and we are trying to report evaluate

28:20.880 --> 28:25.520
a random variable we're saying we should also report what is the uncertainty we have which is if

28:25.520 --> 28:30.000
we were to evaluate seem algorithm again with maybe different seeds or under different

28:30.000 --> 28:35.040
random conditions what are we expected to get so this is why we enter into the solution of confidence

28:35.040 --> 28:39.680
intervals and there's a fun fact which I learned through while going through this so apparently

28:39.680 --> 28:44.160
in statistics also so there were other options also things like p-values and whatnot which people

28:44.160 --> 28:50.720
use in other areas but the main general of statistics in the US actually bans thresholding p-values

28:51.840 --> 28:55.920
so so people often say my result is significantly statistically significant or not

28:55.920 --> 29:01.440
seem to be really significant but I think there's a catch there which is it's like you say oh if

29:01.440 --> 29:07.040
my p-value is less than 0.5 then it's statistically significant but if my p-value is 0.51 then it is

29:07.040 --> 29:13.120
like not significant and that is an issue the other thing is your improvement or whatever your

29:13.120 --> 29:17.600
reporting is actually statistically significant but not practically significant maybe it's really

29:17.600 --> 29:21.840
trivial gain and I don't really care about if it's significant because it's like I don't know

29:21.840 --> 29:27.600
it's a 0.001 percent improvement or whatever you're doing so really what do you want to see is

29:27.600 --> 29:32.800
what is your effect size in some sense like what are the possibilities of gains you expect without

29:32.800 --> 29:36.800
thresholding and then if you really care about statistical significance you can also glean that

29:36.800 --> 29:43.040
from a confidence intervals but I saw like at least in rest of the statistics and other areas

29:43.040 --> 29:49.040
also there was a push for confidence intervals as opposed to using these p-values plus p-values

29:49.040 --> 29:54.480
are also harder to grok like these are something which makes this paper harder to read I think in

29:54.480 --> 30:00.000
some sense so that's why we went on with the solution of confidence intervals and so does the

30:00.000 --> 30:06.640
confidence interval you're asking researchers to publish these confidence intervals in conjunction

30:06.640 --> 30:13.520
with their RL experiments is that replacing asking them to also publish the distributions or

30:13.520 --> 30:17.600
they complement one another yeah so they are both complemented so apparently your distribution

30:17.600 --> 30:23.120
node also have a confidence interval so the funny thing is think about it which is if you repeat

30:23.120 --> 30:27.360
your experiment with a different set of seats your distribution would also change so in some

30:27.360 --> 30:31.360
sense your CDS and this is why I think we say don't use something like or that's why you didn't

30:31.360 --> 30:35.600
post something like a box plot because your distribution itself has uncertainty and if you

30:35.600 --> 30:40.800
plot the CDS you can also plot the confidence bands around your CDS so really like reporting

30:40.800 --> 30:45.200
uncertainties is one of the key things we are saying don't report these point estimates so what

30:45.200 --> 30:49.760
people typically do is report a single number which is saying this is my aggregate result across

30:49.760 --> 30:55.360
my benchmark we're saying oh no no there is actually some uncertainty in your aggregate result

30:55.360 --> 31:00.320
and it can be larger it can be small so it's better to just report this uncertainty also so that

31:00.320 --> 31:05.920
we have some idea of how high variance this algorithm is and how significant maybe the improvement is

31:05.920 --> 31:12.480
so yeah so think about basically any sort of result you obtain from a finite number of seats

31:12.480 --> 31:17.520
is a random variable at the end of the day and now there is some uncertainty associated with

31:17.520 --> 31:23.120
so there is some sort of confidence interval associated with that and that's why I think so

31:23.120 --> 31:27.280
any yeah so this is the sort of view we take that if you just think start thinking about random

31:27.280 --> 31:31.120
variables it's a lot of things make sense so maybe I'll present like one metric which we do

31:31.120 --> 31:36.240
proposes so think about the question of comparing to algorithms really what you have is you have

31:36.240 --> 31:40.320
two random variables and you have a bunch of samples from both of them and you're trying to ask

31:40.320 --> 31:44.800
me which random variable is better than the other now if you really if you think about it this way

31:44.800 --> 31:48.160
in the natural thing which will come to me my end is well what is the probability that this

31:48.160 --> 31:52.640
random variable is better than the other random variable and that's it you really just can compute

31:52.640 --> 31:56.880
this problem now what you have is you really you don't have the distributions you just have the

31:56.880 --> 32:01.360
empirical distributions but there is a way to compare the empirical distributions which is

32:01.360 --> 32:06.720
which is you just to pair by some comparisons for whatever points you have and you get a probability

32:06.720 --> 32:11.520
now here's the fun bit this probability itself is a random variable because if you repeat your

32:11.520 --> 32:16.720
experiment your probability changes so there is a confidence band even though this is a probability

32:16.720 --> 32:22.800
there is a confidence band around this probability but this this metric is a cool measure because

32:22.800 --> 32:26.960
it directly asks the question okay what is the probability that you're baseline or your method

32:26.960 --> 32:31.440
beats the baseline like if you're saying or claiming soda you can just evaluate this probability

32:31.440 --> 32:36.560
itself it does not care so the downside is it does not care about the fixed size which is your method

32:36.560 --> 32:41.920
can be 1% better or 100% better they'll still have the probability of 1 but at least ask this

32:41.920 --> 32:46.480
there a question of okay am I doing better than the baseline and gives you like an estimate of

32:46.480 --> 32:50.960
what is the likelihood that this is happening and you can glean a lot of the things they're

32:50.960 --> 32:56.880
surprisingly so okay so this was the second metric and I think the third metric we realized that

32:56.880 --> 33:01.680
so proposing or plotting distribution is a school and that is good for qualitative comparisons but

33:02.320 --> 33:07.600
it's unlikely that a method is or like a new algorithm is going to outperform another method

33:07.600 --> 33:12.160
in distribution across all the tasks and all the samples it's likely that these distributions

33:12.160 --> 33:17.760
would overlap at certain point and all these things would happen so eventually I think or

33:17.760 --> 33:22.800
research as a result with some sort of aggregate comparisons in some sense so we thought well

33:22.800 --> 33:27.920
okay let's talk about aggregate metrics here which is what do we use we use mean or median

33:27.920 --> 33:32.000
and this is common this is common across machine learning that whenever you have a benchmark

33:32.000 --> 33:35.840
yeah there are these these are the two sort of metrics which are really prevalent everywhere

33:35.840 --> 33:40.800
that either we report the mean performance or the median performance now at least in RL I think

33:40.800 --> 33:45.200
mean is not so common the reason being there are few tasks which can be really high performing

33:45.200 --> 33:49.440
or you're algorithm can be really high performing on some of these tasks and mean is just

33:49.440 --> 33:54.720
dominated by these outlier tasks so let's say you get a score below one on almost all the tasks

33:54.720 --> 34:00.240
except one where you get a score of 100 now mean score is just dominated by that one task so

34:00.240 --> 34:06.240
typically in RL we perform this median score because it's more robust but the fun thing about median

34:06.240 --> 34:11.360
is if I set the score to be zero and half of my tasks the median score wouldn't really change

34:12.240 --> 34:18.480
and so what kind of robust measure is this where I can just really just crash in half of my tasks

34:18.480 --> 34:23.440
and it wouldn't really reflect any sort of scenes so these aggregate metrics are somewhat misleading

34:23.440 --> 34:28.800
so here again we look back let's statistics and we said well what we can do is we can really

34:28.800 --> 34:34.480
again think about we have I don't know maybe five seats and 10 to ask the 15 numbers

34:34.480 --> 34:39.360
we can compute something like the interquartial mean which is like somewhere in between the mean

34:39.360 --> 34:44.080
and the median which is the mean of the middle 50 percent of the numbers you have so the middle

34:44.080 --> 34:48.640
to the five runs and now this is this has like best of the both properties of the mean and the median

34:48.640 --> 34:55.520
it cares about all the tasks in some sense and it also is robust to outliers like median

34:55.520 --> 34:59.920
but it has nicer properties in with now so this is something that was like in general that when

34:59.920 --> 35:04.160
you are trying to report aggregate benchmark performance there's something you can do but

35:04.720 --> 35:10.000
that's it the caveat is that the aggregate really hides a lot of the information which was given

35:10.000 --> 35:14.640
to you by the distribution which is aggregate is talking about a specific property so maybe I'll

35:14.640 --> 35:20.960
give you one sort of anecdote related to this so Natari at least typically what people have

35:20.960 --> 35:27.200
focused on is median scores or median normalized scores now the funny thing is we have algorithms

35:27.200 --> 35:32.480
which are improving on the median normalized scores but we said let's come up with this metric called

35:32.480 --> 35:37.600
which is a better version of mean which is optimality gap which is how far are we doing with respect

35:37.600 --> 35:42.320
to the humans so rather than really talking about what is your average performance let's talk about

35:42.320 --> 35:47.200
how far you are from the humans so lower is better so zero means you're really close to the humans

35:47.200 --> 35:52.640
and one means you're really random like you're as worse as you can be so so this is a metric

35:52.640 --> 35:57.360
be computed for all these existing algorithms and finally enough what we saw that a lot of these

35:57.360 --> 36:02.400
recent algorithms were improving in terms of their median performance or mean performance but they

36:02.400 --> 36:07.600
are doing worse in terms of their closeness to human performance which is in some sense they're

36:07.600 --> 36:13.360
getting better on a lot of the games but they're also doing worse than humans on a lot of these

36:13.360 --> 36:18.080
harder games possibly so that was something interesting and this is what happens I think when we

36:18.080 --> 36:23.760
really focus on a single metric in some sense and that's a downside of like aggregation but I do

36:23.760 --> 36:28.720
think aggregation is sort of a necessary ego because it's really hard to compare I don't know if

36:28.720 --> 36:33.040
you have a lot of tasks and results it's really hard to compare their distributions or these

36:33.040 --> 36:38.400
tables and people would ever want to report some sort of aggregate measure but aggregation has

36:38.400 --> 36:44.400
this harms I was just going to jump in and ask since publishing this have you seen the community

36:44.400 --> 36:51.040
kind of take up these additional measures and include them in their own research and publications

36:51.040 --> 36:56.960
yeah so we have seen some pickup still pretty early to say what could happen but I do think

36:56.960 --> 37:01.600
I at least have seen some pickup of this recently and like some of these I clear submissions and

37:01.600 --> 37:07.280
some of the new to submissions themselves and I think the the reason the community would like

37:07.280 --> 37:11.600
to pick this up is because it's really not asking you to do anything extra other than just

37:11.600 --> 37:16.640
evaluating your results more thoroughly and we also really is actually a pretty good open-source

37:16.640 --> 37:22.720
library so you just give me your row numbers for doing this so I think the reason we did that was

37:22.720 --> 37:27.040
because we realized if you just say these are all the things you do it takes some work to actually

37:27.040 --> 37:31.360
implement something and people are not going to do that if that's something additional they have

37:31.360 --> 37:35.360
to do so we thought well let's just come up with a library so that you give me your numbers

37:35.360 --> 37:39.680
and it'll just do the thing for you we even release the plotting scripts because I think we got

37:41.120 --> 37:45.200
like some apprace for the kind of plots we had in the paper and we thought it might be better

37:45.200 --> 37:50.320
actually if other researchers can use these kind of plots also and so the library you just kind of

37:50.320 --> 37:55.360
call it in your training loop and it's recording your no and I thought you were doing that so it's

37:55.360 --> 37:59.360
like you do whatever you're doing in your training loop at the end of the day you were reporting

37:59.360 --> 38:04.000
performance also send these numbers to the library it'll just give you a plot pack and then

38:04.000 --> 38:09.920
dad you can put directly in your paper without any like changes sort of so that's the night so

38:09.920 --> 38:16.160
I think we tried to make it as easy for people to use this now there's a bigger question about

38:16.160 --> 38:23.200
is there any incentive for researchers to do this because that's a tricky one but I do believe

38:23.200 --> 38:28.160
I think a lot of us are at least trying to do good science and if people are aware of these sort

38:28.160 --> 38:33.120
of issues they will go towards using them a lot of us are trying to publish and I guess those

38:33.120 --> 38:38.720
people may be more worried about oh if I report these uncertainties my results don't look as good

38:38.720 --> 38:46.320
as they did without these but the realities like these are the uncertainties are really just

38:46.320 --> 38:51.520
telling you oh this is the randomness you have in your reporter to serve can you maybe qualify

38:53.440 --> 38:58.960
or characterize the when you look at the existing algorithms did you

39:00.000 --> 39:07.680
did you find anything particularly surprising or dramatic in the reported results or was it kind

39:07.680 --> 39:14.240
of like there's a lot of variation here and you don't really know but it's not something that kind

39:14.240 --> 39:19.280
of fundamentally changes the way you think about a particular algorithm I think some of the rankings

39:19.280 --> 39:24.480
were even flipped in the because of the uncertainties that you see that the uncertainties for one

39:24.480 --> 39:29.920
method are really like much larger than the other and it's likely the other thing I think you'll think

39:29.920 --> 39:35.840
is a lot of the changes we were able to publish are actually not really resulting in an improvement

39:35.840 --> 39:40.400
it's just that happened that there were random fluctuations and you ended up getting it's almost

39:40.400 --> 39:46.240
like terrific but not exactly but it is something along those lines that there were enough random

39:46.240 --> 39:52.800
fluctuations in the setup that it's likely that you benefited from that and because of that the

39:52.800 --> 39:58.080
paper was able to be if like if you look at the empirical results only then it's possible that

39:58.080 --> 40:02.640
the method is not really doing any better than soda you're saying that some percentage of papers

40:02.640 --> 40:07.760
didn't really need to be papers in some sense yeah but I guess those are the papers which really

40:07.760 --> 40:13.440
depend on just the empirical evaluation I would say most papers really are not here's my method

40:13.440 --> 40:17.600
here's the 10 different benchmarks I evaluate them my method is better except I don't think those

40:17.600 --> 40:22.080
are the kind of papers or at least a lot of the papers are like that papers really have a lot of

40:22.080 --> 40:27.200
going on they have some sort of argument why this is a good algorithm or why this paper is worth

40:27.200 --> 40:32.000
publishing and I think the empirical results are only the supporting evidence for all that I don't

40:32.000 --> 40:37.040
think we're trying to sell the algorithm as being the best but we're trying to say oh here's a good

40:37.040 --> 40:42.880
method and here's another argument for why this is a good method in some sense so I don't think

40:42.880 --> 40:49.040
there's any downside of reporting uncertainties is just that I think the maybe as an author you might

40:49.040 --> 40:53.440
feel that the reviewers would not like this if my method is not really beating everything straight

40:53.440 --> 40:58.960
out of the like I don't know it's like they feel hiding the sort of information is better than

40:58.960 --> 41:04.880
actually presenting it but the sort of reality is that if someone else was to run their algorithm

41:04.880 --> 41:09.120
then they need to know what is the variation is in their method because it's possible I'm not

41:09.120 --> 41:14.480
able to reproduce the results and the reason was as simple as oh I ran on a different GPU so this

41:14.480 --> 41:19.440
happened on I think with one of the things I was trying so what happened is at least intensive

41:19.440 --> 41:24.400
low in jacks you can't really set the random seed because there are some operations on GPU

41:24.400 --> 41:30.400
which have randomness and which can't really be fixed now this was the only change in my code pace

41:30.400 --> 41:35.840
I evaluated 100 seeds for a single method twice and what I saw that was the correlation between

41:35.840 --> 41:42.560
the results was something around minus 0.2 to 0.2 which was like really tiny correlation basically

41:42.560 --> 41:48.640
just like this saying all the seeds for really just one change of GPU randomness made a random

41:48.640 --> 41:54.880
ness really a big issue eventually in metasels so this is a huge change even with like small randomness and maybe

41:54.880 --> 41:58.880
it's like a butterfly effect another that initially you collect different data and then you different

41:58.880 --> 42:04.080
update and what not and it expands but this sort of issues I think after our paper and I think

42:04.080 --> 42:08.960
concurrent to our paper there were other works also which point out similar issues in vision

42:08.960 --> 42:14.560
and natural language processing in fine tuning these pre-trained models they say that it really

42:14.560 --> 42:20.560
depends on what exact model you use and all these other hypotheticals we set up so it's just

42:20.560 --> 42:27.200
like I'm saying reporting uncertainty is more true to reality it might make the results look

42:27.200 --> 42:31.920
slightly worse but this is what the reality looks like and now if you deliberately try to hide

42:31.920 --> 42:38.560
reality that is like so I think ignorance is okay like I think most of the researchers were really

42:38.560 --> 42:42.960
sort of not aware that these were the issues because some of our own papers had reported these

42:42.960 --> 42:47.760
point estimates which is we said here's a benchmark here's an algorithm here's the result we get

42:47.760 --> 42:52.640
and we just compared them but now we are aware that there is a huge uncertainty in these sort of

42:52.640 --> 42:57.760
evaluations so we would be more willing to report these sort of things the showcase okay what is

42:57.760 --> 43:03.200
the uncertainty or at least try to make sure that our results are robust statistically that is

43:03.200 --> 43:08.240
like and that's I think this is also one of the reasons we see these papers that say oh here are

43:08.240 --> 43:13.120
here we evaluated a bunch of these soda models and nothing really holds up in these applications

43:13.120 --> 43:18.400
actually on a controlled experiment and the reason is it's just because a lot of the times you're

43:18.400 --> 43:23.600
not reporting these sort of uncertainty measures and we thought like these methods were actually

43:23.600 --> 43:30.560
better but they really were not in some sense yeah one maybe one point I want to say is like

43:30.560 --> 43:35.280
so I think the actual incentive for doing all this is doing good and reproducible science

43:35.280 --> 43:39.920
but I don't think that's really a strong incentive the better incentive would be if conferences

43:39.920 --> 43:44.880
for example try to enforce this and they do to some extent so if you look at Neurif's checklist

43:44.880 --> 43:50.560
for example they do ask that have you reported error bars and now the funny thing is people do

43:50.560 --> 43:55.760
report yes to that question but also not report error bars when it comes to things like aggregate

43:55.760 --> 44:02.160
metrics because like it's harder to think of oh this is an overall result across all the task

44:02.160 --> 44:09.120
what is the error bar mean here in some sense and is the does the effect of the kind of randomness

44:09.120 --> 44:16.560
we're talking about is it outsized in RL due to the kind of repetitive nature of it relative

44:16.560 --> 44:22.000
to you know vision and LP other things yeah yeah I feel so there are like two kind of

44:22.640 --> 44:28.000
yeah so I feel that is the case and there are two sources of it one is when you collect your own

44:28.000 --> 44:33.280
data so it depends really on where you are at and if there was some randomness it will sort of

44:33.280 --> 44:39.040
activate more over time because this randomness affected what was the initial data you collected

44:39.040 --> 44:43.200
and then the second source is we are learning from our own predictions in some sense when we're

44:43.200 --> 44:47.760
whenever we're doing off-false learning with Q learning and if you're so it really highly depends

44:47.760 --> 44:53.520
on what your initial estimates of whatever the function your learning was so and I think both of

44:53.520 --> 45:00.400
these amplify like the randomness so definitely I think it's more prevalent but then again there are

45:00.400 --> 45:06.560
cases like I was talking about NLP earlier so when you do fine tuning a pre-trained model they find

45:06.560 --> 45:11.360
there is actually a huge uncertainty there again depending on for example simple things like

45:11.360 --> 45:16.640
what was the order of the samples you point you down over was the exact point you used to fine

45:16.640 --> 45:23.120
you so there are like probably so I think any area where these sort of statistical considerations can

45:23.120 --> 45:28.400
have large influence it should be the case that we report uncertainties so I I don't know where

45:28.400 --> 45:33.520
did I see this but I saw like a paper where they said that even on image net if I just

45:33.520 --> 45:38.560
framed a resonant model the variation is something around plus minus 1% so it's a Gaussian

45:38.560 --> 45:45.040
distribution but depending on the seed I pick my result can be 1% or better or 1%

45:45.040 --> 45:50.000
force and this is a huge number if we are talking about image net because people publish results

45:50.000 --> 45:55.760
with like 0.1% of differences but yeah coming back to incentives I do think there's

45:56.880 --> 46:01.440
it's unclear what are the actual incentives for researchers to probably or at least to do

46:01.440 --> 46:08.160
these things because if it so rigor makes publications or harder to publish in some sense and

46:08.880 --> 46:14.800
sometimes it does make it easier if your results are really great but most of the scenarios it will

46:14.800 --> 46:21.760
have problems and researchers might not want to be more rigorous so that they can publish easily

46:21.760 --> 46:26.560
so there is this trade-off I think but shouldn't really be a trade-off but if you're really going

46:26.560 --> 46:32.400
for I want to publish this paper then I think you may not want to report uncertainties and work not

46:32.400 --> 46:39.760
so that's something we are not clear on. Where does the these uncertainty measures that

46:39.760 --> 46:49.200
that you're proposing here how do they fit into kind of a broader imagining that others have

46:49.200 --> 46:54.480
published things that they also think that papers should all include and and how many of those

46:54.480 --> 47:00.480
how long is that list and you know where does this fit in that list in terms of potential impact

47:00.480 --> 47:06.800
on the field do you think. Right so I think there were a lot of things we took into consideration

47:06.800 --> 47:12.160
when thinking about these metrics and we said let's try to do or at least build on what people

47:12.160 --> 47:17.200
already actually do so people already actually report some sort of aggregate performance measures

47:17.200 --> 47:22.480
at least in RL and we said what's the minimal change we can do rather than going to like complicated

47:22.480 --> 47:27.200
statistical tests and what all those things people already want to report some sort of standard deviation

47:27.200 --> 47:32.160
and oftentimes they do so if you use mean for example it's easy to compute the standard deviation

47:32.160 --> 47:36.720
now the thing is when you think about median people haven't reported any sort of uncertain

47:36.720 --> 47:40.560
measure and the reason is it's unclear what the standard deviation would look like or how would

47:40.560 --> 47:45.520
you compute it unless you are willing to use that's a more advanced statistical would sharpen tools

47:45.520 --> 47:49.440
so I think this was the reason or at least this is maybe one of the reasons that we typically

47:49.440 --> 47:54.720
don't report uncertainty measures because we already do if we are able to do them it's just that

47:54.720 --> 48:00.640
once your metric is not as straightforward as mean then I think it's become unclear how do you

48:00.640 --> 48:06.960
actually calculate the variance and because the only way to do so is to simulate your distribution

48:06.960 --> 48:15.280
and see what the variance looks like and so I think my and at least our understanding of the tools

48:15.280 --> 48:20.640
we propose is are also some of these tools are already in use in computer science like I've been

48:20.640 --> 48:26.640
saying like the sort of motivation for these performance profiles came from a paper published

48:26.640 --> 48:31.280
in I think 2000 which is I think the standard for optimization software in computer science

48:31.280 --> 48:35.360
they already use some of these tools and these are like tried and tested tools it's just that

48:35.360 --> 48:42.800
maybe we can also adopt them and now the I think the maybe the sort of the question you're asking

48:42.800 --> 48:48.320
for that I feel there might be better ways around reporting uncertainties for example because

48:48.320 --> 48:52.480
we we just propose like the simplest possible measures which we can do and we tested their

48:52.480 --> 48:59.760
like correctness on some of the tasks but it's possible that there might be better ways to report

48:59.760 --> 49:05.040
uncertainty which are more correct in some sense or let's say theoretical guarantees around this

49:05.040 --> 49:09.200
also I think some of the problems which we don't tackle is let's say I give you a single task

49:09.200 --> 49:13.440
and you have really have three proceeds what do you do like I don't think our paper really talks

49:13.440 --> 49:18.000
anything about that but that is a common setup I don't really have a benchmark I just have two

49:18.000 --> 49:23.440
three tasks and I want to compare these algorithms what do I do to do like better comparisons in some

49:23.440 --> 49:30.000
sense and that still is an open problem I think but with regards to where does it fit I would say

49:30.000 --> 49:35.280
it's really the high level message of the paper is we should be reporting interval estimates of

49:35.280 --> 49:40.720
things and here are some tools which we have developed where you can use but there might be other

49:40.720 --> 49:44.640
ways also to do this and please feel free to do so I don't think you should be using these

49:44.640 --> 49:49.280
specific tools if you feel that's not the right way to go but you should be reporting some measure

49:49.280 --> 49:54.960
of uncertainty this is the high level message the maybe the only advantage of the tools is you

49:54.960 --> 49:59.040
don't really have to do anything extra than what you have been doing like no extra logging no extra

49:59.040 --> 50:06.320
sort of seed evaluation would not it is really geared towards a few seed and like multitasking

50:06.320 --> 50:11.360
awesome well we shall thanks so much for taking us through that very cool work and

50:11.360 --> 50:17.360
congrats again on the award

