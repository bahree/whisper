1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,480
I'm your host, Sam Charrington.

4
00:00:23,480 --> 00:00:28,040
A quick thanks to everyone who participated in last week's Twimble Online Meetup.

5
00:00:28,040 --> 00:00:29,960
It was another great one.

6
00:00:29,960 --> 00:00:35,480
If you missed it, the recording will be posted to the Meetup page at twimbleai.com slash

7
00:00:35,480 --> 00:00:36,480
meetup.

8
00:00:36,480 --> 00:00:37,880
Definitely check it out.

9
00:00:37,880 --> 00:00:43,680
I never cease to be amazed by the generosity and creativity of the Twimble community.

10
00:00:43,680 --> 00:00:48,480
And I'd like to send a special shout out to listener Sharon Glander for her exceptional

11
00:00:48,480 --> 00:00:50,280
sketch notes.

12
00:00:50,280 --> 00:00:55,120
Sharon has been creating beautiful hand sketch notes of her favorite Twimble episodes

13
00:00:55,120 --> 00:00:57,160
and sharing them with the community.

14
00:00:57,160 --> 00:01:00,400
Sharon, we truly love and appreciate what you're doing with those.

15
00:01:00,400 --> 00:01:02,920
So please keep up the great work.

16
00:01:02,920 --> 00:01:06,000
We'll link to her sketch notes in the show notes for this episode.

17
00:01:06,000 --> 00:01:11,440
And you should definitely follow her on Twitter at sharingglander for more.

18
00:01:11,440 --> 00:01:15,840
This is your last chance to register for the rework, deep learning and AI assistant

19
00:01:15,840 --> 00:01:22,200
summits in San Francisco, which are this Thursday and Friday, January 25th and 26th.

20
00:01:22,200 --> 00:01:26,560
These events feature leading researchers and technologists like the ones you heard in

21
00:01:26,560 --> 00:01:29,680
our Deep Learning Summit series last week.

22
00:01:29,680 --> 00:01:34,640
The San Francisco event is headlined by Ian Goodfellow of Google Brain, Daphne Kohler

23
00:01:34,640 --> 00:01:37,360
of Calico Labs, and more.

24
00:01:37,360 --> 00:01:43,000
Definitely check it out and use the code Twimbleai for 20% off of registration.

25
00:01:43,000 --> 00:01:48,800
In this episode, I'm joined by Inmar Gavoni, Autonomy Engineering Manager at Uber ATG to

26
00:01:48,800 --> 00:01:53,800
discuss her work on the paper MinMax Propagation, which was presented at Nip's last month

27
00:01:53,800 --> 00:01:55,280
in Long Beach.

28
00:01:55,280 --> 00:01:59,680
And more and I get into a really media discussion about graphical models, including what they

29
00:01:59,680 --> 00:02:04,800
are, how they're used, some of the challenges they present for both training and inference,

30
00:02:04,800 --> 00:02:07,520
and how and where they can be best applied.

31
00:02:07,520 --> 00:02:12,080
Then we jump into an in-depth look at the key ideas behind the MinMax Propagation paper

32
00:02:12,080 --> 00:02:16,600
itself, including the relationship to the broader domain of belief propagation and ideas

33
00:02:16,600 --> 00:02:21,400
like affinity propagation, and how all these can be applied to a use case example like

34
00:02:21,400 --> 00:02:23,240
the Makesband problem.

35
00:02:23,240 --> 00:02:27,960
This was a really fun conversation, and now on to the show.

36
00:02:27,960 --> 00:02:37,800
All right, everyone, I am on the line with Inmar Gavoni.

37
00:02:37,800 --> 00:02:44,080
Inmar is the Autonomy Engineering Manager at Uber ATG, that's Uber's Advanced Technology

38
00:02:44,080 --> 00:02:46,400
Group in Toronto, Canada.

39
00:02:46,400 --> 00:02:48,960
Inmar, welcome to this week in Machine Learning and AI.

40
00:02:48,960 --> 00:02:49,960
Thank you, Sam.

41
00:02:49,960 --> 00:02:50,960
It's great to be here.

42
00:02:50,960 --> 00:02:53,040
It's awesome to have you on the show.

43
00:02:53,040 --> 00:02:57,600
You've been trying to get this coordinated for quite a while, so I'm glad we're able

44
00:02:57,600 --> 00:03:00,400
to make it happen early in the new year.

45
00:03:00,400 --> 00:03:01,400
Absolutely.

46
00:03:01,400 --> 00:03:03,000
I'm excited about this.

47
00:03:03,000 --> 00:03:07,480
As is our tradition here, why don't we get started by having you tell the audience a

48
00:03:07,480 --> 00:03:11,280
little bit about your background and how you got interested in machine learning?

49
00:03:11,280 --> 00:03:12,280
Sure.

50
00:03:12,280 --> 00:03:16,960
So, for me, it actually started pretty early on, in the sense that back in high school,

51
00:03:16,960 --> 00:03:22,320
I thought I wanted to be a neuroscientist, and I was pretty sure that understanding and

52
00:03:22,320 --> 00:03:28,360
researching the brain is the most interesting thing that I could apply myself to.

53
00:03:28,360 --> 00:03:35,160
So going into university, I chose the areas of computer science and biology, because

54
00:03:35,160 --> 00:03:39,680
I thought of brains as machines and thinking that the computer science and engineering

55
00:03:39,680 --> 00:03:43,760
approaches would be useful for understanding them and biology because it is, after all,

56
00:03:43,760 --> 00:03:46,040
a biological substance.

57
00:03:46,040 --> 00:03:50,960
And I spend a lot of my time taking neuroscience courses and talking to your neuroscientists

58
00:03:50,960 --> 00:03:54,680
and trying to understand how they approach solving the problem.

59
00:03:54,680 --> 00:04:00,320
At the same time, towards the end of my undergraduate degrees of the last year, I also took a machine

60
00:04:00,320 --> 00:04:02,040
learning course.

61
00:04:02,040 --> 00:04:05,840
And first of all, the machine learning course was really interesting.

62
00:04:05,840 --> 00:04:12,440
It was such a different way of thinking about how to solve problems that really appeal

63
00:04:12,440 --> 00:04:13,440
to me.

64
00:04:13,440 --> 00:04:15,840
The mathematics of it were beautiful.

65
00:04:15,840 --> 00:04:22,120
It combines a lot of things that I previously learned, and they never really clicked into

66
00:04:22,120 --> 00:04:23,120
one place.

67
00:04:23,120 --> 00:04:29,600
So anything from linear algebra, calculus, probability theory, statistics, graph theory,

68
00:04:29,600 --> 00:04:34,560
computer torques, all of it is used in one way or the other in machine learning.

69
00:04:34,560 --> 00:04:39,640
And also, it allows us to solve problems that you can't really solve with traditional

70
00:04:39,640 --> 00:04:42,240
programming or engineering approaches.

71
00:04:42,240 --> 00:04:46,760
So it's a whole new mathematical way of looking at these problems and then coming up with

72
00:04:46,760 --> 00:04:48,760
really beautiful solutions.

73
00:04:48,760 --> 00:04:53,840
And as for the neuroscience, I felt like maybe instead of trying to understand the brain,

74
00:04:53,840 --> 00:04:59,440
a different way of doing it would be to try and build software that exhibits intelligent

75
00:04:59,440 --> 00:05:00,600
behavior.

76
00:05:00,600 --> 00:05:04,760
So that drove my decision to do a PhD in machine learning.

77
00:05:04,760 --> 00:05:10,520
And so I moved to Toronto, which is one of the best places in the world to do machine learning

78
00:05:10,520 --> 00:05:14,000
research and did that for a few years.

79
00:05:14,000 --> 00:05:20,640
And while I was in school, I did a few internships and I also realized that I really like working

80
00:05:20,640 --> 00:05:25,680
towards a specific product and towards something that is tangible and is out there in the world

81
00:05:25,680 --> 00:05:29,520
and people use and is also up for that type of scrutiny.

82
00:05:29,520 --> 00:05:36,840
So after graduation, I worked in various companies on applications of machine learning to

83
00:05:36,840 --> 00:05:41,960
real world products and I like physical things, so in all of them, there was some sort of

84
00:05:41,960 --> 00:05:45,960
a physical product that you can actually touch.

85
00:05:45,960 --> 00:05:51,640
And of course, self-driving cars is one of the most exciting new technologies.

86
00:05:51,640 --> 00:05:55,720
I mean, maybe the technology itself is not that new or the idea of doing it is not

87
00:05:55,720 --> 00:05:56,720
that new.

88
00:05:56,720 --> 00:06:02,320
It's been around for about a decade at least, but it's now really coming into full attention

89
00:06:02,320 --> 00:06:07,920
from everyone in the world and I thought joining this office would be an incredible opportunity.

90
00:06:07,920 --> 00:06:10,080
So I've been here for a few months now.

91
00:06:10,080 --> 00:06:11,080
Oh, wow.

92
00:06:11,080 --> 00:06:12,880
And what specifically do you do there?

93
00:06:12,880 --> 00:06:17,280
So the office in Toronto is a research and development office.

94
00:06:17,280 --> 00:06:24,280
It's led by Raquel Ortison who is one of the world experts in the intersection of self-driving,

95
00:06:24,280 --> 00:06:26,720
deep learning and computer vision.

96
00:06:26,720 --> 00:06:33,680
And there is a research group of researchers who are working on creating new innovative

97
00:06:33,680 --> 00:06:38,200
cutting edge algorithms for using deep learning for self-driving.

98
00:06:38,200 --> 00:06:46,160
My role is helping take this product, this research prototypes and first phase algorithms

99
00:06:46,160 --> 00:06:48,720
and actually get them into the car.

100
00:06:48,720 --> 00:06:50,720
So get them into production.

101
00:06:50,720 --> 00:06:51,720
Oh, wow.

102
00:06:51,720 --> 00:06:58,240
I manage a team of applied researchers and software engineers who help with everything

103
00:06:58,240 --> 00:07:03,240
that has to happen in order to take a prototype, something that resembles what you would get

104
00:07:03,240 --> 00:07:09,400
out of a publication and actually make it into a production feature.

105
00:07:09,400 --> 00:07:10,400
Hmm.

106
00:07:10,400 --> 00:07:17,360
And we've talked about, we did a series on autonomous driving last year, I think in the fall

107
00:07:17,360 --> 00:07:23,720
towards the end of last year, and one of the things that jumped out of that was the different

108
00:07:23,720 --> 00:07:30,720
approaches and philosophies to doing machine learning for autonomous driving, kind of,

109
00:07:30,720 --> 00:07:37,200
you know, N10 versus integrating different systems, you know, versus, you know, and camera,

110
00:07:37,200 --> 00:07:41,600
camera first versus, you know, sensor fusion approaches.

111
00:07:41,600 --> 00:07:47,280
Does Uber have a kind of a, what's Uber's approach to?

112
00:07:47,280 --> 00:07:51,240
Autonomous vehicles and in that domain.

113
00:07:51,240 --> 00:07:55,960
So we're definitely as a place that specializes in deep learning.

114
00:07:55,960 --> 00:08:02,200
We believe that deep learning approaches for the various aspects of technologies you

115
00:08:02,200 --> 00:08:08,600
need to introduce, you know, around perception, understanding where the car is understanding

116
00:08:08,600 --> 00:08:16,640
who the actors are, can all benefit from deep learning in terms of what kind of architecture

117
00:08:16,640 --> 00:08:20,200
specifically or, you know, the sensors and so on.

118
00:08:20,200 --> 00:08:26,360
I think this is still something that is an ongoing research and understanding what are the

119
00:08:26,360 --> 00:08:32,240
benefits of the different sensors and when is there a time to use them and in what configuration?

120
00:08:32,240 --> 00:08:35,040
So I would say that's still an open question.

121
00:08:35,040 --> 00:08:42,720
And so one of the things that we wanted to dig into in this conversation was a paper that

122
00:08:42,720 --> 00:08:53,720
you co-author that was released that nips or was in the nips proceedings on minmax propagation.

123
00:08:53,720 --> 00:09:00,400
One of the things that I observed that nips was a lot of conversations around graphical

124
00:09:00,400 --> 00:09:04,360
models and graphical approaches in general for machine learning.

125
00:09:04,360 --> 00:09:10,160
And as maybe a segue into that conversation, I wanted to get a sense from you, is that,

126
00:09:10,160 --> 00:09:16,720
would you say that there was a kind of heightened interest in graphical models this year or

127
00:09:16,720 --> 00:09:24,480
is it a conversation that has been, you know, continuing on, you know, without, I guess

128
00:09:24,480 --> 00:09:30,000
is there a heightened level of interest in this particular type of modeling approach?

129
00:09:30,000 --> 00:09:31,000
Right.

130
00:09:31,000 --> 00:09:33,240
So here is my take on it.

131
00:09:33,240 --> 00:09:41,440
Visual models are very powerful mathematical tools to represent our understanding of the

132
00:09:41,440 --> 00:09:46,120
world and our understanding of what we don't understand and the fact that there is uncertainty

133
00:09:46,120 --> 00:09:47,120
and noise.

134
00:09:47,120 --> 00:09:49,880
So as modeling tools, they're very powerful.

135
00:09:49,880 --> 00:09:57,760
The problem has been that they're very difficult to train and to do learning and inferencing.

136
00:09:57,760 --> 00:10:07,720
So they are computationally expensive and they do not scale up to full blown industrial

137
00:10:07,720 --> 00:10:08,720
purposes.

138
00:10:08,720 --> 00:10:14,120
So it was a, there has always been, not always, but you know, there's been an ongoing

139
00:10:14,120 --> 00:10:17,560
flourishing and research in graphical models for a long time.

140
00:10:17,560 --> 00:10:19,920
I entered the field doing research.

141
00:10:19,920 --> 00:10:26,000
I would say around 2006 and these were very popular and like a big area of focus and

142
00:10:26,000 --> 00:10:31,680
in fact my thesis focuses on application of graphical models to clustering.

143
00:10:31,680 --> 00:10:32,680
Okay.

144
00:10:32,680 --> 00:10:38,960
But it remained within, I would say primarily within academia with the exception of specific

145
00:10:38,960 --> 00:10:44,760
models that were simpler to use than others and were used in industry.

146
00:10:44,760 --> 00:10:49,000
And then we had, you know, over the last few years, obviously, we had this intense focus

147
00:10:49,000 --> 00:10:55,280
on deep learning and neural networks and a lot of people worked on them.

148
00:10:55,280 --> 00:11:01,600
And people shifted from working sound graphical models to focusing on innovation in deep learning.

149
00:11:01,600 --> 00:11:07,240
And I think now that we are kind of getting to a place where a lot of the groundwork has

150
00:11:07,240 --> 00:11:12,920
been done and a lot of the, let's call it the low hanging fruits have been picked.

151
00:11:12,920 --> 00:11:17,560
Now there is going interest in looking again at graphical models and then asking, given

152
00:11:17,560 --> 00:11:24,960
all we've learned about how to do deep learning at scale and how to use it and how to be

153
00:11:24,960 --> 00:11:32,920
able to solve difficult problems with it, can we now merge better the two strategies and

154
00:11:32,920 --> 00:11:39,760
can we create new types of models that leverage the leverage both so that maybe graphical models

155
00:11:39,760 --> 00:11:45,160
can become as useful as deep learning models.

156
00:11:45,160 --> 00:11:51,200
And what are the types of applications that lend themselves most readily to use of graphical

157
00:11:51,200 --> 00:11:52,200
models?

158
00:11:52,200 --> 00:12:00,520
There are many different applications, one that is commonly I would say looked at is in,

159
00:12:00,520 --> 00:12:01,920
let's say, healthcare.

160
00:12:01,920 --> 00:12:09,640
So understanding or predicting whether a particular patient has that particular type of cancer,

161
00:12:09,640 --> 00:12:15,280
you can describe all that you know about the different measurements you've taken and

162
00:12:15,280 --> 00:12:22,280
so on using a graphical model and then you can try to infer the actual underlying state

163
00:12:22,280 --> 00:12:27,240
of their illness if they have on the type, the stage and so on.

164
00:12:27,240 --> 00:12:35,880
And the main idea as I understand it behind the application of graphical models is as

165
00:12:35,880 --> 00:12:42,800
opposed to traditional, I guess traditional, we'll start with calling it traditional,

166
00:12:42,800 --> 00:12:48,640
with a typical data set, let's say where you've got data points and the data set doesn't

167
00:12:48,640 --> 00:12:55,280
really express any inherent relationship between the data points as opposed to a relationship

168
00:12:55,280 --> 00:13:02,640
between you know features within those data points within, well graphical models are

169
00:13:02,640 --> 00:13:10,760
really trying to do is identify relationships between the data points and the data set

170
00:13:10,760 --> 00:13:13,480
is that an accurate assessment?

171
00:13:13,480 --> 00:13:20,360
Yes, yes, they deal with representing all sorts of quantities in the world as variables

172
00:13:20,360 --> 00:13:27,080
and either explicitly saying that there are known complex relationships between these

173
00:13:27,080 --> 00:13:32,920
variables or trying to learn the complex relationships between these variables.

174
00:13:32,920 --> 00:13:40,160
And when you say explicitly saying that there are known complex relationships is, are

175
00:13:40,160 --> 00:13:45,320
you there describing kind of pulling in prior knowledge about the relationships between

176
00:13:45,320 --> 00:13:48,520
these variables into the models or?

177
00:13:48,520 --> 00:13:54,840
Yeah, that would be one way of doing it where you represent all sorts of prior knowledge

178
00:13:54,840 --> 00:14:01,720
given with distributions that you believe are well suited to represent your prior knowledge

179
00:14:01,720 --> 00:14:07,400
and also representing the actual relationships between the variables.

180
00:14:07,400 --> 00:14:13,080
So if we're trying to think of a relatively simple example, let's say that I'm trying

181
00:14:13,080 --> 00:14:16,120
to represent an image in terms of a graph.

182
00:14:16,120 --> 00:14:23,440
So maybe every pixel in the image will be a node in the graph and every pixel will be connected

183
00:14:23,440 --> 00:14:28,000
to the pixel that are around it and these connections, these edges on the graph represent

184
00:14:28,000 --> 00:14:33,640
the fact that we think that the value of the pixel is highly correlated to the value of

185
00:14:33,640 --> 00:14:37,840
the pixel or is around it, right? So for example, if we know about the pixels around it,

186
00:14:37,840 --> 00:14:43,120
we can make guesses as to the value of the pixel or we can represent the possible values

187
00:14:43,120 --> 00:14:47,240
of it as a distribution that is not uniform, it learned, it knows something from it's

188
00:14:47,240 --> 00:14:48,240
surrounding.

189
00:14:48,240 --> 00:14:53,400
So that would be one way of representing relationships between variables.

190
00:14:53,400 --> 00:14:59,240
And our images is that a common application area for graphical models?

191
00:14:59,240 --> 00:15:07,280
So images used to be before we were able to successfully work with neural networks.

192
00:15:07,280 --> 00:15:11,320
But in terms of performance right now, I would say that convolutional neural networks

193
00:15:11,320 --> 00:15:19,080
are much better at tasks around predictions and prediction and detection and various tasks

194
00:15:19,080 --> 00:15:21,000
that have to do with images.

195
00:15:21,000 --> 00:15:30,360
So what have been some of the biggest achievements to date of this renewed interest in graphical

196
00:15:30,360 --> 00:15:38,080
models with the background of the progress and deep learning over the recent years?

197
00:15:38,080 --> 00:15:48,600
I think one interesting idea is to allow for neural networks to operate over graphs.

198
00:15:48,600 --> 00:15:57,600
So there is a new researcher in graph neural networks and graph convolutional neural networks.

199
00:15:57,600 --> 00:16:08,160
And other ideas are that within the network queue represents some of the new model, some

200
00:16:08,160 --> 00:16:15,120
of the network explicitly using some formulation of a graphical model.

201
00:16:15,120 --> 00:16:19,880
And so what does it mean to have the network operate over a graph?

202
00:16:19,880 --> 00:16:23,680
So again, going back to the example of images.

203
00:16:23,680 --> 00:16:26,440
If you think about it as a graph, it's a grid, right?

204
00:16:26,440 --> 00:16:33,680
It's a simple graph and so we can very easily do operations on it like convolutional operations

205
00:16:33,680 --> 00:16:39,280
because we just shift them, shift the filter over and it's the same operation.

206
00:16:39,280 --> 00:16:43,840
But you can also obstruct this notion of a convolution to something where it's not

207
00:16:43,840 --> 00:16:47,160
a nice grid, but it's still a graph.

208
00:16:47,160 --> 00:16:54,080
So but that requires the mathematics to be developed for it and to be made efficient.

209
00:16:54,080 --> 00:17:02,080
So then so is the idea then to kind of replicate what a convolutional network is doing, but

210
00:17:02,080 --> 00:17:04,800
with graphical models?

211
00:17:04,800 --> 00:17:05,800
Not necessarily.

212
00:17:05,800 --> 00:17:13,320
It's just to use the, I guess the body of knowledge and work from graphical models and maybe

213
00:17:13,320 --> 00:17:17,440
we can get into inference a little bit in a moment.

214
00:17:17,440 --> 00:17:23,960
So for doing inference in graphical models, that is not really captured within neural networks.

215
00:17:23,960 --> 00:17:30,480
So so neural networks, people talk about doing learning and inference in neural networks,

216
00:17:30,480 --> 00:17:37,320
but really I think that it's come to mean that learning is when you train the algorithm.

217
00:17:37,320 --> 00:17:38,320
Right.

218
00:17:38,320 --> 00:17:42,360
And inference is when at test time you basically run forward propagations, your network

219
00:17:42,360 --> 00:17:45,480
can come up with a prediction.

220
00:17:45,480 --> 00:17:51,120
You know, it's it's a correct use of the term, but it's in the context of neural networks

221
00:17:51,120 --> 00:17:57,320
that the inference part is easy because it's just a straightforward calculation.

222
00:17:57,320 --> 00:18:01,480
Sometimes computing this prediction, so computing in the case of neural networks, like what

223
00:18:01,480 --> 00:18:04,960
is the probability that there is a particular object in the image, let's say, right?

224
00:18:04,960 --> 00:18:10,080
And then it speeds out a probability of whether it's there or not in this particular place,

225
00:18:10,080 --> 00:18:11,080
let's say.

226
00:18:11,080 --> 00:18:16,640
So in some cases, let's say when when the underlying model is a graphical model, computing this

227
00:18:16,640 --> 00:18:21,920
probability is in and on itself a difficult sub problem.

228
00:18:21,920 --> 00:18:29,080
And then there is a large body of work on computing, running this algorithm.

229
00:18:29,080 --> 00:18:35,520
And in some cases, in particular, in graphical models, the within training itself, so not

230
00:18:35,520 --> 00:18:40,640
just when you're a test time, you're interliving the learning and inference part.

231
00:18:40,640 --> 00:18:47,440
So the learning is often referred to coming up with estimates over the hidden variables.

232
00:18:47,440 --> 00:18:50,920
And we didn't really talk about what hidden variables are, but sorry, coming with estimates

233
00:18:50,920 --> 00:18:56,720
of the parameters governing the behavior of the hidden variables, the parameters in

234
00:18:56,720 --> 00:18:59,800
a neural network would be the weights.

235
00:18:59,800 --> 00:19:03,400
And so we're learning, we're basically optimizing the weights.

236
00:19:03,400 --> 00:19:08,320
And in graphical models, the parameters can be, for example, parameters that govern

237
00:19:08,320 --> 00:19:09,320
distribution.

238
00:19:09,320 --> 00:19:13,920
Like, I assume that there is a Gaussian and I don't know what is the mean and the variance

239
00:19:13,920 --> 00:19:14,920
of the Gaussian.

240
00:19:14,920 --> 00:19:17,760
So that would be the parameters that I'm trying to learn.

241
00:19:17,760 --> 00:19:22,640
But then there is also an inference part, which is computing all sorts of typically predictions

242
00:19:22,640 --> 00:19:29,200
or probability distributions on the variables of interest and that requires running some

243
00:19:29,200 --> 00:19:32,480
sort of a sometimes complex computation.

244
00:19:32,480 --> 00:19:36,280
And when I'm done with that, I sometimes iterate back to the learning part.

245
00:19:36,280 --> 00:19:40,800
So given that I've refined my estimate of the probabilities, now I refine my estimate

246
00:19:40,800 --> 00:19:46,160
of the learning part of the parameters and go back and forth between the two.

247
00:19:46,160 --> 00:19:52,600
So what I've just described is a very high level and algorithm called the expectation

248
00:19:52,600 --> 00:19:58,240
maximization algorithm or EM algorithm, which is used for doing learning and inference in

249
00:19:58,240 --> 00:20:03,120
graphical models like a Gaussian mixture model, for instance.

250
00:20:03,120 --> 00:20:06,800
And if you want, we can talk a little bit about what is a Gaussian mixture model.

251
00:20:06,800 --> 00:20:12,240
Yeah, before we do that, so the expectation maximization that you were just just describing,

252
00:20:12,240 --> 00:20:15,120
this is trying to contextualize this.

253
00:20:15,120 --> 00:20:20,320
We were talking about training and inference and you started to talk about inference.

254
00:20:20,320 --> 00:20:26,720
But this is, this is used as part of training, but it includes inference calculations.

255
00:20:26,720 --> 00:20:27,720
Is that the?

256
00:20:27,720 --> 00:20:34,560
Yeah, so again, in non neural network world, where you're talking about the training procedure

257
00:20:34,560 --> 00:20:39,120
itself may include both learning and inference.

258
00:20:39,120 --> 00:20:40,120
Got it.

259
00:20:40,120 --> 00:20:46,720
And so you need to maybe the Gaussian mixture model is a good example to run through for

260
00:20:46,720 --> 00:20:47,720
that.

261
00:20:47,720 --> 00:20:52,240
So in Gaussian mixture models, we basically have a whole bunch of data.

262
00:20:52,240 --> 00:20:54,720
It's a way of clustering the data, let's say.

263
00:20:54,720 --> 00:21:00,480
So in clustering, we assume that the data can naturally be divided into groups, right?

264
00:21:00,480 --> 00:21:01,480
Or clusters.

265
00:21:01,480 --> 00:21:05,920
And what makes a good cluster is that the points in the cluster are quite similar to each

266
00:21:05,920 --> 00:21:06,920
other.

267
00:21:06,920 --> 00:21:10,200
And they're quite different from other points in other clusters.

268
00:21:10,200 --> 00:21:14,680
But now that I'm given a whole bunch of data, how do I go about actually finding these

269
00:21:14,680 --> 00:21:16,280
different clusters?

270
00:21:16,280 --> 00:21:21,320
And in a Gaussian mixture model, I basically make an assumption that there was some process

271
00:21:21,320 --> 00:21:23,200
that generated the data.

272
00:21:23,200 --> 00:21:27,040
Which is, let's say that the data happens to have five clusters.

273
00:21:27,040 --> 00:21:32,320
So so someone, you know, flipped the coin or all the dice and sampled the centers of

274
00:21:32,320 --> 00:21:33,320
the clusters.

275
00:21:33,320 --> 00:21:34,320
Right.

276
00:21:34,320 --> 00:21:35,320
Right.

277
00:21:35,320 --> 00:21:40,160
And then from each center, a bunch of data points were sampled, but there are some noise

278
00:21:40,160 --> 00:21:42,360
involved, right?

279
00:21:42,360 --> 00:21:48,000
And so now if all I get to see is the end result is I get to see a lot of points and I need

280
00:21:48,000 --> 00:21:49,400
to infer.

281
00:21:49,400 --> 00:21:54,040
I need to learn the parameters, so I need to learn the means of the clusters, the centers

282
00:21:54,040 --> 00:21:59,000
of the clusters and their variances, how noisy the cluster are.

283
00:21:59,000 --> 00:22:06,600
And then I need to infer the probability that each point came from anyone particular cluster.

284
00:22:06,600 --> 00:22:10,680
So that would be a distribution over the clusters.

285
00:22:10,680 --> 00:22:18,320
You're typically, are you also inferring the number of latent variables or processes that

286
00:22:18,320 --> 00:22:23,760
are producing your data, or is that always an assumption that you make in your modeling?

287
00:22:23,760 --> 00:22:25,720
Oh, so that's a good question.

288
00:22:25,720 --> 00:22:31,520
So the traditional algorithm assumes that this is actually given as an input somehow,

289
00:22:31,520 --> 00:22:33,760
I figured out that there are five clusters.

290
00:22:33,760 --> 00:22:34,760
Right.

291
00:22:34,760 --> 00:22:39,960
But of course, that's not really a good assumption because I typically don't know.

292
00:22:39,960 --> 00:22:40,960
Right.

293
00:22:40,960 --> 00:22:41,960
Right.

294
00:22:41,960 --> 00:22:45,960
And so the next obvious step is to, you know, run it with a whole bunch of possible number

295
00:22:45,960 --> 00:22:51,840
of clusters and figure out how to measure which one is best, but that's not naturally,

296
00:22:51,840 --> 00:22:54,920
that's not necessarily easy to do.

297
00:22:54,920 --> 00:23:00,360
And then there is a very interesting literature on what's called non-parametric approaches

298
00:23:00,360 --> 00:23:05,920
or non-parametric Bayesian approaches, which basically say we don't want to make the

299
00:23:05,920 --> 00:23:09,680
assumption that we know the number of clusters ahead of time.

300
00:23:09,680 --> 00:23:14,040
And so we're going to also inject that and because it's Bayesian, they're not actually

301
00:23:14,040 --> 00:23:15,760
inferring the number of clusters.

302
00:23:15,760 --> 00:23:18,560
They are integrating over the number of clusters.

303
00:23:18,560 --> 00:23:25,080
So it gets to fairly complicated math, but there's definitely the attempt to try to accommodate

304
00:23:25,080 --> 00:23:26,080
for that.

305
00:23:26,080 --> 00:23:29,800
And going back to what I was saying before, that's an example of some really interesting

306
00:23:29,800 --> 00:23:38,960
and fairly, you know, beautiful mathematics round that area, but it's not yet practical

307
00:23:38,960 --> 00:23:41,880
to run these algorithms at scale.

308
00:23:41,880 --> 00:23:49,800
Is that when you're saying these algorithms in this case, so you're referring to non-parametric

309
00:23:49,800 --> 00:23:54,800
in particular, the Gaussian mixture models are fairly widely used.

310
00:23:54,800 --> 00:23:55,800
Is that right?

311
00:23:55,800 --> 00:24:00,320
Yeah, if you know the number of clusters, or if you're going to assume that you give

312
00:24:00,320 --> 00:24:05,600
it as an input to the algorithm, then that is an algorithm that can be applied quite

313
00:24:05,600 --> 00:24:08,240
successfully to data.

314
00:24:08,240 --> 00:24:14,840
And so going back to this thing, the EM algorithm, what you typically end up doing is

315
00:24:14,840 --> 00:24:18,440
iterating between two types of computations.

316
00:24:18,440 --> 00:24:25,040
One is given some estimate of different means and variances of each cluster.

317
00:24:25,040 --> 00:24:29,880
I will compute, I will do the inference, so I will compute what is the probability for

318
00:24:29,880 --> 00:24:33,080
each point to be associated with each one of the clusters.

319
00:24:33,080 --> 00:24:37,960
And then after I'm done computing that, I go back to re-estimating my parameters and

320
00:24:37,960 --> 00:24:45,200
refining the estimates of the parameters, and I do that until convergence.

321
00:24:45,200 --> 00:24:53,280
So does that mean you're iterating on the cluster centers and the parameters of your distributions

322
00:24:53,280 --> 00:24:59,160
for each of the clusters as you're incorporating the point into the clusters?

323
00:24:59,160 --> 00:25:01,520
So this is for training time.

324
00:25:01,520 --> 00:25:05,280
So in training time, in this case, I assume I have access to all of my data, and I'm just

325
00:25:05,280 --> 00:25:11,640
trying to figure out where are these cluster centers and their variances, and which point

326
00:25:11,640 --> 00:25:14,240
belongs where?

327
00:25:14,240 --> 00:25:18,440
So I will train by running the EM algorithm, I will end up with estimates of the cluster

328
00:25:18,440 --> 00:25:22,560
centers and their variances, and then if I'm getting a new data point, then I can do

329
00:25:22,560 --> 00:25:30,400
the prediction of what is the probability that it came from cluster 1, 2, 3, 4?

330
00:25:30,400 --> 00:25:33,320
So where does minmax propagation fit in?

331
00:25:33,320 --> 00:25:37,600
Right, so now we kind of have to step quite a bit back.

332
00:25:37,600 --> 00:25:42,400
So at a very, because we kind of dove into one particular example.

333
00:25:42,400 --> 00:25:49,080
And even with Gaussian mixture model, the inference itself is relatively straightforward,

334
00:25:49,080 --> 00:25:50,080
let's say.

335
00:25:50,080 --> 00:25:55,640
So at a very high level, this is a very, very high level novel algorithm to approximately

336
00:25:55,640 --> 00:26:02,560
solve a particular set of NP-hard problems, and the algorithm is a new variant of belief

337
00:26:02,560 --> 00:26:07,920
propagation, and the problems that we're looking at are minmax problems, and in particular,

338
00:26:07,920 --> 00:26:10,400
we've demonstrated on makespan.

339
00:26:10,400 --> 00:26:14,120
Now in order to parse through all of them, we have to talk about what each of these things

340
00:26:14,120 --> 00:26:15,120
mean, right?

341
00:26:15,120 --> 00:26:16,120
Right, right.

342
00:26:16,120 --> 00:26:21,600
So I can start talking about it, and you can interrupt me with questions, but I'll try

343
00:26:21,600 --> 00:26:23,760
to take us through all of it.

344
00:26:23,760 --> 00:26:28,160
So NP-hard problems are, you know, roughly speaking problems for which we don't know

345
00:26:28,160 --> 00:26:32,360
if there exists an efficient solution that will run in a reasonable amount of time.

346
00:26:32,360 --> 00:26:36,560
We typically know a brute force solution, so the problem with brute force is that as

347
00:26:36,560 --> 00:26:41,240
the problem scales, it takes longer and longer in a typically exponential manner, so it's

348
00:26:41,240 --> 00:26:42,840
just not practical.

349
00:26:42,840 --> 00:26:47,640
And so finding solutions that are approximations to NP-hard problems that run in a reasonable

350
00:26:47,640 --> 00:26:51,800
amount of time is a pretty big research area in theoretical computer science and other

351
00:26:51,800 --> 00:26:54,120
disciplines like operational research.

352
00:26:54,120 --> 00:26:55,960
Okay, so that's the hard.

353
00:26:55,960 --> 00:27:01,000
Now if you look about minmax problems, they appear a lot in game theory, decision theory,

354
00:27:01,000 --> 00:27:03,760
and other math and science domain.

355
00:27:03,760 --> 00:27:09,360
And the technical definition is that you have a function of two sets of variables.

356
00:27:09,360 --> 00:27:14,760
Let's call them x and y, and you want to find the min over x max over y of the function

357
00:27:14,760 --> 00:27:15,760
xy.

358
00:27:15,760 --> 00:27:21,400
But that doesn't really give a lot of intuition, so let's try and look at an example.

359
00:27:21,400 --> 00:27:25,560
So in the paper, we'll look at a problem called makespan, which should be fairly easy

360
00:27:25,560 --> 00:27:26,560
to grasp.

361
00:27:26,560 --> 00:27:31,640
So let's say you have a bunch of incoming jobs, and I'm talking about jobs in the computer

362
00:27:31,640 --> 00:27:38,040
science sense, so some automated tasks that need to be executed on a computer, and each

363
00:27:38,040 --> 00:27:40,920
will require some amount of resources.

364
00:27:40,920 --> 00:27:45,920
So let's think about CPU or memory to run.

365
00:27:45,920 --> 00:27:51,160
And we have a bunch of machines, sort of like a computing cluster to run it.

366
00:27:51,160 --> 00:27:55,040
And maybe each machine has slightly different capabilities, so each will take slightly

367
00:27:55,040 --> 00:27:59,040
different time to run each one of these jobs, okay?

368
00:27:59,040 --> 00:28:04,480
So we want to run all of them, so we obviously want to divide up the jobs in a way that everything

369
00:28:04,480 --> 00:28:06,760
finishes as soon as possible.

370
00:28:06,760 --> 00:28:11,720
And if we think about it, that time will be determined by the bottleneck, the machine

371
00:28:11,720 --> 00:28:17,320
that will take the longest to process all the jobs that were assigned to that machine.

372
00:28:17,320 --> 00:28:22,640
So what we want to do is we want to minimize the maximum time it could possibly take.

373
00:28:22,640 --> 00:28:26,320
We want to find a way to split up the jobs across the machine so that we minimize the

374
00:28:26,320 --> 00:28:30,000
maximum time it would possibly take to execute the jobs.

375
00:28:30,000 --> 00:28:33,920
And you call this problem, what makes span?

376
00:28:33,920 --> 00:28:34,920
Makes span.

377
00:28:34,920 --> 00:28:38,280
Makes span, okay, it sounds like a job stop problem.

378
00:28:38,280 --> 00:28:41,480
Yeah, yeah, it's related to that, yeah.

379
00:28:41,480 --> 00:28:44,840
And notice that there is a simple brute force solution, which is, you know, try out all

380
00:28:44,840 --> 00:28:47,200
the different ways you can divide up the load, right?

381
00:28:47,200 --> 00:28:51,080
But a problem of course is that this will be exponential into a number of jobs.

382
00:28:51,080 --> 00:28:53,520
And so it becomes very intractable very quickly.

383
00:28:53,520 --> 00:28:54,520
Okay.

384
00:28:54,520 --> 00:28:59,720
So, okay, so now we understand at least one example of a minmax problem.

385
00:28:59,720 --> 00:29:05,840
And so the algorithm we presented is again in the context of it's to solve the minmax

386
00:29:05,840 --> 00:29:11,640
problem and the paper demonstrated on the makes span problem, but it's a little bit more,

387
00:29:11,640 --> 00:29:12,640
it's more relevant than that.

388
00:29:12,640 --> 00:29:18,680
It's relevant for a set of minmax problems and it goes into the technical properties of

389
00:29:18,680 --> 00:29:21,960
which kind of minmax problems can be addressed.

390
00:29:21,960 --> 00:29:23,960
So I think we can skip that.

391
00:29:23,960 --> 00:29:25,560
So now we can talk about the algorithm.

392
00:29:25,560 --> 00:29:31,280
What can you give us a high level characterization of those types?

393
00:29:31,280 --> 00:29:32,280
Minmax problems.

394
00:29:32,280 --> 00:29:37,360
The types of minmax problems just to get a sense for, you know, even what are the, you

395
00:29:37,360 --> 00:29:41,360
know, what are the dimensions, you know, around what you're thinking about characterizing

396
00:29:41,360 --> 00:29:42,880
these problems?

397
00:29:42,880 --> 00:29:47,760
So they are often notion of, I want to minimize my worst case scenario.

398
00:29:47,760 --> 00:29:48,760
Right.

399
00:29:48,760 --> 00:29:55,680
So, another, another example of minmax problems is when you have some sort of a two-player

400
00:29:55,680 --> 00:30:01,760
game and you're trying to come up with a strategy so that whatever the other player can do,

401
00:30:01,760 --> 00:30:04,760
which is some sort of, you know, I will incur some loss.

402
00:30:04,760 --> 00:30:05,760
Right.

403
00:30:05,760 --> 00:30:08,320
So the maximum loss, I want to minimize that.

404
00:30:08,320 --> 00:30:09,320
Okay.

405
00:30:09,320 --> 00:30:12,560
So that's kind of the most abstract formulation for that problem.

406
00:30:12,560 --> 00:30:18,280
So then it sounds like what you've done in the paper in terms of characterizing the types

407
00:30:18,280 --> 00:30:25,520
of minmax problems to which this particular method applies, it's not necessarily an intuitive

408
00:30:25,520 --> 00:30:26,520
characterization.

409
00:30:26,520 --> 00:30:30,600
It's more mathematical detail, I guess.

410
00:30:30,600 --> 00:30:31,600
Yeah.

411
00:30:31,600 --> 00:30:32,600
Yeah, exactly.

412
00:30:32,600 --> 00:30:33,600
It's a settle.

413
00:30:33,600 --> 00:30:34,600
Okay.

414
00:30:34,600 --> 00:30:35,600
Yeah.

415
00:30:35,600 --> 00:30:36,600
There's restrictions, let's say.

416
00:30:36,600 --> 00:30:37,600
Okay.

417
00:30:37,600 --> 00:30:38,600
And you mentioned belief propagation.

418
00:30:38,600 --> 00:30:39,600
Right.

419
00:30:39,600 --> 00:30:40,600
Let's talk a little bit about that.

420
00:30:40,600 --> 00:30:43,560
So now we can talk about the algorithm we actually propose.

421
00:30:43,560 --> 00:30:45,760
So it is a variant of belief propagation.

422
00:30:45,760 --> 00:30:46,760
Okay.

423
00:30:46,760 --> 00:30:51,960
And so in order to understand belief propagation, we need to know a little bit about inference

424
00:30:51,960 --> 00:30:56,200
in graphical models and message passing algorithms.

425
00:30:56,200 --> 00:31:00,960
And we talked already a little bit about inference in graphical models.

426
00:31:00,960 --> 00:31:06,760
In the context of describing, going back again to the notion of, you know, we have data

427
00:31:06,760 --> 00:31:13,400
or observation and we assume that there is some noise in these observations in these measurements

428
00:31:13,400 --> 00:31:17,240
and then there is some uncertainty and we assume that there are some quantities that

429
00:31:17,240 --> 00:31:24,600
we don't have direct way to measure, but are depend, they have dependencies with relationships

430
00:31:24,600 --> 00:31:26,960
with the quantities we're able to measure.

431
00:31:26,960 --> 00:31:32,400
We often talk about these in terms of observed and hidden variables and they are in graphical

432
00:31:32,400 --> 00:31:37,760
models, we try to capture the way in which they are related using a graph.

433
00:31:37,760 --> 00:31:41,960
So basically every variable, whether it's something that we've observed or something

434
00:31:41,960 --> 00:31:45,640
that we can't observe is represented as a node in the graph.

435
00:31:45,640 --> 00:31:51,880
And then the interactions between these variables are represented as edges in the graph.

436
00:31:51,880 --> 00:31:58,800
And once we describe the problem in that manner, we can start borrowing from, you know, graph

437
00:31:58,800 --> 00:32:05,880
theory and graph algorithms to answer different questions about the graph that we have described,

438
00:32:05,880 --> 00:32:12,080
which is basically a representation of the probability distribution over all of these variables.

439
00:32:12,080 --> 00:32:15,560
So belief propagation is a general algorithm.

440
00:32:15,560 --> 00:32:19,040
It has several existing variants that are quite well known.

441
00:32:19,040 --> 00:32:23,960
One of them is the max product belief propagation and another one is some product belief propagation.

442
00:32:23,960 --> 00:32:29,760
And they are algorithms or recipes for computing these quantities of interest.

443
00:32:29,760 --> 00:32:36,400
So let's say that we have a bunch of these hidden variables and one question we can ask

444
00:32:36,400 --> 00:32:40,120
is, you know, let's say that they are binary.

445
00:32:40,120 --> 00:32:43,120
So they can either take the value zero or one.

446
00:32:43,120 --> 00:32:49,360
We can ask what is the particular setting of all of these variables that yields the highest

447
00:32:49,360 --> 00:32:54,640
probability, this is kind of like the mode of the distribution.

448
00:32:54,640 --> 00:32:59,080
And in order to compute that, we can use something like the max product algorithm.

449
00:32:59,080 --> 00:33:04,000
And at an intuitive level, the way these algorithms work is, again, going back to this

450
00:33:04,000 --> 00:33:07,640
graph that we have in mind, they send messages between the nodes.

451
00:33:07,640 --> 00:33:13,440
So the nodes exchange numerical quantities sometimes in an iterative fashion.

452
00:33:13,440 --> 00:33:17,520
And then eventually they reach some kind of a decision.

453
00:33:17,520 --> 00:33:24,680
So the nodes are variables observed in hidden variables.

454
00:33:24,680 --> 00:33:31,640
And can you give an example of a scenario that includes both these observed and hidden

455
00:33:31,640 --> 00:33:32,640
variables?

456
00:33:32,640 --> 00:33:37,480
Are these hidden variables in a sense of latent variables and a different sense?

457
00:33:37,480 --> 00:33:39,680
Yeah, latent variables.

458
00:33:39,680 --> 00:33:41,680
Okay.

459
00:33:41,680 --> 00:33:47,120
And the observed variables are, what's an example where you'd have both observed and

460
00:33:47,120 --> 00:33:50,040
hidden variables.

461
00:33:50,040 --> 00:33:56,600
So I can actually go back to my graphic, a Gaussian mixture model and I can represent that

462
00:33:56,600 --> 00:34:03,280
as a graphical model where my observed variables is the data that I'm trying to cluster.

463
00:34:03,280 --> 00:34:12,080
And the hidden variables are the clusters that I, that are represented by, by, you know,

464
00:34:12,080 --> 00:34:13,360
these parameters.

465
00:34:13,360 --> 00:34:20,480
And, and maybe I can talk in order to kind of segue into, into this particular, the,

466
00:34:20,480 --> 00:34:27,360
the minmax inference, I'd like to talk about another clustering algorithm, which I think

467
00:34:27,360 --> 00:34:34,280
would really focus us on the, the inference side of the, the hidden variables.

468
00:34:34,280 --> 00:34:35,280
Okay.

469
00:34:35,280 --> 00:34:37,160
So, so that, that's a different clustering algorithm.

470
00:34:37,160 --> 00:34:42,640
And this was actually the focus of my thesis work and it's called affinity propagation.

471
00:34:42,640 --> 00:34:43,640
Okay.

472
00:34:43,640 --> 00:34:50,200
And this is an algorithm that was first presented by my supervisor, Brendan Frye, and his

473
00:34:50,200 --> 00:34:52,120
graduate student, Delbert Duke.

474
00:34:52,120 --> 00:34:57,160
And I focused on various extensions of the algorithm and reformulating of them mathematics.

475
00:34:57,160 --> 00:35:02,440
So anyways, the, the way we represent clustering in that algorithm is we basically say, okay,

476
00:35:02,440 --> 00:35:06,640
we have a bunch of, a bunch of data points who want to split it into groups that make sense.

477
00:35:06,640 --> 00:35:14,200
And instead of talking about means and variances, we're going to say that each cluster is basically

478
00:35:14,200 --> 00:35:17,000
best represented by an exemplar.

479
00:35:17,000 --> 00:35:21,320
So this is like the representative of the cluster.

480
00:35:21,320 --> 00:35:28,040
And so really the problem becomes, can we find the right set of representatives?

481
00:35:28,040 --> 00:35:33,480
And then every point that is not a representative just needs to be associated with the representative

482
00:35:33,480 --> 00:35:37,000
that is closest to it, that is most similar to it.

483
00:35:37,000 --> 00:35:38,000
Okay.

484
00:35:38,000 --> 00:35:39,000
Okay.

485
00:35:39,000 --> 00:35:43,560
So the problem, of course, is you don't know which subset of points is best to represent

486
00:35:43,560 --> 00:35:45,200
these clusters, right?

487
00:35:45,200 --> 00:35:53,120
So in, when you do this max product, belief propagation in the context of this graph, your

488
00:35:53,120 --> 00:35:57,040
variables are basically the data points.

489
00:35:57,040 --> 00:36:03,800
The hidden variables are for each point which cluster should it be assigned to?

490
00:36:03,800 --> 00:36:04,800
Okay.

491
00:36:04,800 --> 00:36:05,800
Okay.

492
00:36:05,800 --> 00:36:06,800
So that's something you don't know.

493
00:36:06,800 --> 00:36:08,920
And for each point, which, right.

494
00:36:08,920 --> 00:36:15,320
And so that specifically, as opposed to the parameters of the cluster itself, right?

495
00:36:15,320 --> 00:36:16,320
Okay.

496
00:36:16,320 --> 00:36:19,640
So we're not using parameters anymore for this algorithm.

497
00:36:19,640 --> 00:36:22,280
So you can see that here, here we'll be doing inference.

498
00:36:22,280 --> 00:36:28,600
We're going to come up with estimates of, for each point, what is the cluster it's going

499
00:36:28,600 --> 00:36:33,360
to be assigned to without talking about a notion of means and variances.

500
00:36:33,360 --> 00:36:34,440
So there is no learning here.

501
00:36:34,440 --> 00:36:35,440
It's only inference.

502
00:36:35,440 --> 00:36:36,440
Right.

503
00:36:36,440 --> 00:36:37,440
Okay.

504
00:36:37,440 --> 00:36:41,280
And so the way the algorithm ends up working is basically, again, sending messages.

505
00:36:41,280 --> 00:36:48,400
So the nodes send messages to each other and the messages come out of a mathematical

506
00:36:48,400 --> 00:36:49,400
derivation.

507
00:36:49,400 --> 00:36:53,760
You look at them, you can kind of give them, give an intuitive explanation of what they're

508
00:36:53,760 --> 00:36:59,720
really trying to say, which is they, it's an iterative process where first, all the nodes

509
00:36:59,720 --> 00:37:03,760
send a message to all of their neighbors, to each one of their neighbors saying, to what

510
00:37:03,760 --> 00:37:07,920
extent do I want you to be my representative, right?

511
00:37:07,920 --> 00:37:12,240
And after collecting all the information from my neighbors, so all my neighbors have told

512
00:37:12,240 --> 00:37:14,800
me to what extent they want me to be the representative.

513
00:37:14,800 --> 00:37:21,120
Now I send all my neighbors a message back saying to what extent do I want to be a representative.

514
00:37:21,120 --> 00:37:22,120
Right.

515
00:37:22,120 --> 00:37:27,520
And so you iterate back and forth on these messages until you converge.

516
00:37:27,520 --> 00:37:33,640
So something back and then in belief propagation, we end up having these messages that are exchanged

517
00:37:33,640 --> 00:37:39,400
between the nodes in the graphical model for the purpose of doing inference for the purpose

518
00:37:39,400 --> 00:37:43,200
of computing these quantities that we care about.

519
00:37:43,200 --> 00:37:47,960
So I think now we might be ready to talk about the mean max propagation.

520
00:37:47,960 --> 00:37:48,960
Yeah.

521
00:37:48,960 --> 00:37:51,720
So just to make sure I'm on the same page.

522
00:37:51,720 --> 00:37:56,920
So we're talking about messages and message passing and things like that.

523
00:37:56,920 --> 00:38:05,120
This is basically a computational tool or an accounting tool that we're using to just

524
00:38:05,120 --> 00:38:12,120
keep track of quantities in this algorithm, really, as we're trying to do the inference.

525
00:38:12,120 --> 00:38:17,840
And the side of my brain that thinks about distributed computing is thinking about real

526
00:38:17,840 --> 00:38:22,480
things, passing messages, and it's like, that's not really what we're doing here.

527
00:38:22,480 --> 00:38:23,480
Right.

528
00:38:23,480 --> 00:38:25,760
These are not text messages or JSON messages.

529
00:38:25,760 --> 00:38:31,560
These are numerical quantities that are computed and then used.

530
00:38:31,560 --> 00:38:36,880
But it's convenient to think about them as, you know, there is a numerical quantity computed

531
00:38:36,880 --> 00:38:39,320
for each one of these nodes.

532
00:38:39,320 --> 00:38:44,000
And then there is another one computed which relies on the previous computation.

533
00:38:44,000 --> 00:38:49,040
So it's almost as if the node sent that numerical representation may be a vector of numbers

534
00:38:49,040 --> 00:38:52,040
to all of its neighbors and said, you know, this is my message.

535
00:38:52,040 --> 00:38:56,480
So now you're ready to do your computation in the next iteration of the algorithm.

536
00:38:56,480 --> 00:38:57,480
Okay.

537
00:38:57,480 --> 00:38:58,480
Great.

538
00:38:58,480 --> 00:39:01,240
So now I think we have all the building blocks.

539
00:39:01,240 --> 00:39:06,400
So we have, you know, these graphical models and we know a little bit about message passing

540
00:39:06,400 --> 00:39:13,760
algorithms and the question we ask in this paper is, can we take this belief propagation

541
00:39:13,760 --> 00:39:20,040
type algorithms and we understand that they can work for the four types of questions

542
00:39:20,040 --> 00:39:24,800
that were interested in the context of, you know, finding the assignment of the variable

543
00:39:24,800 --> 00:39:30,040
that gives us the maximum probability or computing marginals and other tasks.

544
00:39:30,040 --> 00:39:36,000
Can we somehow formulate it to solve the mean max problem?

545
00:39:36,000 --> 00:39:41,840
And so it turns out that we can, it's not by running the same algorithm.

546
00:39:41,840 --> 00:39:49,480
It's by borrowing the general idea and kind of the sum of the mathematics around it and

547
00:39:49,480 --> 00:39:54,480
writing down the mean max problem as a graphical model.

548
00:39:54,480 --> 00:40:02,280
And then deriving the form of the messages that need to be sent in order to compute what

549
00:40:02,280 --> 00:40:04,880
should be the solution to the mean max problem.

550
00:40:04,880 --> 00:40:12,840
So they are similar in principle to the messages you send when you're doing max product or

551
00:40:12,840 --> 00:40:13,840
some product.

552
00:40:13,840 --> 00:40:19,000
But of course, it's its own flavor that required figuring out the right derivation.

553
00:40:19,000 --> 00:40:25,120
And another interesting thing is what we do when we have interaction that are between

554
00:40:25,120 --> 00:40:27,040
quite a few variables, right?

555
00:40:27,040 --> 00:40:34,120
So if we have in our graph an interaction between a subset of variables that is more than

556
00:40:34,120 --> 00:40:42,480
just two, some of the computations can on the surface seem like they are exponential in

557
00:40:42,480 --> 00:40:44,480
the number of the variables.

558
00:40:44,480 --> 00:40:51,440
So in order to be able to actually apply this framework to mean max, we had to recognize

559
00:40:51,440 --> 00:40:57,400
that what looks like it's going to be an exponentially expensive computation can be actually done

560
00:40:57,400 --> 00:41:03,320
by some, you know, a little bit of cleverness and bookkeeping in non-exponential time.

561
00:41:03,320 --> 00:41:06,600
So in something that is reasonable to compute.

562
00:41:06,600 --> 00:41:07,920
Okay.

563
00:41:07,920 --> 00:41:16,840
Just kind of summarizing basically what you did with mean max propagation was you borrowed

564
00:41:16,840 --> 00:41:26,520
from this message passing approach that comes out of belief propagation and applied it to

565
00:41:26,520 --> 00:41:34,360
this makespan problem and in addition, you've kind of mathematically characterized the

566
00:41:34,360 --> 00:41:40,760
more broader set of min max problems to which this model would also apply.

567
00:41:40,760 --> 00:41:41,760
Yes.

568
00:41:41,760 --> 00:41:43,680
And importantly, derive the form.

569
00:41:43,680 --> 00:41:49,480
The messages need to take if you're doing a min max computation as opposed to a sound

570
00:41:49,480 --> 00:41:53,200
product computation or a max product computation.

571
00:41:53,200 --> 00:41:55,040
And what is that form?

572
00:41:55,040 --> 00:41:59,680
So that's in the technical details, so it's basically an equation that describes, you

573
00:41:59,680 --> 00:42:05,600
know, some form of a, you know, operations as minimums and maximums that can be computed

574
00:42:05,600 --> 00:42:12,840
in, let's call it linear time for the purposes of this stuff, but it's an equation, right?

575
00:42:12,840 --> 00:42:16,920
It's an equation that when you go about implementing the algorithm, you will just write in

576
00:42:16,920 --> 00:42:17,920
some code.

577
00:42:17,920 --> 00:42:18,920
Okay.

578
00:42:18,920 --> 00:42:26,240
So given the computation requirements of this and the space of potential application,

579
00:42:26,240 --> 00:42:31,040
where do you think this paper and algorithm will have the broadest impact?

580
00:42:31,040 --> 00:42:38,400
So I think it will be interesting to see what kind of things people use it for because

581
00:42:38,400 --> 00:42:43,240
it's relatively, it's a new formulation.

582
00:42:43,240 --> 00:42:52,320
We wanted to look at the case of, you know, max, max span as a particular application,

583
00:42:52,320 --> 00:42:56,480
which is actually, it is useful for things like scheduling tasks.

584
00:42:56,480 --> 00:43:03,960
It's useful for workload in terms of power consumption on turbines and power plants.

585
00:43:03,960 --> 00:43:11,880
So in the operational research world, I believe, you know, they look at these types of applications.

586
00:43:11,880 --> 00:43:19,040
But from, I think one of the interesting things you get to do as a researcher is sometimes

587
00:43:19,040 --> 00:43:27,040
focus more on, you know, the core algorithm and the mathematics of it and, you know, post

588
00:43:27,040 --> 00:43:31,120
it out there for the world to see so that various people who are looking at, you know,

589
00:43:31,120 --> 00:43:36,360
different problem domains can recognize that this actually matches to their problem of

590
00:43:36,360 --> 00:43:39,360
interest and then take it into interesting places.

591
00:43:39,360 --> 00:43:40,360
Right.

592
00:43:40,360 --> 00:43:41,360
Right.

593
00:43:41,360 --> 00:43:46,440
Maybe worth mentioning that this is all work that you did in the U of T context as opposed

594
00:43:46,440 --> 00:43:48,880
to the Uber context, is that right?

595
00:43:48,880 --> 00:43:49,880
Right.

596
00:43:49,880 --> 00:43:50,880
Right.

597
00:43:50,880 --> 00:43:56,320
And it's also worth mentioning my co-author, Christopher, Siamuk and Brendan.

598
00:43:56,320 --> 00:43:57,320
Okay.

599
00:43:57,320 --> 00:43:58,320
Awesome.

600
00:43:58,320 --> 00:43:59,320
Very cool.

601
00:43:59,320 --> 00:44:05,560
You know, it strikes me that this is a little bit of an aside, but I'm not sure that you

602
00:44:05,560 --> 00:44:11,920
know, but we do a monthly meetup where we kind of dig into research papers and we've

603
00:44:11,920 --> 00:44:17,160
done a bunch of kind of deep learning focus ones, but it strikes me that this would be an

604
00:44:17,160 --> 00:44:24,320
interesting one to maybe have you or one of your co-authors present to the group to really

605
00:44:24,320 --> 00:44:27,400
dig into some of the details here.

606
00:44:27,400 --> 00:44:30,480
A lot of it, it seems like a lot of it is in the details.

607
00:44:30,480 --> 00:44:36,720
Yeah, and I would be happy to talk about that and to look into this opportunity.

608
00:44:36,720 --> 00:44:42,040
I think one of the, again, as a side note, one of the exciting things about deep learning

609
00:44:42,040 --> 00:44:49,760
is that they're so successful in terms of, you know, how applicable they are to problems

610
00:44:49,760 --> 00:44:51,120
that we care about.

611
00:44:51,120 --> 00:44:59,000
The one thing that is that they don't have as much as other algorithms is kind of, you

612
00:44:59,000 --> 00:45:07,480
know, deep mathematics and kind of challenging representations that you really need a lot

613
00:45:07,480 --> 00:45:14,080
of time to wrap your head around and so it's really nice to dig into these papers and

614
00:45:14,080 --> 00:45:20,280
there's typically quite a bit of math in them as well, so I think it'll probably be interesting

615
00:45:20,280 --> 00:45:26,160
and we'll challenge people a little bit in a way that's interesting to look into these

616
00:45:26,160 --> 00:45:27,160
types of papers.

617
00:45:27,160 --> 00:45:32,400
Awesome, where do you go from here with this particular work and your research?

618
00:45:32,400 --> 00:45:38,720
Is this something that you're kind of finishing up that relates back to your time at Toronto

619
00:45:38,720 --> 00:45:41,080
or is this ongoing work that you're involved in?

620
00:45:41,080 --> 00:45:46,120
I would say this is more of a, you know, what I call recreational research.

621
00:45:46,120 --> 00:45:56,040
It's something that I worked with on with Chris in the past was a graduate student as

622
00:45:56,040 --> 00:46:01,040
well in the lab and he kind of took it off from there.

623
00:46:01,040 --> 00:46:06,760
And from me right now, the main focus is definitely ear at work and working on self-driving

624
00:46:06,760 --> 00:46:07,760
cars.

625
00:46:07,760 --> 00:46:08,760
Awesome.

626
00:46:08,760 --> 00:46:09,760
Awesome.

627
00:46:09,760 --> 00:46:14,880
Well, I really appreciate you taking the time to walk through this with me.

628
00:46:14,880 --> 00:46:19,880
I know I've had a lot of questions and I still have a lot of questions there, so this

629
00:46:19,880 --> 00:46:24,920
is not a topic that we've gone into in a lot of detail here on the podcast, but I think

630
00:46:24,920 --> 00:46:26,960
I did.

631
00:46:26,960 --> 00:46:33,520
I had several conversations on graphical models at NipSive, forget how many of those came

632
00:46:33,520 --> 00:46:40,320
out in our Nip series off the top of my head, but we've got a few more to release over

633
00:46:40,320 --> 00:46:43,920
the next few weeks or months.

634
00:46:43,920 --> 00:46:48,960
You know, it seems to be, I don't know, I was surprised to, I guess I was going back

635
00:46:48,960 --> 00:46:56,440
to our earlier conversation, I was surprised by this kind of undercurrent of work happening

636
00:46:56,440 --> 00:47:02,260
in graphical models at NipSive and how often I heard it, although I don't know that that

637
00:47:02,260 --> 00:47:07,560
means anything is the first, it was the first time I was at NipSive, so my baseline is

638
00:47:07,560 --> 00:47:15,000
not exactly, you know, it was not exactly very, you know, dialed in, but they're definitely

639
00:47:15,000 --> 00:47:16,000
seats.

640
00:47:16,000 --> 00:47:18,000
There's a lot of people interested in this area.

641
00:47:18,000 --> 00:47:19,000
Yeah.

642
00:47:19,000 --> 00:47:20,000
Yeah, absolutely.

643
00:47:20,000 --> 00:47:26,360
I mean, again, neural networks have kind of taken from stage for the last few years,

644
00:47:26,360 --> 00:47:34,960
but there's always ongoing research on graphical models and on an array of other problems in machine

645
00:47:34,960 --> 00:47:40,160
learning, and so I think it's kind of nice to see things like the focus going back to

646
00:47:40,160 --> 00:47:44,040
some of these models and seeing what can we do with them now.

647
00:47:44,040 --> 00:47:45,040
Absolutely.

648
00:47:45,040 --> 00:47:49,360
Well, on that note, Emar, thank you so much for joining us.

649
00:47:49,360 --> 00:47:50,360
Absolutely.

650
00:47:50,360 --> 00:47:55,960
Thank you for having me, and I hope it makes for an interesting listening, or at least if

651
00:47:55,960 --> 00:48:02,160
not everything was super clear as an introduction to go and explore some more.

652
00:48:02,160 --> 00:48:03,160
Absolutely.

653
00:48:03,160 --> 00:48:04,160
Absolutely.

654
00:48:04,160 --> 00:48:05,160
Thank you.

655
00:48:05,160 --> 00:48:07,160
Thank you, bye-bye.

656
00:48:07,160 --> 00:48:13,160
All right, everyone, that's our show for today.

657
00:48:13,160 --> 00:48:18,400
Thanks so much for listening, and for your continued feedback and support.

658
00:48:18,400 --> 00:48:23,600
For more information on Emar, or any of the topics covered in this episode, head on over

659
00:48:23,600 --> 00:48:27,760
to twimlai.com slash talk slash 101.

660
00:48:27,760 --> 00:48:33,000
Of course, we'd be delighted to hear from you, either via comment on the show notes page,

661
00:48:33,000 --> 00:48:39,680
or via Twitter, directly to me at At Sam Charrington, or to the show at At Twimlai.

662
00:48:39,680 --> 00:48:42,840
Thanks once again for listening, and catch you next time.

