Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. A big thanks to everyone who participated in, voted
in and supported our My AI contest. I'm excited to announce that our first place winner is,
and I get a drum roll, Mohit Nalavati. Mohit sent in the video personal health care applications,
which you can find along with the other entries at Twimbleai.com slash My AI. Second prize
goes to Julian Ford, for his entry, dad and Eleanor. And third prize goes to Matt Taylor
for his poetic and ominously named entry, let Earth not become our tomb. Thank you all
for entering and stay tuned for our next contest. A special shout out to Lighthouse and Anki
for donating the Lighthouse and Cosmo prizes. Check them out at lighthouse.ai and Anki.com.
In this episode, I'm joined by Raja Chitilla, director of intelligence systems and robotics
at Pierre and Marie Curie University in Paris, an executive committee chair of the IEEE Global
Initiative on Ethics of Intelligent and Autonomous Systems. Raja and I had a great chat about
his research, which deals with robotic perception and discovery. We discussed the relationship
between learning and discovery, particularly as it applies to robots in their environments,
and the connection between robotic perception and action. We also dig into the concepts
of affordances, abstract teachings, meta-reasoning, and self-awareness as they apply to intelligent
systems. Finally, we touch on the issue of values and ethics of these systems. Before
we jump into the interview, this is your final reminder that our next Twimmel Online
Meetup is tomorrow, Tuesday March 13th at 5pm U.S. Pacific time. This time, our focus
will be on the Google Deep Mind Paper, playing Atari with deep reinforcement learning,
in a presentation by Sean Devlin. Head on over to twimmelai.com slash Meetup to learn
more or register. Okay, let's get to it.
Alright everyone, I am on the line with Raja Chitilla. Raja is the executive committee chair
of the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems, as well
as an IEEE Fellow. He's also a professor at Sorbonne University, Pierre and Marie Curie,
and Director of the Institute of Intelligent Systems and Robotics there. Raja, welcome
to this weekend machine learning and AI.
Hello everyone, thanks for inviting me. Absolutely. Why don't we get started by having
you tell us a little bit about your background and how you got involved in Intelligent Systems,
Robotics and Ethics? Well, let's see, I'll make a long story short because I've been
working in robotics and artificial intelligence since a number of years now. Actually, I defended
my PhD on Mobile Robot Navigation in 1981. So that's quite a long ago. And I've been
working therefore in robotics and intelligence systems, intelligent robotics since that
time. Working on several aspects of those problems, including planning, action planning,
motion planning, perception, decision-making, learning and human robot interaction. So,
it's a field that I've been familiar with since a long time and which is evolving very,
very fast, especially in the past few years because the technologies that were developed
over the years have come to some maturation. And we understand much better now, a lot
of processes and problems to be able to integrate them and get them to real world solutions.
And there have been also some progress, of course, which is enabling those technologies
to become much more efficient, which is the increasing computer speed, the availability
of memory, of data, enormous amounts of data. So that fostered the whole field recently.
Great. And what's your current focus? Well, currently I'm focusing on learning processes,
how to get a robot to interact with its environment. This is a basic issue, but the point is that
we want the robot to learn through his or its perception, its physical interaction to
connect those two together without giving it much information a priority, so that it discovers
actually what is its environment made of. We don't want to pre-program things. We don't
want to supervise its learning process. We want this to be unsupervised and we want
the robot to connect its perception and action, because it's the only way, we think, to
actually grasp an understanding of the environment. If you take a perception in general, it's
considered and developed as an observation process. You have images and they process the
images, you make some classification and you come up with identifying objects in the
scene. And this can be based, of course, on learning processes, such as deep learning.
But I think that this, of course, is very efficient and has proven efficiency lately, but
by doing this, the robot doesn't really understand what it's looking at. To understand, it means
that the robot has to develop a knowledge about what it can do, what it's action scope,
its action possibility on the object in this environment. And therefore, it has to connect
its perception with its action capacities. And this cannot be done separately. It has
to be done simultaneously. So this learning process, which is based, of course, on processing
data from sensors, but it's also based on reinforcement learning of interaction with
this environment, would enable the system, the robot, to really connect its action capacities
with its perceptual capacities. And this is what I call really understanding the environment.
Interesting. And now is the robot learning all of this from the ground up, meaning you're
firing up a system that has no a priori knowledge of its capabilities and it's exercising
stepper motors and figuring out what degrees of freedom it has or what's the starting
point for this understanding in your model? Well, that's the main difficulty, excellent
question, I would say. What I want to do is to achieve this process with a minimal initial
information. Now, what is minimal, that's the point. It's not very clear. But if we define
very basic and very specific minimal knowledge that the robot has when it's manufactured, I
would say, what is the initial programming that it has? And then starting from this, develop
the whole process, but reducing more and more these basic knowledge. So for example,
if we start from really zero knowledge, just as you said, exercising the stepping motors,
this will lead to a very big complexity because there are many degrees of freedom. And there
is quite a wide complexity also in the perception of the system. So we have to define a minimal
set of processing capacities. For example, the system is able to determine verticals,
horizontals, diagonals, it's able to identify some intersections, it's able to define some
regions which have a common perceptual background. So these are minimal processing capacities
that the systems come with, come with. And in terms of actions, it is able to move
its arm, for example. So we are not going to start with the basic stepping motor or
it's basic motor, but more a little bit more complex capacities that enable, for example,
to move all arm to touch something. So starting from that, the development of the process
can be made to understand how we connect perception and action together. Then we can get back
to reduce maybe those initial assumptions, which are already small, but what we want
to prove is the process itself. Do you want to prove the process or improve? Approve
that it can work, that we can actually arrive to do this understanding. The system has
to discover what is called affordances of the objects, of the elements of the environment
that it's interacting with. And then we want to manipulate those affordances, which
are those perceptual action representations, so that the system is able to extract more
and more information about what it can do with them through this interaction.
And in order to prove or even measure the robot's understanding of its environment, you
need some kind of representation for how, I guess, for these affordances, how does the
robot represent affordances and how do you measure, I guess, the scope of the affordances
that it has determined with an environment relative to the total number of affordances
or space of affordances? Okay, yeah, very good question. So the point is, some affordances
are discovered initially, because there are some initial simple interactions like pushing
or holding or tapping. And this provides a first base, a representation and affordance.
It's not, of course, the complete representation. So then the process, which is based on reinforcement
learning, tends to build more complex affordances. So this is guided, if I may say, through values,
meaning that the system has a basic set of motivations that translates into rewards,
that it has when it discovers some representation that afford for those motivations and reinforce
therefore, this kind of representation. But by repeating this process with those basic
motivations, the idea is to build upon the basic affordances more complex ones. So let
me give an example. When the robot pushes a given object, well, suppose this pushing will
be actually pushing on a switch and then there is light that as a result. And one of the
motivations of the robot is to have a light, to have a clearer environment. So then this
object that it has pushed on, which is the switch, is identified as a forwarding light.
But the initial representation was just a kind of box or a very simple object, which had
other affordances. For example, the robot could be able to hold it or push it. But now
it's associated with something new, which is light. And so it has a more abstract property,
which is not just its geometrical shape or the fact that it was pushable and so on.
Now it's associated with light. And I can then use it for building up on more complex
motivations, which for example are for fetching other objects I need first to put on the
light. And therefore, this object has become a, I don't really dare to pronounce the word
symbolic, but that's what it is. It's not just described in its perceptual features.
It's more described in some more abstract properties. It's related to this property of light
and the idea is that then we can build upon that again and again.
Yeah, I'm thinking a little bit of Josh Tenenbaum's work at MIT and trying to relate some
of what you're doing to what I've heard of that work. And it's interesting in that I
think what the way I think of what he's trying to do is like come up with this notion of
common sense for like how things work. And what you're doing is starting from almost
no common sense in trying to have the robot figure everything out on the fly. I don't
know if you're familiar with the stuff that he's doing, but I'm curious if you can maybe
help me contextualize how they might fit together.
Yes, of course, I know about Josh Tenenbaum's work and work of many others in this area,
which is not recent, of course. And there have been a lot of efforts and research to try
to provide common sense to robots, to computer systems and so on. And so it's absolutely related.
And I think the approach to building common sense should really be grounded in the physical
interaction with the world. So this is how we build common sense by trying to discover
the effects of our actions, our, I mean, I'm speaking about the robot, of course, the
effects of the actions of the robots action on the work. So the without this knowledge
common sense would be just a very abstract representation. It wouldn't be really something
that the robot is able to sense in its own actions. So that's, I mean, it's, of course,
in this general paradigm of developing common sense. And the approach is developmental again
similarly to what children do. They have some basic capacities, basic knowledge, basic
sensing, basic action capacities. But when they start to experiment with the environment,
this is where they discover that there is something called gravity. Of course, they don't
call it like that. But they discover that when they hold something and they just let it
go, it will drop to the ground. So it means there is a specific effect from their action
that is observed. And the connection between this perception and action is the component,
the major component of what is going to become common sense, which is the description of
the laws of physics of what actually is the effect of actions on the environment. Then we
will need something else, which is generalization, because what I'm speaking about is just the
interaction of the system with its environment. But what what happens when you observe, observe
other agents, for example, doing things and how do you develop from that, also representations
that you can embed with your own actions. That's another step, which is, I think, also
very important. But we are not doing this yet.
As opposed to having the robot learn from base principles, everything about the world,
shouldn't it have the benefit of the knowledge that we have? And for example, are there
ways to or should we move towards ways of representing the laws of physics, for example,
and being able to provide that as a, you know, an input model or something to the robot
that it can kind of discover and use affordances with that knowledge as opposed to needing to
figure that all out from scratch?
If I can answer bluntly, I would say the more knowledge you input to the system, the
less it will be able to understand its environment. It has to, to some point, it has to build this
knowledge by itself. Then it can maybe generalize or learn something that it can relate to its
own experience. But let me know if the following example is good. You can explain to me theoretically
with the laws of physics how I can swim or how I can ride a bike. If I don't do it myself,
I wouldn't really understand what you are talking about. So then if I do it myself and then
I read or I learn the laws of physics about flotation, about the conservation of energy,
of the moments and so on, well, then I can really theorize and understand what's going
on when I ride the bike. So that's the idea. If we cannot anchor or ground on actual experience,
these notions, then we wouldn't really be able to understand these notions. So providing
initial knowledge or information, especially if it's described in theoretical ways or ways that
relate to how the humans actually understand this, I don't believe it will be much of help.
So one of the things that you mentioned earlier that I thought was interesting was that you put all
of this or the robot is exploring all of this in a framework, a values-based framework, right?
The robot has some values or motivation and it strikes me that perhaps this is one of the hooks
into the broader idea of ethics. Is that correct? Yes, well, values. Of course, it relates to some point
to ethics because the values I'm speaking about when you start to interact with the environment are
more values that will in the beginning relate to what is able to answer the motivation of the
robot. For example, just for the sake of simplicity, if the motivation of the robot is to have more power,
to plug into a power device, to have more energy, then this would lead the robot to search for those
actions and those perceptual inputs that can lead it to a better reach this goal, which is
actually achieved this motivation. But then the idea is exactly to make those motivations more
and more complex, starting with basic things like energy, for example, but then you can formulate
motivations in a more complex way because, for example, in order to reach the power plug,
I might have to go to somewhere else because it's not rotated here. Therefore, as another motivation,
there is the motivation to move, to have this motion to reach somewhere else where I can
indeed be able to have my initial motivation fulfilled. This is just an example to say that
we can build more and more global or abstract motivations. I mean, that's my purpose. To the point
that motivations can maybe lead to ethics, in this case, because when I mentioned values,
initially the values are those that are going to lead some to some rewards. So starting from
basic rewards to more complex rewards, is it rewarding to have, for example, a positive
interaction with another agent? How is it rewarding to have an interaction that is satisfying
another agent's own motivations? Now, if you build on that, you can reach a point where this notion
of motivation is going to, and the values guarded, is going to lead the robot to respect some
maybe ethical values. Ethics is built eventually when the Greek philosophers, when Plato, Aristotle,
came up with the notion of ethics. The idea was to live the good life. The good life is what enables
society to hold together, what enables people to live in a society in which they have the ability
to reach a form of happiness. That's what is the initial grounding of ethics. How can we live
together and live the good life? So it means that this comes from an ability to respect some values,
which make this social life possible instead of each one trying to kill their neighbors.
And so this can be translated, although it's not what I'm doing now in terms of scientific
research. This can be translated if we want our robots to respect, to develop an ethical
behavior or an ethical common sense. This can be also input as a motivation of the robot system.
However, personally, I don't believe that at least for some time we can build robots that can
grasp the meaning of some abstract notions, such as human dignity, for example. The notion of
dignity, which also is very important in ethics. I don't believe that we can really define this or
I don't see a way for a robot, which is a machine, to discover what this means exactly. So
there will be some order maybe that we will have to cross one day to try to achieve this. But
today I don't see how we can really define or learn by a robot. These abstract notions of human
dignity, dignity, freedom, human rights. I'm just curious, there are abstractness is a spectrum.
I'm not sure if I'm not sure that it's a two-dimensional spectrum. But I'm wondering,
do you have a sense for what's the most abstract thing we've been able to teach a robot?
Or any examples of things that are kind of towards the abstract side of the spectrum?
Not sure, I can answer this. Well, it's an abstract question.
Yes, no, because it's quite difficult to define what the robot has actually learned when we say
what does the robot know exactly. Of course, I can program the robot with some very abstract
notions. For example, speaking about emotions, I can program a robot that says, I love you,
but I don't think the robot understands what you love is. It's just the result of a specific
programming, some parameters that have been input and that the system can measure and react
according to those values of the values of those parameters. But understanding what love means
exactly, I'm not sure, I'm even sure of the contrary, that a robot can grasp this.
So this is why I'm saying, it depends what you mean by teaching and what you mean by understanding
by a robot system of those abstract notions.
I guess the things that came to mind were, I think we've been able to teach robots
an abstract notion of shape, for example, that is at a higher level than point locations.
And an abstract notion of relative position, like something being on top of another thing that,
again, is abstracted away from stepper motor rotations and point locations and things like that.
And it just occurred to me that there's a spectrum of abstractness. I don't know if those are
pretty close to the physical side of on that spectrum. And maybe we're just kind of stuck around
abstracting these base physical things. And I don't even know how to characterize the line between
that and the next set of things. I was just curious if this was something that you've come across
and thought of. Yeah, well, I still believe that the examples you've provided, which are mostly
geometrical examples, for example, or biological examples, so it's above something else.
I still believe that even if these sound as abstract notions that we have defined,
we, I don't believe we have really made a robot understand what this means exactly.
Of course, you can describe what it means box A on top of box B or whatever
by defining some relationships between the shapes. But this is this really defined in any situation.
Is it that abstract? What what will happen, for example, if you have a robot in outer space
and there is no gravity, what does it mean something on top of something else?
I'm not sure robot can grasp the profound notion of something on top of something else
because it doesn't even have this notion of gravity. And notion of up, down, and so on is
very much related to our relationship with the world. So it's on top because my I'm vertical,
for example, and something is on top of something else because it's following this verticality,
for example. But when you change the referential, I'm not sure the robot can grasp what this means.
So I believe our definitions are actually very much grounded in some knowledge that is not
expressed, not explicit in the robot system. So that kind of begs two questions for me. One is
what does understanding even mean and what is what is the test for robot understanding?
And secondarily, it also causes me to question this idea of having robots learn things from scratch
and in particular, if the robots are going to interact with and communicate with us as humans
and so many fundamental things about the way we perceive the world, like, you know, something
being on top of it is fundamentally connected to our humaneness. In this case, our verticality,
like, don't we have to give the robots a lot in order for them to develop an understanding of
the world that even makes sense to us? Yeah. Well, that's exactly the point. I mean, if we want to
build a robot that is useful, that we want to use tomorrow, that we want to use it for achieving
tasks with which we want to communicate and say, put this book on top of this desk, for example.
Of course, we have to program things into those robots. But again, the robot is just a kind of zombie.
It doesn't understand what it's doing. It's doing it, but it doesn't understand what it's doing.
It's just repeating or executing some program that has embedded what is needed for it to do this
task. If we truly, I mean, from a scientific point of view, not from an utilitarian point of view,
if I may say, if we want really to understand what this means, what does it mean for a system,
for a machine to develop those notions? Well, then it's a different story. And this is what
actually AR robotics in terms of scientific research is trying to deal with. And we are still
miles and years from understanding how we are going to do that. So two different things.
And just to be a little bit more provocative, I would say, my personal
view is that to achieve this understanding, you said what understanding is, to achieve this notion
of understanding. I believe something is necessary. It's the notion of self-awareness.
If the machine does not have this notion, I don't believe it will be able to really understand.
Because understanding is related to me, I mean, the self, being at the center, at the core of a
process of interaction with the environment. And the capacity of reflecting on one's own capacities,
one's own reasoning, reasoning on what I'm doing. Meta reasoning, if you wish.
So I believe that if we are not able to develop these meta reasoning capacities, this self-evaluation
capacity that enables to reflect on one's own place in the environment, in the universe,
in the world, this notion of understanding would be just artificial, not genuine. And which leads us
to this notion of are we able to build self-aware machines, conscious machines, self-aware machines?
That's another issue that is much discussed as well. But I think this is fundamental.
Otherwise, we will always have systems that just act like zombies, not like entities that
grasp what their environment and therefore what they are about, what they are doing, really.
Is there anything that you've come across in the research that is showing promising
results at demonstrating some degree of self-awareness?
There are tons of publications on self-awareness. But frankly, I think we are just
rotating around this notion like the planets are rotating around the sun,
except that the sun we can see it and the consciousness or self-awareness we can't, and we
don't have a clear definition of what it is. So this is actually the basic motivation between my work
is to try to understand if and how, and of course the question is still quite open, if and how
we can develop this notion of self-awareness through the interaction of the system with its
environment, the interaction of the system with other agents, other systems. The idea is that
when you interact with the environment, you have to relate things to you. You have to recognize
yourself in this environment. This is my action, this is my arm doing this, and this is therefore
my evolution, my will, my decision to act in this way. This my places the self in the center.
This means something to me because I can do something with it.
If the system is not able to grasp this relationship between itself and its environment,
it will not have self-awareness. So the self-awareness will be developed through this interaction
with the environment. It strikes me that some of the work that's happening in and around
reinforcement learning, playing Atari games and the like, it strikes me that some of that is
approaching self-awareness. The agent is clearly aware, well, I guess we have to define aware,
but there's a clear relationship between the actions of the agent and the environment and the
whole premise of the reinforcement learning environment is that the agent can manipulate this
environment and maximize some objective. Absolutely. The reinforcement learning, in my opinion,
is a core to this, and this is a technique which is absolutely related to try and
need to build, reinforce this understanding. The point is, of course, that in your
reinforcement learning, you have to learn the value function. It should be something that is
discovered as well. This means that I have to discover what is of value tuning. This gets us back
to motivations. There should be some minimal motivations upon which more complex motivations
are going to be built, but there should be some minimal motivations. However, most systems
doing reinforcement learning are focused on solving a specific problem. You have systems with,
of course, the concept is quite wide and quite general, but you have a system that's going to play
Atari, as you said, or that's going to assemble boxes and so on, with the same process. Again,
the process is core, but the values and the way they translate into achieving the motivations of
the system, that's a big point. For example, you achieve a system that is able to play, say,
a reinforcement learning is going to play Go or to play Atari, whatever, but what about a system
that says, I've had enough playing this game, let's do something else. Then it's going to
actually reason differently. It's not just doing something that it has been designed to do.
It's going to decide something else, which maybe is related to a different motivation. That's
a point in achieving a system that is not just doing what I have or learning to achieve what
I programmed to do. It has to be able from a wider set of motivations or a wider set of
self-reflection capacities to jump, to initiate maybe other decisions that were not explicitly
what I asked it to do at the moment. I'm not sure I'm clear, but the point is if the system is not
able to reflect on its own motivations and to change that, then it will remain prisoner
in the compartment of actions and objectives that the programmer has completely defined.
I'm not sure it's possible to get out of that compartment, but that's the issue. That's the point.
The notion of, I guess we can call it self-agency, that you just describe being able to come up
with its own motivations. Do you think that's inseparable from the idea of self-awareness?
Like, could a machine be both self-aware and yet unable to determine its own motivations?
Well, that's just a belief. I'm not sure. I think it's related, maybe to some degree.
I'm not sure we can define. Self-awareness is not necessarily something you have or we don't.
You might have several degrees of self-awareness. There are studies about this for some animals,
for example, for the dolphins or for some apes, but there might be some degrees of self-awareness
with, so I'm not sure there is only one degree, but eventually, yes, I think that this is related
to this capacity of being able, ultimately, to have a kind of free will to step out of the enclosure.
So, and this might lead more explicitly to the notion of ethics, actually, although we are trying
to, in the artically initiative, to work on the ethical issues of today's autonomous systems,
of today's intelligent systems. Those which don't have those capacities,
how do we reach a development or a design process of those systems so that they are compliant
with human values. So, this is a design and methodology, for example, this is something that
has to be considered and taken into account when people, researchers, engineers, designers
develop such systems, but on the more research scientific side, of course, having an autonomous system
or a system that is capable of reflecting on its own, on its own about ethical considerations.
Well, that will be the ultimate thing, but as I mentioned earlier, I don't believe we are
even close to that. It is certainly a fascinating topic in one with many, many kind of interrelated
layers. You know, we're coming up on the top of the hour, we've hit the top of the hour,
I think we're going to have to schedule a part two at some point to go into the ethics of
autonomous systems. There's a lot there to chat about as well, but I really enjoyed learning about
your research and some of the ways you're approaching this idea of abstract thought and
understanding for robotics. Well, thank you. Yes, I would be delighted to have another chat,
because indeed we just alluded very, very briefly to that, but the conversation was very,
very interesting. Great. Well, thank you so much, Raja. Thank you, Sam. All right, everyone, that's our show
for today. For more information on Raja or any of the topics covered in this episode,
you'll find the show notes at twimlai.com slash talk slash 118. If you have any questions for
Raja, please post them there and we'll make sure to bring them to his attention. If you're new to
the podcast and like what you hear or you're a veteran listener and haven't already done so,
head on over to your podcast app of choice and leave us your most gracious and glowing review.
It helps new listeners find us, which helps us grow. Thanks in advance. And of course,
thanks so much for listening and catch you next time.
