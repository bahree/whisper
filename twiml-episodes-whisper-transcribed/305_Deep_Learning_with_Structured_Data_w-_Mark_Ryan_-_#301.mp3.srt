1
00:00:00,000 --> 00:00:05,500
As we gear up for Tumacon AI platforms, now less than two weeks away, I've been out

2
00:00:05,500 --> 00:00:09,800
and about talking up the importance of automating, accelerating, and scaling machine learning

3
00:00:09,800 --> 00:00:11,800
in AI in the Enterprise.

4
00:00:11,800 --> 00:00:16,440
Now, I'm usually the one doing the interviewing, so being on the other side for a change

5
00:00:16,440 --> 00:00:18,440
was a nice experience.

6
00:00:18,440 --> 00:00:23,280
Much thanks to my friend's Alex Williams, host of the New Stack Makers podcast, Mentor

7
00:00:23,280 --> 00:00:28,640
Dial, host of the popular podcast Mentor Dialogue, and James McGuire of DataMation.

8
00:00:28,640 --> 00:00:32,880
I had a great time chatting with each of them, and I encourage you to check them out via

9
00:00:32,880 --> 00:00:37,680
the Twumacon blog at twumacon.com slash news.

10
00:00:37,680 --> 00:00:41,520
And while you're there, be sure to check out the latest speakers and agenda updates, and

11
00:00:41,520 --> 00:00:46,040
if you haven't already registered, take that step to secure your ticket for what's shaping

12
00:00:46,040 --> 00:00:48,920
up to be an amazing event.

13
00:00:48,920 --> 00:00:51,360
And now, onto the show.

14
00:00:51,360 --> 00:00:59,400
All right, everyone, I am on the line with Mark Ryan. Mark is the author of Deep Learning

15
00:00:59,400 --> 00:01:05,400
with Structured Data, a book that is currently in Early Access with Manning and Do for Publication

16
00:01:05,400 --> 00:01:07,960
in the spring of 2020.

17
00:01:07,960 --> 00:01:12,000
Mark, welcome to the Twumacon AI podcast.

18
00:01:12,000 --> 00:01:13,000
Thanks, Sam.

19
00:01:13,000 --> 00:01:14,000
It's great to be here.

20
00:01:14,000 --> 00:01:15,000
Awesome.

21
00:01:15,000 --> 00:01:19,600
Let's get started by talking a little bit about your background, and in particular, Deep Learning

22
00:01:19,600 --> 00:01:21,080
with Structured Data.

23
00:01:21,080 --> 00:01:29,720
That is a topic that folks are starting to talk about, and in fact, via the Meetup group

24
00:01:29,720 --> 00:01:37,080
and association with the podcast, we have a fair amount of experience exploring this

25
00:01:37,080 --> 00:01:42,800
through the fast AI study groups that we do, that's a big part of one of the lessons

26
00:01:42,800 --> 00:01:44,840
in that course.

27
00:01:44,840 --> 00:01:50,960
But I'd love to get a sense for how you came to be interested in this particular topic

28
00:01:50,960 --> 00:01:52,960
enough to write a book about it.

29
00:01:52,960 --> 00:01:53,960
Sure.

30
00:01:53,960 --> 00:01:54,960
Sure, Sam.

31
00:01:54,960 --> 00:02:00,400
So my academic background is from Artificial Intelligence a couple of winters ago.

32
00:02:00,400 --> 00:02:06,480
So I studied, studied at U of T with Graham Hurst back in the late 80s, and it was all

33
00:02:06,480 --> 00:02:09,000
symbolic AI back then.

34
00:02:09,000 --> 00:02:14,800
And there were some interesting use cases, but to a large extent, it didn't work.

35
00:02:14,800 --> 00:02:20,120
And I went to work for IBM, had a great career there, learned a great deal, spent a lot

36
00:02:20,120 --> 00:02:25,240
of time in DB2, the relational database product for my VM.

37
00:02:25,240 --> 00:02:30,640
And about 2016, it became evident to me that Artificial Intelligence was starting to work.

38
00:02:30,640 --> 00:02:35,600
There were things that were actually working, became general knowledge, and that reignited

39
00:02:35,600 --> 00:02:37,160
the spark in me.

40
00:02:37,160 --> 00:02:42,560
So I did Andrew Anges intro course, and the fast AI course.

41
00:02:42,560 --> 00:02:46,400
So it had that of my introduction to deep learning.

42
00:02:46,400 --> 00:02:48,920
And I was very interested in deep learning and the promise of that.

43
00:02:48,920 --> 00:02:52,920
And one of the things I found a little bit frustrating is a lot of the use cases, particularly

44
00:02:52,920 --> 00:03:00,440
outside of the context of the fast AI course, were to do with images or audio, they weren't

45
00:03:00,440 --> 00:03:01,440
structured data.

46
00:03:01,440 --> 00:03:06,120
And what I was looking for was, can I find a way to use this in my day-to-day work?

47
00:03:06,120 --> 00:03:09,360
Because this looks like it'll be very useful, and I wanted to learn more about it, and

48
00:03:09,360 --> 00:03:12,800
the best way to learn about it is to use data sets that you're familiar with.

49
00:03:12,800 --> 00:03:18,120
So when I did the fast AI course, and this would have been version one of the course, so it's

50
00:03:18,120 --> 00:03:20,280
been through a couple of iterations since then.

51
00:03:20,280 --> 00:03:24,160
And there's this section on doing deep learning with structured data.

52
00:03:24,160 --> 00:03:29,280
And that really sparked my curiosity, I thought, wow, this is really cool.

53
00:03:29,280 --> 00:03:34,360
So we got to look around for some code to have as sort of a starter kit to get going.

54
00:03:34,360 --> 00:03:36,120
And it wasn't easy to find.

55
00:03:36,120 --> 00:03:39,920
But there were some Kaggle competitions where people had been working on structured data

56
00:03:39,920 --> 00:03:43,600
sets and applying it to deep learning, and some very elegant little design holes that

57
00:03:43,600 --> 00:03:45,840
Rothman stores and the like.

58
00:03:45,840 --> 00:03:47,720
Exactly, exactly.

59
00:03:47,720 --> 00:03:50,840
And I was really impressed by some of the work that I saw there.

60
00:03:50,840 --> 00:03:55,480
It was very elegant, very, very straightforward, and good for me at the stage I was at then

61
00:03:55,480 --> 00:03:58,880
to get started, start doing some coding.

62
00:03:58,880 --> 00:04:03,680
And at the time I was responsible for the support organization for DB2.

63
00:04:03,680 --> 00:04:08,520
So there were, there's tons of data there, hundreds of tickets coming in every day, lots

64
00:04:08,520 --> 00:04:10,120
and lots of data.

65
00:04:10,120 --> 00:04:14,280
And I thought, you know, it would be good if we could apply deep learning to this to sort

66
00:04:14,280 --> 00:04:17,280
of see, can we do some predictions that are useful.

67
00:04:17,280 --> 00:04:23,560
So I built up a prototype model to predict how long a ticket would take to get closed.

68
00:04:23,560 --> 00:04:27,120
And that's, you know, seemed to work reasonably well.

69
00:04:27,120 --> 00:04:33,920
And then did another project, sort of taking what I've learned to predict duty manager calls.

70
00:04:33,920 --> 00:04:37,480
So those are cases where a client reaches a point of frustration and says, I'm done.

71
00:04:37,480 --> 00:04:43,800
I'm going to pick up the phone and get something happening with this particular problem.

72
00:04:43,800 --> 00:04:50,720
So applying what I had seen from some of the kernels in Kaggle and using the data that

73
00:04:50,720 --> 00:04:53,600
was available, created these prototypes.

74
00:04:53,600 --> 00:04:55,160
And I've learned a lot doing that.

75
00:04:55,160 --> 00:04:58,840
And they were, I think they turned out fairly well.

76
00:04:58,840 --> 00:05:03,520
But one of the problems with those prototypes was that the data was obviously proprietary.

77
00:05:03,520 --> 00:05:04,520
It couldn't share that.

78
00:05:04,520 --> 00:05:09,000
And there's a very strong ethic, you know, in machine learning and data science to share

79
00:05:09,000 --> 00:05:10,480
results.

80
00:05:10,480 --> 00:05:16,960
So I started to look for a more, it's like a general data set that I could use to apply

81
00:05:16,960 --> 00:05:19,360
deep learning to structured data.

82
00:05:19,360 --> 00:05:26,360
And I've written a few blog posts on medium about my experience with the predicting time

83
00:05:26,360 --> 00:05:29,760
to resolution and predicting duty manager calls.

84
00:05:29,760 --> 00:05:32,920
And Manning got in touch with me and said, would you like to write a book, kind of pull

85
00:05:32,920 --> 00:05:33,920
this together?

86
00:05:33,920 --> 00:05:36,960
And I thought, wow, that sounds interesting, sure.

87
00:05:36,960 --> 00:05:40,160
It has been a lot of work.

88
00:05:40,160 --> 00:05:43,440
And I've certainly learned a lot in the course of doing that.

89
00:05:43,440 --> 00:05:50,760
And one of the things I've done is create a sort of a full end-to-end example using an

90
00:05:50,760 --> 00:05:57,560
open data set, which is to do with streetcars in Toronto, Toronto is my hometown now.

91
00:05:57,560 --> 00:05:59,840
And it has a very extensive streetcar network.

92
00:05:59,840 --> 00:06:04,520
These are a light rail system that runs on the regular roads.

93
00:06:04,520 --> 00:06:05,520
And they're great.

94
00:06:05,520 --> 00:06:06,520
They're efficient.

95
00:06:06,520 --> 00:06:08,040
They're relatively cheap to run.

96
00:06:08,040 --> 00:06:12,480
They're cheap to create much cheaper than subways.

97
00:06:12,480 --> 00:06:16,480
The problem is, because they share the roads with regular traffic, if they break down

98
00:06:16,480 --> 00:06:20,200
or there's a delay, it really exacerbates gridlock.

99
00:06:20,200 --> 00:06:25,120
So the city of Toronto publishes a data set that describes all of the delays that have

100
00:06:25,120 --> 00:06:28,240
happened for the last five years.

101
00:06:28,240 --> 00:06:31,800
And I thought, well, I'm going to roll up my sleeves and try to create a simple deep learning

102
00:06:31,800 --> 00:06:37,320
model to analyze this data and see if it can come up with predictions to predict where

103
00:06:37,320 --> 00:06:41,200
they're going to be streetcar delays and hopefully be able to prevent them.

104
00:06:41,200 --> 00:06:47,880
So that was kind of the path I took, and that's how that was the genesis for the book.

105
00:06:47,880 --> 00:06:54,000
And so should I assume that that worked, that you were able to come up with a model

106
00:06:54,000 --> 00:07:00,040
that predicted the streetcar delays or predicted streetcar delays with that data set?

107
00:07:00,040 --> 00:07:02,120
Yeah, it does a decent job.

108
00:07:02,120 --> 00:07:03,360
It's not a huge data set.

109
00:07:03,360 --> 00:07:05,720
There are about 90,000 records right now.

110
00:07:05,720 --> 00:07:06,720
Okay.

111
00:07:06,720 --> 00:07:10,760
So, you know, there's some limitations, but certainly for the purposes of helping somebody

112
00:07:10,760 --> 00:07:18,920
who's going to be taking a trip, the accuracy is good enough to be useful.

113
00:07:18,920 --> 00:07:25,280
But more importantly, in terms of as a learning exercise, I think it's useful because it's

114
00:07:25,280 --> 00:07:26,920
an open data set.

115
00:07:26,920 --> 00:07:31,840
It's big, but not so big, you have to deal with kind of the problems of big data.

116
00:07:31,840 --> 00:07:33,400
It's very messy.

117
00:07:33,400 --> 00:07:37,400
So there's a lot of work to be done to prepare the data, which I think is a good learning

118
00:07:37,400 --> 00:07:39,040
experience.

119
00:07:39,040 --> 00:07:41,960
And it has various different kinds of data.

120
00:07:41,960 --> 00:07:45,400
There's text data, there's categorical data, there's some continuous data.

121
00:07:45,400 --> 00:07:51,440
So it has a lot of the, it's big enough to be interesting, but not so big that it's overwhelming.

122
00:07:51,440 --> 00:07:56,760
And I think it's, you know, it kind of makes a decent end-to-end example to go through

123
00:07:56,760 --> 00:07:57,760
the topic.

124
00:07:57,760 --> 00:07:58,760
Awesome.

125
00:07:58,760 --> 00:07:59,760
Awesome.

126
00:07:59,760 --> 00:08:06,960
Jumping back to the couple of projects that you worked on when you were at IBM, in particular

127
00:08:06,960 --> 00:08:09,920
this looking at how long it took to close tickets.

128
00:08:09,920 --> 00:08:16,440
When I think of a trouble ticket use case, and when I think of that trouble ticket use

129
00:08:16,440 --> 00:08:23,040
case, I think of, you know, not just structure data as being useful, but also the content

130
00:08:23,040 --> 00:08:24,040
of the ticket itself.

131
00:08:24,040 --> 00:08:27,920
So textual data, more like the application of NLP.

132
00:08:27,920 --> 00:08:34,120
Did you use only metadata about the tickets to predict the close time, or did you also

133
00:08:34,120 --> 00:08:35,880
use that content?

134
00:08:35,880 --> 00:08:37,720
That's a great question.

135
00:08:37,720 --> 00:08:41,480
So I did use the content, there's, all the tickets had a description, the description

136
00:08:41,480 --> 00:08:45,640
could sometimes it be two lines like, you know, you suck, a little bit more elaborate

137
00:08:45,640 --> 00:08:46,640
than that.

138
00:08:46,640 --> 00:08:50,720
And sometimes it would be a paragraph of lots of detail.

139
00:08:50,720 --> 00:08:54,040
But that description was really essential, because that's kind of the initial customer's

140
00:08:54,040 --> 00:08:57,360
sense of, you know, what they found.

141
00:08:57,360 --> 00:09:05,160
And that was, we had a simple, the model included a simple recurrent neural network to deal

142
00:09:05,160 --> 00:09:06,160
with that data.

143
00:09:06,160 --> 00:09:15,400
So it, that text field was tokenized, used in beddings, and then there was a layer, an

144
00:09:15,400 --> 00:09:21,040
RNN layer, that was applied in the overall model to take that text into account.

145
00:09:21,040 --> 00:09:25,000
And it was interesting, the difference, because they did some experiments of including that

146
00:09:25,000 --> 00:09:29,840
as a feature and then excluding it, because it was fairly expensive, it added some links

147
00:09:29,840 --> 00:09:32,440
to the time it took to train the model.

148
00:09:32,440 --> 00:09:36,760
And it made a reasonable difference, like it was, you know, between three and four percent

149
00:09:36,760 --> 00:09:40,240
the accuracy, if this, if this field was included.

150
00:09:40,240 --> 00:09:42,680
And that really exciting, I thought, that's really something.

151
00:09:42,680 --> 00:09:46,040
And the other thing is that these descriptions weren't all, so the particular field is the

152
00:09:46,040 --> 00:09:49,440
description field, so all of the text of the ticket.

153
00:09:49,440 --> 00:09:55,240
It, no, it's just the description, the text of the ticket was, wasn't available to me

154
00:09:55,240 --> 00:09:56,240
at that time.

155
00:09:56,240 --> 00:09:58,080
So it wasn't full back and forth.

156
00:09:58,080 --> 00:10:02,920
Because that, there could be, you know, the equivalent of 100 pages of text.

157
00:10:02,920 --> 00:10:06,160
So all it was dealing with text wise was the, was the description.

158
00:10:06,160 --> 00:10:10,160
So it could be up to 500, 500, 600 characters altogether.

159
00:10:10,160 --> 00:10:11,160
Got it.

160
00:10:11,160 --> 00:10:17,760
And so that's typically the, the textual description of the issue, either as provided by the

161
00:10:17,760 --> 00:10:25,720
initial customer who's, who's submitting the ticket or whoever the support rep is that

162
00:10:25,720 --> 00:10:30,040
is taking their call, it would, it would always be the customer.

163
00:10:30,040 --> 00:10:33,400
And that was, it was intentional to say, and that was part of the whole idea of the model

164
00:10:33,400 --> 00:10:37,800
was to only take data that was available when a ticket first hit our system.

165
00:10:37,800 --> 00:10:38,800
Okay.

166
00:10:38,800 --> 00:10:39,800
So, and that description would be there.

167
00:10:39,800 --> 00:10:44,840
There were other things, obviously, like, whether the ticket had changed in severity,

168
00:10:44,840 --> 00:10:48,360
that wouldn't be available when the ticket was first opened, because that's kind of a

169
00:10:48,360 --> 00:10:49,600
data leakage problem.

170
00:10:49,600 --> 00:10:53,200
You start to peak over and see, oh, that looks, you know, use data that you don't actually

171
00:10:53,200 --> 00:10:55,760
have available to you when you're making the prediction.

172
00:10:55,760 --> 00:10:59,200
But the textual description of the problem coming from the client was always there when

173
00:10:59,200 --> 00:11:04,080
the ticket was opened, and that was the, that was one of the features that was fed into

174
00:11:04,080 --> 00:11:05,080
the model.

175
00:11:05,080 --> 00:11:06,080
Okay.

176
00:11:06,080 --> 00:11:12,200
And so you said that the importance of this feature, you know, this, this features presence

177
00:11:12,200 --> 00:11:17,720
gave you an additional 3% increase in accuracy.

178
00:11:17,720 --> 00:11:23,160
That is, you know, relative to what without it, how much of an impact did it have?

179
00:11:23,160 --> 00:11:25,640
So, that was in the, in terms of the absolute accuracy.

180
00:11:25,640 --> 00:11:31,840
So, I think at that time, it was probably going from 73 to 76% accuracy, leaving taking

181
00:11:31,840 --> 00:11:34,840
that, taking that field out or leaving it in.

182
00:11:34,840 --> 00:11:44,080
Which I think says a lot about the, the power, I guess, of doing deep learning with the

183
00:11:44,080 --> 00:11:49,760
structure data that adding the text, the descriptive text of the ticket only gave you an incremental

184
00:11:49,760 --> 00:11:55,120
3% accuracy in terms of predicting how long it'll take to close the ticket.

185
00:11:55,120 --> 00:11:56,120
Is that reasonable?

186
00:11:56,120 --> 00:11:57,920
Is that your interpretation as well?

187
00:11:57,920 --> 00:11:58,920
Yeah.

188
00:11:58,920 --> 00:11:59,920
That's right.

189
00:11:59,920 --> 00:12:02,520
I guess at the time I saw it, you know, it's, it's relative where you are.

190
00:12:02,520 --> 00:12:08,120
I was, I was very happy it had that increase, but you're right that the, the portion of the

191
00:12:08,120 --> 00:12:12,800
data that would have traditionally been dealt with with deep learning was only one of the

192
00:12:12,800 --> 00:12:14,640
feeds that was going into the model.

193
00:12:14,640 --> 00:12:16,680
And that feed by itself, well, it made a difference.

194
00:12:16,680 --> 00:12:20,240
It was a relatively small proportion of the difference in terms of the accuracy.

195
00:12:20,240 --> 00:12:21,240
Yeah.

196
00:12:21,240 --> 00:12:22,240
Yeah.

197
00:12:22,240 --> 00:12:25,480
I would have thought that without the text, it would be very hard to solve this, to solve

198
00:12:25,480 --> 00:12:29,240
this problem with any, any degree of accuracy.

199
00:12:29,240 --> 00:12:30,240
Interesting.

200
00:12:30,240 --> 00:12:35,640
And out of curiosity, did you try training a model only on the descriptive text to see if

201
00:12:35,640 --> 00:12:37,640
you were able to get any results?

202
00:12:37,640 --> 00:12:42,000
I'm wondering if there's a scenario where, if you didn't have it, you know, if you

203
00:12:42,000 --> 00:12:47,160
added it, it only gave you 3% incremental accuracy, but you could get a good part of the way

204
00:12:47,160 --> 00:12:50,600
there if you didn't have any of the other things either.

205
00:12:50,600 --> 00:12:51,600
That's a really good question.

206
00:12:51,600 --> 00:12:54,880
I didn't try that, but that's, that's an interesting question.

207
00:12:54,880 --> 00:12:59,000
I, I guess in this may be kind of a primitive way of thinking about it, but because there

208
00:12:59,000 --> 00:13:05,160
were, I guess, you know, there were another 13 or 14 features that were available.

209
00:13:05,160 --> 00:13:07,320
And sometimes this text was fairly short.

210
00:13:07,320 --> 00:13:11,040
I thought, well, this will make it'll, it'll have some impact, but by itself, it probably

211
00:13:11,040 --> 00:13:16,760
wouldn't be enough to actually come up with a reasonable, a reasonably accurate description.

212
00:13:16,760 --> 00:13:22,320
And then just looking at some of the, at some of the descriptions, they were pretty,

213
00:13:22,320 --> 00:13:23,320
pretty cryptic.

214
00:13:23,320 --> 00:13:26,800
But something I just wanted to mention is that these descriptions weren't even all in

215
00:13:26,800 --> 00:13:27,800
English.

216
00:13:27,800 --> 00:13:32,720
So they were, they were a fair number in Japanese or Chinese.

217
00:13:32,720 --> 00:13:37,680
And just given the aggregated size of the data set, it wasn't massively huge.

218
00:13:37,680 --> 00:13:40,600
It was about, about a little over a million records.

219
00:13:40,600 --> 00:13:43,520
It's still, it's still made a difference.

220
00:13:43,520 --> 00:13:45,920
Like it still provided some, some benefit.

221
00:13:45,920 --> 00:13:46,920
And I see that.

222
00:13:46,920 --> 00:13:51,400
I know some people are kind of critical of the idea of using deep learning, kind of backing

223
00:13:51,400 --> 00:13:55,440
up the dump truck of data and just tipping it open, seeing, seeing what comes out.

224
00:13:55,440 --> 00:14:00,640
But at the same time, it was, I was, I was impressed by what could be done without doing

225
00:14:00,640 --> 00:14:02,880
a whole lot of munking around with the data.

226
00:14:02,880 --> 00:14:07,240
It was just sort of taking the, the structure data as it was applying a relatively simple

227
00:14:07,240 --> 00:14:14,400
model to it, getting, you know, not, not bad results, I was, I was impressed, I was impressed

228
00:14:14,400 --> 00:14:15,400
by that.

229
00:14:15,400 --> 00:14:20,080
And particularly, because there have been so little, other than the fast AI course, so

230
00:14:20,080 --> 00:14:25,120
little spoken about in terms of using deep learning with, with structured data.

231
00:14:25,120 --> 00:14:26,120
Mm-hmm.

232
00:14:26,120 --> 00:14:27,120
Mm-hmm.

233
00:14:27,120 --> 00:14:34,360
So we, we tend to think of, rightly so, deep learning as requiring a ton of data.

234
00:14:34,360 --> 00:14:40,560
And certainly exacerbated by the fact that, you know, some of the things that deep learning

235
00:14:40,560 --> 00:14:48,240
is really good at are, you know, images which can be large speech, which can be large files.

236
00:14:48,240 --> 00:14:53,560
Structured data can be a lot more compact, but when we think of the number of, you know,

237
00:14:53,560 --> 00:15:01,600
rows or examples that you're feeding into a model is, do you require less data from

238
00:15:01,600 --> 00:15:06,920
examples or rows perspective to train a deep learning model accurately on structured

239
00:15:06,920 --> 00:15:12,880
data, or is it kind of about the same, but the data is just more compact.

240
00:15:12,880 --> 00:15:15,840
That's a good, that's a good question.

241
00:15:15,840 --> 00:15:22,080
I'd say that to get a, you know, the rule of thought that you need tens of thousands

242
00:15:22,080 --> 00:15:26,760
of records to have a starting point is probably applicable, as applicable to structured data

243
00:15:26,760 --> 00:15:29,080
as it would be to on structured data.

244
00:15:29,080 --> 00:15:30,080
Mm-hmm.

245
00:15:30,080 --> 00:15:34,000
But one of the things interesting, one of the things I heard is a critique, because certainly

246
00:15:34,000 --> 00:15:38,080
there are people who said, like, don't, don't use deep learning with structured data.

247
00:15:38,080 --> 00:15:39,880
And I'm thinking, well, why?

248
00:15:39,880 --> 00:15:43,200
Because that's where my problem space is, that's where the job I'm trying to do is all

249
00:15:43,200 --> 00:15:44,200
about structured data.

250
00:15:44,200 --> 00:15:45,200
Yeah, don't use that.

251
00:15:45,200 --> 00:15:49,280
You use XGBoost, use something simpler.

252
00:15:49,280 --> 00:15:51,160
And I said, well, why?

253
00:15:51,160 --> 00:15:55,040
And some of the answers I got back were, well, the structured data sets are too small.

254
00:15:55,040 --> 00:15:58,280
If they're not big enough to actually apply deep learning to.

255
00:15:58,280 --> 00:16:02,880
And I think that's a bit of a canard, because there are some huge structured data sets.

256
00:16:02,880 --> 00:16:09,040
They're data sets, certainly, in the world of DB2, which is all about structured data,

257
00:16:09,040 --> 00:16:15,240
structured tabular data, commonly had tables with billions of records in them.

258
00:16:15,240 --> 00:16:20,520
So that objection, I think, is a little bit, it's a little bit short-sighted, and obviously

259
00:16:20,520 --> 00:16:24,480
you're not going to get great results with tiny data sets, whether it's structured or

260
00:16:24,480 --> 00:16:31,000
unstructured, but I think as an objection to attempting to use deep learning with structured

261
00:16:31,000 --> 00:16:36,520
data, the data set size really is not material to the decision about whether it's worthwhile

262
00:16:36,520 --> 00:16:37,520
or not.

263
00:16:37,520 --> 00:16:45,720
Did you attempt to apply XGBoost or any other method to this kind of problem?

264
00:16:45,720 --> 00:16:46,720
I did.

265
00:16:46,720 --> 00:16:50,320
So I tried XGBoost on two of those problems that I described.

266
00:16:50,320 --> 00:16:57,200
And I guess one thing that is true, the results were not significantly better for deep learning.

267
00:16:57,200 --> 00:17:01,640
So I think that's one of the arguments people say, well, it's not a great idea to use

268
00:17:01,640 --> 00:17:06,200
deep learning with structured data, is if there's a simpler way to do it that doesn't have

269
00:17:06,200 --> 00:17:11,320
the complexities or the opaqueness of deep learning, then why not do that?

270
00:17:11,320 --> 00:17:15,040
And I think there's something to be said for that.

271
00:17:15,040 --> 00:17:20,240
But at the same time, if I'm looking down the road a little bit, the amount of

272
00:17:20,240 --> 00:17:27,000
effort that it takes to get a reasonable results with XGBoost versus deep learning, particularly

273
00:17:27,000 --> 00:17:33,000
the human effort required for it, I'm not convinced that XGBoost is going to, for example,

274
00:17:33,000 --> 00:17:35,040
is going to be the winner there.

275
00:17:35,040 --> 00:17:40,480
As there's been so much attention on the efficiency of tools related to deep learning, the

276
00:17:40,480 --> 00:17:46,560
libraries are getting better, the TensorFlow 2.0 coming out, making things much more

277
00:17:46,560 --> 00:17:49,120
straightforward and simpler.

278
00:17:49,120 --> 00:17:53,960
I think some of the objections are a little bit out of date.

279
00:17:53,960 --> 00:17:59,480
And there are certainly cases where a non-deep learning approach could produce better results

280
00:17:59,480 --> 00:18:01,400
and maybe the right way to go.

281
00:18:01,400 --> 00:18:05,400
I guess the argument I'm making is that people keep an open mind about applying deep learning

282
00:18:05,400 --> 00:18:06,720
to structured data.

283
00:18:06,720 --> 00:18:14,440
And I think particularly as the human cost is required to get results, becomes a bigger

284
00:18:14,440 --> 00:18:20,960
and bigger factor, may find that deep learning has more applicability to structured data,

285
00:18:20,960 --> 00:18:22,320
to keep an open mind about it.

286
00:18:22,320 --> 00:18:23,320
Yeah, yeah.

287
00:18:23,320 --> 00:18:24,320
Now that's a really interesting point.

288
00:18:24,320 --> 00:18:31,600
I think we've got an entire conference coming up on the tooling and technology platforms

289
00:18:31,600 --> 00:18:38,560
that are allowing enterprises to increasingly automate and make their ability to deliver

290
00:18:38,560 --> 00:18:42,640
deep learning models into production, as well as traditional machine learning models

291
00:18:42,640 --> 00:18:46,240
in a production more quickly and efficiently.

292
00:18:46,240 --> 00:18:53,040
And I think that's only going to serve to reduce the barriers.

293
00:18:53,040 --> 00:18:58,360
And as you mentioned, the frameworks are getting more powerful and easier to use.

294
00:18:58,360 --> 00:19:08,200
And so now you've got this, yes, kind of a more opaque method, but one that is highly

295
00:19:08,200 --> 00:19:12,560
automated in a sense that you don't need a lot of manual feature engineering to get

296
00:19:12,560 --> 00:19:18,880
good results versus one that requires a lot of human investment to get the same level

297
00:19:18,880 --> 00:19:22,760
of results or comparable results.

298
00:19:22,760 --> 00:19:28,200
As that barrier to entry on the deep learning side is reduced, in addition to all the software

299
00:19:28,200 --> 00:19:32,880
tools and frameworks that are improving, you've got hardware that's making it cheaper

300
00:19:32,880 --> 00:19:37,800
to build these models on the compute side, that's really going to tip the balances.

301
00:19:37,800 --> 00:19:39,640
That's what I'm hearing you say.

302
00:19:39,640 --> 00:19:42,120
Yeah, yeah, I really believe that's the case.

303
00:19:42,120 --> 00:19:48,600
And I think, and this is something in the FAST AI course, a Jeremy Howard said that this

304
00:19:48,600 --> 00:19:51,360
topic, people have kind of scratched the surface of it.

305
00:19:51,360 --> 00:19:54,480
And I think there's much more to be done there.

306
00:19:54,480 --> 00:19:59,440
And obviously my approach, I'm trying to sort of a beaten potatoes, trying to solve problems

307
00:19:59,440 --> 00:20:00,440
approach.

308
00:20:00,440 --> 00:20:05,080
But I hope that there were some research as well, people looking at it from a search

309
00:20:05,080 --> 00:20:07,600
point of view, you're saying what can be done.

310
00:20:07,600 --> 00:20:14,000
And the other thing that I think structured data has, there's sort of by definition, structured

311
00:20:14,000 --> 00:20:16,320
data has very rich metadata.

312
00:20:16,320 --> 00:20:22,600
So you have a database where everything, all of the tables in the database are described

313
00:20:22,600 --> 00:20:23,840
in tables.

314
00:20:23,840 --> 00:20:31,120
And it's very, it's all tooled and instrumented to be able to see what everything that's there

315
00:20:31,120 --> 00:20:32,640
in the database.

316
00:20:32,640 --> 00:20:39,040
And I think there's some potential for work that's kind of exploratory.

317
00:20:39,040 --> 00:20:44,440
I could see something like a web crawling to go through a database and see, are there

318
00:20:44,440 --> 00:20:46,640
potential for a useful model here?

319
00:20:46,640 --> 00:20:51,120
And you can see a situation where as the cost of compute drops, have something that's

320
00:20:51,120 --> 00:20:56,640
running in the background is sort of trying all sorts of different combinations of features

321
00:20:56,640 --> 00:20:57,640
in a large database sale.

322
00:20:57,640 --> 00:20:59,560
And maybe some interesting results there.

323
00:20:59,560 --> 00:21:00,560
And be able to do that.

324
00:21:00,560 --> 00:21:04,640
And that's something the structured data gives you that you don't get from unstructured,

325
00:21:04,640 --> 00:21:08,960
but you don't necessarily get from a structured data where there isn't that sort of rich metadata

326
00:21:08,960 --> 00:21:10,960
describing what's there.

327
00:21:10,960 --> 00:21:14,600
And we'll come back to the structure in just a moment.

328
00:21:14,600 --> 00:21:19,600
But one of the, you mentioned one of the points that Jeremy makes in the fast.ai course.

329
00:21:19,600 --> 00:21:26,240
And that is that in many of these examples, particularly the Kaggle ones, like I think

330
00:21:26,240 --> 00:21:33,400
it was Rothman stores, competition, you know, when you look at the leaderboard, I forget

331
00:21:33,400 --> 00:21:34,840
the details I'm going to watch them.

332
00:21:34,840 --> 00:21:40,920
I think, you know, out of the top, you know, the top N, a good number of them, if not

333
00:21:40,920 --> 00:21:47,480
the top spots were folks that, you know, applied deep learning based approaches.

334
00:21:47,480 --> 00:21:53,480
And the key takeaway is, and this is, you know, them competing against data scientists

335
00:21:53,480 --> 00:22:00,200
with, you know, oodles of experience in this particular retail domain.

336
00:22:00,200 --> 00:22:05,480
And, you know, in many cases, folks with no domain experience, plus deep learning were

337
00:22:05,480 --> 00:22:11,520
able to outperform folks that, you know, brought domain experience and, you know, handcrafted

338
00:22:11,520 --> 00:22:13,560
models to bear.

339
00:22:13,560 --> 00:22:14,560
Yeah.

340
00:22:14,560 --> 00:22:15,560
Yeah.

341
00:22:15,560 --> 00:22:19,880
That's, it's, it's a good observation use and, you know, Kaggle in a way can be, it's

342
00:22:19,880 --> 00:22:25,440
some artificial aspects of it, but it's sort of, it's raw capitalism and what works best

343
00:22:25,440 --> 00:22:27,040
rises to the top.

344
00:22:27,040 --> 00:22:30,360
So yeah, I think there's, there's potential there.

345
00:22:30,360 --> 00:22:37,280
And some of the big players are doing this and there are applications that I've heard

346
00:22:37,280 --> 00:22:43,960
of Google and Amazon are doing with structured data, but it just doesn't seem to be something

347
00:22:43,960 --> 00:22:48,360
that's really front and center in terms of what people, as both as they're learning

348
00:22:48,360 --> 00:22:54,560
about deep learning and where the research is, it seems to be much more on the, the unstructured

349
00:22:54,560 --> 00:22:57,480
that is the non, non tabular data side.

350
00:22:57,480 --> 00:23:01,200
One of the other objections I've heard is, and there's this heuristic thrown out there

351
00:23:01,200 --> 00:23:05,120
is that about 80% of the data in the world is unstructured.

352
00:23:05,120 --> 00:23:10,480
So, you know, if that's the case, it's only like one fifth of data is structured, then,

353
00:23:10,480 --> 00:23:12,520
you know, how interesting is it to apply that?

354
00:23:12,520 --> 00:23:14,040
Still a lot of data.

355
00:23:14,040 --> 00:23:15,040
It's a lot of data.

356
00:23:15,040 --> 00:23:19,400
And I know from experience from working with DB2, I know that the, you know, the world

357
00:23:19,400 --> 00:23:20,640
runs on it.

358
00:23:20,640 --> 00:23:26,840
Every bank, every insurance company, every, every government depends on, on structured data,

359
00:23:26,840 --> 00:23:34,640
you know, relational database may not be sexy, but it's, it underpins our modern lives.

360
00:23:34,640 --> 00:23:39,040
So there's a lot of important data that's, that's structured as well, even if the volume

361
00:23:39,040 --> 00:23:42,520
isn't as much as my structured data.

362
00:23:42,520 --> 00:23:48,320
You were making the point that it's a, the application of deep learning to structured

363
00:23:48,320 --> 00:23:53,200
data in particular is a topic that doesn't get a lot of play out there.

364
00:23:53,200 --> 00:24:01,080
And it made me think of embeddings, which is one of the key techniques that are used

365
00:24:01,080 --> 00:24:05,400
in applying deep learning to structured data.

366
00:24:05,400 --> 00:24:12,760
When I first heard about embeddings through the Fast.ai course, I don't know, a couple

367
00:24:12,760 --> 00:24:15,480
of years ago now, you didn't hear a lot about it.

368
00:24:15,480 --> 00:24:17,800
It was in a couple of obscure papers.

369
00:24:17,800 --> 00:24:21,000
And now everybody's doing it and talking about it.

370
00:24:21,000 --> 00:24:23,160
And it's almost, you know, a pass A, right?

371
00:24:23,160 --> 00:24:29,320
It's just a tool that we, we, we pull out of the toolbox to solve a great deal, a

372
00:24:29,320 --> 00:24:32,320
great number of problems.

373
00:24:32,320 --> 00:24:41,680
It was just an article I saw yesterday about StitchFix, the, the kind of clothing, styling,

374
00:24:41,680 --> 00:24:46,760
in a box subscription company, created a model.

375
00:24:46,760 --> 00:24:50,160
I was talking about some of the different models that they use internally.

376
00:24:50,160 --> 00:24:56,360
And a lot of them are based on style embeddings and things like that.

377
00:24:56,360 --> 00:25:01,560
And so kind of in that vein, I suspect that we'll start to hear a lot more about this.

378
00:25:01,560 --> 00:25:07,960
And the Fast.ai course is just kind of ahead of its time once again.

379
00:25:07,960 --> 00:25:13,400
But why don't we take a second to drill into some of the, you know, how it works and kind

380
00:25:13,400 --> 00:25:16,480
of the details that you'll be talking about in the book.

381
00:25:16,480 --> 00:25:17,480
It's a manning book.

382
00:25:17,480 --> 00:25:22,400
So I'm assuming it's going to be kind of rolled up the sleeves and kind of dig into how

383
00:25:22,400 --> 00:25:23,920
to make all this stuff work.

384
00:25:23,920 --> 00:25:29,280
And I would assume that embeddings is a big, you know, one of the chapters in that book

385
00:25:29,280 --> 00:25:31,960
somewhere, is that, is that the case?

386
00:25:31,960 --> 00:25:32,960
Absolutely.

387
00:25:32,960 --> 00:25:33,960
Absolutely.

388
00:25:33,960 --> 00:25:39,640
So dealing with, I guess one of this, this may sound a bit trite, but when they're first

389
00:25:39,640 --> 00:25:43,360
sort of at one hot encoding, I thought, I hate this idea.

390
00:25:43,360 --> 00:25:44,360
And I know it's necessary.

391
00:25:44,360 --> 00:25:47,400
I know it's something that, you know, we have to use, but I thought, wow, you know, this

392
00:25:47,400 --> 00:25:52,600
is, if you've got a category with more than a dozen values in it, this just gets crazy.

393
00:25:52,600 --> 00:25:53,600
Yeah.

394
00:25:53,600 --> 00:25:57,360
It's huge, huge, you know, pandas, data frames, for example.

395
00:25:57,360 --> 00:26:04,480
And embeddings give you a way you can, you can assign integer encodings to the values

396
00:26:04,480 --> 00:26:10,240
in a categorical feature and then use embeddings to learn their relationships.

397
00:26:10,240 --> 00:26:15,520
So you get away from having to use a one hot encoding and the inefficiencies that are

398
00:26:15,520 --> 00:26:16,920
involved there.

399
00:26:16,920 --> 00:26:17,920
So yeah.

400
00:26:17,920 --> 00:26:22,120
So just in terms of what you said about embeddings completely agree that so it's a very

401
00:26:22,120 --> 00:26:23,120
hot topic.

402
00:26:23,120 --> 00:26:30,520
I think the idea that something that came out of NLP and has more broad applications

403
00:26:30,520 --> 00:26:32,360
is really interesting.

404
00:26:32,360 --> 00:26:37,920
And the idea that you kind of, you kind of get unsupervised learning, not for free, but

405
00:26:37,920 --> 00:26:44,440
as some of some of the exhaust foam from doing supervised learning, solving a supervised

406
00:26:44,440 --> 00:26:45,440
learning problem.

407
00:26:45,440 --> 00:26:50,480
And you can use the embeddings to come up with categorizations, like, you know, movie

408
00:26:50,480 --> 00:26:54,680
categorizations for Netflix, that kind of thing.

409
00:26:54,680 --> 00:26:55,680
That's really interesting.

410
00:26:55,680 --> 00:26:58,440
You get a bit of a two for one value.

411
00:26:58,440 --> 00:27:07,680
One of the things that comes up frequently in our fast AI study groups as a point of confusion

412
00:27:07,680 --> 00:27:14,880
when we're talking about the structured data topic is kind of the relationship between

413
00:27:14,880 --> 00:27:19,800
embeddings from an embedding space perspective and the way it's used in NLP.

414
00:27:19,800 --> 00:27:25,960
And the embeddings that we use in the structured data problems, do you have a, like, is that

415
00:27:25,960 --> 00:27:26,960
real clear to you?

416
00:27:26,960 --> 00:27:29,160
Do you have a great way of explaining that?

417
00:27:29,160 --> 00:27:36,680
Well, I think I guess with a, with NLP, you can have, you know, tens of thousands, millions

418
00:27:36,680 --> 00:27:40,640
of values and just the number of individual words that are each with their own individual

419
00:27:40,640 --> 00:27:41,960
embeddings.

420
00:27:41,960 --> 00:27:48,240
And then in a structured data problem where you're leaving aside textual features, just talking

421
00:27:48,240 --> 00:27:53,000
about categorical features, you're going to have a smaller number of individual values.

422
00:27:53,000 --> 00:27:57,480
So the embedding space may have a smaller dimension.

423
00:27:57,480 --> 00:28:03,920
You may have a, if I could say a simpler embedding space than you would dealing with NLP.

424
00:28:03,920 --> 00:28:08,440
But the fundamental ideas is the same.

425
00:28:08,440 --> 00:28:14,120
And I guess to get back to your question before about how this sort of how things work.

426
00:28:14,120 --> 00:28:20,880
So as I mentioned, the book has the Toronto Streetcar data set as the problem is being solved.

427
00:28:20,880 --> 00:28:22,960
So go through the process of cleaning that up.

428
00:28:22,960 --> 00:28:26,240
So getting getting rid of bad values.

429
00:28:26,240 --> 00:28:31,680
One of the things that I've learned of the course of doing this was some geo coding.

430
00:28:31,680 --> 00:28:35,980
So there are address values that describe where the problem occurred or where the street

431
00:28:35,980 --> 00:28:38,440
car breakdown, where was there a delay.

432
00:28:38,440 --> 00:28:41,120
And those address values are just completely messy.

433
00:28:41,120 --> 00:28:49,560
They're totally free form, you know, say young, young and queen, their date, their misspelled,

434
00:28:49,560 --> 00:28:53,720
their references to sort of known areas in Toronto.

435
00:28:53,720 --> 00:28:54,720
So it's great.

436
00:28:54,720 --> 00:28:57,120
It's a very interesting problem to have.

437
00:28:57,120 --> 00:29:03,840
And it took advantage of Google's geo coding API to turn those values into latitude and

438
00:29:03,840 --> 00:29:05,200
longitude values.

439
00:29:05,200 --> 00:29:09,760
So that's probably that's the feature that required the most, the most effort to get something

440
00:29:09,760 --> 00:29:11,720
coming out the other end.

441
00:29:11,720 --> 00:29:16,040
And then for the categorical values, so that would be, for example, the street car route,

442
00:29:16,040 --> 00:29:21,840
the vehicle number, the day of the week, some Monday, Tuesday, Wednesday, whatever.

443
00:29:21,840 --> 00:29:26,520
Those get get translated to integer IDs.

444
00:29:26,520 --> 00:29:31,880
And then so all the all the categorical values get get translated that way.

445
00:29:31,880 --> 00:29:38,600
Text fields, individual words get translated into IDs.

446
00:29:38,600 --> 00:29:44,200
And then those get prepared to be fed into a fairly simple carous model.

447
00:29:44,200 --> 00:29:50,560
But the thing that is a bit of an innovation is that the model is the layers in the model

448
00:29:50,560 --> 00:29:55,560
are automatically defined based on the columns that are in the table, the input table.

449
00:29:55,560 --> 00:30:02,080
So if they're and all of the categorical columns get a set of layers in the model to get

450
00:30:02,080 --> 00:30:05,040
an embedding layer and a number of other layers.

451
00:30:05,040 --> 00:30:11,000
If there are any text columns in the input data set, they get, they get embeddings.

452
00:30:11,000 --> 00:30:19,800
They also get an RNN layer and then continuous values like temperature just kind of flow

453
00:30:19,800 --> 00:30:21,280
through.

454
00:30:21,280 --> 00:30:26,600
So that model gets built up layer by layer automatically based on the columns that are

455
00:30:26,600 --> 00:30:29,920
in the input, the input data set.

456
00:30:29,920 --> 00:30:35,840
And that's in a nutshell, how the model gets put together.

457
00:30:35,840 --> 00:30:43,080
So this automated generation of the model is this based on a tool that you've built?

458
00:30:43,080 --> 00:30:44,760
This is just Python code.

459
00:30:44,760 --> 00:30:51,760
So there are, there's part of the code is identifying which columns of the overall

460
00:30:51,760 --> 00:30:55,320
data set, which columns are going to be used to train the model.

461
00:30:55,320 --> 00:31:00,440
And then saying which columns are breaking them into three categories, a categorical.

462
00:31:00,440 --> 00:31:05,800
So those would be things like the day of the week, a continuous, like temperature or

463
00:31:05,800 --> 00:31:07,920
time duration and text.

464
00:31:07,920 --> 00:31:11,360
So a description of a of a problem, for example.

465
00:31:11,360 --> 00:31:16,160
This sounds like you've built this tool that you point at a data set and it will almost

466
00:31:16,160 --> 00:31:22,920
like AutoML, I create a model that at least as a starting place for making some predictions

467
00:31:22,920 --> 00:31:27,640
on the structure data as opposed to, you know, the book walking you through, like how

468
00:31:27,640 --> 00:31:31,600
you would perform this analysis by hand.

469
00:31:31,600 --> 00:31:32,600
That's right.

470
00:31:32,600 --> 00:31:36,560
So the book provides, the idea of the book is to generalize a little bit.

471
00:31:36,560 --> 00:31:40,360
So the, the streetcar problem is used as an example, but say more generally here's how

472
00:31:40,360 --> 00:31:41,360
you deal with it.

473
00:31:41,360 --> 00:31:46,320
But the intention of the code is that it could be applied to other structured data sets.

474
00:31:46,320 --> 00:31:47,320
That it wouldn't be limited.

475
00:31:47,320 --> 00:31:50,960
You could, that somebody could fairly quickly take a different structured data set and apply

476
00:31:50,960 --> 00:31:55,840
the code and get, try it out to see how it would work with a different, a different structured

477
00:31:55,840 --> 00:32:00,720
data set with different columns and different, a different mix of categorical texts and

478
00:32:00,720 --> 00:32:01,720
continuous columns.

479
00:32:01,720 --> 00:32:02,720
Okay.

480
00:32:02,720 --> 00:32:03,720
Cool.

481
00:32:03,720 --> 00:32:06,000
And how, how are the chapters structured?

482
00:32:06,000 --> 00:32:13,840
Are they different features of this, you know, you building up this Python code or do you

483
00:32:13,840 --> 00:32:18,320
assume it from the beginning and you're, you know, talking more theoretically about these

484
00:32:18,320 --> 00:32:21,440
topics, how do you organize things?

485
00:32:21,440 --> 00:32:24,120
Um, so Manning's pretty big on being practical.

486
00:32:24,120 --> 00:32:26,960
So they've, they've certainly encouraged me to stay close to the code.

487
00:32:26,960 --> 00:32:32,360
And this is one of the things, and I know that you've, and you've sponsored a number

488
00:32:32,360 --> 00:32:34,680
of sessions going to the fast AI course.

489
00:32:34,680 --> 00:32:38,920
So you've seen that ethic there of really trying stuff that, you know, you get as soon

490
00:32:38,920 --> 00:32:42,040
as you can, I try to apply it in code.

491
00:32:42,040 --> 00:32:43,800
And I've tried to do that in the book as well.

492
00:32:43,800 --> 00:32:45,560
So introduce bits and pieces.

493
00:32:45,560 --> 00:32:50,160
So talk about pandas data frames is one of the essential items, so it's been some time

494
00:32:50,160 --> 00:32:51,240
on that.

495
00:32:51,240 --> 00:32:55,400
Talk about the data set, talk about the different steps take, you need to take to clean up the

496
00:32:55,400 --> 00:33:00,360
data set, including the geocoding problem that I talked about before.

497
00:33:00,360 --> 00:33:06,720
And then a chapter on the automatic, how the, the layers of the carous model are automatically

498
00:33:06,720 --> 00:33:07,720
put together.

499
00:33:07,720 --> 00:33:11,600
And that's, that's one of the ones that's not that that I'm working on currently will

500
00:33:11,600 --> 00:33:14,600
be released a little bit later in the year.

501
00:33:14,600 --> 00:33:21,480
There'll be a, a section, as you said, talking a bit more to tail on embeddings where, what

502
00:33:21,480 --> 00:33:23,280
role they play.

503
00:33:23,280 --> 00:33:27,400
And then the other thing I really want to do, and this is, this has been quite a challenge

504
00:33:27,400 --> 00:33:32,120
is have a, is end to end a process as possible.

505
00:33:32,120 --> 00:33:35,120
So be able to talk about, here's how you deploy the model.

506
00:33:35,120 --> 00:33:40,000
And here's how you get a little, a little website that would let you pop in the description

507
00:33:40,000 --> 00:33:43,960
of a particular trip and get the prediction back whether or not that trip would incur

508
00:33:43,960 --> 00:33:45,520
a delay.

509
00:33:45,520 --> 00:33:50,000
Because that's one of the things that I found in terms of the my learning process, there

510
00:33:50,000 --> 00:33:53,840
was an awful lot that kind of took you up to the point where you had, you had trained

511
00:33:53,840 --> 00:33:57,520
the model and had some sense of its performance.

512
00:33:57,520 --> 00:34:01,600
And then things got a little bit sketchy about how would you actually deploy it?

513
00:34:01,600 --> 00:34:05,680
How would you get it into, into not even production, just get it to the point where somebody

514
00:34:05,680 --> 00:34:08,080
else could use it or play with it.

515
00:34:08,080 --> 00:34:11,200
So I want to, I definitely want to spend some time talking about some options for doing

516
00:34:11,200 --> 00:34:17,160
that and taking the reader through a particular process for deploying the model.

517
00:34:17,160 --> 00:34:19,000
Sounds awesome.

518
00:34:19,000 --> 00:34:29,160
And so the, you've kind of suggested this, but the early access process or program at Manning

519
00:34:29,160 --> 00:34:35,560
is one in which you are kind of incrementally posting chapters of the book.

520
00:34:35,560 --> 00:34:41,080
And folks can kind of buy in early and get access to these chapters, is that right?

521
00:34:41,080 --> 00:34:45,720
So they get access to the book and they get the completed book.

522
00:34:45,720 --> 00:34:48,520
The other part of it is supposed to be a two way thing as well.

523
00:34:48,520 --> 00:34:53,200
There's the opportunity to make comments and to frame the book either to say things

524
00:34:53,200 --> 00:34:58,480
that are that need to be corrected or adjusted or make recommendations for topics that need

525
00:34:58,480 --> 00:35:00,280
to be covered later on in the book.

526
00:35:00,280 --> 00:35:05,200
So people who get involved early, who take advantage of this, have a chance to really

527
00:35:05,200 --> 00:35:13,480
not just be passive participants, but also contribute and be part of making this book

528
00:35:13,480 --> 00:35:18,320
or other books that are in this program successful and meet the needs of the people who will be

529
00:35:18,320 --> 00:35:19,320
reading them.

530
00:35:19,320 --> 00:35:20,320
That's awesome.

531
00:35:20,320 --> 00:35:25,640
Yeah, one of the things that really jumps out at me in this conversation, we keep kind

532
00:35:25,640 --> 00:35:33,760
of coming back to a fast AI, I think, because I'd seen it impact a lot of people, myself

533
00:35:33,760 --> 00:35:40,200
included, and I'm seeing in this conversation how you, the echoes of that course are clear

534
00:35:40,200 --> 00:35:47,840
and what you're doing here and kind of extending what you got out of that course now into

535
00:35:47,840 --> 00:35:48,840
a book.

536
00:35:48,840 --> 00:35:57,120
And that's exactly how the course is kind of designed that you, there's a lot of self-direction

537
00:35:57,120 --> 00:35:59,720
required to go through the course and really get the most out of it.

538
00:35:59,720 --> 00:36:05,600
And a lot of that is taking what's taught and applying it to things.

539
00:36:05,600 --> 00:36:09,280
And so that is awesome to see.

540
00:36:09,280 --> 00:36:10,280
It is a great course.

541
00:36:10,280 --> 00:36:17,640
It's a fantastic opportunity, a really thing I learned so much going through it and also

542
00:36:17,640 --> 00:36:23,720
seeing, it comes up in all sorts of different contexts as well, I mean, in the sessions

543
00:36:23,720 --> 00:36:28,640
that you've sponsored, other people learning about deep learning, and I think it's not

544
00:36:28,640 --> 00:36:35,840
just the course, it's also the approach to teaching, that's saying, get to the coding

545
00:36:35,840 --> 00:36:38,680
as quickly as possible and try different stuff.

546
00:36:38,680 --> 00:36:43,720
And it's pretty bold as well, and making a number of significant changes in the platform

547
00:36:43,720 --> 00:36:49,560
being used, that's a lot of work and there's risk in that, but it got to tip the hat to

548
00:36:49,560 --> 00:36:51,040
the team that puts that course together.

549
00:36:51,040 --> 00:36:58,600
I think they've done a really, really fantastic job and had a really big impact on the industry

550
00:36:58,600 --> 00:37:01,680
and on people's lives.

551
00:37:01,680 --> 00:37:07,880
People have learned a lot, it's been a ladder up for a lot of people into a world that's

552
00:37:07,880 --> 00:37:11,080
really exciting and really interesting and really important.

553
00:37:11,080 --> 00:37:15,560
Yeah, absolutely, absolutely, and I would encourage anyone who, you know, hears us gushing

554
00:37:15,560 --> 00:37:25,240
over this course and is interested in taking the course to do so and Jeremy Howard, who

555
00:37:25,240 --> 00:37:29,960
teaches the course always says that taking the course is best done with other people,

556
00:37:29,960 --> 00:37:35,480
the people that do that are more successful, and that's really why we, with the support

557
00:37:35,480 --> 00:37:43,160
of a bunch of very dedicated volunteers have been doing, I don't even count now how many

558
00:37:43,160 --> 00:37:48,160
kind of cohorts of folks we've done through the various versions of the course.

559
00:37:48,160 --> 00:37:53,200
There are several versions and then a part one and a part two, but hundreds of people

560
00:37:53,200 --> 00:37:59,040
now have participated in our study groups to just get support in working through these

561
00:37:59,040 --> 00:38:00,040
courses.

562
00:38:00,040 --> 00:38:04,400
If, you know, that's interesting to you, encourage anyone who's listening to visit

563
00:38:04,400 --> 00:38:09,280
Twomlai.com slash Meetup, sign up for the Meetup and express interest in the study

564
00:38:09,280 --> 00:38:10,280
groups.

565
00:38:10,280 --> 00:38:16,400
And if we don't currently have one running, when you do that, you know, raise your hand

566
00:38:16,400 --> 00:38:22,960
in the Slack channel in our Slack and express some interest and, you know, we tend to kick

567
00:38:22,960 --> 00:38:26,240
these things off, you know, when folks are interested in them.

568
00:38:26,240 --> 00:38:33,760
So with that, Mark, thanks so much for taking the time to share with us about the book.

569
00:38:33,760 --> 00:38:36,480
It sounds super interesting.

570
00:38:36,480 --> 00:38:37,480
Thank you, Sam and Sam.

571
00:38:37,480 --> 00:38:38,800
I want to also thank you for the podcast.

572
00:38:38,800 --> 00:38:44,400
I've learned so much, been a faithful listener for several years now, and it's been a great

573
00:38:44,400 --> 00:38:45,400
asset.

574
00:38:45,400 --> 00:38:46,400
I learned a great deal.

575
00:38:46,400 --> 00:38:49,480
So thank you very much for all the work you do to get the podcast out.

576
00:38:49,480 --> 00:38:50,480
Really appreciate it.

577
00:38:50,480 --> 00:38:51,480
Wonderful to hear that.

578
00:38:51,480 --> 00:38:52,480
Thanks so much, Mark.

579
00:38:52,480 --> 00:38:53,480
Thank you.

580
00:38:53,480 --> 00:38:59,640
All right, everyone, that's our show for today.

581
00:38:59,640 --> 00:39:05,000
For more information about today's show, visit twomlai.com slash shows.

582
00:39:05,000 --> 00:39:10,000
Remember, less than two weeks to register for Twomlai platforms.

583
00:39:10,000 --> 00:39:13,680
So head over to twomlai.com now.

584
00:39:13,680 --> 00:39:23,680
Thanks so much for listening, and catch you next time.

