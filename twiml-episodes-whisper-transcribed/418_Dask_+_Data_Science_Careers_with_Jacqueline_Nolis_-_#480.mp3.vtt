WEBVTT

00:00.000 --> 00:17.200
All right, everyone. I am here with Jacqueline Nolis. Jacqueline is head of Data Science at Saturn Cloud.

00:17.200 --> 00:21.120
Jacqueline, welcome to the Twoma AI podcast. Thank you for having me.

00:21.920 --> 00:28.080
Hey, I'm looking forward to our conversation. We're going to touch on careers in data science as

00:28.080 --> 00:37.120
well as desk, which is a open source project that your company is working on. But before we dive in,

00:37.120 --> 00:41.440
I'd love to have you share a little bit about your background and how you came to work in data science.

00:42.320 --> 00:49.600
Sure. So I had the odd circumstance where I fell into data science, which is to say, I started

00:49.600 --> 00:54.480
years ago as an undergrad in mathematics. And then I got a master's in mathematics thinking I want

00:54.480 --> 00:58.640
to be a math professor. Then I was, no, I don't want to be a math professor. I want to use math to

00:58.640 --> 01:03.520
help businesses and stuff. So I started a job, which now today would be called a data scientist,

01:03.520 --> 01:10.080
but back then it was business analytics. And I just kind of grew, I worked at an e-commerce

01:10.080 --> 01:15.520
company doing forecasting for them. And then I worked in aerospace for a bit. And then I'm like,

01:15.520 --> 01:23.760
I want to go get more technical skills. I'm going to go get a PhD. So I got a PhD in operations research,

01:23.760 --> 01:28.800
my researchers in electric vehicle networks, like where does Tesla put their charging stations.

01:28.800 --> 01:34.160
And about this time, the field was starting to get called data science. And while I was in my PhD

01:34.160 --> 01:38.240
program, a company, like a boutique consulting firm was looking for data science consultants.

01:38.240 --> 01:41.840
And I'm like, yeah, I'll make some side cash as a PhD student, you kidding me.

01:43.040 --> 01:48.240
And then I finished my PhD and then I did consulting for full time for many years,

01:49.120 --> 01:53.200
to the point where I was a director and like started my own data science team at a consulting firm.

01:53.200 --> 01:59.440
And then I quit and started my own like freelance consulting business. And I did that for a few

01:59.440 --> 02:04.240
years. And then I said, you know, I being a freelancer and working on your own is a lot of work.

02:04.240 --> 02:09.120
And that's very stressful. Like I would like to go back into industry. And so then I ended up,

02:09.120 --> 02:14.720
now I'm here at a Saturn cloud having a lot of fun. So awesome, awesome. Yeah, I think you were doing

02:14.720 --> 02:24.000
the freelance consulting thing or maybe just at the point in transition when we first met. And

02:24.800 --> 02:31.840
the circumstance around our connection was for those who, you know, may remember your name or

02:31.840 --> 02:38.880
the panel that we did, we did a panel discussion on advancing your data science career during

02:38.880 --> 02:48.080
the pandemic just about a year ago. And that was your kind of wrapping up the independent consulting

02:48.080 --> 02:51.360
at the time. That's right. And so it was like, I was giving that, I was part of that panel.

02:51.360 --> 02:56.240
I'm like, I also am advancing my career by trying to get a job again after working independently

02:56.240 --> 03:02.160
for several years. And if I remember correctly, you spent some time talking about your career search

03:02.160 --> 03:09.120
and kind of how that evolved for you and what was, what, how the pandemic made it interesting.

03:09.120 --> 03:13.440
Yeah, because I did start looking for a job right at the beginning of the pandemic when they were

03:13.440 --> 03:17.760
like three data science jobs in the country available, which I think now there are a lot more.

03:17.760 --> 03:23.120
I think we've kind of gotten back to normal from what I've seen. But I was pretty touch and go in

03:23.120 --> 03:33.360
like, what, do you, May of 20, 28, 2020? It was a rough time. Yeah. So, you know, I mentioned

03:33.360 --> 03:39.200
we'll be talking about a couple different things. One of them is you wrote a book on data science

03:39.200 --> 03:44.240
careers. You do a podcast on data science careers and you're generally out talking about

03:44.240 --> 03:50.160
and helping folks kind of build their careers, get jobs in data science. How did that all come about?

03:50.160 --> 03:56.880
Yeah. So that's interesting. So I, you know, I started like a couple years ago going to conferences

03:56.880 --> 04:01.040
and trying to give talks. And I'm like, the thing I really like talking about is like, you know,

04:01.040 --> 04:04.640
how do you become a director? How do you get your first job? Like I kind of always like thought

04:04.640 --> 04:08.640
that stuff was not, or like how do you run a team effectively? And I always thought that stuff

04:08.640 --> 04:14.240
could be talked about more like per hour than maybe some technical topics. And at one point,

04:14.240 --> 04:18.160
I'm manning the publishing company read some blog posts I had and they're like, hey, do you want

04:18.160 --> 04:22.960
to write a book? And I'm like, do I want to write a book? You know, I never considered writing a

04:22.960 --> 04:28.080
book. But yeah, I do want to write a book. And so I reached out to Emily Robinson and we actually

04:28.080 --> 04:32.320
had met several months ago because we're both giving talks at day-to-day Texas. And I said,

04:32.320 --> 04:37.840
Emily, you want to come write a book with me? And she said, maybe give me a month. I'm switching jobs.

04:37.840 --> 04:42.560
And so then a month later, and I just switched jobs too. And then like a month later, we're like,

04:42.560 --> 04:48.320
yeah, let's do it. And so we spent like a year writing this book. And so like each chapter,

04:48.320 --> 04:52.400
like the first half of the book is like, hey, if you're not a data science test yet and you want

04:52.400 --> 04:57.520
to get a job, how do you get the skills? How do you build a portfolio? How do you write your resume?

04:57.520 --> 05:02.480
And then the second half is, okay, cool. You're a data scientist. Now what? Like how do you think

05:02.480 --> 05:07.840
about like making good analysis? How do you know? What do you do when a project is failing? Or

05:07.840 --> 05:12.960
how do you quit a job and get your next one? And so it's really fun. And so Emily and I, we

05:12.960 --> 05:17.040
wrote this book. It did pretty well. We're pretty happy with it. And we're like, we want to talk

05:17.040 --> 05:21.600
about this more. And so then we made a podcast too, where now the podcast is each week, we kind of

05:21.600 --> 05:27.600
talk about the themes of different chapters of the book. So yeah, so I guess the end of the result

05:27.600 --> 05:34.240
is like I fell into it. But also it was a really nice venue because I had been a data scientist

05:34.240 --> 05:37.920
for a bunch of years now. And I have all these thoughts. And I would just like to get them out

05:37.920 --> 05:42.480
of my head and on paper. And I could just hand the paper to other people rather than keeping it in here.

05:44.080 --> 05:50.000
So when you think about the first part of the book, targeting folks that want to get a career

05:50.000 --> 05:58.720
in data science, what are the kind of top insights that you've actually, you know, in the book or

05:58.720 --> 06:04.640
you know, since writing the book for, you know, how you go about that nowadays, you know,

06:04.640 --> 06:12.480
beyond the, the usual, which I imagine is, you know, have some portfolio projects, you know,

06:12.480 --> 06:19.280
get on LinkedIn and network, get on Twitter and network. I what do you find are the, the non-traditional

06:19.280 --> 06:23.840
or the non-obvious, you know, things that folks should be thinking about? Well, I would say,

06:24.560 --> 06:27.520
maybe I'm gonna say the obvious stuff, but I'm gonna think about it. Think about it this way,

06:27.520 --> 06:32.160
this different way, which is the data science field at the entry level. I think it's starting to get

06:32.160 --> 06:37.040
pretty saturated, which is to say, maybe like when I was looking for jobs, it was, you know, like 10

06:37.040 --> 06:41.440
years ago, it was, oh, hey, you got a math degree. Cool. Come on. Oh, come in. And now like it's so

06:41.440 --> 06:45.440
many people want to be data scientists that's getting trickier, which is not to say you can't get a

06:45.440 --> 06:51.200
job, but it's just gonna take more thought and, you know, maybe work to do that. And when people give

06:51.200 --> 06:58.160
the advice of make a portfolio, make a GitHub page, all that, what they're implicitly saying is, hey,

06:59.040 --> 07:03.360
people who don't have data science experience, like pure like data science job experience yet,

07:04.240 --> 07:09.840
companies don't know, hey, can I trust you to be a data scientist? Would you get here and know

07:09.840 --> 07:14.080
what you are doing and how to handle it? And so something like a, you know, a portfolio or

07:14.080 --> 07:19.200
GitHub page, things like that, they are letting the employer know, hey, look, no, you can tell I'm a

07:19.200 --> 07:23.680
data scientist because look at this cool side project I did or whatever. But that's not the only

07:23.680 --> 07:28.480
way you can do it. You could do something like, hey, if you're a, you know, software engineering

07:28.480 --> 07:32.560
job, you can try and start doing data science things within that job, you know, before you look

07:32.560 --> 07:38.080
for your next full data science job, or you can start by getting a data analyst job. And in that

07:38.080 --> 07:42.560
role, which is similar data science, work there for a few years, then transition to data science.

07:42.560 --> 07:48.320
So it's not like the pattern is first you make a GitHub portfolio, then they hire you. It's like

07:48.320 --> 07:53.840
yeah, it's like, think about what are the things that you can do and you know your circumstances

07:53.840 --> 07:58.480
best. What are the things that you can do that will help employers understand you are a good fit

07:58.480 --> 08:03.040
for this position. And that may not be something you can do it over a weekend. It may take years

08:03.040 --> 08:12.560
to really set that foundation to help you out. Given the, the saturation at the early stages of

08:12.560 --> 08:18.640
data science careers now are employers looking for different things or they're different ways

08:18.640 --> 08:24.560
that folks can and should be signaling that they have the required background.

08:24.560 --> 08:30.080
I think that the classic like build a portfolio, that's good. Get something in your job, that's good.

08:30.720 --> 08:37.040
Things like boot camps can also be good, but also you need to like, you need to do the work of like,

08:37.040 --> 08:40.960
hey, I've been to the boot camp, I'm going to take this job and make it like so obvious how,

08:40.960 --> 08:44.240
sorry, this side project I did as part of the boot camp make it so obvious that this is just like

08:44.240 --> 08:48.880
what you're hiring for I can do it. But I think more broadly what I would say is, well,

08:48.880 --> 08:52.640
that's kind of intimidating to know that the field's getting more tight at the entry level.

08:52.640 --> 08:57.680
I would say at the, hey, if you have nine months experience in one data science role, I think it

08:57.680 --> 09:03.760
is very easy right now to get another job, which is to say the hurdle between zero and one unit of

09:03.760 --> 09:12.000
like pure experience is massive. So it may feel really hard to get that first job, but know that once

09:12.000 --> 09:16.800
you get that first job, you stick in a frill, like just a tiny bit, it should be much easier going

09:16.800 --> 09:21.040
forward. So it really is going to like, you are at the hardest part right now, people who are just

09:21.040 --> 09:27.280
trying to get in and it should get easier. Hopefully as, you know, as the your career experience

09:27.280 --> 09:37.680
progresses. Do you find that that kind of tenure or churn in data science jobs, these early data

09:37.680 --> 09:44.880
science jobs is more or less than other areas that like software engineering? I don't know if I

09:44.880 --> 09:50.000
would say data science is more churn than software engineering. I guess it wouldn't surprise me if

09:50.000 --> 09:56.640
it was, but I don't know. Yeah, for the typical tenure for a first, you know, first data science

09:56.640 --> 10:03.520
gig. I will say, I mean, it's probably like, I would guess it's like a year and a half, two

10:03.520 --> 10:08.000
years average, which I think is probably similar to software engineering. And I think the reason

10:08.000 --> 10:12.240
why you leave data science jobs is not that different than the reason you leave software engineering

10:12.240 --> 10:15.280
jobs, right? Like you don't get along with your manager. You don't agree with a couple of these

10:15.280 --> 10:21.200
missions. Be hours aren't good. Like it's kind of like a lot similar. But I do think that

10:21.200 --> 10:26.960
that the people I know who have left after nine months of their first job, it's not like they

10:26.960 --> 10:30.080
left and then they couldn't get another job because they only had nine months experience. It's like,

10:30.800 --> 10:34.400
wow, people see nine months like, cool, you've worked somewhere and done something. Come on in,

10:34.400 --> 10:39.040
you know, like we are not a lot of experience data scientists out there. So we are hiring

10:39.040 --> 10:43.360
pretty quickly at that level. You know, and like disclaimer, what Jacqueline says is not a

10:43.360 --> 10:47.360
career guarantee. This is just what I've seen. Yeah.

10:47.360 --> 10:52.800
Yeah. That's a little bit of a contrast to some of the memes that you see like,

10:54.320 --> 10:59.040
you know, for taking desk, for example, to start our transition over that topic, you know,

10:59.920 --> 11:04.960
data scientists wanted, you know, 10 years of desk experience required, right? Well,

11:04.960 --> 11:11.200
desk hasn't been around that long. Yeah. Well, I think I think this is a separate issue,

11:11.200 --> 11:17.040
which is oftentimes the people writing job posts don't know what they actually want. And that

11:17.040 --> 11:22.560
doesn't mean just like, oh, it's an HR person who's writing this job post. It could be a data

11:22.560 --> 11:26.160
scientist who thinks that they have the ability, like, oh, I'm going to sit the bar so high because

11:26.160 --> 11:31.680
our team needs people that are like, that's fun. You could say this job requires a PhD and

11:31.680 --> 11:35.440
years experience and ask and all that and you will find no one. And then what are you going to do?

11:35.440 --> 11:38.720
Well, you're going to drop it to a reasonable level and then you will find people, you know, like

11:39.520 --> 11:43.360
the fact that those terrible job posts exist is not an indication that the field actually

11:43.360 --> 11:47.440
requires that stuff, not only in an indication that a lot of people don't actually have realistic

11:47.440 --> 11:54.720
expectations for what they should get from an employee. You mentioned that the second part of the

11:54.720 --> 11:59.680
book talks about like why projects fail and what to do if projects are failing, you know,

11:59.680 --> 12:05.280
what are the biggest questions you get from folks that are that are, you know, data scientists

12:05.280 --> 12:09.280
and are at that stage of their career where they're trying to navigate the role.

12:09.280 --> 12:16.160
So I will say on that failure thing in particular. So I give that as a talk, and I've done it a

12:16.160 --> 12:23.440
couple of times at a couple of different places. And when I give the talk on failure, there are

12:23.440 --> 12:27.280
some questions, but in general, what happens after the talk is I get lots of people come up to me

12:27.280 --> 12:32.480
and they're like, oh, thank God, someone else feels the things I feel, which is to say that

12:32.480 --> 12:37.040
especially as a more junior data scientist, when your stuff doesn't work, right? If someone says,

12:37.040 --> 12:41.200
hey, go build a churn model on that customer data and you try and build it and you just can't

12:41.200 --> 12:44.880
get it to work because there's just not enough signal there. It's very easy as a data scientist

12:44.880 --> 12:50.080
to be like, ah, I knew more skills. If I were a better data scientist, I could have made that churn

12:50.080 --> 12:56.640
model actually fit. When in practice, probably that data just has no actual signal in it. So no matter

12:56.640 --> 13:01.360
how experienced of a data scientist, you are, that model won't fit. And I think that that's kind of

13:01.360 --> 13:06.800
a problem for lots of junior data scientists in lots of different ways, which is you have expectations

13:06.800 --> 13:11.120
that the amount of knowledge and skill you have is an indication of how well you will do. But

13:11.120 --> 13:17.040
things like the data isn't actually have a signal in it or your stakeholders don't actually know

13:17.040 --> 13:20.000
what they want this product to do. They're just asking you to build models because they think that's

13:20.000 --> 13:23.600
a good idea. And then when you build them, they're like, wait, what do I do with this? Like,

13:23.600 --> 13:28.240
none of that stuff has to do with your technical abilities and knowing how to like draw that like

13:28.240 --> 13:33.840
emotional boundary and be like, okay, this project failed, but it's not because of me or maybe it

13:33.840 --> 13:40.080
is like related to some stuff that I had to do. But like, what are the things I can take us notes

13:40.080 --> 13:44.160
for the next time I do this project as opposed to I should have known that stuff in the first place

13:44.160 --> 13:48.880
even though this is only my ninth month on the job. That stuff's really important and I don't think

13:48.880 --> 13:56.640
it's talked about very much in data science. You know, I imagine when you first came into the field,

13:56.640 --> 14:02.480
you know, we were certainly a lot closer to this time where, you know, we the data scientist was

14:02.480 --> 14:08.880
also, you know, almost, you know, often preceded by mythical like the unicorn data scientist like the

14:10.640 --> 14:17.200
you know, the someone who knows math, knows stats, can program, can deploy, can do all these things.

14:17.200 --> 14:22.960
And over the past several years, as a field's gotten more mature, there's a lot more

14:22.960 --> 14:33.440
specificity in roles and also roles of kind of bifurcated into now you've got folks that are,

14:33.440 --> 14:38.320
you know, machine learning engineers that are focused on deployment. I'm wondering how,

14:39.200 --> 14:47.200
you know, the evolution of the role has changed or, you know, would change the advice you give to

14:47.200 --> 14:54.800
someone who is, you know, joining the field or early on in their career. Do you, is that something

14:54.800 --> 15:00.240
that kind of impacts the way you you talk to folks and advise folks? Yeah, kind of. I do think,

15:01.040 --> 15:06.000
yeah, I think when I started, there were no, no one had any expectation of data scientists because

15:06.000 --> 15:10.240
they didn't really barely existed. And then I agree they got to the point where your point of,

15:10.240 --> 15:14.640
now every data scientist should do everything. And now we've gotten smarter about that. I think

15:14.640 --> 15:20.800
what I would tell a people entering the field is it is helpful, I think, to have a basic idea of

15:20.800 --> 15:25.440
which of these things you'd rather do. Would you rather build models that are continuously running

15:25.440 --> 15:30.400
that like get hit like APIs and help customers like a recommendation model? Or would you rather be

15:30.400 --> 15:35.840
using, would you rather be using data to help a business make a decision? And like you're,

15:35.840 --> 15:39.200
what you're delivering is a PowerPoint, but it's a PowerPoint filled with interesting ideas that

15:39.200 --> 15:43.040
you found out from data. Or do you want to do just analytics, not just, but do you want to do

15:43.040 --> 15:47.360
analytics, which is like, hey, I'm going to take data from a database, put it in an easy to use

15:47.360 --> 15:52.400
dashboard, get it to you so it's easily digestible, but like just making data easy to use.

15:52.400 --> 15:55.520
Like think about what are the kinds of things that sound the most interesting to you?

15:55.520 --> 15:58.320
That would be the first thing I'd say. And then the second thing I would say is,

15:58.320 --> 16:03.200
but don't feel like you are locked into that and don't feel like you have to, you have to like,

16:03.200 --> 16:08.880
make a full decision before you start as a data scientist, which is to say like one, you may think,

16:08.880 --> 16:12.560
oh, I really want to do machine learning, but then you'd happen to find a job that's your first

16:12.560 --> 16:15.920
jobs, the more decision science, I'm going to make decisions based on this data. You might like

16:15.920 --> 16:19.760
that. And you don't know until you really try these jobs, which ones you like and won't like.

16:19.760 --> 16:22.880
So don't feel like if it's not the one you signed up for, you will not enjoy it.

16:23.680 --> 16:28.640
And then second, it is entirely possible to change. I spent the first 10 years of my career

16:28.640 --> 16:32.720
doing decision science stuff, especially as a consultant, which is give me your data. I will

16:32.720 --> 16:35.840
give you insights out of it. And then in the last couple of years, I've switched to machine

16:35.840 --> 16:40.080
learning, which is, hey, I'm going to build models that continuously run as Docker containers,

16:40.080 --> 16:45.040
blah, blah, blah. And like, was that switch easy? Not really. I mean, it took work, but like,

16:45.040 --> 16:49.440
I could do it. I think other people can do it. And I think we kind of have this weird gatekeeping

16:49.440 --> 16:54.800
as a field of like, oh, you're just a decision scientist. You can't do this stuff. You never will.

16:54.800 --> 16:58.560
And it's like, no, anyone can do any of those stuff. It's just a matter of getting the experience

16:58.560 --> 17:04.880
to learn how to do it. Yeah, yeah. So on the topic of machine learning and Docker containers and

17:04.880 --> 17:13.200
all that jazz, you know, what is, maybe let's start with Dask. What is Dask? Yeah. So, um,

17:14.320 --> 17:19.840
so Dask is basically, here's how I think about Dask, right? You have you're a Python user. You

17:19.840 --> 17:23.440
write your Python code. It's running on your local machine. And you're like, man, I really wish

17:23.440 --> 17:28.880
this Python code was running on a distributed set of machines really easily, right? So Dask is

17:28.880 --> 17:33.920
that tool, which is to say, if you have an operation that could really easily be switched to running

17:33.920 --> 17:38.800
on a bunch of computers at once, Dask can help you do that. Like one instance is, let's say you have

17:38.800 --> 17:43.600
a for loop. And in that for loop, it has a big computation. And it goes up for loop a thousand times.

17:43.600 --> 17:48.000
Why not have 10 computers taking each parts of those for loops and doing them all at once?

17:48.000 --> 17:52.560
Dask will help you do that. Dask also has a lot of built-in libraries and stuff. So you don't even

17:52.560 --> 17:56.480
have to think about how do you distribute this stuff out. You could just do something where it feels

17:56.480 --> 18:00.480
like you're doing normal pandas, but secretly on the back end, the calculations are being farmed

18:00.480 --> 18:06.320
out to this cluster of stuff. So Dask is an open source tool. It's right now. It's kind of like

18:06.320 --> 18:11.520
the alternative you'd use as Spark. But Spark is like, it's written in Scala. It's on the JVM.

18:11.520 --> 18:16.400
It's a hassle set up. And like, Dask is really like, hey, you got Python code, take your Python

18:16.400 --> 18:22.400
code, run it on a bunch of things at once. Nice and easy. Don't have to worry about it. And so Dask is

18:22.400 --> 18:28.080
an open source tool that's been developed since like 2018. And Saturn Cloud, the company I'm at now,

18:28.080 --> 18:33.200
one of the things we do is we make hosted Dask easy use. So if you are doing stuff on your laptop

18:33.200 --> 18:36.640
and you're like, I wish I want to, I wish this was running on 100 computers in the cloud,

18:36.640 --> 18:40.800
you can like in like three clicks have Saturn Cloud be the place where all those computers live.

18:42.320 --> 18:46.800
And it's a really cool tool. I think it's very neat. And I think I am surprised more tools like

18:46.800 --> 18:51.360
this don't exist in other languages and stuff like it just seems such like an intuitive thing

18:51.360 --> 18:54.720
that it should exist. And so I'm just very happy that Dask exists.

18:54.720 --> 19:06.000
And so when you talk about the for loop example, it echoes both stuff that was happening in

19:06.000 --> 19:11.760
high performance computing where you have to be very aware of the program and whether it's

19:11.760 --> 19:19.840
data parallel versus computation parallel, things like that. And on the other side,

19:19.840 --> 19:25.680
infrastructure type things like Kubernetes and distributed computing and things like that.

19:26.880 --> 19:31.200
It sounds like Dask is maybe a little bit of both. Yeah. And okay. So

19:32.080 --> 19:35.840
people who have ever heard my podcast and I think I'm very strongly opinionated about things.

19:36.480 --> 19:41.840
I think that as a data scientist, you should not have to worry about your Kubernetes cluster.

19:41.840 --> 19:45.840
And like, oh no, is that port open on that one work? Like there's a level of abstraction.

19:45.840 --> 19:50.560
You as a data scientist should not have to worry about. You should just be able to use these tools.

19:50.560 --> 19:55.440
I also think that there is for some whatever reason when people talk about the Shabie computing,

19:55.440 --> 19:59.040
it is one of those topics where people love to make it unnecessarily complicated.

19:59.040 --> 20:02.000
Right? Like, ah, but is it a blah blah blah type or a blah blah type for you? Yeah.

20:02.000 --> 20:06.000
And it's like, look, all I do, I have this, I have, it's a for loop. I would like each one of

20:06.000 --> 20:10.640
those for loops to happen all at the same time. Just like that shouldn't be that complicated.

20:10.640 --> 20:14.320
And I think Dask is one of those tools that makes it less complicated to do that.

20:14.320 --> 20:18.320
As opposed to sometimes people would be like, you know, there are some tools that make it more

20:18.320 --> 20:22.000
complicated. I think another great example. I would like for their whole phase with Hadoop where you

20:22.000 --> 20:26.560
had to take that for loop, turn it into a map and a reduce and all that stuff. Yeah.

20:26.560 --> 20:30.240
In order to distribute it. Yeah. And like, this is really a place where it'd be easy to do that

20:30.240 --> 20:35.040
abstraction. And Hadoop did, ah, here's a confession I have, which is when I was a junior data scientist

20:35.040 --> 20:39.600
in like 2012 or whatever, Hadoop was becoming really big. And it was like, oh, you're a

20:39.600 --> 20:44.560
tech scientist, but you work with big data. And I was always like, no, I always work with like 200,000

20:44.560 --> 20:48.480
rows and like linear aggressions and, you know, stuff. And like, I felt really ashamed. I'm like,

20:48.480 --> 20:52.800
should I go learn to do? Should I go learn to do? And thankfully, I never learned to do. And

20:52.800 --> 20:57.040
now I don't need to because people don't really think about that. We have tools like Dask and

20:57.040 --> 21:00.720
Spark that handle that layer of abstraction, which is say, if you're a data scientist and you feel

21:00.720 --> 21:04.640
like, oh, no, I don't use this tool yet. But I feel like I should be using this tool to stay

21:04.640 --> 21:08.800
ahead of the, you know, stay on the curve. You never know that tool might go away by the time.

21:08.800 --> 21:12.560
And it would be simple enough that you don't need that. Another place where I think this happens

21:12.560 --> 21:16.720
a lot is with, I think neural networks for the longest time where like, oh, do you have,

21:16.720 --> 21:20.720
is it a blog layer of this kind of abstraction? And like, like, here's a really complicated,

21:20.720 --> 21:24.480
crazy diagram. And here's eight equations to talk about like neural networks. And like,

21:24.480 --> 21:28.080
you see this, right? You see these like easy to use neural network cheat sheets on LinkedIn that

21:28.080 --> 21:34.080
are just like pages of math equations. And that's crazy. Like a talk I give is literally just,

21:34.080 --> 21:40.720
hey, you can learn how to use do neural networks and are in like 20 PowerPoint slides. And

21:40.720 --> 21:44.880
it's going to be PowerPoint slides filled with pictures of pets. And we're going to be training

21:44.880 --> 21:48.160
a neural network to generate pet names. Like that's a talk I give. And people are like, wow,

21:48.160 --> 21:53.600
I didn't know neural networks are this easy. And I say this so passionately because I did for

21:53.600 --> 21:56.400
the longest time didn't feel like I can learn neural networks because I thought they were too

21:56.400 --> 22:00.240
complicated. And then once I learned them, I'm like, are you kidding me? This is nothing. I could

22:00.240 --> 22:04.720
learn this easily. Other people could too. And I think desk and distributed computing is a thing

22:04.720 --> 22:09.840
that more people could do if we made this easier to understand as opposed to like, look how smart I

22:09.840 --> 22:17.120
am for understanding this complicated thing. Yeah. Yeah. Yeah. So what's the what's the user experience

22:17.120 --> 22:25.360
for a desk user? Are they is it totally transparent? Is it transparent depending on what you're trying to

22:25.360 --> 22:31.840
do? Do you have to, you know, annotate or decorate your functions or like what does the data

22:31.840 --> 22:38.480
scientists need to do to take advantage of desk? So there are a couple different ways you can use it.

22:38.480 --> 22:43.440
So the way I personally like to use it is it is just a decorator on your functions, which is like

22:43.440 --> 22:48.560
you say, hey, does it delayed function? Don't run it right away. And then you create like a desk,

22:48.560 --> 22:52.640
you start your desk cluster, which may be inside our cloud, may just be on your PC, whatever,

22:52.640 --> 22:58.800
like you start a desk cluster. And then you say, hey, use that cluster to map that function or,

22:58.800 --> 23:04.160
you know, basically run those functions on this list in that cluster. Or, you know, there's

23:04.160 --> 23:11.280
other simple operations. Another way you can use desk is like, desk has kind of wrappers. So

23:11.280 --> 23:16.560
there is like a instead of using like PD for pandas, you use like DD for desk and then you use

23:16.560 --> 23:22.080
the same pandas commands. But what that's doing is it for you is doing that figuring out how to make

23:22.080 --> 23:25.520
it into a list and sending that all to the clusters or whatever, like it's doing all that work of

23:25.520 --> 23:30.480
turning it into the distributed code. And so like there's the pandas version of that. There's,

23:31.280 --> 23:36.960
you know, like I think scikit learn, like there's a lot of different packages and stuff that have,

23:36.960 --> 23:41.600
okay, if you want to do desk for your distributed backend, you can do that too. And those are kind

23:41.600 --> 23:46.400
of the different ways you can run it. And I will say you can, like I said, you can on your laptop

23:46.400 --> 23:53.280
using desk and then call like send it over to the satire cloud servers. The one complexity of that

23:53.280 --> 23:58.640
is like if you're using Python 3.6 and that desk cluster is all using Python 3.8 and stuff like

23:58.640 --> 24:03.280
you can run into a little bit of issues. So what we do at satire cloud is you can also we host

24:03.280 --> 24:07.840
Jupiter. So you can be using a Jupiter that has like guarantees that that Jupiter will be consistent

24:07.840 --> 24:13.040
with that desk cluster. Just as like another potential way to use this tool, which is to say there's

24:13.040 --> 24:18.160
a lot of different ways you can try and run this stuff. And you know, a satire cloud, a real

24:18.160 --> 24:22.960
design philosophy is like we don't want to lock you into one way of doing things. You know, I feel

24:22.960 --> 24:28.960
like some like data science tools are like, okay, like you want to use get sure, but you can only push

24:28.960 --> 24:32.560
and you can only commit using this one button. And if you want to do anything complicated, like

24:32.560 --> 24:36.160
branching or all these things you're used to, you won't let you because we're locking you down.

24:36.160 --> 24:41.520
And I don't think those tools usually succeed. Like usually you want your tools to meet data scientists

24:41.520 --> 24:53.120
where they are. On that topic, I guess maybe a couple of ways to go into this conversation,

24:53.120 --> 25:01.040
like where do you think data scientists are with regard to the, you know, the kind of the software

25:01.040 --> 25:06.240
development tool chain? Like that's been something that's been evolving over the past few years.

25:06.240 --> 25:11.360
There was a point in time which Git was a foreign entity to a lot of data scientists. Now it's

25:11.360 --> 25:18.000
becoming more common for data scientists to use Git. And I often see it varies from organization

25:18.000 --> 25:22.960
to organization, you know, some organizations will want all of their data scientists to use Git

25:22.960 --> 25:28.480
and IDE's others are, you know, hey, we're going to figure out how to make the Jupiter notebook

25:28.480 --> 25:37.040
into the Uber IDE and, you know, production allows notebooks and things like that. Any, any observations

25:37.040 --> 25:43.120
on that? Yeah, I would say I've seen a remarkable like compared to like six years ago or

25:43.120 --> 25:48.720
remarkable like gathering around Git. I think six, six, eight years ago, you could be like, we're a

25:48.720 --> 25:54.000
data science team, but all of our code shared in a share point, you know, and like that would,

25:54.000 --> 26:00.960
I've seen it like you laugh, but I've seen it. It might have been a network drive, but it was like

26:00.960 --> 26:06.480
the data wasn't sure. It was not like a good time. I think like the message is now out that like if

26:06.480 --> 26:11.120
you're a data science team and you're not using Git and version control on your stuff that that's

26:11.120 --> 26:15.360
like you, you have a problem. I think that message has successfully gone out. I also do think in

26:15.360 --> 26:19.200
the last couple of years it's becoming more common for data science teams to use things like GitHub

26:19.200 --> 26:25.760
actions, you know, we're docker. And you know, like I think I think data scientists are like like

26:25.760 --> 26:29.040
we're doing a good job of making this stuff more accessible so you don't have to be a software

26:29.040 --> 26:36.560
engineer to use software engineer tools. And I think that's really great. But that said, I also

26:36.560 --> 26:40.880
don't feel like if you don't know those tools, that means you can't be a data scientist merely that

26:40.880 --> 26:46.000
like usually like when you're on your first job, you will pick up a lot of these things. Like maybe

26:46.000 --> 26:49.680
get you should probably learn Git before you like hunt your first job, but beyond that things like

26:49.680 --> 26:54.320
GitHub actions, docker, all that stuff like you don't need to know that when you're applying for

26:54.320 --> 27:00.160
jobs, a job should be teaching you how to use those tools if they need them. Yeah, yeah. And it is

27:00.880 --> 27:10.080
super important to realize that learning Git means some basic things. And it's there's so much to

27:10.080 --> 27:18.800
there's so much to like really, really, really learning Git. But I think a lot of us know enough

27:18.800 --> 27:23.920
Git that we can do the usual things. And if we get into trouble, then it's stack overflow with

27:23.920 --> 27:29.680
everyone else and trying to figure out how to rebase your branches and crazy stuff. Yeah, I've

27:29.680 --> 27:34.720
been using Git. God, I learned it like 2012, like 10 years. And yeah, I feel like this year,

27:34.720 --> 27:40.560
year 10, I'm like, okay, I kind of feel like I get like, yeah, rebasing and stuff. Like I feel

27:40.560 --> 27:45.520
like I kind of like getting more. So like if you've been doing this for less than 10 years,

27:45.520 --> 27:48.960
don't feel like you should know that. And if you've been doing it for 10 years, like,

27:49.360 --> 27:56.160
still probably fine. Yeah, yeah. And you mentioned docker and containers,

27:56.160 --> 28:01.280
Kubernetes, those of all come up in this conversation. What's the relationship between those and

28:01.280 --> 28:08.000
desk? So, um, well, with desk, you can run desk like in a Kubernetes cluster. Like there are ways

28:08.000 --> 28:14.960
to like have that all work. Um, we at sat and clown kind of managed that all for you. So basically,

28:14.960 --> 28:22.080
you just specify, I want this AWS image size. And I want this base Docker image. And then it says,

28:22.080 --> 28:27.120
cool, you know, I have a desk cluster of that. Um, as, but if you don't know docker, it doesn't

28:27.120 --> 28:31.360
really matter that you don't, you don't need to know docker to know, I need this particular base

28:31.360 --> 28:36.640
image. But if you're like, I want to specify exactly what's on these desk clusters when they start.

28:36.640 --> 28:40.960
Like that is a thing with sat and cloud, you'd specify through like a Docker container or,

28:40.960 --> 28:46.480
you know, a post build file. And I think there is, I do think data scientist, regardless of type

28:46.480 --> 28:51.120
of data scientists, it is a good thing to learn, like a little bit of Linux commands. Like what does

28:51.120 --> 28:55.440
pseudo apt get mean? What does, you know, blah, blah, blah, because you start to learn that, then you

28:55.440 --> 29:00.800
can pretty easily make Docker files and Docker containers. And, you know, do like kind of speak

29:00.800 --> 29:04.800
the common language of a lot of these tools. Um, so I would recommend learning that kind of stuff.

29:04.800 --> 29:08.880
But again, I don't think that's something you need to learn. Um, before you get a job that,

29:08.880 --> 29:12.960
that really, I would consider a skill you can learn while you are, you know, growing in your career.

29:14.400 --> 29:20.160
And so does this, is Saturn based on Kubernetes and all that stuff? Or is it using its own

29:20.160 --> 29:28.960
Pixie dust under like under the covers? I am not on the engineering team. So I, I don't want to say,

29:28.960 --> 29:33.200
because I know I'm going to get it slightly wrong, but I believe we do have a Kubernetes cluster.

29:33.200 --> 29:37.360
And like, well, what we usually do is we, you know, because a lot of times we are being hired by

29:37.360 --> 29:41.760
enterprise to like run, like an enterprise doesn't want to have their data pass all the way to some

29:41.760 --> 29:48.160
other persons, AWS, you know, whatever. So we install Saturn cloud in your company's environment.

29:48.160 --> 29:52.240
And I think we set up one Kubernetes cluster within that like BPC or whatever.

29:52.960 --> 29:58.240
But I'm just spouting words. I don't know exactly the intricacies of this. I'm much more on the

29:58.240 --> 30:03.040
using Dask than setting it upside. Got it. Yeah. And we, I think we, we skip right by that.

30:03.040 --> 30:06.400
What is your specific role at the company? What are you focused on?

30:06.400 --> 30:10.560
Oh, yeah. So as head of data science, I help the data science team. So we do things like,

30:11.360 --> 30:16.400
you know, helping customers get up to speed with Dask and Saturn cloud.

30:16.400 --> 30:19.920
We also, you know, develop white papers, blog posts, things like that.

30:19.920 --> 30:24.320
So like we're kind of like the internal team that uses this tool and helps other people use this tool.

30:24.800 --> 30:29.040
And in addition to that, it's a lot of product development too, because this is a tool for data scientists.

30:29.040 --> 30:32.480
So if we're using this product and we're like, oh, we find this part of it, you know,

30:32.480 --> 30:37.040
complex or unintuitive, then we are the ones to kind of say, hey, we should be prioritizing this kind

30:37.040 --> 30:43.520
of solution. Which is again, it's nice because I think compared to a lot of technical products for

30:43.520 --> 30:47.840
data scientists are still built by software engineers. And so you kind of get that feel of like,

30:47.840 --> 30:52.160
are they? I'm not sure the people designing this understand what data scientists do every day.

30:52.160 --> 30:55.920
And I do not feel that that was the case with our product. I feel like, you know, well,

30:55.920 --> 31:00.640
I like to think I'm doing my job, which is making that easy. But, you know, who's, who's to say,

31:00.640 --> 31:13.040
really? Is there a particular use case or a set of use cases that you see more commonly

31:15.280 --> 31:22.080
among desk users? Are there, are there, you know, are there specific

31:23.360 --> 31:27.600
signals or indicators that say, hey, this is probably going to be a good place to apply to ask?

31:27.600 --> 31:33.600
Yeah. So I think that, I mean, it's kind of the same as like any parallel distributed computing,

31:33.600 --> 31:38.000
which is if you're running out of memory, because one machine isn't enough,

31:38.800 --> 31:42.880
then desk is a good solution because it can, it could spin up to arbitrary amounts of memory,

31:42.880 --> 31:46.640
right? Because everything is split over a bunch of computers. So if you're running out of memory,

31:46.640 --> 31:51.200
that's a good sign you need to ask. Or if you have one, like one thing that's running,

31:51.200 --> 31:55.440
but it's taking forever. And it, you know, you're like, man, I could probably chop this up into

31:55.440 --> 32:00.560
smaller tasks. Then desk is great, because you can, if you can swap, chop it up into smaller tasks,

32:02.000 --> 32:07.040
you can then put it in a desk cluster. And including like most common pandas commands and stuff,

32:07.040 --> 32:12.320
pandas, you can switch pandas to desk and then use these sorts of tools. So that stuff's a

32:12.320 --> 32:17.440
pretty good sign. Also, I will say, you know, with Saturn Cloud, we have it all set up to use GPUs

32:17.440 --> 32:22.480
just as easily. Like with no setup, it's just you hit go in the GPU CUDA drivers or whatever

32:22.480 --> 32:26.640
already correctly installed and stuff. So, you know, if you're finding, you're like, hey,

32:26.640 --> 32:30.880
I need a train 10 neural networks and compare how well they do. Well, like wouldn't be nice to train

32:30.880 --> 32:36.480
all those 10 at the same time at one desk cluster. Or, you know, you can, in fact, in PyTorch and

32:36.480 --> 32:41.440
stuff say, hey, I have this one neural network and this neural network is too big for one. Even a

32:41.440 --> 32:47.520
computer with several GPUs is not enough GPUs for this, this neural network to train. And so you can

32:47.520 --> 32:51.760
get a desk cluster where each computer has a bunch of GPUs and like you connect all two. I don't

32:51.760 --> 32:55.680
think that's a common use case. But like, you know what, if you're there, if you're like, I just

32:55.680 --> 33:01.200
do not have GPUs running at the same time to do this kind of work. And I think desk is a really

33:01.200 --> 33:10.160
nice solution for a lot of those. And is desk equal useful for folks that are doing kind of

33:10.160 --> 33:17.360
traditional tabular data and analytics types of workloads as opposed to neural nets,

33:17.360 --> 33:22.320
media files, that kind of thing, images. Yeah, I think it's more one or the other.

33:22.960 --> 33:26.400
No, you can do it. If you're not doing like, you know, networking stuff, that's still totally

33:26.400 --> 33:32.000
useful. It's, it's, you know, there is a penalty. If you're switching stuff to like distributed

33:32.000 --> 33:36.880
parallel thinking, there's a penalty if it's going to take you a little bit of time to like rewrite,

33:36.880 --> 33:41.440
you know, rethink the code of like, hey, this one for loop. Okay, yes, I can split it or whatever.

33:41.440 --> 33:45.120
You know, can I spend this for like, you got to put a little thought into it. And I think, you

33:45.120 --> 33:49.520
know, if you're training on like a 50 kilobyte file, then like, I don't think you're going to get

33:49.520 --> 33:54.240
much benefit out of the effort of switching to desk. But I do think if there's a lot of situations

33:54.240 --> 33:58.720
where it's like, I have a huge data set. And yeah, what I'm doing is training a linear regression.

33:58.720 --> 34:02.400
You know, I have a huge data set, but what I'm doing is simple. Even in that case, because the data

34:02.400 --> 34:08.400
is so big, it is still faster or more convenient to just have it go on desk, rather than trying to

34:08.400 --> 34:17.760
get it to take forever on one machine. Is it used equally in the data processing

34:19.680 --> 34:26.400
transformation part of the workflow or exploratory analysis part of the workflow,

34:27.440 --> 34:32.640
training part of the workflow? Is there any particular sweet spot or? Yeah, so so a couple good

34:32.640 --> 34:36.800
a couple good places training models can be very good on desk because you can train a bunch of

34:36.800 --> 34:42.000
models at the same time or you can train one model across a bunch of them. And we have customers

34:42.000 --> 34:48.000
who do that at Saturn cloud. Another thing that's really useful for is, yeah, like a scheduled daily

34:48.000 --> 34:52.560
processing job, right? Where if every day you're taking your entire company's data and you're trying

34:52.560 --> 34:57.360
to calculate some aggregated statistics and then put that into a new table, you can schedule that

34:57.360 --> 35:02.000
to run every day and have it scheduled to run on a desk cluster and then it will be faster and you

35:02.000 --> 35:07.200
won't run out of memory and things like that. And then at Saturn cloud, we have customers who

35:07.760 --> 35:12.400
don't use desk at all. They just like the ability to just start and stop data science instances

35:12.400 --> 35:16.880
and start and stop GPU instances and start and stop instances that have like 500 gigs of RAM.

35:16.880 --> 35:22.400
So just having for us, just having the ability to have one computer easily accessible in the cloud

35:22.400 --> 35:28.640
has been useful. But that is all just to say that there's a lot of utility in having these

35:28.640 --> 35:32.320
sorts of data science resources readily available for data scientists.

35:34.320 --> 35:44.400
Cool. Any thoughts or thoughts on where the tool is going and what folks should look out for?

35:46.160 --> 35:51.920
So I think it's just one of these things where it only came out in 2018. So it's still quite young,

35:51.920 --> 35:57.120
which is just to say I think there's going to be a lot more taking other packages and

35:57.120 --> 36:03.600
getting them to run within desk directly. I think, you know, continuously making the tool easier

36:03.600 --> 36:10.080
to use, creating new documentation, things like that. And you know, so Saturn cloud uses desk,

36:10.080 --> 36:14.480
but we're not the only company that does. And so there's lots of people contributing to this,

36:14.480 --> 36:18.480
which is kind of cool. I've never worked at a company where we were so directly involved in the

36:18.480 --> 36:22.480
open source community. And that's been, I know it's been a lot of fun. I've been really happy with that.

36:22.480 --> 36:29.040
Mm-hmm. Nice. Nice. Well, Jacqueline, thanks so much for taking the time to chat with us and

36:29.040 --> 36:33.840
bring us up to speed on desk. No problem. No, it's been a lot of fun being back on the show.

36:33.840 --> 36:37.840
Yeah. And now at what point do we become friend of the show, Jacqueline Nollis? Is it like one more?

36:40.880 --> 36:43.680
You are friend of the show, Jacqueline Nollis. Absolutely.

36:43.680 --> 36:53.520
Great. Well, yes, thank you for having me. Thank you.

