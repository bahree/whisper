WEBVTT

00:00.000 --> 00:16.120
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:16.120 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.440
I'm your host Sam Charrington.

00:32.440 --> 00:33.840
Contest alert.

00:33.840 --> 00:38.960
This week we have a jam-packed intro, including a new contest we're launching.

00:38.960 --> 00:43.040
So please bear with me, you don't want to miss this one.

00:43.040 --> 00:46.720
First, a bit about this week's shows.

00:46.720 --> 00:51.440
As you may know, I spent a few days at CES earlier this month.

00:51.440 --> 00:56.400
While there, I spoke with a bunch of folks applying AI in the consumer electronics industry,

00:56.400 --> 01:01.040
and I'm including you in those conversations via this series of shows.

01:01.040 --> 01:05.440
Stay tuned as we explore some of the very cool ways that machine learning and AI are being

01:05.440 --> 01:08.600
used to enhance our everyday lives.

01:08.600 --> 01:13.680
This includes work being done at Anki, who built Cosmo, the cutest little computer vision-powered

01:13.680 --> 01:15.280
robot.

01:15.280 --> 01:22.120
Nighthouse, whose smart home security camera combines 3D sensing with deep learning and NLP.

01:22.120 --> 01:28.120
Intel, who's using the single-shot multi-box image detection algorithm to personalize video

01:28.120 --> 01:31.680
fees for the Ferrari Challenge North America.

01:31.680 --> 01:36.440
First beat, a company whose machine learning algorithms analyzed your heartbeat data to

01:36.440 --> 01:42.360
provide personalized insights into stress, exercise, and sleep patterns.

01:42.360 --> 01:48.400
3AI and Koito, who have partnered to bring machine learning-based adaptive driving beams

01:48.400 --> 01:52.280
or automatically adjusting high beams to the U.S.

01:52.280 --> 01:59.480
And last but not least, aerial.ai, who applies sophisticated analytics to Wi-Fi signals to

01:59.480 --> 02:05.640
enable some really interesting home automation and healthcare applications.

02:05.640 --> 02:11.000
Now as if six amazing interviews wasn't enough, a few of these companies have been so

02:11.000 --> 02:15.520
kind as to provide us with products for you, the Twimmel community.

02:15.520 --> 02:19.360
And keeping with the theme of this series, our contest will be a little different this

02:19.360 --> 02:20.360
time.

02:20.360 --> 02:25.400
To enter, we want to hear from you about the role AI is playing in your home and personal

02:25.400 --> 02:28.640
life, and where you see it going.

02:28.640 --> 02:36.200
Just head on over to twimmelai.com slash myaicontest, fire up your webcam or smartphone camera,

02:36.200 --> 02:39.120
and tell us your story in two minutes or less.

02:39.120 --> 02:43.600
Go post the videos to YouTube, and the video with the most likes wins their choice of

02:43.600 --> 02:50.480
great prizes, including an Anki Cosmo, a lighthouse smart home camera, and more.

02:50.480 --> 02:55.040
Submissions will be taken until February 11th, and voting will remain open until February

02:55.040 --> 02:56.040
18th.

02:56.040 --> 03:02.360
Good luck.

03:02.360 --> 03:06.560
Before we dive into today's show, I'd like to thank our friends at Intel AI for their

03:06.560 --> 03:09.520
continued support of this podcast.

03:09.520 --> 03:14.760
Intel was extremely active at this year's CES, with a bunch of AI autonomous driving

03:14.760 --> 03:17.200
and VR related announcements.

03:17.200 --> 03:21.440
One of the more interesting partnerships they announced was a collaboration with the Ferrari

03:21.440 --> 03:24.720
Challenge North America race series.

03:24.720 --> 03:29.440
Along with the folks at Ferrari Challenge, Intel AI aspires to make the race viewing experience

03:29.440 --> 03:35.360
more personalized by using deep computer vision to detect and monitor individual race

03:35.360 --> 03:40.640
cars via camera feeds and allow viewers to choose the specific cars feeds that they'd

03:40.640 --> 03:42.200
like to watch.

03:42.200 --> 03:46.960
Look for my conversation with Intel's Andy Keller and Emil Chindicki earlier in this series

03:46.960 --> 03:52.920
for an in-depth discussion about this project, and be sure to visit ai.intel.com where you'll

03:52.920 --> 03:56.200
find Andy's technical blog post on the topic.

03:56.200 --> 03:58.880
Now about today's show.

03:58.880 --> 04:05.440
In this episode, I'm joined by Stuart Feffer, co-founder and CEO of Reality AI, which provides

04:05.440 --> 04:11.240
tools and services for engineers working with sensors and signals, and Brady Tsai, business

04:11.240 --> 04:17.680
development manager at Kui Tu, which develops automotive lighting solutions for car manufacturers.

04:17.680 --> 04:22.360
Stuart and Brady joined me at CES a few weeks ago after they announced a partnership to bring

04:22.360 --> 04:27.760
the adaptive driving beam or ADB headlights to North America.

04:27.760 --> 04:33.000
Brady explains what exactly ADB technology is and how it works, while Stuart walks me

04:33.000 --> 04:38.400
through the technical aspects not only of this partnership, but of the reality AI platform

04:38.400 --> 04:40.040
as a whole.

04:40.040 --> 04:44.400
And now on to the show.

04:44.400 --> 04:53.160
Hey everyone, we are here at CES and I am with Stuart Feffer of Reality AI and Brady

04:53.160 --> 04:56.080
Tsai of Kui Tu.

04:56.080 --> 04:59.200
Stuart and Brady, welcome to this week in machine learning and AI.

04:59.200 --> 05:00.200
Hi.

05:00.200 --> 05:01.200
Hi.

05:01.200 --> 05:06.720
It's great to have you guys here, and thanks for braving the driving rain and horrendous

05:06.720 --> 05:09.640
traffic at CES to make it here.

05:09.640 --> 05:11.280
It is a phenomenon, that's for sure.

05:11.280 --> 05:14.360
It is definitely a spectacle.

05:14.360 --> 05:19.800
Why don't we get started by having the two of you guys introduce yourselves and tell

05:19.800 --> 05:24.640
us a little bit about your background and kind of how you got to what you're up to nowadays.

05:24.640 --> 05:25.880
Yeah, sure.

05:25.880 --> 05:30.040
So yeah, I'm Stuart Feffer and I'm a co-founder and CEO of Reality AI.

05:30.040 --> 05:35.160
And we're an AI startup, I'm sure you have a lot of them on these past podcasts.

05:35.160 --> 05:38.720
And our focus is a little different than most I think.

05:38.720 --> 05:45.120
We are very much focused on problems related to sensors and signals.

05:45.120 --> 05:51.360
And we're not deep learning, we are, we use a different set of approaches that are very

05:51.360 --> 05:56.360
much grounded in signal processing math, and I'm sure we'll talk about that.

05:56.360 --> 06:01.840
We're here at CES with one of our customers, Coeto, also known in the United States as

06:01.840 --> 06:06.280
North American lighting, and they're making a product announcement that features our

06:06.280 --> 06:07.280
technology.

06:07.280 --> 06:08.280
Nice.

06:08.280 --> 06:09.280
Brady?

06:09.280 --> 06:10.280
Hi, I'm Brady.

06:10.280 --> 06:15.440
I'm business development manager with NL and Coeto.

06:15.440 --> 06:22.360
I work in a Silicon Valley lab, which is based in San Jose, just to give you a brief introduction

06:22.360 --> 06:23.360
of Coeto.

06:23.360 --> 06:26.720
Coeto is a tier one supplier for automotive lighting.

06:26.720 --> 06:35.600
And we are the supplier for major OEMs, such as Honda, Toyota, and Ford.

06:35.600 --> 06:38.840
There is a product called ADP.

06:38.840 --> 06:45.400
That's where we work with Stuart Reality AI, try to bring the AI into automotive lightings.

06:45.400 --> 06:46.400
It's called ADP?

06:46.400 --> 06:47.400
ADP.

06:47.400 --> 06:48.400
Okay.

06:48.400 --> 06:49.800
Adaptive driving beam.

06:49.800 --> 06:52.480
Adaptive driving beam.

06:52.480 --> 06:57.400
And tell me a little bit about what's the main idea there.

06:57.400 --> 06:58.400
Yeah, go ahead, Brady.

06:58.400 --> 07:04.320
Yeah, so adaptive driving beam, it's a vehicle lighting mechanism where it allows user to

07:04.320 --> 07:07.320
have high beam always on.

07:07.320 --> 07:08.320
Okay.

07:08.320 --> 07:13.760
But in order to do that, we don't want to blind the income in traffic or the traffic

07:13.760 --> 07:15.160
in front of you.

07:15.160 --> 07:27.000
So now that all the headlights and tail lights are based on LEDs, that allows us to turn

07:27.000 --> 07:32.320
on and off a section of our headlights.

07:32.320 --> 07:40.720
And in order to know which section to shut off, we have to first be able to detect the

07:40.720 --> 07:42.720
vehicle in front of us.

07:42.720 --> 07:43.720
Oh, interesting.

07:43.720 --> 07:46.320
And that's where AI technology comes in.

07:46.320 --> 07:53.280
What sensors do you assume or require on the vehicle in order to be able to do that?

07:53.280 --> 07:58.280
I'm imagining if we're talking about technology that's going to be available in the near-term,

07:58.280 --> 08:01.320
we're not expecting every car to have a light hour on it.

08:01.320 --> 08:02.320
Right.

08:02.320 --> 08:05.920
So for ADB purpose, we only need cameras.

08:05.920 --> 08:06.920
Okay.

08:06.920 --> 08:07.920
Yeah.

08:07.920 --> 08:08.920
All right, cool.

08:08.920 --> 08:10.920
And ADB, by the way, it's available today, right, Brady?

08:10.920 --> 08:14.480
This is a live product.

08:14.480 --> 08:15.440
It's on the road in Japan and...

08:15.440 --> 08:16.440
Yes, yes.

08:16.440 --> 08:20.560
It's been widely used in Japan, also in Europe.

08:20.560 --> 08:24.080
And it's come into North America in a very short period of time.

08:24.080 --> 08:25.080
Yeah.

08:25.080 --> 08:26.080
Okay.

08:26.080 --> 08:30.640
So what we've been doing, what we've been showing today, starting today at CES, is the

08:30.640 --> 08:32.360
next generation of ADB.

08:32.360 --> 08:33.360
Okay.

08:33.360 --> 08:39.200
The existing iteration is based more on traditional machine vision techniques.

08:39.200 --> 08:43.480
And which is great, it works pretty well.

08:43.480 --> 08:46.000
But it is prone to some false positives.

08:46.000 --> 08:57.040
And the idea here is to use AI to reduce the false positive rate so that you've got to

08:57.040 --> 09:02.560
be able to tell the difference between a headlight and a stoplight or a bright vending machine

09:02.560 --> 09:05.280
that's not a truck, that type of thing.

09:05.280 --> 09:10.400
So the idea is to refine the prediction and deliver a more accurate prediction.

09:10.400 --> 09:11.400
Okay.

09:11.400 --> 09:13.160
I'm using a machine learning.

09:13.160 --> 09:19.040
What is it about the traditional techniques that lends itself to the false positives?

09:19.040 --> 09:22.920
Well, you know, traditional machine vision techniques without getting into the specifics

09:22.920 --> 09:27.000
of what COETO's current product does because we can't really do that.

09:27.000 --> 09:32.080
But you know, these traditional machine vision like template matching, you know, they're

09:32.080 --> 09:38.840
very good in constrained environments where you don't have a great deal of variation in

09:38.840 --> 09:40.320
target and background.

09:40.320 --> 09:41.800
That's when they tend to perform best.

09:41.800 --> 09:42.800
Okay.

09:42.800 --> 09:43.800
Right.

09:43.800 --> 09:47.160
So pattern matching machine vision techniques are great, say, on an assembly line where

09:47.160 --> 09:49.560
you're doing quality control.

09:49.560 --> 09:56.120
But out in that dynamic real world where you have a lot more variation, both in your target

09:56.120 --> 10:00.080
and in the background against what you're trying to separate that target.

10:00.080 --> 10:04.520
Well, you know, you start to come up against the limitations of the technique.

10:04.520 --> 10:09.960
Now one of the main constraints here though is, you know, we're not talking about an autonomous

10:09.960 --> 10:11.720
vehicle necessarily.

10:11.720 --> 10:17.920
This is a product that's going to go on regular cars starting in, you know, they're available

10:17.920 --> 10:18.920
as I said.

10:18.920 --> 10:21.320
It was pretty, we're saying they're available in Japan and Europe today.

10:21.320 --> 10:22.320
Right.

10:22.320 --> 10:25.560
And in the United States in 2019 or 2020, something like that.

10:25.560 --> 10:26.560
Okay.

10:26.560 --> 10:29.360
So it's got to fit within a certain price point.

10:29.360 --> 10:30.360
Right.

10:30.360 --> 10:31.360
Right.

10:31.360 --> 10:32.360
Right.

10:32.360 --> 10:33.360
Right.

10:33.360 --> 10:35.240
You can't have an expensive processing brick just turning the high beams on them off.

10:35.240 --> 10:37.000
It doesn't work as a product.

10:37.000 --> 10:38.000
Right.

10:38.000 --> 10:45.000
So the challenging bit here is not just accomplishing the detection using machine learning and suppressing

10:45.000 --> 10:47.920
false positives where you need false positive suppress.

10:47.920 --> 10:55.920
The real challenge is doing that and then delivering that prediction in a form that can

10:55.920 --> 11:00.720
run on cheap hardware that meets the price point requirements of the product.

11:00.720 --> 11:05.400
And this is cheap hardware that is presumably mounted, you know, within the lens, within

11:05.400 --> 11:08.080
the light headlight assembly, right?

11:08.080 --> 11:09.080
Yeah.

11:09.080 --> 11:10.080
Inside the headlight assembly.

11:10.080 --> 11:11.080
That's exactly right.

11:11.080 --> 11:16.760
So it's got to, you know, and it's got to, you can't make the car too much more expensive.

11:16.760 --> 11:17.760
Right.

11:17.760 --> 11:21.400
You can make it only as more expensive as people perceive value in being able to turn their

11:21.400 --> 11:22.920
high beams on it off automatically.

11:22.920 --> 11:23.920
Yeah.

11:23.920 --> 11:24.920
Right.

11:24.920 --> 11:25.920
Right.

11:25.920 --> 11:33.480
He was saying we're actually tracking vehicles leaving the high beams always on and tracking

11:33.480 --> 11:39.200
that vehicle that's in front of you or that's on coming as it moves across the field

11:39.200 --> 11:43.360
division so that he's selectively blocked out.

11:43.360 --> 11:47.560
But you can still see animals, pedestrians or other things that are peripheral to that

11:47.560 --> 11:48.560
car.

11:48.560 --> 11:50.080
Yes.

11:50.080 --> 11:54.200
So this is resonating really strongly with me because I, you know, while I'm, you know,

11:54.200 --> 11:58.680
normally live in a city environment over the holidays, I was in a more rural environment

11:58.680 --> 12:01.960
and made frequent use of the high beams.

12:01.960 --> 12:02.960
Yeah.

12:02.960 --> 12:06.720
And when you depend on the high beams and then you turn them off because there's an

12:06.720 --> 12:07.720
upcoming traffic light.

12:07.720 --> 12:08.720
It's like, where am I?

12:08.720 --> 12:10.080
It's like it's totally dark.

12:10.080 --> 12:15.640
And so I can relate to, you know, wanting to just, you know, a, you know, as cars get more

12:15.640 --> 12:19.600
complex, they're going to have more knobs and stuff and, you know, one less thing to worry

12:19.600 --> 12:20.800
about would be great.

12:20.800 --> 12:26.880
But, you know, there's, you know, if you can have, you can offer me, you know, visibility

12:26.880 --> 12:35.360
into kind of my field of view, even while oncoming cars are approaching, that sounds like a great

12:35.360 --> 12:38.480
proposition and one that will increase safety.

12:38.480 --> 12:41.800
You know, tell me a little bit about some of the technology that goes into making this

12:41.800 --> 12:42.800
happen.

12:42.800 --> 12:43.800
Yeah.

12:43.800 --> 12:44.800
Sure.

12:44.800 --> 12:47.800
So the, you know, on the sensing side, which is our contribution, right?

12:47.800 --> 12:51.120
Coeto, North American lighting, they're the headlight experts.

12:51.120 --> 12:57.280
And in terms of controlling the beam and shaping the beam and figuring out exactly how to adapt

12:57.280 --> 13:01.480
the beam to the driving patterns, that's their area of expertise for sure.

13:01.480 --> 13:08.920
Our area of expertise is sensing the car in front, so that in delivering that location

13:08.920 --> 13:14.760
of the car to their control mechanism so that it can appropriately adapt.

13:14.760 --> 13:18.280
So it knows where the car is, it can do the calculations, it needs to do to figure out

13:18.280 --> 13:19.840
what to do with those LEDs.

13:19.840 --> 13:24.080
So you give them like a vector of angles and distances, for example?

13:24.080 --> 13:28.440
Yeah, basically, you know, we're delivered, it's very similar to what you would see in

13:28.440 --> 13:33.120
an ADAS system, the sort of collision avoidance system for autonomous vehicles.

13:33.120 --> 13:36.560
Where, you know, you see bounding boxes on cars and pedestrians, that kind of thing.

13:36.560 --> 13:37.920
It's a very similar sort of output.

13:37.920 --> 13:38.920
Okay.

13:38.920 --> 13:44.720
So that's what, that's our contribution to this is the sensing piece.

13:44.720 --> 13:49.800
And I think, you know, I mentioned in the introduction, reality AI, our approach to machine

13:49.800 --> 13:51.840
learning is a little different, right?

13:51.840 --> 13:54.160
We're not using deep learning.

13:54.160 --> 13:59.280
Deep learning, unfortunately, would probably require more compute power than we can afford

13:59.280 --> 14:03.640
on that, you know, on that control unit.

14:03.640 --> 14:05.400
Even on the inference side alone?

14:05.400 --> 14:06.400
Yeah.

14:06.400 --> 14:07.400
Yeah.

14:07.400 --> 14:14.680
So our approach in general to machine learning is we spend a lot of energy on

14:14.680 --> 14:18.440
the feature engineering.

14:18.440 --> 14:24.200
And we take a signal processing based approach.

14:24.200 --> 14:30.120
And we have a process that will, you know, looks through to 300 different feature types

14:30.120 --> 14:34.160
and compute each of them, test them and predicts which one's going to be best for any given

14:34.160 --> 14:35.160
situation.

14:35.160 --> 14:41.360
And what are the sensors that you have available on assuming you don't have access to vehicle

14:41.360 --> 14:42.360
sensors?

14:42.360 --> 14:46.720
The sensor is in the headlight assembly and this is maybe a camera or there are other

14:46.720 --> 14:47.720
sensors.

14:47.720 --> 14:48.720
Well, in this case, we're working with a camera.

14:48.720 --> 14:49.720
Okay.

14:49.720 --> 14:52.440
I mean, it's certainly conceivable that in the future, other sensors might be in play.

14:52.440 --> 14:53.440
Okay.

14:53.440 --> 14:55.080
But at the moment, it's purely camera based.

14:55.080 --> 14:56.080
Okay.

14:56.080 --> 14:59.880
And our technology is not camera specific.

14:59.880 --> 15:03.920
I would, you know, if I'm going to be candid, I would actually say image based things is

15:03.920 --> 15:11.320
probably our weakest, our weakest area in image based things where we tend to be strongest

15:11.320 --> 15:17.200
is in problems that could be reduced to a question of texture.

15:17.200 --> 15:22.680
So if you're doing object recognition, right, you want to know, that thing over there

15:22.680 --> 15:28.960
is that a pedestrian or a person on a bicycle, right, that's a good deep learning problem,

15:28.960 --> 15:31.200
object recognition, deep learning is good at that.

15:31.200 --> 15:32.200
Yeah.

15:32.200 --> 15:34.400
And it requires compute, but it can do it.

15:34.400 --> 15:38.240
Our stuff tends to work much better on texture based problems.

15:38.240 --> 15:39.240
Okay.

15:39.240 --> 15:44.800
In fact, that's the way in which we approach this with the headlight detection and the

15:44.800 --> 15:52.080
false positive suppression, is looking at spatial relationships between pixels of different

15:52.080 --> 15:54.480
colors inside of a decision window.

15:54.480 --> 15:55.480
Okay.

15:55.480 --> 15:56.480
Right.

15:56.480 --> 15:58.120
It's just a different way of going about it.

15:58.120 --> 16:02.440
Now the fact is our stuff is much more widely used, I mean, we do, we do some things with

16:02.440 --> 16:09.120
images that are texture based, but sound, vibration, accelerometry, electrical signals.

16:09.120 --> 16:14.120
Those are really a sweeter spot for us most of the time.

16:14.120 --> 16:19.360
And we are getting ready to launch a couple of things with coeto that will involve other

16:19.360 --> 16:24.440
types of sensors beyond cameras as well, different kind of product, different kind of use.

16:24.440 --> 16:31.400
And when you say your stuff, what are we talking about here, is it, you know, some hardware

16:31.400 --> 16:36.160
that goes in the headlight assembly, is it some algorithms, is it IP, is it services?

16:36.160 --> 16:37.160
Sure.

16:37.160 --> 16:39.640
But the headlight is coeto's product.

16:39.640 --> 16:40.640
Yep.

16:40.640 --> 16:47.720
What reality AI offers is a tool for the R&D engineer to create that product.

16:47.720 --> 16:48.720
Okay.

16:48.720 --> 16:54.720
So by our stuff, what I mean is the algorithms and the application that allows an engineer

16:54.720 --> 17:02.080
to use those algorithms to expose the algorithms to data and generate detection code, which can

17:02.080 --> 17:10.600
then be either hung in the cloud, if it's a cloud-based application or pulled out of that

17:10.600 --> 17:17.640
cloud-based environment, pulled into the IDE for an embedded environment, and then run

17:17.640 --> 17:24.280
in the embedded target, which is, oh, we have plans to use this.

17:24.280 --> 17:25.280
Right.

17:25.280 --> 17:26.280
Right.

17:26.280 --> 17:30.000
Maybe tell me a little bit about the, what can you tell me about kind of the experience

17:30.000 --> 17:35.680
of your engineers working with this technology, do you?

17:35.680 --> 17:41.760
I'm imagining that your engineers don't typically have machine learning and AI expertise, or am

17:41.760 --> 17:43.760
I wrong about that?

17:43.760 --> 17:44.760
Right.

17:44.760 --> 17:52.600
So as a lighting company, most of our effort is optics and mechanical and how to control

17:52.600 --> 17:54.520
the heat in the headlight.

17:54.520 --> 18:04.920
So we're not experienced in putting sensors or more computing embedded system into our

18:04.920 --> 18:05.920
headlamps.

18:05.920 --> 18:12.080
So we're quite excited to be able to work with reality AI and try to find possibility

18:12.080 --> 18:21.200
to put sensors into headlamps and try to make it a smarter headlights and rear lights.

18:21.200 --> 18:26.200
And so the platform, do you refer to it as a platform, or a toolkit, or?

18:26.200 --> 18:29.280
Yeah, we call it a toolkit or an application, even.

18:29.280 --> 18:32.160
This is kind of like an SDK that's got some built-in.

18:32.160 --> 18:38.240
Like how is the, think of it as a code generation application, right?

18:38.240 --> 18:42.720
So there's, this is a podcast, so I can't pull up a demonstration here, right?

18:42.720 --> 18:48.880
But think of this as a tool set where you can provide examples of what you're looking

18:48.880 --> 18:55.640
for in the case of what we're doing with Coeto, what those are, you know, here are images

18:55.640 --> 19:02.200
where this is, this over here is a, the, the tail light of a car that we're following

19:02.200 --> 19:06.760
and we want to block out, we don't want to, we don't want to blind over, right next to

19:06.760 --> 19:10.360
it over there, that's a, that's a reflection of, of a stop sign.

19:10.360 --> 19:14.200
So that's a counter example, right, don't count that as a headlight.

19:14.200 --> 19:18.480
That red light off in the distance, that's a stop light, don't count that either, right?

19:18.480 --> 19:19.480
So that, that's our input.

19:19.480 --> 19:27.560
We have snippets of images taken by the camera and with some labels on them, the tell us, tell

19:27.560 --> 19:29.040
us what they are.

19:29.040 --> 19:37.200
In our application, we can load it with these examples and run first a process we call AI

19:37.200 --> 19:42.120
Explorer, what AI, and AI Explorer does the feature engineering.

19:42.120 --> 19:47.520
It's a machine learning driven process, sort of, you could almost think of it as an expert

19:47.520 --> 19:53.680
system, but it isn't really, but it, what it will do is go through and try to identify

19:53.680 --> 20:01.280
an optimized feature set, which can then be exposed to a machine learning algorithm,

20:01.280 --> 20:06.320
which is, you know, could be an SVM, could be a neural network.

20:06.320 --> 20:12.320
We pick, we can pick, we can pick that based on what's the most appropriate form of

20:12.320 --> 20:17.760
output forward with the customer, what we will need for their technical requirements for

20:17.760 --> 20:18.960
the product.

20:18.960 --> 20:23.720
But you know, I mean, your audience is all about machine learning and AI, so I'm sure

20:23.720 --> 20:28.120
you're, you know, you and they know that when you have the right feature set, your choice

20:28.120 --> 20:30.560
of algorithm becomes much less important.

20:30.560 --> 20:37.000
And if you have good, really solid features that separate, that give you a good separation

20:37.000 --> 20:41.040
between classes, well, heck, almost any algorithm will find what you're looking for,

20:41.040 --> 20:42.040
right?

20:42.040 --> 20:47.360
So, you know, that's really what the point of our application is, is to do that feature

20:47.360 --> 20:57.520
engineering and identify the most optimized features possible, such that we can then use

20:57.520 --> 21:04.120
the latest touch machine learning possible, and therefore delivered prediction code that

21:04.120 --> 21:08.040
is as compact and computationally efficient as possible.

21:08.040 --> 21:14.920
So what extent do the features that, that this tool spits out, you know, that, do they

21:14.920 --> 21:20.880
tend to be kind of intuitive features versus, you know, kind of artificial features, kind

21:20.880 --> 21:26.960
of mathematical combinations of the inputs that don't really have any intuitive interpretation?

21:26.960 --> 21:28.120
Most of the time it's the latter.

21:28.120 --> 21:29.120
The latter?

21:29.120 --> 21:30.120
Yeah.

21:30.120 --> 21:31.120
Okay.

21:31.120 --> 21:33.360
So, you know, look, by the time someone gets to us, if it's a sound problem, for example,

21:33.360 --> 21:39.120
a vibration problem, by the time a customer gets to us, their engineers have already tried

21:39.120 --> 21:43.600
an FFT, put that into a neural net to see what would happen, right?

21:43.600 --> 21:47.120
So, you know, if it was a problem, was that easy to solve, they wouldn't be calling us

21:47.120 --> 21:48.880
in the first place.

21:48.880 --> 21:53.480
So, you know, look, our algorithm will check an FFT just to be, you know, a couple of

21:53.480 --> 21:56.800
different flavors and a couple of different varieties to it, just to make, just for

21:56.800 --> 21:58.480
completeness sake.

21:58.480 --> 22:02.920
But generally speaking, we're going to need to carve, we're going to need to carve up that

22:02.920 --> 22:05.000
feature space in a very different way.

22:05.000 --> 22:06.600
And how do you, how do you do that?

22:06.600 --> 22:10.600
Well, you know, we use, we use mathematics, you'd find in the literature under sparse

22:10.600 --> 22:13.800
coding, compressive sensing, that type of thing.

22:13.800 --> 22:15.960
And what, what is, what are those things?

22:15.960 --> 22:16.960
What's sparse coding?

22:16.960 --> 22:17.960
What's compressive sensing?

22:17.960 --> 22:22.320
Well, you know, you, I guess what, what we're doing is you could think of us as carving

22:22.320 --> 22:31.080
up time, frequency in a much more complex way than an FFT, which is just using bands.

22:31.080 --> 22:38.960
And so it can be very responsive to things like transients and phase and, you know, those

22:38.960 --> 22:40.200
kinds of phenomenon.

22:40.200 --> 22:44.880
Now, in the image kinds of problems like, like we're dealing with here with ADB, that stuff

22:44.880 --> 22:46.400
isn't relevant.

22:46.400 --> 22:53.200
But it turns out when we use these same mathematics on images, what that basically translates to

22:53.200 --> 22:56.760
is a texture kind of relationship.

22:56.760 --> 23:01.480
And we, we tend to be good at finding textures and discontinuities and textures.

23:01.480 --> 23:06.120
But that's sort of a side, it's, it's a side usage, which turns out to be very useful

23:06.120 --> 23:08.880
in certain cases like with ADB.

23:08.880 --> 23:19.880
But our primary focus is more in the vibration, electrical signal, sound, light are even.

23:19.880 --> 23:26.880
And so in the past, when I've talked to folks who have, are taking similar approaches to

23:26.880 --> 23:33.880
kind of automating feature engineering, there's, you know, there's often a lot of like Monte

23:33.880 --> 23:36.360
Carlo type simulations and that kind of thing.

23:36.360 --> 23:37.760
Do you do that kind of stuff as well?

23:37.760 --> 23:38.760
Yeah, not so much.

23:38.760 --> 23:43.160
I mean, we'll basically judge which one, which feature set is the best on the basis.

23:43.160 --> 23:47.840
I will take, well, basically with, with want with feature sets that look promising according

23:47.840 --> 23:49.600
to their to our algorithm.

23:49.600 --> 23:54.160
We train a quick machine learning model on a subset of the, on a subset of the training

23:54.160 --> 23:59.240
data, do a quick k-fold analysis and we rank them on the basis of their performance under

23:59.240 --> 24:00.240
that k-fold.

24:00.240 --> 24:04.280
So it's a pretty straightforward accuracy based ranking.

24:04.280 --> 24:09.680
The other thing we do is we do generate a relative measure of the complexity of the

24:09.680 --> 24:11.840
feature computation.

24:11.840 --> 24:18.080
Because again, our customers are, by and large, coming to us because they intend to deploy

24:18.080 --> 24:24.160
to an embedded target where compute is going to be a limited resource, either cycles or

24:24.160 --> 24:25.960
memory.

24:25.960 --> 24:31.680
And so we'll give them a relative ranking of, you know, if it's green and the bar is

24:31.680 --> 24:37.600
hardly filled in, well, you can probably fit it on a cortex M3, M4, right?

24:37.600 --> 24:41.880
But if it's, if it's almost, the bar is almost filled up and it's turned red, well, you're

24:41.880 --> 24:45.160
probably going to need server grade hardware to execute that particular model.

24:45.160 --> 24:47.360
And we basically make that an engineering choice.

24:47.360 --> 24:56.800
Now the engineer who's using this stuff can trade off computational complexity for accuracy

24:56.800 --> 24:58.320
in some cases.

24:58.320 --> 25:04.920
I think something trying to wrap my head around like the, so we get the problem.

25:04.920 --> 25:10.920
The problem is you've got limited computational capacity and a lot of these environments.

25:10.920 --> 25:17.320
And as exciting as deep learning is, it requires a significant compute capability, even

25:17.320 --> 25:20.640
for the inference.

25:20.640 --> 25:24.240
But deep learning is exciting because you don't have to do feature engineering.

25:24.240 --> 25:25.240
Right.

25:25.240 --> 25:26.240
You want to go the other way.

25:26.240 --> 25:30.040
You got to do some feature engineering, which is difficult manually.

25:30.040 --> 25:32.040
You guys automate it.

25:32.040 --> 25:36.240
I'm trying to wrap my head around kind of the next level of detail, which is like if I

25:36.240 --> 25:42.880
wanted to, you know, if I wanted to build something like this, like, you know, what are the things

25:42.880 --> 25:47.880
that I should be thinking of as a, you know, data scientist or engineer, you know, if

25:47.880 --> 25:52.800
I wanted to, you know, if I needed to build my own kind of automated feature engineering

25:52.800 --> 25:57.520
pipeline, like understanding that there's proprietary IP and, yeah, yeah, yeah, yeah, sure,

25:57.520 --> 25:58.520
and all that.

25:58.520 --> 26:00.280
Like, what are the, the things that I should be thinking about?

26:00.280 --> 26:01.280
Okay.

26:01.280 --> 26:07.600
Well, so the first case and the first thing I would say is that, you know, features are domain

26:07.600 --> 26:09.200
specific isn't quite the right word.

26:09.200 --> 26:10.400
That's not what I'm going for.

26:10.400 --> 26:13.760
But the, the, the, the, the differences of them or something like that.

26:13.760 --> 26:14.760
Yeah.

26:14.760 --> 26:18.920
So, you know, our approach, the kinds of features where you, we're going to try from

26:18.920 --> 26:23.400
suit to nuts are going to be the kinds of features that are relevant when you're talking

26:23.400 --> 26:28.280
about an, an input you could think of as a wave form in some way.

26:28.280 --> 26:29.280
Yep.

26:29.280 --> 26:30.280
Right.

26:30.280 --> 26:35.600
And those kinds of features are going to be completely different than the kinds of features

26:35.600 --> 26:41.800
you would use if, you know, you're looking at business records of some sort, right?

26:41.800 --> 26:42.800
Obviously.

26:42.800 --> 26:47.960
But even with sound, the kinds of features we're looking at are not going to be the same

26:47.960 --> 26:52.360
kinds of features you're going to want to use if you're building a competitor to Amazon

26:52.360 --> 26:58.400
Echo or Siri or OK Google, right, where the problem is natural language recognition.

26:58.400 --> 27:03.040
Our stuff isn't actually the kinds of features we employ aren't actually very good at language

27:03.040 --> 27:04.680
recognition at all.

27:04.680 --> 27:07.000
But it's really good at machine hums.

27:07.000 --> 27:08.000
Ah, OK.

27:08.000 --> 27:09.000
OK.

27:09.000 --> 27:15.440
So, you're doing, you probably have like, you're kind of doing different types of FFTs,

27:15.440 --> 27:17.920
different types of windows, different kind of release.

27:17.920 --> 27:18.920
Yeah.

27:18.920 --> 27:21.640
We don't, we, we check FFTs for completeness, right, right, we're more likely to be using

27:21.640 --> 27:25.400
sparse coding, compressive sensing and other kinds of more complex features sets.

27:25.400 --> 27:26.400
Got it.

27:26.400 --> 27:27.400
OK.

27:27.400 --> 27:28.400
I think that's good.

27:28.400 --> 27:33.880
So, you've got, so there's some set of algorithms that are particularly good at identifying,

27:33.880 --> 27:38.000
you know, either frequency components or something like that, you know, in this type of

27:38.000 --> 27:39.000
signal.

27:39.000 --> 27:40.000
Transient phase frequency.

27:40.000 --> 27:41.000
Yeah.

27:41.000 --> 27:42.000
OK.

27:42.000 --> 27:43.000
OK.

27:43.000 --> 27:46.440
And so you, you're just kind of sweeping across those with different parameters and, you

27:46.440 --> 27:49.840
know, maybe there's some kind of grid searching or something like that that you're doing

27:49.840 --> 27:53.280
or randomized searching or something like that or something like that in there.

27:53.280 --> 27:54.280
Yeah.

27:54.280 --> 27:59.120
Something that, you know, it's guided, but there's still a fair amount of, we're going to

27:59.120 --> 28:00.320
try a spectrum of things.

28:00.320 --> 28:01.320
Yeah.

28:01.320 --> 28:05.640
And, you know, we try a spectrum of things and we find a family of features that's promising

28:05.640 --> 28:11.560
the algorithm will dive in and do more exploration within that promising family, right?

28:11.560 --> 28:13.640
But yeah, you kind of have the idea.

28:13.640 --> 28:18.240
And what's the, by what's the origin of the kind of the company and the product?

28:18.240 --> 28:19.240
Yeah.

28:19.240 --> 28:20.240
Yeah.

28:20.240 --> 28:21.240
Great question.

28:21.240 --> 28:23.920
So, you know, where this stuff really came from is really the other co-founder, truthfully.

28:23.920 --> 28:24.920
OK.

28:24.920 --> 28:26.320
So I'm the, I'm the business guy.

28:26.320 --> 28:27.920
My background's Wall Street.

28:27.920 --> 28:32.040
I've spent just enough time in, you know, math and physics and that type of thing to be

28:32.040 --> 28:34.080
able to follow along.

28:34.080 --> 28:39.640
But the real genesis of this came from our other co-founder, Jeff Strackey, and Jeff's

28:39.640 --> 28:40.640
our CTO.

28:40.640 --> 28:41.640
OK.

28:41.640 --> 28:44.440
Turns out he's been my best friend since we were 13 years old.

28:44.440 --> 28:45.440
Oh, wow.

28:45.440 --> 28:46.440
Yeah.

28:46.440 --> 28:55.000
But for the last, I guess, 10, 12 years or so before we started Reality AI, Jeff was

28:55.000 --> 29:03.520
doing contract R&D for US federal government customers in military and intelligence community.

29:03.520 --> 29:09.960
So, you know, always in this area of applying this new field of machine learning to complex

29:09.960 --> 29:17.120
signal processing, signal recognition problems, surveillance, target acquisition, that kind

29:17.120 --> 29:18.920
of thing.

29:18.920 --> 29:24.960
And you know, during that developed a fairly comprehensive body of IP, we have 10

29:24.960 --> 29:29.400
patents awarded, six patents pending, most of which come from that period.

29:29.400 --> 29:30.400
OK.

29:30.400 --> 29:34.120
And but that's really where the expertise for this came from.

29:34.120 --> 29:40.040
And a couple of years ago when we decided to create Reality AI, we took all of that

29:40.040 --> 29:46.080
IP out of the contracting entity used for the, those federal government customers.

29:46.080 --> 29:49.720
Everything that wasn't classified wasn't subject to export control because we don't want

29:49.720 --> 29:50.720
that headache.

29:50.720 --> 29:56.520
But anything that could be freely used commercially, we moved that intellectual property into a new

29:56.520 --> 29:57.520
company.

29:57.520 --> 30:01.120
We sunsetted the old thing that had been used for the defense contracting.

30:01.120 --> 30:09.880
And we created Reality Analytics Inc reality AI to apply this technology commercially.

30:09.880 --> 30:17.400
We turned that into an application usable by an R&D engineer version 1.0 of that came

30:17.400 --> 30:19.400
out in June of 2016.

30:19.400 --> 30:20.400
OK.

30:20.400 --> 30:23.640
2.0 is coming out in just a couple of weeks.

30:23.640 --> 30:24.640
OK.

30:24.640 --> 30:25.640
Nice.

30:25.640 --> 30:26.640
Yeah.

30:26.640 --> 30:33.880
And we've been, you know, adding customers and automotive, probably our number one area

30:33.880 --> 30:37.000
right now, followed very closely by industrial.

30:37.000 --> 30:41.240
And we also have a couple of consumer product customers that are doing interesting things.

30:41.240 --> 30:48.720
And so for industrial, this might be an industrial machinery supplier who wants to be able to

30:48.720 --> 30:50.640
do predictive maintenance.

30:50.640 --> 30:57.520
You just drop in the algorithms and at home is home in kind of those kind of, you know,

30:57.520 --> 31:02.360
frequency based vibration vibration and occasionally sound.

31:02.360 --> 31:03.360
Yeah.

31:03.360 --> 31:06.360
And even some of the automotive customers, by the way, are doing that same kind of thing

31:06.360 --> 31:08.680
but on the vehicle.

31:08.680 --> 31:13.440
But the industrial customers are always, you know, one former and other of, you know,

31:13.440 --> 31:16.200
figuring out when the wing of a jigger needs to think of a barber place.

31:16.200 --> 31:17.200
Right.

31:17.200 --> 31:18.200
Right.

31:18.200 --> 31:22.560
I'm trying to make it, you know, it's come up called the machine whisperer.

31:22.560 --> 31:26.400
There's always like the machine whisperer that knows when it sounds like this, you need

31:26.400 --> 31:29.160
to whack it here with a hammer four times or whatever.

31:29.160 --> 31:30.160
You got it.

31:30.160 --> 31:31.160
You got it.

31:31.160 --> 31:34.400
So, you know, our approach commercially there is that we are generally working with the

31:34.400 --> 31:35.400
equipment makers.

31:35.400 --> 31:36.400
OK.

31:36.400 --> 31:40.400
So that, you know, much like Kuido is trying to, who's building the smarts into the headlight,

31:40.400 --> 31:44.320
we're working with the, you know, industrial equipment makers, the pump makers would have

31:44.320 --> 31:49.400
you to build the smarts into their equipment, as opposed to some kind of aftermarket

31:49.400 --> 31:50.400
add-on.

31:50.400 --> 31:51.400
It's interesting.

31:51.400 --> 31:58.920
I've asked several of the folks that I've talked to today, you know, what kind of things

31:58.920 --> 32:06.120
have they learned trying to introduce artificial intelligence to consumer products?

32:06.120 --> 32:12.800
And, you know, universally, the answer has to do with the user experience and, you know,

32:12.800 --> 32:16.960
from the perspective of the consumer, like, they don't really care about AI.

32:16.960 --> 32:21.840
And like, this is like, at the far end of that spectrum, nobody, no one who's driving

32:21.840 --> 32:26.000
a car is even going to, even if they know that, you know, hey, I don't have to turn

32:26.000 --> 32:29.880
on my headlights, I think my high beams anymore, you know, this is something that you just

32:29.880 --> 32:32.840
want to be invisible to them and just work.

32:32.840 --> 32:40.200
That being said, have you, as a company kind of learned anything about applying AI in

32:40.200 --> 32:43.600
these kind of situations?

32:43.600 --> 32:45.760
Well, it's still early days.

32:45.760 --> 32:46.760
Yeah.

32:46.760 --> 32:47.760
Yeah.

32:47.760 --> 32:53.080
So, we're actually looking into possibility of embedding more sensors into headlights from

32:53.080 --> 32:57.240
those driving as well, like, such as light hour and radars.

32:57.240 --> 33:03.960
And after we, they, right now, the full factor of light are just too big to put into the

33:03.960 --> 33:04.960
lightings.

33:04.960 --> 33:09.680
But we're waiting for that size to shrink into a reasonable size so that we can put

33:09.680 --> 33:10.680
into the headlights.

33:10.680 --> 33:19.000
And is the idea there that, you know, just that Quido would be, would become a sensor

33:19.000 --> 33:27.080
provider to the OEM in addition to a lighting provider or is it somehow tied to lighting

33:27.080 --> 33:30.240
in the delivery of lighting?

33:30.240 --> 33:36.920
That, I can't disclose that part yet, but we are looking into that kind of a possibility

33:36.920 --> 33:45.160
and also a possibility of doing some kind of an edge computing in headlights for autonomous

33:45.160 --> 33:46.160
driving.

33:46.160 --> 33:47.160
Interesting.

33:47.160 --> 33:52.400
You know, the cool thing about what Quido is doing here, right, is that because they're

33:52.400 --> 33:57.760
providing the headlights, the tail lights, the turn signals, right, that's their market,

33:57.760 --> 34:01.680
they own strategic real estate on the car.

34:01.680 --> 34:09.360
They have, they have the, the placement on all of the corners, right, and if you want

34:09.360 --> 34:14.960
to place sensors to get them the best possible field of view around the vehicle, where you

34:14.960 --> 34:19.880
got to put them, right, plus these guys have, I mean, I've had, since I've been standing

34:19.880 --> 34:23.560
next to them all day at CES and the booth, I've had a chance to hear them, hear them

34:23.560 --> 34:30.400
pitch and, you know, putting, being able to put these sensors in a form factor where it

34:30.400 --> 34:38.480
can stand up to a car wash and weather, you know, these guys are expert in creating electronics

34:38.480 --> 34:44.480
that are protected from the elements and still can be, you know, can still see through.

34:44.480 --> 34:50.880
So you know, that's actually, it turns out that as you sensor up a car, the real estate

34:50.880 --> 34:57.360
that their stuff owns and their ability to deliver it in a form factor that fits with

34:57.360 --> 35:03.120
a car's design that is protected from the elements that could stand up to a power washing

35:03.120 --> 35:08.880
or whatever Mother Nature is going to throw at it, that's actually very, very important.

35:08.880 --> 35:13.400
And something that, you know, the automotive industry looks to be only just beginning to

35:13.400 --> 35:18.200
grapple with as they start to think about the reality of making cars that are instrumented

35:18.200 --> 35:19.200
in this way.

35:19.200 --> 35:20.200
Right, right.

35:20.200 --> 35:26.560
Yeah, I can imagine the modularity being, you know, a lighting assembly is pretty plug

35:26.560 --> 35:33.960
and play relative to, you know, changing out something that's kind of built into the

35:33.960 --> 35:36.800
frame of the car or the sheet metal or something like that.

35:36.800 --> 35:41.920
It seems like a, I can see how it would be a strategic place to be.

35:41.920 --> 35:46.920
Is this something that you envision becoming available like as an aftermarket type of thing

35:46.920 --> 35:54.040
or is it, you know, primarily you're going to market through the OEMs, the manufacturers?

35:54.040 --> 35:56.760
Oh, you mean ADV?

35:56.760 --> 36:03.040
So I think right now in North America, we're just waiting for the regulation to go through.

36:03.040 --> 36:12.800
I think sometime in 2018 or 2019, the regulation will go through and we can see vehicles on

36:12.800 --> 36:16.800
the street with ADV within two years.

36:16.800 --> 36:21.480
And what's the nature of the regulation that is over this?

36:21.480 --> 36:26.080
Like the transportation from the FTC, they have to approve everything.

36:26.080 --> 36:30.600
Say that the ID, right, for, yeah, okay.

36:30.600 --> 36:34.920
And apparently they have this kind of regulation in Japan already.

36:34.920 --> 36:42.320
So that's why they, if you see vehicle in Japan, they already have ADV embedded in, this

36:42.320 --> 36:43.320
is already on the street.

36:43.320 --> 36:44.320
Oh, wow.

36:44.320 --> 36:45.320
Yeah, wow.

36:45.320 --> 36:48.720
The first versions of ADV are on the street in Japan and Europe today.

36:48.720 --> 36:49.720
Huh.

36:49.720 --> 36:54.880
Yeah, it's interesting how much of this stuff, you know, there's a lot of the stuff in

36:54.880 --> 37:01.720
this space, AI in general, that is, you know, behind regulation and then there's still

37:01.720 --> 37:06.480
even more of it that's like a head of regulation, we're never quite just right, right?

37:06.480 --> 37:07.480
Yeah, yeah.

37:07.480 --> 37:08.480
Yeah.

37:08.480 --> 37:09.480
Yeah.

37:09.480 --> 37:10.480
It's been a great conversation.

37:10.480 --> 37:11.480
Yeah.

37:11.480 --> 37:12.480
Awesome.

37:12.480 --> 37:14.160
Any final parting words?

37:14.160 --> 37:19.520
You know, I always like to say, because this is a machine learning audience, right?

37:19.520 --> 37:20.520
Yep.

37:20.520 --> 37:26.480
And, you know, there's so much focus on deep learning for a lot of good reasons, right?

37:26.480 --> 37:33.760
I mean, deep learning is an incredibly powerful approach that has made progress on problems

37:33.760 --> 37:37.360
where very little progress has been made in a long time before it.

37:37.360 --> 37:42.000
So I'm certainly not knocking deep learning in any way, shape, or forms relevant to a very

37:42.000 --> 37:45.400
wide class of issues.

37:45.400 --> 37:48.720
But it's not the only, it's not the only tool at toolbox.

37:48.720 --> 37:56.000
And there are cases in, as I said, in particular, with edge cases where you need real-time prediction

37:56.000 --> 38:02.760
at the edge in a product with a price point, that, you know, it may not be the best tool.

38:02.760 --> 38:06.440
So, think broadly about your options as you're trying to solve real problems.

38:06.440 --> 38:07.440
That's it.

38:07.440 --> 38:11.160
And, you know, we are, for certain kinds of problems, we are one of those kinds of options.

38:11.160 --> 38:15.000
But I think the, I think the statement is true generally.

38:15.000 --> 38:16.000
Cool.

38:16.000 --> 38:20.800
And we'll, we'll make sure to link to, uh, to both your websites.

38:20.800 --> 38:26.680
I'm anxiously awaiting the automated high beams as well as the other aspects of the, uh,

38:26.680 --> 38:28.080
the smarter car.

38:28.080 --> 38:30.560
Uh, thank you both for taking the time to chat.

38:30.560 --> 38:31.560
Thank you.

38:31.560 --> 38:32.560
Thank you.

38:32.560 --> 38:33.560
Thank you.

38:33.560 --> 38:38.600
All right, everyone, that's our show for today.

38:38.600 --> 38:43.160
Thanks so much for listening and for your continued feedback and support.

38:43.160 --> 38:49.640
Remember, for your chance to win in our AI at home giveaway, head on over to Twimlai.com

38:49.640 --> 38:54.280
slash my AI contest for complete details.

38:54.280 --> 38:59.000
For more information on Stuart, Brady, or any of the topics covered in this episode, head

38:59.000 --> 39:03.520
on over to twimlai.com slash talk slash 105.

39:03.520 --> 39:07.520
Thanks once again to Intel AI for their sponsorship of this series.

39:07.520 --> 39:11.520
To learn more about their partnership with Ferrari North America Challenge and the other

39:11.520 --> 39:16.120
things they've been up to, visit ai.intel.com.

39:16.120 --> 39:21.160
Of course, we'd be delighted to hear from you, either via a comment on the show notes page

39:21.160 --> 39:27.720
or via Twitter directly to me at at Sam Sharrington or to the show at at Twimlai.

39:27.720 --> 39:42.720
Thanks once again for listening and catch you next time.

