All right, everyone. I am here with Samary Potoufet. Samary is an Associate Professor at Columbia University.
Samary, welcome to the Twomol AI podcast. Thank you. Thank you, Sam. Let's get started by
having you share a little bit about your background. You work in statistical machine learning. How
did you get there? How did I get into statistical machine learning? Somehow, somehow, I
started in math in undergrad in mathematics. At some point, I was doing mostly pure math in
undergrad. At some point, I wanted to do something where I could see the applications of math,
where I could see the impact of mathematics. I wasn't so sure, so I went off and worked for
some time. Then, at one point, I was watching TV and so robots on TV and got interested in how
that was done. So, I started applying for grad school and eventually I got into machine learning.
I didn't know at the time that it was going to become a big field and now it's a big field. So,
great. That's awesome. Tell us a little bit about your research interest. What do you focus on?
Very generally, I mean, I'm interested mostly in machine learning itself. However, in statistical
aspect of machine learning, and by statistical aspect, I mean questions such as how much resources
are needed to achieve low error in classification, for instance. By resources, resources could be
number of samples. It could also involve constraints on the problem. So, maybe we cannot always have
labeled samples. Maybe we have to return, maybe the classifier has to return a classification
fast. So, there are all these potential constraints in real world applications that I'm interested in.
And given those constraints, what are the best, what is the best possible performance of classification
procedures? So, at a high level, that's that's my general interest. But, but I'm interested in
being able to say mathematically, being able to give guarantees, being able to say mathematically,
this is the nature of the problems we are looking at and this is the nature of the best possible
algorithms for these problems. Okay. Yeah, I was going to mention that, you know, we all care
about the amount of data that's required. Yeah. But, there's a little bit of a different,
you approach that question from the perspective of a theoretician as opposed to a practitioner.
I'm wondering if you can comment on kind of the relationship between theory and practice. And
you're the relationship for your work, you know, to practitioners. So, I'll say this, I feel like
the theoreticians within machine learning should mostly be inspired by practice. And so,
I try my best to be inspired by practice in the sense that I will look at practical problems.
And as myself, what would be the key questions that a practitioner might need answered,
and that the data might not reveal at once? Right. And so, I'll give you some examples of such
questions. And can we use math to sort of understand these questions? And so, that's at a very high
level, given to give an exact example, if someone thinks for instance of clustering, right?
There are tons of different clustering procedures out there, tons of clustering algorithms out there.
And the practitioner has data in front of them. And they might ask, which one of these clustering
algorithms should I use for my particular application for my particular data? That's a question
that is very hard to answer directly just from practice. Why? Because I'm taking clustering here,
because it's hard to even test anything. You don't have label data. It's hard to test the
quality of your clustering if you don't have ground truth. So, here you don't have ground truth. So,
so the main thing you can rely on is modeling your problem and asking, on the these assumptions
I can make on my data, what is the best possible procedure? And so, we try to answer those questions
mathematically. I hope that goes at it and says it a bit for you. And maybe talk more concretely
about the types of problems that you are trying to model. Do you think about them? Kind of very
broadly at the level of clustering, classification, that kind of thing, or do you make more granular
assumptions about, I guess, both the problems and the data and the constraints and other things?
It depends on how far the particular question I'm interested in is. There are some questions where
we are very much at a high level, where we are thinking about the basic task, such as classification,
clustering, and such. And then there are problems that we understand quite well already. And in
those cases, I might start thinking about specific algorithms and how to distinguish between
specific algorithms. I hope I'm answering the question right. Maybe you want to make it a bit more
precise. Should I make it? I think the way to make it a bit more concrete is for us to talk about
specific problems that you are interested in and research and kind of drill into detail there.
Let's start with a recent paper or something that you're excited about.
Since we were talking about clustering, let me just give some concrete problems in clustering.
Right. First, we all know at a high level how one might be fine clustering. The problem of
clustering is, I believe that the data comes from different groups, different subgroups.
I don't have labels. However, I'm hoping that at least geometrically in some space,
I should be able to discover that the data groups well into so many groups. Right.
Right. So the issue that comes up right away is what do we mean by the data groups well?
We can say, for instance, that the data groups well in the sense that the data points belong
into the same group are closer to each other than to other data points. And that gives a sense of
they close their well. But the moment I say they are closer to each other, that means I'm
making an assumption on a notion of distance. How far they should be from each other.
And then the question comes up, what notion of distance is the right notion of distance
here that I should be using. Right. So that all depends sort of on the downstream task.
And so the theoretician might then start asking that question, what is the downstream task?
And then what is the right notion of distance for this downstream task? Or we can go higher and say,
how do we define clustering at all properly? Maybe this is not the right way to define clustering.
Another way to define clustering is probably through densities.
What I mean by that is, if I throw a sand on the floor,
right, I might say that or it closes into the sand clustered into a few groups. And what do I mean
by that? There is a different density of sand in different parts of the space, the floor here.
Right. And so that gives me a different notion, a different notion of clustering, maybe what I'm
trying to find in my data are regions of high density of points. Now that's a whole other, sorry.
Yeah, you would think that the two ways of looking at this distance and density there,
you know, they're measuring the same underlying phenomenon. There's a lot of
there, but maybe one mathematical formulation or expression lends itself to the way you're
trying to approach the problem. Yeah. Exactly. Exactly. Because that falls back also again into
what do I even mean by the distance between points in the first formulation? In the first
formulation, there are many different notions of distance that I might be looking at.
Yeah. And in this formulation here, the notion of a density is a bit more of a robust notion
in this other formulation. And then let's say that there is such a, that I agree with that notion
that this is my notion, it's the regions of highest density. Right. Then there are tons of
questions that come up. What level of density do I look at? If I look at a particular resolution,
I might see particular clusters. If I zoom down, I might see even more clusters. So there are
questions that come up right away as to what threshold and what threshold am I calling my notion
of density? And can we answer that automatically with an algorithm? Can an algorithm discover that
automatically? And in what situations, depending on the downstream task, what notion of
closer ratio we'll be using? So the type of questions that come up there are, so first, you
define a notion of clustering. Next, you ask the question, on the notion of clustering,
is it possible at all to discover the clusters? To what level can I discover the clusters,
meaning what error do I expect if I have so many data points? So here I'm assuming there is a
ground truth, of course. And what error do I expect with respect to the ground truth,
depending on the number of data points that I have? And then the next question
becomes that of what I call adaptivity. Adaptivity of people might call it self-tuning and such.
Every cluster in procedure comes up with all sort of tuning parameters. Here, an implicit
tuning parameter that I threw in a second ago is if I talk about densities while density level,
while level, what resolution? And that's sort of a tuning parameter. And so are there algorithms
that can look at the data by themselves? And discover also the right tuning parameter.
And I called, we call that, at least in statistics, it's called adaptivity. And machine learning
people tend to call that auto-ML or self-tuning and such. And so, but those are the type of questions
that me and other theoreticians ask, are these possible at all? Is this possible? And if it is
possible, what are the family of procedures that achieve these goals?
And so tying back to kind of this, you know, the problem of the practitioner as the touchstone
or inspiration is the ultimate goal to be able to say, if your problem looks like this or if your
data looks like this, then you want to use L1 norm, L2 norm, whatever, or, you know, this set of
density metrics versus that set of density metrics. Exactly. So that would be one goal. That would be
one way the theoretician can be useful to the practitioner, right, is to be able to say that
for these types of problems, this is the algorithms that seem to be, or these are the decisions,
the practical decisions that seem to be most appropriate. Yeah. Right. The eventual, the big goal,
I think, for any machine learner, whether theoretician or non-territician, is to come up with data algorithm
that is sort of almost a black box and can decide on its own which situation it is in by just looking
at the data. And that falls back into what I was calling adaptivity. It self-tunes. At the end,
it has nearly no tuning parameter because all its internal tuning, it does on its own.
And identifies the setting or the situation in which it is and adjust itself. So it's a
grand goal. We do know that very generally, it's not possible. But we also do know that we didn't
turn on the certain restrictions on the universal problems to say it is possible. And in fact,
to some extent, humans do it. But it's because sort of our universal problem is restricted.
And these kind of adaptive algorithms is that something that you that, well, clearly,
this is something that you're interested in and research are there. You know, specific examples
of research that you've done that in this area? Yeah, yeah. So, for instance, I'll give a
high-level example. Here, during my PhD and maybe a little bit after my PhD,
here are some of the problems I was looking at. I was looking at simple questions
having to do with nearest neighbor methods. These are the most basic procedures out there,
right, nearest neighbor methods. We do know that nearest neighbor methods when the dimension
of the data is very high, do quite poorly in general. And you can show it mathematically that
their convergence rates, whether you're in classification or regression, that their convergence
rate gets worse with dimension. And dramatically worse with dimension. In other words,
how dramatic you need a sample size exponential in dimension in order to achieve decent convergence,
right, at least in the worst case. So that's how we quantify that. So that's always been known,
that dimension, and it's called the curse of dimension, that dimension is an issue.
However, people then felt like maybe we can, if it so happens that the data lies, the data is
very high-dimensional, but intrinsically is low-dimensional. So examples of that, one example
that I like to give is consider a robotic arm with sensors on the robotic arm. It has a lot of
sensors, so it's generating a very high-dimensional data set. However, the arm has few degrees of
freedom. And so we expect that that data lies on the lower-dimensional surface in a very high-dimensional
space. And so what people don't try to do was come up with procedures called manifold learning,
procedures, which will try to discover that manifold, and then re-represented that in that
sort of manifold space so that it's now lower-dimensional, and then try to run these algorithms
such as nearest neighbors in this lower-dimensional space, where the hope is that it will perform much
better here. So again, stepping back, we're starting with a very high-dimensional problem,
but we know that in a lot of machine learning applications, even though the data appears
high-dimensional, it's in fact often low-dimensional, we just need to discover that lower-dimensional,
medium, and then run the data, run the algorithm in that new space, in that lower-dimensional
space, right? So is manifold learning an example of dimensionality reduction?
Yeah, exactly. It's an example of non-linear dimensionality reduction.
So now, what you've done is you've increased the pipeline.
So now you have your manifold learning procedure, which also has to make a decision, multiple
internal decisions. What dimension is the manifold? Is it really a manifold? Is it really a smooth
manifold, or is it just a bunch of subspaces of different dimension? It has a lot of internal
decisions to make. So in the end, your entire pipeline becomes this manifold learning plus the
eventual classification, right? And you increase the pipeline, you've added in a lot of
tuning parameters. And so the question of adaptivity comes in. Is there just one algorithm
that I can look at that automatically does as well as if I found the right manifold, the right
dimension, everything? Okay. So that's where adaptivity comes in. What am I being adaptive to? I'm
being adaptive to the structure of the data without knowing a priori the structure of the data.
So one of my earlier work was trying to understand to what extent it is that existing procedures,
such as nearest neighbor, already adapt to the structure of the data, to the intrinsic structure
of the data without needing manifold learning, without needing the other ways of
of re-representing data, reducing dimension in some ways, dictionary learning is another one.
So without needing dictionary, dictionary learning applies in the cases of sparsity where we
believe that the data is high dimensional, but it's very sparse. And so it so turns out that a
lot of existing procedures are automatically adaptive to the structure of the data. And that came
out mathematically by looking at the problem mathematically and then asking, oh, if we say now that
my data that is very high dimensional lies very close to a low dimensional subspace,
what convergence can we prove? And it so turns out that the convergence we could prove is
then in terms of the lower dimension rather than in terms of the higher dimension.
So just so I can replay that the you know these algorithms like nearest neighbor that we could
throw you know tons of different types of problems at some high dimensional
will have problems in the general case with high dimensional data, but if the data happens to be
the data that has this low dimensional structure. Yeah, exactly. Then the performance of those
out it sounds like the performance of that algorithm is going to be more akin to what we'd
expected the dimension of the data was low dimensionality. That's exactly what I'm saying. Yeah.
Yeah. Here all of a sudden it turns out that these algorithms and it's not just nearest neighbors.
We are starting to understand that it's many other algorithms. It's it's particular classification
trees. It recently people understand that even things such as neural net net adaptive to
intrinsic dimension in in these ways things such as support vector machines being understood as
adaptive in these ways, but at that time when I started working on these problems it was a bit
unclear what was adaptive in these ways. And so happened that it was yeah. Go ahead. I think I
got my I have my question from before. It's like okay. Is it two different things to say that
the algorithms will perform better because the data has this inherent low dimensionality,
this inherent structure versus the algorithms are adaptive. Is it is it is it is it some combination
of the hyper parameters or or something. Oh, okay. Okay. I see. Yeah. That makes them adaptive
under a certain set of constraints of the data or is okay. I see. I see that. No, no. This is a very
good question. Let me put it this way. So I had two things in mind that I would explain the
question. So first I was talking about the entire pipeline. If you were to do dimension reduction
and all that you could you see right away that one question that comes up right away there is
what dimension should I reduce to. Okay. Which is the parameter of the problem. Yep. Right.
What sort of low dimensional structure do I have? Right. It's another hyperparameter of the problem.
And I'm using the term adaptivity here loosely to say that I'm adapting to these various hyperparameters
of the problem without knowing them a priori. Yep. Exactly in the sense that you just explained
which is that if the data happens to be so structured then we do automatically better without knowing
that a priori. Now there is a nuance. What does it mean to do automatically better? Nier's
never methods. I know just in fact I know just one algorithm. It's a family of algorithms. And it's
a family even in the simplest possible ways. So I can decide to run a nearest neighbor by saying
I'm only going to use the nearest data points or I'll use the k nearest data points. The two
nearest or three or four those become parameters of the problem. Right. So now there is another
level of adaptivity which is how do I choose those parameters automatically of the problem.
So it so turns out that if I'm say in regression depending on the loss that I'm using if I'm say
in regression then regression is a hard enough problem that finding, finding tuning to the best
possible parameter is not as hard. Does that sort of make sense? Find tuning to the best possible
parameter is not as hard since the problem itself is hard and we cannot do so well anyway
in a non-parametric regression. The complexity of the problem is self-masked. Exactly, exactly.
And so here you can show that in fact you can just do cross validation on the number of neighbors
and you'll get the best possible rate as if you knew what structure the data lie down.
Right. Okay. In classification it's a bit more complicated. Classification is a much easier
problem than regression and so it's a bit more complicated. Cross validation gets you half the way
for hard classification problems but for super easy classification problems to get the best
possible result you might need more refined, quote-unquote adaptive procedures to choose now the
parameters of your algorithm and there are various ideas out there and those are the various things
I'm interested in. So I think the initial point that you made that you're talking about the
pipeline being adaptive as opposed to the algorithm and even talking about the algorithm
in this way. Yeah. The reality of these things is that they are all the same.
Right. Another of them are the pipeline. It's all the same because I can just make my algorithm
more complicated and it becomes a whole internal pipeline. And in fact that's what I'll say this
way that for me that's what neural networks are. It's just a whole pipeline of sub procedures.
Yeah. Yeah. Interesting. Interesting. And you're saying also the kind of the process that we
typically use in practice to optimize these pipeline slash algorithms is the adaptive part.
And we know that that's why. Yeah. Yeah. It's the adaptive part. So they are really
multiple. I use the term adaptivity here at least here fairly loosely. In the research paper
I have to be very much more precise as to what I mean by adaptivity because yeah. But you're
bringing exactly the type of questions why we need to make it more precise when we say adaptivity.
There is the notion of adaptivity to various sub problems, families or problems.
Right. And then there is also the side problem of our algorithms themselves.
You can view every parameter of your algorithm as trying to address one particular sub problem.
And that's how they are tied. Does that sort of make sense? The parameters of our procedures.
An example would be helpful I think. Here all I'm saying is that if I fix the parameter of my
procedures, if I just fix the parameters, there is always one problem on which it's going to do
well. Right. Right. Okay. So in some sense it's easy to think about it this that way that the
parameters or the various configurations of my algorithm are addressing as subfamily or problems.
And as I'm trying to be adaptive to a whole hierarchy of families or problems, I need to also
be tuning my algorithm. Right. Funding the right parameter for the right problem. And right. And
so those are those interactions that we are carrying about that we try to understand. Got it. So
you're saying a broken clock is right twice a day. Exactly. Yeah. And that's the perfect algorithm
at that time. But what we're really carrying about is an algorithm that works well across
problems. Right. Right. Right. And we do know that in very in general, it's not possible. In
general, in the following sense, you probably've heard such things as people talk about the no-free
large theorem. Yeah. And so we know that in general sense, it's not possible. But we do know that
if we restrict the family of problems, it is possible to some extent. There are situations where
adaptivity is not possible at all. And we might get into that when we talk about transfer
learning, multitask learning and all that. So and where I hope at least what I'm trying to
to bring out is the fact that adaptivity in some ways is what we are seeking in machine learning.
We are seeking that black box procedure that looks at the data and just knows this is the type of
that I'm dealing with. This is how I should self configure to do as well as possible on this data.
Yeah. So yeah, let's move on to transfer learning. What are you looking at in that area?
Yeah. So in that area, I feel like most theoretical questions are quite open.
Right. So I don't know if I need to define transfer learning quickly here for, I mean, in fact,
I should because there is one thing. People use the term transfer learning and domain adaptation,
all the all these terms fairly loosely. And so when I'm using it, I need to be clear what I'm
trying to refer to. Okay. I was going to say in general, I would expect our listeners to be
familiar with the term. Of course, of course. Yeah. I'm sure that the listeners are familiar with
the term, but the key point for me here is that when I use the term, I need to make clear this is
what I'm trying to say, not some other thing that I might have in mind. Right. And so transfer
learning, what is transfer learning for me? Generally, we have, I have data from a set of tasks.
Sorry, I have data from either a set of tasks or from a single task. And that data gave me
information about a particular problem. Maybe it's a classification problem of some sort. However,
the problem on which I like to do well is a different problem. So mathematically, we tried to
look at it as two different distributions that drew data from a particular distribution,
but that distribution is not representative of the eventual distribution that's that I would like.
Right. So that a different problem, but that could also be the same problem with different data.
Exactly. That could be the same problem with different data. And that maybe the data has shifted
somehow. And here is a simple case of the of the same problem. And in fact, that's where we
starting. Let's look at first the case where it's the same problem essentially. Right.
So I've given various talks on this on this so far. And I always start with the same
motivating example. And the motivating example is Apple's Siri voice assistant. Right.
And if Apple people are listening to me, I hope they understand why I'm saying this.
Apple Siri still doesn't understand me that well at all because of an accent. And why?
Because it was mostly trained on American English speakers at first. Right. And
and it's doing much better now, but one has to think about what they had to do. They had to acquire
a lot more representative data. So you could view the original task as that where they got
data from a particular distribution, but it's a distribution of American accent. Right. And then if
you try to deploy it, let's say in Scotland, it's a different distributions. It's the same problem.
It's a different distribution of accent. You still have Americans there. You have some Scottish
there. Here in the US, you have a few Scottish there in Scotland. You have a lot more. So it has
shifted, but it's the same problem. So at this point, there are various proposals out there to
try and solve this problem faster. But what are the key issues? The key issues will be, for the
practitioner, how much more that I should acquire from let's me call the target desk, how much more
represented that I should acquire. So that's a question of resources. Right. And then once,
and here there is a cost, right, a financial cost, and we want to acquire as little as possible.
So how little can I get by with? And that's really where the idea of transfer is coming in.
What information can I transfer? And then there is the question of the how, how do I transfer
such information? So what is the proper procedure? Once I have that additional data to transfer
the information, or do I need that additional data at all? Right. So those are very, those are
basic questions. Right. And those questions, we want to try and understand, we want to try and
understand the principles behind them. The principles behind transfer. Yeah. Just to jump in, I
love where this is going because I think, and maybe this is more practitioner than a theorist
perspective. I think, you know, it's easy to kind of get, take a snapshot of, you know, where
transfer learning is today, like you've got a neural network, you train it, you take, you know,
the lower layers, and then you kind of retrain on your, your, your top layers for your specifically.
For your new data, exactly. That's transfer learning. But I think you're suggesting that,
actually, there's probably a lot of different things that one could think about doing to affect
transfer and maybe some are better than others. Exactly. There are many more potential procedures
there. Right. Here, for instance, you just describe one. And another could be, and actually,
people do this, maybe I'm going to take the new data and keep, take my system data, put it all
together, pretend it's from the same distribution and retrain everything. Right. That's another
potential procedure. And so there are many procedures out there. And, but before, let's put it this
way. Let's even say that your favorite procedure is the one that you just described. I take the,
a layer of a neural network. This is still presuming that I knew how to pick the target data.
I knew which, which amount of target data I should be using. Right. So there was a decision there,
also. So, so another basic question is, is there even a way for me to know a priori or to discover
over time what amount of target data is required is needed? In full of a sudden, I acquire 1000 data,
1000 data points. And I'm not doing well. What does that tell me? Does that tell me that the,
uh, I didn't acquire enough? Or not the right or I'm not doing the right, uh, transfer.
Or not the right data or not the right data. Yeah. Right. Yeah. It's kind of getting to like active
learning and exactly. So they get into active learning. It gets into into a different form of
active learning, active learning on the transfer. What are the limitations of that? Uh, it gets into,
uh, um, uh, it gets into understanding what's the right notion of distance or what's the,
by distance here, I really just mean information. What's the notion, the right notion of information
to sort of quantify the information my original data already had about the target application.
To be able to say that it doesn't have enough information, so I need that much more.
Mm-hmm. Representative data. I think what you're saying is if we had a way to know
what it was about our initial training data that made it unique and special and most informative
to the model that we've trained. Yeah. And what it was about the data that we need in order to
have our model perform better on the transfer task, you know, there may be a shortcut
than what we're exactly. Then we might start identifying the right data to pick. Yeah. Yeah.
And the right amount to pick. Yeah. Right. And so some of my early theoretical work on this problem,
I've only started on this problem a few years back. And some of the, the first questions there
are that question. What is the right notion of information? The source has about the target.
Mm-hmm. Right. And how do you begin to characterize that? So, so you cheat, right? That's the first
thing you do. You go and you read a lot of papers because a lot of small people have thought about
the problem. And you try to see, okay, what are the notions they've come up with and what are the
limitations of such notions? Right. And then what's there still to be answered? And, and here,
one of the things that I claim is that they are told, okay, this is the one thing that I'm still
doing that everybody, every theoretician has done on this problem so far, which is model the problem
as just probability distributions. I have a, I have data that I drew from a probability
distribution. That's my source. And then there is another probability distribution, which is my target.
And then there are tons of notion of information that relate to probability distributions. And also
notions of distance between probability distributions. And then we can start there and start asking,
do these notions of information or notion of distance, do they sort of capture the hardness of
transfer? And I can say how hard transfer is, if I had a million data points from my source,
and just only 1,000 from my target, how hard would transfer be? And those are the places people
have started. And so we started looking at those notions and then saying, okay, a lot of the
various notions that people have looked at are notions that were developing other areas for
in probability theory or in information theory and such. But we're developed for other problems.
And not necessarily for the exact problems we are looking at, classification on the transfer
setting. And in which case we have to start looking at classification and then start asking,
what makes it easy to transfer a classifier from one distribution to another distribution?
And what's entering that? So then we can ask, as we bring in the question, as we refining the
question, we can start seeing what's essential to the problem, right? So for instance,
if I'm using neural networks, I can ask the question and I'll step away from neural networks,
even I'll just say, I'm using a family of classifiers, right? I'm using a family of classifiers,
then I can step back and ask, okay, when is transfer easy? I can say transfer is easy,
if on my original data, my original data, my source has information about the target,
if on a for the particular family of classifiers I'm using, if whenever a classifier in my family
does very well in on the source, I expected to not be too far from the best classifier on the target.
I expected not to do too bad on the target. That could be one simple notion.
And that in itself start driving down a notion of distance between them, given my preferred
set of classifiers. And now we're talking about distance between the classifiers or distance
between the data sets. Distance between the data sets or in fact between the problems,
because I'm viewing the data sets as being drawn from a distribution, which is now the problem
that I have. And part of the problem is transferring using just my given family of algorithms.
Maybe I'm using neural networks, maybe I'm using random forests, and that's my given family
of algorithms. And that's the family of algorithms within which I would like to stick as the
practitioner. From my family of algorithms, the problem of transfer might be different,
or the hardness of transfer might be different if I were to switch to another set of algorithms.
And I hope that that one should be intuitive to people. If I'm using neural net, transfer
on the neural net might not be the same as transfer on the classification trace.
And even though the data is the same. So it's questions of that type that we try to answer.
And then from there, try to answer more refined questions. Once we start getting a sense of good
notions, like meaning notions that are predictive of hard, hard transfers, then we can start asking
other questions. Okay, now under these notions, we know how what's the best we could possibly do.
And then we can start asking are there algorithms that can attain
these rates that can attain these performance? And then are there adaptive algorithms?
Are there algorithms that can attain these performance without any knowledge
about the underlying problem parameters? How much information about the problem parameters do I
need to give an algorithm before it can do as best as possible for the problem?
And that's again where I fall back into adaptivity.
Yeah, maybe just summarizes part. I think, you know, a lot of what we're seeing in this conversation
is you hear often, you know, machine learning works. We just don't know how or why it works.
And I think you're illustrating in the context of transfer learning,
how a theoretician goes about trying to advance our understanding of how a particular thing.
Exactly, exactly. And so, and I can tell you some of the simplest questions we,
we are starting to address, take the simplest or most naive heuristic in transfer.
I take both data sets, throw them together, pretend it's the same,
and rerun my training out and retrain algorithms on this new data.
How well does this do?
And so, we can start answering those questions. Okay, it's nearly
the best you can do in particular situations. And we can say what exact situations it often
has to do with how noisy your data is. And if it's not so noisy, and whether your,
whether the pattern, your best classifier is shared between source and target is the problem,
the same how close are the problems. And so, on the various situations, we can say,
okay, this particular set of algorithms will do as well as possible.
In other cases, the algorithms that do well will be or adaptive will be closer to meta-learning
type procedures. And so, so we can start addressing those questions. But again, the whole thing
for me is step back completely and start asking the most basic questions about the problem.
Yeah, yeah, yeah. One more area I'd like to explore. You've also been doing some work in
unsupervised learning. Can you talk a little bit about that?
Yeah, so, so it's mostly the, I mean, I've talked a bit about that so far with the
clustering, although I didn't quite as specifically talk about my work in clustering.
But, and then lately, we've been looking at some applications in IoT,
in allied detection and such in IoT. So, so in clustering, mostly the type of questions I've
looked at and I'm still looking at, there are two lines of questions. The first is what I
alluded to earlier, which is, let's imagine that I define clustering as just notions of
in terms of density levels, in terms of finding regions of high density in the data.
To what extent can I do this? And can I do this also adaptively? Can I? And there there was a key
question that we had, which was, there are tons of in density based clustering itself,
there are tons of heuristics, I will call them heuristics and it's not a bad term,
there are tons of heuristics that do extremely well in practice. And yet, when you try to prove any
guarantee about these heuristics, it's really hard. You cannot show that day. And however,
they do better than anything we can prove beautiful guarantees about. And so one of the questions
there was, can we come up with a heuristic that does, that compete with existing heuristics?
And yet can guarantee the recovery of clusters if we define clusters as regions of high density of
the data. So it's that upper question there. Then, then lately, I was, I've been looking at
more, I mean, it's coming up in more in the IoT domain, internal of things. And here,
the type of questions that are coming up, I have to do with constraints on the clustering and
alloy detection and all that. In IoT, we want to just monitor traffic, network traffic. And be
able to quickly say, when there is an anomaly in the network traffic. And the anomaly could just be
a new device was introduced into the network, into the house. Or we are observing a new modality
of the device. And these devices could be anything. It could be a smart coffee maker,
smart fridge, it could be a network, it could be a sensor in a city, et cetera.
And applications might be, you know, anything from cybersecurity to performance,
management. Exactly. Yeah, et cetera. And, and even detecting, yeah,
cybersecurity goes into it, right? Detecting simply that a new device was introduced,
and this device looks a lot like a camera. So somebody threw a camera onto my network.
Right. So something like that. And, and from your, I guess you're getting to this, but your,
your data sources like net flow traffic traffic. And we are assuming that I can only observe the
pattern of a packet. I cannot read a packet in network packet. All I know is that so many
packets are being sent per second to these particular addresses and all that. And so it's very unsupervised.
I don't know. It's an unsupervised problem. I don't have any, any labels. And however,
this is where constraints come in. Huge constraints come into the into this right away.
And the constraints of the following form, we have to be able to run the detector, train and
run the detector. Let's say on a router at home, on a very small device. We don't have a huge
server to, to do this. So all of a sudden, a lot of our library detectors out there,
let's, for instance, let's think about one-class SVM. It just don't run well in these settings.
Why I'm taking one-class SVM? One-class SVM like support vector machines in general
require a lot of computation. So require a lot of computation because they deal with
this, this, this very large matrices that they have to, to, to play with. And so right away,
the question that comes up, questions of how do we reduce the data in some ways, right?
Or how do we reduce this? If you're familiar with support vector machines, they work on
a so-called gram matrix, which is essentially a representation of interactions between
data points, okay? But because the moment I talk about interactions between data points,
if I have n data points, I'm talking about n squared operations already, right? And so
we want to reduce this and reduce, reduce this quickly and to be able to run on a nano computer
on Raspberry Pi or something like that, right? And so it's not only space savings, but it's also
time savings. And so for me as a theoretician, the first question that comes up is,
first of all, can I maintain performance of a clustering
after reducing data? And what type of data reduction is useful here? There are various things
that people have tried. Nitron is a method called Nitron sketching method, all sort of
a sub-sampling method that reduce these data representations. And then the questions for me was,
okay, can I modify this method and guarantee that let's say in the context of clustering or in
the context of allied detection, the performance is essentially maintained and yet I've achieved
my constraint on size. So it's questions of that type. And here it's very practical because I
cannot just answer the question. I have to, in fact, implement it and get it working and eventually
deploy it. And presumably is this work in progress or have you come up with another question?
Oh, so we have a couple, yeah, we have a couple papers on it. And you probably heard of Johnson
leader in Strauss dimension reduction and random projection, things like that. So part of that
the work, the initial paper on this work was all theory was trying to understand, okay,
all these problems out there, all these methods out there sketching, nice from, can we recast
this method in different terms that we can understand better, right? So sketching doesn't look at
right away like a random projection. And we're asking sketching, which is really just
sub sampling of a matrix. Can we view it as a random projection in a very high dimensional
space, in an infinite dimensional space? Why infinite dimensional space? Because that's exactly
where OCSVM's and all that work. They work in an infinite dimensional space. And I'm doing this
sketching on a finite matrix. Can I view that somehow as a projection in an infinite dimensional
space? Why do I want to view it that way? Because I do understand projections. We understand what
projections do and what they maintain and all that. So that was the first step in the work.
And then the next step has been, okay, now that we know how it works, we know for what type of
clusters we preserve the clustering. And now do we have that type of clusters in this IoT
applications? It so turns out that we did a lot of data analysis and, okay, we do have that
type of clusters often. And now we are implementing it. And we have a paper on archive trying to show
that, okay, look at this running on a nano computer on the Raspberry Pi. It runs almost, it runs
20, 30 times faster than the original OCSVM, but we have the same performance. And so, but it's
understanding this type of things. Very cool. Very cool. There's maybe a bit of a
digression before we close out, but I was thinking earlier when we were talking about clustering
and density and like zooming in and out. And kind of seeing the data at different granularities,
it reminded me a little bit of like fractals and like fractal theory. And I'm just have,
does that come up in theory? Yeah, it does come up a lot. So, so for instance, you might,
so when I was talking about dimension earlier and I was even when I was saying, okay,
let's say data is very high dimensional, but it's very structured, right.
There are tons of structures, tons of intrinsic structures out there. Manifold is just one,
right? And Manifold is just one such structure. And so, a first thing in that line of research was to
try and understand what notion of intrinsic dimension can we use to capture all these
low dimensional structured at once rather than working on understanding each one of them separately.
Is there a notion that captures them at once? And there are fractal notions of dimension that
do capture these notions of intrinsic dimension at once. And then once you understand that you can
then ask the question, okay, let me now say that my data is low dimensional in the sense that
it's fractal dimension is small. How do nearest neighbors work? Okay. Right. So, so that's how we
approach those those problems. We first have to sort of step back again and say, okay, what is the
essential quantity we need to work with here? And and and fractals somehow have to do with
is representability of data or the information in something in in in the data space. And so,
so so yeah, so a lot of notions we work with have this sort of recursive structure to them.
Awesome. Awesome. And one last thing, you're chairing a conference that conference is happening now,
so I assume. Yeah. Yeah. Tell us about the conference. So, this is called conference on learning
theory. And so, called has been sort of the flagship conference for machine learning theory.
I don't know how many years now, 20, 30 years, something like that. And so yeah, so it's
happening this year in what we all know, sort of unfortunate circumstances, the times we live in.
And yeah, and this year, one of the things I'll say is that this year we have a really, really,
really nice program. So, so let me put it this way. I feel that somehow at the same time we are
in strange times and hard times for a lot of people, but people also tend to focus and solve
hard problems around this sort of tough times. And I feel that reflecting in somehow in the program
we have this year, a lot of beautiful problems were solved. And we were just odd at the type of results
that to be presented next week at the conference. Yeah. Awesome. Awesome. Yeah. Well, we will include a
link to the conference in the show notes as well as the archive link to the network traffic
representation paper. Samry, so great to catch up with you. Thanks so much for taking the time.
Yeah, thank you so much Sam for having me and thanks for all the very interesting questions
and the very pointed questions. Awesome. Thank you.
