1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,200
I'm your host Sam Charrington.

4
00:00:32,200 --> 00:00:37,880
With less than three weeks to go, time is running out to register for the event we are all looking

5
00:00:37,880 --> 00:00:41,840
forward to, Twimblecon AI platforms.

6
00:00:41,840 --> 00:00:47,200
At Twimblecon, you'll hear from industry luminaries like Andrew Aing, Ubers Fran Bell

7
00:00:47,200 --> 00:00:52,080
and Cruises Hussein Mahana on what leading organizations are doing to increase the efficiency

8
00:00:52,080 --> 00:00:57,680
of developing machine learning and deep learning models and getting them into production.

9
00:00:57,680 --> 00:01:02,040
You'll learn about the practices and platforms being put into place by companies like Airbnb,

10
00:01:02,040 --> 00:01:07,480
Capital One, Comcast, Liva, SurveyMonkey, Zappos, and many more.

11
00:01:07,480 --> 00:01:11,880
And you'll get to engage with fellow data science, machine learning, platform engineering,

12
00:01:11,880 --> 00:01:18,000
and MLOPS practitioners and leaders to learn, share and connect on how to accelerate, automate

13
00:01:18,000 --> 00:01:22,480
and scale machine learning and AI in the real world.

14
00:01:22,480 --> 00:01:27,800
We are super excited that we'll get to see you all on October 1st and 2nd in San Francisco.

15
00:01:27,800 --> 00:01:33,240
For more details, visit twimblecon.com and while you're there, send me a message on the

16
00:01:33,240 --> 00:01:42,440
chat for information about our team discounts, and now on to the show.

17
00:01:42,440 --> 00:01:45,320
Alright everyone, I am on the line with Gary Marcus.

18
00:01:45,320 --> 00:01:52,040
Gary is the CEO and founder at roboss.ai, he was also the CEO and founder of the machine

19
00:01:52,040 --> 00:01:58,280
learning startup Geometric Intelligence, which was acquired by Uber in 2016.

20
00:01:58,280 --> 00:02:03,720
Gary is the author of five books, including his latest rebooting AI, which will be available

21
00:02:03,720 --> 00:02:06,040
on the day this podcast is published.

22
00:02:06,040 --> 00:02:08,640
Gary, welcome to this week machine learning and AI.

23
00:02:08,640 --> 00:02:09,960
Thanks for having me.

24
00:02:09,960 --> 00:02:15,320
I am really excited to jump in and chat with you about this book.

25
00:02:15,320 --> 00:02:19,320
I had a chance to dig into it.

26
00:02:19,320 --> 00:02:22,160
And awesome, awesome book.

27
00:02:22,160 --> 00:02:23,400
Let's just jump in.

28
00:02:23,400 --> 00:02:26,920
Before we really dive into talking about the book, I'd love to explore a little bit about

29
00:02:26,920 --> 00:02:32,960
your background you spent quite a bit of your career at NYU as a professor of psychology

30
00:02:32,960 --> 00:02:34,960
and neuroscience.

31
00:02:34,960 --> 00:02:39,920
Tell us a little bit about your background and the perspective that this creates for you.

32
00:02:39,920 --> 00:02:44,000
So I'm trained primarily as a cognitive scientist.

33
00:02:44,000 --> 00:02:48,920
My research for many years and my PhD with Steve Pinker was all about how children learn

34
00:02:48,920 --> 00:02:53,200
language and how children start to understand the world.

35
00:02:53,200 --> 00:02:56,760
So I'm a developmental cognitive scientist by training.

36
00:02:56,760 --> 00:03:00,160
And at the same time, I've been interested in AI since I was about eight years old when

37
00:03:00,160 --> 00:03:02,640
I first learned about programming computers.

38
00:03:02,640 --> 00:03:08,080
And in the last seven years or so, I've focused almost exclusively on answering the question,

39
00:03:08,080 --> 00:03:10,720
what can cognitive science bring to AI?

40
00:03:10,720 --> 00:03:16,520
So AI is currently dominated by certain statistical approaches that from my perspective as a cognitive

41
00:03:16,520 --> 00:03:20,440
scientist, as someone who studies how humans work, seem a little weird to me.

42
00:03:20,440 --> 00:03:25,720
So I don't think of children as giant data machines, but the way that AI is kind of rolling

43
00:03:25,720 --> 00:03:28,280
right now, it's all about big data.

44
00:03:28,280 --> 00:03:33,880
And I've been trying to see what I can contribute to AI from the perspective of cognitive science.

45
00:03:33,880 --> 00:03:39,600
So when you were, when you created geometric intelligence, was that a company that really

46
00:03:39,600 --> 00:03:44,960
commercialized a cognitive science based approach or was there a statistical approach involved

47
00:03:44,960 --> 00:03:47,520
in your work there?

48
00:03:47,520 --> 00:03:52,040
Well, geometric intelligence, which was my first company, was inspired in some ways by

49
00:03:52,040 --> 00:03:53,040
a cognitive science.

50
00:03:53,040 --> 00:03:54,040
It wasn't slavish to it.

51
00:03:54,040 --> 00:03:58,600
So there's always this tension of, you know, if you're building airplanes, you don't want

52
00:03:58,600 --> 00:04:03,280
to fly exactly like birds, because that wouldn't make any sense and who wants to flap their

53
00:04:03,280 --> 00:04:05,480
wings so many times a minute.

54
00:04:05,480 --> 00:04:10,040
But you also want to understand something about the dynamics of flight in my last company

55
00:04:10,040 --> 00:04:15,000
and also in this company, we're trying to take some lessons from biology, in particular

56
00:04:15,000 --> 00:04:19,920
from how humans think, and apply those to AI problems.

57
00:04:19,920 --> 00:04:26,520
So we're not in the last company was not like trying to be neuroscientifically, to perfectly

58
00:04:26,520 --> 00:04:27,520
accurate.

59
00:04:27,520 --> 00:04:28,520
We're not trying to be faithful to the brain.

60
00:04:28,520 --> 00:04:30,960
We're trying to take inspiration from the brain.

61
00:04:30,960 --> 00:04:34,520
The last company, the broad problem that it was trying to address is how do you learn

62
00:04:34,520 --> 00:04:36,400
from small amounts of data?

63
00:04:36,400 --> 00:04:39,480
And that question itself in some ways comes from cognitive science.

64
00:04:39,480 --> 00:04:43,000
I think machine learning is catching up to it now in the last couple of years, but

65
00:04:43,000 --> 00:04:48,320
it's always been clear from cognitive science, especially from the field of language acquisition

66
00:04:48,320 --> 00:04:51,160
that learning from small data is the name of the game.

67
00:04:51,160 --> 00:04:54,800
Children can generalize from tiny amounts of examples.

68
00:04:54,800 --> 00:05:00,120
My dissertation was about how children learn the ADD rule for forming the past tense,

69
00:05:00,120 --> 00:05:02,040
which they sometimes use incorrectly.

70
00:05:02,040 --> 00:05:04,320
They'll say gold or went to things like that.

71
00:05:04,320 --> 00:05:06,360
They learn that from a small amount of data.

72
00:05:06,360 --> 00:05:11,640
Sometimes they make mistakes and over-apply it, but they don't have gigabytes of data the

73
00:05:11,640 --> 00:05:16,160
way that say the GPT system does now.

74
00:05:16,160 --> 00:05:21,760
And so the last company was really focused on one particular way of solving this small

75
00:05:21,760 --> 00:05:22,760
data problem.

76
00:05:22,760 --> 00:05:27,600
And our, I think, most impressive results were we were beating deep learning in terms

77
00:05:27,600 --> 00:05:28,920
of data efficiency.

78
00:05:28,920 --> 00:05:34,160
So we could learn things with half as much data without having specific priors about

79
00:05:34,160 --> 00:05:35,640
the nature of the things we were learning.

80
00:05:35,640 --> 00:05:39,200
So we would take M-NIST, which is benchmarked probably a lot of your audience knows,

81
00:05:39,200 --> 00:05:40,440
recognizing characters.

82
00:05:40,440 --> 00:05:44,920
We could do M-NIST with half as much data without having to build in anything about the nature

83
00:05:44,920 --> 00:05:46,520
of letters or anything like that.

84
00:05:46,520 --> 00:05:50,560
So we were working towards a general way of doing supervised learning and maybe some

85
00:05:50,560 --> 00:05:53,320
other things using less data.

86
00:05:53,320 --> 00:05:55,000
And we were inspired there by humans.

87
00:05:55,000 --> 00:05:58,400
We weren't necessarily doing it exactly the way the humans do.

88
00:05:58,400 --> 00:06:03,720
But I think the core intellectual property is something that Zubin Garamani and I developed

89
00:06:03,720 --> 00:06:07,920
and I sort of set a direction that was based on some things that made sense to me from

90
00:06:07,920 --> 00:06:12,000
a cognitive science perspective and Zubin, who's brilliant mathematician, figured out

91
00:06:12,000 --> 00:06:13,520
how to apply it.

92
00:06:13,520 --> 00:06:21,720
And so, you know, I think a lot of our listeners, when they hear the idea of creating AI on

93
00:06:21,720 --> 00:06:26,960
limited data, we'll think about things like, you know, one-shot learning, zero-shot learning.

94
00:06:26,960 --> 00:06:31,400
But it sounds like your approach was very different from these or was it?

95
00:06:31,400 --> 00:06:35,400
I mean, there's some interrelations and I can't say too much because Uber owns the IP

96
00:06:35,400 --> 00:06:38,400
and there's NDAs and all that kind of stuff.

97
00:06:38,400 --> 00:06:43,680
But I would say that zero-shot learning and one-shot learning, first of all, are names

98
00:06:43,680 --> 00:06:48,440
of problems, they're not names of techniques and people use different kinds of techniques

99
00:06:48,440 --> 00:06:49,440
to do them.

100
00:06:49,440 --> 00:06:53,040
And they're often, I think, narrowly construed.

101
00:06:53,040 --> 00:06:56,280
So, you know, there are lots of problems in the world where you have some data.

102
00:06:56,280 --> 00:07:00,880
It's not the zero data, but you just don't have that much.

103
00:07:00,880 --> 00:07:04,880
Something I often like to talk about is what my daughter did when she climbed through

104
00:07:04,880 --> 00:07:05,880
a chair.

105
00:07:05,880 --> 00:07:08,680
So, we were sitting in a Whole Foods about a year and a half ago, she was about four

106
00:07:08,680 --> 00:07:12,280
and a half years old or four years old at the time.

107
00:07:12,280 --> 00:07:17,280
And we sat in a chair that had a back and then a gap between the back and the base of

108
00:07:17,280 --> 00:07:20,200
the chair, if you can kind of visualize that.

109
00:07:20,200 --> 00:07:22,880
And she'd never seen the TV program, The Dukes of Hazard, where they climbed through

110
00:07:22,880 --> 00:07:23,880
the windows.

111
00:07:23,880 --> 00:07:28,720
So, she didn't have any data from like a model of people doing wacky things, sticking

112
00:07:28,720 --> 00:07:33,160
their bodies through, you know, an aperture inside of a chair.

113
00:07:33,160 --> 00:07:38,400
So, this was not a big data problem, or at least there wasn't a lot of directly relevant

114
00:07:38,400 --> 00:07:39,400
big data.

115
00:07:39,400 --> 00:07:43,480
She had data about how her body worked, the size of her body, and she probably explored

116
00:07:43,480 --> 00:07:45,960
other apertures before.

117
00:07:45,960 --> 00:07:51,440
She did what a lot of people might call abstract least unsupervised learning, but she didn't

118
00:07:51,440 --> 00:07:54,080
use any of the techniques that we would call unsupervised learning.

119
00:07:54,080 --> 00:07:59,400
So, it was unsupervised in the sense that she didn't have training examples saying this

120
00:07:59,400 --> 00:08:06,160
is the right, you know, torque to apply to your torso in order to spin through the chair,

121
00:08:06,160 --> 00:08:11,960
right, in the way like a reinforcement learning robot might try it a million times and get

122
00:08:11,960 --> 00:08:12,960
reinforcement.

123
00:08:12,960 --> 00:08:13,960
I got stuck this way.

124
00:08:13,960 --> 00:08:15,440
I didn't get stuck that way and so forth.

125
00:08:15,440 --> 00:08:18,800
She just did it in the space of like a minute.

126
00:08:18,800 --> 00:08:21,840
And then the second time that she did it, I asked her to reenact it and I took pictures

127
00:08:21,840 --> 00:08:22,840
of the second time.

128
00:08:22,840 --> 00:08:26,920
I wish I had taken pictures the first time or taken a video of the second, but anyway,

129
00:08:26,920 --> 00:08:31,240
you look at this sequence of pictures that I took and she actually got stuck at one point

130
00:08:31,240 --> 00:08:33,240
and then she figured out how to get unstuck.

131
00:08:33,240 --> 00:08:37,840
And so there was problem solving process there and there was also kind of leveraging modest

132
00:08:37,840 --> 00:08:38,840
amounts of data.

133
00:08:38,840 --> 00:08:43,520
She had no direct data on this problem except what she got from trying it herself in that

134
00:08:43,520 --> 00:08:44,520
moment.

135
00:08:44,520 --> 00:08:48,280
And then she had a bunch of background data from other kinds of problems that she had

136
00:08:48,280 --> 00:08:49,280
solved.

137
00:08:49,280 --> 00:08:53,360
And she knew enough, maybe not consciously, but unconsciously, about physics and how

138
00:08:53,360 --> 00:08:56,360
her body moved and so forth that she could integrate all of that.

139
00:08:56,360 --> 00:09:00,280
So that doesn't fall into the paradigm of zero shot learning, although you could sort

140
00:09:00,280 --> 00:09:03,680
of call it a zero shot problem, but it's not like the things that people do in literature

141
00:09:03,680 --> 00:09:08,080
and it doesn't fall into the one shot learning and it doesn't really fit with how people

142
00:09:08,080 --> 00:09:12,000
think about unsupervised learning where they like take clusters of things or predict the

143
00:09:12,000 --> 00:09:13,480
next frames in the video.

144
00:09:13,480 --> 00:09:16,200
It's not really like any of those problems.

145
00:09:16,200 --> 00:09:20,200
And yet it's kind of what little kids like my children do all the time.

146
00:09:20,200 --> 00:09:24,040
They say, here is some challenge that I have never confronted before.

147
00:09:24,040 --> 00:09:25,560
I'm going to figure it out.

148
00:09:25,560 --> 00:09:31,040
It's like 80% I feel like I'm probably exaggerating, but it's a large fraction of what my kids

149
00:09:31,040 --> 00:09:32,440
do is they set new challenges.

150
00:09:32,440 --> 00:09:37,440
So right now my son's a little older, he's six and a half, my daughter's five now.

151
00:09:37,440 --> 00:09:41,720
And they like play games all day long and they don't play existing games, they play games

152
00:09:41,720 --> 00:09:42,720
that they invent.

153
00:09:42,720 --> 00:09:46,000
And so they're like, well, let's pretend you can't fly anymore because you broke your

154
00:09:46,000 --> 00:09:47,000
wing or whatever.

155
00:09:47,000 --> 00:09:51,880
They're constantly making up assumptions and then doing problem solving relative to those

156
00:09:51,880 --> 00:09:52,880
reference points.

157
00:09:52,880 --> 00:09:57,000
It's just completely far away from what people are doing in AI now.

158
00:09:57,000 --> 00:10:00,960
And part of the reason that Ernie Davis and I wrote this book rebooting AI is to like

159
00:10:00,960 --> 00:10:04,560
reorient the field and reboot just like start over.

160
00:10:04,560 --> 00:10:09,200
So we're doing great on all this supervised learning stuff where we have a ton of data,

161
00:10:09,200 --> 00:10:10,200
ton of label data.

162
00:10:10,200 --> 00:10:13,840
But you know, the reality is that's not really what the real world is like.

163
00:10:13,840 --> 00:10:18,120
And it's certainly not like what children do as they come to understand the world.

164
00:10:18,120 --> 00:10:22,840
And there's a gap right now between I think memorizing or doing something is a little

165
00:10:22,840 --> 00:10:25,440
bit better than memorization and understanding.

166
00:10:25,440 --> 00:10:29,400
So deep learning is like a better way of doing memorization, you can interpolate between

167
00:10:29,400 --> 00:10:32,840
examples you've seen before, but it's not really about comprehension, it's not really

168
00:10:32,840 --> 00:10:37,400
about like building a model of chairs and apertures and bodies and understanding how

169
00:10:37,400 --> 00:10:38,400
those interrelate.

170
00:10:38,400 --> 00:10:43,480
And so what Ernie and I are trying to do is to get the feel to look in a different direction

171
00:10:43,480 --> 00:10:46,920
that's more about comprehension and understanding and so forth.

172
00:10:46,920 --> 00:10:50,880
Going back to your question for a second, I mean, did my last company do all of that?

173
00:10:50,880 --> 00:10:55,080
No, I mean, we were a small startup, we were, when we were bought, we were 15 people,

174
00:10:55,080 --> 00:11:00,320
we had one very specific way of solving a supervised learning problem with less data.

175
00:11:00,320 --> 00:11:05,240
There's a lot that goes into the human way approach to less data.

176
00:11:05,240 --> 00:11:08,120
Another thing that goes into it that we didn't work on in the last company at all is an

177
00:11:08,120 --> 00:11:09,120
ateness.

178
00:11:09,120 --> 00:11:14,560
So in Chomsky's arguments, which I think are correct, is that we start with something

179
00:11:14,560 --> 00:11:16,520
that constrains how we learn language.

180
00:11:16,520 --> 00:11:21,480
We're not open to any possible language. We're born knowing certain things about language.

181
00:11:21,480 --> 00:11:25,240
I differ from him a little bit about what those things are, but I would say we're probably

182
00:11:25,240 --> 00:11:30,120
born knowing that you can concatenate symbols in order to express things.

183
00:11:30,120 --> 00:11:31,120
Is it not conscious?

184
00:11:31,120 --> 00:11:32,120
Is it not conscious?

185
00:11:32,120 --> 00:11:33,120
Is it not conscious?

186
00:11:33,120 --> 00:11:34,120
Is it not conscious?

187
00:11:34,120 --> 00:11:38,080
I was about to say, it may not be conscious, but I'll tell you about an experiment that

188
00:11:38,080 --> 00:11:44,040
I did, which is probably my best known result in the psychology literature.

189
00:11:44,040 --> 00:11:50,160
I taught seven-month-old kids in artificial language, and I didn't tell them the rules

190
00:11:50,160 --> 00:11:51,160
for the language.

191
00:11:51,160 --> 00:11:55,120
It just gave them examples, two minutes, and that was enough for seven-month-old babies

192
00:11:55,120 --> 00:11:56,760
to figure out the abstract grammar.

193
00:11:56,760 --> 00:12:02,960
So they heard sentences like La Tata, Ganana, so they had an ABB pattern, of course we're

194
00:12:02,960 --> 00:12:07,760
psychologists, we counterbalance, so others saw the AAB pattern and some saw an ABA pattern,

195
00:12:07,760 --> 00:12:08,760
et cetera.

196
00:12:08,760 --> 00:12:13,000
So you hear one of these grammars, and now you have to hear new sentences that are made

197
00:12:13,000 --> 00:12:14,000
up.

198
00:12:14,000 --> 00:12:18,800
Well, they're all made up of the same words, but some of them, or sorry, the new words,

199
00:12:18,800 --> 00:12:21,400
and some of them also have a new grammar, some of the same grammar.

200
00:12:21,400 --> 00:12:25,400
So you hear ABA sentences, now you get tested on ABB sentences, or not.

201
00:12:25,400 --> 00:12:29,240
And what we found was that the seven-month-olds, they're not paid for their participation

202
00:12:29,240 --> 00:12:33,680
and they're getting course credit for intro psych, they're just sitting there listening,

203
00:12:33,680 --> 00:12:35,720
but they want to know what's going on here.

204
00:12:35,720 --> 00:12:38,320
So they hear two minutes of this stuff, and they can already tell when you change to

205
00:12:38,320 --> 00:12:39,320
a new grammar.

206
00:12:39,320 --> 00:12:40,640
They start paying attention more.

207
00:12:40,640 --> 00:12:44,480
So they habituate, they get bored by hearing the same thing over and over again, and then

208
00:12:44,480 --> 00:12:48,200
they dishabituate, they show interest, when we change the grammar.

209
00:12:48,200 --> 00:12:52,880
I was hoping that this was going to be teaching them Klingon, or Dothraki, although it's

210
00:12:52,880 --> 00:12:55,720
probably going to be a little bit after that time.

211
00:12:55,720 --> 00:13:01,240
My favorite line of all of the Star Trek movies was about hearing Shakespeare in the original

212
00:13:01,240 --> 00:13:03,720
Klingon.

213
00:13:03,720 --> 00:13:14,040
So subjects in our experiment heard something that was even proto-Klingon, proto-human.

214
00:13:14,040 --> 00:13:17,520
In any case, they picked this up.

215
00:13:17,520 --> 00:13:22,120
And then what happens at developmental psychology is if you do an experiment, you show that kids

216
00:13:22,120 --> 00:13:26,000
of a certain age can do things, then if it's interesting enough, people try to extend

217
00:13:26,000 --> 00:13:29,840
it in different ways, as many people have done for this.

218
00:13:29,840 --> 00:13:32,400
And they also try to show that even younger kids can do it.

219
00:13:32,400 --> 00:13:36,040
So somebody's actually shown using brain measures.

220
00:13:36,040 --> 00:13:40,400
It's not a perfect experiment, but a pretty good one, showed that even newborns are doing

221
00:13:40,400 --> 00:13:44,000
the same kind of grammatical analysis that our seven-month-olds were doing.

222
00:13:44,000 --> 00:13:49,240
We show that kids were more likely to do this with speech than with musical tones and

223
00:13:49,240 --> 00:13:50,240
stuff like that.

224
00:13:50,240 --> 00:13:55,360
And if they could get the gist of it from speech, then they could transfer it to musical tones,

225
00:13:55,360 --> 00:13:57,640
but they didn't analyze musical tones in the same way.

226
00:13:57,640 --> 00:14:02,040
It's a very interesting set of results with the bottom line for what we're talking about.

227
00:14:02,040 --> 00:14:07,200
It does actually suggest that some of the roots of grammar are there as early as we can

228
00:14:07,200 --> 00:14:09,000
test children.

229
00:14:09,000 --> 00:14:12,200
As an aside, something can be innate and not be there at birth.

230
00:14:12,200 --> 00:14:16,040
So my capacity to grow a beard was innate, I didn't learn how to do it.

231
00:14:16,040 --> 00:14:20,200
But it wasn't expressed until I was, I don't know, 14 or 15 years old.

232
00:14:20,200 --> 00:14:24,480
So people in developmental psychology literature get confused and think if it's not there

233
00:14:24,480 --> 00:14:26,280
at birth, it's not innate.

234
00:14:26,280 --> 00:14:30,520
But the human brain comes out of the oven before it's fully developed, not everything

235
00:14:30,520 --> 00:14:33,600
that happens afterwards is about learning.

236
00:14:33,600 --> 00:14:36,680
In any case, I would say that we have innately that.

237
00:14:36,680 --> 00:14:41,520
We also have innately probably a distinction about subjects and predicates, like these entities

238
00:14:41,520 --> 00:14:45,840
are undergoing this change or something like that, and various other things.

239
00:14:45,840 --> 00:14:49,480
So when we start getting exposed to language, we have some hooks already built in that we

240
00:14:49,480 --> 00:14:51,200
can attach that to.

241
00:14:51,200 --> 00:14:56,240
Now you compare that to the kind of reinforcement learning for language acquisition experiments

242
00:14:56,240 --> 00:15:00,560
that deep mind has played around with over the last few years.

243
00:15:00,560 --> 00:15:06,240
If I can jump in here, I think to contextualize reinforcement learning is one of the things

244
00:15:06,240 --> 00:15:13,720
that many in the field are really excited about because in a lot of ways, it appears as one

245
00:15:13,720 --> 00:15:19,200
of the closest things we have to learning the way children learn, right?

246
00:15:19,200 --> 00:15:23,360
There's an environment, you know, there's an agent that interacts with that environment

247
00:15:23,360 --> 00:15:28,920
and learns things if we can put that learning around air quotes.

248
00:15:28,920 --> 00:15:31,920
It's all true, but it's missing something and that's sort of where it was going.

249
00:15:31,920 --> 00:15:36,160
So I think you characterize correctly what reinforcement and deep reinforcement learnings

250
00:15:36,160 --> 00:15:37,160
to do.

251
00:15:37,160 --> 00:15:40,400
And at the level of abstraction that you described, it has to be right, right?

252
00:15:40,400 --> 00:15:44,600
I mean, what you do is you try things in the environment and you look for feedback.

253
00:15:44,600 --> 00:15:47,080
And one of the forms of feedback is sort of am I right or am I wrong?

254
00:15:47,080 --> 00:15:50,840
And that's what does this lead to a good outcome or outcome?

255
00:15:50,840 --> 00:15:52,880
And that's what reinforcement learning is all about.

256
00:15:52,880 --> 00:15:57,200
And you know, and saying would argue that some of that happens.

257
00:15:57,200 --> 00:15:58,200
Right.

258
00:15:58,200 --> 00:16:03,440
And there's a lot of conversation around and how difficult it is in practice, you know,

259
00:16:03,440 --> 00:16:09,800
for a variety of reasons, sample inefficiencies and how difficult it is to get the reward function

260
00:16:09,800 --> 00:16:11,280
correctly.

261
00:16:11,280 --> 00:16:15,280
But I think you're saying something more fundamental about the difference between

262
00:16:15,280 --> 00:16:17,080
real learning and reinforcement learning.

263
00:16:17,080 --> 00:16:18,080
Yeah.

264
00:16:18,080 --> 00:16:22,360
You know, in the improv school, the improv school, they teach you, yes, and, right.

265
00:16:22,360 --> 00:16:28,440
So yes, and you need a whole lot of other stuff to make it actually work.

266
00:16:28,440 --> 00:16:34,320
So the problem right now is like a stereotypical version of this is the Atari game system that

267
00:16:34,320 --> 00:16:39,040
DeepMind built, which led to their sale or was part of why Google bought them for all

268
00:16:39,040 --> 00:16:40,040
that money.

269
00:16:40,040 --> 00:16:44,200
And what was cool about it was they could play a lot of different games.

270
00:16:44,200 --> 00:16:47,320
They could play breakout, play space invaders and something they could play at superhuman

271
00:16:47,320 --> 00:16:52,840
level, and there was nothing built in about the rules of any of those games, which by

272
00:16:52,840 --> 00:16:56,240
the way is not true of their go demonstrations and so forth.

273
00:16:56,240 --> 00:17:01,000
But that original demo, it was really like interesting intellectual proof of concept.

274
00:17:01,000 --> 00:17:03,880
However, it didn't really work that well.

275
00:17:03,880 --> 00:17:07,840
In the sense that if you change the circumstances at all, it all fell apart.

276
00:17:07,840 --> 00:17:12,120
So Vicarious had a really nice demo, and actually my company had a similar demo.

277
00:17:12,120 --> 00:17:16,040
My last company that we never published, but it makes the same point.

278
00:17:16,040 --> 00:17:21,160
So Vicarious did was they took breakout and they moved the paddle up a few pixels, and

279
00:17:21,160 --> 00:17:27,080
you have like this famous video on the web of the DeepMind thing playing breakout learning

280
00:17:27,080 --> 00:17:32,840
to break through the wall and people kind of talk in very cognitive language about how

281
00:17:32,840 --> 00:17:35,520
the system has learned to go through the wall or whatever.

282
00:17:35,520 --> 00:17:40,880
And then when Vicarious moved the paddle up three pixels suddenly performance went from

283
00:17:40,880 --> 00:17:43,360
superhuman to mediocre.

284
00:17:43,360 --> 00:17:47,680
As a system hadn't really learned what a ball is, what a paddle is, or what a wall is.

285
00:17:47,680 --> 00:17:51,520
What it really learned was the statistical contingencies that worked on the screen in

286
00:17:51,520 --> 00:17:53,600
which it had been trained.

287
00:17:53,600 --> 00:17:58,560
And that's much more superficial than the human way of playing breakout, which is to learn

288
00:17:58,560 --> 00:18:04,760
about the kind of physics within the game of the ball, the paddle, and the bricks.

289
00:18:04,760 --> 00:18:09,840
And so going back to reinforcement learning, DeepMind's right to set it up as a reinforcement

290
00:18:09,840 --> 00:18:13,560
learning problem, but they're actually wrong to build nothing at all in.

291
00:18:13,560 --> 00:18:18,520
And when they want to, because they, if you start from zero, you just don't get that far.

292
00:18:18,520 --> 00:18:23,920
So in fact, when they play Go, which is in some ways a hard to problem, some ways not,

293
00:18:23,920 --> 00:18:28,360
they build in the rules of Go, they build in Monte Carlo tree search, which sets up the

294
00:18:28,360 --> 00:18:32,440
problem is kind of if I go there, then you go there, they don't learn that stuff.

295
00:18:32,440 --> 00:18:38,840
And they've been putting out all of this kind of PR around a very blank slate approach

296
00:18:38,840 --> 00:18:42,640
and where there's nothing built in, but they don't actually do that when they solve

297
00:18:42,640 --> 00:18:43,640
the hard problems.

298
00:18:43,640 --> 00:18:48,160
And then they had this paper that really kind of aggravated me called mastering Go without

299
00:18:48,160 --> 00:18:49,160
human knowledge.

300
00:18:49,160 --> 00:18:52,720
And in fact, they had like a world class Go player on the team, and there's lots of ways

301
00:18:52,720 --> 00:18:54,480
in which human knowledge was embedded.

302
00:18:54,480 --> 00:19:00,040
I have an archive article, ARXIV, maybe you can link and show notes, taking apart all

303
00:19:00,040 --> 00:19:02,600
the human knowledge that actually went in there.

304
00:19:02,600 --> 00:19:05,960
But they're doing it kind of through the back door, like, hey, nobody pay attention to

305
00:19:05,960 --> 00:19:11,120
the fact that we built in the rules and carefully designed how many layers through a lot of

306
00:19:11,120 --> 00:19:15,080
experimentation and haven't tested it on a different size board.

307
00:19:15,080 --> 00:19:19,120
What we really need to do is to think about the principle information that needs to be

308
00:19:19,120 --> 00:19:24,080
built in in conjunction with reinforcement learning, so it will work.

309
00:19:24,080 --> 00:19:28,200
You could say that what my daughter did was like online reinforcement learning, but she

310
00:19:28,200 --> 00:19:33,360
wasn't just like, what happens if I do this small torque on this limb, she was trying

311
00:19:33,360 --> 00:19:39,080
to figure out how to do this relative to a pretty rich model of her own body in the three-dimensional

312
00:19:39,080 --> 00:19:43,760
geometry of the world and a knowledge about the physics of like rigid objects, you probably

313
00:19:43,760 --> 00:19:48,440
would have done it differently if the back was made of string, and she had more space

314
00:19:48,440 --> 00:19:49,440
and it was flexible.

315
00:19:49,440 --> 00:19:54,600
And I think you said a lot of actually reasoning in the context of reinforcement learning.

316
00:19:54,600 --> 00:19:55,600
Yeah, yeah.

317
00:19:55,600 --> 00:20:00,200
I think it's an important word there in model.

318
00:20:00,200 --> 00:20:07,200
One of the recurring themes on this show that I've talked about quite a bit that really

319
00:20:07,200 --> 00:20:12,480
just kind of jumped out for me after a number of interviews was like this pendulum swing

320
00:20:12,480 --> 00:20:19,280
from a world in which the way we understood the environment that we interact with this

321
00:20:19,280 --> 00:20:24,800
through creating these models around physics and engineering, et cetera, now we've kind

322
00:20:24,800 --> 00:20:30,040
of gone to the other end of the pendulum swing and everything is, there's a lot of focus

323
00:20:30,040 --> 00:20:36,120
and excitement around statistical approaches, and there is a lot of interesting work happening

324
00:20:36,120 --> 00:20:42,360
kind of at really more of an equilibrium point where folks are looking at marrying the

325
00:20:42,360 --> 00:20:47,480
model-based approaches and statistical approaches, and it sounds like what you're really proposing

326
00:20:47,480 --> 00:20:57,280
is something similar to the, you know, in support of AGI, artificial general intelligence

327
00:20:57,280 --> 00:21:04,000
or GAI general artificial intelligence, whichever of those acronyms you prefer, do you agree

328
00:21:04,000 --> 00:21:05,000
with that?

329
00:21:05,000 --> 00:21:09,520
I would slightly rephrase what you just said, but basically I agree with it.

330
00:21:09,520 --> 00:21:14,400
The problem right now is we're mostly focusing on narrow intelligence and deep learning is

331
00:21:14,400 --> 00:21:20,200
a very good tool for that or deeper reinforcement learning for certain narrow problems, but those

332
00:21:20,200 --> 00:21:24,160
tools are not good for general intelligence, whether you want to call that AGI or GAI

333
00:21:24,160 --> 00:21:29,400
or what have you, but the kind of intelligence that is inherent in a flexible person for

334
00:21:29,400 --> 00:21:33,240
example, you can solve problems in different ways if the assumptions change a little bit

335
00:21:33,240 --> 00:21:37,680
from what you started with, and to get there, you need to do exactly what you just said,

336
00:21:37,680 --> 00:21:43,440
you need to marry the modeling approach which may use, for example, symbolic techniques

337
00:21:43,440 --> 00:21:48,120
from classical AI with the more statistical techniques that we have now, and if you look

338
00:21:48,120 --> 00:21:50,040
at humans, that's exactly what they do.

339
00:21:50,040 --> 00:21:54,400
We do some perceptual stuff that seems to be kind of driven by statistics and a lot

340
00:21:54,400 --> 00:21:59,600
of experience, and we do some abstract reasoning and language and so forth that don't seem

341
00:21:59,600 --> 00:22:01,600
to use the same mechanisms.

342
00:22:01,600 --> 00:22:06,560
Connemons cut on that is system one versus system two, the kind of reflexive system versus

343
00:22:06,560 --> 00:22:08,640
deliberative systems, the way I like to talk about those.

344
00:22:08,640 --> 00:22:10,400
I think you fast thinking slow.

345
00:22:10,400 --> 00:22:15,520
And I think that the AI techniques that we have right now are good, I wouldn't even

346
00:22:15,520 --> 00:22:20,440
call it a thinking, but at classifying, which is a little bit like his system one, classifying

347
00:22:20,440 --> 00:22:25,400
it's seen this pattern before, it looks like this other thing that I've seen, but we're

348
00:22:25,400 --> 00:22:30,080
not right now as a community focusing that much on the kind of system two stuff where

349
00:22:30,080 --> 00:22:34,440
you deliberate where you recognize your assumptions are wrong and you change, and that's where

350
00:22:34,440 --> 00:22:35,440
the model stuff lives.

351
00:22:35,440 --> 00:22:39,760
The system one stuff, but you don't have time for a model, you're just relying on kind

352
00:22:39,760 --> 00:22:43,680
of memory traces effectively of things you've seen before.

353
00:22:43,680 --> 00:22:47,040
The system two kind of stuff, you have to have a cognitive model.

354
00:22:47,040 --> 00:22:48,200
What is going on here?

355
00:22:48,200 --> 00:22:51,480
What are the causal relationships between these entities?

356
00:22:51,480 --> 00:22:53,920
If I change this thing, what would happen to this other thing?

357
00:22:53,920 --> 00:22:58,760
That kind of reasoning, which humans do all the time, they do it grad school, but we

358
00:22:58,760 --> 00:23:04,640
also just do it in daily life, that part's not being captured and so much money is going

359
00:23:04,640 --> 00:23:11,400
into the deep learning side that it's kind of perverted, I think, the mission of AI,

360
00:23:11,400 --> 00:23:16,320
which was originally to go after general AI, it was much more, I think, originally sympathetic

361
00:23:16,320 --> 00:23:18,120
to learning from humans.

362
00:23:18,120 --> 00:23:22,040
Right now, the kind of mathematicians and cluster builders have the upper hand, like computer

363
00:23:22,040 --> 00:23:26,520
cluster builders have the upper hand, because these techniques are yielding a lot of short-term

364
00:23:26,520 --> 00:23:31,440
fruit, but I think it's perverting the overall direction of the field, and that's what this

365
00:23:31,440 --> 00:23:34,400
book is about, is to try to correct the shape.

366
00:23:34,400 --> 00:23:37,920
Not to throw that stuff overboard, we need it too.

367
00:23:37,920 --> 00:23:41,600
The deep learning or some success or two, it's going to stick around, but we need to have

368
00:23:41,600 --> 00:23:46,360
some focus on other things, like, how do you do causal reasoning about what happens if

369
00:23:46,360 --> 00:23:48,920
I do this thing to this system, or temporal reasoning?

370
00:23:48,920 --> 00:23:51,120
Where is this system going to be 20 minutes from now?

371
00:23:51,120 --> 00:23:55,800
I just saw a paper showing that people are getting pretty good at using deep learning

372
00:23:55,800 --> 00:24:00,320
system to predict the next clock tick, like the next, say, quarter second in a video, or

373
00:24:00,320 --> 00:24:05,720
the next frame in a video, and they're pretty terrible, just bouncing balls, billiard

374
00:24:05,720 --> 00:24:06,720
balls.

375
00:24:06,720 --> 00:24:10,120
They're good at that, but they had no way of predicting what would happen five minutes

376
00:24:10,120 --> 00:24:11,120
later.

377
00:24:11,120 --> 00:24:14,160
You could look and say, well, they're going to come to rest, and they're going to be scattered

378
00:24:14,160 --> 00:24:18,320
in distribution like this, and the deep learning systems couldn't do that.

379
00:24:18,320 --> 00:24:23,160
They could make these very short-term predictions, essentially, by consulting a library of videos

380
00:24:23,160 --> 00:24:27,160
they've seen before, interpolating over that library, but that's not the same thing as

381
00:24:27,160 --> 00:24:28,160
temporal reasoning.

382
00:24:28,160 --> 00:24:33,760
I tell you that I have an airplane ticket to go to California next week, you can predict

383
00:24:33,760 --> 00:24:37,880
it, I'll be at the airport on Tuesday morning, or if I give you enough information, you

384
00:24:37,880 --> 00:24:41,440
can make these long-term predictions that aren't about looking at frames in a video library.

385
00:24:41,440 --> 00:24:45,680
They're about reasoning about abstract entities like, okay, he needs to be in California

386
00:24:45,680 --> 00:24:49,400
on Tuesday, then he's probably going to have to go to the airport to do that, who probably

387
00:24:49,400 --> 00:24:50,400
have to go through a security line.

388
00:24:50,400 --> 00:24:55,160
You make all these inferences about what's typical and what might happen, what if I missed

389
00:24:55,160 --> 00:24:58,400
my flight, and that will be the alternatives I would consider.

390
00:24:58,400 --> 00:25:02,200
You can do all of this higher-level reason that's very different from predicting the

391
00:25:02,200 --> 00:25:04,120
next frame of a video.

392
00:25:04,120 --> 00:25:07,560
I guess one of the questions I have that's maybe a little bit of a pushback on that is,

393
00:25:07,560 --> 00:25:13,800
is it just okay that we're making a lot of progress in a narrow way in AI right now?

394
00:25:13,800 --> 00:25:18,800
Even in the book, you talk about curing or treating cancer and venting new materials,

395
00:25:18,800 --> 00:25:27,800
addressing climate change, big AI, AGI, could have a huge impact on those areas, but we're

396
00:25:27,800 --> 00:25:33,720
also having small but significant impacts in a lot of those areas today with the narrow

397
00:25:33,720 --> 00:25:35,400
techniques that we have.

398
00:25:35,400 --> 00:25:38,000
There's two things there.

399
00:25:38,000 --> 00:25:40,760
There's two things there, okay, go ahead.

400
00:25:40,760 --> 00:25:46,320
One is, I just hate leaving that much potential for human progress on the floor.

401
00:25:46,320 --> 00:25:48,320
I think we could be doing better.

402
00:25:48,320 --> 00:25:52,360
I think with the amount of money that's being invested right now, if it was spent a little

403
00:25:52,360 --> 00:25:56,960
bit more wisely, we could make huge progress, and I think it's worth thinking about that

404
00:25:56,960 --> 00:26:00,680
rather than just sort of taking the next step because it's the obvious next step.

405
00:26:00,680 --> 00:26:04,440
Sometimes the obvious next step is not the efficient one.

406
00:26:04,440 --> 00:26:05,440
That's one reason.

407
00:26:05,440 --> 00:26:06,440
I deal with this.

408
00:26:06,440 --> 00:26:11,160
I'd like to see us make as rapid progress as possible, and I think we could do that by

409
00:26:11,160 --> 00:26:14,200
compensating where the ship is going a little bit.

410
00:26:14,200 --> 00:26:18,960
The other side of it is we're relying a lot on these techniques right now that are

411
00:26:18,960 --> 00:26:24,040
pretty dumb, and the people who made them more smart, but the techniques are dumb in

412
00:26:24,040 --> 00:26:27,720
the sense that they don't have cognitive models of what's going on.

413
00:26:27,720 --> 00:26:31,720
They're just relying on brute force or variations on brute force.

414
00:26:31,720 --> 00:26:36,200
By using those techniques that don't have rich understanding of the world, we get in

415
00:26:36,200 --> 00:26:37,200
trouble.

416
00:26:37,200 --> 00:26:40,600
People are applying those techniques, for example, to driverless cars.

417
00:26:40,600 --> 00:26:45,320
Within the driverless cars, a couple of days ago from when we were recording this, it

418
00:26:45,320 --> 00:26:49,960
looks like a Tesla drove into a tow truck that was parked on the side of the road.

419
00:26:49,960 --> 00:26:54,920
Because you mentioned this example in the book a couple of times about fire trucks and

420
00:26:54,920 --> 00:26:55,920
tow trucks.

421
00:26:55,920 --> 00:26:56,920
I had not heard this.

422
00:26:56,920 --> 00:26:57,920
This is a thing.

423
00:26:57,920 --> 00:27:01,840
I had mentioned the tow truck actually when we finished the book.

424
00:27:01,840 --> 00:27:02,840
Books go to press.

425
00:27:02,840 --> 00:27:03,840
They take a while.

426
00:27:03,840 --> 00:27:08,000
In the book, we mentioned fire trucks and police trucks who were stopped on the side of

427
00:27:08,000 --> 00:27:09,000
the road.

428
00:27:09,000 --> 00:27:13,400
Then a few days ago in Moscow, a Tesla rose into a tow truck.

429
00:27:13,400 --> 00:27:16,280
There's clearly a pattern there.

430
00:27:16,280 --> 00:27:20,720
It's continuing pattern, even as we're recording the interview.

431
00:27:20,720 --> 00:27:25,280
You want a system where at least if you have a couple of fatalities, oh, and there's tractor

432
00:27:25,280 --> 00:27:27,280
trailers too, that they've run over.

433
00:27:27,280 --> 00:27:33,560
We've had multiple fatalities from the running into tractor trailers, and I don't think

434
00:27:33,560 --> 00:27:39,000
any of these were fatal, but now multiple accidents basically running into emergency vehicles

435
00:27:39,000 --> 00:27:40,000
stopped.

436
00:27:40,000 --> 00:27:44,880
Well, we want to have an AI system where you can specify an abstract language, don't

437
00:27:44,880 --> 00:27:48,240
run into stopped emergency vehicles, and have the system that's smart enough to figure

438
00:27:48,240 --> 00:27:49,240
that out.

439
00:27:49,240 --> 00:27:53,080
We actually have our systems that need a lot of label data, and there aren't a lot of

440
00:27:53,080 --> 00:27:56,280
label data of tow trucks stopped on the side of the road.

441
00:27:56,280 --> 00:27:57,280
Nobody thought to get them.

442
00:27:57,280 --> 00:27:59,760
Maybe they'll collect them now, but there'll be some other case.

443
00:27:59,760 --> 00:28:01,720
This is what I mean by edge cases or outliers cases.

444
00:28:01,720 --> 00:28:05,440
There are going to be some other kind of emergency vehicle that looks a little bit different.

445
00:28:05,440 --> 00:28:08,400
A person's going to be able to reason that must be an emergency vehicle, but it's not

446
00:28:08,400 --> 00:28:09,920
going to be in the data set.

447
00:28:09,920 --> 00:28:13,760
The other part of the answer to your question is we're using this technique now.

448
00:28:13,760 --> 00:28:16,320
These are at stake, so we have to do something.

449
00:28:16,320 --> 00:28:22,120
Either we outlaw driverless cars until we figure out something better, or we work actively

450
00:28:22,120 --> 00:28:26,280
right now to think about what a better approach to AI would be.

451
00:28:26,280 --> 00:28:33,880
I thought this was a really well-stated and important point in the book, and that is

452
00:28:33,880 --> 00:28:36,240
around this core issue of trust.

453
00:28:36,240 --> 00:28:43,520
Other words around what you were just describing, and that we, in particular, in the general

454
00:28:43,520 --> 00:28:49,080
public sense of the word, we don't understand the way the things we're calling AI today

455
00:28:49,080 --> 00:28:56,360
are operating and don't understand the failure modes, and as a result of that, we trust

456
00:28:56,360 --> 00:29:01,040
them too much, and there's a potential as you were just describing that we put them

457
00:29:01,040 --> 00:29:07,040
in the situations where lives are at stake and people don't understand that they're likely

458
00:29:07,040 --> 00:29:08,040
to fail in some ways.

459
00:29:08,040 --> 00:29:09,040
That's right.

460
00:29:09,040 --> 00:29:12,680
And the same thing's happening over and over again, so people put a lot of stock and face

461
00:29:12,680 --> 00:29:16,520
recognition, and it's really not that good, so you have a lot of police departments running

462
00:29:16,520 --> 00:29:23,000
around using it, and effectively adds to our profiling problems, and it's not that good.

463
00:29:23,000 --> 00:29:28,120
All these things, they might be like 90% accurate, but we're using some of them in situations

464
00:29:28,120 --> 00:29:29,120
where we need more.

465
00:29:29,120 --> 00:29:36,440
It's fine if an advertisement recommendation system is 90% accurate, but if we use a face

466
00:29:36,440 --> 00:29:40,800
recognition system in crimes and it's 90% accurate, that's actually pretty bad.

467
00:29:40,800 --> 00:29:44,280
You don't want one in 10 missed calls there.

468
00:29:44,280 --> 00:29:49,880
And a driverless car thing, even if it's 99% accurate, that's not nearly good enough.

469
00:29:49,880 --> 00:29:54,120
And so when we're doing mission critical things with AI, that we don't understand, that's

470
00:29:54,120 --> 00:29:58,720
the interpretability problem you were just referring to, and they themselves don't really

471
00:29:58,720 --> 00:30:03,240
understand the world in which they're operating, that's a recipe for problems.

472
00:30:03,240 --> 00:30:09,840
And that's why the whole book is really about trust, is because we are increasingly assigning

473
00:30:09,840 --> 00:30:14,600
autonomy to systems that don't really understand the world, and that we don't really understand,

474
00:30:14,600 --> 00:30:16,440
we are getting ourselves in a bad position.

475
00:30:16,440 --> 00:30:19,080
The worst case here is AI could get shut down.

476
00:30:19,080 --> 00:30:24,280
We could have a winter kind of firm without, you know, multiple AI winters, because funding

477
00:30:24,280 --> 00:30:25,280
gets dried up.

478
00:30:25,280 --> 00:30:29,400
Like if enough people died on one day in a driverless car thing, like Congress could like say,

479
00:30:29,400 --> 00:30:33,320
you know, enough, no more AI research, and we certainly don't want that.

480
00:30:33,320 --> 00:30:37,480
And so we have to make people's expectations realistic, you and I haven't talked about

481
00:30:37,480 --> 00:30:40,320
hype today, but that's another theme in the book.

482
00:30:40,320 --> 00:30:45,840
And we have to branch out and look more broadly in the space of possible architectures,

483
00:30:45,840 --> 00:30:51,600
including a bunch of things that are really out of fashion, like symbol manipulating

484
00:30:51,600 --> 00:30:55,480
classically AI, not that we should be rebuilding that stuff, we should be borrowing from it,

485
00:30:55,480 --> 00:30:58,680
we should be borrowing from the old stuff, you know, something old, something new, the

486
00:30:58,680 --> 00:31:02,040
right kind of marriage, as you were saying before, but we need to do that if we're going

487
00:31:02,040 --> 00:31:05,840
to be counting on the machines, and we are, and we can't really, we can't put them all

488
00:31:05,840 --> 00:31:07,720
in a box, we got any better.

489
00:31:07,720 --> 00:31:14,440
I'd like to come back to the areas that you see as having promise in, you know, taking

490
00:31:14,440 --> 00:31:24,360
us past where we are today, but before we do that, you mentioned hype, and I think that

491
00:31:24,360 --> 00:31:28,880
is going to, you know, addressing the hype and kind of counter balancing it is going

492
00:31:28,880 --> 00:31:36,240
to be, you know, big contribution of this book, a big part of the first several chapters

493
00:31:36,240 --> 00:31:41,200
of the book is really trying to address that hype, like there are a lot of really good

494
00:31:41,200 --> 00:31:46,440
examples in here of, you know, where the hype underlives the, or where the hype doesn't

495
00:31:46,440 --> 00:31:53,600
live up to, or where the reality rather doesn't live up to the hype, and, you know, in particular,

496
00:31:53,600 --> 00:31:59,120
you spend a lot of time talking about reading, you know, walk us through, you know, that

497
00:31:59,120 --> 00:32:03,360
as an example, and, you know, some of the ways that, you know, you gave some examples

498
00:32:03,360 --> 00:32:07,640
about Microsoft and Alibaba, and the squad results that, you know, were published some

499
00:32:07,640 --> 00:32:08,640
years ago.

500
00:32:08,640 --> 00:32:10,880
There are a bunch of examples in there.

501
00:32:10,880 --> 00:32:11,880
Sure.

502
00:32:11,880 --> 00:32:18,480
So I don't have the book in front of me, but basically the case that we made there is,

503
00:32:18,480 --> 00:32:22,720
there were a bunch of, I mean, in that specific example was a bunch of media accounts saying

504
00:32:22,720 --> 00:32:29,680
that Microsoft had just achieved a superhuman reading or a match human reading, excuse me,

505
00:32:29,680 --> 00:32:34,320
in the reality, and there were even news stories that said, like, you know, humans are running

506
00:32:34,320 --> 00:32:39,600
out of jobs, and like, you know, the sky is falling kind of stuff because of this result.

507
00:32:39,600 --> 00:32:43,280
And what actually happened was Microsoft, it's slightly better than it had done, like,

508
00:32:43,280 --> 00:32:48,320
either anyone else had done a couple weeks earlier, and they now happened to match humans

509
00:32:48,320 --> 00:32:53,920
on this one test, which was called squad, but squad is not really that much of a test

510
00:32:53,920 --> 00:32:54,920
of reading.

511
00:32:54,920 --> 00:32:58,840
It's really a test of, like, can you underline the piece of the passage that corresponds

512
00:32:58,840 --> 00:33:00,840
to a question?

513
00:33:00,840 --> 00:33:04,920
And that's easy sometimes, but a lot of times, or, I mean, that's adequate sometimes,

514
00:33:04,920 --> 00:33:07,800
but most of the time when we're reading, we're trying to figure out things that aren't

515
00:33:07,800 --> 00:33:10,160
in the text, we're trying to go beyond the text.

516
00:33:10,160 --> 00:33:16,080
So if I just want to, you know, read a story about what Donald Trump did today and say,

517
00:33:16,080 --> 00:33:19,920
you know, who's the president? You can probably match the words president and Donald Trump

518
00:33:19,920 --> 00:33:20,920
with a system like that.

519
00:33:20,920 --> 00:33:25,600
And you can call that reading if you want, but a lot of reading is about reading between

520
00:33:25,600 --> 00:33:27,640
the lines and you make inferences.

521
00:33:27,640 --> 00:33:31,720
So I can say somebody walked into a store and you can figure out they're probably a human

522
00:33:31,720 --> 00:33:37,280
being, you know, all kinds of things that are just, like, logically obvious or inductively

523
00:33:37,280 --> 00:33:42,760
obvious to human beings all the time that go into the process of reading.

524
00:33:42,760 --> 00:33:44,880
And that test just didn't happen to measure any of them.

525
00:33:44,880 --> 00:33:50,120
And you would never hire that machine to, like, summarize news stories for you, or, or

526
00:33:50,120 --> 00:33:53,960
to tell you, especially like the implications, even the most obvious implications that aren't

527
00:33:53,960 --> 00:33:56,080
written on the page.

528
00:33:56,080 --> 00:33:59,080
And reading isn't, you know, focused throughout the book.

529
00:33:59,080 --> 00:34:03,480
One of the reasons that Arnie Davis and I wrote this book is we had written op-eds and

530
00:34:03,480 --> 00:34:08,840
things like that, trying to make some of these points, but there's a real mismatch between

531
00:34:08,840 --> 00:34:14,760
what current machines do and what you actually need to do in reading in terms of all the inferences

532
00:34:14,760 --> 00:34:15,760
that you draw.

533
00:34:15,760 --> 00:34:21,760
So we had some worked examples a little bit later in the book, not in the opening chapter,

534
00:34:21,760 --> 00:34:25,680
where we look at a children's story and just show all the things that you figure out when

535
00:34:25,680 --> 00:34:29,640
you're reading something by the woman who wrote Little House on the Prairie.

536
00:34:29,640 --> 00:34:34,840
Just like a paragraph and all the inferences and no system around even cries to do that.

537
00:34:34,840 --> 00:34:40,920
And we wanted to go at length in depth to try to help people better understand why the

538
00:34:40,920 --> 00:34:47,080
direction, the thrust of current AI research is just nothing like what you need to actually

539
00:34:47,080 --> 00:34:48,400
read.

540
00:34:48,400 --> 00:34:51,000
Just wrapping up on high for a second.

541
00:34:51,000 --> 00:34:54,600
One of my favorite parts of the book is just early in the first chapter, we give advice

542
00:34:54,600 --> 00:34:59,520
about questions that people at home can use when they read news stories.

543
00:34:59,520 --> 00:35:04,200
News stories are often like the byproduct of some press release that a big company puts

544
00:35:04,200 --> 00:35:05,200
out.

545
00:35:05,200 --> 00:35:08,040
And, you know, there are some very good people in the media, but there are a lot that just

546
00:35:08,040 --> 00:35:12,360
kind of report the press release and so the press release is often wind up more or less

547
00:35:12,360 --> 00:35:14,560
on editing media.

548
00:35:14,560 --> 00:35:17,120
So we give a set of questions that you can ask.

549
00:35:17,120 --> 00:35:22,400
Like did they do this on some toy problem or a bigger problem, what were the data like

550
00:35:22,400 --> 00:35:28,520
to help people to be able to go and read the news and have a healthy sense of skepticism.

551
00:35:28,520 --> 00:35:30,040
And this should be true for all of science.

552
00:35:30,040 --> 00:35:33,880
Of course, anytime you read science, probably anything that you read in the news, you need

553
00:35:33,880 --> 00:35:38,440
to be careful as we try to arm people with a set of questions to sift through the hype

554
00:35:38,440 --> 00:35:42,440
and figure out like, is this the real result, you know, does this mean systems can read

555
00:35:42,440 --> 00:35:45,880
or just that they passed this one test, what else do they need to do?

556
00:35:45,880 --> 00:35:50,680
And for that matter was any, you know, skeptic like me, for example, consulted to give

557
00:35:50,680 --> 00:35:54,960
any kind of, you know, counterview, if not, that's a sign in itself that this is, you

558
00:35:54,960 --> 00:35:57,960
know, press by press release.

559
00:35:57,960 --> 00:36:05,600
Yeah, I've got this particular one dog year in a note in my notes where I call the Gary's

560
00:36:05,600 --> 00:36:08,200
bullshit detector.

561
00:36:08,200 --> 00:36:14,000
But since I do have it in front of me, the six points were stripping away the rhetoric.

562
00:36:14,000 --> 00:36:16,800
So what did the system actually do?

563
00:36:16,800 --> 00:36:20,920
And so trying to get to, you know, is it actually reading or the squad results?

564
00:36:20,920 --> 00:36:24,960
How general are those results, you know, is there a demo, you know, a lot of times we

565
00:36:24,960 --> 00:36:29,360
see these results, either in academic papers or, you know, commercial and there's not

566
00:36:29,360 --> 00:36:34,320
a demo that anyone can go see.

567
00:36:34,320 --> 00:36:38,640
You know, if there's a comparison about the systems performance relative to humans, then,

568
00:36:38,640 --> 00:36:43,400
you know, which humans in particular, are we talking about and how much better?

569
00:36:43,400 --> 00:36:49,680
How far it is succeeding in this particular task actually take us towards building genuine

570
00:36:49,680 --> 00:36:55,840
AI and then the robustness of the system, which I kind of take as like failure modes, like

571
00:36:55,840 --> 00:36:57,720
where does it fall down?

572
00:36:57,720 --> 00:36:59,600
Is that, is that what you meant there?

573
00:36:59,600 --> 00:37:03,120
You will notice that the direct consequence of writing this book is that I founded a

574
00:37:03,120 --> 00:37:08,720
company called Robust at AI, one of the things that, that, I mean, exaggerate a little bit

575
00:37:08,720 --> 00:37:12,360
for comic effect, but it's also true.

576
00:37:12,360 --> 00:37:16,080
Ernie Davis and I wrote this chapter on reading that I was just alluding to and then we wrote

577
00:37:16,080 --> 00:37:21,560
a similar chapter on robots, like talking about the gap between, you know, a demonstration

578
00:37:21,560 --> 00:37:25,440
of a robot doing a backflip and having a robot actually working your home like Rosie

579
00:37:25,440 --> 00:37:31,360
the robot and the gap is just immense and the biggest problem is robustness, you can make

580
00:37:31,360 --> 00:37:36,840
a lab demonstration of any one action, but you really want the robots to decide for themselves

581
00:37:36,840 --> 00:37:37,840
what to do.

582
00:37:37,840 --> 00:37:42,360
I've, I've started thinking about a kind of distinction between automation and autonomy.

583
00:37:42,360 --> 00:37:47,520
So the field has learned of robotics has learned very well how to automate certain things if

584
00:37:47,520 --> 00:37:51,200
the environment doesn't change at all, but they're not very robust.

585
00:37:51,200 --> 00:37:56,040
So, you know, you can pack a million iPhones into boxes and as long as the boxes are exactly

586
00:37:56,040 --> 00:37:58,880
where you expect them to, it all works out.

587
00:37:58,880 --> 00:38:03,320
But if something unusual happens, then the system may have no idea what, what to do about

588
00:38:03,320 --> 00:38:08,480
it and that's basically what, the problem is with the driverless cars, those are robots.

589
00:38:08,480 --> 00:38:12,880
And they're fine under ordinary circumstances and then, you know, it rains or someone has

590
00:38:12,880 --> 00:38:16,920
a hand-lettered sign or there's a stop-toe truck that isn't in your training set and they

591
00:38:16,920 --> 00:38:18,480
don't work very well anymore.

592
00:38:18,480 --> 00:38:23,800
And so the key challenge, I think, in robotics, which is a great test of how good you're

593
00:38:23,800 --> 00:38:28,040
AI is, is to be robust, to get the same thing to work when the lighting is different, when

594
00:38:28,040 --> 00:38:32,280
there's somebody there that you weren't expecting, who is an object that you weren't expecting,

595
00:38:32,280 --> 00:38:36,400
when your map turns out to be out of date or wrong, all of these kinds of things.

596
00:38:36,400 --> 00:38:40,280
This is really a measure indirectly of intelligence.

597
00:38:40,280 --> 00:38:44,800
An intelligence system will recognize that its assumptions were wrong and compensate for

598
00:38:44,800 --> 00:38:45,800
that fact.

599
00:38:45,800 --> 00:38:48,160
A blind system just keeps doing what it's doing.

600
00:38:48,160 --> 00:38:53,320
So I'm driving on the road, I don't see anything in my little test set, seems good and

601
00:38:53,320 --> 00:38:55,080
I just drive right into the tow truck.

602
00:38:55,080 --> 00:38:57,840
Like that's the opposite of robustness.

603
00:38:57,840 --> 00:39:03,880
One of the chapters in the book is called Insights from the Human Mind.

604
00:39:03,880 --> 00:39:08,680
It jumped out at me that you said mind and not brain and I was curious what was that distinction

605
00:39:08,680 --> 00:39:09,680
for you and what.

606
00:39:09,680 --> 00:39:11,920
Sorry, that was very deliberate.

607
00:39:11,920 --> 00:39:16,520
My view is that neuroscience is a prestige field right now but hasn't come up with the

608
00:39:16,520 --> 00:39:21,920
goods and psychology is the study of the mind and more generally cognitive science is

609
00:39:21,920 --> 00:39:25,920
the study of my and actually already has a lot to offer that isn't really being paid

610
00:39:25,920 --> 00:39:26,920
attention to.

611
00:39:26,920 --> 00:39:32,560
So someday we will build better AI by understanding the wiring diagram of human brain and getting

612
00:39:32,560 --> 00:39:36,040
some insights from it, but right now we don't understand that wiring diagram.

613
00:39:36,040 --> 00:39:40,280
We don't understand even basic things like how does the brain do short term memory?

614
00:39:40,280 --> 00:39:43,920
So if I say let's pause I'll call you back in five minutes, you'll be expecting my

615
00:39:43,920 --> 00:39:44,920
call in five minutes.

616
00:39:44,920 --> 00:39:49,400
You don't need a thousand trials to hammer that into your brain, but we don't know how

617
00:39:49,400 --> 00:39:50,400
the brain does that.

618
00:39:50,400 --> 00:39:54,960
We really don't ask all the neuroscientists, you know, how did we do that?

619
00:39:54,960 --> 00:39:56,120
I'll call you back in five minutes.

620
00:39:56,120 --> 00:39:57,120
How does that work?

621
00:39:57,120 --> 00:39:58,440
What are the brain?

622
00:39:58,440 --> 00:40:02,520
People can sort of say, well, you know, something lights up in your prefrontal cortex

623
00:40:02,520 --> 00:40:05,480
and like it's so hopelessly vague.

624
00:40:05,480 --> 00:40:08,640
So neuroscience is not going to be our salvation in the short term.

625
00:40:08,640 --> 00:40:12,200
On the other hand, we know a lot about, for example, the dynamics of human memory from

626
00:40:12,200 --> 00:40:16,760
a cognitive science perspective and we know a lot about things like goals and plans

627
00:40:16,760 --> 00:40:20,440
and intentions and beliefs and desires, stuff like that.

628
00:40:20,440 --> 00:40:24,760
We don't know how they work in the brain, but we obviously have them and we can build

629
00:40:24,760 --> 00:40:26,760
those things into our machines.

630
00:40:26,760 --> 00:40:31,040
And so the point of that chapter is to look at cognitive science and see what clues it

631
00:40:31,040 --> 00:40:32,040
gives us.

632
00:40:32,040 --> 00:40:36,160
I've already alluded to a couple of those like, it's okay to have an eight structure.

633
00:40:36,160 --> 00:40:39,160
The machine learning world right now is so obsessed with learning.

634
00:40:39,160 --> 00:40:42,480
It doesn't want to build anything in except for convolution.

635
00:40:42,480 --> 00:40:47,840
Well, convolution is actually in a neat prior, your technical people will know exactly how

636
00:40:47,840 --> 00:40:51,720
it works that allows you to recognize an object that's in different places in a visual

637
00:40:51,720 --> 00:40:53,920
field in cognitive science.

638
00:40:53,920 --> 00:40:56,760
They call that translation invariance for a very long time.

639
00:40:56,760 --> 00:41:00,480
And that's a clever way of building in, convolution is a clever way of building that into a neural

640
00:41:00,480 --> 00:41:01,480
network.

641
00:41:01,480 --> 00:41:02,960
There's so much more of that.

642
00:41:02,960 --> 00:41:06,440
But there's this like attitude and machine learning that that's cheating, you shouldn't

643
00:41:06,440 --> 00:41:07,440
build anything in.

644
00:41:07,440 --> 00:41:09,120
There's a crazy attitude.

645
00:41:09,120 --> 00:41:10,120
Look at biology.

646
00:41:10,120 --> 00:41:14,360
You know, it's spent a billion years building in the right set of genes to make us have

647
00:41:14,360 --> 00:41:18,080
brains that could react with the environment in a flexible way, like let's not throw all

648
00:41:18,080 --> 00:41:19,080
that away.

649
00:41:19,080 --> 00:41:22,720
So that's one example of the things that are in the cognitive science chapter.

650
00:41:22,720 --> 00:41:29,760
Yeah, one of my recent guests said, and in fact, I think this became the title of the interview

651
00:41:29,760 --> 00:41:32,960
episode that, and he was talking about something very different.

652
00:41:32,960 --> 00:41:37,840
I want to make that clear, but that, you know, AI is a systems engineering problem.

653
00:41:37,840 --> 00:41:44,120
And it sounds like in a lot of ways you might agree with that that, you know, but a lot

654
00:41:44,120 --> 00:41:48,120
of the pieces of the system that we need to engineer don't exist yet.

655
00:41:48,120 --> 00:41:53,080
I agree with that too, although I would say that we are as a community not doing a great

656
00:41:53,080 --> 00:41:59,240
job of leveraging the ones that do exist, that as a community, we're fragmented.

657
00:41:59,240 --> 00:42:04,400
So there's a small kind of classical AI group that's kind of doing what it's always been

658
00:42:04,400 --> 00:42:06,840
doing and not really paying that much attention to deep learning.

659
00:42:06,840 --> 00:42:11,000
There's a huge deep learning community that's not paying much attention to all to classical

660
00:42:11,000 --> 00:42:12,000
AI.

661
00:42:12,000 --> 00:42:15,440
They're kind of smuggler like we have good equations, we don't need that stuff.

662
00:42:15,440 --> 00:42:22,360
And so the divisiveness between the culture which goes back all the way to the 50s has kept

663
00:42:22,360 --> 00:42:25,840
people from doing the right systems engineering.

664
00:42:25,840 --> 00:42:29,000
But I also agree that we probably need to invent a whole bunch of new stuff too.

665
00:42:29,000 --> 00:42:32,200
So we can do much better even with the existing tools.

666
00:42:32,200 --> 00:42:36,280
And I think if we worked harder to integrate the existing tools we get a clearer idea

667
00:42:36,280 --> 00:42:40,520
of where we actually need the new tools and that's part of the thinking behind my new

668
00:42:40,520 --> 00:42:43,880
company with Rodney Brooks, the robust AI company.

669
00:42:43,880 --> 00:42:47,560
And is this Rodney Brooks, Brooks of Mythical Manman fame?

670
00:42:47,560 --> 00:42:54,640
This is Rodney Brooks best known I guess for the Roomba which he co-invented and a fantastic

671
00:42:54,640 --> 00:42:58,960
set of blogs about kind of what's possible and not that he's been writing recently.

672
00:42:58,960 --> 00:43:05,440
He was the chair of MIT's AI CSAIL laboratory for many years.

673
00:43:05,440 --> 00:43:11,200
He's done many great things and it's fantastic that we have him in the company.

674
00:43:11,200 --> 00:43:15,280
From a kind of concrete technical perspective, what are the opportunities?

675
00:43:15,280 --> 00:43:21,600
And it sounds like maybe a context for this is the things that you think that we have

676
00:43:21,600 --> 00:43:25,600
that we could be integrating into these systems.

677
00:43:25,600 --> 00:43:31,760
So I don't want to say too much in a way to protect details of what I'm thinking about

678
00:43:31,760 --> 00:43:36,800
within the company context, but broadly speaking, deep learning is part of the picture clearly.

679
00:43:36,800 --> 00:43:39,320
We need to do perceptual classification.

680
00:43:39,320 --> 00:43:44,040
But we also need that kind of knowledge representation, the classical AI used, in order to figure

681
00:43:44,040 --> 00:43:48,360
out how to represent common sense in machine interpretable form.

682
00:43:48,360 --> 00:43:52,160
There has to be a way of putting together the vast knowledge that humans have already

683
00:43:52,160 --> 00:43:56,160
accumulated with the kind of experiential knowledge that deep reinforcement learning

684
00:43:56,160 --> 00:43:57,920
is good at acquiring.

685
00:43:57,920 --> 00:44:01,520
That has to be the focus.

686
00:44:01,520 --> 00:44:09,720
And you mentioned a couple of areas in the conversation thus far, calls or reasoning,

687
00:44:09,720 --> 00:44:10,720
temporal reasoning.

688
00:44:10,720 --> 00:44:15,480
Are there any independent of what you're doing with the company?

689
00:44:15,480 --> 00:44:22,000
Are there things that folks that are practitioners that are listening to this and

690
00:44:22,000 --> 00:44:28,160
wondering, OK, how can I buy into this idea of kind of swinging the pendulum back to

691
00:44:28,160 --> 00:44:31,920
the middle and doing more kind of model-based approaches?

692
00:44:31,920 --> 00:44:34,440
You mentioned knowledge representation.

693
00:44:34,440 --> 00:44:35,440
What's there for folks?

694
00:44:35,440 --> 00:44:39,360
Where should folks be looking to really start working in this direction?

695
00:44:39,360 --> 00:44:43,560
I would say they should do a couple of things first, which are to read the new book of

696
00:44:43,560 --> 00:44:50,760
mine, repeating AI with Ernie Davis, because we talk about what common sense is and whether

697
00:44:50,760 --> 00:44:53,080
the challenge is in representing it and so forth.

698
00:44:53,080 --> 00:44:59,280
And I think we do that in a more accessible form that is in the technical literature.

699
00:44:59,280 --> 00:45:00,440
I think people should look at that.

700
00:45:00,440 --> 00:45:05,960
I also think they should look at Judea Pearl's very recent book, The Book of Why, on the

701
00:45:05,960 --> 00:45:08,600
question of causal reasoning.

702
00:45:08,600 --> 00:45:09,600
OK.

703
00:45:09,600 --> 00:45:12,640
So I think those are the two best places to start.

704
00:45:12,640 --> 00:45:15,280
They're not the most technical places.

705
00:45:15,280 --> 00:45:18,800
Both books refer to lots of technical literature from there.

706
00:45:18,800 --> 00:45:23,400
Another thing I would say is everybody in the field should know about the CYC psych system

707
00:45:23,400 --> 00:45:27,920
that Doug Lennett created, which still exists, been working on it for 30 years.

708
00:45:27,920 --> 00:45:33,280
Most people think it's a failure, and I think everybody who wants to work in general AI

709
00:45:33,280 --> 00:45:39,080
should have a view about that system, what it can do, why it didn't entirely work.

710
00:45:39,080 --> 00:45:43,160
What strikes me a lot is a lot of young people in machine learning have never even heard

711
00:45:43,160 --> 00:45:44,160
of it.

712
00:45:44,160 --> 00:45:51,560
It's the most sophisticated and comprehensive attempt to have a machine, understand everyday

713
00:45:51,560 --> 00:45:57,880
reasoning, like how objects work, how people interact with each other, kind of intuitive

714
00:45:57,880 --> 00:46:01,120
physics, intuitive economics, intuitive psychology.

715
00:46:01,120 --> 00:46:05,760
And maybe it didn't work, but I think that the mountain that Doug Lennett was trying to

716
00:46:05,760 --> 00:46:07,960
get across is one that we still have to get across.

717
00:46:07,960 --> 00:46:12,960
I don't see how you get a reading system, for example, that really works if you can't

718
00:46:12,960 --> 00:46:17,400
ask part of the system, what happens when somebody loses their wallet?

719
00:46:17,400 --> 00:46:21,360
You want to know that they care, that their money isn't it, they might try to go find

720
00:46:21,360 --> 00:46:22,360
it.

721
00:46:22,360 --> 00:46:28,440
And you're not going to get all of that by just memorizing movies that you watch, which

722
00:46:28,440 --> 00:46:30,480
is kind of what I think the field is trying to do.

723
00:46:30,480 --> 00:46:34,760
So Lennett had an approach, and I think people need to use that as a mental crucible.

724
00:46:34,760 --> 00:46:39,160
So when you're getting started, you should read, rebooting AI, you should read the book

725
00:46:39,160 --> 00:46:44,160
of why, and you should read something about psych, you know, you can find various review

726
00:46:44,160 --> 00:46:48,400
papers that Lennett has written over the years.

727
00:46:48,400 --> 00:46:51,480
And like you got to start there by understanding what the problem is.

728
00:46:51,480 --> 00:46:55,120
None of the three things that I just mentioned have the answers.

729
00:46:55,120 --> 00:47:00,080
They just have a better cut on the questions, you know, Ernie and I don't know exactly

730
00:47:00,080 --> 00:47:01,960
how to build a general purpose AI.

731
00:47:01,960 --> 00:47:07,800
But I think that we are able to articulate some of the problems that need to be solved,

732
00:47:07,800 --> 00:47:09,160
that are being neglected.

733
00:47:09,160 --> 00:47:10,160
Fantastic.

734
00:47:10,160 --> 00:47:14,160
Well Gary, thank you so much for taking the time to share with us what you're working

735
00:47:14,160 --> 00:47:15,160
on.

736
00:47:15,160 --> 00:47:16,160
Thanks very much for having me.

737
00:47:16,160 --> 00:47:17,800
It's a really fun interview.

738
00:47:17,800 --> 00:47:23,400
All right, everyone, that's our show for today.

739
00:47:23,400 --> 00:47:29,920
For more information on Gary or any of our other guests, visit twimmelai.com.

740
00:47:29,920 --> 00:47:32,760
Register now for Twimmelcon, AI platforms.

741
00:47:32,760 --> 00:47:35,640
You definitely don't want to miss out.

742
00:47:35,640 --> 00:47:39,480
As always, thanks so much for listening and catch you next time.

