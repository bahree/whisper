WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.200
I'm your host, Sam Charrington, hey what's up everyone, we've got an amazing Twimble

00:34.200 --> 00:38.600
Con AI platform's event shaping up for you and I wanted to share a bit about it.

00:38.600 --> 00:41.920
The event is going to be anchored by what we're calling keynote interviews.

00:41.920 --> 00:46.800
These are live podcasts recorded right on the main stage and I'll be joined by some incredible

00:46.800 --> 00:52.680
guests, here are three that I'm really excited about.

00:52.680 --> 00:57.880
One, Andrew Eng, I could not be happier to have Andrew kicking off this event with me.

00:57.880 --> 01:02.640
Andrew is one of my AI heroes and no one has done more to bring new practitioners into machine

01:02.640 --> 01:05.240
learning and deep learning than he has.

01:05.240 --> 01:08.480
Andrew is going to be sharing a bit about what he's learned helping many businesses get

01:08.480 --> 01:12.640
productive with machine learning and AI and also speak with us about where he sees the

01:12.640 --> 01:13.960
field going.

01:13.960 --> 01:18.800
Number two, Hussein Mahana, Hussein is the head of AI and ML at Cruz, the self-driving

01:18.800 --> 01:20.160
car company.

01:20.160 --> 01:25.080
Before Cruz, Hussein helped build Google's cloud ML platform and Facebook's internal

01:25.080 --> 01:27.520
FB learner platform before that.

01:27.520 --> 01:31.480
Hussein is going to be sharing some of the lessons he's learned building ML platforms

01:31.480 --> 01:35.440
from scratch at some of the most advanced companies in the space.

01:35.440 --> 01:37.720
And number three, Fran Bell.

01:37.720 --> 01:41.560
Fran leads a team responsible for data science platforms at Uber.

01:41.560 --> 01:46.560
These are use case focused ML platform supporting application areas like forecasting, anomaly

01:46.560 --> 01:51.560
detection, NLP and conversational AI segmentation and more.

01:51.560 --> 01:56.440
Her platform sit on top of Uber's Michelangelo, putting here on a unique position to speak

01:56.440 --> 02:01.800
with us about how low level and higher level platforms come together to support data scientists

02:01.800 --> 02:04.040
and developer productivity.

02:04.040 --> 02:07.720
Beyond keynote interviews like this, we've got a bunch of outstanding speakers lined up

02:07.720 --> 02:12.200
to share their successes and failures, helping their organizations build and production

02:12.200 --> 02:15.120
lies, ML and deep learning models.

02:15.120 --> 02:21.280
If this lineup sounds interesting to you, visit twimmocon.com today to register.

02:21.280 --> 02:26.800
Use the coupon code great content through July 31st for an additional 10% off of our

02:26.800 --> 02:28.880
special early bird pricing.

02:28.880 --> 02:30.440
Hope to see you at Twimmocon.

02:30.440 --> 02:34.880
Alright everyone, I am on the line with Emma Strubel.

02:34.880 --> 02:38.120
Emma is a PhD student at UMass Amherst.

02:38.120 --> 02:40.720
Emma, welcome to this weekend machine learning and AI.

02:40.720 --> 02:41.720
Thanks a lot.

02:41.720 --> 02:43.720
Thanks for having me.

02:43.720 --> 02:47.400
Excited to talk to you and I'm particularly excited to have an opportunity to talk to you

02:47.400 --> 02:48.400
now.

02:48.400 --> 02:52.360
Just a few days before you'll be defending your thesis.

02:52.360 --> 03:00.120
So the best of luck on that, I'm sure by the time this, by the time this shows errors,

03:00.120 --> 03:05.040
you will, actually, it sounds like you'll be hiking someplace and on your way to being

03:05.040 --> 03:11.200
a visiting scientist at Facebook AI research for a year before you start as an assistant

03:11.200 --> 03:15.160
professor in the Language Technologies Institute at Carnegie Mellon.

03:15.160 --> 03:17.840
So congratulations on all that.

03:17.840 --> 03:18.840
First of all.

03:18.840 --> 03:19.840
Thank you so much.

03:19.840 --> 03:22.320
Yeah, it's very, very, lots of exciting changes.

03:22.320 --> 03:24.560
Yeah, absolutely, absolutely.

03:24.560 --> 03:31.040
So how did you get to this place where you have so much opportunity, like, and focused

03:31.040 --> 03:32.040
on AI?

03:32.040 --> 03:34.680
Like, what tell us a little bit about your path and your story?

03:34.680 --> 03:37.480
Yeah, so it was not at all on purpose.

03:37.480 --> 03:40.640
And actually, I have thoughts about that, which we could talk about if you want.

03:40.640 --> 03:41.640
Okay.

03:41.640 --> 03:44.040
Thoughts are always a good place to start.

03:44.040 --> 03:48.200
Yeah, I didn't even know that, like, sort of grad school or, like, getting a PhD was

03:48.200 --> 03:50.720
really a thing that I could do until undergrad.

03:50.720 --> 03:54.120
And I had professors who kind of, like, maybe pushed me in that direction.

03:54.120 --> 03:56.880
I got involved in some undergraduate research.

03:56.880 --> 04:01.280
But actually, when I applied to graduate school, I intended to study, sort of, like, computational

04:01.280 --> 04:02.280
biology.

04:02.280 --> 04:05.360
So I was actually interested in studying and, like, understanding, like, computational

04:05.360 --> 04:07.640
power of biological systems.

04:07.640 --> 04:12.360
But my relationship with my initial advisor did not work out.

04:12.360 --> 04:17.400
And I was, like, really fortunate to just be introduced to Andrew McComb, who's my current

04:17.400 --> 04:21.600
PhD advisor by someone else I knew in my PhD program.

04:21.600 --> 04:25.440
And then I started working with him, and I actually just, like, found that I really enjoyed

04:25.440 --> 04:26.440
the work.

04:26.440 --> 04:29.360
So what department are you in the CS department, or?

04:29.360 --> 04:32.880
Yeah, it's technically the College of Information and Computer Sciences.

04:32.880 --> 04:36.560
When I joined, it was actually the computer science department, but we've grown a lot.

04:36.560 --> 04:41.120
And that was also my, yeah, my undergraduate degree was in computer science and, like,

04:41.120 --> 04:42.120
a minor in math.

04:42.120 --> 04:45.480
And I also took some biology classes and stuff like this, because I do, I like biology

04:45.480 --> 04:46.480
a lot.

04:46.480 --> 04:47.480
Okay.

04:47.480 --> 04:48.480
Cool.

04:48.480 --> 04:49.480
Also at Amherst?

04:49.480 --> 04:50.480
Nope.

04:50.480 --> 04:51.480
I actually went to the University of Maine.

04:51.480 --> 04:52.480
Okay.

04:52.480 --> 04:53.480
So, yeah.

04:53.480 --> 04:58.480
How do you kind of describe your, your primary research interest nowadays?

04:58.480 --> 05:03.680
I'd say, like, the overall goal of my research is I want to build, like, I want to bring

05:03.680 --> 05:08.440
state-of-art NLP systems to, like, actual practitioners who actually want to use them.

05:08.440 --> 05:11.360
And I actually think there's, like, kind of, like, a large disconnect here between, you

05:11.360 --> 05:14.720
know, people who are doing, like, state-of-art NLP research and sort of, like, chasing these,

05:14.720 --> 05:17.520
like, high accuracies on these benchmark data sets.

05:17.520 --> 05:21.360
And then, but if you take these models that get really high accuracy on some benchmark

05:21.360 --> 05:28.000
data set, like, and then run them on your, like, actual data, like, your actual documents

05:28.000 --> 05:31.600
or whatever, like, that accuracy tends to go way down.

05:31.600 --> 05:37.000
And then similarly, I think people, like, the most accurate systems tend to be, like,

05:37.000 --> 05:42.360
also the slowest, oftentimes, sort of, like, the biggest models that take the most computation.

05:42.360 --> 05:47.240
So, I guess, like, two angles of my work have been, you know, developing machine learning

05:47.240 --> 05:48.840
models that are as efficient as possible.

05:48.840 --> 05:52.520
So trying to get that state-of-art accuracy, but requiring, like, much less computation.

05:52.520 --> 05:56.880
And also developing models that are going to be robust so that we can train them on, like,

05:56.880 --> 05:58.280
the annotated data that we have.

05:58.280 --> 06:02.520
But hopefully, those same models will also perform well on, like, data from many different

06:02.520 --> 06:03.520
sources.

06:03.520 --> 06:07.520
And when you think about practitioners, are you thinking about any particular domain

06:07.520 --> 06:10.400
or are you, kind of, tackling it broadly?

06:10.400 --> 06:15.520
Yeah, I mean, it's generally, like, I hope that my research will apply to practitioners,

06:15.520 --> 06:16.920
like, kind of, from any domain.

06:16.920 --> 06:17.920
That's the hope.

06:17.920 --> 06:18.920
I do.

06:18.920 --> 06:23.680
So specifically, I've worked with collaborators in material science, which has been really

06:23.680 --> 06:24.680
cool.

06:24.680 --> 06:30.680
So they're interested in being able to extract, like, recipes for new materials, essentially,

06:30.680 --> 06:31.680
from the literature.

06:31.680 --> 06:35.720
And so I've been working with them to try to develop models that can do that.

06:35.720 --> 06:37.720
And it's been really, really interesting.

06:37.720 --> 06:38.720
Oh, interesting.

06:38.720 --> 06:45.160
That use case is the literature has the recipes in it, but it's in English, if that's what

06:45.160 --> 06:49.640
you, if that's the language you're studying in B, if that's what you can call what's written

06:49.640 --> 06:54.040
in material science, academic, journal, particles, and you're trying to-

06:54.040 --> 06:55.040
Yeah, exactly.

06:55.040 --> 06:56.040
I can't understand.

06:56.040 --> 06:57.040
Exactly.

06:57.040 --> 06:58.040
Exactly.

06:58.040 --> 07:02.240
And so you're trying to capture from there some representation of what the recipe is.

07:02.240 --> 07:03.240
Yeah, exactly.

07:03.240 --> 07:07.840
Going from some of the unstructured, it is English, yeah, research, article text, to, like,

07:07.840 --> 07:11.680
some structured representation, that's, like, these are the material precursors, then,

07:11.680 --> 07:16.600
you know, you heat them for this amount of time, and then, like, drive them and add this

07:16.600 --> 07:19.440
other thing in, and then you get this result that has these properties.

07:19.440 --> 07:22.720
So then, once you have that information, you can kind of, like, analyze it at a large

07:22.720 --> 07:27.640
scale to, like, poss- like, our dream goal would be to, like, be able to generate new material

07:27.640 --> 07:31.480
recipes, like, given some target material generating the recipe, because, basically, it's

07:31.480 --> 07:36.360
done through, like, trial and error research in, like, a lab right now, so it's a very slow

07:36.360 --> 07:38.200
and error-prone process.

07:38.200 --> 07:39.200
Awesome.

07:39.200 --> 07:42.680
It's a fun application, because it has, you know, this is, like, how we're going to

07:42.680 --> 07:45.640
develop, like, sustainable energy, right?

07:45.640 --> 07:49.560
These are, like, the materials that are used for, like, solar cells and stuff like this.

07:49.560 --> 07:52.160
So it's a fun project.

07:52.160 --> 07:56.480
You kind of describe this problem of, you know, having these state-of-the-art results

07:56.480 --> 08:04.400
in academic literature and practitioners not being able to use them, and you're primarily

08:04.400 --> 08:13.560
focused on, it sounds like, kind of, the general-generalized ability of the results, but it also

08:13.560 --> 08:18.800
kind of speaks to, like, a whole, you know, repeatability in machine-learning research

08:18.800 --> 08:20.200
kind of issue, as well.

08:20.200 --> 08:21.840
Are you tackling that?

08:21.840 --> 08:26.520
I'm not yet, but I actually have been thinking about doing something like that, like, sort

08:26.520 --> 08:31.800
of similar to this energy and policy considerations paper, which, I guess, we haven't necessarily

08:31.800 --> 08:33.320
talked about yet.

08:33.320 --> 08:34.320
But we will.

08:34.320 --> 08:35.320
Yeah.

08:35.320 --> 08:36.320
But we will.

08:36.320 --> 08:37.320
Yeah, yeah.

08:37.320 --> 08:38.320
So that's something I haven't tackled yet.

08:38.320 --> 08:42.320
But I actually was thinking about, like, sort of, like, doing an analysis of, like, I guess

08:42.320 --> 08:46.480
I was thinking about statistical significance in particular, which is kind of, like, a related

08:46.480 --> 08:47.480
issue.

08:47.480 --> 08:51.520
So, like, an NLP, we don't usually measure significance of our results.

08:51.520 --> 08:54.720
And there's kind of, like, issues with people reporting the max number out of many random

08:54.720 --> 08:55.720
seeds.

08:55.720 --> 08:57.320
And then it's hard to repeat these experiments.

08:57.320 --> 08:58.320
Yeah.

08:58.320 --> 09:01.320
But, yeah, I do think that's, like, a really interesting, interesting thing, yeah.

09:01.320 --> 09:07.280
Okay, well, since you spilled the beans, you introduced the topic that we're going to be

09:07.280 --> 09:08.280
talking about this.

09:08.280 --> 09:09.280
Sorry.

09:09.280 --> 09:10.280
No, no, no, I'm just teasing.

09:10.280 --> 09:11.280
I'm just teasing.

09:11.280 --> 09:14.600
So this is a paper that you, it's a pretty recent paper, at least according to the, the

09:14.600 --> 09:18.360
most recent one that's up on archive, was this month, right?

09:18.360 --> 09:19.360
Yeah, yeah.

09:19.360 --> 09:21.760
I actually haven't presented it at the conference yet.

09:21.760 --> 09:26.240
So, like, the speed of research is this.

09:26.240 --> 09:27.240
Yep.

09:27.240 --> 09:32.680
So the paper is called Energy and Policy Considerations for Deep Learning in NLP.

09:32.680 --> 09:35.240
And what conference are you presenting it at?

09:35.240 --> 09:36.240
It'll be A.C.L.

09:36.240 --> 09:42.280
So it's the, uh, annual conference of the Association for Computational Linguistics is, like, one

09:42.280 --> 09:47.080
of the top, like, international NLP conferences, um, it's in Florence in July.

09:47.080 --> 09:48.080
Oh, nice.

09:48.080 --> 09:49.680
That's a good one to go, too.

09:49.680 --> 09:50.680
Yeah.

09:50.680 --> 09:51.680
Well, I think it's, like, peak tourist season.

09:51.680 --> 09:55.680
So, like, I don't know if it's, like, totally the best time to go there, but, okay.

09:55.680 --> 09:56.680
Okay.

09:56.680 --> 09:57.680
Yeah.

09:57.680 --> 10:02.640
Um, so, uh, why don't you tell us a little bit about this paper?

10:02.640 --> 10:04.960
What were your goals in writing it?

10:04.960 --> 10:06.480
Yeah, absolutely.

10:06.480 --> 10:12.480
Um, so I talked about how, like, one of the focuses of my research has been developing

10:12.480 --> 10:13.480
these efficient models.

10:13.480 --> 10:18.280
And so the motivation for a lot of that research is, um, basically that we want, you know,

10:18.280 --> 10:20.880
that we even want machine learning models to be efficient and that we want them to be

10:20.880 --> 10:25.200
efficient because we care about the cost in terms of money, but also in terms of, like,

10:25.200 --> 10:28.800
the cost of the environment, because just bigger models require more energy, which has

10:28.800 --> 10:31.520
more, like, CO2 output, basically.

10:31.520 --> 10:35.400
So that was, it has been our, like, motivation and a few of the papers that are written.

10:35.400 --> 10:40.080
Um, and, but then I realized that no one is really, like, uh, quantified this, especially

10:40.080 --> 10:41.080
not an NLP.

10:41.080 --> 10:43.960
So there's been, like, but, yeah, I don't know of any work actually really quantifying

10:43.960 --> 10:47.280
this in terms of, like, the carbon footprint of training these NLP models.

10:47.280 --> 10:53.560
Um, and at the same time, there's been this, um, surge recently in NLP, um, or this,

10:53.560 --> 10:58.560
I guess, trend of training, like, bigger and bigger models on more and more data and,

10:58.560 --> 11:03.080
um, these models are performing really well, like, getting really high accuracies, um,

11:03.080 --> 11:06.880
but they're just, like, the computational resources required to train them are, like,

11:06.880 --> 11:11.560
enormous, like, such that, you know, a researcher like me who's not currently at, like, a big

11:11.560 --> 11:15.800
company, like, Facebook or Google, like, cannot afford and just, and I don't even have

11:15.800 --> 11:18.800
access to, like, the hardware to train these models.

11:18.800 --> 11:25.520
So we're talking about, like, language models, like GPT-2 or, okay, Google just a couple

11:25.520 --> 11:26.520
of days ago.

11:26.520 --> 11:32.640
Uh, I think we're in the Excel net, uh, and some folks did, uh, kind of back at the

11:32.640 --> 11:40.600
envelope analysis that, uh, showed concluded that it costs $245,000 to train that model.

11:40.600 --> 11:41.600
Yeah, exactly.

11:41.600 --> 11:42.600
Yeah.

11:42.600 --> 11:43.600
That's not accessible.

11:43.600 --> 11:44.600
Yeah, no, not really.

11:44.600 --> 11:50.520
I love, like, it's a pretty large portion of, like, an NSF grant, but, uh, yeah, yeah.

11:50.520 --> 11:53.640
So basically we were inspired by, yeah, the fact that we didn't see people quantifying

11:53.640 --> 11:57.640
this and, like, I think there's been sort of, like, an exponential growth in the actual,

11:57.640 --> 12:00.400
like, energy and computation requirements of models recently.

12:00.400 --> 12:03.760
Um, so, yeah, we just wanted to, to quantify that.

12:03.760 --> 12:07.640
And then also, I guess, talk about some conclusions based on our, our results.

12:07.640 --> 12:15.000
Yeah, and so one of the things that, uh, that you do is kind of compare the estimated

12:15.000 --> 12:21.120
emissions for training some of these models with kind of the things that we usually associate

12:21.120 --> 12:27.480
with, you know, environmental damage or at least emissions, like, air travel and, you

12:27.480 --> 12:29.920
know, car travel, things like that.

12:29.920 --> 12:30.920
What did you find there?

12:30.920 --> 12:31.920
Yeah.

12:31.920 --> 12:42.040
There's like a number of numbers, um, I guess one thing is training one of these models

12:42.040 --> 12:43.200
in this particular way.

12:43.200 --> 12:46.000
That's like very sort of computationally intensive.

12:46.000 --> 12:52.600
Um, so this is like training, uh, a machine translation model using this new technique called

12:52.600 --> 12:57.920
neural architecture search required basically multiple, let's see, what's it three, four?

12:57.920 --> 13:01.880
I'm not going to do math in my head, but multiple times the entire carbon footprint

13:01.880 --> 13:08.440
of like a car in its lifetime, including fuel and manufacturing, um, which is ridiculous.

13:08.440 --> 13:11.960
And that's like, you know, that's even more carbon footprint than like the average American

13:11.960 --> 13:12.960
life.

13:12.960 --> 13:13.960
I guess.

13:13.960 --> 13:17.440
So I should say one of the, one of the things we did is we, um, we analyzed like the total

13:17.440 --> 13:22.880
carbon footprint of, um, basically my last paper, like the research and development required

13:22.880 --> 13:27.000
to, you know, develop the, the last model that I published.

13:27.000 --> 13:28.000
Okay.

13:28.000 --> 13:32.680
It's like just like the carbon footprint of that, well, the work that I did, like easily,

13:32.680 --> 13:38.160
like doubled or tripled, uh, my personal carbon footprint last year, which I found, like

13:38.160 --> 13:39.160
super alarming.

13:39.160 --> 13:40.160
Okay.

13:40.160 --> 13:41.160
Wow.

13:41.160 --> 13:42.160
Yeah.

13:42.160 --> 13:43.160
Yeah.

13:43.160 --> 13:44.160
Huh.

13:44.160 --> 13:48.960
Were those the two models or scenarios that you were primarily, primarily looking at

13:48.960 --> 13:57.160
that your, uh, NLP pipeline and this kind of transformer, neural architecture search model?

13:57.160 --> 13:58.160
Yeah.

13:58.160 --> 14:04.120
I mean, I'd say we looked at a variety of these, um, like these popular pre-trained language

14:04.120 --> 14:05.120
models.

14:05.120 --> 14:10.800
Um, so we looked at, uh, like LMO for GPT-2, um, and then yeah, we were interested in looking

14:10.800 --> 14:16.200
at neural architecture search in particular because, uh, I think like the, yeah, I mean,

14:16.200 --> 14:20.600
it's a very, uh, computation, it's a very computation-hungry approach.

14:20.600 --> 14:27.600
And I think like the, uh, like the resulting benefit is relatively small, um, and so I

14:27.600 --> 14:30.960
had a feeling it would be kind of like a drastic number, so I wanted to compute it and kind

14:30.960 --> 14:35.640
of maybe encourage people to be a little bit more responsible about, uh, like sort of like

14:35.640 --> 14:37.400
these brute force techniques.

14:37.400 --> 14:38.400
Yeah.

14:38.400 --> 14:43.880
I've had a number of conversations with folks and seeing like blog posts, uh, where folks

14:43.880 --> 14:52.720
try to capture, uh, in this case, the case I'm thinking of the, like incremental cost

14:52.720 --> 15:00.800
of training a model, you know, per incremental percentage accuracy benefit or business benefit

15:00.800 --> 15:07.800
and like try to encourage people to think about that, you know, as they're devoting resources

15:07.800 --> 15:12.920
to building out these models, um, yeah.

15:12.920 --> 15:19.400
And, you know, it's not something that I hear people talking about a lot, um, and I haven't

15:19.400 --> 15:28.040
come across any particularly like rigorous way of doing it or, you know, frameworks for

15:28.040 --> 15:30.000
doing it or anything like that.

15:30.000 --> 15:36.160
And I wonder in your research, did you, did, were you somehow able to like, you know, really

15:36.160 --> 15:43.680
get into that kind of the incremental carbon emissions per incremental benefit and accuracy

15:43.680 --> 15:44.680
or something like that?

15:44.680 --> 15:47.520
Did you compare that across different techniques?

15:47.520 --> 15:49.360
That's a great question.

15:49.360 --> 15:53.200
We didn't, but, um, people should totally do that.

15:53.200 --> 15:55.200
But I'm out of here.

15:55.200 --> 15:59.200
I'm not really happy to do it too.

15:59.200 --> 16:01.880
I just had to, you know, buy the time.

16:01.880 --> 16:02.880
Yeah.

16:02.880 --> 16:08.600
So, like, take away as from this paper that we write about is, um, basically that authors

16:08.600 --> 16:11.920
should report numbers that make it easier to do that comparison.

16:11.920 --> 16:16.880
And so I think there's some pretty straightforward, um, things that people could compute for like

16:16.880 --> 16:21.440
any machine learning model that would like allow us to compare sort of like, like how

16:21.440 --> 16:25.200
much computation is required basically to get that accuracy.

16:25.200 --> 16:28.240
And then it would be like super easy to just, yeah, compute those numbers.

16:28.240 --> 16:33.840
So if people could compute, um, or report just like gigaflops to convergence, that's like

16:33.840 --> 16:34.840
a pretty easy thing.

16:34.840 --> 16:37.680
Like, uh, there's like a couple papers where they've reported that like when they're trying

16:37.680 --> 16:41.600
to compare and show their model is like better, um, I think if that was just like a standard

16:41.600 --> 16:45.360
thing we reported alongside, you know, accuracy or whatever of their metric, I think that would

16:45.360 --> 16:50.480
be like really, um, it's like not that hard and, um, it would just really allow us to make

16:50.480 --> 16:57.560
that comparison directly, um, and similarly like something that's a little bit less straightforward

16:57.560 --> 17:02.440
but that would be really helpful in a number of ways is reporting, um, sort of like sensitivity

17:02.440 --> 17:07.640
to hyper parameters, um, because like a lot, yeah, a lot of this energy use actually comes

17:07.640 --> 17:12.280
from sort of like how wasteful we are in terms of, um, you know, tuning these neural network

17:12.280 --> 17:17.400
models that, you know, notoriously require like a lot of finessing and tuning.

17:17.400 --> 17:20.520
And that tends to be like if you want to train out a new data set, you know, you have to

17:20.520 --> 17:23.880
do some kind of tuning, but we don't, we do like a really bad job.

17:23.880 --> 17:28.520
This goes back to what you mentioned about reproducibility. I think we do a really bad job of like

17:28.520 --> 17:33.160
actually being really precise about how much of that tuning happens, um, and then how

17:33.160 --> 17:37.160
much is required like there's like, you know, there's a lot that goes into development of a model

17:37.160 --> 17:42.520
and develop it as a new technique. When you think about this, are you thinking about it like

17:42.520 --> 17:48.920
from a pipeline perspective and hey, you know, this is an iterative process and we're, you know,

17:48.920 --> 17:55.400
kind of playing with all these hyperparameters, uh, to actually get to a model or are you thinking

17:55.400 --> 18:01.800
about it like, I don't know if this actually makes any sense, but like somehow dropout is more

18:01.800 --> 18:08.440
expensive than, you know, something else from a, you know, technique perspective during training.

18:08.440 --> 18:12.600
Oh, that's super interesting. No, I wasn't thinking about that, but dropout is like more,

18:12.600 --> 18:15.800
probably more expensive than like some other technique just because it's like way less

18:15.800 --> 18:19.880
simple efficient, like if you train with dropout versus I thought there's other like more direct

18:19.880 --> 18:24.280
regularization approaches. I was not talking about that, but if you want to collaborate on,

18:26.280 --> 18:31.960
something else for someone else to figure out. Yeah. That's, that's great. Yeah, people should

18:31.960 --> 18:36.280
like listen to your podcast for like cool research ideas. Yeah, no, I was thinking more just

18:36.280 --> 18:40.600
in terms of like if you want to take some like published model and then apply it to your data

18:40.600 --> 18:45.400
to sort of like a new domain, I think it's really hard to tell like how much tuning is actually

18:45.400 --> 18:49.720
going to require it and people just don't report that. Yeah. So it's both kind of like not super

18:49.720 --> 18:54.840
reproduced. You know, you'll say, oh, we did a good search of these things, but like, I think

18:54.840 --> 19:01.240
it's not, it's not actually that straightforward always. You know, characterizing the cost of just

19:02.040 --> 19:09.720
inefficiencies in the way we do this stuff like, you know, imagining there's, you know, some set of

19:09.720 --> 19:16.840
techniques that, you know, people still do all the time that are like far from, you know, best

19:16.840 --> 19:23.160
practice or state of the art that, you know, probably just like throw, you know, tons of CO2

19:23.160 --> 19:27.160
emissions out there for not. Like, have you given any thoughts on anything like that?

19:27.800 --> 19:33.400
That's interesting. I mean, this is not exactly what you said, but something that came to mind

19:33.400 --> 19:40.680
is sort of, I think people are very eager to use like deep learning when they can definitely get

19:40.680 --> 19:47.400
away with like just like a single like logistic regression. So that comes to mind as like something

19:47.400 --> 19:56.680
where, yeah, like at the call, yeah, people want their technology to sound cool and to use the

19:56.680 --> 20:03.240
cool stuff. But yeah, I think for, you know, for some, yeah, for many tasks, you don't really need like a

20:03.240 --> 20:08.120
big deep fancy model. You can just do like classification, like a, yeah, shallow classification.

20:08.120 --> 20:13.560
It'll work fine. Right. Right. Yeah. I mean, all of these, all of these questions are kind of

20:14.360 --> 20:21.720
interrelated. And I guess that goes back to the fundamental contribution of this work, which is,

20:21.720 --> 20:26.600
hey, we really need to be thinking about how much it costs us to get this incremental, you know,

20:26.600 --> 20:30.680
percentage of, you know, performance by whatever metric you're using.

20:31.320 --> 20:35.880
Absolutely. Like something like neural architecture search, where it's kind of,

20:36.840 --> 20:42.280
I guess it's going back to the reproducibility thing. Like, you know, take GPGP2, right? A bunch of

20:42.280 --> 20:49.400
people have reported like trying to reproduce GPGP2 and OpenAI hasn't said everything that you

20:49.400 --> 20:56.120
need to do to get the exact kind of results that they got. Like, do you, you know, if you're

20:56.120 --> 21:02.760
trying to benchmark these things, do you even know like how close you are to the actual

21:04.120 --> 21:10.360
cost to produce their model? Does that make any sense? Yeah. Yeah. That's an interesting question.

21:10.360 --> 21:16.200
I think if I understand what you're saying correctly, it's kind of like, uh, do we actually know

21:16.200 --> 21:23.000
what goes into training? Like, are estimates even reasonable? Like, did we, uh, yeah? Well, I guess,

21:23.000 --> 21:28.520
you know, so part of the question, so maybe taking a step back, like to train the, the neural

21:28.520 --> 21:37.000
architecture search based model did presumably you actually implemented that as opposed to

21:38.040 --> 21:45.640
determine the cost analytically or no? Yeah, no. So yeah, I will describe to you our methodology.

21:45.640 --> 21:53.720
Okay. With the exception of GPGP2 actually, we, we basically took so all the models we analyzed

21:53.720 --> 22:02.280
had open source code. We took that code. We like just had it perform training for like up to a day

22:02.280 --> 22:10.920
and then we sampled like the actual energy draw of the GPU or GPUs and the CPUs and the main memory.

22:10.920 --> 22:15.480
So making the assumption that like the main energy draw from the computer is going to be from

22:15.480 --> 22:20.600
like the, these copy, the computational hardware essentially. And then we extrapolated that

22:20.600 --> 22:27.880
based on, uh, the amount of time and the hardware, um, as reported in the paper that they

22:27.880 --> 22:32.600
used to train the model. Oh, okay. Yeah. So we didn't, because it would have taken, I mean,

22:32.600 --> 22:38.360
as I said, at least two hundred and forty five thousand dollars. Yeah. Right.

22:38.360 --> 22:41.960
So like, I'm like, I think the entire thing, but I figured this is like a reasonable, uh,

22:41.960 --> 22:46.600
way to estimate it. And I'm happy to talk more about like how we then convert that to carbon

22:46.600 --> 22:49.880
and stuff like this because it's a little bit like, you just have to make a lot of assumptions

22:49.880 --> 22:53.720
along the way. But we figured it's like, we tried to make reasonable assumptions and it's like,

22:53.720 --> 23:00.200
we think our estimates are like ballpark reasonable. Did you run it for the day on the exact

23:00.200 --> 23:05.080
hardware that they said they used? If they said they, if they reported the hardware that they used

23:05.080 --> 23:10.520
or did you have to extrapolate to that as well? Yeah. We had to extrapolate to that as well.

23:10.520 --> 23:15.080
But we did make up effort to use hardware that's like fairly similar. So like,

23:16.200 --> 23:22.040
so the GPUs that we used basically have it have the same. So like one of the metrics like

23:22.040 --> 23:27.080
that's reported for a GPU is sort of like, it's maximum power draw. And so the maximum power draw,

23:27.080 --> 23:31.800
the GPUs that we were using, uh, is the same as the maximum power draw, the GPUs they were using.

23:31.800 --> 23:36.760
So yeah, I think it's like probably the, yeah, power use is similar. And they're,

23:36.760 --> 23:40.680
they're not like incredibly different. They're like, you know, similar age GPUs and stuff like

23:40.680 --> 23:45.640
this. But yeah, it wasn't in most cases. It was not the exact same hardware. And then in here,

23:45.640 --> 23:53.640
you talk a little bit about the ultimate source of the energy, like whether it's renewable versus

23:53.640 --> 24:03.480
gas, coal, etc. Like how did that, how were you able to fit that into this model? Do the various,

24:03.480 --> 24:06.680
like data center providers, cloud providers, do they report on this stuff?

24:08.360 --> 24:15.400
Yeah. So they're not super transparent about it. Surprise. Yeah. And so these numbers, we got

24:15.400 --> 24:22.280
these numbers from, well, so for the cloud providers, we got the numbers from a 2017 white paper

24:22.280 --> 24:26.040
that Greenpeace did. And it seemed like they surveyed the companies and got these numbers

24:26.040 --> 24:35.400
somehow from them. Yeah. And so, and then so in our computation, we used basically the mapping

24:35.400 --> 24:44.680
from like kilowatt hours of energy to carbon produced or like CO2 produced as provided by the

24:44.680 --> 24:51.560
US EPA, like that's like the average for power consumed in the United States. Okay. And so if you

24:51.560 --> 24:58.520
look at the breakdown of AWS's energy, and so like obviously like where the mapping from like

24:58.520 --> 25:02.440
electricity, the carbon footprint is very dependent on like these sources. So if it's from

25:02.440 --> 25:07.960
renewable sources, it maps to zero. And then if it's from nuclear, you know, maps to less than from

25:07.960 --> 25:15.160
gas or coal, like much less. So yeah. So if you look at the actual breakdown of energy resources

25:15.160 --> 25:21.880
as of 2017 of Amazon AWS, it like pretty closely mirrors the breakdown from the US. And AWS is

25:21.880 --> 25:28.120
like the largest cloud provider. So for that reason, we thought that it was a like a pretty like

25:28.120 --> 25:33.880
reasonable estimate for mapping. So since coming out with this work, people from some of these

25:33.880 --> 25:38.200
companies have contacted me and let me know that these numbers are out of date and they're doing

25:38.200 --> 25:44.840
better now. So yeah, we're considering, yeah, we're considering doing an update like in the fall

25:45.800 --> 25:50.120
for something with more up to date numbers. And of course, and something that we don't like address

25:51.320 --> 25:56.360
a ton in the paper is that a lot of companies are also moving more towards actually many of them.

25:56.360 --> 26:02.840
I know like Google at least is like 100% renewable in terms of like I don't want to say renewable,

26:02.840 --> 26:10.040
but they're yeah, basically they buy carbon offsets to make up for like energy in their cloud

26:10.040 --> 26:15.240
that does not confirm renewable sources. So that's good. That's better than nothing, but it's also not

26:15.240 --> 26:21.800
the same as like using actual renewable energy. Yeah, but they are relative to the other providers

26:21.800 --> 26:32.520
given this 2017 data kicking, but on the renewable 56% for them relative to 32 for Microsoft and

26:32.520 --> 26:39.880
2017 for AWS. Yeah, definitely. It seems like they really care about that. And also like TPUs are

26:39.880 --> 26:44.360
actually, so all the numbers that we were able to compute were for GPU just because we don't,

26:44.360 --> 26:51.080
we aren't able to get the power draw of TPUs. But I do think TPUs are much more, I mean because

26:51.080 --> 26:54.840
they're better at doing this computation, they just like don't take as much time to do the same

26:54.840 --> 26:59.880
amount of training. So also a technology developed by Google that I think, yeah, it's just like much

26:59.880 --> 27:04.600
more energy efficient than GPUs. So that's nice. Although it's a little sad that it's like,

27:04.600 --> 27:08.920
you know, this, I guess they're all proprietary technologies, but GPUs feel, I mean,

27:08.920 --> 27:12.600
sorry, TPUs feel a little bit more proprietary than GPUs because I can't like buy one.

27:14.120 --> 27:19.720
I'm like pro energy usage. Right, right, right. So you, so you could not

27:20.600 --> 27:26.360
include Excel net in this paper. You just don't have enough information because it's so

27:26.360 --> 27:31.640
all the data they provide it was TPU based. You'd have to do a lot more extrapolation than even

27:31.640 --> 27:38.120
you've done here. Yeah, exactly. So for like the BERT model, for example, this was also originally

27:38.120 --> 27:45.640
trained, you know, at Google on only TPUs. But since then, like Nvidia, like retrained it on GPUs.

27:45.640 --> 27:50.840
So we were able to use like those numbers. But yeah, for, yeah, exactly for Excel net, it's like

27:50.840 --> 27:56.920
unclear. Yeah, exactly what the footprint would be. I guess if I made, if I made like a follow-up

27:56.920 --> 28:01.080
update, make like a blog post or something, I would try to get some, hopefully if it's not

28:01.080 --> 28:06.120
too proprietary, try to get some like energy draw information for TPUs so we can do that estimate.

28:06.120 --> 28:09.080
Right. Because I think it'd be really interesting to actually see like how they compare it

28:09.080 --> 28:20.360
concretely. This work is very specific to NLP. Have you, do you have any specific thoughts on

28:20.360 --> 28:25.160
like how it applies to, I mean, I guess the methodology is pretty general. And what you're

28:25.160 --> 28:30.600
really saying here is that we should be doing this. And that's another exercise for someone

28:30.600 --> 28:38.760
else is to apply to computer vision models and the like. Yeah, yeah. So there is some other people

28:38.760 --> 28:45.320
have done work comparing, let's see, in computer vision in particular, you know, it's like really

28:45.320 --> 28:49.880
a popular area. Let's see, what were they looking at? So I don't know if anyone doing the mapping

28:49.880 --> 28:54.120
to like carbon footprint, but there are people who have compared to like the energy use, like

28:54.120 --> 28:59.000
the energy draw required by the GPU for using sort of like different neural network layers in,

28:59.720 --> 29:04.360
you know, like these big image classification networks and stuff like this are like, you know,

29:04.360 --> 29:08.760
different batch sizes and stuff like this. Yeah. So there's some work they didn't do, like you could

29:08.760 --> 29:13.400
take, I think their numbers and then do like that extra step with mapping the carbon footprint.

29:13.400 --> 29:18.360
I do think like the NLP models, especially these like, you know, notoriously enormous

29:18.360 --> 29:26.200
pre-trained language models are just like some of the most like energy, hungry models in machine

29:26.200 --> 29:30.920
learning. I would say like the other other area that's probably it was just like using insane

29:30.920 --> 29:38.760
amounts of energy would be the work at like open AI on doing like training, you know, training

29:38.760 --> 29:44.360
reinforcement learning models to do like various games. I think that's going to be like probably,

29:44.360 --> 29:48.040
I mean, I think the technique for training this is like kind of similar to neural architecture search

29:48.040 --> 29:53.000
and that you're actually training like a ton of different models. And you kind of early in our

29:53.000 --> 30:03.480
conversation, there's a hint of judgment that, you know, the incremental gains that we're seeing,

30:03.480 --> 30:11.160
you know, going from, you know, one model to the next Elmo, the birth to GPT-2 to excel in that,

30:11.160 --> 30:18.120
like, doesn't necessarily justify the increase in training costs. Like, did you try to quantify

30:18.120 --> 30:23.720
that specifically? Yeah, so I think the question you asked earlier, I'm kind of would have quantified

30:23.720 --> 30:29.160
that like we had just computed like a ratio. Yeah, yeah. Yeah, like the difference in, you know,

30:29.160 --> 30:33.400
compute required versus like the increase in accuracy, we totally showed on that like I think

30:33.400 --> 30:37.640
that would be nice because a lot of these models do evaluate on sort of like some of the same metrics.

30:37.640 --> 30:42.600
Yeah, so we didn't really quantify that except, yeah, that we noted for the neural architecture

30:42.600 --> 30:49.960
search that they get an increase of like 0.1 blue score, which is really not a lot on like English

30:49.960 --> 30:56.520
and German machine translation. Yeah, at the cost of, we say like at least $150,000 in on-demand

30:56.520 --> 31:02.520
compute time. Yeah, I think, I mean, I think yeah, that sort of that approach in particular,

31:02.520 --> 31:08.920
is just a very brute force approach. I do have like a little bit of judgment about that. I mean,

31:08.920 --> 31:15.880
I think like the gains. So I didn't make that up. Yeah, it's true. Elmo versus Bert versus

31:15.880 --> 31:21.000
Excel now. You know, I do think that they are, like, there are actually pretty substantial differences

31:21.000 --> 31:25.320
in accuracy between those models on, you know, these like standard NLP benchmarks. And I think

31:25.320 --> 31:29.880
that that's that is important to do. Something with the neural architecture search, I feel like maybe

31:29.880 --> 31:37.640
there's like like some more like well thought out research that we could be doing to like not just

31:37.640 --> 31:43.640
do a brute force search on architectures to find like really small increase on like a single

31:43.640 --> 31:49.800
language pair. Is there a kind of argument or devil's advocate thing here that says like really

31:49.800 --> 31:58.440
the innovation and all these models is in their use in transfer learning. And if you think about

31:58.440 --> 32:07.960
that, the cost to train these models is amortized across lots and lots of users. And similarly,

32:07.960 --> 32:14.520
the incremental performance gains accrue across lots and lots of users. And so it's worth it.

32:15.480 --> 32:20.680
Yeah, totally. I mean, I am definitely not sick. You know, I'm an NLP researcher. One of them

32:20.680 --> 32:25.480
closely analyzed is like my model. Yeah. And I'm just gonna like stop doing research or something.

32:25.480 --> 32:29.480
Like I'm gonna continue working on these models. Right. Right. And I do think like Elmo and

32:29.480 --> 32:35.000
Bird. And like I guess now ex on that like, yeah, I mean, the aggregate increases on these core

32:35.000 --> 32:40.040
NLP tasks resulting from these models is like awesome. It's like really cool to be like in this field

32:40.040 --> 32:44.280
at this time. Yeah. So I like by no means saying we shouldn't be doing this research and we shouldn't

32:44.280 --> 32:49.160
be using these models. And so I totally agree with you that. Yeah, that's like a great sort of

32:49.160 --> 32:53.800
like counter argument that especially these models in particular, you know, they're sort of like

32:53.800 --> 32:59.240
pre-trained on this large amount of like data. They don't require like explicit supervision. So

32:59.240 --> 33:02.200
you can like kind of easily collect this large amount of data and then just train on it.

33:03.800 --> 33:07.560
And then these models are like really applicable in a lot of different. Yeah, I can sort of

33:07.560 --> 33:12.440
provide them as input in a lot of different tasks. Yeah, I mean, that's totally true. I still think

33:12.440 --> 33:19.480
it's important to characterize the costs. Especially, yeah. So like another one of our conclusions is

33:19.480 --> 33:25.240
sort of like in terms of like access to computation resources. So I do think if you want to use one

33:25.240 --> 33:30.520
of these models in a new domain. So they tend to be trained on like web crawls and like news,

33:30.520 --> 33:36.360
corpora. Yeah, that kind of thing. Yeah, yeah. And so I still think like the so I'm sure if you

33:36.360 --> 33:40.680
initialize some model, let's say that you wanted to run on these like material science journal

33:40.680 --> 33:46.040
articles or like whatever. So I'm like, yeah, research articles in a very specific domain. I think if

33:46.040 --> 33:50.760
you, you know, if you initialize your model with one of these models, like probably it will help,

33:50.760 --> 33:54.120
but what you really want to do is train one of these models on a corpus of that data,

33:54.840 --> 33:59.240
which is actually something that we did in our research. And are you making a distinction between

33:59.240 --> 34:05.400
training and tuning? Yeah, so we fine tune. So that's true. You don't need to, yeah, given the model,

34:05.400 --> 34:09.080
it's already learned a lot of sort of stuff about, let's see if you pair about English. It's learned

34:09.080 --> 34:14.440
a lot of stuff about, you know, English that is sort of that domain independent. I guess it

34:14.440 --> 34:18.840
probably is domain independent. Yeah, so that's a good point. You only need to fine tune it,

34:18.840 --> 34:23.880
but it still takes like a lot to fine tune it basically in, yeah. Yeah.

34:25.000 --> 34:31.000
Oh, yeah, so sorry, what I was saying is, yeah, basically like in terms of like funding and stuff

34:31.000 --> 34:37.240
like this, yeah, just developing and using these models can still be pretty expensive for academic

34:37.240 --> 34:42.280
researchers or even, you know, just, you know, smaller companies and smaller groups who want to

34:42.280 --> 34:47.880
use this technology, but just don't have a ton of money or ton of resources. So it would be nice

34:47.880 --> 34:54.520
to have like public resources or something so that people have more access. Any other takeaways

34:54.520 --> 35:03.080
or conclusions that are worth noting or exploring in this paper? Yeah, so there's one other thing

35:03.080 --> 35:08.360
that we talked about in the paper, which I guess I maybe touched on a little bit earlier,

35:08.360 --> 35:14.600
but like I think that we already have, so like when I analyze like the model that I trained,

35:15.400 --> 35:18.920
a lot of the cost was just like, yeah, doing this like tuning and development and stuff,

35:18.920 --> 35:22.840
basically doing these like just grid search as a hyper parameters, so like just like considering

35:22.840 --> 35:28.200
like every pair of, or every like combination of like K hyper parameters, which is like a really

35:28.200 --> 35:33.400
dumb approach. That's really like wasteful and we have techniques, you know, like Bayesian

35:33.400 --> 35:38.280
hyper parameter search and other stuff that are that are better, but at least in my experience,

35:38.280 --> 35:42.040
like most people are not using them. So I know Google has like an internal tool that does this,

35:42.040 --> 35:45.800
so people do do it at Google, but the other tool is like not a good source, for example.

35:46.680 --> 35:52.120
So I think like another thing that would be great is if these big like deep learning tool cuts

35:52.120 --> 35:57.880
that like everyone is using like TensorFlow and PyTorch, etc, you know, made more of an effort to build

35:57.880 --> 36:04.280
in these more energy efficient approaches that already exist, so it's like easier for people to do

36:04.280 --> 36:10.680
more efficient modeling. That would be great. Nice. Shout out to our friends at Sigopt that does this

36:10.680 --> 36:15.880
for Bayesian optimization, but it's I think another take on it, like they, you know, go in and are

36:15.880 --> 36:21.320
talking to people about like performance and, you know, making better models, and this is a whole

36:21.320 --> 36:29.960
different take on it and looking at the efficiency of doing it. Yeah, absolutely. I think, yeah, that's,

36:29.960 --> 36:36.280
I mean, for me, that's like a big gain of, yeah, like Bayesian searches. Yeah, just like being able

36:36.280 --> 36:41.000
to actually search the space intelligently. Right. Well, I guess yeah, in terms of energy use,

36:41.000 --> 36:45.720
rather than just like the accuracy. I think it's often like easier to show that, at least in my

36:45.720 --> 36:51.080
research, something I've enjoyed is like rather than trying to like make some number higher, I tend

36:51.080 --> 36:54.840
to be just trying to like match some number, but like make the model more efficient. And I think

36:54.840 --> 37:01.640
it's actually a lot easier. Yeah, because like everyone's trying to make the number higher. It's

37:01.640 --> 37:06.440
at a certain point. It can be challenging. Cool. Well, Emma, thanks so much for taking the time to

37:06.440 --> 37:11.960
share what you're working on. It's very cool stuff and an important paper for sure. Yeah, thanks

37:11.960 --> 37:17.000
so much for inviting me. Yeah, I'm really glad because I think like, yeah, it's a really important

37:17.000 --> 37:21.640
thing for people to think about. So I'm grateful that you invited me on the podcast and talk about

37:21.640 --> 37:25.640
this so that we can get some more visibility and more people thinking about this. Yeah, absolutely.

37:25.640 --> 37:29.640
Oh, I guess one more thing that I'd like to say is that there's been kind of like a lot of

37:29.640 --> 37:34.600
discussion about this paper online and we want to like encourage that discussion and provide,

37:34.600 --> 37:38.840
you know, like a place for that discussion to happen. So we were going to upload the paper on

37:38.840 --> 37:42.920
OpenReview, which is like a forum where you have like a paper and then you can have like discussion

37:42.920 --> 37:47.640
by people. So I'm playing on doing that like today or tomorrow. So maybe we can think or something.

37:47.640 --> 37:52.440
But yeah, hopefully if people want to like discuss more, hoping to provide a forum for that to happen.

37:52.440 --> 37:56.200
Okay. Well, yeah, shoot us the like and we'll include it in the show notes page.

37:56.200 --> 37:59.000
Awesome. Thank you so much. Thanks Emma. Thanks.

38:03.000 --> 38:08.040
All right, everyone. That's our show for today. For more information on today's show,

38:08.040 --> 38:15.960
visit twomolai.com slash shows. Make sure you head over to twomolcon.com to learn more about

38:15.960 --> 38:39.880
the twomolcon AI platform's conference. As always, thanks so much for listening and catch you next time.

