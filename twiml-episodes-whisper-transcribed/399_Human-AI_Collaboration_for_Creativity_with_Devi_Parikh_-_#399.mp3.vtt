WEBVTT

00:00.000 --> 00:06.960
All right, everyone. I am here with Davey Pratik.

00:06.960 --> 00:11.760
Davey is an associate professor in the School of Interactive Computing at

00:11.760 --> 00:16.640
Georgia Tech, as well as a research scientist at Facebook AI Research.

00:16.640 --> 00:19.520
Davey, welcome to the Tomo AI podcast.

00:19.520 --> 00:24.640
Thanks for having me. It's great to get a chance to speak with you and learn a bit about what you're up to.

00:24.640 --> 00:32.400
As is typical, I'd love for us to start by having you introduce yourself a little bit to our audience.

00:32.400 --> 00:42.160
And in particular, share the source of your interest in computer vision and AI and what led you to the field.

00:42.960 --> 00:49.600
Sure. So I think my interest in this field started in I think about the third year of undergrad,

00:49.600 --> 00:55.120
my junior year, where our program had several research projects that students could get involved in.

00:56.000 --> 00:59.920
And especially a funny story, I was interested in computer architecture at the time.

01:00.640 --> 01:03.920
And I thought I had signed up for a computer architecture project, but then when they were

01:03.920 --> 01:08.240
matching students to project, I somehow got assigned to this machine learning project,

01:08.240 --> 01:12.240
which at the time we were calling patent recognition, because I was in the EC department,

01:12.240 --> 01:17.520
and that's what we called it then. But yeah, as I was working with it, that you can accidentally end

01:17.520 --> 01:22.480
up on a bit over two choices. It sounds like a popery kind of class.

01:22.480 --> 01:31.280
Yeah, so this was meant to be like a free form class. It wasn't in class. It was meant to be research

01:31.280 --> 01:36.240
projects, industry projects that students could work on for credit. And so this was through that.

01:36.960 --> 01:42.800
And so that's how I started working in this space. I enjoyed it enough to go to grad school and

01:42.800 --> 01:48.800
pursue this in grad school. And then the transition to computer vision happened about in the first

01:48.800 --> 01:54.960
year of when I started my PhD, where I was working on patent recognition and machine learning

01:54.960 --> 02:01.600
problems for intrusion detection in computer networks. But then I had colleagues around me who were

02:01.600 --> 02:06.400
working on images with computer vision, and they could sort of visually see the output of the

02:06.400 --> 02:11.840
things that they were working on, which to me felt very sort of accessible, intuitive,

02:11.840 --> 02:16.800
and more appealing. And I think that's where the draw came from. And I switched over to that.

02:16.800 --> 02:19.680
And that's what I've been doing for all these years since.

02:20.640 --> 02:24.320
Awesome. And you've been at Georgia Tech for how long?

02:25.840 --> 02:28.880
I think about four years, three and a half four years now.

02:28.880 --> 02:34.960
Okay. Cool. And you're also, as I mentioned, at Facebook,

02:34.960 --> 02:43.200
what are you kind of equally at both, or how do you, how do those go out for you? Yeah.

02:43.200 --> 02:49.280
Yeah. So I split my time between Georgia Tech and Fair. I'm at Georgia Tech in the fall.

02:49.280 --> 02:55.600
So I'm physically in Atlanta from about mid-August to mid-December or so. But I'm teaching classes

02:55.600 --> 03:01.040
and things of that sort. And then I'm on leave from Georgia Tech in the spring. And then that's

03:01.040 --> 03:09.040
when I'm spending time at Fair in the spring and summer. That's a much cleaner, or split

03:09.040 --> 03:12.720
than I imagined. Yeah. And I like that it's this clean. I can't

03:12.720 --> 03:17.920
that our colleagues who have like one day a week and flying back and forth goes to coast,

03:17.920 --> 03:23.680
and that just seems like a lot. So this is a much cleaner and more peaceful split.

03:23.680 --> 03:29.520
Yeah. Yeah. So tell us a little bit about your current research interests. How do you

03:29.520 --> 03:38.080
focus your research at Georgia Tech's last fair? So like we were talking about my background is in

03:38.080 --> 03:44.560
computer vision. In the last several years, five, seven years at this point, I've done a lot of work

03:44.560 --> 03:49.040
at the intersection of vision and language. So things like which were question answering, image

03:49.040 --> 03:55.760
captioning, and things of that sort. So that's been sort of my main research agenda. This whole

03:55.760 --> 04:00.400
time and continues to be. I still spend a lot of time on it. But in the last couple of years,

04:00.400 --> 04:06.080
I've gotten more and more interested in problems at the intersection of AI and creativity.

04:07.040 --> 04:12.640
And so it's still a very early, very exploratory, but I've been thinking about that quite a bit as well.

04:12.640 --> 04:19.840
So I spent my time between vision and language things. Also some body AI work of virtual agents

04:19.840 --> 04:23.760
in virtual environments and things like that. But like I said more recently, I've been thinking a

04:23.760 --> 04:32.320
lot about AI and creativity. And did the work in AI and language or vision and language lead directly

04:32.320 --> 04:40.640
to the AI and creativity or what kind of spurred that interest? Yeah, I think it's hard to kind of

04:40.640 --> 04:47.760
backtrack and figure out exactly what got me interested in this. I think overall I've generally

04:47.760 --> 04:53.680
had an inclination towards AI systems that are interacting with people. And so I think my interest

04:53.680 --> 04:57.680
in vision and language also was the language part of it. My background is in vision, but I think

04:57.680 --> 05:03.680
what drew me to language was the fact that it's sort of a very natural interface for humans for

05:03.680 --> 05:08.560
people to interact with these systems and ask questions or get the descriptions from the machine

05:08.560 --> 05:13.120
and get a sense for what the machine might be seeing and things like that. So I think it is that

05:13.120 --> 05:19.920
human AI interaction collaboration aspect that also interests me is also one of the reasons why

05:19.920 --> 05:28.640
I'm excited about AI and creativity. Cool. And so you're delivering a keynote at the AI

05:28.640 --> 05:37.360
and creativity or AI plus creativity workshop at CVPR. First off, maybe tell us a little bit about

05:37.360 --> 05:45.360
that workshop. What's the focus of the workshop? Is it a new workshop for CVPR? Is this a workshop

05:45.360 --> 05:51.280
that's been going on for a long time? I think it's been around for at least a few years. It's called,

05:51.920 --> 05:57.840
I think the workshop is called Computer Vision for fashion art and design. And I know they've

05:57.840 --> 06:03.200
had at least one iteration of it before this and there may have been others earlier as well.

06:03.200 --> 06:09.440
Neurips also has a workshop on AI and creativity that has been going on for at least a few years

06:09.440 --> 06:16.320
and again may have been longer. Okay, I think the AI and creativity is the title of your presentation

06:16.320 --> 06:25.840
at the computer vision for fashion art and design. And so what's your, tell us a little bit about

06:25.840 --> 06:36.080
your presentation there? Sure. So I guess I start the talk by even talking about why I think AI

06:36.080 --> 06:43.920
and creativity is exciting and could be impactful. And so to do that, I first start by what creativity

06:43.920 --> 06:50.960
even is. And I think it's sometimes hard to forget how general and how powerful just creativity is.

06:50.960 --> 06:59.120
It's essentially sort of any new idea that is of value period. It doesn't have to be in the context

06:59.120 --> 07:06.000
of art specifically or music specifically or poetry specifically. It's all of this but it's also just

07:06.000 --> 07:12.080
even anything of scientific progress and technological progress. All of that stems from new ideas

07:12.080 --> 07:19.200
that are of value in some way. So in that sense, one could even argue maybe a little bit of an

07:19.200 --> 07:25.280
exaggeration, but one could argue that all of progress of any kind of rests and creativity.

07:25.280 --> 07:31.920
And so if we can think of if there are ways in which AI can assist us in this creative endeavor,

07:31.920 --> 07:38.160
I feel like that's that could be that could be very powerful. Is that a is that an accepted

07:38.160 --> 07:45.200
definition of creativity? I'm particularly curious about the value part that seems. And for a lot

07:45.200 --> 07:50.000
of the things that we think of as creative, that seems to be particularly difficult to

07:51.840 --> 07:57.360
nail down. Yeah, yeah. So it is well accepted definition. I mean to the I guess well,

07:57.360 --> 08:03.200
accepted in the sense that that's what Wikipedia says. Wikipedia defines creativity that way as well.

08:03.200 --> 08:08.240
But then a lot of researchers like a lot of computational creativity researchers who do spend

08:08.240 --> 08:13.840
a good amount of time thinking about how we can define creativity. Maybe even attempt to evaluate it.

08:13.840 --> 08:20.000
Those definitions and those ways of thinking also tend to have these two components of novelty

08:20.000 --> 08:27.040
where you want something new. But there are ways of being novel just by being completely random or

08:27.040 --> 08:31.680
sort of sort of if it's a language domain, it's just gibberish and yes, sure, it's new, but

08:32.240 --> 08:39.600
is it really off value in some way? So I think that value component is also often is often thought

08:39.600 --> 08:45.200
about as part of creativity. It doesn't say anything about how to define value. So I think it kind

08:45.200 --> 08:50.400
of still offloads that issue of subjectivity. It just moves it from creativity into that value piece.

08:51.360 --> 08:56.080
And even novelty, if you think about it is not that easy to quantify, even that can be quite

08:56.080 --> 09:01.840
subjective. So I think it's useful to break it down into these components. I found it useful to

09:01.840 --> 09:06.400
think about it that way in the various domains where I've thought about it. But it doesn't really

09:06.400 --> 09:10.640
take the subjectivity away. It doesn't really make it any easier to define each of these spaces.

09:11.200 --> 09:16.560
Just thinking about it in terms of those pieces directly impact some of the things you've done

09:16.560 --> 09:27.280
in the space. I think it does. It's over hasn't directly impacted what kind of problem I look at

09:27.280 --> 09:34.480
in the AI and creativity space. It also hasn't necessarily impacted the kind of approach that I might

09:34.480 --> 09:38.800
use to the problem. But it has very directly impacted how I evaluate the approach.

09:39.520 --> 09:45.040
And the kinds of baselines that I think about when I want to compare the approach. So it's useful

09:45.040 --> 09:50.000
where if there's some approach that we have in mind. And then if we go back and think about where

09:50.000 --> 09:55.440
is the novelty coming from? Where is the value coming from? Then it makes it easier to think about

09:55.440 --> 10:00.160
what baseline would capture just the novelty but wouldn't have the value piece. And what baseline

10:00.160 --> 10:04.160
would capture just the value piece but might not have the novelty. And then those become

10:04.160 --> 10:10.240
national baselines to compare our approach to to see if we are getting a healthier mix of novelty and

10:10.240 --> 10:14.720
value. So I know that's a little abstract, but hopefully that's useful.

10:16.480 --> 10:21.680
When you think about or when you kind of plan out your work in the space, are you

10:21.680 --> 10:33.920
primarily interested in kind of AI augmented creativity where the AI is helping us to be

10:34.880 --> 10:41.680
more creative? You mentioned that earlier. Or do you also look at just kind of creative or

10:41.680 --> 10:50.240
or pseudo creative depending on how rigid you want to define things. Output of AI algorithms.

10:50.240 --> 11:00.160
Even that question, can AI be creative in and of itself? There's a lot of AI-generated art.

11:00.160 --> 11:07.280
You do some of that stuff yourself. Does that qualify as creativity?

11:08.880 --> 11:13.360
I think that's a great question. There's a lot of interesting conversations that are happening

11:13.360 --> 11:18.640
in the community around exactly this. What does it even mean for machines to be creative? Can

11:18.640 --> 11:26.320
machines be creative at all? There are fairly strong opinions on both sides of this and everything

11:26.320 --> 11:35.440
in the middle. I think it's useful to think about that. I don't think that directly impacts

11:35.440 --> 11:41.840
the kind of work I do primarily because like I was saying whether or not machines can be creative

11:41.840 --> 11:46.320
feels like a secondary question in the way I approach it. I'm more interested in knowing whether

11:46.320 --> 11:51.200
machines can help humans be more creative than they would have been on their own. I think if it

11:51.200 --> 11:55.600
has sort of a team setting where if you had the machine alone that was trying to produce something,

11:55.600 --> 12:00.400
if you had a human alone that was where they were trying to produce something and instead if both

12:00.400 --> 12:08.480
work together is the final creative artifact more creative or even if it's not the case but is the

12:08.480 --> 12:13.440
process is the creative process more engaging and more enjoyable for the human than it would have been

12:13.440 --> 12:20.080
if a machine wasn't involved. So that's how I tend to think of it. I do think that there are

12:20.080 --> 12:25.760
multiple stages at which a machine could interact with the human in the creative process. It could be

12:26.640 --> 12:31.440
a very tight engagement where throughout the creative process both are sort of working with

12:31.440 --> 12:35.840
each other to get there as you might with sort of a human collaborator, a human team member.

12:36.480 --> 12:41.920
But I think it can also be something where sort of the seed of inspiration comes from something

12:41.920 --> 12:47.840
the machine did and then the human takes over from there and if we can show that what the human

12:47.840 --> 12:54.080
produces at the end of it or the process that they go through after being inspired by whatever

12:54.080 --> 12:59.600
the machine did is somehow better, more creative, more enjoyable, more satisfying than what would

12:59.600 --> 13:05.440
have happened if if this seed of inspiration didn't wasn't there from the machine. I would still

13:05.440 --> 13:10.240
think of that as success even though the human and the machine are not necessarily closely working

13:10.240 --> 13:16.320
with each other. So in that piece of the machine providing a seed for inspiration, the machine could

13:16.320 --> 13:21.600
be working fairly autonomously where there isn't necessarily human involvement, but I still think of

13:21.600 --> 13:31.200
that as AI assisted creativity for the human. Besides that particular categorization of whether

13:31.200 --> 13:41.440
the two are working together or in series is there like a framework for thinking about

13:42.800 --> 13:49.840
the different directions that people are going in terms of AI and art or even AI and

13:49.840 --> 14:02.880
computer assisted creativity just in terms of laying out a landscape for the different

14:02.880 --> 14:10.560
directions of research and practice. So I don't know if there's a well-established framework for

14:10.560 --> 14:14.720
it and so there's a good chance that I'm missing some frameworks or some ways of thinking about it

14:14.720 --> 14:21.360
that other people have, but if I were to think about it now, I can think of a couple of different

14:21.360 --> 14:32.960
ways of organizing it. One is where the creative task is a very task-driven one and so where

14:34.080 --> 14:39.040
the human might be trying to do something very specific where they're trying to design a bridge

14:39.040 --> 14:44.640
that will work well for a particular scenario. So they're trying to get a particular task done

14:44.640 --> 14:49.760
and to do that task there is a lot of creative thinking that's required and the machine could help

14:49.760 --> 14:56.080
in that in some way and the other is where a human is more just exploring where they are

14:57.040 --> 15:01.760
just sort of like you might like the contrast between sort of drawing versus doodling.

15:01.760 --> 15:06.080
In the former you are very intentionally trying to create something specific whereas if you're

15:06.080 --> 15:10.880
doodling you're just kind of sketching different things. So that kind of thing where you're just sort

15:10.880 --> 15:15.760
of exploring trying to encounter something that is of creative value to you and I think those two

15:15.760 --> 15:22.800
scenarios require different kinds of tools to help you in that. So that's one kind of separation

15:22.800 --> 15:31.440
that we can think of. Another kind of separation could be where the machine tries to mimic humans

15:31.440 --> 15:36.160
in the process of trying to be creative. So for example if you think about trying to get machines

15:36.160 --> 15:42.640
to find a new dance, a new choreography that will go well with music. You could train the machine

15:42.640 --> 15:48.320
using supervised data where you have data of dancers dancing to different forms of music and so

15:48.320 --> 15:53.680
there the machine is trying to mimic humans to be creative whereas you could instead also approach

15:53.680 --> 15:58.320
it as getting the machine to just discover movements that are well-signed with music

15:58.320 --> 16:02.960
where through something like reinforcement learning is just trying to find a series of movements

16:02.960 --> 16:08.000
that is well aligned with the music but is not at all influenced by the kinds of dances that already

16:08.000 --> 16:14.000
exist in the world and those might have pros and cons and different values based on the kind of

16:14.000 --> 16:19.360
application that you're looking at. So these are two ways of organizing the work that I can think of

16:19.360 --> 16:30.160
but there are probably others as well. So in your talk you are going to be reviewing a handful of

16:30.160 --> 16:38.400
projects that you've worked on kind of across this space. I think the first one is the casual

16:38.400 --> 16:46.320
creator or is it causal or casual? It's casual creator and so that actually goes back to the first

16:46.320 --> 16:54.000
point that I was making. So casual creators and this was a term coined in one of the ICCC

16:54.000 --> 16:58.720
papers International Conference and Computational Creativity from a few years ago so it's not

16:58.720 --> 17:06.240
it's not a term that I've come up with but it refers to tools that are meant to aid humans who are

17:06.240 --> 17:13.680
exploring sort of are just exploring and are not trying to build something or create something

17:13.680 --> 17:19.520
for a particular downstream task and so those are called casual creators. And so we've looked at

17:20.400 --> 17:25.680
what we could do in that space to give people tools that might make it a little bit easier

17:25.680 --> 17:32.320
to find something that interests them. And what are some examples of the specific things that you

17:32.320 --> 17:40.320
explored there? Yeah so one is where we have this I've been I guess on the side I've been

17:40.320 --> 17:48.480
dabbling a bit in algorithmic art which which involves it's basically creative coding where

17:49.360 --> 17:55.920
I create these geometric patterns that I think look interesting and so I set the rules of what

17:55.920 --> 18:00.640
these patterns could look like and there's an element of randomness every time you run the code

18:00.640 --> 18:05.120
you're going to see a different pattern. It's going to follow the parameters that I set up so

18:05.120 --> 18:08.640
it's not going to be completely arbitrary but within those parameters you're going to see

18:08.640 --> 18:14.800
different samples. And in that context there are some parameters that I have set but then there

18:14.800 --> 18:19.360
are also some parameters that somebody else could set for example what color palette should these

18:19.360 --> 18:25.040
colors be sampled from and that you as a user could set that to a variety of different

18:25.040 --> 18:30.800
palettes and then see samples from that. So in that context you could think of the tool that I've

18:30.800 --> 18:37.600
built as being a casual creator and that's a tool that you as a user could use to just explore

18:37.600 --> 18:41.280
different patterns try different parameters and see if you find something you like you're not

18:41.280 --> 18:47.200
trying to look for something specific and what we looked at is as you are interacting with this tool

18:47.200 --> 18:52.560
as you're playing with these different parameters can I can we build a model that can predict

18:53.120 --> 18:57.440
what else you might like. So as you're let's say picking one of the different color palettes based

18:57.440 --> 19:02.800
on your choice of the color palette can I make a guess for whether you will like lines with more

19:02.800 --> 19:08.480
curvature or whether you will prefer straight lines and sort of an angular design and if we can

19:08.480 --> 19:14.000
predict that then as you're interacting with these with the stool I can already narrow down the

19:14.000 --> 19:19.200
space of things that you should explore in the future in hopes that you will find something that

19:19.200 --> 19:24.640
you like faster than you would have if you were just exhaustively trying all these parameters out

19:24.640 --> 19:32.800
so that's that's that's one thing that we've looked at in that space. And so the can you talk a

19:32.800 --> 19:43.920
little bit about the kind of the technical contributions or the particular areas of particular

19:43.920 --> 19:51.600
challenge for a work like that it sounds like part of it is the modeling which sounds a little bit

19:51.600 --> 19:57.200
like a recommendation system I'm almost envisioning like you know sometimes I'll pull up an adobe

19:57.200 --> 20:04.000
you know art app or something similar and there are all these palettes of tools and I would just

20:04.000 --> 20:08.080
love for that thing to guess the thing that I need next as opposed to having me having that you know

20:08.720 --> 20:15.120
exactly exactly exactly so yeah it is you can think of it as a smart tool as a recommendation

20:15.120 --> 20:21.200
system I think the the bigger so that the technical approach is fairly straightforward in this

20:21.200 --> 20:27.040
where we sort of collector pairwise preferences from people in terms of what they like better than

20:27.040 --> 20:31.440
something else and we use a subset of those preferences to see if we can reliably predict the

20:31.440 --> 20:37.600
others I think the bigger question here was whether that signal even exists that based on your

20:37.600 --> 20:43.280
choice of a color palette is there would I be able to guess that you might like straight lines over

20:43.280 --> 20:50.160
lines with more curvature or not kind of thing and it's not obvious going in whether that correlation

20:50.160 --> 20:55.200
would exist or not and the answer to this I'm sure depends heavily on the specific domain or

20:55.200 --> 21:01.200
even specific algorithmic art form that you're looking at and so our my main curiosity in doing

21:01.200 --> 21:08.560
this was to see whether this correlation exists or not to begin with and then we have found some

21:08.560 --> 21:13.600
signal that at least in the specific algorithmic art domain that we were looking at that these

21:13.600 --> 21:20.000
correlations do exist we can predict better than chance at least what else you might like based

21:20.000 --> 21:25.200
on some of the preferences that you've given us so far in the algorithmic art the challenge

21:25.200 --> 21:30.400
then remains of how do you plug this into the tool and sort of you would have to look at sequential

21:30.400 --> 21:34.880
decision making whereas you're interacting with this in a sequential fashion we want to be making

21:34.880 --> 21:39.600
these predictions right now everything that we've done is very sort of snapshot at one

21:40.320 --> 21:48.800
instance in time and so the pairwise preferences was that data that you had to collect from

21:48.800 --> 21:56.880
from scratch or were you able to yeah I find that somewhere yeah no we had we had to collect

21:56.880 --> 22:02.160
that from scratch because we were doing this in the context of this particular algorithmic art

22:02.160 --> 22:07.920
tool that I had so we weren't like using preferences on other things that might already exist out there

22:07.920 --> 22:14.320
and so we did this on Amazon Mechanical Turk where we showed people pairs of art pieces and

22:14.320 --> 22:19.760
and and what's nice about these tasks compared to a lot of other things that tend to be on Amazon

22:19.760 --> 22:24.240
Mechanical Turk is these are much more interesting tasks people like looking at pieces of art and

22:24.240 --> 22:29.200
telling you which one they like better and things like that so it's it's quite easy to collect a lot

22:29.200 --> 22:38.400
of data for a bit more about how you set up the task did you show them like two complete pictures

22:38.400 --> 22:46.320
that varied in one particular dimension or did you or or what yeah exactly that exactly that

22:46.320 --> 22:51.600
where we wanted to make sure that the preferences that we are collecting are along specific

22:51.600 --> 22:56.880
dimensions so we tried to keep everything else the same between these two pieces and only change

22:56.880 --> 23:00.960
one variable like just the color palette or just the thickness of lines and things like that

23:01.760 --> 23:06.560
and based on that we collected preferences so that we can then check for correlations across these

23:06.560 --> 23:16.960
okay very cool and is that an example of one of of of portfolio things that you've done in the

23:17.520 --> 23:24.880
this kind of genre of casual creativity or are there others yeah there is there is one other project

23:24.880 --> 23:30.960
that we've looked at in this space which we've been calling neurosymbolic in generative art

23:30.960 --> 23:36.480
okay so it's and it's perhaps an interesting kind of there's a lot of debate in AI right now in

23:36.480 --> 23:41.760
terms of it's sort of neural networks kind of reasoning pattern matching the right kind of

23:41.760 --> 23:47.680
approach to use for many of these challenges challenging tasks or if we need more symbolic reasoning

23:47.680 --> 23:53.600
to do these things and so it's a it's a little bit of a play on that debate in the context of

23:53.600 --> 24:00.240
a generative art or algorithmic art and so what we did there was we look we took these algorithmic

24:00.240 --> 24:07.920
art tools like the ones I told you where the pattern being generated is through these very symbolic

24:07.920 --> 24:11.760
parameters that have been set like the color palettes and shapes and things of that sort

24:12.640 --> 24:17.840
so there's been that line of work in a generative art where in this setting you can

24:18.400 --> 24:22.400
sample different random samples within those parameters and then there's been a huge amount

24:22.400 --> 24:27.840
of work especially with GANS with generative adversarial networks where people have trained

24:27.840 --> 24:32.960
neural networks that allow you to model the distribution of data and then again you can sample

24:32.960 --> 24:38.480
random samples through that and people have found in some domains that to also have artistic value

24:38.480 --> 24:42.160
where some of these generations are very interesting and you can sort of walk through the latent

24:42.160 --> 24:46.960
space and look at interpolations which make for very interesting visualizations and so we were

24:46.960 --> 24:51.920
curious whether there's something that falls at the intersection of these two generative approaches

24:51.920 --> 24:57.840
the sort of the algorithmic and symbolic versus the the neural generative approach and so what we

24:57.840 --> 25:04.560
did was something fairly straightforward again as more of a pilot study where we a generator

25:04.560 --> 25:09.680
many different samples from the algorithmic approach and in theory we can get as much data as we

25:09.680 --> 25:14.480
want because these are just different random samples from the same system and we train a neural

25:14.480 --> 25:19.600
generative modern on it so we trained again on it and we were interested in seeing that if again

25:19.600 --> 25:26.080
trained on these symbolically generated images is is of value to people whether people find it

25:26.080 --> 25:32.800
interesting to look at to play with to look at these interpolations and what they think there

25:33.680 --> 25:40.080
and so what we found is that when we compare these neurosymbolic generations both the final artifact

25:40.080 --> 25:46.240
and the process of interacting with these systems to the symbolic counterpart people prefer the

25:46.240 --> 25:53.440
neurosymbolic approaches fairly frequently compared to the symbolic one and so that seemed like

25:54.160 --> 25:58.880
good validation that there might be something in this direction that is that is worth exploring more

25:59.680 --> 26:04.960
and did your work in the area give you any intuition for why that is?

26:04.960 --> 26:13.920
So I think it maybe goes back to the novelty and value

26:15.200 --> 26:19.280
distinction the trade off that we were talking about earlier I guess not trade off which is

26:19.280 --> 26:29.040
decomposition where I think these patterns looked interesting they looked they look good

26:29.040 --> 26:34.400
to people they seem to be high quality because they are coming from this symbolic generation process

26:34.400 --> 26:38.640
where somebody has thought through and picked these parameters to make sure these patterns look

26:38.640 --> 26:44.960
interesting but then the neural artifacts that tend to be there in these generations

26:44.960 --> 26:50.320
I think probably look intriguing to people and look different than what they are used to

26:50.320 --> 26:56.240
and so I think that combination is our hypothesis for why they may have found it to be interesting

26:57.520 --> 27:01.600
yeah it's hard to know for sure but that's our hypothesis based on the test that we've run

27:01.600 --> 27:06.160
okay very cool and I should have mentioned this earlier but all of the

27:07.840 --> 27:12.640
examples that you know we're talking about we'll be linking to in the show notes

27:13.840 --> 27:20.560
including your CVPR presentation which walks through these as well so that's the

27:20.560 --> 27:29.040
casual creator side the next set of examples were around kind of this idea of machines inspiring

27:29.040 --> 27:37.520
humans talk a little bit about how you you know how you created a project in that area how did you

27:37.520 --> 27:45.280
set that up yeah so one project that we've looked at in that space is on I alluded to this a

27:45.280 --> 27:52.560
little bit earlier is on seeing if machines can discover dance can discover movements that are

27:52.560 --> 28:00.800
in sync with music and so we had what we were the sort of main motivation there or main

28:00.800 --> 28:06.400
goal was there to not train the machine with dances that already exist we wanted to see

28:06.400 --> 28:11.840
what dance sort of emerges if the machine is just trying to produce movement that sings well

28:11.840 --> 28:18.560
with music what does that look like the people find value in that and so so that was that was one

28:18.560 --> 28:26.240
project where we take in as input a snippet of music we have a music representation that essentially

28:26.240 --> 28:32.640
gives us a sense for which to at different points in time when are the pieces of music similar

28:32.640 --> 28:39.920
versus not and then what we try and do is generate a sequence of movements such that when the music

28:39.920 --> 28:46.880
is similar at two points in time the movements are similar at those points in time the similarity

28:46.880 --> 28:52.640
could be in terms of where the agent is on this virtual stage if you will or it could be in terms

28:52.640 --> 28:57.520
of what actions it is taking at those points in time and so we evaluate which one of these

28:57.520 --> 29:03.440
similarities is better and things like that okay now for someone listening to this conversation

29:03.440 --> 29:10.480
who has in senior presentation you might be envisioning like you know open a gym simulation of

29:10.480 --> 29:19.200
humanoid object or something like that or but actually the example that you showed is quite a bit

29:19.200 --> 29:27.280
more simplistic than that absolutely yeah absolutely and it's not even a stick figure if you actually

29:27.280 --> 29:33.920
think about it it's the agent is parametrized in an extremely simple way like you said it can only

29:33.920 --> 29:40.480
take one of K different states at any point in time okay and at any point in time it can go either

29:40.480 --> 29:47.680
one stay once one state up one stay down or stay where it is and it's not allowed to go out of bounds

29:47.680 --> 29:52.720
so it can't go outside of the range and that's it that's all it can do it can just take three actions

29:52.720 --> 29:58.960
at any point in time and it's characterized with just this one ordinal value of states what is nice

29:58.960 --> 30:06.080
about that though is that because it's it's this general you can visualize or instantiate this

30:06.080 --> 30:13.440
agent in many different ways I can make these K states be a sequence of poses that a stick figure

30:13.440 --> 30:19.200
can take and I think that's what you're referring to it can be even simpler where it's just the size

30:19.200 --> 30:25.440
of a dot where it can become bigger or smaller as the music is changing it can be a dot that's

30:25.440 --> 30:32.720
traversing left to right as the music changes and it can even be a fairly complex geometric pattern

30:32.720 --> 30:38.480
that can be set that can be changed based on one parameter so sort of these I don't know if you

30:38.480 --> 30:43.040
have if you saw these videos and I guess the listeners can watch it offline I'm happy to send

30:43.040 --> 30:49.920
your pointers to it but you can have these very leafy visualizations where the sort of the sway

30:49.920 --> 30:54.960
of that of that leafy pattern changes based on this one parameter and so you can have all these

30:54.960 --> 30:59.440
leaves sort of moving and synch with the music and so that is something that I was very excited

30:59.440 --> 31:04.000
about that it's so general that you can have many different visualizations and hopefully

31:04.000 --> 31:08.960
some visualizations are more inspiring in terms of what movements make sense than than others

31:09.840 --> 31:15.920
and I thought that was that was kind of cool I didn't see that the the leafy one if I'm thinking

31:15.920 --> 31:22.880
of the same one and it's really interesting to understand now that those are generated by the

31:22.880 --> 31:27.360
same underlying model because I found the leafy one much more compelling than the stick figure

31:28.320 --> 31:34.320
it's like really interesting visually I think exactly exactly and so there's a lot of yeah

31:34.320 --> 31:39.040
it's exactly that that all of these different visualizations it's the same underlying agent the same

31:39.040 --> 31:43.440
characterization anything a lot of interesting work can be done in figuring out what these

31:43.440 --> 31:49.120
visualizations should look like to make the same underlying movement more or less inspiring more

31:49.120 --> 31:55.040
or less appealing for what a person is trying to do at the end of it and so how do you go about

31:55.040 --> 32:01.440
figuring out what that model should look like as you're starting a project like this so by the

32:01.440 --> 32:07.840
model you mean like what the characterization of the agent should be or right right I think to be

32:07.840 --> 32:15.200
honest in this case it was it was essentially us trying to think about what instantiation captures

32:15.200 --> 32:21.440
the sense of what we're trying to get out so our interest in this was not to figure out how we can

32:21.440 --> 32:28.880
get a humanoid to stay stable and learn the laws of gravity or anything of that sort which sort of

32:28.880 --> 32:34.080
a lot of the reinforcement learning opening I gym like things are meant for so we were not interested

32:34.080 --> 32:40.720
in those things we were interested in this question of if we produce movements that are just in

32:40.720 --> 32:45.520
sync with the music that's that's all the constraint is what does that look like does that look

32:45.520 --> 32:51.280
interesting or not and so we were trying to figure out what is a characterization of the agent

32:51.280 --> 32:56.480
that leaves out all the challenges that we're not interested in but maintains the sense of the

32:56.480 --> 33:02.240
question that we are interested in exploring and this instantiation that that I described was

33:02.880 --> 33:07.680
one that seemed to capture that and so that's what we went with but that could be other starting

33:07.680 --> 33:15.440
points that are equally reasonable and in this particular case where do you go with the

33:15.440 --> 33:22.000
the research does it matter even to try to scale up the you know the model so that it you

33:22.000 --> 33:26.480
know has more stage or is continuous or something like that or is that kind of beyond the point

33:26.480 --> 33:32.240
of what you're trying to show here yeah no I think I think that would be of interest in terms of

33:32.240 --> 33:37.200
because the kinds of movements that we might be able to get might be more interesting if the agent

33:37.200 --> 33:42.560
can take more actions and be in more states than things of that sort so there's probably I'm sure

33:42.560 --> 33:47.200
not probably I'm sure there's a space of movements that cannot be captured with the kind of

33:47.200 --> 33:52.480
instantiation that we have and so I think that would be of interest those could be more inspiring

33:52.480 --> 33:57.840
another thing though is even before that the current approach that we've used to find

33:58.480 --> 34:04.720
this movement that is synced is actually a fairly straightforward just greedy search like approach

34:04.720 --> 34:10.720
there isn't in that sense sort of actual learning that's happening it's more a search process where

34:10.720 --> 34:16.400
we're just optimizing for this movement being synced with music and so what that means is whenever

34:16.400 --> 34:21.840
there's a new piece of input music we're doing the search from scratch it's fairly fast because

34:21.840 --> 34:25.440
it's a greedy approach but it's still every time you give me a piece of music we're just doing

34:25.440 --> 34:30.720
the search from scratch and then finding a dance that goes with it I would be very interested in

34:30.720 --> 34:37.280
doing this in a more machine learning fashion where we've learned mappings of given an input

34:37.280 --> 34:43.440
music what are the characteristics that the output dance needs to have so that at test time we

34:43.440 --> 34:48.480
can now just given an input piece of music just directly predict what the movement should look like

34:48.480 --> 34:54.560
rather than having to run the search process at test time and so starting with a large database of

34:54.560 --> 35:00.720
music of songs and sort of training this model to be able to figure out what pattern makes sense

35:00.720 --> 35:05.200
pattern of movement makes sense and then using that to do the prediction is something that I

35:05.200 --> 35:11.200
think would be would be interesting to do. You mentioned the the search that you're doing on the

35:11.200 --> 35:18.720
music is looking for parts of the music that are similar is that is it analogous to like a beat

35:18.720 --> 35:27.120
detection kind of approach or something different. It is it is related to that like beats would be one

35:28.400 --> 35:33.520
piece of information that affects where the music is repeating but it's it's quite a bit more

35:33.520 --> 35:38.720
fine grained than that that even if the beat so yes if you look at it's I guess it's hard to

35:38.720 --> 35:43.280
describe in words but you can look at this visualization of a matrix that tells you how similar

35:43.280 --> 35:47.280
the music is at different points in time and there's a lot of rich structure there

35:47.280 --> 35:54.080
that is beyond that includes the beats but goes quite a bit beyond that. Okay I'm envisioning

35:54.080 --> 35:59.680
something like a auto correlation where you're kind of shifting the music and trying to protect.

35:59.680 --> 36:05.360
Exactly exactly very much is an autocorrelation in the in an acoustic feature space so where we're

36:05.360 --> 36:10.400
using these rich acoustic features and we're looking for autocorrelation there and in it's the

36:10.400 --> 36:14.000
same thing that we're looking at in the movement that we also have a similar autocorrelation

36:14.000 --> 36:19.600
like matrix for the movement and we're trying to say that the autocorrelation matrix of the movement

36:19.600 --> 36:24.640
should be similar to the autocorrelation matrix of the music and that's the reward if you will

36:24.640 --> 36:33.200
that the agent gets as it decides what actions to take. Okay cool cool you've also got an example

36:33.200 --> 36:41.520
that is illustrating the collaboration that you have spoken about called sketches or focused

36:41.520 --> 36:47.280
on sketches you talk a little bit about that one. Sure yeah so there we've there isn't a machine

36:47.280 --> 36:53.920
there yet this was we studied this in the context of humans with the idea being that if we can

36:53.920 --> 36:59.680
so the the setup is that if you have you're trying to create sketches and if you have a blank canvas

37:00.320 --> 37:05.600
we were trying to look at what collaboration mechanisms lead to sketches that are more interesting

37:05.600 --> 37:10.800
and more creative than others and so the way this collaboration plays out is that you start with

37:10.800 --> 37:17.120
the blank canvas and then one person comes in and draws some strokes on it and then somebody else

37:17.120 --> 37:23.200
comes and draws more strokes on it and and we keep going and we can see how this sketch evolves

37:23.200 --> 37:27.840
and what the final sketch looks like and we were trying to see like I said what collaboration

37:27.840 --> 37:35.600
mechanisms might make sense in that context and so we found a few interesting things here where

37:35.600 --> 37:41.760
what we found is that if just one person draws the whole sketch from start to finish there's

37:41.760 --> 37:46.080
a variance in quality depending on the motivation and skill level off that person

37:47.680 --> 37:52.000
so that's one there's there's large variance in quality and the other is that even when the

37:52.000 --> 37:59.680
quality is high these sketches don't seem surprising or novel to people they kind of like there might

37:59.680 --> 38:04.160
be a sketch of a tree with a bird on it with a sound in the background and sort of things of that

38:04.160 --> 38:08.800
sort that maybe we've seen before and so people don't find them particularly intriguing

38:10.160 --> 38:15.200
the other setup is where you have different people coming and drawing these different strokes

38:15.200 --> 38:20.960
and what we found there is that these sketches look entirely different the qualitative will look

38:20.960 --> 38:26.000
very different from what what happens when one person draws it all out and so people find

38:26.000 --> 38:30.480
them very interesting they're very intriguing but they also find them to be a little too chaotic

38:30.480 --> 38:34.560
where there's just all sorts of things happening on this canvas and it's harder to sort of make

38:34.560 --> 38:40.160
sense of it sometimes it can also look like it's poor quality and so what we found is that a

38:40.160 --> 38:46.880
collaboration setting where at each stage there is some form of a voting mechanism where let's say

38:46.880 --> 38:52.960
I am the person who now has to add strokes to this canvas if I'm shown five versions of the canvas

38:52.960 --> 38:57.760
and I get to pick which one I want to add strokes to then what we found is the evolution of the

38:57.760 --> 39:03.200
canvas through this mechanism leads to pictures that are still quite interesting still very

39:03.200 --> 39:09.600
different from what someone would make if they drew it alone but are not quite as chaotic and noisy

39:09.600 --> 39:14.480
as what happens when there isn't a voting mechanism involved because what happens is the sketches

39:14.480 --> 39:19.200
that are a little bit more coherent are a little bit higher quality are the ones that tend to get

39:19.200 --> 39:24.080
more votes and those are the ones that proceed forward whereas the ones where someone may have

39:24.080 --> 39:29.040
kind of scribbled something or added something to the canvas that didn't that sort of broke its

39:29.040 --> 39:35.600
coherence tend to not get votes and then those don't go forward so it kind of is a good balance

39:35.600 --> 39:41.680
again of novelty and value where you get of these interesting compositions because so many

39:41.680 --> 39:46.240
different people are contributing to it but the voting mechanism keeps sort of the value and the

39:46.240 --> 39:52.080
quality and the coherence high to eventually give you sketches that were rated as more creative

39:52.080 --> 39:58.000
the idea of these sort of other scenarios interesting so does this you know as this plays out does it

39:59.360 --> 40:05.680
produce something that is amazing like is this a mechanism for allowing and you know a crowd of

40:05.680 --> 40:11.440
kind of the unwashed masses you know with no particular art skill to produce like incredible

40:12.480 --> 40:16.240
things that an individual probably couldn't or wouldn't or

40:16.240 --> 40:24.560
is it you know less modest than that and it's in the output so I would say it is less modest than

40:24.560 --> 40:29.920
that I think the way you put it was yeah I think it is it is less modest than that but I think it

40:29.920 --> 40:36.320
is along those lines I think it is taking steps in that direction where as a result of this crowd

40:36.320 --> 40:42.640
of people who may have varying levels of skills in terms of making these sketches we ended up

40:42.640 --> 40:47.280
creating something that no individual would have created it was qualitatively different

40:48.320 --> 40:53.520
whether you like it better or not again a lot of these things are subjective we did find that more

40:53.520 --> 40:58.320
people in our studies liked these collaborative sketches better than they like the ones that

40:58.320 --> 41:02.800
individuals have created yeah there's variance along that but they're certainly qualitatively

41:02.800 --> 41:06.640
very different they're not they're not the same thing so we are getting artifacts that are

41:06.640 --> 41:12.000
different as a consequence of this collaboration yeah the setting sounds really interesting and I've

41:12.800 --> 41:19.360
got to imagine that there have been lots of attempts at you know collaborative art of various forms

41:20.800 --> 41:26.240
and adding in you know some kind of voting mechanism or or something like that or you know

41:27.360 --> 41:34.240
a vetting mechanism of each individual's contribution to this thing or the the prior

41:34.240 --> 41:41.120
or round of contributions to this thing sounds like an interesting way to help the end result evolve

41:41.120 --> 41:47.040
more quickly into something that is appealing yeah exactly exactly and it touches on

41:48.320 --> 41:53.760
much larger questions of what collaboration should look like when people are engaging in a creative

41:53.760 --> 41:58.160
activity I mean we've obviously studied this in a very narrow domain in a very specific setting

41:58.160 --> 42:06.080
but I think the underlying question is quite important in sort of a larger context as well and then

42:06.080 --> 42:14.240
you've got a last category of projects that you've been exploring focused on visual journaling

42:14.240 --> 42:20.720
what's that one about yeah so that one is is a fun project where our our thought was that

42:20.720 --> 42:29.760
if people could see an abstract visualization of their sort of daily journal entry that might

42:29.760 --> 42:34.720
be a way to keep people more engaged that might increase the probability that they will journal

42:34.720 --> 42:40.560
on a regular basis maybe they could also sort of share this entry in a visual way with sort of

42:40.560 --> 42:44.800
family and friends that they're close to or things of that sort and so we're curious to see what

42:44.800 --> 42:50.480
we can do there and if we can create something that is that people do find interesting and so what we

42:50.480 --> 42:58.960
do is we take we asked people with their with their consent to write up a short journal entry of what

42:58.960 --> 43:03.520
the day of what their day looked like and obviously they could decide what they wanted to share in

43:03.520 --> 43:11.200
that journal entry or not and from that we run some sort of natural language processing techniques

43:11.200 --> 43:18.400
to extract what three salient topics were that they were talking about so we had a handful of

43:18.400 --> 43:25.120
categories I think maybe about a dozen or so things like work family friends food sleep

43:25.680 --> 43:29.600
those kinds of things that we can automatically extract figure out which one of these topics

43:29.600 --> 43:34.160
they're talking about and we automatically extract what the associated emotion seems to be I

43:34.160 --> 43:40.160
think we had about 18 different emotions like happy sad frustrated things of that sort that we

43:40.160 --> 43:46.000
can associate with these topics and so with these topics and associated emotions we then create

43:46.000 --> 43:51.040
this abstract visualization where there's a certain shape there is associated with the topic

43:51.040 --> 43:55.440
and there are certain colors that we were associated with these emotions and we have a variety

43:55.440 --> 44:01.440
of visualizations that we produce using these shapes and colors and again we run some evaluation

44:01.440 --> 44:06.560
to see whether people like the fact that the topic is described through a shape whether they like

44:06.560 --> 44:10.880
the fact that the emotion is shown through color do they like having visualizations do they

44:10.880 --> 44:16.240
think they would journal more regularly if their journaling app had this associated visualization

44:16.240 --> 44:21.600
and things like that and we saw a lot of in general positive responses to these things.

44:22.800 --> 44:33.920
Cool. Do you have kind of an overarching message for your workshop audience at the workshop?

44:33.920 --> 44:40.960
I don't know I think the overarching message would just be that I think this intersection of AI

44:40.960 --> 44:47.200
and creativity can be very powerful I think it can be very impactful and it can be a lot of fun to work

44:47.200 --> 44:55.280
on and so I think there's a lot of space for creative ideas no pun intended I guess in terms of what

44:55.280 --> 45:00.560
kinds of things we can look at here what human AI collaboration could look like and things of

45:00.560 --> 45:05.360
that sort I would just sort of encourage people to think about this more and see if they have ideas

45:05.360 --> 45:10.400
in this space and engage if they seem if they feel like they're interested. Awesome awesome well

45:10.400 --> 45:30.720
Davey thanks so much for taking the time to share with you. Thanks for having me this was fun. Thank you.

