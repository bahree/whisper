Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. A couple of weeks ago I spent some time at the Pegaworld
conference in Las Vegas. The theme of the conference was automation, particularly in service
of the customer experience, and I had a great time seeing all the advancements coming
into this field by way of machine learning and AI.
In this the final episode of our Pegaworld series, I'm joined by Vince Jeffs, senior director
of product strategy for AI and decisioning at Pegasystems. Vince and I had a great talk
about the role AI and advance analytics will play in defining future customer experiences.
We do this in the context provided by one of his presentations from the conference, which
explores four technology scenarios from Pegasystems innovation lives. These look at a connected
car experience, the use of deep learning for diagnostics, dynamic notifications and
continuously optimized marketing. We also get into an interesting discussion about how
much is too much when it comes to hyper personalized experiences and how businesses can manage
this challenge.
Before we jump into the interview, a huge thanks one more time to Pegasystems for sponsoring
this series. Amidst the discussion of automation and enhanced customer experience
as a Pegaworld, the company announced several new AI powered capabilities as part of its
new Pegat Infinity process automation platform.
For more info on Pegat Infinity, head on over to pegat.com slash infinity.
Alright, let's go.
Alright everyone, I am here at Pegaworld in Las Vegas and I have the pleasure of being
seated with Vince Jeffs. Vince is the senior director of product strategy for AI and
decisioning. Vince, welcome to this week in machine learning and AI.
Thanks Sam, great to be here.
It's great to have you here. Let's get started by talking a little bit about your journey
to AI. How did you get into this space? What's your background?
Yeah, I've got a surturicus background to coming to where I am, but it's been a fun
journey. I actually studied applied statistics at Georgetown in the University of Maryland
long time ago and I think that's kind of what AI has become. We called it applied statistics
back then. Now it's fancy term AI, but it's all good. From there I worked on the client
side with UPS, working, building, decisioning and marketing technology systems. I worked
with an agency, Rapp Collins. I worked with SAS, the business intelligence and analytics
vendor and did that for about four years. Then I joined a company called Unica. Unica
was actually doing predictive modeling with their first solution. Then they got into the
marketing technology space and I spent ten years with Unica before they were bought
by IBM. I spent a few years with IBM. I like to say, sort of, kiddingly, that my career
took me from Big Brown to Big Blue. Then I ended up with PEGA. I spent the last four years
with PEGA in that product strategy role. So did you know John Hogan? Yes, absolutely.
We worked together in a past life. John's a great guy. He headed up engineering at Unica
for years. Awesome. Tell us a little bit about your role at PEGA. What's your focus at
PEGA systems? It's a lot of fun at this point in my career because I do get to think
about the strategic factors that a software vendor needs to consider as they build product
as they decide whether or not they need to maybe buy something or partner. That's really
the kind of, when I tell people, there's three things I do. I worry about building buy-in
and partnering and the right mix of that and when to do that. That's a fairly broad set
of items there. I really do work closely with our product team. We have a product team
in Amsterdam and a product team in Cambridge. We think a lot about facets of the way
we would prioritize what we would build into our product. We want to be market driven.
We want to do things that are innovative. We want to balance that with what our customers
need and what they demand to make the software successful. Then we also have a lot of people
call it technical debt that you have to always be paying back and fixing the underlying
technology, being able to do things faster and more efficiently. We try to balance those
three things and we've come up with a way of doing that. That's a big part of my job
is working with everyone that's involved in that process. Then also I do work very much
with our partner team. You're here at PEGA World with us. We announced this morning
our launch of an ISV partner program. I've had a big hand in that and some of the partners
that were mentioned that are going to be in that program, such as movable link and
celebrous. In Prasado, I've worked very closely and helped bring those partners along and
nurture them in their journey with us. Fantastic. Fantastic. You gave a couple of presentations
here at the conference yesterday. What were those on? I did. I did. It was a fast, fun,
furious day for me. I did a couple of presentations yesterday. One was a really cool thing that I'm
very passionate about and that is driving new innovations into our product by having
the idea of an innovation lab. It's not so much research. I like to think as research
is more academic. It's more thinking about really applied ways that we can do cool new
things in ways that we can actually affect customer journeys and customer experience
with AI and machine learning. We did what we called for sneak peeks where we wove that
into a story about a gal named Danielle who has a connected car and we're trying to
predict things that might go wrong with her car. We're trying to help her experience
by diagnosing things quicker. We're also trying to be more personalized when we communicate
with her. Then ultimately we're trying to manage any of the communications we're doing
to make sure we're always doing something intelligent, relevant and smart. What was the
other presentation? The other presentation was a panel discussion. I was very thrilled
to be able to host three of our great customers, British gas, GM and HSBC. As I kidded when
I opened it up, we had gas, we had a car and we had financing. We had a nice mixture
of use cases so you could go somewhere. We could go somewhere with that. That's exactly
right. It was fun. They have great stories. I let them do most of the talking. I think
we ended up with a nice blend of where they are today with AI and their use of it in a
practical way and where they're on their journey to what we like to call Omni Channel
experiences with customers and then where they're taking that, how they're expanding
that out in their organizations to try to continue to drive more customer-centric value.
Awesome. Maybe let's go back to that first presentation and talk through these four scenarios
with an emphasis, of course, on the AI and machine learning bits that you were showing.
Yeah, absolutely. As I said, we had four sneak peeks and the first one was really about
Danielle having some car problems. There's connected cars today. GM tells a great story
and they were on my panel and telling that story. Travis Bradburn tells that he's been
10 years working with Pega and the connected car with GM. They really pioneered some of
this. I based on Star in particular. On Star, exactly. Customers of GM that have the
on-star package are familiar with the whole connected car experience. We based that
on that sort of use case a little bit, but where we were taking it was into the deep learning
area. We've been very big about applying what we call practical AI. There's plenty of
models today that are very useful, supervised machine learning models such as logistic regression
and Bayesian and lots of things that we use today, but what we are really experimenting with
is the ability for deep learning to listen to a lot of different data sources and there's
a lot of different data sources coming off both the connected car and Danielle herself
using her mobile app and going on the website and engaging with the brand. We were learning
sort of what might be going wrong with both Danielle's experiences and her car and basically
that was about using deep learning to trigger off a early warning that there may be something
trending bad with her car before we have to actually wait, which is the typical car
users experience today. Suddenly, they get a check engine light and usually when they
get that, there's already something wrong with their car. Sometimes it's something simple
like maybe a gas caps loose, but more often than not, it's something where there's actually
a part that's already failing. What we were trying to use is deep learning to preempt
that to actually listen to all those different sensors and be able to give an early warning
and maybe get that car into the shop to have a look at it before something actually breaks
down.
Can you take us inside the process of building out this demo in the lab? Where did the
data come from, for example, and what were some of the specific data sources that you
used?
Right. The data sources were pretty much the device sensors that are on the connected
car. We were listening to things like breaking gas mileage, oil pressure, and then any
sensors themselves, like sensors that might be able to tell us that the oxygen levels
in the system are trending in a bad direction. That could actually be an indication that
some underlying part is about to fail in the near future. As we're in remember, this
is a connected car. We're getting this data constantly. Every time they call it key
turns, every time there's a key turn, the decision hub is being fed new data. We're able to
learn, again, every time there's a key turn and she takes that car out on her commute,
we're able to learn more about the performance of that vehicle.
What we were doing is we were using deep learning to actually look into the future and say,
we predict that the performance, not necessarily there's going to be a breakdown. It wasn't
so much about actually a bad breakdown, but it was more about gas mileage performance
going down. We want to be able to prevent her from just losing money on paying for unnecessary
gas. That's the alert that we gave her. We said, nothing urgent. Your performance is degrading.
If you come in, she was still under warranty. We'll have a look at this and then that led
to the second sneak peak.
You mentioned that the scenarios incorporated both data from the vehicle as well as data
from Danielle herself did. Any of this Danielle originated data come into play in this
first scenario? It did not in the first sneak peak. It did later on. We'll make sure
to hit on that. You're leading us into the second sneak peak.
What we had done is we had provided her with, again, a friendly alert when it's convenient
and we could also automate the process of helping her schedule an appointment and using
some AI technologies. Those aren't real fancy AI technologies, but they're good customer
experiences to get her into a dealership maybe that she's used in the past and she's comfortable
with and at a time that's comfortable for her. We can maybe not so much predict but give
her convenience associated with her past behaviors.
Then once we got her into the shop, really what we had, the second sneak peak was all about
helping that technician through the diagnostic process. We're using some rules which Pega
has a great rules engine along with some models to help that technician get quicker to
a solution. As the technician was already given a next best set of service actions that
they might look at and they were inspecting the car and saying, well, no, this is probably
not the problem even though you're saying it's a 60% model saying it's a 60% propensity
as soon as the agent gives the feedback to the model that that particular item wasn't
the problem, then the model can recalculate and hone in on what is the likely problem. We
had six or seven different things that were teed up as potential problems in this situation
and we were able to help that technician quickly narrow down to the problem probably faster
than they would have just buy somewhat trial and error which happens in the shop sometimes.
It's interesting that example where we are with machine learning and AI today is kind
of easy to think of that as passe, what's the right way to come at this. What's interesting
to me about that specific example that you give as well, it doesn't strike us as kind
of the way we think of machine learning and AI today. AI in the 70s and 80s was all about
these expert systems that were trying to guide maintenance people through processes to help
them move, help them resolve issues more quickly. It's interesting that we're kind of combining
kind of in this rule system you're describing traditionally or older approaches or thinking
around AI with more modern deep learning and other types of technologies.
Absolutely and I think that's you've hit on what we feel is like you know sometimes innovation
and I'll try to pull out a quote is rediscovering things from the past and then and then putting
a new twist on them right or combining multiple innovations from the past and I think expert systems
still have you know a very good role to serve in the right use cases it's just like when I'm
sure you would agree with me with machine learning it's not about one algorithm it's not about
this new deep learning which is you know really cool new ways to do neural networks which have
been around for you know 20 years or more right but and there's some great advances there don't get
me wrong but it's about picking the right algorithm for the right problems and then sometimes
combining those together into a real solution for somebody like a technician or a car owner to
benefit from the application of multiple sets of analytics together so yes there there wasn't one
piece of analytics I could point to in that second sneak peak that's terribly new or innovative
but it is the combination of these things in practical ways of application that really do drive
value for consumers and organizations and so what was the third sneak peak then so the third
sneak peak is a little cooler okay so so that's good because we don't want to get you know on an AI
podcast and not have a few things that are cool to talk about so deep learning is cool and also
this one is which is this one was about what we really called dynamic email notifications but
we really believe this is going to any kind of dynamic treatments the customers might get and
when I say treatments I mean you know whether you get something that is presented to you on a
website or whether you get it in your mobile app or whether you get it in an email I would think
of all those is sort of consumer treatments it's a brand having a conversation with you and this
sneak peak was about making that email much more dynamic and when you think about it emails kind
of an older channel now almost getting like direct mail right where you think yeah you know that's
a lot of people have have you know predicted the death of email like they did cobalt right but
as we heard on stage it at at Peggy yesterday from Affleck I think there's still cobalt running
so so there's going to be email I believe for a while and it's still a very viable channel
but what we're doing is making it dynamic so that it is essentially like a web page and so really
a lot of people think about emails outbound treatment outbound marketing we really like to think
of it now with this new sneak peak as inbound and that might sound crazy but what I mean by that
is when that email goes out it's nothing more than a template there's containers in it that will
be pre-populated that aren't pre-populated excuse me with content that's the way most emails
are done today organizations have to back up weeks and get approval and decide exactly what's
going to go into that email and then they send it out and then maybe you don't open it for another
two or three days because you're behind on your email right or that's a personal email account
you don't check it that often but maybe you go in there because you do a search for a brand
you're interested in something and pop up that email now suddenly that content is three or four
weeks stale that's not going to make for a great experience very often in the contextual world
we live in so what we did in this sneak peak is we showed how working with a natural language
generation partner that we're now partnered with Pursado and a dynamic email content provider
movable link and Pega we could dynamically populate that email with content at open time
so at open time when you open that email now it calls back to Pega it gets the next best
content to put into that email in this case it was thanking Dan Yale for coming into the
you know get her car service quickly and actually encouraging her to sign up to a new loyalty
program that we had and even encouraging her with some you know points that were specific to her
value and then the language that was used in there which is the Pursado piece was generated
for Dan Yale so knowing what emotional language she responds to and you can do that if you have
enough interactions you need a lot you need maybe 75 interactions with Dan Yale but if you do that
and you test and learn you can learn that she might respond to appreciative language or
urgent language and there's a set of emotions that Pursado sort of keeps track of and they tune
the language and to you know what resonates with her and when you think about it that's great
that's more like a brand having a conversation with a customer rather than having some one-size-fits-all
treatment so there's AI happening at the Pursado level around this natural language generation
and customization is there a lot of what I've seen at at the conference here and in the key notes
is around optimizing the presentation of all offers kind of referring back to the the podcast I
did with Rob Walker and the next best action and next best offer is there talk a little bit about
the AI that's you know that is involved in that piece in this third sneak peak right right so
when when Dan Yale opens that email Pursado has already sort of figured out probably a number of
variations that they're going to test with Dan Yale they've maybe been again if you you know
walk with my scenario and assume that yes we have interacted with Dan Yale before we've had a
number of chances to test and learn with that with that language that we're going to use so they've
got some other variations maybe call it 16 or 20 variations sitting out there that now they're
continuing to test so that loop is going on each time we have an interaction with Dan Yale
what we're doing is we're actually providing the the the actual piece of content that we want to
so the image that we want to put in there maybe we want to put a you know an image of Dan Yale
in her you know not her Dan Yale herself but somebody that you know Dan Yale might resonate with
and the type of car she has and and then we're also putting in very specific copy that we might
be controlling like the fact that she had just been in to the shop yesterday we can actually
tune that to the fact that we put that language in that says thanks for coming in yesterday
as opposed to you know just thanks for coming in so just little things like that that might sound
a little trivial but they add up what we've what we've been able to prove is that as that email
becomes more and more relevant and then we're populating in some very specific content again like
and please sign up for our new rewards program that we just launched you know two days ago again we
can be very specific about the context of that we would love to reward you with 100 value points
for signing up and those value points again we can at real time recalculate and decide how many
points we want to give to Dan Yale based on a model we have about her lifetime value and her
loyalty and this sort of thing so we can be discriminative about how much points we give her versus
somebody else based on her loyalty and we can do that again at open time if we can maybe
digress from the sneak peeks for a second sure one question that is jumping out at me and
this relates to a conversation I had with someone here at the event yesterday I think you know we've
established that consumers appreciate personalized experiences we all know that you know we've
all had the experience the the opposite of that where you you call into a call center to get
something done you get bounced around four times and each time you have to tell them who you are
and what the problem is right that's you know the antithesis of a personalized experience and now
what we're talking about are these hyper personalized experiences where you're drawing on this vast
you know database of knowledge you know based on interactions and I guess that the observation
is that you know sometimes it comes across as creepy yeah right even for me I'm like very
technology for I thought that's where you were going right very technology for like I love the
technology that's enabling all this but you know I hear it and sometimes like and I'm wondering
yeah you can talk about your general perspective on this but I'm wondering specifically
if you've done research into you know from the perspective of innovation labs or if you've
you know seen research elsewhere that tries to you know explore this personalization versus you
know creepiness and and you know the way consumers you know do we have data that says that consumers
really want that beyond the fact that they do actually click more yeah yeah it's it's it's a
really interesting area and I'd say we don't have a lot of data on that and it is a good
area for us as an industry to really dig into more especially with you know more of these you
know regulations that are becoming more prominent right with like everybody knows about GDPR
which just launched May 25th but I think that's just a that's really a swell that's going on
it's not just in Europe I think it's happening everywhere quite frankly where consumers are waking
up a little bit to the fact that there's been a lot of data collected about them and it's being
collected and they need to sort of seize a little bit control of that you know let's face it
consumers are empowered but they're also sometimes naive about what's going on in the background
but then once they become less naive about that then they become more empowered and I think brands
want to you know buy in large most trusted big name brands smart companies want to be responsible
about that but they don't know where that line is exactly and they need to they need to be very
careful about how they test that they don't want to test that in actual practice if they don't
know what they're doing so that makes them a lot more conservative and that's not always good so
what we believe is that they can test some of that using simulations and using other means to
push the envelope a little bit in a lab if you will and then sort of slowly roll out and test
that you know that line how do you test consumer attitudes and consumer reactions to you know
a campaign and a set of interactions and via simulation yeah well I think first of all you need
to do some of those tests with voice of the customer you know with panels and actually you know
ask your customers right you don't have to ask all of them you can use statistics to figure out
you know approximately whether your customer base is happy or not with this kind of a thing that
you're doing but then what you can do is you can use simulations to kind of like run the scenarios
that might play out in terms of things like say opt out it's not always a catastrophic event that
happens when you cross the creepy line but it can be a bad marketing event right like if somebody
opts out and now there's better ways to be a little bit more granular as a customer as a company
about how you set permissions and allow consumers to set those permissions and so they you know
you can get smart about them being able opt out of certain things but then you know entice them
still to want to engage with your brand and other areas even if they're going to opt out of these
these channels or these communications so I think you need to you know be smart about how you test
that I think where you're going which is kind of interesting and it goes back to the the scenario the
the sneak peak of you know using the data that you have and kind of additudinal
learnings about a particular customer maybe there's a way to determine customers that will
be creeped out and then send them more generic if you will emails or less kind of edgy
types of emails I don't it's not clear to me like how you would learn that but it's a really
interesting area and I guess to your to your other point you know maybe just ask right yeah yeah
maybe ask in sample and use you know some traditional methods of statistics to extrapolate right
but I think you know you're on to what you know we're kind of digging into a little bit which is
you know you really have to you know you have to kind of blend that together and
and and then you know be careful about how you roll it out and and and monitor it very closely
right monitor what's going on is you're testing because you don't have to roll these things out
in mass right you roll them out in very small increments when you put it into production and then
you watch what's happening and you compare that with what you thought was going to happen in the
simulations you did and and you adjust right that's one of the hallmarks we think of what Pega
helps do is it really helps customers be able to be more agile about once they do do things in
production about being able to quickly rotate and change you know directions if they need to
or pull something back out if it's not working commonwealth bank of australia and the keynote
this morning talked about how they were able to do those things you know in five hours now to make
changes as opposed to something that took eight weeks to do so that's I think a key you know part of
it is hey you know when you're using AI and when you when you're affecting customer experience
it's going to be a test and learn process you're not going to your lab is not going to have it
completely right right but if you have the ability to at least make an educated guess going in
and then and ask your customers right so we touched on that and then you know pivot fairly
rapidly when you see that it's not exactly going as we drew it up on the drawing board that's key
and I think you can you know get smart then about how you treat those customers in which ones like
you said which ones your modeling which ones you feel are you know going to be more sensitive versus
not and and then your models can learn and get smarter about that is you get more and more
customers that you're learning you know you're getting more granular about the sensitivity almost
like prosados getting more and more granular about the language that you respond to you get more
granular about your ability to decide whether Sam's going to be creeped out or Vince is going to
be creeped out interesting interesting so popping back over to this fourth sneak peak yep yep so
the fourth sneak peak the the story kind of culminated with with Danielle being happy being
in a back in a happy place right she had her car fix she got this you know offered a sign up for
this loyalty program and get points so she accepted that everything was looking great until the
next morning she gets in an accident okay so we had to add a little drama to the to the scenario
and that and of course she has a connected car so we have some we have some intelligence about
maybe how bad that accident was now we don't want to cross the creepy line you know so we don't
like reach out to her or anything but what we do do is and and the scenario continued with her
then get it wasn't a major accident so the airbag didn't go off but she got on right after that
happened she got on her mobile and she was looking on our service pages right maybe she was looking
up to see if you know it's cosmetic damage you know sort of whatever whatever she needed to kind
of you know do a little investigation on and that were those were cues that we were picking up
so we're picking up the fact that suddenly Danielle's like looking at service pages and looking
at these pages that nothing to do with maybe buying a new car or you know how to use her mobile app
so we decided that and we call it next best moment that we had some marketing we had a marketing
treatment teed up to go out to her in two days after sneak peak number three had concluded and
that was going to be we were going to make her this big great offer on a new car because she had
a older connected car well we decided that the next best action and the next best moment for that
was going to be you know that night because she checks emails that night and we kind of learn
that that's the time she's most likely to engage with emails what we did is we actually took that
communication out of the queue because we recognize that there's something going on we don't know exactly
what it is but we predict that she's probably going to be calling us you know for some more service
and so therefore it's not time to be marketing to her so we actually pause that you know maybe
we'll resume it later on again when you know when the time's right but the time wasn't right so we
pause that and we really manage that next touch point proactively interesting interesting and
so across the maybe talk a little bit about what what goes into that from is there MLAI
involved in that fourth scenario and and how is it expressed yeah yeah good question we like
to categorize decision management and the arbitration of decisions as AI we think it's fair
actually there's there's been you know other third parties like Forster that have agreed with
that that decision management is a form of AI I guess those things can be argued by the AI
purists right and and you know to some extent I understand their arguments that you know let's say
basic rules or even some of the basic robotics that's just pure automation right there isn't anything
you know terribly sophisticated algorithmically going on let's just for the sake of this
conversation say that that's part of AI decision management so we can we can decide the you know
the the decisions that we're going to make either today when Danielle comes in channel in the
mobile app we decide what we want to actually present in a container there or on the website
but we also can look at the decisions that we've teed up right that our marketing organization
has teed up and they're in a queue ready to go out and and we can arbitrate all that we can capture
you know things we can sort of you know capture and kill things if we need to or capture and pause
things so that was really more about again not fancy AI technology so to speak but the orchestration
of decisions across channels which again what we've seen is that that that just has tremendous
benefits for organizations yeah and that's been a core theme across the keynotes that I've seen here
and and one of the things that I've observed is that a lot of emphasis has been placed on
the distinction between this traditional approach to marketing that is characterized by
set segments that are determined say on the on the period of you know weeks you know versus a
continually optimized marketing loop that's in a real time putting the other offers for
for customers and you know this model extends beyond marketing but marketing is an example that's
been frequently used can you allow rate on that distinction and how you see that playing out
absolutely I think that's a really important distinction and it actually is a you know we sometimes
call it like a hundred and eighty degrees shift in in the way you think about how you approach
understanding a customer and then taking action on that customer so when you use the more
traditional approach as you called it which by the way is has you know still has applications
today in some cases where you just simply don't don't have any customer data right if you have
to go out and do advertising and you really don't have customers that are aware of you you might
have to use segmented approach to reach those customers but once you have some customer data
and that doesn't take very long as an organization right once you or on board a customer
and they've transacted with you a little bit you've got a wealth of information about that individual
customer why not use that in the moment for any conversation or treatment that you're going to
have with that customer as opposed to putting them into buckets where they're going to be you know
batch treated um sometimes uh in in a way that lots of other customers will be treated and maybe
a few of those it's very relevant too but maybe a lot of them it doesn't resonate very well
so it really is a more always on transactional approach to say that what we're going to do is we're
really going to let this engine wake up and get the data that it's got about this customer
as of right now and make a decision of what to put into something and do that populate it
in real time literally in hundreds of milliseconds doesn't matter if it's a web container
doesn't matter if it's a mobile app that's calling to get some next best action doesn't matter
if it's a you know call center rep that that needs more information or wants something recalculated
but you do that in real time as opposed to as you said sort of pre-calculating it sometimes weeks
you know ago which things can lose their relevance very rapidly maybe that customer you know
bought something that you're now you know you're you're marketing to them something they already
you know purchased or or they're now interested in something else and you're not factoring that
in to your conversation you need to sell this to Amazon because they do that all the time they do
you were just shopping for shoes yes I bought this shoes why are you still telling me about this
shoes and you know that's because that's a you probably know that's an algorithm that you know
collaborative filtering and wisdom of the crowd algorithm again it has its applications
especially when you've got tight product affinities it can matter you know this tie goes with this
shirt sort of thing but then there's other times when that algorithm isn't working so well
right and where you know something about that customer you need to use that intelligence rather
than just the fact that other people buy these things together yeah yeah um and so maybe to to wrap
up the session that you did with HSBC it was GM GM British gas British gas what were the highlights
of that session yeah the highlights were really we spent about half the time talking about
how they got to where they are and the fact that they're not there yet right they're getting
great value but it's it's a job in these big these are all big companies right huge companies
and so it's a long journey to to get a big company to really change its mindset about how it
approaches these things and and to do that across all the channels that they're interacting with
uh in in case of GM they started with on star that's great on star customers are getting a more
dynamic and you know personalized and you know they're using more data to help improve the experience
of the on star customer but what about the all the other GM customers that don't use on star right
and so Travis talked about how they're starting to roll that out into more of their you know
programs beyond just the on star program British gas uh Joe Allen told a similar story about where
they really did tackle a lot in their first couple of years of putting peg in they really felt like
you know if you're a gas or electricity customer you don't interact with those brands too often
right but when you do it's really important right it might just be before your contract
is up or so there's these really important moments of truth where you've got to get
things well with synchronized so she talked about how they they couldn't start in just one
channel they really went in you know in and she calls it we did inbound outbound and we ended up
with unbound um which was I I love the tagline because the idea there is it's more channelless
it's really not about channels it's about making decisions for the customer and being coordinated
across omnichandals um but then she talked about how they're taking that to paid media um and
you know outside of their own media which is really interesting and then she also talked about
how they're using it now in their loyalty program and they're expanding the use of it in their
loyalty program so um and then HSBC also talked along similar lines Fabian uh was great
about explaining how the bank is really interested in getting those experiences right when customers
are doing things on the website you know when they're when they're using a mortgage loan calculator
that's incredible information you know if they stop using it suddenly you know there's opportunities
to again reach out and try to help the customer maybe they were struggling with something you know
maybe there's questions that you've got to answer for them and so they he told you know great
examples of where brands need to continue to think about AI and the ability for it to drive
you know self-service but also how the human comes into the loop in those it's really important
that the agents be well equipped and they can get AI help to be better equipped to have great
conversations with customers and be more you know be more convenient and more helpful with them right
so we don't have those experiences that we started out with that you talked about with somebody
bouncing around an IVR you know getting a rep that transfers you to another rep that asks you
the same questions over and over and you're just about to pull your hair out um so it is really
important to get the human well you know integrated into that loop with AI and you know um Fabian told
great examples of how they're doing that fantastic Vince it's been great catching up on what you're
up to anything you'd like to add to close us up no it's been great to talk here with you Sam it's
it's an exciting topic I love your program and um I'm passionate about it as you can tell I know
you are too so thanks for having me on absolutely thank you all right everyone that's our show for
today for more information on Vince or any of the topics covered in this episode head over to
twimmelai.com slash talk slash 154 to follow along with the pegaworld series visit twimmelai.com slash
pegaworld 2018 for more information on pegassystems for their new pegat infinity offering visit pegat.com
slash infinity as always thanks so much for listening and catch you next time
