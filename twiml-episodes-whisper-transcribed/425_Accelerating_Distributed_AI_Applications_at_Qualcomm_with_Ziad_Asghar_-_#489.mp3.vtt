WEBVTT

00:00.000 --> 00:16.760
All right, everyone, I'm here with Ziyad Asgar. Ziyad is vice president of product management

00:16.760 --> 00:22.600
for Snapdragon technologies and roadmap at Qualcomm. Ziyad is great to see you again and

00:22.600 --> 00:26.560
welcome to the Twomo AI podcast. Thank you for having me, Sam.

00:26.560 --> 00:32.600
Hey, I'm looking forward to digging into our chat. We are going to cover a lot of ground.

00:32.600 --> 00:41.280
We'll be talking about AI in the cloud on edge devices and cars and more. But before we dig in,

00:41.280 --> 00:45.680
I would love to have you introduce yourself to our audience and share a little bit about your

00:45.680 --> 00:51.600
background and how you came to the field of AI. Thank you. You know, it's been really exciting.

00:51.600 --> 00:58.480
I have just great job where I get to work on fancy new technology that really changes the way

00:58.480 --> 01:04.560
people use their devices. And it's been quite interesting because I started with Texas instruments

01:04.560 --> 01:11.200
when they used to be kind of the lead on smartphone and the very early beginnings of what you can call

01:11.200 --> 01:15.920
an operating system and how things have changed. So being fortunate enough to kind of see that whole

01:15.920 --> 01:22.480
smartphone, you know, evolution that has happened right in front of our eyes. And then really the

01:22.480 --> 01:26.800
advantage that I've been able to do at Qualcomm is to take some of these technologies and then to

01:26.800 --> 01:32.160
bring it into even additional fields beyond the smartphone. And you know, what's been really

01:32.160 --> 01:38.640
amazing is that AI is this new technology that is really spanning across multiple product lines,

01:38.640 --> 01:45.120
across multiple technologies. And it's really bringing so many new capabilities that we at least

01:45.120 --> 01:49.280
are super excited. Our customers are very excited. Just the value that we are able to bring

01:49.280 --> 01:54.640
based on that is superb. So very excited about what he is going to enable us to do.

01:55.360 --> 02:02.880
Awesome. And I think our listeners are familiar with Qualcomm. But I'd love to have you maybe

02:02.880 --> 02:10.480
spend a few minutes just contextualizing the company's role in the AI ecosystem. And we hear about

02:10.480 --> 02:17.120
the Snapdragon thing all the time. What exactly is Snapdragon? Sure. So Snapdragon is the name

02:17.120 --> 02:22.400
of our platform that takes the best in-class technologies that we work on from, you know,

02:22.400 --> 02:28.800
camera, graphics, processors, modem, and put it all together into these products that we call

02:28.800 --> 02:34.880
Snapdragon product line. And essentially, it has best in-class capabilities across all of those

02:34.880 --> 02:41.040
vectors. And at the same time, we are relating this wave on 5G right now. And really,

02:41.040 --> 02:46.080
I always say this, but 5G and AI are technologies that are very symbiotic. They go together hand

02:46.080 --> 02:53.280
and hand. 5G makes AI better, AI makes 5G better. So really, a very key focus area for us right now

02:53.280 --> 02:57.760
are those two technologies. And they come together in the form of a Snapdragon product.

02:57.760 --> 03:04.080
We of course make the hardware, the software, all the experiences that go around it to be able to

03:04.080 --> 03:10.800
create these end products that are just amazing and throw all our consumers that use them.

03:11.760 --> 03:18.400
Can you elaborate a bit on the symbiosis between 5G and AI? I think my sense is that

03:19.200 --> 03:25.200
the carriers in some cases have kind of jumped the gun and labeled their 4G plus technologies

03:25.200 --> 03:31.680
as 5G and have kind of disillusioned a lot of us over 5G. I don't know if that's a broadly held

03:31.680 --> 03:39.280
take, but how does 5G and AI complement one another? Yeah, I think that's a great question. But,

03:39.280 --> 03:43.520
you know, you remember, this is pretty much the same thing that we saw with the advent of 4G.

03:43.520 --> 03:48.560
There was a lot. It's from not a drone. And you know, it met very different things and did it

03:48.560 --> 03:52.720
really mean 4G or not? So there is some of that going on, but I think the way you should think

03:52.720 --> 03:59.120
about 5G is millimeter wave brings in the amazing capabilities of unbelievable throughput,

03:59.120 --> 04:05.280
unbelievably low latency. And that is what really changes the game. I think that's superb.

04:05.280 --> 04:10.880
And the way AI and 5G come together is just think about it this way. We are applying AI techniques

04:10.880 --> 04:16.240
to how our modems work. Modems are basically the blocks that allow you to receive the signal

04:16.240 --> 04:21.440
from the air. And by applying AI techniques, we're actually able to make them work more efficiently

04:21.440 --> 04:29.040
to be able to green the signal from most complex of channel conditions. And that's how AI makes 5G

04:29.040 --> 04:34.640
better. But the way 5G makes AI better now is that basically with the advent of 5G, we have

04:34.640 --> 04:39.600
intelligence that's interspersed across different parts of the network. We have it on our mobile

04:39.600 --> 04:44.640
phones. We have it in our connected cameras. We have it in our devices of all sorts. But what 5G

04:44.640 --> 04:50.640
does is that it now bridges the gap between the end nodes, which might be these end products

04:50.640 --> 04:55.760
all the way to the cloud. And in doing so, what it allows you to do is even though the right

04:55.760 --> 05:02.160
place to do AI is on the device for privacy for immediacy reasons. But now with the leverage of 5G,

05:02.160 --> 05:08.080
if there is need for accessing more AI capability, you can basically extend your intelligence into

05:08.080 --> 05:12.560
the cloud, do more work over there, and be able to bring it back to the device very, very fast.

05:12.560 --> 05:18.480
So this is what we're calling kind of the advent of distributed intelligence across the whole network.

05:18.480 --> 05:27.840
And that's what is possible due to 5G. Well, let's maybe start with or continue with the device

05:27.840 --> 05:38.400
and dig into what's new and enabling developers to take full advantage of AI on mobile devices.

05:39.200 --> 05:44.800
Sure. I mean, today many people might not realize it, but there are many use cases that are

05:44.800 --> 05:51.280
already using AI on the device. So for example, the imaging experience has been completely changed,

05:51.280 --> 05:56.560
transformed because of the application of AI, right? For example, now your cameras on your devices

05:56.560 --> 06:01.520
can see into the darkest of rooms. It allows you to be able to do right. I mean, sometimes my

06:01.520 --> 06:06.000
eyes can't see and I take a picture and my phone can see through it. But that's the power of AI,

06:06.000 --> 06:11.120
right? We are basically training those models to be able to take out the image or the picture

06:11.120 --> 06:17.520
from the darkness. Very, very powerful. It's been an incredible revolution. And just how much

06:17.520 --> 06:26.080
of photography has shifted from kind of these physical characteristics like the lenses to software.

06:26.080 --> 06:31.760
And AI has been a big part of that. Absolutely. Right. The fact that you can do face-based

06:31.760 --> 06:37.040
payments, it's driven by AI because of the fact that now you can use AI to basically check

06:37.040 --> 06:41.200
for live-ness. Because if somebody tries to trick that image by putting a picture in front of you,

06:41.200 --> 06:47.120
AI can actually tell you if that's a person in real life or not, right? But the power of AI

06:47.120 --> 06:53.200
really is not just on imaging. We applied it to audio. We applied it to speech. I mean, in the past,

06:53.200 --> 06:58.000
we were able to use this to be able to do machine language translation, right? And to be able to do

06:58.000 --> 07:02.960
that on the device is a very different experience than to do it in the server or the cloud. Because

07:02.960 --> 07:07.440
now what you can do is you can actually have two people that live, you know, in far-flung places

07:07.440 --> 07:11.600
can talk to each other, but your devices are going to make it almost seamless for you to be able

07:11.600 --> 07:16.240
to converse together with each other. Very, very powerful. And then what we have done is we have

07:16.240 --> 07:22.480
applied that to echo and noise cancellation. We applied that to imaging. We applied that to video

07:22.480 --> 07:28.080
now more recently where you can literally take a video and take out the object or the subject of

07:28.080 --> 07:33.760
that video and replace it with a different person. Now, that is, of course, room for a lot of fun

07:33.760 --> 07:38.560
exercises that you might see on social networking as well, where people are saying things or doing

07:38.560 --> 07:42.880
things that they're not really doing. And that's why there's also security element that comes

07:42.880 --> 07:48.400
through. And we talked about that also, which is content authenticity initiative that we have

07:48.400 --> 07:53.440
basically engaged with to make sure that what you are seeing is truly what the camera was able to

07:53.440 --> 07:59.440
capture. But what makes this work on the device is basically best-in-class hardware. And what I

07:59.440 --> 08:04.480
mean by that is not just peak performance, you got to be able to do that AI processing at the lowest

08:04.480 --> 08:09.440
power possible. For it to be able to work in these handheld devices, it has to be able to do it

08:09.440 --> 08:15.040
very low power. Then we need to have pristine software. Software that's able to work at many

08:15.040 --> 08:20.640
different levels of the stack. So we offer what we call Snapdragon neural processing SDK at a higher

08:20.640 --> 08:26.720
level. We offer something that we call hexagon AI direct at a lower level to be able to write very

08:26.720 --> 08:31.760
close to metal to get the most out of the hardware. And then the last piece is really tools.

08:32.720 --> 08:40.000
And methodology or algorithms that make it possible for people to be able to use AI more efficiently.

08:40.000 --> 08:45.280
What I mean by that is, so we've introduced what we call AI model efficiency toolkit. And we've

08:45.280 --> 08:50.720
open sourced a lot of this. But what we do is we work with our corporate R&D team. They develop

08:50.720 --> 08:56.320
certain methodologies to quantize the networks to be able to compress the networks. And by the

08:56.320 --> 09:03.120
way, those techniques are providing so much uplift to the amount of AI processing that we can do

09:03.120 --> 09:07.760
for a given amount of power that we are able to get a lot out of it. So like for example, if you

09:07.760 --> 09:13.360
quantize a 32-bit floating point model and take it to 8-bit integer, you get 4x improvement.

09:13.360 --> 09:18.400
We have shown that by compressing and taking out the redundant parts of a network, we get

09:18.400 --> 09:23.680
3x compression. So many times you can reduce the, you know, the consumption by three times.

09:24.240 --> 09:28.400
And that we do by applying, you know, single-valued decomposition or Bayesian

09:28.400 --> 09:33.040
approach is to be able to do that. We have talked about add-around, you know, recent paper,

09:33.680 --> 09:37.200
which is adaptive rounding. So, you know, if you run to the nearest

09:37.200 --> 09:43.200
number, the accuracy is different than if you are able to do adaptive rounding. So, you know,

09:43.200 --> 09:47.200
those are the kind of technologies and techniques that we are spending a lot of time and effort

09:47.200 --> 09:53.600
on to really be able to make it work across different domains to be able to do more processing

09:53.600 --> 10:01.040
at the lowest power as well. Where did these tools fit in in the kind of the tool chain of the

10:01.040 --> 10:10.400
the mobile developer? Are they typically using the snapdragon tools directly or are they

10:11.360 --> 10:20.560
accessing them via their OS, iOS, Android, whatever the device is? And there are kind of OS

10:20.560 --> 10:26.880
layers that take advantage of the snapdragon tools. Yeah, that's a good question. So,

10:26.880 --> 10:31.760
the way it's done right now is these techniques are open-source. So, technically, anybody can apply

10:31.760 --> 10:37.760
and use them. But they are fitting, of course, very closely into the set snapdragon neural processing

10:37.760 --> 10:42.240
SDK, because that's where you're able to get a lot of the benefit out of it. It's done specific

10:42.240 --> 10:48.000
to our hardware such that we can get all that benefit out. But yeah, there isn't anything that

10:48.000 --> 10:51.200
prevents those techniques to be used by others as well.

10:51.200 --> 11:02.480
And so, digging a little bit deeper, the snapdragon architecture at the hardware level

11:03.520 --> 11:12.480
is constantly evolving. And as an Android user and a phone connoisseur, I'm always kind of waiting

11:12.480 --> 11:19.280
for the latest and greatest. And this time around this year, it's the 888, is that right?

11:19.280 --> 11:25.360
That's right. What does that do for me as an AI user? Sure. So, let me spend a little

11:25.360 --> 11:30.880
time talking about the hardware. So, we are continually updating hardware to get the most out of,

11:30.880 --> 11:36.400
you know, what we are able to do on the smartphone device. But we specifically don't snapdragon 888 is,

11:36.400 --> 11:41.280
we typically take a heterogeneous approach. So, our AI engine is a combination of what you can do

11:41.280 --> 11:45.840
on the CPU, what you can do on the graphics, and then what we call our hexagon processor. And what

11:45.840 --> 11:51.680
we're able to do with the snapdragon 888 is a combined peak performance of almost 26 trillion

11:51.680 --> 11:56.560
operations per second. That's a heck of a lot for a device that fits in the palm of your hand

11:56.560 --> 12:02.640
and can already run the whole day. So, what we have done specifically on 888 is, we typically in

12:02.640 --> 12:07.360
the hexagon processor have three different components. We basically have the scalar part,

12:07.360 --> 12:12.960
the vector part, and then the tensor component. And if you look at a typical model, a neural

12:12.960 --> 12:18.560
model neural network, Sam, what you would notice is that there is an initial part, which basically maps

12:18.560 --> 12:23.040
very well to the scalar component. And then there is a, you know, typically an end portion, which is

12:23.040 --> 12:27.840
like the fully connected layer, which kind of maps very well to the vector part. And then many of

12:27.840 --> 12:33.120
the intermediate layers map very, very well to the tensor processor. So, that's how we're able to

12:33.120 --> 12:39.840
basically put organ together. Now, in the past, some of these sub blocks, they were not all together

12:39.840 --> 12:44.480
into a fused architecture. With the 888, we actually brought them together. In one architecture,

12:44.480 --> 12:50.320
we have wrapped around a much larger shared memory. And by doing that, we are actually able to

12:50.320 --> 12:55.360
reduce power a lot. So, I'll give you an example. One of the problems in AI is that people need to

12:55.360 --> 13:00.720
move a lot of data, which is the weights and activations from where they're stored into the AI block

13:00.720 --> 13:04.720
where they're processed. Now, if you have a larger shared memory, which in this case, it's many

13:04.720 --> 13:09.520
times more than what we basically had on the previous generation part, you're able to bring a lot

13:09.520 --> 13:14.240
of power consumption savings. So, we are able to bring in almost two to three x power consumption

13:14.240 --> 13:19.600
saving in some scenarios by doing that. At the same time, as you context switch between those

13:19.600 --> 13:24.960
different sub blocks, the vector or the tensor component, you're actually able to reduce the

13:24.960 --> 13:30.640
latency significantly. In some cases, 1,000 x more. To really improve the efficiency all around

13:30.640 --> 13:35.200
from a power, from peak performance, from a latency perspective with this new architecture

13:35.200 --> 13:40.560
that we have come out with, we pretty much do like a major upgrade of AI architecture every

13:40.560 --> 13:46.080
other year with minor upgrades in the middle as well. But just because this field is evolving so

13:46.080 --> 13:50.800
fast, we study the different networks from the perspective of what we're seeing or consumers

13:50.800 --> 13:55.840
and our customers do with the device. And based on that, we're continually upgrading different

13:55.840 --> 14:03.840
components to make it run even more efficiently than before. And do these advances kind of come from

14:03.840 --> 14:13.680
the traditional ability to squeeze more transistors onto a given device size? Or are you pulling in

14:15.120 --> 14:20.080
innovations from all the research labs that you're doing, papers, things like that? What's the

14:20.080 --> 14:28.480
balance of the evolution of the product versus incorporating research concepts?

14:28.480 --> 14:33.600
Yeah, definitely. It's coming a lot from the research that our corporate R&D team does.

14:33.600 --> 14:37.840
I mean, just to give you perspective, we have more than a decade of research before we actually

14:37.840 --> 14:42.960
brought AI to a product. And our first product that actually enabled this was in 2015.

14:42.960 --> 14:48.000
So we're really continually changing the architecture because you can get a lot out of that today.

14:48.000 --> 14:52.640
I mean, what has made AI work on the devices is the fact that you can do so much processing

14:52.640 --> 14:57.120
on the device at low power, which you could not do in the past. And there is still, by the way,

14:57.120 --> 15:02.880
a lot more that we can take out with the combination of, like I said, hardware software and the tools.

15:02.880 --> 15:08.000
So definitely, we are also continually looking at new approaches. There are other methodologies

15:08.000 --> 15:11.840
and architectures that people are studying. We keep an eye on those too. But of course,

15:11.840 --> 15:16.240
we're very close to the products. We want to make sure these are techniques that we can commercialize.

15:16.240 --> 15:20.640
So there are other things that our teams are looking at more from a pure R&D perspective that you

15:20.640 --> 15:28.480
will see coming to the front-end in future years. Okay. So we've talked about in the in the setup

15:28.480 --> 15:34.720
to this, we talked about 5G enabling this kind of distributed environment between the mobile devices

15:34.720 --> 15:41.840
and the cloud. Let's maybe jump over to that cloud side of things. I had the opportunity to attend

15:41.840 --> 15:49.200
the cloud AI 100 launch a couple of years ago. This is a new infrastructure hardware that

15:49.200 --> 15:55.280
Qualcomm has been working on. What's new on that front? Yeah, I think it's actually a very

15:55.280 --> 15:59.760
exciting area. So, you know, these advantages, Sam, that I talked about from the perspective of

15:59.760 --> 16:05.600
mobile. By the way, they are as important also on the cloud side. And the reason for that is,

16:05.600 --> 16:11.280
I mean, if you look at, you know, big data center or big platforms, like let's say Facebook,

16:11.280 --> 16:15.520
you will notice that they have publicly said that they have inferences that are happening in

16:15.520 --> 16:22.080
the range of about 200 to 400 trillion inferences per day. That's a pretty massive number. And then

16:22.080 --> 16:27.600
that old trajectory is very steep. So some of the data that people are looking at is the data center

16:27.600 --> 16:33.600
power is doubling every year. So, unless and until people are able to change the architectures used

16:33.600 --> 16:38.640
in the data center, it is really start to push against a wall. So what the cloud AI 100 does is that

16:38.640 --> 16:44.160
it basically takes that advantage that we've created on the mobile side. And really, of course,

16:44.160 --> 16:50.720
in a very different setup, leverages all of our advantages, all of qual comes perigree for best

16:50.720 --> 16:56.160
in class performance for the least power and then brings to the cloud. And we're really seeing

16:56.160 --> 17:00.800
applications on, you know, on the cloud side, we're also seeing applications in what we call,

17:01.600 --> 17:05.840
you know, like smart city like applications. So you can envision, right, that you have,

17:05.840 --> 17:10.880
that's a multiple cameras installed at a junction. And then of course, you can't have as many

17:10.880 --> 17:15.760
people staring at each of those streams. So that data is actually going into a device like the cloud

17:15.760 --> 17:21.760
AI 100, where we are able to make sense of what that video stream means. And if an event has happened

17:21.760 --> 17:26.720
an event like an accident, perhaps at the intersection or something else, right. So those are kind of

17:26.720 --> 17:32.480
the scenarios that we're talking about. And we recently actually published data for ML Commons.

17:32.480 --> 17:38.320
And it actually shows that, you know, cloud AI 100 really leads for, you know, 50 watt and for even

17:38.320 --> 17:43.680
smaller configurations of power for the amount of performance we're able to get out of that platform.

17:43.680 --> 17:48.160
So very exciting, really, to be able to move into this new area. We have multiple partners

17:48.160 --> 17:53.040
that we're working with right now. And Emma comments is a benchmarking effort? That's right. So

17:53.040 --> 17:59.120
it's ML Commons is basically kind of the umbrella, I believe, for ML Perth. And they published a lot

17:59.120 --> 18:04.240
of results very recently for the cloud side of things. And very, very good story for us. I mean,

18:04.240 --> 18:08.160
if you look at something like ResNet 50, if you look at other networks, you'll see the power

18:08.160 --> 18:15.840
of the Cloud AI 100 like product that shows very well on ML Commons. And with Cloud AI 100, is it

18:16.640 --> 18:23.200
taking that core Snapdragon architecture or the architecture that underlies the Snapdragon

18:23.200 --> 18:30.480
and scaling it up to kind of the scale of a card that might go on a server in the cloud for

18:30.480 --> 18:37.600
inference? Or is it a, is it a different architecture? Yeah, I think the way we developed that

18:37.600 --> 18:43.280
IP is that we, we have a root IP where we leave a lot of configurabilities the way to think about

18:43.280 --> 18:48.800
it, right? So for example, the TCM or the memory that you have in the block can be modified.

18:48.800 --> 18:54.400
The amount of processing that you have in the tensor core can be modified or in the vector section

18:54.400 --> 18:59.680
can be modified or the precision that you are making available in those cores can be modified.

18:59.680 --> 19:03.920
The process node comes into play also here and there. So my point is there is a lot of

19:03.920 --> 19:08.800
configurability. The number of cores that you might have, right? So the root IP you can say is

19:08.800 --> 19:14.320
probably very similar, but as you make all those changes, it leads to a very different product,

19:14.320 --> 19:19.760
but it still has those fundamental benefits that we brought in at that research level. And that's

19:19.760 --> 19:24.560
why we think Cloud AI 100 really shines in terms of the performance that you're able to see. So

19:24.560 --> 19:28.480
leveraging all the benefits, but definitely a very different machine than what we have in the

19:28.480 --> 19:36.400
mobile side. And I mentioned this, but I want to make sure that I'm correct. The focus for the

19:36.400 --> 19:43.280
Cloud AI 100 is primarily or maybe even exclusively inference as opposed to training and inference.

19:43.280 --> 19:49.680
Is that correct? That is right. So it's primarily focused on inferences. It's a machine that's

19:49.680 --> 19:54.320
built ground up with the right architecture for inferencing rather than leveraging a CPU or GPU

19:54.320 --> 19:59.040
like architecture because that makes a fundamental difference in the power consumption that you can get.

20:01.040 --> 20:09.520
And you mentioned that this is primarily targeting these large platforms like

20:11.200 --> 20:18.240
Facebook. This isn't a GPU that you're going to just go and install, you know, get one on Amazon

20:18.240 --> 20:22.080
and install it in your server. Am I correct about that? You're absolutely right.

20:22.080 --> 20:27.920
But I think what people are realizing that as the inference need is, you know, really going up

20:27.920 --> 20:33.280
and ramping up, people are realizing that the CPU or GPU architecture cannot scale. It cannot scale

20:33.280 --> 20:37.680
to be able to give you the level of inferencing needed at a given amount of power. And I think

20:37.680 --> 20:42.640
that's where the advantage really comes through for the architecture that we have within the Cloud

20:42.640 --> 20:48.720
AI 100. Very, very strong story for us. And I think what we've also been able to do as we

20:48.720 --> 20:54.320
kind of talk about, I think, is take some of these processors like the Cloud AI 100 and take it

20:54.320 --> 20:59.840
also into other areas. For example, on the right platform that we have launched, which is for

20:59.840 --> 21:06.080
automotive. So it's giving us a lot of flexibility in being able to take that wherever there is a

21:06.080 --> 21:10.720
need for massive inferences at low power, this becomes the platform that we're able to use.

21:11.680 --> 21:17.200
And is it available? Are there partners in the ecosystem that offer it kind of as a service

21:17.200 --> 21:26.160
underlying cloud service that is accessible to developers? Yeah, so it is available today.

21:26.160 --> 21:30.960
We already have partners that we're working with to launch on multiple different product segments.

21:32.000 --> 21:38.160
And yeah, you will be talking about some fairly soon here, but we did announce one very recently.

21:38.160 --> 21:43.200
So this is an engagement that we had done when Gigabyte. And what it does is it actually takes

21:43.200 --> 21:49.920
not just one, but multiple Cloud AI 100s into a server rack. And by the way, we start talking about

21:49.920 --> 21:59.040
tops and that configuration. We talk about pops as in peta. So yeah, very, very strong platform,

21:59.040 --> 22:06.960
which you can scale very well. Awesome. You mentioned the ride platform. The company's been active

22:06.960 --> 22:14.400
in automotive segments for a while. Can you give us a summary of that ride platform and what you're

22:14.400 --> 22:20.080
up to in automotive? Sure, sure. I think again, from an AI perspective, you know, it's really an

22:20.080 --> 22:26.000
amazing area, which we're seeing with automotive. I think it's starting with, of course, just looking

22:26.000 --> 22:31.600
at the very low level at inside the cockpit. So driver monitoring at a very, very low level.

22:31.600 --> 22:37.200
And then going into L1, L2, like autonomy, and then it scales all the way into L4, L5. And what

22:37.200 --> 22:42.480
we're able to do is the ride platform is that we have that ability to be able to scale from say

22:42.480 --> 22:48.560
about, you know, 50 tops, then to 400 tops and all the way to like 700 or 800 tops of capability

22:48.560 --> 22:54.560
that allows you that scaling that continuum of performance. And then you have algorithms

22:54.560 --> 23:00.720
that you're able to scale. I think auto, of course, has very special needs and poses very special

23:00.720 --> 23:06.320
challenges. As you move into electric vehicles, I think power consumption is absolutely pristine

23:06.320 --> 23:11.600
over there as well. The kinds of models and networks are more perhaps focused on, of course,

23:11.600 --> 23:16.320
object detection. But now you have very different kinds of sensors. You have light hours, you have

23:16.320 --> 23:21.360
radars, you have multiple cameras, you have very high bits. So it's not about image quality,

23:21.360 --> 23:26.240
but it's more about cleaning information from that. So I think in that context as well, the

23:26.240 --> 23:30.320
architecture that I explained, which is, you know, this heterogeneous-like approach actually works

23:30.320 --> 23:35.360
extremely well. We're seeing very good traction with multiple customers. And as you know,

23:35.360 --> 23:40.160
the way we've done this is that we started our work on automotive site with

23:40.960 --> 23:46.160
infotainment. And within we've taken a step towards, you know, very simple ADAS and then, of course,

23:46.160 --> 23:50.560
we're continuing to build on that as we move into autonomy. So we feel we have all the building

23:50.560 --> 23:55.040
pieces. We think we're very good engagement with partners today that give us a very good path

23:55.040 --> 24:00.640
to growing that side also. Yeah, that was going to be my next question. You've, you know, clearly

24:00.640 --> 24:10.800
have the research capability on the pure AI side. Are you primarily going after this market with

24:10.800 --> 24:18.240
partners to develop the vehicle specific capabilities? Or are you investing in kind of domain

24:18.240 --> 24:23.920
specific research that's relevant to, you know, these auto and autonomy challenges?

24:23.920 --> 24:29.120
Yeah, I think it's a mix of both, really, because we need to do some fundamental research to be

24:29.120 --> 24:35.200
able to make sure we're creating the right hardware software tools. But now, also developing some

24:35.200 --> 24:40.400
key algorithms that are able to make sure and make use of that hardware. So we build those building

24:40.400 --> 24:44.720
blocks. And then, of course, on top of that, we work with partners who are doing a lot of research,

24:44.720 --> 24:49.360
who have a lot of data based on their current experience. And then we can build on top of that

24:49.360 --> 24:54.160
platform that we are built. But, you know, the strength that we are able to bring to this is we have

24:54.160 --> 24:59.120
that very large investment already done on the mobile side in many cases. And it's very easy for

24:59.120 --> 25:04.480
them us for us to leverage this into these new areas that we are growing in all the different domains

25:04.480 --> 25:10.000
from hardware software, but also from an algorithm's perspective, from tools perspective that I

25:10.000 --> 25:16.400
think gives us a leg up that other people don't have in this space. You mentioned this kind of path

25:16.400 --> 25:25.120
from I forget the low end of that, maybe 25 tops to our operations per second up to, you know,

25:25.120 --> 25:33.200
into the hundreds. And you also mentioned kind of infotainment to driver monitoring, to level one,

25:33.200 --> 25:41.120
to level and autonomy. Like, are those correlated? And how are those correlated? And, you know,

25:41.120 --> 25:46.560
what's the tail that's wagging the dog here? Are there impute requirements in the vehicle

25:47.200 --> 25:52.720
a limitation or is the limitation for the higher levels of autonomy? Is that, you know, currently

25:52.720 --> 26:00.080
on the algorithmic side or integration? How do you just see the challenges that are, you know,

26:00.080 --> 26:05.360
that are taking place in the vehicle platforms today? Yeah, indeed. I think it's the need for

26:05.360 --> 26:11.520
more peak performance, but it's also the need for specific kinds of algorithms. And then

26:11.520 --> 26:17.200
according to of those algorithms to the hardware that you are able to run at the highest performance

26:17.200 --> 26:20.800
level. And I think there are challenges in each of those steps. So you need to be able to

26:21.600 --> 26:25.520
optimize models very quickly. Again, tools come into play over there. You need to be able to get

26:25.520 --> 26:30.400
to that peak performance and be able to sustain it, right? Automotive is a very sustained use case.

26:30.400 --> 26:34.480
It's not a use case where you, you know, go up for a little bit and you got to come down. And then,

26:34.480 --> 26:39.440
of course, you have to have redundancy. You need to have basically a surety and you need to have

26:39.920 --> 26:46.240
multiple different layers of, you know, redundancy built into it. So yes, it's it's algorithms

26:46.240 --> 26:52.320
that have to improve with time, but it is also the enablement, which is what I mean by that is hardware,

26:52.320 --> 26:58.080
software, and then of course, the tools and everything that come along with it. But I think

26:58.080 --> 27:03.440
automotive is very exciting too, because the problems that we are solving are automotive. I think

27:03.440 --> 27:07.760
we'll find it's used in many other places. For example, what I mean by that is we're working a lot

27:07.760 --> 27:13.520
on robotics. We're working a lot on, you know, autonomous. For example, you know, not just cars,

27:13.520 --> 27:18.960
but drones and other things. We demonstrated this some time ago. And I think it's it's really

27:18.960 --> 27:24.480
quite powerful that you can then learn and leverage those things into these adjacent and new areas

27:24.480 --> 27:29.200
that will come with time as well. Let's dig into that a little bit more. I didn't realize this,

27:29.200 --> 27:34.880
but I think you had a role in the Mars chopper. Yeah, I'm quite excited about that actually. Yes,

27:34.880 --> 27:41.840
so the ingenuity chopper that you have on Mars. I mean, this is an engagement with Qualcomm and

27:41.840 --> 27:47.920
NASA. It's really amazing to have a product that we that I worked on actually quite some time ago

27:47.920 --> 27:55.280
that now sits on another planet. And actually, it's just amazing to see that. And actually,

27:55.280 --> 28:01.280
it's using the navigation. It's using, you know, virtual throughout the visual inertial

28:01.920 --> 28:07.200
adornatory. It's using, you know, object detection, many of the capabilities that we have baked

28:07.200 --> 28:13.040
into these products that are being used on that chopper. It's really surreal to some extent to be

28:13.040 --> 28:19.200
able to see, you know, human presence. And for the first time, you know, something that is like

28:19.200 --> 28:24.000
a chopper on a different planet. So we're very excited with this engagement. And to see many of the

28:24.000 --> 28:29.120
tools that we developed being used actually in this experience and, you know, you can see and you

28:29.120 --> 28:34.960
can envision that as we do a lot more on autonomous, for example, route mapping and things of that sort,

28:34.960 --> 28:39.760
you can do a lot more of this stuff in these kind of environments where you don't have direct human

28:39.760 --> 28:46.640
interaction to some extent. Just amazing. I mean, you can envision, you know, very tough environments

28:46.640 --> 28:51.760
even on Earth, where you can leverage a lot of those experiences and learnings that we have gotten

28:51.760 --> 28:58.720
from there. So very exciting. I mean, we were watching it when, you know, the, it was landing

28:58.720 --> 29:06.240
on Mars and we've been tracking pretty much closely ever since. This may be a silly question,

29:06.240 --> 29:14.080
but I'm thinking about like 5G on Mars. Is this a connected application or is this a autonomous

29:14.080 --> 29:22.240
application or the hybrid? I believe there is some link to the to the mother rover that they have

29:22.240 --> 29:28.080
over there, but to great and it can be controlled separately as well. But it would of course be other

29:28.080 --> 29:34.160
more shorter term technologies that we would be using to to have the tool. Otherwise, I'm on

29:34.160 --> 29:42.400
for those base stations on Mars. And so then, you know, we talked about cars, we talked about

29:42.400 --> 29:51.200
choppers. I mean, these are examples of kind of this broader IOT and devices landscape that

29:52.320 --> 29:58.320
enterprises are increasingly interested in. You mentioned smart cities earlier.

30:00.400 --> 30:08.960
Are these, you know, to what degree are you specializing to enable these various applications?

30:08.960 --> 30:16.640
Or is the kind of the core capability, you know, are the developers kind of getting you there

30:16.640 --> 30:21.680
to the specific applications? Sounds like you're doing a little bit of specialization and automotive.

30:21.680 --> 30:27.200
And I'm curious if that is a pattern that extends across to other areas in IOT.

30:27.760 --> 30:33.040
Yeah, I think IOT and some of the fields have usually a very varied sort of application,

30:33.040 --> 30:38.080
right? So you'll find many different applications. So in that case, our plan usually is to enable

30:38.080 --> 30:42.800
some of these things. And then our partners, you know, whether those are ISVs or customers,

30:42.800 --> 30:47.600
they're able to take it from their customizer to their specific needs. For example, very recently

30:47.600 --> 30:52.080
at the China Tech Day that we just had, I think a couple of weeks ago, we actually showed an

30:52.080 --> 30:57.520
application of the robotics platform where you actually have a robot playing ping pong. And that

30:57.520 --> 31:03.360
actually uses the AI technology that we have developed to actually train the players to be able

31:03.360 --> 31:08.960
to play very good table tennis, right? Of course, this is one example of it, but there is a lot more

31:08.960 --> 31:14.400
that's happening on IOT side in also what we call big IOT. So, you know, one example that you can

31:14.400 --> 31:18.960
think of is like smart retail. Now, here is the case where basically, you know, you would have

31:18.960 --> 31:24.240
multiple cameras again with people coming into a store like, you know, a completely autonomous store,

31:24.240 --> 31:28.960
the buy certain things. And you're automatically checked out as you walk out of it. You, of course,

31:28.960 --> 31:32.800
have multiple cameras that are tracking the devices or the products that you're buying.

31:32.800 --> 31:38.160
It's able to object detect, able, it's able to figure out the cost of that one. It's automatically

31:38.160 --> 31:42.720
able to check you out. So, it's quite interesting what we'd be able to do on many of these applications,

31:42.720 --> 31:47.600
but these are still, of course, developing. But there are, there are simpler cases of this that

31:47.600 --> 31:52.960
can be done today. Like in the self-checkout line, there might be concerns of, say, PILFrage or

31:52.960 --> 32:00.240
of errors in people, basically buying certain goods and already, you know, plans like this where you

32:00.240 --> 32:06.000
have, you know, some intelligence on the camera versus some intelligence in the cloud, you're actually

32:06.000 --> 32:12.480
able to cover it quite well. When you've referred to big out of T is the distinction you're making

32:12.480 --> 32:22.640
there, kind of multiple devices needing to collaborate and kind of this distributed AI use case

32:22.640 --> 32:30.640
or, yeah, and if so, what are some examples, some other examples of where you've really seen us

32:31.840 --> 32:38.880
kind of pushed that? I know that I say that kind of knowing that the, at the research level,

32:38.880 --> 32:44.560
you're doing a lot on federated learning and that's an evolving field and I'm really curious

32:44.560 --> 32:52.240
like how far are we in deploying solutions based on that kind of research? That's a good question.

32:52.240 --> 32:57.600
So already when I say big IAT, I mean multiple devices, kind of the way you explained it and then

32:57.600 --> 33:03.280
different levels of intelligence within those devices. So maybe you can do a course AI assessment

33:03.280 --> 33:09.040
and then based on that outcome, you're able to pass on to a central more capable AI that now does

33:09.040 --> 33:13.440
the finer level of assessment, right? So that's kind of an example of big IAT, but at the same time,

33:13.440 --> 33:18.000
I think the experience has changed a lot. Like I always did this example that, you know, I have this

33:18.000 --> 33:24.960
security camera outside and in every time a person passes by, it basically pings me. It's really

33:24.960 --> 33:28.640
quite wasteful, right? I mean, if you have a little bit more intelligence there, you would have

33:28.640 --> 33:33.600
their ability to be able to assert and if that's a car, if that's a person, and if that's a person,

33:33.600 --> 33:39.760
is that the person that is a safe person or not? Right? So it has application of what I call big IAT,

33:39.760 --> 33:44.800
but also as we pack in more AI into connected cameras and other devices, you'll see a lot more

33:44.800 --> 33:50.000
over there. And then I think then the kind of the second part of your question, sorry,

33:50.000 --> 33:59.280
second part of your question was on the related learning and the degree to which that is starting to

33:59.280 --> 34:05.200
see use and industry. Yeah, so I think by the way, federated learning is something that's already

34:05.200 --> 34:10.720
happening on the devices. So if you take the example of the Google keyboard, what you're able to do

34:10.720 --> 34:16.800
there is that it actually takes some learnings from the way you use the keyboard and then actually,

34:16.800 --> 34:22.480
it's able to apply that into in the cloud where basically aggregates a lot of those needs

34:23.280 --> 34:27.520
to get to the best in class experience. So the other people who then download the Google

34:27.520 --> 34:33.040
keyboard is are actually able to leverage and take advantage of that learning. So that continues

34:33.040 --> 34:38.720
to get better over time. We're also doing research on some limited training on the device. I think

34:38.720 --> 34:43.600
it can actually open up some very interesting use cases. Like another set of use case we haven't

34:43.600 --> 34:49.840
talked about is like always on AI. We're doing this on many devices and you know, the idea would be

34:49.840 --> 34:54.720
there is a there's a large engine that can do very complex AI, but there's also a very small

34:54.720 --> 34:59.440
engine that we've introduced on products that uses less than a milliamp of current, a milliamp

34:59.440 --> 35:05.840
year of current and it's always on, which means it can listen, it can take in multiple streams

35:05.840 --> 35:11.920
coming into a product, for example, location stream, other streams to create a contextual picture

35:11.920 --> 35:17.840
where the user is and based on that takes certain actions. So really the options are very wide

35:17.840 --> 35:23.760
in what we are able to do in always on versus active like scenarios as well. And is the idea there,

35:23.760 --> 35:30.000
the next evolution of the wake word like Alexa or Siri? Absolutely, but a lot more than that. See,

35:30.000 --> 35:35.280
that is just taking the microphone input, but now you can also take in the camera stream.

35:35.280 --> 35:38.960
You can take in the location stream, you can take in, you know, video memory,

35:39.760 --> 35:44.880
and all the sensors that you have e-composed, gyro and all, you aggregate all of that and you can

35:44.880 --> 35:50.320
now do some very interesting things already on the device. And have you seen any specific

35:50.320 --> 35:56.160
applications that jump out of you? Yeah, so like there are some very good health and security

35:56.160 --> 35:59.680
applications that already come to mind, right? If you are able to assess with certain degree of

35:59.680 --> 36:03.600
accuracy, for example, that you are the driver of the car, there are certain functions that

36:03.600 --> 36:08.560
automatically should not be available, for example. Or let's say if you know that based on the

36:08.560 --> 36:13.200
timeline that there is a loud noise in the middle of the night, for example, you know that there

36:13.200 --> 36:17.680
is an event that has happened that probably needs to be catered to, right? So there are some very

36:17.680 --> 36:22.320
interesting ideas like that that are coming up. I mean, for example, if your phone is able to

36:22.320 --> 36:27.200
determine that you're in a very noisy environment, it can automatically increase the volume of your

36:27.200 --> 36:34.080
ringer, for example. Right, many convenience factors, but also health and security aspects there.

36:34.080 --> 36:42.000
Yeah, we've talked about use cases like retail and smart cities and

36:43.760 --> 36:49.600
yeah, even the always on AI application brings the question to mind. There's a growing concern

36:49.600 --> 36:55.760
about surveillance and kind of pervasive cameras and things like that. And I'm curious what the

36:55.760 --> 37:02.000
Qualcomm take is on that. Sure. I think definitely privacy comes first, right? We need to ensure that

37:02.720 --> 37:08.000
our private data stays private. And what we are able to do in that regard is we have a very strong

37:08.000 --> 37:13.360
security components that we build into each and every one of our products, stepdragon products.

37:13.360 --> 37:17.280
And that is meant to make sure that the data that is supposed to stay on the device stays on the

37:17.280 --> 37:23.200
device. And that's why I mentioned I think there is a very good benefit of having more AI capability

37:23.200 --> 37:28.000
on the device because now we can do all of that processing on the device. The data doesn't need to go

37:28.000 --> 37:32.160
anywhere. And you're able to actually make sense of it, especially data that has like let's say

37:32.160 --> 37:39.200
your face information or anything that's very, very important from a value perspective should stay

37:39.200 --> 37:46.640
on the device in that sense. So we've talked about a bunch of different areas and you know what's

37:46.640 --> 37:53.360
available today and where things are going, you know, near term, you know, what are you most excited

37:53.360 --> 37:58.640
about kind of looking into the future and the different applications you're seeing arising?

37:59.360 --> 38:03.840
Sure. I think the part that I really get excited about, especially from an AI and its applications

38:03.840 --> 38:09.440
is I think what I call AI for good. And there's a lot of potential there. So there's one initiative

38:09.440 --> 38:14.640
that we had done with the Tata in India where basically you could take a smartphone and put a small

38:14.640 --> 38:19.840
lens behind that. And then you could basically point it at the eye of a person and you'd be able

38:19.840 --> 38:24.960
to glean information from the retina. And then you can actually run an algorithm where there might

38:24.960 --> 38:30.400
be a location where there isn't a doctor close by. And it does a very coarse but a good assessment

38:30.400 --> 38:36.800
of if there is any concern or let's say conditions like diabetic retinopathy, right? If any such

38:36.800 --> 38:41.440
condition is coming in, it can automatically go on, it can tell you to go see a doctor. I think

38:41.440 --> 38:47.120
there are many applications like that where as we start to get more AI processing on the device

38:47.120 --> 38:53.040
which is able to monitor our health, I think there's a lot that we can do there. Make sense of

38:53.040 --> 38:57.680
it just from the amount of information that's already there. If you have a smartwatch, you know,

38:57.680 --> 39:02.640
you have your heart rate, but to be able to make sense of that in combination with all the other

39:02.640 --> 39:06.320
factors that you have. Because for example, if you have a smartphone and a smartwatch,

39:06.320 --> 39:10.480
now you can a certain very well if the person was standing and he's no longer standing for example,

39:10.480 --> 39:16.160
right? There is a lot of such what I call AI for good. And I'm quite excited that it will

39:16.720 --> 39:22.240
make life better. It will make the people's life safer, right? For example, cars.

39:22.240 --> 39:26.880
As you do more this autonomy, I think autonomous cars will turn out to be a lot more safer than

39:26.880 --> 39:32.160
human drivers. And if they can save lives, if they can make the experience, you know, safer for

39:32.160 --> 39:37.040
especially teenagers, people who are just learning how to drive, I think that those are all,

39:37.040 --> 39:41.280
you know, great things that I think if you're a technologist and if you can get something like

39:41.280 --> 39:45.600
this, it's very heartening and it makes you feel good that your technology is going in a place

39:45.600 --> 39:52.160
which makes human's lives better. Awesome. Awesome. Yeah, thanks so much for joining us and sharing

39:52.160 --> 39:58.240
a little bit about what you and Qualcomm are up to. Thank you for having me. Enjoy the discussion

39:58.240 --> 40:09.600
very much. Thank you. Thank you.

