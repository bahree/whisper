WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.240
I'm your host Sam Charrington.

00:32.240 --> 00:37.000
If you missed our last show and if you did you definitely want to go check it out because

00:37.000 --> 00:39.600
it was a great conversation.

00:39.600 --> 00:44.160
But if you missed that show you missed the first of the many exciting updates we have for

00:44.160 --> 00:45.760
you this summer.

00:45.760 --> 00:50.720
Last time we announced Twimble's third birthday and our 5 millionth download which happened

00:50.720 --> 00:53.040
right around the same time.

00:53.040 --> 00:58.320
To help us celebrate this occasion and to request your commemorative Twimble birthday sticker

00:58.320 --> 01:02.800
visit TwimbleAI.com slash birthday 3.

01:02.800 --> 01:09.240
This week we're continuing the action by kicking off volume 2 of our AI platform series.

01:09.240 --> 01:14.440
You recall that last fall we brought you AI platform's volume 1 featuring conversations

01:14.440 --> 01:21.440
with platform builders from Facebook, Airbnb, LinkedIn, OpenAI, Shell and Comcast.

01:21.440 --> 01:27.640
This series turned out to be one of our most popular series of shows ever and over 1000

01:27.640 --> 01:33.040
of you downloaded our first ebook on machine learning platforms, Kubernetes for machine

01:33.040 --> 01:35.920
learning, deep learning and AI.

01:35.920 --> 01:40.880
Well we'll be back at it over the next few weeks sharing more experiences from teams

01:40.880 --> 01:46.880
working to scale and industrialize data science and machine learning at their companies.

01:46.880 --> 01:51.320
And we've got even more in store on this topic so if it's an area you're interested

01:51.320 --> 01:53.880
in be sure to stay tuned.

01:53.880 --> 02:00.760
You can follow along with the series at twimbleai.com slash AI platforms 2 and by following us on

02:00.760 --> 02:07.720
Twitter at at Sam Charrington and at Twimble AI.

02:07.720 --> 02:12.720
Before we dive in, I'd like to send a giant thanks to our friends over at Sigopt.

02:12.720 --> 02:16.960
They've been huge supporters of my work in this area and I'm really excited to have them

02:16.960 --> 02:21.840
as a sponsor of this series of shows on machine learning and AI platforms.

02:21.840 --> 02:27.640
If you don't know Sigopt, I spoke with their CEO Scott Clark back on show number 50.

02:27.640 --> 02:31.480
Their software is used by enterprise teams to standardize and scale machine learning

02:31.480 --> 02:37.680
experimentation and optimization across any combination of modeling frameworks, libraries,

02:37.680 --> 02:40.560
computing infrastructure and environment.

02:40.560 --> 02:45.680
Teams like 2 Sigma who will hear from later in this series rely on Sigopt software to

02:45.680 --> 02:50.480
realize better modeling results much faster than previously possible.

02:50.480 --> 02:55.960
Of course to fully grasp the potential of a tool like Sigopt is best to try it yourself.

02:55.960 --> 03:01.240
That's why Sigopt is offering you the twimble community an exclusive opportunity to try

03:01.240 --> 03:06.240
their product on some of your toughest modeling problems for free.

03:06.240 --> 03:14.360
To take advantage of this offer, visit twimbleai.com slash Sigopt.

03:14.360 --> 03:16.800
All right everyone, I am here with Adrian Gaden.

03:16.800 --> 03:21.000
Adrian is a machine learning lead at Toyota Research Institute.

03:21.000 --> 03:23.600
Adrian, welcome to this week in machine learning and AI.

03:23.600 --> 03:24.600
I'm super happy to be here.

03:24.600 --> 03:26.080
Thank you for inviting time.

03:26.080 --> 03:33.000
So we are here in Las Vegas at the AWS re-invent conference where you gave a talk and we

03:33.000 --> 03:38.920
will dig into the topic of your talk, which was about advancing autonomous vehicle development

03:38.920 --> 03:40.760
using distributed deep learning.

03:40.760 --> 03:44.840
But before we do that, I'd like to hear a little bit about your background.

03:44.840 --> 03:47.480
How do you get into machine learning?

03:47.480 --> 03:49.240
Yeah, absolutely.

03:49.240 --> 03:55.960
So I've been doing deep learning and machine learning for more than 10 years now.

03:55.960 --> 04:01.320
I was really interested initially in human learning, human psychology, but I also really

04:01.320 --> 04:05.000
like computers and building stuff, so machine learning and AI kind of was like a natural

04:05.000 --> 04:07.040
match made in heaven.

04:07.040 --> 04:12.760
And so I started doing a double major in computer science and math and at the same time looking

04:12.760 --> 04:14.320
into AI more.

04:14.320 --> 04:19.880
And then I did an internship at Inria in the very one on group from a Codidia Schmidt

04:19.880 --> 04:23.720
and Computer Vision, where I participated to some competitions like the ancestors of

04:23.720 --> 04:30.000
ImageNet, so Pascal VOC, Visual Object Challenges, which I won in 2008.

04:30.000 --> 04:35.040
And then I continued the PhD that was with Microsoft Research in Inria, I joined Center in

04:35.040 --> 04:40.440
Paris where I was working on video understanding, more specifically human action recognition.

04:40.440 --> 04:47.400
And after that, I joined XRC, XRC Center Europe, and you actually interviewed a friend of mine

04:47.400 --> 04:53.560
in my former boss, Nile Murray, and computer vision team there, great episode, by the way.

04:53.560 --> 04:59.040
And so I joined them as a research scientist and worked in video analysis in general, so

04:59.040 --> 05:00.920
I started that research effort there.

05:00.920 --> 05:05.360
At the same time, deep learning emerged, so that's when I really transitioned from principled

05:05.360 --> 05:09.960
convex optimization and kernel methods into the alchemy of deep learning and never looked

05:09.960 --> 05:12.080
back since.

05:12.080 --> 05:17.080
And are we saying alchemy just in celebration of the fact that Neurops is next week?

05:17.080 --> 05:20.080
Yeah, that's a good one.

05:20.080 --> 05:26.280
And yeah, and from then on, I did a lot of work on tracking and especially domain adaptation.

05:26.280 --> 05:30.320
And because we didn't have a lot of data, I had to make my own, so I started looking

05:30.320 --> 05:31.840
into simulation a lot.

05:31.840 --> 05:35.640
Game engines to generate data.

05:35.640 --> 05:39.240
And I did a couple of CUPR papers on the topic that were noticed by the industry at large

05:39.240 --> 05:43.480
nut and mistriving, which at the time was like really getting into simulation.

05:43.480 --> 05:47.880
And that's how I joined TRI, because I really dedicated to simulation and very, very large

05:47.880 --> 05:48.880
scale.

05:48.880 --> 05:50.280
These Brahms only happened at a large scale.

05:50.280 --> 05:55.400
If you have just small needs, like Robotaxi, et cetera, you can just label data, but at

05:55.400 --> 05:59.400
a very, very large scale and like Toyota's number one car maker in the world, 100 million

05:59.400 --> 06:02.720
cars on the road today, you need to think about these problems.

06:02.720 --> 06:06.840
And that's what gets me excited as a machinery in person, because it's all about generalization.

06:06.840 --> 06:12.520
And when you think about worldwide, like Japan, Australia, US everywhere, it has to work.

06:12.520 --> 06:16.400
And that's what's really cool, because you both have to invent new things in the research,

06:16.400 --> 06:18.880
but you also have to make it work.

06:18.880 --> 06:20.480
And you get to touch on all these things.

06:20.480 --> 06:21.960
So that's how I got into it.

06:21.960 --> 06:25.320
And I got really hooked into Robotax, based in general, and at the time it was driving in

06:25.320 --> 06:28.120
particular, because it's such a great application for machine learning.

06:28.120 --> 06:30.120
Okay.

06:30.120 --> 06:36.760
Before we get too deep into what you spoke about here, what's the focus that TRI in general,

06:36.760 --> 06:39.000
and then your focus there?

06:39.000 --> 06:40.000
Yeah, absolutely.

06:40.000 --> 06:45.160
So TRI was created almost like three years ago now.

06:45.160 --> 06:51.040
It's basically a separate company that was created by Toyota with $1 billion funding initially,

06:51.040 --> 06:55.240
and we got $2.8 billion more, and it's been up a new company called TRI AD Advanced

06:55.240 --> 06:57.320
Development recently.

06:57.320 --> 06:59.880
Our focus is really, like we're a Robotax company.

06:59.880 --> 07:05.440
And our focus is really about autonomous driving, home robots, and we also do some material

07:05.440 --> 07:10.040
science research for designing better batteries and things like this.

07:10.040 --> 07:12.480
But most of our efforts is really in driving.

07:12.480 --> 07:16.400
And the team I lead in machine learning is really about research for autonomous driving.

07:16.400 --> 07:21.640
We do also things for Robotax a little bit, because from our perspective, a car is a robot.

07:21.640 --> 07:25.680
As a sensory motor loop, essentially, you have perception, prediction, planning, decision

07:25.680 --> 07:29.640
action, and these feedback loops from the real world, which is what exciting is a physical

07:29.640 --> 07:31.320
system.

07:31.320 --> 07:35.520
And TRI really has a mission to improve quality of life in general.

07:35.520 --> 07:39.760
I know it sounds very Silicon Valley, but in that case, it's actually true, because we

07:39.760 --> 07:42.160
have already hundreds of millions of users.

07:42.160 --> 07:46.560
And so the goal is one is the project called Guardian, which is to make a car that can

07:46.560 --> 07:47.560
crash.

07:47.560 --> 07:50.160
So it's the ultimate driver assistance system.

07:50.160 --> 07:54.520
Another one is Schofr, which is the real autonomous car, like not the ones we're talking

07:54.520 --> 07:58.640
about today, but the long-term, the long game, which is real autonomy, like these cars

07:58.640 --> 08:02.320
that can drive themselves completely, autonomously, everywhere, all the time, which obviously is

08:02.320 --> 08:03.320
not going to happen tomorrow.

08:03.320 --> 08:05.000
We talk a lot about that one today.

08:05.000 --> 08:06.920
We talk, yeah.

08:06.920 --> 08:09.960
So this is, but that's, it depends on the product, right?

08:09.960 --> 08:13.920
What people think and hear to your eyes thinking really about the long-term thing.

08:13.920 --> 08:17.800
The cool thing is that these two, Guardian and Schofr, in terms of the machine learning

08:17.800 --> 08:20.040
side of things, they have a huge intersection.

08:20.040 --> 08:24.360
You still need a semantics segmentation, object detection, tracking, a lot of the algorithms

08:24.360 --> 08:28.200
that we're talking about in computer vision are actually completely in common, almost completely

08:28.200 --> 08:29.200
in common.

08:29.200 --> 08:33.640
So from the perspective of my research, I don't make a difference necessarily between

08:33.640 --> 08:37.680
these products, because most of the research I do is very well aligned with those purposes.

08:37.680 --> 08:43.240
And then we also do home robotics, so we have really, really good teams there, XNAS, JPL,

08:43.240 --> 08:48.360
et cetera, where they work on mobile manipulation platforms, so that to assist the elderly for

08:48.360 --> 08:50.360
home care and these kind of things.

08:50.360 --> 08:54.920
And does Toyota have products in market in these, in the home robotics space?

08:54.920 --> 09:00.160
So actually, Toyota manufactures the robot that's called the HSR, the human support robot.

09:00.160 --> 09:04.480
That was, I think, the official platform for the Robocop recently.

09:04.480 --> 09:06.760
So Toyota is really big in robotics.

09:06.760 --> 09:07.760
Robocop.

09:07.760 --> 09:08.760
I was thinking Robocop.

09:08.760 --> 09:13.760
Robocop with a U, pardon my French.

09:13.760 --> 09:14.760
No.

09:14.760 --> 09:20.000
So, yeah, so the goal is basically, how do we transform Toyota into a robotics company?

09:20.000 --> 09:21.000
Yeah.

09:21.000 --> 09:24.800
They have this amazing industrial robotics side, of course, but really, what is the future

09:24.800 --> 09:25.800
of cars?

09:25.800 --> 09:30.080
It's going to be Robocars, but it's also going to be robots beyond cars.

09:30.080 --> 09:34.240
And also how they become a software company and actually a machine learning company.

09:34.240 --> 09:36.120
That's really what's exciting.

09:36.120 --> 09:40.240
Because at this scale of a company that they want to change and see your Toyota, I was

09:40.240 --> 09:44.360
really talking about like, you know, the song that Andy Jassy is talking about is key

09:44.360 --> 09:46.000
notes, the clash song.

09:46.000 --> 09:47.000
We don't do it.

09:47.000 --> 09:48.000
It's not good.

09:48.000 --> 09:50.000
We do it.

09:50.000 --> 09:51.000
We have to do it, right?

09:51.000 --> 09:52.000
Right.

09:52.000 --> 09:53.520
So that's what's really exciting.

09:53.520 --> 09:58.680
Tell me a little bit about the key message of your talk here at MianBet.

09:58.680 --> 09:59.680
Yeah.

09:59.680 --> 10:05.400
So here, what we wanted to talk about was how we, you can do a distributed deep learning

10:05.400 --> 10:10.880
infrastructure in the cloud that actually scales really well and is highly performance.

10:10.880 --> 10:16.680
So when we started this thing, when I took over the team a bit more than a year and a half

10:16.680 --> 10:21.240
ago, I, like, Jerry, we're really well funded, as I mentioned.

10:21.240 --> 10:24.640
So I had dollar signs in my eyes and I was like, all right, I'm going to buy so many GPUs.

10:24.640 --> 10:25.640
I'm going to splurge.

10:25.640 --> 10:28.560
I'm going to, and we had a server room.

10:28.560 --> 10:29.720
We had everything there.

10:29.720 --> 10:33.400
And, and then actually was still, even if you have the money, even if you have the

10:33.400 --> 10:37.520
means that your disposal, it's still fairly slow to ramp up.

10:37.520 --> 10:40.840
And we had Mike Garrison, which was doing a talk with me, which is our lead of infrastructure

10:40.840 --> 10:43.400
engineering, was telling me, hey, what about Amazon?

10:43.400 --> 10:48.840
I was like, they have K80s, you know, they have old GPUs, it's slow, et cetera, but keeping

10:48.840 --> 10:52.200
an open mind, we tried a couple of things.

10:52.200 --> 10:56.160
And we got in touch with the AWS folks and, and we did a lot of infrastructure work to

10:56.160 --> 11:01.480
really, like, make it work, first single node, then multi nodes.

11:01.480 --> 11:05.840
And using PyTorch, we're a PyTorch shop, we used to be in anything shop, and then a

11:05.840 --> 11:06.840
tens of those shop.

11:06.840 --> 11:11.800
And we really like switched to PyTorch full time a year ago or something.

11:11.800 --> 11:17.000
And, and the talk was really about this kind of journey through which we went from like,

11:17.000 --> 11:23.200
yeah, you have a on-prem compute and you can do stuff to really, really large scale distributed

11:23.200 --> 11:25.200
deep learning in the cloud that's efficient.

11:25.200 --> 11:28.120
And efficiencies is really the key here.

11:28.120 --> 11:31.720
And driving in particular, there's one thing that is very different from, let's say,

11:31.720 --> 11:36.920
normal machine learning that you would see at NIPs or CPR, which is we care about small

11:36.920 --> 11:39.080
networks that operate at a high resolution.

11:39.080 --> 11:41.120
And there's two reasons for that.

11:41.120 --> 11:44.480
One is that they need to be small because even if you can compress them, quantize them,

11:44.480 --> 11:47.640
and all these kind of things that we know, we can make them more efficient.

11:47.640 --> 11:53.840
Still, you need a smaller model initially to fit in like computational budget that we

11:53.840 --> 11:57.880
have in the car because safety critical, so you have to have like, really efficient models.

11:57.880 --> 12:02.000
The second thing is you need very high resolution because time is equal space, so I was talking

12:02.000 --> 12:05.960
in my talk about like these weird equations, talking about lean deep learning, so you want

12:05.960 --> 12:07.800
like faster and around time and these kind of stuff.

12:07.800 --> 12:12.360
We want to create some kind of Toyota production system of deep learning and stuff.

12:12.360 --> 12:16.520
So that we can iterate really quickly from idea to model, to validation, and go back

12:16.520 --> 12:19.400
to the drawing board because it's research.

12:19.400 --> 12:25.120
And this idea of very high resolution is part of one of our constraints that we have to

12:25.120 --> 12:29.040
deal with because we want to predict things from far.

12:29.040 --> 12:34.000
And so seeing far is like when you read the California Handbook of Drivers, it tells you

12:34.000 --> 12:37.720
you have to look far in the distance to look far into the future.

12:37.720 --> 12:40.680
And so resolution is kind of a key thing.

12:40.680 --> 12:42.960
It's actually talking literally camera resolution.

12:42.960 --> 12:43.960
Camera resolution.

12:43.960 --> 12:44.960
Camera resolution.

12:44.960 --> 12:48.360
And specifically for the computer vision models that we're using.

12:48.360 --> 12:51.680
And so that means that the compute workload is kind of different because you have small

12:51.680 --> 12:57.600
models and very high resolution. So in terms of data flow operations, time you spend in

12:57.600 --> 13:01.080
this metrics multiplies and all these kind of things, it's very different.

13:01.080 --> 13:03.880
So we had can't down sample or crop everything to 224.

13:03.880 --> 13:04.880
Not.

13:04.880 --> 13:05.880
No.

13:05.880 --> 13:09.720
Or site far resolution doesn't cut it for us, sadly, no, it doesn't.

13:09.720 --> 13:13.360
And so we had to, if you use the standard tools like the data parallel or distributed data

13:13.360 --> 13:16.920
parallel from PyTorch, which are amazing at the image net and these kind of stuff, they

13:16.920 --> 13:18.000
didn't scale for us.

13:18.000 --> 13:21.240
And so we had to rewrite a couple of things and that's what we talked about.

13:21.240 --> 13:22.240
Okay.

13:22.240 --> 13:23.640
So let's walk through that journey.

13:23.640 --> 13:31.440
So you mentioned that one of the first steps was you kind of had to build up the infrastructure

13:31.440 --> 13:35.440
like at a node level from scratch, was that where it started or was there where there's

13:35.440 --> 13:36.440
steps before?

13:36.440 --> 13:37.440
Yeah.

13:37.440 --> 13:38.440
No, no.

13:38.440 --> 13:39.440
So we started.

13:39.440 --> 13:42.320
So that's the cool thing about TRI is that we're fairly young and we're small.

13:42.320 --> 13:47.560
And so there is no, no technical debt because there's nothing when I started, right?

13:47.560 --> 13:52.200
And that was super cool because I'm, as a research scientist, I was mostly, you know, use

13:52.200 --> 13:53.200
this, use that.

13:53.200 --> 13:54.200
All right.

13:54.200 --> 13:55.200
It's there.

13:55.200 --> 13:56.200
You know, use Slurm because it's this way.

13:56.200 --> 13:57.200
I use that file system.

13:57.200 --> 13:58.200
It's there.

13:58.200 --> 13:59.200
Okay.

13:59.200 --> 14:00.400
And here was really just sky's the limit.

14:00.400 --> 14:01.400
What you should do.

14:01.400 --> 14:06.480
And so we really got the opportunity to use the best partner with the best so we worked

14:06.480 --> 14:08.720
directly with a lot of different partners.

14:08.720 --> 14:12.360
And then we really created the thing from scratch and first single node because it was

14:12.360 --> 14:15.480
really easy and ended all kinds of tricks.

14:15.480 --> 14:18.680
Now you have some machines that are monster machines, you know, 700 gigs of RAM.

14:18.680 --> 14:21.280
And so you can scale quite well, but up to a point.

14:21.280 --> 14:24.800
And so that's when we started to switch to using distributed file systems.

14:24.800 --> 14:27.600
So we did a BGFS base, distributed file system.

14:27.600 --> 14:32.640
Before we leave that initial node, I thought I heard you say earlier that it was difficult

14:32.640 --> 14:36.160
and you had to go through a lot of steps to get on that first, they get that first

14:36.160 --> 14:37.160
node up and running.

14:37.160 --> 14:41.320
But you just said it was really easy, assuming that means relative to a full distributed

14:41.320 --> 14:42.320
kind of.

14:42.320 --> 14:50.040
I'm kind of curious about the, you know, the pain points that you had to go through just

14:50.040 --> 14:51.040
to get this up and running.

14:51.040 --> 14:56.880
And also the extent to which there's still pain points are there other things that have

14:56.880 --> 14:58.520
kind of wiped that all the way.

14:58.520 --> 15:02.160
So yeah, okay, the first one is this base, right?

15:02.160 --> 15:06.640
So the first one is because of the scale of the data we have, you cannot.

15:06.640 --> 15:11.080
So for a lot of like, that's a debug experience or research experiments on small data sets,

15:11.080 --> 15:14.280
you can fit them on the RAM and you should do that because that's just like the best

15:14.280 --> 15:15.440
thing for your bug.

15:15.440 --> 15:20.720
But when you have a large data, a large data sets, then that becomes much more complicated.

15:20.720 --> 15:28.080
And so we, we first switch from the RAM disks to EBS volumes or more EFS or we tried everything.

15:28.080 --> 15:32.840
But for like, these kind of like, this high resolution, small networks to not be network

15:32.840 --> 15:33.840
bound, right?

15:33.840 --> 15:38.080
To not have this GPU starvation problem where your average utilization of the GPU is like

15:38.080 --> 15:41.120
15% or something ridiculous.

15:41.120 --> 15:42.120
And these machines are expensive.

15:42.120 --> 15:46.480
So you want to bump that to 90% or above.

15:46.480 --> 15:51.120
That's what we had to actually, even before we started really doing distributed computations,

15:51.120 --> 15:56.360
using a distributed file system enabled us to really download the data once and not every

15:56.360 --> 15:59.960
time you set up a machine because if you auto provision machines and you have to download

15:59.960 --> 16:04.000
data from a three, every time you start a machine, then you're saying like, oh, I have

16:04.000 --> 16:05.000
this idea.

16:05.000 --> 16:08.000
That's way two hours before I can just like press play, right?

16:08.000 --> 16:12.120
So that was a big pain points for research to have this faster and around time.

16:12.120 --> 16:16.680
So the distributed file system was something that was very useful at a single node level

16:16.680 --> 16:18.360
and of course, scaled to the multi node.

16:18.360 --> 16:21.280
So we did it two birds with one stone.

16:21.280 --> 16:23.080
So where did you end up with that?

16:23.080 --> 16:26.120
We used the BGFS as file system.

16:26.120 --> 16:30.520
And we're going to look at Luster, like these announcements that were made recently.

16:30.520 --> 16:31.840
That's very interesting.

16:31.840 --> 16:36.680
Another pain point that we had was the BGFS you're managing yourself.

16:36.680 --> 16:39.920
We just deploying it on the node in your amy or whatever.

16:39.920 --> 16:40.920
Yeah.

16:40.920 --> 16:44.280
So we have like, instead of instances that serve that file system that is then mounted

16:44.280 --> 16:45.760
on these instances.

16:45.760 --> 16:49.040
And we have some infrastructure as code to just like spin this off, like all configured

16:49.040 --> 16:50.560
and ready.

16:50.560 --> 16:52.120
There's something around containers.

16:52.120 --> 16:56.840
So we were baking stuff a lot into the amy, into the machines themselves that were when

16:56.840 --> 17:01.320
they started, you're just there directly because not everybody was familiar with a Docker.

17:01.320 --> 17:05.400
But we picked up Docker too because there's obvious reproducibility benefits.

17:05.400 --> 17:09.440
And when you hack a lot of things quickly at the beginning of a research project, having

17:09.440 --> 17:16.000
this kind of Docker file where people can reproduce your environment and not just, you know,

17:16.000 --> 17:17.000
your experiments.

17:17.000 --> 17:20.280
That's actually extremely helpful for collaboration in the team.

17:20.280 --> 17:23.040
So we used to tie us back to that agility and being able to move quickly.

17:23.040 --> 17:24.040
Exactly.

17:24.040 --> 17:25.040
Close to booting up a whole machine.

17:25.040 --> 17:26.040
Yes.

17:26.040 --> 17:27.040
Yes.

17:27.040 --> 17:28.880
And our IT folks were so happy because it's not like this.

17:28.880 --> 17:29.880
This doesn't work.

17:29.880 --> 17:30.880
Yeah.

17:30.880 --> 17:34.240
But because you happy to get installed something that wrecked the system and that's of course.

17:34.240 --> 17:35.240
So DevOps.

17:35.240 --> 17:39.840
Tracing DevOps, even for researchers actually was quite powerful because you can only do

17:39.840 --> 17:44.440
the research that, you know, the mastery of the tools is really important to empower you

17:44.440 --> 17:48.480
to do research beyond, you know, just pipe Jupyter notebook, let's say.

17:48.480 --> 17:49.480
It's an awesome tool.

17:49.480 --> 17:52.080
But if you want to go beyond, you need to master other tools.

17:52.080 --> 17:54.280
And that's what we've been doing.

17:54.280 --> 17:59.080
It's a journey through engineering, craftsmanship as much as deep learning research.

17:59.080 --> 18:05.400
Is the, you know, when you talk about kind of applying DevOps in this world to what degree

18:05.400 --> 18:11.160
in your experience does it apply directly or are there, you know, gaps or it only takes

18:11.160 --> 18:12.160
you so far.

18:12.160 --> 18:15.480
You have to modify the way you think about it.

18:15.480 --> 18:19.120
And I realized that I'm saying that as if DevOps is this well-defined thing.

18:19.120 --> 18:20.120
Yeah.

18:20.120 --> 18:22.360
But I think it's a good question.

18:22.360 --> 18:27.320
I think there's like two ways to, like let's say there's two extremes, right?

18:27.320 --> 18:29.680
There's the extreme of you do everything yourself.

18:29.680 --> 18:33.600
And there's the extreme of you just use blindly something that someone does for you.

18:33.600 --> 18:37.800
And in that space of, you know, all the grad students in the world in machine learning,

18:37.800 --> 18:41.440
they spend considerable amount of time configuring their environment.

18:41.440 --> 18:44.400
That's a skill we developed during our PhDs.

18:44.400 --> 18:50.040
And Docker and these kind of things, if you don't become an IT guy or DevOps guy, but just

18:50.040 --> 18:52.200
learn from the best there.

18:52.200 --> 18:56.520
And they do some of the things that around security and that's really important for

18:56.520 --> 18:58.840
data that we have that I don't know.

18:58.840 --> 19:03.280
I don't have an inkling, but they expose us to AWS services, they expose us to some

19:03.280 --> 19:04.280
Docker stuff.

19:04.280 --> 19:05.280
So I'm not an AWS expert.

19:05.280 --> 19:06.280
I'm not Docker expert.

19:06.280 --> 19:07.440
I'm not a Kubernetes expert.

19:07.440 --> 19:12.440
But knowing a little bit of that enables empowers you to try more bold research ideas and

19:12.440 --> 19:13.760
actually debug.

19:13.760 --> 19:17.640
And when you care about the performance of your model, not just in terms of its accuracy,

19:17.640 --> 19:24.360
but its speed, having these knowledge enables you to do research much faster actually, which

19:24.360 --> 19:27.000
is counterintuitive a little bit.

19:27.000 --> 19:30.600
But again, when you're beyond MNIST, that's what it takes.

19:30.600 --> 19:31.600
Right, right.

19:31.600 --> 19:37.640
You started out doing a lot of this yourself, yourself, meaning like within, you know, as

19:37.640 --> 19:42.840
research, a community of research scientists, it sounds like you're presenting with an infrastructure

19:42.840 --> 19:43.840
person.

19:43.840 --> 19:46.640
So now you've got kind of, you know, professional support.

19:46.640 --> 19:48.840
Yeah, we do, we do work really tightly with them.

19:48.840 --> 19:52.280
I also, my team is like probably like 30% engineers.

19:52.280 --> 19:53.280
Okay.

19:53.280 --> 19:58.200
And it's really, I think it's really good for research teams to have this mix of really

19:58.200 --> 20:01.200
scientists and engineers.

20:01.200 --> 20:05.560
And because again, as I said, the lines are blurred at large scale research, and you need

20:05.560 --> 20:06.560
these two skills.

20:06.560 --> 20:09.800
And obviously, also like all the DevOps and infrastructure engineering teams.

20:09.800 --> 20:12.400
So the collaborative spirit of terror is really, really good.

20:12.400 --> 20:14.960
Like because we're small, we're very tightly in it.

20:14.960 --> 20:18.760
And because there was no technical debt, we're building everything together.

20:18.760 --> 20:23.560
And really nothing that the infrastructure engineering built was done in isolation without

20:23.560 --> 20:24.560
consulting us.

20:24.560 --> 20:28.680
So that's why we have a system that works really smoothly because all the concerns were

20:28.680 --> 20:32.720
shared and addressed at the same time from all the pieces of the puzzle.

20:32.720 --> 20:39.080
So it's really nice to have that like kick-ass modern infrastructure built around, around

20:39.080 --> 20:40.960
you somehow and with you.

20:40.960 --> 20:41.960
Yeah.

20:41.960 --> 20:47.080
And so did that infrastructure engineering team and support?

20:47.080 --> 20:53.800
Was that always there or did that come at a certain point after you'd built some things?

20:53.800 --> 20:54.800
Yeah.

20:54.800 --> 20:55.800
It's a fairly recent addition.

20:55.800 --> 20:56.800
Okay.

20:56.800 --> 20:59.640
So it started kind of organically and then you had some people that were there.

20:59.640 --> 21:04.720
And it started to be formalized only recently as we scaled up and where that need became

21:04.720 --> 21:05.720
much more obvious.

21:05.720 --> 21:07.200
So yeah.

21:07.200 --> 21:15.080
And is that infrastructure team primarily responsible for like where's kind of the line

21:15.080 --> 21:16.960
that they how far up the stack did they go?

21:16.960 --> 21:23.360
Are they worrying about like tools and frameworks and software platforms or is it primarily infrastructure

21:23.360 --> 21:29.400
and network and disk and file systems and connections to the cloud and all of that stuff?

21:29.400 --> 21:31.440
So I would say the latter.

21:31.440 --> 21:34.640
So I think the lines are blurry.

21:34.640 --> 21:39.240
But you need this single responsibility principle that applies well for software.

21:39.240 --> 21:40.720
It also applies for organization.

21:40.720 --> 21:48.640
There's this conways law that says that software organization writes software that is architected

21:48.640 --> 21:51.040
in a way that reflects the organization.

21:51.040 --> 21:55.920
And so I think it's really good if you have like clear responsibilities but also the lines

21:55.920 --> 22:00.240
are a bit blurred because that means that you get a system that is flexible.

22:00.240 --> 22:02.120
But you need these kind of responsibilities too.

22:02.120 --> 22:03.680
So there's some separation.

22:03.680 --> 22:06.720
And in my team in machine learning research and we are the ones that made the decision

22:06.720 --> 22:11.120
to switch to PyTorch for instance and the way we did that is that for instance I re-implemented

22:11.120 --> 22:16.880
YOLO myself a year and a half ago in all the different deep learning frameworks.

22:16.880 --> 22:20.640
And it was after doing that like object detection is really nice because it's a structured

22:20.640 --> 22:23.800
prediction problem that's shoehorned into a classification one.

22:23.800 --> 22:29.280
And so it breaks the APIs that most frameworks support like from the get go.

22:29.280 --> 22:35.000
And so if you use that you know you're stretching a little bit the capabilities of the network

22:35.000 --> 22:38.760
in terms of the framework in terms of their APIs.

22:38.760 --> 22:44.360
And so re-implementing YOLO in all these different frameworks made it clear that as a research

22:44.360 --> 22:47.520
scientist I value flexibility and PyTorch had the flexibility.

22:47.520 --> 22:48.760
Chainer is also very good.

22:48.760 --> 22:51.600
There is other alternatives but debugging and extra.

22:51.600 --> 22:55.720
So at certain levels like that's why I said like research scientists were making engineering

22:55.720 --> 23:01.520
decisions because choosing PyTorch is something that we wanted to make as a research scientist

23:01.520 --> 23:03.160
group.

23:03.160 --> 23:06.560
And for the reason of also the particular research we're doing.

23:06.560 --> 23:11.400
So for instance one of the things we're doing is with the paper recently called SuperDepth

23:11.400 --> 23:16.920
which is a paper about predicting the depth of a scene from a single image.

23:16.920 --> 23:23.440
And so we self-supervised method where is geometry a supervision instead of using labels

23:23.440 --> 23:26.200
because for that you can't label.

23:26.200 --> 23:30.600
And this is again another example where you super-resolution so this idea of high resolution

23:30.600 --> 23:33.160
is actually important also for accuracy.

23:33.160 --> 23:36.880
If you super-resolve the images this helps you predict better depth maps.

23:36.880 --> 23:40.040
It was one of the key findings that we made in the paper.

23:40.040 --> 23:43.760
And so all that is also enabled because of the choices we made on the software sites

23:43.760 --> 23:45.920
and PyTorch and all these kind of things.

23:45.920 --> 23:48.800
And also around the community that there's around it so that enables us to really move

23:48.800 --> 23:51.560
fast and set on the shoulder of giants.

23:51.560 --> 23:57.320
So I talked to different organizations that have differing opinions on how opinionated

23:57.320 --> 23:59.440
to be for their organizations.

23:59.440 --> 24:08.560
It sounds like you're of the mind to kind of stand it as in this case on PyTorch at TRI

24:08.560 --> 24:12.440
as opposed to other places.

24:12.440 --> 24:17.920
We're going to build a framework of platform and it's going to be able to support whatever

24:17.920 --> 24:22.320
the research scientists or engineer wants to use.

24:22.320 --> 24:25.720
Talk me through a little bit of the way you think about that.

24:25.720 --> 24:26.720
Yeah.

24:26.720 --> 24:28.720
I think about it in almost mathematical terms.

24:28.720 --> 24:32.920
That's the bias of our interest trade-off.

24:32.920 --> 24:40.200
If you have small bias and if you have high variance and you're really favoring exploration

24:40.200 --> 24:44.480
for these kinds of stuff, you need a lot of people that are willing to support you.

24:44.480 --> 24:49.280
So if you say, oh yeah, Slurm and Kubernetes and PyTorch and TensorFlow and everything and

24:49.280 --> 24:53.840
the little framework that that random guy made on his own free time.

24:53.840 --> 24:58.720
So first of all, what is actually your business?

24:58.720 --> 25:02.360
Is it making those that infrastructure and no, for us, it's not for us.

25:02.360 --> 25:04.360
It's making awesome robots, awesome machine learning.

25:04.360 --> 25:11.960
So I clearly err more in the bias area, but it's just a little bit of map-produced exploration

25:11.960 --> 25:12.960
and exploitation trade-off.

25:12.960 --> 25:17.080
When you first have high variance and for a little while, you go wild, you explore and

25:17.080 --> 25:18.560
you're maybe not bound by...

25:18.560 --> 25:20.560
You implement yellow and every framework?

25:20.560 --> 25:21.560
Exactly.

25:21.560 --> 25:22.560
Something like this.

25:22.560 --> 25:26.680
But then, at some point, you need to make a decision, because that's not sustainable.

25:26.680 --> 25:29.360
And you want to move fast in a clearly identified direction.

25:29.360 --> 25:33.280
Once you have identified that direction and you never have enough data to prove that

25:33.280 --> 25:34.280
you're right.

25:34.280 --> 25:38.000
So at some point, you have to have expressed leadership and just go with it.

25:38.000 --> 25:39.000
And then you go for it.

25:39.000 --> 25:42.280
And of course, you keep an open mind because then there's the next phase of exploration

25:42.280 --> 25:47.520
because you're right for only a short amount of time in this field of deep learning.

25:47.520 --> 25:52.760
So we take a diversion on kind of the path that you laid out in the present.

25:52.760 --> 25:53.760
Oh, yeah.

25:53.760 --> 25:55.200
We take a turn at step one.

25:55.200 --> 25:58.960
We got beautifully sidetracked, but in a wonderful direction.

25:58.960 --> 26:05.120
So yeah, so we were single nodes, everything in the RAM, and then moved to try the existing

26:05.120 --> 26:08.880
storage solutions, then moved to more distributed file system.

26:08.880 --> 26:13.840
And once we had this, because it's an in-memory distributed file system, we didn't have GPU

26:13.840 --> 26:14.840
starvation anymore.

26:14.840 --> 26:18.480
But then our training was slow because we were limited to a single machine.

26:18.480 --> 26:23.520
And then, Peter instances happened, so we started to use the 100 GPUs much better that

26:23.520 --> 26:26.760
required also tuning the storage again to avoid GPU starvation.

26:26.760 --> 26:30.080
And then we again, augmented to go into multi-node.

26:30.080 --> 26:34.400
And with the distributed file system, that at least the data was easily accessible from

26:34.400 --> 26:36.200
all the different nodes.

26:36.200 --> 26:39.960
And then that's when we started to hit the limitations of distributed PyTorch, which

26:39.960 --> 26:41.840
was very recent at the time.

26:41.840 --> 26:47.880
Before we jump to distributed, I'm curious about the, you know, you've got some, I guess,

26:47.880 --> 26:52.240
quote unquote hyperparameters like virtual CPUs, or you know, the machine configuration

26:52.240 --> 26:53.240
parameters.

26:53.240 --> 26:56.600
Like, you know, they're kind of universal rules of thumb for that kind of thing that

26:56.600 --> 26:58.840
you figured out, or do you experiment with it a lot?

26:58.840 --> 26:59.840
Is it job-dependent?

26:59.840 --> 27:00.840
A lot?

27:00.840 --> 27:04.520
Are you overly focused on economic optimization?

27:04.520 --> 27:05.800
Like how do you work through all that stuff?

27:05.800 --> 27:07.200
So we optimized for time.

27:07.200 --> 27:09.160
We don't optimize for cost yet.

27:09.160 --> 27:10.160
That one was easy.

27:10.160 --> 27:11.160
Yeah.

27:11.160 --> 27:12.800
That was easy.

27:12.800 --> 27:13.800
We haven't.

27:13.800 --> 27:16.360
So that's more again, the job of the infrastructure engineering people.

27:16.360 --> 27:20.120
So does that mean you just get the biggest one with the best GPU and?

27:20.120 --> 27:21.120
You got it.

27:21.120 --> 27:22.120
Exactly.

27:22.120 --> 27:23.120
That's exactly it.

27:23.120 --> 27:27.160
And also because our workloads, it was obvious that that was the only thing to do.

27:27.160 --> 27:28.720
So go big or go home.

27:28.720 --> 27:29.720
That's basically what we did.

27:29.720 --> 27:30.720
Yeah.

27:30.720 --> 27:31.720
Yeah.

27:31.720 --> 27:33.960
So for a single machine, we just like try to scale as much as possible on a single machine.

27:33.960 --> 27:38.880
And that meant these big, big instances, we're psyched to use soon the new ones that

27:38.880 --> 27:39.880
were announced or even bigger.

27:39.880 --> 27:43.600
So actually, that's feedback that we directly gave AWS.

27:43.600 --> 27:46.400
It's quite cool to see that we give them feedback a year ago.

27:46.400 --> 27:49.040
And then, like, keynotes was, oh, and we heard you.

27:49.040 --> 27:50.040
We did this.

27:50.040 --> 27:51.040
Yeah.

27:51.040 --> 27:54.440
And so the biggest instances that they made, that's something that we had asked for and

27:54.440 --> 27:55.600
a couple of other cool stuff.

27:55.600 --> 27:56.600
So.

27:56.600 --> 27:58.480
But you're still limited on a single machine.

27:58.480 --> 28:03.040
And so when you were kind of topping out at a single machine, how long were your jobs

28:03.040 --> 28:04.880
running for?

28:04.880 --> 28:09.280
So at this stage, it was more in the order of weeks.

28:09.280 --> 28:11.000
But that's what kind of job is this?

28:11.000 --> 28:15.400
So the main one in terms of like computational, the most computational expensive one is

28:15.400 --> 28:16.880
semantics segmentation.

28:16.880 --> 28:17.880
Okay.

28:17.880 --> 28:19.400
Because again, it's like high resolution.

28:19.400 --> 28:20.400
It's very dense.

28:20.400 --> 28:22.080
It's dense prediction.

28:22.080 --> 28:25.520
And so that was the most computational expensive job.

28:25.520 --> 28:32.120
Another type of job that we do that is also very expensive is imitation learning.

28:32.120 --> 28:34.680
So we do a lot of research on end-to-end driving.

28:34.680 --> 28:38.000
The main reason is not so much that we believe that it's all you need to driving, obviously

28:38.000 --> 28:39.000
not.

28:39.000 --> 28:41.920
But we get a lot of data from actual cars.

28:41.920 --> 28:43.320
And so we get a lot of demonstrations.

28:43.320 --> 28:46.280
And so there's this really interesting research question that we're working on, which

28:46.280 --> 28:49.440
is how much value can you derive from these demonstrations?

28:49.440 --> 28:54.320
This is a form of supervision on driving that you want to distill down into your models.

28:54.320 --> 28:55.800
And so we do a lot of research there.

28:55.800 --> 29:00.040
And that's, you know, use all the data is really the question that animates us.

29:00.040 --> 29:01.440
How can we use all the data?

29:01.440 --> 29:04.520
And because we can't label everything, we're not going to active learning routes and

29:04.520 --> 29:07.480
the same thing that everybody else is doing because obviously we're doing that.

29:07.480 --> 29:09.280
And that's not the open research challenge.

29:09.280 --> 29:12.560
Everybody knows active learning is a good thing to do when you labels things.

29:12.560 --> 29:14.520
We're really interested in self-supervised learning.

29:14.520 --> 29:17.520
How can we really use all the data by leveraging geometry?

29:17.520 --> 29:18.520
Right.

29:18.520 --> 29:21.000
For instance, how do we use demonstrations at scale?

29:21.000 --> 29:25.440
And so those are the workflows because motivated by the research direction we're going

29:25.440 --> 29:28.160
in, those were the most intensive ones.

29:28.160 --> 29:32.600
And a single machine, these are things that easily take weeks.

29:32.600 --> 29:33.600
Okay.

29:33.600 --> 29:37.680
So then that necessitated jumping over to distributed training?

29:37.680 --> 29:38.680
Yes, absolutely.

29:38.680 --> 29:46.480
Did you do that after the decision to go with PyTorch or did you have to figure that out

29:46.480 --> 29:47.480
twice?

29:47.480 --> 29:52.640
No, we had made so because also we have a lot of like we're in Silicon Valley.

29:52.640 --> 29:57.560
So it's really nice that there's a lot of dense communication between people are not

29:57.560 --> 29:59.880
afraid to share their plans or are going.

29:59.880 --> 30:03.160
So we know to some extent where things were going.

30:03.160 --> 30:04.320
And we know where we wanted to go.

30:04.320 --> 30:07.440
So we also were open about this with different partners.

30:07.440 --> 30:13.720
And so we knew that when we were going to hit the distributed wall, we would be ready

30:13.720 --> 30:14.720
for it.

30:14.720 --> 30:21.000
So we had all those factors were factored in at the decision time at the first one.

30:21.000 --> 30:22.640
So we didn't have to revisit it later.

30:22.640 --> 30:23.640
Okay.

30:23.640 --> 30:24.640
Thankfully.

30:24.640 --> 30:31.480
But you did have to, it sounds like weighed on some PyTorch features to support doing distributed

30:31.480 --> 30:32.480
the way you wanted.

30:32.480 --> 30:33.480
Absolutely.

30:33.480 --> 30:36.280
So initially we were starting to be a little bit afraid that we would have to either

30:36.280 --> 30:41.480
fork or do some like really big upstream contribution to PyTorch too.

30:41.480 --> 30:45.920
And as again, as I was mentioning, it's kind of like a niche application from the deep learning

30:45.920 --> 30:46.920
era.

30:46.920 --> 30:49.800
Like a high resolution semantic segmentation, for instance, it's not something that a

30:49.800 --> 30:51.840
lot of people are pursuing.

30:51.840 --> 30:59.840
So we were starting to wonder if there was another way than to hit like low in the stack.

30:59.840 --> 31:05.240
And we did like fairly intense debugging performance profiling, which is not easy in the cloud because

31:05.240 --> 31:07.920
everything is like in the ether.

31:07.920 --> 31:12.080
And what we found actually, and that's kind of like was an interesting end of the debugging

31:12.080 --> 31:15.920
journey for the performance optimization, was that in the distributed setting when we

31:15.920 --> 31:22.520
had many machines and a very efficient distributed file system, our epochs, right, our passes

31:22.520 --> 31:28.400
over the entire training data became really fast because we had this huge batch sizes.

31:28.400 --> 31:33.160
And everything was flowing really well to the GPUs, GPUs were crunching really quickly.

31:33.160 --> 31:38.520
And what happened is that there was like huge down times, like wait, like there was a bottleneck

31:38.520 --> 31:39.520
somewhere.

31:39.520 --> 31:44.440
And it turns out that bottleneck, which was hard to find, was in the data loaders when

31:44.440 --> 31:50.960
you do, you know, you how you do multiple workers that pre-fetch the data for you in parallel

31:50.960 --> 31:54.680
to feed the GPUs, like the super hungry GPUs, like really quickly.

31:54.680 --> 31:58.360
And in PyTorch, because you have this global interpreter lock, you have to use processes

31:58.360 --> 32:00.560
and not threads to do that.

32:00.560 --> 32:04.440
And so it's stuck PyTorch data loaders, it starts workers, which starts their multiple

32:04.440 --> 32:06.240
processes.

32:06.240 --> 32:11.640
And forking, like creating a process is much more heavy than creating a thread.

32:11.640 --> 32:17.640
And when you do this very quickly in a distributed setting, that actually became the bottleneck.

32:17.640 --> 32:21.120
So we had to change the data flow and the way we were doing this pre-fetching and those

32:21.120 --> 32:26.080
queues by having some kind of like always warm queues that were kind of like infinitely

32:26.080 --> 32:29.880
producing and then infinitely consuming on the other hand.

32:29.880 --> 32:34.240
And we're playing with fire a little bit there because we're creating racing conditions.

32:34.240 --> 32:36.480
And so that locks can happen.

32:36.480 --> 32:42.760
But because this doesn't sound like a plug-in or something that's like a funnel plug-in.

32:42.760 --> 32:44.240
So this was on top, right?

32:44.240 --> 32:49.240
This was something that we were using in stock PyTorch except for the data loaders.

32:49.240 --> 32:54.080
Except for the data loaders, where we changed the data loaders to something else.

32:54.080 --> 32:58.720
And that's what I mentioned by this, warm producers, infinite and this racing conditions.

32:58.720 --> 33:00.720
And recently we've been playing more and more with Horovod.

33:00.720 --> 33:03.880
This is also an open-source library made by Uber.

33:03.880 --> 33:04.880
And it's with PyTorch?

33:04.880 --> 33:05.880
It works with PyTorch.

33:05.880 --> 33:06.880
I didn't realize that.

33:06.880 --> 33:07.880
It's TensorFlow and PyTorch.

33:07.880 --> 33:10.040
It's starting with TensorFlow and now it's PyTorch.

33:10.040 --> 33:14.000
And actually, this provides this great MPI interface.

33:14.000 --> 33:17.400
And that enables, so it's a little bit less efficient for our niche application.

33:17.400 --> 33:19.080
But we have other applications.

33:19.080 --> 33:22.800
And so the flexibility that you get with Horovod might be worth the price in performance.

33:22.800 --> 33:26.320
So we're considering moving more and more stuff to Horovod.

33:26.320 --> 33:31.640
It sounds like you were able to, you invested a little bit in kind of tweaking PyTorch to make

33:31.640 --> 33:32.640
it work.

33:32.640 --> 33:33.640
But it kind of caught up.

33:33.640 --> 33:36.800
And now you've got some solutions that work for you.

33:36.800 --> 33:45.440
And so you are able to do distributed training like were you done, like pop the champagne.

33:45.440 --> 33:47.240
So it's interesting.

33:47.240 --> 33:51.280
And one way, yes, because there was a lot of internal questions.

33:51.280 --> 33:53.800
So like I said, TRIRI is a robotics company.

33:53.800 --> 33:58.400
And one thing you have to understand is that in autonomous driving, roboticists, they do

33:58.400 --> 34:03.800
very things very differently than the really hardcore deep learning crowd, which is they

34:03.800 --> 34:07.600
used to light our sensors, clustering methods, like DARPA challenge stuff.

34:07.600 --> 34:11.800
They work awesomely well and have like much stronger safety guarantees than what we

34:11.800 --> 34:12.960
do in deep learning.

34:12.960 --> 34:19.720
And so they're not necessarily very experienced in the deep learning way.

34:19.720 --> 34:23.840
And so doing these kind of things also means that like training for weeks to develop an

34:23.840 --> 34:26.080
algorithm, that sounds insane.

34:26.080 --> 34:30.280
And so here doing this distributed training and showing them internally that, hey, you can

34:30.280 --> 34:33.080
do things really quickly in the cloud at scale.

34:33.080 --> 34:37.040
And you can tweak your models and do your develop your algorithms almost as quickly as if

34:37.040 --> 34:38.680
you were not doing deep learning.

34:38.680 --> 34:43.000
That was kind of like a champagne popping bottle popping moments.

34:43.000 --> 34:44.000
Where is it?

34:44.000 --> 34:45.000
Oh, that's super cool.

34:45.000 --> 34:47.280
Now we actually are going to run with it.

34:47.280 --> 34:49.840
Of course, we're not done on the research side.

34:49.840 --> 34:55.000
Now we can basically study what happens when you do self-supervised learning on a lot

34:55.000 --> 34:59.680
of videos, what happens if you do imitation learning on really a lot of demonstrations.

34:59.680 --> 35:04.840
And actually we have a paper that we're going to push on archive soon, where we really

35:04.840 --> 35:10.040
push the boundaries of imitation learning and show that you can go quite far with like

35:10.040 --> 35:11.240
deeper models and more data.

35:11.240 --> 35:15.160
It's kind of like a prototypical deep learning story, more data, deeper models, that works

35:15.160 --> 35:16.160
really well.

35:16.160 --> 35:20.960
That's only thanks to the infrastructure that we had that we had an awesome intern, Felipe,

35:20.960 --> 35:23.080
that could do these experiments, thanks to that.

35:23.080 --> 35:27.240
So we're not there, but we're definitely enjoying the fruit of our labor.

35:27.240 --> 35:28.240
Nice.

35:28.240 --> 35:33.280
So there's a semantic segmentation that before you made it over to distributed was taking

35:33.280 --> 35:36.320
weeks, what does it take now typically?

35:36.320 --> 35:38.720
So we can do things in like under two hours now.

35:38.720 --> 35:39.720
Oh, wow.

35:39.720 --> 35:40.720
Wow.

35:40.720 --> 35:42.480
That's really fast, yeah.

35:42.480 --> 35:45.000
What does that require in terms of a cluster size?

35:45.000 --> 35:54.960
So we typically run jobs at, I think right now, beyond eight machines, so beyond 64 GPUs

35:54.960 --> 35:57.240
for single network, right?

35:57.240 --> 36:02.040
We find that we don't need to go beyond that at this stage, so we don't do like a single

36:02.040 --> 36:07.320
network on 256 GPUs or something like this, which is the most people that do that at least

36:07.320 --> 36:14.000
publicly do that is just to beat speed records on ImageNet, which you know, it's nice.

36:14.000 --> 36:15.640
That's not really what we're going for.

36:15.640 --> 36:22.280
So for the jobs we do, let's say between four and eight machines, so 32 and 64 GPUs,

36:22.280 --> 36:27.320
provides us with like a small turnaround time and good deterioration speed for our research.

36:27.320 --> 36:32.600
Is it that, you know, the complexity involved in going from eight to, you know, some multiple

36:32.600 --> 36:38.320
of eight isn't, you know, is overburdened some?

36:38.320 --> 36:45.680
Is it that the value of going from two hours to, you know, 30 minutes isn't there?

36:45.680 --> 36:50.920
So there's, there's some like more infrastructure problems around like limitation of supply,

36:50.920 --> 36:56.440
you know, like we often joke at TRI, we have infinite GPUs because we're in the cloud.

36:56.440 --> 37:00.320
But in reality, it's not necessarily there because availability zones, et cetera, so some

37:00.320 --> 37:02.520
things that I don't fully understand.

37:02.520 --> 37:09.080
The other thing is also at some point, you start to hit algorithmic difficulties.

37:09.080 --> 37:14.560
So like for instance a year ago, people were convinced you couldn't do large batch SGD

37:14.560 --> 37:17.440
because you would have generalization performance issues.

37:17.440 --> 37:21.600
And that's when Facebook made their, you know, oh, actually no, it's just a numerical optimization

37:21.600 --> 37:22.600
problem.

37:22.600 --> 37:26.880
You just got to do the linear scaling rule, this warm up, you have to twit a little bit.

37:26.880 --> 37:28.720
And then yes, it generalizes the same way.

37:28.720 --> 37:34.440
And that's when you have this explosion of large batch training methods, but still there's

37:34.440 --> 37:35.760
a limit to that, right?

37:35.760 --> 37:39.960
And depending on your data sets, depending on your learning algorithm, depending on also

37:39.960 --> 37:41.200
the data at hand, right?

37:41.200 --> 37:48.000
So the particular generalization gap that you have to overcome large, like there's a good

37:48.000 --> 37:49.880
size of batch size.

37:49.880 --> 37:54.080
So beyond like very, like there's a limit to how big your batch can be.

37:54.080 --> 37:55.080
Okay.

37:55.080 --> 38:00.720
So if you have a single cluster running at a time, or do you, you know, spin up multiple

38:00.720 --> 38:05.480
clusters and run multiple training jobs kind of constantly all the time, and does that,

38:05.480 --> 38:12.680
you know, if that's the case, or even if not really, does that level of change drive

38:12.680 --> 38:17.440
you to use something like Kubernetes or some kind of infrastructure, may you've mentioned

38:17.440 --> 38:19.960
Kubernetes and Slurm and some other things.

38:19.960 --> 38:20.960
Yeah.

38:20.960 --> 38:25.240
So the way we do it is we provision clusters on demand by the researchers.

38:25.240 --> 38:32.160
So we tend to have a couple of like clusters per researcher, per project.

38:32.160 --> 38:36.520
So that's really nice also because it helps a lot with experimental management, you know,

38:36.520 --> 38:37.520
like babysitting experiments.

38:37.520 --> 38:44.480
It's this full time job when you get closer to the deadline and having like these separate

38:44.480 --> 38:48.400
clusters for the separate workflows for the separate people.

38:48.400 --> 38:53.840
That helps with just the cognitive load of where you were, et cetera.

38:53.840 --> 38:58.040
And so we didn't feel like, and again, my team is like fairly small, we're like 12, 13

38:58.040 --> 38:59.040
people.

38:59.040 --> 39:00.040
Okay.

39:00.040 --> 39:04.160
So we don't need necessarily, we do very large experiments, but we don't necessarily

39:04.160 --> 39:08.200
do many, many different experiments, we like probably have four or five projects at the

39:08.200 --> 39:09.200
same time.

39:09.200 --> 39:10.200
Okay.

39:10.200 --> 39:14.120
So no need for like complex scheduling or monitoring or queuing or these kind of things.

39:14.120 --> 39:15.640
It's going to get there and we know it.

39:15.640 --> 39:19.000
So that's why we're preparing for that.

39:19.000 --> 39:21.640
And I have like more than HPC experience.

39:21.640 --> 39:26.320
So that's why I favor a bit Slurm also because when we started having this discussion,

39:26.320 --> 39:29.480
Kubernetes was not supporting GPUs now they do.

39:29.480 --> 39:34.400
And the only thing I'm a little bit afraid of is adding interaction levels because again,

39:34.400 --> 39:35.920
we care about speed and performance.

39:35.920 --> 39:40.760
So the story, the stock that I was talking mentioning before, we could do that because

39:40.760 --> 39:45.440
we were a working tightly with the infrastructure engineering team or AWS or Nvidia.

39:45.440 --> 39:50.480
We were actually talking to them directly and we actually knew what was going on under

39:50.480 --> 39:51.480
the hood.

39:51.480 --> 39:56.760
So we could pop up the look under the hood and say, oh, yeah, this is wrong or this is

39:56.760 --> 39:58.680
wrong or this smells funny.

39:58.680 --> 40:00.000
Can you check this?

40:00.000 --> 40:05.440
And so if we do too much, add too many layers of interaction like Kubernetes might be

40:05.440 --> 40:06.440
that.

40:06.440 --> 40:07.440
I don't know.

40:07.440 --> 40:08.440
I'm not sure.

40:08.440 --> 40:11.960
I'm a little bit afraid that we lose control and we lose interpretability in a sense.

40:11.960 --> 40:16.320
And our models are already hard to interpret.

40:16.320 --> 40:21.400
You mentioned in passing the managing experiments, experiment management.

40:21.400 --> 40:27.280
Have you built any higher level tooling or infrastructure to help research scientists

40:27.280 --> 40:28.280
do that?

40:28.280 --> 40:31.520
Or is there something that you're using off the shelf or is it, you know, post-it notes

40:31.520 --> 40:33.320
and Excel spreadsheet something?

40:33.320 --> 40:34.320
Yeah.

40:34.320 --> 40:36.640
So we did our fair share of Excel scheduling.

40:36.640 --> 40:39.280
Of course, do that a little bit.

40:39.280 --> 40:43.720
But we had an interesting journey where we initially used TensorBoard, but then TensorBoard

40:43.720 --> 40:46.520
didn't scale for us because that's on disk.

40:46.520 --> 40:48.760
And so it just didn't work.

40:48.760 --> 40:52.320
We switched to VISDOM, but VISDOM is a little bit too bare bones.

40:52.320 --> 40:56.880
It's very flexible, but there was also other issues there.

40:56.880 --> 40:58.800
So we're really starting to think about this.

40:58.800 --> 41:07.760
And at the same time, we got in touch with a company startup called WNB, WNB, WNB, WNB.

41:07.760 --> 41:12.840
And they basically, because we're just creating this company and basically talk to us and

41:12.840 --> 41:17.000
open AI and looking for like, what do you guys need?

41:17.000 --> 41:19.880
And we really worked really tightly with them.

41:19.880 --> 41:22.120
We have the customers now.

41:22.120 --> 41:26.560
And we have this really cool, like, experiment dashboard, experiment management system, where

41:26.560 --> 41:30.600
we can do a lot of visualization of experiments, multi-user, multi-project.

41:30.600 --> 41:32.840
It scales really well.

41:32.840 --> 41:34.840
And yeah, so that's what we used today.

41:34.840 --> 41:37.600
And so we're really, again, not optimizing for cost.

41:37.600 --> 41:39.400
We're optimizing for time.

41:39.400 --> 41:43.280
And because there's a lot of excitement around machine learning, there's a lot of opportunities

41:43.280 --> 41:44.640
to work with great partners.

41:44.640 --> 41:46.440
So that's our approach.

41:46.440 --> 41:50.400
When you first mentioned scale there, you were talking about on disk performance of

41:50.400 --> 41:58.400
TensorBoard, but then later when you're talking about its scaling, like, how is it doing

41:58.400 --> 42:02.120
in terms of, I mean, you're not a huge organization, but is it scaling in terms of the number

42:02.120 --> 42:05.040
or experiments that you do?

42:05.040 --> 42:09.440
So right now, we're, like, probably less than 20 users, so that's service.

42:09.440 --> 42:14.600
So I can't say about the scaling in the user, but we do a lot of experiments.

42:14.600 --> 42:19.760
Like, I mean, as you know, researchers, we, we, a lot of research is the speculative

42:19.760 --> 42:22.840
late strategy, which is you throw it at the wall and you see what sticks.

42:22.840 --> 42:27.000
So you have in a high-merch optimization, all these kind of things means that the single

42:27.000 --> 42:31.000
researcher, especially when you have this nice infrastructure in terms of machines and

42:31.000 --> 42:35.600
experiments you can run, you're going to, like, have a fire hose of metrics you want

42:35.600 --> 42:37.080
to visualize.

42:37.080 --> 42:39.600
And so that scales really well for that.

42:39.600 --> 42:42.920
And again, we're not in the business of making dashboards or these things, so we're really

42:42.920 --> 42:47.560
happy to partner or buy whatever is not our core business, which is really about this deep

42:47.560 --> 42:49.760
learning models for driving.

42:49.760 --> 42:54.280
And does WNB do the hyper-performer optimization for you?

42:54.280 --> 42:55.280
Or do you do service?

42:55.280 --> 42:57.680
Yeah, so we do our own stuff there.

42:57.680 --> 43:03.000
They have some services there, which we don't use, but we, I think hyper-optimization

43:03.000 --> 43:06.200
is, like, I'm still on the fence whether this is something internal or something that

43:06.200 --> 43:11.320
we can partner with, because there is, like, typical patterns and typical, like, algorithms

43:11.320 --> 43:15.880
and I've been a big user of hyper-opt, so you can do biogen hyper-optimization and these

43:15.880 --> 43:18.040
kind of things.

43:18.040 --> 43:21.360
And sure, this is, like, almost standardized, so you could imagine having a service for

43:21.360 --> 43:22.560
that instead.

43:22.560 --> 43:26.800
But some of these things actually have to hook up really deep into, so it depends on the

43:26.800 --> 43:29.440
model and the research project you're doing.

43:29.440 --> 43:34.240
So there's a blurred line there, and there's awesome, like, general purpose algorithms,

43:34.240 --> 43:35.240
I think.

43:35.240 --> 43:39.000
The one that I use the most recently that I really like is hyper-band, which is kind

43:39.000 --> 43:43.920
of, like, a band-it approach that's using the fact that it's our optimization is sequential,

43:43.920 --> 43:47.760
so you can restart from a checkpoint and continue any of these kind of things.

43:47.760 --> 43:52.760
So yeah, on the fence, some of the things in-house, some of the things, black box, it's

43:52.760 --> 43:55.000
not, I'm on the fence for this.

43:55.000 --> 44:00.720
And so I'm making an assumption that you don't have to worry about, besides from the kind

44:00.720 --> 44:05.960
of distributive file system issues that we've talked about, a lot of the kind of traditional

44:05.960 --> 44:11.240
enterprise data management, you know, data lakes, data warehouse, hooking into data stores,

44:11.240 --> 44:12.240
that stuff.

44:12.240 --> 44:13.600
You just need big, dist store stuff.

44:13.600 --> 44:14.600
Yeah.

44:14.600 --> 44:19.960
So we have, like, we use S3 a lot, and we use to use S3 directly.

44:19.960 --> 44:23.320
And so basically, what we've been doing is just, like, what happened with processors.

44:23.320 --> 44:27.640
We add layers of cache, you know, and hotter and hotter caches, which are getting smaller

44:27.640 --> 44:30.080
as they are getting hotter.

44:30.080 --> 44:34.240
And we have to, like, asynchronously, preemptively fill those caches and these kind of things.

44:34.240 --> 44:35.240
So it's always the same thing.

44:35.240 --> 44:40.440
Is that these caches, like, the, you know, the various S3 features, like, glacier and all

44:40.440 --> 44:41.440
these?

44:41.440 --> 44:44.440
Yeah, so you can see basically all these different types of storages, right, as these

44:44.440 --> 44:49.040
cache, like, like, these are, I call them caches, but it's a metaphorical cache, right?

44:49.040 --> 44:50.040
Right.

44:50.040 --> 44:57.640
S3, we used to use S3 directly as, like, S3 to GPUs, and that obviously didn't scale.

44:57.640 --> 45:02.880
And so we added this, like, you know, different storages and distributed files, yeah, EBS,

45:02.880 --> 45:03.880
etc.

45:03.880 --> 45:04.880
Yeah.

45:04.880 --> 45:05.880
Yeah.

45:05.880 --> 45:09.520
And then you have these prefetch and queues that are, literally, feeling the RAM with,

45:09.520 --> 45:13.320
next to your GPUs, and so you have this ultimate layer of cache, right?

45:13.320 --> 45:14.320
Okay.

45:14.320 --> 45:19.600
And then on the back end, are you, do you have to worry about inference and model serving?

45:19.600 --> 45:25.440
So at this moment, the inference is the models we serve are in our robots, right?

45:25.440 --> 45:27.040
So that's a big thing, right?

45:27.040 --> 45:28.040
Right.

45:28.040 --> 45:31.520
Is that the models we serve are the ones that are going to be in the path to actuation

45:31.520 --> 45:32.520
of the car.

45:32.520 --> 45:37.520
So there we have, like, amazing driving technology teams, like, that own parts of the

45:37.520 --> 45:41.600
stack, like, we have an object perception team, we have a slam team, we have a planning

45:41.600 --> 45:43.000
and controls team.

45:43.000 --> 45:48.160
And these guys, they basically take our models or make their own.

45:48.160 --> 45:54.200
And they make them more efficient, fit them in the computational budget that they have.

45:54.200 --> 45:55.720
And that's how we serve models.

45:55.720 --> 46:00.920
So that's a fairly different model than, let's say, web-based application.

46:00.920 --> 46:01.920
Yeah.

46:01.920 --> 46:10.480
Is that the process of getting the models to fit compression or pruning or what have

46:10.480 --> 46:14.480
you, is that there's still a manual process to large degrees, right?

46:14.480 --> 46:21.520
So to some extent, it's kind of a little bit weird because there are some ways that are

46:21.520 --> 46:24.440
productized, a little bit like there's sensor RT and these kind of things.

46:24.440 --> 46:27.920
But it's still more an art than a science, so it doesn't always work.

46:27.920 --> 46:33.040
It works for certain types of networks like out of the box with some it doesn't.

46:33.040 --> 46:37.800
And so there you have some tools or bag of tricks that are general purpose that you can

46:37.800 --> 46:42.400
throw at this and that you should throw at this, definitely, and that our teams are doing.

46:42.400 --> 46:46.320
So upstream or upstream on the research side and my team what we're doing, you can learn

46:46.320 --> 46:52.560
models that are compressible or that are amenable to compression by having some compressibility

46:52.560 --> 46:53.960
factor built in.

46:53.960 --> 46:57.680
You can have also small models, like as I mentioned, right?

46:57.680 --> 47:02.040
And there's more and more research results that show that small models can generalize as

47:02.040 --> 47:03.200
well as big models.

47:03.200 --> 47:07.400
They just have to train for longer or you have to change the learning algorithm.

47:07.400 --> 47:13.960
And another thing is one of the big things we're trying to do is how far can we do multi-test

47:13.960 --> 47:15.360
learning, right?

47:15.360 --> 47:20.320
Because if you can have a shared backbone and squeeze many different things, that's awesome.

47:20.320 --> 47:24.960
And the one recent project that we did around panoptic segmentation is basically this.

47:24.960 --> 47:30.200
It basically takes a mental segmentation and takes instant segmentation, so mask our

47:30.200 --> 47:36.320
CNN and these kind of things and that works really well, but that's extremely slow.

47:36.320 --> 47:43.280
I think like mask rescinds me like 150 milliseconds per image or something and you have to basically

47:43.280 --> 47:49.000
reduce those models and merge them together with maybe different heads to make it efficient.

47:49.000 --> 47:53.520
And we made a recent paper, it's going to be an archive soon called tasknet for things

47:53.520 --> 47:59.120
and stuff consistency network where we basically merge them together and have a task consistency,

47:59.120 --> 48:02.160
a cross task consistency because the main problem with multi-test learning is if you just

48:02.160 --> 48:05.880
sum the losses, it doesn't necessarily, they maybe contradict each other.

48:05.880 --> 48:09.720
So when you're arriving at an intersection, it says turn left or turn right, you don't

48:09.720 --> 48:12.840
know you go in the middle, there's a lot of work, not a good idea.

48:12.840 --> 48:17.120
So like imagine the gradients might be pushing in orthogonal directions.

48:17.120 --> 48:22.360
So one of the key things we did is we actually augmented the objective to have a consistency

48:22.360 --> 48:28.560
encouraging objective and between the stuff classes, like so road, sky, et cetera, on

48:28.560 --> 48:34.000
the semantic segmentation side and the thing classes on the instant segmentation side.

48:34.000 --> 48:39.000
So merging these networks is one way to be more efficient and that's some of the very

48:39.000 --> 48:40.720
recent work that we've been doing.

48:40.720 --> 48:45.000
How have we done in terms of kind of getting a lay of the land of your presentation?

48:45.000 --> 48:47.000
We went way beyond.

48:47.000 --> 48:52.280
Yes, so excellent, nice, awesome.

48:52.280 --> 48:55.560
Any kind of parting thoughts or words?

48:55.560 --> 49:02.080
No, I think we're really excited to continue this direction of large-scale deep learning

49:02.080 --> 49:08.560
in the cloud and tackling this really, really challenging open research questions, yeah.

49:08.560 --> 49:12.520
So we're continuing to grow very, very fast and excited to be in that space of self-driving

49:12.520 --> 49:14.320
robots and with deep learning.

49:14.320 --> 49:16.720
So very happy to have been able to talk about it.

49:16.720 --> 49:17.720
Awesome.

49:17.720 --> 49:18.720
Well, thanks so much, Adrian.

49:18.720 --> 49:19.720
It was awesome to have you on the show.

49:19.720 --> 49:20.720
It was a pleasure.

49:20.720 --> 49:26.080
All right, everyone, that's our show for today.

49:26.080 --> 49:30.960
For more information about today's guests or to follow along with our AI platform's volume

49:30.960 --> 49:36.960
2 series, visit twimmelai.com slash AI Platforms 2.

49:36.960 --> 49:42.280
Thanks once again to Sigop for their sponsorship of this series and support of the show.

49:42.280 --> 49:47.560
To check out what they're up to and take advantage of their exclusive offer for Twimmel listeners,

49:47.560 --> 49:51.280
visit twimmelai.com slash Sigop.

49:51.280 --> 50:01.280
As always, thanks so much for listening and catch you next time.

