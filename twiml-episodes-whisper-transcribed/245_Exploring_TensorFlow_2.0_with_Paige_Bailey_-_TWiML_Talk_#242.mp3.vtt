WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.720
I'm your host Sam Charrington.

00:31.720 --> 00:36.520
A few weeks ago I had a chance to attend the TensorFlow Developer Summit on the Google

00:36.520 --> 00:39.520
Cloud campus in Sunnyvale, California.

00:39.520 --> 00:43.800
I had a great time at the summit, meaning a bunch of Twimble listeners and friends of the

00:43.800 --> 00:50.040
show and learning about all the new features released as part of the TensorFlow 2.0 Alpha.

00:50.040 --> 00:53.880
This week on the show, we'll be bringing you a series of conversations from the event,

00:53.880 --> 00:58.040
which we'll be kicking off with today's interview with Paige Bailey, a TensorFlow Developer

00:58.040 --> 01:00.160
Advocate at Google.

01:00.160 --> 01:04.280
Paige and I sat down to talk through the latest TensorFlow updates and we cover a ton of

01:04.280 --> 01:09.920
ground, including the evolution of the TensorFlow APIs and the role of Eager mode, TF.Kiris

01:09.920 --> 01:15.600
and TF.Function, the introduction of TensorFlow for Swift and its inclusion in the new fast.ai

01:15.600 --> 01:22.760
course, new updates to TFX or TensorFlow Extended, Google's end-to-end machine learning platform,

01:22.760 --> 01:28.800
and the emphasis on community collaboration with TensorFlow 2.0 and a bunch more.

01:28.800 --> 01:34.040
Now Dev Summit attendees received an awesome gift box from Google that included some really

01:34.040 --> 01:38.400
fun toys that will appeal to the hackers and builders in our audience, including the

01:38.400 --> 01:45.480
new Coral Edge TPU device and the Spark Fun Edge Development Board powered by TensorFlow.

01:45.480 --> 01:49.600
If you're a geek like me and get excited about toys like this, I'm excited to share that

01:49.600 --> 01:54.160
we've got our hands on a few of these for you, our dedicated listeners.

01:54.160 --> 02:00.480
To enter to win one, just visit twimmelai.com slash TF giveaway and let us know what you

02:00.480 --> 02:03.320
would do with this kit if you got your hands on it.

02:03.320 --> 02:07.680
Of course, we'd like to send a huge thanks to the TensorFlow team for helping us bring

02:07.680 --> 02:10.880
you this podcast series and giveaway.

02:10.880 --> 02:14.200
With all the great announcements coming out of the TensorFlow Dev Summit, including the

02:14.200 --> 02:20.400
2.0 Alpha, you should definitely check out the latest and greatest over at TensorFlow.org,

02:20.400 --> 02:23.880
where you can also download and start building with the framework.

02:23.880 --> 02:26.680
And now on to the show.

02:26.680 --> 02:31.520
All right, everyone.

02:31.520 --> 02:32.920
I am here with Page Bailey.

02:32.920 --> 02:38.680
Page is a developer advocate for TensorFlow with Google, and we're here at the TensorFlow

02:38.680 --> 02:39.680
Developer Summit.

02:39.680 --> 02:41.880
Page, welcome to this week in machine learning and AI.

02:41.880 --> 02:42.880
Excellent.

02:42.880 --> 02:43.880
I'm delighted to be here.

02:43.880 --> 02:48.920
I'm delighted to have you on the show and looking forward to diving into all that's new and

02:48.920 --> 02:50.680
interesting with TensorFlow.

02:50.680 --> 02:54.440
But before we do that, a little about your background, how did you get started working

02:54.440 --> 02:55.440
and all this stuff?

02:55.440 --> 02:56.440
Yeah.

02:56.440 --> 03:01.480
So I got started working in machine learning and data science before data science was

03:01.480 --> 03:05.160
really a term, I guess.

03:05.160 --> 03:13.520
My background in undergrad was very geophysics applied math, I did research in planetary

03:13.520 --> 03:21.600
science, doing data analysis on large amounts of data from NASA equipment, so I got to

03:21.600 --> 03:26.200
work at places like the laboratory for atmospheric and space physics and Southwest Research Institute

03:26.200 --> 03:30.160
doing lunar ultraviolet research.

03:30.160 --> 03:35.560
And that today, doing statistical analysis on multivariate data would be called data science.

03:35.560 --> 03:39.560
But back then, it was just called research.

03:39.560 --> 03:42.480
So that was when the bug first started.

03:42.480 --> 03:48.640
And then I started doing machine learning for work when I worked at Chevron for predictive

03:48.640 --> 03:51.560
modeling in the oil and gas industry.

03:51.560 --> 03:56.680
So I guess in terms of data science things, I've been doing it for about 10 years for machine

03:56.680 --> 03:58.800
learning about five to six.

03:58.800 --> 04:01.560
Did you jump from Chevron to Google?

04:01.560 --> 04:02.560
No.

04:02.560 --> 04:07.760
So that was an interesting sort of career progression, I guess.

04:07.760 --> 04:12.440
I had done planetary science research as an undergrad for my first two internships and

04:12.440 --> 04:15.240
then my third was with Chevron.

04:15.240 --> 04:21.320
And they had me on a really cool project I got to create a database and do some scripting,

04:21.320 --> 04:24.480
some geospatial analysis using Python.

04:24.480 --> 04:28.200
And after it, they said, hey, do you want to come work for us?

04:28.200 --> 04:32.680
And that sounded like a cool job opportunity, especially since they were offering to pay

04:32.680 --> 04:34.800
for grad school.

04:34.800 --> 04:39.280
Because I went to go work there for about five years.

04:39.280 --> 04:47.560
And after Chevron, it was time to kind of leave and to explore new opportunities.

04:47.560 --> 04:48.960
And that was Microsoft.

04:48.960 --> 04:54.360
So I worked for Microsoft doing machine learning developer advocacy and then as a senior software

04:54.360 --> 04:56.720
engineer in the office of the Azure CTO.

04:56.720 --> 05:00.680
For those that aren't familiar, what exactly is a developer advocate and how does that translate

05:00.680 --> 05:02.840
into how you spend your day-to-day?

05:02.840 --> 05:03.840
Absolutely.

05:03.840 --> 05:05.560
That is a great question.

05:05.560 --> 05:10.240
And developer advocacy means different things to different companies.

05:10.240 --> 05:18.720
Here at Google, developer advocacy is very focused on improving the experience for our end

05:18.720 --> 05:22.920
users and for basically anybody who would go about using our tool.

05:22.920 --> 05:28.600
And that can include everything from improving documentation to user experience research,

05:28.600 --> 05:33.320
to building tutorials and quick starts, to creating curriculum materials like the Udacity

05:33.320 --> 05:40.240
and Deep Learning AI courses that we just released, to also engaging with the community via

05:40.240 --> 05:44.280
social media, but also through things like special interest groups and making sure that

05:44.280 --> 05:49.680
if there is a problem with the tool, it's surfaced, fed back to the engineering team and we can

05:49.680 --> 05:53.200
build a plan to resolve whatever that issue happens to be.

05:53.200 --> 05:59.560
So if anything is frustrating, so if maybe a higher level API isn't intuitive, it might

05:59.560 --> 06:07.000
require making a pull request and resolving that or say we're missing a symbol in our

06:07.000 --> 06:14.120
documentation, then making a pull request and fixing that in the docs.

06:14.120 --> 06:20.960
So it's really a variety of things, but as I mentioned, it's all very focused on communication,

06:20.960 --> 06:28.600
improving the developer experience, education, and also very much engineering tasks.

06:28.600 --> 06:32.320
And if you're a person who enjoys all of the above, then maybe developer advocacy is

06:32.320 --> 06:33.320
right for you.

06:33.320 --> 06:34.320
Awesome.

06:34.320 --> 06:35.320
Awesome.

06:35.320 --> 06:40.160
So, there was a bunch of interesting TensorFlow news yesterday.

06:40.160 --> 06:42.640
Yes, there was so much.

06:42.640 --> 06:46.680
It felt like a fire hole was almost and that's coming from so many new, it was all about

06:46.680 --> 06:47.680
to happen.

06:47.680 --> 06:49.680
So many new products.

06:49.680 --> 06:50.680
Yep.

06:50.680 --> 06:56.040
Yeah, so I'm really interested in kind of digging into them, one of the things that I've

06:56.040 --> 07:00.160
mentioned this to a couple of times, maybe yesterday, one of the things that's been a little

07:00.160 --> 07:05.520
bit difficult for me to parse is an outsider is like, what exactly is new and all of these

07:05.520 --> 07:07.240
things that are kind of new?

07:07.240 --> 07:13.120
Like, eager mode was highlighted very extensively yesterday, but that was announced last year.

07:13.120 --> 07:17.320
There were other things that I'm like, I know I've seen this before, there was a paper

07:17.320 --> 07:23.480
that was a couple of years ago, you know, so for me, you know, probably the best thing

07:23.480 --> 07:26.480
to do is to have you kind of walk through from your perspective, you know, what were the

07:26.480 --> 07:31.000
highlights and what was new and then we can kind of dig in from there.

07:31.000 --> 07:32.000
Right.

07:32.000 --> 07:41.000
So, it is difficult to kind of place what's actually new and then what's just been kind

07:41.000 --> 07:42.680
of bundled and packaged, right?

07:42.680 --> 07:49.280
So the biggest announcement yesterday by far was the alpha release of TensorFlow 2.0.

07:49.280 --> 07:57.280
And TensorFlow 2.0 is a new version of TensorFlow focused on developer productivity, ease of

07:57.280 --> 08:04.400
use, carous as the recommended higher level API, an eager execution by default, which enables

08:04.400 --> 08:13.280
you to do things like better debugging, but also the pathonic experience that you, especially

08:13.280 --> 08:18.960
folks coming from scikit-learn and the data science community would expect.

08:18.960 --> 08:25.800
So TensorFlow 2.0 is, I mentioned it's eager execution by default, and you're correct

08:25.800 --> 08:31.320
in that eager execution was announced last year, but in order to enable it, you had to

08:31.320 --> 08:36.320
do something called like tf.enable eager execution at the top of your script.

08:36.320 --> 08:41.240
So you had to kind of specify that at the beginning and you had the option of sticking

08:41.240 --> 08:47.120
in graph mode if you didn't enable eager execution, tf2.0 is eager by default, which means

08:47.120 --> 08:54.080
that you don't have to have that line, but you're thrown directly into this experience

08:54.080 --> 08:58.880
where you don't have to create a static graph, you don't have to have sessions, you don't

08:58.880 --> 09:03.280
have to create the thing before you can interact with it, before you can actually start using

09:03.280 --> 09:04.280
it.

09:04.280 --> 09:08.880
It feels much more pathonic.

09:08.880 --> 09:14.920
So that's kind of a new thing in the eager execution by default.

09:14.920 --> 09:20.360
Another very, very new thing is that TensorFlow 2.0 was created entirely in collaboration with

09:20.360 --> 09:22.760
the community through the RFC process.

09:22.760 --> 09:27.880
So every change that was made to the API had to be proposed by an engineer or proposed

09:27.880 --> 09:36.040
by someone endorsed by the entire community, discussed, and before it was incorporated.

09:36.040 --> 09:42.380
So if you have any curiosity about these RFCs, you can go and take a look at it on github.com

09:42.380 --> 09:44.520
slash TensorFlow slash community.

09:44.520 --> 09:49.000
We have a list of all of them, and if you care about details, they're all important.

09:49.000 --> 09:53.000
We also have a community of special interest groups, everything from TensorBoard, which

09:53.000 --> 09:59.440
is a visualization tool, to build, which is sort of building TensorFlow for various Linux

09:59.440 --> 10:04.720
distributions, and then also with Nvidia graphics card strivers and the like.

10:04.720 --> 10:07.600
We have a special interest group for testing.

10:07.600 --> 10:12.400
So if you're trying out TensorFlow 2.0 and you're running into Snags, or if you need migration

10:12.400 --> 10:15.520
support, there's that option for you as well.

10:15.520 --> 10:20.640
And then others are on networking and data ingestion and lots and lots of different ways

10:20.640 --> 10:21.880
to get involved.

10:21.880 --> 10:28.720
And really, as I mentioned, the focus on TF2.0 is community, sort of building something

10:28.720 --> 10:34.760
that people would love to use, as opposed to the first TensorFlow release, which kind

10:34.760 --> 10:40.120
of felt like, hey, here's some code, like go forth and prosper, okay?

10:40.120 --> 10:46.320
And not exactly the most straightforward sort of code, so that's the general idea.

10:46.320 --> 10:53.680
A lot of the kind of top line takeaway is around kind of APIs and developer experience.

10:53.680 --> 10:59.440
Eager by default, you can kind of think of that as, okay, so there's one line of code

10:59.440 --> 11:01.000
less that I have to write.

11:01.000 --> 11:08.240
But is it deeper than that in terms of the underlying architecture and the way that TensorFlow

11:08.240 --> 11:09.240
supports that?

11:09.240 --> 11:10.600
That's a great question.

11:10.600 --> 11:11.600
And absolutely.

11:11.600 --> 11:18.080
So Carmel, during the keynote yesterday, she had this great slide, where, so if you're

11:18.080 --> 11:24.720
a carous developer, if you have experience using TF.carous, you can create a neural network

11:24.720 --> 11:26.480
with about 10 lines of code.

11:26.480 --> 11:28.280
And it's very straightforward.

11:28.280 --> 11:29.280
It's intuitive.

11:29.280 --> 11:31.480
It's easy to understand.

11:31.480 --> 11:33.520
And again, feels very private.

11:33.520 --> 11:38.640
She showed a slide saying, here's what your model would look like if you're using TensorFlow

11:38.640 --> 11:44.200
like 113 or TensorFlow 112 or whatever version.

11:44.200 --> 11:45.960
And it's 10 lines of carous.

11:45.960 --> 11:48.880
And she said, and here's what it would look like in TensorFlow 2.0.

11:48.880 --> 11:50.480
It's the same code.

11:50.480 --> 11:52.560
The same code expressed both ways.

11:52.560 --> 11:58.520
But the reality is that TF2.0 is doing a lot of things under the hood to improve model

11:58.520 --> 12:06.920
performance and to take that same carous code and make it scalable in a way that it wasn't

12:06.920 --> 12:09.080
experienced before.

12:09.080 --> 12:14.160
So even though the code looks exactly the same, there are a lot of optimizations under

12:14.160 --> 12:20.200
the hood that makes it a bit more performant, especially with distributed architecture,

12:20.200 --> 12:22.360
so with distribution strategies.

12:22.360 --> 12:28.480
One of the things that was presented yesterday was there was a fair amount of detail that

12:28.480 --> 12:30.960
was gone into around TF.function.

12:30.960 --> 12:32.800
Is that totally new?

12:32.800 --> 12:35.800
Yeah, TF.function is new.

12:35.800 --> 12:44.120
TF.function is basically a way for you to take any sort of Python function, rapid

12:44.120 --> 12:48.120
and define it as a core TensorFlow operation.

12:48.120 --> 12:54.440
So you could have anything from adding two numbers, so you could have a function that would

12:54.440 --> 13:00.440
take, you would define something that would add A and B. And if you preface it with an

13:00.440 --> 13:05.600
at TF function, it would define it as a core TensorFlow operation that you could then

13:05.600 --> 13:08.160
use in your graph.

13:08.160 --> 13:10.880
That is pretty new.

13:10.880 --> 13:16.920
People have had a lot of positive feedback about autograph and TF function both when using

13:16.920 --> 13:23.320
TensorFlow 2.0, and it makes it a lot easier to take things that traditionally would have

13:23.320 --> 13:32.480
been very difficult to do with static computation, static graphs and sort of use them in a friendly

13:32.480 --> 13:33.480
dynamic way.

13:33.480 --> 13:34.480
Okay.

13:34.480 --> 13:36.000
And what is autograph?

13:36.000 --> 13:39.720
Autograph helps you write complicated graph code using normal Python.

13:39.720 --> 13:44.680
So it automatically transforms your code into the equivalent TensorFlow graph code, and

13:44.680 --> 13:48.160
it supports a lot of the Python language.

13:48.160 --> 13:53.680
So that is that's a kind of one-liner explanation of autograph.

13:53.680 --> 14:01.640
2.0 introduces some significant changes to the API, but that's not the only kind of developer

14:01.640 --> 14:04.840
facing developer experience kind of announcement.

14:04.840 --> 14:09.680
One of the ones that's gotten a lot of conversation going is around the Swift for TensorFlow

14:09.680 --> 14:14.440
and curious what your thought, do you deduce, do you know Swift?

14:14.440 --> 14:16.440
I have to play with that at all.

14:16.440 --> 14:24.080
So I have fiddled with it, but only since I got to Google, I hadn't really experimented

14:24.080 --> 14:30.120
with Swift before kind of, well, I get to sit next to Chris Latner, which is Chris

14:30.120 --> 14:32.600
Latner and Brennan and the entire Swift team.

14:32.600 --> 14:35.160
So that's kind of rad.

14:35.160 --> 14:43.560
And the idea behind Swift for TensorFlow is that if you're a Python developer, Python

14:43.560 --> 14:46.720
is great in terms of user experience.

14:46.720 --> 14:52.760
As a user interface, as a programming language, it's a great way to build out products.

14:52.760 --> 14:55.720
But it also has a lot of limitations and drawbacks.

14:55.720 --> 15:02.480
So for example, gradient tape is something that causes a great deal of frustration whenever

15:02.480 --> 15:04.240
you're using TensorFlow.

15:04.240 --> 15:05.240
gradient tape?

15:05.240 --> 15:06.240
gradient tape.

15:06.240 --> 15:07.240
What is that?

15:07.240 --> 15:12.280
It's how you deal with variables as you're creating your graph.

15:12.280 --> 15:18.200
And so as a Python developer, it's often very frustrating to try to manage all of these

15:18.200 --> 15:20.520
different aspects.

15:20.520 --> 15:31.320
But with a lower level tool like Swift, a language that's a little bit closer to C++,

15:31.320 --> 15:39.080
you get afforded a lot more control over what you're creating.

15:39.080 --> 15:40.560
And that's kind of the idea.

15:40.560 --> 15:45.240
And I think that's what captured the imagination of Jeremy Howard.

15:45.240 --> 15:49.080
So I'm sure you saw that in Alzheimer's as well.

15:49.080 --> 15:54.920
He's using Swift for TensorFlow for his latest iteration of the FAST AI curriculum, which

15:54.920 --> 15:56.680
we're all very excited about.

15:56.680 --> 16:03.080
And I'm very curious to see how that progresses.

16:03.080 --> 16:08.020
Another really cool thing about Swift is, or Swift for TensorFlow rather, is that you

16:08.020 --> 16:13.520
can use Python within Swift just within port Python.

16:13.520 --> 16:21.640
And there is a great demonstration from Brennan and from Chris yesterday where you could create

16:21.640 --> 16:28.560
something like a plot of performance or accuracy over time with Matplotlib, just as simply

16:28.560 --> 16:33.440
as you would with Python, except all you have to do is practice it with LAT for your

16:33.440 --> 16:34.440
variable.

16:34.440 --> 16:40.560
So I think Swift for TensorFlow is a really interesting space.

16:40.560 --> 16:44.720
They're still very new, so they're still developing their story and they're still developing

16:44.720 --> 16:46.520
the product.

16:46.520 --> 16:50.000
But I'm curious to see how it progresses this next year.

16:50.000 --> 16:52.040
And they certainly have a vibrant community.

16:52.040 --> 16:58.640
If you have any, I mentioned the special interest groups a little bit before, but they also

16:58.640 --> 17:04.240
have these really active mailing lists where people are asking questions and sharing products

17:04.240 --> 17:10.800
that they've created and sort of ideas that they have about changing designs of API and

17:10.800 --> 17:12.800
technical things.

17:12.800 --> 17:19.960
And the Swift for TensorFlow user group and mailing list is consistently one of the most

17:19.960 --> 17:20.960
active.

17:20.960 --> 17:28.320
So if you have curiosity about it, absolutely join and send your questions to swiftittensorflow.org.

17:28.320 --> 17:29.320
Awesome.

17:29.320 --> 17:30.320
Awesome.

17:30.320 --> 17:31.320
Yeah.

17:31.320 --> 17:35.880
People keep asking me like what I think about it and like, I don't know, I've never seen

17:35.880 --> 17:40.400
Swift outside of a slide on the presentation today.

17:40.400 --> 17:41.400
Yeah.

17:41.400 --> 17:48.800
It's very, when I first, because it was announced last year, Swift for TensorFlow, but in

17:48.800 --> 17:50.800
a very sort of low key way, right?

17:50.800 --> 17:51.800
Okay.

17:51.800 --> 17:52.800
I missed that.

17:52.800 --> 17:53.800
Yeah.

17:53.800 --> 17:54.800
It was a presentation.

17:54.800 --> 17:59.280
It was also the best non-leak of TensorFlow history.

17:59.280 --> 18:05.720
I think like nobody anticipated that it was coming, but so it was announced last year.

18:05.720 --> 18:12.040
And all of the progress that you've seen since has been made since that initial announcement.

18:12.040 --> 18:13.040
But yeah.

18:13.040 --> 18:17.520
Like usually you would think Swift and think, you know, iOS or, you know, something.

18:17.520 --> 18:18.760
That's exactly what I think.

18:18.760 --> 18:19.760
Yeah.

18:19.760 --> 18:20.760
Yeah.

18:20.760 --> 18:25.200
You know, I tell people, I'm kind of excited about it because Jeremy's excited.

18:25.200 --> 18:31.280
Part two of the new course is starting up soon and sounds like he and Chris will be collaborating

18:31.280 --> 18:34.400
on a couple of lessons in that course to kind of highlight.

18:34.400 --> 18:35.400
Yeah.

18:35.400 --> 18:40.840
Well, not just kind of highlight the Swift for TensorFlow, but also this may be a little

18:40.840 --> 18:46.680
bit of kind of inside baseball or whatever, but like the way that the way that he builds

18:46.680 --> 18:51.920
the courses is basically by building out the framework, right?

18:51.920 --> 18:55.840
And that becomes the process of building out the framework becomes the course.

18:55.840 --> 19:01.720
And so it sounds like what he and Chris will be doing in this course is like starting

19:01.720 --> 19:12.360
to build out the fast.ai Swift version as kind of part of this course is really fascinating

19:12.360 --> 19:15.680
kind of approach for multiple reasons.

19:15.680 --> 19:20.800
I took a look at his blog post about kind of why he was so excited about Swift and one

19:20.800 --> 19:27.360
of the reasons that that he's real playing about is like, this is the ground floor opportunity

19:27.360 --> 19:33.400
like Swift, you know, TensorFlow or rather Python, you know, they're all of these challenges

19:33.400 --> 19:40.320
associated with like the barrier between Python and C when you have to get really deep

19:40.320 --> 19:44.360
and not being able to debug all the way through and not be able to trace variables all

19:44.360 --> 19:46.960
the way through that kind of thing.

19:46.960 --> 19:53.160
And needing to kind of shim out underneath Python to get performance and one of the reasons

19:53.160 --> 19:58.160
why he's excited about Swift is because it is high performance.

19:58.160 --> 20:03.880
And you know, to the extent that over time kind of the entire stat gets built in a high

20:03.880 --> 20:08.840
performance language, you don't have these barriers that you have to figure out.

20:08.840 --> 20:09.840
Absolutely.

20:09.840 --> 20:17.480
Like it's a fascinating space and also so one of the things that shocked me the most

20:17.480 --> 20:23.960
I guess about Swift was how intuitive it was to understand as a language.

20:23.960 --> 20:29.720
Like if you look at the Swift code used to generate a neural network, it feels very

20:29.720 --> 20:32.880
similar to Keras.

20:32.880 --> 20:39.320
It's not because I took some people as well as classes in college and I am not a fan

20:39.320 --> 20:40.960
and will not be shy about saying that.

20:40.960 --> 20:46.720
Like I love the idea of it conceptually as a language, but it's something that's very

20:46.720 --> 20:49.400
it doesn't feel natural for me to use.

20:49.400 --> 20:51.920
Swift feels like a great deal more natural.

20:51.920 --> 20:59.880
And one of the challenges I think that they're going to face is the that Swift as you mentioned

20:59.880 --> 21:04.320
hasn't traditionally been a data sciencey language.

21:04.320 --> 21:10.400
So they don't have all of the vast libraries of tools that Python has.

21:10.400 --> 21:15.840
So things like matplotlib and psychic learning for traditional machine learning tasks and

21:15.840 --> 21:17.640
the like.

21:17.640 --> 21:23.400
But it's really interesting and that so we're doing Google Summer of Code for the very

21:23.400 --> 21:25.680
first time this year.

21:25.680 --> 21:31.480
And five of our projects or five of our proposed projects are Swift for TensorFlow related.

21:31.480 --> 21:37.320
Summer of Code for those that don't know as a program where open source projects can get

21:37.320 --> 21:42.440
matched with like college students or students in general maybe and get some funding to pay

21:42.440 --> 21:44.000
them to work on the project.

21:44.000 --> 21:45.000
Absolutely.

21:45.000 --> 21:48.680
So the Google Summer of Code is one of my favorite things about Google.

21:48.680 --> 21:55.880
And it was also when I applied for it a long time ago when I was still doing the grad school

21:55.880 --> 21:56.880
thing.

21:56.880 --> 22:03.640
I applied to a project called AIMA and got a response back from Peter Norvig who was

22:03.640 --> 22:09.840
apparently the person who who was sponsoring the project and that was my first email from

22:09.840 --> 22:10.840
a Googler.

22:10.840 --> 22:11.840
And it was Peter Norvig.

22:11.840 --> 22:12.840
Wow.

22:12.840 --> 22:13.840
Yeah.

22:13.840 --> 22:14.840
It was amazing.

22:14.840 --> 22:22.320
But it's it's so enchanting in that open source projects can apply to be part of this

22:22.320 --> 22:29.040
GSOC program if they're selected that they can have a partnership of mentors who are

22:29.040 --> 22:32.760
directly working on the project and students.

22:32.760 --> 22:38.360
The mentors work collaboratively with the students, doesn't matter where they're located.

22:38.360 --> 22:42.760
And it's there's certainly encouraged to be remote.

22:42.760 --> 22:47.000
And the students are kind of guided through making their first substantial open source

22:47.000 --> 22:51.760
contribution and also paid for it, which you know as a student that's kind of like a

22:51.760 --> 22:52.760
dream come true, right?

22:52.760 --> 22:56.520
Like you get to work on open source, you get to be paid and potentially you get to be

22:56.520 --> 23:00.160
mentored by like in the Swifferents for TensorFlow case.

23:00.160 --> 23:05.840
Like you would be mentored by Chris Ladner or yeah, or the AIMA case you would be mentored

23:05.840 --> 23:10.720
by Peter Norvig or for the rest of our for the rest of our project opportunities, right?

23:10.720 --> 23:18.000
We've got autograph and TF function and sort of TensorFlow data sets, you're mentored

23:18.000 --> 23:23.440
directly by the often the people, the engineers who've created those products.

23:23.440 --> 23:30.360
So at least in TensorFlow's case and in the other projects that are sponsored, it's really

23:30.360 --> 23:32.480
a phenomenal opportunity.

23:32.480 --> 23:36.240
And I'm excited to see what the GSOC students build out for Swiffer TensorFlow.

23:36.240 --> 23:38.240
Interesting.

23:38.240 --> 23:44.400
You mentioned something earlier that kind of tied to a question that I had in kind of seeing

23:44.400 --> 23:50.560
the Swiffer TensorFlow announcement, how would a Swiffer TensorFlow kind of play or not

23:50.560 --> 23:52.400
play with a carous?

23:52.400 --> 23:53.400
Yes.

23:53.400 --> 24:00.680
So Swiffer TensorFlow, like you can use carous within it, but for the most part it's like

24:00.680 --> 24:03.000
you can use any Python within Swift.

24:03.000 --> 24:09.840
Yes, but it's recommended that you that you would program using Swift.

24:09.840 --> 24:14.880
But again, the Swift syntax looks very, very similar to carous syntax.

24:14.880 --> 24:20.320
And we can, if folks are interested, we can share some of the Colab notebooks that like

24:20.320 --> 24:23.240
links to it that were shared yesterday, right?

24:23.240 --> 24:26.600
But yeah, I would love to see that.

24:26.600 --> 24:31.080
And another really cool thing about Swift for TensorFlow is that it works in Jupyter

24:31.080 --> 24:34.200
notebooks, it works in Google Colab.

24:34.200 --> 24:38.360
So really the notebook experience, if you're a data scientist, feels very familiar as

24:38.360 --> 24:39.360
well.

24:39.360 --> 24:45.960
And the big kind of announcements in addition to kind of the under the cover stuff was kind

24:45.960 --> 24:54.600
of a rationalization of the APIs, it sounds like it's more than just, hey, curious is the

24:54.600 --> 25:01.880
default and more than even, I don't know the right way to ask this, but we're getting

25:01.880 --> 25:02.880
organized.

25:02.880 --> 25:04.880
Is that what it is?

25:04.880 --> 25:09.840
I think so, or at least that's what it feels like.

25:09.840 --> 25:15.560
So the original, I started using TensorFlow, like not for real substantial work, but just

25:15.560 --> 25:21.000
for, you know, personal projects and prototyping things in 2015 when it was released.

25:21.000 --> 25:26.400
And it was, oh man, like it was, it was not fun, like it was, it was not good at all.

25:26.400 --> 25:31.920
Like TensorFlow, the original version, like you, you had to write so much boilerplate in

25:31.920 --> 25:35.240
order to get something to work.

25:35.240 --> 25:40.920
And then since the initial release, it's just, it had grown just sort of exponentially.

25:40.920 --> 25:47.960
We had this module called Contrib, TF Contrib that was kind of a generalized catch-all for

25:47.960 --> 25:50.320
things that folks wanted to add.

25:50.320 --> 25:55.880
So if you were a grad student and you had created, you know, a collection of loss functions,

25:55.880 --> 26:02.040
you would, you would add those in Contrib or we had TF dot Contrib dot GAN, which was

26:02.040 --> 26:11.600
really sort of beloved, you know, GAN, GAN creation tooling for like a straightforward

26:11.600 --> 26:14.200
path to creating GANs using TensorFlow.

26:14.200 --> 26:18.080
And we'll still have it just migrated to its own repo.

26:18.080 --> 26:23.360
But the, but the sad reality of Contrib was that none of those contributions had a support

26:23.360 --> 26:24.360
plan.

26:24.360 --> 26:28.440
None of them had a defined owner, so like a proxy maintainer.

26:28.440 --> 26:33.680
And a lot of them may be functioned in an earlier version of TensorFlow, but failed to work

26:33.680 --> 26:34.680
later.

26:34.680 --> 26:39.520
And there was no clear way to understand what worked, what didn't, what was a duplicate,

26:39.520 --> 26:45.280
and like how would you migrate from what, like a symbol to the core API.

26:45.280 --> 26:46.880
So that really wasn't defined.

26:46.880 --> 26:53.120
We also had a lot of mathematical operations and sort of more traditional statistics capabilities

26:53.120 --> 26:58.960
that were in various places in the API, but not really kind of structured and grouped together.

26:58.960 --> 27:03.800
And it was, gosh, I don't want to, I don't want to count how many symbols there were,

27:03.800 --> 27:05.360
but there were certainly thousands.

27:05.360 --> 27:11.040
There were thousands of symbols without any clear structure or consistent aiming conventions

27:11.040 --> 27:17.440
and a lot of duplication and a lot of sort of things that kind of failed to work together

27:17.440 --> 27:19.240
nicely.

27:19.240 --> 27:23.560
So now we've taken a more modularized approach.

27:23.560 --> 27:27.960
So all of the statistics and mathematical capabilities have been kind of bracketed

27:27.960 --> 27:31.680
off and turned into something called TF probability.

27:31.680 --> 27:38.560
So if you don't want to use all of the things in TensorFlow, like you don't really care

27:38.560 --> 27:44.680
about, you don't really care about, you know, Keras, or you don't care about some other

27:44.680 --> 27:49.160
portion of the API, you can just use TF probability and you can download it as a

27:49.160 --> 27:53.480
pit package or estimators, you can just download estimators.

27:53.480 --> 28:00.360
So all of these things, we've removed duplication, we've kind of taken all of the symbols that

28:00.360 --> 28:06.200
were just kind of dispersed to the winds and grouped them together in a way that seems

28:06.200 --> 28:15.040
more logical and also applied a lot of sort of well thought out renamed to make things

28:15.040 --> 28:22.400
more consistent and to make the sort of calls feel more intuitive.

28:22.400 --> 28:28.040
But that's kind of the idea is that instead of one monolithic application that you have

28:28.040 --> 28:33.760
to download in its entirety in order to use a single bit of functionality, you just take

28:33.760 --> 28:38.760
the pieces that fit for you in your organization and that helps a lot with the build and deployment

28:38.760 --> 28:39.760
process as well.

28:39.760 --> 28:46.200
I definitely saw that in the discussion yesterday and one of the analogies that kind of popped

28:46.200 --> 28:52.080
up in my head is like Hadoop and not all the negative associations with Hadoop but like

28:52.080 --> 28:56.920
people think of Hadoop as, you know, they think of it as this one thing, MapReduce, right?

28:56.920 --> 29:01.240
When really it was like a storage system and MapReduce and like this whole ecosystem

29:01.240 --> 29:02.240
of stuff.

29:02.240 --> 29:03.240
All of the Apache products.

29:03.240 --> 29:04.240
Yeah.

29:04.240 --> 29:05.240
All of the Apache stuff, right?

29:05.240 --> 29:09.520
And TensorFlow, like I think we're transitioning from the point where you would think of TensorFlow

29:09.520 --> 29:14.000
as this monolithic thing to like, you got an ecosystem, right?

29:14.000 --> 29:19.800
With the probability stuff, which we can talk maybe a little bit more about the TFX stuff,

29:19.800 --> 29:25.000
which I'm really excited about, which is almost its own ecosystem.

29:25.000 --> 29:30.760
So that I think that's an interesting kind of transition and one that, I mean, it's kind

29:30.760 --> 29:36.640
of a, there's a natural point in a large open-source project where you start to see that kind

29:36.640 --> 29:37.640
of thing happen.

29:37.640 --> 29:43.320
Yeah, and it's really interesting to, at least from a developer advocacy perspective,

29:43.320 --> 29:47.680
to see the different communities that build up for each one of those tools.

29:47.680 --> 29:54.360
So for example, we mentioned Swift for TensorFlow and usually the folks who gravitate towards

29:54.360 --> 29:59.880
that camp are very interested in things like algorithmic fusion, they're very interested

29:59.880 --> 30:04.240
in compilers and, you know, sort of, performance, and then you have folks in the TensorFlow

30:04.240 --> 30:12.080
JS community that are, so TensorFlow JS is JavaScript in the browser that are very focused

30:12.080 --> 30:17.840
on creating these sort of artistic experiences often.

30:17.840 --> 30:24.320
Like you would have code pens to do neural drum machines or you would have, you know, really

30:24.320 --> 30:31.120
interesting sort of, you know, like puppy ears placed on people heads that, you know,

30:31.120 --> 30:36.480
would follow you around as you, as you look in your, your phone camera or your browser,

30:36.480 --> 30:43.200
or, you know, even things like, sort of in a more product-centric way, like if you're

30:43.200 --> 30:49.440
a developer who wanted to include a text box in your application, automatically, including

30:49.440 --> 30:51.680
something like sentiment analysis within it.

30:51.680 --> 30:57.040
So as a person is typing, yeah, it would be like, hey, man, you know, your tone's kind of,

30:57.040 --> 31:00.040
your tone's kind of negative there, maybe you should make it a little bit happier or

31:00.040 --> 31:03.880
maybe you should be a little bit, like, change this word to be this other word and that

31:03.880 --> 31:05.200
should be fine.

31:05.200 --> 31:11.160
So those sorts of tools and then TensorFlow Core, you know, you get a lot of academic researchers

31:11.160 --> 31:16.480
sure, but you also get businesses who are really interested in productizing machine learning

31:16.480 --> 31:22.600
and deep learning and then they also are interested in TFX and, yeah, it's really cool to see.

31:22.600 --> 31:23.600
Yeah.

31:23.600 --> 31:26.960
Well, let's maybe talk a little bit about TFX.

31:26.960 --> 31:27.960
Yeah.

31:27.960 --> 31:32.760
So that was one of the things that I was most excited about.

31:32.760 --> 31:34.800
TFX is TensorFlow Extended.

31:34.800 --> 31:35.800
Yes.

31:35.800 --> 31:38.200
And there's a workshop for it today, is it?

31:38.200 --> 31:41.480
And there's a workshop for it today that I will be popping in on.

31:41.480 --> 31:43.800
I'm excited about that.

31:43.800 --> 31:47.040
So my, I've mentioned this to a couple of people.

31:47.040 --> 31:52.200
Yes, I, I've, you know, we've done a series of podcasts on the show about machine learning

31:52.200 --> 31:57.360
infrastructure and I've been writing ebooks about machine learning infrastructure and it's

31:57.360 --> 32:03.600
a space that I'm really excited about and I've looked at TFX in the past and it was hard

32:03.600 --> 32:08.400
to really like wrap my head around it, it's just like this kind of random collection of

32:08.400 --> 32:09.400
tools.

32:09.400 --> 32:10.400
Yeah.

32:10.400 --> 32:16.960
But what was announced yesterday seemed like the beginning of making it more coherent and

32:16.960 --> 32:22.840
kind of like, when you read the TFX like the papers, like it's clearly Google has this

32:22.840 --> 32:28.480
kind of very elaborate sophisticated infrastructure internally.

32:28.480 --> 32:33.240
And I think looking at that and then seeing what was previously available made it even

32:33.240 --> 32:34.240
more visible.

32:34.240 --> 32:37.480
It's like, that's all you're giving me is like, do you have a melody?

32:37.480 --> 32:38.480
Yeah.

32:38.480 --> 32:39.480
And certainly.

32:39.480 --> 32:40.480
Yeah.

32:40.480 --> 32:41.480
Yeah.

32:41.480 --> 32:42.480
Yeah.

32:42.480 --> 32:43.480
Yeah.

32:43.480 --> 32:44.480
Yeah.

32:44.480 --> 32:47.840
So maybe kind of what you, it's your, you know, what's the kind of the top line on TFX

32:47.840 --> 32:50.520
or what are you excited about from that perspective?

32:50.520 --> 32:55.320
I'm so glad that you mentioned TFX because TFX is one of the reasons why I wanted to

32:55.320 --> 32:57.360
come work at Google in the first place.

32:57.360 --> 33:03.000
Like I read the paper and I think I mentioned a little bit before I worked as a data scientist

33:03.000 --> 33:07.880
in the, in the energy industry for a long time and the hard bit like it was, it was always

33:07.880 --> 33:11.960
delightful to create the models, you know, it was cool to have this like crunchy engineering

33:11.960 --> 33:17.000
challenge and to take data and to figure out how to solve it in a way that, in a way that

33:17.000 --> 33:21.680
it sort of made sense and that would, and that would be performant and explainable.

33:21.680 --> 33:24.360
But then the hard part was always, okay, cool.

33:24.360 --> 33:25.600
Now how do I deploy it?

33:25.600 --> 33:27.240
Like how do I get it placed?

33:27.240 --> 33:31.120
How do I have this bottle placed into a format that somebody can use in a software application?

33:31.120 --> 33:34.920
And then once it's released, how do I make sure that it's doing its job and that it's

33:34.920 --> 33:36.240
kept up to date?

33:36.240 --> 33:40.440
And how do I make sure that the incoming data stays consistent with the data that was used

33:40.440 --> 33:41.840
to train the model initially?

33:41.840 --> 33:45.720
And if I want to do online training, how would I go about doing that?

33:45.720 --> 33:51.400
So all of these like really interesting questions where the model was just like this tiny piece

33:51.400 --> 33:59.560
of this big, big overall, like sort of continuous integration, continuous deployment strategy,

33:59.560 --> 34:01.600
like DevOps for data science.

34:01.600 --> 34:13.040
And the, and TFX, the paper sort of gave, it sort of elaborated on a strategy that Google

34:13.040 --> 34:17.840
has been using successfully over the past 10 years to do that effectively.

34:17.840 --> 34:23.120
Because most businesses they create custom glue code, they, they don't really, they might

34:23.120 --> 34:29.560
have a component of DevOps for their machine learning models.

34:29.560 --> 34:37.320
But a lot of it might be manual, so not necessarily an example would be if your model falls

34:37.320 --> 34:42.520
below a given accuracy for a stent of time.

34:42.520 --> 34:43.520
How would you?

34:43.520 --> 34:49.440
Yeah, you know, like, or it's like, and if you're at the other, the other piece, right,

34:49.440 --> 34:54.680
is that data scientists, they're probably creating their model in Python or R. In order

34:54.680 --> 35:01.080
to get it integrated with a software application, that model, at least where I worked previously,

35:01.080 --> 35:06.560
it would have to be refactored into something like Java or C++, which means that you're,

35:06.560 --> 35:12.920
you know, usually your accuracy goes down, like, it just, it's, it's not a good situation.

35:12.920 --> 35:17.440
And then if your model needs to be updated, how are you going to take that C++ and Java,

35:17.440 --> 35:21.360
give it back to your data science team and be like, well, fix it now.

35:21.360 --> 35:25.240
You know, there's, there's no straightforward path to do that.

35:25.240 --> 35:30.200
With TensorFlow, you know, it's, it's the same framework for everything.

35:30.200 --> 35:32.480
So the data scientists can create it.

35:32.480 --> 35:36.720
It can be deployed by your, you know, your DevOps team. It can be maintained by them as

35:36.720 --> 35:37.720
well.

35:37.720 --> 35:41.320
And then if it needs to be modified, it's the same TensorFlow code that your data scientists

35:41.320 --> 35:43.080
know how to use.

35:43.080 --> 35:48.880
So TFX, you're right in that when it was announced last year, they only had like two little

35:48.880 --> 35:57.040
components of this, of this sort of toolbox of things, the date, everything from data validation,

35:57.040 --> 36:02.440
which allows you to do those data checks that I was mentioning before, transformation.

36:02.440 --> 36:08.960
So TFT, TensorFlow transform, which allows you to do pre-processing on data pipelines,

36:08.960 --> 36:15.520
to make sure that an in full past data processing, to make sure that your statistical distributions

36:15.520 --> 36:19.840
for each feature are similar as they're coming through your pipeline.

36:19.840 --> 36:23.800
Model analysis, which gives you insight into how your model is performing over time.

36:23.800 --> 36:28.200
There's even a GUI to show you sort of metrics associated with that.

36:28.200 --> 36:34.160
Of course, they're serving, and then there's also sort of additional tools that I believe

36:34.160 --> 36:44.120
Clemens mentioned yesterday, collaboration with Cubeflow, but all of these sort of products

36:44.120 --> 36:48.280
that bundled up together, alleviate the need for you and your company to have to write

36:48.280 --> 36:52.520
Blue Code and to have to refactor models.

36:52.520 --> 36:56.720
It gives you the tools to deploy models in a maintainable way.

36:56.720 --> 37:03.160
Yeah, there's also an interesting bit in there about a metadata repository where you're

37:03.160 --> 37:13.440
able to track all of the configs and parameters associated with various data linux and data

37:13.440 --> 37:14.440
problems.

37:14.440 --> 37:15.440
Right.

37:15.440 --> 37:18.120
So you have your experiments, and then when you have a model that you're pushing out to

37:18.120 --> 37:22.840
production, all that information, and there were some use cases presented to your point

37:22.840 --> 37:29.520
that shows how you can use that information to go kind of work backward from a model decision

37:29.520 --> 37:36.280
to what training data impacted that model decision and what the experiments were that kind

37:36.280 --> 37:39.680
of drove to those sets of model parameters and stuff.

37:39.680 --> 37:40.680
Absolutely.

37:40.680 --> 37:44.240
And that's huge in terms of model explainability.

37:44.240 --> 37:48.400
So if you're working in a high impact industry, again, like the oil industry, you had to be

37:48.400 --> 37:53.240
able to explain why you made a decision just in case it was the wrong one, and then also

37:53.240 --> 37:54.240
reproducibility.

37:54.240 --> 37:59.520
So if somebody wants to take your results and try to replicate them on their own data,

37:59.520 --> 38:06.160
or try a different model and see if the performance is, if the performance varies, then having

38:06.160 --> 38:09.920
all of that metadata associated with the input is huge.

38:09.920 --> 38:12.920
Anything else that we've not talked about yet?

38:12.920 --> 38:17.480
You left excited about yesterday or while you knew about it already, but it was still

38:17.480 --> 38:19.440
exciting to hear.

38:19.440 --> 38:23.920
So I'm really, it's not a secret.

38:23.920 --> 38:31.840
Like I am most excited about the community focus for TensorFlow, and I really love, I really

38:31.840 --> 38:38.920
love that 2.0 has been pushing for community involvement and like not making any changes

38:38.920 --> 38:45.920
unless everybody who's using the tool is, you know, is informed and has had the opportunity

38:45.920 --> 38:48.640
to give feedback.

38:48.640 --> 38:54.600
So the original release of TensorFlow felt very much like, you know, delivering tablets

38:54.600 --> 38:56.640
down from a mountain kind of almost.

38:56.640 --> 39:02.680
It's, you know, the code was released and over time, like community could get involved,

39:02.680 --> 39:06.400
but it was really only through this thing called Contrib.

39:06.400 --> 39:10.480
And for the most part, the engineering was done by the engineering team at Google.

39:10.480 --> 39:15.920
And since then, it's, it's developed into more of a partnership between individuals in

39:15.920 --> 39:20.440
the community, but also companies who are using TensorFlow extensively.

39:20.440 --> 39:28.640
And you know, students or application developers or, you know, not just deep learning researchers,

39:28.640 --> 39:32.360
but really people coming from all walks of life who want to incorporate machine learning

39:32.360 --> 39:33.360
into their tools.

39:33.360 --> 39:37.440
I have crunchy problems that machine learning can help solve.

39:37.440 --> 39:44.800
So that is, that is my favorite thing is that TensorFlow 2.0 is all about community.

39:44.800 --> 39:52.720
You mentioned the RFC process and that's a, that's a new process since that's a new,

39:52.720 --> 39:59.520
the RFC process is a new process that was begun, I believe, halfway through last year.

39:59.520 --> 40:03.040
So maybe July, August of last year, okay.

40:03.040 --> 40:09.480
And again, if you, if you want to make a change to the API, anybody can do it.

40:09.480 --> 40:13.760
So if you have an idea that you think would be solid, you just propose it.

40:13.760 --> 40:19.800
The engineering team, both at Google and then also externally reviews it and delivers commentary.

40:19.800 --> 40:24.120
And if everybody agrees, then development work can start.

40:24.120 --> 40:25.120
Awesome.

40:25.120 --> 40:27.920
Well, Paige, thanks so much for taking the time to chat with me.

40:27.920 --> 40:29.240
Thank you so much for having me.

40:29.240 --> 40:30.240
This was fun.

40:30.240 --> 40:31.240
Absolutely.

40:31.240 --> 40:32.240
Absolutely.

40:32.240 --> 40:38.400
All right, everyone, that's our show for today.

40:38.400 --> 40:44.600
For more information on Paige or any of the topics covered in this episode, visit twimlai.com

40:44.600 --> 40:47.560
slash talk slash 242.

40:47.560 --> 40:52.000
Thanks again to the TensorFlow team for their sponsorship of this series.

40:52.000 --> 40:57.480
Make sure you check out the tensorflow 2.0 alpha at tensorflow.org and enter our TensorFlow

40:57.480 --> 41:02.360
Edge giveaway at twimlai.com slash TF giveaway.

41:02.360 --> 41:29.120
As always, thanks so much for listening and catch you next time.

