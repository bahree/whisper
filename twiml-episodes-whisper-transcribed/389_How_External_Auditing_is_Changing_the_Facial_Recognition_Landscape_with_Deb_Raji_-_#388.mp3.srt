1
00:00:00,000 --> 00:00:11,840
All right, everyone. I am on the line with Deb Rajee. Deb is a technology fellow at the AI

2
00:00:11,840 --> 00:00:17,680
now Institute at New York University. Deb, welcome to the Twimal AI podcast.

3
00:00:17,680 --> 00:00:23,520
Hey, thanks for having me. It is great. Yeah, absolutely. I'm really looking forward to this

4
00:00:23,520 --> 00:00:29,280
conversation. We've certainly got a lot to catch up on. Yeah. There's a lot been going on.

5
00:00:29,280 --> 00:00:33,360
It's been exciting couple of weeks. Yeah. It's been an exciting few weeks, absolutely, in the

6
00:00:34,320 --> 00:00:42,080
the field that you work in. But let's maybe start from the beginning and have you share a little

7
00:00:42,080 --> 00:00:48,560
bit about how you came into artificial intelligence and where it all started.

8
00:00:49,360 --> 00:00:56,560
Awesome. Yeah. So I kind of, I guess it all started in university. So I'm Canadian for reference.

9
00:00:56,560 --> 00:01:04,000
And I kind of entered on a whim the engineering program at the University of Toronto.

10
00:01:04,720 --> 00:01:08,640
And that was where I actually learned how to code. So my first semester of my first year was

11
00:01:08,640 --> 00:01:18,000
like my first coding class. And I hadn't encountered that before coming in. So it was just like

12
00:01:18,000 --> 00:01:22,960
interesting, awesome experience. And I kind of just kept doing it and kept getting involved.

13
00:01:22,960 --> 00:01:27,520
And my degree was in robotics engineering. So I got a lot of exposure to the computer vision

14
00:01:27,520 --> 00:01:32,640
space and built some skills there as well. So I spent between my third year and my fourth year,

15
00:01:32,640 --> 00:01:36,720
I had the opportunity to do this, to take a year off and do a year-long internship.

16
00:01:37,280 --> 00:01:41,600
So I did that internship at Clarify, which is this computer vision company in New York,

17
00:01:41,600 --> 00:01:46,400
at New York City. And I was on their applied machine learning team there. And that's really where

18
00:01:46,960 --> 00:01:51,840
I like learned about the machine learning research community. It was like the first time I went

19
00:01:51,840 --> 00:01:58,400
to Neurips, which is the big machine learning conference. And it was sort of the first time that I

20
00:01:58,400 --> 00:02:04,400
kind of identified the issue of bias and the ethical concerns involved in facial recognition.

21
00:02:05,360 --> 00:02:10,400
In the computer vision as a field in general. So while working on models there, I kind of

22
00:02:10,400 --> 00:02:16,800
began to notice that even the research data sets and even some of the data sets we were using as

23
00:02:16,800 --> 00:02:23,120
part of our engineering processes did not have, you know, faces that look like me. Like I was,

24
00:02:23,120 --> 00:02:29,280
I was very aware of the lack of representation that was there. And it got to a point where I kind

25
00:02:29,280 --> 00:02:34,640
of just started complaining about it. And people were like, we don't know what to do. And,

26
00:02:34,640 --> 00:02:40,160
you know, like it's already so hard to collect data at all. How do we think about bias? What do we,

27
00:02:40,160 --> 00:02:45,040
like, you know, we don't understand this problem. And then it became very clear that it was just kind

28
00:02:45,040 --> 00:02:50,160
of this understudied phenomenon. And that was when I started kind of scourging the internet,

29
00:02:50,160 --> 00:02:56,240
trying to identify anyone that was doing the same thing or had noticed the same problem. And that's

30
00:02:56,240 --> 00:03:02,320
how I kind of landed on Joy Blemwini's TED Talk, which she had given, I think probably a year or

31
00:03:02,320 --> 00:03:08,240
so before, I should have given this TED Talk on her experience with attempting to use open-source

32
00:03:08,240 --> 00:03:13,120
facial recognition software and having the technology not identify her face because she was,

33
00:03:13,120 --> 00:03:17,280
you know, darker skinned and having to use a white mask in order to be identified. And that was

34
00:03:17,280 --> 00:03:21,520
pretty much what her TED Talk was about at the time. And I was like, okay, cool. So when I was like,

35
00:03:22,400 --> 00:03:25,360
what does that even mean? Having to use a white mask to...

36
00:03:26,400 --> 00:03:32,800
So she had to use a white mask to have the facial recognition system like identify her or...

37
00:03:32,800 --> 00:03:35,040
Oh, meaning identify that there was a face there?

38
00:03:35,040 --> 00:03:39,760
Identify that there was a face there. Oh, wow. Yeah. So it was pretty, yeah. This was like,

39
00:03:39,760 --> 00:03:45,200
that was sort of the extent of the articulation of the problem at the time. And that was what prompted

40
00:03:45,200 --> 00:03:52,560
her to start the algorithmic justice league project. So I kind of reached out to her like very recently

41
00:03:52,560 --> 00:03:57,280
after I started at clarifying. I was like, hey, you know, I'm noticing this thing. You give a talk

42
00:03:57,280 --> 00:04:03,040
about this thing. Like, can we talk? And it was a very extra email. Like, I was like, already like,

43
00:04:03,040 --> 00:04:08,400
really deep in the woods of like, here's these data sets that we use in computer vision. And like,

44
00:04:08,400 --> 00:04:13,280
here's like the very tiny percentage. Like, the data sets we use are, you know, 80 to like,

45
00:04:13,280 --> 00:04:18,800
95% lighter skinned subjects. So it was like really bad. So I was like, these are the stats.

46
00:04:18,800 --> 00:04:23,120
Like, I've been trying to like get more info. Can you help? And I think she was just kind of like,

47
00:04:23,120 --> 00:04:30,960
wow, this person like cares. So happy to respond. And her response was something like, yeah,

48
00:04:30,960 --> 00:04:36,400
let's talk in a month or something like that. And I was like, okay. So yeah, like a couple months

49
00:04:36,400 --> 00:04:40,640
later, we actually started talking and we started, you know, collaborating and working on some

50
00:04:40,640 --> 00:04:46,160
stuff. So at the time, she was working on her thesis, which was around gender shades. So I helped

51
00:04:46,160 --> 00:04:51,440
her with that. And then, you know, as gender shades sort of, so gender shades came out February

52
00:04:51,440 --> 00:04:56,400
that year. So as gender shades kind of began kind of gaining steam. And like, we understood that it

53
00:04:56,400 --> 00:05:02,000
was a problem that other people could also recognize and empathize with. I kind of was like, okay,

54
00:05:02,000 --> 00:05:06,800
cool. Like, you have enough support that like, I can kind of work with you full time over a summer.

55
00:05:06,800 --> 00:05:11,440
Or it was ended up being a summer and a fall. But we ended up working together on this sort of

56
00:05:11,440 --> 00:05:17,200
follow-up study trying to identify what, what about gender shades made it an effective audit to sort

57
00:05:17,200 --> 00:05:22,880
of characterize and communicate these problems in a way that pressured these companies to sort of

58
00:05:22,880 --> 00:05:28,000
feel cornered to take action. So that was a lot of the follow-up work I did with Joy. And a lot

59
00:05:28,000 --> 00:05:33,440
of my subsequent work is thinking about, you know, how do we actually capture some of these,

60
00:05:33,440 --> 00:05:37,760
like, limitations or these failures at these models experience? And how do we communicate it

61
00:05:37,760 --> 00:05:43,120
to the public, but also to other researchers, to other engineers in a way that actually makes

62
00:05:43,120 --> 00:05:48,400
that limitation super clear. And like, raises concern in a way that prompts people to take action.

63
00:05:49,200 --> 00:05:53,120
So that was a lot of what my journey is. And that's a lot of the work I'm doing today. So, you know,

64
00:05:53,120 --> 00:05:58,720
following that, I started working with Timnett and Meg at Google and we worked on the model

65
00:05:58,720 --> 00:06:05,200
cards project, which was a way of sort of documenting and communicating audit results and ended

66
00:06:05,200 --> 00:06:08,960
up sort of becoming part of the engineering process at Google for machine learning models.

67
00:06:09,760 --> 00:06:13,600
And then, you know, following that, like, the National Institute of Standards and Technology is

68
00:06:13,600 --> 00:06:18,480
sort of taking up some of the findings in our work and the terminology in our work.

69
00:06:18,480 --> 00:06:25,840
So that was, it sort of become a thing, you know, and it's really sort of stem from this desire to,

70
00:06:25,840 --> 00:06:31,200
like, identify the problem in a consistent way and communicate in a consistent way. So that's

71
00:06:31,200 --> 00:06:35,280
kind of the ongoing work I'm doing today at AINow and wherever I end up in the future.

72
00:06:36,000 --> 00:06:42,720
That's awesome. That's awesome. And so you think of the broad area as you've referred to audits

73
00:06:42,720 --> 00:06:49,920
on several occasions. What all is kind of captured in that terminology?

74
00:06:50,640 --> 00:06:57,360
Yeah. So the reason I mentioned, I talk about sort of auditing and, you know,

75
00:06:57,360 --> 00:07:01,600
some of the work I've done with Google will, like, refer to it as, like, internal auditing.

76
00:07:01,600 --> 00:07:06,480
Is this, you know, anchor to the idea that, so especially the work that I do with Joy,

77
00:07:06,480 --> 00:07:12,800
we look at models that are already out there that, like, someone already decided was, like,

78
00:07:12,800 --> 00:07:18,000
sufficient to deploy. It had already passed whatever deployment conditions were already there,

79
00:07:18,000 --> 00:07:22,160
and the person had already sort of, like, thrown it over the fence. So we look at, like,

80
00:07:22,160 --> 00:07:26,640
models that are already built and deployed, and then we try to understand, you know,

81
00:07:26,640 --> 00:07:31,680
how they actually operate within society. How do they actually operate within a deployed context?

82
00:07:31,680 --> 00:07:38,960
So, you know, for gender shades, for example, building a test set where we identify different

83
00:07:38,960 --> 00:07:45,440
populations that could potentially be affected by such a product, represent that, like,

84
00:07:45,440 --> 00:07:49,680
each of these subgroups within a test set, and then evaluate for each of these subgroups and

85
00:07:49,680 --> 00:07:53,680
discuss the results for each of these subgroups. And the disparities between these subgroups

86
00:07:53,680 --> 00:07:58,480
is us trying to sort of simulate discussion around, you know, within society, how can we

87
00:07:58,480 --> 00:08:02,800
anticipate this model that's already, this product that's already out there? How can we

88
00:08:02,800 --> 00:08:07,200
anticipate its performance on these subgroups that we've decided that we want to look at that we

89
00:08:07,200 --> 00:08:12,560
care about? So that's why it's framed as an audit versus just, kind of, like, an assessment or

90
00:08:12,560 --> 00:08:19,600
an evaluation. It's kind of these quantitative tests to see, like, when you've deployed this thing,

91
00:08:19,600 --> 00:08:23,360
and it's already out there, is it actually good enough for these specific groups, these specific

92
00:08:23,360 --> 00:08:27,360
populations that we've decided that we care about, and we want to see, we want to observe the

93
00:08:27,360 --> 00:08:33,680
performance on. And this is a lot of a lot of the innovation of gender shades, too, was not just

94
00:08:33,680 --> 00:08:38,720
looking at subgroups along, you know, one axis of race or gender, but looking at that intersection

95
00:08:38,720 --> 00:08:43,360
of, for this darker female subgroup that we've decided to, to, you know, study the performance

96
00:08:43,360 --> 00:08:49,920
of this deployed system on, you know, how well does that, that model work for this, the subgroup

97
00:08:49,920 --> 00:08:54,480
that's at the intersection of different identities. So that was also kind of an interesting

98
00:08:54,480 --> 00:08:59,920
difference between how gender shades worked versus how other, kind of, assessments had worked

99
00:08:59,920 --> 00:09:04,400
in the past. And then the other sort of element of it, which I alluded to earlier, was this idea of

100
00:09:04,400 --> 00:09:09,920
a, of a user representative test set of, I identify all these different populations that matter,

101
00:09:10,480 --> 00:09:15,280
and it's not about, you know, the fact that, let's say, like, 10% of the population in

102
00:09:15,840 --> 00:09:21,360
Kansas is, like, darker skin. So 10% of my test set is darker skin. It's like, no, there are

103
00:09:21,360 --> 00:09:25,760
darker skin people. So it needs to work for them. So they're going to be equally represented in

104
00:09:25,760 --> 00:09:29,920
the test set so that, you know, their performance matters just as much as the performance of any

105
00:09:29,920 --> 00:09:35,680
other type of user that I care about. Yeah. So we kind of implemented these strategies to really

106
00:09:35,680 --> 00:09:41,840
look at, or allude to understanding better how these models perform, you know, in society,

107
00:09:41,840 --> 00:09:46,160
once they're deployed, once they're already out there. Okay. So we've kind of talked about,

108
00:09:46,160 --> 00:09:49,440
we've talked about gender shades, but we haven't really said what it was, it's this,

109
00:09:49,440 --> 00:09:55,680
this audit, but in particular, you developed this audit set or test set, and then you deployed it

110
00:09:56,400 --> 00:10:01,680
against some of the public facial recognition technologies that were offered by

111
00:10:02,960 --> 00:10:08,240
several of the cloud vendors. And it was, I think there were two different iterations of it,

112
00:10:08,240 --> 00:10:14,400
or releases of it with different, different vendor communities. Yeah. And that was on purpose.

113
00:10:14,400 --> 00:10:21,280
So the first, the first audit was IBM, Facebook Plus, and Microsoft. And it was, so the name of the

114
00:10:21,280 --> 00:10:26,800
test set, by the way, is the pilot, the pilot, the pilot parliament's benchmark, which is like

115
00:10:26,800 --> 00:10:32,800
PPB for short, and the test set where, you know, and if there's a great paper called diversity

116
00:10:32,800 --> 00:10:40,000
and faces where they actually just sort of PPB in comparison to all the other test sets in

117
00:10:40,000 --> 00:10:46,320
facial recognition at the time, or up to date, which is sort of, you can see that PPB is balanced

118
00:10:46,320 --> 00:10:52,640
for gender and also balanced for skin type. So you have like a set of darker images that are

119
00:10:52,640 --> 00:10:58,400
sort of equivalent to the number of lighter images. And in other benchmarks in this space,

120
00:10:58,400 --> 00:11:03,120
you can see that the proportion is, you know, highly skewed towards lighter images and highly skewed

121
00:11:03,120 --> 00:11:09,440
male. So it was sort of the first benchmark that encompassed sort of this balance and enabled this

122
00:11:09,440 --> 00:11:15,920
intersectional testing, which was really sort of the key differentiator between other, like just sort

123
00:11:15,920 --> 00:11:22,160
of the typical facial recognition evaluation process. And then with respect to the companies that

124
00:11:22,160 --> 00:11:28,080
we were looking at, we kind of picked these very specific targets. So the first iteration was

125
00:11:28,080 --> 00:11:33,200
Microsoft, IBM and Facebook Plus, these were, you know, huge vendors in the space. And they were selling

126
00:11:33,200 --> 00:11:40,000
sort of off the shelf facial recognition APIs, so like application program interfaces. So they

127
00:11:40,000 --> 00:11:45,040
would sell pretty much the access to the models, their facial recognition models. And if I'm an,

128
00:11:45,040 --> 00:11:49,600
you know, and I'm an app developer, I can just take that model and send my images to the model

129
00:11:49,600 --> 00:11:54,240
to get a certain set of predictions. So, you know, these models are being integrated in all kinds

130
00:11:54,240 --> 00:12:00,800
of applications through developer clients. So we knew it was like very impactful technology. So

131
00:12:00,800 --> 00:12:07,920
what we did was we evaluated the performance of these different models on our test set and

132
00:12:08,480 --> 00:12:14,240
observed sort of how well the models performed on these different subgroups. So darker female,

133
00:12:14,240 --> 00:12:19,360
lighter female, darker male and lighter male. And the result of that initial gender shake study

134
00:12:19,360 --> 00:12:24,400
was that there was a, you know, almost 30% disparity between the darker female and the lighter male subgroup,

135
00:12:24,400 --> 00:12:30,160
which was really surprising. Like you would never, you know, from my time working on a machine

136
00:12:30,160 --> 00:12:34,880
learning team, I know that you would never sort of deploy anything that's, you know, if it had a

137
00:12:34,880 --> 00:12:39,920
60% accuracy, right, you'd never deploy that system. So it was really surprising to see that

138
00:12:39,920 --> 00:12:44,880
on the darker female subgroup, it was performing at like 60% accuracy. So yeah, that was sort of

139
00:12:44,880 --> 00:12:50,960
the initial shock of the first study. And then the follow-up study was to say like, well, after

140
00:12:50,960 --> 00:12:55,920
that was revealed, there was a very public, it was a very public situation. So a lot of the companies

141
00:12:55,920 --> 00:13:01,520
released statements saying that they acknowledged the issues, a lot of them reproduced the results

142
00:13:01,520 --> 00:13:06,720
and committed to doing better. So, and all of them re-released. So within seven months, all of them

143
00:13:06,720 --> 00:13:11,360
had released new, so they had released new models, they'd retrained models and redeployed them.

144
00:13:11,360 --> 00:13:18,400
So we in the follow-up study sort of tested, well, how well, how much did these original

145
00:13:18,400 --> 00:13:23,760
audited companies actually improve their performance on the benchmark that we've, you know, designed?

146
00:13:24,640 --> 00:13:28,880
But also, you know, the companies that were not evaluated, that were not audited, did they,

147
00:13:28,880 --> 00:13:32,720
in any way, get affected by this? And the response was that, you know, the companies that were

148
00:13:32,720 --> 00:13:38,640
directly audited did make that improvement, but the companies that were not audited, including

149
00:13:38,640 --> 00:13:43,360
Amazon, which is sort of, you know, one of the big players in the space and at the time was

150
00:13:43,360 --> 00:13:48,800
selling their technology to facial recognition. All those companies, including Amazon, did not

151
00:13:48,800 --> 00:13:53,200
still have that disparity of, you know, up to 30 percent between the darker female subgroup

152
00:13:53,200 --> 00:13:57,680
and the lighter male subgroup. So they were still demonstrating that bias that we had initially

153
00:13:57,680 --> 00:14:01,840
identified, which was really alarming, especially for Amazon at the time.

154
00:14:01,840 --> 00:14:08,960
Alarming, but also I think it says a lot of interesting things, right? It says that, you know, with,

155
00:14:08,960 --> 00:14:14,560
you know, with some investment, you can change it, right? But you have to care. But it also says that

156
00:14:14,560 --> 00:14:19,520
external pressure is what makes you care, not just that, I mean, you've got to believe that Amazon

157
00:14:19,520 --> 00:14:24,560
and everyone else that's selling products in the space knew about the original data study and,

158
00:14:25,360 --> 00:14:31,360
you know, could have taken the steps proactively to address the issues, but based on the results of

159
00:14:31,360 --> 00:14:37,680
the second study, that didn't appear to be the case. Yeah, for sure. It also just reveals that if

160
00:14:37,680 --> 00:14:44,000
they're not audited for it, it's very easy for them to ignore. Yeah, like exactly what you said

161
00:14:44,000 --> 00:14:48,960
about external pressure, but also kind of like targeted, targeted pressure to very specific,

162
00:14:49,520 --> 00:14:55,360
to specific companies. Yeah, if they hadn't sort of been called out by name, the probability of

163
00:14:55,360 --> 00:15:01,920
them doing better is not is very low. And you know, in a study after that, even another paper that

164
00:15:01,920 --> 00:15:08,080
we've very, very recently put out called staving face, we look at the different tasks. So, you know,

165
00:15:08,080 --> 00:15:12,800
gender shades is looking at the gender classification tasks. So how well does it do on the binary task of

166
00:15:12,800 --> 00:15:16,560
identifying if it's a male or female? But there's other tasks, you know, there's like a small

167
00:15:16,560 --> 00:15:22,800
detection task, there's the actual face detection, you know, there's age detection, things like that.

168
00:15:22,800 --> 00:15:30,240
And we also evaluated for some of these other tasks using sort of a more a benchmark with a lot

169
00:15:30,240 --> 00:15:35,520
more metadata. And what we found was that the companies that were initially audited, not only did

170
00:15:35,520 --> 00:15:39,840
they, they only, they all only improved on gender classification. So they had, they still had

171
00:15:39,840 --> 00:15:44,720
like large disparities for age classification, for example. So even Microsoft that had been audited

172
00:15:44,720 --> 00:15:49,680
a couple years ago, now they have like, you know, very small disparities between their performance

173
00:15:49,680 --> 00:15:56,000
on darker females and lighter males for gender classification as a task. But for age classification,

174
00:15:56,000 --> 00:16:01,200
for example, they still have like a 30% disparity between the groups. So yeah, you have to be very

175
00:16:01,200 --> 00:16:06,960
specific about which populations and which subgroups you're, you're looking at your evaluating

176
00:16:06,960 --> 00:16:11,200
performance for, but you also have to be very specific about the task and very specific about the

177
00:16:11,200 --> 00:16:18,000
target. And all of this is really more or less a case for like a regulatory regime where like

178
00:16:18,000 --> 00:16:23,840
everyone has to sort of restrict their use of facial recognition in specific ways or get assessed

179
00:16:23,840 --> 00:16:29,120
kind of universally across the industry in very specific ways for very specific tasks that we are

180
00:16:29,120 --> 00:16:34,800
worried about as a society. And also, you know, for very specific populations that we are concerned

181
00:16:34,800 --> 00:16:40,480
about as a society. So yeah, it just kind of reveals how specific you have to be with respect to

182
00:16:40,480 --> 00:16:47,280
how you design these audits. Yeah, I was going to add you answered the question that I was going to

183
00:16:47,280 --> 00:16:55,840
ask, which is around like, well, I maybe you didn't, right? I think what, you know, what you saw is

184
00:16:55,840 --> 00:17:03,680
that folks can kind of engineer systems for the test, like engineer their system for the benchmark.

185
00:17:03,680 --> 00:17:13,280
And it sounds like an, to some extent, what you're saying is that that's what we need to happen

186
00:17:13,280 --> 00:17:21,440
is that we establish the benchmarks broadly and we, you know, through regulation or some other

187
00:17:21,440 --> 00:17:28,400
measure encourage the companies that are offering these technologies to engineer for these tests.

188
00:17:28,400 --> 00:17:32,000
And I guess there's part of me that says, you know, should it be something else? Should we show them

189
00:17:32,000 --> 00:17:36,320
like the error of their ways? And they say, oh, we should have a diverse team and we should kind

190
00:17:36,320 --> 00:17:43,760
of go off and think about all of these things. And yeah, you know, maybe the, you know, can the,

191
00:17:43,760 --> 00:17:49,600
can the test ever be exhaustive? Yeah, that's a great question. I don't think I answered that question.

192
00:17:52,240 --> 00:17:56,080
So that's a great question, because that was something that like plagued me for a long time,

193
00:17:56,080 --> 00:18:00,400
too, where I was like, because the other thing too is not even just with respect to the task,

194
00:18:00,400 --> 00:18:04,160
because I think with respect to the task, and this is what the National Institute of Standards and

195
00:18:04,160 --> 00:18:11,600
Technology is really into is, you know, they identify that the, that the, the, the facial sort of

196
00:18:11,600 --> 00:18:17,040
identification task, which is the ability for me to like identify you as Sam, you know, identify

197
00:18:17,040 --> 00:18:21,360
myself as Deb, or the facial verification task. If there's two images of me to be able to say

198
00:18:21,360 --> 00:18:26,160
that they're the same image, like they see those as the most important sort of pertinent tasks

199
00:18:26,160 --> 00:18:30,240
with respect to facial recognition. So they will sort of focus on that. And they're like, we don't

200
00:18:30,240 --> 00:18:34,560
actually care about these other things happening. So we've already identified the tasks, and they're,

201
00:18:34,560 --> 00:18:38,080
and then they'll say like, oh, you know, these are the groups that we care about. So we've already

202
00:18:38,080 --> 00:18:43,360
identified these groups. But then we, we rose the question. And this was a lot of our, our third

203
00:18:43,360 --> 00:18:48,000
paper, I call it sort of like like an existential crisis paper, because it's kind of, it talks about

204
00:18:48,000 --> 00:18:53,600
some of these like, who exists that we need to talk about. One of which is this idea of, you know,

205
00:18:54,240 --> 00:18:59,680
looking at the intersection of, you know, skin type and gender is just one way to look at it.

206
00:18:59,680 --> 00:19:04,000
There's other contexts in which it's really important to look at age. And how, you know,

207
00:19:04,000 --> 00:19:10,480
how do you identify the intersection of age and gender and and skin type? Like that just gives

208
00:19:10,480 --> 00:19:17,040
you an infinite number of permutations. And the way that we ended up sort of gaining some level

209
00:19:17,040 --> 00:19:23,760
of like peace is to sort of, I guess, reflect on this as limitations of the audit structure of

210
00:19:23,760 --> 00:19:27,760
of an audit in general to say that there are certain things that you can learn from an audit

211
00:19:27,760 --> 00:19:31,200
and certain things that will actually be very difficult to learn from an audit and actually

212
00:19:31,200 --> 00:19:36,480
require different types of evaluations such as pilots, for example, piloting the technology

213
00:19:36,480 --> 00:19:41,760
or even like overall restriction. So this is one of the cases for the idea of a moratorium where

214
00:19:42,480 --> 00:19:46,160
we understand that there's concerns with this technology and it's not just the racial bias,

215
00:19:46,160 --> 00:19:52,000
right? So, you know, when the technology doesn't work in the case of what we've identified

216
00:19:52,000 --> 00:19:54,160
through gender shades, you know, when the technology is less

217
00:19:54,160 --> 00:19:59,600
performing on darker skin individuals, for example, that puts them at higher risk. You know,

218
00:19:59,600 --> 00:20:05,280
if I get misidentified as a criminal at a higher rate than others, that's because of,

219
00:20:06,000 --> 00:20:10,560
you know, the technology not being as functional for me as it is for another person. But then there's

220
00:20:10,560 --> 00:20:16,560
also the situation of, you know, especially in cases where it's difficult to properly assess the

221
00:20:16,560 --> 00:20:21,520
functionality of the system, you know, maybe we should just restrict the use of the system in general.

222
00:20:21,520 --> 00:20:25,360
Maybe we should just take it out of the market as we figure out and we learn these things that we

223
00:20:25,360 --> 00:20:30,480
have these sort of more nuanced conversations and discuss the other sort of facets of concern as

224
00:20:30,480 --> 00:20:35,200
we discuss other issues such as privacy and transparency that exist that we need to have honest

225
00:20:35,200 --> 00:20:41,360
conversations about in addition to like the complexities of that bias situation. So I think that's

226
00:20:41,360 --> 00:20:46,880
sort of been my approach to it is that there are actually very clear limits to these kinds of

227
00:20:46,880 --> 00:20:51,520
audits. We need to be very aware of. There's a lot that we can learn from them, but there's also

228
00:20:51,520 --> 00:20:56,960
limits to what a standard can tell us and what, you know, NIST can actually do and can actually

229
00:20:56,960 --> 00:21:01,760
provide with respect to insight into how these technology, how the technology operates in deployment.

230
00:21:02,480 --> 00:21:05,840
So we need to be very careful about that. We need to do a lot more thinking about that. And while

231
00:21:05,840 --> 00:21:11,680
we're thinking about that, maybe this technology that's like very clearly immature in certain ways

232
00:21:11,680 --> 00:21:20,400
needs to be taken off the market. Well, I definitely want to dig into the kind of the broader

233
00:21:20,400 --> 00:21:27,120
implications of facial recognition technology and the question of moratorium or not. But before we do

234
00:21:27,120 --> 00:21:34,640
that, I want to continue to kind of pull on some threads around your research and the auditing

235
00:21:34,640 --> 00:21:43,040
and you alluded to this in your last statement. But I thought that the savings face paper was

236
00:21:43,040 --> 00:21:50,800
really interesting in that it was essentially saying, you know, this is our third, you know,

237
00:21:50,800 --> 00:21:57,520
auditing paper, you know, our third go at auditing. And it's still hard and we still mess up and

238
00:21:57,520 --> 00:22:03,920
it's probably not enough. And you need to be really careful even, you know, not just trying to

239
00:22:03,920 --> 00:22:09,200
feel the technology, but just auditing it. Yeah. Can you talk a little bit more about what you

240
00:22:09,200 --> 00:22:13,120
found there? Yeah. So that's saving face paper. I said it's sort of like an existential crisis

241
00:22:13,120 --> 00:22:17,840
paper because following gender shades and actionable auditing, we were sort of seeing these gender

242
00:22:17,840 --> 00:22:22,960
shades like audits appearing everywhere. Like everyone was kind of just building their own version

243
00:22:22,960 --> 00:22:28,880
of PPB, which is the benchmark for sort of any kind of task or any kind of situation.

244
00:22:28,880 --> 00:22:35,200
And people were like you were like you mentioned earlier sort of building to the test. So they

245
00:22:35,200 --> 00:22:42,480
were saying, okay, we're going to improve performance on on PPB or whatever PPB shadow we created

246
00:22:42,480 --> 00:22:46,960
for ourselves. And that is going to be sort of the bar. And once we're over that bar, then that

247
00:22:46,960 --> 00:22:53,360
is something sort of significant. And that whole paper was us saying like hold up something like

248
00:22:53,360 --> 00:22:58,960
an audit like gender shades is a demonstration of, you know, some of these clear oversights with

249
00:22:58,960 --> 00:23:03,680
respect to testing. So the fact that, you know, some of these really huge companies, they're deployed

250
00:23:03,680 --> 00:23:10,400
products, you know, failed. So these are, you know, companies that, you know, they have many people

251
00:23:10,400 --> 00:23:14,720
on their teams and no one on the team had sort of tested for this. Previously, that was the

252
00:23:14,720 --> 00:23:20,960
demonstration was to show that lack of oversight in that negligence. So an audit like gender

253
00:23:20,960 --> 00:23:25,680
shades is really a demonstration of that negligence and to point to the fact that there are,

254
00:23:26,320 --> 00:23:30,240
there are very clear gaps in the way that we currently evaluate and assess this technology

255
00:23:30,240 --> 00:23:37,280
before we deploy it. And it's not necessarily the bar. It's not necessarily a high bar to cross.

256
00:23:37,280 --> 00:23:40,720
So it's kind of like if you triple over the bar, that's embarrassing and you should be ashamed of

257
00:23:40,720 --> 00:23:45,360
yourself. But if you like, you know, if you pass the bar, it doesn't actually mean anything.

258
00:23:45,360 --> 00:23:49,200
There's still, that doesn't necessarily mean that there's no more work to do. So the saving

259
00:23:49,200 --> 00:23:53,920
face paper was us to say, it was us saying, you know, just because Microsoft is now doing well

260
00:23:53,920 --> 00:23:58,880
on PPB, it doesn't mean that it's doing well with respect to, you know, age classification. Like

261
00:23:58,880 --> 00:24:02,560
I mentioned, they still have huge disparities there. But that doesn't mean that, you know,

262
00:24:02,560 --> 00:24:06,720
just because IBM is doing okay on PPB now, it doesn't mean that they haven't thought a lot about,

263
00:24:06,720 --> 00:24:10,480
you know, the privacy concerns that came up with their diversity and faces paper and flicker,

264
00:24:11,200 --> 00:24:17,680
you know, just because we're having conversations around, you know, some of these audits being

265
00:24:17,680 --> 00:24:23,440
sort of conditions for use. So, you know, we're seeing a lot of policy conversations of

266
00:24:24,080 --> 00:24:28,000
gender-shaped-out audit being sort of the condition, you know, you have to pass gender-shades in

267
00:24:28,000 --> 00:24:36,640
order for your model to be able to be deployed. And we're like, there are a lot of things that

268
00:24:36,640 --> 00:24:41,600
need to happen before a system, like a facial recognition system gets deployed. For one thing,

269
00:24:41,600 --> 00:24:46,400
like community consultation, so community participation in that decision-making process,

270
00:24:46,400 --> 00:24:51,600
like transparency around the performance of the system, you know, there's, you know, privacy

271
00:24:51,600 --> 00:24:57,440
concerns being addressed. So, there's so many other things involved. It's not the barred across.

272
00:24:57,440 --> 00:25:02,560
So, that was the main takeaway from that work was us saying, you know, there's so many other

273
00:25:02,560 --> 00:25:08,960
elements to this. And, you know, those considerations can be integrated into the way that we assess

274
00:25:08,960 --> 00:25:15,200
our systems, right? So, you know, NIST could, you know, take on these sort of more qualitative aspects

275
00:25:15,200 --> 00:25:19,760
of, you know, reflecting on the assessment of these systems and facilitating some of these

276
00:25:19,760 --> 00:25:26,000
processes required for any kind of deployment. But, you know, we need to recognize that whole,

277
00:25:26,000 --> 00:25:30,880
you know, this is a whole can of worms that is much more complicated than a lot of us actually

278
00:25:30,880 --> 00:25:34,640
understand at the moment. And we need to, you know, while we're having these more nuanced

279
00:25:34,640 --> 00:25:39,680
conversations, take the product off the market or at least, you know, support that moratorium stance

280
00:25:39,680 --> 00:25:45,760
of pressing pause, as we all have this like deeper conversation. The other thing too, that I was

281
00:25:45,760 --> 00:25:49,840
at least personally realizing at the time, and I think Timnit and Joy were kind of going through

282
00:25:49,840 --> 00:25:56,640
this as well, where we were noticing that there were situations where the issue was not that facial

283
00:25:56,640 --> 00:26:02,480
recognition wasn't working. So, it wasn't that it wasn't even that like, you know, the data was

284
00:26:02,480 --> 00:26:07,360
an encrypted property or the privacy within, or the data was a managed property. It wasn't even that,

285
00:26:07,360 --> 00:26:13,920
you know, it wasn't it wasn't working for different subgroups, but it was just that it was being

286
00:26:13,920 --> 00:26:19,360
like actively weaponized by like this authority figure. So, when you think about facial recognition

287
00:26:19,360 --> 00:26:24,880
as a technology, I like to remind people that a face is the equivalent of a fingerprint. It's

288
00:26:24,880 --> 00:26:29,760
an identifiable biometrics. So, you know, I have your face. I can do a lot of things with that,

289
00:26:29,760 --> 00:26:34,400
except, you know, we are so careful with our fingerprint data. There are so many standards around,

290
00:26:34,400 --> 00:26:39,440
you know, how to store that information, how to how much you can centralize that data about,

291
00:26:39,440 --> 00:26:44,400
you know, how many people, but when it comes to faces, there's, you know, no rules. So, people

292
00:26:44,400 --> 00:26:50,800
have these, you know, immense repositories of people's, you know, identifiable biometrics,

293
00:26:50,800 --> 00:26:54,640
all in this sort of like centralized location that can be controlled by this authority figure.

294
00:26:54,640 --> 00:26:58,880
This is very dystopian. I apologize in the current times to like bring up this image, but

295
00:26:58,880 --> 00:27:06,960
I think we've seen, I think we've seen like the Clearview example come to light earlier this year

296
00:27:06,960 --> 00:27:11,440
where I'm sure you're familiar with that. Do you want to share a little bit about, you know,

297
00:27:11,440 --> 00:27:17,120
kind of what you know about that one? Yeah, so Clearview AI was, you know, a group that actually,

298
00:27:17,120 --> 00:27:21,600
you know, was really, it was really great reporting by the New York Times to actually identify

299
00:27:21,600 --> 00:27:25,600
that group and really expose them because they were, they were sort of intent on being sort of

300
00:27:25,600 --> 00:27:31,680
this covert under the radar stealth company for a very long time. But what they did was they looked

301
00:27:33,280 --> 00:27:39,040
they kind of collected social media face data. So they were, they sort of did something that I'm

302
00:27:39,040 --> 00:27:42,800
reflecting on as sort of digital surveillance where they would, you know, if you upload your face

303
00:27:42,800 --> 00:27:47,520
to Instagram, Facebook, they were collecting all of this information and using that.

304
00:27:47,520 --> 00:27:51,120
Like swapping all public pictures, they can get there. Yeah, yeah.

305
00:27:51,120 --> 00:28:00,240
I mean, it was pretty egregious what they were. Yeah, like a cartoon villain type plot. Like,

306
00:28:01,440 --> 00:28:05,360
really awful. Yeah. And they were, and the worst part is that they were actually cooperating

307
00:28:05,360 --> 00:28:08,320
with law enforcement in different ways and pitching to law enforcement and different

308
00:28:08,880 --> 00:28:14,640
government agencies. So they were using that information to like identify either, they were

309
00:28:14,640 --> 00:28:19,200
using it to identify you online or to identify you through, you know, surveillance camera footage

310
00:28:19,200 --> 00:28:24,640
and other sort of sort of terrifying modes of surveillance. So it was kind of a situation where,

311
00:28:24,640 --> 00:28:28,400
you know, they would give that power to any authority figure that could easily abuse it,

312
00:28:28,400 --> 00:28:34,800
easily weaponize it against you. Yeah, I would agree that the law enforcement examples were the

313
00:28:34,800 --> 00:28:42,400
worst in terms of kind of mass potential harm. But there was also a total lack of of governance

314
00:28:42,400 --> 00:28:46,560
where like from what I remember from the New York Times article, like board members would say,

315
00:28:46,560 --> 00:28:52,400
hey, can you find this person for me or something like that? And they would. Yeah. It was, yeah,

316
00:28:52,400 --> 00:28:56,240
there was also a story that I had heard of. I'm not sure how like, true this is someone like,

317
00:28:56,240 --> 00:29:01,440
try to identify like employees trying to identify like personal like ex-girlfriends and stuff. And

318
00:29:01,440 --> 00:29:07,200
it's very hard. I remember reading that as well. Yeah. Yeah. Yeah. I was going to say there are,

319
00:29:07,200 --> 00:29:12,320
like there's, there has been reports in the past that kind of hinted at this kind of technology

320
00:29:12,320 --> 00:29:16,160
where, you know, ice would just show up at people's houses and people wouldn't understand how they

321
00:29:16,160 --> 00:29:22,240
found them. And then it was later revealed that there they were matching sort of facial recognition

322
00:29:22,240 --> 00:29:26,320
data of like, you know, we know that you look like this based off of whatever mugshot or whatever

323
00:29:26,320 --> 00:29:32,000
information we have from your visa or whatever and identifying your Facebook profile or identifying

324
00:29:32,000 --> 00:29:38,000
your your trace online that way. So yeah, it really is this dangerous technology that like empowers

325
00:29:38,000 --> 00:29:44,000
some of these institutional bullies to kind of just barge into people's lives and like really

326
00:29:44,000 --> 00:29:49,760
affect them. So it has these like kind of whimsical situations, but also just like very important

327
00:29:49,760 --> 00:29:56,000
kind of dire consequences as well. And we keep getting pulled into kind of these broader questions.

328
00:29:56,000 --> 00:30:02,320
I'm still very curious about the the auditing thing. One of the questions that occurs for me is

329
00:30:02,320 --> 00:30:15,680
I, well, I'm curious if you if you explored, I'm not sure the kind of the taxonomy of different

330
00:30:15,680 --> 00:30:22,880
stand types of standards, but when I think of standards like the ISO 9001, they're like process

331
00:30:22,880 --> 00:30:27,680
standards as opposed to like checklist standards. I don't know what the proper names for these are,

332
00:30:27,680 --> 00:30:34,000
but you know, strikes me that, you know, the analogy that comes to mind is like the Volkswagen

333
00:30:34,000 --> 00:30:40,080
gaming the emissions standards. Like they, you know, they had the car set up so that when they

334
00:30:40,720 --> 00:30:46,560
learned that they were in, you know, being tested like it, it switched the engine to

335
00:30:46,560 --> 00:30:52,640
more environmentally friendly mode. But then the regular mode was, you know, just doing what it

336
00:30:52,640 --> 00:31:01,200
was doing through. Yeah. And in this case, there's, I don't think there's like, well, is there an

337
00:31:01,200 --> 00:31:08,720
objective state of, you know, a facial, a good facial recognition system or a facial recognition,

338
00:31:08,720 --> 00:31:14,400
separate from that question of the use of facial recognition and should exist and all that kind

339
00:31:14,400 --> 00:31:23,120
of stuff, there's not really an objective, all encompassing measure of, you know, goodness from

340
00:31:24,080 --> 00:31:31,040
diversity perspective, like there's all different things that you might want the system to be

341
00:31:31,040 --> 00:31:38,080
capable of and you have to engineer them. They're not emergent qualities. You have to engineer them

342
00:31:38,080 --> 00:31:46,880
in. Yeah. And so maybe the, you know, have you looked at this idea of like being around the process

343
00:31:46,880 --> 00:31:51,760
as opposed to, you know, checking off boxes at the end of the process? Oh, yeah. Yeah. This is

344
00:31:51,760 --> 00:31:58,000
like a huge part of my current work. So yeah, just to like, because you said a lot of interesting

345
00:31:58,000 --> 00:32:03,040
things in there, like one thing around standards is that you're, you're totally right. I'm kind of

346
00:32:03,040 --> 00:32:07,600
curious as to, because not a lot of people are reading standards. I'm like, I don't know. I'm curious

347
00:32:07,600 --> 00:32:14,800
as to, you know, which standards are you reading, Sam, but in my, yeah, I've been, I've been looking

348
00:32:14,800 --> 00:32:19,520
a lot at this question of standards in facial recognition mostly because it comes up so much in

349
00:32:19,520 --> 00:32:27,200
policy. So a lot of policy bills will sort of either reference the National Institute of Standards

350
00:32:27,200 --> 00:32:33,040
and Technology in the US, which is sort of the governing body around establishing some of these,

351
00:32:33,040 --> 00:32:39,280
like metrics or these, these bars that need to be jumped over by industry players in order for

352
00:32:39,280 --> 00:32:44,560
them to be considered, you know, as potential vendors within, you know, the sort of

353
00:32:45,280 --> 00:32:50,080
space of working with government agencies. So the National Institute of Standards and Technology

354
00:32:50,080 --> 00:32:57,520
is really the key kind of industry indicator of the performance of your system. If you're kind of

355
00:32:58,880 --> 00:33:02,560
hoping to, you know, work with different governments or different official bodies,

356
00:33:02,560 --> 00:33:07,040
and they very recently, like literally within the last year, you know, citing our paper too,

357
00:33:07,040 --> 00:33:12,240
which was very exciting, they only very recently started evaluating performance across different

358
00:33:12,240 --> 00:33:17,040
demographic subgroups. And that just happened literally last year for the first time. So they,

359
00:33:17,040 --> 00:33:23,120
before that, they hadn't incorporated that into their understanding of, you know, assessment

360
00:33:23,120 --> 00:33:27,280
and evaluation. And I think like with the saving face paper, we were actually challenging them to

361
00:33:27,280 --> 00:33:32,640
go even further and to say, there's all of these other considerations within the process of

362
00:33:32,640 --> 00:33:38,560
how official recognition system, quote unquote, works. You know, can you really say it works if it's

363
00:33:38,560 --> 00:33:42,640
violating the privacy of millions of people and there, and there's no consideration for that,

364
00:33:42,640 --> 00:33:48,240
there's no privacy policy included or incorporated or reflected on. Can you say it works if there's

365
00:33:48,240 --> 00:33:53,440
no method of, you know, transparent communication around its deployment and its use case and there's

366
00:33:53,440 --> 00:33:58,960
no clear evidence of like ethical consideration? Like, is that a system that makes sense to even

367
00:33:58,960 --> 00:34:03,600
consider to use? So yeah, some of those questions, some of those more holistic questions,

368
00:34:04,320 --> 00:34:09,680
they're currently like not even in that kind of space of assessment or evaluation. Like at the

369
00:34:09,680 --> 00:34:13,840
moment, I think as far as it goes is kind of thinking about like, how easy is it to use this and

370
00:34:13,840 --> 00:34:18,240
incorporate this into an application? You know, that's kind of the, they're looking at it at a very

371
00:34:18,240 --> 00:34:24,880
sort of product level. So that's like missed, but ISO and IEEE and other groups have actually

372
00:34:24,880 --> 00:34:30,640
thought about facial recognition. Wef is also sort of proposed some ideas around sort of assessing

373
00:34:30,640 --> 00:34:38,000
facial recognition technology. Yeah, so they actually, they sort of put together like a working

374
00:34:38,000 --> 00:34:42,480
group and they put it together white paper on facial recognition assessment and there's other

375
00:34:42,480 --> 00:34:48,320
sort of think tanks that are attempting to kind of build frameworks for the evaluation or the

376
00:34:48,320 --> 00:34:54,480
assessment of this kind of technology because it's kind of this contentious, controversial tool.

377
00:34:55,520 --> 00:35:00,080
And people are trying to identify all the axes of concern and understand what you have to think

378
00:35:00,080 --> 00:35:05,520
about with respect to will it ever be okay to use this tool? So that's why I think our voices are

379
00:35:05,520 --> 00:35:11,520
in there. Yeah. I think the distinction I was trying to get at was one is a set of standards

380
00:35:11,520 --> 00:35:18,640
around the output of the the process. The other is a set of standards around the process itself.

381
00:35:18,640 --> 00:35:25,040
Like, you know, I can envision a standard that says that, you know, a compliant facial recognition

382
00:35:25,040 --> 00:35:32,240
system has to be developed in a company where there's, you know, some kind of mathematical

383
00:35:32,240 --> 00:35:37,440
review board project, you know, process and there's, you know, some percent of diversity on the

384
00:35:37,440 --> 00:35:44,320
team or something, some metric of diversity on the team that's working on it and it needs to,

385
00:35:44,320 --> 00:35:52,320
you know, be the database, the training data set has to have some set of qualities as opposed

386
00:35:52,320 --> 00:35:57,280
to the output, you know, this process through which it's developed. Yeah. So this was, this was

387
00:35:57,280 --> 00:36:02,160
something that we brought up in our saving face papers as well as a huge where a lot of the

388
00:36:02,160 --> 00:36:06,560
current standards, even the ones that are focused on privacy, like the ISO standard is very,

389
00:36:07,360 --> 00:36:11,600
you know, into this idea of like, hey, this is an identifiable biometric. It goes under all of

390
00:36:11,600 --> 00:36:15,680
these things that we have where identifiable biometrics and it can only be stored in this way

391
00:36:15,680 --> 00:36:19,680
and it needs to be encrypted in this way and they'll check the output. They won't check any process

392
00:36:19,680 --> 00:36:24,080
that you, like, they won't, like, the privacy policy of data collection, like, the way that you

393
00:36:24,080 --> 00:36:29,200
collect data. If that was completely unethical, like, they, they just care that the data is encrypted

394
00:36:29,200 --> 00:36:34,240
at the end, right? So that's the definition of privacy and it's very removed from these processes

395
00:36:34,240 --> 00:36:40,160
like you mentioned that if they were mandated would allow for kind of richer measures to be in

396
00:36:40,160 --> 00:36:45,440
place and richer guardrails to be in place. I worked on a paper with colleagues at Google

397
00:36:45,440 --> 00:36:51,600
of called closing the AI accountability gap. I had a funer name for it, but they all refused to

398
00:36:51,600 --> 00:36:59,280
let me. Are you bringing that up because you want to say the name? No, I'm not going to, I'm not

399
00:36:59,280 --> 00:37:04,560
going to, so I wanted, you know, I wrote it with a colleague named Andrew Smart. And I wanted to

400
00:37:04,560 --> 00:37:09,040
name the framework, the smarter framework because his name is smart. So I thought it was really funny.

401
00:37:09,760 --> 00:37:14,560
No one else thought it was funny. So we ended up naming it the smacked air framework, which is kind of,

402
00:37:14,560 --> 00:37:20,480
you know, it doesn't ring off the tongue as well, but Andrew Smart, I think, just didn't want that

403
00:37:20,480 --> 00:37:25,680
attention to like smart of the smarter framework was a little bit too much for him, but effectively,

404
00:37:25,680 --> 00:37:30,960
yeah, with that framework, we talk about sort of these, some of these procedural considerations,

405
00:37:30,960 --> 00:37:36,320
like that framework is pretty much us trying to say like, you know, and us using the approach of

406
00:37:36,320 --> 00:37:41,360
documentation to try to really identify all of these decisions that engineers make. So for

407
00:37:41,360 --> 00:37:46,000
example, you know, there might be someone that would have a facial recognition system where they

408
00:37:46,000 --> 00:37:50,800
would say, oh, this, you know, this system has no bias because we've evaluated it on like a

409
00:37:50,800 --> 00:37:56,240
gender-shaped style audit. You know, we have these different subgroups and according to whatever,

410
00:37:56,240 --> 00:38:01,760
you know, whatever taxonomy or whatever labels that we have, you know, the performance of the

411
00:38:01,760 --> 00:38:08,000
model is equal for group A and group B. So there's no issue. However, you know, with, you know,

412
00:38:08,000 --> 00:38:12,720
some of these, you know, with some of these process-oriented audits that we did at Google,

413
00:38:12,720 --> 00:38:17,440
we saw that, you know, perhaps the way that the data was collected was incredibly unethical,

414
00:38:17,440 --> 00:38:23,040
and that's where a lot of the issues arose. Or perhaps the way that the labels were set up.

415
00:38:23,040 --> 00:38:28,320
So the taxonomy of the labels and the way that, you know, because when you create a computer vision

416
00:38:28,320 --> 00:38:32,560
system, you actually set up the targets for the system, implicitly or explicitly. So you actually

417
00:38:32,560 --> 00:38:38,000
define, you know, this is the objective of what I want my model to do. And here are, you know,

418
00:38:38,000 --> 00:38:41,840
a set, you know, if I wanted to predict between a cat and a dog, I actually give it that label of

419
00:38:41,840 --> 00:38:47,600
cat and dog, and I select the images that represent cat and the images that represent dog. And that

420
00:38:47,600 --> 00:38:52,480
process at the moment is sort of seen very callously as like, oh, this is, you know, I scrape whatever

421
00:38:52,480 --> 00:38:57,520
I get from the internet, and that's what I use. But there's sort of this realization that even some

422
00:38:57,520 --> 00:39:02,240
of these very subtle engineering decisions that we don't like to admit to ourselves are actually

423
00:39:02,240 --> 00:39:07,600
very important. And there's ways that you can articulate, you know, goals for the model that are

424
00:39:07,600 --> 00:39:12,800
implicitly discriminatory, whether or not it performs well on different subgroups. You know,

425
00:39:12,800 --> 00:39:17,760
so that's some of those procedural sort of elements or some of those engineering decisions,

426
00:39:17,760 --> 00:39:22,640
even outside of some of the governance structures that you've mentioned around, has there been an

427
00:39:22,640 --> 00:39:27,280
ethical review word that looked through this? Or some of the questions that you were mentioning

428
00:39:27,280 --> 00:39:32,960
around diversity of the people involved, right? So or even consultation with the community or with

429
00:39:32,960 --> 00:39:37,840
the public. So some of those even some of those governance issues separate from those in the

430
00:39:37,840 --> 00:39:43,200
engineering process, even there's a last there's a loss of accountability. Like one of the most

431
00:39:43,200 --> 00:39:51,120
sort of surprising things for me is how little we understand, especially as, you know, a machine

432
00:39:51,120 --> 00:39:56,000
learning engineer, sort of a typical machine learning engineer, there's not a lot of accountability

433
00:39:56,000 --> 00:40:02,080
currently around, you know, data providence where I get my data from. So I can create a data set

434
00:40:02,080 --> 00:40:07,920
coming from anywhere and there's no sort of accountability with respect to where that data is

435
00:40:07,920 --> 00:40:12,160
collected from and what that actually represents in which world view that's coming from and which,

436
00:40:12,160 --> 00:40:18,960
you know, all these politics to that data source. And there's a great project that actually

437
00:40:18,960 --> 00:40:23,200
happened at AI now called excavating AI. And I've been talking a lot about it because I think it

438
00:40:23,200 --> 00:40:29,280
does a good job discussing sort of the politics of like, you know, which labels that you pick and,

439
00:40:29,280 --> 00:40:34,560
you know, where your data sources are coming from and the ethics of that as part of, you know,

440
00:40:34,560 --> 00:40:39,520
as an integral part of the ethics of the entire development system of the model. Yeah. So that

441
00:40:39,520 --> 00:40:45,600
work was us trying to get people to write some of that stuff down. So at minimum, we can start talking

442
00:40:45,600 --> 00:40:57,040
about it. Yeah. So you've the risk of going on another. It's all good tangent divergence rabbit hole.

443
00:40:57,040 --> 00:41:06,160
You mentioned the decisions that machine learning engineers make, accountability in the engineering

444
00:41:06,160 --> 00:41:15,920
process, reminding me of the recent kind of thread with Jan LaCoon where he essentially, at

445
00:41:15,920 --> 00:41:23,840
least the part that I'm referring to kind of absolve research of any responsibility for bias in AI

446
00:41:23,840 --> 00:41:28,640
and said it's, you know, related to the things that that you just talked about, you know,

447
00:41:28,640 --> 00:41:35,040
engineering process and discipline and, and the like, does that mean that you agree with his take on?

448
00:41:39,360 --> 00:41:49,040
It was bait. I didn't realize I came to be baited like that. No. That was great. No. I think there's

449
00:41:49,040 --> 00:41:55,360
responsibility on all sides. So one of the reasons why I, I personally gravitate towards engineering

450
00:41:56,400 --> 00:42:00,720
decision making and accountability with respect to engineering decision making is because

451
00:42:01,440 --> 00:42:06,480
through documentation is because I understand that sometimes actually engineers do not understand

452
00:42:06,480 --> 00:42:11,280
their sense of responsibility and do not have the resources to support them in communicating about

453
00:42:11,280 --> 00:42:17,440
it. But a lot of the things that we've worked on with respect to, you know, you know, action

454
00:42:17,440 --> 00:42:21,920
closing the AI accountability gap, but also in earlier work, the earlier work on model cards

455
00:42:22,800 --> 00:42:27,040
and, you know, related work in like data sets, data sheets for data sets and other projects.

456
00:42:27,680 --> 00:42:32,240
All of that has been widely used by the research community before anyone else. Like the research

457
00:42:32,240 --> 00:42:38,480
community, especially applied machine learning work, including computer vision and natural

458
00:42:38,480 --> 00:42:44,240
language processing, you know, a lot of that work involves the engineering of these models.

459
00:42:44,240 --> 00:42:49,840
You know, and I recently like rage tweeted about this where it is so strange to me that

460
00:42:49,840 --> 00:42:54,720
you on the concept that because he literally works at an industry lab where they works, they,

461
00:42:54,720 --> 00:43:01,360
a lot of the work that fair does is, you know, a step towards productization and a lot of the

462
00:43:01,360 --> 00:43:05,280
models that they build, especially some of these larger models, all of these industry labs,

463
00:43:05,280 --> 00:43:10,320
a lot of the models that they build are models that other companies and other groups build off of.

464
00:43:10,320 --> 00:43:15,040
So they will sort of build these quote unquote general models, curate the data set required

465
00:43:15,040 --> 00:43:20,960
to train these large immense models that are then kind of fine tuned by different groups using

466
00:43:20,960 --> 00:43:27,200
that model for different purposes. But that idea of building this general model for whatever

467
00:43:27,200 --> 00:43:33,200
purpose is a lot of control. It's a huge engineering decision. It's applied science effectively.

468
00:43:33,200 --> 00:43:38,400
So it's so strange to me that he thinks the best that research process is separate from

469
00:43:38,400 --> 00:43:43,760
sort of the engineering step and the applications of that, yeah, of that work.

470
00:43:44,400 --> 00:43:49,600
The other thing too is, you know, there's a whole separate set of issues connected to research,

471
00:43:49,600 --> 00:43:55,760
I think. The problems that you choose to work on in research, the way that you test systems

472
00:43:55,760 --> 00:44:01,760
and research really sets the precedent for the field in a way that I'm not sure he acknowledges

473
00:44:01,760 --> 00:44:06,400
with his response. He kind of downplays. So, you know, with a specific example that I think

474
00:44:06,400 --> 00:44:11,840
we're thinking of, it was this model that depixelized faces. So, you know, take

475
00:44:11,840 --> 00:44:17,600
face faces and attempt to reconstruct it. And the my main issue with that work or one of the big

476
00:44:17,600 --> 00:44:22,720
issues with that work, there's many concerns. But one issue is that they didn't seem to test

477
00:44:22,720 --> 00:44:28,000
for people of color. There were multiple examples of people depixelizing the faces of people of color

478
00:44:28,000 --> 00:44:33,600
and then those people being reconstructed to look Caucasian. And it was just very clear that because

479
00:44:33,600 --> 00:44:37,520
it's not mentioned at all in the paper and it's not mentioned by the creators until it kind of

480
00:44:37,520 --> 00:44:43,680
blew up on Twitter. You know, the lack of testing for people of color continues to be this

481
00:44:44,640 --> 00:44:49,840
theme of, it's sort of procedural negligence where it's like, why how could you not,

482
00:44:50,480 --> 00:44:56,160
like, people of color exist, you know, how could you not evaluate or assess for this particular

483
00:44:56,160 --> 00:45:00,160
group that is definitely going to be part of the people, the group of people affected by this

484
00:45:00,160 --> 00:45:06,800
tool. So, I think him downplaying the severity of that oversight was kind of is one of the main

485
00:45:06,800 --> 00:45:12,400
reasons I don't agree with him. But also, I think there's, there's multiple layers of issues there.

486
00:45:16,400 --> 00:45:25,200
Yeah. I don't fully get the, I don't fully get why it's so important to some people to

487
00:45:25,200 --> 00:45:35,840
distinguish between algorithms being biased and data sets being biased. Why is that the hill

488
00:45:35,840 --> 00:45:44,240
that we have to die on some hill? No, but I was going to say, like, I think that it comes from a

489
00:45:44,240 --> 00:45:52,320
place of like really not wanting to making sure that people understand that there's not a quick fix

490
00:45:52,320 --> 00:45:57,600
to this. So, you know, we heard for a long time, data is all we need, you know, we just need more

491
00:45:57,600 --> 00:46:03,600
data, we need more diverse data. In which case, I do not have to think about, you know, diversity or

492
00:46:03,600 --> 00:46:08,960
ethics at this moment because I'm just creating a prototype and the next iteration, I can think

493
00:46:08,960 --> 00:46:13,520
about diversity and I can think about bias and I can think about data because this is just a data

494
00:46:13,520 --> 00:46:17,760
issue. But there's been a lot of great work. And I think you just kind of triggered the entire

495
00:46:17,760 --> 00:46:23,280
sort of fairness, machine learning fairness community. It was fascinating to watch.

496
00:46:26,320 --> 00:46:31,920
And we'll link for the, the tweet and threads. I think that would be, you know,

497
00:46:32,400 --> 00:46:42,160
as well as my pixelated. There's one of you. There's one of me. Yeah, Robert Ness did it for me

498
00:46:42,160 --> 00:46:50,480
and gave me hair. Oh, gosh. Someone said it. Someone said the output had me looking like Steven

499
00:46:50,480 --> 00:46:56,560
Seagal. No, actually, no, I said that. Someone says some other someone's TV that actually Steven

500
00:46:56,560 --> 00:47:04,320
Seagal. Oh, my gosh. Yeah. And it's just so silly to me that like, yeah, that it's sort of

501
00:47:04,320 --> 00:47:07,520
something that's discovered on Twitter. In the same way that it was silly to me that, you know,

502
00:47:07,520 --> 00:47:12,560
gender shades is something that is happening so recently. And it's this new discovery of like,

503
00:47:12,560 --> 00:47:16,640
you know, when you test on people of color or women of color, some of these things don't work.

504
00:47:17,120 --> 00:47:22,960
I think that's for me that continues to be sort of this recurring frustration is like, how can

505
00:47:22,960 --> 00:47:27,920
you not like see this group of people that clearly exist and not evaluate before it's on them?

506
00:47:28,800 --> 00:47:36,320
It's a silly example. Yeah, it sounds like you're saying that this, this, it's not the algorithm,

507
00:47:36,320 --> 00:47:42,480
it's the data set is essentially, you know, either, you know, saying it's someone else's problem

508
00:47:42,480 --> 00:47:47,120
or kind of kicking the can down the road. That's why it keeps coming up. Yeah, because people

509
00:47:47,120 --> 00:47:53,040
had designed algorithms. They actually do influence some of these outputs. They do influence like

510
00:47:53,040 --> 00:47:57,760
the fairness of some of these outcomes. And with respect to this particular work, there was

511
00:47:57,760 --> 00:48:02,480
an interesting thread that didn't get as much traction, but there's another thread of

512
00:48:02,480 --> 00:48:08,320
someone that had tried alternate approaches to the problem. And he had actually gotten sort of more

513
00:48:08,320 --> 00:48:14,320
faithful representations reconstructed. And it was because like so his, his represent,

514
00:48:14,320 --> 00:48:18,480
when he reconstructed his faces, they were not all Caucasian looking, they were all white looking.

515
00:48:19,120 --> 00:48:22,640
And he had taken just a different algorithmic approach to the problem.

516
00:48:22,640 --> 00:48:33,200
Well, they'm underlying data set the face HQ data set, but actually cared to. Yeah, or just even

517
00:48:33,200 --> 00:48:38,160
took a different approach and got different results, right? Like it wasn't even, it was more of that

518
00:48:38,160 --> 00:48:43,360
heat. It was not fixing the problem. It was just, he was approaching the problem, understanding

519
00:48:43,360 --> 00:48:47,520
that like black people exist. And we should test for that as well. And if it doesn't work for this

520
00:48:47,520 --> 00:48:52,160
group, and you know, if your black faces end up looking like white people, that is not a functional

521
00:48:52,160 --> 00:48:56,640
product that you can use for black people. Like no black person can use that product. And I think

522
00:48:56,640 --> 00:49:00,880
that for, like because that was already at the forefront of the discussion, some of these like

523
00:49:00,880 --> 00:49:05,360
counter proposals of these other algorithmic methods that were that kind of preserved the black

524
00:49:05,360 --> 00:49:10,400
faces to put it lightly or even other people are to their Asian faces as well that it wasn't

525
00:49:10,400 --> 00:49:17,920
working for Hispanic faces. Yeah, so people that had kind of come in with this use case in mind.

526
00:49:17,920 --> 00:49:21,600
We're sort of discussing and exploring alternative approaches that I kind of saw some of their

527
00:49:21,600 --> 00:49:26,960
preliminary results. And I was like, yeah, that like didn't completely morph the face into like,

528
00:49:26,960 --> 00:49:32,880
you know, Adam Sandler, whoever. So I think like, yeah, there's definitely a role of algorithmic

529
00:49:32,880 --> 00:49:37,280
approaches. And this is like a well study thing. You know, there's, there's so much literature on

530
00:49:37,280 --> 00:49:44,480
this and a lot of people bombarded y'all looking with that literature last night. It's crazy

531
00:49:44,480 --> 00:49:48,320
because I literally feel like this happened a week ago, but I was like, no, that all happened

532
00:49:48,320 --> 00:49:57,280
last night. But yeah, it was kind of good to see some of these papers come out too because so many

533
00:49:57,280 --> 00:50:01,760
people have so many misconceptions about how bias works. Like we often get the responses like,

534
00:50:01,760 --> 00:50:08,320
oh, isn't it just the data? It's like, no, there's so many layers to this. I think it was a good

535
00:50:08,320 --> 00:50:13,840
prompt for people to reopen the discussion of like just how complex that question of bias is

536
00:50:13,840 --> 00:50:20,800
and just how hard fairness is as a problem as a problem space. And how many different factors

537
00:50:20,800 --> 00:50:28,240
kind of like lead to it. Yeah. I said I wanted to come back to the topic of the broader topic of

538
00:50:28,240 --> 00:50:35,200
facial recognition and we probably should do that if we're going to get to it. You know, tell me,

539
00:50:36,560 --> 00:50:41,440
I'm just curious kind of your perspective on that kind of from the beginning. Like does it

540
00:50:41,440 --> 00:50:46,800
start with, I'm imagining it starts with the potential for harm and what those harms are. And

541
00:50:46,800 --> 00:50:52,880
maybe you can kind of talk through what you've seen. Yeah, so just like in terms of broader issues

542
00:50:52,880 --> 00:50:58,640
of facial recognition. Right. And you know, if the question that, you know, we want to put on the

543
00:50:58,640 --> 00:51:04,240
table or talk through right now is like, should facial recognition be on the market? Oh, I see,

544
00:51:04,240 --> 00:51:09,120
yeah. Right. You know, where does the answer to that question start? Yeah. So I think like a lot

545
00:51:09,120 --> 00:51:16,400
of the work that a lot of the work that gender shades did I think was break the myth that facial

546
00:51:16,400 --> 00:51:21,680
recognition worked because I think for a long time the debate was, oh, this system already works,

547
00:51:21,680 --> 00:51:27,520
you know, do we want it to be enabled within society as this surveillance tool as the system of

548
00:51:27,520 --> 00:51:32,800
control, as the system that can be weaponized, you know, debates around what does it mean to be safe,

549
00:51:32,800 --> 00:51:36,480
what does it mean to have a tool that promotes or discourages safety? What does it mean to have

550
00:51:36,480 --> 00:51:41,680
different authority figures in charge of this tool? What does it mean to allow or restrict specific

551
00:51:41,680 --> 00:51:46,560
use cases? So something like gender shades just broke the myth that it worked in the first place.

552
00:51:46,560 --> 00:51:51,920
And I think that was an important myth to break because this is very immature technology. So,

553
00:51:51,920 --> 00:51:57,200
you know, currently, there's a lot of hype around people, you know more than anyone. There's a lot

554
00:51:57,200 --> 00:52:02,000
of hype. There's a lot of hype around machine learning. There's a lot of hype. Yeah, I did a

555
00:52:02,000 --> 00:52:07,440
little bit, a little bit. And it's really important to have some of these audits come in to say like,

556
00:52:07,440 --> 00:52:11,840
wait, actually, it doesn't work for this group of people. Wait, actually, it's really biased

557
00:52:11,840 --> 00:52:18,400
in discriminatory in this particular way. And it opens up the conversation for future reflections

558
00:52:18,400 --> 00:52:24,240
of, wait, not only is it biased, but also there's these privacy issues. Oh, not only is it biased

559
00:52:24,240 --> 00:52:29,200
and are there privacy issues, but there's also very specifically concerning use cases that we need

560
00:52:29,200 --> 00:52:34,960
to pay attention to. And maybe, you know, the benefit is not actually worth the harm. So like, when

561
00:52:34,960 --> 00:52:38,560
we start having these, we start with this place of weight, it doesn't actually work and it's

562
00:52:38,560 --> 00:52:43,280
an immature technology. And we move towards a place of like, oh, wait, there's actually, even if it

563
00:52:43,280 --> 00:52:47,040
did work, there's all of these other concerns. Like I said, when it doesn't work, there's issues,

564
00:52:47,040 --> 00:52:51,680
but there's also issues when it does work. So I'm starting with that place of like, wait, this

565
00:52:51,680 --> 00:52:58,160
is not this magical, you know, functional thing. Just breaks like the rose colored glasses kind

566
00:52:58,160 --> 00:53:02,560
of come off and people are much more comfortable questioning other aspects of the technology. And I

567
00:53:02,560 --> 00:53:08,400
think where we are today is that, you know, Amazon, Microsoft and Facebook, or that Facebook Plus,

568
00:53:08,400 --> 00:53:15,280
Amazon, Microsoft and IBM have all kind of very recently come out publicly to say, there are

569
00:53:15,280 --> 00:53:20,240
clear limitations of this technology. We have been confronted with the facts. But also, you know,

570
00:53:20,240 --> 00:53:26,000
we understand this concern and we now resonate with it in the, in the current context of sort of,

571
00:53:26,800 --> 00:53:30,880
you know, racial injustice that we're seeing in this, in this country. So we understand that

572
00:53:30,880 --> 00:53:35,840
the risks are here and we understand that, you know, this technology is immature and not really

573
00:53:37,520 --> 00:53:42,560
ready for market. So I think that's why that idea of a moratorium of like, let's take it off of

574
00:53:42,560 --> 00:53:47,840
the market while we have these conversations around regulation. So like, you know, proper restriction.

575
00:53:48,560 --> 00:53:53,040
But also disclosure, like, you know, if any kind of agency is using facial recognition,

576
00:53:53,840 --> 00:53:59,920
how can we empower sort of members of society and the community to like be part of that process

577
00:53:59,920 --> 00:54:03,520
and to understand when facial recognition is being used on them? Because right now,

578
00:54:04,240 --> 00:54:11,040
we don't know the extent to which, you know, Georgetown's 2017 work looking at sort of the

579
00:54:11,040 --> 00:54:17,040
prevalence of facial recognition to use in the US, you know, by police, by law enforcement specifically,

580
00:54:17,040 --> 00:54:22,000
we don't even know about immigration and other groups. So, you know, that was so shocking to so

581
00:54:22,000 --> 00:54:26,240
many people that no one had any idea because it's a technology that someone can use on you without

582
00:54:26,240 --> 00:54:31,760
you having any clue. So, you know, can we actually enforce disclosure? Can we actually enforce,

583
00:54:31,760 --> 00:54:36,960
you know, some of this community participation? But also, you know, can we reconsider what it means

584
00:54:36,960 --> 00:54:42,640
for the technology to work? Is it just accuracy or do we actually have to understand that it has

585
00:54:42,640 --> 00:54:46,560
to work for different subgroups or do we actually have to start having conversations around,

586
00:54:46,560 --> 00:54:52,160
like you mentioned, process and privacy and all these other complex things? I think the work of

587
00:54:52,160 --> 00:54:57,680
exposing, you know, the bias is sort of like a good way to just expose the complexity of the

588
00:54:57,680 --> 00:55:03,920
technology itself and break that like myth of just like a perfectly functional kind of tool.

589
00:55:04,720 --> 00:55:08,400
And then once that myth is sort of broken, then people understand that like, oh, this is a

590
00:55:08,400 --> 00:55:13,440
Pandora's box. This is a very complex system. There's so many dimensions of concern here.

591
00:55:13,440 --> 00:55:18,240
We need to be way more careful than we are currently being around about it. And as a result,

592
00:55:18,240 --> 00:55:24,400
you know, maybe it's not ready for it's not ready to be so widely used, you know, right now it's

593
00:55:24,400 --> 00:55:28,880
used in a lot of places. So, maybe we should just be more careful and should just pull it off the

594
00:55:28,880 --> 00:55:35,040
market as we have these deeper, longer conversations about sort of the complexity of what it is.

595
00:55:35,040 --> 00:55:40,240
Yeah. So, that's sort of how I approach kind of the facial recognition issues in general. Yeah.

596
00:55:40,240 --> 00:55:48,960
Okay. Now, if I parse those recent announcements and remember them correctly, IBM's announcement

597
00:55:48,960 --> 00:55:53,520
was the broadest of the ones that I remember seeing. They said that they were going to stop

598
00:55:53,520 --> 00:55:59,120
developing facial recognition technology for, I don't know if there was a time frame associated

599
00:55:59,120 --> 00:56:09,520
with that or if it was indefinite. Yeah. Amazon, on the other hand, it was a fairly restricted

600
00:56:09,520 --> 00:56:15,120
moratorium on the sale of facial recognition to law enforcement agencies. Yeah.

601
00:56:16,000 --> 00:56:22,880
And I don't recall Microsoft's the scope of their announcement.

602
00:56:22,880 --> 00:56:27,360
Your announcement. It was very vague. It was very vague on purpose, I think.

603
00:56:29,920 --> 00:56:34,480
Yeah, I'm also trying to figure out what Microsoft's announcement was about.

604
00:56:34,480 --> 00:56:41,120
Having read it several times. Yeah. And watch the video. I'm still trying to figure out what

605
00:56:41,120 --> 00:56:48,960
Brad Smith was saying, but yeah, but they all kind of made broad sweeping statements in different

606
00:56:48,960 --> 00:56:56,080
ways. I think most shocking was Amazon because they had been so stubborn, especially with us,

607
00:56:56,080 --> 00:57:04,160
they had sort of, yeah, they had sort of, you know, one of our second paper came out and we had

608
00:57:04,160 --> 00:57:10,640
kind of called out Amazon and shattered their rose colored glasses to like, you know, how functional

609
00:57:10,640 --> 00:57:20,160
their situation was. We had to face a lot of, a lot of, I guess, like, aggressiveness coming from

610
00:57:20,160 --> 00:57:25,920
them. But they posted a blog post. A fairly flawed blog post. Yeah.

611
00:57:25,920 --> 00:57:33,920
Very quick kind of like off the cuff, like, yeah, like, I feel like it was just him being angry

612
00:57:33,920 --> 00:57:38,400
and just wrote a couple paragraphs and put it out. But also, and subsequent interviews too,

613
00:57:38,400 --> 00:57:43,920
you know, I read a lot of articles that coded him. And this is something that came up in

614
00:57:45,200 --> 00:57:50,080
the documentary that we recently, there's a recent documentary that I'll give me just this

615
00:57:50,080 --> 00:57:56,480
week sort of released, coded bias. And there's a scene of like me, Joanne Timnett, who's the co-author

616
00:57:56,480 --> 00:58:02,560
for Gender Shade. And we were sort of sitting there talking about this blog post. And I remember

617
00:58:02,560 --> 00:58:07,040
one of the things that struck me about that conversation that always remembers like us, we were like,

618
00:58:07,040 --> 00:58:12,240
we worked for like months, we wrote this paper, it passed peer review, all of these things happened

619
00:58:12,240 --> 00:58:16,480
in order to validate our results. We had like, you know, so many supplementaries that we like,

620
00:58:16,480 --> 00:58:20,800
because we wanted to make sure that our results were sort of validated and could stand up. And

621
00:58:20,800 --> 00:58:26,640
this guy writes like a overnight, like, blog post with zero citations. And like, you know, in the

622
00:58:26,640 --> 00:58:31,040
press articles, they're quoting these things as if they're like equivalent sort of rebuttals. And

623
00:58:31,040 --> 00:58:38,400
we're like, what? But do you have a sense for? So yeah, that was that was my frustration with Amazon.

624
00:58:38,400 --> 00:58:45,200
But they, I think you have a sense for what drove, you know, them to respond in that way. I mean,

625
00:58:45,200 --> 00:58:50,160
IBM and Microsoft and others were kind of confronted with the same realities. And I said, okay,

626
00:58:50,160 --> 00:58:53,920
yeah, this is probably pretty bad. Let me, you know, let's do something about that. But Amazon

627
00:58:54,720 --> 00:59:02,640
resisted that. Yeah, I suspect, I suspect this happened for a couple of reasons. One being that

628
00:59:02,640 --> 00:59:07,840
ACLU, so I'm not, you know, I'm not privita exactly what's going on internally at Amazon. But

629
00:59:07,840 --> 00:59:14,560
HTMLU had a couple months before the summer before our paper came out, released a couple reports

630
00:59:14,560 --> 00:59:19,120
that they had found of Amazon attempting to pitch at the time. They were in the process of

631
00:59:19,120 --> 00:59:23,680
trying to pitch their technology to ICE to, you know, different intelligence agencies to different

632
00:59:23,680 --> 00:59:30,080
law enforcement agencies. Also, you know, Amazon through their ring product. So ring is sort of the

633
00:59:30,080 --> 00:59:34,800
smart doorbell product. Sports are available in storebell where they'll sort of monitor your

634
00:59:34,800 --> 00:59:40,560
porch. They were trying, they were thinking of the idea of implementing sort of facial recognition

635
00:59:40,560 --> 00:59:45,520
and Amazon recognition to help process the footage from these ring products. And they had a lot of

636
00:59:45,520 --> 00:59:51,280
partnerships with, you know, thousands to the order of over 3000, I think police police departments

637
00:59:51,280 --> 00:59:57,600
at the time. So for them, it was sort of at the cost of this very like promising economic

638
00:59:57,600 --> 01:00:02,320
opportunity. We had kind of just like clip their wings a little bit by revealing the fact that

639
01:00:02,320 --> 01:00:07,680
their technology fell short for for people of color, especially, you know, darker darker skin

640
01:00:07,680 --> 01:00:14,160
women. So by just demonstrating and questioning the functionality of that product, we really sort

641
01:00:14,160 --> 01:00:20,800
of threw the whole, threw the whole product into sort of this period of like people really truly

642
01:00:20,800 --> 01:00:25,360
questioning and revisiting like does this thing actually work? Oh, but also privacy. Oh, but also

643
01:00:25,360 --> 01:00:29,440
surveillance. And like it kind of just was like the, it's always the tip of the iceberg to this

644
01:00:29,440 --> 01:00:33,840
larger conversation. And I think they understood that because that had, that's what had happened

645
01:00:33,840 --> 01:00:39,280
for Microsoft and for IBM. So I think that was why they were super defensive initially.

646
01:00:40,000 --> 01:00:44,720
But thankfully, you know, the research community really came out to support us. They wrote this

647
01:00:45,280 --> 01:00:50,080
public letter. There was a lot of press around that letter where they had kind of just refuted all

648
01:00:50,080 --> 01:00:55,360
the like things that Matt Woodson said in his blog post, but also, you know, other statements

649
01:00:55,360 --> 01:01:02,400
by Amazon later on. And it kind of ended up in a place where they conceded that we needed policy.

650
01:01:02,400 --> 01:01:06,880
They sort of kind of made this appeal of like, okay, so regulate us, which is a similar appeal

651
01:01:06,880 --> 01:01:12,080
to what Microsoft had said. And the difference between kind of those earlier appeals, you know,

652
01:01:12,080 --> 01:01:18,240
right at the publications of our paper, there's always this response of, oh, we support regulation

653
01:01:18,240 --> 01:01:21,840
in facial recognition. You know, this is what Amazon said. This is what IBM said in St.

654
01:01:21,840 --> 01:01:27,360
Microsoft early on. And the difference between that stance and the current stance is that I think

655
01:01:27,360 --> 01:01:34,240
currently, they now understand or they're now sort of pressured to understand that, you know,

656
01:01:34,240 --> 01:01:39,200
you can't just say that you support facial recognition policy and that you acknowledge the

657
01:01:39,200 --> 01:01:45,120
concerns without while also having the product on the market. So, you know, if you say that you

658
01:01:45,120 --> 01:01:49,840
care about the concerns that have been brought up around your technology and that you support the

659
01:01:49,840 --> 01:01:55,200
development of policy and regulation for facial recognition, you can't keep selling, you know,

660
01:01:55,200 --> 01:02:00,080
that technology at the same time. So, you know, the recent announcements around we are no longer

661
01:02:00,080 --> 01:02:05,680
sort of selling this technology, we're no longer allowing it to be used. It's a step forward to

662
01:02:05,680 --> 01:02:11,280
say that, you know, we recognize that until there is some greater understanding and that work has

663
01:02:11,280 --> 01:02:16,640
been done, you know, we really should not keep promoting this technology or keep deploying it.

664
01:02:16,640 --> 01:02:26,400
Yeah. Yeah, their response for a long time was, I don't even, I'm trying to remember if they

665
01:02:26,400 --> 01:02:31,920
were saying it was in their terms of service, but or, you know, in general, they were saying that,

666
01:02:31,920 --> 01:02:36,400
you know, no law enforcement agencies were making decisions based on this

667
01:02:37,360 --> 01:02:43,040
recognition law and it was, you know, people were making the decisions. And I'm curious,

668
01:02:43,040 --> 01:02:49,920
you know, what you've seen in terms of kind of the failure mode of that, you know, rationale

669
01:02:49,920 --> 01:02:56,240
and, you know, or, you know, specific examples of, you know, have there been well-publicized

670
01:02:56,240 --> 01:03:03,520
examples of, you know, the use of facial recognition technology and policing for harm.

671
01:03:04,320 --> 01:03:08,800
Oh, for harm? You know, that resulted in harm, like the current state of that.

672
01:03:08,800 --> 01:03:17,040
Yeah, I think most cases resulted some harm. There's a great, there's a great report from Georgetown

673
01:03:17,040 --> 01:03:22,000
law. I think Georgetown law has done a great job tracking the use of facial recognition in law

674
01:03:22,000 --> 01:03:27,200
enforcement. And honestly, just exposing some of the malpractice that happens in that space.

675
01:03:28,400 --> 01:03:33,520
I think one of the, you know, more striking examples is the way that law enforcement and this

676
01:03:33,520 --> 01:03:39,200
was reported on multiple occasions, you know, law enforcement will try to use facial recognition

677
01:03:39,200 --> 01:03:45,760
in order to identify as suspects in video footage, right? So they might, you know, have a crime

678
01:03:45,760 --> 01:03:52,400
happen. I get some footage of the video scene and then attempt to match, you know, faces in their

679
01:03:52,400 --> 01:03:58,000
mugshot database with, you know, whatever they can identify from the footage. Or, you know,

680
01:03:58,000 --> 01:04:03,760
the more kind of scary situations, they'll take, you know, sketch descriptions of a face and try to

681
01:04:03,760 --> 01:04:08,800
match that with mugshots in their database. You know, or other sort of iterations of, you know,

682
01:04:08,800 --> 01:04:14,240
insane, you know, there was one reported case from Georgetown of them photoshopping like a celebrity

683
01:04:14,240 --> 01:04:19,840
face kind of because they were told that the suspect looked like, you know, this particular

684
01:04:19,840 --> 01:04:23,760
celebrity. So they're like, okay, let's see like in our mugshot database, if we can try to match,

685
01:04:23,760 --> 01:04:28,320
you know, this celebrity face, like an arbitrary celebrity space, you know, to, you know,

686
01:04:28,320 --> 01:04:33,440
somewhat in our database. And, you know, they, they definitely, and this is a huge element of it as

687
01:04:33,440 --> 01:04:39,520
well, you know, the idea of how these, how the technology is actually being used is another layer

688
01:04:39,520 --> 01:04:45,520
of concern to another dimension to think about because Amazon in their rebuttal to us always talks

689
01:04:45,520 --> 01:04:51,120
about the idea of thresholds, you know, we want us, you know, we tell our clients to use a 99,

690
01:04:51,120 --> 01:04:55,760
so certainly with a 95% threshold. And then when the press got worse, they were like, oh,

691
01:04:55,760 --> 01:05:01,920
actually, we met a 99% threshold, a 99% threshold for all groups. And then if you do that,

692
01:05:01,920 --> 01:05:07,520
then there's no more bias. And then there was a great reporter at Gizmodo that actually went to

693
01:05:07,520 --> 01:05:11,440
investigate and talk to one of their police clients. And the police client was like, what is,

694
01:05:11,440 --> 01:05:18,160
what is a threshold? We don't know what that is. And mind you, the default is 80%. This is like

695
01:05:18,160 --> 01:05:23,040
the confidence threshold to make a prediction. So, yeah, there's so many ways in which, you know,

696
01:05:23,040 --> 01:05:28,480
this technology is being incredibly misused by, you know, different police clients in ways that

697
01:05:28,480 --> 01:05:34,000
are problematic that just it's not built to be used as. And then there's sort of the other case

698
01:05:35,200 --> 01:05:40,880
of the technology being explicitly weaponized. So one situation that I think of often is the

699
01:05:40,880 --> 01:05:48,800
Atlantic Tower sort of Plaza is, you know, a rent-controlled rent-stabilized building in Brooklyn. And

700
01:05:48,800 --> 01:05:55,520
the residents of that building recently protested the landlords installation or applications

701
01:05:55,520 --> 01:06:00,800
to install facial recognition in the building. And if you like sort of study the details of that case

702
01:06:00,800 --> 01:06:06,960
and discuss with the tenants, it becomes clear that their concern is not just around sort of the

703
01:06:06,960 --> 01:06:11,840
discriminatory performance of these systems or even, you know, the level of privacy. But the fact

704
01:06:11,840 --> 01:06:16,560
that their landlord is, they know that their landlord is trying to evict certain people because

705
01:06:16,560 --> 01:06:21,040
it's rent-controlled. So he can kind of raise the rent if he kind of gets rid of certain people

706
01:06:21,040 --> 01:06:26,480
and brings a new tenants. But also he kind of has a history with these tenants of, you know,

707
01:06:26,480 --> 01:06:32,720
harassing them in different ways and over-modeling them and attempting to track their movements

708
01:06:32,720 --> 01:06:38,800
and kind of jeopardize, kind of paradoxically jeopardize their safety by virtue of installing

709
01:06:38,800 --> 01:06:44,080
all of this surveillance tech. So they understood that the technology was not for their own safety,

710
01:06:44,080 --> 01:06:48,320
but for the sake of this landlord to be able to kind of weaponize its use to monitor them,

711
01:06:48,320 --> 01:06:53,920
to control their actions, to threaten them and, you know, threaten their safety and their

712
01:06:53,920 --> 01:07:00,320
security, their home security. So it was an interesting situation of like not even the functionality

713
01:07:00,320 --> 01:07:04,720
of facial recognition being part of the conversation, but just the conversation of like, wait,

714
01:07:04,720 --> 01:07:10,000
this is a tool where, you know, one person has a lot of identifiable biometric information about

715
01:07:10,000 --> 01:07:14,080
you and, you know, this authority figure can choose to use that for good or they can choose to

716
01:07:14,080 --> 01:07:21,360
manipulate that and really, you know, weaponize that against you. So yeah, there's sort of those

717
01:07:21,360 --> 01:07:25,680
situations that come up with respect to how facial recognition is used is like the client that

718
01:07:25,680 --> 01:07:30,640
doesn't understand how to use it and messes up in a way that hurts people. The technology not even

719
01:07:30,640 --> 01:07:35,440
working and being sort of mature to actually do its job and then sort of the situation of

720
01:07:36,800 --> 01:07:42,240
the concern being around the authority figure not being very trustworthy and we're in a current

721
01:07:42,240 --> 01:07:46,880
sort of state of society where not a lot of people trust the police right now. There's a lot of

722
01:07:46,880 --> 01:07:52,640
questioning of, you know, the police authority and sort of the validity of some of these,

723
01:07:52,640 --> 01:07:56,720
some of these groups that, some of these authority figures that actually currently have a lot of

724
01:07:56,720 --> 01:08:01,440
the access to facial recognition today. So people are really beginning to question, you know,

725
01:08:01,440 --> 01:08:05,760
what does it mean for us to build these tools and put it in the hands of, you know, certain authority

726
01:08:05,760 --> 01:08:11,120
figures that we're now questioning that we now don't trust this easily. So yeah, lots of very

727
01:08:11,120 --> 01:08:15,040
complicated questions with respect to, you know, how it ends up hurting people in the end.

728
01:08:15,040 --> 01:08:24,960
Yeah, it makes me think of some of the work that Aviva Barhan is doing around trying to shift

729
01:08:24,960 --> 01:08:32,320
the frame of reference from, you know, where is this possibly working to, you know, who is

730
01:08:32,320 --> 01:08:40,480
this possibly harming and using that as the, using that as kind of the core question that we're

731
01:08:40,480 --> 01:08:50,160
asking. Yeah, no, I totally, I totally agree with that shift and I actually, I've been like telling a

732
01:08:50,160 --> 01:08:54,960
lot of people about, you know, because there's this whole community in AI and the machine learning

733
01:08:54,960 --> 01:08:59,440
community about like AI for social good, right? So it's a space where there's sort of this active

734
01:09:00,080 --> 01:09:04,640
imagination or imagining of like positive use cases and it's kind of this exploration of

735
01:09:04,640 --> 01:09:11,120
different positive applications of AI. And I've been sort of pushing my idea, which, you know, it

736
01:09:11,120 --> 01:09:15,920
might get some pickup soon of this idea of AI for social bad, where like, like we need to actually

737
01:09:15,920 --> 01:09:21,680
understand, you know, how it hurts people and we need to like taxonomize, you know, the harms

738
01:09:21,680 --> 01:09:27,840
that come up, we need to, we need to really reflect on some of these downstream consequences and

739
01:09:27,840 --> 01:09:33,440
like build a vocabulary for, you know, all the ways things can go wrong and all the things impact

740
01:09:33,440 --> 01:09:38,000
society. Yeah. Yeah, I root that. I think, you know, part of my question earlier around the

741
01:09:38,000 --> 01:09:46,800
examples is trying to get at that. And, you know, when I think about, um, I guess I, you know,

742
01:09:46,800 --> 01:09:56,000
I haven't, I haven't seen a kind of broadly publicized like pro-publica compass version of

743
01:09:56,000 --> 01:10:01,280
facial recognition and law enforcement, you know, I mean, and I think that that, you know,

744
01:10:01,280 --> 01:10:05,440
we've seen kind of the, the implication I can't tell you how many times I've seen pro-publica

745
01:10:05,440 --> 01:10:10,640
compass, you know, images and slides and things like that. You know, I think we need those kinds of

746
01:10:10,640 --> 01:10:15,120
examples so that people understand what the, what the challenges are. Yeah, applications are,

747
01:10:15,120 --> 01:10:19,280
yeah. I would, I would really recommend to anyone that's interested in that to check out George

748
01:10:19,280 --> 01:10:25,120
Town Law's work on this because I think they're probably the closest to identifying, you know, the

749
01:10:25,120 --> 01:10:31,520
pro-publica, um, the pro-publica article was really compelling because they had against kind of selected

750
01:10:31,520 --> 01:10:38,320
a very specific target and they were able to recreate the situation using very specific examples

751
01:10:38,320 --> 01:10:42,880
and, and pull everything together in a beautiful story. Um, and I think the George Town Law

752
01:10:42,880 --> 01:10:48,800
work with respect to looking at some of these, um, real world applications raises up important

753
01:10:48,800 --> 01:10:53,200
questions and maybe what we need is to anchor that to a narrative that we can resonate with, like,

754
01:10:53,200 --> 01:10:57,680
you know, very specific example or very specific tool that's being used, a very specific police

755
01:10:57,680 --> 01:11:03,360
department. Um, but I would encourage anyone that's, that's curious to understand how facial recognition

756
01:11:03,360 --> 01:11:08,400
is being used in law enforcement to check out that work. Um, but if they're kind of looking for

757
01:11:08,400 --> 01:11:12,400
that story, you're right, that's something that, like, it still work to do with respect to telling

758
01:11:12,400 --> 01:11:17,920
that narrative of how facial recognition kind of interfaces with some of these real world, um,

759
01:11:17,920 --> 01:11:22,560
uh, uh, uh, institutes and structures. The other thing I'm going to say is like, it's not just law

760
01:11:22,560 --> 01:11:28,960
enforcement and this is something that is hard for people to understand because it's never communicated

761
01:11:28,960 --> 01:11:34,320
to us when facial recognition is used on us. But, um, you know, for example, higher view is a company

762
01:11:34,320 --> 01:11:39,200
that uses facial recognition as part of their scoring system. You know, they'll evaluate or assess

763
01:11:39,200 --> 01:11:43,840
for a different emotional cues to different questions and have that be part of your scoring system

764
01:11:43,840 --> 01:11:49,600
to get a particular job. So like in a recorded interview, such as this one, um, or, uh, you know,

765
01:11:49,600 --> 01:11:56,800
there's a lot of cases that I personally encounter an immigration of, you know, different, uh, you know,

766
01:11:56,800 --> 01:12:03,440
a lot of, a lot of the impetus for facial recognition becoming a field was because, you know, um,

767
01:12:04,160 --> 01:12:10,480
in like 1996, there was a push by the government in the US to, to literally funnel millions of

768
01:12:10,480 --> 01:12:15,920
dollars, uh, to sort of, uh, propel the, the community forward, like the research community and kind

769
01:12:15,920 --> 01:12:19,840
of build a research community. And they're incentive at the time, you know, a lot of the sponsors for

770
01:12:19,840 --> 01:12:26,080
that initial effort was Homeland Security, different intelligence agencies. So, um, it's a huge

771
01:12:26,080 --> 01:12:31,680
part of the kind of processes for immigration for verifying identification of, and I think

772
01:12:31,680 --> 01:12:36,240
probably a lot of people as part of the immigration pipeline, you know, when you're at an airport

773
01:12:36,240 --> 01:12:40,560
to match a passport to your face, they go through a process of verification through facial

774
01:12:40,560 --> 01:12:46,080
recognition. Um, and like that's already kind of been rolled out for a while now. So there's a lot

775
01:12:46,080 --> 01:12:52,160
of these interesting, um, use cases that like we actually experienced, but we're kind of

776
01:12:52,160 --> 01:12:56,240
not necessarily registering as like, oh, this is actually facial recognition happening to me at

777
01:12:56,240 --> 01:13:02,880
the airport to verify my identity right now. Um, but yeah, like immigration is another space where

778
01:13:02,880 --> 01:13:07,840
this is very prevalent. Um, there's also, you know, situations and education of like monitoring

779
01:13:07,840 --> 01:13:14,960
people and, um, uh, kind of student security systems. So yeah, there's definitely a lot of,

780
01:13:15,600 --> 01:13:21,360
uh, a lot more applications and it's a lot more prevalent than people assume. Um, and the other

781
01:13:21,360 --> 01:13:25,280
thing I will kind of mention, uh, and it's connected to immigration and connected to law enforcement,

782
01:13:25,280 --> 01:13:31,280
but the idea of sort of digital surveillance of, you know, I post my face on my Twitter profile,

783
01:13:31,280 --> 01:13:35,440
and I also post it on, you know, Facebook or whatever. And because of that, people can kind of

784
01:13:35,440 --> 01:13:40,880
connect these different accounts in different ways or find me, even if I change my name completely,

785
01:13:41,680 --> 01:13:45,120
even if I change my hair completely and change everything about me, because I can't change my,

786
01:13:45,120 --> 01:13:50,240
like, actual facial structure. Uh, yeah. So some of that digital surveillance is something that I

787
01:13:50,240 --> 01:13:55,200
think was very much exposed through clear view of like, oh, wait, uh, you know, when I put this

788
01:13:55,200 --> 01:14:02,080
information out there, um, it's actually very traceable and it can kind of be used against me. Yeah.

789
01:14:02,880 --> 01:14:12,240
Yeah, I think there's still this, um, you know, there's still the, the answer to this, well,

790
01:14:12,960 --> 01:14:18,320
you know, I don't care, you know, the government knows who I am. I haven't done anything wrong.

791
01:14:18,320 --> 01:14:25,920
And I think, you know, my hope is that through, you know, what's happening now in, in terms of,

792
01:14:25,920 --> 01:14:35,760
you know, the kind of increased awareness of the, um, you know, for example, police brutality

793
01:14:35,760 --> 01:14:41,760
that we're seeing in this country. And, you know, maybe we can connect that to, you know,

794
01:14:41,760 --> 01:14:48,960
Abibas, algorithmic injustice and relational ethics and, and, you know, kind of minimizing

795
01:14:48,960 --> 01:14:55,840
potential harm to undervoiced or, you know, communities without voices. And, um,

796
01:14:58,240 --> 01:15:02,160
I'm not sure where I'm going with that. I think I'm expressing frustration. Like, how do you,

797
01:15:02,160 --> 01:15:08,160
how do you, I think that there's, you know, and even, even personally, like, I think, you know,

798
01:15:08,160 --> 01:15:13,440
I thought about it when I got global entry, but I still did it. And I'm like, yeah, yeah, you

799
01:15:13,440 --> 01:15:19,680
know, it's like, it's a convenience in a lot of cases, but, you know, how do we parse through,

800
01:15:19,680 --> 01:15:26,240
like, what's the cost of that convenience and for who and at the, at what level, um, and there's

801
01:15:26,240 --> 01:15:32,880
just so many complex questions and issues here. Like, when I talk to sort of, um, my friends

802
01:15:32,880 --> 01:15:36,160
outside of the space about facial recognition, they'll be like, oh, like this, the thing that,

803
01:15:36,160 --> 01:15:41,520
like, let's me have a Snapchat filter. Like, I want that. Like, I want to be able to have,

804
01:15:41,520 --> 01:15:47,600
you know, facial recognition, identify, you know, my, my, my, um, my key features so that I can

805
01:15:47,600 --> 01:15:51,600
sort of, like, the, like, face landmarks I can put on, like, you know, the buddy ears or whatever.

806
01:15:51,600 --> 01:15:55,360
Like, that's important to me. And I'm like, okay, no one's threatening your Snapchat filter

807
01:15:55,360 --> 01:16:00,800
here. Calm down. And, you know, if anything, if you're someone that sees yourself as this, like,

808
01:16:00,800 --> 01:16:06,880
citizen that, like, you know, doesn't necessarily, um, that isn't guilty of anything. Like, if anything,

809
01:16:06,880 --> 01:16:11,600
that is sort of a reason to care more because, you know, especially if you're a person of color,

810
01:16:11,600 --> 01:16:17,120
I think that, um, there's definitely risk, uh, with respect to just being in these systems,

811
01:16:17,120 --> 01:16:22,080
which a lot of us are already, you know, a vast majority of Americans already in these systems,

812
01:16:22,080 --> 01:16:27,120
already embedded in these systems. Um, you know, some of the test sets, um, coming in from

813
01:16:27,120 --> 01:16:32,720
Homeland Security are coming from like visa images and images that you, uh, you know, and there's

814
01:16:32,720 --> 01:16:37,760
also, you know, recent reports of DMV, you know, driver license images sort of being shared with

815
01:16:37,760 --> 01:16:44,640
ISM being integrated into these mugshot databases, right? So there's no way to tell, you know,

816
01:16:44,640 --> 01:16:48,720
when someone takes their picture, where that picture will land, especially if it's a government

817
01:16:48,720 --> 01:16:53,120
agency taking that picture. And I think that that is something that we should sort of be concerned

818
01:16:53,120 --> 01:16:58,400
about on an individual basis, because, um, especially if you're innocent, because they can kind of

819
01:16:58,400 --> 01:17:04,240
duplicate you in these processes and these, um, these, uh, these, these systems that, you know,

820
01:17:04,240 --> 01:17:10,000
you really have no business sort of, uh, being pulled into. And it, and it encompasses your

821
01:17:10,000 --> 01:17:14,720
life, or could it has the potential to future for future inconveniences that could really, uh,

822
01:17:14,720 --> 01:17:18,800
disrupt your way of life right now. So on an individual basis, I do think there's enough

823
01:17:18,800 --> 01:17:23,760
reason for concern. But if you're like fully like, I have, you know, I, I have enough money for

824
01:17:23,760 --> 01:17:29,360
bail type person, like, I, like, even if I get like misidentified, I can protect myself on any,

825
01:17:29,360 --> 01:17:32,880
because there are people that are like that. Um, and then it's at that moment that you kind of

826
01:17:32,880 --> 01:17:37,840
appeal to, um, okay, well, think about those that do not have that privilege and people that are

827
01:17:37,840 --> 01:17:43,120
sort of increasingly vulnerable because of this technology. And I think that's why, um, a lot of

828
01:17:43,120 --> 01:17:49,600
the conversations around restriction, you know, I appreciate the companies for publicly sort of

829
01:17:49,600 --> 01:17:55,360
denouncing the use of the technology in a way that, um, um, you know, has been sort of well

830
01:17:55,360 --> 01:18:00,000
received by the public and sort of well understood by the public. But I, I don't think I'm going to

831
01:18:00,000 --> 01:18:04,080
depend on these companies to go all the way. Like, I don't think, I don't think they're going to

832
01:18:04,080 --> 01:18:08,960
shoot themselves so in the foot that, um, you know, it actually achieves some of the protections

833
01:18:08,960 --> 01:18:14,080
that need to happen for the sake of some of these marginalized groups and communities

834
01:18:14,080 --> 01:18:19,360
most at risk. Um, I think that, um, you know, at best or at worst, what we can expect from these

835
01:18:19,360 --> 01:18:25,360
companies is to, you know, uh, perhaps protect the majority of their, their clients or their users,

836
01:18:25,360 --> 01:18:28,560
because they're, their users and the concerns of their, well, that's how we got here in the first

837
01:18:28,560 --> 01:18:33,600
place. Yeah. Um, like, that's like a best, like, the people that would buy, you know, like,

838
01:18:33,600 --> 01:18:36,960
whatever product they're selling, like, maybe they'll think about them. But for them to think about

839
01:18:36,960 --> 01:18:42,480
people that, you know, might never have interacted with Amazon in any way, um, are completely out

840
01:18:42,480 --> 01:18:46,800
of their scope of concern. You know, how do we actually support and protect them? And that's when

841
01:18:46,800 --> 01:18:52,240
we start thinking about how important regulation is and how important it is to restrict its use, um,

842
01:18:52,240 --> 01:18:57,920
in very particularly sort of like, uh, predatory cases against some of these very vulnerable groups.

843
01:18:57,920 --> 01:19:03,200
So I think that's the impetus for that direction is like, it really shouldn't depend on companies

844
01:19:03,200 --> 01:19:06,960
sort of when, the other thing too, is like, a lot of the companies that are most notorious.

845
01:19:07,840 --> 01:19:12,640
It sounds awful, but like, you know, uh, Palantir, NEC, like, you know, some of these names,

846
01:19:12,640 --> 01:19:18,080
people don't even know because they're, they're not consumer-facing. Even if Palantir was to,

847
01:19:18,080 --> 01:19:22,160
to make a statement, I'm not sure how many, um, you know, how many people like, you know,

848
01:19:22,160 --> 01:19:26,080
within my family or people that aren't in this space would like recognize that name, NEC,

849
01:19:26,080 --> 01:19:30,880
a lot of people don't understand what that company is. Um, but they're really a large,

850
01:19:30,880 --> 01:19:34,080
a large part of this market. They're actually a lot of the players and a lot of these small

851
01:19:34,080 --> 01:19:38,400
startups too that I don't even, you know, I haven't even been able to identify as vendors because

852
01:19:38,400 --> 01:19:44,160
they're so, it's such an opaque system. It's such an opaque process. Um, you know, how can we actually

853
01:19:44,160 --> 01:19:49,680
force like, uh, government agencies to disclose and identify some of these groups so that we can

854
01:19:49,680 --> 01:19:54,480
begin to like question the functionality of their systems and audit them. Um, but also, you know,

855
01:19:55,200 --> 01:19:59,360
how can we protect the people that are currently affected by these companies that will very likely

856
01:19:59,360 --> 01:20:04,320
not change because facial recognition is their main product. Um, unlike with IBM, um, uh,

857
01:20:04,320 --> 01:20:09,040
Microsoft and Amazon, where they have other sources of revenue. Um, so, you know, how do we,

858
01:20:09,040 --> 01:20:13,280
you know, rather than focusing on convincing those companies to change, how can we actually

859
01:20:13,920 --> 01:20:18,000
push the regulation that can protect everybody that can protect everyone, especially those at risk?

860
01:20:18,880 --> 01:20:23,120
So yeah, I think that there's still a lot of incentive, even though there's sort of

861
01:20:24,000 --> 01:20:29,040
potential for personal apathy, you know, uh, there's still a lot of incentive to work on this

862
01:20:29,040 --> 01:20:33,840
and to think about sort of these broader implications and, uh, push for regulation specifically.

863
01:20:35,760 --> 01:20:43,520
Yeah. Cool. Well, Deb, thanks so much for taking the time. Yeah, I know, with you covered a lot of

864
01:20:43,520 --> 01:20:49,040
ground. You look like you're like, every time I talk to someone about this, you're just like,

865
01:20:49,040 --> 01:20:58,800
what just happened? Don't talk about facial recognition at the end of anybody. Awesome. Don't have to hear

866
01:20:58,800 --> 01:21:04,800
about it, but okay. I think I'm looking at the, uh, I'm looking at the kind of recording time

867
01:21:04,800 --> 01:21:10,240
right here. And I'm like, we could go for another hour. That's very complicated.

868
01:21:11,840 --> 01:21:18,800
And end up in a similar place, actually. Yeah. Unless we're rolling up sleeves and like, you know,

869
01:21:18,800 --> 01:21:23,760
coming up with a textonomy or something like that. And it's moving so quickly too, right? Like,

870
01:21:23,760 --> 01:21:27,280
things are happening every day, something happened last week, something. How many of the things

871
01:21:27,280 --> 01:21:31,920
that we talked about just happened this week? Like, and we didn't, yeah, I was going to ask you about

872
01:21:31,920 --> 01:21:38,560
the, uh, did you see the, was it one of the Springer publications? Yeah, I was trying to detect

873
01:21:38,560 --> 01:21:44,880
criminality out of off of facial images. Yeah. That was just a couple of days ago. I saw,

874
01:21:44,880 --> 01:21:48,560
at least that's when I saw the tweet. Yeah, I've been telling people like, I wonder what's going to happen

875
01:21:48,560 --> 01:21:55,840
tomorrow? Like, the summer that I was trying to write the saving face paper, we, we wrote a section

876
01:21:55,840 --> 01:22:00,720
on sort of a policy developments and facial recognition. And we were keeping track of all the state

877
01:22:00,720 --> 01:22:05,040
developments, all the bills and all the federal bills coming out literally every week. I would have

878
01:22:05,040 --> 01:22:09,680
to rewrite the section just because it was just like constant stream of like bills coming out. Yeah,

879
01:22:09,680 --> 01:22:15,520
it's crazy. The, the activity happening and because Amazon set this arbitrary one-year deadline,

880
01:22:15,520 --> 01:22:20,400
people are going into hyperdrive and things are moving even quicker. So we'll see what happens,

881
01:22:20,400 --> 01:22:26,400
but definitely an active space. There's a lot to talk about. Yeah. Awesome. Well, thanks so much, Deb.

882
01:22:26,400 --> 01:22:41,120
Yeah, for sure. Thanks for having me. Okay. All right. Take care.

