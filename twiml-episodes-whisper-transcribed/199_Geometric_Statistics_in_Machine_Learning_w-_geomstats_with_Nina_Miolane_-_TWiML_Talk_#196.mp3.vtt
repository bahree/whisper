WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.160
I'm your host Sam Charrington.

00:32.160 --> 00:38.840
In this episode, we're joined by Nina Mielehn, researcher and lecturer at Stanford University.

00:38.840 --> 00:43.080
Nina and I recently spoke about her work in the field of geometric statistics and machine

00:43.080 --> 00:44.400
learning.

00:44.400 --> 00:49.240
Specifically, we discussed the application of reminion geometry, which is the study of

00:49.240 --> 00:52.360
curve surfaces to ML.

00:52.360 --> 00:57.120
Reminion geometry can be helpful in building machine learning models in a number of situations,

00:57.120 --> 01:01.840
including in computational anatomy and medicine, where it helps Nina create models of organs

01:01.840 --> 01:03.680
like the brain and heart.

01:03.680 --> 01:08.160
In our discussion, we review the differences between reminion and euclidean geometry and

01:08.160 --> 01:13.320
theory and practice and discuss several examples from Nina's research, which is a Python

01:13.320 --> 01:21.760
package that simplifies computations and statistics on manifolds with geometric structures.

01:21.760 --> 01:25.720
Before we dive into the conversation, I'd like to send a huge thanks to our friends at

01:25.720 --> 01:29.160
IBM for their sponsorship of this episode.

01:29.160 --> 01:35.160
Interested in exploring code patterns, leveraging multiple technologies, including ML and AI,

01:35.160 --> 01:37.280
then check out IBM Developer.

01:37.280 --> 01:43.200
With more than 100 open source programs, a library of knowledge resources, developer advocates

01:43.200 --> 01:49.560
ready to help and a global community of developers, what in the world will you create?

01:49.560 --> 01:55.960
Dive in at IBM.biz slash ML AI podcast, and be sure to let them know that Swim will

01:55.960 --> 01:57.360
send you.

01:57.360 --> 02:01.960
And now on to the show.

02:01.960 --> 02:04.760
Alright everyone, I am on the line with Nina Mielehn.

02:04.760 --> 02:09.840
Nina is a researcher and lecturer in machine learning for computational anatomy at Stanford

02:09.840 --> 02:10.840
University.

02:10.840 --> 02:13.600
Nina, welcome to this week in machine learning and AI.

02:13.600 --> 02:17.840
Hello, hello Sam, thanks for calling me today and hello everyone.

02:17.840 --> 02:19.600
Absolutely, absolutely.

02:19.600 --> 02:23.840
So, let's just start with a little bit of background.

02:23.840 --> 02:28.360
And in particular, you started out in theoretical physics.

02:28.360 --> 02:32.920
How did you make your way from general relativity to machine learning?

02:32.920 --> 02:39.240
Yes, that's correct, I started with theoretical physics, I did my master in theoretical physics

02:39.240 --> 02:45.680
and mathematical physics, which I think it might be an original background for someone

02:45.680 --> 02:48.000
in machine learning these days.

02:48.000 --> 02:54.680
And so yeah, initially I was looking at different mathematical models of reality, specifically

02:54.680 --> 03:02.360
at the infinitesimally small and the very big, so I was looking at models of particles

03:02.360 --> 03:08.520
elementary particles and their collisions at very high energies and also at models of

03:08.520 --> 03:13.800
the very big, like the stars, the galaxies, space, time and all these things.

03:13.800 --> 03:19.760
And so yes, I had courses like general relativity, black holes, and this was very interesting

03:19.760 --> 03:26.520
because after my master, in fact, I moved to machine learning for the medical imaging field.

03:26.520 --> 03:31.040
And at that time, a lot of people asked me the question like, what does this have to

03:31.040 --> 03:32.760
do with machine learning now?

03:32.760 --> 03:33.760
Why, why are you switching?

03:33.760 --> 03:34.760
What's the link?

03:34.760 --> 03:39.720
I started a PhD in machine learning and how could I do that after a matter in mathematical

03:39.720 --> 03:40.720
physics.

03:40.720 --> 03:48.960
And in fact, both fields are really related and the main link they have is geometry.

03:48.960 --> 03:53.720
So in theoretical physics, when I was looking at space time, in fact, I was looking at

03:53.720 --> 03:55.960
the geometry of space time.

03:55.960 --> 04:01.680
So you might know that Einstein in the early 90s used geometry, actually a particular kind

04:01.680 --> 04:04.840
of geometry, which is called remanion geometry.

04:04.840 --> 04:10.160
And so Einstein used geometry to describe space time and how the geometry of space time

04:10.160 --> 04:14.720
was changing depending on the massive objects that are in space time.

04:14.720 --> 04:20.240
So for example, if you go very close to a black hole, then time will slow down.

04:20.240 --> 04:24.400
And so the geometry of time near the black hole is changed.

04:24.400 --> 04:30.720
So geometry has a lot to do with the mathematical model that you put in theoretical physics.

04:30.720 --> 04:36.080
And then between my master and my PhD, I went to computational anatomy.

04:36.080 --> 04:40.040
So medical imaging, I was looking at organ shapes.

04:40.040 --> 04:45.800
And here again, I got to do a lot of geometry and remanion geometry.

04:45.800 --> 04:51.440
Because in fact, the data, the data in machine learning that I was looking at are geometrical

04:51.440 --> 04:52.440
data.

04:52.440 --> 04:59.400
So they live in a space, in a data space that also has a particular geometry.

04:59.400 --> 05:06.640
And so I was studying the geometry of this data space just in the same way that I was studying

05:06.640 --> 05:10.080
the geometry of space time during my master.

05:10.080 --> 05:15.760
So that's how I got from theoretical physics to machine learning for data belonging to

05:15.760 --> 05:18.520
these geometric spaces.

05:18.520 --> 05:21.160
And then after that, I kind of stayed in machine learning.

05:21.160 --> 05:26.720
So I was a software engineer for a little bit in a machine learning startup.

05:26.720 --> 05:30.200
And now I've come back to academia.

05:30.200 --> 05:35.000
And I'm now a postdoc at Stanford University, as you said it.

05:35.000 --> 05:42.640
And from my PhD to my work in software engineering and my postdoc through these three stages,

05:42.640 --> 05:47.520
three in these three steps, I was looking at data belonging to geometric spaces.

05:47.520 --> 05:51.440
And these data have been mostly organ shapes.

05:51.440 --> 05:54.200
So you said it, I'm in computational anatomy.

05:54.200 --> 05:58.400
So I built computational models of the human body.

05:58.400 --> 06:01.600
And so the data, I have are organ shapes.

06:01.600 --> 06:07.800
During my PhD, I looked at brain shapes, how brain shapes vary when you have a disease.

06:07.800 --> 06:11.920
For example, isomers disease or other forms of dementia.

06:11.920 --> 06:15.480
When I was a software engineer, I looked at hot shapes.

06:15.480 --> 06:21.240
So how the shape of your heart may vary if you have different cardiovascular diseases.

06:21.240 --> 06:24.360
And now back as a postdoc, I do two things.

06:24.360 --> 06:32.840
I look at brain shapes again, but I also look at the shapes of abstract data spaces.

06:32.840 --> 06:38.320
So that's how yeah, I'm kind of merging all the geometric background that I have gathered

06:38.320 --> 06:40.720
to come to the field of machine learning.

06:40.720 --> 06:46.240
Maybe we should start from the beginning and have you explain what rhymanian geometry

06:46.240 --> 06:52.800
is and how is it distinguished from the Euclidean geometry that we tend to think of?

06:52.800 --> 06:54.920
Yes, sure.

06:54.920 --> 07:01.960
So remanian geometry is this theory of mathematics that allowed Einstein to describe the geometry

07:01.960 --> 07:03.520
of space time.

07:03.520 --> 07:08.080
And actually, it's a very powerful mathematical theory, which is why I think Einstein has

07:08.080 --> 07:11.600
used it to describe this very complicated space.

07:11.600 --> 07:14.560
And it's a generalization of Euclidean geometry.

07:14.560 --> 07:18.440
Actually, rhymanian geometry is locally Euclidean.

07:18.440 --> 07:20.160
So let's take an example.

07:20.160 --> 07:26.840
If you consider the earth as the planet, you look at the surface of the earth locally.

07:26.840 --> 07:32.400
So for you and me in a given room, in a building, locally, the geometry is flat.

07:32.400 --> 07:37.920
It looks like we live on a flat to dimensional plane.

07:37.920 --> 07:42.360
But if you look at the global geometry of the earth's surface, it's a sphere.

07:42.360 --> 07:43.600
It's a 2-d sphere.

07:43.600 --> 07:46.240
So the geometry is in fact curved.

07:46.240 --> 07:47.240
No way.

07:47.240 --> 07:48.240
No way.

07:48.240 --> 07:49.240
I know.

07:49.240 --> 07:51.880
I'm just trolling the flat earthers out there.

07:51.880 --> 07:52.880
Sorry.

07:52.880 --> 07:53.880
No, no, no.

07:53.880 --> 08:00.760
But that's a simple example to explain the generalization of Euclidean geometry to

08:00.760 --> 08:07.840
rhymanian geometry, that's one example that everybody uses, because it's more intuitive for

08:07.840 --> 08:08.840
everybody.

08:08.840 --> 08:11.680
But it describes very well, actually, what it is.

08:11.680 --> 08:15.000
So rhymanian geometry is locally Euclidean.

08:15.000 --> 08:20.360
But if you look at the global shape of your space, then it's going to be curved.

08:20.360 --> 08:25.280
And when you start to do statistics on the curved spaces, then actually, everything

08:25.280 --> 08:29.360
that we know about usual statistics breaks down.

08:29.360 --> 08:32.240
So let me come back to the example of the earth for a little bit.

08:32.240 --> 08:35.080
And then we'll go into other examples if you prefer.

08:35.080 --> 08:40.400
But for the earth, imagine you take two cities on the surface of the earth.

08:40.400 --> 08:44.480
And you want to do statistics on the position of these two cities.

08:44.480 --> 08:45.720
So very simple example.

08:45.720 --> 08:48.600
We have only two data points.

08:48.600 --> 08:53.680
And an example of statistics that you may want to compute if, for example, an average

08:53.680 --> 08:56.200
in position of these two cities.

08:56.200 --> 09:03.640
If you use usual statistics in Euclidean space, you're going to compute a city that will

09:03.640 --> 09:08.480
be in between the two cities that you have on the surface of the earth.

09:08.480 --> 09:13.600
But this city, if it's the middle of the chord joining the two cities that you have on the

09:13.600 --> 09:19.560
surface of the earth, the average city will be inside the earth, will be on the surface

09:19.560 --> 09:20.560
of the earth.

09:20.560 --> 09:25.560
And so you are in a situation where you want to compute an average of cities that cannot

09:25.560 --> 09:30.000
possibly be a city, because it's not even on the earth.

09:30.000 --> 09:34.560
And that's a very simple example again, but that's a very, very important example because

09:34.560 --> 09:40.960
it shows that if you do statistics for data that belongs to curved spaces, even the very

09:40.960 --> 09:47.040
definition of mean cannot apply, because the very definition of mean is a linear definition.

09:47.040 --> 09:53.000
The mean is a weighted sum of the elements, so it's linear applying this linear definition

09:53.000 --> 09:56.960
to a nonlinear space, this completely breaks down.

09:56.960 --> 10:03.560
And that's why we got into developing this new field that we call geometric statistics.

10:03.560 --> 10:08.520
And we are asking the question, how should we generalize everything that we know about

10:08.520 --> 10:11.880
statistics to these curved spaces?

10:11.880 --> 10:18.560
When we're thinking about geometric statistics and Romanian geometries, are we exclusively

10:18.560 --> 10:28.120
looking at statistics as it relates to the points on these curves, as opposed to the relationships

10:28.120 --> 10:32.000
between different curves, for example?

10:32.000 --> 10:38.720
Yeah, so when we look at geometric statistics, we mean statistics for data that belong to

10:38.720 --> 10:41.200
these curved spaces.

10:41.200 --> 10:48.600
But actually, you can look at curves on these curved spaces, and the space of curves on curved

10:48.600 --> 10:53.960
spaces is also itself a space that is nonlinear.

10:53.960 --> 10:59.320
So you can look at different kind of data space, and as soon as you have something curved

10:59.320 --> 11:03.400
somewhere, it's going to propagate, and you're going to have some geometric somewhere.

11:03.400 --> 11:11.760
And I think the thought that was the origin of that question was trying to apply this to

11:11.760 --> 11:15.800
your computational anatomy types of problems.

11:15.800 --> 11:23.560
I'm imagining in those fields, yes, I'm interested in describing statistically the surface

11:23.560 --> 11:24.560
of the heart.

11:24.560 --> 11:31.640
But I'm also interested in things that are happening within the heart, and maybe there

11:31.640 --> 11:39.200
are complex surfaces that are composed of different things that are easier described with

11:39.200 --> 11:42.400
multiple curves as opposed to a single curve.

11:42.400 --> 11:48.600
So maybe you can use that as a segue to explain how this is applied in that field.

11:48.600 --> 11:54.560
Yes, so for sure, there are two types of geometry that you can consider when you talk about

11:54.560 --> 11:58.480
these geometric objects, like the surface of the heart.

11:58.480 --> 12:03.280
First of all, you can take one heart and look at its surface, and this surface of this

12:03.280 --> 12:09.240
one heart will have a particular geometry, but I'm not looking at one heart and the surface

12:09.240 --> 12:10.240
of one heart.

12:10.240 --> 12:15.720
Actually, I'm saying, in the way we model things currently, I'm saying that this heart

12:15.720 --> 12:23.360
is just a point in very high dimensional space, abstract space, that represents its shape.

12:23.360 --> 12:29.200
So the shape of the space I'm looking at is the shape of the data space.

12:29.200 --> 12:34.360
So actually, for example, if you go back to the example of the heart, imagine you have

12:34.360 --> 12:39.320
a heart and you want to study its shape, what we usually do is that we say, okay, the

12:39.320 --> 12:44.960
shape of this heart is actually going to be the deformation of a template heart.

12:44.960 --> 12:50.280
So we assume that there is a template shape, that is, for example, the healthy heart

12:50.280 --> 12:51.280
shape.

12:51.280 --> 12:57.680
So we fix that and then we represent each patient heart by the deformation of the template

12:57.680 --> 13:00.440
shape to the patient shape.

13:00.440 --> 13:06.240
And so now each heart shape is represented by a deformation.

13:06.240 --> 13:08.920
So one data is a deformation.

13:08.920 --> 13:13.040
And so if you have different patients, each of them is represented by the deformation

13:13.040 --> 13:16.200
from the template heart to the heart.

13:16.200 --> 13:22.600
And so your data are these deformations that are a point in a high dimensional space,

13:22.600 --> 13:24.960
space, which is the space of deformation.

13:24.960 --> 13:29.720
So it's a bit more abstract that looking at the surface of one heart is actually one

13:29.720 --> 13:34.040
data is a deformation from a template to the patient space.

13:34.040 --> 13:38.680
And the space of data is the space of deformations, which is curved.

13:38.680 --> 13:39.680
Okay, okay.

13:39.680 --> 13:41.240
I think I'm following here.

13:41.240 --> 13:50.640
So a question that pops up for me is I can imagine representing these, you know, a particular

13:50.640 --> 13:57.680
heart, a particular deformation as, you know, some set of parameters that kind of describe

13:57.680 --> 14:04.040
in the physical world, you know, how the heart is deformed from the template.

14:04.040 --> 14:10.000
But I can also imagine something more abstract, kind of like an embedding applied to, you

14:10.000 --> 14:12.320
know, this heart space.

14:12.320 --> 14:14.320
Does that concept apply here?

14:14.320 --> 14:16.040
Yeah, the completely makes sense.

14:16.040 --> 14:23.080
So I think initially people while looking in physical models, like mechanical models and

14:23.080 --> 14:29.720
imagining which clinical parameters may control the deformation from template heart shape

14:29.720 --> 14:32.280
to a patient heart shape.

14:32.280 --> 14:36.920
But then there are so many parameters that you can learn and maybe you're not going to

14:36.920 --> 14:42.960
find all of them and then even if you find this clinical parameters that govern the transformation

14:42.960 --> 14:48.520
from a heart shape to another, then how do you translate that to the next organ shape?

14:48.520 --> 14:53.520
You will need to do that all over again to do a deformation from a healthy brain shape

14:53.520 --> 14:55.720
to another brain shape.

14:55.720 --> 15:00.200
And so in our case, we are mourning the second scenario that you've described.

15:00.200 --> 15:06.520
We just look at the abstract deformation and we parameterize it just in terms of geometry.

15:06.520 --> 15:13.200
So this deformation, this point in our curved space, we represent it mathematically as a

15:13.200 --> 15:14.800
deformorphism.

15:14.800 --> 15:21.160
And then we look at how many parameters we need to describe this deformorphic transformation.

15:21.160 --> 15:25.120
So we look at that in the abstract way.

15:25.120 --> 15:30.800
And also because, you know, I don't have a medical background, so I come from the mathematical,

15:30.800 --> 15:32.040
from a mathematical background.

15:32.040 --> 15:38.680
So I think that's also a reason why we use our mathematical tools to describe transformation.

15:38.680 --> 15:44.120
Now the hope is that eventually both fields will merge together.

15:44.120 --> 15:50.480
So by describing these transformations, these deformations of organ shapes of heart shape

15:50.480 --> 15:57.440
by mathematical or geometric parameters, maybe by doing so we will discover new clinical

15:57.440 --> 15:59.040
parameters.

15:59.040 --> 16:06.800
And actually we started to see a hint of that when we did a study on heart shapes.

16:06.800 --> 16:12.880
So we had many hearts, together many heart shapes, that are set of many heart shapes.

16:12.880 --> 16:18.840
And we performed a principal component analysis on these heart shapes by using deformations.

16:18.840 --> 16:25.520
So we were looking at what were the main variations in terms of geometric shapes within these

16:25.520 --> 16:26.880
heart shapes.

16:26.880 --> 16:34.280
And the main variation geometric deformation that we found was the size of the heart, which

16:34.280 --> 16:41.360
is a clinical irrelevant parameter because it's very correlated with the body mass index.

16:41.360 --> 16:46.400
So basically just by looking at which general direction we were seeing in this data set

16:46.400 --> 16:53.960
of heart shapes, we found one principal component, which was linked to the body mass index.

16:53.960 --> 17:01.320
So yes, we have an abstract mathematical geometric approach to describing this transformation,

17:01.320 --> 17:06.400
but the hope is that we will recover the clinical, the usual clinical parameters and maybe

17:06.400 --> 17:09.480
even discover more clinical parameters.

17:09.480 --> 17:15.320
And so I think this is a very good instance of a pretty disciplinary approach because

17:15.320 --> 17:21.120
historically medicine and computer science have not been that linked.

17:21.120 --> 17:26.920
And now that we have more medical images being produced everyday in hospitals and also

17:26.920 --> 17:31.960
more computing power to actually analyze these images and do machine learning on these

17:31.960 --> 17:32.960
images.

17:32.960 --> 17:38.560
Now is a very good time to have both fields of mathematics and medicine or of computer

17:38.560 --> 17:44.880
science and medicine merging to see if we can establish a dialogue between the geometric

17:44.880 --> 17:47.680
parameters and the clinical parameters.

17:47.680 --> 17:50.440
Can you elaborate a bit more on that example?

17:50.440 --> 17:53.720
What did the data set look like?

17:53.720 --> 17:56.720
Was it an image-based data set?

17:56.720 --> 18:03.920
How did you go from presumably again some set of images in two or three-dimensional Euclidean

18:03.920 --> 18:11.120
space, again, another assumption to geometric representations and then how exactly did you

18:11.120 --> 18:23.480
apply PCA to that to determine these deformations, right, the deformations and derive this size

18:23.480 --> 18:24.480
factor?

18:24.480 --> 18:25.480
Yes.

18:25.480 --> 18:31.600
So originally it was an image data set, but we extracted, so image of the heart, but we

18:31.600 --> 18:36.160
extracted heart surfaces in terms of measures.

18:36.160 --> 18:41.640
So in order to do that, you have in medical imaging community what we call segmentation algorithm,

18:41.640 --> 18:45.760
so there are many data available and they basically do that.

18:45.760 --> 18:50.200
They are able to take an image and extract the surface of the heart.

18:50.200 --> 18:56.280
So for us, the starting point was this data set of meshed surfaces of the heart.

18:56.280 --> 19:02.000
In order to go from the surface, one data point to the deformation, one data point, we

19:02.000 --> 19:05.160
use this template modelization.

19:05.160 --> 19:11.160
In the sense that we took one heart shape and we named it the template.

19:11.160 --> 19:18.640
And so each of the other heart surfaces was a deformation from this template to the

19:18.640 --> 19:19.880
data point.

19:19.880 --> 19:24.720
You arbitrarily picked a heart shape to what you call the template.

19:24.720 --> 19:32.600
I envisioned earlier that the template was derived in some way, akin to an average

19:32.600 --> 19:35.520
or some statistical template.

19:35.520 --> 19:36.520
Exactly.

19:36.520 --> 19:43.360
So yeah, and actually template shape estimation is the main point of my PhD thesis on his

19:43.360 --> 19:44.360
whole.

19:44.360 --> 19:47.520
I wanted to simplify a little bit, but you're completely right.

19:47.520 --> 19:53.720
So in fact, the template shape and the deformations are jointly computed.

19:53.720 --> 19:59.440
So you have an iterative algorithm that is called template shape estimation that works

19:59.440 --> 20:06.680
as follows first, you take all the heart surfaces and you register them.

20:06.680 --> 20:08.880
So you align them in order to correct them.

20:08.880 --> 20:13.640
Find a centroid and try to make them overlap as much as possible.

20:13.640 --> 20:14.640
Exactly.

20:14.640 --> 20:19.120
So that the surface themselves, the meshes overlap as much as possible.

20:19.120 --> 20:21.400
This is called registration.

20:21.400 --> 20:24.960
After this registration step, you compute an average.

20:24.960 --> 20:29.800
If you're in images, you do that in terms of pixels, if you're with surfaces, you find

20:29.800 --> 20:32.440
a middle for each corresponding meshes.

20:32.440 --> 20:36.640
And so after the registration step, you have another edging step.

20:36.640 --> 20:41.000
This averaging step gives you an initial estimate of the template.

20:41.000 --> 20:43.200
And then you iterate the algorithm.

20:43.200 --> 20:47.880
Now that you have an initial estimate of the template, you register again, everybody,

20:47.880 --> 20:50.600
not among themselves, but to the template.

20:50.600 --> 20:52.040
And you have reg them again.

20:52.040 --> 20:54.760
You have a second estimate of the template.

20:54.760 --> 20:59.200
And you do that over and over again until you converge to a template.

20:59.200 --> 21:01.920
And so in the end, you have a template shape.

21:01.920 --> 21:08.480
And each single image or each single surface being described as a deformation from this

21:08.480 --> 21:09.480
template.

21:09.480 --> 21:15.400
And yeah, actually, as I said, my PhD thesis was exactly on this algorithm because this

21:15.400 --> 21:20.040
is an algorithm that is very well known both in medical imaging community, but also in

21:20.040 --> 21:27.880
signal processing community, where you want to align signals and then compute an average

21:27.880 --> 21:28.880
of signals.

21:28.880 --> 21:34.760
So in terms of signals, you might imagine spikes on neurons or stuff like that, temporal signals.

21:34.760 --> 21:40.040
And so they also have, if they want to see the shape of a signal, they also often compute

21:40.040 --> 21:42.040
a template shape and do registration.

21:42.040 --> 21:46.000
So it's a very well known algorithm in the field.

21:46.000 --> 21:52.160
And the goal of my PhD study was to analyze the statistical properties of this algorithm,

21:52.160 --> 21:57.560
which can be done using geometry, and which hadn't been done before.

21:57.560 --> 22:04.440
And so I'm imagining if you've got this template heart and you've got a corpus of other heart

22:04.440 --> 22:10.800
meshes or representations, that one of the things that you might want to do is like

22:10.800 --> 22:19.320
find a distribution of the way that a heart or heart's in general differ from the template

22:19.320 --> 22:26.880
and that part of what makes that difficult in trying to do that in Euclidean geometry

22:26.880 --> 22:32.200
is that you have to do it in any given point on the surface, a bunch of different directions.

22:32.200 --> 22:41.360
And is that part of what makes geometric models easier to apply here?

22:41.360 --> 22:43.720
Yeah, so that's exactly what we want to do.

22:43.720 --> 22:48.600
We want to describe the viability, for example, in a hard shape.

22:48.600 --> 22:52.280
And we do that on a geometric space.

22:52.280 --> 22:58.960
Not necessarily because it's going to be easier, but it's because it is what it makes sense.

22:58.960 --> 23:04.040
So if we go back, you know, to the example, when we were averaging two CDs on the surface

23:04.040 --> 23:09.760
of the earth, if we are computing Euclidean average, then we get something that is not a

23:09.760 --> 23:10.760
city.

23:10.760 --> 23:16.760
Going to the heart now, if we want to compute the average of all the healthy heart, for

23:16.760 --> 23:23.000
example, to get a template healthy heart, if we were to do that using Euclidean geometry,

23:23.000 --> 23:26.400
we will get something that doesn't look like a heart at all.

23:26.400 --> 23:31.280
So it's more in order to be able to describe the variability of heart by summary statistics

23:31.280 --> 23:36.640
like a mean, and then a standard deviation, we want this summary statistics to make sense.

23:36.640 --> 23:40.920
So we want to the mean of hard surfaces to be a hard surface.

23:40.920 --> 23:44.240
And that's why we use geometric statistics.

23:44.240 --> 23:45.960
Makes sense, makes sense.

23:45.960 --> 23:50.080
And then to describe the geometry of the hard shapes, we can do many things.

23:50.080 --> 23:56.760
So I'm taking the example of the mean heart shape, but then we can look at variations.

23:56.760 --> 23:59.480
So that's where PCA comes in.

23:59.480 --> 24:03.240
What are the healthy variations of a heart shape?

24:03.240 --> 24:07.920
For example, size can be a healthy variation of a heart shape.

24:07.920 --> 24:10.880
But then we can also look at clusters.

24:10.880 --> 24:17.200
And the intuition is that the different clusters that you might see will correspond.

24:17.200 --> 24:23.400
One will be for healthy heart shapes, and another one may be for a certain pathology that

24:23.400 --> 24:25.640
can be seen in a heart shape.

24:25.640 --> 24:31.280
And I'll show you what this is what we do for brain shapes, where you can, if you do that,

24:31.280 --> 24:37.680
a clustering on brain shapes, and you want to separate healthy brain shapes versus brain

24:37.680 --> 24:43.320
shapes with Alzheimer's disease, then you can easily separate that because Alzheimer's

24:43.320 --> 24:48.120
disease is a disease that you can see on the brain shapes.

24:48.120 --> 24:52.600
So there is an atrophy of the cerebral cortex that changes the brain shapes.

24:52.600 --> 25:00.520
How do you describe these types of geometric statistics in terms of, are there different

25:00.520 --> 25:07.920
sets of distributions or different fundamental laws, what's different about the way things

25:07.920 --> 25:13.240
work in the geometric world compared to what we're used to?

25:13.240 --> 25:14.240
Yes.

25:14.240 --> 25:21.240
So basically every time that you had something that was linear in the Euclidean word, then

25:21.240 --> 25:26.400
you need to say that into something that is not linear in the Riemannian word.

25:26.400 --> 25:30.080
And even the very basic operations don't work anymore.

25:30.080 --> 25:35.760
So if you're in a Euclidean space and you have two points on this Euclidean space, you

25:35.760 --> 25:40.360
can do a subtraction of a point to another and you get a vector.

25:40.360 --> 25:46.080
So if you have point a and point b, if you do b minus a, you get vector a b.

25:46.080 --> 25:51.920
Now even this very simple operation, the subtraction, you cannot do it in Riemannian geometry.

25:51.920 --> 26:00.040
So this variation has been translated to the Riemannian geometric framework and we call

26:00.040 --> 26:01.720
that the logarithm.

26:01.720 --> 26:06.520
It's not linked to the logarithm as a function, it's just called like that.

26:06.520 --> 26:12.640
Same thing in the Euclidean word, if you have a point a, you can add it a vector u a plus

26:12.640 --> 26:14.720
u and you get another point.

26:14.720 --> 26:20.120
You use that vector to shoot from point a and you arrive at another point.

26:20.120 --> 26:25.200
So the addition, this addition doesn't even exist in Riemannian geometry neither because

26:25.200 --> 26:30.080
if you imagine that you're in the surface of the earth, if you shoot a long attention

26:30.080 --> 26:33.880
vector and you see what you arrive, you're not going to be on the earth.

26:33.880 --> 26:39.720
So you want to generalize this operation in order to do very basic computation in Riemannian

26:39.720 --> 26:40.720
geometry.

26:40.720 --> 26:45.200
And so I took the example of the subtraction of the addition, basic operation that don't

26:45.200 --> 26:50.480
exist as is in Riemannian geometry and it we needed to generalize.

26:50.480 --> 26:55.600
And in fact, the way that we need to generalize these very basics of operations, links to the

26:55.600 --> 27:00.120
fact that we have generalize optimization algorithm, for example.

27:00.120 --> 27:05.280
So if you think about an optimization algorithm, you're in a machine learning framework and

27:05.280 --> 27:10.800
you want to find the parameter theta that optimizes your criterion.

27:10.800 --> 27:14.000
Then when you do gradient descent, that's what you're doing.

27:14.000 --> 27:18.640
You have your current estimate of theta and you shoot along the gradient.

27:18.640 --> 27:24.160
So in Euclidean geometry, you add you have a point which is theta and you use a vector

27:24.160 --> 27:27.320
which is the gradient to shoot along it.

27:27.320 --> 27:32.880
So now this operation, which is addition of a vector to a point, we cannot do it in

27:32.880 --> 27:33.880
Riemannian geometry.

27:33.880 --> 27:40.600
I mean, now we can, because we have generalized the addition, but we couldn't do it as is

27:40.600 --> 27:45.960
in Riemannian geometry, otherwise we would have gotten out of the space of parameters.

27:45.960 --> 27:50.200
Just the same way that we would have gotten out of the surface of the earth.

27:50.200 --> 27:57.080
So we've generalized the addition, but we've also made an analogous construct to a

27:57.080 --> 28:03.560
vector that's, you know, a curved vector on the surface, correct?

28:03.560 --> 28:12.720
Yes, yes, but in fact, actually in Riemannian geometry, we use the concept of tangent vectors.

28:12.720 --> 28:17.680
So if you take your curved space, if you take the surface of the earth, remember I said

28:17.680 --> 28:22.640
that Riemannian geometry was locally Euclidean, so on the surface of the earth, if you are

28:22.640 --> 28:26.560
in your office on the earth, then the earth looks flat.

28:26.560 --> 28:32.120
So Riemannian geometry is locally Euclidean, and in fact, it means that at each point of

28:32.120 --> 28:39.240
a Riemannian manifold, so a curved space, there is a tangent space that locally approximates

28:39.240 --> 28:41.800
very well your curved space.

28:41.800 --> 28:48.000
And so actually we use vectors all as well, because we use at the point of a curved space,

28:48.000 --> 28:52.640
we consider the tangent vectors at this point.

28:52.640 --> 28:58.640
And then we transform them into curved vector, if you want, so we add a tangent vector to

28:58.640 --> 28:59.640
a point.

28:59.640 --> 29:04.720
This brings us to another point on the curved space through a curved vector, you can imagine

29:04.720 --> 29:05.720
it like that.

29:05.720 --> 29:11.840
But since we have so many tools in Euclidean geometry, we wanted also to bring these tools

29:11.840 --> 29:17.040
back on Riemannian geometry, so we bring them back locally at each tangent space of the

29:17.040 --> 29:18.360
curved space.

29:18.360 --> 29:23.720
There's some vector at this tangent space that gets projected onto the surface, and that's

29:23.720 --> 29:27.200
your Riemannian vector, so to speak.

29:27.200 --> 29:30.080
Yes, yes, that's the way it works.

29:30.080 --> 29:34.720
Butchering terminology of course.

29:34.720 --> 29:36.800
That gives us a little bit of a foundation.

29:36.800 --> 29:43.160
Part of what you've been up to recently is building out a set of tools to make this

29:43.160 --> 29:45.600
a little bit more accessible.

29:45.600 --> 29:47.640
Can you talk a little bit about that work?

29:47.640 --> 29:54.720
Yes, so yeah, as I said, I'm very passionate about geometry, and even though I learned geometry

29:54.720 --> 29:59.720
in theoretical physics, when I saw how it could be applied to computational anatomy and

29:59.720 --> 30:05.600
computational medicine, I really wanted to democratize the use of geometry.

30:05.600 --> 30:12.640
And so that's why starting last year, I started to build a Python package to be able to

30:12.640 --> 30:14.240
democratize geometry.

30:14.240 --> 30:20.360
So it's a package that is called GeomStats, it stands for geometric statistics.

30:20.360 --> 30:26.480
And basically, it allows you to do this addition and subtraction, so generalization of addition

30:26.480 --> 30:30.720
and subtraction on Riemannian spaces, easily.

30:30.720 --> 30:35.720
It encapsulates the geometry so that you just have to think about, okay, I'm going to use

30:35.720 --> 30:37.120
the generalization of the addition.

30:37.120 --> 30:39.960
I'm going to use the generalization of the subtraction.

30:39.960 --> 30:45.880
And I wanted to do that because we have seen that, actually, in machine learning, a lot

30:45.880 --> 30:50.120
of spaces, data spaces have geometric properties.

30:50.120 --> 30:55.440
For example, if you're looking at protein shapes and you want to describe the shape of a protein

30:55.440 --> 31:01.840
by the list of angles from one carbon of the carbon bone to another, then you have a

31:01.840 --> 31:05.720
list of angles that belong to a set of spheres.

31:05.720 --> 31:11.240
How many ways, many examples in machine learning where data belong to geometric spaces?

31:11.240 --> 31:16.440
And so we wanted to give a package that allows people to do the computations on these geometric

31:16.440 --> 31:17.440
spaces.

31:17.440 --> 31:24.920
So we have implemented various spaces like hypersphere, hyperbolic spaces, also space of matrices.

31:24.920 --> 31:31.280
For example, symmetric positive definite matrices or rotation matrices, et cetera, so that

31:31.280 --> 31:35.840
people could do proper averaging in these geometric spaces.

31:35.840 --> 31:39.360
So this is a Python package that is available on GitHub.

31:39.360 --> 31:44.520
And we're also currently writing a journal paper that explains the need and the use of this

31:44.520 --> 31:48.360
package for the machine learning community.

31:48.360 --> 31:49.360
Interesting.

31:49.360 --> 31:55.240
So does Python support something like polymorphism that would allow you to just kind of drop

31:55.240 --> 32:00.200
in and replace the operations provided by this package?

32:00.200 --> 32:01.200
By plus and minus.

32:01.200 --> 32:02.200
By plus and minus.

32:02.200 --> 32:03.200
Yeah, exactly.

32:03.200 --> 32:07.640
Or do you have to kind of update your code to use the package?

32:07.640 --> 32:08.640
Yeah.

32:08.640 --> 32:09.640
No.

32:09.640 --> 32:10.640
Right now you have to update your code.

32:10.640 --> 32:15.680
It's true that it will be a very interesting use of polymorphism.

32:15.680 --> 32:16.680
No.

32:16.680 --> 32:20.400
Right now we've given these operations their names, so exponential and logarithm.

32:20.400 --> 32:22.920
The name they have in mathematics.

32:22.920 --> 32:25.400
And it's up to the user to do the translation.

32:25.400 --> 32:29.920
But that's actually an amazing idea we could use polymorphism.

32:29.920 --> 32:35.840
But I guess we also don't want to completely hide the fact that it's geometric, otherwise

32:35.840 --> 32:37.720
people might forget about it.

32:37.720 --> 32:41.920
And then when people run into bugs, then they won't understand what's going on.

32:41.920 --> 32:47.960
So for example, if you remember the example of averaging two hard shapes, if you do that

32:47.960 --> 32:52.080
in the Euclidean way, you're going to get something that is not a hard shape.

32:52.080 --> 32:53.800
And this is not a bug of your code.

32:53.800 --> 32:55.640
This is not a precision problem.

32:55.640 --> 32:59.320
This is a mathematical, this error as a mathematical foundation.

32:59.320 --> 33:03.280
Because the space of hard shapes is curved.

33:03.280 --> 33:08.760
So we don't want to completely hide the fact that there is geometry, but more encapsulate

33:08.760 --> 33:14.080
some subtle concept into a user-friendly environment.

33:14.080 --> 33:15.080
Yeah.

33:15.080 --> 33:22.320
And so using this library to implement something like gradient descent, is it, besides some

33:22.320 --> 33:30.120
of the fact that you're not using polymorphism, is it essentially substituting your pluses

33:30.120 --> 33:33.520
and minuses with your exponents and logarithms?

33:33.520 --> 33:34.520
Yes.

33:34.520 --> 33:35.520
Yes.

33:35.520 --> 33:36.520
So that's a way of thinking.

33:36.520 --> 33:43.000
So the library, we've used it at three points, at three levels in machine learning pipelines.

33:43.000 --> 33:49.120
So if you imagine a supervised learning, a typical supervised learning pipeline where you

33:49.120 --> 33:55.200
have an input, it's x, and then you want to predict an output y from this input x.

33:55.200 --> 34:00.040
And you do that by learning a function that can have a given parameter theta.

34:00.040 --> 34:02.600
Now we can put geometry at theta.

34:02.600 --> 34:06.520
So you say, oh, the space of parameter is actually curved.

34:06.520 --> 34:13.400
And the gradient descent in that curve space will need to use our exponential and logarithm,

34:13.400 --> 34:16.600
which are the generalization of the addition and the subtraction.

34:16.600 --> 34:21.720
So that's one thing that we've put into the package, actually, by creating our own version

34:21.720 --> 34:23.120
of keras.

34:23.120 --> 34:30.840
So now when you, when you, we have created all of keras, no, no, no, no, so we have re-implemented

34:30.840 --> 34:31.840
keras.

34:31.840 --> 34:32.840
No, no, no, no.

34:32.840 --> 34:37.040
We have forked the keras repository and just modified a little bit so that now you can

34:37.040 --> 34:42.520
add a parameter in some keras function, this parameter is called manifold.

34:42.520 --> 34:48.480
And basically, if you say manifold is hypersphere, then it knows that instead of using the addition

34:48.480 --> 34:54.520
and subtraction, they will need to use, the code will need to use exponential and logarithm.

34:54.520 --> 34:59.840
So that's the first way of putting geometry in this supervised learning pipeline is putting

34:59.840 --> 35:02.480
it for the space of the parameter theta.

35:02.480 --> 35:07.280
But now the way we've been using geometric statistics as well is by putting geometry

35:07.280 --> 35:14.400
on the space of the input x, but also on the space of the output y.

35:14.400 --> 35:20.440
And we've done an interesting example, putting geometry on the space of the output y.

35:20.440 --> 35:23.880
So we did a regression on the groups.

35:23.880 --> 35:31.840
So the situation was as follows, it was again, medical images and we had slices of brain

35:31.840 --> 35:34.680
volumes, actually, a fetal brain volume.

35:34.680 --> 35:40.080
With fetal brain MRI, actually, you have a lot of motion, so the images are motion

35:40.080 --> 35:45.680
corrected because the fetal moves in the belly of the mother, the mother may move as well.

35:45.680 --> 35:51.000
And so this is a case of medical images, when if you take slices, then the slices are

35:51.000 --> 35:53.680
not going to be aligned within another.

35:53.680 --> 35:58.200
And when you want to reconstruct the 3D volume of the fetal brain, then you're running

35:58.200 --> 35:59.200
to problem.

35:59.200 --> 36:05.520
And so what we've done using GM stats is that for each slice of the fetal brain, we

36:05.520 --> 36:13.800
were predicting the optimal pose of this slice in the reconstructed brain volume.

36:13.800 --> 36:19.440
And we did that using computational neural network, predicting a pose where a pose is

36:19.440 --> 36:20.440
a translation.

36:20.440 --> 36:26.880
So it's the position of the slice in the 3D brain volume and the orientation, a rotation.

36:26.880 --> 36:32.720
The rotation of the slice in the 3D brain volume, so we were able to take which slice was supposed

36:32.720 --> 36:37.600
to be where in the future reconstructed 3D brain volume.

36:37.600 --> 36:44.000
And in order to do that, we put some geometry on the space of these positions and orientations,

36:44.000 --> 36:48.680
which was the space of the output for this supervised learning algorithm.

36:48.680 --> 36:55.560
Input is slice of brain and output is position and orientation of this slice of brain.

36:55.560 --> 37:01.080
And by considering the fact that positions and orientations, they don't belong to your

37:01.080 --> 37:02.080
rotating space.

37:02.080 --> 37:07.960
They naturally belong to a curved space, which is a lead group, is the space of translations

37:07.960 --> 37:09.160
and rotations.

37:09.160 --> 37:14.920
And by taking into account the geometry of the space of the output of the supervised learning

37:14.920 --> 37:21.680
algorithm, we were able to get better brain volume reconstruction.

37:21.680 --> 37:27.400
So you can put geometry on the space of parameters, you can put geometry on the space of the input,

37:27.400 --> 37:34.040
you can put geometry on the space of the outputs, and you might increase the accuracy of your

37:34.040 --> 37:35.120
results.

37:35.120 --> 37:43.600
In general, are these fundamental operations in the remanion space of the same order of

37:43.600 --> 37:50.600
complexity as the Euclidean analogs?

37:50.600 --> 37:56.800
Which will depend for which space, but what's interesting is that they're actually exactly

37:56.800 --> 37:59.240
the same on the infinitesimal level.

37:59.240 --> 38:04.920
So if you really close, if you imagine your curved space and you close and you imagine

38:04.920 --> 38:09.520
your curved space, which has a tangent space at a given point, if you're really close to

38:09.520 --> 38:15.960
this given point, then using your curved vector or your tangent vector is the same.

38:15.960 --> 38:19.880
So at the infinitesimal scale, yeah, they're exactly the same.

38:19.880 --> 38:25.200
I'm wondering if you've ever tried to apply this someplace where just to try it and were

38:25.200 --> 38:29.920
surprised that it actually works, or are you usually applying this because you have a strong

38:29.920 --> 38:34.440
intuition based on the problem that you should apply it?

38:34.440 --> 38:37.840
No, usually it's based on a strong intuition.

38:37.840 --> 38:46.080
I'm trying to think, yeah, I think it's more the second scenario because I was just wondering

38:46.080 --> 38:53.680
if there are problems that might have some latent geometricness to them that we don't

38:53.680 --> 38:54.680
know.

38:54.680 --> 39:00.400
Now that you've made this easy with a library, if we could just apply it willy-nilly to

39:00.400 --> 39:03.920
see if that has some value.

39:03.920 --> 39:05.400
Yeah.

39:05.400 --> 39:12.600
So I think one set of problems is every time you're dealing with rotations and translation,

39:12.600 --> 39:14.480
but especially rotations.

39:14.480 --> 39:20.840
So rotation is a very intuitive geometric example because we have an intuition of how it works.

39:20.840 --> 39:23.400
You can rotate an object.

39:23.400 --> 39:27.240
And even if it's simple, it appears in so many fields.

39:27.240 --> 39:33.720
One example is in medical imaging when you want to register or align two shapes.

39:33.720 --> 39:38.640
You're rotating one shape to match as closely as possible the other shape.

39:38.640 --> 39:43.520
But also in robotics, when you want to control a robotic arm to do such and such, you want

39:43.520 --> 39:49.240
to move it in space, but you may also want to rotate the robotic hand, that is at the

39:49.240 --> 39:50.920
end of the arm.

39:50.920 --> 39:57.000
So rotations is, even if it's simple, it's a very good example of a geometric space.

39:57.000 --> 40:03.400
And a lot of people haven't been considering the geometry of this rotation space.

40:03.400 --> 40:07.240
And so this is an example where it actually makes a real difference.

40:07.240 --> 40:12.920
And we've shown it with this supervised learning approach where we are able to reconstruct

40:12.920 --> 40:17.880
better brain volumes using, taking into account the geometry of that space.

40:17.880 --> 40:25.160
OK, what I'm hearing is that as opposed to just kind of throwing this approach at a

40:25.160 --> 40:31.720
given problem from a modeling perspective, because it's easier now that we've got gyms

40:31.720 --> 40:38.560
that's rather use it as an opportunity to think about where geometric statistics might

40:38.560 --> 40:44.680
apply or if it might apply to the problem more deeply than you might otherwise, because

40:44.680 --> 40:48.760
now if it does, you've got an easier way to use that information.

40:48.760 --> 40:49.760
Yeah, exactly.

40:49.760 --> 40:53.960
And that's the all point of the paper that we are writing right now.

40:53.960 --> 41:00.560
So we do a review of geometry in machine learning because our feeling is that a lot of people

41:00.560 --> 41:03.280
are using geometric data spaces.

41:03.280 --> 41:08.520
It might be an intuitive space like the space of rotations, but sometimes it's a more abstract

41:08.520 --> 41:14.000
space like a hyperbolic space or the space for symmetric positive definite matrices,

41:14.000 --> 41:18.760
which has a geometric touch, but people don't often use that.

41:18.760 --> 41:23.080
So the first goal of the paper is to do a review and be like, oh, in that field, in fact,

41:23.080 --> 41:25.280
you have some geometry with this data.

41:25.280 --> 41:27.320
In fact, you have some geometry.

41:27.320 --> 41:32.720
And then once we've convinced people that deep down their data space is curved, then

41:32.720 --> 41:37.680
we tell them with gyms that which tools they can use.

41:37.680 --> 41:45.760
And a very important case, a set of tools that they can use is the metrics, the set of

41:45.760 --> 41:46.760
metrics.

41:46.760 --> 41:52.760
So if you have a curved space and you want to measure the distance between two elements

41:52.760 --> 41:58.440
of this curved space, you need to follow the curvature of the space and take the length

41:58.440 --> 42:02.040
of the shortest curve that links these two points.

42:02.040 --> 42:04.320
So we call that a geodesk.

42:04.320 --> 42:09.800
It means that the distance between two points in a curved space is defined as the distance

42:09.800 --> 42:12.440
as the length of the geodesk.

42:12.440 --> 42:17.480
And this means that if you are losing loss functions, if you are learning on a geometric

42:17.480 --> 42:22.200
space, the loss functions that you should use, which is the distance between the ground

42:22.200 --> 42:25.120
roofs and the prediction is the geodesk.

42:25.120 --> 42:26.120
Exactly.

42:26.120 --> 42:30.720
You should use a geodesk distance because you will encompass, you will take into account

42:30.720 --> 42:32.800
the geometry of your space.

42:32.800 --> 42:39.520
So that's what gyms that is about is first telling people, oh, your space is geometric.

42:39.520 --> 42:45.160
You should have a look at this geodesk distance because it will be more suited for your problem.

42:45.160 --> 42:47.640
What's next in this line of research?

42:47.640 --> 42:48.640
Oh.

42:48.640 --> 42:58.080
So in terms of implementation, so gyms that has been around for a year now and we are

42:58.080 --> 42:59.080
in development.

42:59.080 --> 43:02.560
So we are currently developing at different backends.

43:02.560 --> 43:06.960
So we want gyms that since we want to apply it to machine learning, we want gyms that

43:06.960 --> 43:10.240
to be able to be used on GPU.

43:10.240 --> 43:15.160
So we are currently implementing TensorFlow backend and also PyTouch backend.

43:15.160 --> 43:18.280
So this is from an implementation point of view.

43:18.280 --> 43:24.560
And now the goal will be to use gyms that so for me to study organ shapes, but also other

43:24.560 --> 43:25.880
kind of shapes.

43:25.880 --> 43:31.320
So initially I was looking at brain shapes or hard shapes, now I'd like to go into small

43:31.320 --> 43:35.120
the scales and maybe look at cell shapes and protein shapes.

43:35.120 --> 43:40.440
And actually use the tool that we've developed to look at these statistics on these new kinds

43:40.440 --> 43:41.440
of shapes.

43:41.440 --> 43:42.440
Sounds fantastic.

43:42.440 --> 43:47.040
Well, Nina, thanks so much for taking the time to chat with us about this really interesting

43:47.040 --> 43:48.040
work.

43:48.040 --> 43:49.520
Thank you very much.

43:49.520 --> 43:57.600
All right, everyone, that's our show for today for more information on Nina or any of

43:57.600 --> 44:05.040
the topics covered in this episode, visit twimmelai.com slash talk slash 196.

44:05.040 --> 44:09.200
If you're a fan of the podcast and you haven't already done so or you're a new listener

44:09.200 --> 44:14.960
and you like what you hear, visit your Apple or Google podcast app and leave us a five-star

44:14.960 --> 44:16.680
rating and review.

44:16.680 --> 44:22.600
Your reviews are super helpful and they inspire us to create more and better content as well

44:22.600 --> 44:26.080
as help new listeners find the show.

44:26.080 --> 44:29.600
Thanks again to our friends at IBM for their sponsorship of this episode.

44:29.600 --> 44:36.320
Be sure to check out the IBM Developer Portal at IBM.biz slash MLAI podcast.

44:36.320 --> 45:04.440
As always, thanks so much for listening and catch you next time.

