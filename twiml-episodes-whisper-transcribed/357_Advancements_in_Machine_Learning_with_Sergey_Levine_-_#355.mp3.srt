1
00:00:00,000 --> 00:00:16,000
Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.

2
00:00:16,000 --> 00:00:27,920
Alright everyone, I am here in Vancouver at the 33rd NURPS conference and I've got the

3
00:00:27,920 --> 00:00:33,200
pleasure of being seated with a repeat Twimal guests. Sergey Levine. Sergey is an assistant

4
00:00:33,200 --> 00:00:38,480
professor in the Department of Electrical Engineering and Computer Science at UC Berkeley.

5
00:00:38,480 --> 00:00:46,480
Sergey, welcome back to the Twimal AI Podcast. Thank you. So your team has been continues to be

6
00:00:46,480 --> 00:00:53,200
very productive and prolific. You've got a dozen papers submitted here or accepted here at the

7
00:00:53,200 --> 00:00:59,680
main conference as well as a number of workshop papers and I would love to use this opportunity to

8
00:00:59,680 --> 00:01:05,840
kind of get caught up with you and hear about what you've been excited about recently. I think it

9
00:01:05,840 --> 00:01:12,320
was July, well not I think, I just checked actually. It was July of 2017 that we last spoke on

10
00:01:13,440 --> 00:01:18,400
deep robotic learning. What have you been up to? What are you excited about?

11
00:01:18,400 --> 00:01:23,920
I guess a lot has happened since then. Broadly speaking, a lot of what my lab has been trying to do

12
00:01:23,920 --> 00:01:30,560
since then is really to try to make it, try to move towards sort of a future where we could have

13
00:01:30,560 --> 00:01:34,240
machines that are out there in the real world learning continues through their own experience.

14
00:01:34,240 --> 00:01:39,520
And while we're doing a lot of very different things, in many ways much of this work is

15
00:01:39,520 --> 00:01:42,480
centered around the components that we believe are necessary to make that happen.

16
00:01:43,520 --> 00:01:47,760
So I could tell you, for example, about some of the things in this conference that I'm pretty

17
00:01:47,760 --> 00:01:51,840
excited about it that I think are giving us sort of a non-trivial step towards that direction.

18
00:01:51,840 --> 00:01:58,400
Absolutely. So one of those things is basically techniques for combining model free reinforcement

19
00:01:58,400 --> 00:02:04,480
learning with planning. So this is something that I think is actually a really big deal.

20
00:02:04,480 --> 00:02:08,400
You know, conventionally when we think about rational decision making, we think about kind of

21
00:02:08,400 --> 00:02:12,160
a planning that is that process, you know, if you're imagining how you're going to get to the

22
00:02:12,160 --> 00:02:16,560
airport, you think, well maybe you'll like get the train or get the taxi, how do you pay for the

23
00:02:16,560 --> 00:02:20,560
taxi, how do you pay for the train, you know, what do you do when you get there, and there's a

24
00:02:20,560 --> 00:02:24,320
sequence of steps that you have in your mind. But the thing that's always been kind of puzzling is

25
00:02:24,320 --> 00:02:29,440
that those steps are, they're a little bit abstract. So you don't plan how you're going to move

26
00:02:29,440 --> 00:02:33,360
every muscle in your body in order to get on the train, you plan through individual steps.

27
00:02:33,920 --> 00:02:39,600
And that has always been the big challenge in marrying planning and learning.

28
00:02:39,600 --> 00:02:45,200
You can't plan over the low level kind of instantiation of your behavior and it seems like

29
00:02:45,200 --> 00:02:49,440
learning doesn't by itself give you these kind of planning type behaviors. So we have actually

30
00:02:49,440 --> 00:02:55,440
two papers in the main conference that study different facets of this problem, essentially using

31
00:02:55,440 --> 00:03:00,960
learning to learn behaviors that achieve goals and then planning over the instantiation of those

32
00:03:00,960 --> 00:03:06,640
goals to achieve much more temporarily extended outcomes. And I'm pretty excited about that because

33
00:03:06,640 --> 00:03:11,360
that gives us some ideas about how reinforcement learning can, it sounds almost provide the abstractions

34
00:03:11,360 --> 00:03:17,920
over which planning can be done. What's the relationship between planning and model-based

35
00:03:17,920 --> 00:03:23,360
approaches where you're integrating in some, you know, prior knowledge about how a task should be

36
00:03:23,360 --> 00:03:29,840
performed? So planning is typically regarded as a very model-based thing. And I think that one of

37
00:03:29,840 --> 00:03:33,680
the, one of the things that's kind of an interesting shift away from the conventional way of thinking

38
00:03:33,680 --> 00:03:37,360
is that conventionally people think that, well, planning is this thing that you do, well,

39
00:03:37,360 --> 00:03:40,960
first people think that it's something that you do that you publish and kind of dusty,

40
00:03:40,960 --> 00:03:46,080
old robotics venues. But they think it's like this thing that you do on top of a very

41
00:03:46,080 --> 00:03:49,760
physics-based, manually designed model, you know, you open up the first textbook right out the

42
00:03:49,760 --> 00:03:54,480
equations. And I think, you know, my group is not the only one working on this. I think this

43
00:03:54,480 --> 00:03:57,840
is actually something that we've seen come up more and more over the past year or two is that

44
00:03:57,840 --> 00:04:04,160
people are thinking, well, can you learn abstractions that you can plan over that are not just purely

45
00:04:04,160 --> 00:04:08,000
predictive models? So these abstractions don't try to predict, you know, if you take this very low

46
00:04:08,000 --> 00:04:12,320
level action, here's what will happen immediately at the next point in time. But they're kind of

47
00:04:12,320 --> 00:04:17,120
higher level abstractions. They're like, if you execute this intention, how will the world change

48
00:04:17,120 --> 00:04:21,520
in some representation that sort of lifts you away from the low level physical grounding?

49
00:04:21,520 --> 00:04:25,200
And by lifting you off from that low level physical grounding, you actually get a much

50
00:04:25,200 --> 00:04:30,240
easier planning problem. So in effect, learning serves to simplify the planning by putting it into

51
00:04:30,240 --> 00:04:37,200
the right abstract representation. And why is that? If the planning still needs to be done at a

52
00:04:37,200 --> 00:04:43,920
low level theory to actually, you know, move the robot or get us from point A to point B.

53
00:04:44,640 --> 00:04:49,200
Yes, exactly. So in effect, the learning kind of takes care of the low level details.

54
00:04:49,760 --> 00:04:55,360
So if you have a model-free learned behavior for, let's say, you know, walking to the door,

55
00:04:55,360 --> 00:04:59,120
then you don't have to plan how you're going to do that. You can plan at the level of, you know,

56
00:04:59,120 --> 00:05:03,920
I want the door open and then kind of, you know, let your body do its thing to get you there.

57
00:05:03,920 --> 00:05:07,520
And I think that's kind of the intuition behind how these methods work. They let the learning

58
00:05:07,520 --> 00:05:12,640
take care of these low level details and then remove them from consideration for the higher level planning.

59
00:05:12,640 --> 00:05:16,160
So it's a kind of a hierarchical approach. I was just going to ask that it sounds like you're

60
00:05:16,800 --> 00:05:25,920
proposing a hierarchical approach in which, as opposed to assuming low level physics-based models,

61
00:05:25,920 --> 00:05:34,240
for example, you are able to, you're maybe learning in both levels of the hierarchy, but the hierarchy

62
00:05:34,240 --> 00:05:42,480
itself simplifies the, doesn't simplify both or just the higher level problem. So the hierarchy

63
00:05:42,480 --> 00:05:46,080
simplifies the higher level problem, but it doesn't, in a sense, simplify the lower level problem

64
00:05:46,080 --> 00:05:50,720
because now the learning component doesn't need to take care of being able to achieve very

65
00:05:50,720 --> 00:05:56,160
temporarily extended goals. So often in reinforcement learning, one of the hardest things is if you have

66
00:05:56,160 --> 00:06:01,440
a task where you don't realize that you've reached a successful outcome until you've performed a

67
00:06:01,440 --> 00:06:06,960
very long sequence of behaviors that are by themselves unrewarding. So in effect, by chunking out

68
00:06:06,960 --> 00:06:13,280
the problem into these intermediate sub-goals, which you can, which the algorithm can basically

69
00:06:13,280 --> 00:06:17,040
invent, so basically make some, you know, here are some places I could go for each place that I've

70
00:06:17,040 --> 00:06:22,720
reached repeatedly. Let me just compartmentalize that into a little skill. So that doesn't require

71
00:06:22,720 --> 00:06:25,920
achieving any temporarily extended goal, like getting to the airport, and then the higher level

72
00:06:25,920 --> 00:06:29,680
planner can go in and say, oh, you know how to reach all these sub-goals. Let me figure out how

73
00:06:29,680 --> 00:06:34,640
to sequence them so that you accomplish your end goal. The notion of hierarchical learning like

74
00:06:34,640 --> 00:06:39,280
this is, is not by any means new, but one of the things that has been sort of coming to fruition

75
00:06:39,280 --> 00:06:44,880
recently is that people have figured out how to abstract away both the behaviors and the states,

76
00:06:44,880 --> 00:06:50,320
right? A planning problem consists of states and the actions, right? So the actions get abstracted

77
00:06:50,320 --> 00:06:54,720
away as these skills and the states get abstracted away via representation learning. And if you have

78
00:06:54,720 --> 00:06:59,440
both of those, then you can get a substantially simplified problem for the higher level planner,

79
00:06:59,440 --> 00:07:03,280
and you can actually get some of these things to work on interesting like image-based tasks,

80
00:07:03,280 --> 00:07:08,320
for example. One of the interesting nuances that came out for me in the way you described that

81
00:07:08,320 --> 00:07:14,880
is when we think about the analogy that you used, I want to get to the airport and I need to take

82
00:07:14,880 --> 00:07:22,480
the bus and the train and the taxi, whatever, that is maybe let's say a top-down kind of plan. I'm

83
00:07:22,480 --> 00:07:29,440
thinking, you know, I had, I start by identifying the intermediate states that's going to get,

84
00:07:29,440 --> 00:07:37,360
that are going to get me to the end goal. But what I thought you just said was that perhaps the

85
00:07:37,360 --> 00:07:44,000
intermediate states can be learned in the process of, you know, you're still specifying the end

86
00:07:44,000 --> 00:07:48,880
goal maybe and intermediate states are learned and, you know, I'm almost thinking like that,

87
00:07:48,880 --> 00:07:55,040
you know, they become kind of a compact or a better optimized representation of memory, you know,

88
00:07:55,040 --> 00:08:00,880
there's a lot of work happening here to try to, you know, in RL to incorporate different memory

89
00:08:00,880 --> 00:08:06,000
schemes so that we're not throwing away what we've learned in the process. This is a little bit.

90
00:08:06,000 --> 00:08:10,640
So it's, so there's always for a hierarchical scheme, there's a top-down in the bottom-up

91
00:08:10,640 --> 00:08:14,880
component and I think it's conventionally actually something that's been a lot, a lot more

92
00:08:14,880 --> 00:08:19,600
common is to think about it mostly as a top-down process. So there's a single task that you want to

93
00:08:19,600 --> 00:08:24,560
accomplish, it's a very complex task and you'll go in and, and sure chunk that task up into pieces.

94
00:08:24,560 --> 00:08:29,520
Yeah. But what has really worked out much better and this is what we use actually in our work

95
00:08:29,520 --> 00:08:34,400
in the main conference too, is that the discovery of the behaviors works very well when it's bottom-up.

96
00:08:34,400 --> 00:08:38,720
Essentially, if you go and try to do something in the world and in the process you've accomplished

97
00:08:38,720 --> 00:08:43,200
accidentally, maybe something else, like maybe, you know, I'm trying to get to the kitchen,

98
00:08:43,200 --> 00:08:47,200
but I happen to have figured out how to reach the bedroom. I'm not thrilled about that, but I can

99
00:08:47,200 --> 00:08:51,600
sort of file that away as something I've learned to reuse later. And this bottom-up discovery of

100
00:08:51,600 --> 00:08:55,520
skills ends up working very well for these kinds of hierarchical methods because you can discover

101
00:08:55,520 --> 00:09:00,080
those skills before you actually know how to solve some really complex tasks. And so is this

102
00:09:00,080 --> 00:09:06,560
analogy to memory one that's useful or? Yeah, I think so. I think in effect, everything that you

103
00:09:07,760 --> 00:09:11,280
experience in the course of solving whatever tasks you have to solve in the world,

104
00:09:11,280 --> 00:09:15,200
even if it's not useful to that task, you sort of file it away in your memory as behaviors that

105
00:09:15,200 --> 00:09:20,880
you can draw on later. And in practice, the way we represent this is not quite so discrete.

106
00:09:20,880 --> 00:09:24,880
It's actually a continuous space of all the possible goals you could reach and you can think of

107
00:09:24,880 --> 00:09:28,800
this as basically filling out that space. So for all the possible states you've seen, can you get

108
00:09:28,800 --> 00:09:33,600
a behavior that reaches them and it's continuous representation not a discrete one? What did you

109
00:09:33,600 --> 00:09:38,560
call this particular kind of line of research? We don't really have a clear name for it, although

110
00:09:38,560 --> 00:09:43,040
maybe we should. So we don't call it hierarchical reinforcement learning because the higher level

111
00:09:43,040 --> 00:09:47,440
here is not really reinforcement learning. So I've been calling it kind of hybridizing planning

112
00:09:47,440 --> 00:09:52,560
on learning. To me, the important bit about is that the learning provides the abstractions to

113
00:09:52,560 --> 00:09:57,680
the planning. So maybe I should come up with a name for it that reflects that. Okay, so that's

114
00:09:59,280 --> 00:10:03,360
that's a couple of the papers that you had. Yeah, so we have two papers on this. We have

115
00:10:04,240 --> 00:10:08,880
one called search on the replay buffer by Benjamin Eisenbach, which applies kind of a non-parametric

116
00:10:08,880 --> 00:10:13,440
graph search approach to it. And then we have another one called learning with goal-conditioned

117
00:10:13,440 --> 00:10:19,600
policies by Serucia Nasseriani, which applies more of a continuous trajectory optimization approach

118
00:10:19,600 --> 00:10:24,160
for the higher level. So where you continue to see optimize over goals. And so are they it sounds

119
00:10:24,160 --> 00:10:30,800
like they're both they're alternate approaches to the higher level optimization problem as opposed

120
00:10:30,800 --> 00:10:39,120
to kind of picking off small pieces of. So the lower level skills actually in both papers are

121
00:10:39,120 --> 00:10:44,560
built by learning goal-conditioned policies with RL. And that's a technique that has been explored

122
00:10:44,560 --> 00:10:48,560
in prior work too. So that's in some sense. It's very important, but it's not really the new part.

123
00:10:48,560 --> 00:10:55,920
Cool. So that's two of the top papers. Yeah. What else? What else? There was another thing

124
00:10:55,920 --> 00:11:00,560
that I could tell you about that is maybe a little bit outside the norm for what we typically do

125
00:11:00,560 --> 00:11:05,120
because it's not quite so focused on autonomous robotic learning, but it's a topic that I think

126
00:11:05,120 --> 00:11:08,880
was very interesting to work on. And maybe something that could be interesting to your listeners too.

127
00:11:08,880 --> 00:11:13,520
So we had a presentation. This was actually an oral presentation by a student who was actually

128
00:11:13,520 --> 00:11:18,640
a visiting student with us named Pim Dehan. And what he worked on was understanding the role of

129
00:11:18,640 --> 00:11:23,600
causality and imitation learning. So you know causality is obviously a big topic. I'm sure you've had

130
00:11:23,600 --> 00:11:28,240
guests on your show talk about it. Not enough though. Not enough. It's something that it took me a

131
00:11:28,240 --> 00:11:32,720
while to understand why this was important. And for that project we really wanted to isolate.

132
00:11:32,720 --> 00:11:38,960
Well, let's pause there. What did you learn after the while? You know, what was your realization

133
00:11:38,960 --> 00:11:45,600
about why it's important? Yeah. So I guess maybe to answer this I should first say why I was not

134
00:11:45,600 --> 00:11:49,680
sure that it was important before. Please. And the reason that I think this is actually a fairly

135
00:11:49,680 --> 00:11:56,000
typical doubt that comes from a reinforcement learning research. And to kind of go back,

136
00:11:56,000 --> 00:12:02,320
you know, inside the onion one more layer maybe, a summary of causality and kind of, yeah,

137
00:12:02,320 --> 00:12:09,200
you know, how do you describe you know causality? Well, so a causal model is a model that

138
00:12:09,200 --> 00:12:14,640
associates causes with effects. Whereas in correlative model is just a model that tries to notice

139
00:12:14,640 --> 00:12:19,600
that, you know, when two things seem to occur together, you can guess in your data what the value

140
00:12:19,600 --> 00:12:23,360
of one of those things will be from the value of the other one. But if the relationship there is

141
00:12:23,360 --> 00:12:27,680
not causal, that doesn't necessarily mean that when you see new things that same relationship will

142
00:12:27,680 --> 00:12:34,240
still hold. So to use an example, it's maybe a bit more relevant for our work. If you imagine

143
00:12:34,240 --> 00:12:39,520
a driving scenario, you have a car, the car in front of you stops that causes you to stop. But if you

144
00:12:39,520 --> 00:12:44,160
stop, the car in front of you will not necessarily stop. It just so happens that if you look at data,

145
00:12:44,160 --> 00:12:48,880
almost always, if your car stops, the car in front of you stops, but the causality goes the other

146
00:12:48,880 --> 00:12:53,040
way, right? So there's a correlation in the data because people don't just stop for no reason,

147
00:12:53,040 --> 00:12:57,680
but you can't exploit that correlation to affect change in the world. And that's tremendously

148
00:12:57,680 --> 00:13:00,880
important for robotics, of course, because the whole point is to do things that affect change in

149
00:13:00,880 --> 00:13:05,360
the world. But you didn't think that was important for a while. I didn't think that was important

150
00:13:05,360 --> 00:13:09,840
for the following reason, which is that if you train predictive models and you use those models to

151
00:13:09,840 --> 00:13:14,960
act, then maybe your models will make mistakes. But when you go and use them to act, if you actually

152
00:13:14,960 --> 00:13:18,560
try stopping to get the guy in front of you to stop and you notice that it isn't working,

153
00:13:18,560 --> 00:13:22,320
then you will update your model. You'll learn that. Yeah, you'll patch up that whole. And that's

154
00:13:22,320 --> 00:13:25,920
why I said earlier that oftentimes we're enforcing learning researchers. Maybe don't worry about

155
00:13:25,920 --> 00:13:30,720
these things so much because once you're in this sort of closed loop scenario where you're constantly

156
00:13:30,720 --> 00:13:36,240
updating your model, maybe those correlations very quickly get fixed up and turn into causal

157
00:13:36,240 --> 00:13:39,120
relationships because otherwise you make mistakes and you have to fix those mistakes.

158
00:13:40,560 --> 00:13:46,240
But we did figure out a particular setting where this is very important that is actually a setting

159
00:13:46,240 --> 00:13:51,040
where the issue even comes up in practice, like in actual practical things that are deployed there

160
00:13:51,040 --> 00:13:55,360
in the world right now, which is imitation learning. So imitation learning is a very simple and very

161
00:13:55,360 --> 00:14:00,640
powerful tool that people use today for things like autonomous driving and many other about

162
00:14:00,640 --> 00:14:06,000
applications where you collect data from a person operating a machine, maybe driving a car,

163
00:14:06,000 --> 00:14:10,000
and then you just treat that as a supervised learning problem. So predict the action of the person

164
00:14:10,000 --> 00:14:14,480
took from the observations, which seems like a great idea like for driving you can get lots and

165
00:14:14,480 --> 00:14:18,800
lots of data doing this. But there are all sorts of correlations that happen like that stopping

166
00:14:18,800 --> 00:14:23,680
example that I gave. Another correlation could be that maybe you turn on the turning signal when

167
00:14:23,680 --> 00:14:28,560
you're about to turn and then you turn. But of course the turn signal is not what caused you to turn.

168
00:14:28,560 --> 00:14:34,080
What caused you to turn is your intention to make a right turn. And that's perhaps a bit risky,

169
00:14:34,080 --> 00:14:38,480
you know, typically your camera and your car will be mounted outside the car, but if it happens

170
00:14:38,480 --> 00:14:42,000
to be inside the car and you can see your turn signal that it might decide, well, I'll just wait

171
00:14:42,000 --> 00:14:47,920
for someone to turn on the turn signal before I turn, which makes sense of course. So there are

172
00:14:47,920 --> 00:14:52,080
all sorts of these correlations that can happen. And the way it shows up in practice is that when

173
00:14:52,080 --> 00:14:58,640
people add more inputs to their system, maybe they'll have a camera, they'll also add a history,

174
00:14:58,640 --> 00:15:02,720
or they'll add a LiDAR, although that's some other sensor. They sometimes find that their

175
00:15:02,720 --> 00:15:06,800
imitation learning system actually does worse. And why does it do worse? Well, because all those

176
00:15:06,800 --> 00:15:11,680
additional sensors that they're adding add more potential correlations, which might result in

177
00:15:11,680 --> 00:15:20,000
this causal confusion. So one of the things that we did with PIM is that we worked on formalizing

178
00:15:20,000 --> 00:15:25,040
this problem, identifying why it happens, where it happens, reproducing it kind of in a little

179
00:15:25,040 --> 00:15:30,640
peach tradition, some toy domains, and then coming up with some simple causal discovery techniques

180
00:15:30,640 --> 00:15:35,600
that could be used to partially address it. So our solution is not perfect, it doesn't fix it

181
00:15:35,600 --> 00:15:40,400
every time. But to me, the thing that's pretty exciting about this work is that it actually

182
00:15:40,400 --> 00:15:45,040
allows us to make a little bit more formal something that people have observed anecdotally

183
00:15:45,040 --> 00:15:49,040
before, which is this phenomena that adding more information to your imitation learning system

184
00:15:49,040 --> 00:15:53,120
makes it do worse. And I'm pretty happy with that work because I think it's also a technique

185
00:15:53,120 --> 00:15:56,720
that people are using in the real world, and if they're not aware of this issue, they might get

186
00:15:56,720 --> 00:16:02,000
into a bit of trouble. Can you give us a sense for the technique itself, how what it's doing?

187
00:16:02,000 --> 00:16:06,480
Yeah, it's actually kind of neat. So this is something that PIM and Dinesh, the LiDAR

188
00:16:06,480 --> 00:16:14,640
authors on this, came up with. The idea is that you can think about discovering causal graphs.

189
00:16:14,640 --> 00:16:17,280
So if you have a bunch of variables, let's say you have a discrete set of variables,

190
00:16:18,480 --> 00:16:22,400
the variables you're predicting are the actions, the variables that you're observing are the

191
00:16:22,400 --> 00:16:28,480
observation variables of states, and some of those state variables are actual causes of the actions,

192
00:16:28,480 --> 00:16:33,360
and some of them are are are spurious, some of them you should not pay attention to, like the

193
00:16:33,360 --> 00:16:38,640
turn signal or whatever. Now the trouble is that in a if your observations are things like

194
00:16:38,640 --> 00:16:42,960
images, you can't just treat every pixels a possible cause. You know, there's a lot more structure

195
00:16:42,960 --> 00:16:48,080
underneath. So what they did is they actually combined a representation learning phase where you

196
00:16:48,080 --> 00:16:52,800
actually take the your observations and you turn them into a smaller set of independent latent

197
00:16:52,800 --> 00:16:58,880
variables with a causal graph discovery phase. So they have one model that learns this representation,

198
00:16:58,880 --> 00:17:03,760
these disentangled variables, and then they have a second phase where they train a model that

199
00:17:03,760 --> 00:17:09,840
takes in those variables and a mask that represents the graph. So the graphs always map inputs to

200
00:17:09,840 --> 00:17:14,480
outputs. So basically you can represent all possible graphs with a vector of bits that says which

201
00:17:14,480 --> 00:17:21,040
edges there and which edges absent. So is the graph learned or the graph is the graph imposed?

202
00:17:21,040 --> 00:17:25,440
It's actually a little weird. It's the model actually simultaneously represents all possible

203
00:17:25,440 --> 00:17:29,680
graphs, which seems really hard because they're exponentially many graphs. But of course the

204
00:17:29,680 --> 00:17:34,560
wonderful thing with neural nets is that they do generalize. So it turns out that you don't have to

205
00:17:34,560 --> 00:17:40,240
have it train on every possible graph to get it to generalize meaningfully to all or most possible

206
00:17:40,240 --> 00:17:46,080
graphs. So you basically randomly delete or remove edges and then train it on on these randomly

207
00:17:46,080 --> 00:17:50,400
selected graphs and then hope that it generalize to the others. And so long as those training graphs

208
00:17:50,400 --> 00:17:55,680
are drawn from the right distribution then you should get generalization if you sample enough graphs.

209
00:17:55,680 --> 00:17:59,440
Now that doesn't let you discover the right graph. It just lets you compactly represent all the

210
00:17:59,440 --> 00:18:05,120
possible graphs. The discovery then requires intervention. So to discover which of those graphs

211
00:18:05,120 --> 00:18:08,640
is correct you have to somehow break the correlations in your data. So you do that by actually

212
00:18:08,640 --> 00:18:13,360
attempting the task but a very very small number of times just so you can figure out which of those

213
00:18:13,360 --> 00:18:19,440
graphs is the right one. And those attempts can have one of two forms either you attempt the task

214
00:18:19,440 --> 00:18:23,920
and you ask for additional human supervision so you say well oh if I were in this intersection

215
00:18:23,920 --> 00:18:28,240
how should you drive or you assume that you have access to a reward function so you try

216
00:18:28,240 --> 00:18:32,720
yourself and then you look at the rewards. Of course in both cases you could learn the task

217
00:18:32,720 --> 00:18:37,440
entirely from scratch if you have enough interventions but the point is that if you have these

218
00:18:37,440 --> 00:18:42,080
pre-trained candidate graphs you can discover the right one with a very very small number of

219
00:18:42,080 --> 00:18:50,720
interventions. Okay I'm trying to wrap my head around all this. It sounds like if I'm understanding

220
00:18:50,720 --> 00:19:00,240
what's happening that the training effort is like some exponential multiplier on the training

221
00:19:00,240 --> 00:19:06,560
effort for not trying to figure out this causality. In other words it's not a lot harder because

222
00:19:06,560 --> 00:19:13,840
you have to it's almost like you are coming up with this graph and then applying like a dropout

223
00:19:13,840 --> 00:19:20,080
kind of thing where you're breaking these connections and you've got to you know in the graph is

224
00:19:20,080 --> 00:19:26,400
potentially you know you have a kind of end by and fully connected thing like it I'm imagining it

225
00:19:26,400 --> 00:19:32,560
exploding. Yeah well so the reason that this ends up being not exponentially difficult is that

226
00:19:32,560 --> 00:19:36,960
there's a bit of regularity so there's a particular variable and you include that variable or exclude

227
00:19:36,960 --> 00:19:42,880
that variable. While in principle the behavior of the resulting model depends a lot on all the other

228
00:19:42,880 --> 00:19:49,120
variables in practice you can generally get a pretty good idea for the for whether that variables

229
00:19:49,120 --> 00:19:54,640
of cause or not just from whether it by itself is excluded or not. So if you exclude for example the

230
00:19:54,640 --> 00:20:01,760
extra turn signal input and things just kind of work okay maybe the right graph is like no turn

231
00:20:01,760 --> 00:20:06,560
signal and some of these other things but just from the fact that you included the turn signal in

232
00:20:06,560 --> 00:20:10,320
these five graphs and excluded in these five graphs that's a really good hint that the turn

233
00:20:10,320 --> 00:20:15,680
signal is should not be included. So while in theory the whole thing is exponentially bad in

234
00:20:15,680 --> 00:20:19,920
practice for most problems that's not usually the case and you can identify these individually

235
00:20:19,920 --> 00:20:25,120
so that's why we can get away with substantially less than exponential time training for this

236
00:20:25,120 --> 00:20:29,600
even though in principle there are exponentially many graphs. Of course you can construct pathological

237
00:20:29,600 --> 00:20:34,800
problems where this would not work. So if there's some very very specific set of edges that works

238
00:20:34,800 --> 00:20:38,480
and everything else fails then through random sampling you probably won't discover that.

239
00:20:38,480 --> 00:20:46,320
Right right. You mentioned that you formulated a toy problem to illustrate this presumably that's

240
00:20:46,320 --> 00:20:52,400
also the problem that you presented your experimental results in the context of tell us a little

241
00:20:52,400 --> 00:20:56,560
bit about those problems. It's actually a very simple toy problem we just took an Atari game

242
00:20:56,560 --> 00:21:01,600
and we drew on the screen a little number that indicates what action was taken at the last

243
00:21:01,600 --> 00:21:06,080
time step. You think this would be completely innocuous because the action you already took that

244
00:21:06,080 --> 00:21:12,560
action and just draw it on the screen but for good Atari playing strategies usually the current

245
00:21:12,560 --> 00:21:16,800
action is strongly correlated with the previous one. So if you're moving the pong paddle down you

246
00:21:16,800 --> 00:21:21,120
probably move it down for several steps. So there's a strong temporal correlation which means that

247
00:21:21,120 --> 00:21:26,240
drawing that previous action as a digit on the screen gives the imitator a very strong hint about

248
00:21:26,240 --> 00:21:32,400
the next action. But of course that's not a causal relationship because the action you took before

249
00:21:32,400 --> 00:21:37,040
is not the cause of your next action. So it turns out that just adding that digit to the screen for

250
00:21:37,040 --> 00:21:42,800
an Atari game completely breaks your ability to imitate an optimal Atari player which is kind of

251
00:21:42,800 --> 00:21:51,840
a disturbing thing to me. Why is that because the agent kind of over indexes on the thing that you

252
00:21:51,840 --> 00:21:57,920
drew or yeah it's because basically learning machines like to be lazy if they can be lazy. So very

253
00:21:57,920 --> 00:22:02,880
lazy strategies to just say hey I know that the next action strongly correlates with the previous

254
00:22:02,880 --> 00:22:08,160
one not always but it's a pretty good clue. So it's very easy for me to read this digit. It's very

255
00:22:08,160 --> 00:22:12,240
hard for me to figure out what's going on in the game. So I'll just start off by looking at that

256
00:22:12,240 --> 00:22:17,280
little hint that you gave me and I can get some decent performance out of that and I'm so happy

257
00:22:17,280 --> 00:22:20,720
with that that I'll kind of not really pay as much attention to everything else and never bother

258
00:22:20,720 --> 00:22:26,560
learning the real task. So it of course doesn't pay attention exclusively to that thing but it

259
00:22:26,560 --> 00:22:32,160
pays so much attention to it that once that relationship is broken once the action is not

260
00:22:32,160 --> 00:22:36,000
actually coming from an optimal policy once it has to play for itself the performance just tanks.

261
00:22:36,880 --> 00:22:43,520
And so since you're optimizing on the ultimate ultimate outcome it's kind of like

262
00:22:43,520 --> 00:22:52,160
in a sense crude sense like a multitask thing it's like find an optimal policy given

263
00:22:54,560 --> 00:23:00,880
the previous action. Well there's an important distinction which is that imitation learning

264
00:23:00,880 --> 00:23:06,720
is not explicitly trying to find the optimal policy. So this is one of the things about

265
00:23:06,720 --> 00:23:11,360
annotation learning that's a little bit of a shortcoming but it's why it's so simple is that

266
00:23:11,360 --> 00:23:14,560
imitation doesn't care about what's good or what's bad it just cares about what the

267
00:23:14,560 --> 00:23:18,960
demonstrator did. Which is great because you can train it from data without all this complexity

268
00:23:18,960 --> 00:23:23,280
of reinforcement learning but it's not so great because of course it can't understand what's

269
00:23:23,280 --> 00:23:29,360
optimal and what's not. Did you do it and can I copy it? I'm still trying to kind of narrow down

270
00:23:29,360 --> 00:23:35,120
what exactly we've done by drawing the number because we already knew the thing that the agent

271
00:23:35,120 --> 00:23:40,000
is trying to imitate is what was done. It seems like it almost changed the problem.

272
00:23:40,000 --> 00:23:46,400
Yeah. But the subtlety there is that the data that you trained the agent on all came from your

273
00:23:46,400 --> 00:23:52,480
expert demonstrator. So while when the agent is playing the game itself that previous action

274
00:23:52,480 --> 00:23:57,680
indicator doesn't really give it any additional information. When it's trying to copy the expert

275
00:23:57,680 --> 00:24:01,920
it does actually give it a bit of additional information because it tells it that the action

276
00:24:01,920 --> 00:24:05,760
that the expert took in the previous state which is probably pretty similar to the current one.

277
00:24:05,760 --> 00:24:15,520
So it is in the otherwise formulated imitation learning problem the agent doesn't have

278
00:24:15,520 --> 00:24:21,520
access to the action itself. It has access to the observation of. Yeah. So it's supposed to

279
00:24:21,520 --> 00:24:25,440
predict the action. So in the same way that an image class far predicts the label. Yeah.

280
00:24:25,440 --> 00:24:30,160
You're noticing predicts the action from you. Yeah. What other interesting things do you

281
00:24:30,160 --> 00:24:35,920
presenting here? So there is one more thing that maybe would be interesting to discuss a little

282
00:24:35,920 --> 00:24:41,120
bit which is some work by a student named Michael Jan or studying model-based reinforcement learning.

283
00:24:41,120 --> 00:24:46,000
Okay. So model-based reinforcement learning is something that we often think about as being

284
00:24:46,000 --> 00:24:51,040
kind of a more efficient but perhaps somewhat more complicated solution to reinforcement learning

285
00:24:51,040 --> 00:24:54,400
problems as opposed to model-free reinforcement learning. Model-based reinforcement learning

286
00:24:54,400 --> 00:24:59,920
first learn how to predict the future and then you use that predictive model to actually

287
00:24:59,920 --> 00:25:04,720
act in the world to accomplish some tasks. And what Michael wanted to understand is can we

288
00:25:04,720 --> 00:25:11,120
actually analyze the degree to which model-based reinforcement learning provably results in a

289
00:25:11,120 --> 00:25:15,760
better policy at each iteration. So there's been a lot of analysis in model-free RL,

290
00:25:15,760 --> 00:25:20,240
particularly for policy gradients. So we did some work for example back in 2014 with John

291
00:25:20,240 --> 00:25:24,480
Schollman on the Trust Research and Policy Optimization algorithm TRPO where we did analyze

292
00:25:25,440 --> 00:25:28,720
that under some circumstances you provably get an improvement in your policy.

293
00:25:28,720 --> 00:25:32,080
Admittedly under some assumptions that are pretty strong like having infinite samples,

294
00:25:32,080 --> 00:25:37,280
but it's a beginnings of a theory. But such a thing did not exist for model-based RL and one of

295
00:25:37,280 --> 00:25:42,160
the things that Michael and the other students on the paper were able to figure out is that you

296
00:25:42,160 --> 00:25:48,080
can actually write down a similar kind of theoretical proof that given enough samples model-based

297
00:25:48,080 --> 00:25:53,920
reinforcement learning will produce an improvement at each iteration and how much an improvement

298
00:25:53,920 --> 00:25:58,720
it achieves depends on the error in the model obviously and also how much you change your policy.

299
00:25:58,720 --> 00:26:03,360
So if you collect some data and then change your policy be totally different now on that totally

300
00:26:03,360 --> 00:26:06,960
different policy the model will probably be very inaccurate so that's no good. So you need to change

301
00:26:06,960 --> 00:26:11,840
the policy by bounded amount and change the model by bounded amount. That's all fairly straightforward.

302
00:26:11,840 --> 00:26:16,480
But there's another term that showed up in the in the bound that we had which is the number of

303
00:26:16,480 --> 00:26:21,680
time steps for which you utilize the model. So if I predict five steps in the future with my model,

304
00:26:21,680 --> 00:26:25,840
I'll get lower error than if I predict 50 steps in the future. That's again pretty obvious.

305
00:26:26,480 --> 00:26:30,000
What was not obvious is that if you actually look at that bound you know the bound is derived

306
00:26:30,000 --> 00:26:35,760
using standard proof techniques that people have used in other RL model free RL work.

307
00:26:36,400 --> 00:26:40,640
If you actually try to find the optimal value for that horizon you end up with the optimal value

308
00:26:40,640 --> 00:26:45,440
being zero. So it says you know the proof says the model will result in a model they don't use.

309
00:26:45,440 --> 00:26:50,480
Yeah. Yeah. The proof says you will improve but the most improvement is obtained by just not

310
00:26:50,480 --> 00:26:55,280
using your model. That doesn't seem to make any sense that defies our intuition. It also defies

311
00:26:55,280 --> 00:27:00,800
our experimental results. So to a degree this is illustrating some shortcomings in the current

312
00:27:00,800 --> 00:27:04,080
theoretical tools that we have. But then one of the things that we did is we tried to

313
00:27:05,760 --> 00:27:12,240
that number the number of time steps is multiplied by some coefficient which theoretically you know

314
00:27:12,240 --> 00:27:16,640
whenever you do theory you derive sort of the most pessimistic coefficients because you want to

315
00:27:16,640 --> 00:27:20,640
always be true. But then we tried to measure that empirically we tried to actually train some models

316
00:27:20,640 --> 00:27:25,200
train some policies and measure those error terms and see what do they look like in reality.

317
00:27:25,200 --> 00:27:29,760
In reality they're not nearly as pessimistic and in reality models seem to generalize a lot better

318
00:27:29,760 --> 00:27:33,200
than the theory would suggest. So then we did something that you shouldn't really do when

319
00:27:33,200 --> 00:27:37,600
you're doing theoretical work which is that we sort of eyeballed what the actual relationship

320
00:27:37,600 --> 00:27:42,000
and the experiments looks like kind of basically did a linear fit. Subtune that term for the

321
00:27:42,000 --> 00:27:45,680
theoretically motivated term in the bound and then we do actually get a trade off that says

322
00:27:45,680 --> 00:27:50,560
that with the actual observed error patterns you should not use zero length roll outs.

323
00:27:50,560 --> 00:27:53,920
You should also not use very long roll outs. You should limit how many time steps you use your

324
00:27:53,920 --> 00:27:59,280
model for and it turns out that if you actually do that if you use your model for only short periods

325
00:27:59,280 --> 00:28:03,680
you actually get a much better algorithm than anything that was done in previous model based

326
00:28:03,680 --> 00:28:07,360
our all work at least at the time when we published the paper since then of course much better

327
00:28:07,360 --> 00:28:13,520
things have come out. So if you arrived at an analytical relationship between the number of

328
00:28:13,520 --> 00:28:20,480
time steps in some characterization of the problem. Well yes except that this is often the case

329
00:28:20,480 --> 00:28:25,520
for kind of theoretical analysis is that there is a relationship and you can derive the optimal horizon

330
00:28:25,520 --> 00:28:29,200
except that it's derived in terms of quantities that are in practice very difficult to measure.

331
00:28:29,200 --> 00:28:35,360
So probably the actionable conclusion is that you should use a length that is not too long

332
00:28:35,360 --> 00:28:40,400
and not too short probably if you're solving a real problem you're going to select that length

333
00:28:40,400 --> 00:28:50,320
empirically. And so does the takeaway is that the the later analytical result kind of nullifies

334
00:28:50,320 --> 00:28:56,560
the initial analytical result but in practice it kind of works the way we would expect anyway.

335
00:28:56,560 --> 00:29:01,920
So that was a lot very hand wavy. How do you how do you how do you attribute value to the you know

336
00:29:01,920 --> 00:29:07,120
these sets of results? Well I think it works the way we expect it to a degree. I do think that

337
00:29:07,120 --> 00:29:13,120
there were some actionable conclusions from it like for example we did end up with sort of after

338
00:29:13,120 --> 00:29:17,040
combining the empirical observation in the theory we ended up with with an algorithm that uses

339
00:29:17,040 --> 00:29:20,800
pretty short rollouts from the model probably you know shorter than what people typically thought

340
00:29:20,800 --> 00:29:25,920
would be suitable and that did end up working very well. I think it also tells us a little bit about

341
00:29:25,920 --> 00:29:29,680
where we should look to next if we're going to develop better theoretical tools so tells us that

342
00:29:29,680 --> 00:29:34,000
you know you can do some of this analysis but the current tools give you a slightly nonsensical

343
00:29:34,000 --> 00:29:39,360
result there's some term if you substitute in the will you empirically measure that term to be

344
00:29:39,360 --> 00:29:43,040
into the theory you get a much more reasonable conclusion so that maybe give us some guidance

345
00:29:43,040 --> 00:29:47,680
where we can look to next. And I think that does it also say that we should all shift our efforts

346
00:29:47,680 --> 00:29:54,240
to model based RL as opposed to model free RL because it's provably better? Not necessarily so

347
00:29:54,240 --> 00:29:58,240
what does the result say? It says that with model based RL it could improve your policy further

348
00:29:58,240 --> 00:30:02,640
than you could with just model free RL but how much further depends on the error in your model?

349
00:30:02,640 --> 00:30:06,560
So in some cases you can get models with this is very obvious statement in some ways in some

350
00:30:06,560 --> 00:30:10,000
case you can get models with very low error in some cases with very high error if you can get a

351
00:30:10,000 --> 00:30:14,560
model with low error then it seems like you should use it if you can get a model but in some case

352
00:30:14,560 --> 00:30:19,280
that's that's very hard to do so if you have very complex let's say image observations maybe you

353
00:30:19,280 --> 00:30:23,760
really just can't get a model that's accurate enough for more than you know what a single time

354
00:30:23,760 --> 00:30:28,960
somewhere even no time steps into the future so then you're still out of luck. And so one thing

355
00:30:28,960 --> 00:30:39,760
that I often do is maybe conflate the idea of model based as like incorporating physical world

356
00:30:39,760 --> 00:30:45,600
models with what you're talking about which sounds different like you know learn models almost

357
00:30:45,600 --> 00:30:53,600
another type of hierarchical RL. Yeah so for this discussion I was just talking entirely about

358
00:30:53,600 --> 00:30:59,200
learned models right that said the the results and the analysis doesn't really distinguish between

359
00:30:59,200 --> 00:31:03,280
them and there's a very blurry line between them so if you if you do have a physics-based model let's

360
00:31:03,280 --> 00:31:08,320
say you have a CAD model of your robot and a physics simulator and maybe you don't know the masses

361
00:31:08,320 --> 00:31:12,640
of some of the links you can view the process of identifying those masses as a learning process

362
00:31:12,640 --> 00:31:16,240
so you're learning a very small number of parameters we typically work with much more generic

363
00:31:16,240 --> 00:31:20,480
models so basically neural nets that predict the future so there you have a very large number of

364
00:31:20,480 --> 00:31:25,760
parameters but in principle the same lesson should apply to both. Does this particular research

365
00:31:26,880 --> 00:31:31,920
parameterize the relative benefit in terms of number of parameters or is that interesting for you?

366
00:31:32,720 --> 00:31:36,880
We typically don't worry too much about the number of parameters because we're more concerned

367
00:31:36,880 --> 00:31:41,360
with sort of how well we can get this thing to work and how efficiently in terms of data.

368
00:31:41,360 --> 00:31:44,480
This is this is maybe like a little bit of a philosophical point you know I kind of trust

369
00:31:44,480 --> 00:31:48,800
the systems people and the architecture people I think they'll build better chips for us and

370
00:31:48,800 --> 00:31:53,600
we'll be able to have lots of parameters and also in general in deep learning research

371
00:31:53,600 --> 00:31:58,240
one of the things we found time and time again is that when you have a huge number of parameters

372
00:31:58,240 --> 00:32:02,480
things seem to just work better and right now people actually starting to understand why

373
00:32:02,480 --> 00:32:05,920
they're starting to understand that over parameterization is actually a very good thing for

374
00:32:05,920 --> 00:32:11,920
optimization so it seems like trying to keep the number of parameters down is not such a rewarding

375
00:32:11,920 --> 00:32:16,960
process so we're more concerned with data because data comes at a cost and final performance.

376
00:32:16,960 --> 00:32:23,760
Okay so this particular result then to kind of summarize is not necessarily

377
00:32:24,800 --> 00:32:28,800
an about face or shift in perspective from the last time we talked that was very much

378
00:32:29,920 --> 00:32:35,840
in the camp of kind of an end-to-end learned approach with a highly parameterized model you

379
00:32:35,840 --> 00:32:41,440
know a deep neural network as your core model you know towards hey this result says that we should

380
00:32:41,440 --> 00:32:48,080
be incorporating more physics-based models into our RL. Yeah it doesn't necessarily say that

381
00:32:48,080 --> 00:32:52,560
although of course if you do have the luxury of having some knowledge about physics you'll probably

382
00:32:52,560 --> 00:32:56,320
have a more accurate model which means that you'll be able to get more improvement at every step

383
00:32:56,320 --> 00:33:01,280
of this process but yeah it's really all about how much error you have. And you've also got some

384
00:33:01,280 --> 00:33:07,120
work being presented here on off-policy RL? Yeah so this is this is an area that I've been

385
00:33:07,120 --> 00:33:11,840
pretty passionate about for a while because I think it's it's important in robotics it's also

386
00:33:11,840 --> 00:33:17,040
very important in many fields outside of robotics so basically when we think about reinforcement

387
00:33:17,040 --> 00:33:21,520
learning we typically think about it as a very active process so if you open up the you know the

388
00:33:21,520 --> 00:33:25,840
Sutton and Bartow textbook you see this diagram the agent interacts with the world and then it

389
00:33:25,840 --> 00:33:29,440
improves its policy and then interacts with the world some more it's fundamentally an online

390
00:33:29,440 --> 00:33:35,920
active thing but if we want models that generalize very well we want to be able to train those

391
00:33:35,920 --> 00:33:39,760
models on large amounts of data because that's where you get your generalization so if you want

392
00:33:39,760 --> 00:33:44,880
you know to have very good computer vision systems as part of your RL policy you need to see lots

393
00:33:44,880 --> 00:33:50,640
of images and it's very hard to have lots of data and to have this fundamentally active online

394
00:33:50,640 --> 00:33:55,120
learning process. If you need to collect data every time you improve your policy and you need

395
00:33:55,120 --> 00:33:58,880
image net size data sets you essentially need to collect something the size of image net

396
00:33:58,880 --> 00:34:02,960
improve your policy then throw it out and then collect it again and that's just not a very scalable

397
00:34:02,960 --> 00:34:10,480
proposition. So what we've been working on a fair bit is this problem of off policy or offline

398
00:34:10,480 --> 00:34:13,440
reinforcement learning is sometimes they're called fully off policy it's also sometimes referred

399
00:34:13,440 --> 00:34:17,200
as battery reinforcement learning people can't seem to agree on the name but the basic idea is

400
00:34:17,200 --> 00:34:21,840
that you have some data and in the most extreme version you're not even collect allowed to collect

401
00:34:21,840 --> 00:34:26,800
any more data the data is all you've got you just have to extract the best policy you can out of it.

402
00:34:26,800 --> 00:34:30,400
In reality we'll probably do something a little in between we'd use the data and then interact

403
00:34:30,400 --> 00:34:33,920
with the world a little bit but let's just say we were not allowed to interact at all that's kind

404
00:34:33,920 --> 00:34:40,000
of the the most rigid formulation. It turns out that a lot of standard RL methods like Q

405
00:34:40,000 --> 00:34:44,960
learning for example while in principle applicable to that setting in practice perform very very

406
00:34:44,960 --> 00:34:49,200
poorly and people have tried this before and they look at these learning curves where you're

407
00:34:49,200 --> 00:34:53,760
using fully off policy data your policy seems to get better and then it gets worse and they look

408
00:34:53,760 --> 00:34:57,680
at it and say well maybe I have an overfitting problem like that that looks like overfitting

409
00:34:57,680 --> 00:35:01,760
why don't I add more data they add more data same thing happens it's like okay what what is this

410
00:35:01,760 --> 00:35:05,440
an overfitting problem that doesn't go away as you add more data. Turns out that it's not an

411
00:35:05,440 --> 00:35:09,600
overfitting problem turns out that what's happening is that the structure of the Q learning algorithm

412
00:35:09,600 --> 00:35:14,800
itself actually causes it to perform very poorly if it's not allowed to interact with the world on

413
00:35:14,800 --> 00:35:19,760
its own and it actually ties a little bit almost to that causality point that I mentioned before yeah

414
00:35:19,760 --> 00:35:26,240
see in Q learning you you're making counterfactual queries people often don't don't realize this

415
00:35:26,240 --> 00:35:32,080
where does the counterfactual query come up it comes up when you calculate a target value right

416
00:35:32,080 --> 00:35:36,560
because in Q learning you're saying you took this action you got this reward and you're going to

417
00:35:36,560 --> 00:35:40,000
land in this state but then you're going to run a different policy not the one that was used in

418
00:35:40,000 --> 00:35:44,800
the data so ask your Q function how good that new policy would be you don't get to actually run

419
00:35:44,800 --> 00:35:49,520
that policy you just have to ask your Q function so that means plug in a different action

420
00:35:49,520 --> 00:35:53,440
and that action that you plug in is not the action that was taken in the data and people say okay

421
00:35:53,440 --> 00:35:58,480
that's okay the Q function will generalize and it will generalize if the distribution matches

422
00:35:58,480 --> 00:36:02,560
the distribution it was trained on so you can plug in a different action that's okay but you can

423
00:36:02,560 --> 00:36:06,880
plug in an action that comes from a different distribution and when you optimize your policy

424
00:36:06,880 --> 00:36:11,200
of course your policy is going to find a different distribution in fact if your policy can find

425
00:36:11,200 --> 00:36:15,760
an action for which your Q function makes a mistake and erroneously predicts a very high value

426
00:36:15,760 --> 00:36:19,360
it will find that because that's what you're asking you to do so essentially your policy ends up

427
00:36:19,360 --> 00:36:24,880
exploiting your Q function essentially comes up with like an adversarial action it fools your

428
00:36:24,880 --> 00:36:28,240
Q function to thinking that it's a good one and then because you don't interact with the world

429
00:36:28,240 --> 00:36:31,520
because you don't actually end up trying that action you never learn that it's actually bad

430
00:36:32,240 --> 00:36:36,080
in the extreme case you could imagine there's some action that was never ever taken in the data

431
00:36:36,640 --> 00:36:40,560
your Q function will make some completely nonsensical prediction for it and if that prediction

432
00:36:41,200 --> 00:36:46,000
is large it's a large number then your policy will just start taking that action yeah

433
00:36:46,000 --> 00:36:50,880
so it's not an overfitting problem it's actually this kind of counterfactual out of distribution

434
00:36:50,880 --> 00:36:55,440
action problem and once you recognize it for what it is then you can actually study

435
00:36:56,000 --> 00:37:01,280
possible solutions and so why does the existence of this counterfactual problem

436
00:37:02,000 --> 00:37:08,080
manifests itself more acutely in offline well because in online learning you would still make

437
00:37:08,080 --> 00:37:12,400
that mistake but then you would go and take that action and then you would add it to your training

438
00:37:12,400 --> 00:37:16,240
action but if you're not allowed to interact with the world then you don't get the opportunity to do

439
00:37:16,240 --> 00:37:20,560
we're actually not the only ones to recognize this there was so there was also some wonderful work

440
00:37:20,560 --> 00:37:26,320
by a student named Scott Fujimoto who also had a paper that studies kind of a similar type of problem

441
00:37:27,280 --> 00:37:32,320
one of the things that seemed to work out really well in our work is the particular formulation

442
00:37:32,320 --> 00:37:37,440
for the constraint that you can use to alleviate that issue and that turns out to work actually very

443
00:37:37,440 --> 00:37:43,200
well for a lot of offline problems so we have the evaluation this conference is kind of on standard

444
00:37:43,200 --> 00:37:48,640
benchmark tasks but now we're looking to see if we can use it for actual robotics tasks another

445
00:37:48,640 --> 00:37:52,560
thing that I think is super exciting about this line of work is that once you have this fully

446
00:37:52,560 --> 00:37:58,960
data driven way of doing RL you could also imagine applying RL to domains where traditionally

447
00:37:58,960 --> 00:38:03,680
online active collection is very very hard like for example medical applications you don't want to

448
00:38:03,680 --> 00:38:07,600
run a reinforcement learning agent interact with the real patients but maybe you can get some logs

449
00:38:08,720 --> 00:38:14,800
you know maybe applications for e-commerce for educational support agents you know decision-making

450
00:38:14,800 --> 00:38:19,040
support that sort of thing these are all areas where you can get data but it's very costly and

451
00:38:19,040 --> 00:38:25,120
dangerous to actually have active interactions the example you use in the in describing the

452
00:38:25,120 --> 00:38:34,480
off policy offline RL was something like an image net if image net is the data that the data set

453
00:38:34,480 --> 00:38:40,000
that you're working on what's an example of the kind of the problem formulation or the thing

454
00:38:40,000 --> 00:38:45,440
that you're trying to learn yeah so I was maybe a little quick in saying that so talking about a

455
00:38:45,440 --> 00:38:51,040
data set generally I meant image net size got it so so you know image net is a giant data set that

456
00:38:51,040 --> 00:38:55,120
we know enables generalization it's not I mean it's not an RL data sets an image classification

457
00:38:55,120 --> 00:38:58,800
got it got it but if you imagine that you need similar generalization RL you probably need a

458
00:38:58,800 --> 00:39:06,240
similar scale so in the order of you know millions of samples sure sure and so are there what are

459
00:39:06,240 --> 00:39:12,160
the data sets you know for which you are a experiment or with which you're experimenting with

460
00:39:12,160 --> 00:39:18,720
off policy yeah so in the paper that we have in the main conference we you know this is just kind

461
00:39:18,720 --> 00:39:24,560
of standard benchmark tasks so we basically took regular RL benchmarks like the the open AI

462
00:39:24,560 --> 00:39:30,160
gym benchmarks and we made them off policy so that's that's not a that's not an application that's

463
00:39:30,160 --> 00:39:35,760
just a you know a little benchmarking procedure and how do you do that do you just have an agent go

464
00:39:35,760 --> 00:39:40,240
do a bunch of stuff and then erase a bunch of the data or yeah we just we just have we just have

465
00:39:40,240 --> 00:39:44,640
some existing agent interact with a task save the data to disk and then pretend as though we're

466
00:39:44,640 --> 00:39:50,000
given that data okay but that's just for testing um one of the things that we've been doing since

467
00:39:50,000 --> 00:39:54,640
then which is not part of this paper but it's actually something we'll be presenting at a workshop

468
00:39:54,640 --> 00:39:59,920
is actually trying to collect such a data set for real uh we actually called it RoboNet uh and

469
00:39:59,920 --> 00:40:04,240
this was actually kind of a joint effort with a number of different universities so uh we had

470
00:40:05,280 --> 00:40:10,640
some folks from from Stanford Chelsea Finn and her lab some folks from University of Pennsylvania

471
00:40:10,640 --> 00:40:16,480
uh some folks from Carnegie Mellon contribute data to this uh large data set of robotic interactions

472
00:40:16,480 --> 00:40:20,720
and one of the things that we did that was a little unusual is that it's actually a data set

473
00:40:20,720 --> 00:40:25,440
collected from multiple different robots so these are all robotic arms and they're all performing

474
00:40:25,440 --> 00:40:29,200
kind of similar tasks relocating objects on table moving things around but they're actually

475
00:40:29,200 --> 00:40:35,360
different robots and one of the things that uh we studied in that work is well first you know

476
00:40:35,360 --> 00:40:39,600
can we collect this data can we make it available to the community but also can we train a model

477
00:40:39,600 --> 00:40:44,080
that actually is robot agnostic so you could imagine whatever robot arm you have you take the

478
00:40:44,080 --> 00:40:49,120
small plug it in and maybe we'll uh control that robot we didn't quite achieve that so we couldn't

479
00:40:49,120 --> 00:40:53,680
get a single model that actually generalizes zero shot to new robots but maybe somebody else we

480
00:40:53,680 --> 00:40:58,480
get to work later but what we did achieve is we managed to use it as effective pre-training so you

481
00:40:58,480 --> 00:41:03,440
can take all the robots but except for one of them pre-trained and then get a new robot get a

482
00:41:03,440 --> 00:41:08,480
little bit of data and then fine tune and that works so far and you know maybe with a data set out

483
00:41:08,480 --> 00:41:12,880
there and available perhaps people can take it and see if they can move towards zero shot

484
00:41:12,880 --> 00:41:18,480
generalization and of course since it's a fixed data set it's kind of an ideal fit for

485
00:41:18,480 --> 00:41:25,200
fully off-ball CRL research. Well Sergei thanks so much for taking the time to chat with us give

486
00:41:25,200 --> 00:41:30,560
us an update on what you're up to sounds like a bunch of really interesting stuff. Thank you.

487
00:41:30,560 --> 00:41:40,720
We'll have lots of homework to do after this podcast. Thank you. All right everyone that's our show

488
00:41:40,720 --> 00:41:48,960
for today. For more information on today's show visit twomolai.com slash shows. As always thanks

489
00:41:48,960 --> 00:42:04,880
so much for listening and catch you next time.

