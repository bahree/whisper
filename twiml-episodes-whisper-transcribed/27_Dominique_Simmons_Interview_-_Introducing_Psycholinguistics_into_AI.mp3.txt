Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
I think you're really going to enjoy today's show.
Our guest this week is Dominique Simmons, applied research scientist at AI tools vendor
dimensional mechanics.
Dominique brings a really interesting background in cognitive psychology and psycho linguistics
to her work in research in AI and well to this podcast.
In our conversation, we cover the implications of cognitive psychology for neural networks
in AI systems and in particular, how an understanding of human cognition impacts the
development of AI models for applications such as media processing.
We also discuss our research into multi modal training of AI models and how our understanding
of the human brain has influenced this work.
In addition, we explore the debate around the biological plausibility of machine learning
and AI models.
Really, this was a great discussion.
Before we jump in though, I've got a question for you.
How would you like one of our beautiful this week in machine learning and AI laptop stickers?
I know you want one.
We've already sent stickers all around the world and we'd love to send you one as well.
All you need to do is pull up the show notes page, which in this case will be at twimmolai.com
slash talk slash 2323 and drop us a note with your favorite quote from the show.
You can also post a quote via Twitter, just mention at twimmolai or on our Facebook page.
Links to all of these will be available in the show notes.
I can't believe it's mid may already.
Next week, I'll be hosting my future of data summit in Las Vegas as part of the interop
ITX conference.
I've got a ton of great speakers lined up for the event, including folks from Intel,
Microsoft, GE, Capital One, level three communications and Walmart, as well as leading industry analysts
and startup executives.
Topics will span IOT and edge computing, data management, and of course machine learning
in AI, blockchain, and much more.
If you're planning to attend interop, I hope you join us at the summit.
And if you've been meaning to attend the summit, but held out until the very last minute,
it is not too late to register.
In fact, you can do so using my 20% off discount code by visiting twimmolai.com slash interop.
Of course, if you have any questions about the summit, please feel free to reach out to
me via the contact page.
And now on to the show.
All right, hey, everyone, I am excited to have Dominique Simmons on the line.
Dominique is an applied research scientist with dimensional mechanics.
How are you, Dominique?
I'm doing great.
Thank you.
How about yourself?
Doing very well.
Very well.
I wanted to start this conversation by talking a little bit about your background.
You have a master's degree in cognitive psychology and psycho linguistics, and you have ended
up doing work in artificial intelligence.
Tell us a little bit about your background and kind of your path to working in AI.
So I'll start from the very beginning.
As a child, I was always fascinated by the brain.
I grew up as an only child, and I found myself journaling.
When I wasn't with my other friends, I'd journal and make observations about what people
are doing, what was going on in my environment, and what I was fascinated by what made them
do the things that they did.
And that carried on throughout school, and eventually in college, I had a great mentor
who brought neuroscience into our curriculum, and I started, you know, tabling for brain
awareness week.
I started, you know, learning about the brain and the circuits and, you know, just how
these processes come about.
And so, again, that carried me through.
I knew that I wanted to do that in graduate school.
And after becoming a lab manager, University of Illinois, Urbana Champaign, being a post-back
intern at UMass Amherst, I got a lot of exposure to different types of psychology, anything
from, you know, infant psychology, cognitive psychology, psycho linguistics, music cognition,
you name it.
I was just very fascinated with all these aspects, and you could see these parallels coming
together as well.
So in graduate school, I studied specifically multi-sensory perception, which is the influence
of one sense over another, and how senses interact.
So I come from a school of thought where the brain is agnostic, if you will, to input.
And once the input, you know, is processed, then, you know, it becomes sound or it becomes
hearing or, you know, any one of these senses.
But the actual input is just information.
The brain likes getting information and, you know, learning and processing.
And then at the, the later stages, that's when it becomes what we know as, as, as senses,
sensory input.
Okay.
So with that, that gave me a unique background.
So I will say that in graduate school, I got a little bit tired of the theory.
You know, of course, it's, you know, critical, it's a critical foundation for a lot of work.
But I started to get, you know, to itch for applications of these ideas.
Right.
You know, this idea of integrating senses.
How can we, how can we make this into a device?
You know, what, how can it help, you know, non-hearing populations or populations that
have sensory deficits?
And so I worked on a project, a brain training project on hearing, veterans with hearing
loss.
And essentially it was a, a program, like an auditory game, auditory video game, where they
had to, to learn complex sounds in order to navigate through the game.
When the sound goes up, they had to jump, but the sound goes down, they had to, you know,
back under something.
Oh, interesting.
And so, yeah.
And so that was my first taste of the applied aspect, you know, of things of the applied
world.
And the goal of that was assessment or trying to rebuild new neural pathways to improve
their hearing or something else.
So it was a little bit of both, but really the latter.
Really trying to re-strengthen those connections, those auditory connections through training
them with, with complex sounds.
And so we build the complex sounds in MATLAB and other tools.
And you know, at that point, we were doing some initial testing, but it's gotten, gotten
pretty far.
It's gotten to the point where it's in, in, in app.
Oh, wow.
Yeah.
So that was part of my graduate work, and then I ended up, so it's funny how I got into
AI.
It's a bit of a jump.
So, still on that applied path, I ended up here at Dimensional Mechanics as a user experience
researcher in VR.
That was the initial space that we were in, creating VR content.
And optimizing it.
Oh, interesting.
Yeah.
And so that's, that's why, so I was trying to work on things like immersion, enriching the
user experience of someone in VR environments.
What are the perceptual aspects that come into it, and how to build a better immersive environment
for users, and also, you know, avoiding things like uncanny valley and nausea and all the,
the not so good stuff that goes with VR.
But eventually, and uncanny valley is what?
Oh, yes.
Uncanny valley.
That's what I forget what, what the phenomena is.
Yeah.
So it's, when you see a character that is human-like, but not quite there.
Okay.
And usually, you can tell in the eyes that's usually a giveaway, and you start to get this
uneasy feeling, because, you know, you see the human-like quality that could be there,
but it's not quite human-like.
Okay.
And that's a big issue with characters in VR and virtual environments, so.
But eventually, we pivoted into the AI space.
And so, I've been using my background to build, help build in the cognitive components
into our system, things like decision-making and perception, and memory as well.
Inside of curiosity, what drove the pivot from VR to AI?
I, it was, you know, business decision, but I believe that we saw a bigger opportunity
if you will in AI, especially a general AI platform that we're working on.
VR, that is, is, you know, is vast as well, and there are a lot of areas that you can
go into that people don't necessarily recognize.
It's way more than the entertainment space, but AI can, it affects almost every single field
there is, almost every single area of business.
I was on a panel a few months ago and someone asked, you know, is there, is there any area
that you can think of that hasn't been touched by AI?
And they're, you know, that we tried to come up with one example, but if it's not already,
it is going to be affected, yeah.
And so just for context, and I really want to dig into your, you know, how your background
ties into your current work, but for context, you said that at dimensional mechanics, you're
working on a platform for, did you say a general platform for AI or a platform for general
AI?
I thought I saw the ladder on the website or something or someplace.
Yeah.
So it's a general AI platform, essentially a set of development tools for companies so
that they can reduce their costs in engineering costs and AI and machine learning.
It's, AI is very expensive to produce and you need quite a bit of manpower and especially,
you know, you have to have essentially, you know, experts currently, but we are trying
to do away with that so that someone who is interested in AI or, you know, a company
that maybe doesn't necessarily have AI and ML experts, they can still build models to
solve their problems.
Okay.
Got it.
I thought I read that you guys were going after the, the AGI problem, artificial general
intelligence, but it sounds like, it sounds more narrowly constrained than that a little
bit.
Yeah.
I mean, we're, we're definitely adding to that conversation and adding to those efforts.
We want to build general AI, but it actually should mention that we are currently in the
media space.
So, you know, you have to narrow it down somewhat.
So we're currently working with companies to build AI models for images, video, and text.
Okay.
And I imagine that the media applications lend themselves particularly well to your
background and a cognitive approach to, you know, there are clear cognitive elements
to AI with regards to media.
Can you talk a little bit about how those intersect?
Right.
So, with, let's say, you know, images and video, for example, I come from it actually as
a matter of fact, I'm working on a computer vision project with a local university.
And we try to find parallels between, you know, how humans ingest or process, I guess,
perceive media.
That's the best way to put it and how a system would do the same thing.
And it's, you know, obviously there's going to be vast differences between how a machine
does and how a human does and we're not trying to make a one-to-one mapping, but there's
a lot to be said.
There's a lot of information and there's a lot of inspiration in how a human perceives
media and that we can apply to how a machine perceives media.
So, for example, and, you know, in computer vision, you can think of it as like a multi-disciplinary
field because you're drawing from vision science, you're drawing from computer science,
psychology, you know, cognitive science.
And so, you know, and part of that is looking at, you know, exactly.
So, part of the way that researchers create ground truth for computer vision experiments
is they look at human ratings.
They'll have, you know, humans view a set of videos, do eye tracking and whatnot and
then try to apply that, you know, that eye gaze data to the system at hand.
So that can be built in.
So, you know, in by using those human ratings, you can build smarter systems.
Just to make sure I'm understanding all of what you're saying.
So we can envision a system where a labeling system where you've got some humans that
are trying to label a set of images.
And you're saying that in addition to just the label that they're, you know, that they
may type in to, you know, or a set of labels that they may type in, you're also doing
something like, you know, eye tracking with the camera or something like that to see where
they're looking on the images and that helps you to refine the training process.
Right.
There's a lot of, there's a lot of rich information.
There can be from eye tracking studies and, you know, how humans, what is the eye gaze
data look like?
What do, you know, one of my focuses is no pun intended but an intention, user attention
in that when we take in an environment, when we're looking around, there's tons and tons
of stimuli.
The way that we can process it all of once.
And so with humans, the whole scene is not relevant at the same time.
Right.
It's going to be relevant in basically spotlights.
So you can apply that with a system as well.
If you may want to enhance certain parts of images or video, but you may want to do it
in a fashion where it's focusing on the most relevant information in that scene.
And so back to your grad school research on multimodal and kind of this model of the
brain where, you know, you have just a bunch of inputs and you're processing them, you
know, kind of as equals.
Are there other examples of inputs or other examples of this, of the multiple modes
that you're incorporating into your work today?
Not particularly currently, but I would like to see, and this is something that I'd like
to implement later and it's the, I guess, intersection or combination of audio and
visual.
Okay.
I have seen some early work in this, but, you know, if, let's say, let's say you're watching
a video and there's likely going to be audio as well.
And where you're focusing your attention is going to be not only influenced by the visuals,
but also, and also by the audio, but the, what's critical is the combination of the, you
know, space spatial temporal information.
That's, that's something that's going to heavily influence where your attention is guided
in the scene.
And so I think there needs to be a lot more research in that space, you know, previously,
there's been a lot of studies on this, but, you know, there's been a lot of focus on
the visual, obviously, and there's been a lot, there's been some focus on the audio,
but really where you're going to find fascinating insights is the, the combination, what's
going on spatially, what's going on temporarily, and that, that intersection.
And practically speaking, how might you expect that to impact an AI project?
So let's say you build a model to detect the tone of a commercial, well, that's going
to use both video, the visuals and the, and throughout the commercial, and then also,
you know, advertisers put a lot of focus on the audit, auditory aspect as well, the, you
know, the, the mood of the, the mood of the music, going with, you know, what's going
on in the scene.
So with an AI system, if you're able to train it on the video, and then train it on the,
the audio, and then, yeah, I can think of like a kind of a thought experiment right now.
So you can see if it can recognize the, the tone of the video just solely through the
visual, and then see if how well it does with just the audio, but I bet you that when
you combine both of them, you have both types of information that will allow it to best
categorize the, the tone of the video.
Got it.
I mean, it sounds like the basic idea is just try to use all the information that you have
to, to make more accurate models, and often it turns out that the information that you
have comes in, you know, multiple modes.
Some of it is, some of it is audio, some of it is video, you know, and there may be others
as well.
Right.
That's, yeah, exactly.
If we, you know, the more information that we have available, the more information we
can train our systems with, and that's how we, as humans learn as well.
For example, in the education room, you know, you have a video of a lecture, you have an
audio of a lecture, but if you are not only, you know, you're using both, and you know,
the best is if you're actually in the classroom and immersed and you have both, both sensory
streams coming in, the visuals and the auditory, you're going to have a better chance of, you
know, remembering the content and, and being able to build off of that as well.
Right.
Right.
Now a lot of your work calls into question the whole notion of, you know, biological
plausibility for neural nets and the extent to which we should be trying to model neural
nets and AI systems after human systems.
And in fact, you wrote about this in a blog post last year.
What are your thoughts on, you know, this whole question?
It's, it's debatable, definitely, and depending on, you know, what your background is, I've
seen some hardcore computer scientists where they're like, you don't need this plausibility.
But since my, my background is very brain heavy, I definitely want to include that in the
conversations, hey, okay, well, to what extent do we need to model these systems after the
human brain?
It does not need to be a one-on-one mapping.
I do believe that because with the brain, the human brain and, you know, a computer, the
lower level bits are very different.
There's some similarity there with neurons firing and the binary bits for computers, but
it wouldn't be in our best interest.
There are so many other things that we need to tackle right now.
It wouldn't be in our best interest to try to make a one-on-one mapping.
But these systems, in my opinion, should be biologically inspired.
So we can take concepts like modularity or integrated systems, localization.
We can take all of these aspects from neuroscience and cognitive science and apply them to building
these models.
And I personally try to keep that in mind when I'm building various AI models.
And I think of modularity and locality as computer science things, as technical things.
Can you talk about how those concepts express themselves biologically and how they've influenced
the types of models you build?
Sure.
So modularity, it's the idea that the brain processes, there are different areas for different
processes.
There's an area for language, there's an area for motor, there's an area for other types
of processes and systems.
So and they're believed to be all encompassing, if you will, or self-contained.
I'm not particularly from that school of thought.
I think there's relevance in that school of thought.
But there's also a lot of ton of interconnectivity.
And that's really, it's literally all connected, and even if there's some localization, a particular
area for language, that's going to influence the visual processes that's going to influence
the auditory processing.
So in my opinion, it's really all about interconnectivity, but as far as building AI models, so this is
kind of going back to the thought experiment that I brought up.
But you can think of it as when you're training these models.
You can think about what kind of, I guess, you can think of it as terms of sensory input
and separate streams or combined streams.
So you can feed an audio, you can feed in video, and then you can feed in the combination
of that.
And you can think of that as the interconnectivity equivalent, if you will.
So this reminds me of some conversations I've had recently about folks working on deep
neural nets, and this question comes up when they're trying to develop these complex,
deep neural networks, whether they, you know, whether they're ultimately developing
this kind of the single model that takes in all the input and produces all of the outputs,
and it's kind of monolithic, or whether they develop, you know, what looks more like
an ensemble, in a sense of, you know, or a hierarchical neural network model where
they've, you know, they're taking in, you know, inputs and training the model to be able
to determine, you know, some kind of higher level feature.
And then they have, you know, many of these in parallel that they feed into kind of a higher
level neural network to, you know, make the ultimate decision.
And this may happen in several layers.
It sounds like what you're saying is that that is ultimately more, you know, that's closer
to what the brain is doing, than the, you know, we think of the brain, I guess, and I
guess as laypeople is kind of this monolithic thing, but you're saying that the, you know,
in many ways is kind of hierarchical.
Is that, is that accurate?
Yeah, I mean, and you can take a particular system, the visual system, for example.
So, you know, you get the early visual sensory input, you know, it's really globs and
shapes and shadows.
And then as the input gets further up the streaming to V3 and V4, then you start to actually
make out a particular image, you know, a scene of a park or a beach or whatever it is.
So yeah, it goes from sort of like a lower level abstract to the more.
And yeah, I mean, I agree with the idea that, you know, we have these lower level processes
that are done first and then as they are being carried out and further processed, then
you get things that are more fine, fine tuned, fine grain, if you will.
You can also apply that to the motor system as well.
But first, when your motor system is still developing, you know, you're going to have
clunky, non-coordinated movements, but then as you go further along and you can fine tune
them and fine train them, then you're able to write, you're able to type, you're able
to do more fine tune movements.
Are there a set of principles? Do you think that, you know, anyone working in this field
should be thinking about as they're approaching, you know, developing AI systems with regard
to, you know, this whole issue of biological reference or plausibility?
I would say, I think in terms of, you know, being interdisciplinary, there are a lot of
areas that come together in AI and I think in order to be, to develop successfully, you
need to borrow from each of those.
Like I said, you know, computer science, cognitive science, psychology, all of these, these
different areas are going to get you, are going to help you build a better foundation,
as opposed to, you know, just focusing on the computer science theories or the, you know,
cognitive science theories, you really need a combination of all of them.
Are there a set of canonical references that folks should take a look at to dig into
this area more?
It wouldn't hurt to read up on the history of AI, I think it's a little known fact for
some that it's actually been around since the fifties.
A man named Marvin Minsky was, you know, he's considered to be the father of AI.
I would, I would read up on, on him, his work, I'd read up on Alan Turing.
He laid some of the foundational, you know, thinking in AI, yeah, it's start there.
Yeah.
Awesome.
Awesome.
So maybe let's, let's try to dig into, you know, some, some more concrete things are there.
Specific applications of what you guys are doing at dimensional mechanics that we can
maybe talk about or what, you know, ways that you're helping customers develop AI applications
with your platform.
Yeah.
So we have a demo on our site, actually, I wish I could go into a lot more detail.
But for now, I'll just mention the demo and the site and that's an image-rinking demo where
you can upload a photo and it will give it a score.
Okay.
And relative to the other, the existing photos on there, and you can see them, you can
scroll through them.
There's a top, top 10 list.
Okay.
And it's scoring it from what perspective?
The, so it's trying to give it the best ranking.
So it's, it's using a lot of different metrics, can't really go into the particulars, but
is it, is it trying to label the image or trying to rate its aesthetic quality or?
It's, yeah, it's trying to rate its aesthetic quality.
Okay.
Got it.
So it's kind of, I don't know if anybody remembers the hot or not app, it's kind of the hot or
not app for, for images.
Right.
We, that's brought up and, that's been brought up at our discussions.
Okay.
But yeah, it's, like I said, it's rating the more aesthetic qualities of, of an image.
Okay.
And in what ways has the, your work around the cognitive psychology influenced how a
system like that works?
So part of this is going, is going to go back to, you know, how, how a person perceives
an image.
And I'll just give, you know, very general example.
So, you know, when we see an image, we're taking in a lot of aspects, the shadows involve
the lighting, the angle of object, the, you know, the, the, the, sort of the busyness
or the, you know, the, the contrast involved.
So we're assessing all of those things when we're an image.
And you could say that the system is doing something similar.
Okay.
Uh, this is, this is kind of getting me into another space.
So one thing, it's interesting.
So have you heard of the black box problem?
Generally, yes, generally, I've heard of a black box problem.
I don't know if it's the same one that you're, when I think of the black box problem,
I think of that, uh, from, uh, an AI explainability perspective.
Right.
Right.
And that's, that's what I was going to get into.
Okay.
So for, and then there's a parallel with humans as well.
So, you know, we, as researchers, we present a set of stimuli and then, you know, a person
responds to those given stimuli, but what's exactly is going on in between is, uh, you
know, it's debatable sometimes more, more times than not.
And with AI, uh, that's one of the things that we're trying to work on as well, not necessarily
dimensional mechanics, but as a, as a field, um, trying to demystify what's going on
in between, um, and they're, they're, I, I think that taking a good, hard look at the,
the training sets that you and, and manipulating those in a way where, you know, you're feeding
in sections at a time and, you know, these have, this section has particular features.
This one doesn't, that can possibly get at, at that question, but it's going to take
a lot more, more work.
Mm-hmm.
And I think once we do that, it'll, we'll be able to, AI models will be that much more
valuable because we'll be able to tweak as, as necessary.
Right.
Right.
Uh, so going back to this demo application, you know, presumably you've trained, you
developed some AI model, uh, to rank these images and you've trained it on lots of input
data.
Uh, did the multi-modal training come into play in, in this case?
Not in the current iteration, but that is something we would definitely want to explore
on how to make it smarter, if you will.
Right.
Yeah.
Yeah, I can imagine, um, you know, if someone is, if someone is rating images on a, uh,
numeric scale, but you also had a camera looking at them, observing them, then you, there's
a ton of additional information like, you know, the creases in their eyes when they smile,
you know, the smile, the eyes, you know, they could, that could perhaps, uh, lend some
additional insight as to whether this is a visually appealing image.
Right.
There's so many factors that, that come into play, you know, like I said, whether you
have the right lighting, whether the, the expression on someone's face, um, what that's
conveying, there, there's so many things.
So, uh, it would be great to further explore it.
We've built it on a, you know, particular set of features, but, um, we would definitely
like to expand that in the, in the future.
Mm-hmm.
Are there any other, uh, kind of applications or use cases that, um, might be interesting
to explore?
Oh, there are tons.
It's really about having time to explore them.
Yeah.
Yeah.
It would be great to get a sense for, you know, I think we, you've given us a sense thus
far that having a cognitive psychology background can really lend insight into, you know, ways
to think about, you know, your modeling and your training, uh, and it would be great
to, you know, then talk through some examples of, you know, how, how that's kind of played
out in kind of a customer scenario or a, you know, just a practical scenario so that folks
can kind of see the line kind of go from beginning to end, if that makes sense.
Okay.
And I've taught, yeah, and I've touched on this a little bit earlier, but, you know, we're
looking at what, what we're looking at, user interest and engagement, um, essentially,
you know, when you see a scene, what is, what's relevant to you, what, what catches your
attention?
Right.
Right.
And once various factors of that can, can be identified and what, by the way, we're,
you know, looking at both lower, lower level features, things like, uh, line orientation
and color, you know, very low level features that would grab your attention automatically,
if you will.
And then also higher level things like, uh, emotion, things that, uh, more, I guess there's
a term top down, um, attention, which is, is intentional based, you're, you're, um,
looking at something intentionally, okay, um, or, you know, goal or oriented.
So the possible applications of that, that research.
So I've, you know, talked about, uh, the advertising space, right.
So, you know, for example, if advertisers can take this research and realize, okay, at
the 32nd mark, this is where this particular, you know, location in the, the scene, this
is where people are going to be most engaged, uh, most, they feel like there's something
most relevant in that, in that, uh, particular area.
Well, that could be a great placement for product ad, you know, or product logo.
You can also get a sense of how long you're going to be able to sustain someone's, how,
how much, how much time someone's attention is going to be sustained, you know, in a world
where we're on our phones all the time, um, that, that gap is shrinking.
So, you know, with advertisers, it's that much more critical to find what's relevant
and, and get in there before, uh, the user's, uh, attention is lost.
Okay.
So, then you're through this research, you're developing models that can, that can look
at, uh, is it only static images or is it, uh, video as well in this project?
It's video as well.
Okay.
Actually, in this particular project, it's video.
Oh, okay.
So, you're, you're looking at, you're basically training models to, to model human attention
and interest, uh, on these videos, and then you can use that to help advertisers assess
their work, for example.
So as opposed to convening a panel or a focus group to try to get a sense for whether an
advertisement is effective, uh, you know, which is probably expensive in time consuming
and maybe not even all that accurate, you can use these models to screen the, the advertisements
to, you know, for screen the advertisements for effectiveness.
Is that the general idea?
Right.
And going off the accuracy point.
So, you know, a lot of times in focus groups and whatnot, there, there's a, uh, uh, verbal
or written response, but a lot of times what we say is not necessarily reflecting what's
going on internally.
Right.
Right.
Right.
And we try to get at that as well, um, we currently use tools like, um, eye tracking and
biometric sensors.
So to get at the physiological responses to, to the input, to the video input.
Okay.
Uh, and, uh, I forget if we cover this, but what do you call that that phenomenon?
Like I know a related idea is, I guess I, you call it attribution error or, uh, you
know, issues around attribution, meaning if you, if I look at an image, I can tell you
I like it, but I can't tell you necessarily why I like it.
Is there a specific, is there specific terminology for, um, you know, the, um, you know, the,
uh, other side of this, which is, you know, I might not even, I might say I like it,
but I might not really like it or vice versa.
Right.
Well, there, there are a lot of things that come to play, especially in an experimental
setting.
So there is experiment or bias.
If, you know, the, there might be some influence of the experiment or you have to be careful
in the way that it's asked if you are going to ask.
Because the question itself can be loaded and, and, and bias the response.
This is getting into another area, but, uh, with eyewitness experiments, the way that
you pose a question can definitely influence how the person will, will respond.
You can, you know, let's say there is, um, an accident and, you know, you, you throw
in an object in the question, like, uh, at the stop sign, blah, blah.
Well, the fact that you mentioned the stop sign, you know, even if there wasn't one there,
the person may still agree, I say, yeah, okay, I remember the stop sign.
It's like, no, there wasn't even a stop sign there at that particular scene.
So, so there's experiment or bias, there's, so you mentioned attribution, but there, you
also want to be seen in a particular way as well when you respond.
Um, so you have to keep that in mind, and we may not even be conscious of that.
We just, we, we want to have, we want to produce positive responses.
Mm-hmm.
So, you know, to get away with all, get away at all that, I think a great measure is,
is more something physiological that, you know, it doesn't get a chance to reach our
thoughts.
Mm-hmm, interesting, interesting.
So, beyond just this attribution issue and my ability to articulate, there is a whole
host of cognitive biases that can, um, that can distance someone's real perception of,
an image or video from what they ultimately say and some kind of panel or focus group
and thus being able to develop machine models that can, you know, not just rate the, the
image or model, the, you know, the human reception of an image, but also maybe tell us a little
bit, you know, as we kind of get further along with the black box issue, tell us what the
issues are, you know, this one may be too dark here or too contrasty there, that kind
of thing.
Right.
Right.
To give, you know, possibly a more objective measure, if you will.
Mm-hmm.
It sounded like you were wrapping up this research project, um, what's on the horizon for
you?
Are there any areas that you're, any particular areas that you're looking forward
to exploring further?
Right now, I am working on a, learning more about the natural language processing space,
which I find really fascinating, especially with my psycho linguistics background.
So for those who don't know, natural language processing, um, is the ability for systems
to take natural text, you know, anything as short as words, phrases, and to documents,
full documents, novels, even, and, uh, be able to get insights out of that input data.
Things like, I mean, you can get more technical things like frequency counts and, and whatnot,
but you can figure out the, the sentiment of a particular, uh, text.
You can figure out all kinds of things that, you know, it would take a person hundreds
and hundreds of hours to do.
You can just load in all of these, uh, documents and, you know, get a score for sentiment.
Is it particularly positive or negative or all kinds of, uh, different insights?
And what, uh, what is peaking your curiosity around, uh, NLP and, uh, the application of
psycho linguistics to, to that field?
Maybe we can start with, uh, what is psycho linguistics relative to, you know, traditional
linguistics or other aspects of linguistics?
Sure.
So linguistics is, you know, the study of language, the parts of language, the structures
and whatnot, and psycho linguistics, it's bringing in psychology into that.
You know, you're, you're looking at things like the way in which people say things, um,
that term is prosody, the inflection and someone's invoice to convey certain messages.
Okay.
It's, you know, it's fascinating.
You take one sentence, the man went to the park.
The man went to the park.
The man went to the park, you know, if the, you can say the exact same thing, but put different
inflection on it and has a completely different meaning.
Okay.
There's, you know, the way, uh, the way in which we produce sentences and the, you know,
syntactic structure and how that affects both the producer of the speech and then also
the, you know, listener as well.
Um, it also gets into, you know, co-articulation and these other mechanics of speech that's
what's co-articulation?
Coarticulation is when it's the, it, it's so it has to do with the flow of words.
When, when we're speaking, all the words seem discreet, but if you look at an audio form,
uh, waveform, you'll notice that the sounds actually overlap and so that's the coarticulation.
And you also get into the social aspects of speech, so when, uh, two people are conversing
and there's a, uh, phenomena that occurs called common ground and that's when, um, you
know, you start out using different terms, but over the course of the conversation, you
start to use similar terms to each other, not the same ones, um, because you've built
up, built up a report, uh, gosh, I mean, there's, there's so much, there's, there's also the
phenomena where, um, you know, you're building up that common ground, but then you also take
on a similar speaking style to that person, um, and that can also convey that you, you
like that person.
You can also, and that's called convergence.
Okay.
There's also divergence where maybe, um, you're, you're not a big fan of that person.
You start, uh, changing your speaking style, maybe, you know, unconsciously, but, um, changing
your speaking style, nevertheless.
Hmm.
Interesting.
So when I think of natural language processing, I tend to think of applications that are,
you know, either primarily textual or, um, you know, translation types of applications,
uh, but you're, you know, just your little explanation of cycle linguistics and some,
given some of the background we've talked about, you know, it, it strikes me that there's
a ton of interesting work and exploration to go into, uh, what's, how to articulate
this.
Well, maybe to articulate it by example, like when you create a neural network to recognize
images, right, when, you know, we know a little bit about how those neural network structure
themselves and you've got kind of your edge detectors and your, you know, shading detectors
and all those things that emerge, I wonder the extent to which, uh, the concepts like
prosody and other things, if you're training a neural network on speech samples, you
know, if there are regions in the neural network that emerge, that somehow reflect, you
know, prosody, for example, or if that's a, if that's, you know, is that kind of the
current frontier of research?
Yeah, that's definitely a frontier of this research in this particular space, um, especially
audio, you know, visual inputs have been fairly well studied in the AI space, but audio
less much part of the issue is the shortage of data in that space there.
There are, there are quite a few open source video and image data sets, but not so much
on the audio side, but going back to your point, yeah, I believe that what you would do
is you would set, you know, prosody, up as a feature, you'd set, I don't know, co-articulation
or these other, you know, syntax, you'd, you'd start to set these up as individual features
to train the models.
Meaning that you would, you would have humans identify them somehow and label them or
you would have, how would you set up prosody, for example, as a feature?
It's a good question, so initially it would likely have to be partially, at least partially
supervised and you'd want to use some sort of existing ratings, so probably human ratings
and I can imagine, you know, you have a waveform and they, you know, listen to the speech
snippet and mark places where, you know, goes high or goes low.
This is also evident in the physical waveform as well, so I could see that even being done
without human ratings, but, you know, the system, you know, you feed it various waveforms
and you feed at the sounds to go with it, it should be able to learn the parts where the
wave, the audio goes up and then, you know, the times where the inflection goes down.
But the tricky part would be associating that with a particular meaning and that's where
you'd probably need humans to come in, so, you know, humans would tag the particular
sentence as, oh, the emphasis was on this, so the emphasis was on that.
And then, once you have those lists of different emphasis, you could train the model on that.
And so, ideally, it would know when the inflection goes up, that's where the, the emphasized
meaning is.
I would imagine that traditional linguistics has a lot to offer in terms of, you know,
just how to represent all of this stuff.
Like, it strikes me that, you know, just there's a representational challenge and, you
know, if someone were to try to take this approach to, you know, building and training
models, but, you know, certainly linguists have been, you know, representing prosody
and some kind of way and developing ways to map that to specific meanings.
Is that correct?
Yes, that is, that is correct.
And so, it's definitely, definitely worth a look if someone is trying to train on audio
models to, you know, take a look at that space.
That's why, going back to what I said about being, you know, thinking multidisciplinary,
wouldn't maybe, you know, wouldn't maybe be obvious to go to the cycle linguistics area.
But it, like we've been, we've been saying, it can give you some great insight into how
to train an audio model.
And it'll give you more than just the surface characteristics of a waveform or the audio.
It'll, you'll be able to, you know, train it on meaningful insights as well.
His prosody is not something that you can necessarily, you can see the inflections, but you
can't necessarily see the meaning on the physical waveform.
You need, you need to add that insight, right, in addition.
Right.
Hmm.
Oh, this is a really, really interesting space.
Any, any other thoughts before we wrap up?
No, I, I think this is a great space as well.
What I, I particularly appreciate about it is, like I said, the multidisciplinary aspect
of it.
We're building these very complex systems.
And I just mean that as a, as a field, right, in addition to my company.
But, you know, we're, yeah, we're building these complex models that, you know, in some
respect, reflect what's going on in a human brain.
But we need to keep in mind that, you know, the more complex they get, the more information
that we're going to need to seek out, and it's going to come from different places.
Yeah, I, I can definitely see that.
Yeah, I don't, I, I think I've commented here on the podcast or, um, certainly on Twitter
that if I wasn't so busy trying to figure out this machine learning and I think linguistics
would be high on my list of things to figure out.
Uh, it's a fascinating field and it's great that you get to, uh, combine the two.
Yeah, it is great.
And one thing I did want to mention is that ideally, you know, the system would be language
agnostic.
Um, so, you know, we're really teaching it about human language, whatever, uh, it could
be French, Spanish, you know, English, whatever it is, um, which is very helpful.
And it can be used in, like you alluded to, um, translation tools.
Mm-hmm.
Yeah, my favorite example of this, in fact, is from a conversation I did with, uh, recently
with Shubos and Gupta from Baidu Labs, and he talked about how, uh, they were able
to build a, uh, English to Mandarin translator.
I believe it was English to Mandarin translator before they even had any, you know, without
having any Mandarin speakers, you know, on their staff, you know, just based on, you
know, this, this property that you're describing, the, the, the fact that a lot of the application
of this is, uh, language agnostic.
Exactly.
Because that's, yeah, it, it's not working on those particular nuances, if you will.
It's, it's looking at it with a more agnostic view.
Mm-hmm.
Awesome.
Uh, well, before we go, what's the best way, uh, folks want to connect with you or get
in touch?
Uh, what's the best way to do that?
You can connect with Dimensional Mechanics on Twitter at DM, INC, underscore AI, and, uh,
we're also on Facebook and LinkedIn.
You can connect with me personally at ArtSci, ART, SCI, uh, two, with two zeroes, um, at Twitter.
Okay.
So at ArtSci, zero, zero.
Yeah.
Awesome.
Awesome.
Well, Dominic, thanks so much for being on the show.
It was great chatting with you and looking forward to reconnecting soon.
Thank you, Sam.
I really appreciate it.
Thanks for having me on.
Absolutely.
Take care.
Bye.
All right, everyone, that's our show for today.
Once again, thanks so much for listening and for your continued support.
Don't forget to share your favorite quote from this show via the show notes page, Twitter
or our Facebook page, and if you do, we'll be happy to send you one of our laptop stickers.
If you're planning to attend the future of data summit next week, please reach out
and let me know to look out for you.
The notes for this show will be up on twimmelai.com slash talks slash 23, where you'll find
links to Dominic and the various resources we mentioned in the show.
Once again, thanks so much for listening and catch you next time.
