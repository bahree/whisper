WEBVTT

00:00.000 --> 00:25.760
Hey everyone, hope you all had a wonderful holiday.

00:25.760 --> 00:30.520
For the next few weeks we'll be running back the clock with our second annual AI Rewind

00:30.520 --> 00:31.960
series.

00:31.960 --> 00:37.000
Join by a few friends of the show, we'll be reviewing the papers, tools, use cases,

00:37.000 --> 00:43.160
and other developments that made us splash in 2019 in key fields like machine learning,

00:43.160 --> 00:49.520
deep learning, NLP, computer vision, reinforcement learning, and ethical AI.

00:49.520 --> 00:55.720
Be sure to follow along with the series at twomolai.com slash rewind 19.

00:55.720 --> 01:00.120
As always, we'd love to hear your thoughts on this series, including anything we might

01:00.120 --> 01:01.120
have missed.

01:01.120 --> 01:06.640
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via

01:06.640 --> 01:11.520
a comment on the show notes page you can find at twomolai.com.

01:11.520 --> 01:14.480
Happy New Year, let's get into the show.

01:14.480 --> 01:18.080
Alright everyone, I am on the line with Chelsea Finn.

01:18.080 --> 01:23.800
Chelsea is an assistant professor of computer science at Stanford University.

01:23.800 --> 01:27.160
Chelsea, welcome back to the twomolai podcast.

01:27.160 --> 01:28.160
Thank you Sam.

01:28.160 --> 01:32.400
Alright, so we have not spoken since it's been quite a while.

01:32.400 --> 01:39.120
You were at Berkeley at the time, it was back in June of 2017 for twomol talk number 29,

01:39.120 --> 01:43.080
we're over 300 shows now, so quite a while.

01:43.080 --> 01:50.920
Before we jump into our focus for this conversation, which is a look back at 2019 and all the

01:50.920 --> 01:55.120
exciting developments in reinforcement learning, why don't you catch us up on what you've

01:55.120 --> 01:57.200
been up to over the past couple of years?

01:57.200 --> 02:03.000
Yeah, so I finished my dissertation at UC Berkeley, now I'm an assistant professor at

02:03.000 --> 02:07.520
Stanford, and I guess in my research, some of the things I've been really thinking about

02:07.520 --> 02:13.480
recently are how we can build machine learning systems and especially embodied systems such

02:13.480 --> 02:18.040
as robots that can generalize two different objects, two different environments, two different

02:18.040 --> 02:19.040
settings.

02:19.040 --> 02:24.960
And this is through the lens of reinforcement learning algorithms, as well as what's called

02:24.960 --> 02:29.200
metal learning algorithms, where you try to accumulate previous experience in a way that

02:29.200 --> 02:33.280
allows you to quickly learn new things or quickly adapt to new settings, rather than

02:33.280 --> 02:36.840
trying to learn from scratch for every new thing that you might want to do.

02:36.840 --> 02:42.120
So my group at Stanford has been starting to study some of these problems in generalization

02:42.120 --> 02:47.720
and reinforcement learning and robotics, and yeah, excited to be on the show today.

02:47.720 --> 02:51.800
Thanks, and you're also teaching a course at Stanford now on metal learning, is that right?

02:51.800 --> 02:52.800
Yeah, absolutely.

02:52.800 --> 02:55.600
I'm teaching a new course that is just wrapping up.

02:55.600 --> 03:01.400
We have our last class on Monday of next week, and then after actually after the course,

03:01.400 --> 03:07.040
all of the videos are going to be available online for the public to see all the videos

03:07.040 --> 03:08.040
of the lectures.

03:08.040 --> 03:09.360
Oh, that's fantastic.

03:09.360 --> 03:11.040
That is fantastic.

03:11.040 --> 03:18.200
So the format of this particular conversation is going to be, again, focused on 2019

03:18.200 --> 03:25.320
in review and reinforcement learning before we jump into the specific papers or topics

03:25.320 --> 03:28.920
that you thought weren't interesting this year.

03:28.920 --> 03:33.240
Can you kind of characterize the year for us, you know, relative to other years that you've

03:33.240 --> 03:39.080
been following RL, you know, how was 2019 shaped up?

03:39.080 --> 03:43.200
Yeah, I think that research and reinforcement learning has really been picking up, and

03:43.200 --> 03:48.120
there have been an increasing interest in it, so there are more labs that were crucially

03:48.120 --> 03:52.920
focusing on supervised learning or unsupervised learning that are now going into this setting

03:52.920 --> 03:56.520
where agents need to make multiple decisions, and people have different motivations for

03:56.520 --> 03:57.520
doing that.

03:57.520 --> 04:02.640
But I think that really the field is expanding, and that's been an exciting time, and also

04:02.640 --> 04:07.400
with that expansion, I think that people have been studying a broader range of reinforcement

04:07.400 --> 04:08.400
learning problems.

04:08.400 --> 04:13.560
So before people were kind of narrowly focused on a few benchmarks, and I think that now

04:13.560 --> 04:17.000
that is opening up, and people are kind of reconsidering different formulations and

04:17.000 --> 04:21.080
different problem settings within the context of reinforcement learning, and with that

04:21.080 --> 04:25.880
there's also been some of the same big players that have been trying to advance the capabilities

04:25.880 --> 04:28.680
of our reinforcement learning systems as well.

04:28.680 --> 04:33.720
So I think that the, there's been progress on a lot of fronts, and it's been a pretty

04:33.720 --> 04:34.720
exciting year.

04:34.720 --> 04:40.800
Now, the benchmarks that have traditionally been used in RL, at least the ones that come

04:40.800 --> 04:46.840
to mind most immediately for me are video games, and in particular, historically it's been

04:46.840 --> 04:52.280
kind of simple Atari-style video games, but these have been getting a lot more complex

04:52.280 --> 04:53.280
over time.

04:53.280 --> 04:55.280
Yeah, absolutely.

04:55.280 --> 05:00.760
So one of the big focuses in reinforcement learning has been looking at Atari games,

05:00.760 --> 05:05.720
and there's still actually worse and really interesting progress made on those benchmarks,

05:05.720 --> 05:09.280
but I think that also a lot of research has been opening up and focusing on other problems

05:09.280 --> 05:10.280
as well.

05:10.280 --> 05:16.120
Another very common benchmark in previous years has been the, these continuous control tasks

05:16.120 --> 05:22.480
of these simulated robots in the Bajako physics engine, and using like open-air gym, for

05:22.480 --> 05:23.480
example.

05:23.480 --> 05:26.240
An example of that would be like the cart pull.

05:26.240 --> 05:31.960
It's like cart pull, but also what's called the half cheetah or the ant, which is actually

05:31.960 --> 05:37.320
a four-legged creature, these types of locomotion, simulated locomotion tasks.

05:37.320 --> 05:38.320
Wow, okay.

05:38.320 --> 05:43.560
So these are, we'll see these videos with an agent, essentially learning to walk or trying

05:43.560 --> 05:46.040
to get from one place to another most efficiently.

05:46.040 --> 05:48.840
Those types of benchmarks got it, okay.

05:48.840 --> 05:49.840
All right.

05:49.840 --> 05:50.840
Awesome.

05:50.840 --> 05:56.200
So your task in preparing for this session is to come up with a few or was to come

05:56.200 --> 05:59.000
up with a few papers that you thought were significant.

05:59.000 --> 06:04.800
You, in fact, came up with topics, some of which had multiple papers that you thought were

06:04.800 --> 06:05.800
really interesting.

06:05.800 --> 06:08.920
Why don't we just jump in and have you walk us through these?

06:08.920 --> 06:09.920
Yeah.

06:09.920 --> 06:15.080
So I think that it, there's often never just one single paper that really solidifies, and

06:15.080 --> 06:20.800
a result is often many multiple results that actually really show you what has been,

06:20.800 --> 06:25.800
what is capable and what you can, what these algorithms are capable of and what these

06:25.800 --> 06:29.880
what can be done with their current technology, and so that's why I wanted to focus on these

06:29.880 --> 06:34.880
different topics because I think that there are, it's not just one paper for each thing.

06:34.880 --> 06:40.240
So the kind of the first thing that I wanted to highlight was thinking about reinforcement

06:40.240 --> 06:41.160
learning in the real world.

06:41.160 --> 06:46.880
So there's been some pretty impressive progress on reinforcement learning on real robots

06:46.880 --> 06:48.640
for dexterous manipulation tasks.

06:48.640 --> 06:55.920
So think like a five-fingered hand that can do things such as turn, turn Rubik's cube size

06:55.920 --> 07:01.800
or can manipulate to what's called bounding balls in the palm of the hand and rotate

07:01.800 --> 07:04.640
them with a single hand.

07:04.640 --> 07:08.400
And so we've seen reinforcement learning algorithms that are able to learn both of these

07:08.400 --> 07:12.480
very dexterous manipulation tasks with five finger hands in the real world.

07:12.480 --> 07:16.520
So one, but one thing that was interesting, there were actually two papers that showed

07:16.520 --> 07:22.160
these results, one that actually trained completely in simulation and then tried to transfer

07:22.160 --> 07:25.880
what was learned in simulation to the real robot, and one which learned completely in

07:25.880 --> 07:30.280
the real world and was actually efficient enough to run in the real world.

07:30.280 --> 07:33.760
And so one of the things that I found really exciting about these results is it showed

07:33.760 --> 07:39.480
the sort of complexity that we could learn in the real world using reinforcement learning

07:39.480 --> 07:44.840
algorithms, and it was interesting to see how divergent the two approaches were for

07:44.840 --> 07:48.200
accomplishing somewhat similar objectives.

07:48.200 --> 07:51.920
How would you characterize the divergent nature of these two approaches?

07:51.920 --> 07:55.400
What were the key things that they did differently?

07:55.400 --> 07:58.520
Yeah, so the key thing was simulation versus real.

07:58.520 --> 08:03.040
So one of them was, and it's a little bit more that one of them was trying to take a

08:03.040 --> 08:08.040
really powerful reinforcement learning method and train it in a variety of different simulated

08:08.040 --> 08:10.920
settings in a way that allowed it to transfer to the real world.

08:10.920 --> 08:12.760
And one was extremely focused on efficiency.

08:12.760 --> 08:16.520
So if you're running on a real robot, a five-fingered hand that is a bit fragile, it isn't

08:16.520 --> 08:21.040
something that you can really put a lot of wear and tear on, then you need to be extremely

08:21.040 --> 08:22.040
efficient.

08:22.040 --> 08:27.480
You need to be learning in a way that you don't break the hand in the process of reinforcement

08:27.480 --> 08:28.480
learning.

08:28.480 --> 08:33.920
So, and so the two algorithms that were developed here were in many ways just like completely

08:33.920 --> 08:38.600
different from each other, but yet achieved a kind of similar result in the real world.

08:38.600 --> 08:45.240
And the Rubik's Cube paper, that's a relatively recent one in the year, and that was some

08:45.240 --> 08:50.720
results by OpenAI, and those caused a bit of a stir in the community that the results

08:50.720 --> 08:53.200
they presented were overhyped or over marketed.

08:53.200 --> 08:55.640
Do you have a take on that?

08:55.640 --> 09:02.600
Yeah, so I think that the concern was that the kind of the title of the approach or of

09:02.600 --> 09:09.200
the blog post was about solving Rubik's cubes using reinforcement learning, and what they

09:09.200 --> 09:13.680
were actually doing was they were using a Rubik's Cube solver, and then figuring out how

09:13.680 --> 09:19.760
you could do the, they were using a Rubik's Cube solver to figure out which face to turn

09:19.760 --> 09:26.000
in which way, and so that kind of was in some ways pretty specified, and then the reinforcement

09:26.000 --> 09:31.720
learning part was figuring out how you can turn that face with a physical hand.

09:31.720 --> 09:35.960
And so this was, this was a bit controversial because it was people got the impression

09:35.960 --> 09:41.040
that they were learning all of the moves of solving the cube with reinforcement learning

09:41.040 --> 09:45.120
and not just the physical aspect of it.

09:45.120 --> 09:48.720
In many ways, I actually think that the physical aspect of it, while it seems like it should

09:48.720 --> 09:55.480
be simpler, because we are like basic manipulation skills are so intuitive and basic to us.

09:55.480 --> 10:01.760
I think that actually in reality, these sort of physical contact is actually much harder than

10:01.760 --> 10:02.760
solving the Rubik's Cube.

10:02.760 --> 10:05.920
We've had solvers for Rubik's cubes for a very long time.

10:05.920 --> 10:09.440
This is one of the first times that we've actually seen, or robotic hand, be able to have

10:09.440 --> 10:12.520
the dexterity that allows it to rotate one of the paces.

10:12.520 --> 10:18.040
Well, and there are so many more degrees of freedom with the hand than the cube itself.

10:18.040 --> 10:23.240
Yeah, yeah, and so yeah, there was, there was some controversy there, and also just generally

10:23.240 --> 10:29.240
the way that things were handled with like the press and everything, because they also

10:29.240 --> 10:36.360
put a significant effort into producing a video and like marketing the work, whereas

10:36.360 --> 10:41.920
in contrast, a lot of research labs don't do that as you might expect.

10:41.920 --> 10:44.760
So there was also some controversy there.

10:44.760 --> 10:49.320
For the record, we'll include links to all of these papers in the show notes, the ones

10:49.320 --> 10:54.200
we just spoke about were solving Rubik's Cube with a robot hand, and deep dynamics for

10:54.200 --> 10:55.960
learning dexterous manipulation.

10:55.960 --> 11:02.720
Yeah, and the second paper was by Anusha Nagarbadi, who is a PhD student at UC Berkeley, and

11:02.720 --> 11:05.560
was doing an internship at Google Brain at the time.

11:05.560 --> 11:07.280
Okay, great, great.

11:07.280 --> 11:13.760
So lots of progress on applying reinforcement learning to manipulating physical objects

11:13.760 --> 11:15.520
with robotic hands.

11:15.520 --> 11:20.160
So the second approach that we talked about by Anusha was using a technique called model

11:20.160 --> 11:24.040
based reinforcement learning, where you learn a dynamics model of the world, and then you

11:24.040 --> 11:28.840
do reinforcement learning using that dynamics model to optimize a policy or to optimize

11:28.840 --> 11:30.520
over actions.

11:30.520 --> 11:35.320
And so this kind of approach in general, I think, has in some ways received less attention

11:35.320 --> 11:39.600
by the machine learning community than model free reinforcement learning methods.

11:39.600 --> 11:43.160
But this year, I think that we actually saw an increased interest in model based reinforcement

11:43.160 --> 11:46.000
learning methods, and one of the reasons why people do like to use them is they tend to

11:46.000 --> 11:48.520
be more sample efficient.

11:48.520 --> 11:53.000
But the challenge is if you're in a vision based domain, if you only see images, if you

11:53.000 --> 11:58.320
only see image pixels as your observations, then in principle, learning a model involves

11:58.320 --> 12:01.840
potentially even predicting pixels forward into the future, like learning a video prediction

12:01.840 --> 12:04.760
model, and that can be very challenging.

12:04.760 --> 12:10.040
And so this year, we saw some real progress, I think, on model based reinforcement learning

12:10.040 --> 12:15.120
on vision based domains, one approach that predict pixels into the future.

12:15.120 --> 12:19.760
So I was actually learning a video prediction model and showed how you can use this for things

12:19.760 --> 12:24.720
like Atari Games, and one which bypassed the need to predict pixels by pushing forward

12:24.720 --> 12:28.880
in a latent representation learned by neural network, and using that latent representation

12:28.880 --> 12:34.440
to predict things like the future values and the actions of a policy.

12:34.440 --> 12:37.840
And so I guess one of the things that I was really impressed by and thought was interesting

12:37.840 --> 12:42.640
with these two works is that it really showed the viability of model based reinforcement

12:42.640 --> 12:47.440
learning as an approach, even for some of the benchmarks that have been so heavily studied

12:47.440 --> 12:49.480
by model free approaches.

12:49.480 --> 12:53.320
So I think there are a number of benefits of model based, and I wasn't necessarily thinking

12:53.320 --> 12:58.280
that those benefits would come on things like Atari, for example, but we were able to

12:58.280 --> 13:03.400
see significant progress from these works on those domains.

13:03.400 --> 13:08.280
In the case of the Atari work, what does a model look like in that context?

13:08.280 --> 13:13.960
Yeah, so for the first paper, the model was actually generating images into the future.

13:13.960 --> 13:16.360
So it's a big neural network.

13:16.360 --> 13:18.800
It's fully convolutional.

13:18.800 --> 13:26.360
So it's basically taking in an input frame, it produces convolutional feature maps and

13:26.360 --> 13:30.800
produces a representation and then has deconvolutions to produce the next image.

13:30.800 --> 13:33.880
And it also takes us and put the action to produce the next image.

13:33.880 --> 13:37.800
So it's trying to predict what will the next image or the next sequence of images look

13:37.800 --> 13:42.360
like given the current image and a sequence of actions.

13:42.360 --> 13:48.960
And then that future predicted image becomes additional input to the RL learner.

13:48.960 --> 13:53.800
So yeah, so then that image you can use to essentially generate more data for your

13:53.800 --> 13:54.800
reinforcement learner.

13:54.800 --> 13:59.240
So instead of always taking actions in the real environment, you could also take actions

13:59.240 --> 14:03.960
in this learned environment and use that to improve your policy.

14:03.960 --> 14:09.240
Sometimes when we use model-based approaches, the model isn't necessarily a learn model.

14:09.240 --> 14:11.880
Is it a learn model in both of these cases?

14:11.880 --> 14:14.520
Yeah, so in both of these cases, it is a learn model.

14:14.520 --> 14:20.520
There's a field, especially prominent in robotics called model-based control that typically

14:20.520 --> 14:23.640
assumes that you know the model of the environment.

14:23.640 --> 14:26.280
And I guess it's not always, it's not just in robotics as well.

14:26.280 --> 14:28.920
There are a number of approaches where it assumes that you know the model.

14:28.920 --> 14:35.800
You know the true simulation of your system and use that known simulation in order to

14:35.800 --> 14:36.800
learn the task.

14:36.800 --> 14:42.040
And in many ways, in Atari, it's in some ways a little bit silly to actually learn the

14:42.040 --> 14:46.080
model because you actually have the real simulation system.

14:46.080 --> 14:49.640
But I think that in many ways, the reason why people care about these approaches that actually

14:49.640 --> 14:53.960
learn the model is that it means that it's applicable in settings such as the real world

14:53.960 --> 14:56.320
where you don't know the model.

14:56.320 --> 15:02.640
And so in both of these cases are, is the approach essentially the same where you're using

15:02.640 --> 15:06.400
model to predict images into the future?

15:06.400 --> 15:09.560
So the first one is predicting images into the future and the second one is actually

15:09.560 --> 15:12.040
predicting other quantities into the future.

15:12.040 --> 15:18.880
So it's predicting things like values, rewards, and actions into the future.

15:18.880 --> 15:22.360
And this is all, all of these predictions are conditioned on some latent representations.

15:22.360 --> 15:28.760
You can think of it as predicting the only the quantities that are relevant for the game.

15:28.760 --> 15:33.120
But in some latent representation, rather than having to predict all of the pixels, which

15:33.120 --> 15:36.560
include things that aren't necessarily relevant to doing well on the game.

15:36.560 --> 15:37.560
Got it.

15:37.560 --> 15:42.760
And so the pixels that you're predicting here might be pixels that represent things on the

15:42.760 --> 15:48.560
screen that are specific to specific to actions that might be taken, like I don't know

15:48.560 --> 15:54.480
a score or some kind of trying to come up with a good example of what that might be.

15:54.480 --> 16:02.240
But specific symbols on the screen, or are we out of, is the model predicting out of

16:02.240 --> 16:03.240
pixel space?

16:03.240 --> 16:04.240
Yeah.

16:04.240 --> 16:08.200
So in the second one, it's not actually predicting at all in the pixel space.

16:08.200 --> 16:14.440
It's basically, as a neural network that takes the image and produces a vector of representation.

16:14.440 --> 16:18.840
And then it predicts that representation forward, rather than predicting the pixels.

16:18.840 --> 16:25.000
But the fundamental, and both of these cases, we're starting from vision and pixels and

16:25.000 --> 16:31.680
using that to using models based on these pixels to kind of inform an RL learner, an

16:31.680 --> 16:32.680
RL agent.

16:32.680 --> 16:33.680
Yeah.

16:33.680 --> 16:34.680
Okay.

16:34.680 --> 16:35.680
All right.

16:35.680 --> 16:40.880
And so the next category that you wanted to cover is focused on batch off policy RL.

16:40.880 --> 16:41.880
What is that?

16:41.880 --> 16:47.560
Yeah, so, and actually the next two topics are focusing really on generalization and

16:47.560 --> 16:48.920
reinforcement learning in a way.

16:48.920 --> 16:54.320
So what batch off policy RL is, is say that you have, so I guess, let's first talk about

16:54.320 --> 16:55.960
kind of the standard reinforcement learning settings.

16:55.960 --> 16:59.560
So in the standard reinforcement learning setting, your agent collects some data, you learn

16:59.560 --> 17:03.240
from that data, then you collect some more data, you learn from that data, and you iterate

17:03.240 --> 17:05.200
this process.

17:05.200 --> 17:10.680
And what off policy reinforcement learning methods are methods that, I guess, maybe I'll start

17:10.680 --> 17:15.200
with on policy methods, on policy methods, they collect data, learn from that data, and

17:15.200 --> 17:19.560
then they throw away that data and collect a new batch of data and learn.

17:19.560 --> 17:23.240
And so they're always collecting data from their current, their current policy, their current

17:23.240 --> 17:28.000
actor, and they can't, they don't have a way to reuse any data that they collected previously

17:28.000 --> 17:31.600
because they need the data to be from their current policy.

17:31.600 --> 17:35.640
Now off policy methods are ones that can actually leverage data that they collected from

17:35.640 --> 17:36.640
previous policies.

17:36.640 --> 17:42.120
One that can leverage what's called off policy data, data that's not from your current policy,

17:42.120 --> 17:44.080
your current behavior.

17:44.080 --> 17:46.760
And these algorithms tend to be a lot more efficient because you don't, you're not throwing

17:46.760 --> 17:50.880
away data at every single iteration of your algorithm.

17:50.880 --> 17:55.960
Now batch off policy RL algorithms take this to an extreme where they assume that you

17:55.960 --> 18:00.840
actually are just given a batch of data from some, from some policy from, according to

18:00.840 --> 18:05.200
some behavior, and then try to learn from that and I don't have any ability to collect

18:05.200 --> 18:07.480
more data in that environment.

18:07.480 --> 18:12.200
And the reason why this is really important and interesting is that first, if you think

18:12.200 --> 18:17.800
of just the majority of machine learning, you consider like, you often have settings

18:17.800 --> 18:21.240
where you have some data set, maybe something like image net, you're just given a batch

18:21.240 --> 18:23.920
of data and you want to learn from that data.

18:23.920 --> 18:29.960
And if we have algorithms that can just learn behaviors and learn policies from a batch

18:29.960 --> 18:34.480
of data, that means that we can start just accumulating very large and diverse data sets

18:34.480 --> 18:38.720
and allowing algorithms to learn from them without having to kind of have this iterative

18:38.720 --> 18:40.560
data collection process in the loop.

18:40.560 --> 18:44.320
And the second reason why it's important is that if there are a number of settings

18:44.320 --> 18:50.360
where it's just, it's unsafe or not possible to collect more data such as if you imagine

18:50.360 --> 18:53.960
for example, wanting to learn how to make medical decisions.

18:53.960 --> 19:01.200
You want to learn from data of doctors' decisions or maybe you have another system that's interacting

19:01.200 --> 19:08.680
with users in a way that isn't safe to take actions in random ways, then you want to

19:08.680 --> 19:14.240
just be able to use data from doctors, for example, and learn from that data without having

19:14.240 --> 19:20.280
to experiment or collect more data by kind of randomly taking actions or by exploring.

19:20.280 --> 19:21.640
Does that make sense?

19:21.640 --> 19:22.640
It does.

19:22.640 --> 19:30.440
One question I have is, does the sequence of actions, is that necessarily included from

19:30.440 --> 19:35.800
or excluded from the typical problem set up for batch off policy RL?

19:35.800 --> 19:40.080
Yeah, so typically you do assume that you have the actions that were taken as well.

19:40.080 --> 19:45.200
So you know what action or what decision the doctor made in the medical decision making

19:45.200 --> 19:47.200
example.

19:47.200 --> 19:50.840
But one quite interesting setting would be, maybe you have some data with actions, but

19:50.840 --> 19:55.480
some data without actions, like you're just observing humans on the internet doing stuff

19:55.480 --> 20:00.120
and maybe you could try to learn how to manipulate objects from that data and that's a setting

20:00.120 --> 20:03.320
that some students in my lab have actually been studying recently.

20:03.320 --> 20:04.320
Interesting.

20:04.320 --> 20:10.720
Yeah, that question was prompted by your idea that at some point we might just be able

20:10.720 --> 20:18.480
to take some collection of data and learn from it using off policy RL agent, but that

20:18.480 --> 20:23.840
would seem to assume that there's some known sequence of actions that you'd have to

20:23.840 --> 20:30.600
have that and that seems to make it less of a natural data set if that makes any sense.

20:30.600 --> 20:35.640
With that background in mind, what were the specific advancements in the couple of papers

20:35.640 --> 20:37.600
you identified on this topic?

20:37.600 --> 20:43.080
Yeah, so I guess getting back to benchmarks, we actually don't have really good benchmarks

20:43.080 --> 20:48.880
for this setting that actually have meaningful real world settings, but the algorithms themselves

20:48.880 --> 20:54.320
showed a lot of promise towards enabling good learning in these settings.

20:54.320 --> 21:00.120
So the first one actually took the replay buffer of an agent on Atari and when they only

21:00.120 --> 21:07.180
took this replay buffer, they were actually able to outperform the policy in that replay

21:07.180 --> 21:10.840
buffer just by learning from that data.

21:10.840 --> 21:17.320
So that result I think was quite impressive and also is just a suggestion that we should

21:17.320 --> 21:22.080
be able to learn very well from these types of data if we set up our algorithms well.

21:22.080 --> 21:26.920
And the second approach, which actually, the second paper that I linked, which actually

21:26.920 --> 21:30.240
predates the first one, also showed quite strong results.

21:30.240 --> 21:33.600
They weren't looking at the Atari domain.

21:33.600 --> 21:37.880
They're looking more at these continuous control domains, but they also showed the ability

21:37.880 --> 21:47.240
to learn behavior from these batches of data, not quite to the extent of the replay

21:47.240 --> 21:51.480
buffer, if I remember correctly, but we're showing pretty strong results there.

21:51.480 --> 21:57.760
The paper that's looking at the replay buffer, the result seems counterintuitive, if I'm

21:57.760 --> 22:05.600
interpreting it correctly, you basically have this replay data from a RL agent.

22:05.600 --> 22:10.360
So everything the agent kind of saw as it was exploring these games and then you give

22:10.360 --> 22:16.960
it to another agent to learn off of and the agent somehow performs better than the

22:16.960 --> 22:22.960
original agent, but it doesn't see necessarily anything more than the original agent saw

22:22.960 --> 22:28.560
as it that it has access to all of it at the same time, whereas the original agent only

22:28.560 --> 22:32.440
saw it in snapshots.

22:32.440 --> 22:33.680
So yeah, that's a good point.

22:33.680 --> 22:39.400
I think that the reason is that they didn't give it, they didn't train the first agent

22:39.400 --> 22:40.680
completely to convergence.

22:40.680 --> 22:49.280
They took a batch of the log data from that agent before it had reached its max performance

22:49.280 --> 22:54.600
and then showed that there was some room for improvement.

22:54.600 --> 23:01.000
If you give the online agent more data, it would have achieved the full maximum performance

23:01.000 --> 23:04.600
or would have achieved as well as the offline agent.

23:04.600 --> 23:08.680
It was just that they stopped things early and then wanted to test, can you do better

23:08.680 --> 23:11.160
with this batch of data?

23:11.160 --> 23:16.000
This gets to the question of how should we set up these experiments with batch off policy

23:16.000 --> 23:17.000
reinforcement learning methods.

23:17.000 --> 23:22.960
I think that things like training on logged DQN data or logged data from Atari isn't a

23:22.960 --> 23:27.960
great experimental set because it doesn't necessarily test the types of things that we want

23:27.960 --> 23:33.800
from these algorithms necessarily because we may not have that sort of data when we're

23:33.800 --> 23:38.360
performing reinforcement learning in real world settings, but it still I think is a step

23:38.360 --> 23:42.520
in the right direction and at least people are starting to study these kinds of problem

23:42.520 --> 23:43.520
settings.

23:43.520 --> 23:46.520
And proposing benchmarks for them.

23:46.520 --> 23:51.520
Yeah, so I guess one of the reasons why I found it interesting wasn't just the kind of

23:51.520 --> 23:57.320
the results, but also the visualizations and some of the, I think you provided an understanding

23:57.320 --> 24:03.760
of the problem in terms of understanding where what we can do in these problem settings

24:03.760 --> 24:05.240
to do better.

24:05.240 --> 24:10.080
And basically what are the kind of sorts of things that we might try to do and like in

24:10.080 --> 24:11.400
terms of solving this problem.

24:11.400 --> 24:15.400
So when you move from this kind of batch setting to a policy that you're learning from that

24:15.400 --> 24:19.960
batch of data, you have this distribution mismatch between the states and actions visited

24:19.960 --> 24:25.480
by the first policy and that batch of data and the states and actions visited by the policy

24:25.480 --> 24:27.880
that you're learning, the behavior that you're learning.

24:27.880 --> 24:32.240
And so it provides some nice visualizations and to understanding how we might try to handle

24:32.240 --> 24:38.320
this distribution mismatch and in particular they focus on the action setting, the distribution

24:38.320 --> 24:43.320
mismatch and the actions and made some nice visualizations for understanding what is actually

24:43.320 --> 24:46.040
happening under this distribution mismatch.

24:46.040 --> 24:49.920
And this distribution mismatch is what they're calling the bootstrapping error.

24:49.920 --> 24:51.160
I believe so, yes.

24:51.160 --> 24:52.160
Got it.

24:52.160 --> 24:58.160
You mentioned that this work on batch off policy as well as the next paper that you had in

24:58.160 --> 25:05.760
mind, RL with diverse offline data sets are kind of common in that they're both tackling

25:05.760 --> 25:08.760
generalization for RL.

25:08.760 --> 25:13.840
That can mean a lot of things in what sense are these focused on generalization?

25:13.840 --> 25:14.840
Yeah.

25:14.840 --> 25:18.440
So I guess the papers that we just talked about, they actually aren't really focused

25:18.440 --> 25:22.680
on generalization, but I think that if we build better batch off policy reinforcement

25:22.680 --> 25:27.040
learning methods, we'll have the ability to learn from more diverse data sets because

25:27.040 --> 25:32.560
we won't be collecting data for every, like within the context of our algorithm.

25:32.560 --> 25:38.520
So for example, in the context of a robotics, say, if you want to generalize to something

25:38.520 --> 25:42.960
at the level of ImageNet, that would mean that you'd have to collect an ImageNet style,

25:42.960 --> 25:48.320
ImageNet diverse size data set in the context of your reinforcement learning experiment.

25:48.320 --> 25:50.040
That just isn't practical, right?

25:50.040 --> 25:57.120
So we need to think about how we can have algorithms accumulate data into a large data set

25:57.120 --> 26:02.200
and then actually start sharing data just like the rest of machine learning does.

26:02.200 --> 26:06.280
So if we can build these very large data sets by having robots collect data and then store

26:06.280 --> 26:10.080
that into a very large buffer of data and then have algorithms that can learn from that

26:10.080 --> 26:14.800
very large buffer of data without having to kind of recollect it in the loop of reinforcement

26:14.800 --> 26:15.800
learning.

26:15.800 --> 26:21.960
And then we can start to see the generalization that we see in supervised learning settings.

26:21.960 --> 26:26.280
So this next paper is actually trying to start to study that, like can we accumulate,

26:26.280 --> 26:29.280
can we build a very large and diverse data set and then learn from it?

26:29.280 --> 26:34.320
Is it too far of a stretch to say that this is kind of pointing us in the direction

26:34.320 --> 26:40.000
of kind of a transfer learning type of approach as applied to RL?

26:40.000 --> 26:41.720
That's a good question.

26:41.720 --> 26:46.280
So in this particular paper, it is in many ways pointing at a transfer learning setting

26:46.280 --> 26:50.680
where you start, where you learn from this data set and then you learn representations

26:50.680 --> 26:57.560
for control and then you take that representation and then try to transfer that to a new setting.

26:57.560 --> 27:01.160
So analogous to ImageNet pre-training, for example, like ImageNet, you can pre-training

27:01.160 --> 27:05.880
on the ImageNet data set and then fine tune your features to a new task that allows you

27:05.880 --> 27:11.240
to transfer all of the rich diversity from ImageNet to your new problem setting.

27:11.240 --> 27:16.520
And we did something similar in this paper where we collected a very large data set,

27:16.520 --> 27:21.080
learned on it and then transferred to new robots, transferred to new experimental setups.

27:21.080 --> 27:23.200
But it's what I think that it potentially points towards that.

27:23.200 --> 27:28.200
But I also think that we may be able to ideally be able to not just study transfer learning

27:28.200 --> 27:34.000
and also be able to just generalize in zero shot to new domains and new problems.

27:34.000 --> 27:38.240
And so this paper that we're talking about is RoboNet large scale multi robot learning.

27:38.240 --> 27:42.480
If I remember correctly, you did some work in grad school at Google Brain

27:42.480 --> 27:47.360
on a kind of a large scale parallel robot platform. Is this similar?

27:48.560 --> 27:55.040
Right. So the work back at Google in 2017 or like 2016, 2017,

27:55.040 --> 27:58.960
it was actually paralyzing across 10 robots at Google,

27:58.960 --> 28:02.880
but they were all the same robot platform in the same environment.

28:02.880 --> 28:08.400
And so if we want, if we care about, it was really an important step towards this work here,

28:08.400 --> 28:15.680
but it wasn't. And it mainly served as the foundation for the work that we're doing in this paper

28:15.680 --> 28:21.440
in RoboNet. The key question that we're trying to answer now is if we don't have 10 robots,

28:21.440 --> 28:24.880
like which most labs don't, my lab at Stanford does not have 10 robots.

28:26.320 --> 28:30.960
Can we share data across institutions in a way that allows us to get the diversity

28:30.960 --> 28:36.880
of having multiple robots, having many robots. And if we can think about having institutions

28:36.880 --> 28:42.880
in universities share data across robots and across labs and across experimental setups,

28:42.880 --> 28:48.880
then we can get away from having each individual lab to have to collect their own like image net size thing.

28:49.440 --> 28:53.680
What is the nature of the data that we're talking about sharing it? Is this image data?

28:54.560 --> 28:56.480
Is it some other type of data?

28:56.480 --> 29:04.080
Yeah, so in this paper, the data corresponded to trajectories of images and the actions

29:04.080 --> 29:09.440
the robot took. So essentially video plus a sequence of actions that corresponded to that

29:10.080 --> 29:16.800
to that video, to that outcome of images. And is it video kind of off robot video or video

29:16.800 --> 29:20.720
from the perspective of a manipulator or something else?

29:21.280 --> 29:25.600
Yeah, so we actually included multiple camera viewpoints in the data sets such that some of them

29:25.600 --> 29:29.840
were somewhat of a first person's perspective. Some of them were more of a third person perspective,

29:29.840 --> 29:33.120
really corresponding to just different camera placements around the robot.

29:33.120 --> 29:36.720
And the total number of viewpoints we had in the data set was actually over 100.

29:36.720 --> 29:43.600
Okay. And the data set also included data not just from like, so in the Google data set,

29:43.600 --> 29:50.160
it was like 10 robots, one robot platform. And this data set, it's around 10 robots, but it's like

29:50.160 --> 29:55.360
seven robot platforms. So a significant diversity actually, the kinds of robots and the colors of

29:55.360 --> 30:01.360
the robots and the kinematics of the robots across across these different labs and across actually,

30:01.360 --> 30:08.000
across four different institutions. Okay. And the specific learning objective in this set up

30:08.000 --> 30:12.720
is what? Right. So that's where this is where off policy, batch off policy reinforcement learning

30:12.720 --> 30:17.360
methods come in. So if you have, if you have all this data, you need to figure out how to learn from

30:17.360 --> 30:23.600
it. And then yeah, the good question is like, what is the reward function, right? What should you

30:23.600 --> 30:29.600
learn from this sort of data? In this particular setting, we actually use model-based

30:29.600 --> 30:33.760
reinforcement learning methods. So we were learning to predict video based off of the actions that

30:33.760 --> 30:41.840
the robot took. And then once we had this this model of the of the world, then we would actually

30:41.840 --> 30:46.000
use that model of test time to accomplish different tasks. So we would give it a you kind of give

30:46.000 --> 30:50.960
it a new task at test time and then it would use its model to plan to plan to achieve that task,

30:50.960 --> 30:57.520
using its learn model rather than trying to learn a policy during during training. So it actually

30:57.520 --> 31:03.200
is kind of learning behavior on the fly at test time using its learn model. And then the kind of

31:03.200 --> 31:08.880
the tasks that we were testing on correspond to like manipulation tasks like picking up a cup

31:08.880 --> 31:14.880
and positioning it next to other cups or pushing a pencil to be next to some other pencils.

31:14.880 --> 31:21.840
Okay, so in the case of a cup, you've got a model that you've learned off of the data set offline

31:22.560 --> 31:31.440
that has basically kind of built out a view of the world. If I do take some action on the cup,

31:31.440 --> 31:40.880
the future world is probably going to look like this. And then you then give a live robot access

31:40.880 --> 31:49.280
to this and it's also being trained in a RL. Is it is it also an RL agent that's training that

31:49.280 --> 31:56.240
when it's live or is it some other kind of approach? It's so it's it's not quite reinforcement learning.

31:56.240 --> 32:01.600
It's more just in in some ways like model based control in some in some ways as we were talking

32:01.600 --> 32:06.800
about before, but with the learn model. So it's what's called planning. So if you have a model,

32:06.800 --> 32:10.640
if you know what kind of view you have an understanding of what will happen if you take some actions,

32:10.640 --> 32:15.840
and you can use that model to plan a sequence of actions for trying to accomplish a goal. So

32:15.840 --> 32:22.080
some more like an optimization problem across the likely outcomes from the model that you already have.

32:22.880 --> 32:27.360
Yeah, exactly. It's this optimization problem over actions given a goal and your model.

32:27.360 --> 32:32.640
Mm-hmm. Okay. But one of the things that we're doing next is we're we're trying to look into if we

32:32.640 --> 32:39.360
can annotate rewards in the context of in his data set for different tasks. And then that would

32:39.360 --> 32:44.880
allow us to try to do some of this training, do some of this optimization over policies or behaviors

32:44.880 --> 32:50.240
at training time so that we could study algorithms like model free methods. And a reward annotating

32:50.240 --> 32:56.560
a reward in this context means that if the goal is to stack cups that the cup is you know a

32:56.560 --> 33:00.560
picture where the cup is stacked, or are we talking about something more liable than that?

33:00.560 --> 33:05.600
Yeah, it could be something like that. And it may one of the things that's a little bit

33:05.600 --> 33:09.840
challenging in like in robotics context is that if you have a picture of cups that are stacked,

33:09.840 --> 33:13.760
that doesn't mean that you actually have a good reward function for that. So it's actually hard

33:13.760 --> 33:18.480
to compare two images like if you have an image of cup stack, how do you know that another image

33:18.480 --> 33:23.200
also has cup stacked or also isn't achieving that goal in some way. So one of the things we'd

33:23.200 --> 33:29.360
be thinking about is just really simple things like label if an object is being grasped by the robot.

33:29.360 --> 33:34.320
Like one if it's being grasped and zero otherwise. And then we could use that that reward

33:34.320 --> 33:39.200
function for grasping to learn a policy for grasping. So we're thinking about very simple

33:39.200 --> 33:44.000
reward functions like that like are there cup stacks in this image or is there about holding

33:44.000 --> 33:49.520
something things like that. All right, so then the next couple of papers that you called out are

33:49.520 --> 33:56.160
focused on the the broad curriculum learning area. What's been going on there? I remember

33:56.160 --> 34:01.200
correctly we talked briefly about that a couple years ago on that podcast we did. There were two

34:01.200 --> 34:05.840
curriculum learning curriculum based approaches that I found to be quite interesting and exciting

34:05.840 --> 34:11.680
this year. And in both the cases the agent was actually coming up with its own curriculum

34:11.680 --> 34:19.520
for solving tasks. So one of them was this paper from some folks at Uber AI that was actually

34:19.520 --> 34:27.760
using genetic algorithms and evolutionary methods to generate increasingly complex environments

34:27.760 --> 34:33.040
for the agent to learn. So it was a locomotion based task where there was this agent

34:33.040 --> 34:37.280
this legged robot insinuation that needed to move forward. And then it was generating

34:38.000 --> 34:42.320
different different stepping stones and different environments that made it more challenging

34:42.320 --> 34:48.960
for the robot to to traverse the terrain. And the second one was a single environment but

34:48.960 --> 34:55.280
that what it was trying to do it was essentially trying to play sort of a kind of hide and seek game

34:55.280 --> 35:00.880
where there were some agents that were trying to that were trying to hide. And so they were given

35:00.880 --> 35:07.120
like five seconds or some some amount of time to like go hide. And then the secret agents after

35:07.120 --> 35:12.880
that had to go try to find the agents that were hiding. And all this was insinuation. But the one

35:12.880 --> 35:16.720
of the interesting things was that it would kind of learn this sort of curriculum where initially

35:16.720 --> 35:23.200
the hiders were doing very obvious things that were pretty easy to find. But then later they would

35:23.200 --> 35:28.880
actually start making barricades and making it harder for the the secret agents to actually go

35:28.880 --> 35:37.280
and find the hiders. So the curriculum was in that latter case the learning agent was the

35:37.280 --> 35:43.600
hider or the finder. So in this case actually both the hider and the finder were learned.

35:43.600 --> 35:49.440
Okay. They were both being learned simultaneously and the curriculum emerged simply from the fact

35:49.440 --> 35:56.080
that this was a multi agent optimization. And so as the first the kind of hiders would learn some

35:56.080 --> 36:01.040
sophisticated behaviors and then they would be winning and then the the the seekers or the

36:01.040 --> 36:07.200
finders would learn a would start to learn more sophisticated behaviors as well to go find them.

36:07.200 --> 36:13.120
And then you would have this kind of alternating thing where one of them like starts winning and then

36:13.120 --> 36:20.080
the other agents will need to learn more sophisticated behaviors to overcome the types of strategies

36:20.080 --> 36:24.720
that the other set of agents learned. Right. It's probably worth taking a step back and

36:24.720 --> 36:31.120
and kind of providing a high level overview of curriculum learning. As I understand it again,

36:31.120 --> 36:34.960
I think for me, I think you were the first person that explained this to me. The idea is that

36:35.600 --> 36:41.200
in a you know any kind of scenario like that you might have a reinforcement learning agent

36:41.200 --> 36:49.120
in a big part of the problem is that the the state space is huge. And you know, that's not not

36:49.120 --> 36:55.120
unlike how we as humans learn, you know, if we had to learn everything, you know, the possibilities

36:55.120 --> 37:01.200
are huge. So in school, for example, we create a curricula that has us learn, you know, X,

37:01.200 --> 37:07.920
then Y, then Z and in doing so that kind of creates a more narrow path for us to get through this,

37:07.920 --> 37:13.840
you know, the possibilities and curriculum learning is trying to do something similar where first

37:13.840 --> 37:18.240
you teach the agent to do the first thing, then you teach it to do like in the case of,

37:18.800 --> 37:24.080
you know, some of these locomotion examples, you first you teach it to crawl, then you teach it

37:24.080 --> 37:28.480
to stand up, then you teach it to run, then you teach it to jump that kind of thing. Yeah, exactly.

37:28.960 --> 37:34.160
Does it count as a as curriculum learning if it's just something that the agent does without,

37:34.160 --> 37:40.880
you know, some special capability to learn curricula? Does that make sense? Particularly around that

37:40.880 --> 37:46.560
second example with the hiders and the the seekers, you know, what we're doing, we're kind of

37:46.560 --> 37:52.080
observing after the fact, you know, the things that these agents did and calling it curriculum

37:52.080 --> 37:59.520
learning, but was there something specific to the agent that, you know, made it learn in that way

37:59.520 --> 38:06.000
or is that kind of orthogonal to the point anyway? It doesn't not matter. Yeah, so to me,

38:06.000 --> 38:12.080
I think that a curriculum is characterized by an increase in complexity and if for the kind of

38:12.080 --> 38:17.680
that the beginning, you learn very simple things and at the end, you're doing much more sophisticated

38:17.680 --> 38:23.600
and complex behaviors than it seems like there's this there's this progression of simple to complex.

38:24.240 --> 38:29.440
In the case of the first paper, this was fairly explicit. In the case of the second paper,

38:29.440 --> 38:34.080
I think that this was more of an emergent property of the algorithm or an emergent property of

38:34.080 --> 38:39.440
this multi agent optimization and it wasn't necessarily something that was built in to the algorithm

38:39.440 --> 38:45.120
from the start. And I think that just this progression of from very simple behaviors to very complex

38:45.120 --> 38:50.960
behaviors is the thing that's very exciting to me because it means that if we can if we can move

38:50.960 --> 38:55.520
from very simple behaviors to more complex behaviors, then maybe we can also move from very complex

38:55.520 --> 39:01.920
behaviors to even more complex behaviors. And it seems like a path towards agents that are

39:01.920 --> 39:07.200
increasingly sophisticated and increasingly intelligent. And so from that perspective,

39:07.200 --> 39:12.960
it doesn't matter to you whether that's because of a training regime that has curricula built in

39:12.960 --> 39:19.520
that we create or whether it's just something that this complex that emerges in a complex system.

39:19.520 --> 39:25.120
Yeah, and I actually think that potentially it's maybe even more exciting if it emerges or if the

39:25.120 --> 39:30.800
agent creates it itself because that means that it won't rely on us for moving to the next level.

39:31.360 --> 39:34.320
The next couple of papers were focused on exploration problems?

39:34.960 --> 39:41.760
Yeah, so these are kind of getting back to Atari Land. And in terms of the applications that

39:41.760 --> 39:49.600
they were studying. And in general, exploration is kind of a huge challenge and reinforcement

39:49.600 --> 39:56.400
learning. It's the problem of discovering the right thing to do in settings where you're not

39:56.400 --> 40:01.120
given a lot of supervision oftentimes about what the right thing to do is you basically need to explore

40:01.120 --> 40:05.520
your environment and find the parts of your environment, the parts of your state space that give

40:05.520 --> 40:14.800
you high reward. And in the past, one of the kind of classic problems or benchmarks in exploration

40:14.800 --> 40:19.360
and reinforcement learning has been a Atari game called Monizuma's Revenge. I don't think it's

40:19.360 --> 40:24.800
the only problem that we care about in the context of exploration, but it's one that has been notably

40:24.800 --> 40:32.160
challenging for our current reinforcement learning systems. And these two papers were both

40:32.160 --> 40:40.960
proposed means for actually solving Monizuma's Revenge to a very large degree, scoring tens of

40:40.960 --> 40:46.640
thousands of points on this game when I think previous approaches were often only getting zero

40:46.640 --> 40:53.600
points or just a few points on the game. So I think that these approaches were making a

40:53.600 --> 40:57.680
lot of progress there. And the key insight of the first one, which is actually a little bit

40:57.680 --> 41:05.760
controversial, was to find the parts of the game where you're getting some, making some progress.

41:05.760 --> 41:11.920
And then actually, if you then die at those parts of the game, actually reset to that state and

41:11.920 --> 41:16.880
start kind of exploring in that region of the state space again. And so really trying to

41:17.760 --> 41:22.800
remember, really trying to remember the visits states that are promising, returning to those

41:22.800 --> 41:28.480
promising states and then explore from it. And so one of the reasons why it was controversial is

41:28.480 --> 41:32.640
that they were using this reset ability to kind of go to a particular state that you had been to before.

41:33.760 --> 41:37.440
And in many real world contexts, that's of course not something that you can do, but if you're

41:37.440 --> 41:42.080
an Atari game, that is something that you could conceivably do. And then the second paper

41:42.800 --> 41:47.200
took a very similar approach, but they lifted this assumption by using an imitation learning approach

41:47.200 --> 41:53.840
to figure out, to kind of remember how to get back to that state. So both of these papers

41:54.560 --> 42:04.560
are a share in common that they take advantage of ways to spend more time exploring difficult

42:04.560 --> 42:13.440
areas of much as soon as revenge, one using some kind of God mode reset capability. And the other

42:13.440 --> 42:19.120
is just remembering how they got to given states and getting there before they start doing

42:19.120 --> 42:24.400
exploration. Exactly, yeah. Okay, I can see how the first one is controversial because in the

42:25.360 --> 42:30.160
in the paper, they're kind of report their scores. But if you keep doing these unnatural things

42:30.160 --> 42:35.760
like resetting and going to hard places and kind of accumulating more score, it doesn't seem

42:35.760 --> 42:41.440
to be comparable to the other scores that are reported for this game. Yeah, I think it's just

42:41.440 --> 42:46.400
worth briefly mentioning that they're, I focused on a number of, I think, conceptual advances

42:46.400 --> 42:52.160
in various approaches. And at the same time, there have been these fairly large efforts to try

42:52.160 --> 42:58.800
to use reinforcement learning and other approaches to solve harder types of games. So StarCraft,

42:59.680 --> 43:04.240
there's a large team of deep mind that was trying to solve a version of StarCraft

43:04.240 --> 43:10.880
through reinforcement learning and also, and also imitation learning. And there was also a fairly

43:10.880 --> 43:15.760
large team at OpenAI that was trying to study reinforcement learning algorithms on the game Dota 2.

43:15.760 --> 43:19.920
And both of them made quite notable progress in those settings. And I feel like I can't,

43:20.640 --> 43:25.840
I can't summarize 2019 without at least making a very brief mention of those two results.

43:25.840 --> 43:33.360
And OpenAI on the Dota front has been working on this for a while. This goes back to 2018, at least,

43:33.360 --> 43:37.760
first potentially earlier than that. Yeah, and I think that StarCraft actually also goes back

43:37.760 --> 43:45.600
into 2018 as well. I think that they've been long efforts. So one of the other things that we

43:45.600 --> 43:55.040
like to cover in these AI rewind segments is more kind of practical, tangible advances in terms

43:55.040 --> 44:00.640
of new tools and libraries and open source projects, things like that. And you had a number that

44:00.640 --> 44:05.760
that come to mind. Where do you like to start? Yeah, so I'll start with some of the libraries that

44:05.760 --> 44:11.840
have been made available. So in general, reinforcement learning, building reinforcement learning

44:11.840 --> 44:16.880
algorithms is challenging. And we're actually still in the stage where for for deep reinforcement

44:16.880 --> 44:22.640
learning algorithms, many, many algorithms can be, I guess the first is just so many design

44:22.640 --> 44:27.600
decisions when designing a reinforcement learning agent such as when do you collect data, how do

44:27.600 --> 44:33.200
you collect data and like at a per time step level versus like an episodic level? Do you normalize

44:33.200 --> 44:40.160
your state's in actions? Do you how do you kind of estimate your return? There's just all of these

44:40.160 --> 44:45.280
really tiny design decisions that that can actually make a pretty large difference on the result

44:45.280 --> 44:48.320
of the algorithm. And so having implementations of reinforcement learning algorithms that are

44:48.320 --> 44:55.360
trustworthy and and are actually kind of ready to use out of the box for for different applications.

44:55.360 --> 45:01.280
I think it's quite important for the advancement of reinforcement learning. For kind of specific

45:01.280 --> 45:06.080
algorithms, there have been a number of open source implementations released by the authors of

45:06.080 --> 45:10.240
those papers and may wait in may times those those implementations are in some ways the most

45:10.240 --> 45:15.440
trustworthy because they're the ones that should reproduce the results in the paper. But there's also

45:15.440 --> 45:21.280
been a library called TF agents or TensorFlow agents that provides actually a platform of many

45:21.280 --> 45:27.360
different algorithms, many of it reinforcement learning algorithms. And it's trying to basically

45:27.360 --> 45:34.320
provide a kind of a unified code interface and framework for running these different types of

45:34.320 --> 45:40.640
algorithms and making it with the goal of making it easier for people to use these algorithms on

45:40.640 --> 45:48.160
their problems. And so how much of the the problem of getting an RL agent up and running does

45:48.160 --> 45:56.080
does TF agent solve? Does it get you all the way there? It's always struck me with RL there are

45:56.080 --> 46:02.480
just so many moving pieces. You've got your simulation environment or your game environment. You've

46:02.480 --> 46:10.000
got your agents. You've got to figure out your optimization, your loss function. How much of that

46:10.000 --> 46:15.840
does TF agents take care of for you? So if you want to use an environment like OpenAI

46:15.840 --> 46:20.800
Jim, for example, then I think that you should be able to run it like it basically will solve

46:20.800 --> 46:28.320
everything for you. If your environment interface is different from Jim, then of course it you'll

46:28.320 --> 46:34.480
need to do some plumbing essentially to hook it up with that. And it also doesn't solve the

46:34.480 --> 46:38.960
fact that if you have a new environment, these algorithms may not work out of the box on that

46:38.960 --> 46:44.240
environment because of you may need to tune it or maybe just that your problem is harder than

46:44.240 --> 46:49.360
the current kinds of reinforcement learning problems that we can solve. But if you want to reproduce

46:49.360 --> 46:55.760
one of the standard algorithms using a known environment like Jim, you should be able to do it

46:55.760 --> 47:01.280
fairly handily. I think you should be able to do it fairly handily with with this code base.

47:02.000 --> 47:06.720
It's also worth mentioning I think that this is from very late 2018, but there was also a

47:08.880 --> 47:13.200
a framework called dopamine that was trying to do something somewhat similar, but they were

47:13.200 --> 47:18.480
had a more narrow and narrow focus, which was to study Q learning algorithms, various types of

47:18.480 --> 47:23.600
Q learning algorithms for Atari games. So things like deep Q networks, distributional reinforcement

47:23.600 --> 47:27.280
learning, prioritize experience, replay all the bells and whistles that you might want in your

47:27.280 --> 47:33.120
DQ Ed agent, specifically looking at Atari games. That code base I think provided a very reliable

47:33.120 --> 47:37.440
implementation of that kind of suite of algorithms, which are basically with a more narrow

47:37.440 --> 47:43.200
focus than TF agents. And you also mentioned PyTorch higher. What's that all about?

47:44.000 --> 47:50.240
Yeah, so this isn't necessarily a reinforcement learning thing, but it's a library that's been

47:50.240 --> 47:54.640
very useful for metal learning research. So I don't know how much of the details I want to get

47:54.640 --> 47:59.440
into with metal learning, but kind of the goal is to learn, as I kind of mentioned at the beginning,

47:59.440 --> 48:03.280
the goal is to learn priors from previous experience in a way that allows you to learn very quickly,

48:03.280 --> 48:08.640
like learn with only a few data points for new tasks. And one very popular approach for metal

48:08.640 --> 48:14.480
learning is to perform a what's called a bi-level optimization, where you're actually embedding

48:14.480 --> 48:20.400
a optimization process inside another optimization process, and higher provides a way to

48:21.120 --> 48:26.080
perform these higher order optimizations, like for example, if you're embedding one optimization

48:26.080 --> 48:32.800
inside another, you have a second order optimization process. And this this library allows it,

48:32.800 --> 48:38.080
makes it very easy for people to do that. And I haven't used it personally myself, but my PhD

48:38.080 --> 48:41.840
students have been using it and have said have said wonderful things about it.

48:42.800 --> 48:52.480
Is the idea with bi-level optimization and metal learning that you're optimizing whatever

48:52.480 --> 48:57.360
problem you're trying to solve at one level and at another level you're optimizing the way you

48:57.360 --> 49:00.640
learn how to solve that problem? Exactly. Yeah, that's a great way to put it.

49:00.640 --> 49:09.520
And so higher isn't necessarily a metal learning library, but it's a general bi-level optimization

49:09.520 --> 49:15.920
library, or is it more specifically geared towards metal learning? It is a more general thing,

49:15.920 --> 49:20.320
but they also provide a number of optimizers that make it good specifically for metal learning

49:20.320 --> 49:27.440
use cases. Okay. And maybe to further kind of skirt this line of going too deep in on metal

49:27.440 --> 49:35.280
learning, is there a classic problem setup or hello world of metal learning that would help

49:35.280 --> 49:40.400
make it more concrete for folks that aren't familiar with it? It's a good question. Yeah,

49:40.400 --> 49:45.520
so I think that maybe one one very simple like one that kind of very standard problem in metal

49:45.520 --> 49:52.000
learning is can you learn to recognize characters of a new alphabet with a few examples? So can you

49:52.000 --> 49:57.920
give an example of five different handwritten digits? Can you learn a classifier that can

49:57.920 --> 50:02.400
distinguish those five handwritten digits? And the way that these metal learning algorithms work

50:02.400 --> 50:07.840
is that they take a handwritten digits from a number of different alphabets and languages

50:07.840 --> 50:12.880
and learn at the lower level they're trying to learn how to recognize distinguished digits from

50:12.880 --> 50:17.200
a particular alphabet and at the higher level they're trying to change the way that that lower

50:17.200 --> 50:23.440
level learns across alphabets in a way that makes it generalize faster and in a way that makes

50:23.440 --> 50:28.560
allow it to learn from only five examples. So for example, if you trained a deep neural network

50:28.560 --> 50:32.560
on five examples it would probably overfit massively or not be able to learn very much.

50:33.520 --> 50:39.440
And these algorithms try to actually train for the ability to generalize from a few examples by

50:39.440 --> 50:45.520
changing the way that neural networks are learning. So we've talked a little bit about open AI gem

50:45.520 --> 50:50.880
and some of these other simulation environments. There were also some new ones that were introduced

50:50.880 --> 50:55.920
this year. Absolutely. And I think that kind of maybe jump me ahead a little bit too much is

50:55.920 --> 51:00.880
one of my predictions for next year is that we're going to really need better environments for studying

51:00.880 --> 51:05.280
the kinds of problems that we care about. So I think that if we care about things like generalization

51:05.280 --> 51:11.360
like the ability to learn new tasks quickly like the ability to solve longer horizon tasks maybe with

51:11.360 --> 51:17.440
the use of demonstration data, then we need environments that allow us to actually evaluate those

51:17.440 --> 51:21.680
abilities and our algorithms. And there have been a number of environments that were introduced

51:21.680 --> 51:27.520
this year that tried to focus on different aspects of this. So for example, there was the AI habitat

51:28.080 --> 51:32.800
environment that was developed specifically for visual navigation. And I was specifically trying

51:32.800 --> 51:38.720
to target the setting where you want to learn how to navigate environments with photo realistic

51:38.720 --> 51:44.560
rendering where basically you're dealing with images with where your observations are highly

51:44.560 --> 51:49.600
realistic images such that you aren't learning from these kind of really simple game graphics.

51:49.600 --> 51:54.240
So it's such that you're actually studying both the vision aspect of the problem as well as

51:54.240 --> 51:59.600
the control aspect. The second environment is the meta world environment which is an environment

51:59.600 --> 52:04.080
that one of my PhD students and some of my collaborators have been working on where we've been

52:04.080 --> 52:10.000
trying to study or allow ourselves to study generalization across tasks. So we create a benchmark of

52:10.000 --> 52:16.400
50 manipulation tasks and simulation. And with the hope of seeing if it can allow us to study one

52:16.400 --> 52:20.400
whether or not we can have algorithms learn across all of the tasks and two whether or not we have

52:20.400 --> 52:26.960
algorithms that can use say 45 of the tasks in a way that allows you to quickly learn new tasks

52:26.960 --> 52:32.960
like the next five tasks. And that was kind of targeting meta-reinforce and learning algorithms

52:32.960 --> 52:38.720
that can learn how to learn from small amounts of data or small amounts of data for new tasks.

52:38.720 --> 52:45.040
The minor realm competition that I also mentioned has been looking at Minecraft environments which

52:45.040 --> 52:49.280
I think is a really interesting taskbed for reinforcement learning methods because it's more open

52:49.280 --> 52:53.680
ended. And they specifically one of the things that they've been focusing on there was the ability

52:53.680 --> 52:57.840
to learn from example behavior and from demonstrations. And so they collected a very large data set of

52:57.840 --> 53:02.960
humans playing in these Minecraft environments with the hope of building reinforcement learning agents

53:02.960 --> 53:07.760
that can use this data as well as their own data collected that they collected themselves in the

53:07.760 --> 53:16.000
environment in order to learn in order to learn complex and long horizon skills. The recism that I

53:16.000 --> 53:24.400
mentioned is a kind of a simulation platform for studying whether or not you can learn a recommender

53:24.400 --> 53:28.320
system. And I think that one of the things that I really like about this is that it actually allows you

53:28.320 --> 53:34.720
to study a more real world problem that's quite different from things like games and things like

53:34.720 --> 53:38.320
robotics. Yeah, I was not expecting recommender systems to come up here.

53:39.360 --> 53:44.800
Yeah, and I think that people like in many ways the reinforcement learning community has been so

53:44.800 --> 53:50.960
focused on control robotics and video games. And I think that maybe we're overfitting to some of

53:50.960 --> 53:54.720
the challenges in those domains. And if we care about building reinforcement learning algorithms

53:54.720 --> 53:59.520
that are useful in a variety of settings, then we should be testing them on things like recommender

53:59.520 --> 54:04.960
systems. And I hope to see more of that too. Like you could imagine education as I mentioned before

54:04.960 --> 54:09.040
medical decision making. I think that there are a lot of potential applications of a reinforcement

54:09.040 --> 54:13.920
learning where a sequential decision making where you need a really reason about the effect of your

54:13.920 --> 54:21.680
actions on future states. Yeah. And then the last thing is this Google research football

54:21.680 --> 54:29.120
environment. And this one I believe is specifically focusing on the ability to study multi-agent

54:29.920 --> 54:35.040
reinforcement learning in the context. And by football for those of us that are Americans,

54:35.040 --> 54:42.720
they're referring to soccer. Yeah, and actually I did an interview with one of the principles in

54:42.720 --> 54:49.840
this work not too long ago. Actually, this is relatively recent, but it was episode 293 of the

54:49.840 --> 54:55.840
show and it was with Olivier Bachin. Yeah, so those were all the environments that I've seen come

54:55.840 --> 55:01.040
up that look quite promising and really filling a gap that we I think don't have in our current

55:01.040 --> 55:07.600
environments. Well, you started kind of foreshadowing into your predictions there. One of them

55:07.600 --> 55:14.400
being that we need to see even more of these types of environments. What are some of the other

55:14.400 --> 55:19.840
things that you expect to see in the field in the next year? And I guess we're going into 2020,

55:19.840 --> 55:25.600
so we could we could even talk about the next decade if you dare. Yeah, so I think that in addition

55:25.600 --> 55:30.800
to an increase in environments that will hopefully allow us to meaningfully study things like

55:30.800 --> 55:35.920
generalization and batch off policy RL instead of repurposing old environments for those things.

55:35.920 --> 55:40.800
I think we'll also start to see an increase in papers that study settings like batch off policy

55:40.800 --> 55:44.800
reinforcement learning. So I think that this year we saw a pretty big increase in in people

55:44.800 --> 55:48.720
that are studying that and I think that that's probably going to continue because that's really a

55:48.720 --> 55:54.480
problem that matters in the context of real world, the real world deployment of reinforcement learning

55:54.480 --> 56:02.720
systems. So greater focus on batch off policy RL? Yeah, and then I also think that we didn't quite

56:02.720 --> 56:06.720
this was the kind of one of the papers that I was going to mention on meta reinforcement learning

56:06.720 --> 56:12.640
and multitask reinforcement learning, which we didn't quite get to cover in 2019. And I think that

56:12.640 --> 56:16.720
I think that we're also really going to see an increase in papers that study this. So I think

56:16.720 --> 56:22.000
that in general the the community has been fairly focused on like trying to solve individual tasks.

56:22.000 --> 56:25.680
And that's I think in some ways that actually been by nature of the environments that are focusing

56:25.680 --> 56:30.720
on let's learn one Atari game, let's learn to run with this one agent in this one environment.

56:30.720 --> 56:34.720
And I think that a lot of people really do care about generalization and we did actually see an

56:34.720 --> 56:40.320
increase in paper this paper is this year that we're focusing on generalization. To some degree

56:40.320 --> 56:44.080
and I think that we'll see that actually kind of the breadth of generalization that we try to study

56:44.080 --> 56:49.360
increase and in particular trying to study generalization across tasks across goals across objectives

56:50.400 --> 56:54.720
such that we can move towards general purpose reinforcement learning agents rather than these very

56:54.720 --> 57:02.960
narrow and specialized agents. And do you have a sense for what that will likely look like? Is it

57:03.920 --> 57:08.560
analogous to what you were doing with the collaboration across institutions and

57:09.680 --> 57:14.960
in the off-policy work where you were looking at multiple different robotic platforms and trying

57:14.960 --> 57:21.920
to train on different environment simultaneously? Or is it some new I don't know some new training

57:21.920 --> 57:27.280
technique or something that results in greater generalization? Yeah so I think that I would love

57:27.280 --> 57:30.800
for everyone to start studying robotics but I think that in practice people won't be

57:31.840 --> 57:38.240
a little bit scared of robots and not scared in terms of them being dangerous but just scared of

57:38.240 --> 57:42.480
the effort that goes into actually getting things working on a real robot in the real world.

57:43.280 --> 57:47.120
And so I think that in practice what that mean will mean is that people will be studying

57:47.120 --> 57:53.840
simulation agents like simulate control agents, simulate robots, maybe simulate different

57:53.840 --> 58:01.120
like video game levels for example and studying how agents can learn across, can generalize across

58:01.120 --> 58:06.960
these video game levels that can generalize across reward functions of a robot for example.

58:06.960 --> 58:13.760
And yeah I think that basically that the algorithms are mostly ready for this. I think that

58:13.760 --> 58:18.160
there's still advances to be had on interest in terms of the ability for kind of just basic

58:18.160 --> 58:23.120
reinforcement algorithms to be stable and work well with images etc. But I think that many of

58:23.120 --> 58:28.880
the algorithms are ready to take the jump towards these more complex settings where you need to be

58:28.880 --> 58:33.120
doing multiple things and not just one thing. And maybe actually I mentioned Muldi's task RL

58:33.120 --> 58:36.880
that the ability to do these different tasks but it could also be doing tasks and sequence

58:37.680 --> 58:42.160
or kind of hierarchical reinforcement learning where you're performing like picking up an object

58:42.160 --> 58:46.800
and then placing it into a bin and then taking that bin and putting it somewhere else.

58:47.680 --> 58:50.960
And I think that in general people have been studying some of these problems for a long time

58:50.960 --> 58:55.600
but I think that the will actually start to see meaningful advances in these problems

58:55.600 --> 59:01.360
in the deep RL setting and in more complex and challenging environments.

59:01.360 --> 59:02.720
Any other predictions?

59:02.720 --> 59:08.320
I think that the batch of policy multi task RL and metarole in environments are my main ones.

59:08.320 --> 59:12.640
I would guess that I guess we also talked to Affair about model based and model free. I think

59:12.640 --> 59:18.320
that people will continue to show interest in model based methods and will also continue to see

59:18.320 --> 59:23.760
a number of hybrid methods that combine elements of model based and model free algorithms as well.

59:26.240 --> 59:31.200
Yeah, so I think that kind of the things that are going up will continue to actually maybe

59:31.200 --> 59:36.960
kind of increasingly more popular things like batch off policy model based RL and metarole

59:36.960 --> 59:38.800
reinforcement learning and multi task free reinforcement learning.

59:39.520 --> 59:44.000
And one of the things that I guess I'm really excited about is that I think people will actually

59:44.000 --> 59:49.360
really start to meet and study generalization and I think that this is something that's been overlooked

59:49.360 --> 59:51.440
for a very long time in reinforcement learning.

59:52.480 --> 59:57.600
One of the questions that I get all the time about reinforcement learning, particularly deep

59:57.600 --> 01:00:03.200
reinforcement learning is who's using it and for what? Have you come across any notable

01:00:03.200 --> 01:00:07.920
kind of real-world use cases for deep RL this year?

01:00:07.920 --> 01:00:12.160
Yeah, that's a really good question. I think that in general people aren't using it

01:00:12.720 --> 01:00:19.360
because these algorithms are they work in some context but they are still a long ways away from

01:00:19.360 --> 01:00:22.720
being just like a plug and play thing like like deep learning for example with neural networks

01:00:22.720 --> 01:00:27.680
we like with batch norm and and all the and like resnets and stuff like that. We've really figured

01:00:27.680 --> 01:00:33.200
out a way to kind of really be able to up deploy these deep neural networks in a wide range of

01:00:33.200 --> 01:00:38.240
settings if you have enough data and can actually formulate your problem as a supervised learning

01:00:38.240 --> 01:00:41.680
problem. In reinforcement learning I don't think that we're quiet at that stage yet

01:00:42.560 --> 01:00:47.680
and there have been some some applications so I know that there are some folks that use it

01:00:47.680 --> 01:00:55.200
actually for recommender systems. So like Craig Bhutiliyare for example at Google is someone who's

01:00:55.200 --> 01:01:01.360
notably who's been studying reinforcement learning in these sorts of settings. There was also one

01:01:01.360 --> 01:01:06.480
example I think a couple years ago of using reinforcement learning for like data center power

01:01:06.480 --> 01:01:13.520
management but I haven't really seen that see not really expand yeah at least from from what I've

01:01:13.520 --> 01:01:17.280
seen. I think it's also hard to say because it's not necessarily people aren't always public about

01:01:17.280 --> 01:01:22.160
the sorts of things the sorts of algorithms are using in industrial applications but my guess is

01:01:22.160 --> 01:01:28.000
that they really aren't being used much in real world applications. One of the things that makes

01:01:28.720 --> 01:01:34.560
this particular question challenging is you'll see a lot of people talking about

01:01:34.560 --> 01:01:39.280
using reinforcement learning but when then you go under the covers it's not deep reinforcement

01:01:39.280 --> 01:01:46.080
learning it's like single step kind of traditional reinforcement learning which is different.

01:01:46.080 --> 01:01:52.080
Yeah I guess one other thing worth mentioning is that there's a startup company by

01:01:53.520 --> 01:02:00.960
led by Peter Biel and others that is looking at robotic automation and they're looking at

01:02:00.960 --> 01:02:06.160
deep imitation learning and deep reinforcement learning for doing this. That's covariance.

01:02:06.160 --> 01:02:11.680
covariant yeah but I also don't know they are very public about the the kinds of techniques that

01:02:11.680 --> 01:02:20.560
they're using or the applications. I recently saw the Mark Hammond who has been on this show he

01:02:20.560 --> 01:02:25.440
founded a company called Bonsai that was acquired by Microsoft they're still doing interesting

01:02:25.440 --> 01:02:32.560
stuff with RL like I think it is happening but in terms of to your point kind of public

01:02:33.600 --> 01:02:39.040
detailed case studies about what folks are doing they are difficult to come by.

01:02:39.040 --> 01:02:44.720
Yeah and like Osaro is another startup company that is looking at deep reinforcement learning but

01:02:44.720 --> 01:02:50.160
they I don't know they're like if they're actually using it for their for the use cases or not

01:02:50.960 --> 01:02:55.600
and there's also I guess another kind of maybe more real-world examples that they're but the not

01:02:55.600 --> 01:03:01.680
deep is that there was some work I think a couple years ago by Joel Pinou looking at

01:03:01.680 --> 01:03:10.160
reinforcement learning for for brain simulation for seizures to try to stimulate the brain

01:03:10.160 --> 01:03:16.560
in a pattern that used less stimulation than kind of a standard thing while also still preventing

01:03:16.560 --> 01:03:21.680
seizures but that was using that was by no means using a deep reinforcement learning it was just

01:03:21.680 --> 01:03:26.640
using reinforcement learning with smaller models because they didn't have enough data in order to

01:03:26.640 --> 01:03:33.120
deploy these techniques with deep networks. Cool anything else we should keep our eyes peeled

01:03:33.120 --> 01:03:40.800
for in 2020? I think that that's it I'm excited to see what's to come and then I think that there

01:03:40.800 --> 01:03:46.000
is maybe one other thing worth mentioning is that there's been increasingly other labs other

01:03:46.000 --> 01:03:49.280
research labs that have really been entering the reinforcement learning space and it's been

01:03:49.280 --> 01:03:53.840
exciting to see that people are getting more interested in in the problem setting labs that

01:03:53.840 --> 01:03:58.640
were traditionally doing things like supervising and unsupervised learning and I guess I mentioned this

01:03:58.640 --> 01:04:03.600
also at the beginning of the podcast. Any particular ones that folks should be

01:04:04.640 --> 01:04:10.800
keeping an eye on I mean there are the you know deep mind open AI or you know traditional

01:04:10.800 --> 01:04:17.440
folks that have been publishing a lot in this space certainly yourself and Sergei Levine and Peter

01:04:17.440 --> 01:04:23.920
Biel and many others you know academic labs what who are some of the new ones that might be worth

01:04:23.920 --> 01:04:28.400
taking a look at? Yeah so in addition to the folks that you mentioned some of the kind of existing

01:04:28.400 --> 01:04:37.040
ones are like the group brain team in addition to to divine efforts and folk labs at McGill and

01:04:37.040 --> 01:04:44.640
Montreal like Joanna Precap and Joel Panou folks at Oxford like Ash Shaman Whiteson and then also

01:04:44.640 --> 01:04:50.560
folks at Michigan like Satinder Singh and Hungluck Lee and then also at Stanford is also Emma

01:04:50.560 --> 01:04:56.640
Brunskill and I think that these folks study different aspects of the problem for example Emma

01:04:56.640 --> 01:05:02.720
has been looking at applications like education and really kind of human impact settings and

01:05:02.720 --> 01:05:07.600
really actually focusing on like off policy methods for example while Satinder's group and

01:05:07.600 --> 01:05:13.040
Hungluck's group are often looking at more video game applications and Joel has done kind of at

01:05:13.040 --> 01:05:16.560
McGill has done a variety of different applications so I think that it's been interesting to see

01:05:17.120 --> 01:05:21.360
how different methods are using different approaches and then in terms of newer labs I think that

01:05:21.360 --> 01:05:28.240
it's hard to list them but I think that some some maybe people who are probably rather recognizable

01:05:28.240 --> 01:05:31.760
in the deep learning community and then are starting to do more reinforcement learning or

01:05:31.760 --> 01:05:37.280
folks like Gashua Bengeo and Jan Lecune, Jan it's more on the model-based reinforcement learning

01:05:37.280 --> 01:05:42.160
side of things and Gashua I think has been been looking at representation learning in the context

01:05:42.160 --> 01:05:48.720
of embodied agents and then also folks like Jimmy Baw for example has had some interesting work

01:05:48.720 --> 01:05:54.880
in in model-based reinforcement learning recently and then also Tany Maw at Stanford has

01:05:55.680 --> 01:06:00.720
Jimmy is at University of Toronto and then Tany Maw at Stanford has done a lot of really great work

01:06:00.720 --> 01:06:05.520
on theoretical deep learning and has been starting to move in towards towards the kind of look at

01:06:05.520 --> 01:06:11.520
the theoretical aspects of deep reinforcement learning. Well Chelsea that was a ton of stuff to

01:06:11.520 --> 01:06:17.760
cover thanks so much for you know doing this show for helping us get caught up on RL and giving

01:06:17.760 --> 01:06:22.320
us a peek into what's coming next. Yeah absolutely my pleasure.

01:06:26.240 --> 01:06:32.000
All right everyone that's our show for today for more information on today's guest or for links

01:06:32.000 --> 01:06:40.240
to any of the materials mentioned check out twimmelai.com slash rewind 19. Be sure to leave us a five-star

01:06:40.240 --> 01:06:45.200
rating and a glowing review after you hit that subscribe button on your favorite podcast catcher.

01:06:45.200 --> 01:07:12.160
Thanks so much for listening and catch you next time.

