WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.320
I'm your host Sam Charrington. For those challenged with promoting the use of machine learning

00:34.320 --> 00:40.760
in an organization and making it more accessible, a key to success is to support data scientists

00:40.760 --> 00:46.320
and machine learning engineers with modern processes, tools and platforms.

00:46.320 --> 00:51.480
This is a topic we're excited to address here on the podcast with the AI Platforms podcast

00:51.480 --> 00:56.080
series that you're currently listening to, as well as a series of e-books that will be

00:56.080 --> 00:58.080
publishing on the topic.

00:58.080 --> 01:02.760
The first of these e-books takes a bottoms up look at AI platforms and is focused on the

01:02.760 --> 01:08.120
open source Kubernetes project, which is used to deliver scalable machine learning infrastructure

01:08.120 --> 01:12.840
at places like Airbnb, booking.com and open AI.

01:12.840 --> 01:17.640
The second book in the series looks at scaling data science and ML engineering from the top

01:17.640 --> 01:23.000
down, exploring the internal platforms, companies like Airbnb, Facebook and Uber have

01:23.000 --> 01:26.600
built and what enterprises can learn from them.

01:26.600 --> 01:31.280
If these are topics that you're interested in and especially if part of your job involves

01:31.280 --> 01:36.880
making machine learning more accessible, I'd encourage you to visit Twimbleai.com slash

01:36.880 --> 01:45.320
AI platforms and sign up to be notified as soon as these books are published.

01:45.320 --> 01:51.360
In this episode of our AI platform series, we're joined by Daniel Jevins, General Manager

01:51.360 --> 01:54.080
of Data Science at Shell.

01:54.080 --> 01:58.400
In our conversation, Daniel and I explore the evolution of analytics and data science

01:58.400 --> 02:04.040
at Shell and cover a ton of interesting machine learning use cases that the company is pursuing,

02:04.040 --> 02:07.360
like well drilling and charging smart cars.

02:07.360 --> 02:11.840
A good bit of our conversation centers around IoT related applications and issues, such

02:11.840 --> 02:17.560
as inference at the edge, federated machine learning and digital twins, all key considerations

02:17.560 --> 02:20.280
for the way they apply machine learning.

02:20.280 --> 02:25.000
We also talk about the data science process at Shell and the importance of platform technologies

02:25.000 --> 02:28.280
to Daniel's organization and the company as a whole.

02:28.280 --> 02:32.120
And we discuss some of the technologies that he and his team are excited about introducing

02:32.120 --> 02:34.000
to the company.

02:34.000 --> 02:35.640
And now on to the show.

02:35.640 --> 02:44.360
All right, everyone, I am on the line with Daniel Jevins.

02:44.360 --> 02:48.280
Daniel is the General Manager for Data Science at Shell.

02:48.280 --> 02:51.680
Daniel, welcome to this weekend machine learning and AI.

02:51.680 --> 02:53.280
Thanks so much for having me.

02:53.280 --> 02:59.320
Before we jump in, I'd love to hear a little bit about your background and how you got

02:59.320 --> 03:02.840
to working in data science at Shell.

03:02.840 --> 03:08.000
I know previously you were the General Manager for Advanced Analytics at the company and

03:08.000 --> 03:10.640
you've worked in that capacity for a while.

03:10.640 --> 03:13.080
Tell us a little bit about your background.

03:13.080 --> 03:17.480
Yeah, so it's not a straightforward story.

03:17.480 --> 03:23.440
When I left university, I joined Accenture as many of us did in that era and spent a

03:23.440 --> 03:26.520
bunch of time developing big systems.

03:26.520 --> 03:31.400
And I guess the Eastward described what I was doing was data engineering without any of

03:31.400 --> 03:32.400
the tools.

03:32.400 --> 03:33.400
We used a lot of SQL.

03:33.400 --> 03:36.480
That sounds fun.

03:36.480 --> 03:38.000
It was tough back then.

03:38.000 --> 03:43.720
We hacked a lot of stuff together in Excel and made some crazy business insight for executives

03:43.720 --> 03:49.040
based on core business processes and taking raw data out of SAP.

03:49.040 --> 03:50.040
It was a fun time.

03:50.040 --> 03:54.600
We spent most of the evenings extracting data and then run it overnight and come back and

03:54.600 --> 03:57.320
hope the results worked in the morning.

03:57.320 --> 04:02.320
And that worked pretty well, but it was a fun start and I learned a lot from that.

04:02.320 --> 04:05.800
So I also got really interested in the interface between data and process.

04:05.800 --> 04:11.720
And I guess the easiest way to describe my career is that I've always been in that interface.

04:11.720 --> 04:16.920
And so then I then spent a lot of time learning about things like Six Sigma and Lean and trying

04:16.920 --> 04:21.360
to apply some of those techniques into the business processes that I was extracting

04:21.360 --> 04:26.320
data for and trying to improve the way that our business ran.

04:26.320 --> 04:31.840
And I went through a whole series of roles in that sort of area from data architecture

04:31.840 --> 04:36.680
to process architecture in the CI office in a more strategic role.

04:36.680 --> 04:42.960
And then eventually got to the point where I saw back in about 2012 that increasingly

04:42.960 --> 04:48.960
there are a whole series of new tools coming out that were starting to apply data science

04:48.960 --> 04:52.440
in really interesting ways to improve the way that the business ran.

04:52.440 --> 04:55.480
And what I saw was that this was going to be a big thing and it was going to change

04:55.480 --> 04:57.640
the way in which industry did business.

04:57.640 --> 05:02.640
And so I went to my manager who was at the time Shell's lead architect and I said to him,

05:02.640 --> 05:05.800
look, I see a big opportunity in this space.

05:05.800 --> 05:09.360
Would you let me go and start something?

05:09.360 --> 05:11.640
And he said, actually, we've just created a new role.

05:11.640 --> 05:15.080
The IT executive wants to do something as well.

05:15.080 --> 05:16.080
Why don't you apply?

05:16.080 --> 05:20.360
And so I applied for what we then called the predictive analytics center of excellence

05:20.360 --> 05:22.680
lead position.

05:22.680 --> 05:25.560
And I think they didn't really know what it was or what it was supposed to be.

05:25.560 --> 05:28.960
So they appointed me.

05:28.960 --> 05:33.080
And I sort of went back to my roots and I said, well, what do I need to make this successful?

05:33.080 --> 05:38.760
Well, I need some good data engineers and I need some people who understand stats.

05:38.760 --> 05:43.760
And so I hired a statistician and I went back to some of my old colleagues and brought

05:43.760 --> 05:46.640
in a data engineer and we got started.

05:46.640 --> 05:49.280
And we didn't really know what we were doing back then.

05:49.280 --> 05:51.040
We were sort of making it up as we were going along.

05:51.040 --> 05:54.800
We got a lot of input from others and talked to a lot of different companies.

05:54.800 --> 05:59.000
So what was really interesting was we figured out pretty early on that the way this works

05:59.000 --> 06:04.480
is you focus on the value and actually getting good at articulating where the value is from

06:04.480 --> 06:09.320
a business use case is really fundamental.

06:09.320 --> 06:14.280
And what we did was we effectively built out some cases around the customer space.

06:14.280 --> 06:20.480
We also focused on some work in our finance space and some work in the asset space.

06:20.480 --> 06:25.240
And all three of those paid pretty modest dividends pretty quickly.

06:25.240 --> 06:30.560
And that gave us a bit of a mandate and a bit of a momentum to start to grow.

06:30.560 --> 06:34.080
And I think maybe skipping forward a little bit, we went through a couple of years of sort

06:34.080 --> 06:35.160
of modest growth.

06:35.160 --> 06:40.840
But what were really changed was we hit on something that really resonated and we ran

06:40.840 --> 06:46.320
our first big project, which was focused on spare part inventory optimization.

06:46.320 --> 06:49.720
And we were able to deploy that quite quickly as a minimum viable product and it suddenly

06:49.720 --> 06:52.120
started to make millions of dollars.

06:52.120 --> 06:57.320
And it was at that point that the game shifted and suddenly this became a big thing and it

06:57.320 --> 07:01.520
became very much on the agenda of the executives and they got very interested in what we could

07:01.520 --> 07:03.320
do.

07:03.320 --> 07:06.440
And from there, really the team has been growing very, very rapidly.

07:06.440 --> 07:11.760
And I've been through various incarnations of positions, as you mentioned, to my current

07:11.760 --> 07:17.280
position where I'm now running about a team of about 130 people worldwide based in four

07:17.280 --> 07:22.240
global locations, combination of data scientists and data engineers.

07:22.240 --> 07:25.520
And we're running use cases right across the business.

07:25.520 --> 07:26.520
Interesting.

07:26.520 --> 07:29.040
I'm really looking forward to digging into some of those use cases.

07:29.040 --> 07:35.320
I didn't want to ask though, seeing this transition from advanced analytics, COE to data

07:35.320 --> 07:42.080
science, COE, tell me about the kind of background behind that change.

07:42.080 --> 07:50.160
Is it more significant than rebranding the role and the mission or does it have a significant

07:50.160 --> 07:52.200
meaning to the way you do business?

07:52.200 --> 07:57.880
Well, I think if you go back to the beginning, we had the idea really of putting statistical

07:57.880 --> 08:02.880
methods into software applications and deploying them to end users.

08:02.880 --> 08:06.400
That was if you like the initial mission that we came up with.

08:06.400 --> 08:11.440
But I think we always saw how this could evolve because even back in, we're talking back

08:11.440 --> 08:16.200
in 2013 now, we could see the early writing on the wall around the development of machine

08:16.200 --> 08:22.400
learning and also things like natural language processing and AI and the way in which this

08:22.400 --> 08:23.400
could evolve.

08:23.400 --> 08:27.800
And so we had that in mind, but we have focused very much on the simpler things and trying

08:27.800 --> 08:31.120
to deliver value with what we're building out then.

08:31.120 --> 08:35.000
I think it's been a pretty natural evolution, but of course, as you evolve, you need to

08:35.000 --> 08:37.080
describe what you do a bit better.

08:37.080 --> 08:41.040
And where it started off with, as I said, deploying those statistical methods, we're now

08:41.040 --> 08:43.520
deploying advanced machine learning at scale.

08:43.520 --> 08:45.840
And so we need a title that reflects that.

08:45.840 --> 08:50.000
So I think it's not that the mission or the vision has changed, but just we've managed

08:50.000 --> 08:52.920
to follow the roadmap and hence needed to rebound.

08:52.920 --> 08:56.360
One of the things that you mentioned in your background is this.

08:56.360 --> 09:00.280
Actually, there are a couple of things that I wanted to probe into.

09:00.280 --> 09:08.320
One is around this idea of kind of the interface between process and data and analytics and

09:08.320 --> 09:12.400
how you apply these techniques at that interface.

09:12.400 --> 09:22.640
But you also mentioned really needing to understand the value that these types of projects can

09:22.640 --> 09:25.080
bring to the table.

09:25.080 --> 09:30.560
Starting with that, what have you learned about kind of capturing the essence of the value

09:30.560 --> 09:33.840
proposition of these kinds of projects?

09:33.840 --> 09:36.000
So I think there's a couple of things I would say.

09:36.000 --> 09:37.920
So just linking to that process and data point.

09:37.920 --> 09:42.120
I find that people tend to look at the world through one or other lens.

09:42.120 --> 09:45.960
So business folks often tend to look at it through the lens of the process and forget

09:45.960 --> 09:47.800
about the data.

09:47.800 --> 09:52.240
And data practitioners often come at it from the data side and forget about the process.

09:52.240 --> 09:56.720
And the answer is you've got to understand both to really be successful.

09:56.720 --> 10:02.400
If I talk about that inventory use case, which I talked about as our big launching case,

10:02.400 --> 10:05.520
the key there was that interface of process and data.

10:05.520 --> 10:10.480
So what we understood was the data allowed us to develop certain recommendations around

10:10.480 --> 10:14.600
the stock levels that we were operating.

10:14.600 --> 10:17.960
And off the back of that, we could improve statistically the recommendations we were

10:17.960 --> 10:19.640
giving to the industry analyst.

10:19.640 --> 10:21.840
And that was relatively straightforward.

10:21.840 --> 10:28.640
But what we then had to do was work really closely with the experts in the business through,

10:28.640 --> 10:31.640
in this case, the materials management center of excellence internally.

10:31.640 --> 10:36.720
So the team that looked after materials management as a discipline and try and work with them

10:36.720 --> 10:39.400
to say, how are we going to fit a tool into your business process?

10:39.400 --> 10:43.760
And the reason it was successful was because we made a tool which made it really, really

10:43.760 --> 10:47.320
easy for the inventory analyst to do their job.

10:47.320 --> 10:49.480
And ultimately, that's why it was so successful.

10:49.480 --> 10:52.040
So I think that's one element of how you get the value.

10:52.040 --> 10:56.600
I think the other elements of getting to the value is understanding the friction.

10:56.600 --> 11:00.720
So again, if I use this as an example, what we understood was the inventory analyst could

11:00.720 --> 11:05.800
probably get to the same recommendations that the algorithm was making or close to, but

11:05.800 --> 11:07.920
it would take them several weeks.

11:07.920 --> 11:11.640
And that's not scalable, and it's not even doable because what that means is it doesn't

11:11.640 --> 11:13.120
get done.

11:13.120 --> 11:17.320
So what we quickly figured out was that actually understanding that friction point, which

11:17.320 --> 11:22.880
was this saves the inventory analyst time, was key to getting the value of which also leads

11:22.880 --> 11:27.440
to better decision making and off the back of that bottom line impact.

11:27.440 --> 11:30.720
So it's that friction understanding that's been really key to our journey.

11:30.720 --> 11:35.240
I think the other thing is, and maybe one of the other big learnings and learn from some

11:35.240 --> 11:41.840
of my failures here as well, often the business will ask questions which are thinly veiled.

11:41.840 --> 11:45.840
I have lots of data looking for a problem problems.

11:45.840 --> 11:51.920
And that's the worst type of definition because ultimately more data is not better.

11:51.920 --> 11:56.520
What's better is to have better insight, and that typically means small data.

11:56.520 --> 12:01.600
And so the challenge of any analytics project is taking big data into small insights, which

12:01.600 --> 12:04.200
allows someone to make a better decision.

12:04.200 --> 12:08.360
You mentioned that better insights typically mean small data.

12:08.360 --> 12:09.360
Can you elaborate on that?

12:09.360 --> 12:15.840
I think that is counterintuitive to the way we think about a lot of these problems nowadays.

12:15.840 --> 12:16.840
Yeah, I think so.

12:16.840 --> 12:20.560
I mean, it's not saying that you don't use a lot of data in coming to the insights,

12:20.560 --> 12:26.320
but ultimately, if you look at, let's take an operator in the North Sea, for example,

12:26.320 --> 12:33.200
we're currently asking them to deal with probably 10,000 data points on any given day.

12:33.200 --> 12:38.880
So they should be looking at the incoming variables from 10,000 different sensor feeds about

12:38.880 --> 12:40.480
their plans.

12:40.480 --> 12:42.920
No one can process that amount of data.

12:42.920 --> 12:47.560
And ultimately, you end up with just information overload, which means you shut down or you switch

12:47.560 --> 12:51.600
off or you manage on intuition or gut feel.

12:51.600 --> 12:56.600
And the challenge of data science is, can I take all of that incoming data that I have

12:56.600 --> 13:00.920
and turn it into the three things that that operator needs to look at and make sure that

13:00.920 --> 13:04.760
they really pay attention to those three things because that's what we're comfortable

13:04.760 --> 13:07.080
with dealing with as human beings.

13:07.080 --> 13:12.360
So a lot of my team and the thinking that we try to instill in people is, look, you've

13:12.360 --> 13:14.680
got to make it easy for your user.

13:14.680 --> 13:19.720
We like our iPhones because the app is typically only giving you one or two or three pieces of

13:19.720 --> 13:22.960
information, that's what a well designed app does.

13:22.960 --> 13:26.080
And you might deal with a lot of apps in a day, but actually each one is pretty targeted

13:26.080 --> 13:28.960
in the way that it surfaces information to you.

13:28.960 --> 13:32.560
And that's really the same sort of thinking we try and bring into the design of the things

13:32.560 --> 13:35.240
that we build for our business users.

13:35.240 --> 13:40.800
So I recently had an opportunity to hear one of your colleagues talk a little bit about

13:40.800 --> 13:47.560
the various use cases of machine learning at Shell at a conference.

13:47.560 --> 13:52.440
And I'd love for you to kind of run through that list.

13:52.440 --> 13:57.480
I mean, there were, you've got a ton of things going on in this space.

13:57.480 --> 13:59.000
Can you give us a taste?

13:59.000 --> 14:00.320
Yeah, of course.

14:00.320 --> 14:07.600
So we're looking at the application of machine learning into areas in the subsurface,

14:07.600 --> 14:13.160
for example, like well drilling where we're trying to automate the way in which we do

14:13.160 --> 14:18.920
geo-steering using machine learning, we're also doing work in production looking at predictive

14:18.920 --> 14:19.920
maintenance.

14:19.920 --> 14:23.320
So in other words, how can we predict failure on piece of equipment?

14:23.320 --> 14:27.680
That's everything from valves to compressors to heat exchangers.

14:27.680 --> 14:31.000
And going back to the example I gave on the operator, trying to give our operators really

14:31.000 --> 14:33.840
meaningful insight around the areas they need to focus on.

14:33.840 --> 14:36.520
We're also looking at optimization of those assets.

14:36.520 --> 14:41.680
So how can we use machine learning and other techniques to optimize the throughput of our

14:41.680 --> 14:45.600
manufacturing equipment, our production equipment?

14:45.600 --> 14:51.880
We're working with trading in a number of areas, trying to improve the way in which we manage

14:51.880 --> 14:57.640
our trading portfolios, the way we manage risk, the way in which we also make bets and take

14:57.640 --> 15:00.360
positions in the market.

15:00.360 --> 15:07.320
We are looking at optimizing our lubricants in the way that we blend our lubricants, effectively

15:07.320 --> 15:13.120
trying to leverage larger data sets to understand how we can make our products more effectively

15:13.120 --> 15:14.840
or more efficiently.

15:14.840 --> 15:20.600
We're working in retail, trying to develop new insights for our customers, but also trying

15:20.600 --> 15:23.920
to make our sites safer by using things like machine vision.

15:23.920 --> 15:29.520
And actually machine vision is an area that we're looking at far more broadly because leveraging

15:29.520 --> 15:34.680
video footage across our very physical value chain is a huge opportunity.

15:34.680 --> 15:39.680
And then we're also working in the new energy space where we're starting to try to develop

15:39.680 --> 15:48.040
algorithms to charge cars more smartly and also save our customers money in the process.

15:48.040 --> 15:53.080
So that's just a few examples that I could go on for a long time.

15:53.080 --> 16:01.000
How does your organization support such a broad portfolio of application areas?

16:01.000 --> 16:10.640
Are you kind of embedded into the different business units or business processes or do

16:10.640 --> 16:13.560
you operate in a more centralized way?

16:13.560 --> 16:15.720
So I think it's a combination.

16:15.720 --> 16:20.320
So the way I describe my team is we try and provide a technical backbone to these projects

16:20.320 --> 16:22.720
to make sure that we do them consistently.

16:22.720 --> 16:30.040
We operate with common standards and we deploy them to a high level of delivery.

16:30.040 --> 16:34.840
So we're trying to act as a true center of excellence if you will.

16:34.840 --> 16:38.600
We're often bringing in resources, I would say almost always bringing in resources from

16:38.600 --> 16:40.720
other parts of the organization.

16:40.720 --> 16:44.520
We partner very closely with our business service centers because they have some great

16:44.520 --> 16:46.360
data scientists there.

16:46.360 --> 16:51.560
We also partner closely with our IT colleagues because we need them to help us operationalize

16:51.560 --> 16:53.240
the things that we're building.

16:53.240 --> 16:57.440
And of course we want business people embedded in these projects because a lot of this

16:57.440 --> 17:01.400
doesn't work unless you're very close to the user and the person with the problem who

17:01.400 --> 17:04.000
really understands the case that you're trying to develop.

17:04.000 --> 17:09.440
So our most successful stories are those where all of those ingredients are there.

17:09.440 --> 17:13.760
So I think it's the best way to describe it is we're involved in a lot of these projects.

17:13.760 --> 17:19.920
We're providing technical assurance, expertise, we're often hands-on in terms of delivery,

17:19.920 --> 17:21.320
but we're not doing it on our own.

17:21.320 --> 17:26.000
We bring together cross disciplinary working teams to deploy the outcome.

17:26.000 --> 17:32.840
You mentioned operationalization as well as technical standards to what degree have you

17:32.840 --> 17:42.040
established a standard set of tools, practices or a concrete platform upon which you build

17:42.040 --> 17:43.680
these types of projects?

17:43.680 --> 17:45.400
That's a great question.

17:45.400 --> 17:47.880
So I think I'll answer that in two ways.

17:47.880 --> 17:51.040
We certainly have established tools and best practices.

17:51.040 --> 17:55.920
So I talk about my center of excellence having three core functions.

17:55.920 --> 17:58.680
Number one, we develop the underlying platform.

17:58.680 --> 18:00.920
We work with our IT colleagues to do that.

18:00.920 --> 18:07.640
The second thing is that we develop a series of use cases on top of that platform to demonstrate

18:07.640 --> 18:12.720
the value of it, but we also democratize that platform to allow others to use the platform

18:12.720 --> 18:13.720
as well.

18:13.720 --> 18:18.840
And we run a network to share best practice with about 2,000 people across Shell around

18:18.840 --> 18:19.840
how we do this.

18:19.840 --> 18:23.320
So that's the operating model, if you will.

18:23.320 --> 18:27.120
If you talk about the platform specifically, it's always evolving.

18:27.120 --> 18:34.240
So we built the first platform in about 2014, early 2015, I guess.

18:34.240 --> 18:38.120
And of course, the technology now has moved on significantly since then.

18:38.120 --> 18:42.600
And so we're in the process of working through some of that and refreshing some of our tools

18:42.600 --> 18:44.200
and our ways of working.

18:44.200 --> 18:47.120
But we've had a number of core components that have been very successful and will be with

18:47.120 --> 18:48.520
us for a long time to come.

18:48.520 --> 18:55.480
Things like Altrix and Databricks have been the standards as well as R and Python.

18:55.480 --> 18:58.840
And we've done a lot of work in those tools.

18:58.840 --> 19:03.120
Increasing though, we're also looking at others, things like C3 IoT.

19:03.120 --> 19:05.360
We do a lot of work with math works as well.

19:05.360 --> 19:09.880
So it's very much, we've brought together a best of breed toolset.

19:09.880 --> 19:15.200
And of course, we're really excited as you may have heard around Microsoft and working

19:15.200 --> 19:19.640
very closely with Microsoft now to move things into more of a paths set up in the public

19:19.640 --> 19:20.640
cloud.

19:20.640 --> 19:23.520
And so that's very much the direction that we're heading in.

19:23.520 --> 19:30.280
And a lot of the use cases that you support are, you know, what we've come to start calling

19:30.280 --> 19:32.280
edge applications.

19:32.280 --> 19:40.960
So can you talk a little bit about how the IoT and edge use cases impact the requirements

19:40.960 --> 19:43.920
that you have on the underlying platform?

19:43.920 --> 19:44.920
Yeah, of course.

19:44.920 --> 19:47.480
So I think a couple of things.

19:47.480 --> 19:52.600
I think in the strictest sense, Shell has been an IoT player for a very, very long time.

19:52.600 --> 19:57.720
And we've had a consolidated sensor infrastructure for many years.

19:57.720 --> 20:02.440
It's based on a technological OSI, so PIE and we effectively aggregate all of our sensor

20:02.440 --> 20:04.840
data into centralized repositories.

20:04.840 --> 20:08.320
And that's been a huge enabler for us because of course, that means you don't have to deploy

20:08.320 --> 20:11.480
all the sensors and you don't have to deal with some of the IoT problems.

20:11.480 --> 20:14.960
We just pick up the brilliant work that others have done to aggregate all that data for

20:14.960 --> 20:15.960
us.

20:15.960 --> 20:19.800
So that's been a big play and the challenge there has been making that available in the

20:19.800 --> 20:23.240
cloud and starting to leverage machine learning on top of that.

20:23.240 --> 20:26.200
And that's been a really exciting journey.

20:26.200 --> 20:32.000
But increasing needs as we deploy new solutions, we also need to deploy things to the edge.

20:32.000 --> 20:33.960
And in particular, deploying machine learning.

20:33.960 --> 20:38.960
So the best example is some of the work we've been doing in retail recently, where we're

20:38.960 --> 20:44.520
now deploying cameras into retail sites in Singapore and in Thailand.

20:44.520 --> 20:49.040
And into those cameras, we actually, because we've got six cameras generating about 200

20:49.040 --> 20:53.680
megabytes per second in data volumes, we need to be able to filter that before we pass

20:53.680 --> 20:57.040
it into the cloud, because otherwise it just becomes unmanageable.

20:57.040 --> 21:00.960
So what we're doing is we're using effectively edge deployment.

21:00.960 --> 21:07.040
And we're allowing the containerized cloud based environment to push machine learning

21:07.040 --> 21:12.560
into the edge to act as a filter to allow us to only retrieve the elements that we need

21:12.560 --> 21:16.920
into the cloud for type processing in a scaled environment like Spark.

21:16.920 --> 21:19.920
So that's pretty, that's pretty cutting edge and that's pretty exciting.

21:19.920 --> 21:23.080
And that's one of the areas that I'm really enjoying working in.

21:23.080 --> 21:26.280
The other example, of course, is some of the new devices that are coming online, so things

21:26.280 --> 21:27.280
like charge posts.

21:27.280 --> 21:32.200
So we're doing a lot of work around how can we deploy algorithms, which have full connectivity

21:32.200 --> 21:37.920
into charging posts, so we can optimize the way in which we provide electricity to electric

21:37.920 --> 21:39.400
vehicles.

21:39.400 --> 21:45.960
The retail application you mentioned was one that was highlighted at the Ignite conference.

21:45.960 --> 21:48.200
Can you talk about that in more detail?

21:48.200 --> 21:49.200
What are its goals?

21:49.200 --> 21:50.200
Of course.

21:50.200 --> 21:53.640
So I mean, our retail businesses is huge.

21:53.640 --> 21:57.440
We're the largest single branded retailer in the world.

21:57.440 --> 22:04.080
We operate in about 70 different markets, we have about 44,000 sites and we deal with

22:04.080 --> 22:07.080
about 30 million customers every day.

22:07.080 --> 22:08.800
That's an incredible scale.

22:08.800 --> 22:13.320
And of course, in that business, we want to make sure that our customers are safe and

22:13.320 --> 22:16.360
secure and that we protect them from risks.

22:16.360 --> 22:20.600
And as you will know, I mean, having hydrocarbons there has an elements of risk to it.

22:20.600 --> 22:26.040
And so we need to make sure that we're able to intervene in situations like smoking,

22:26.040 --> 22:29.160
which is remarkably still an occurrence on some of these sites.

22:29.160 --> 22:33.720
As well as looking at other risks like people speeding or going in the wrong direction,

22:33.720 --> 22:38.520
which may have an impact on pedestrians going into the store or situations where we may

22:38.520 --> 22:44.560
have robberies having a bid, we want to be able to make sure we can detect and ideally detect

22:44.560 --> 22:49.320
the symptoms of those things about to happen so we can make an intervention.

22:49.320 --> 22:53.600
And so what we've done is we've developed a machine learning system that's looking for

22:53.600 --> 22:58.640
some of those well-known symptoms that are about to cause issues.

22:58.640 --> 23:03.200
And then providing those as alerts back to the service champion.

23:03.200 --> 23:07.920
So that's the person who looks after the customer in the store.

23:07.920 --> 23:10.000
And we're currently in a pilot phase with that.

23:10.000 --> 23:14.640
We're testing it out in Singapore and in Thailand, as I mentioned.

23:14.640 --> 23:15.640
It's going pretty well.

23:15.640 --> 23:20.440
It's early days, but it's a really exciting area for us because there's so many use cases

23:20.440 --> 23:24.960
we can think of that can help improve our retail business, but way beyond that into our

23:24.960 --> 23:29.240
production business and into our refining business and elsewhere.

23:29.240 --> 23:33.840
Because at the end of the day, what we've built is a standard platform that runs in the

23:33.840 --> 23:40.600
public cloud that allows us to operate all of these cases at scale on a common infrastructure.

23:40.600 --> 23:47.080
And so this particular application, as you mentioned, is based on capturing video.

23:47.080 --> 23:54.120
You mentioned that the video that's being pushed to the centralized processing in the cloud

23:54.120 --> 23:56.440
is reduced bandwidth.

23:56.440 --> 24:02.440
Are you also doing inference at the edge or are you, tell us a little bit more about how

24:02.440 --> 24:04.240
that's what's happening there?

24:04.240 --> 24:05.240
Yeah, of course.

24:05.240 --> 24:08.400
So I mean, if you think about it, what we have is about six different cameras that are

24:08.400 --> 24:13.200
all pushing footage into an edge device.

24:13.200 --> 24:19.640
We've then deployed a thin machine learning model, so a thin, deep neural network, if

24:19.640 --> 24:25.480
you will, to that edge device based on TensorFlow, but it's kind of a yolo type model.

24:25.480 --> 24:30.440
What it's doing is fast and loose inference, pulling out frames of interest and passing

24:30.440 --> 24:35.640
those frames into the cloud, we're then loading them into a blob store environment, but

24:35.640 --> 24:39.240
then automatically bringing them up into the memory of the spark cluster and the spark

24:39.240 --> 24:45.840
cluster is then using effectively a Kafka stream, passing those through and identifying

24:45.840 --> 24:51.720
potentially interesting events that we want to notify the service champion about.

24:51.720 --> 24:58.040
Those events are then pushed into a database which provides an alert back then into the

24:58.040 --> 25:01.200
service champion in a dashboard which runs on an iPad.

25:01.200 --> 25:05.600
So that's a very brief overview of the architecture, but the core elements are inference at

25:05.600 --> 25:10.400
the edge and tighter inference in the cloud to reduce false positives.

25:10.400 --> 25:16.680
And then Kafka acting as the bus to pass those alerts through into the service champion

25:16.680 --> 25:18.520
in the form of a mobile device.

25:18.520 --> 25:26.920
And so the inference that's happening in the cloud is based on a more robust model presumably.

25:26.920 --> 25:31.120
Yeah, it's deeper, of course, because what we've got is a smaller data volume and you

25:31.120 --> 25:37.640
can do tighter inference, so you're using effectively deeper neural nets to make more

25:37.640 --> 25:40.880
robust recommendations back to the service champion.

25:40.880 --> 25:49.960
And are you in any way federating the training or the models, you know, pushing the knowledge

25:49.960 --> 25:55.880
in the centralized models out to the edge, because periodically or training them in concert

25:55.880 --> 25:58.680
with one another, are you doing anything in that space?

25:58.680 --> 26:04.440
So we're continually retraining based on new data and feedback from the service champion

26:04.440 --> 26:07.520
because both of those two things are important in refining the models.

26:07.520 --> 26:12.160
We're also working with offline training data to also try new things to try and improve

26:12.160 --> 26:17.040
the models as well and new ideas based on the new footage that we now have.

26:17.040 --> 26:20.920
I think the, I mean, I'm sure you'll hear this from a lot of people, but training machine

26:20.920 --> 26:23.640
learning is obviously it's a bit of trial and error.

26:23.640 --> 26:28.120
You've got to try a bunch of things and iterate quickly and that's very much the phase that

26:28.120 --> 26:31.040
we're in now to try and make our models more robust.

26:31.040 --> 26:34.840
The beauty is that we have these things fully containerized though, and as they improve,

26:34.840 --> 26:40.560
we can pass those down to the edge to improve the way in which they perform.

26:40.560 --> 26:45.080
On that note, how do you do experiment management?

26:45.080 --> 26:49.040
Well, it's a great question.

26:49.040 --> 26:50.440
It's a tough one.

26:50.440 --> 26:54.120
Maybe the best way to describe it is what we're trying to do across the department.

26:54.120 --> 26:58.640
I mean, of course, we have design of experiments training that we roll out for a lot of our

26:58.640 --> 27:00.720
data scientists.

27:00.720 --> 27:05.840
I think the other thing is that we have a set up around quality where we put in place

27:05.840 --> 27:13.200
a bunch of peer reviews to try to make sure that we're doing things in a sensible way across

27:13.200 --> 27:15.080
the team.

27:15.080 --> 27:20.520
I think though as well, and this is maybe a bit of a non-answer that the challenge with

27:20.520 --> 27:23.360
this is, some of this is quite new for us.

27:23.360 --> 27:28.560
And so in some ways, as I said, it's a bit of trial and error as to what works, particularly

27:28.560 --> 27:31.720
given the architecture that we're using is quite new.

27:31.720 --> 27:34.840
So we're trying to learn a lot from the way that others are handling this.

27:34.840 --> 27:38.920
We're learning a lot from other organizations who have perhaps ahead of us, some of the

27:38.920 --> 27:44.040
tech giants, for example, but I think it would be unfair to say we've got this right.

27:44.040 --> 27:46.240
We're still very much learning.

27:46.240 --> 27:53.920
One other thing I'm curious about is the degree to which you've instilled formalized processes

27:53.920 --> 28:03.040
around screening for a bias in data sets or results in, for example, in this application

28:03.040 --> 28:11.240
that's very much consumer facing and based on video, which is we've seen recent examples

28:11.240 --> 28:18.680
of bias creeping into the use of object detection, image detection, facial recognition, things

28:18.680 --> 28:20.160
like that.

28:20.160 --> 28:29.200
How mature would you say your data science practices are in terms of understanding and

28:29.200 --> 28:33.440
building processes in place to protect against that kind of bias, and where do you see that

28:33.440 --> 28:34.440
going?

28:34.440 --> 28:37.560
Yeah, it's a really good question.

28:37.560 --> 28:45.320
I'll answer that in two ways, I think we've merged in my group, the statistics group and

28:45.320 --> 28:50.680
the data science group, and I think that has really helped in the way we think about

28:50.680 --> 28:56.120
quality of models, because as you can probably appreciate, the statistics group have a much

28:56.120 --> 29:02.280
deeper understanding, if you will, of the potential for bias to cause issues.

29:02.280 --> 29:06.840
So we've had a stats group since the 1970s, it's a very mature group, and I think they've

29:06.840 --> 29:12.800
really helped in terms of bringing the challenge into some of the data scientists who are perhaps

29:12.800 --> 29:17.240
more focused on some of the more emerging technologies and have a deeper knowledge there.

29:17.240 --> 29:20.480
So I think it's that cross-pollination between different disciplines that's helping us

29:20.480 --> 29:22.960
in the way that we develop these models.

29:22.960 --> 29:26.480
And we're trying to encourage that across the team, and in particular, I mentioned the

29:26.480 --> 29:29.200
quality initiative that we're starting to roll out.

29:29.200 --> 29:33.720
We're trying to make sure that we have peer reviews to look at this and look at things

29:33.720 --> 29:37.000
like systematic bias in the way that we're developing our models.

29:37.000 --> 29:40.800
I think the other thing that's really important that we're talking a lot about at the moment

29:40.800 --> 29:45.960
is estimating uncertainty and distribution at the source of the data sets that are coming

29:45.960 --> 29:51.440
in, and really trying to understand that so you're not building in systematic bias through

29:51.440 --> 29:55.520
basing your models on a small set of observations.

29:55.520 --> 30:00.560
And I think in particular, one area that I think has a lot of promise here is using synthetic

30:00.560 --> 30:02.920
data to train the algorithms.

30:02.920 --> 30:06.760
We're not there yet with that, but I think that's an area that we're really interested

30:06.760 --> 30:10.840
in because we think it has strong potential to remove some of those biases.

30:10.840 --> 30:17.080
Is there a particular use case for which you see that as being most promising?

30:17.080 --> 30:21.080
Well, I think it's more of a... I mean, it's very early days with this, right?

30:21.080 --> 30:26.120
And we really don't know if this is going to work, by the way.

30:26.120 --> 30:32.440
Because the question is, can you accurately generate enough realistic data to be

30:32.440 --> 30:34.440
representative?

30:34.440 --> 30:39.960
I think what we're excited about is, if you look at the occasional event from a machine

30:39.960 --> 30:45.600
vision perspective, it's very hard to get a representative set of footage around that,

30:45.600 --> 30:46.600
right?

30:46.600 --> 30:52.840
So you end up very easily overfitting, whereas I think what's quite interesting is, if you

30:52.840 --> 31:01.440
think about synthetic data, you can potentially generate a vast quantity of data in a VR world

31:01.440 --> 31:07.120
that gives a real indication of potentially all the scenarios in which this could be

31:07.120 --> 31:11.520
observed and creates a much more representative data set to do a first-pass training on your

31:11.520 --> 31:12.520
model.

31:12.520 --> 31:16.000
Now, we still don't think that synthetic data will take us to real world in one step.

31:16.000 --> 31:19.760
It's going to be a two-step process, but if we can build that into the way we think

31:19.760 --> 31:22.440
about machine vision, it could be a real game changer.

31:22.440 --> 31:23.440
We think.

31:23.440 --> 31:25.440
But like I said, I can be wrong on this one.

31:25.440 --> 31:33.560
I think you're right, I agree. I wonder what the timeframe is for all of the pieces to

31:33.560 --> 31:34.560
come together.

31:34.560 --> 31:41.840
There's been some interesting results in combining synthetic data with real world data and

31:41.840 --> 31:47.120
applying techniques like domain transfer, but a lot of that stuff is really bleeding

31:47.120 --> 31:48.120
it.

31:48.120 --> 31:54.280
And I'd love to hear if there are any tools or papers or things like that that are top

31:54.280 --> 31:58.040
of mind for you in having looked at this.

31:58.040 --> 32:04.120
Yeah, I mean, we're like I said, we're really early stages, and we're kind of going

32:04.120 --> 32:10.240
in alone because we haven't found anything that's bang on point yet for not the use cases

32:10.240 --> 32:12.160
that we are interested in.

32:12.160 --> 32:15.120
And so I think this is actually an area where we think we might be able to publish some

32:15.120 --> 32:20.960
useful material that's going to drive some of the discussion in this space.

32:20.960 --> 32:22.960
So watch this space is what I would say.

32:22.960 --> 32:29.080
So we talked a little bit about the experiment management side of things.

32:29.080 --> 32:37.480
How do you approach model management and deploying models out to production and managing their

32:37.480 --> 32:40.040
performance over time?

32:40.040 --> 32:43.640
What kind of tooling have you built up around that and processes?

32:43.640 --> 32:46.280
That is a great question as well.

32:46.280 --> 32:48.440
So this is a real headache.

32:48.440 --> 32:51.280
So let me just be completely honest with you.

32:51.280 --> 32:56.560
Look, I can answer it with a simple answer, which is, you know, we containerize our models,

32:56.560 --> 33:01.480
we use things like Azure Container Services and Kubernetes to deploy them.

33:01.480 --> 33:04.400
We have, we embed metrics so we can manage it.

33:04.400 --> 33:09.560
We're doing some work looking at things like ML flow in Spark to help with some of this.

33:09.560 --> 33:13.880
However, I'll be completely honest and say, we're actually looking at, we're working

33:13.880 --> 33:16.240
with C3 IoT to try and crack this problem.

33:16.240 --> 33:22.560
And I'll explain in the scenario that we're working through, we have thousands of valves

33:22.560 --> 33:28.240
across our business, literally thousands of them, and we want to be able to run machine

33:28.240 --> 33:30.120
learning models for every single valve.

33:30.120 --> 33:31.800
Now every single valve is slightly different.

33:31.800 --> 33:32.800
The flow is different.

33:32.800 --> 33:34.840
The temperature, the pressure is different, right?

33:34.840 --> 33:38.640
So you basically need a model per valve, but you don't want to manually manage that because

33:38.640 --> 33:40.640
it's just unmanageable.

33:40.640 --> 33:48.560
So you need drift management, you need the ability to automatically transfer between different

33:48.560 --> 33:49.880
types of models.

33:49.880 --> 33:54.560
You need human override on those valves to actually replace the model with one that you think

33:54.560 --> 33:56.600
is going to perform better.

33:56.600 --> 34:01.240
You need to be able to retrain those models on a regular basis based on new incoming data

34:01.240 --> 34:02.240
sets.

34:02.240 --> 34:07.040
And you need methods to actually manage the overall performance of the system in a simple

34:07.040 --> 34:11.920
way that gives an end user the ability to look across the entire set of valves.

34:11.920 --> 34:16.160
So if you're giving an idea on a refinery, you might have 10,000 of these things.

34:16.160 --> 34:22.000
So that is a real world problem, it's a great real world problem, it's a really exciting

34:22.000 --> 34:23.000
one.

34:23.000 --> 34:24.080
But it's also a real challenge.

34:24.080 --> 34:31.000
And so like I said, we've gone and we've worked with C3 IoT to look at, can we leverage

34:31.000 --> 34:37.000
their platform and their type system to help us to manage that level of machine learning

34:37.000 --> 34:42.440
deployed into an asset where we have to be able to support a user interacting with that

34:42.440 --> 34:43.440
every day?

34:43.440 --> 34:44.440
Right.

34:44.440 --> 34:53.280
Adding the edge components to the model management challenge makes it a lot more complex.

34:53.280 --> 34:57.480
And you're relative to serving up models to some centralized website or via centralized

34:57.480 --> 34:58.480
website.

34:58.480 --> 35:01.960
I can definitely see where the complexity comes in there.

35:01.960 --> 35:09.160
We mentioned the valves and building individual models for these valves.

35:09.160 --> 35:13.800
I'm assuming that's kind of this digital twin type of use case.

35:13.800 --> 35:16.520
How do you end up using those digital twins?

35:16.520 --> 35:18.000
And do you like that terminology?

35:18.000 --> 35:21.520
Do you use that internally or not really?

35:21.520 --> 35:22.600
Yeah, we do.

35:22.600 --> 35:26.600
I think the problem with the terminology is that everyone means something different by

35:26.600 --> 35:27.600
it.

35:27.600 --> 35:34.600
And it's everything from a simulated set of the physics of the operations of the system

35:34.600 --> 35:41.240
through to a 3D model based on CAD drawings and all the variations in between.

35:41.240 --> 35:42.960
So you have to be a bit careful with it.

35:42.960 --> 35:47.040
And we're trying to standardize on a definition internally so that we can say this is what

35:47.040 --> 35:48.040
we mean.

35:48.040 --> 35:49.960
And then it has these attributes.

35:49.960 --> 35:53.200
That's the current discussion that we're going through.

35:53.200 --> 35:56.160
So interested if anyone else has had a similar problem.

35:56.160 --> 36:00.800
But what we're trying to do really here is it is kind of a digital twin setup.

36:00.800 --> 36:05.120
We're taking a hierarchy of the equipment that we have on the site and then we're effectively

36:05.120 --> 36:09.760
tagging the things we want to monitor with a model against that hierarchy.

36:09.760 --> 36:15.200
And then we're able to manage that basis the way in which the asset is set up.

36:15.200 --> 36:18.320
But it tends to be specific pieces of equipment.

36:18.320 --> 36:22.280
And of course, within the context of that, we're taking the historic data feeds about that

36:22.280 --> 36:23.800
piece of the asset.

36:23.800 --> 36:28.960
And then we're running an algorithm, typically a machine learning model, not always a deep

36:28.960 --> 36:34.440
learning model, by the way, against that particular piece of equipment in order to provide

36:34.440 --> 36:36.400
results back to the user.

36:36.400 --> 36:43.920
Do you ultimately end up with for a complex piece of equipment as you're trying to make

36:43.920 --> 36:48.640
predictions against the performance of that complex piece of equipment and ensemble

36:48.640 --> 36:55.800
of thousands or tens of thousands of submodels representing the individual parts?

36:55.800 --> 37:01.200
And have you built a framework that allows you to do that kind of thing repeatedly or

37:01.200 --> 37:02.600
are we not there yet?

37:02.600 --> 37:05.960
Well, so you know what's really interesting is actually we're going the other way right

37:05.960 --> 37:06.960
now.

37:06.960 --> 37:07.960
Really?

37:07.960 --> 37:14.440
So one of the things that we found is the difference in performance between the submodels that

37:14.440 --> 37:20.160
we're building out in the complex system versus basically having a master model that cuts

37:20.160 --> 37:23.360
across all the pieces of equipment is not differentiating.

37:23.360 --> 37:28.760
And of course, having the master model is much, much easier.

37:28.760 --> 37:32.920
We're currently playing around with that, but we actually think that more and more will

37:32.920 --> 37:35.640
be moving towards master models for certain things.

37:35.640 --> 37:40.400
But to be honest, it depends because it's not a one-size-fits-all for certain pieces

37:40.400 --> 37:45.920
of equipment we found having one model monitoring a whole sort of train, if you will, works

37:45.920 --> 37:46.920
really well.

37:46.920 --> 37:50.120
And for other things, you've really got a monitor that piece of equipment using a very

37:50.120 --> 37:51.120
specific model.

37:51.120 --> 37:56.440
So I think I would say we're learning all the time with this because we're sort of looking

37:56.440 --> 37:57.440
at it.

37:57.440 --> 37:58.440
We're learning as we go.

37:58.440 --> 38:02.200
And it's something we believe quite passionately in, right, which is you go through a process

38:02.200 --> 38:05.680
and you iterate on that and you don't overthink it.

38:05.680 --> 38:09.400
And you may try three, four different approaches, even within the same team.

38:09.400 --> 38:13.400
And that's OK, because I don't think anyone's done this, not at the scale that we're trying

38:13.400 --> 38:14.400
to do it now.

38:14.400 --> 38:18.760
So it was actually featured pretty prominently at this conference we keep referring to, the

38:18.760 --> 38:23.400
Microsoft Ignite conference in the Satya's key notes.

38:23.400 --> 38:30.840
And one of the examples was the use of not just the work with C3 IoT, but their recent

38:30.840 --> 38:36.080
acquisition of bonsai and some of the work you're doing around reinforcement learning

38:36.080 --> 38:40.760
and the application of that, that's also, you know, like all of the things we've talked

38:40.760 --> 38:47.720
about very new, but can you talk a little bit about that work and what you've seen there?

38:47.720 --> 38:49.520
Well, yeah, absolutely.

38:49.520 --> 38:54.720
I mean, what this came out of was basically the idea of the self-driving car.

38:54.720 --> 38:58.320
And the idea was basically, well, if they're making self-driving cars, why can't we have

38:58.320 --> 39:01.680
self-driving wells, basically?

39:01.680 --> 39:05.200
Because if you think about it, in some ways it's a simpler problem to solve.

39:05.200 --> 39:09.640
There's less dimensionality to it, there's less uncertainty to it in some ways, there's

39:09.640 --> 39:12.920
less unexpected events that you still have them.

39:12.920 --> 39:18.000
And so effectively, what we've been trying to do is develop based on a three-dimensional

39:18.000 --> 39:24.840
model of the subsurface, a set of automated geosteering algorithms that allow us to move

39:24.840 --> 39:29.000
the drill bit through the subsurface in line with the well plan, and that's the simplest

39:29.000 --> 39:30.000
way to describe it.

39:30.000 --> 39:34.000
Now, if you think about that, it's actually a very, very suitable problem for reinforcement

39:34.000 --> 39:39.400
learning, because it's got a very simple penalty function, right?

39:39.400 --> 39:44.360
Out of well plan, you assign a penalty and off the back of that, you can very, very quickly

39:44.360 --> 39:49.680
start to bring the well back into line as you train the algorithm over time.

39:49.680 --> 39:56.760
And what we found was that the bonsai solution effectively gave us the opportunity to do

39:56.760 --> 40:00.440
that very easily at scale.

40:00.440 --> 40:04.080
And it's one of those things, you can do this yourself, but actually, if someone's built

40:04.080 --> 40:08.880
a framework for doing this at scale, it's going to be much easier for us to do that as

40:08.880 --> 40:12.360
we go forward, rather than having to maintain our own software.

40:12.360 --> 40:15.280
And that was something that was very appealing about bonsai, that's why we started working

40:15.280 --> 40:16.440
with them.

40:16.440 --> 40:21.760
And so we're now looking at them to help us to scale this as we look at applying it into

40:21.760 --> 40:24.080
different scenarios across our business.

40:24.080 --> 40:32.000
So in formulating this self-driving well problem, what are the different control variables?

40:32.000 --> 40:34.960
I imagine there are tons, but what are the kinds of things that we're talking about,

40:34.960 --> 40:40.000
like rotational speed of the drill bits, and that kind of thing?

40:40.000 --> 40:46.360
Yeah, exactly, and sort of the geospatial positioning, the azimuth, et cetera, right?

40:46.360 --> 40:51.880
You've got a whole series of different measurements that you're taking all the time, and the best

40:51.880 --> 40:59.280
way to describe it, is it's like driving forward, but you're only getting the data in retrospect.

40:59.280 --> 41:03.280
And that's the challenge with it, because what happens, of course, is the data comes

41:03.280 --> 41:07.800
off the bit, and you get it kind of a foot behind where you are if you see what I'm saying.

41:07.800 --> 41:12.920
And so that's quite an interesting dilemma in the whole process.

41:12.920 --> 41:17.840
But certainly, it's looking at a multi-dimensional, three-dimensional problem set based on your

41:17.840 --> 41:22.280
understanding of the subsurface continually, and constantly iterating and adjusting, as

41:22.280 --> 41:29.280
well as trying to really look at the optimum way of doing it, because it's not always

41:29.280 --> 41:35.360
intuitive, because obviously you'll go, this is directional drilling, it's not only sort

41:35.360 --> 41:37.080
of linear drilling.

41:37.080 --> 41:42.960
So your speed also changes depending on the way in which you're drilling, and obviously

41:42.960 --> 41:47.360
that has knock-on effect from an optimization perspective.

41:47.360 --> 41:51.960
And so with reinforcement learning in general, bonds are in particular, the notion of

41:51.960 --> 41:56.000
using simulation is key.

41:56.000 --> 42:02.240
Did you already have a simulation of this in place, and were you able to easily use that

42:02.240 --> 42:03.960
with reinforcement learning?

42:03.960 --> 42:10.240
So I think the beauty of this is we have a lot of internal simulations set up for this

42:10.240 --> 42:15.080
of course, because more or less everything that we do here has been looked at and has

42:15.080 --> 42:18.080
been simulated in the past for optimization purposes.

42:18.080 --> 42:21.920
So if you take something like rate of penetration, we have endless simulations of the way in

42:21.920 --> 42:23.960
which that can be optimized.

42:23.960 --> 42:28.280
And so we can bring some of those things into something like bonsai by having that very

42:28.280 --> 42:32.280
good understanding of the first principles, simulators that we've developed over many

42:32.280 --> 42:36.200
years, as well as of course, simulators of the subsurface, which is another element

42:36.200 --> 42:37.640
of the simulation.

42:37.640 --> 42:41.720
So I think that's been the real benefit, and it's the bringing together our existing

42:41.720 --> 42:46.800
scientific knowledge with the new technology of reinforcement learning that's really driving

42:46.800 --> 42:48.800
this change.

42:48.800 --> 42:53.200
Well, Dan, we covered a ton of ground here.

42:53.200 --> 42:59.040
I appreciate you taking the time to chat with us any kind of final thoughts or words of

42:59.040 --> 43:05.520
wisdom to folks that are maybe at an enterprise that doesn't quite have as much experience

43:05.520 --> 43:12.160
with this intersection of data and process as you and Shell.

43:12.160 --> 43:14.120
What should they be thinking about?

43:14.120 --> 43:17.920
I think it always comes back to really understanding your business problem.

43:17.920 --> 43:21.200
I mean, I know I talked about that earlier on in the call, but the thing I'd encourage

43:21.200 --> 43:24.280
you is that I talked about a lot of sophisticated techniques.

43:24.280 --> 43:28.200
I talked about machine vision, I talked about reinforcement learning, a lot of the biggest

43:28.200 --> 43:31.720
value things we've done are actually the simplest things.

43:31.720 --> 43:36.480
And so if you're feeling like you're just getting started with this, and I know a number

43:36.480 --> 43:41.520
of companies are actually the biggest thing is understanding where the big value is for

43:41.520 --> 43:45.320
your business, and trying to solve the problem in the simplest way you possibly can.

43:45.320 --> 43:50.160
I've got a lot of time for deep science, I absolutely love it, as you can probably tell,

43:50.160 --> 43:54.200
I love some of the things we're doing, but I do think there's so much low hanging

43:54.200 --> 43:56.400
fruit in this space that getting started.

43:56.400 --> 44:00.680
And also, I would say there's some great tools out there that make it very accessible

44:00.680 --> 44:02.440
to get stuck in very, very quickly.

44:02.440 --> 44:06.480
So we've had a great relationship with Ultrix over many years, and we've had huge value

44:06.480 --> 44:10.920
from them, just because it makes it so easy for citizen data scientists in the business

44:10.920 --> 44:13.280
to get stuck in and start to add value.

44:13.280 --> 44:17.480
And I think a lot of this is about a cultural change, trying to drive cultural change across

44:17.480 --> 44:22.640
an organization where data and data science is central to the way that we do business.

44:22.640 --> 44:27.160
That's something which is exciting, it's essential for any company I believe, but it's

44:27.160 --> 44:31.480
also more and more accessible with the latest advances in technology.

44:31.480 --> 44:37.480
Yeah, we didn't get into the cultural side of this, but your colleague Yuri spent a

44:37.480 --> 44:44.200
lot of time talking about that in a session that I had with him at Ignite, clearly something

44:44.200 --> 44:50.280
that's very central to the way you think about scaling data science and machine learning

44:50.280 --> 44:51.280
at Shell.

44:51.280 --> 44:52.280
Yeah, it's massive.

44:52.280 --> 44:57.560
I mean, I think the biggest problem of this is adoption and belief and adoption in the

44:57.560 --> 45:03.240
sense that how do you persuade people to use the output of a data science project?

45:03.240 --> 45:06.720
And the second is do you believe that this is going to improve the way in which you do

45:06.720 --> 45:09.320
your work and make your life easier?

45:09.320 --> 45:14.680
And I think it also comes down to, we've got to learn as an organization to work in a way

45:14.680 --> 45:17.480
that's for more familiar to software houses.

45:17.480 --> 45:23.000
So things like developing minimum viable products, having strong product ownership, iterating

45:23.000 --> 45:29.360
quickly, failing fast, being able to pivot, having the setup in which you are willing

45:29.360 --> 45:33.040
to work with a minimum viable product, not tell the people that are developed it that

45:33.040 --> 45:38.560
is rubbish, those sorts of things are not commonplace and not comfortable for an organization

45:38.560 --> 45:39.560
like ours.

45:39.560 --> 45:43.520
And so it's as much a challenge of building and understanding on both sides, building

45:43.520 --> 45:47.160
and understanding on the business side of the way in which we need to work and building

45:47.160 --> 45:51.040
and understanding from the way that we're working with the challenges that the business

45:51.040 --> 45:54.320
people have every day and bringing those two worlds more closely together.

45:54.320 --> 45:58.840
And it's an exciting challenge, but it's one that's going to take time and it's core

45:58.840 --> 46:01.680
to how we're trying to get the value out of what we're doing.

46:01.680 --> 46:02.680
Fantastic.

46:02.680 --> 46:04.440
Well, Daniel, thank you so much.

46:04.440 --> 46:05.440
Thank you.

46:05.440 --> 46:12.120
All right, everyone, that's our show for today.

46:12.120 --> 46:17.040
For more information on Daniel or any of the topics covered in today's show, visit

46:17.040 --> 46:22.040
twimmelai.com slash talk slash 202.

46:22.040 --> 46:28.360
To learn more about our AI platform series or to download our eBooks, visit twimmelai.com

46:28.360 --> 46:31.240
slash AI platforms.

46:31.240 --> 46:49.440
As always, thanks so much for listening and catch you next time.

