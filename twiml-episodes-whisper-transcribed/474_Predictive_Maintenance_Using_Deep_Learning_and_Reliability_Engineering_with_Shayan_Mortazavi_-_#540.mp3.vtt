WEBVTT

00:00.000 --> 00:20.000
All right, everyone. I am here with Shion More Tazavi. Shion is a data science manager at Accenture, and Shion, I'm excited to welcome you onto the podcast.

00:20.000 --> 00:29.000
Oh, wow. It's pleasure to be here. I'm really thrilled to talk to you. And yeah, looking forward to it.

00:29.000 --> 00:41.000
Let's do it. So you recently presented at Sigops AI and HPC Summit, and we're going to dig into your presentation and some of the work that you're doing there at Accenture.

00:41.000 --> 00:47.000
But to get us started, why don't you share a little bit about your background and how you came to work in data science?

00:47.000 --> 01:15.000
Sure. Well, I'm an engineer by background, and I started as a mechanical engineer in energy domain, building structures and pressurized systems in energy sector oil and gas industry, and then I moved slightly into simulation of the structures and components that are used mostly in the heavy industry domain.

01:15.000 --> 01:43.000
And then I got interested into material science and worked a little bit conjunction between the simulation and the software is built for analyzing basically structures in that domain and material in terms of the characteristics, the properties, and then through that, I do work a lot with the data and doing a lot of tests.

01:43.000 --> 01:55.000
And these structures and the material properties and making sure that there are fit for their service and integrity.

01:55.000 --> 02:16.000
So dealing with a lot of data kind of got me to think about are there other applications for these data that we receive, which in the engineering domain, it's usually people working with this formula and this scary calculation,

02:16.000 --> 02:38.000
chips and software and simulations. So yeah, and then I got excited about using data to solve some of the difficult problems that exist in engineering domain, slowly slowly moved into machine learning and statistical learning, and then yeah.

02:38.000 --> 02:53.000
Nice, nice. Well, tell us a little bit about your group at Accenture and your role there.

02:53.000 --> 03:18.000
Which is primarily focusing on application machine learning and statistical inference in the heavy industry and resources, what we call kind of covering mining sector, oil and gas, renewables, every, every sort of manufacturing base that have some assets heavy, heavy assets.

03:18.000 --> 03:46.000
And yeah, in there, we kind of look into multiple different type of problems from production efficiency, supply chain, predictive maintenance and optimization of the production plants and, yeah, these sort of things.

03:46.000 --> 03:56.000
Got it. And predictive maintenance, which you mentioned was the topic that you focused on for your talk at the Sega summit.

03:56.000 --> 04:07.000
Can you give us a little background on predictive maintenance and in particular, some of the use cases that you've worked on.

04:07.000 --> 04:32.000
Well, that's an interesting topic because especially in oil and gas and energy sector, because they're heavily regulated and are, you know, decades of applying best practices into the operations and the maintenance systems.

04:32.000 --> 04:46.000
It is kind of, there is a inertia and changing or shifting the two towards more advanced sort of solutions.

04:46.000 --> 05:10.000
So maintenance itself as a problem is trying to maximize or increase the reliability of assets. And this can be cars could be trucks could be stationary static sort of equipment tanks and towers and drums could be some rotating machines.

05:10.000 --> 05:20.000
So, if the objective is to maximize the reliability, increase their availability and efficiency.

05:20.000 --> 05:45.000
There has been traditionally kind of a myriad of strategies that asset operators depends on complexity of assets and they have applied traditional practices like fixed space maintenance where you let the machine run to failure and then you fix when it is broken.

05:45.000 --> 06:14.000
There are costly and you don't have any view what is the state of the machine and then there are some kind of more clever, more systematic sort of approaches and maintenance that are called time based maintenance or scheduled maintenance where you define a sort of strategy of hope to regularly.

06:14.000 --> 06:19.000
It's kind of getting it set for the meantime, the failure of different components and using that to build up to.

06:19.000 --> 06:20.000
Exactly.

06:20.000 --> 06:21.000
One thing might break and.

06:21.000 --> 06:22.000
Absolutely.

06:22.000 --> 06:24.000
Yeah, these sort of things.

06:24.000 --> 06:51.000
So, in the past, let's say three, four decades, when IOT and the SCADA systems came into heavy industry domain, a branch of, let's say, preventative maintenance emerged called condition based monitoring where you the objective is to monitor the health of the equipment at any level.

06:51.000 --> 07:10.000
And using the sensor sensor data to receive and then use some early warning sort of functionality on top of that to detect any problems at system level or different hierarchy and then act on top of that.

07:10.000 --> 07:24.000
And the maintenance under the condition based monitoring, these CBMs are kind of resolved a lot of the traditional issues in the in this domain.

07:24.000 --> 07:39.000
And predictive maintenance is a child of the CBM systems like the condition based monitoring, which simply the objectives can be used a fully data driven system to.

07:39.000 --> 07:45.000
And predict when assets are going to fail in future.

07:45.000 --> 07:53.000
And based on that, optimize their maintenance actions and optimize over the resources and costs.

07:53.000 --> 07:54.000
So, yeah.

07:54.000 --> 07:58.000
And so your presentation.

07:58.000 --> 08:08.000
The long title, a novel framework for predictive maintenance using deep learning and reliability engineering.

08:08.000 --> 08:14.000
So the kind of the broad context or problem that you're working on is predictive maintenance.

08:14.000 --> 08:21.000
What was the specific use case that gave rise to some of the work that you talked about.

08:21.000 --> 08:49.000
So, in this work, we try to not just fully rely on a data driven machine learning sort of approach, but see how we can combine and marry the domain knowledge exist for decades and been built up with the sort of capabilities we get from machine learning.

08:49.000 --> 09:17.000
To address this very specific problem of can we detect failures at different levels of these machineries in advance and provide some insights that can be meaningfully taken as maintenance actions in order to move away from like reactive sort of maintenance or.

09:17.000 --> 09:27.000
And also to let them to judge whether they can delay the maintenance actions or interventions or shutdown of their plans.

09:27.000 --> 09:45.000
So this framework was specifically trying to break down the systems, these complex systems into multiple different levels in terms of the complexity and functionality in the system.

09:45.000 --> 10:14.000
And then use machine learning to kind of monitor the performance of these little building blocks and then by by the means of aggregation through the science of reliability engineering then make sense of what is the global picture.

10:14.000 --> 10:33.000
This framework, specifically, yes, was applied into rotating machineries, which we use everywhere in the petrochemical plants and power plants and refineries pharmaceutical industry everywhere.

10:33.000 --> 10:45.000
It likes generators, turbo generators, pumps, compressors and turbines, this type of thing, exactly.

10:45.000 --> 11:13.000
So these are complex, these machineries are, you can look at them like the turbo generators is jet engines, they have a very high RPM, rotating parts like 20,000 to 30,000 sometimes RPM, like exactly like a jet engine, lots of moving parts, high vibration on the significant loads, temperature, pressure and yeah, heavily censored.

11:13.000 --> 11:37.000
I don't remember the specific stat, but, you know, a few years ago when in the early days of IOT, like there was this Boeing jet engine or GE aircraft engine example that we would throw around all the time and just how many sensors were in this engine and it was an astounding number.

11:37.000 --> 11:42.000
And so this is all time series data that you're collecting.

11:42.000 --> 12:02.000
So this is an interesting point because these these machines are, let's say in the energy sector, there are heavily censored up to 2,000, 3,000 sometimes, number of sensors monitoring,

12:02.000 --> 12:27.000
pressure, temperature, vibration, flow rate, some process parameters, but the purpose of these sensors are not necessarily to provide maintenance inputs into maintenance engineers that are there to, for safety purposes,

12:27.000 --> 12:32.000
or to regulate the process.

12:32.000 --> 12:56.000
So, in order to address a problem of maintenance, we don't necessarily have the adequate type, location of the sensors in terms of the type of time series that we want, or the parameters we want to monitor.

12:56.000 --> 13:16.000
Also, the redundancy in the type of sensors that we want to have the monitor specific modes of failures, for example, and also there is no scope for retrofitting type of sensors onto these complex machines.

13:16.000 --> 13:33.000
So, yeah, we receive a lot of time series data and we have to apply a predictive solution, which is a kind of new generation approach on top of an old fashioned censoring sort of practices.

13:33.000 --> 13:36.000
And that makes it a little bit interesting.

13:36.000 --> 13:52.000
And it's really interesting that you put it like that, the idea that the sensors that are in these machines aren't necessarily there for you to make predictions from that that

13:52.000 --> 14:09.000
that was always left out of this narrative of like IoT and predictive maintenance and digital twins that I've heard over and over and over again, but what you're describing is like you've got all these data points that are being spun out of these machines.

14:09.000 --> 14:25.000
We've got a bunch of, you know, many, many years of equations to predict the reliability of the machines and when you might need to repair them, the whole field of reliability engineering that's evolved.

14:25.000 --> 14:48.000
But there's a gap, it's like the bridge was built from two shores and doesn't meet in the middle and it sounds like that's what the core of your presentation was you're using predictive models to map from the data you're collecting off of these sensors to the equations that you would traditionally use.

14:48.000 --> 15:12.000
Precisely, that's a very good summary of, yeah, and of the problem itself, yeah, maybe going forward into future where operators and the manufacturers of these machines realize the necessity of censoring the machines adequately for the specific purpose of predictive maintenance.

15:12.000 --> 15:25.000
Which I think it's a trend in advanced sort of manufacturing fields in robotics and the machines which are employing a lot of robotics in them.

15:25.000 --> 15:30.000
There are some sort of monitor the health of the machines and parts of them.

15:30.000 --> 15:53.000
But in this specific case, exactly to bridge this gap, we wanted to really connect the failure mechanics, the knowledge within the failure mechanics with predictions from machine learning.

15:53.000 --> 16:09.000
And what do we mean by the failure mechanics, so there is a type of analysis in the domain of engineering of machineries and a structure design, which is called FNA failure mode and effect analysis.

16:09.000 --> 16:35.000
So what is that? It is basically a procedure to utilize the research and the standard exists to analyze the modes of failures of the machine, break down the modes of failures and the effect of each failure mode onto these specific type of equipment,

16:35.000 --> 16:51.000
components from bottom up, and then identify the indicators that can kind of provide a bit of symptom of these type of failure modes and then act on top of that.

16:51.000 --> 17:02.000
So this is a very rigid structural engineering practice that is performed in the beginning of the design or operation and then gets updated.

17:02.000 --> 17:25.000
So we were thinking kind of the novelty of the thinking that maybe we try to deploy was can we use this failure mode and effect analysis and

17:25.000 --> 17:43.000
build a sort of matrix that from all sort of information you get at component level, we provide a kind of more insightful picture of what can go wrong.

17:43.000 --> 18:10.000
This is building blocks of the machines and then again by the use of the reliability engineering concepts, how we can combine these information at these building block levels and make sense from maintenance engineering point of view, what is going on at higher hierarchies like assembly levels or at system level.

18:10.000 --> 18:16.000
So yeah, this was the perhaps the core novelty of the approach.

18:16.000 --> 18:30.000
And so the matrix that you described is that part of the traditional way of solving like is that the input to the traditional reliability engineering approach or is that

18:30.000 --> 18:42.000
an artifact of the solution that you've applied using deep learning new this was let's say one of the pillars of this of this approach.

18:42.000 --> 19:04.000
This didn't exist so the FMEA does exist that's failure mode analysis that kind of cause and effect relationship mapping which you may in like the science domain use different approaches like Bayesian approaches to map or build the causal mapping of the system.

19:04.000 --> 19:24.000
But in our case we adopted the cause and effect maps from engineering domain and we built something called false matrix and this false matrix is relational matrix map between the failure modes and really the sensors.

19:24.000 --> 19:42.000
So in this matrix what you have is a map of what are the importance of each sensor in terms of redundancy and the influence in ability to detect some of the failure mode at component level.

19:42.000 --> 19:58.000
So this can be very static matrix in terms of the importance relationship between the sensors and failure mode or it could be learn matrix or it could be probabilistic.

19:58.000 --> 20:14.000
Let's say importance factor in terms of sensor to failure mode could be kind of incorporating uncertainty through the means of probably probability density functions.

20:14.000 --> 20:30.000
And so going back to the long title that I mentioned where does deep learning come in one of the first steps of kind of detecting the problem.

20:30.000 --> 20:58.000
Is it still an open question for us how we can detect accurately the problem and then link it to you know an understandable mode for engineers but to detect it I think traditionally at least in the field of condition based monitoring.

20:58.000 --> 21:20.000
I have been applying some fixed thresholds onto sensors and then by this means of like operating bands they were able to detect animals behavior and then based on some rules acts upon those animals sequences or so on so far.

21:20.000 --> 21:49.000
In our case through some literature review we wanted to see can we predict the behavior of these machines in their all possible states so these machines as you know they are operated in multiple different modes in terms of operation depends on how much load they want to apply to the machine.

21:49.000 --> 22:05.000
And also because they are in operation for 30 years many several hundred thousands of hours in operation they see multiple different changes in the state of operation in the entire assets.

22:05.000 --> 22:27.000
For example oil and gas industry the reservoir pressure temperature depletes the regime operation changes so whatever sensor reading that you receive from these all all part of the machines is subject to shift or drift or change.

22:27.000 --> 22:53.000
So setting fixed let's say set of thresholds on these first would require continuous maintenance and observation of the false alarms and so on so forth and also it may not be fully kind of aware of all possible modes of operation.

22:53.000 --> 23:08.000
So we were thinking we haven't seen everything of these machines operations should we use machine learning to learn from the past of these machines do we know the entire state of the data.

23:08.000 --> 23:22.000
The answer was knew we were in the best case we only knew 10 15 to 20% of the entire operation state of these complex machine sensor.

23:22.000 --> 23:45.000
I mean 10 15% of the operational state as defined by a specific set of sensors over a lifetime meaning you have the 10% of the lifetime of the machine or that the sensors themselves only capture some small percent of the operational state of the machine itself.

23:45.000 --> 24:03.000
That's a very good question. I think it's exactly the combination of both. If there is a machine which has been just in operation you can have some prior belief about how the sensor should behave or how should receive the data but the rest is mystery and

24:03.000 --> 24:31.000
the two identical machines have different sensor readings if you start them at the same time even set set up on the same plant. So this is one side of the unknown uncertainty and also a lot of operational modes as you said in terms of the state of operation is unknown throughout life of the life span.

24:31.000 --> 24:51.000
The other thing is the more interesting one is the failures the patterns of failures trends in majority of the cases when you want to cover the entire let's say 200 or 300 different individual components of these

24:51.000 --> 25:11.000
series you haven't seen failures of a lot of these components in their in their life so to maximize the coverage and also not make rigid assumptions you have to move away from traditional supervised that learning

25:11.000 --> 25:31.000
to detect failures that doesn't work or if it works it covers maybe 5% of the problem. So where did machine learning and deep learning came to help us was we said okay let's let's say do we know what is

25:31.000 --> 25:49.000
going to our best of knowledge what is the temporal transient healthy behavior of these machines and the sensor observations that we get we said possibly yes we have good knowledge of that to some degree.

25:49.000 --> 26:07.000
Okay let's let's build these models which in this case we use recurrent neural network LSTM's at sensor level to monitor and learn the healthy behavior kind of sequences of these machines.

26:07.000 --> 26:33.000
So from what we get in terms of the sensor will either stream data let's say if we build these representative model of the sensor from purely healthy behavior point of view can we predict a healthy vector into future and then checking that sort of sequence of time series versus what we actually

26:33.000 --> 26:54.000
received we were able to predict any deviation so yes this kind of difference or residual signal for us was the outcome of using machine learning in the case of

26:54.000 --> 27:18.000
the outcome of this book LSTM's to detect abnormalities got it got it and so it's a playback at least one part of that you started talking about the traditional approach and like setting these thresholds you know like you might see in other places where you you would use anomaly detection.

27:18.000 --> 27:47.000
And you know one approach might be to like automatically set the threshold but that's not what you did here what you did is create a maybe lower level or kind of a richer model of the machine itself and didn't use a threshold based approach you use this more residual based approach to determine when an anomaly was happening exactly so.

27:47.000 --> 28:12.000
Basically this detection of abnormalities was the first sort of step let's say in this workflow as soon as you detect some sort of abnormal or outliers within your residual signal which is the error signal then what we could do was to use some

28:12.000 --> 28:33.000
parametric sort of techniques that you build a distribution of what is your error signal and then set some parameter around the distribution and whatever goes out of the sort of the threshold bands you detect them as abnormalities but again we found

28:33.000 --> 28:50.000
we don't know a lot about these signals this is going to produce a lot of false alarms so what what else is out there so we got into this novel approach that scientists in NASA

28:50.000 --> 29:16.000
developed and applied to their telemetry data received from the space shuttles which is called non parametric thresholding it is a kind of a dynamic non parametric thresholding and what is this is basically trying to fit a dynamic sort of set of bands around your residual error signal

29:16.000 --> 29:44.000
and then these dynamic bands then allow you to detect any sort of abnormalities or outlier sequences that sort of is more robust in terms of not reacting to sort of temporal behavior of the sensor data that you see

29:44.000 --> 30:06.000
as some cushioning sort of kind of buffer zone to prevent false alarms and also it is able to with the help of LSTM to capture the multimodal operational base like the heterogeneity in the data

30:06.000 --> 30:32.000
so the technique itself is very simple but kind of beautiful technique it is at any given time define a wide range of threshold bands and then try to maximize a criteria by analyzing the whole historical distribution of the error

30:32.000 --> 30:52.000
and then that criteria is can we detect any sort of new set of observation that can deviate the properties of the error distribution most at any given time

30:52.000 --> 31:12.000
that set of band is set as the new threshold dynamically so once you have this set of thresholds then you detect the strings that sits outside of the threshold and then you calculate the score for the for the sequence

31:12.000 --> 31:22.000
itself is calculated based on the second moment of area of the sequence outside of the threshold band

31:22.000 --> 31:46.000
so you mentioned in their LSTMs that's the way that you're modeling the state of the individual components sure yeah exactly so the use of LSTMs was to exactly to what sort of algorithms are out there

31:46.000 --> 32:02.000
that can understand the temporality in the signal and the multiple modes and provide the best representation of these noisy sensor readings

32:02.000 --> 32:20.000
so in our case it was very important for us to make these predictions in future of the healthy bit pattern by understanding the kind of the historical changes in the signal

32:20.000 --> 32:42.000
so long term short term memory type signals allowed us to kind of use the previous states of them of the system and incorporate that because of this specific architecture of these algorithms to provide the correct representation of the signal

32:42.000 --> 33:02.000
and it was very expensive to build LSTMs at sensor level so we talked about few hundreds and few thousands of sensors per compressor or in our case we had to monitor about 200 sensors or 100-200 sensors per machine

33:02.000 --> 33:18.000
so building these number of LSTMs and optimize them based on few years of time series data with high sampling rate was a challenge

33:18.000 --> 33:34.000
the question I have about that is the you mentioned that you the each of the LSTMs is mapping to a sensor the it's kind of like you have the hierarchy right

33:34.000 --> 33:56.000
we got the sensors the sensors presumably map to components and the components map to whatever the machine is I guess the reason why you do it at the sensor level is because you really don't know the relationship between the sensors and the components per se

33:56.000 --> 34:10.000
the good point because we adapted this reliability based system which is aiming to detect the modes of failure that are active at any given time

34:10.000 --> 34:22.000
we had to go from to a bottom-up approach we had to utilize the entire sensors that are monitoring one component

34:22.000 --> 34:35.000
let's say in a compressor we have shafts we have bearings we have seal systems and impellers of the compressors

34:35.000 --> 34:50.000
so let's say in a bearing case of bearings you have bearings on either side of the shaft and you have multiple different sensors monitoring the behavior of these components

34:50.000 --> 35:02.000
a bunch of temperature sensors a bunch of flow rate that are injecting lubricants the component a lot of vibration type sensors in multiple different directions

35:02.000 --> 35:11.000
so making sense of these active failure mode at any given time is a complex problem

35:11.000 --> 35:29.000
we had to understand what sort of abnormalities working together would relate to which failure modes for that we needed to know abnormalities at bottom layer at sensor layer

35:29.000 --> 35:45.000
so you created these kind of models or representations of your sensors and I'm trying to put the pieces together now

35:45.000 --> 35:53.000
you've got these representations somewhere in the middle you're trying to get to the fault matrices or

35:53.000 --> 35:59.000
you've got on the other side of the fault matrices your traditional reliability engineering equations

35:59.000 --> 36:04.000
how do you get from the LSTM output to the fault matrices?

36:04.000 --> 36:27.000
yes so I think the keys LSTMs are providing these healthy sequences of healthy behavior as a kind of mimicking the machine behavior from healthy point of view

36:27.000 --> 36:35.000
just where the non-parametric came in you've got this and yes

36:35.000 --> 36:50.000
by creating this residual signal or error signal and then applying the parametric thresholding on top of that

36:50.000 --> 37:00.000
comparing the healthy from actual then you calculate the score what we call anomaly score let's say at sensor level

37:00.000 --> 37:08.000
so any sequence of time at any time stamp let's say you have an anomaly score at sensor level

37:08.000 --> 37:22.000
so feeding anomaly scores from 10 sensors related to one component into the fault matrix or this cause and effect sort of matrix

37:22.000 --> 37:28.000
then you get a score per component per failure mode at any given time

37:28.000 --> 37:42.000
so this is let's call it like component defector score or failure score and then having all of this failure mode lined up and components accordingly

37:42.000 --> 37:51.000
then you can have a representation of a ranking of what are the anomalies at any component level

37:51.000 --> 37:59.000
and then as you said by the use of the reliability engineering logic we can aggregate the scores at component level

37:59.000 --> 38:11.000
let's say lots of different bearings and multiple different seal systems together and aggregate their abnormality scores together

38:11.000 --> 38:21.000
and then roll it up into the assembly level then you can aggregate again at assembly level into the system level which is compressor

38:21.000 --> 38:25.000
you can follow the same and go and cover your entire asset

38:25.000 --> 38:31.000
by entire asset we're talking about like a gigantic oil rig or something

38:31.000 --> 38:34.000
exactly have you gotten that far?

38:34.000 --> 38:46.000
we've gone up to critical pass machineries and critical pass machineries means those which are sitting on the critical production line

38:46.000 --> 38:57.000
you've talked a little bit about the kind of the challenge of just training all of the LSTMs that are involved in doing the kind of modeling here

38:57.000 --> 39:05.000
can you elaborate a little bit on what some of the biggest challenges were?

39:05.000 --> 39:19.000
well LSTMs are not easy to train and optimize especially that we are dealing with quite a lot of data

39:19.000 --> 39:29.000
we're talking about one second sampling rate of few years of one sensor and when we talk about thousands of sensors

39:29.000 --> 39:39.000
it's quite extensive so LSTMs have multiple different hyper parameters and to get the best results

39:39.000 --> 39:43.000
you have to really invest time to optimize them

39:43.000 --> 39:49.000
the thing was we are dealing with multiple different type of parameters we want to build these representations

39:49.000 --> 39:57.000
vibration types sensors are very different in terms of the noise patterns and temporality

39:57.000 --> 40:07.000
and the condition dependence is historically to what has happened in order to build a more accurate representation in the future

40:07.000 --> 40:16.000
as opposed to for example temperature type signals which have a weak delay and there is not much temporality in it

40:16.000 --> 40:25.000
the noise characteristics is different so optimizing LSTMs at these sensor levels was a challenge

40:25.000 --> 40:38.000
for that we are lucky to collaborate with SIGOP which is one of the black box optimization platform

40:38.000 --> 40:47.000
and then that helps us a lot using the Bayesian type optimization to quickly learn the

40:47.000 --> 40:56.000
computer optimization parameters space very quickly and efficiently because there we have the ability to optimize

40:56.000 --> 41:08.000
over multiple different objectives training time was one of our main objective as well as reducing the loss

41:08.000 --> 41:25.000
you just said something interesting the optimization problem that you the way you ultimately frame your optimization problem wasn't solely constrained to system level criteria

41:25.000 --> 41:32.000
but you also had this was it a train time criteria or compute train time?

41:32.000 --> 41:40.000
yeah exactly so I think this this is a complex sort of pipeline of connected pieces together

41:40.000 --> 41:53.000
performance of LSTMs in order to detect healthy patterns or sequences and detecting the residual or error

41:53.000 --> 42:04.000
and feeding that into the kind of this cause and effect matrix and calculating the scores and evaluating these is

42:04.000 --> 42:13.000
entirely together is one massive optimization group so you can look at the LSTMs itself and optimize the

42:13.000 --> 42:26.000
hyper parameters there but getting the best fit there may not necessarily you know give you a global optimized path

42:26.000 --> 42:35.000
so you may get into a local optimization here so connecting all of these together building this

42:35.000 --> 42:45.000
gigantic sort of optimization loop with multiple different hyper parameters which are not necessarily all from coming from the LSTM

42:45.000 --> 42:56.000
sort of hyper parameters was was a big challenge and not so not just from the LSTM but not just from a single LSTM you've

42:56.000 --> 43:11.000
got parameters from multiple LSTMs say 50 or 100 plus these kind of broader constraints on your run time

43:11.000 --> 43:23.000
was your your accuracy represented as a threshold or constraint or how did you incorporate that into your optimization

43:23.000 --> 43:35.000
the ultimate ultimate kind of objectives were into two different dimensions

43:35.000 --> 43:43.000
we had to have the human in the loop in order to evaluate the entire optimization

43:43.000 --> 43:57.000
and that was what is the intuition and engineering domain knowledge in checking whether an abnormality score at system level or

43:57.000 --> 44:04.000
assembly level is really an abnormality based on their experience and their knowledge of failure more than

44:04.000 --> 44:16.000
analysis so that was our ultimate objective that let's say we introduced a bit of supervised sort of training into the loop

44:16.000 --> 44:26.000
so we had a bit of labeling going on here in terms of what is this ultimate score at system or assembly or

44:26.000 --> 44:38.000
component level and then we had multiple hyper parameter sets at LSTM stage at the scoring calculation stage

44:38.000 --> 44:50.000
and so yes we were ultimately optimizing over the frontier of reducing the false alarms for indications and

44:50.000 --> 45:00.000
maximizing the correct detection per failure so that was our frontier

45:00.000 --> 45:14.000
can you elaborate a little bit on the human in the loop aspect of that how many you know how much labeling had to go into being able to build these models

45:14.000 --> 45:23.000
that's a very good question I think we built some blueprints in hours and hours of discussions with

45:23.000 --> 45:33.000
lateritating machinery those who have spent designing these machines for 20 or 30 years so we had lots of workshops to go through these

45:33.000 --> 45:45.000
patterns historical patterns of past 10 years or so of the machine operation and then get all of these experts in

45:45.000 --> 45:56.000
agreement of what is their final judgment of which sort of sequences are abnormal sequences which are kind of

45:56.000 --> 46:06.000
noise artifacts which are sensor drifts and so on so forth which of them combined together are representative of failure mode

46:06.000 --> 46:18.000
so through that process we labeled several of these examples and then it was a labor intensive process for sure

46:18.000 --> 46:30.000
but to be clear what your labeling is a time series data set from of what all of the sensors in a given machine or

46:30.000 --> 46:34.000
a particular sensor or I guess it couldn't it would have to be system level

46:34.000 --> 46:49.000
yeah so we covered 15 or 20 critical components of these let's say turbo compressors which are sitting on the

46:49.000 --> 46:56.000
critical path of the machine itself failure of each would result in the failure of the entire machinery

46:56.000 --> 47:06.000
down time so we identified those and some high critical failure modes that statistically are from reliable to

47:06.000 --> 47:11.000
point of view resulted in majority of the failures and then we focused on that

47:11.000 --> 47:23.000
and ultimately what the what the SIGAP element of the optimization process was was kind of narrowing in on the

47:23.000 --> 47:34.000
point on the frontier that you had to care about and optimizing across that so you weren't trying optimize across the entire

47:34.000 --> 47:42.000
state space of hundreds of LSTMs and these constraints and all that stuff is that the general idea?

47:42.000 --> 47:52.000
that's very true yeah so hyperparameters that we're talking about are like learning rate and look back look back sequence which in the case of

47:52.000 --> 48:02.000
recurring neural network is important also increase the time training time and regularization parameters and so on so forth

48:02.000 --> 48:16.000
it was about 12 15 number hyperparameters per LSTMs to optimize and multi-objective basically multi-metric optimization

48:16.000 --> 48:27.000
so using the Bayesian optimization definitely helped a lot to quickly reduce the space of exploration

48:27.000 --> 48:32.000
and so what's what's next for this effort?

48:32.000 --> 48:44.000
Sure it's very exciting actually so what we what we talked about today so far is primarily focusing on one aspect of

48:44.000 --> 48:53.000
let's say predictive maintenance domain there are multiple different things you can do if the main objective is to

48:53.000 --> 48:59.000
make the best maintenance action this is a very difficult question to answer

48:59.000 --> 49:07.000
because detecting problem is one thing making action and correct action is a different thing

49:07.000 --> 49:16.000
and the action space is influenced by what sort of resources do you have available

49:16.000 --> 49:25.000
which part of the geography you are operating are you in the island or an offshore 200 kilometers away from everything

49:25.000 --> 49:36.000
which apply complexity and the complexity of the maintenance team and the maintenance tools that you have to have on board

49:36.000 --> 49:45.000
the lead time of the components that you need to order and be built and tested to come may take 10 months or 12 months to come

49:45.000 --> 49:48.000
so you're saying that you've solved the easy part of the problem

49:48.000 --> 49:55.000
that's correct that's correct and this is a very complex question to answer

49:55.000 --> 50:02.000
so what other things that can help in terms of prediction side on this first aspect

50:02.000 --> 50:10.000
before getting into prescriptive sort of side of maintenance is how much how much time do I have to

50:10.000 --> 50:15.000
wait till ultimate failure this is a very valuable information

50:15.000 --> 50:21.000
so we talked about I think a lot of of the shelf tools that you see existing in the market

50:21.000 --> 50:28.000
the GE tools and so many other things are very good in detecting the anomalies

50:28.000 --> 50:35.000
but detecting what how much time do I have at component level and failure mode

50:35.000 --> 50:46.000
from that point of view is a very valuable because it allows them to safely operate their systems without shutdown

50:46.000 --> 50:56.000
because these machines are not designed to kind of constantly shut them down to repair or do some interventions

50:56.000 --> 51:11.000
so yeah predicting time to failure is and linking that to the knowledge of failure mechanics is an interesting topic

51:11.000 --> 51:15.000
and then and then you get to take on the what to do about it

51:15.000 --> 51:23.000
that's very true yes and I think there have been a lot of efforts in research

51:23.000 --> 51:31.000
and also beautiful tools exist at the moment which are trying to optimize the action

51:31.000 --> 51:38.000
over cost over resource availability and over health

51:38.000 --> 51:48.000
so health sort of let's say at system level health plot or health indicator can come from these sort of approaches

51:48.000 --> 51:53.000
in the early detection time to failure aggregation and system level

51:53.000 --> 52:01.000
but understanding the cost this is the whole set of you know new sort of formulation

52:01.000 --> 52:10.000
of what is the my cost function from operational plan point of view

52:10.000 --> 52:18.000
the revenue that the system is generating the criticality so on so forth

52:18.000 --> 52:29.000
but to build this cost function and then resources also is also a majority of the cases is human related

52:29.000 --> 52:35.000
and we are talking about the materials like the spare parts point of view

52:35.000 --> 52:47.000
and also human repair experts who are going to be deployed on shore or on site or off shore kind of external software

52:47.000 --> 52:54.000
and building a function massacizing that is a task

52:54.000 --> 53:05.000
and do you expect to be able to apply similar ideas meaning introduce machine learning or deep learning

53:05.000 --> 53:12.000
to complement existing domain knowledge to solve those other problems as well

53:12.000 --> 53:17.000
definitely definitely I think a series of like probabilistic approaches

53:17.000 --> 53:25.000
a Bayesian based and Bayesian networks for example or even a lot of machine learning applications

53:25.000 --> 53:33.000
in predicting different aspects of the cost function in terms of the supply chain representation of the supply chain

53:33.000 --> 53:40.000
as a kind of sequential model and also reinforcement learning can come into effect

53:40.000 --> 53:48.000
in building the function of your own in learning your cost function and supply chain

53:48.000 --> 53:56.000
also a lot of NLP people are applying in kind of feeding into sort of knowledge graphs

53:56.000 --> 54:07.000
and by building these knowledge graphs you can link the entire asset data in terms of the operational plans

54:07.000 --> 54:16.000
what is going on in terms of you know the inner outputs and then driving information

54:16.000 --> 54:22.000
kind of from these knowledge graphs and feeding that into some sort of analytical layers

54:22.000 --> 54:34.000
to predict what is the consequences of actions and on top of these knowledge graphs is another thing

54:34.000 --> 54:37.000
got it got it awesome awesome

54:37.000 --> 54:44.000
well Cheyenne thanks so much for taking the time to share a bit about your experiences in this area

54:44.000 --> 54:47.000
very cool stuff

54:47.000 --> 54:48.000
pleasure

54:48.000 --> 54:50.000
pleasure talking to you

54:50.000 --> 54:51.000
thank you

54:51.000 --> 55:04.000
thanks

