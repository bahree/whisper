1
00:00:00,000 --> 00:00:16,320
All right, everyone. I am here with Jamie McBeth. Jamie is an assistant professor in the Department

2
00:00:16,320 --> 00:00:21,120
of Computer Science at Smith College. Jamie, welcome to the Twoma AI podcast.

3
00:00:22,160 --> 00:00:27,600
Thank you. Thanks for having me. Hey, I'm super excited to dig into our conversation and learn

4
00:00:27,600 --> 00:00:33,200
a bit about your research and what you're up to. Let's get started by having you share a bit

5
00:00:33,200 --> 00:00:39,040
about your background with our audience. How did you come to work in AI and cognitive systems in

6
00:00:39,040 --> 00:00:48,640
particular? Sure. So originally I was, I would say a physicist actually a mathematician and a

7
00:00:48,640 --> 00:00:56,800
physicist as an undergraduate and also sometime as a grad student. I then fell in love with

8
00:00:56,800 --> 00:01:02,560
computer science. I studied computer science in graduate school. And towards the end of my

9
00:01:02,560 --> 00:01:07,840
career in graduate school, I also fell in love with the specific topic that I work on now,

10
00:01:08,560 --> 00:01:14,720
which is artificial intelligence systems and cognitive systems for performing natural

11
00:01:14,720 --> 00:01:19,840
language understanding and the the issues associated with that. That's great. That's great.

12
00:01:19,840 --> 00:01:28,720
When we were chatting earlier, you spoke a little bit more about your, the way you think about

13
00:01:28,720 --> 00:01:36,320
cognitive systems and kind of how that's different from a lot of the contemporary application

14
00:01:36,320 --> 00:01:40,560
of machine learning and AI. I'd love to hear you elaborate on that a bit for our audience.

15
00:01:41,280 --> 00:01:45,520
Sure. Sure. Yeah. So those of us in the cognitive systems community,

16
00:01:45,520 --> 00:01:52,960
where we're a part of the artificial intelligence community, but people are focused, people in the

17
00:01:52,960 --> 00:02:00,240
cognitive systems community are focused quite a bit more on using artificial intelligence as a

18
00:02:00,240 --> 00:02:08,240
vehicle for a better understanding of human intelligence and not particularly of using AI to

19
00:02:08,240 --> 00:02:15,920
just score well at particular tasks and do well on the leaderboard. I think some of the negative

20
00:02:15,920 --> 00:02:22,000
things that have been associated with artificial intelligence these days, such as biases and things

21
00:02:22,000 --> 00:02:30,320
like that, have to do with there being a little bit too much hype around the systems that people

22
00:02:30,320 --> 00:02:39,680
are building and the way you're able to show good numbers at these test problems and focusing

23
00:02:39,680 --> 00:02:46,000
less on the actual science. Okay. What really can these systems do? So yeah, in the cognitive systems

24
00:02:46,000 --> 00:02:52,800
community, or I care much more about building systems that have a human-like intelligence.

25
00:02:52,800 --> 00:03:02,320
Yeah. It's an interesting contrast. I think we see all the time in popular media nowadays,

26
00:03:03,520 --> 00:03:08,000
you know, research or publishes a paper about AI scoring well against some benchmark,

27
00:03:08,000 --> 00:03:14,400
and then journalist writes article, AI learns to understand x, y, z, and you must be like,

28
00:03:14,400 --> 00:03:27,200
no way, not understand. Yeah, yeah. Yeah, I agree. So I think that the, I think things that have

29
00:03:27,200 --> 00:03:32,720
happened in the machine learning community, I think we've, we've, machine learning our

30
00:03:32,720 --> 00:03:38,400
artificial intelligence have come a long way in the past couple of decades, but I think another

31
00:03:38,400 --> 00:03:47,040
thing that's happened is that that we've begun or not begun, but there has evolved a particular

32
00:03:47,040 --> 00:03:55,760
kind of machine learning research that's just focused on doing doing well, scoring higher than

33
00:03:55,760 --> 00:04:01,840
a little bit higher than the last person did on that one particular data set. And then what I

34
00:04:01,840 --> 00:04:06,960
found in my work is that if you scratch the service a little bit, you find that there are important

35
00:04:06,960 --> 00:04:15,920
issues with things like the metrics that people are using, and the possibility that you, you,

36
00:04:15,920 --> 00:04:23,360
you have systems that are fairly overfit to particular data sets. You know, you throw adversarial

37
00:04:23,360 --> 00:04:29,040
examples at a system and you see it kind of crumble. It's unable to really do the things that,

38
00:04:29,040 --> 00:04:32,480
that you thought it was doing because it scored well in that data set.

39
00:04:32,480 --> 00:04:42,720
So when you, when you talk about AI as a vehicle for better understanding human intelligence,

40
00:04:42,720 --> 00:04:51,120
what are some examples of, of that, you know, ways that we understand human intelligence

41
00:04:51,120 --> 00:04:57,840
now better because of AI research. Oh, yeah, that's a, that's a great question.

42
00:04:57,840 --> 00:05:06,720
You know, I, my examples for this come from the kind of decades ago,

43
00:05:09,520 --> 00:05:18,160
traditions of AI. For example, Roger Shank and Robert Abelson's original work on

44
00:05:18,160 --> 00:05:25,280
scripts, goals, plans, and understanding. And that's a, that's a, that's a text or those are ideas

45
00:05:25,280 --> 00:05:32,640
that have kind of permeated a large, a large path throughout, throughout, throughout,

46
00:05:32,640 --> 00:05:38,640
through many different, throughout many different communities, including the cognitive psychology

47
00:05:38,640 --> 00:05:44,240
community, the cognitive linguistics community, even people performing social science,

48
00:05:45,280 --> 00:05:50,720
quote, this idea of scripts, which basically that means that there are knowledge structures that

49
00:05:50,720 --> 00:05:58,160
people use for navigating a commonly encountered social situation. So that's, that's a, I think

50
00:05:58,160 --> 00:06:04,880
that's a really good example that book from 1977 is still cited. It, you know, it's got tens of

51
00:06:04,880 --> 00:06:10,400
thousands of citations and people still cited over and over again. And when I talk to social scientists

52
00:06:10,400 --> 00:06:15,680
who know about this concept of scripts, they don't even, they don't even realize that it comes from

53
00:06:15,680 --> 00:06:21,840
artificial intelligence research, but it's important concept that people use across many disciplines.

54
00:06:21,840 --> 00:06:23,840
So that, that, that's a good example, I think.

55
00:06:24,880 --> 00:06:30,320
Got it. Yeah. And great to hear you mentioned that book. It doesn't get mentioned on the podcast

56
00:06:30,320 --> 00:06:36,320
very often. I think it's come up once or twice before. I went to grad school at Northwestern,

57
00:06:36,320 --> 00:06:43,680
and I think that's where Roger Shank was for quite a bit of time. And that was the first book

58
00:06:43,680 --> 00:06:49,680
that I ever came across about AI. In fact, I picked it up to like a use, you know, either like the,

59
00:06:50,480 --> 00:06:55,040
you know, use bookshelf at Barnes and Nobles or like a flea market or something like that.

60
00:06:57,840 --> 00:07:05,040
The, you also talk a little bit about, well, you mentioned that a big focus

61
00:07:05,680 --> 00:07:09,920
from an application perspective for you is on natural language understanding.

62
00:07:09,920 --> 00:07:17,200
How does that tie into the broader goal of your cognitive systems work?

63
00:07:18,240 --> 00:07:26,160
Yeah. What you, what you end up realizing, when I started studying this, I knew what,

64
00:07:26,160 --> 00:07:32,560
what lots of people can see as natural language processing, what I understood was, oh, you know,

65
00:07:32,560 --> 00:07:36,720
you just have these systems that are calculating statistics having to do with words or

66
00:07:36,720 --> 00:07:42,560
parsing to figure out what the grammatical structure is of a sentence and maybe doing some things

67
00:07:42,560 --> 00:07:48,720
like that. What you eventually realize is that if you, if you want to build systems and build

68
00:07:48,720 --> 00:07:54,800
systems that are performing something like the way humans understand language and produce language,

69
00:07:55,760 --> 00:08:03,760
because language expresses all, expresses ideas, you realize that your systems need to be able to

70
00:08:03,760 --> 00:08:08,960
represent ideas and manipulate ideas and basically have thought representations. So,

71
00:08:08,960 --> 00:08:14,560
your systems that, that at first you think, natural language processing is just about messing

72
00:08:14,560 --> 00:08:19,840
with words and grammar, you eventually come to the conclusion that natural language understanding

73
00:08:19,840 --> 00:08:24,960
is about building systems machines that can really think like people if you're really going to

74
00:08:24,960 --> 00:08:30,400
get it to understand that text and say, answer a question about that text.

75
00:08:30,400 --> 00:08:37,440
And so, are the tasks that you're focused on, the same tasks that we see,

76
00:08:38,800 --> 00:08:42,240
you know, traditional tasks in the NLU community like question answering?

77
00:08:43,520 --> 00:08:49,040
Yes, yeah, that's one important task, not the only thing. I mean, you can think of others

78
00:08:49,040 --> 00:08:56,400
such as translation or summarization and things along those lines, but more recently,

79
00:08:56,400 --> 00:09:02,240
folks in the machine learning, natural language processing community have started to generate

80
00:09:02,240 --> 00:09:09,680
large data sets that have those tasks in them that have things like, you know, a data set that

81
00:09:09,680 --> 00:09:15,840
has paragraphs and lots of questions and answers. But when you scratch the surface of the machine

82
00:09:15,840 --> 00:09:23,680
learning deep learning systems that are working on those tasks, what you, what you find out basically

83
00:09:23,680 --> 00:09:29,840
is that they're taking advantage of patterns in the texts and in the questions in order to come

84
00:09:29,840 --> 00:09:38,320
up with correct answers to the questions according to what's in the test answers for the data set.

85
00:09:38,880 --> 00:09:42,560
And if you change things around just a little bit, all of a sudden the system doesn't really

86
00:09:42,560 --> 00:09:50,480
understand. But yeah, those are things like, you know, reading a story and answering questions

87
00:09:50,480 --> 00:09:56,240
about the story, summarizing a story, paraphrasing a story, those, those, those are the kinds of

88
00:09:56,240 --> 00:10:05,840
things that I work on. Okay, and so in the, I don't think traditional is the right word here,

89
00:10:05,840 --> 00:10:15,040
but in the typical NLU setup, you've got this task of let's say question answering and you're just

90
00:10:15,040 --> 00:10:21,440
trying to perform well on the question answering data set. You're not necessarily trying to perform

91
00:10:21,440 --> 00:10:28,000
while you're trying to gain deeper insights into something. What are the things that you're trying

92
00:10:28,000 --> 00:10:34,560
to gain deeper insights to and like, what are, how do you approach that problem or what's your

93
00:10:34,560 --> 00:10:40,480
problem set up and where does your learning, you know, come out of thinking about these kinds of

94
00:10:40,480 --> 00:10:49,280
problems? Yeah, one of the deeper, deeper insights I want to get to by looking at, say,

95
00:10:50,880 --> 00:10:57,840
story understanding or question answering is what are the, how, how do people form a mental

96
00:10:57,840 --> 00:11:04,640
picture of what the text describes? How do people create that mental picture and then how do the

97
00:11:04,640 --> 00:11:12,320
people or how to systems manipulate that mental picture in order to be able to reason about the text

98
00:11:12,320 --> 00:11:19,600
and also the questions that were asked about the text? And the main thing I found in my work,

99
00:11:19,600 --> 00:11:26,640
the most important thing to me is that it turns out ironically that to represent

100
00:11:28,160 --> 00:11:33,120
knowledge thought meaning well, you have to have a significant part of your representation that

101
00:11:33,120 --> 00:11:38,720
really has nothing to do with the language at all. It really has to do with these imagery,

102
00:11:38,720 --> 00:11:44,720
mental model pictures that you create that represent the meaning and then those get mapped

103
00:11:44,720 --> 00:11:50,720
onto language. They get mapped from language in understanding to that representation and then when

104
00:11:50,720 --> 00:11:57,280
people say things or produce language, the idea which is this non-linguistic conceptual structure

105
00:11:57,280 --> 00:12:05,280
then gets mapped back onto language and that's one of the important things I've found in my research

106
00:12:05,280 --> 00:12:11,920
and an important thing I'm trying to get to by looking at tasks like question answering or paraphrasing

107
00:12:11,920 --> 00:12:17,920
because in many ways that's what paraphrasing is, you realize that to say, well, there are many

108
00:12:17,920 --> 00:12:25,040
different ways of saying the same thing. That same thing stuff must be this other thing that's

109
00:12:25,040 --> 00:12:34,400
different from the language, the ways of saying it. Yeah, I'm wondering what that means in practice.

110
00:12:34,400 --> 00:12:45,200
Like, I think of a deep learning model is building some representation that is some vector space,

111
00:12:45,200 --> 00:12:51,200
is what you're saying that are you adding additional constraints to that representation that says

112
00:12:51,200 --> 00:13:01,360
that it has more image-like properties that should I be taking what you're saying about building

113
00:13:01,360 --> 00:13:10,320
a picture literally or is that more figurative? It's in some senses, literal in some senses,

114
00:13:10,320 --> 00:13:20,480
figurative. Yeah, when it comes to say, for example, artificial neural networks or deep learning

115
00:13:20,480 --> 00:13:27,920
systems that are professed to be able to build their own representations, the problem that I see

116
00:13:27,920 --> 00:13:34,480
is that I haven't seen people demonstrate very well, particularly when it comes to natural

117
00:13:34,480 --> 00:13:40,960
language processing problems, demonstrate well that the representations these systems are building

118
00:13:40,960 --> 00:13:49,920
are like the ones that I think we should be building to build natural language understanding

119
00:13:49,920 --> 00:13:56,160
systems. So, you're supposed to be building your own, the system's supposed to be building its own

120
00:13:56,160 --> 00:14:01,840
representations. It's not obvious based on the inputs that deep learning systems are being given,

121
00:14:01,840 --> 00:14:08,480
say, for example, examples of paragraphs and examples of questions and answers to those

122
00:14:10,480 --> 00:14:16,000
relevant to those paragraphs. It's not clear that the deep learning systems are building

123
00:14:16,000 --> 00:14:25,840
that they're building representations that are like the ones that I think probably would be good

124
00:14:25,840 --> 00:14:34,000
for representing things. In my opinion, the representation systems that I work on, for the time

125
00:14:34,000 --> 00:14:40,880
being, I'm building them by hand, and these representation systems try to decompose meaning into

126
00:14:40,880 --> 00:14:47,040
complex combinations of conceptual primitive structures. For example, if I said something like

127
00:14:48,880 --> 00:14:55,760
Mary kicked the ball on one hand, and I said something on the other hand, like Mary moved

128
00:14:55,760 --> 00:15:02,480
her foot towards the ball and struck it. I didn't use the word kick in that other expression,

129
00:15:02,480 --> 00:15:07,440
but you're still in that other expression able to compose a picture of what happened,

130
00:15:07,440 --> 00:15:18,000
and see that it's equivalent to the first sentence that I gave. So, I've been building systems by

131
00:15:18,000 --> 00:15:25,280
hand that can start with a conceptual structure that looks like this. It's got these decomposed

132
00:15:25,280 --> 00:15:31,440
conceptual primitives, like, for example, for kick. One conceptual primitive meaning that

133
00:15:31,440 --> 00:15:37,680
Mary moved her foot, another primitive act, meaning that the foot struck the ball, and another

134
00:15:37,680 --> 00:15:44,160
primitive act, meaning that the ball took off and went somewhere. Then I have these other systems

135
00:15:44,160 --> 00:15:50,240
that actually can generate lots of paraphrases of this conceptual structure. It can generate

136
00:15:50,240 --> 00:15:56,720
language from it. Then what I've been doing in some ways is feeding these paraphrases into deep

137
00:15:56,720 --> 00:16:03,600
learning systems that supposedly can understand language and finding that they don't really.

138
00:16:06,480 --> 00:16:12,080
So, when you talk about this representation that you're hand crafting and you're

139
00:16:13,680 --> 00:16:20,720
creating equivalences between kicking and moving feet is that within the

140
00:16:20,720 --> 00:16:30,800
natural language domain, or is that in some vector space, or are you mapping to some kind of

141
00:16:32,640 --> 00:16:37,360
image-based representation of these things, or something totally different?

142
00:16:38,320 --> 00:16:45,520
Yeah. So, these kinds of representations have been studied by AI people before we were talking

143
00:16:45,520 --> 00:16:51,600
about Roger Shank, and we were talking about script plans, goals, and understanding. If you

144
00:16:51,600 --> 00:16:57,520
read that book, you might have remembered the original restaurant script had all these conceptual

145
00:16:57,520 --> 00:17:10,000
primitive acts in it, such as the waiter, P. Francis, his or herself, themselves to the customer,

146
00:17:10,000 --> 00:17:17,520
and then leads the customer to the table, and then the customer, M. Francis, their order to the

147
00:17:18,480 --> 00:17:26,960
server, and things like that. So, Shank had the system called conceptual dependency,

148
00:17:26,960 --> 00:17:33,440
and the idea was that try to reduce the number of primitive acts that you have to represent

149
00:17:33,440 --> 00:17:42,560
things, and then to represent more complex things, just add more primitive acts to make it more specific.

150
00:17:44,320 --> 00:17:49,920
So, for the time being, those are the kinds of representations that I work on.

151
00:17:50,880 --> 00:17:58,000
They have primitive, such as some object moving through space, some object moving to the inside

152
00:17:58,000 --> 00:18:03,280
of another object that was called ingest, some object moving from the inside to the outside of

153
00:18:03,280 --> 00:18:09,520
another object, expel, and then other primitive acts like that, trying to break things down in

154
00:18:09,520 --> 00:18:16,240
kind of molecular structure kind of way. There are other systems that people have developed in the

155
00:18:17,920 --> 00:18:23,440
cognitive linguistics community, specifically these systems called image schemas that were popularized

156
00:18:23,440 --> 00:18:30,960
by Lake Hoff and Johnson. And in research, we're only just now over the past few years figuring out

157
00:18:30,960 --> 00:18:37,440
some of the correspondences between a Shank's old system of conceptual dependency,

158
00:18:37,440 --> 00:18:41,840
and these other systems that were developed by people in the cognitive linguistics community,

159
00:18:41,840 --> 00:18:48,480
and trying to learn from them. Sorry, if that's kind of too long. That's great.

160
00:18:48,480 --> 00:19:01,200
I'm wondering about you kind of, as part of the setup you talked about how deep learning systems,

161
00:19:01,200 --> 00:19:11,680
as an example, are often overfitted on a given benchmark, and there's a lot of competition

162
00:19:11,680 --> 00:19:17,600
in the research community to one up the next and achieving state-of-the-art performance on a

163
00:19:17,600 --> 00:19:31,920
given benchmark. Is your research, are you like saying, hey, I'm not going to play that game,

164
00:19:31,920 --> 00:19:37,680
I'm trying to get understanding, and if you aren't, if you're opting out of that game,

165
00:19:37,680 --> 00:19:41,120
how do you evaluate the performance of your representations?

166
00:19:41,120 --> 00:19:50,400
Yeah, that's a great question. In some ways, I am not playing that game, and in some ways,

167
00:19:51,280 --> 00:19:58,320
I am playing that game. In recent work that was published at the Advanced

168
00:19:58,320 --> 00:20:04,480
Executive Systems Conference last summer, what I did was basically, and I think I described

169
00:20:04,480 --> 00:20:11,840
this a little bit earlier, there are deep learning systems that can do paraphrase recognition,

170
00:20:11,840 --> 00:20:17,440
and there are data sets that are devoted to training and evaluating systems that can

171
00:20:18,880 --> 00:20:25,840
read two sentences and determine to some degree whether those sentences are paraphrases or not.

172
00:20:26,720 --> 00:20:32,320
What I did is I took one of those systems, and then I took a system that Shank and his students

173
00:20:32,320 --> 00:20:39,600
worked on at Stanford in the early 70s and I enhanced it, and what the system does is the system

174
00:20:39,600 --> 00:20:44,240
was called Margie, and what it could do was you could input some natural language, it would

175
00:20:44,240 --> 00:20:50,080
translate it over to this non-linguistic language-free conceptual representation, conceptual dependency,

176
00:20:50,800 --> 00:20:58,080
and then it could, based on its language-free thought representation of the original sentence,

177
00:20:58,080 --> 00:21:03,680
it could generate paraphrases of the original sentence. I enhanced the system so that it

178
00:21:03,680 --> 00:21:10,560
could generate a lot more paraphrases than it could in the 70s because in the 70s, the computers

179
00:21:10,560 --> 00:21:17,040
that people had and the systems that people had were very limited, and so I generated a set of

180
00:21:18,000 --> 00:21:27,600
tens of thousands of paraphrased pairs that meant the same thing, and I also tested that these

181
00:21:27,600 --> 00:21:33,200
pairs meant the same thing because I sent them to crowd workers and asked, okay, do these sentences,

182
00:21:33,200 --> 00:21:39,040
you think these sentences mean the same thing, and overwhelmingly they did. Then what I did is I

183
00:21:39,040 --> 00:21:44,000
took these pairs of sentences that all meant the same thing, and then I sent them to one of these,

184
00:21:44,960 --> 00:21:51,760
I didn't send them, but I used one of these deep learning paraphrase detection paraphrase

185
00:21:51,760 --> 00:21:57,920
recognition systems, and I asked this paraphrase recognition system, okay, do you think these sentences

186
00:21:57,920 --> 00:22:03,840
mean the same thing, and what was interesting about the sentences that my system generated was that

187
00:22:04,480 --> 00:22:14,400
they encompassed a wide range of linguistic variation, they were able to express the same idea

188
00:22:14,400 --> 00:22:21,120
using lots of different words and different syntactic structures. So you had sentences that

189
00:22:21,120 --> 00:22:27,280
meant the same thing that maybe only had one word in common, and syntactically they're very different,

190
00:22:27,840 --> 00:22:33,040
and humans looked at these pairs of sentences and said, yeah, those mean the same thing,

191
00:22:33,040 --> 00:22:38,000
the deep learning systems that were trained to do paraphrase recognition and detection,

192
00:22:38,720 --> 00:22:46,480
they fell off a cliff, when the sentences stopped using similar words, and you got to the point

193
00:22:46,480 --> 00:22:51,280
where the sentences were not using the same words at all, meant the same thing, the deep learning system

194
00:22:51,280 --> 00:22:56,480
was basically like a coin flip, half of the time I would say they were paraphrases, half of the time

195
00:22:56,480 --> 00:23:00,480
they would not, it was basically just guessing because it wasn't really understanding

196
00:23:01,680 --> 00:23:07,600
what the sentences meant. So in that way, I play the game a little bit.

197
00:23:07,600 --> 00:23:20,160
I was just going to push a little further on that and try to have you explain in that context,

198
00:23:21,040 --> 00:23:26,000
what are you benchmarking your systems results against, like is it?

199
00:23:29,600 --> 00:23:34,800
It's not necessarily trying to, well, maybe the goal of this research was to identify the

200
00:23:34,800 --> 00:23:44,560
deficiencies of the deep learning systems, and your metric was the number of paraphrases that they

201
00:23:44,560 --> 00:23:54,720
couldn't recognize or the performance, but I'm curious, do you ever compare the performance of

202
00:23:54,720 --> 00:24:00,240
your representations against the performance of a deep learning type of system,

203
00:24:00,240 --> 00:24:10,480
or are there other ways for you to understand whether the systems that you're able to create

204
00:24:10,480 --> 00:24:20,240
with your representations are more robust. It also does strike me that some ways maybe the metrics

205
00:24:20,240 --> 00:24:31,600
aren't rich enough, like envisioning a scenario where you're creating these paraphrases that

206
00:24:31,600 --> 00:24:38,800
are so much richer than what a deep learning model might create. And is the metric,

207
00:24:39,840 --> 00:24:45,440
have we created the metric for richness and what is that expressiveness? Maybe the traditional

208
00:24:45,440 --> 00:24:51,520
competitions aren't really judging the things that your representations are better at,

209
00:24:51,520 --> 00:24:58,080
but I'm also wondering broadly, if you opt out like how do you compare and how do you know

210
00:24:58,080 --> 00:25:03,600
when you're on the right track and when your research is getting you closer to understanding?

211
00:25:04,560 --> 00:25:13,040
Yeah, you raise a bunch of important questions. Let me try to address each of them in turn.

212
00:25:13,040 --> 00:25:18,720
Yeah, so in the study that I was describing, there are metrics that you can apply,

213
00:25:18,720 --> 00:25:26,240
but these are metrics that, well, not metrics, but we did statistical tests to determine

214
00:25:26,240 --> 00:25:38,960
that basically humans were better at recognizing that the sentences in this set of paraphrase

215
00:25:38,960 --> 00:25:44,240
pairs, that these paraphrase pairs, humans were better at seeing that these sentences meant the

216
00:25:44,240 --> 00:25:49,440
same thing, and we were able to perform statistical tests showing that humans saw that they meant

217
00:25:49,440 --> 00:25:56,080
the same thing, whereas the deep learning systems that we were testing, they basically declined

218
00:25:56,640 --> 00:26:04,080
in performance with greater degree of linguistic variation. You raise the important issue of,

219
00:26:04,080 --> 00:26:10,800
well, if linguistic variation is important, what are the measures for doing that?

220
00:26:11,840 --> 00:26:19,520
And I've developed some measures for linguistic variation such as you can do things like

221
00:26:20,320 --> 00:26:27,360
if you've got a corpus of text, if you've got two different corpore of text, you can do things like

222
00:26:27,360 --> 00:26:35,040
parse all the content, and then look at things like what were the part of speech tags that were given?

223
00:26:36,960 --> 00:26:42,320
What is the distribution of the part of speech tags that were given in this data set versus that

224
00:26:42,320 --> 00:26:48,960
data set? And I did that in a previous study where I was trying to better understand the differences

225
00:26:48,960 --> 00:27:00,080
between human caption generation and captions that were generated by the typical deep learning

226
00:27:00,080 --> 00:27:08,400
systems that people have used for caption generation problems. But I think it's something that needs

227
00:27:09,440 --> 00:27:12,880
studying these kinds of metrics is something that I want to work on in the future.

228
00:27:12,880 --> 00:27:20,240
There are projects that I'm working on now where I'm hoping I'll be able to actually build systems

229
00:27:20,240 --> 00:27:26,640
that, so I talked to you about this paraphrase generation system. I'm working on systems now that

230
00:27:26,640 --> 00:27:34,560
ideally can do the opposite. They can read and understand language and bringing into this

231
00:27:34,560 --> 00:27:42,080
bringing into this non-linguistic form. And we're using a data set from a data set called

232
00:27:42,080 --> 00:27:53,440
ProPara from the Allen AI Institute, I believe, and this data set has these texts that are about

233
00:27:54,720 --> 00:28:01,120
ProPara short for process paragraphs, and these texts are about things like geological processes

234
00:28:01,120 --> 00:28:05,920
and physical and chemical processes, and we're going to treat those like their stories. And that

235
00:28:05,920 --> 00:28:15,120
data set has tests that you can use to see if your system is really measuring up. So that is part

236
00:28:15,120 --> 00:28:24,480
of our plan. But I think the larger issue of having the right metrics is important.

237
00:28:25,840 --> 00:28:32,400
When it comes to judging these systems that perform paraphrase recognition, the metrics that

238
00:28:32,400 --> 00:28:38,000
are usually used are these bag of words metrics, like, for example, the blue metric. Basically,

239
00:28:38,000 --> 00:28:43,760
what they do is they take, they want to compare one text to another's text to see if they are

240
00:28:43,760 --> 00:28:50,640
kind of the same, and they just treat each text like a bag of words. The metric doesn't care

241
00:28:50,640 --> 00:28:56,560
about the order in which words come in and things like that, or you could have rearranged the words

242
00:28:56,560 --> 00:29:01,360
and totally, it could have meant something different. You're basically just trying to get your system

243
00:29:01,360 --> 00:29:12,400
to throw the right words in there that are relevant to the thing. You can say the same thing

244
00:29:12,400 --> 00:29:19,440
with different words, and then your metric doesn't really work anymore. But generally, this is an

245
00:29:19,440 --> 00:29:29,840
issue with the kind of research that people are doing where they just want to score better than

246
00:29:29,840 --> 00:29:34,960
the last person on that data set. And so they don't really care what the metric is very much,

247
00:29:34,960 --> 00:29:38,160
or whether the metric means anything. All they care about is their score.

248
00:29:42,720 --> 00:29:48,960
In some ways, your research makes me think of folks like Josh Tenenbaum at MIT.

249
00:29:50,400 --> 00:29:56,720
His is, I think, less natural language focused and more kind of visual. At least, I think about

250
00:29:56,720 --> 00:30:03,920
it like that. And a lot of what he'll talk about is trying to kind of capture and understand

251
00:30:03,920 --> 00:30:11,680
this idea of common sense, and, you know, external opportunity knowledge or things like that.

252
00:30:12,800 --> 00:30:17,680
Is that part of what you're looking to understand in your research and the language domain?

253
00:30:17,680 --> 00:30:27,840
Yeah, yeah. I'm flattered that you mentioned Josh Tenenbaum when talking about my work,

254
00:30:27,840 --> 00:30:32,160
and then you think of Josh Tenenbaum. I'm very honored that you would say that.

255
00:30:32,720 --> 00:30:38,240
And, yeah, I think that we are interested in many of the same issues.

256
00:30:39,760 --> 00:30:47,040
I'm trying to remember the term that Josh Tenenbaum uses. I can't remember it at the moment,

257
00:30:47,040 --> 00:30:52,160
but I do believe that from what I've heard him talk about, that he is interested in

258
00:30:52,800 --> 00:30:57,840
trying to figure out what the most abstract conceptual primitives or

259
00:31:00,400 --> 00:31:05,120
structures that people are using to kind of break things down and understand them.

260
00:31:05,920 --> 00:31:11,200
I think he's what I've seen. I've seen him do some interesting and important work involving

261
00:31:11,200 --> 00:31:19,360
children and their interactions with people around certain kinds of tasks that involve

262
00:31:19,360 --> 00:31:27,120
reasoning and social interaction and things like that. Yeah, I think it's a great important work.

263
00:31:28,640 --> 00:31:35,680
And so that's something I'm interested in, too, is if you believe like me that there are some

264
00:31:35,680 --> 00:31:43,280
structures that some cognitive structures that people evolve or, let's say, people develop

265
00:31:43,280 --> 00:31:47,600
at a young age before they even learn language. If you're going to say that they're these kind of

266
00:31:47,600 --> 00:31:54,160
pre-linguistic representations that people are using in their building language on top of those

267
00:31:54,160 --> 00:31:59,680
representations, you'd be interested to know what those are. And I think there are folks like

268
00:31:59,680 --> 00:32:07,280
Josh Tannenbaum who are trying to get it what that is or what those things are.

269
00:32:08,560 --> 00:32:14,800
But perhaps not so much doing it through language. You can see I'm trying to do it

270
00:32:15,360 --> 00:32:19,520
through these tasks that have to do largely to do with language and texts.

271
00:32:19,520 --> 00:32:32,320
Yeah. In the kind of traditional deep learning approach, like the knowledge of the system is

272
00:32:32,320 --> 00:32:37,760
kind of stored in like weights and embeddings and things like that. It's kind of inherent in the

273
00:32:37,760 --> 00:32:49,600
model. Sounds like your representations are more external. I wonder if there's anything you can

274
00:32:49,600 --> 00:33:00,320
elaborate on there. Does your work have this kind of traditional view of a model? How does that

275
00:33:00,320 --> 00:33:07,680
relate when you're building systems around your representations? What do they look

276
00:33:07,680 --> 00:33:12,240
like? Are they similar to what we might be used to with deep learning and machine learning?

277
00:33:12,240 --> 00:33:20,720
Or are they very different? In many ways they would look like structures,

278
00:33:21,600 --> 00:33:29,920
kinds of models and systems and structures that were way more popular perhaps in the 1970s

279
00:33:29,920 --> 00:33:37,840
in 1980s when people would call symbolic artificial intelligence, good old-fashioned artificial

280
00:33:37,840 --> 00:33:48,080
intelligence. Some people characterize them also as rule-based systems. I've been building

281
00:33:48,080 --> 00:33:56,320
a lot of systems by hand. Are you okay with all of those terminologies or are you

282
00:33:56,320 --> 00:34:03,760
air quoting it because you don't really like those terms? Well, I'm air quoting it because

283
00:34:03,760 --> 00:34:15,280
if you say symbolic AI, I have no problem with that. I guess the reason why air

284
00:34:15,280 --> 00:34:24,240
quoted is because people have negative associations with those kinds of technologies where you are

285
00:34:24,240 --> 00:34:31,840
building things by hand. Historically, people know that there have been ups and downs in artificial

286
00:34:31,840 --> 00:34:38,640
intelligence. AI winters, as they're often referred to, and there were AI winters that were

287
00:34:38,640 --> 00:34:44,400
associated with people building rule-based systems, symbolic systems where they were actually

288
00:34:44,400 --> 00:34:58,080
coding things by hand instead of using machine learning. My view is that there's a general

289
00:34:58,080 --> 00:35:05,120
negative association with doing any sort of building systems by hand. I think part of it is because

290
00:35:05,920 --> 00:35:11,920
the old AI tradition people thought about this idea that you'd have expert systems that were

291
00:35:11,920 --> 00:35:19,280
able to replace people completely and so what people came to realize is man, there's so much

292
00:35:19,280 --> 00:35:24,080
that people know and so much that I would need to encode in my own that there's no way I would

293
00:35:24,080 --> 00:35:30,000
ever be able to, if people were working around the clock, lots of them just encoding this knowledge

294
00:35:30,000 --> 00:35:36,160
by hand, it's impossible that we'd ever be able to build a real system. Notwithstanding

295
00:35:36,160 --> 00:35:45,600
double-enit and psych and projects like that, but my point of view is different. I'm not building

296
00:35:45,600 --> 00:35:52,640
systems by hand in the hopes that I would eventually be able to build an artificial general

297
00:35:52,640 --> 00:35:57,520
intelligence that I was coding by hand and I would eventually be able to encode all of the knowledge

298
00:35:57,520 --> 00:36:05,600
that people have, but I'm using symbolic systems, building systems by hand, or you can say,

299
00:36:05,600 --> 00:36:10,320
you can call them rule-based systems if you like to, if it's not too pejorative. I mean,

300
00:36:10,320 --> 00:36:17,920
the reason why I build these systems is to help try to understand what's going on, what the

301
00:36:17,920 --> 00:36:25,520
representation should be so that when we turn to say, okay, we're going to now use machine learning

302
00:36:25,520 --> 00:36:32,160
to try to build systems to do these tasks, we have better ideas. Instead of kind of what I see

303
00:36:32,160 --> 00:36:41,040
today is, there's deep learning, right? And it feels people, it seems like people think that

304
00:36:41,040 --> 00:36:46,720
deep learning can do anything as long as you supply enough data to it. But then the question is

305
00:36:46,720 --> 00:36:53,360
what kind of data should we supply to it? If we just supply text, is there stuff that we know

306
00:36:53,360 --> 00:36:59,520
that helps us understand text that is outside of the text itself? And do we also need to supply that?

307
00:36:59,520 --> 00:37:10,720
No. And so those, that's why I'm not opposed to building systems by hand. And in some of these

308
00:37:10,720 --> 00:37:16,880
studies, what I've done is I've built systems by hand that are able to say, for example, generate

309
00:37:16,880 --> 00:37:26,640
lots of paraphrases as an example and create a data set like that that demonstrates, okay,

310
00:37:26,640 --> 00:37:31,680
maybe these are the kinds of representations we should be aiming for in our deep learning,

311
00:37:31,680 --> 00:37:38,320
machine learning systems. I think that the new target, so right now the way things are is that people,

312
00:37:39,680 --> 00:37:43,840
people build these data sets say, for example, paragraphs, questions, and answers,

313
00:37:43,840 --> 00:37:47,920
and the target is just try to get the deep learning system to give the right answer.

314
00:37:47,920 --> 00:37:55,520
In my opinion, the new target should be, should be that you should be supplying data sets that

315
00:37:55,520 --> 00:38:01,440
help deep learning machine learning systems build the right representations. And those representations

316
00:38:01,440 --> 00:38:07,120
might be non-linguistic or language free. And in turn, those representations help you get the

317
00:38:07,120 --> 00:38:12,640
right answers. And then you can see, oh yeah, this thing is really kind of thinking the way I want

318
00:38:12,640 --> 00:38:18,080
it to think. Whereas right now it feels like deep learning is just giving us a black box

319
00:38:19,280 --> 00:38:24,800
where we can't quite see inside and see whether the representations it's building to solve the

320
00:38:24,800 --> 00:38:32,160
task, making sense. Yeah, that the way you describe that resonates really strongly with the

321
00:38:32,160 --> 00:38:38,560
conversation I had just the other day with Peter Abiel, a research out of Berkeley focused on

322
00:38:38,560 --> 00:38:47,600
robotics. And we were kind of comparing and contrasting his views as an academic thinking about

323
00:38:47,600 --> 00:38:53,280
kind of end-to-end deep learning. And you know, as you said, just throw enough data at a problem.

324
00:38:53,280 --> 00:39:02,080
And his more evolved views as an entrepreneur and a roboticist that's trying to solve problems

325
00:39:02,080 --> 00:39:13,600
for for companies. And the need to try to capture and incorporate knowledge that we have about

326
00:39:13,600 --> 00:39:19,040
these these problems. And he made this really interesting point that, you know, one, you know,

327
00:39:19,040 --> 00:39:23,920
if you say, okay, we want to incorporate, you know, this knowledge that we have about a problem,

328
00:39:25,040 --> 00:39:31,440
you know, one way to do it is to, you know, build rules into your system, you know, build your

329
00:39:31,440 --> 00:39:39,440
if-then statements or whatever that looks like. But what he's said that struck me as really interesting

330
00:39:39,440 --> 00:39:47,440
was another way to do that is to use your rules to generate more data for your deep learning systems.

331
00:39:47,440 --> 00:39:58,000
And so in that way, you're kind of training them on the cases that you know a lot about. But still

332
00:39:58,000 --> 00:40:06,160
not having to, you know, not having to take on the technical debt, if you will, of having a lot

333
00:40:06,160 --> 00:40:11,440
of rules, you know, the brittleness of rules, that kind of thing. And it strikes me that maybe your

334
00:40:11,440 --> 00:40:15,920
systems could be used in a simpler way. Like you've demonstrated the ability to create these really

335
00:40:15,920 --> 00:40:24,800
robust paraphrases that, you know, could be really interesting augmented data for a paraphrasing

336
00:40:24,800 --> 00:40:30,160
system that you might want to train. Maybe that is the kind of glue between your world and the deep

337
00:40:30,160 --> 00:40:37,760
learning world. Yeah, that is that is one way. I mean, so anytime you come up with an adversary,

338
00:40:37,760 --> 00:40:44,240
anytime someone comes up with a with anything that shows that the hey, the deep learning system

339
00:40:44,240 --> 00:40:50,960
is really isn't really doing what you want. Then the natural, you know, what the what the folks

340
00:40:50,960 --> 00:40:56,640
who are doing big data, the machine learning will just say, I think many of them will probably

341
00:40:56,640 --> 00:41:02,880
say is, well, okay, just give me the data that you just created that made my system. Give me those

342
00:41:02,880 --> 00:41:08,480
examples and I'll just feed them in. And then my system will, well, then be able to handle those

343
00:41:08,480 --> 00:41:16,880
kinds of will be able to handle those adversaries and will be better as a result. That's that's one

344
00:41:16,880 --> 00:41:24,320
way of looking at it. But then from from my standpoint, it's it's, you know, I can I can continue

345
00:41:24,320 --> 00:41:31,120
doing work on on these kinds, these these representational issues. And, you know, perhaps generating

346
00:41:31,120 --> 00:41:43,120
generating data. But I think I think generally, yeah, I think I think generally it's it's one one

347
00:41:43,120 --> 00:41:49,120
way to interface between those communities. I should say, though, also that I think part of the

348
00:41:49,120 --> 00:41:59,120
reason part of the reasons why the the in the export systems era that the rule based systems

349
00:41:59,120 --> 00:42:06,800
people had had a lot of difficulty was that they were using in many cases, in many cases, perhaps

350
00:42:06,800 --> 00:42:13,680
they're using certain kinds of logical inference engines. And I think the issue with logic is that

351
00:42:13,680 --> 00:42:19,280
people what people mostly tend to do is they create logical symbols that correspond to words. They

352
00:42:19,280 --> 00:42:25,760
don't necessarily create symbols in those systems that correspond to non linguistic conceptual

353
00:42:25,760 --> 00:42:31,200
representations that I think we have. And so I think if we're if we're creating knowledge structures

354
00:42:31,200 --> 00:42:37,680
and reasoning systems that have more of that non linguistic language free abstract primitive

355
00:42:37,680 --> 00:42:46,320
decomposition stuff that that we could we could do much better even even if we're even if we're

356
00:42:46,320 --> 00:42:50,640
not going to use machine learning at all. If we were just back to building export systems like we

357
00:42:50,640 --> 00:42:57,600
were in the early, well, I suppose in earlier decades, if we're back to building those

358
00:42:57,600 --> 00:43:03,040
export systems, if we're using better representations, those systems could have been better too. You know,

359
00:43:03,040 --> 00:43:08,720
maybe we would not have failed and had a had AI winters the way we way we did.

360
00:43:12,800 --> 00:43:17,760
What are, you know, kind of looking forward? What are you most excited about in terms of

361
00:43:17,760 --> 00:43:27,040
directions for your research? Yeah, so the systems that the these representational systems that

362
00:43:27,040 --> 00:43:38,880
they keep talking about for the time being they're we find that things are fairly straight much

363
00:43:38,880 --> 00:43:44,160
more straightforward when you're talking about things that are happening in the physical world

364
00:43:44,160 --> 00:43:50,800
like Mary kicking the ball being decomposed into someone's you know Mary's foot moving and striking

365
00:43:50,800 --> 00:43:59,120
the ball and then the ball moving and things like that. One thing that even even in earlier decades

366
00:43:59,120 --> 00:44:03,360
where they were working on this research of de you know trying to represent trying to come up with

367
00:44:03,360 --> 00:44:09,520
these non linguistic representations that they never I in my opinion they didn't really get a good

368
00:44:09,520 --> 00:44:17,120
handle on and and this is also true if you read scripts plans goals and understanding was well

369
00:44:17,120 --> 00:44:23,360
how do you how do you decompose the idea that someone should have a goal or how do you decompose

370
00:44:23,360 --> 00:44:33,760
the idea that someone should have a plan? How do you decompose a decision or or or other kinds

371
00:44:33,760 --> 00:44:41,840
of activities that involve thoughts? And they had a I feel like they got stuck back in those days

372
00:44:41,840 --> 00:44:47,360
and didn't make much progress in in figuring those things out. They in many cases created

373
00:44:49,440 --> 00:44:55,840
more and more diverse kinds of structures without doing the decomposition into primitive thing

374
00:44:55,840 --> 00:45:03,600
that I think made their work in the early 70s more cool versus their work in the mid mid 80s

375
00:45:03,600 --> 00:45:11,200
or so. And so that's something that I'm in future work that I'm then curious about interested in

376
00:45:12,720 --> 00:45:19,280
again about scripts plans goals and understanding one of the one of the important basic ideas from

377
00:45:19,280 --> 00:45:28,480
that book is that some of or some important reasoning involves from your episodic memories.

378
00:45:28,480 --> 00:45:35,680
So remember the restaurant script the idea that you tell this story about Mary going into the

379
00:45:35,680 --> 00:45:42,400
restaurant or you know she she eats the lobster and leaves and you're able to reason that

380
00:45:43,120 --> 00:45:48,640
the she ordered the lobster from the server and all these other things and those knowledge

381
00:45:48,640 --> 00:45:53,360
structures are built out of or at least theoretically the theoretical idea is that those

382
00:45:53,360 --> 00:45:58,240
knowledge structures are built out of your episodic memories of your experiences with restaurants.

383
00:45:59,280 --> 00:46:06,720
So you start wondering well you know what if what if other kinds of reasoning actually work that

384
00:46:06,720 --> 00:46:13,760
way where you can you can build lots of script structures representing people's common experiences

385
00:46:13,760 --> 00:46:19,680
and that's how people do a lot of their reasoning and a lot of their reasoning may involve scripts

386
00:46:19,680 --> 00:46:30,400
combining with each other to reason about unusual events such as you know what happens when you

387
00:46:30,400 --> 00:46:36,320
have a birthday party at the restaurant you combine the birthday party script with the restaurant

388
00:46:36,320 --> 00:46:40,480
script and then you start reasoning about things like well if so and so pays for my dinner is that

389
00:46:40,480 --> 00:46:48,000
considered a birthday gift at the restaurant something like that. And so that's that's something

390
00:46:48,000 --> 00:46:55,200
I'm really excited about is learning more about general general reasoning through these structures

391
00:46:55,200 --> 00:47:03,120
that are meant to represent people's episodic memories just people's experiences rather than saying

392
00:47:03,120 --> 00:47:10,560
well it must be first-order logic or or I guess on another you know if you take it the other way

393
00:47:10,560 --> 00:47:16,640
it must be deep learning is the only way or something like that so you know can you can you

394
00:47:16,640 --> 00:47:25,680
start building databases of scripts or or databases of episodic memories and start using those as

395
00:47:25,680 --> 00:47:31,120
the basis for structures for reasoning and things like that so those are those are just a couple

396
00:47:31,120 --> 00:47:35,280
a couple of examples of things that I'm really excited about in the future.

397
00:47:35,920 --> 00:47:43,040
Awesome awesome well as always we will link to your website and some of your recent work on the

398
00:47:43,040 --> 00:47:50,640
show notes page that is will be available when the episode is published. Thanks so much for

399
00:47:50,640 --> 00:47:56,640
taking the time to share a bit about what you're up to. Thank you Sam thank you so much everybody

400
00:47:56,640 --> 00:48:13,200
for having me.

