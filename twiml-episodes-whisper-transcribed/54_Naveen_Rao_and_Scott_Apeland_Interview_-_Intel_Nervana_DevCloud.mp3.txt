Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
A few updates before we jump into our show.
This week, October 10th through 11th, I'll be in Montreal for the rework deep learning
summit.
We held a flash giveaway last week for one lucky listener to get a chance to join me
at the conference.
And that winner is Matt Stone, a listener from Toronto.
Thanks to everyone who entered and be on the lookout for more giveaways to close out
the year.
Hey, if you're in the Montreal area, I'd love to connect.
Reach out to me via Twitter or the contact page at twimlai.com if you'd like to meet
up.
The show you're about to hear is the first of a series of shows recorded in San Francisco
at the Artificial Intelligence Conference, which was hosted by our friends at O'Reilly
and Intel Nirvana.
In addition to their support for the event itself, Intel Nirvana is also our sponsor for this
series of podcasts from the event.
A huge thanks to them for their continued support of the show.
In this episode, I speak with Navine Rao, Vice President and General Manager of Intel's
AI Products Group and Scott Appland, Director of Intel's Developer Network.
It's been a few months since we last spoke with Navine, so he gives us a quick update on
what Intel has been up to and we discuss his perspective on some recent events in the
AI ecosystem.
Then, Scott and I dig into Intel Nirvana's new dev cloud offering, which was announced
at the conference.
We also discussed the Intel Nirvana AI Academy, a new portal offering hands-on learning
tools and other resources for various aspects of machine learning and AI.
In addition to my conversations with Navine and Scott, this show is packed with more interviews
that I know you'll love, including Paul Tepper of Nuance, on conversational interfaces,
Gunner Carlson of Stanford and IASD on topological data analysis, Laura Freilich and Mo Patel
of Think Big Analytics on some interesting machine learning use cases they've implemented,
and Ian Steuiger of UC Berkeley on his work at the Ryze Lab and Ray, a new platform for
reinforcement learning.
Finally, a quick reminder about the upcoming Twimble Online Meetup.
On Wednesday, October 18th, at 3pm Pacific time, we'll be discussing the paper visual
attribute transfer through deep image analogy by Jing Li Yao and others from Microsoft Research.
The discussion will be led by Duncan Stothers.
To join the meetup, or to catch up on what you missed from the first two meetups, visit
twimbleai.com slash meetup, and now on to the show.
All right, everyone.
I am here at the AI conference brought to you by O'Reilly and Intel Nirvana and I'm here
with Naveen Rao, who is the general manager of Intel's Artificial Intelligence products
group.
Welcome, Naveen.
Thanks, Sam.
Great to be here.
It's great to have you here.
We're going to spend a few minutes this morning just catching up on what Intel Nirvana
has been up to, and then we've got a really interesting interview schedule to talk about
one of the products that you guys are announcing here at the event.
Sounds great.
What's new?
Well, it's been a very eventful few months.
We started the Artificial Intelligence products group about five months ago now, and really
getting resources around Intel aligned around this focus area now.
The company has been super supportive from the top down to make this happen, and I think
it's a reflection of the importance of AI to Intel.
We want to be at the forefront of it, not just from the product standpoint, but also
from the research standpoint.
And so you made some announcements at the event today.
Tell us a little bit about those.
A lot of it is around our connection with developers.
The Dev Cloud was one where we can connect with developers, get them working on our tools,
but also training people.
There's a big appetite right now for knowledge and understanding how to build AI solutions
and how to actually solve industry problems.
We put together a very nice set of tutorials in Tel Nirvana Academy, and also having computational
support for those tools and actually computational resources available is what the Dev Cloud is
about.
And you know, that'll actually go hand in hand with Intel Nirvana Cloud coming up in the
future where you can get access to the latest and greatest technologies.
Okay.
So where can folks find the tools that you, the documentation and tools that you released
from Intel Nirvana.com or just Google Intel Nirvana Academy?
Okay.
Awesome.
And so is the Dev Cloud?
What's the relationship between the Dev Cloud and Intel Nirvana Cloud that's forthcoming
to the Dev Cloud?
The Dev Cloud is really meant for developers and education, whereas the Intel Nirvana
Cloud is actually the enterprise grade solution for companies.
So you know, one is more, here's access to our tools and a package format.
The other thing is the Intel Nirvana Cloud is really our latest technology and a very kind
of low cost way of getting access to it.
Okay.
And then you've got a keynote tomorrow that you're doing with Steve Jarvison.
What are you going to be talking about there?
Well, as you know, Steve may have been, as you may know, Steve was one of our early investors
at Nirvana and has really become an industry expert in this area.
And you know, I find it very fortunate to have him involved with us from the start guiding
some of our direction.
And it'll really just be a conversation between us talking about some of the things that
are exciting coming up, you know, how hardware is going to evolve for the future of AI and
drive the future of AI.
What is the future of AI?
What kind of problems will you be solving?
Why is it important?
It's really going to be a conversation and I think it should be entertaining.
Oh, what is the future of AI?
I'm going to save that for tomorrow.
Well, I promise you no one will hear this before tomorrow.
Okay.
I think one of the main messages from my perspective is AI is really a set of techniques
that allow us to scale intelligence.
We can now take in more resources that we could possibly do as individual humans, you
know, loosely coupled by language that we're hitting that fundamental limitation now
because we've gotten really good at gathering data.
And now we have too much data and we actually can't do much with it as individuals.
Humans can't scale that level.
So we need to build technologies to allow us to do it.
And it's really kind of continuing along the evolution of the scientific method.
Humans forms this procedure to really make testable, repeatable results and try to get
to the ground truth of the world.
And I think this is part of that, right?
We just want to do it on a bigger scale.
You know, one human can only experience so much, but we can actually now put devices
out in the world that can experience a lot and bring it all together.
So that's really what I think AI is about and why it's important.
It's really allowing us to continue along the evolution of what it means to be human.
Mm-hmm.
And one of the things that came up in the key notes this morning was, it's related
to a comment you just made.
We've become really good at gathering and collecting data, but not necessarily so good at collecting
label data sets that are trainable to create machine learning models and AI models, is
Intel doing anything in that space at all?
Yeah.
So, I mean, there are three fundamental types of learning.
There's what you just described would be called supervised learning.
This is really about taking data that's already been annotated by human and saying this
is what we want to get out of a model.
Reinforcement learning is more like kind of how you train your dog.
Dog gets something good, you give him a treat, and he does that good thing more often.
The other, I would say, the big frontier right now is unsupervised learning.
Right.
And that's really about finding potentially useful structure and data before you know
what you want to do with it.
And that's really what needs to be cracked to enable this kind of scalability, because
actually going through and labeling all the data in the world is not possible.
Right.
Just if there's too much already, if we froze the world today and handed out 100 megabytes
to every man, woman, and child on the planet, we would take us 30 years to get through
all of it.
So, you know, that's not something we can, that's possible.
The only way to do it is to crack it from an algorithmic standpoint and then throw
the computational horsepower at it that we're building.
So that's, I think, one of the big, big things we're going to see in the next couple of
years is research into the unsupervised training world.
And we are doing that as well and providing tools, so our researchers do move this field
forward.
Are there any particular examples of progress in that space that comes to mind for you,
or that you've helped, your tools are helping facilitate?
I think GANs, generative adversarial networks are a big, big one.
This is something that started becoming popular about a year, year and a half ago, showing
some really promising results now.
And this is towards more unsupervised kind of method.
What our tools have helped there is really the speed.
GANs are computationally intensive.
They take a lot of horsepower, and a lot of time, and, you know, four years ago, you
simply couldn't do it.
You just didn't have the tools and computational capabilities that we have today by providing
better tools faster, more scalable, larger parameter memories, these kinds of things.
We can actually drive these fields forward.
And that's the way I see it as, like, when we put out a new architecture that has fundamental
and new capabilities, smart researchers are going to do things with it that we had never
even thought of.
And that's what I think is really cool.
It's not the stuff that we're thinking about, that should work, but it's other stuff
that's going to happen.
Yeah.
Very interesting.
One of the things I remember from our last conversation was we were talking about Intel
and Intel's role in the space, and you made a comment to the effect that, hey, you
know, we're in the first inning, is that, right?
And in the context of tools, I think last time we talked a little bit about the support
for Nervonograph and some of the products projects that were announced last time at the AI
conference in New York.
And a lot of the conversation was around support for TensorFlow as kind of one of these
back-end frameworks.
And interestingly enough, like, I think, you know, a few months ago, everyone thought
everyone thought that TensorFlow was like the Crown King and the game was over.
And now, like, out of nowhere, we're talking about PyTorch all over the place.
And I'm just going to go there.
I'm just wondering if you have any, you know, thoughts or perspective on, you know, the
market and how it will evolve and what we should expect to see.
You know, you can look at the past.
We saw the same thing happen with web technologies, right?
Or the same thing happening.
There's a new web technology to do responsive dynamic web pages out every year.
I can't keep up because I don't do that stuff anymore.
So I think you're going to see a very similar thing happening here when there's, you know,
a new set of capabilities that kind of catapult some set of researchers, everyone was going
to want to use it.
Right.
TensorFlow was kind of favored for the last eight months, nine months.
We'll see what happens in the future.
I think there's plenty of room for innovation here.
It's not a done deal by any means.
I think I even said that then.
It's like, yeah, I've seen this happen.
I've seen this game before.
And, you know, we'll support what's out there, of course.
You know, we want to provide innovations through our own software stack that we own and open
source neon on top of end graph, but we obviously want to support what the community is using
more broadly.
And if that's pie towards great, if it's TensorFlow, great too, we have no problem with
it.
I think, you know, we're providing that computational substrate under the hood.
And so we want to make sure that researchers have access to what they need.
More broadly, what is the role of ecosystems and enabling AI solutions and AI tools?
To evolve ecosystems are absolutely key, right?
There's not one person who can do this on their own, right?
It's just not going to happen.
Not even one single big company, I think.
There's too many smart people working on this problem to not work together in a common
set of tools in a common language, right?
So the way the big problems are going to solve, like, some supervised problems, things
like that, is really through this openness of publishing, you know, people putting stuff
on archive is great.
But then having peer review and, you know, papers accepted by NIPs and ICML, these big
conferences, and then providing the code to actually show the implementation.
That's been a very virtuous cycle, I think.
We've seen a lot of innovation happen quickly.
You get something out there, the codes out there, you can download it, play with it, modify
it, and then put out the next thing on top of it, right?
I think it's actually a great paradigm for any kind of innovation.
Yeah, absolutely, absolutely.
Anything else you'd like to share with us?
Yeah, I think, you know, it's going to be very exciting in the next year or so.
We're going to be making a lot of announcements about some of the things coming to fruition.
I mean, we're making a lot of bets right now.
You know, we've come from a place where Intel is a CPU manufacturer, and that's how people
are perceiving it a year ago, and now we're going to be providing leadership in AI.
I think it's very exciting to see that transition and be part of it.
So stay tuned for some cool things coming up.
Along the line of some of those bets, you recently, or Brian recently published a blog
post that talked about the billion dollars that the company is investing across the AI
spectrum.
Any comments on that?
Yeah, there were a number of startups mentioned and some other things.
Yeah, I mean, Intel is a huge, if not the biggest VC in the valley.
So we invest in a lot of different kinds of companies.
I mean, obviously, we look at it from strategic value standpoint, but also just from a, you
know, like a monetary return, just like a VC.
So we've placed a lot of bets there, and, you know, we want to see the ecosystem to build
and innovate, and startups will wear a lot of that happens.
Then beyond that, some of that investment is also internally what we're doing in aligning
our resources around.
So I think it's, again, it's a really good readout of the emphasis and focus we're putting
on AI at Intel.
Yeah.
Absolutely.
Well, great.
Thanks so much, Naveen, for joining us and looking forward to catching up with you next
time.
Absolutely.
Great talking with you.
Great.
All right.
Everyone, I'm here at the AI conference with Scott Appland, who is the director of developer
programs with Intel Nirvana, and we're here to talk about the dev cloud that was launched
today.
Welcome, Scott.
Well, thank you.
Good to be here.
Great to have you.
So it sounds like you had a big launch last night.
How did it go?
Really went well.
So back about 10 months ago in November last year, we announced our AI Academy.
So this was our program for helping students, developers, teachers really have more, learn
more about AI, how to use it, get access to the technology and the tools.
So we kicked that off.
Today, we really announced the next stage of that, which is making cloud compute accessible
to broad set of developers and students with our new Nirvana dev cloud.
Okay.
So tell us a little bit about the dev cloud and what the focus is there.
And in particular, you know, compute, unlike a few years ago, compute is much more readily
available on various public facing consumer clouds.
What's different about dev cloud?
Yeah.
Well, you know, one of the challenges for folks getting started in AI is actually having
access and being able to get started with compute in a way that's economical for them
without having to invest in it.
So for all of our Academy members, they'll have free access to the dev cloud.
And this is a, yeah, this is a very large scale cluster and will have the latest Xeon scalable
processors on there.
And developers can sign up, they'll be, they can use it to sandbox, new projects they're
working on, they can use it for their homework exercises they're doing in class.
They can do just test out things or if they have a compelling project, they really want
to get started and use it as Academy members, they sign up, they'll be given access.
They can start for four weeks and use it for four weeks and then at the end of the four
weeks, if they need it longer, then post the project they're working on.
We'd like to see what they're working on and let other developers see what they're
working on and they can get extended for another four weeks or even longer if it's necessary.
So in this way, they're getting access to as much cloud compute as they're going to need
for quite a while to get them up and started.
It's not for commercial production type of applications, but a great way to get started.
Oh, great, is there use of the dev cloud limited to, it doesn't sound like it's limited
to just exercises that are part of the Academy, they can do any project that they come up
with.
Yeah.
Let me just spend a few more minutes talking about the Academy what it is and you can
understand how they use the dev cloud.
So the Academy includes a lot of learning resources.
So tutorials, online classes, webinars, webcasts, from basic getting started, machine learning
101, deep learning 101, and intermediate to advanced.
So a really good curriculum to learn about AI.
But if you're already a professional developer and you just want to say, let's use the latest
software that's optimized, let's maybe use neon framework or some other new Intel
technology, you'll have access to that in the Academy and then you'll have technical
support.
So we have a real mix of professional developers, students are getting started, graduate students
are working on research projects and they can all use the dev cloud.
For example, some of, so just last night we had a dev jam and here in San Francisco is
kind of a day zero of the O'Reilly conference and we had about 500 developers, students,
startups attend this and on the stage I did a fireside chat with six of our students
who are working on projects.
And the types of projects that we're going on were just amazing, especially when you look
at the variety.
So one of them was doing a project on epilepsy and he was doing EG scans and connectivity
analysis of the brain and using that to help patients manage epilepsy and predict seizures
and things like that.
So he's working on just a fascinating project there.
Another student was doing a trail cam project.
So if you're out in the wild and you have a trail cam, not only does it turn on record
when a wildlife passed by, but it will try to recognize what type of wildlife that is
and a lurch.
So if it's a day they're run, yes it's a dangerous one and tell the campers nearby time to
get out of there.
Another student is working on mosquito detection and identification because mosquitoes there's
thousands of varieties, but there's only a small number that are real dangerous that
maybe carry malaria or Zika.
So with his phone, he does create an application where it would take a photo, recognize what
type of mosquitoes and tell if it's a dangerous type.
Interesting.
So that's just a sample, oh another fun one that I liked was a student from Rutgers who
has developed an application where it will scan your head, look at your facial structure,
your head structure and then based on popular hairstyles that you have similar facial structures
recommend hairstyles for you and maybe a beard style as well.
So you never have to worry anymore about what you should get to haircut, just have this
app tell you exactly what your hair should look like and go get a cut like that.
Oh wow.
Quite a variety of things that they're working on and they have access to the dev cloud
and can and to our technical support as well because we announced also today that we have
a partnership with Tata Consulting Services to put in place an AI center of excellence.
And we're going to leverage TCS's expertise in AI and use that to help to support the
Academy members.
So when they need to get stuck on a project or using the dev cloud, we'll have special
engineers who can support them and provide the support to help them and they'll be located
around the world because TCS is a worldwide organization.
Now TCS is a consulting company it's hard to get a consulting company to do anything
without dollars changing hands like if I'm an Academy member and I run into something
and I need some help like how does that do I raise my hand and yeah.
So the beauty of this is for the Academy members it's free.
Of course there's a business model for Tata Consultancy in this as well but for Academy
members, both the dev cloud and the support and the tutorials and training it's all free.
And our desire is to help as many students, developers get smart, learning and discover new
ways of using it as possible.
And then is there a mechanism whereby if I say I'm working on something it starts out
as a little project, side project in the corner of the office or something like that and
then it grows into something that's more important for my company, like to get more help.
Yeah, we do look for that.
So we're always kind of monitoring and watching for those really cool things to emerge and
see how we can help them be more successful.
A lot of times they'll start to attract all kinds of support and interest in general but
early on it's really helpful that we can say hey, this guy's got something going here.
Let's give him a little extra boost so we're looking for that constantly and on my team
what we've developed to do this is a program called Student Ambassadors.
So since we rolled out the Academy, we've been going out to universities worldwide and
run workshops, AI workshops and we'll introduce technology to give them the basics and then
we'll hear what the students are working on and those students are really passionate
doing something really cool, we'll see if they want to become ambassadors for Intel.
And if they're interested then we'll give them even more training, more access to technology
and in return just ask them to go tell other students about what they're doing and share
their knowledge.
So these six that were on the stage with me last night were ambassadors, one from UC Santa
Barbara, one from MIT, one from Rutgers, one from let's see ASU and that's just the US.
We started to have ambassadors all across the world from India, China, lots of them in
Europe as well and I'd say those are the cream of the crop that we're really monitoring
to see what cool things are they going to come up with.
And if someone's listening and wants to get involved in the ambassador program, is there
a mechanism for signaling their interests or do they just get involved in the Academy
and do cool things and you'll figure you'll find them?
No, they actually can apply.
So online at the Academy you can just search on Intel Nirvana Academy online and you'll
find it and then there's a apply to become a student ambassador.
And you can fill out what you're working on, why you want to be an ambassador and then
we'll have someone follow up to do an interview and see if you're ready for that.
Okay, so interesting, sounds like the DevCloud is supported by some pretty interesting programs.
In terms of the DevCloud itself, if I'm an AI developer and I'm coming to, I discover
the Academy and DevCloud and have an existing tool chain instead of tools that I'm using,
will those work on the DevCloud or do I need to port what I'm doing to the Intel Nirvana
tool stack in order to use the DevCloud?
There's a really good chance that they will already be preloaded on the DevCloud.
So the DevCloud is not just for specific frameworks.
It's really for, first of all, the Intel Xeon scalable processor is our hardware platform
and that will extend that when we have the new flavors of Nirvana technology come out.
So we'll keep building that hardware.
But on the software side, it's whatever software that will run great and that developers
and students want to use.
So today, the DevCloud supports Neon, Spudges, TensorFlow, Spudges, Cafe, Spudges,
Deano, Keras, most all the popular frameworks.
And then Intel spent a lot of time optimizing them to run well on CPU.
If you go back a couple of years ago, the frameworks ran great on GPUs but not so well on CPUs.
We put a lot of investment over the last nine months or so and seen the performance improve
up to 100X on running those frameworks on CPUs.
So now we have optimized frameworks, pretty much the choice of the user gets to pick that
and then he'll get 200GB of secure storage area for his files and load up his, you know,
queue up his project and then we'll run it in the batch mode and then he gets notified
when it's ready and can do it again.
And so I suppose there are no GPUs in the DevCloud.
No, they were not necessary.
And so you talked about some of the performance metrics that you've seen recently.
These are for this 100X this improvement over time.
This is for training or inference or both.
That's for training really.
And you know, deep learning, first of all, machine learning, the vast majority of the
servers that are running machine learning workloads are powered by Xeon platforms, Xeon servers.
Deep learning is a subset and a fairly small but important subset of machine learning.
And that's an area where we've seen the software optimizations make a huge difference on
the time to train.
And so developers now are seeing, you know, what used to take months down to minutes to
do training on Xeon scalable processor.
So the DevCloud will give you access to that.
If developers have it tried it lately, they should try it now and see really, you know,
the great performance will get.
And so the 100X performance in the months down to minutes, that's relative to past performance
on the Xeon with CPUs.
Do you have any published comparisons relative to GPUs?
And I'll just note that a lot of it is really based on the software optimization more
than anything.
It's made seem the huge difference.
And we're seeing folks in the industry now start to publish really good results on the
training site as well, compared to other alternatives in the market.
So I don't know that we have any yet published on our site, but in general, we're seeing
some really good results in the industry.
And then we're seeing really good results on cloud service providers and what they're
using as well for the technology.
You mentioned cloud service providers is DevCloud hosted by Intel Nirvana, or have you
partnered with one or more of the cloud service providers to make it available?
It's really a part of our Nirvana cloud and really aligning those together.
For the service providers, AWS or Google Cloud, they have great services today.
They're using Xeon scalable and developer students can use those too.
However, it can become costly for a student just getting started.
So this is a way for them to stand bucks and get started and then move into a CSP model
when they have a business that can support that.
Okay.
Okay.
So then the Intel Nirvana cloud is, that's something that you're building and built and
are building and host in your own data centers and manage independent of the large cloud
providers.
That's correct.
That's correct.
Right.
And another thing I'll mention too is that Intel, one of the differentiators from Intel
is we have such a broad portfolio of offerings and from the data center to the edge.
And in the data center, of course, there's a training, there's inference on the Xeon,
but we also have the FPGA product line and recently Microsoft announced that they're going
to use the Stratics 10 FPGA for their brainwave project to really power all of the inference
in their DL platform and brainwave.
And we see a lot of opportunity with FPGAs as well, both in the data center and also at
the edge for the inference piece.
Can you tell us a little bit about brainwave or folks that missed that announcement?
Well, I'm not the necessarily expert on brainwave, but it is Microsoft's deep learning platform
and they announced it's going to use FPGA and Stratics 10 for it because of the low latency,
low power, high throughput.
It provides and the flexibility, FPGA is great in the flexibility it provides as AI evolves
and changes.
Right.
Okay.
All right, great.
And then the last thing I'll mention too is if we talk about the portfolio is we also
have at the edge, we have people developing solutions at the edge for deep learning that
use Adam, core processor, Movidius for the video processing and whole host of this.
All of this is going to be part of the academy too, so developers can not only learn about
okay, I want to do the training, the inference in the data center, but at the edge too and
put full end to end solution.
Okay.
Awesome.
Awesome.
One question I've got for you is in your role as overseeing developer programs, there
are some interesting differences between the needs of traditional enterprise developers
and more data science users, you know, data scientists that, you know, may also probably
also fall under developer programs for you.
Can you talk a little bit about how, you know, the different offerings you have that, you
know, target these different communities and more generally, like how you're looking
at these communities and how you plan, you know, evolve your offerings to serve both
of them and give them what they need?
Sure.
And that is a great point.
AI really introduced us to new types of an audience for us because I run developer programs
for Intel and we cover the gamut from server to mobile and game developers and IOT developers
across the board and we jumped into AI, yeah, all of a sudden there's this data scientist
and they're like, okay, we're used to maybe dealing more with C++ developers and helping
them optimize their code for the latest hardware that we had and for data scientists, it's
a different ball game and first of all, most of them are using Python.
What we've had to do is, okay, let's focus on how do we help them?
They're going to be using frameworks, they need optimized frameworks, they need optimized
versions of Python, they need really to understand the basics of machine learning and how to
manage the data, how to apply machine learning versus how to really code in many cases.
So we really had to look at this differently and start to say, okay, for these guys, let's
teach them the basics on how to get started.
Let's look at the tools that they will need from, like Python, for example.
We have an Intel distribution of Python that is really good for performance and really
good for AI and developer.
And Intel distribution of Python?
Yes, it's a parallel Python.
Really?
Oh yeah, so when we tell data scientists about this too, they're actually really excited
as well because it can really help the performance of their application.
Where do you find this Intel distribution of Python?
It's easy to find, you just search on exactly that, you'll find it, you'll find it on our
Academy, we have lots of information about it there and the benefits.
And is it new kind of as part of the Academy or has it been around for?
It's been around a little while, but it's not that long, I'd say, yeah, last year, I believe
it was when we rolled it out in 2016.
And you said it's optimized around parallel and distributed?
Yeah, particularly for parallel, running really well in a parallel environment and giving
the best performance out of Python applications.
And what's an example of a parallel environment and a workload that you might use this Python
distribution with?
Well, I don't know if I have a real good example right there, but let me tell you about an example
that something we did just recently, we just wrapped up as we did a contest with Kaggle.com.
You heard it from other Kaggle.
And we said, let's find a partner in the industry who wants to solve a real world AI problem
and then eat some help.
And so we partnered with mobile ODT and they're all about early detection of cancer and
how do they do, how do they provide low cost devices to early deduct cancer?
And so we, they provided a data set of 10,000 images, okay?
And we provided the developers with parallel Python with access.
This was an early version of our dev cloud, we were kind of in the pilot mode.
So they got parallel Python, they got a dev cloud, they got, in this case, it was optimized
cafe and we said, all right, whoever can provide the best algorithms and become the best
at the detection of the images that are cancerous will win lots of good prizes.
And the response was great.
We had up to 1,000 data scientists and developers competing on this and accessing the dev cloud
on a daily basis.
So it was a great test of this, it was a great test of the tool set and the whole model
here.
And at the end of the day, mobile ODT was really excited about what they learned and are
now falling out with the winners to say, okay, how do we productize what you've done
here together?
Interesting.
I think a lot of folks use Python from, you know, they'll use like condos or some of these
other Python distributions like does, do you envision, you know, partnerships to, how
do you get this distribution of Python out there, right?
To have people aren't, a lot of, I imagine to not know about it that a lot of people don't
know about it.
And I mean, I've seen this, this, yeah, I think just based on other things that I've seen
Intel do in the past like, for example, in the Hadoop space, right?
So there's a ton of interesting innovations that have happened in taking, you know, Intel's
deep knowledge of the hardware and like kind of driving that into, you know, the mainstream
Hadoop distributions and making them, you know, just making them out of the box more
performant based on or more secure, like there's some security and encryption stuff in
the Hadoop example, you know, I know of examples where, you know, folks from Intel have partnered
with, you know, vendors like Docker and, you know, other, you know, lots of, lots of open
source engagement.
But in this Python case, like, how do you get this, how do you get this out into the
wild?
Well, a lot of that happens organically if you have a, you know, good product and there's
a Python community out there, which is quite active and they start to hear about the word
will spread organically.
But to help that, we're becoming pretty proactive on awareness, thriving just social media
outreach, some digital online marketing and other academy awareness in general where Python
is becoming a bigger message there that, hey, did you know this distribution is available
and with these benefits?
So I think it's a combination of those and it won't be long when it's before it's pretty
well known within that community.
Oh, interesting.
Any other cool stuff to tell me about that I didn't know about it?
I don't think so.
I think we hit on the main ones.
I think the mention is that the academy is growing fast.
We already have, coming up on a 50,000 members and we're, we've been running workshops
around the world at universities, we'll have probably 200 universities that will be participating
by the end of this year and trained about, I think we're on 25,000 developers and students
so far this year.
So it's really ramping up fast and we have some big aggressive goals, really helping to
bring AI to the masses and through this dev cloud make compute available, through the
training, help them learn about it, the technical support with TCS, we're pretty serious
about helping people learn and grow and use AI to do really cool things.
Well, it's a huge growth opportunity so I think it's a smart move and makes sense.
It makes me think a little bit of Apple's strategy back in the day to get into colleges
and universities with the Macintosh which help propel them later on.
So congrats on that and congrats on the success of the academy so far.
Thank you very much.
Yeah.
Thanks so much.
God.
All right, everyone, that's our show for today.
Thank you so much for listening and for your ongoing feedback and support.
For more information on Naveen and Scott, for links to dev cloud or the AI Academy and
any of the other topics covered in this episode, head on over to twimaleigh.com slash talk slash
51.
For the rest of this series, visit twimaleigh.com slash AISF 2017.
And please, please, please send us any questions or comments that you may have for us or for
our guests via Twitter at twimaleigh or at Sam Charrington or leave a comment on the
show notes page.
There are a ton of great conferences coming up through the end of the year.
To keep up to date on which events will be attending and hopefully meet us there, check
out our new events page at twimaleigh.com slash events, twimaleigh.com slash events.
Thanks again for listening and catch you next time.
