1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,360
I'm your host Sam Charrington.

4
00:00:23,360 --> 00:00:30,240
A big thanks to everyone who joined us last week for our second Twimble online meetup.

5
00:00:30,240 --> 00:00:36,720
Led by Nicola Cucherova, we discuss the paper Learning Long-Term Dependencies with Gradient Descent

6
00:00:36,720 --> 00:00:40,320
is Difficult by Yasuo Benjio and Company.

7
00:00:40,320 --> 00:00:43,760
I had a great time and learned a ton.

8
00:00:43,760 --> 00:00:50,400
For those who weren't able to attend live, we have the video posted for you on Twimbleai.com

9
00:00:50,400 --> 00:00:56,480
slash meetup and if you're interested in joining us next month, please head on over to that site

10
00:00:56,480 --> 00:01:02,880
to get signed up. We're also accepting presenters so feel free to shoot me a note with your ideas.

11
00:01:03,680 --> 00:01:09,520
Next up, the time has come for the Artificial Intelligence Conference brought to you by O'Reilly

12
00:01:09,520 --> 00:01:16,000
and Intel Nirvana. I'll be in San Francisco the entire week next week and we have a ton of great

13
00:01:16,000 --> 00:01:21,600
interviews lined up. So this should be another awesome series. If you haven't had a chance to

14
00:01:21,600 --> 00:01:28,080
check out our series from the New York event, you can find it at twimbleai.com slash O'Reilly AI

15
00:01:28,080 --> 00:01:35,520
and Y. Also, if you'll be at the conference, please send me a shout out on Twitter or via email.

16
00:01:35,520 --> 00:01:43,680
I'd love to connect. See you then. Okay, about our show. This week I'm bringing you an interview

17
00:01:43,680 --> 00:01:51,200
with Bruno Gunn's office, a more Sloan Data Science Fellow at NYU. As you're hearing the interview,

18
00:01:51,200 --> 00:01:57,120
Bruno is a longtime listener of the podcast. We were able to connect at the NY AI conference back in

19
00:01:57,120 --> 00:02:02,960
June after I noted on a previous show that I was interested in learning more about Word Tevek.

20
00:02:02,960 --> 00:02:07,920
Bruno graciously agreed to come on the show and walk us through an overview of Word embeddings,

21
00:02:07,920 --> 00:02:14,640
Word Tevek, and a bunch of related ideas. He provided a great overview of not only Word Tevek,

22
00:02:14,640 --> 00:02:20,880
but related natural language processing concepts such as skipgram, continuous bag of words,

23
00:02:20,880 --> 00:02:28,080
no Tevek, TF IDF, and much more. By the time you hear this, it'll be too late to catch it live,

24
00:02:28,080 --> 00:02:34,320
but Bruno is doing a half day tutorial on Word Tevek and friends on Monday at the AI conference.

25
00:02:34,320 --> 00:02:41,760
If you'd like to see it, I'm sure it'll be available via the O'Reilly Safari website and now on to the show.

26
00:02:49,120 --> 00:02:54,480
All right everyone, so I am here on location at the O'Reilly AI conference and I have the

27
00:02:54,480 --> 00:03:00,560
pleasure of being here with Bruno Gunn's office who is apparently a longtime listener of the podcast

28
00:03:00,560 --> 00:03:08,160
and reached out to me after hearing me note that I wanted to learn more about Word Tevek and embeddings

29
00:03:08,160 --> 00:03:13,360
and made it happen. So we're here to talk about that. Welcome, Bruno. Thank you. It's a pleasure

30
00:03:13,360 --> 00:03:17,760
to be here. So I've been listening for a very long time and I'm very happy to be here and be able

31
00:03:17,760 --> 00:03:23,200
to participate. Awesome. Awesome. I was just asking you, you mentioned that you've been listening since

32
00:03:23,200 --> 00:03:28,720
before the interview format and I don't know, I'm still stuck on like the transition from the

33
00:03:28,720 --> 00:03:33,040
from the news format to the interview format. How was that for you? Like what's your take on kind

34
00:03:33,040 --> 00:03:39,520
of the before and after? No, I like the new interview format actually. But this might be a

35
00:03:39,520 --> 00:03:44,400
personal buy as well. When it was news, a lot of the news I'd already seen or listened somewhere

36
00:03:44,400 --> 00:03:49,040
else. So it wasn't as new. In this way, since you're interviewing people that I don't necessarily

37
00:03:49,040 --> 00:03:54,160
listen to it and then no personally, it's good to have a fresh view from them. That's great.

38
00:03:54,160 --> 00:03:59,440
That's great. Like I said, I was, you know, and I guess I still am hung up on the fact that I

39
00:03:59,440 --> 00:04:04,640
switched the format. But I've been told by people that things along the same lines that the news

40
00:04:04,640 --> 00:04:10,320
is essentially commoditized and it's hard to, you know, beat out tech crunch or whatever that people

41
00:04:10,320 --> 00:04:15,440
count on for their news. But the folks really seem to be enjoying the interview format. And so,

42
00:04:15,440 --> 00:04:20,080
you know, again, it's great to have you on for an interview instead of talking about the news.

43
00:04:20,080 --> 00:04:25,040
So tell us a little bit about kind of your background and what you're up to. So I'm,

44
00:04:25,040 --> 00:04:29,280
I can say, I'm originally a physicist, but I've always been involved in optimization problems,

45
00:04:29,280 --> 00:04:34,320
originally in spin glasses, soft optimization and data mining. Spin glasses. Spin glasses. So it's

46
00:04:34,320 --> 00:04:38,480
this, it actually has interesting connections with neural networks and how they work. But

47
00:04:38,480 --> 00:04:44,880
okay, basically a disorder magnetic system. Okay. Put it very, very simply. But the mathematical

48
00:04:44,880 --> 00:04:51,120
structure is very rich and it leads itself to exploring different types of optimization problems.

49
00:04:51,120 --> 00:04:56,240
Okay. From there, I evolved very quickly. And this was because I noticed that in spin glasses,

50
00:04:56,240 --> 00:05:00,560
a lot of the behavior you see has to do with the way that the different elements are connected.

51
00:05:01,680 --> 00:05:06,400
I moved very quickly towards networks and studying basically connections between components of

52
00:05:06,400 --> 00:05:11,120
a system. Okay. More deeply. And when you're working with network and complex networks in general,

53
00:05:11,120 --> 00:05:16,720
what you do a lot is basically data mining. You're, you're crawling websites, you're parsing a

54
00:05:16,720 --> 00:05:22,960
patchy logs to look for connection between a URLs. For example, you're, you're basically doing

55
00:05:22,960 --> 00:05:28,240
in a sense applied graph theory. Okay. But applied to real data. So the transition towards data

56
00:05:28,240 --> 00:05:33,280
came very early and very naturally, even before, I mean, people were talking about data science.

57
00:05:33,280 --> 00:05:40,080
This was back in maybe 2006, 2007. Okay. And then more recently, I've been moving more towards

58
00:05:40,080 --> 00:05:45,680
more data science, intensive aspects and hence my interest in the work to vac and that type of

59
00:05:45,680 --> 00:05:51,520
algorithms. Okay. You know, I think the best way to do this is to just jump in and have you

60
00:05:51,520 --> 00:05:58,400
kind of walk us through work to vac, but the kind of the foundational thinking that led to work to

61
00:05:58,400 --> 00:06:04,160
vac and embeddings. As I mentioned the other day, it's, it's an area that I've been meaning to dive

62
00:06:04,160 --> 00:06:10,560
into and I'm glad you're, you're here to tell us about it. So the original idea is actually as

63
00:06:10,560 --> 00:06:16,400
many things doing this field comes from outside the field. So it's comes from linguistics. Okay.

64
00:06:16,400 --> 00:06:23,440
And it's expressed more or less very in a very general way by James Firth, if I'm not mistaken,

65
00:06:23,440 --> 00:06:29,280
around the 1953. Okay. And basically what he said is you, you'll know the meaning of a world by

66
00:06:29,280 --> 00:06:34,080
the company it keeps. So if you don't understand what a world means, you kind of look at the context

67
00:06:34,080 --> 00:06:39,040
and the context gives clues about what the world says. Okay. Intense to say. So we're to vectorize

68
00:06:39,040 --> 00:06:46,800
to take advantage of this by saying that the embedding of a world is defined by the context in

69
00:06:46,800 --> 00:06:53,200
which it appears. So words that appear in the same context are more somewhat equivalent. So they

70
00:06:53,200 --> 00:06:57,280
will have vectors that are close to each other. While words that tend to appear in very different

71
00:06:57,280 --> 00:07:02,960
contexts will be very different and have very different vectors or vector representations in this

72
00:07:02,960 --> 00:07:10,240
case. And so how do you get to the word vectors? I guess my impression is that you're, it's all

73
00:07:10,240 --> 00:07:17,280
relative to a specific corpus, right? There's not like a, you know, we haven't done word to veck on

74
00:07:17,280 --> 00:07:21,840
everything like some grand unification of word to veck. Is that the right way to think about it?

75
00:07:21,840 --> 00:07:26,240
And then you're doing some math on that corpus to get you to the vectors. Can you talk a little

76
00:07:26,240 --> 00:07:30,880
bit about that process? Yes. So basically what it does, it's a very simple neural network. So

77
00:07:30,880 --> 00:07:35,520
it just says one hidden layer. The activation function is linear. So it just passes through.

78
00:07:36,080 --> 00:07:41,840
It's very simple. What the actual wording embedding that are basically the weight on the

79
00:07:41,840 --> 00:07:48,480
hidden layer. Depending on which model can be skip grammar, can be continuous bag of words. It

80
00:07:48,480 --> 00:07:55,120
can be the weight on the leading to the hidden layer. It can be the weights outside heading out

81
00:07:55,120 --> 00:07:59,840
of the hidden layer. But in practice, it's the same mathematically. And so the skip

82
00:07:59,840 --> 00:08:04,160
ram refer to one of those and continuous bag of words refer to the other skip grammar is

83
00:08:04,720 --> 00:08:09,520
leading in the weight's leading in. And yes, so the way you look at what first was saying that

84
00:08:09,520 --> 00:08:14,000
you know a word by the company it keeps is if you have the word, you can kind of guess the

85
00:08:14,000 --> 00:08:17,920
context in which it appears, right? Or if you have the context, you can kind of guess what word

86
00:08:17,920 --> 00:08:23,040
would fit in the middle. So the skip ram and the continuous bag of words basically look at these

87
00:08:23,040 --> 00:08:28,880
two approaches. In one case, your input is the word and you're trying to guess in the output

88
00:08:28,880 --> 00:08:34,960
layer what the context is. On the other case, you have the context words as input and you're trying

89
00:08:34,960 --> 00:08:40,000
to guess what is the word that might go in the middle. And here, just to clarify, the context is

90
00:08:40,000 --> 00:08:45,840
usually defined by a window of words before that word and the words after that word. So if you're

91
00:08:45,840 --> 00:08:51,280
looking at word, I, so your context would be, for instance, I minus one, I minus two and I plus

92
00:08:51,280 --> 00:08:55,920
one, I plus two, like two words before, two words after or five words before five words after.

93
00:08:56,480 --> 00:09:03,920
Okay. And so that windows what's used to compute the vector for that representation for

94
00:09:03,920 --> 00:09:08,080
a given word. Yes. So that window will give you what is the context and the context will define

95
00:09:08,080 --> 00:09:13,360
what is the word. Okay. Now, like you're saying, people haven't done word-to-vec in all the text in

96
00:09:13,360 --> 00:09:18,720
the world. So every time you run this, you will get different vectors. Right. Also, if you run

97
00:09:18,720 --> 00:09:23,760
twice in the same corpus, the vectors will be different. The reason for this is you're initializing

98
00:09:23,760 --> 00:09:28,560
all the weights or the vectors randomly. Right. And then you're adjusting them. Okay. So you

99
00:09:28,560 --> 00:09:33,200
will end up with something different. However, if you do it well enough and it converges and you get

100
00:09:33,200 --> 00:09:37,360
something that this reasonable, the vectors will be different, but they'll basically be a rotation.

101
00:09:37,360 --> 00:09:43,680
So you can get vectors trained into corpus and align them so that they match. Because what the word

102
00:09:43,680 --> 00:09:50,560
-to-vec algorithm and similar algorithms are trying to do is learn the relations between vectors.

103
00:09:50,560 --> 00:09:56,160
So you're basically, you're trying to define the differences of vectors, but not the actual vectors.

104
00:09:56,160 --> 00:10:00,880
Right. So there's some distance metric or something. So if you rotate the distances all remain

105
00:10:00,880 --> 00:10:05,040
the same. Right. So there's still valid. Okay. And it's because the distances are preserved.

106
00:10:05,680 --> 00:10:10,000
Right. It's the the distance that preserves the semantic meaning that gives you the semantic

107
00:10:10,000 --> 00:10:15,440
relations. Right. And the idea is very simple. So if the same word, or rather if two words,

108
00:10:15,440 --> 00:10:21,520
appearing in the same context, they have to be defined by similar vectors. And this also means

109
00:10:21,520 --> 00:10:26,400
that if the relation between these pair of words and relation between these pair of words is similar,

110
00:10:26,400 --> 00:10:31,360
the difference between them in terms of vectors will also be similar. Okay. So this is why you can do

111
00:10:32,160 --> 00:10:37,920
word arithmetic and say the vector for Italy minus the vector for Rome has to be equal to the

112
00:10:37,920 --> 00:10:42,000
vector for France minus the vector for Paris. Right. Right. So if you have three of these,

113
00:10:42,000 --> 00:10:46,080
you can calculate the other one. Right. And basically, it's one of the ways they use to calculate

114
00:10:46,080 --> 00:10:51,120
or to measure how good the embeddings are. They use this called analogies. Right. So

115
00:10:51,120 --> 00:10:57,120
Okay. Paris is to France's Rome is to and they'll give you Italy. If you look for the vector that's

116
00:10:57,120 --> 00:11:02,640
closest to that difference. Okay. Interesting. So one of the questions that, well, two questions

117
00:11:02,640 --> 00:11:10,640
come off from me. One is if the context is defined by this narrow window, you said it's usually

118
00:11:10,640 --> 00:11:15,840
used two words before two words after is that example of two words in the in the official implementation.

119
00:11:15,840 --> 00:11:21,520
I think the default is set to five, but it depends. You can vary it. It's an input parameter. And

120
00:11:21,520 --> 00:11:26,880
also in practice, if you're going to the details, when you're you're pre-processing the corpus,

121
00:11:27,600 --> 00:11:31,760
you will sometimes remove some words because they're too common, kind of like what you do with

122
00:11:31,760 --> 00:11:35,920
stop words. Right. And that effectively changes the size of the window. Right. Because you do this

123
00:11:35,920 --> 00:11:40,560
before you calculate the context. So it's almost as if sometimes your windows are a bit bigger.

124
00:11:41,360 --> 00:11:45,360
Okay. And that is because you remove the word. Right. So you're catching more information.

125
00:11:45,360 --> 00:11:51,200
Okay. So the question is, do folks experiment with a lot with bigger windows or is it possible

126
00:11:51,200 --> 00:11:58,320
to do this on the entire corpus and get, I guess in my mind, what I'm hearing is the word

127
00:11:58,320 --> 00:12:03,280
relationships are only relative to this very small window. And wouldn't we have, wouldn't the

128
00:12:03,280 --> 00:12:08,880
vectors capture more information if we were somehow creating them based on bigger windows or the

129
00:12:08,880 --> 00:12:14,400
entire corpus? Is that the right way to think about it? Yes, and no. So if you make the window too

130
00:12:14,400 --> 00:12:20,640
big, you're basically including information that's not relevant for the word. Right. You can have

131
00:12:20,640 --> 00:12:24,800
a very long sentence. The word at the end of the sentence doesn't necessarily have anything to do

132
00:12:24,800 --> 00:12:29,200
with the word at the beginning of the sentence. So you're trying to keep nearby words that help you

133
00:12:29,200 --> 00:12:36,560
define what that word means. So adjectives verbs that directly relate to what the concept is.

134
00:12:36,560 --> 00:12:42,000
So what the word is in particular. Right. I guess I'm thinking about it in the context of

135
00:12:42,960 --> 00:12:50,240
like running TF IDF on a Wikipedia article. Right. If I'm looking at a Wikipedia article that's

136
00:12:50,240 --> 00:12:57,600
talking about neural networks and CNNs and RNNs and, you know, artificial neural networks, all

137
00:12:57,600 --> 00:13:05,040
these, these are all related terms. And I'd want to capture that relatedness. But they may be in,

138
00:13:05,040 --> 00:13:11,840
you know, very different. They may be far from one another spatially in my document. But I still

139
00:13:11,840 --> 00:13:17,600
want to capture that context. Can I just not use word to effect for that? You can. So basically what

140
00:13:17,600 --> 00:13:22,320
you're doing is you're scanning through the entire document. Right. Right. And and embedding the

141
00:13:22,320 --> 00:13:28,320
that you learn for each word as to do with all the contexts it appears in. So it's not just one.

142
00:13:29,200 --> 00:13:34,640
So I can give the example of artificial neural network. Right. Artificial will appear next to

143
00:13:34,640 --> 00:13:39,680
neuron in this context. But in a maybe a few paragraphs later it will appear next to intelligence

144
00:13:39,680 --> 00:13:45,040
or to approach. Right. So that means that artificial will be defined by all of these contexts.

145
00:13:45,040 --> 00:13:51,680
Mm-hmm. And do look very differently than the word maybe like convolutional that always appears

146
00:13:51,680 --> 00:13:56,080
next to neural network. Right. And it doesn't appear in other contexts. Right. So it does take

147
00:13:56,080 --> 00:14:01,280
the whole information of the of the corpus into account. But if, for example, I'm running it on

148
00:14:01,280 --> 00:14:07,360
a Wikipedia article on neural networks. And there's one section on convolutional neural network. And

149
00:14:07,360 --> 00:14:14,800
then another section on RNNs and another section on LSTMs. Since those individual terms are separated,

150
00:14:14,800 --> 00:14:21,280
by these sections, will it capture those relationships? Yes. Just the neural network part or...

151
00:14:21,280 --> 00:14:26,080
Yes. So one thing maybe I should be clear. So when I say corpus in this case, I mean,

152
00:14:26,080 --> 00:14:31,840
all of Wikipedia. I don't mean one page of Wikipedia. Okay. So these are very large corpus. Got it.

153
00:14:31,840 --> 00:14:36,640
And the reason why you use a very large corpus is because you want to learn what is the meaning of

154
00:14:36,640 --> 00:14:43,040
the word in a sense. Okay. So this is what you're trying to capture. It's not necessarily what does

155
00:14:43,040 --> 00:14:47,760
this word mean in this document? Got it. So it's more of a it's a more generic thing. Okay. And this

156
00:14:47,760 --> 00:14:53,840
is why actually people have started publishing high quality embeddings. Google has published some

157
00:14:53,840 --> 00:14:59,440
Facebook as far as the trend on billions of words or rather corporate that are billions of words

158
00:14:59,440 --> 00:15:04,240
long because they're trying to capture what is a exact meaning. So you can use, for instance,

159
00:15:04,240 --> 00:15:09,680
these vectors. One very simple application is, for example, for queries and biguation. Because

160
00:15:09,680 --> 00:15:13,360
you know the word, you know, what the vector is, you can look around that word to see what

161
00:15:13,360 --> 00:15:17,760
are words that are related to that one. And maybe you can show results that use those other words.

162
00:15:17,760 --> 00:15:24,400
Okay. You can use the vectors and this translation invariance distance preservation as a way of

163
00:15:24,400 --> 00:15:29,600
mapping the word, for instance, to a verb version of the word or a noun version of the word

164
00:15:29,600 --> 00:15:35,760
or a past tense version. So you can use all of these relations in a sense, all of these linear algebra,

165
00:15:35,760 --> 00:15:41,440
as a way of getting more knowledge about what the text is. Okay. Interesting. So the other question

166
00:15:41,440 --> 00:15:49,280
I had was the length of the vector. How do you know what the right dimensionality is for these

167
00:15:49,280 --> 00:15:54,400
vectors? As far as I know, there is no well-defined way of measuring what is the optimal size.

168
00:15:54,400 --> 00:16:00,960
Okay. In practice, people tend to use dimensions between about a hundred and five hundred.

169
00:16:00,960 --> 00:16:07,440
Okay. Which is relatively small for corporate or other dictionaries. So the number of individual

170
00:16:07,440 --> 00:16:13,360
words of the order of hundreds of thousands. In a sense, what one of the things you're also doing

171
00:16:13,360 --> 00:16:18,400
is you're doing very much a dimensionality reduction. You're mapping this from this very large

172
00:16:18,400 --> 00:16:24,080
high dimensional space, which dimension is a word into this very small space in a way that

173
00:16:24,080 --> 00:16:30,000
is still preserving the meaning of the semantic value of the words. Okay. Okay. Do you recall the

174
00:16:30,000 --> 00:16:37,040
show that I did with Francisco Weber? Maybe. It was from a cortical that I only talked about

175
00:16:37,040 --> 00:16:42,000
the kind of neural representations. Yes. I actually remember that I wanted to look into that

176
00:16:42,000 --> 00:16:46,960
more carefully. Okay. It's explicitly mentioned that this is actually related with this type of

177
00:16:46,960 --> 00:16:50,960
vector representations, because yeah, it seems that each word is actually represented in different parts

178
00:16:50,960 --> 00:16:55,840
of the brain at the same time. Right. So it's something I'm curious to look more deeply into it.

179
00:16:55,840 --> 00:17:01,600
I tried looking at their website, but I couldn't get the technical details to it. Okay. I was curious

180
00:17:01,600 --> 00:17:05,920
if you had looked into that at all. And if you were familiar with that model and and it's

181
00:17:05,920 --> 00:17:10,480
thought you might have on. I don't know the details. I found some things online, but it seemed to be

182
00:17:10,480 --> 00:17:16,160
more marketing oriented. So there wasn't like the scientific articles behind it. Okay. So embeddings

183
00:17:16,160 --> 00:17:22,880
has been around since the 50s word to back now is a did you say 1950s something? So 95s is the

184
00:17:22,880 --> 00:17:27,520
is the idea is this idea that the meaning of the word is coming from the context appears. So this

185
00:17:27,520 --> 00:17:32,640
got the distribution of the hypothesis in linguistics. Okay. We're embedding some so I think they've

186
00:17:32,640 --> 00:17:38,160
been around for maybe 10 years, maybe maybe a little bit more. Okay. In practice or like in

187
00:17:38,160 --> 00:17:42,160
wide adoption, they became very popular and get a lot of attention with word to back when

188
00:17:42,160 --> 00:17:48,000
we were published in 2013. Right. But I might as me call off if I'm not mispronouncing his name horribly.

189
00:17:48,000 --> 00:17:54,880
Okay. Okay. And so, you know, we're a few years into word to back and now there are a bunch of

190
00:17:54,880 --> 00:18:00,880
other to Vex. Right. Have you looked into any of those as well? Yes. So there's some that are

191
00:18:00,880 --> 00:18:05,280
very interesting. There's one for instance that is DNA to back and you try to find embeddings for

192
00:18:05,280 --> 00:18:12,080
sequences of DNA. Okay. And see how that might relate to protein structure and

193
00:18:12,080 --> 00:18:19,920
genome organization, which I find fascinating. There is one by Eurel Laskovek in Stanford

194
00:18:19,920 --> 00:18:27,280
where it's called note to Vex and it tries to use this note like back in a graph where it's

195
00:18:27,280 --> 00:18:31,600
basically trying to do that to find embeddings for nodes and graphs as a way of measuring

196
00:18:31,600 --> 00:18:36,640
relations between nodes. And basically the way it does it since it doesn't have a sequence

197
00:18:36,640 --> 00:18:41,360
of words. It doesn't have a sequence of nodes. So it basically runs a random walk process on the

198
00:18:41,360 --> 00:18:45,520
net on the node. Okay. And that generates the sequence. And then based on that sequence,

199
00:18:45,520 --> 00:18:51,120
then you can treat each node as if it was a word. It appears next to other nodes because there's

200
00:18:51,120 --> 00:18:55,440
some how it's the neighborhood. And that can and from there you can define. And it's from that

201
00:18:55,440 --> 00:18:59,680
is able to capture some of the structure of the network. I wouldn't. So the idea is that you're

202
00:19:00,240 --> 00:19:06,960
I guess I would have thought that that a graph is its own kind of representation of all this

203
00:19:06,960 --> 00:19:12,640
information. It is and it isn't in a sense right. The point is it's not necessarily easy and this

204
00:19:12,640 --> 00:19:19,680
is a one on problem to compare graphs. So I'll give you a graph and tell you the nodes here

205
00:19:20,480 --> 00:19:25,680
are words. I give you another graph and I tell you the nodes here are people. So you can't just

206
00:19:25,680 --> 00:19:30,320
match the labels directly. Right. It's very hard to see if the structure of the graph is the

207
00:19:30,320 --> 00:19:35,040
same. It's actually the total permutations and all the things. So if I'm not mistaken, I think

208
00:19:35,040 --> 00:19:39,280
that can be complete problem. Okay. So people have been trying this idea of graph embeddings

209
00:19:39,280 --> 00:19:44,560
for a while. Okay. When you try to map the graph into some set of points that does not depend

210
00:19:44,560 --> 00:19:48,720
on the details of the graph or the labels that you give to nodes or anything like that. Okay.

211
00:19:48,720 --> 00:19:54,240
Or any permutations you do. Hmm. Interesting. So I've been fascinated by basically how

212
00:19:54,240 --> 00:20:00,240
versatile this idea of world to back is. Yeah. Are there others? I'm trying to I'm

213
00:20:00,240 --> 00:20:06,880
meta-loss for I know I've seen it. It seems like a dozen of these different X to X of late.

214
00:20:06,880 --> 00:20:12,960
But it sounds like anytime you have a space with some complex structure, this is a tool that you

215
00:20:12,960 --> 00:20:21,280
can use to allow you to compare both individual points within that space to other spaces. Is that

216
00:20:21,280 --> 00:20:26,720
a general way to think about it? More. It's more whenever you have a sequence of tokens. Let's say

217
00:20:26,720 --> 00:20:31,600
you can use it as a way of finding a representation of these tokens that then you can use to find

218
00:20:31,600 --> 00:20:36,080
the relationships. Okay. In this sequence. Right. So it works very well for text because you have

219
00:20:36,080 --> 00:20:41,600
sequences of words. Hmm. Like in the case of DNA2VAC, it works very well because you have a sequence

220
00:20:41,600 --> 00:20:47,600
of nucleotides in DNA. Hmm. For node2VAC, you have a sequence of nodes that generated by these

221
00:20:47,600 --> 00:20:52,560
random processes. This random walk process is on the graph. And they actually tried different

222
00:20:52,560 --> 00:20:57,680
definitions of the random walk, different rules. And so that that's somehow able to capture

223
00:20:57,680 --> 00:21:04,240
different aspects of the of the network structure. Hmm. Interesting. And the sequences have to have

224
00:21:05,280 --> 00:21:10,400
I guess the the relationships between the tokens and the sequences are defined by the corpus.

225
00:21:10,400 --> 00:21:16,480
I guess the I was thinking of I wonder if you can do like transaction to VEC and like do word

226
00:21:16,480 --> 00:21:22,400
of embedding on the sequence of transactions and use that for fraud detection or something like that.

227
00:21:22,400 --> 00:21:27,600
I have actually saw something like this yesterday on medium. I didn't I didn't really because

228
00:21:27,600 --> 00:21:31,520
I didn't have time. But there was literally I don't remember the details. It was really something

229
00:21:31,520 --> 00:21:36,240
like stock to VEC that somebody published on medium yesterday. It's on my reading list,

230
00:21:36,240 --> 00:21:40,400
but I have not read it yet. Interesting. Interesting. I don't know how they're doing it. I just saw

231
00:21:40,400 --> 00:21:46,000
the text. Okay. But notionally, there's there's something in there somewhere. Yes. So in principle,

232
00:21:46,000 --> 00:21:51,120
every time you have a sequence, you can do something like this. Yeah. I've also been getting more

233
00:21:51,120 --> 00:21:56,080
into learning about blockchain and how that all works. Yes. I'm wondering now if there are some

234
00:21:56,080 --> 00:22:01,280
applications to that as well. Possibly. I have not seen anything with blockchain using this.

235
00:22:01,920 --> 00:22:07,360
I've looked in detail at blockchain a couple of years ago. Those interested in basically these

236
00:22:07,360 --> 00:22:13,680
transaction networks. It might be possible to do something. I said, I haven't seen it yet.

237
00:22:13,680 --> 00:22:19,440
Interesting. Maybe an idea for some of your listeners. Yeah. Yeah. What are some of the most

238
00:22:19,440 --> 00:22:25,920
interesting or what are some of the interesting applications you've seen of word to VEC and friends?

239
00:22:25,920 --> 00:22:30,720
I've fascinated by basically how much power these word vectors have. Because they are capturing

240
00:22:30,720 --> 00:22:34,960
these semantic relations, means that you can use them for disambiguation, query expansion,

241
00:22:35,600 --> 00:22:42,400
analogies, like the original metric that they used. So basically, they're a very powerful tool to

242
00:22:42,400 --> 00:22:49,200
map text into a vector representation or to an numerical representation. The then is very powerful

243
00:22:49,200 --> 00:22:54,560
in how you can manipulate it. Because everybody knows computers have a hard time understanding text,

244
00:22:54,560 --> 00:22:59,040
but they're very good at understanding vectors. So here you're mapping text to vectors. So you're

245
00:22:59,040 --> 00:23:04,240
making computers' lives much easier. So people have used this for machine translation. Because you

246
00:23:04,240 --> 00:23:12,240
can translate vectors into different languages. Then you align them. Because of relationships

247
00:23:12,240 --> 00:23:18,160
have to be the same mostly. And then you can use them basically too much. So you find the word in

248
00:23:18,160 --> 00:23:23,280
embedding in one language. You find what is the most similar word, not similar vector in the

249
00:23:23,280 --> 00:23:28,240
other language. And you can find the translation. Also, an idea that came up in the Francisco

250
00:23:28,240 --> 00:23:33,760
Webber conversation. So yeah, we both need to dig into that one and try to figure out what they're

251
00:23:33,760 --> 00:23:38,000
doing. That's different than regular embeddings. I mean, and then there's that variation. There's

252
00:23:38,000 --> 00:23:42,400
also glove, glove, glove, glove, glove of vectors that tries to do a different formulation.

253
00:23:43,680 --> 00:23:47,680
Another very interesting application, which is actually the reason why I started getting

254
00:23:47,680 --> 00:23:53,840
interested in word-to-vec in particular is this paper I saw, which is actually tracking linguistic

255
00:23:53,840 --> 00:24:01,200
change over time. So they train using word-to-vec. They train word vectors using Google and Gramps

256
00:24:01,200 --> 00:24:07,680
data for different decades. And then they align the vectors. And then they look for the words that

257
00:24:07,680 --> 00:24:12,880
are changing over time. Oh, that's interesting. So you can track basically how the meaning of a word

258
00:24:12,880 --> 00:24:22,240
is evolving. Is there a metric for measuring the, I guess, like the dispersion of the vectors

259
00:24:22,240 --> 00:24:27,840
in a given embedding? Like I'm wondering if in that example, where you train an embedding on

260
00:24:27,840 --> 00:24:34,960
this engram data, can you look at something analogous to like the standard deviation of the words

261
00:24:34,960 --> 00:24:42,560
or like the spread of the degree to which the not really I don't think? I mean, if you look at the

262
00:24:42,560 --> 00:24:47,120
entire set of vectors that made itself, I don't think so. Because what you're doing is you're

263
00:24:47,120 --> 00:24:52,800
starting with a bunch of random vectors. Then you're slightly adjusting them. So that specific

264
00:24:52,800 --> 00:24:58,880
pairs have specific relations between them. And that specific groups have specific relations

265
00:24:58,880 --> 00:25:03,040
between them. So they're more or less at the same distance from each other. But all of these

266
00:25:03,040 --> 00:25:08,080
groups and all of these words can be arbitrarily rotated with respect to each other. So I suspect

267
00:25:08,080 --> 00:25:13,200
that if you just get even very high quality embeddings and you just start measuring distances between

268
00:25:13,200 --> 00:25:19,920
them, it will look random. Because you're putting in too much noise. The distance between, I don't know,

269
00:25:19,920 --> 00:25:27,920
bed and blue might, doesn't necessarily mean any material. But then more generally, there's not

270
00:25:27,920 --> 00:25:36,800
a set of statistics or metrics or things that are relevant at the aggregate level for the embedding.

271
00:25:36,800 --> 00:25:41,520
Not that I know of now. Usually what people do is they look for specific relations between

272
00:25:41,520 --> 00:25:47,760
specific words. So they do this analogy problem. Yeah. Yeah. Because that's kind of how you check

273
00:25:47,760 --> 00:25:52,000
that what you're capturing is what you think it is. They're actually capturing the semantic

274
00:25:52,000 --> 00:25:58,240
meaning of the words and the semantic relations between between words. Why do we end up using

275
00:25:58,240 --> 00:26:04,800
neural nets to do embeddings that seem like pretty basic math that you can do like more

276
00:26:04,800 --> 00:26:12,080
deterministically? Yes. So I think that's one of the one of the motivations for people to do

277
00:26:12,080 --> 00:26:17,920
to invest in glove and glove is the reaction for global vectors. And this is I want to say it's

278
00:26:17,920 --> 00:26:23,200
coming from Stanford. Okay. And basically they try to do it basically in a deterministic

279
00:26:23,200 --> 00:26:27,120
way, in an optimization process, defining an optimization process by specifically saying,

280
00:26:27,120 --> 00:26:32,080
I want factors that where the relationships are given in this specific way. Right. The reason

281
00:26:32,080 --> 00:26:38,720
I think one of the motivations for using neural networks for this is because now you have very powerful

282
00:26:38,720 --> 00:26:47,680
tools to optimize and train neural networks in large scales. So you can use all of that machinery.

283
00:26:47,680 --> 00:26:51,760
Because when you actually look at the mathematics and the network structure it's actually using

284
00:26:51,760 --> 00:26:57,040
it's actually very simple. So it's an extremely simple neural network. Right. And where it's mostly

285
00:26:57,680 --> 00:27:02,240
vector multiplications and then a soft max at the end with some exponentials. So it's nothing

286
00:27:02,240 --> 00:27:08,400
particularly sophisticated. If you look at networks like ImageNet or AlexNet or something that have

287
00:27:08,400 --> 00:27:13,040
dozens of layers, they're very complex. Right. So you have all the technology to develop for

288
00:27:13,040 --> 00:27:16,480
these very complex things. This being applied to something that's very simple, so it makes it

289
00:27:16,480 --> 00:27:22,240
very efficient. So single layer, the matrix multiplications are just applying your weights coming in

290
00:27:22,240 --> 00:27:27,200
and coming out and then remind us what softmax is. So softmax is just an optimized way basically

291
00:27:27,200 --> 00:27:32,960
to calculate what is the maximum value of a vector. In a very simplified way, while at the same

292
00:27:32,960 --> 00:27:38,000
time basically turning a vector of numbers into something that's normalized. So the only thing you

293
00:27:38,000 --> 00:27:44,000
do is for each element of the vector, just take the exponential of that value and then you divide by

294
00:27:44,000 --> 00:27:50,560
the sum of all the exponentials. That's a softmax. And what that means is basically makes any value that

295
00:27:50,560 --> 00:27:56,320
is slightly larger than the others will become much larger. So it's easier to capture the maximum

296
00:27:56,320 --> 00:28:00,560
value. And then of course, there's many optimizations on top of that. That's the general idea.

297
00:28:00,560 --> 00:28:05,440
Because you don't necessarily want to calculate all the exponentials for the entire vector.

298
00:28:05,440 --> 00:28:11,520
Because these vectors are basically one hot team bedding of the words in the in the corpus.

299
00:28:11,520 --> 00:28:16,400
So this can be 100,000 or a million words. Okay. So you don't necessarily want to have to do that

300
00:28:16,400 --> 00:28:21,280
at every iteration. So then there's hierarchical softmax. There's all sorts of tricks to train

301
00:28:21,280 --> 00:28:25,920
optimizations on the more like computational tricks. But the concept is very simple. And then there's

302
00:28:25,920 --> 00:28:30,960
of course optimizations that people apply to it to make it more efficient and more robust.

303
00:28:30,960 --> 00:28:39,280
Okay. And so what's the relationship between the one hot encoded, 10,000 dimensionality vectors

304
00:28:39,280 --> 00:28:46,720
and 100, 500 vectors. So like I said before, the computers have a hard time understanding text,

305
00:28:46,720 --> 00:28:50,800
but they're very good at understanding numbers. Yep. So what you do is you map in the beginning,

306
00:28:50,800 --> 00:28:56,160
the first approximation is you map words to numbers. So just have a dictionary. This is word number

307
00:28:56,160 --> 00:29:00,160
one. This is word number two. That's your one hot encoding. And that's one hot encoding.

308
00:29:00,160 --> 00:29:03,920
So it's just the way you represent the world so that then you can manipulate them

309
00:29:03,920 --> 00:29:10,320
numerically to so that you can calculate these vectors. So you I'm imagining that you're very

310
00:29:10,320 --> 00:29:16,720
first step. Then your your input layers and your, well, it's just one layer, but your inputs,

311
00:29:16,720 --> 00:29:23,040
you're sending in the inputs to the hidden network. And then it's the one hot encoded. So basically

312
00:29:23,040 --> 00:29:30,000
corpus, you have the input values here. And these are, let's say in the case of skipgram,

313
00:29:30,000 --> 00:29:36,080
this is just the context word that you have in them. And this would be if your dictionary is

314
00:29:36,080 --> 00:29:43,680
10,000 words, this would be a one hot encoding, 10,000 dimensional vector. Right. This gets fed

315
00:29:43,680 --> 00:29:49,040
into the hidden layer in the center, which is the dimension of the embedding, say 300.

316
00:29:49,840 --> 00:29:55,920
And then from this hidden layer, you're trying to calculate the context. And the context can be

317
00:29:55,920 --> 00:30:00,480
depending on the window size, can be, let's say, 10 times the size of the dictionary.

318
00:30:01,360 --> 00:30:06,400
Okay. So it'd be 10 times 10,000, so the 100,000, because they're trying to predict 10 words,

319
00:30:06,400 --> 00:30:14,240
5 words before 5 words after. Okay. So then you've got your, so dimensionality 10,000 or so vector

320
00:30:14,240 --> 00:30:21,360
coming in your network is your, the dimensionality of your hidden vector, that's your hidden layer.

321
00:30:21,840 --> 00:30:28,720
And then the output side is 100,000, it's like 10 times 10,000. If your window size is 5,

322
00:30:28,720 --> 00:30:33,280
you're trying to predict all the context from the single word, then you're trying to go from 10,000

323
00:30:33,280 --> 00:30:41,760
to 300, to 100,000. But you're not really using the 100,000, you're just using that as kind of

324
00:30:41,760 --> 00:30:46,640
an optimization to get at the weights for the 300. And that's what you use. Yes. So basically,

325
00:30:46,640 --> 00:30:52,400
you can think of over to back actually as unsupervised learning problem, because you're feeding it,

326
00:30:53,520 --> 00:30:57,920
you're feeding it, the inputs and the outputs, so that you can learn something about the system.

327
00:30:57,920 --> 00:31:03,680
So you're not really in practice, you're never really trying to predict the output. In the set,

328
00:31:03,680 --> 00:31:07,280
I guess you could use that if you're trying to generate, use a generate text without

329
00:31:07,280 --> 00:31:12,880
anything unusual. Okay. So you're trying to basically see what the network is learning from the

330
00:31:12,880 --> 00:31:17,280
text that you're giving it, and you're giving it the word and the context, and you say, okay,

331
00:31:17,280 --> 00:31:21,520
figure this out, figure out what is the right representation that from this input can generate

332
00:31:21,520 --> 00:31:25,360
this output. Okay. And then in the end, what you're interested in is actually this internal

333
00:31:25,360 --> 00:31:31,040
representation, this wording beddings, inspectors. Awesome. Awesome. This has been super helpful.

334
00:31:31,040 --> 00:31:35,360
Like, I feel like I finally understand it. It's easier to explain with pictures and drawings,

335
00:31:35,360 --> 00:31:42,000
but yeah, for folks that are not in the room, we're not moving our hands around and drawing on,

336
00:31:42,000 --> 00:31:46,000
you know, layer in the air and stuff like that. I can send you a link to my slides. Actually,

337
00:31:46,000 --> 00:31:49,680
I was actually just going to ask, like, if someone wants to learn more, what's the best way for them

338
00:31:49,680 --> 00:31:54,240
to learn more? I mean, all of these papers are online, you can find them easily. I'm preparing

339
00:31:54,240 --> 00:31:59,600
this tutorial for O'Reilly AI in San Francisco in September. Okay. I will post all the slides and

340
00:31:59,600 --> 00:32:06,000
all the code in my GitHub. Okay. Right now, there is already the slides for a shortened version that

341
00:32:06,000 --> 00:32:10,640
I presented a couple of weeks ago. So I can, I can share that with you. And then eventually,

342
00:32:10,640 --> 00:32:15,120
after O'Reilly AI, I will update it with all the, the newest stuff. Okay. Great. Yeah. So,

343
00:32:15,120 --> 00:32:19,040
send that over and we'll get that in the show notes. Excellent. Yeah. Great. Thanks so much for

344
00:32:19,040 --> 00:32:28,480
stopping by. Thank you for inviting to the pleasure. Awesome. All right, everyone. That's our show

345
00:32:28,480 --> 00:32:35,760
for today. Thank you so much for listening. And of course, for your ongoing feedback and support.

346
00:32:36,400 --> 00:32:42,720
For the notes for this episode, head on over to twimolei.com slash talk slash 48.

347
00:32:43,600 --> 00:32:48,560
If you like this episode or have been a listener for a while and haven't done so yet,

348
00:32:48,560 --> 00:32:53,840
please, please, please take a moment to jump on over to iTunes and leave us a five-star review.

349
00:32:54,400 --> 00:32:59,760
We love reading these and it lets others know that the podcast is worth tuning into.

350
00:33:00,480 --> 00:33:07,120
One last note, you've probably heard me mention Strange Loop. A great technical conference held

351
00:33:07,120 --> 00:33:13,680
each year right here in St. Louis. We're a bit over a week away from that conference,

352
00:33:13,680 --> 00:33:19,440
so I encourage you to check them out. It's thesestrangeloop.com. I'll be there,

353
00:33:20,080 --> 00:33:25,600
let me know if you'll be there too. For more info on any of these events, check out the show notes.

354
00:33:25,600 --> 00:33:44,400
Thanks again for listening and catch you next time.

