Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
We are back with our third show this week and the third in our Autonomous Vehicle Series.
My guest this time is Katie Driggs Campbell, postdoc in the Intelligent Systems Lab at Stanford
University's Department of Aeronautics and Astronautics.
Katie joins us to discuss her research into human behavioral modeling and control systems
for self-driving vehicles.
Katie also gives us some insight into her process for collecting training data, how social
nuances come into play for self-driving cars, and more.
Our Autonomous Vehicle Series is supported by Mighty AI and I'd like to take a brief
moment to thank them for their support.
Mighty AI helps companies working in the Autonomous Vehicle market create training and validation
datasets to support computer vision.
Their platform combines guaranteed accuracy with scale and expertise and includes annotation
software, consulting and managed services, proprietary machine learning, and a global
community of pre-qualified annotators.
If you haven't caught my interview with their CEO, Darren Nakuda, which was the first show
in this series, please be sure to check it out at twimlai.com slash talk slash 57.
And of course, be sure to visit them at www.mty.ai to learn more and follow them on Twitter
at Mighty underscore AI.
Before we jump in, if you're in New York City next week, we hope you'll join us at
the NYU Future Labs AI Summit.
As you may remember, we attended the inaugural summit back in April.
This year's event features more great speakers, including Karina Cortez, head of research
at Google New York, David Venturelli, Science Operations Manager at NASA Ames Quantum AI
Lab, and Dennis Mortensen, CEO and founder of startup x.ai.
For the event homepage, visit aiSummit2017.futurelabs.nyc, and for 25% off of all tickets, use the
code twimla25.
And now on to the show.
All right, everyone, I am on the line with Katie Driggs Campbell.
Katie is a postdoc at Stanford in the Intelligent Systems Laboratory in the Department of Aeronautics
and Astronautics.
Katie, welcome to this week in machine learning and AI.
Yeah, thanks for having me.
I am really looking forward to this conversation.
So as you know, we are kind of in the midst of a series of podcasts on autonomous vehicles.
And usually we end up talking to folks that are in CS or electrical engineering or other
disciplines, but you are out of the Department of, again, Aeronautics and Astronautics.
And so I'm really looking forward to digging into the connection between that and autonomous
vehicles.
And I guess a good way to start is to have you tell us a little bit about your background
and your path to what you're doing now.
Yeah, yeah, sure.
So I'll tell you a little bit about, I guess, how I ultimately ended up in an aerospace
department.
I started out in electrical engineering, actually.
So I did my undergrad at Arizona State University, but I was really interested in control systems
and ultimately robotics.
So I was really interested in how we can control and interact ultimately with people.
So when I applied to grad school, I applied to mostly robotics programs.
So then I ended up at UC Berkeley, where I worked with Ruzuna Baichi in robotics, and
that was in electrical engineering and computer science.
So I slowly shifted to computer science.
And that's, I guess, when I first started working on intelligent vehicles.
So when we first got started on this, this was, you know, six some years ago.
So autonomous vehicles weren't, I guess, they hadn't really quite hit the road.
So we were actually looking at...
More pun intended.
Exactly.
Exactly.
We were actually working on semi-autonomous vehicles, and we found that the big problem
with semi-autonomous vehicles was how do you model the human?
So we started to think about how we can model the human, how we can design control systems
to keep people safe.
So we thought a lot about texting while driving, how different environmental influences
will affect your driving abilities, and how we can ultimately, you know, design active
safety systems to keep you safe.
And slowly over the years, this shifted to autonomous vehicles like everyone else.
We're still the human had a huge impact.
So we were still thinking about how people either interact with autonomy in the vehicle,
or how the autonomous vehicle can interact with other people or human drivers on the road
around them.
That's sort of how I slowly shifted into autonomous vehicles.
Yeah.
So then I finished my PhD, and I started working in an aerospace lab.
So we're the AI section of the aerospace department here.
And so the lab I work in is famous for air flight and collision avoidance systems, actually.
Okay.
Yeah.
So thinking about how we can design safety systems for planes, which isn't all that different
from vehicles, actually.
So the lab I'm in is actually mostly funded by vehicle companies and working on autonomous
vehicles and decision making for vehicles.
So it's sort of a different than maybe you're what you might expect from an aerospace
department, but a lot of the systems are pretty similar.
And I imagine that aerospace departments have been working on this problem of, you know,
auto pilot, right, comes from, you know, piloting a plane and, you know, space programs and
things like that have been trying to have autonomous or semi-autonomous remote vehicles
on, you know, like the lunar land or things like that.
I imagine it was that there's quite a history in aeronautics and aerospace departments
around this kind of work.
Is that the case?
Yes.
Yes, definitely.
So I've heard aerospace described as the department that is or studies the system of systems.
Which is exactly what autonomous vehicles are, exactly what planes are, but it's really
thinking about how all these different components and the pilots and autonomy and all these things
sort of fit together.
So yeah, definitely.
And exactly what you said, there's a long, long history of dealing with automation in
the aerospace industry.
So it really sort of fits nicely into a lot of the same curtain.
Awesome.
Awesome.
So what's your research focus there?
So here I am currently working on generally autonomous vehicles.
So a lot of the decision making and control.
So how can we use things like deep learning to come up with very human-like decisions?
Okay.
And how then do we use these sort of, perhaps not the most trustworthy systems like deep
learning can be?
How can we design that robust controllers to execute these decisions?
So how do we still get some of the robustness from the more traditional learning control techniques
while using these more advanced sort of AI tools?
Hmm.
So what do you mean when you say human-like driving?
Right.
So I think, well, personally, when I think about autonomous vehicles, hitting or coming
onto the road relatively soon, sorry, no more puns.
We need to think about how they'll integrate with the human drivers that are currently
on the road.
We can't expect that there's going to be homogeneous autonomous vehicles anytime soon.
So they need to be able to interact with other humans on the road, and the other humans
on the road need to be able to interact with that autonomous vehicle.
So you can't have the autonomous vehicle doing things that are unexpected, even if they
might be optimal in some sense, they need to be optimal in the social context that it
will be driving in.
Mm-hmm.
And so responding to erratic St. Louis or New York drivers, those are the ones that I know
the best, but I'm sure everyone has their, I guess, probably wherever you are.
Yep.
The drivers that you hate.
Yep.
California stops.
Exactly.
Exactly.
So you mentioned that.
And then you also mentioned an aspect of the research that is, so we're doing all these
things to build out these deep learning models.
And then maybe we think about the deep learning models as directly controlling things, but
it sounds like you're suggesting that a big part of your research is, well, maybe if we
put some other stuff between the deep learning models and the drive by wire systems, you
know, we can get better results.
Yes.
Yes.
And I think, from my perspective, it's really important, until you can really prove things
about how the deep learning and the decision making that comes from these learning models
or how they're going to function.
I think it is important that you have sort of some more reliable method for actually executing
these maneuvers and making sure that they're safe and interpretable even.
So can you maybe walk us through some of the specifics about the research in each of
these camps?
I guess we can start with what are all of the research challenges associated with autonomous
vehicles interacting with human driven vehicles?
Yeah.
Sure.
I think one of the really big problems is that you basically, you can model people, but
every model is only going to, or it's going to have places where it fails.
So coming up with a way to balance, you know, a general model of how people typically behave,
while still capturing sort of the crazy things that people also do, is a really hard problem.
As you get something like, you're either not accounting for the places where you really
need to be safe.
So when people do something really unexpected or maybe do something kind of crazy, but
on the flip side, you'll be over-conservative if you only model those things.
So you still want to be able to get to your destination.
So there's some balance between, you know, being safe and actually thriving normally.
So figuring out how to balance that in an intelligent way is really, really difficult.
And what's the general approach to that that you've taken in your research?
So the general approach that I've taken is trying to do something like switching maneuvers.
So if you have, you try and detect basically when people are starting to deviate from the
normal sort of expected behaviors.
If you have a typical model of how people behave, you can use that for the most part, but
you have to always be aware and always be monitoring people for when they start, as I said,
deviating.
So you look for sort of like the anomaly driver.
And then you can start saying, okay, this person's acting a little bit weird.
So I'm going to be more conservative around this driver.
What does it even mean to have a model of how people behave?
Like I guess I think of, I think of the stuff that I've seen, you know, around deep learning
is maybe like a lower level, you know, kind of lower level models or capturing lower level,
you know, ideas about how to interact, how a vehicle might interact with the world.
Is there also a part of the system that is, you know, trying to capture at large, like
the behavior of people?
Yeah.
So at least in my work, there is a lot of my PhD work was coming up with general models
of how people behave.
So how do you take all these big data sets and come up with models that are useful from
that?
So yeah.
So whether some examples of things that you can get a model to, that you can kind of capture
in a model about a person's behavior with regard to driving in particular.
So in driving in particular, we've really been thinking about lane changing behaviors.
So it's a pretty common maneuver and it's pretty easy to find examples of.
So in this work, we've been thinking about how people respond to merging behaviors.
So if you try to cut someone off, how are they likely to respond?
If you want to execute a maneuver, what sort of cues do you need to send to that person
to make sure that they will actually let you in if the gap is not big enough?
So how do you sort of handle these like social nuances in your motion and in your trajectory
planning?
Okay.
What's the general approach you've taken to address that kind of thing?
Like is it changes to the way you model or is it, you know, a set of heuristics that
you kind of build around the model or are you injecting things into the system otherwise?
Right.
So in this original work for modeling how people behave and how people respond, we were
really trying to think about how we can come up with a robust prediction.
So we actually came up with a new modeling method to capture these both behaviors in
a sort of a more general fashion.
So instead of thinking about trying to predict a person by guessing their exact trajectory,
we started thinking about how we can come up with basically sets that they might follow.
So we think of an area that they might enter basically.
So when you start thinking about things in terms of set behavior instead of just an exact
trajectory, you get a much more robust prediction.
So you might be off a little bit but you'll still capture the general behavior.
And this is sort of where what I was mentioning before with balancing being over conservative
and being quite precise kind of comes in.
So when you start basically reducing the uncertainty in your prediction, you'll shrink
this set down to something that's smaller and more precise.
But if you're more uncertain or you start detecting some anomalies, you can grow this
set out and capture more of the uncertainty.
And that will just automatically influence how you change.
So if you basically take these sets and incorporate them into your low level controller that
is planning and trying to keep you in safe regions, this will sort of automatically be captured
by that.
And in this work, does the set represent are you evaluating the set in terms of kind
of, you know, in or out or likelihood of in or out or are you looking at like geographic
regions as probabilistic fields that are maybe more continuous?
Right.
Right.
So a little bit of both.
So the ultimate or the output of this model is something like confidence intervals.
So you get these sort of strict boundaries and a probability associated with these different
basically level sets of trajectories. So it's a little bit of both.
So once you pick what confidence interval you would like to pick, you have a strict set
and you can basically evaluate this by inner out.
But you can also look at this as basically a series of confidence intervals.
So then you get something like an empirical distribution back out.
Okay.
Okay.
When you're implementing this in the lane changing context, is it, I guess I'm trying
to to picture the, I guess, dimensionality is overloaded.
But like if you are you thinking about it from the perspective of a car and like the lane
ahead, you know, how many feet from the vehicle, another vehicle is likely to intrude on.
So like a two dimensional kind of interaction or is it more a three dimensional interaction
where you are thinking about the distance between your car and your lane and the other car
in the other lane as well as where it might intrude into your lane?
That's a great question.
So in most of the work that I've been doing, I've been looking at basically a set of three
vehicle interactions.
So if you think about predicting the vehicle that you want to merge in front of, you can
think about the influences of that vehicle basically being what your actions will be in
a vehicle in front of them.
We've been sort of looking at that network, but we can expand this out by sort of looking
at things like clusters of behavior.
So if you think about different vehicles, sort of inserting more influences on vehicle,
you can think about how this sort of generally happens by looking at clusters of behavior.
So if there's more vehicles, you can basically do a look up and get more clusters or similar
clusters, similar situations.
And by the similarity metric, you can expand this out to more vehicles and larger networks
and things like that.
Okay.
Yeah.
I've seen some interesting videos of how to driver behavior.
And I think even autonomous vehicle interactions with drivers where, you know, the vehicle isn't
given a certain level of aggressiveness.
It will just get stuck like it cannot execute a lane change.
And so there's, you know, there's this need that you're describing for the vehicle to,
you know, not just be able to anticipate human behavior, but to start to emulate human behavior
because like signaling to go into another lane isn't just, you know, putting on the blinkers.
It's also like starting to go into the other lane.
Exactly.
Exactly.
I was talking to someone in LA about this and they said, if you turn your blinker on that
is a cue for the other driver to just speed up.
Exactly.
So you don't just want that.
Right.
So you've got, you've got this ability to kind of predict other driver's behaviors and
when they're likely to enter your lane and you can kind of expand that to multiple vehicles.
I, how do you, what's the next step?
How do you kind of build on that to create a more robust system?
Right.
So now that we have a good model of our environment, if we believe this is a good model of our environment.
So the next layer that we were thinking about is how can we safely execute sort of high
level decisions.
And this is where we started using deep learning.
So if we have a good representation of the environment we want to think about, should
I execute a lane change?
Will this help me achieve my goal?
And also there is some, we wanted to see if we could capture things like sort of implicit
rules.
So there are some rules that aren't necessarily captured by the letter of the law.
So like at intersections, there's a strict ordering.
That happens.
So if one vehicle comes first, they get to pass through.
There's some yielding rules, but people don't always follow these rules.
There's so much certainty in these rules.
So we've been using deep learning and simulation to try and capture some of these sort of nuance
behaviors to come up with these sort of high level decisions.
Like what high level action should I do?
So these high level actions can be things like execute a lane change or go ahead and move
through the intersection now.
And so we've been doing that with, as I mentioned, deep warning.
But deep learning in and of itself is not very trustworthy.
So a lot of what we've been doing is trying to think about how we can develop new tools
and new learning algorithms to try and make these systems more, or give some insight to
the confidence of the system.
So can we determine when our deep learning algorithm is uncertain about its decision?
And if we know when it's uncertain, we can decide whether or not we should listen
to it or not.
And this example, what's the training data?
So in this example, we've been putting a lot of effort into coming up with good simulated
traffic models.
So we can basically train in simulation and then transfer it to a real vehicle, which
is a whole other problem that we're also working on.
Okay.
So the simulated traffic data is, I mean, I'm envisioning like the video game Frogger.
You've just got all this traffic and you're, it's kind of moving at different speeds
and is that how you're not literally, but is, are you just generating several lanes
of traffic and how are you representing the vehicles in this training data set, for example?
Right.
So there's a lot of work in the lab that I'm in that goes into coming up with good driver
models for generating traffic.
So there's lots of really interesting work going on and actually using deep learning to
mimic these driving behaviors so you can validate your autonomous vehicle.
So how do you generate these scenarios?
How do you generate realistic behavior?
So then you can either train your system or just do some validation.
So if you just need to make sure it works or you get some metric for how likely this
should crash, you can use these validation tools to do that.
Mm-hmm.
I'm still trying to visualize the training data set.
Is it like are you looking at a car as a, like a two dimensional kind of representation
of points or space or something like that or are there some simplifying assumptions
or is it maybe more complex than that, you know, based on camera imagery or something?
So these ones we are just using, we have a simulated LiDAR sensor.
So from our ego vehicle, we basically just use these detection points, which are pretty
similar to what we can extract from sensors on a real vehicle.
So from the ego vehicle's perspective in simulation, you basically just get something
like a LiDAR image, but projected down to just a 2D plane.
Okay.
Got it.
So is it the 2D plane from above the vehicle or the 2D plane like looking ahead from a vehicle?
So for us, you can think of it as like an occupancy grid so you can like look down at the
world, your vehicles in the center of it, you get sort of this distance around the vehicle
there.
Okay.
Okay.
So the question was around the objective function, like how did you, how did you construct
the objective function for this model that you trained?
Right.
So for these initial models to start, what we were using was a, some tools called imitation
learning.
So we basically wanted to figure out how we can imitate a model or imitate some expert
model.
So we have some sort of expert behavior that we want to mimic.
So this can be a human, for example.
So if we have some examples of how the human is going to behave or some example trajectories
of this person driving, we basically want to try it and be as similar to this driver as
possible or similar to this human.
So mimicking the expert in training and transferring this knowledge over to our novice or our
deep learning algorithm.
Okay.
So you've got your driver behavior model.
You've trained a deep learning model that can try to optimize an objective relative
to, you know, some expert that it's imitating what, what's next.
So then once we have a model that can effectively mimic these high level decisions, hopefully
pretty well and hopefully in a safe way, that's when we started thinking about how we can
actually implement this in a robust controller.
So we've been using some pretty standard tools from control, like model predictive control
and robust control so we can take these high level commands and turn them into trajectories
that the vehicle then can follow quite smoothly.
So basically using these commands or these high level commands from the deep learning,
now you can sort of start thinking about different models and sort of some of the differences
between the simulation and the real world car.
So you can sort of extract the high level information and execute it in the real vehicle.
So that's actually what we're testing now.
So we're putting this on a real vehicle and testing all of that.
Nice, nice.
You mentioned a couple of disciplines in control systems, robust control and model, something
control.
Model predictive control.
Model predictive control.
Can you walk us through what those are and the assumptions that they're making and what
they're trying to accomplish?
Yeah, yeah.
So the heart of all of what we do is model predictive control and then model predictive control.
So basically using a model of your vehicle and the environment.
It basically solves an optimization problem to give you an optimal trajectory over some
finite time horizon.
So say two seconds in the future, I'm going to plan the optimal trajectory to achieve
my goal.
And since this is just an optimization program, effectively you can easily put in things
like safety constraints, you can tune your trajectory.
So you have a smooth trajectory and basically by solving this problem, you can come up with
your optimal trajectory.
And the kind of cool thing about model predictive control is even though it's a finite time
horizon.
So it only works for about two seconds in the future, whatever your finite time is.
And it's an open loop trajectory.
You basically take one step in the future and then you re-solve the problem.
So you re-solve this trajectory or for this trajectory at each time step.
And so by doing this, receding horizon by sort of sliding along and constantly planning
some time in the future, you actually approximate things like the infinite horizon.
So basically the optimal control policy.
So it's an open loop policy executed in a close up action.
I was recently reading some reviews of some of the production driver assistance, like
autonomous driver assistance, if you will, technologies like Cadillac has one, Mercedes
has one.
Obviously Tesla, I forget the other one.
I think it was infinity that was in this article.
And they talked about how one of the things that they noticed was that for most of these
systems, I think Tesla was the only exception.
Like the car would basically bounce back and forth between the lane markers.
And it, and I'm speculating a little bit, but it sounds like what you're doing with model
predictive control would tend to smooth out that kind of effect as opposed to, you know,
maybe deriving a path straight out of a deep learning model.
Like is that a reasonable kind of intuition about this?
Or does it show up in other ways?
No, that's exactly right.
So by using this basically constant smoothing and optimization technique, you do come up with
much smoother trajectories.
It not only addresses things like the sort of jerky chattering, I guess that happens when
you sort of oscillate between lanes, but it also helps for things like overshoot and
things like that as well, or is jumping back and forth when you do things like turning,
that usually comes from common planning and techniques like that.
And now you're making me remember, you know, grad school control systems courses with
like dampening and all these other things that you need to think about to avoid, you know,
overshooting and oscillation.
Yep, yep.
Yeah.
It's amazing how now that I'm implementing things on a real vehicle, how all of these
original controls that are really coming back, I think I haven't thought about like 10
years.
Nice.
Nice.
Very important.
What are you seeing as you're trying to, you know, go from these models to implementing
them on a real vehicle?
Are you finding that, are you surprised by anything or are there things that, you know,
were working fine in simulation, but, you know, you needed to be tweaked as you moved
to a real vehicle?
Yes.
I think one of my favorite quotes was, or is, everything is doomed to work in simulation.
So.
Nice.
We have all these nice simulation tools, but, and we think we have things working really
well, but a lot of it is still has to be hand-tuned and it's amazing how some of these simple
things are, especially when you kind of get caught up in a lot of the really cool stuff
that's happening in AI, you think a lot of the really cool stuff comes out of the deep
learning, but when you go to actually test things on the vehicle, so much of it comes
down to this low-level control, actually.
So it is kind of amazing how much time is actually spent on the, I guess, more traditional
things.
So as you said, like, the dampening and making sure everything is smooth and tuning
cost functions and things like that.
So I think there's still a lot of work that has to go into the end to end stuff to make
sure it works well.
And is there kind of research into applying some of the ways we optimize machine learning
to optimizing, you know, these control systems, like, you know, can you think of, can you
apply hyperparameter optimization to these control systems to find the right dampening
constants and all that kind of stuff, or is it, are the, you know, traditional techniques
kind of good enough there?
Yeah, so I think you can do some of those things, but again, you still have this problem
where you ultimately have to put all this on a vehicle, you have to test it there,
and because you're testing on a real vehicle that has had a lot of very expensive equipment
on it.
You have to be kind of careful with what you are willing to test and make sure you're
very confident in this.
So you can't do a full, you know, cross validation set on there unless you're confident
that it's going to work really well.
But yeah, there's some work that we've been working on in robust reinforcement learning.
So trying to do things like take into account some of the uncertainty that we think we might
see or some of the things like model mismatch and trying to create basically make sure
that our control systems and our deep learning algorithms are robust to these uncertainties.
So I think that should help it, but we'll see if that actually works or not when we actually
go to the real vehicle without work.
Can you elaborate on that work a little bit more?
Yeah, sure.
So we're building up with some tools in from what's called robust adversarial reinforcement
learning.
So in this framework, the basic idea is if you have some uncertainties in your model
or uncertainties in your environment, you can actually treat this as a game between
two players.
So if you have your controller, which is we'll call it the protagonist and your antagonist,
which is basically these disturbances or these uncertainties that are trying to sort
of perturb your model in a negative way.
So if you basically treat these both as agents that you want to train a model for, you can
train your protagonist to achieve some goal and then you can train your antagonist to basically
get you to not achieve that goal.
So by training these two, basically, your antagonist and your protagonist iteratively,
you develop something that approximates a robust controller.
So you have your ultimate system is able to handle things like uncertainty in your model
because you've been training it against a adversarial uncertainty or disturbance.
So that's one way we're trying to deal with these model mismatches.
We'll see how it works.
I'll get back to you on that.
Yeah.
Yeah.
Are you finding that there's a kind of a cultural mismatch between the folks that come
from the traditional control systems perspective where you've got established practices
and well-defined guarantees or at least laws of physics to kind of rely on versus the folks
that are more of the deep learning camp where you're kind of just throwing a bunch of
data against the wall and this thing is miraculously training itself.
Yes.
Yeah.
There's a huge cultural gap there.
So one of the things I try and do is try and sort of bridge that gap.
But I think there's just a, it's almost like they're speaking different languages.
These two communities tend to get a little bit territorial in what they can and can't
do.
So yes.
In short.
Awesome.
Well, to close us out, what's next for you beyond getting your vehicle working, not
that that's like a short task or a foregone conclusion, what are some of the other things
on the horizon for you and your work?
Yeah.
So some of the other things I've been thinking about is thinking about autonomous vehicles
at a little bit of a higher level or a broader perspective.
So some of the recent work that I'm just starting has been on how can you use autonomous vehicles
and some of these tools from planning to do things like assistant evacuation and disaster
responses.
So using some of these same tools to help there.
I've also been thinking about how you can start applying some of these tools to fleets.
So you can think about optimizing overall performance by making small changes on a minor level,
have to do things when you have many, many vehicles all operating together.
So.
And so are these things like cooperative autonomy and swarming behaviors and the like?
Yeah.
Yeah.
A little bit of that.
And so I think the evacuation planning is a little bit more of the cooperative side for
the fleet performance if you still have individual operators.
So if you think of like a delivery fleet, you still have an individual there driving the
vehicle and making decisions.
So how can you optimize maybe their behaviors on a minor level and then have greater performance
overall?
Interesting.
Interesting.
So what's the best way for folks to learn more about your research or connect with you?
Yeah.
You can find me on the internet.
So you can look at my website.
So just stampord.edu, backstop, tilde, krdc, or you can shoot me an email, krdc at stampord.edu.
Awesome.
Awesome.
Thank you so much for joining me to discuss this is really interesting research and I'm looking
forward to keeping tabs on what you're up to.
Thanks.
Yeah.
It was a great conversation.
Thank you for having me.
Thank you.
All right, everyone.
That's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on Katie or any of the topics covered in this episode, head on
over to twomolei.com slash talk slash 59.
To follow along with the autonomous vehicle series, visit twomolei.com slash AV 2017.
Of course, you can send along your feedback or questions via Twitter to at twomolei or
at Sam Charrington or just leave a comment on the show notes page.
Thanks again to my DAI for their sponsorship of this series.
Be sure to check out my interview with their co-founder and CEO Darren Nikuda at twomolei.com slash
talk slash 57 and take a look at what the company's up to at www.mty.ai.
Thanks again for listening and catch you next time.
