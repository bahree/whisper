Welcome to the Tumel AI Podcast.
I'm your host Sam Charrington.
Hey, what's up everyone?
I want to send a huge thanks to Emily Bender and everyone who joined us for the viewing
party and AMA, we held with her earlier this week.
These have been a really, really fun way for us to connect with you and to connect you directly
with our guests.
If you miss any of them, you can check them out at twomelai.com slash viewing party.
I'm also really happy to share more details on our next panel, which will focus on advancing
your data science career during the pandemic.
The panel will be held next Tuesday, May 26th at 12 noon Pacific time.
I'll be joined by Hilary Mason, who you know from her time at Cloudera Fast Forward
Labs and this podcast, Caroline Chavier, Data Science Recruiting Maven and co-founder
of Paris Women in Machine Learning and Data Science, Jacqueline Nolis, co-author of Build
a Career in Data Science, which is Hot Off the Presses, and Anna Maria Eshevery of IBM
and the Open Data Science for All Project.
With the help of your questions, this amazing panel and I will explore best practices, tips,
advice and direction for those of you affected by layoffs, new to the job market, or ready
to accelerate your careers.
To register for this panel, visit twomelai.com slash DS Careers.
And now on to the show.
All right, everyone, I'm here with Alpha Lee. Alpha is a group leader in the Department
of Physics at the University of Cambridge, as well as co-founder of the Startup Post-Error.
Alpha, welcome to the Twomelai podcast.
Thank you very much for having me.
It's great to get a chance to meet and speak with you and I'm looking forward to learning
a bit about what you're up to, both from a research perspective as well as with your
start-up.
Why don't we get started by having you share a little bit about your background and how
you came to work at this intersection of chemistry and materials and machine learning?
Sure.
So, I started out as a chemist, and by undergraduate training, I was always fascinated
about making molecules and how can you make very complicated natural products from simple
selling materials.
So, I started off my academic training trying to become a chemist as an undergrad and
then I soon realized that being a lab is interesting and fun, but I also wanted to think
a step back and understand why and how chemical reactions work and understand from a theoretical
perspective.
That's why I did my PhD in mathematics at Oxford, where I looked into the physics of particles
in solution.
And then through my postdoc at Harvard, I thought, well, mathematical theories and physical
theories are very, very powerful, but there seems to be a gap between these theories and
actual materials and chemistry, and that's where the data part comes in.
Can we leave rich experimental data that has been done before to build better models, but
obviously adding the physics in as well?
A nice part, a nice bit is that actually the physics of particles and the physics of data
are actually very much related.
They are brought family of physics or statistical physics.
And that's how I got into some machine learning through this physics chemistry angle.
So since two and a half years ago, I returned back to the UK and started my own research group
in Cambridge, where we now work at the intersection between a chemistry of drug discovery, physics
of materials and machine learning, and so six months ago, I co-founded postera, which
offers medicinal chemistry as a surface powered by machine learning, which retakes the machine
learning approach that we developed once that further and deployed in the wild, so to
speak, in drug discovery projects.
Awesome.
Awesome.
You mentioned the physics of data.
What does that mean to you?
Is that just statistics or is there something more to it than that?
I think it's both about how do we understand noise in the data sets, for example, some
of my research pertains to estimating uncertainty in model predictions, both in terms of how
do we estimate measurement noise and how to measure noise on certain, due to not having
enough data in a certain area of chemical or material space.
And that motivates a line of research on patient deep learning or patient approaches to machine
learning in general.
And that's actually quite important in we link it back to real life experiments, because
experiments are often extremely expensive and you really want to be able to probe regions
of chemical or material space, which are the most fruitful and that you really gain the
most information from doing the least amount of experiments.
So the physics of machine learning and the physical process of doing experiments have become
pretty joined up together when you think about the whole process.
Several of the conversations I've had with folks working in areas like this rely heavily
on simulation for their work, do you as well?
We take simulations obviously as one sort of data, and then there you're often extremely
powerful in the middle of pure ML and pure experiments, which usually experiments are
usually slightly slower and often more costly.
But I think we are now trying to put the physics behind those simulations into the construction
machine learning models or conversely interpret the architecture of the simulations as a machine
learning model by itself.
So for example, a lot of physics-based simulations often contains parameters, often contains equations
that are empirically parameterized and a strand of my research is indeed trying to interpret
these very powerful simulation engines as machine learning models themselves.
And then you can think about, well, can I judiciously tune and decide these parameters based
on data?
Because I think the physics is so much to offer in terms of the frameworks, modeling frameworks
and data is so much to offer in terms of fine tuning, the gap between predictions and observables.
Can you elaborate on that a bit more?
What does that mean to turn a physics simulation into a machine learning model?
Right.
So for example, we recently tried to use machine learning to predict the structure of complex
liquids.
So if you have a bunch of particles moving around in the liquid state, a question in physics
as well, what will be the structure of these particles, what will be the structure of the
liquid?
That's one of these very basic questions in soft condense metal or condense metal physics.
And a branch of physics known as liquid state theory parameterized, discussed a very elegant
way to solve this question.
And there's one equation called the constitutive equation or the closure relation that is unknown.
And although the physical framework is there, that equation is what sort of hinders a lot
of progress.
And we say, hey, why not just take the framework and parameterize the equation using data.
Just do a lot of simulation data and parameterize the equation using ML.
And that's a lot better than throwing away the whole physical framework.
Just use ML to learn everything from scratch because it's so much progress there.
So that's one example where we can take a classical theory, look at the weakest link,
which is usually an empirical equation and say, okay, let's tackle the empirical equation
using ML, but leaving the classical theory intact.
Awesome.
And so ultimately you're trying to apply this to medicinal applications, drug discovery.
But also, I guess I'm curious about the relationship between drug discovery and materials and material
science.
I often think of drugs more from the perspective of their chemical properties and materials
in terms of wanting to create new kind of macro materials.
I'm not sure what the question is there, but I'm trying to get at the techniques the same
across drug and materials discovery, or are they very different?
I think the questions that we are trying to ask, obviously, are somewhat different.
Because as you alluded to from materials, you're usually interested in self-bulk properties
or drugs you're interested in properties at the molecular scale.
But I think in terms of the models that we construct, and in particular the philosophy
that we take and construct the models, that's very similar.
So for example, we use a lot of graph neural networks for chemistry because we can look
at a chemical compound or chemical molecule as a graph, and you can perform operations
on the graph.
And we have extended this to think about Bayesian graph neural networks because uncertainty
is extremely important for drug discovery.
But conversely you can also think about a formula of an inorganic material, let's say a
battery cathol material as a graph as well.
So you can think of a formula of a material as a graph, and recently showed that we can
basically featureize, let's say, a cathol material of battery as a graph, and then using
this material graph to predict materials property and also estimate uncertainty and drive
experiments.
So in those are the methodology, I think there's a lot of synergies and similarity.
And I think in terms of thinking about the whole design cycle in chemistry and drug discovery
and everything about the design, make test cycles or how to design compounds, how to synthesize
compounds in the lab, which usually is the rate determining step actually, and how to design
experiments to test compounds.
And if you think about that framework, then we can see a lot of parallels between why
it's done in chemistry and why it's done in material science.
Obviously, I think the key difference in material science is that experiments actually
even costlier than chemistry.
So in drug discovery, a lot of these measurements can be done relatively, there are protocols
to power chemical assays that can do these experiments to test where the compound is
actually potent against the protein relatively quickly.
From materials trying to make a new superconducting material and test it can be some a PhD project
in of itself.
So I think the throughput is very different, which means that it's a lot more interesting
ML, specifically in the low data limit that we can think about in material science.
You've mentioned a few times estimating uncertainty is being one of the goals or at least a useful
property to have in this type of work.
What are the types of uncertainties that you're trying to estimate and how do they play
out in the chemical and material realms?
Right.
I think the two types of uncertainty is first uncertainty due to having insufficient
data in a particular chemical space or material space, and that's usually known as the epistemic
uncertainty.
The second type of uncertainty is the uncertainty due to the measurements themselves because
a measurement inherently is not noise free and usually the noise can be an inhomogeneous
function of where you are in chemical space and that is usually overlooked in a lot of
approaches.
If you actually do an experiment, you know that some molecules are, some materials are
inherently more difficult to deal with than others.
And that's called, it's about allotoric uncertainty.
And we capture both within the framework of probabilistic patient deep learning and actually
capturing the variation of uncertainty as a function of where you are in chemical space
due to measurements.
I think it's something that is super important because in a lot of material discovery, typically
want to discover material that is robust rather than a material that will sometimes give
you good results, but other times less so.
For these different types of problems, how are you formulating your problem?
What are you modeling?
What are you trying to predict ultimately that you want to incorporate these different measures
of uncertainty into?
So we are to be trying to predict figures, figures of merit, for example, in drug discovery
that would be a balance between how strongly would a molecule bind towards a protein and
also how strongly would the molecule bind to other proteins which you would want to
avoid because that causes toxicity effects and as well as other properties like solubility
or other properties that require to make a molecule drop.
In materials, it would be the property trying to optimize, for example, so the band gap of
materials should try to think about photovoltaics or the strength of material, think about functional
materials like alloys, but nicely about our framework is that by thinking about materials
and molecules in a more abstract way, we're able to create frameworks that are generalizable
across chemical material space.
In the case of materials, the characteristics that you've mentioned, strength, for example,
is often the target you're looking to build stronger materials.
In the case of drug discovery, it's not like the things that you're trying to predict
aren't so much.
I think that the primary focus is efficacy against something.
Is that a prior or something that you know operating and you have a laundry list of things
that you want to test for these secondary characteristics or is efficacy also a part
of what you're trying to use machine learning to identify?
That's a great question, actually.
So in drug discovery, typically, we think about in terms of three stages, the biology,
the chemistry, and the medicine, I don't know why that way.
So the biology part is indeed about asking what is the question.
So if you want to cure a disease, which proteins you should target?
Yeah.
And then the chemistry part is conditioned on these proteins from the target, give me the
best molecule that can hit those proteins from the others and the medicine part is
so carrying forward the molecule to how do you think of drugs and impact patients.
So right now, we are not really working that much on the biology part.
I think it's a fascinating question, it's a very difficult question, we're not redoing
that much there yet.
I think a lot of very talented folks are dedicating a lot of effort in the more biological
parts.
So I will mostly put things to the chemistry part.
So we've talked a lot about the chemistry part thus far.
We also focus in your research on more theoretical questions around machine learning and algorithms
and tell us a little bit more about that aspect of your research.
Right.
So I think in the process of trying to come up with algorithms or chemistry materials,
we realize there are some interesting questions that are so general across the different algorithms
that we are dealing with.
For example, the question of well, we all use gradient optimizers to optimize algorithms
but from an optimist parameters rather but from a physics perspective, do we know why
these algorithms work and why do they not get stuck in bad solutions, high energy solutions
or high loss function solutions, do we know how the loss function landscape looks like.
I mean, those are questions that obviously has been considered by a lot of machine learning
pioneers in the past.
So we build on those works and so use techniques in statistical physics and pure mathematics
to sort of analyze and sort of toy models of these neural networks and trying to get
to the heart of why certain optimizers work and we in fact show that there's a very
nice synergy and mapping between machine learning and a statistical physics problem of
sort of free energy optimization that allows us to explain why these optimizers work and
we also then move on to think about what's the landscape of the loss function in machine
learning algorithm.
And so to derive a set of very interesting analytical results showing why deep networks
are easier to optimize than shallow networks.
And obviously, the end goal of these research is to try to think about whether they are
better inference algorithms and they we can obtain by taking a more principled view to
why and how algorithms work.
Okay.
Is there a short answer to why deep networks are easier to optimize than shallow networks?
So we find is that deep networks, minimized deep networks are actually closer together
than shallow networks that deep networks actually of good solutions are easier to find than
that solutions.
I think that's something in a nutshell.
So the summary of the analytical results, if you just minimize, it's just easier to find
good solutions than bad ones.
In our conversation thus far and in some of your work, you mentioned energy, energy landscape
and free energy and things like that.
Can you elaborate on how you use that concept and how it comes into play?
Right.
So energy landscape in physics and chemistry relates to the concept of how the potential
energy of a system changes as you change the system.
So for example, if I have two atoms connected by a spring, pull the atoms apart, the energy
increases because atoms really want to be together.
If I squash atoms too much, the energy also increases but atoms want to be sort of not overlapping
each other.
Therefore, you get a chemical bond.
You think of this, generalizing this concept to many, many degrees of freedom, so many,
many atoms and you have a whole landscape, depending on the coordinates of each atom,
you get a different energy.
Okay.
If you think of each atom as a parameter in the machine learning model, then aha, this
and you think of the energy as the loss function, then you can basically map a lot of what
we have, people have fallen about in physics and chemistry into ideas in machine learning.
So for example, glasses in physics correspond to the first certain interesting time of physics.
Sorry.
Sorry.
What in physics?
Glasses.
The idea of glass of a glass system.
So if you think about a sort of a glass, then that system is disordered, it doesn't really
have an order or crystalline structure in it.
It flows extremely slowly, in fact, a glass is technically not a solid because it flows
extremely, extremely slowly.
And the energy landscape of glasses actually shares some similarity with the energy landscape
or the loss function landscape machine learning models and it says that it's highly
disordered.
The parameters are definitely not regular at all, like you see very bumpy energy landscapes.
So that's one extreme.
The other interesting example to think about is proteins, for example, the energy landscape
of proteins are very fun.
So go funnel like because the protein needs to reach a state very quickly as it won't
vote.
And if we can engineer a machine learning landscape that is funnel like, rather than glass
like, and that's great because that means the machine learning model quickly finds the
best parameters with minimal effort.
So you can think about these physical objects and this analogies of particular models to
think about how we can optimize the models.
So you draw this parallel between physics and the kind of loss function optimization space
of models.
I guess I'm, here's what are some concrete results that you have, you know, either, you
know, from physics to machine learning or the other way around.
You mentioned a few things.
Are those, you know, how concrete are those as opposed to ideas if we could think about
the machine learning, the protein example that you use, like, have you actually implement
that implemented that and use that to find concrete deep learning architectures that converge
faster because they use some properties of physical protein?
That's a great question.
So we have had, we are still working towards that.
There will be a dream of this direction that research released from my perspective is
to find some math maps between the two and actually accelerate ML.
Yeah.
Right now we have characterized ML algorithms both numerically and analytically and
created, I guess, an idea of how how ML algorithms can be mapped to physical objects, physical
heuristics, right?
And then an active area of research is indeed trying to go once that further and think
about how to create some more optimizable ML algorithms.
Do you envision that there are things that we can learn about physics by observing machine
learning or is physics so much further ahead that, you know, there's probably nothing interesting
there?
Oh, I think physics is a lot to gain from machine learning.
I think a lot of physical theories are derived based on be looking at data and say, and
the physics is saying, oh, like, because I can only keep track of, well, two degrees of
freedom, if I'm plotting in 2D, that I want to get a line and I only focus on two things.
And that biases you towards sort of neglecting a lot of data, which could actually be interesting
and useful.
So in this theme, for example, we published a recent paper on a battery degradation where
a lot of practitioners in the field has sort of looked at spectroscopic ideas to predict
how the lifetime of battery, given non-invasive spectroscopic measurements, if you can do
that, then you can tell them, so how whether battery is still alive or not, or how many
cycles is left before it would die, and that's very useful for some like electric vehicles
or personal consumer electronics.
But the upshot is that a lot of folks look at, so very clear, observable features of
these spectrum, whereas if you just train the machine learning model on the spectrum to
use it to predict degradation, the machine learning model actually identifies very subtle
features.
So very high order correlations of features that you will never even expect to be important.
And then you can go back and go to the physics and ask, well, what does this feature correspond
to?
And in fact, we are now thinking about are there new explanations of battery degradation
based on just assigning, or this feature is actually important for degradation.
That's not something you can just do looking at the data or conversion of physical theories.
Interesting.
Do you think it's possible that some of the well-known analytical, close analytical results
that we take for granted in physics that are low-order polynomial actually have a lot
of these subtle additional features that will come to discover based on this line of thinking?
Or am I oversimplifying, I'm thinking like maybe it's not equals MC squared, but there's
all these other, you know, there are all these other kind of higher order features that,
you know, because of our bias to clean polynomial solutions, we've, you know, ended up, you
know, holding these relationships up that aren't as, you know, nuanced enough.
Yeah.
I think I would have a distinction between, um, solve fundamental physics frameworks and
how physics is being implemented and executed.
I think if you think about fundamental physics frameworks, like special relativity, general
relativity, quantum mechanics, gravity, I think.
And I was just in a group of low-order polynomial relationships.
I wasn't necessarily talking about relativity.
So I'm not an expert in those areas, and I think that the frameworks they are relatively
robust.
Um, but I think the implementation of these frameworks into material discovery or not usually
require lost simplifications, like quantum mechanics, for example, um, can we solve quantum
mechanics?
Um, numerically even, like super computers, like exactly numerically.
So you make a lot of simplifications on the way, the same with other areas of physics.
And the quality of these approximations can be massively improved with machine learning.
Got it.
So it's not that you're, you know, fundamental laws of thermodynamics aren't robust.
They probably are.
Yeah.
Pretty good.
But when in the real world, there are a lot of, there are a lot of other factors that aren't
captured by kind of idealized models that we tend to overlook and machine learning, you
know, what we learn from machine learning might give us a path to incorporating those
as features.
Yeah.
And I think also it's one of the lot of these physical frameworks are so very easy to express
mathematically.
Yeah.
But actually, I'm solving it for real world, for real molecule or real world problem is actually
exponentially complex, the quantum mechanics, for example, is very, it's very straightforward
to write down an ordinary or partial differential equation, but then to solve it for an, an
electron problem is really non trivial.
And so then that's why people start making a lot of these very good approximations, but
so these approximations could be improved when you take a step back and say, well, why
not just use existing measurements as a way to fine tune the approximations?
Do you in your work look at the activity that's happening around neural ODE research and
does that work?
I've heard of, I've certainly read briefly the paper and I think it's a very interesting
approach to how to integrate physics.
So frameworks like ODE's into machine learning, I think so much could be done to physics if
we start thinking about some semi empirical and empirical physical frameworks as machine
learning.
So people in automatic differentiation field, for example, think about differentiating
through machine learning models.
I've not done research in this area per se, but I am aware of a lot of amazing work coming
out of this community of thinking about maybe three simulations as machine learning and
do automatic differentiation through simulations, et cetera.
You are currently in Santa Clara in California, a long way from either of the Cambridges.
And what brought you there is the startup that you founded, Posterra, and the opportunity
that you had to go through the Y-combinator accelerator.
Can you talk a little bit about Posterra and what it's specifically looking to do?
Yep.
So Posterra is a company that tries to offer some medicinal chemistry as a surface powered
by machine learning.
And we realized that although a lot of excitement has gone into AIML for drug discovery, we take
a step back, we realized there are two key pain points amongst the whole life cycle.
So the first pain point is how do we make molecules?
So the chemical synthesis of molecules, and that usually is the rate determining step
when you try to make thousands of molecules.
Making a molecule has been the focus for a lot of academic and industrial groups, but
how to make molecules relatively less so.
And so we developed a state of the art algorithm.
Part of it is actually published a molecular transformer for reaction prediction and
retro synthesis, which is now the state of the art for reaction prediction, predicting
how to make molecules react and inverse from how to make molecules.
And we know that, and we think that this can really accelerate medicinal chemistry.
And the second pillar that we focus on is uncertainty.
How do we use uncertainty to design experiments such that we explore the most fruitful chemical
space?
And I think we are the, the first really thing about how to integrate the whole design
mech test cycle together in one machine learning platform and offering it to accelerate
drug discovery in both farm and biotech.
Okay, you mentioned molecular transformers that related to the concept of transformers
that we see in like natural language processing?
Yep.
It is inspired by that where we take the basically reaction prediction as a machine translation
problem.
Okay.
We're reacting reagents.
So what goes into the flask is treat as a language or comes out of the flask.
The product is in another language and you think about whether you can use translation
to as a as a as a as a heuristic concept to think about processing these inputs and outputs.
And what we have shown is that actually this perhaps very simple, simple, but direct way
of thinking about chemical reaction actually outperforms a lot of the more sort of heuristic
way of sort of hand crafting reaction rules or hand crafting chemistry templates from textbook.
This very simple approach based on translation achieves over 90% accuracy in predicting
correctly what comes out of the flask given what goes into the flask and that accuracy
outperforms even trained human chemists.
And so how do you go about training a model like that?
Are you doing similar types of techniques where you're doing close kinds of, you know,
you're trying to train in the model to predict things that are left out of your training
data or this is something different.
So we have over 9 million reactions reported in patents, which we have cleaned, aggressively
cleaned and augmented using sort of chemistry knowledge and we train the model on those
data and obviously we invalidated the model using standard training test splits.
Okay.
So you've got the existing relationships that you're training on, right?
Okay.
We use data has been published and we use patent data, which which is a data source of
robust chemical reactions and that means a lot of these chemical transformations can be
readily translated into industrial context.
What's the granularity of the or the format of the input data?
Is it kind of the chemical equations that we're used to seeing from basic, you know, chemistry
class and some representation?
Are you providing lower level physical information about the various molecules or atoms?
Yeah, so the data format is the molecular structure of the reactants, the molecular structure
of the reagents, these reactants plus reagents are what goes into the flask and the product
is what you isolate from this reaction again, the molecular structure is given and then
transformer takes these two inputs and outputs and performs machine translation to predict
the output given inputs.
And that reaction prediction step or reactions in general is actually, I think, very important
part in the whole chemistry stage in drug discovery.
For example, we're actually leading a COVID project, a life COVID project trying to develop
new ND virus against COVID is open source and non-profit initiative that we are having
to lead with academic groups.
And so we launch a crowdsourcing platform, sourcing compounds based on a fragment
merge, so a high throughput screen that our colleagues at Oxford ran, we got like
four, a few thousand structures.
If you were to just look at it one by one and decide which ones can be made easily, it
would take you weeks, whereas an algorithm like molecular transformer would take a weekend
to triage, like which compounds are easily make up both from purchasable starting material
and is that kind of speed, which allows rapid iterations and allows you to sort of make
more molecules, test more molecules and ultimately, the discovered drugs faster.
Okay.
And have any interesting candidates come out of that process yet for COVID?
Yeah, we have got several pretty promising hits against the target, which we are now
rapidly developing.
Oh, that's awesome.
So you started why a commonator in January, are you close to your demo day?
What's your January?
You just did our demo day.
Oh, you just did it.
I go, yep.
Oh, how did it go?
It went really well.
We were able to close our seat ground within a week.
Okay.
This is really interesting stuff, and I appreciate you taking the time to chat with me.
Are there, do you have any parting words or thoughts or resources that folks can follow
up on if they're interested in learning more about this area?
Yeah, I'm happy to chat with any folks interested in our work and so our COVID platform,
the plug on that is COVID.posterror.ai.
Our whole project is non-profit and completely open, so if folks are interested in tripping
in ideas or commenting on the ML or the chemistry, if we do so, we have a forum, a live forum
where a lot of the chemist discuss ideas, welcome people, so we're chiming in.
Awesome.
Awesome.
Well, Alpha, thanks so much for joining us.
Yeah.
All right, everyone, that's our show for today.
For more information on today's show, visit twomolai.com slash shows.
As always, thanks so much for listening and catch you next time.
