WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.280
I'm your host Sam Charrington.

00:23.280 --> 00:28.480
I'd like to start off this show by sending out a huge thank you to everyone listening.

00:28.480 --> 00:32.800
We've dropped a ton of great interviews over the past few weeks and through your dedication

00:32.800 --> 00:39.040
we continue to see a growing outpouring of feedback comments and shares with each release.

00:39.040 --> 00:43.600
If you're a regular listener but don't normally send in feedback we'd really love to hear

00:43.600 --> 00:44.640
from you.

00:44.640 --> 00:50.560
So please head on over to Apple Podcasts or wherever you listen and leave us a review.

00:50.560 --> 00:55.760
A five star review is of course appreciated but what's most important is that your voice

00:55.760 --> 00:56.960
is heard.

00:56.960 --> 01:01.840
It lets us know what you like or what you feel we can improve on and it also lets those

01:01.840 --> 01:06.880
looking for a new machine learning and AI podcast know that they should join the Twimble

01:06.880 --> 01:09.120
community.

01:09.120 --> 01:15.040
Speaking of community, the details of our next Twimble online meetup have been posted.

01:15.040 --> 01:22.480
On Tuesday November 14th at 3pm pacific time, we'll be joined by Kevin T who will be presenting

01:22.480 --> 01:28.080
his paper, active preference learning for personalized portfolio construction.

01:28.080 --> 01:32.000
If you've already registered for the meetup, you should have received an invitation

01:32.000 --> 01:33.960
with all the details.

01:33.960 --> 01:39.840
If you still need to register, head on over to twimbleai.com slash meetup to do so.

01:39.840 --> 01:41.480
We hope to see you there.

01:41.480 --> 01:45.760
Now as some of you may know, we spent a few days last week in New York City hosted by

01:45.760 --> 01:49.120
our great friends at NYU Future Labs.

01:49.120 --> 01:53.920
About six months ago, we covered their inaugural AI Summit and event they hosted to showcase

01:53.920 --> 01:58.960
the startups in the first batch of their AI Nexus Lab program as well as the impressive

01:58.960 --> 02:02.080
AI talent in the New York City ecosystem.

02:02.080 --> 02:06.160
While we were more than excited when we found out they would be having a second summit

02:06.160 --> 02:11.360
so soon, this time we had the pleasure of interviewing the four startups of the second

02:11.360 --> 02:13.520
AI Nexus Lab batch.

02:13.520 --> 02:23.840
We also interviewed a bunch of the speakers from the event and we'll be sharing those discussions

02:23.840 --> 02:25.840
over the upcoming weeks.

02:25.840 --> 02:32.080
In this episode, you hear from bite.ai, a startup founded by Vene Anantharaman and Michael

02:32.080 --> 02:38.280
Walski, founders who met working at Clarify, another NYU Future Labs alumni company,

02:38.280 --> 02:43.200
whose CEO Matt Zealer, I interviewed on twimbletalk number 22.

02:43.200 --> 02:47.320
Data is using conversational neural networks and other machine learning to help computers

02:47.320 --> 02:49.920
understand and reason about food.

02:49.920 --> 02:54.920
Their product is an app called BiteSnap, which provides users with detailed nutritional

02:54.920 --> 03:00.240
information about the food they're about to eat using just a photo and a serving size.

03:00.240 --> 03:04.560
We dive into the details of their app and service, the machine learning models and pipeline

03:04.560 --> 03:16.640
that enable it and how they plan to compete with other apps targeting dieters and more.

03:16.640 --> 03:23.360
Alright everyone, I am here at NYU Future Labs in New York City and I am with the co-founders

03:23.360 --> 03:25.120
of BiteAI.

03:25.120 --> 03:29.720
In particular, I'm with Vene Anantharaman and Michael Walski.

03:29.720 --> 03:32.240
Guys, welcome to this weekend machine learning and AI.

03:32.240 --> 03:33.240
Thanks for having us.

03:33.240 --> 03:38.120
Awesome, awesome, why don't we get started by having you tell us a little bit about

03:38.120 --> 03:42.160
your backgrounds and we'll get to what the company is up to.

03:42.160 --> 03:47.160
Cool, as I was introduced, my name is Vene and I kind of work on data infrastructure

03:47.160 --> 03:51.000
and for the last 10 years I've been crawling the web trying to turn that into structured

03:51.000 --> 03:57.200
data and currently what we're working on is building a food intelligence platform that

03:57.200 --> 03:58.960
can understand the world's food.

03:58.960 --> 04:03.400
Okay, so I'm Michael, actually two of us met here in this building at Clarify where the

04:03.400 --> 04:09.000
first 10 threes and I got to work this together for 10 years before starting this company.

04:09.000 --> 04:12.960
Before joining Clarify I was at Columbia studying the pipeline and computer science, the focus

04:12.960 --> 04:13.960
on computer vision.

04:13.960 --> 04:15.960
Okay, awesome, awesome.

04:15.960 --> 04:18.800
You said understanding food, tell me a little bit more about what that means.

04:18.800 --> 04:22.440
I'm assuming since you met at Clarify, you're doing something visual.

04:22.440 --> 04:27.680
Yes, exactly, so to understand the world's food we need to be able to have examples of

04:27.680 --> 04:31.480
what people are eating day to day and most ways that we communicate about food these days

04:31.480 --> 04:32.800
is with pictures.

04:32.800 --> 04:38.040
So we started by building an image recognition model that can understand what we're eating,

04:38.040 --> 04:41.680
which means that we can take something that we're eating and translate into a set of

04:41.680 --> 04:44.320
tags and also give you nutritional information.

04:44.320 --> 04:47.120
And where we're going with that is actually we'd love to be able to take in other ways

04:47.120 --> 04:48.120
we talk about food.

04:48.120 --> 04:52.600
It could be through menus or through text and be able to say, hey, you know, it's XYZ,

04:52.600 --> 04:58.120
it could be made with these ingredients and we want to build a system that can let other

04:58.120 --> 05:02.600
people also be able to, you know, use the intelligence and understanding we have about food

05:02.600 --> 05:04.040
to power their food application.

05:04.040 --> 05:07.280
So it could be a nutrition tracker in a healthcare setting.

05:07.280 --> 05:10.400
It could also be for a recipe website, they have a bunch of pictures, they want to know

05:10.400 --> 05:12.200
what ingredients it is.

05:12.200 --> 05:14.000
That's kind of what we're working on.

05:14.000 --> 05:15.000
Interesting, interesting.

05:15.000 --> 05:18.520
Now, I don't know if you've come across Hillary Mason.

05:18.520 --> 05:22.920
She has a company called Fast Forward Labs here in New York that was actually recently

05:22.920 --> 05:28.560
acquired, but they do kind of, they do data science experiments, if you will.

05:28.560 --> 05:30.520
That's one of the things they do at least.

05:30.520 --> 05:35.200
And they experimented with trying to do this, determine nutritional information based

05:35.200 --> 05:37.720
on pictures of food.

05:37.720 --> 05:41.040
And I think she mentioned this on the podcast I did with her.

05:41.040 --> 05:43.360
She said it was an incredibly difficult prop walk.

05:43.360 --> 05:47.240
Like they end up, I think they end up giving up on it and moving on to something else.

05:47.240 --> 05:50.480
What are you doing differently that makes it tractable?

05:50.480 --> 05:53.600
We kind of approach this from a consumer side of the product called BitesNap.

05:53.600 --> 05:56.560
It's a nap that helps you to track what they're eating, kind of like a knife in this

05:56.560 --> 05:58.480
palette alternative, using images as input.

05:58.480 --> 05:59.480
Okay.

05:59.480 --> 06:02.320
What we do is every time a user comes in and wants to log in the meal, they take a picture

06:02.320 --> 06:06.560
of their food, we give them suggestions of what might be, users that they can correct

06:06.560 --> 06:11.200
this, like the correct choice or kind of pick something else that we didn't predict.

06:11.200 --> 06:12.240
Tell us the portion size.

06:12.240 --> 06:15.800
So every time some bugs in the meal would get training data to improve the algorithm.

06:15.800 --> 06:19.000
And what we do in the app now is, if you take a picture of the meal that we haven't

06:19.000 --> 06:20.840
seen before, we can't recognize.

06:20.840 --> 06:24.320
We allow you to tell us what it is and now the variance to recognize it.

06:24.320 --> 06:25.320
So we do one shot learning.

06:25.320 --> 06:27.400
So the next time you come around, we can recognize it.

06:27.400 --> 06:30.960
And in doing this, we collect all this data, the users on our take about three or four

06:30.960 --> 06:31.960
pictures a day.

06:31.960 --> 06:32.960
Okay.

06:32.960 --> 06:35.440
We're going to have this human loop system to help us keep improving.

06:35.440 --> 06:36.440
Interesting.

06:36.440 --> 06:37.440
How do you measure accuracy?

06:37.440 --> 06:42.960
On the image recognition or on the nutrition side, like kind of the full loop.

06:42.960 --> 06:47.640
So portion size are important and for now we have the users toss the portion size.

06:47.640 --> 06:52.800
So we have a prior, there's this big study called inhanes where the government collected

06:52.800 --> 06:55.080
all this data on people eat, how much they eat.

06:55.080 --> 06:59.720
So we have kind of a distribution of, for example, a fries, how many fries some of my food

06:59.720 --> 07:00.720
for dinner.

07:00.720 --> 07:04.200
And using that as a prior, as like an example of what the portion size might be, okay,

07:04.200 --> 07:07.920
we ask these just to correct that, okay, so it's still on then to kind of tell us how

07:07.920 --> 07:11.320
much they, because it's so professional these days, I mean, just in general, to be able

07:11.320 --> 07:15.480
to measure food and get the accurate numbers, you have to be able to tell us how much

07:15.480 --> 07:16.480
there is.

07:16.480 --> 07:20.800
But we're hoping that by getting this data later on, we actually start using computer

07:20.800 --> 07:22.400
vision to predict the portion size.

07:22.400 --> 07:23.400
Okay.

07:23.400 --> 07:27.040
You know, I'll definitely be downloading this and playing around with this.

07:27.040 --> 07:33.120
So I've been experimenting with a ketogenic diet, which my use fitness pal, like if you're

07:33.120 --> 07:38.280
really into it, you're tracking your macronutrients with every meal and it's a pain in the

07:38.280 --> 07:39.280
butt.

07:39.280 --> 07:44.080
And so I've experimented with just taking a picture of the things that I eat and then

07:44.080 --> 07:47.480
going back afterwards and then trying to look at it and figure out, and it's hard for

07:47.480 --> 07:52.200
me to figure out, okay, what the portion size was, but then also, like, you know, what the

07:52.200 --> 07:53.200
fat content was.

07:53.200 --> 07:57.840
If you go to a restaurant, the fat content and a given meal can vary, you know, pretty

07:57.840 --> 07:58.840
dramatically.

07:58.840 --> 08:03.200
I mean, those aren't things that you can address just by training data, because in those

08:03.200 --> 08:05.880
cases, the users don't even know themselves.

08:05.880 --> 08:09.480
So we're hoping to use the data we call some users and start integrating location into

08:09.480 --> 08:10.720
the system.

08:10.720 --> 08:13.560
So if you know that you go to ShakeShack and you have a burger at ShakeShack, and we've

08:13.560 --> 08:17.560
seen an example of that burger at ShakeShack and some print information, we'll be able

08:17.560 --> 08:20.120
to say it's not just a general burger, it's the ShakeShack burger of the interesting

08:20.120 --> 08:22.920
data that we get from there many.

08:22.920 --> 08:23.920
Okay.

08:23.920 --> 08:30.040
I was going to ask if you were targeting, like, chain types of meals as a, it seems like

08:30.040 --> 08:34.440
that's easier than, so we haven't gotten to it yet, but it's kind of on our roadmap

08:34.440 --> 08:38.280
in the next few months, it's still kind of pulling in information, starting to location.

08:38.280 --> 08:41.640
I kind of have the other signals to improve the accuracy.

08:41.640 --> 08:42.640
Okay.

08:42.640 --> 08:46.360
So tell me a little bit about what some of the big challenges have been for you.

08:46.360 --> 08:49.440
I think for us, the biggest one is just, like you said, it's a hard problem.

08:49.440 --> 08:51.120
The coverage is an issue.

08:51.120 --> 08:54.560
People eat all kinds of foods, there's probably millions of combinations.

08:54.560 --> 08:56.520
And kind of the first explanation that matters.

08:56.520 --> 09:00.080
If someone comes in, they try it, it doesn't work the first time, they might not come

09:00.080 --> 09:01.080
back.

09:01.080 --> 09:03.800
So for us, making sure we have good coverage of all the foods and they have high accuracy

09:03.800 --> 09:08.040
and stuff that you had at home, that it's not at our start menu, this is important.

09:08.040 --> 09:09.040
Right.

09:09.040 --> 09:11.080
How about for you, Bina?

09:11.080 --> 09:16.480
I would say that for us, because we do have a consumer app, I think marketing is pretty

09:16.480 --> 09:19.960
difficult in the consumer space, and our background is a little more technical.

09:19.960 --> 09:25.160
So it's kind of new for us to be marketing a consumer app, and for us, if the more consumers

09:25.160 --> 09:26.880
we get, the more data we can get.

09:26.880 --> 09:31.120
So we're, at least we've been lucky with the futures lab, we've been getting advisors

09:31.120 --> 09:35.640
and help, but it's still kind of an ongoing process that we're learning, how to actually

09:35.640 --> 09:37.880
market the app and get it out there.

09:37.880 --> 09:38.880
Interesting.

09:38.880 --> 09:45.440
Can you talk a little bit about the pipelines that you've created to process all the data?

09:45.440 --> 09:46.920
Yeah, sure.

09:46.920 --> 09:50.840
I mean, we, as we mentioned, we, you know, we scraped a bunch of images from that.

09:50.840 --> 09:54.520
We basically built our own tools to annotate and clean data.

09:54.520 --> 09:56.400
I guess Michael can fill in there.

09:56.400 --> 10:00.840
We built this tool to pull in images, use active learning to help us quickly annotate

10:00.840 --> 10:05.280
millions of images, the two of us managed to annotate three million labeled images of

10:05.280 --> 10:06.280
the different foods.

10:06.280 --> 10:11.040
The two of you met three million over how long it was about a week.

10:11.040 --> 10:12.040
Wow.

10:12.040 --> 10:15.800
So we just clustered and some classifiers to rank and be smart about how we're doing this.

10:15.800 --> 10:17.800
I've kind of done it before, I'd clarify.

10:17.800 --> 10:18.800
Yeah.

10:18.800 --> 10:21.000
I have an idea of how to handle this scale of a data set.

10:21.000 --> 10:22.000
Okay.

10:22.000 --> 10:27.000
And we went from a model of 60 classes to now a bit of over 1500 different foods.

10:27.000 --> 10:30.800
Now that that is out, we're getting all this data from users as well, and we're starting

10:30.800 --> 10:33.120
to chain known that data as well.

10:33.120 --> 10:35.960
I used to say 1500 different foods.

10:35.960 --> 10:41.160
When I think about like how many different foods fitness palette has in it, it's multiple

10:41.160 --> 10:42.160
of that, isn't it?

10:42.160 --> 10:43.160
Yeah.

10:43.160 --> 10:44.160
So there's two issues.

10:44.160 --> 10:45.160
There's whole food state.

10:45.160 --> 10:46.160
You might, you know, go get a grocery store.

10:46.160 --> 10:47.160
There's homey meals.

10:47.160 --> 10:48.160
That's where we focus on.

10:48.160 --> 10:49.160
Right now.

10:49.160 --> 10:50.160
Okay.

10:50.160 --> 10:51.160
And most of these are basic ingredients.

10:51.160 --> 10:55.800
So if you take a play of pasta, tomatoes, we might say it's pasta, but also give you

10:55.800 --> 10:58.840
options for the noodles, the sauce, the cheese on top.

10:58.840 --> 10:59.840
Okay.

10:59.840 --> 11:00.840
Okay.

11:00.840 --> 11:01.840
Okay.

11:01.840 --> 11:02.840
Okay.

11:02.840 --> 11:04.840
So kind of a combination between the food detection as well as like a Google look on

11:04.840 --> 11:05.840
most.

11:05.840 --> 11:06.840
Yeah.

11:06.840 --> 11:07.840
I can't find the products.

11:07.840 --> 11:08.840
Yeah.

11:08.840 --> 11:09.840
Interesting.

11:09.840 --> 11:11.840
And do you also, are you also allowing them?

11:11.840 --> 11:12.840
Yeah.

11:12.840 --> 11:13.840
I do.

11:13.840 --> 11:14.840
I do.

11:14.840 --> 11:15.840
I do.

11:15.840 --> 11:16.840
I do.

11:16.840 --> 11:17.840
I do.

11:17.840 --> 11:18.840
I do.

11:18.840 --> 11:19.840
I do.

11:19.840 --> 11:20.840
I do.

11:20.840 --> 11:21.840
I do.

11:21.840 --> 11:22.840
I do.

11:22.840 --> 11:23.840
I do.

11:23.840 --> 11:24.840
I do.

11:24.840 --> 11:25.840
I do.

11:25.840 --> 11:26.840
I do.

11:26.840 --> 11:33.660
Also are you also allowing them to track the nutritional information over time?

11:33.660 --> 11:38.000
Is it kind of like take plane that role as well or just do they take the data and plug

11:38.000 --> 11:39.000
it into something else.

11:39.000 --> 11:41.200
It's a full tracker flagger.

11:41.200 --> 11:44.480
Logger will give you the breakdowns for the day, for the week and you can see your history

11:44.480 --> 11:45.480
every time.

11:45.480 --> 11:46.460
Okay.

11:46.460 --> 11:47.460
I need this.

11:47.460 --> 11:48.460
Yeah.

11:48.460 --> 11:51.680
And you should go after this keto market like there's an out of news that something

11:51.680 --> 11:52.680
that's on your radar.

11:52.680 --> 11:54.560
We're actually getting feedback from our users a lot of them.

11:54.560 --> 11:55.800
We're doing keto chains diet, okay.

11:55.800 --> 12:01.200
And this first division for that in the future is kind of build it up to start putting communities together.

12:01.200 --> 12:01.700
Yeah.

12:01.700 --> 12:05.500
We have users who have all this content in their pictures, and we know that a person's a key to giant diet,

12:05.500 --> 12:08.000
you might be able to connect them to other people who are doing keto,

12:08.000 --> 12:11.800
help them discover foods, share up their eating, start recommending other stuff to them.

12:11.800 --> 12:12.800
Okay.

12:12.800 --> 12:17.100
What are some of the techniques that you're using on the ML and AI side here?

12:17.100 --> 12:21.800
So for image recognition, we're just using standard neural networks, conversion neural nets.

12:21.800 --> 12:26.200
Stories of the I know might have to switch since it tends to fall apart by turrets.

12:26.200 --> 12:33.500
For active learning, just simple like logistic regression, simple like logistic regression to rank the results based on the embedding.

12:33.500 --> 12:35.600
Yeah, it's pretty much a lot of neural networks.

12:35.600 --> 12:36.700
Okay.

12:36.700 --> 12:37.600
Cool.

12:37.600 --> 12:42.500
Where are you kind of in the lifecycle of product is out and on the various app stores,

12:42.500 --> 12:44.800
both you're not going to tell me that you don't support Android.

12:44.800 --> 12:47.400
No, we actually do support iOS and Android.

12:47.400 --> 12:48.900
We realize this is incredibly important.

12:48.900 --> 12:52.100
Okay. So we wear on both things because we're using React Native,

12:52.100 --> 12:58.100
which for a small team, it really helps us because we can have an app that's available for both platforms.

12:58.100 --> 13:01.500
In terms of the product, we actually launched an MVP earlier this year,

13:01.500 --> 13:06.100
and we've kind of been developing that towards kind of product market fit with other people,

13:06.100 --> 13:09.100
for with other apps like my fitness pal, people have some expectations.

13:09.100 --> 13:12.200
So we're really, really close on kind of closing that out.

13:12.200 --> 13:17.000
And in terms of the product, we're now actually focusing to start launching an API

13:17.000 --> 13:20.100
so that people can use our technology and other applications.

13:20.100 --> 13:21.900
That's interesting.

13:21.900 --> 13:24.600
I think I look for an API for my fitness pal.

13:24.600 --> 13:30.100
And if there was one, it was like you had to submit a form and get approved

13:30.100 --> 13:34.000
and talk to their BD people and see what's pretty painful.

13:34.000 --> 13:37.900
Yeah, I mean, I think that with because we're taking a little bit of different approach

13:37.900 --> 13:43.000
in terms of our app strategy, because we actually are fine with giving people macros

13:43.000 --> 13:43.900
and micronutrients.

13:43.900 --> 13:47.800
We find that the information is currently valuable for people to make decisions

13:47.800 --> 13:48.600
about what they're eating.

13:48.600 --> 13:50.300
So we're offering that up.

13:50.300 --> 13:55.400
And we do have plans and thoughts about opening up the API to users themselves

13:55.400 --> 13:58.400
so that they can be creative, so that they can view and visualize their data

13:58.400 --> 13:59.300
in their own ways.

13:59.300 --> 14:03.100
We've experimented with that because we can let you export to CSV or JSON.

14:03.100 --> 14:05.700
So you can pull that in and kind of build your own visualizations

14:05.700 --> 14:09.700
and a few people from the self-quantified community are really excited about it

14:09.700 --> 14:13.300
because they can actually then kind of make their own collages and their own charts

14:13.300 --> 14:16.700
and integrate that into their own self-quantified solutions

14:16.700 --> 14:18.500
because you know, that's not our focus.

14:18.500 --> 14:22.000
But we would love to enable creativity for them around what they're eating

14:22.000 --> 14:24.000
and that's kind of a big part of their life.

14:24.000 --> 14:30.300
And so do you, you know, thinking about again, where you're coming from with Clarify,

14:30.300 --> 14:38.000
do you see the bite snap product is kind of just bootstrapping an API business

14:38.000 --> 14:40.000
or is that the product?

14:40.000 --> 14:41.600
It's a core product for us.

14:41.600 --> 14:45.000
But we see that it's a great way to get data to have this human look system.

14:45.000 --> 14:46.000
It keeps improving.

14:46.000 --> 14:50.300
We've gotten a lot of inbound requests for an API from customer research firms,

14:50.300 --> 14:54.500
from healthcare side, from hospitals, people dealing with diabetic patients.

14:54.500 --> 14:57.500
So we're kind of looking to open it up to other people.

14:57.500 --> 15:00.400
There's a lot of core technology building up to other people using other ways.

15:00.400 --> 15:01.400
Okay.

15:01.400 --> 15:01.900
Interesting.

15:01.900 --> 15:06.700
Maybe we can spend some time and you can kind of walk me through the annotation framework

15:06.700 --> 15:09.100
that you built out for getting through those images.

15:09.100 --> 15:11.900
But like I'm still that's an impressive feat.

15:11.900 --> 15:12.900
Yeah.

15:12.900 --> 15:15.900
The tool pretty much, we get content from the web or you have a class.

15:15.900 --> 15:19.200
If I was to say you have a new one hour day chain before, what you do is you predict

15:19.200 --> 15:22.400
on the other images, you get some kind of ranking for each class.

15:22.400 --> 15:27.000
So let's say for strawberries, we'll take a classifier that's a week classifier for

15:27.000 --> 15:31.100
strawberries, rank all of our images throughout, get a signal of what might be a strawberry.

15:31.100 --> 15:32.400
We use that to rank.

15:32.400 --> 15:34.600
We also do visual clustering.

15:34.600 --> 15:38.500
So we can say, you know, for all these images, food, these are the similar looking cluster

15:38.500 --> 15:42.200
for images of strawberries that are in the middle of these strawberries.

15:42.200 --> 15:46.500
Let me look at that cluster and for the whole cluster and make a response of yes, this

15:46.500 --> 15:47.500
is a strawberry.

15:47.500 --> 15:51.700
Then we have a classifier that is that input to train and improve.

15:51.700 --> 15:54.000
So now we can re-rank the results again.

15:54.000 --> 15:57.000
And also get to a high enough accuracy, you can say for the rest of the image, because

15:57.000 --> 16:01.600
the classifier is 99% accurate, or let's say it's 95%, you know, whatever the prediction

16:01.600 --> 16:03.600
is, assume it's a cooling label.

16:03.600 --> 16:04.600
Got it.

16:04.600 --> 16:11.500
So when you annotated 3 million images, like that initial, you look at the cluster of

16:11.500 --> 16:14.500
strawberries and say, yes, these are strawberries.

16:14.500 --> 16:18.500
That might have knocked out 100,000 images for you, or some large number.

16:18.500 --> 16:19.500
Right.

16:19.500 --> 16:23.600
So if you take the ranking for the images, for let's say the strawberries, you kind of see

16:23.600 --> 16:26.800
stuff on the bottom that's definitely not strawberries, so you can say, let's say the

16:26.800 --> 16:28.800
top of the bottom 20%, it's negative data now.

16:28.800 --> 16:29.800
Right.

16:29.800 --> 16:32.800
You take the top 10%, if you look at it and see that, it looks like positives.

16:32.800 --> 16:38.200
You assume that's positive, you update your classifier based on that data, and you retrain

16:38.200 --> 16:42.720
and re-rank again, kind of moving that data away, and kind of keep diving into a clustering

16:42.720 --> 16:46.120
we found was working literally as well, because now you can, instead of looking at single

16:46.120 --> 16:50.320
image at a time, you can say, oh, for this cluster, so all the images look similar, they

16:50.320 --> 16:51.320
look like strawberries.

16:51.320 --> 16:55.280
Yeah, I'm sure you've seen like clustering on the vetting space, so now you can, instead

16:55.280 --> 16:58.600
of responding to an image at a time for a whole cluster, say, yes, this is a strawberry

16:58.600 --> 16:59.600
and opposite strawberry.

16:59.600 --> 17:00.600
Okay.

17:00.600 --> 17:08.800
And so you annotate your strawberries, and then you have all that information for kind

17:08.800 --> 17:14.400
of the next thing you're looking at, so what extent does the annotation for strawberries

17:14.400 --> 17:17.400
impact bananas if that's what you're doing next?

17:17.400 --> 17:18.400
Doesn't at all.

17:18.400 --> 17:20.400
So we've probably started again with some bananas.

17:20.400 --> 17:21.400
Okay.

17:21.400 --> 17:25.000
It turns out in the opposite, very good at handling noise, so if you have a classifier

17:25.000 --> 17:29.600
you're trying on the dirty data, there's probably 60% probably that the thing that mentions

17:29.600 --> 17:31.600
strawberry has strawberries in the image.

17:31.600 --> 17:35.600
If you're trying to go already getting pretty good signal to recognize those items, so you

17:35.600 --> 17:39.600
can kind of push up with that and then kind of keep improving it with the actual annotations.

17:39.600 --> 17:44.000
The other one for us is like we'd like to be able to do segmentation or bounding boxes,

17:44.000 --> 17:46.520
but it's really expensive to get that data at scale.

17:46.520 --> 17:50.600
For us, we want to recognize like thousands of different foods and take us forever and

17:50.600 --> 17:53.960
write some of these at dollars, it's actually annotated at not level.

17:53.960 --> 17:55.640
So yeah, that's what I envision you were doing.

17:55.640 --> 18:00.640
So right now you're identifying images that have a single thing and using that to train.

18:00.640 --> 18:05.240
So even if you have multiple items in an image, what do you want class at a time?

18:05.240 --> 18:06.240
Okay.

18:06.240 --> 18:08.840
So we'll do multiple passes over it if it has multiple items.

18:08.840 --> 18:09.840
Okay.

18:09.840 --> 18:12.240
So most of the time we've released stuff, it's like four or five items on the plate.

18:12.240 --> 18:13.240
Right.

18:13.240 --> 18:15.480
Or it would be a soda, you know, a program fries.

18:15.480 --> 18:17.960
So an example like that would do all the fries first.

18:17.960 --> 18:21.480
We might be able to pick up on correlations, they're super good as well, but we do want

18:21.480 --> 18:24.000
class at a time when you do sweet love.

18:24.000 --> 18:28.400
When you say you look at the plate and you do all the fries first, are you talking about

18:28.400 --> 18:31.200
when you're training or when you're doing inference?

18:31.200 --> 18:35.240
Oh, no, when we're doing the cleaning annotations, the cleaning annotation phase.

18:35.240 --> 18:36.240
Yeah.

18:36.240 --> 18:39.440
But kind of we're hoping that if the class arrives good, the next time we go around,

18:39.440 --> 18:40.960
the burger class five picks it up.

18:40.960 --> 18:41.960
So we'll see that image again.

18:41.960 --> 18:42.960
Right.

18:42.960 --> 18:44.800
We'll see if the burger prediction is correct or not.

18:44.800 --> 18:45.800
Okay.

18:45.800 --> 18:51.560
And so this kind of iterative process, so what the degree is it, like have you automated

18:51.560 --> 18:56.600
all that into some kind of pipeline or is it still, are you like manually kicking off

18:56.600 --> 19:01.800
runs to do annotate for object X or so we kind of have a batch job to do the initial

19:01.800 --> 19:02.800
annotations.

19:02.800 --> 19:07.040
We have a system that's called a Persian Django, like a React front end that will take

19:07.040 --> 19:11.600
those annotations and then have a simpler class of like an SVM or a logistic regression

19:11.600 --> 19:13.200
to do ranking after.

19:13.200 --> 19:17.120
So once you have the initial like suggestions for labels, it doesn't have to be a little

19:17.120 --> 19:18.120
network.

19:18.120 --> 19:19.120
It could be just web data.

19:19.120 --> 19:23.120
You pick up on keywords of strawberries and images on the website that also matches

19:23.120 --> 19:24.120
strawberry.

19:24.120 --> 19:28.080
You kind of, it seems to weak label as a chance of it having a stripper.

19:28.080 --> 19:31.840
And so you index that in this database, you do the skins and kind of can go class by

19:31.840 --> 19:32.840
class.

19:32.840 --> 19:33.840
Okay.

19:33.840 --> 19:34.840
So that pretty much is what I mean.

19:34.840 --> 19:35.840
Okay.

19:35.840 --> 19:36.840
Interesting.

19:36.840 --> 19:38.160
And how about the crawling part?

19:38.160 --> 19:41.640
Was that where there are any challenges involved in that or was that pretty straightforward?

19:41.640 --> 19:46.680
I mean, for the most part, it's just, it's not a challenge to download lots of stuff.

19:46.680 --> 19:51.200
What the real challenge is actually getting decent labels and, you know, we figured out

19:51.200 --> 19:55.440
some techniques to basically, you know, the right sites and the right places that actually

19:55.440 --> 19:56.800
have decent labels.

19:56.800 --> 20:00.600
And we actually even took some unlabeled data so that we could actually supplement the

20:00.600 --> 20:02.280
weak labels we have.

20:02.280 --> 20:05.560
So we can get more examples and that's kind of how we started.

20:05.560 --> 20:08.480
It's just, you know, it takes a little bit of time to get it right.

20:08.480 --> 20:09.480
Mm-hmm.

20:09.480 --> 20:10.480
Okay.

20:10.480 --> 20:11.480
Interesting.

20:11.480 --> 20:13.960
What else are you doing that's cool and interesting and that we should know about?

20:13.960 --> 20:16.680
So right now, we have an issue with package products.

20:16.680 --> 20:19.480
We have a model that works well on whole foods, people seem pretty happy with it.

20:19.480 --> 20:20.480
Okay.

20:20.480 --> 20:23.320
But one, we don't have a huge database of bulk of products, of scans and distribution

20:23.320 --> 20:24.320
data.

20:24.320 --> 20:28.920
So now we're also working on being able to use OCR and computer vision to kind of scan

20:28.920 --> 20:32.800
a product, be able to index it, be able to tell what brand it is, what the health claims

20:32.800 --> 20:36.200
are, what the ingredients are just using computer vision.

20:36.200 --> 20:39.800
Another thing we're playing with is using a ARCAD, and you say, our technologies to do

20:39.800 --> 20:41.280
the portion size estimation.

20:41.280 --> 20:42.280
Interesting.

20:42.280 --> 20:45.480
You know, we have an entire working, you know, an initial idea for this, but the idea

20:45.480 --> 20:50.520
does it will be, we take a picture, we'll get a point cloud, we have a user, pick a portion

20:50.520 --> 20:54.000
size for now, but now we have a point cloud, we have the image, we can do some segmentation

20:54.000 --> 20:55.680
and we have the portion size.

20:55.680 --> 21:00.920
So over time, we'll get a more data to actually nail that part of the problem.

21:00.920 --> 21:01.920
How does ARCAD work?

21:01.920 --> 21:04.400
I haven't had a chance to look under the cover, sir.

21:04.400 --> 21:06.960
We have, we have an intern environment now.

21:06.960 --> 21:11.280
For now, you can get horizontal planes and if you only get a point cloud.

21:11.280 --> 21:15.640
So for us, like, we have a camera open, not only to get picture, but also to get a point

21:15.640 --> 21:16.640
cloud.

21:16.640 --> 21:19.440
So we can get some depth information and figure out the size of the items.

21:19.440 --> 21:21.320
Oh, interesting.

21:21.320 --> 21:25.680
And are you like thinking about taking it the next step where you kind of project on

21:25.680 --> 21:29.800
top of the, like use ARCAD the way it's designed and like project something on top of the

21:29.800 --> 21:35.960
plate that says like, don't need this or, you know, put an X over the surprise.

21:35.960 --> 21:41.240
Yeah, that would be a great, I mean, we were, in terms of ARCAD, we've

21:41.240 --> 21:43.120
we're playing with a different user interfaces.

21:43.120 --> 21:47.560
So one would, one is like Michael's mentioning is that we could project a cup to help you

21:47.560 --> 21:52.080
understand what a 12-ounce cup versus a 16-ounce cup is and kind of put that next to each

21:52.080 --> 21:53.080
other.

21:53.080 --> 21:55.760
So you can select, oh, yeah, this is, you know, a pint glass versus this.

21:55.760 --> 21:59.160
There's like another mode where maybe we need to also do measuring because it's not

21:59.160 --> 22:01.320
perfectly a cup or perfectly a plate.

22:01.320 --> 22:02.560
We can allow people to do measurement.

22:02.560 --> 22:06.880
I don't know if you've ever played with these measurement apps are pretty easy to use.

22:06.880 --> 22:08.880
That's kind of how we envision using AR now.

22:08.880 --> 22:13.120
I mean, in the future, if there were AR glasses, we'd actually love to be able to project

22:13.120 --> 22:17.440
like the information that we know about food onto the real world and that's like a friction

22:17.440 --> 22:18.440
free environment.

22:18.440 --> 22:23.440
A, we're recording what you're eating so you don't have to log because if you have another

22:23.440 --> 22:27.280
passive device absorbing that information, it can now understand, hey, you're eating

22:27.280 --> 22:32.160
pizza with burgers today, maybe tomorrow, you should eat something else, you know, to,

22:32.160 --> 22:33.400
you know, feel better.

22:33.400 --> 22:37.040
So I said, General, the goal is to get to this very passive mode, right now we still

22:37.040 --> 22:39.900
give you suggestions, but for some classes, we're pretty much at a Himalayan lackier

22:39.900 --> 22:40.900
seat.

22:40.900 --> 22:44.120
It's kind of things like burgers fries, like when we predict it from the most part, it

22:44.120 --> 22:45.120
is accurate.

22:45.120 --> 22:46.120
Okay.

22:46.120 --> 22:48.120
So we want to get to a point where this promotional user interaction, you take a picture

22:48.120 --> 22:51.760
or you have some classes, you go by your daily and you're able to measure all the stuff

22:51.760 --> 22:55.560
about your diet, tell if there's weaknesses, tell you how to improve, kind of have this

22:55.560 --> 22:57.520
dashboard where you, track it, don't have to do any work.

22:57.520 --> 23:01.960
The other thing we've noticed is people tend to eat similar stuff every day.

23:01.960 --> 23:05.360
So soon we'll be able to predict where you ate before eating.

23:05.360 --> 23:08.800
If you sit around at home, if you go to coffee shop and you always have a lot to have this

23:08.800 --> 23:12.200
coffee shop, we should be able to predict that you had it and log it for you without

23:12.200 --> 23:13.200
too many of the work.

23:13.200 --> 23:17.000
Like using location services, you just walked in a Starbucks and I had a pumpkin spice

23:17.000 --> 23:18.000
latte to your daily.

23:18.000 --> 23:19.000
Yeah.

23:19.000 --> 23:20.000
Yeah.

23:20.000 --> 23:21.000
Yeah.

23:21.000 --> 23:27.440
I mean, even actually for other meals like, you know, during the week you usually eat XYZ,

23:27.440 --> 23:29.320
so why should you have to go and tap stuff?

23:29.320 --> 23:33.760
We can just kind of fill that in so that, you know, the meals that really are different

23:33.760 --> 23:35.440
or the ones you have to actually log.

23:35.440 --> 23:38.400
We can actually kind of build part of that experience already now, because, you know,

23:38.400 --> 23:41.080
we don't need necessarily AR glasses.

23:41.080 --> 23:45.960
We can just do that based on location, post time, day, you know, what meal it is.

23:45.960 --> 23:46.960
Hmm.

23:46.960 --> 23:51.080
So you mentioned some of the challenges you're seeing in terms of getting this out to

23:51.080 --> 23:52.080
market.

23:52.080 --> 23:56.520
Like what are the top, you know, end things, top three things that you feel like you need

23:56.520 --> 23:59.640
to be successful based on where you are now?

23:59.640 --> 24:03.440
I would say that we need to have product market fit, like relative to the other food

24:03.440 --> 24:04.440
logging apps.

24:04.440 --> 24:06.960
We just can't be, you know, having features missing because people say, oh, I'm so used

24:06.960 --> 24:07.960
to this.

24:07.960 --> 24:08.960
Yeah.

24:08.960 --> 24:09.960
I need it.

24:09.960 --> 24:10.960
That's kind of a, that's a known known.

24:10.960 --> 24:11.960
We know how to do it.

24:11.960 --> 24:15.000
We just, it just takes us a little bit of time to get the features right and, you know,

24:15.000 --> 24:16.000
make them look beautiful.

24:16.000 --> 24:19.560
I mean, that's another very important thing with consumer apps is that it has to look

24:19.560 --> 24:20.560
really good.

24:20.560 --> 24:22.040
The bar is really high now.

24:22.040 --> 24:25.680
Secondly, the thing is there is a lot of other apps in this space.

24:25.680 --> 24:27.840
And for us, you know, they're incumbents.

24:27.840 --> 24:30.520
People know them by name brands like my fitness pal.

24:30.520 --> 24:34.720
So well, for us, it's we have to, you know, kind of become visible and we probably have

24:34.720 --> 24:38.920
to become visible through some other, through other channels and then just pure search because

24:38.920 --> 24:40.920
most people are searching for those main brands.

24:40.920 --> 24:44.520
So I think those are kind of our top two challenges in terms of marketing and the app out

24:44.520 --> 24:45.520
there.

24:45.520 --> 24:50.120
And on the tech side, data is always, for us it's always about having data.

24:50.120 --> 24:53.240
So there's, right now for some of the classes that we predict, we don't have the niche

24:53.240 --> 24:54.240
for data.

24:54.240 --> 24:55.240
So we don't need to show us suggestions.

24:55.240 --> 24:59.160
So we might be able to predict that data, but because we don't have niche for data tied

24:59.160 --> 25:00.160
to it.

25:00.160 --> 25:02.840
Like, we don't have to show that prediction to users.

25:02.840 --> 25:08.520
So did you, did you gather a database of nutrition data by searching and that kind of thing

25:08.520 --> 25:11.120
or by just finding a database?

25:11.120 --> 25:14.120
So the USD has a very big and detailed data set.

25:14.120 --> 25:17.920
It's about a hundred thousand different common things people eat in America.

25:17.920 --> 25:18.920
Oh wow.

25:18.920 --> 25:20.720
And it's broken down by ingredients.

25:20.720 --> 25:24.080
The common portion size is, it has the full nutritional breakdown.

25:24.080 --> 25:25.080
So we're using that for now.

25:25.080 --> 25:26.080
Okay.

25:26.080 --> 25:30.000
But now we're working ways to like pull in restaurant data, to pull in product data and

25:30.000 --> 25:32.240
kind of help us increase that database.

25:32.240 --> 25:33.240
Okay.

25:33.240 --> 25:34.240
Shout out to the USDA.

25:34.240 --> 25:35.240
They do really good work.

25:35.240 --> 25:37.880
Like, their nutrition database is quite complete.

25:37.880 --> 25:41.400
What's interesting is that actually we learn that other nutrition databases for other

25:41.400 --> 25:43.520
countries actually depend on the USDA database.

25:43.520 --> 25:49.640
They actually have references to the English database and that's long term international

25:49.640 --> 25:53.640
expansion is a little bit more tough for problem for us than most people A, we have to convert

25:53.640 --> 25:54.640
our labels.

25:54.640 --> 25:57.560
So the things that we call pizza is probably universal.

25:57.560 --> 26:01.120
But you know, in the morning pastries, people call those things different things in different

26:01.120 --> 26:02.120
languages.

26:02.120 --> 26:04.920
We have to do all that translation, not only that with the nutritional facts, there's

26:04.920 --> 26:08.080
regional variations with how things are prepared.

26:08.080 --> 26:12.360
And then of course, regional dishes, which actually vary by look depending on where you

26:12.360 --> 26:13.520
are.

26:13.520 --> 26:17.400
And we've been getting a lot of requests from international users, hey, can you recognize

26:17.400 --> 26:18.720
XYZ for me?

26:18.720 --> 26:20.760
And we're like, we don't even know what that is.

26:20.760 --> 26:21.760
Nice.

26:21.760 --> 26:26.120
Do you have any, there's so many challenges there like all kinds of foods that are, you

26:26.120 --> 26:29.160
know, look like one thing on the outside, but they have stuff inside.

26:29.160 --> 26:30.160
Yeah.

26:30.160 --> 26:31.160
All right.

26:31.160 --> 26:34.560
The nice part about having this app that you get to control yourself is that we get to

26:34.560 --> 26:39.200
consider correlations, we get seeding patterns, and I use other signals to improve the

26:39.200 --> 26:40.200
predictions.

26:40.200 --> 26:44.640
Based on the time of day, you can say this is probably a coffee, not say hot chocolate.

26:44.640 --> 26:47.720
Kind of using other signals to improve the predictions.

26:47.720 --> 26:51.040
We're hoping that one point will be able to say given, given that you selected these

26:51.040 --> 26:54.880
foreign ingredients, we can figure out the portion size based on how we see it calling.

26:54.880 --> 26:55.880
Okay.

26:55.880 --> 26:56.880
Yeah.

26:56.880 --> 27:00.440
I mean, just basically taking a recipe, you know, the ratios, and this is actually like,

27:00.440 --> 27:04.000
you know, this is actually integrated into some of the USDA data, like you have, they have

27:04.000 --> 27:08.080
some internal equations, basically from that we realize, hey, you can actually take recipes

27:08.080 --> 27:12.360
and understand what the ratios are, and we can integrate that to things that the, you

27:12.360 --> 27:17.360
know, other new things that we learn about from restaurant menus or from web recipes,

27:17.360 --> 27:18.840
which makes it even easier to log.

27:18.840 --> 27:21.520
You just take a picture of this sandwich, which is probably, you know, these things.

27:21.520 --> 27:26.120
For most people, that accuracy is great, you know, because otherwise, they have no infertional

27:26.120 --> 27:27.600
information in front of them.

27:27.600 --> 27:28.600
Okay.

27:28.600 --> 27:29.600
Awesome.

27:29.600 --> 27:34.600
Well, Vinay and Michael, thanks so much for taking the time to chat with us, really appreciate

27:34.600 --> 27:36.800
it, and really enjoyed longing about your company.

27:36.800 --> 27:37.800
Cool.

27:37.800 --> 27:38.800
Thank you for your time.

27:38.800 --> 27:39.800
Thanks.

27:39.800 --> 27:45.400
All right, everyone, that's our show for today.

27:45.400 --> 27:50.400
Thanks so much for listening and for your continued feedback and support.

27:50.400 --> 27:56.240
For more information on Michael, Vinay, bite.ai, or any of the topics covered in this episode,

27:56.240 --> 28:00.640
head on over to twomolei.com slash talk slash 65.

28:00.640 --> 28:05.760
To follow along with the NYU Future Labs AI Summit series, which will be piping to your

28:05.760 --> 28:12.120
favorite pod catcher all week, visit twomolei.com slash AI Nexus Lab 2.

28:12.120 --> 28:17.760
Of course, you can send along your feedback or questions via Twitter to add Twomolei or

28:17.760 --> 28:23.040
at Sam Charrington or leave a comment right on the show notes page.

28:23.040 --> 28:27.160
Thanks again to NYU Future Lab for their sponsorship of the show.

28:27.160 --> 28:33.640
For more information on the AI Nexus Lab program, visit futurelabs.nyc.

28:33.640 --> 28:50.800
And of course, thanks again for listening, and catch you next time.

