Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
As we approach Twimblecon AI platforms, I'd like to let you all in on our first major announcement
from the conference.
Now you all love this podcast for great guests and interviews and we're bringing that concept
right to the Twimblecon stage.
I am super excited to announce that Andrew Eng will be joining me on stage at Twimblecon
for a live keynote interview.
Many of you know Andrew from his work at Stanford, Coursera or his many other efforts in the
industry including recently founding deeplearning.ai.
Andrew and his work have been super impactful on my life and career and I know that's the
case for many of you as well.
In our conversation we'll be discussing the state of AI in the enterprise, the barriers
to using deep learning and production and how to overcome them, his views on tooling and
platforms for efficient AI delivery and other topics from his recently published AI Transformation
Playbook.
Be on the lookout for more great speaker announcements rolling out over the course of the next few
weeks.
You don't want to miss this event.
Get your tickets now at twimblecon.com slash register.
Alright everyone, I am on the line with Zach Lipton.
Zach is an assistant professor in a temperate school of business and an affiliate faculty
in the machine learning department and high school of public policy at CMU.
Zach, welcome to this week a machine learning in AI.
Thanks for having me.
Let's get started by having you share a little bit about your background.
How did you end up in this intersection of business, machine learning, public policy?
Lots of different areas.
That's a good question.
I'm not sure completely, but I can't really be completely honest.
I can't claim it was planned or anything.
I think if anything maybe it came from sort of just doing what I want to do at every
given point and not following a very specific path that was laid out and as a result wound
up maybe some plays different than the standard thing.
You recently transitioned into your professorship at CMU from your PhD.
What was that in?
So I did my PhD in computer science.
I kind of had a circuitous path to PhD and out of PhD, but I did my undergraduate in
math and economics at Columbia.
That was a long time ago and I was a musician sort of before, during and after and that was
my main thing.
I was playing this saxophone and I wanted to be a hustling jazz musician which is a hard
life even if you're the very best and luckiest, but certainly if you're not the luckiest.
So I was doing that for a long time after I think I was happiest playing music when I was
an undergrad and there was this kind of amazing balance about being at Columbia and having
the academic side of my life be a little bit more on the technical side which felt more
natural for me at least within academic environment.
I enjoyed jazz music as sort of a folk music and there's something very organic about learning
that music as an oral tradition and learning it by spending your nights out until I am
playing it with people who speak that music or who play that music kind of natively.
And there was something that I never quite loved about taking classes in jazz music
like in a university or conservatory setting and it always felt a little bit artificial.
I think maybe other other art forms might be more amenable to that.
And so I guess after spending a certain amount of time playing music being outside I had
some like personal setbacks so I was like hemorrhaging money and living in the lower
east side and the only way I was able to live in the lower east side is I was in a rent
control department and usually that means that unfortunately in New York was kind of neglected
by the landlords and so it was like a moldy apartment as people vomiting on your sidewalk
and all that and then I went out and visited a friend who was actually a musician who
was doing a PhD in music composition so he was a jazz musician but he had he was an amazing
pianist and has you know the kind of rounded chops that he could also make it in a composition
program.
He came out and started a graduate program at UC Santa Cruz so I came out and visited
him and I didn't want to go to grad school for music but that experience of being out there
and like after just kind of being in this sort of falling apart type state in New York
city in this rundown apartment and feeling unhealthy and everything then I was out in Santa
Cruz and Santa Cruz like the sun is shining it's the most beautiful place in the world.
The 90 year olds, the 90 year olds look 30 years old and then they you know that has
group of composers would get together every week and they had these listening sessions
and it was almost like a reading group you know like they would like it was very different
than maybe like my experience in music which is a being a social music was hard to have
like a kind of critical intellectual discourse about it on the level because you sort of
were it was very personal to people and you were trying to get gigs working with people
and I thought there was this missing part of my life that you know it's like very academic
slash New York thing and me that just wants to like have very like candid arguments with
people about things and I was I was in Santa Cruz and it was just beautiful and I was amazing
and these composers would get together once a week and someone would curate and they play
a bunch of music and then people would just scream at each other about it and like really you
know not not in an ad home in a way but just really you know express really really strong
kind of critical opinions about it and there was something about this this environment that
made me think you know like that oh that's it like it wasn't that I want to go to
grad school for music but it was like oh I'm going to move to California I'm going to do
a PhD I'm going to get into that kind of environment where I feel like I have that kind of intellectual
life and so I went back home and I like got my landlord degree to let me break my lease
early I asked my parents if I could steal our old like 2004 Toyota Corolla I took the sign
up take the GREs and then I was like I'm going to go do a PhD and I haven't even decided
like what the PhD was going to be in so then like the next step was like well what am I going
to do and actually came together as whole like really just kind of like ridiculous you
know like unqualified plan to do PhD came together extremely fast like two to three weeks
it was just kind of all set up it just kind of I don't know why I didn't even know what
machine learning really was but I knew I wanted to do PhD and I went I had a good friend who
was sort of had been a you know a bit of a mentor for me who was a biophysics professor
and I used to I built him I taught myself to program a little bit and I built him a website
and so we would hang out and play chess sometimes and drink coffee and he's to invite me to
the reading group so I had a little bit of a sense of what academic life was like even though
I I don't really know biology or physics and he was just always for some reason he took
an interest in me and we were we were close and so I talked to him I was like you know maybe
I should do PhD in biology and he's like you know that's not fun you know it's like that's
a top heavy basically it was like that's you know it's a great job and it's an amazing field
if you're on top but he's like you know it's going to take you six years to figure out how to
be a useful lab tech before you can even start doing anything creative and by that time you're
going to be you know you know on your way to 40 years old like that's that's not the route
and I thought about some more it's like what do I taught you know I've taken one only one or two
undergraduate computer science classes but somehow that felt like the right thing like there was
something just like really I think I think a lot of people had the first time you learn computer
science you learn about algorithms and you suddenly start learning how to think about how to
start formalizing things you see in the world where how would you model that computationally
and then you start thinking about things like the structure in real world problems that
makes them amenable to a efficient algorithm and something about that like kind of clicked with
me when I was young so even if I hadn't sort of followed up on it just having these like two or
three undergraduate computer science classes made this impression on me and I knew how to program
just enough to cause trouble and so thought about it well you know that's that's the thing I
could maybe sort of run with you know but that was you know it was a very thin thin basis to hang
my hat on but fortunately fortunately when I applied to PhD there are some people who were
willing to take a chance on me and and one of them was UC San Diego which is a absolutely fantastic
school that you know everyone in the world should apply to if you're interested in not if you're
interested in computer science or surfing or beautiful weather just building a new life
sounds like you pattern match Santa Cruz pretty well except for maybe I don't know eight degrees
warmer on average or something yeah it's a bit warmer it's you know they both I think have the year
round moderate weather thing right kind of happened to both have the surfing I think Santa Cruz is
a bit happier you know San Diego has multiple sides like you've got the La Jolla kind of stuffy
Mitch Romney kind of side and then you've got the I got a lot Navy presence there so that that's a
big player in San Diego but yeah you've got that that beautiful year round weather and that
attracts something cool great food also and so did you just jump right in take some courses
find your way to machine learning or what was that initial connection to the the world of ML
well ML was just the thing from the start like that was my I didn't know any ML like literally
if you told me to like write out you know explain to you on the whiteboard like a classic algorithm
like logistic regression or something I would not have known I would not have been able to do it
or you know explain gradient descent like I hadn't I hadn't ever implemented a machine learning
algorithm but what I knew was that I spent one year in San Francisco 2012 so like when I made
this plan you know that there's one problem with the whole grad school pipeline which is that
it's a great pipeline or you know maybe maybe people have some fault with it or something but
overall like it works well if you're already in the system so like if you're an undergrad and
then you want you you know you want to go to masters you're like well you know I'm a junior now
I got to start thinking about that and and and you do it and you're not like off the path
while you're making that plan right but the the trick is if you're doing something completely
different you know if you're like yo I'm I'm playing a saxophone at three in the morning for 40 bucks
in some weird dive that's gonna close down it a couple months and I want to go do a PhD
then it's like you're you're out of the system like what are you gonna so so so this was like
spring two thousand like end of spring 2012 is what you need that's that's like your last moment
to make a move like a serious like hard hard turn even if you play everything right to like
get in the PhD for fall 2013 right so you have to kind of not just like have this thing come
together but you then have to somehow stick with it despite not being like in that rhythm for
some period of time so so so my move is basically I knew that if I stayed in New York I would just
keep doing what I was doing so my move is to like break my lease sign up for the GREs get that part
out of the way because that was the part that I knew about and then move to California and you
know I was like well you know I'll I'll basically go to San Francisco maybe find a way to be uh
be a lemming for a startup or something until um you know while while I'm putting that together
at least I wouldn't be like in New York still hanging out till five in the morning like trying
to hustle for gigs because I'd be I'd be out of you know and it just also felt natural like change
change your life change your location like to wake up every morning in a new place so I moved to
San Francisco actually I did everything where you know it was a really wild time like I moved I
lived in one of these like totally could not possibly have been legal they call like hacker hostels
or something oh yeah like I saw some I saw I have this like problem which is maybe part of why
I have wound up in all kinds of weird situations but if you can cast like a situation as a choice
to have an adventure or not have an adventure and that's like a valid lens on the situation
then I'll choose the adventure and so and so I read and you know I was thinking like San Francisco
like you know whatever Silicon Valley whatever whatever you know it didn't it was still a little
more romantic at the time I think in 2012 it wasn't quite as like evil empires it is now but it
was already pretty expensive and there was these articles in New York times about these weird
hacker hostels we were like people would wrench them through Airbnb for like a month at a time
and they were like packed in and so I went out and I lived like literally in a bunk bed with
six people for a month until trying to make it big and Silicon Valley and yeah just trying
uh yeah I don't know what it was just just found find out what it was I didn't know anyone out there
so I really saw me some it was all like Germans met like a bunch of Germans who were hanging out
for a minute yeah they love their hostels yeah so I moved to California bite the California coast
and then set up in San Francisco and lived in this weird Airbnb for a minute and then I moved out
to Oakland and that was great I actually ended up playing a lot of jazz again when I was in Oakland
and getting to know that community and worked with the startup and then you know spent a significant
portion of my time then I'm applying a PhD and I got lucky that someone took a chance on me
nice nice and so you applied to ML programs you know not to be all CMU showblinist or something
but I think CMU's unusual in having an ML program and was in the past was exceedingly unusual
and having like a like an ML department like so I applied I applied to computer science and you
know you check off maybe some interest areas or something but it's not like a a separate program
you applied to computer science I think now things have gotten weird enough that you look at a
a typical school has a has a typical university as a school of engineering within a department
of computer science and within that some subgroup of people some working group that works on
machine learning not as like a formal distinction although maybe there's some kind of committees or
they band together for making hiring decisions or something but you know that's how CMU is very
unusual a CMU has a school of computer science and within it a department of machine learning
a department of robotics the department of a natural language you know it's called language
technologies institute department of human computer action and stuff like that so you know at
the time of applying most of the place you apply you just apply to CS and maybe lists or
interest I think now things are getting weird because I think more places are copying the CMU model
as places try to keep up with the band especially for like courses and in AI machine learning
I think the other thing that's happening is just a lot of schools get I've read from colleagues
who are professors elsewhere that you know at a lot of CS departments you'll get maybe CS will
be like six faculty out of 50 or 40 or something but maybe 50 60 percent of the applications
for grad school for PhD are people saying they want to do machine learning right so that creates
a whole other dynamic where maybe they end up treating it even if it's not formally a separate
application they have to throttle it a little bit because they're thinking well who are these
people going to work with right right right and the alternatives you accept them all you know
and then you know the half of them get the advice that they want and the other half end up doing
compilers or something let's maybe talk a little bit about your your broad research interest nowadays
what are you focusing on well you know I think the lens I mean okay there's a few a few kind of
lenses that that I have in research right there's you know a lot of people are more applied a lot of
people are more like theoretical or core algorithms and I kind of straddle that line a little bit
and on the applied side my my biggest interest has been since before I started PhD and has
continued to be throughout and as a young faculty member has been working a machine learning for
healthcare so so that's kind of you know if I just step back and think about not you know papers
in terms of like their aesthetic beauty or something but in terms of like
if I could you know build something big like what would the grand vision be it would be I
would like to have a positive impact in healthcare and I think there are opportunities to do it um
but at the same time working on problems in healthcare I think it's also a great application
not just because uh it's you know uh much better for yourself than working on advertisements
but also because I think it just sort of puts you in touch with what's wrong right because you
you basically you just can't afford like it's too important and the stakes are too high
that if you're going to if you're gonna go out there and say this is how we should do decision-making
or something or you know we could you people there's these sensational headlines your next doctor
might be an AI and it's such absolute crap right but the reason why is because it really you know
if you really think deeply about these things that puts you in touch with like the discrepancy
between the tools that we uh have mastered and that we're building and in the actual real world
problems we're claiming to to to make some impact on right so the the one hammer we have that
everybody uh you know it is throwing all over the place wherever you can stick it in it's called
supervised learning I'm sure you've talked about you know machine learning I guess you know
you've run 9000 podcasts already and everyone who was talking about an actual working system
probably was talking about supervised learning right they're they're limited exceptions or maybe
someone just and with banded algorithms on advertisements but for the most part you know supervised
learning is basically based on this idea that you're gonna get data that comes in right and it's
gonna consist of inputs and corresponding outputs and you're gonna um try to predict the outputs
based on the inputs and uh fortunately you know what makes it supervised is that you for the purposes
of training are gonna have as large data set for which the outputs are known so it's like someone's
standing over your shoulder and telling you what the right answer is and now the big big assumption
is that the the data that you're then gonna see in uh in the real world you your training data
was representative of it right so like basically the the historical data and the future data are
assumed to be what called iid which means they're like independently sampled from the same exact
distribution yeah and and that's just a oh oh wild assumption right when you then think like okay
wait a minute so so what can we do it's a we can infer a likely output given an input
um assuming that the future is in every like statistical way sort of you know the the the
historical data is perfectly representative of the future in every you know important statistical
way and that's just something that completely breaks down when you look at a lot of real world
problems right so uh one thing that happens is that just well the the historical data is not
representative so so this is a question that you know i think uh formally we talk about it in terms
of uh we're called distribution shift and distribution shift could be kind of benign or not benign
but um could it be kind of organic in a sense that it could be that hey you know one's day is different
from Tuesday because it's because it's different from Tuesday right so uh if you're classifying
news articles are somewhat people are you know different different stories are trending uh more
you know if you're classifying by topic or something there's more sport stories you know today
because Wimbledon is happening or something or you know the women's world cup and uh maybe there
will be less uh one week from now who knows so that that's one way that things change but then
there's more in serious ways that things change which is um the other key thing is that we often
you know the the machine learning problem the formal statements all about making predictions
but we're often not really concerned with making predictions we're concerned with um taking actions
right it's all about driving decisions if you're accompanying your thumb out automation and machine
learning is coming up as this multi billion dollar concern largely because of the hope that
you know what makes technology that valuable it's it's something that you can do at scales not
because it's just uh people are doing offline data analysis or you know trying to understand
their customers qualitatively because they're trying to drive decisions and once you start making
decisions now suddenly you're impacting the world and very often that very same environment that
generates your future data and we just don't have great tools for understanding these kind of
feedback loops right once we once you take the data extract information from it and then use it
to change the way that you make decisions in a way that you know influences the world everything
kind of falls apart and so I've done a lot of work recently um trying to look at well under what
assumptions can you make models um one that are sort of guaranteed to be robust against certain
kinds of distribution shift to short of that you know at least under what conditions uh what tools
can you use to try to detect as efficiently as possible as quickly as possible when somehow your
environment has changed um and beyond that to try to sort of gain some qualitative insight into
is that is that shift pathological or not is this something that you expect to to break your model
or or destroy the validity of your predictions anything about a medical setting right like you're
trying to you're trying to you know you want to have the the doctor AI your next doctor is going
to be an AI it's like presumably they have to be able to make treatment decisions not just uh
uh you know uh predict what would have happened if a different doctor you know if if the doctor
who would have treated you anyway uh had done their thing so that's that's a bit of a nuance
how does that correspond to the distribution shift and the feedback loops that you were talking about
because I think the fundamental premise of as to your point most everything we're doing here which
is supervised learning is you know we're going to collect this data that represents the sage wisdom
of all the best doctors and train our models on it and so then if you know our models making the
the decisions you know that are close to what are you know the doctor that would have otherwise
done them and does a good job at making those decisions and everything is good and rosy right
the problem is that well there's a number of big problems but one is you know what one is that
the act you know the world is changing naturally right and the actual doctor is has some understanding
of of the biology of the disease and and is somewhat adaptable in this way right like when we
look at the ways that machine learning models break because you you move the few pixels and an image
that kind of stuff doesn't fool the humans so the humans are pretty robust and I think actually
this is a and this is something that um my friend Jacob Steinhart and I talked about
in in a paper about some kind of misleading trends or some problematic trends in scholarship
is that there's this tendency in papers to to sort of make a kind of hyperbolic claim that is
insubstantiated by by by the research and what one of the classic ways this happens is people
talk about human level of performance right so the human level of performance it's actually not
it's not quite the right compare if you're going to talk about sort of human capacity the human
capacity isn't just for doing doing well in this very very constrained sort of like artificial
environment that only exists when you truly have a randomized trained test split the human
dermatologist is going to continue to be a good dermatologist even if the the light contrast
slightly changes on the images that they're looking at right or even if the skin tone of the
patients is different than it was in the training set problems where the the machine learning
potentially is going to fall apart in a catastrophic way the other thing is right you know
so there's there's an issue even with matching performance because like matching performance
you know when we talk about performance which I'm about accuracy offense like well accuracy is
itself a statistic right accuracy is only true it's only valid assuming a certain distribution
of data and if that's something that could change like if you know the the the patients something
something different happens you know the the patients start coming in with a different distribution
of illnesses some disease starts becoming you know that there's an epidemic it's not quite you
know pit not every patient you know you have to make adjustments to what what what illnesses or
patients likely to have given their symptoms things like that the other side is that ultimately
what you'd like to do is you'd like to not the dream of machine learning isn't in health care
isn't that we're going to somehow replace a bunch of doctors and in the process keep health care
like slightly worse but almost as good right like that's what you're talking about when you're like
if you can predict what when you talk about this imitation type learning thing so so a bigger dream
would be that you would actually be able to assist a decision-making process and so you'd be able
to like like ultimately the thing that gets most people excited isn't just say hey we're going to
automate doctors which you know already maybe misses the point about just how much of the work
is involved is is not you know taking the the data that's already there and like trying to
predict the doctor's decision but it was actually meeting with the patient and determining which
tests to run in the first place you know which resulted in the data which actually was already
all the work that the machine learning's not doing for you so you know that's another way that it's
misleading you know if we we start the machine learning and we think we're doing what the doctor
does but it's like well what about those test results where they can come from well because someone
who ordered those tests so what you know so then what oh we also have to predict which test to
order and then given the test we have to predict which disease given the disease we have to predict
you know also which treatment they're going to recommend and so you know you start if you actually
do a kind of fair analysis that puts together all the compounded errors that pop up and then
account for a world that's constantly changing things get messy but then even on top of that
it's like our goal isn't to just freeze medicine at this point in time get rid of all the doctors
and then just like put into place machine learning algorithm that is based on what medicine looks
like in 2018 what we'd like to do is be able to understand disease processes better and be able
to understand be able to analyze data using say for example models that don't just make predictions
but estimate treatment effects so that's something we call causal inference and ultimately right you
like to be able to say hey if I intervene and do something different than what the doctor normally
would do what do I expect the outcome would be or which patients should I assign which drugs can I
can I you know can we make a dent in personalized medicine to do that we're not just trying to make
predictions about what people would have otherwise done we're trying to figure out what are better
things that we could do and that requires causal inference and causal inference actually now
gives you gives you a whole other lens on the ways that humans and computers potentially need to
combine what they're good at because causal inference is not something that you in general can just
do given offline data and no prior information causal inference actually requires a certain
inductive assumption certain assumptions about the causal graph or the mechanism that relates to data
or which things listen to which things you know for example like the you know smoking causes cancer
potentially but cancer probably doesn't cause smoking i'd be like a classic toy you know textbook
example but building in certain certain kind of advanced knowledge together with offline data
you know then we're able to to potentially identify a causal effect even without you know running
experiments in the wild but we have to you know I think ultimately you know working on a task
like medicine really exposes and thinking about what it would take to actually do something that
you could actually run the wild exposes you you know makes you think about those things in a way that
I think you know maybe you should think about it if you're doing recommender systems but the truth
is that even if you don't think about it there's going to be a large company that's willing to
throw it out in the world and see if they make more money or lose money and if they make money is
kind of going to roll with it even if it's kind of doing the wrong thing right so we do that a lot
with recommender systems where it's like what is the real task we're trying to solve I don't know
curate interesting content but what do we actually do it's like we predict clicks because that's
a data we capture so and that's another way we that the contract breaks right I've talked about
the way contract breaks because the distribution changes because we actually interfere in the world
and that kind of messes with all the future data we see is now from a different world the world
in which you know customers are interacting with this this system that that changes everything
but another way of the contract breaks is we're just predicting the wrong thing in the first place
because the thing we really care about is something when we don't we don't capture a structure of
data right if you think about that like with a lot of these issues potentially that that
make people worried and concerned about problems uh regarding say for example racial or gender bias
in in automated systems another area that's been a lot of time thinking about and working on
you know one of the the clear failure mechanisms is the data that you capture that your model is
the thing that's convenient not the not the the true the true data it's not the only way that things
can fail but that's one of them and so you can imagine that you know you predict who's going
who to hire based on who the people hired in the past but you know that that's what you measure
is is who got hired in the past or who how are they rated by the interviewers in the past
well you don't capture necessarily is the thing you actually care about which is you know how
strong a candidate are they which is a more abstract concept so we end up relying on
instinct that we have data for which is not the same thing and you know what what is what is
that consequence of sort of optimizing the wrong thing and you know in the case of like YouTube
recently they had a scandal where the consequence was sort of curating pedophilia or curating
naked baby videos for people with this you know so yeah that's uh you know I think that that's a
especially dark or scary uh consequence but you know this is something we we have to think about
is is when the contract breaks um you know and I think by you know that sort of forces us to sort of
go beyond just the narrow confines of doing and evaluating supervised learning models where
mostly the technical content consists of people um you know just sort of the get squeezing out
incremental predictive performance improvements assuming the task is the right task and sort of
forces us to step back and think about either a more challenging or fundamental task like
estimating causal effects or thinking about the consequences of some of these systems like
you know using tools to say economic modeling and and that's one thing that's cool about sitting
in the business school and having a social scientist and economists as colleagues.
Well it talks me about some of the economic modeling uh tools and applications of those tools
in the space. What are some examples of how that plays out in uh healthcare and other areas?
I think the area where I personally am encountering economics the most like I came I got hired by
you know kind of strangely early in PhD I got approached by the the Tepper school um was made
looking to make um you know kind of uh move move in data science direct I think I was you know
I was a little bit of an experiment um and uh for for a little while I was a bit separated I think
um I wasn't like reading many economics papers or something I was wrapping up you know my kind of
core like I see a milner of type work and and actually what two things that me recently have put
me in touch with them is one you know uh starting to think a lot more about causality you take a
step back and say well who's who's doing empirical causal effect modeling in in the world broadly
and it's largely social scientists and you know like applied applied uh uh economists and
econometrations right like this is our bread and butter is um there was some shock to this is
dealing the is it true then increasing the minimum wage uh um decreases employment or is that
not true right and then you have these various ways of trying to draw these inferences from from
um you know shocks of the system in the natural world or you know various you know
they're you know they're the you know the handful of tools that that they're kind of tried
and true like instrumental variable analysis regression discontinuity um difference and
difference some of these tools you know rely on some very strong assumptions and I think
can sometimes drop you know if those assumptions are violated can lead to some wrong conclusions
but that that's one way that I've kind of started crossing over and I think you know a lot of those
tools are important because if you look at the work modeling you know when you look at the effect of
this technology in the real world and you you want to start saying well what is the impact of um
you know especially when you have these fuzzy systems right like uh risk scoring systems that
have been driving policing decisions that are then influencing these outcomes and it's very hard
you know from the machine learning standpoint if you there's big pile of papers that are just
considering the classification aspect right I've got these people they belong to this group
if you live belong to that group these are the these are the their inputs these are the um ground truth
outputs these are the predictions let me let me do some uh kind of arithmetic on them but what
you're not seeing is that like oh this system is generating a prediction this prediction is some kind
of score the score is an input to a human who's making a decision and if you actually care about
the downstream problem which is how is how is the introduction of this kind of system that is
mining this information to make predictions driving decisions and influencing outcomes then you
start crossing over into a land where actually the people who have the experience doing this are
the social scientists of actually um looking at real world data and trying to figure out um you know
what what what are what are what are these various effects um I think the other side actually
if I could jump in there that's a really interesting point because I think we often hear from
um the kind of the vendor community that's providing tools that are enabling uh these examples
these use cases that you're describing that hey you know we're providing these tools to uh to feed
into a human decision process as if you know that that somehow you know means that there's not
going to be anything wrong with the system like the overall system and also that we don't need to
further like study and understand that overall system with you know the impact of these new
tools in it and I think you're pointing to that well actually studying you know the impact of things
like this is you know something that we've been doing it for a while but just in other domains
and these are techniques that we can apply to these systems yeah absolutely and I think
I mean there's there's a lot to say about that right all in hand I feel like the easiest thing
to say which um maybe is not it's like intellectually profound but like I think as like a public service
announcement needs to be out there is like first of all like no um I think it's almost a bit
or well it's like first of all no it's not okay just because there's an algorithm involved
but at the same time it's also not just okay just it's not okay just because there's a human
involved right right um and there's there's a lot of issues to sort through but both in terms
of how we analyze these systems also in terms of you know I think one of the fundamental
difficulties in the area is also figuring out you know what is what even what is our goal
or what is what is the right thing to do here it's not always obvious um I think there are some cases
that to us are like very obvious that something is wrong you know but it's not always obvious what
is the right basis for making certain kinds of decisions especially when there's certain kinds
of like intrinsic uh trade-offs okay so so so there's question about what's the right thing to do
the other thing the other problem is it's also okay so the big meta point there I think also
is that hey these aren't new problems that emerge just because we stuck an algorithm in there
um but they sort of get seen through a new light and get a new kind of attention I think a lot of
us in the machine learning community get stuck in this loop of sort of trying to re like you know
you see a lot of people sort of talking about like AI ethics which is great it's great that people
are trying to think about um what is right um and uh think about these sort of like social impacts
of applying these automated systems but at the same time a lot of people are doing it because there's
a little bit of a bandwagoning effect going on right where a lot of people are jumping into it
kind of completely oblivious of the fact that people have been mulling over like what what are people
you know what what are people's rights and what um what what is like an ethical way to to engage
in a lot of these systems even before machine learning was introduced um and I was actually I
the benefit of uh in early June Cynthia Dwork um who's uh an absolutely inspirational researcher
um she's the inventor of differential privacy in one of the pioneers in the more algorithmic fairness
world and Patricia Williams who's a professor at Columbia Law um and uh they they put together this
fantastic um workshop at the Simon's Institute for theoretical computer science and so they've
been branching out into some more interdisciplinary type workshops and this one was uh about sort of
uh the you know race and data and and and and injustice and brought together people from I liked
it because you know you normally have a spectrum that brings together people who are working in um
computer science ML and then like computer science ML adjacent like you know the the I feel like
there's a sweet spot that like the the spectrum for interdisciplinary interdisciplinary runs from
um like uh information school type technical social scientist to theoretical computer scientist
and that's the spectrum which is great that's still very broad and it's a wonderful set of people
that I consider to be like a large part of my you know my home like the people who you will
meet if you go to like a fat star right the fairness accountability transparency conference um
but the cool thing about this workshop is a brought together people like uh Ruha Benjamin who
is in uh Princeton and writes um you know teaches in like African-American studies and has studied
very clearly um issues about race and technology in ways um that transcend not just machine learning
or this moment in time but uh more broadly um kind of history of um ways of technology has been used
as a as a tool that maybe exacerbates inequality you had a lot of people from public health and
epidemiology who were there you had a lot of people who had um been involved on the ethical aspects
from like the medical ethics community and uh when when genetics was like the big hot technology
and there were all kinds of ways that genetics was being used in ways that um had kind of um maybe like
ways that were expressed kind of dubiously in terms of um the you know ethical consequences
like various things various sort of studies and population genetics that were sort of sort of
sort of like neo eugenicist type uh work and um you know there there there's been a long history
here that is outside machine learning I think is you know where we came from this and um being
in touch with that and and you know sometimes it's actually a surprisingly you know when you come
at it from the perspective of like us you know people a lot of people who are who are first thinking
about these things and haven't like learned to um are only first thinking about it and in a relatively
immature way and go to an area um people who've been looking at say criminal justice and um thinking
about it and in a very mature way over a very long period of time and have a really deep understanding
of the the kind of systemic problems in a way that goes beyond just um kind of some tried formalism
that maybe fails to capture the the kind of like institutional level issues then um I think that
you know there's a lot to learn there and I think you know this is also you know one of the big
things that came out of it for me from that workshop that was I think um a really kind of profound
insight was that we have a tendency in the technical community to try to reduce things to some kind
of abstract problem but there's a big danger that we can actually I think a lot of us even a lot
of us sort of purporting to work on problems of algorithmic fairness but through a technical lens
by not understanding the broader context and not understanding the kind of systemic problems like
it's not just the matter of making the false positive race equal between two groups or something
like this but actually understanding um you know stepping back with like what does the data even
mean right or where was this data collected or what you know um truly asking um the kind of
foundational questions about the broader system in which the kind of this formalism is embedded
we run the risk of of actually doing uh what some people are calling fairwashing
which is that we could like take these fundamentally flawed systems where maybe the the precise way
that machine learning is being used there there's something fundamentally wrong with it
and we could go in and say oh you know I'm I'm into machine learning I'm into social good
let me let me do some work at that area and come up with a little tweak on the you know there's
a danger of coming up with just a small tweak on some existing algorithm right that sort of
preserves precisely qualitatively what we're already doing but gives somehow the impression
that you've made it like fair right I mean that's kind of reflective of a broader argument I
guess that's happening in the ML research community at large that the extent to which you know
the you know these kind of revolutionary advances versus you know incremental you know with
regard to publishing papers like you know that we're you know a lot of the papers that we're seeing
are kind of incremental application of some new training technique or something like that and
there is some commentary that you know we're seeing kind of this huge exponential growth in
the number of papers that are published but some are arguing that the field isn't advancing
you know accordingly and so this is this kind of fairwashing argument you know in one hand
you know the fairwashing element of it is different but it's also somewhat reflective of kind
of the broader dynamic that's happening in the research community would you agree with that?
Yeah I mean I think that the broader thing that it's kind of endemic is
I think there's a lot of trends in the community largely due to the success of the field
and the fact that it's grown in such a really I don't know if it's unprecedented in the
scoop of all fields but it's unprecedented maybe in the scope of of of AI machine intelligence
that has sort of resulted in this weird situation where the the review system is buckling a
little bit the review system the pool of reviewers has had to grow in a way that maybe changes
the standards of who's a qualified reviewer a lot of people are in a very volume driven I think
what one thing that happens is as a community grows so fast not that many people are getting
mentorship from you know true masters in the field but are really taking their cues from
machine learning subreddit or something and you know are looking for looking for a splashy or
just you know trying to get papers in or get citations so there's a lot of ways that the community
may be struggling a little bit now in terms of quality control and I think that maybe is a natural
thing that'll self correct is you know I think even this ICML was actually a step in the right
direction then you know the other side is like the specific trend that you're talking about here
which is maybe this particular thing that we talk about about maybe being a little bit abusive
with language misusing language you know making one of the things about technical papers even
when technical review works it has certain blind spots right like you you you submit a
a nirips paper you submit an ICML paper you have an expectation there are certain things other
reviewers are going to if the system works be very critical about and they are going to spot and
among them you know you expect that they're going to spot if your your your notation makes no sense
they're going to spot if your theorem is wrong hopefully they're going to spot if your experiments
don't sort of support the kind of performance claim that you are making in a in a like absolute
sense right but what they tend to miss and I think there's a blind spot in the review is the
reviews are not generally critical about the this sort of you know they're not able to assess
the critical arguments that are made and this is an area where you know the typical ml paper
is a bit sloppy so typical ml paper will you know there's a caricature that you can make which is
that your introduction is just kind of you know something is a problem and other people have worked
on it or it's you know used to say some kind of generic mumbo jumbo there's more and more data
and bigger and bigger computers and something something something and then you say something is
a problem and right now people have to do the work and we can automate it with machine learning
or something but it's basically in too many papers the introduction is not like an opportunity
for for like some profound philosophical argument that that justifies the why you're solving
a problem you are in the first place it's sort of a throw away piece that just is some kind of
stage setting so you can move on to the meat of the paper which is here the equations and here
are the quantitative results and that leaves you know is that the right bar for a paper a profound
philosophical argument is that the case you know in other fields not for every paper but um you
know well in other fields it is the contribution right now in other fields like the the argument is
the substance of the paper I think a philosophy paper um depending on which area of you know say
economics or something that that could be the essence of the paper can be the critical argument
that you're making which is very different where where we sort of get to that I have a very
well-formed machine learning problem and either I have an algorithm for it or I have a a smashing
empirical result for it that is the that in the sweet spot I think for like a a NURPS or ICML
paper like the sweet spot from a perspective of like most likely to get in is something like that
I've got a very well-formed problem everyone's already decided it's important I don't have to argue
for its validity and then you have the real sweet spot would be you had a just enough of a algorithmic
contribution that there's a non-trivial theorem in there and then you also had some amount of
experiments you know because there's one of these things like whoever it's whoever doesn't like
your paper the most it probably is going to get it you know uh you know like there's some argument
I don't know you know I don't know if this is truly true but you know there's maybe an
ask a negative truth in it that like a paper doesn't get accepted it gets not rejected or
something like that and so this way it's you know the person who would say ah you know if there's
no real experiments I don't know it really works that that person is satisfied and the person who will
their lazy way of accepting the paper as ads trivial they they get the they get the math that
they're looking for than everyone is happy but but you know I think I think the language is a bit
of a blind spot like for example it's extremely difficult to publish a position paper no matter
how well argued it I think if you if you submitted a an eight-page essay that like really
meticulously picks apart say the foundations of some kind of problem or some kind of application
of machine learning in a way that it's extremely knowledgeable or well researched or whatever
I think you'd have virtually no chance of getting it accepted at ICML or NURPS
I think the NLP community is a bit better like some of their conferences will explicitly
it if not you know say that's the number one priority and I don't think they should it's
technical conference well these include position papers as like within the mandate of the conference
where you do have a little bit more license for something like that is a lot of workshops
are familiar to that kind of material but you know I think the danger is what you end up getting
is what you get is a technical paper that maybe maybe it makes technical sense I mean even even
there I think we have some weird issues but that's a whole other you know topic but you know even
say the technical content of the paper makes sense what you have is a problem which is
ostensibly valuable in part because of the virtue signaling that it is addressing some important
real world problem right so so you start from that point but then the claim maybe you know the
paper says like we make our algorithm fair by doing whatever you know this is the fair the
fair DQN the fair GAN the fair whatever it is so the the claim like from the titles in the very
title of the paper is the claim is somehow we have satisfied whatever it is this thing the
thing called fairness fair and check you know we've made the ethical the ethical
reinforcement learning agent or something like this so like the claim is sort of at the outset
is not it's not like a firm technical mathematical claim is sort of like we solved ethics or something
like this and then the introduction very often you know for for and I'm not saying every paper is
like this but I think there's a pattern where the motivation might might either be you know on the
worst side it could be you know there's plenty of words it's just like babbling but on the other
side you could have work that is even intelligent but it's just that the the applicability to to
that real world problem that you're claiming that you uh solved is just missing or it's not there
or you know it's they're serious caveats and need to be stated um and then you know you have this
these weird things that you like you have to argue that your your quantitative result is correct
you have to argue you have to be meticulous about laying out like how you did your splits between
train validation test data um how many runs did you do um you know make sure that your uh work is
is reproducible by by communicating certain technical details you have to put the proofs for
the theorems you can't just make a outlandish claim that some mathematical factors true you have
to you have to prove it right but then there's things that you don't have to prove and I think these
are subtle ways that that things go wrong like for example you don't have to argue for what you call
something and some reviewers might get predicted they might be particular about language but I think
a lot of people are willing to let that slide um and and this becomes I think especially a problem
in this new dynamic where um the research isn't just among the research community right it's not
just technical research is being read by PhD students there's this weird loop between research um
industry uh governance and uh a huge part of that is the popular press um and so you wind up
and assist more like people uh are maybe motivated I think I think you know people I don't think
are malicious I think you wind up with someone maybe motivated by a genuine desire to uh do something
that they feel has more um social you know sort of some kind of positive social impact or something
like that but then they start working on and they think well um you know how am I get you know
what would be a good title for this paper it's gonna get people to read it or something you know
and and they're not necessarily thinking like hey I'm making a claim that this is this thing that
solves this there's this real world problem and I'm making a claim that this algorithm that I'm
putting which solves some very uh sort of like toy very reductive version of that problem um that
uh you know that has some applicability to that that real world situation which it might not right
and and then you know people you know like you know you put out this thing like um you know I'm gonna
you know my thing will tell you if your algorithm is fair or whatever without even being clear about
how we even address like the right ingredients or even speaking in the right language to be able
to capture something like that and we even access the right data or any any of that and then it gets
picked up in a story and say you know oh uh you know like for example like IBM has been a bit of a
a bit of a like cavalier actor in this way right both with this both with medical and with fairness
related things right so so IBM um did this thing where you know with Watson forever where they kind
of made people think they were solving fundamental medical problems when they really didn't have
they didn't have the goods to back it up and then now it's like oh we're getting ahead of the
fairness thing which is you know it's great that on one hand you know the running is ad campaign
they're raising awareness that's one thing but on the other hand they put out uh like an open
source repo and they made it sound like hey you know here's a set of algorithms that'll make your
stuff fair um and the big danger right is that someone would use that and think that like okay that
that that'll clear us and and work it's even crazier and this may be ties into a paper that I
wrote uh recently with Alex Cholde-Trova at the Heinz School of Public Policy and Julian McCauley
from UCSD is that you know sometimes the algorithm actually could be super horrible from from any
I think like reasonable ethical standpoint because people have miscast the problem right
those simplifying decisions that you made that sort of seemed intuitive from setting up a toy
problem when you weren't thinking about any real world data might actually result in something that's
absolutely horrible um that you would never want to use right there's some examples of that
so here's a problem in the in the United States our sort of perspective on like discrimination
in a lot of context is is informed by um title seven of the Civil Rights Act of 1964
now that gives I think our dominance in the research community means that maybe like the
overall like fairness work is a little bit skewed by technical interpretations of United States law
versus other countries which maybe have their own legal precedent um but that that you know that's
that's where I think a lot of like thinking um comes from and and in that context you know there
there's these basically the Civil Rights Act has these two um legal doctrines that have emerged
from it and one is this notion of disparate treatment and the other is this notion of disparate
impact and you know um these are motivated by discrimination and in housing and employment
and a number of cases and you know disparate treatment is basically about intentional discrimination
so it includes it subsumes discrimination based on like explicit consideration of something that
is deemed uh what they call a protected um characteristic or like membership in a protected class
right so this could be like um the what is someone's race what is someone's gender and in fact in
the original Civil Rights Act of 1964 there's like some some some some listing of set of protected
classes probably in 2019 there's some that we would want to include in things that we would think
of as kind of like salient uh like wedges of discrimination in society that we would want to be
especially productive of that were not included then like a sexual orientation or something but um
I guess like 1964 America wasn't there yet so it's not part of the Civil Rights Act
so disparate treatment is about intentional discrimination and um that includes but is not
limited to like direct use of that characteristic so you know it's important to keep in in mind how
the law works versus how math works with math we want to say I want to come up with a a crystal
clear simple set of principles that's going to govern any kind of situation that I could possibly
encounter right just it describes the world or you know physics or something like that we would
kind of operate in this sort of way the law is more like a patchwork that is is plugging in laws
in different places to try to deal with different things that are actually happening in the world
right so they're there there's a set of drugs that are illegal there's a set of drugs that are not
illegal but you know maybe you're qualitatively similar the reason why they're not illegal is
because uh they haven't but they haven't been abused yet you know once they are then maybe
they would become illegal right we would make a law we say oh there's a problem let's make a law
and puts us in a weird situation for governing technology because technology might bring up new
problems but our law is about dealing with uh certain when we think of discrimination when you're
creating the law it's a career thinking of you're thinking of the racist person who has a sign
out as we don't hire whoever that's one case right so let's so this retreatment I think and I
and I want to be clear that I'm not a I'm not a legal scholar I'm just a uh a fake legal scholar or
something um I'm a machine learning president who tries to be uh play one on TV I tries to be bilingual
maybe more than is standard but um I don't you know I'm not like uh you know I'm sure that there's
a number of people from from the other side that know this better but fortunately by by having
dialogue with them you know we we've had a more refined we've been able to refine our you know
um analysis on it right so that's this one thing this retreatment and the other side is disparate
impact and disparate impact basically is trying to cover those situations whereby you can have
what the law calls a facially neutral policy so facially neutral means like you know like on this
it appears not to in any way like explicitly be um taking it you know using this kind of
information and yet at the same time it can have an unjustified um disparate impact you know
an unjustified disparity and so it's really important to think about like what's going on here
and that on one hand we have disparate treatment disparate treatment is a doctrine that is
concerned with intent right and then on the other side we have disparate uh impact uh a
different legal doctrine which is concerned with um whether or not something is justified
and then we're trying to uh people basically look to the law because they say hey well I want to
make algorithms that you know don't do bad things so so let me look to the law because that that's
like my existing body of work that I could draw from but the law talks about intention the law
talks about justification and um you know I don't know what justification is in in the language of
supervised learning but I'm pretty sure that basically it's not expressable in the like like
basically we've got an insufficient language the the the way that we're talking about machine
learning problems is too reductive and it doesn't doesn't possess the vocabulary to express
this concept it's kind of like I know you've seen Judah Pearl if you read the book of lies kind of
like that the way he's he's you know he's he's he's ringing that bell talking about um you know
the all the long line of statisticians that didn't realize that causality was something that
lied outside classical statistics and like no matter how hard you try there's not like just
an expression in purely probability terms that you know tells you what causality is you have to
introduce like external notation um I think it's a little bit like that you know we have the law is
is coming from from this richer family of notions and and we don't know what it means to justify
something because all we know how to do is basically say is something associated with something else
we don't know why it's associated we don't know how to differentiate for example um
you know uh why what is the difference between um so uh the difference between if you were to look
at like educational outcomes right and say what what should what should institutes of higher learning
do you know in their in their admissions processes vis-a-vis um black versus white Americans versus
white you know versus Jewish Americans and those are two very different uh cases to think about and
part of why they're different to think about is because um you know there's this whole um background
of how that data was created it's not just is a group overrepresented versus another group
but there's also you know what was the process you know in one case I think we have a very deep and
well-documented knowledge of uh systematic you know like institutional um oppression of one group
that you know created opportunities and um with held opportunities from the other and in the other
case you sort of have a group that's maybe overrepresented despite being um discriminated against
and and and you know these kind of point to different uh I think to most people's like ethical
sensibilities different senses about you know how you should correct them but supervised learning
doesn't give you those tools right supervised learning doesn't distinguish these them
between these because it doesn't the the tools that we have in supervised learning for trying
to address fairness don't don't look into where the data comes from so back back to the those
two cases what people have done is they've tried to formalize you know disparate treatment and
disparate impact as technical ideas that you can express just as simple statistical parodies
and the reason why is because they want to build an algorithm right you want to we want to say hey
here's a problem I can I can define it in technical terms I can fix it in technical terms
so the way people interpret disparate treatment in technical terms is they say um uh disparate
treatment says explicit use of the protected class or intentional use of the you know intentional
kind of discrimination on that basis even if you know it's not you know used directly well we
well let's throw out the intentional part because we don't really know how to model intent we
don't know what that means it's not even clear if it means anything in the context of the
supervised learning model you know who's intent are we talking about the human the model so you
throw that part out and so then disparate treatment just becomes blindness so it means we just don't
use that feature then disparate impact on the other hand we don't know um how to deal with the
justification part because justification has we have to start thinking about where the data comes
from we have to start thinking about what does it you know we you know that's two it's too philosophical
so let's throw out the justification part let's just look at demographic parity right then
among other things you know you say well um let's say that I wanted to minimize the demographic
disparity between two groups while as much as possible maintaining the accuracy of the model right
how would I do that and the answer is actually simple it's I would go in there and explicitly
set the thresholds of the groups differently so that I would accept more people from one group
is slightly less than the other than if I had just like run a supervised model instead of universal
threshold and by doing that you would actually like optimally trade off you know the demographic
parity versus the accuracy and then what people end up saying as well but we can't do that
because we want to we don't want to have disparate treatment and the question is is it disparate
treatment if it's in the service of diversity um and that actually becomes this actually you know
it's so so the big problem like where we you know we actually we'll rent our own problem and got
to work through this issue is that fortunately we thought instead of others we had friends who are
in the legal community who work on these issues and are they passionate about them and are no
hit when they're talked about the wrong way from technical community thinking back to us and said
well you know you can't you can't have legal disparate treatment like what do you mean you're like
well it's by definition if it's legal it's not disparate treatment right so there was this there was
this kind of tension of so disparate treatment then doesn't actually mean blindness disparate treatment
means it's unjustified right it's like disparate it's not like having having disparate treatment
isn't just not being blind it's like having disparate treatment means not being blind in a scenario
that's like illegal unless it's somehow over and considered to be legal so what we've done I think
the big danger is we've we've overloaded these like reductive technical terms with the name of
a legal doctrine so we've purported you know at the end of the day we say we've solved you know
we've we've solved this for treatment we've solved this for an impact but we've solved this like
these sort of toys statistical parodies we've characterized these trade offs it's useful work
but the big danger I think is that we end up sort of misrepresenting the public that we've solved
like a legal conundrum and we have it right so what we ended up looking at is so what then people do
right is they they say well you can't have this for treatment because the loss is you can't even
though actually you can potentially you know and this is I think people there's all kinds of
other arguments like people are wary of doing anything that looks like affirmative action
or calling it affirmative action whether or not they believe in affirmative action because
there's a worry that you know even though affirmative action is legal that small changes in
composition of the Supreme Court or something could or you know it could be in a precarious state
so you want to come up with a solution that doesn't rely on that's a that's a separate issue for
I think for more from like a strategic angle in terms of pitching policy then from a technical angle
but what we end up doing is paper is basically look at this problem like one first how to how do
these how do these technical notions relate to these legal ones but then even further when you
try to trade off having you know this representational parity this demographic parity or or what
we call you know what they call satisfying you know disparate impact or not having disparate impact but
you know it's actually this like demographic parity in a statistical sense with accuracy and you
try to do it without explicitly looking at the group membership you come up with these algorithms
that have very very strange behavior and so we start analyzing you know what's the behavior of these
algorithms and two really weird things happen one is that basically well you have all these features
that are sort of predictive of whatever is the protected attribute right so correlated to yeah
right which is sort of why you have a problem in the first place right well why disparate impact
is the problem in the first place is that well it's not enough to just not look at that feature
because it's associated with with a label it's associated with the other covariates like
everything's all entangled if it's perfectly entangled like in the way that like you could just
with a hundred percent accuracy recover that feature right then it turns out well because the
optimal thing is to just set a class dependent threshold like and just move the threshold to sort
of in a diversity promoting way that's what your model is going to do anyway right if your model's
expressive enough that's explicitly what it's going to do so you know you've done all you've
done through all these hoops to pretend that you're doing something else but when the feature is
fully recoverable you're going to end up doing that exactly anyway so you're only kind of pretending
to do something qualitatively different in which case why bother but then the the more troubling case
is what happens when you know everything's correlated but not perfectly like you can only
imperfectly infer someone's gender say from their non-protected traits and then the weird
thing that happens is that if you take a lot of these algorithms off the shelf and you apply them
on data so we looked at CS admissions data and we said what if we wanted to increase the number
of a women admitted relative to men while without looking at the gender right so that's that's
the that's the kind of problems that was adopted by a lot of problems is as we want to
not have disparate impact but also not have disparate treatment you know keep in mind I'm like
using a scare quotes because that's not you know they're not really talking about the legal doctrine
but about the so we we tried to not use those terms so we called it impact disparity and treatment
disparity to be like just you know it explicitly say in the introduction like you know that we
use this to just be clear we're not talking about the the legal doctrine so if you say I don't want
to have either of those and then subject to that I want to maximize accuracy what happens
and what ends up happening is the model implicitly is trying to infer what is the what is the gender say
so in our case it was gender was a sensitive trait and then it's trying to use the inferred implicit
gender to flip decisions so basically you know you look at what was the threshold that you would
have had anyway and you have some people that are a little bit above the threshold some people that
are a little bit below the threshold um both among you know the men and above the women and what
basically what the model is doing is it doesn't know which of the men are the women it knows which
ones the things are more likely to be men or more likely to be women so end up being if you look
at as compared to the unconstrained model whose decisions get flipped and it's well one because
there there's an in-norded amount of men in the cs admissions data um it's it's largely just men
having their decisions flipped from men who the model thinks are women having their models flip
to positive and men who the model thinks are men having their very confidently having their decisions
flipped to negative and then there's also some flips of women who and women who the model thinks are
men might get hurt by the algorithm and women who the model thinks are most likely to be women but
are a little bit below the threshold are benefited by it um and then you think well you know what you
know if you step back if you think in terms of supervised learning and you just think hey what I
care about is that there's some group level justice then you know in that through that lens you
think you might still say this is you know this is worth it this is okay right even though it sounds
acid on more it yeah it sounds a little bit weird especially when it's like what we could have
just gone in there and just set the thresholds a little bit differently and just you know like our
goal was we wanted to increase the representations in group like why aren't we just doing that but
the weirder thing though I think that then this is the part where I think it kind of brings
things full circle to like thinking a little bit more like say an economist or like a social scientist
is you think not just about a prediction but a system of incentives right not just I'm making
classifications but I'm making decisions and this is telling people what they should do to get
into grad school right and it's like so if you start breaking it down you look into it and you say
who are the people that the model thinks are most likely to be women I like say among the women
like what what what are the the features that make the model think they're most likely to be women
versus men and it turns out that there's discrepancy is based on who your requested advisor is which
area you say you want to work in things like this right and so like to me that's like the scariest
part about it it's not just that it's like a little bit like if it was just your injectors all
these layers of unintended consequences and potentials for gaming and all kinds of stuff
and even more than that right because it's not just like unintended consequences like who are the
women like how do you how do you benefit from this policy as a woman you have to be in the field in
which women are already well represented so that it's in the subfield so that the algorithm knows
that you're a woman but the women who are actually hurt by it are the ones who are like in the field
that is underrepresented those ones who are hurt because the model thinks they're men right so
or things are more likely to be men and therefore things that's more likely to get more credit
towards it's like you know equality constraint by rejecting them and so the danger here and so like
I don't mean like I want to be very clear that one I think we need to do something to address these
social issues and I think it's like a pressing concern of our time and I'm all for it I also
think we need technical people working on it and thinking about these problems and even investigating
the potential of technical solutions but I think you know the the danger is we need to one we need
to be very clear about what is the problem we're solving and is this is this a solution anyone should
even think about using off the shelf right because like these policy makers are saying we need an
interpretability or explainability or or you know some kind of fairness whatever and when technologists
are showing up because there's a bit of a career cookie to say hey I'm working on that and I'm
I've made progress what about my research the big danger is well what what if what you're doing
actually is really bad like would be really really bad for anyone to deploy that um compared to
even doing something kind of naive right and and we have to be I think just really careful that we
don't like sort of create this uh sense that like we've we've solved this societal problem in a way
where like oh you know you could outsource the judgment to us we don't need we don't need the
the kind of think about it in a much way it's like we we've actually turned this into a technical
problem and solved it so so you know like you know outsource outsource the the like really really
pertinent like debate that needs to happen to society to the technocrats or something
so so right you know I think it's absolutely important work in an important area but the danger
that I think we kind of highlight there is there is there is a huge risk of a certain kind of
um folly of solutionism uh was that it was great uh getting a chance to finally catch up with you
and definitely lots of important issues here and things to figure out but not via solutionism
yeah yeah we gotta stay humble all right well thanks so much thanks for having me Sam
all right everyone that's our show for today for more information on today's show
visit twomolai.com slash shows make sure you head over to twomolcan.com to learn more about the
twomolcan ai platforms conference as always thanks so much for listening and catch you next time
