Hey everyone, hope you all had a wonderful holiday.
For the next few weeks we'll be running back the clock with our second annual AI Rewind
series.
Join by a few friends of the show, we'll be reviewing the papers, tools, use cases,
and other developments that made us splash in 2019 in key fields like machine learning,
deep learning, NLP, computer vision, reinforcement learning, and ethical AI.
Be sure to follow along with the series at twomolai.com slash rewind 19.
As always, we'd love to hear your thoughts on this series, including anything we might
have missed.
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via
a comment on the show notes page you can find at twomolai.com.
Happy New Year, let's get into the show.
Alright everyone, I am here with Timnett Geberu, Timnett is a research scientist at Google
Brain, she co-leads a team in ethical AI.
Timnett, welcome back to the Twomolai AI podcast.
Thanks for having me.
It is so great to see you once again, I think it's been a year since I last saw you in person.
Oh yes, yes.
Well, last year's Neurops and Black and AI, but two years since we spoke on the podcast
about your research.
This time around, we are going to be talking about advances in the fairness and ethics conversation
around AI over the past year, but before we do that, maybe a little bit of a kind of
update background from you for folks who want to hear what you've been up to in the past
couple of years.
Wow, it's like, it feels like so much time has passed.
Yeah, so I think when we last spoke, I was a postdoc at Microsoft Research in their
fate group, and I had just finished my PhD in computer vision, and I had just been at
fate for like a few months, actually.
And so I had, you know, it started to do work in fairness ethics my last year of my PhD,
and then I wanted to do this postdoc to be more embedded in that community and to have
more time to explore.
And I had, you know, we had just done the first Black and AI workshop.
And now it has, you know, this is our third time that we're doing Black and AI, and it
has grown so much over the last two years.
And so then I, I, then, oh, I was in New York when we spoke, and then I, now I'm, since
about a year and three months, I've been at Google Brain, and so, co-leading the Ethical
AI team with Meg Mitchell.
And yeah, there's many, many challenges doing fairness and ethics in industry, but so
far I love, you know, my team, and I love what I'm doing.
So, you know, there's many struggles, but like it's been a journey in the last two years.
Absolutely, absolutely.
So we can probably start with Black and AI, since that plays such a big role in your work
and life, and just so folks understand it, I think one of the things that really jumped
out at me from the workshop this year, I think it's easy to think of it, and I think
even NURPS calls these affinity organizations, and sure, you know, it represents an affinity
for me and identity in a sense, but you know, there's also kind of real work that's happening
not just to allow folks that have an affinity to come together, but to kind of expand the
presence of Blacks and AIU, or someone mentioned that at the, a couple of years ago, or three
years ago now, what?
In 2016, yeah.
In Barcelona.
In Barcelona, there were five.
Yeah, I counted five, yeah.
You counted five.
Yeah, I couldn't see anybody, yeah.
But, you know, for those that don't have this experience in Blacks, sometimes you just
know how many people are in.
Yeah.
And there was no formal count, too, right?
That's one thing that has changed, like they're doing diversity statistics.
They're counting statistics.
Now, they had, Helen had this amazing survey that they spent so much time on that they
put out last year, and now we have, you know, people advocating for accessibility, so we
have disability in AI, we have Latinx and AI do not exist, you know.
We're querying it.
Did not exist at the time.
Yeah.
Yeah.
New and ML, you know, I think there's much more of an understanding now that NURPS has
been transformed, in my opinion, you know, because I, my first time being here was 2015,
and I really, this was a conference I did not want to come back to, and I had written
about it actually.
I was going to send in an anonymous as an anonymous article, and like everybody, my friends
were like, yeah, no, everybody's going to know it was you, so it's not going to be anonymous,
but I was complaining about, you know, I was harassed at a party.
If there was just so much, so much stuff that happened, I did not feel welcome, I didn't
feel like the community was trying to pull me in.
I felt like the community was basically trying to pull me out, push me out, you know.
Why do I belong here?
Why am I here, you know?
And now when I come here, it's really big and overwhelming, that not only has, I feel
like it's changed in terms of diversity and inclusion, but also the scope of, you know,
workshops, they have all sorts of meridethwitticurs here, for example, and this is not just about
ethics, but also organizing labor and why that's important.
So I mean, it's, I cannot believe I'm saying this, but I find it so much more inclusive
than my whole, my vision community, for example, and now it's like when I go to the vision
community, I'm like, whoa, it's like three black people there, and the size is basically
about the time, CVPR I think was like 6,500 people or something like that.
And now when I see all the work that has been done at New Europe's to make it so much
more inclusive, I just, I go to like my vision conferences and I'm just like, wow, there,
it like hasn't even started yet, you know?
Well, the thing that I specifically wanted to acknowledge you for and the team that works
on black and AI is that I think, you know, the affinity aspect of it is great and making
those of us that are in the community, you know, feel more included is wonderful, but this
community is actually going out and finding other people and making it so that they can
be here.
Yeah.
That was not possible before and it became really clear to me, you know, at the workshop,
like after the workshop, there was some administrative thing that was happening that caused the
people that were to line up to want, you know, that we're brought here by black and
black and AI was the only reason they could be here to be lined up, and it was a lot of
people.
We were handing daily, you know, I don't think people understand how much work we do because
you know, there is a lot of affinity groups, right?
But many of them, they, what they do is they organize the workshop and then they raise
some money and they determine how much they can give in funds to, you know, if you're
coming from the African continent, if you're coming from, et cetera, et cetera.
And then they basically give you a certain, they refund you a certain amount of money
after the workshop, right?
And we know that we can't do that for our participants because many of them wouldn't
be able to just pay in advance and then, you know, get refunded, right?
And so we give out a per-dem cash for people to use for food purposes.
So that's what was happening.
For the workshop, we were handing these Visa cards, prepay Visa cards, so that they
can just use it for the rest of the week for their food expenses, yeah.
And you know, there's a lot of advocacy that happens in Black NIA too, you know, we once
again had our Visa issues, and then within Black NIA, you know, we also have to, of course,
there's intersectionality, right?
Just because, you know, somebody's Black doesn't mean we're addressing all of the issues.
There are, you know, US-based, Brazil-based, the huge African continent with 1.1 billion
people.
And, you know, HBCUs, which community are we neglecting?
Which community is underrepresented?
Which communities are overrepresented?
So those are things we had to, we constantly have to think about.
So this year, I was really excited by our partnership with OHAB.
They're great, and like we had some 25-26 people come through our partnership with OHAB,
and they were primarily students from HBCUs, others were founders, and it was like one
amazing moment for me was when this, and we also had many more Brazilians that this
year than we did before, because, you know, just because we're in the US, it doesn't,
we might have a different concept of, we don't always understand the severity of the problems
in other places, right?
So because Brazil was like the last place to abolish slavery, right?
And we have like almost 60% of their population is Black.
And so imagine, right?
And then we, I don't ever see a Black Brazilian in any of my professional settings.
And so this Ramon who came to the workshop was telling them some of the students that
for two years in his undergrad, he never saw a single Black person in his entire undergrad,
right?
In a country where more than 50% of the population, so you understand the severity of
the issues that exist.
And what was interesting is like one of them was saying, so the first time he came to
me, she was like, you're a Brazilian, and he was like, yeah, I was like, I never, you know,
the image of a Brazilian that I see here does not look like you at all, so I didn't even
know that country was majority Black, you know?
And so that, I love that interaction, like it's not just, you know, having this space
for the Black community to be here, but it's also like educating each other about the
different communities and understanding global, like how diverse this community is.
Yeah.
Yeah, so that was a great moment for me when I sat up.
Awesome.
Awesome.
Well, thanks for all your work on that front.
We're a great person to talk to about what's going on and, you know, what is a, a very
broad field, AI ethics, fairness, accountability, et cetera.
So maybe let's start by having you talk about, you know, things that come to mind as, you
know, milestones in 2019 in the field.
Yeah, I think that one of, I was thinking about this, and I think one of the things I'm
seeing right now is the fairness and ethics community starting a little bit to be, to
look and work a little bit.
So it's kind of wondering whether some of the technical approaches, whether they're
causing more harm or whether they're, like, putting bandaid on, you know, just a bandaid
or whether they're financially affecting things, how, you know, our approach needs to
be sociotechnical.
So I think for me, that's the milestone I'm seeing is that that conversation is starting
to happen about the approaches themselves.
So I actually, I'm not sure if this paper came out this year or was it last year, but
I do believe it's like within the last year, it was this paper on delayed impacts of fairness.
So examples of these kinds of papers, I think another paper, the title was, I believe
it was lipstick on a pig, and it was talking about how, you know, these debiasing word
embeddings might not actually be fixing the underlying problem.
And what I'm personally, like, actually most excited about is the understanding that this
can't just be a technical fix.
And that, so for example, and now they had this report discriminating systems, right?
And for them, it's a system, right?
So they're not looking at discrimination in the images, discrimination, they're looking
at the system.
And the system includes the people who are building it.
And so in that report, they were talking about gender discrimination and, you know, they
were giving all sorts of references for people and how basically if you have a group of
people who are homogeneous building these systems, you're never going to be able to achieve
any sort of parity.
And I think it's also the unders, I think the starting to discuss the fact that fairness
is not just about equalizing performance across certain subgroups.
So this is the first year where I'm starting to see more discussion, a lot more discussion
about the fact that fairness is not just equalizing, you know, some error rates across
different subgroups.
Right?
So, like in our team, Alex Hanna, Emily Denton, who just, so I came, I had to attend
her talk right before here, gave a talk.
And Jimi Lesmith, Lawden, Andy Smart just wrote a paper about critical, a critical
race theory approach for fairness, right?
So, so for example, the fact that, so Joanne and I were talking about how in our paper
in gender shades how race is a social construct, right?
It's unstable cross time and space, et cetera, et cetera.
In this paper, they were, they were talking about how you have to really engage with critical
race theory methods and how you have to, you know, you have, races again, an, a social
construct that sometimes maybe that's not what we want to use for annotating data sets,
same with the gender, right?
And, but then there's a tension between, you know, if you're annotating for, let's say,
gender, right?
You, you need to make sure that you're not further harming communities by, you know,
making a binary by further adding like data, additional data that needs to be added from
groups of people.
So that's like additional privacy risk, making sure I think I forgot this paper that talks
about like the burden on the, on minority groups when you're, you're when you're trying
to get more data from them, but at the same time in order to equalize parity across subgroups
or even not even equalize parity, but like just to have parity.
But just to test out how well something is doing across subgroups, you have to define
these subgroups, what they are, what those boundaries are, and you have to then go and gather
additional data sets for that.
So what problems occur when you're thinking about that process itself?
Now is that.
I'm seeing a lot more discussion on that, on that process itself.
You see what I mean?
When I first started working on it, it was like even, you know, the notion that you have
to do intersectional testing, so you don't just test for, you know, if my model is doing
well on everybody, what does well mean?
So there's a lot of, you know, paper is own defining notions of fairness.
There was, you know, and then we had to introduce the concept of how you have to do disaggregated
testing, which means, you know, don't just say, is my model doing well for like women?
Is it just doing well for like, you know, is it doing well for like this group of people,
this race, that race, you have to say, is it doing well for darker skin women, lighter
skinned women, darker skinned men, you know, that concept was a concept that was like
kind of introduced, right?
I mean, we're just saying like, how important it is to do that, to break it down like
that.
Now the question is to make sure people understand, hey, that's really not the only thing.
We don't assume that every single question about fairness is whether the performance is
equal across different separate groups, right?
It's about who has the data, who doesn't, how it's being used, whether a task should
exist or not, et cetera, et cetera.
And then even in cases where you have to define subgroups to see the models performance
on those subgroups, you have to like creating those subgroups is not a piece of cake, right?
Like in the complexities that arise when you define subgroups, who's doing the defining,
the taxonomy that you use?
So like, I'm seeing much more nuanced combo, like discussions around this.
So this paper that I was discussing, critical race theory message for fairness, so I'm
so bad at like these titles is.
We'll find it and put it in the show notes.
Yeah, it's those kinds of things.
And that's what I'm excited about, and especially just like the understanding, the complexity
of what we're talking about, right?
It's not just about defining what is fair, and so you have a mathematical definition
for it.
It's also like how your model interacts with the society that you're in, what kind of
documentation you have available, how you take feedback from that, and I'm starting
to see that conversation right now, right?
So what do you think is driving kind of the increased appreciation of the nuances here?
Is it kind of broadening of the folks that are in the field, or just...
I think they're longer.
I think broadening of the folks that are in the field, I think that for example, more
of us, more people finding each other's voices and kind of amplifying each other's voices.
So for example, Emily Denton is very well known for generative models.
That's what this community knows her for, right?
Like the Neurops community and everything.
So she was invited to give a talk on this workshop, Retrospectives.
So it's basically, do you know about this workshop?
You're supposed to talk about a paper or something that you wrote and like what would you
say now kind of thing?
And she did a retrospective on computer vision as a field.
And what she was discussing is all of these things I'm telling you about, the view from
nowhere.
She was talking about this concept of the view from nowhere and how this has been critiqued
by feminist studies, for example.
And so the view from nowhere means it's basically saying that scientists assume that the view,
it's subject, you know, science is objective.
You're finding these, trying to find this objective truth.
So it's not from anyone's point of view.
It's that like there's objective truth and it's a view from nowhere.
So in fact, it's called the view from nowhere.
So she was describing how this, you know, underlying foundation of this feeling that our work
is the view from nowhere and how that's kind of driving a lot of these, and power dynamics
and how that's driving a lot of these issues we're discussing.
So for example, she was naming certain data sets even that we don't question, right?
And like one example of Celeb A data set, which is Celeb A, which is this data set of
faces that is very widely used in computer vision.
And so one example I want to give you is even when people are thinking about fairness,
they would use Celeb A.
So Celeb A is this data set that has 40 facial attributes that are kind of annotated.
Okay.
And so let's say it's like smiling versus not, young versus not attractive versus not,
you know?
And so when people are using this data set, people have used this data set to train all
sorts of models and especially in the GAN literature.
And when they're even writing about fairness, many people would use this data set and they
would just say, oh, like we're trying to make sure that attractiveness, you know, label
is not dependent on gender or something.
Like, I don't know.
So it's this kind of thing.
And then what we, but also you have to question how, whether we should have an attract
a data set within attractiveness label in the first place, who annotated this data set,
you know?
And where is this kind of model going to be used by whom?
And so that was the kind of stuff that Emily was talking about.
And data annotation practices, data collection practices.
So we, at least there, you know, there's a long, long way to go to even have people listen
to this conversation FYI.
But what I'm saying is, oh, yeah, for sure.
I mean, I was called an actor like I was given all sorts of feedback for having this kind
of talk.
But I'm saying that like, at first, we had to just be like, hey, you have to pay attention
to this thing, you know, and we couldn't be too super nuanced about it.
And now we're saying, hey, like when you're now thinking fairness, just don't think just
equalizing metrics of some metric across the groups and just publishing a paper.
Here are all the nuances.
It's a system.
And here is how we should think about it as a system.
And so that's really the biggest thing I've seen.
And how, when I say a system and like how it's really, you know, labor organizing that
Meredith especially does, right?
And how that feeds into, for example, labor organizing for contract workers, who are
the ones who annotated these data sets that we're talking about, right?
What their incentives are, how they're treated, et cetera, et cetera.
What the economic incentives are, and how that drives some of our decisions in AI.
And then how that feeds in, so like we're starting to look at it as a, or more and more
people are starting to look at it as a system.
But at the same time, because fairness is also now like a much more, I would say it's become
like more of a mainstream thing like for CBPR for the computer vision call for papers.
I saw like an actual explicit like bullet point for fairness, accountability, transparency
and ethics and computer vision.
Whereas last year I co-organized the first workshop of fairness, accountability, transparency
and ethics and computer vision, right?
And so it's starting to be like this mainstream thing in all of these different conferences.
One shift that I'm seeing, and I'm wondering if it's new or you're seeing it also or
is it interesting, it feels like prior conversations about fairness and ethics, or at least
a lot of them that I've been exposed to have been around like, you know, with just what
you've been describing tools to analyze the predictions you're making or your data sets
for biased things like that, and I'm hearing more conversations that are asking more fundamental
questions like, should we be doing thing X at all?
Exactly.
So I think that is that conversation I'm seeing more of now.
In the beginning it was more like, hey, there's this thing called fairness, or, you know,
oh, that's a paper, this thing called fairness, every step of the paper, yeah, that was
people in my team really, really love.
This year?
Yeah.
Okay.
And yeah.
So, so.
Yeah.
So there's, you know, in the beginning, I think, you know, many, many people, especially
in the theoretical, more people in theory, I would say, in the theoretical computer science
we're working on fairness as a, just a set of field, and I would say, a lot of people
were not engaging with like critical waste theory or feminist, you know, theory or anything
like that, from some feminist works or anything like that, and now there are, you know, still
a few people, but who are engaged in both, like, bridges between those two communities,
and then there are some people who are still in one community or the other community,
but I believe that the conversation right now is almost trying to address the fairness
community itself.
So initially, we were, I think people were just trying to have a community at all, and
now, like, I don't think the justification needs to be made that people in, at sea,
in Europe, or at CBPR, or these other conferences should work on fairness, but another conversation
is more, hey, like, when you think of fairness, like, don't just go, you know, take a data
set that's already problematic, think about how it's problematic, and have, you know,
what kinds of tools can you have to analyze why something is a problem in the first place,
right?
The view from nowhere, critical waste theory, like, you know, another conversation we've
been having a lot is about global notions of fairness, a lot of fairness work, because
is, I guess, is being driven by North American people in general, people who might not be
from here, even, but, like, from the U.S.
Have a certain perspective, though.
Yeah, from the U.S. or from Canada, I guess, Mexico, but I wouldn't say Mexico is very
highly represented in this work, have a certain perspective, and a lot of it is grounded
in, like, let's say, civil rights, or our U.S. law, or something, right?
And so, what about other perspectives, what, what transfer is, and what doesn't transfer,
you know, these are some of the conversations that people are starting to have.
So, that's sort of what I'm excited about.
I'm excited about how, you know, how, like, more awareness of how you can't just sit in
a corner and write equations and do fairness.
I'm really, I just, I honestly believe, for me, that it really always comes back to what
people do in international development as well, right?
There's a term called the, I believe, the seductive reductionism of other people's problems,
right?
It's like, okay, so when you're walking down the street, and in your own neighborhood,
you see homelessness, you realize how complex it is to work on homelessness, you know?
But when you're, you know, watching TV and, like, some person is holding an African
baby somewhere, you know what I mean?
There's always these shows, right?
You might be like, oh, let me go solve that, because you don't understand the complexity,
right?
You're not thinking that maybe there are a lot of people who have ideas on how to fix
their own problems, and they don't have, you know, visibility.
They don't have toolkits, right?
And so, how can you uplift them?
And one thing I worry about in the theoretical fairness community is, like, for example, there
was just a fairness workshop that was held at, you know, the Simon's Institute, I believe,
it was all, like, not a single black person.
You know what I'm saying? Like, you have 15, 16 people.
It's like a closed thing, and they're doing research, mathematical research from fairness,
their papers are getting published, they're getting tenure, talking about black people,
and there's not a single black scholar there.
I think that's exploitative.
And then they would probably say, well, like, we're looking for just this specific expertise
and, well, there's no black people with this specific experience, but then part of your
job, if you do really, really care about fairness, whether it's theory or whether something
else, is to make sure there are people in that community who can also get the scientific
credit for this work, right, like, get, and so that's really, really, for me, what's missing
in this community and the thing I worry about the most.
Do you see any salient changes over the past year in the way fairness is approached or
advanced from a kind of practical industry perspective?
Yes, definitely.
So I guess I'll do a plug here.
One example, oh man, yeah, I mean, I didn't mean to do it like this, but one example is
model cards for model reporting, which, Meg Mitchell and so many people in our team and
across collaborations, worked on, right?
And so a bunch of us had been there.
Like this related to the data sets for data sets, for data sets, yeah, yeah, exactly.
Exactly.
Basically, yeah.
And so a bunch, like, you know, I realized, when I was thinking about data sets and people
at MSR, like Hannah, Jen, and Hal, and Jamie, and Brianna, and all these people, and Hal
and Kate Crawford were thinking about it, then Emily Bender, Beth, and Friedman also had
to have this paper data statements for NLP.
So that was like two years.
It's really interesting, right?
And we're independently sort of like thinking about this around the same time.
Of course, we're talking to each other, we're inspired by what we, the other person says.
And a couple of years ago, it was like, hey, we think we think we should do this.
We think, you know, shavdishies for data sets.
And here's why you have to justify it, like here's why you think we, and then around
the same time, like before I arrived at Google, there were also working on, you know, disaggregated
testing and thinking about how they could apply this to models and stuff way back, right?
And now, fast forward, just down, like just like right before Thanksgiving, Google announced
model cards for model reporting as part of their Cloud AI, like explainability toolkit.
And like, you know, you could go see example models.
I mean, this is just V1 kind of, and it's also to help like gather feedback from people
to see like what works well, what doesn't work well.
But this is actually a thing for a real Google product.
You and folks on an ethics team have worked on that is now part of a Google product that
people can go swipe with.
Yeah.
So that's what I mean.
So it's a, you know, before we were trying to convince everybody, this is something
that should happen.
Now it's a start, right?
Like so for a real model, a real Google model is not a toy one, right?
That is being sold to people.
You can see you have a model card for it, right?
And it was, it's not a piece of cake to get an institution to do this, right?
Like it's not.
And it like requires so much hard work and like a mega and Parker and Andrew, I mean,
so many people have like, it requires so many people from different, with different expertise
and different organizations.
And so that's one thing I'm seeing, right?
And I'm seeing actually conversations around data consortiums, just like higher level
concept, concept, conversations, I believe they use having all these conversations.
One thing I see, I think, is that so many organizations are talking about having ethics
principles or guidelines and things like this, but then the question is like, how do they
get enforced, right?
And like, that's one shift I've seen in the last year, right?
Like there's a lot of AI principles, guidelines and all that stuff.
And then now I think the next kind of year or two, whatever is like, how do they get
enforced, you know, how do we beat, how do people, how do people believe that they will
be enforced?
We wrote recently, like with the Debra Rajin, Andrew Smart and many other people, we wrote
a paper trying to kind of think about this, but like, but I think that's the next.
So I guess I was doing like more retrospective of what I want to see in the future kind
of thing, right?
Like, so yeah, so I think that like model cards being real for me is a huge thing, right?
And I don't know, I think, you know, there's many, the partnership in AI has just about
a Mel project now and it's about a Mel is like the acronym.
Oh, okay.
And so they have this like, it's based on, you know, data sheets and model cards and fact
sheets from IBM and they're trying to see how this can actually be a standard, right?
And it's, you know, it's funny, like when we write papers and stuff, we like, this should
happen, but then when you're implemented, it's also like, how many stakeholders are there?
Like, what's the right granularity of documentation for whom and what format?
I mean, it's so much stuff, right?
So, but actually, but the fact that it's actually starting to happen is, is a, is a pretty
big deal for me.
There was a, I don't know if it was a workshop.
I imagine it was a workshop that I saw, I only saw the title of this on one of the electronic
signs in the conference center here at Neurops, that was intriguing.
It was my name, the gap ethics and fairness, do you know what that was about?
I see a lot of people, actually, from my team, Alex Hannah and Vinod from my team are speaking
at the panel there, Meredith was there, I think, I know, I know about this workshop.
I wasn't involved, like, so basically maybe next year, this will change, but when I come
to Neurif's, I will, I don't get involved in any workshops, I don't, I don't give
any talks, I don't, because basically Black and I is then, so it's really hard to do anything
else.
Yeah.
So, I, I chair a session, whatever session, I have to chair, I go there, but like, it's
hard to take on anything additional, but yeah, but this workshop I know, I've, I've heard
about it because of this, but it's actually good that, you know, they have people like,
Meredith, they have another person, I believe who was one of the people who just got fired
before Thanksgiving for, I, I don't know, I mean, one of, he was one of the people who
organized, who were organizing against like Google's relationship with, I guess, ICE, right?
And so it's hard, right?
Like when you're working at an institution, like, there's their policy and then there's
you as an individual working and being like, well, like, I'm an immigrant, you know what
I mean?
I was a refugee and so how can I talk, how can I work in ethics and whatever and I don't
care if you have a model card for the model that you're giving them, right?
These are the layers, right?
Right, right.
And, and you have a, you know, like a relationship with them.
So like, he was, you know, one of the people who was working on this, organizing against
this, who was fired right before Thanksgiving, and as you know, Meredith could organize
the walk out all, all of this anti-maving activism and she's, you know, huge obviously
in the ethics community and she's no longer there, right?
So it, it, it makes me wonder like if my days are numbered or not, you know, let's hope
not.
I hope not.
You know, I mean.
I've got model cards.
Yeah.
What are you seeing happening on the commercial side of things like I'm hearing more and
more like IBM, for example, you know, got fairness 360 and I forget what the other, they've
got several of these kind of ethics, fairness, focused things.
There are startups that are, you know, focusing on explainability, which kind of plays into
this.
You know, how do you think about the, you know, the commercial space or even open source
like the set of activities going towards making this more tangible and accessible to practitioners?
Yeah.
Like I think, you know, again, like a lot of companies that are coming out with toolkits and like
so the IBM one is a good one.
Google like is integrating a lot of things into TensorFlow, like there's fairness indicators.
For example, there's, there's also a lot of educational material that actually like
Andrew Salvador on our team really like worked so hard on various colabs, which you know,
like on other educational material that's out.
So like for anybody who wants to kind of learn a little bit more about, you know, what
things to watch out for, et cetera, like there's this stuff happening.
Let's see.
There's also, you know, when you're on the topic of IBM, so again, there's this brings up
the complexity, right?
Like so they also came out with a data set, right?
So diversity and faces data set, but then there's also other things that show like the complexity
having this kind of data set, right?
So people are talking about like the fact that that data set was scraped through Flickr,
but then, you know, even though you have creative comments licensing, like when people put out
their pictures on the Flickr, they didn't know like 10 years later and that it was going
to be used for this kind of thing.
And so I know there's like some cases of people suing.
I don't know if it's specifically IBM, but also other academic institutions.
I mean, this is the kind of stuff everyone in the community has down, right?
And so that's also other stuff that's happening.
And then, you know, at Google, there's like facets and other kind of tools to visualize
your facets.
Fascists.
You should look it up.
It's great.
It's a toolkit that came from pair.
We actually use it.
Joy and I used it for gender shades.
I mean, by Joy and I, I mean Joy and Deb, like they worked on and the website and it's
a visualization tool helps you visualize your data sets and all sorts of other things.
So it's through like a lot of companies and are coming out with these kinds of toolkits
to help you analyze things.
And then I don't remember, but I do believe there was a whole bunch of people working
on like Python libraries, you know, for some of these things.
But another side of the story is that some people, again, are cautioning against, you
know, having these toolkits is just should be seen as a way to explore what's happening
in your model and data, right?
It shouldn't be seen as a check mark, right?
And so one thing that has happened with the proliferation of a lot of fairness related
to works, I believe, is this were, you know, debiasing X or fixing X or, you know, and
so that gives the illusion that you do X, Y and Z and then it's fixed, right?
And so inherently, this is a very complex problem.
It's context dependent, you know, it's domain knowledge dependent.
And so I think like sometimes I worry about how to get that across even when all of these
toolkits and documentations and things are around, right?
It shouldn't be seen.
It's just inherent tension between kind of democratization and raising the level of abstraction
and providing tools and, you know, what we call leaky abstractions, like the fundamental
complexity of the thing that you're trying to make more accessible.
Yeah.
And I would even say like, you know, I and Howard and many other people have done work
on like automation bias, right?
Like people trusting automated tools more and like more than more than, you know, people
are sometimes, right?
And like people do work in trust.
So, so if I give you a bunch of tools and you do a bunch of tests, like it shouldn't
be like, okay, I did test Y and Z and it's passed, right?
There's also like a framework that needs to be developed around how to use those tools
and at which point in the product development process they're appropriate, right?
It just shouldn't be seen as like this is like a smog check, X, Y and Z done past, you
know?
Right, right, right.
Yeah.
I'm curious what you, you know, if you had to kind of make some predictions about the
field kind of shifting our attention forward, what do you think we accomplished in 2020?
Or, you know, since we're talking about round numbers the next decade for that matter.
Oh, wow.
Oh, oh, I definitely can't do about the next decade.
I do, I do think we're probably moving towards a lot of discussions around standards such
as like the ones that, you know, model car status sheets and stuff.
And like how do those fit into, you know, governance of these kinds of like we've said like many
organizations have AI principles and things but like how are they enforced, right?
So, and also, but then again, like how do we ease the burden on people who don't have
such resources?
So I wrote a paper recently with Ansoon who's a history student and so we were talking
about like lessons that we can learn from from archival history and their data collection
processes and they have data consortiums where they pull resources together and if one
library doesn't have this kind of collection, another library can use it can, you know,
they can use another library's collection.
And so with this increased, you know, burden of documentation which in the end we believe
is necessary and checks and, you know, like GDPR and all this stuff, like how do smaller
institutions and nonprofits and those without much resources?
How can they also, you know, not be left behind, right?
Because I believe that's also a fairness question, right?
If you, if you are doing things such that more, you know, larger institutions than the ones
that benefit, that's, that's a fairness question because that constraints who gets access
to what resources?
And so I think I believe there were like conversations at the governance level around data consortium,
conversations around standards.
So I, I'm wondering if maybe within the next year we might start to see some organize it,
like maybe the EU or maybe, maybe some other organizations starting to think more about
governance and whether some of these things that we've worked on as researchers, like,
you know, data sheets and model cars and things like that will be part of that governance
structure.
You know, for example, when, when I, there's, oh, I forgot a big thing in the last year,
you know, there's been all sorts of legislature pass about around face recognition, right?
And that for me, it's pretty fast from like writing, gender shades to, like, this kind
of, and, and of course, many people like the, at the center for a security, what is it?
Privacy of security, a Georgetown law, you know, Laura Moig, Claire Garvey, Alverbertoia,
all these people, they, they do the most amazing work on like tracking the use of face recognition
or really automated facial analysis in the US by law enforcement, whether it's ICE, whether
it's, you know, other types of law enforcement, and they had the perpetual lineup report
with the first one, America Underwatch was the second one.
So in partnership, you know, in the conjunction with like a lot of people's works, I feel
like that has resulted in some amount of change in legislation within the last year,
right?
And I, I think we're going to start to see more of that.
Another thing I've seen actually recently is that so much more, so many more people in
civil, in civil society are part of this conversation now like the ACLU and many of, many other people
are part of this conversation about whether some sort of technology should exist or not
and like AI governance and things like that.
And I think that'll continue to grow.
One thing I'm really worried about and I, I am predicting this will happen because
it started to happen already is once again, the taking over of marginalized voices, right?
In this space.
So like I said, like people work really hard to make something a thing and then once
it's a thing, the people who didn't suffer the consequences of trying to make it a thing
kind of become the faces and the heads and the, you know, and the people who steer the
ship.
And at that point, it starts going in the wrong direction because it's not really trying
to address the issues of the people who took the risks to make it a thing because they
took that risk because what they really care about is addressing the issues themselves,
right?
And so that's one thing I think will happen increasingly.
And you said you're seeing that already, is there?
Oh yeah, absolutely.
I mean, it's hard to give specific, you know, but no, I, I, I will say like a lot of institutions
like, you know, for example, MIT, Stanford, Stanford, actually Stanford HAI was started
and went partly by my advisor right by PHA and the MIT, it's, there was a, a big announcement
saying like, there's going to be a school of computing and there's going to be a focus
on ethics and things like that, right?
But like, literally, if you can't even have like two black professors, like, and you know,
what I'm, that's what I'm talking about.
So I, I think that kind of, that's going to be very hard to tackle.
It's a surprising absence on the part of ATI.
Oh yeah, so, so H, I was actually also talking about MIT, but, but true, exit, exit.
So with HAI, it's true, right?
And I, I believe they're like, I, I did talk to my advisor about it too.
And like, I think now they're starting to be a little bit more cognizant about it.
But I mean, as an institution, you're going to have money from like different groups of
people and those groups of people are going to be, if you have the name and if you're not
really from that much of a marginalized community, you're going to be more likely to, like,
raise this money and stuff like that, right? Whereas like a lot of other groups of people
that are from marginalized communities and smaller institutions are going to struggle
to, to get credibility and, and, and raise money.
But like, yeah, what I was seeing was, you know, for example, at MIT, like, they're
now seeing this huge endowment and this, you know, ethics and all of, all of this stuff.
But when I look at their faculty, just in general, right, like in computer science or an
engineering, you know, like, they, they, I, I, they might have one black person.
I don't know, but you know what I mean?
So it's, it's really hard to, to think, I mean, at Stanford, I, I believe, I used to
think that there were zero black, they graduated zero black people with a computer science,
um, PhD, they've never graduated black people with a computer science, PhD.
I believe there's, there was maybe one person ever in San Francisco, that's graduated with
a PhD in computer science, right?
And like, you know, and so it's just impossible to have, you know, ethics things without addressing
these kinds of issues, right?
Like if you don't interact, if you're not, you know, there is something wrong in that
system that is not allowing these groups of people to thrive, right, um, and, and to even
be present.
So, so this distinction between quote unquote, ethics and like, um, in, in, in theory and
then in practice and the real world, um, um, I've seen more of it, um, in the last year,
but I've seen more of it being discussed in the last year, but I don't anticipate more
of it happening like in the future.
In the next year?
Yeah.
Yeah, there's, I don't know that I would necessarily say it's a shift in the conversation,
but there's definitely something happening in the conversation.
There's maybe a tension in the conversation about the extent to which AI ethics and fairness
and these conversations should be grounded in self-interest, grounded in protecting the
most, uh, you know, marginalized at risk, uh, you know, you know, who and why should this
be about?
Is, uh, is, are you seeing anything there?
Yeah.
And I'm seeing, I think, a bifurcation in the community and, uh, and that could happen in
the future, right?
So, there's the camp that says, and I'm, I'm probably in that camp that says, you know,
I just don't like this separation between, oh, yeah, like there's this theory that we're
doing that theoretical work, which is the math and the proofs and stuff.
And then there's like all this activism that's happening, you know, and like the diversity
and inclusion and labor organizing and, um, you know, that kind of stuff and like those
should be separate, right?
Like, and so if we are having a theory thing about fairness for, with 30 people and they're
none of them are black, that's okay because we're doing the theory people and I would just
completely disagree with that, right?
And I would say that's expletative because then that's exploiting a particular community
that you're talking about, that you're writing about in your papers, um, to, to kind of advance
your career and this is not helping the community at all, right?
And then there's a camp that says we have to be much more interdisciplinary.
We have to have less boundaries between disciplines and it's like, you know, diversity and inclusion
work and all of this labor organizing and stuff is just a part of this.
I'm very much in that camp and many people in my, in our team are in that camp and, and
I think that's why they gravitate to our team because our team is one of the few homes
where people who believe that and so like, you know, and so going back to where I started
because I came from Emily's talk, it's that idea of the view from nowhere.
So if you say that, you know, it's okay, we're only doing fairness related, um, theory
work and there's 30 white people here and that's fine.
And that's adhering, that is the view from nowhere.
That's assuming that the, that view of those 30 people is not affecting your three at,
a theory that you think is going to help this particular, correctness of the view that
these 30 people are seeking that is independent of the 30 people themselves.
Yeah.
So that's the assumption, right?
This view from nowhere assumption, which feminists have critiqued for a long time.
And so I'm in that camp.
Well, Tim, thanks so much for taking some time to catch us up on kind of your view of
the ethics and fairness landscape.
Certainly we could not do justice to this conversation and, you know, just under an hour, um, but
it's something that will continue to explore on the podcast and looking forward to our
next conversation with you.
Thank you for having me.
Thank you.
All right, everyone, that's our show for today for more information on today's guest
or for links to any of the materials mentioned, check out twimmelai.com slash rewind 19.
Be sure to leave us a five star rating and a glowing review after you hit that subscribe
button on your favorite podcast catcher.
Thanks so much for listening and catch you next time.
