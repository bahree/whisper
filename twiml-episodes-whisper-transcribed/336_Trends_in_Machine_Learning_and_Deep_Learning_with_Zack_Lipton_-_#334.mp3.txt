Hey everyone, hope you all had a wonderful holiday.
For the next few weeks we'll be running back the clock with our second annual AI Rewind
series.
Join by a few friends of the show.
We'll be reviewing the papers, tools, use cases, and other developments that made a splash
in 2019 in key fields like machine learning, deep learning, NLP, computer vision, reinforcement
learning, and ethical AI.
Be sure to follow along with the series at twomolai.com slash rewind 19.
As always, we'd love to hear your thoughts on this series, including anything we might
have missed.
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via
a comment on the show notes page you can find at twomolai.com.
Happy New Year.
Let's get into the show.
All right, everyone.
Welcome to Twomolai's AI Rewind 2019, where I check in with friends of the show to hear
from them on their favorite papers, perspective, thoughts, reflections on the year 2019 in their
area of work and research.
I've got the pleasure of speaking with Zach Lipton.
Zach is a jointly appointed professor in a Tepper School of Business and the machine
learning department at CMU, where he's also affiliated with the Heinz School of Public
Policy.
Zach was my guest in July, where we talked about fairwashing and the folly of ML solution
mechanism.
I encourage you to check out that show.
If you're not already familiar with Zach in his background, Zach, welcome back to the
Twomolai podcast.
Thanks for having me, Sam.
So I'm really looking forward to digging into some of the papers that you identified for
us to walk through.
But before we do that, I'd love to just start with your general sense of 2019 and what
it meant for you or what you think it means for us kind of for machine learning in general.
I think that we've been for the last few years kind of in this just mad dash, just sort
of just catch up with what's changed following, I think 2010 to 2012, disruption in terms
of the kind of like new set of tools that were made available by kind of line of successful
works that got deep learning actually working.
And so the metaphor I settled on at NURBS is it's sort of like the task of doing research
which is kind of like wandering around in the dark like swinging around for like a pinata.
And somebody smashed it open in 2012 and just suddenly became really easy to do research.
Like it was really easy to collect candy because there was a bunch of it sitting on the
ground.
And I think people have just been trying to pick all the low hanging for 2013.
You could get really smashing successful papers just by saying we played with a number
of layers in the neural network and the results to be gotten by doing that were sufficiently
compelling that you know it didn't matter how much sort of intellectual content there
was.
I think you know over the over the last few years it's gotten a little bit more refined
and it's taking a bit more work to make an interesting paper.
But I think right now more than any other point I think what you're starting to find is
people just kind of coming up against the limitations of the general paradigm of like we collected
the big data said you know we trained it on one partition we evaluated on the whole
doubts that we kind of saw how we did and you know you can come up with a whole bunch
of sophisticated tools to sort of either analyze that process or to try to sort of improve
its efficiency a little bit.
But there's a fundamental gap I think between sort of where people sort of got near dreams
about what they thought was technology would could go and sort of like what we actually
kind of do with it.
I think what you're really seeing now in 2019 is people starting to think stock of the
limitations of the current setup and the current paradigm and I think a lot of the creative
people starting to think more seriously about going beyond just doing supervised learning
and thinking really seriously whether it's about causal inference or robustness under
domain adaptation or starting to think more seriously about the economic aspects of if
you're deploying systems to make decisions then you're you're interacting in an environment
with other agents who are also going to update their behavior and response to whatever
it is that you changed and I think that this is kind of what's going on right now is
that the sort of like rocket fuel from deep learning and starting to in my eyes I think
it's starting to run out but I think the hope is that people are starting to get ambitious
again and starting to get ambitious not just about scale but sort of about the kinds of
problems that they pick on.
Yeah, when I think back on this time of year a couple of years ago post-Nerrips a lot
of people were talking about Ali Rahimi's kind of call for increased rigor in deep learning
strikes me that that's kind of somewhat related to what you're describing and what we're
seeing now but what I'm hearing you saying is less about kind of this shift to really
digging into trying to find rigor but more looking elsewhere incorporating other ideas
I'm hearing a ton about causality as well coming up in conversations do you know are those
related kind of themes do you think?
I think it's an important distinction to be made between doing science in a rigorous
fashion and what are the set of scientific questions that you're working on and I think
that this sort of big discussion that started after Ali's talk about sort of doing this
work more carefully is I think very important and I've participated in that discussion
a bit I think also like many people inspired by Ali's call to attention in 2017 but I
think that's a sort of separate question from this other question about the set of problems
that we start looking to like you could be very rigorous but still just focus entirely
on this sort of trained test mode of machine learning and that's a little bit different
from like you can become rigorous without leaving that world right you could just do just
there's enough there's enough questions to probably still keep a large number of researchers
working for a lifetime just about generalization of from one night samples to some some distribution
but there's a different question about doing work that addresses qualitatively sort of
broader sets of questions I think causality offers that right which is it's not just about
doing work rigorously it's sort of an all-concentration which is what what are the categories
of questions that you can answer and so you've identified some papers that were particularly
meaningful for you in 2019 let's jump into let's jump into those that where would you like
to start yeah so I try to include papers that or groups of papers that that sort of capture
multiple aspects of this paper's papers sort of doing the old thing and interesting why
paper is doing something new I guess to start off I thought two really interesting papers one
of them was 2018 I guess but we could bundle it with a more recent one so don't hold that
against me are these papers from Ben Wreck and Ludwig Schmidt and Becca Roelhoff's call do
see far 10 classifiers generalized see far 10 and do image net classifier generalized to image net
and in both cases these were basically like taking a hard look at this we we've had a whole
community that part of how we put so many people to use is that we've really embraced this leaderboard
in benchmark way of evaluating research or at least empirical research only been able to organize
a large community around doing this work because we had these objective benchmarks and they're
kind of asking this critical question of well after 10 million different researchers have trained
their models on the see far turning set and evaluated on the the same exactly far holdouts that
are we actually getting sort of like faithful sense of how these models generalize or
or are we just overfitting that particular holdouts that basically we tapped out the the
capacity of that holdouts that to tell us anything interesting about which bed models are better
than what's other ones and so they did in both cases they undertook this really incredibly
laborious effort to follow to the tee to the extent possible and you know in a good faith effort
to create new holdouts sets for both both data sets and to basically say okay it's uh
if instead of having 50,000 trained images and 10,000 holdout images if now suddenly in 2019
an additional independent sample you know 10,000 holdout images fell out of the sky how well
with all these models that have been basically all fit in the same kind of rat race of fitting
it gets single as a how well would they generalize to this sort of new fresh data
ostensibly from the same distribution and so a big part of the effort to went into these papers
it sounds like was curating this new data set that to the best of their ability they demonstrated
met you know felt were in line with the the distributions of the the popular data sets
right so so in both cases there isn't actually an additional 10,000 seafarer images sitting
around other aren't also an additional however many image net images sitting around so they
actually had to go and sort of read the papers and say exactly what procedures that they followed
to create this data set right so in the case of image net they first created some kind of ontology
of of nouns based on the word net hierarchy and once they had these categories they used them
to create Google image search these created set of candidate images which then were subjected
to a crowdsourcing protocol that they used to basically you know to do something like identify
several thousand images that were likely to be Persian cats but then you needed to show them
to a crowd worker and say is this a Persian cat and then like this was their way that we're
able to create the data sets so they tried to go through and this included you know creating
sets of candidate images going through the crowdsourcing pipelines it was really you know interesting
work I think sort of on a sort of qualitative sense it they're sort of taking a fresh look at
this data set construction process as they're going through trying to follow in the footsteps of
the original data set creators and they come up with a bunch of interesting observations like what
precisely is you know you get the weird questions that you wouldn't think would be so
vexing like what really is a basketball you know where you'll find some of these images that they
annotators disagree on it's because maybe it's not a real basketball maybe it's a toy basketball
like is it maybe it's a picture of basketball what precisely is going on there and and how
off the thing could be categorized but then the sort of key result right is that besides this
sort of really interesting aside about how the data set were collected some of what they believe
were in the paper is in the number in some really nice talk that the others have given a long
away an event right given I just talked about this work at the Simon's Institute last summer
is that then what they do is they evaluate basically our current leaderboard right to take all
these great models that are sort of the state of the art classifiers for these different data sets
that you know have some kind of ranking among them and they evaluate them all on the new the
new C bar test that right on the new image net test that the funny thing about it is that basically
the model is all performed so here's here's like the like don't get too excited or too angry or
whatever the first step is that the model is all performed of course right and because of that
there's some you know this is sort of like usual crowd on Twitter the guy could be critical on
Twitter but like like like the evidence say you know what it says and not you know the usual like
whether it was like see like I told you deep learning doesn't work and sort of interpreted it
that way like this is like a damning result about deep learning but actually this was a really
positive result and that's because the other side of the result was that it's not yes all the models
did worse but they were still arranged sort of in the same order so basically like the best model
was a certain amount better than you know the fifth best model and that was also true on the new
data right yeah so what it sort of showed was that like if you think of the leaderboard as being
that's like step of like incremental progress towards like ever better models that the models that
we thought were better because they did better on that C bar like the actual C bar holdout set
actually were better on the new C bar holdout on the fresh C bar data or on the fresh
image net data it's just that they were the entire benchmark like all the models in the benchmark
sort of across the board were a bit worse but in the same order so the better models really were
better and we kind of talk about this informally as the industry overfitting on image net the
industry overfitting on these kind of standardized data sets and that this paper is is demonstrating
is that perhaps that's true but at least there's still some order here right this stuff is really
complicated and I think the general problem is that basically we have one way of talking about
generalization normally and supervising which is from a finite sample to the the underlying
distribution and then when people started saying like well what about on some different distribution
does the generalization we start overloading the word generalize in different ways yeah and so
that actually that part of the story also comes in here which is why it's interesting the question
is then okay all the models are appearing the same order which means basically if nothing else
it means we didn't scorch the test set to the point that we got no value out of continuing to
do this leaderboard chasing right right but then the question is why did we do worse on the fresh
data and there's two possible explanations right one possible explanation is that well we sort of
are overfitting the image net cold outside but this is having sort of it somehow like an equal
effect on all the models that they all deteriorate by the same amount or something like that
but the other one which I believe the authors seem to believe is the most likely explanation
and we've done some experiments it suggests that it's plausible the other explanation is that
the reason why they do worse is because they're not actually see far data and they're not actually
image net data so basically like despite the authors very best effort and very best like
intention to in every way like you couldn't expect you couldn't hope for a better better behave
deployment scenario right like imagine that you trained on some data and then you go like this
is from like whatever your business is and you go to deployment time and basically at deployment time
what you do is you face a team of academic researchers an absolute best faith effort to in every
single way create data that statistically is identical to the data you saw during training
like you can never hope for this in a real world scenario right exactly of all the ways you can
go out of domain this is the most friendly right someone really trying hard and extremely
confident president even PhDs who want nothing more than to like make this a valid scientific
experiment do everything possible to make it so that this is i.e. like true holdout data and still
despite their best efforts the distribution of data is slightly different because it's
it's a few years later so maybe photographs look slightly different maybe compression algorithms
using cameras that then get logged in image search or whatever is slightly different somehow
you're getting you're getting new images in that that just are slightly different in distribution
and that's enough to make your classifiers say 7% worse 8% worse something like that if we get
the exact number yeah so that's having the interesting thing is that like there's all these
different notions of generalization here one is that it seems that they actually like the benchmarks
for whatever reason like this process with the only way that we're squeezing juice out of the
holding set is changing our neural network architectures and hyper parameters doesn't leak as
much information as in my fear the models do generalize as well the benchmark like leaderboard
chasing kind of works better than we have any right to hope it would and at the same time also we
sort of see how in the context of the experiments how brittle our classifiers are through distribution
jet so I think the fact that all this is going on in these papers and and I have a fondness for
this kind of paper that they're not introducing a new model they're not claiming a new leaderboard
result but they're asking is a scientific question and I think doing that precise kind of hard work
experiment to kind of produce that knowledge and that whole story kind of comes out of here
are there other papers that come to mind that took similar approaches this year there were
feet of different papers that have various different aspects I think sometimes this this is unique
and then it's sort of saying do these models generalize do the very same distributions that they
were trained on there are also a lot of papers that kind of look at various ways of saying basically
these models were trained on some data set that was meant to sort of capture some kind of real world
task you know like you had some kind of idea of the confidence we thought you were evaluating and
a lot of papers are more like these performance on this data set really indicating that confidence
like as as you might hope it would and there were a number of these over the last year and a half
or so so I had a student did young's project we had a paper at EMNLP last year that basically was
said something like how much reading does reading comprehension require and so we looked at these
reading comprehension data sets and we found that essentially in reading comprehension you're given
a question and a passage you have to produce an answer and so presumably producing the answer
correctly if it's really testing reading comprehension it should require that you've actually read
the question and should require that you've actually read the passage otherwise it's hard to
claim that the what the model is doing is question answer this passive based question answer
and right if it doesn't actually look at the passage or if you can't be sure that it actually
looked at the passage or if you can't actually be sure that it read the question we had a paper
basically just said hey how come nobody's run this baseline on all these data sets just training
the exact same models but looking only at the passage and not the question just get it based
on performance or just looking only at the question and looking at a randomized passage and so
when we did that it turned out that you can match a lot of the best results reported in the literature
either not looking at the question or not looking at the passage so it says something about
does that exactly the same but it's a similar spirit and it's sort of saying what you know asking
out a question about the trying to ask the sort of fundamental question about this data or another
example was there was a English polia at a paper that did a similar thing with natural language
inference this is a task where basically you have two sentences the similar kind of said you have
two sentences one is called a trimis and the other is called a hypothesis and these could be in
a relationship with each other which is either entailment contradiction or like a neutral posture
so it's a three-way classification problem and so the trick is to sort of read the two sentences
and to deduce like which one of the three best describes the relationship of these sentences
to each other and they basically found that if you just the way these data sets had been created
for that task you could often get the same performance as safety art models just by only looking
at the hypothesis and ignoring the premise so you know someone might have thought that what they
saw was this task of entailment but what they really did was made a sentence classifier
yeah they made a sentence classifier and it turned out there were some clues like
the hypothesis tended to have like negation words and it when it was a contradiction or something
like that something so this kind of gets that high-level thing that we're talking about of like
beyond supervised learning of hey from from a pure supervised learning standpoint what's wrong
with what the classifier is doing there's nothing wrong with it right it's getting good predictive
performance the problem is that what we want is something a little bit more than that we want
something that's going to perform well in other environments but we know we don't know how to
you know we're still very immature about how to incorporate that into our kind of learning setup
right so most of what people know how to do is to say here's represented a data fit a model and
and everything you know everything that we have a right to expect about the model is that it'll do
well on new data from the same distribution and now we reveal that actually what we really wanted
or what it really takes to do interesting things in the real world is is something that that other
notion of generalization to different data sets to different environments yeah speaking of
beyond supervised learning the next paper that you identified is the the birth paper technically
I think we first saw that one late 2018 but we certainly came to understand it a lot more in 2019
right so I think you know if we have our different pools of things going on people trying to get
beyond the current paradigm and sort of counterargument that there's a lot of juice yet to be squeezed
maybe births actually that latter category but it is basically the idea more you know the
the highest level idea is just basically semi-supervised learning right and and I want to semi-supervised
learning and machine learning right is the approach we say I have lots of unlabeled data a small
amount of labeled data how can I make magic out of that right how can I how can I basically the
the baseline would be I only have labeled data like ignore the unlabeled data it's just
trying to classify or using the labeled data right or the unlabeled data so what can I do with
that unlabeled data and deep learning gives you a nice kind of answer to it like with a lot of
things which is well use the unlabeled data to learn the representation the earliest forms of this
that I saw where I think around 2015 or so I think quickly in Andrew die had a paper that was doing
as I'm sure I bet your papers that predated that's one that I remember were the they were doing
stuff like just train a language model on a bunch data and then fine tune the language model to
make predictions on your downstream classification task now the models they were using weren't that big
they weren't using tens of TPUs they were using you know probably they were still probably I don't
even know if tenser flows out they might have written their code into the auto like I was at time
and and I don't think they were using a specially enormous data set or whatever but like you know
that key idea has been there for a while we've talked about you know training auto encoder is
fine tuning to you know supervised task here that the idea basically put if the next word
and then Elmo came out basically 2018 it's basically the same exact idea that there's a
slight wrinkles the wrinkles are on the details right like they say we train a forward
language model from left to right backwards language models from right to left you can catenate them
and then you take some mixture of their representations but the qualitative idea is
train a giant language model on more data play around with some some variants but that are really
not conceptually different they're just kind of you know different levers different offsiturn
and basically see what could do the best performance on the number of downstream tasks and
Elmo was this break your moment in that whether or not you found a conceptually interesting
every single like virtually every single NLB task experience some significant bump in accuracy
in those moments are not so common right where you just say oh there's a new state of the art
for every single task effective tomorrow and so Bert it was Elmo didn't have that much time
in the sun before the the mubbid meme took off and Bert which I think has sort of stayed now it's
been there've been enough variations on Bert but not enough that anybody's willing to you know
this is the same thing happens image of rights like there's been slight changes on resonance
or whatever but for the most part everyone still uses resonance now four years later yeah Bert I
think was this moment where you know they made a few more modifications one key modification was
that they use transformers instead of LSDMs like in Elmo another big change is that for their
modeling objective instead of saying that what they're going to do is like auto regress from left
to right and just try to predict the next word given all the previous ones they do that sort of fill
in the blank type of objective where they mask out certain words and then try to predict which words
were masked out but Bert gave us absolutely massive boots to sort of after after that had already
happened shortly before sort of did it again across the board and at this point you basically cannot
publish a paper in natural language processing without building on top of it you know the question
that then arises is sort of what is the sort of two perspectives one is to say if you want to do
interesting work in this field you have to go to Google and get a TPU farm and this is what you
have to do to to move the state of the art the other way to think about it is to say that that's no
longer the interesting part that the architecture is sort of something that someone will come out with
there's Bert on those Roberta someone will come out with yeah I think they've already come out with
every other method but you know right so someone will go out with something did you create a research
do you say that that is the creative research do you say hey that is the tool upon which anything
that I want to do that's just one part of what I do is this function setting and if I'm doing that
for natural language that this is the base model that I use and someone will come up with another
base model but that you know on one hand it takes a massive amount of resources to train Bert on
the other hand it takes very few resources comparatively to you to fine tune Bert to sundown
stream pass it's kind of opening this interesting wedge of life we're sort of now in this position
where sort of the only way you can you can do at least like leaderboard competitive and alp
work is the build on top of one of these models what's next up in your list of tapers
yeah so kind of continuing this theme of going beyond the standard just like I have some offline
data I say the model I evaluate how predictive it is on some holdout that right there's sort of
a number of things that we're interested in going beyond there one is under what settings can you
detect or adapt to distribution shift the other side is I think the social component which is
you know we're always talking about using machine learning and it came up when we were done
of fairness before right we're always when we talk about machine learning technically we just
use the language of prediction I have some data and make a prediction how accurate it isn't like
the notion of accuracy assumes like a fixed reference distribution right accuracy is a self is
is a is a probable statement what fraction of the times are we right assuming that there is some
objective background distribution that's generating our data but the reality is that you've
been look at what people say they're doing with machine learning like we want to build self-driving
cars we want to make your doctor in AI we want to do whatever all of these tasks involve not just
making predictions but actually driving some kind of decisions in some real world process in a lot
of those cases you know you can think of it like like Google search right what happens the moment you
change to Google search algorithm right the very instant you change to Google search algorithm like
the reddit message boards light up and people are going to start adapting they're going to start
chatting about strategies to take advantage of the new algorithm in some way or another right
and so basically you're making decisions and so our predictions are informing decisions we usually
kind of alive that step but once you turn a prediction into a policy and you deploy it and like a
real social setting where you're not the only agent the next thing that happens is everybody else
is going to start updating their behavior which fundamentally shifts the distribution
against what you're trying to make predictions right exactly like the same individual
who may or may not have the same but it's going to start changing their behavior right people
in aggregate are going to change their behavior which is this is going to somehow manifest in the
data that you see in the next round right I don't think there are necessarily cleanly separated
rounds but we could idealize it that way for you know like analytic purposes so there are
couple papers I think I think I think sort of two groups of people have been thinking really
seriously about these kinds of problems I just wanted to highlight their work is one is Lily
who who's a PhD student at Harvard I believe in like something crazy like I think she's like
joined philosophy and applied math but works with computer scientists something like this but anyway
I think she's done really fantastic work for a long time but specifically I've been looking
at problems with this flavor which is sort of like oh especially in a context of fairness which is
okay you're proposing that we make predictions in this particular way or the design classification
this way but then what happens in the next step right well we're always like missing the next part
of the picture is like okay so then then we make those decisions then how do people think their
behavior or how does this influence censorship or like the data that we see in the next round and
then you know basically what are we going to see next and another group of people that have worked
on this is more it's hard with and he's working on it for a long time I think even before he
joined faculty and now is a faculty member with his students Smith and Millie among others
and so they work you know with the student Lydia Lo that Lydia Leo they won the best paper
at ICML a year ago and this was about the delayed impacts of fair machine learning
and so the two papers that I thought you're just seeing which were both from this year at the
FastStar conference so this is the fairness accountability and transparency conference these
were sort of two contemporary works both called one is called by Lily who called the disparate
effects of strategic manipulation one by Smitha called Smith Millie called the social cost of
strategic classification but they're both addressing the setting more basically someone all the
individuals are characterized by some covariates then there's a firm that drives some kind of decisions
by fitting some kind of classifier which assigns people to predictions they've positive or negative
based on their covariates but then in the very next round basically individuals are going to they
have some power to sort of manipulate the value of their covariates so I can motivate an example
in Lily's paper that I think is a very good one is think about like school admissions
and people notice that maybe people who get better SAT scores tend to do you know in somehow
this tends to be predictive of success in college or whatever but if you then lean more heavily
on the SAT scores or you incorporate this as a more prominent category and how you make your decisions
if you do admit the very next thing that happens is that people who have some resources like
as much as the college board wants you to think that these things are not manipulable they are
and what people are going to do is go out and find a way to spend money to prepare for the test
and when they start preparing for the test the same individuals who you know
they give an individual by preparing really hard for the SATs doesn't necessarily become
fundamentally a better student but they do increase their test score right and so this is like the
the key idea is that you have someone making decisions but you have other people who in response
are able to sort of invest in manipulating their their features or their covariance to influence
their prediction so someone right the decision maker has to publish their model then other people
get to sort of respond to it strategically and then both of these papers they they look both
as dynamic but also in the context of fairness so so one way that they look at things is this
context of well what if not everyone has access to the same resources to manipulate their features
and I think that the SAT example is a really good example of that right where you have people
have spent really exorbitant sums on test prep I remember I had friends when I was in
ecolumbia there was some service I don't want to say what they are maybe they'll help my friend
who works with them you know there was some service they go to his fault so I'm like kind of like
posh like Manhattan test prep service the chart I think the tutors made I'm sure the tutors were
getting a small slice of the cut and they were getting like 100 200 bucks an hour something
absolutely ridiculous I didn't think the requirement was you had to have a perfect SAT score
like the tutors so that was like part of their limit but you know you have these these
services and people will really spend enormous amounts of money and then you rate you create
this dynamic database but people who have lots of resources to influence their decisions will do
so people who have less will you know can't really compete and kind of analyzing this dynamic of
sort of what happens depending on how you make your predictions and how people adjust strategically
and they also both came up with interesting scenarios where sort of often everyone could be made
worse off and like the institution making decisions becomes better off like in the setting of
strategic manipulation like a somehow like works against the this might have something to do
the fact that like you're sort of assuming like a monopolistic party on one end and like competitive
parties on the other but you can have these settings where basically lots of people are competing
with each other to manipulate their predictions and net overall like they're incurring some cost
for doing so and are made worse off and the institution on the other side sort of
is made strictly better off. I don't know that either and I think both papers are addressing a
fairly idealized setting and it's not clear that they're giving you a a tool in the algorithmic
sense that you could go out and sort of analyze it and you know directly say a lending decision
or school admissions or not that I would endorse you know making those decisions based on
machine learning classifier in the first place but I think they are giving and I think this is
true of a lot of this stuff as we start going sort of towards causality and for thinking about
economic mechanisms is there at least giving us I think a framework for thinking coherently
about the problem and I think the problem is you know when we don't do that work you wind up with
people pretending that these are just that the more or less we can just continue to think of these
things as prediction and that you know just like there's some kind of really simple trivial fix
that I think it's around us and I think these are you know nice steps towards thinking coherently
about these problems. Yeah it strikes me that part of the issue in the examples you described
is kind of the gap between the real world thing that you're trying to optimize and how you formulate
that as a machine learning problem does the the framework that they provide in their discussion
help provide additional tools or ways to think about that core problem formulation question.
I think what it gives us at least is a way of stepping back and appreciating at least one component
of yeah there's this thing that is always omitted which is like what is our data the what is the
data where does it come from and to some extent you know when the data represents individuals the
data is the data is subject to choices that they make and some of those choices are directed
specifically to to have them classified and treated it in a certain way yeah and I think just like
that very sort of realization starting to model it into starting to create sort of mathematical models
of this interaction as itself you know that the it's a conceptual tool you know again I don't
think it's a practical tool like I don't need to give you an algorithm is it oh just run this
on the bank and then it'll it'll tell you what is you know and in fact you need to know things
going into it right like you need to know what are the cost of manipulating different features
some things that might not be specified in the data but it's at least sort of starting to cast
these problems in a way that is sort of richer in captors these things that you know we're sort of
naïve like ignoring like if nothing else is about like what is this conceptual tool to do we see
all these people talking about like just using big data to do all kinds of whatever I have access
to someone's Facebook likes and this and that and I can say whatever I can make whatever kinds
of decisions based on that data this is giving you a reason not to right you know if you start
thinking it if only in just like the very consideration of these interaction dynamics because
well why should you use Facebook likes well because if somebody knows that their career and all
these other sort of choices that are really consequential to them are being driven based on
these like really easily manipulable features they're going to start changing that behavior right
and so there's multiple sort of aspects of this right one is just the very consideration of strategic
behavior and then the other side is a sort of fairness implications when you're making decisions
about people and then you'll have different groups of people with different abilities sort of
disparate ability can manipulate yeah right you know it's like a new craze of joy somewhere you
know maybe just rich people get whatever classification they want or something like that
okay cool the next paper on your list kind of returns to this theme of generalization
and machine learning it's the invariant risk minimization paper tell us why that paper made your
list yeah so this is a paper that I don't know if they've hit the exact solution to these kind
of family problems but I think was a really solid and interesting attempt that I think got a lot
of people thinking and basically you know they're they're just sort of a connection between prediction
out of domain between causality and some of this is sort of well known but one aspect that sort of
hasn't been folded into it is is representation learning so so the way that causality is like
the prediction out of domain is that you say well my source data came from some distribution my
target data came from a different distribution what changed presumably in some ways that these two
different distributions are related to each other right otherwise like why should I expect that I
could apply a class by returning on one to the other causality gives you one way of thinking about it
like there's you know there was an intervention of some sort something you know one one variable
that used to take its data in some organic way has now been intervened upon and so but otherwise
the process is the same but this paper starts getting at is this idea of well thinking about
predicting well on a range of environments in terms of the representation that you learn which is
the idea that came up in some earlier more a classical domain adaptation work like from Shibon David
but it's developed here and some some novel series developed in the context of classical models
but then also some interesting experiments in the context of deep models but the core nugget
here is this idea that what is the ideal representation that you should learn and the intuition the
thing that they roll with is this idea that a good representation is one such that the optimal predictor
built on top of that representation should be the same for all environments so if you've got a
bunch of different environments or a family of different environments you want to learn a representation
such that the predictor that you would then fit on top of that representation with the sort of
environment diagnostic and what is environment in this context is it the the the world that generates
the distribution of your input and targets that's right so this is for example like I might have
to see you had this example of yeah classical example of like if I see fish they're always
against the water in the background and I see something else that's always against certain
back on these things are correlated but they're also you know exist I could I could also create a
data set where I'd have like the fish is not against is you know that you can try for example
but you need to get the idea where that correlation would be broken yeah right and the reason
why is because it is not you know like the the key fundamental difference you know there there
exists some environment where it's maybe even like anti-correlated or something like that so
that even though like if you if you're just being supervised learning you can be very well by just
using the the water background to produce the fish like in the worst case in the environments where
all the fish are on land and all the you know the zebras are in the ocean or something then you're
going to do really badly right right and so this paper kind of simply is saying we want a classifier
that can perform equally well on fish whether there's water in the background or they're in an aquarium
or they're mounted on a wall or whatever yeah I think if this is you know we rounded off a lot of
the mathematical detail but I think we are you're getting at the ideas and in the key ideas like the
deal representation learning that you should produce this representation such that the predictor
on top of that is the same across all environments then there's these questions of well how many
environments do I need to see and what kind of like under what like conditions have I actually
covered adequately you know all the environments that I would like reasonably anticipate and in this case
they sort of have a in the case of linear models have like a very sort of well developed and
kind of involved theory but sort of the questions wide open about when this makes sense and how
many environments we would need to have represented in our sort of training system in their set like
your training set is you have multiple distinct environments and from each environment you have a
data set they could say something that if if each of these environments is related to each other and
some kind of complicated way then you could say that you know for fitting linear models that that
we you know we need however so many environments in order to find this in varying predictor
but things get a little bit more complicated when we start stepping towards you know I think the
kinds of data and kinds of models that we want to work with which are in general not linear
which is why we care about the representation learning at least in context of deep learning
and so is the the contribution of this paper kind of a theoretical framework for these
invariant risk minimization models and you know how to know if you have one or is it something
more concrete where I can go create an invariant risk minimization model given the tools of the
paper the the contribution right is I think there's this principle for how do you create a predictor
there is a set of simple environments where you could argue that in a more like theoretical
way that you're doing the right thing and then a set of empirical experiments and I believe
doing empirical experiments are still very toy it's something like I think they look at something
like like image and ad s phn images things like this us or not in a genesis image I'm like
amnest where where you have some kind of distractor like there's colors is is associated with
the category but this is not necessarily going to you know the degree to which color is associated
but the category is changing and obviously you know the correlation the hope is that you know
you're going to produce a model that depends less on color and then it's going to do better you know
in the test environments where you know those the the the correspondence between color and
category are that you've come to sort of rely on don't actually stand up unhauled out data okay
so maybe you maybe case on price on the data sets where that is a predictive signal for not
relying on it but you produce backup by producing a model that doesn't rely on color then
if you go find a world where you know the correspondence between color and images is
completely flat then you know your model will be robust something like that yeah there's
another interesting feature of this paper that you pointed out that you don't see a lot of
in paper there's a bit of a secratic dialogue in the back of the paper yeah you know I think I'm
just going to have to lead that to the to the listener to read through this there's form their own
opinions it's certainly different you're not it's not the typical section six in a technical
machine learning paper uh-huh uh-huh I think they maybe wanted to keep with the theme of uh
Greek since there's a lot of Greek symbols earlier in the paper yeah you know the dialogue does
jump the shark at that but it's interesting that throughout the paper you know the authors and I
think this is just they they are they are thinking seriously about these problems about causality
and invariance and are in dialogue a little bit more than maybe you usually encounter initially
in learning with the um philosophy of science which is not to say that you would expect to find a
you know a forpage long secratic dialogue in a modern work on philosophy of science either
but you know that there is uh the treatment of of the kind of like core philosophical questions is
I think sincere and serious and and that the authors are unusually broad in their reading which I think
you know in general is really nice to see mm-hmm so the next paper on your list is one that I was
excited that you included and it's one that I've seen pop up quite a bit of late it's the your
classifier is secretly an energy-based model and you should treat it like one paper uh what's that
one about yeah so this is a paper um that I just came across because uh someone I know um left
to comment and pointed me to it but this this paper was I just accepted with an oral presentation
at ICLR and the key idea is basically so an energy-based model basically it is a model that
assigns scores associated with each input and these scores correspond like unnormalized probability
densities and you know there there's a literature on discriminative models where people try to
produce classifiers that do really good at assigning conditional probabilities of predicting labels
given some input uh there's a corresponding literature of people taking on you have to be very
careful with the word generative modeling because there's a set of tasks associated with generative
modeling that I think most of what we're doing almost what we're calling generative modeling in
the context of GANS not really addressing but using it lightly in that sense of like trying to
learn explicitly or implicitly uh you know uh a probability density over over your inputs over
over your x over your images there's this other literature that does that right including
variational auto encoders including GANS and there have been plenty of approaches on that side
that have adopted this sort of energy-based approach to incorporating it either into the GANS
framework or whatever but sort of there's always this kind of tension that sort of it seems that
like when you train a generative model you know a generative models are capable of performing
prediction but it seems like you never do as well as when you just train a proper discriminative
model itself right if you want to do prediction you know sort of like the theme of like if you
want to do prediction the takeaway so far is like you're better off straight up training a
discriminative model not training a generative model and then trying to trying to run it in some
kind of manner to to generate predictions and with this this model this we says that it sort of
just says hey let's just take a discriminative model like a standard image classifier and you can
sort of interpret it you're keeping the architecture and everything as you would for just uh
sort of off the shelf the discriminative model you are changing uh the learning objective and how
you sample from it but the key idea here is to sort of say that you feed an input to a discriminative
model you can get out some logits then you run the softmax function it turns you logits into
predictive probabilities and you also have like an extra degree of freedom there which is that
if you were to like move all your logits up by some fixed amount and move it down by some fixed
amount you would get out the same softmax probabilities and that's just because sort of um
all of your probabilities need to sum up the one so uh there's actually a degree of freedom there
so it says hey we'll just keep the entire architecture we'll keep all the parameters we keep
everything the same degree of discriminative model but what we're going to do is we're going to
basically interpret sort of pre softmax logits as sort of unnormalized probability densities
like probability of x and y and then they're going to train in uh you know the manner of an energy
based model sort of doing a train with like two objectives one is to just maximize the classification
performance using like the standard cross entropy loss and they're also going to train using
this like energy type objective to make it so that basically the sum of the pre softmax logits
has like a sort of high energy for in distribution data and hopefully you know low energy for
out of distribution data and what what it ends up being really interesting about this is that
in this tension between discriminative and generative models you find basically like usually
if you train a generative model you do good at the things that you want the generative model do
you do bad at things uh you do bad at prediction even when you try to you know leverage your generative
model in the predictions and vice versa when there's ways of using it the discriminative models
generative model they're usually not as good as if you just did uh you know use them more familiar
I mean I'm using the word generative model here but like gans or something yeah yeah and so the end
result here which you know if this stands up I think it's super exciting is that they basically
have a single model that they have trained and this model ends up being good at classification
it also um basically you can now sample from it by basically doing something uh calls the
caustic gradient the launch of end dynamics and so you basically you have some initialization
at the input and you take uh gradient steps over the energy function with some amount of noise
injected and this is sort of equivalent to like sort of sampling if you run it for a long time
and so there's other people out there that are much better experts than I am about uh
London dynamics but the the interesting thing is that if you sample from this thing the samples
that come out end up doing better or like as well are better on the generative model type
objectives as a lot of generative models out there the model does you know still does really good
as a discriminative model and then it turns out that the uh probabilities that you get out of this
model tend to be uh pretty well calibrated whereas in general a neural network probabilities
that you know conditional probabilities assigned to class is tend to be way overconfident
the predicted probability of like you know whichever class you know is the art max of the
salt max tends to be overconfident and then this model also turns out that these these scores
is unnormalized densities that basically give higher probability to in some you know things that
you know you think are high density data and lower lower unnormalized probabilities things that are
you know low density data turn out to be good on a range of empirical benchmarks for detecting
shifts like out of distribution and then finally it seems that the models that they get out of it
also do a strangely good job as evaluated against they do slightly worse than the state of the art
for adversarial robustness and so it's a kind of interesting and exciting here is hey this is
the model that's doing well for discriminative modeling it's getting generative performance that
is similar to uh GANS at least by determined by I think like Frichet inception distance or one
of those or inception similarity one of those you know metrics that they use it's doing well
in calibration which I'm a little bit fuzzier on why that should be on in whether that
that there's any principle that this classifier should be calibrated but perhaps interesting that
it tends to be at least empirically on the data sets that they've looked at it's getting adversarial
robustness and it's useful for out of distribution shift detection so there is something kind
of interesting here and it'll be really interesting to see over the next few months do
does it turn out that basically you know that there's a long history of someone sort of
coming up with a new kind of model that appears to be adversarial robust but that's just because
there's some you know as of yet there's some smart way to attack it that they hadn't considered
because they're proposing the new model we hadn't set the adversaries against it yet
right the adversary is being the graduates right exactly so yeah so I think that you know there
is something interesting here and I think that it also captains the maybe a broader current
in the literature so there's these other papers by Alex Majrey's group at MIT where they've shown
things that adversarial robust models have all kinds of properties that you would normally associate
with GANS or like you know basically models performing this if we don't want to use the
abuse or generative any more because that synthesis is high pass right and so you could do in
sailing with an adversarial robust model or you could generate images or when you do a targeted
adversarial attack that turn an image from one class to another so basically if you take a picture
of a kitten and you try to turn it into a picture of a banana a picture that's classified as a
banana right exactly if you do this with a vanilla classifier what you get is a picture that looks
like a slightly noisy kitten that's misclassified as a banana would you do with the adversarial robust
model you update the pixels of a kitten to look like banana so instead it looks to the classifier
like a banana the weird thing is that it actually makes it look like a banana so like the weird
thing is it's sort of of all the things that could have made it look like it could have made it
just look like something so bizarre that it was not an image in which case like we don't have any
right to expect the model classified as a banana or as a not banana right it could have just made
it look like total garbage and that would sort of it's not clear why the adversarial objective
would have a problem with that as long as it's sufficiently different from the input image right
along it doesn't look like a cat the weird thing is that it ends up making it actually look visually
like a kitten and so there's already this evidence that like some of these things that we maybe
didn't have any good reason to believe a related like robustness and this sort of like image synthesis
capability are actually related to each other and in a way that still right now I think it's
floating around at the level of intuition but it'll be interesting to see if we can kind of like
unify why these tasks are related to each other vis-a-vis you know maybe the dynamics of neural
network training all right cool so you've also got a couple of papers or things that your group
has been working on since we last spoke what do you have going there so one paper that I'm really
excited about just got accepted at ICLR 2020 congrats thanks um so this is work with my student
Vivian Kaushik and basically the idea we talked a little bit earlier about this concern and natural
language processing about the models you know it's okay made it's accurate but is it is it really
like learning the right correlations right and the problem is that you it's not so well defined
what makes a correlation like like you know these people starting to say like is it picking is
a really learning the past or is it learning superficial correlations or bad correlations or
spurious ones the problem is those words don't have like a clear formal meeting within statistical
learning like if you've got two sets of features and they're both associated with the output what
is the general principle according to which you should your model should be relying on one first
the other right and so we kind of cast this problem of within sort of a causal framework you know
we don't use the mathematical machinery of causality but we use this sort of thinking that the
relationship here is vis-a-vis intervention so the reason we give you a clear example to make
this like real so we take a movie reviews and if you're trying to classify our movie reviews
you find that the classifier puts a lot of weight you know just a linear classifier on bad words
it puts a lot of weight positive weight on words like fantastic excellent but it also puts positive
weight on words like romance which is a genre not really a sentiment and then if you it puts
negative weight on words like terrible but also on the word horror which is again the genre like
why can't you have a great horror movie or why why should the fact that it's a horror movie
matter and the reason why is because for whatever other reason perhaps like the confounder is
something like low budget that like Hollywood romance movies tend to be better received tend to
have higher reviews and that horror movies tend to be lower rate right yep so you think like why
shouldn't why shouldn't that be the case why shouldn't the model put weight on those and one
reason why is to say it's the formalizing these like through the the concept of an intervention
that if I took a movie and I changed the genre keeping everything else the same it shouldn't
change the sort of applicable label to that movie review whether it's positive or not right even
if it's correlated vis-a-vis this like sort of confounding that actually intervening upon it
should make a difference so the key idea we had here was to use is that what is real the you know
the the actual like substantial the causal connection verse what is not might not even be identifiable
from the observational data alone so the key idea is to use humans in the loop to supply that
information and so what we do is we kind of flip the crowdsourcing paradigm so instead of saying
here's a review and here's a label or so here's a review give me the label instead what we do is we
say here's a review and here's the actual label associated with it like positive or negative
then below that review we have the same exact review like pre-populating an editable text box
and then to the right of it we have a counterfactual label which is like the opposite you know like
this was a positive review now make it negative it was a negative review now make it positive
and so their goal is to edit the review such that it accords with a counterfactual label right
so then what we're basically told to say editor review said it accords with a kind of actual label
basically leave it in a in a internally consistent state so don't just like edit one sentence you
basically have to edit it so that it's fully coherent yeah however three don't make
gratuitous changes so basically don't change any facts that don't need to change to flip the
applicability of the label and so that's what's happening is that we now for every single original
image every single original review that was negative we have a mirror image review that's positive
for every original review positive we have a mirror image review that's negative it's like you
know evil twin yeah and in these ones what's happened is that you know in that original
data set horror occurred in all these negative reviews but in the new data set those are all
positive reviews but they all still have the genre horror and that's because the human knows that
horror is not the thing that needs to be flitted to change the applicability of labels so they
don't intervene on it so the humans are actually like communicating some information to us that
wasn't even necessarily in that sort of an argument we make it's not clear that information is
even identifiable from the original data set alone but the humans by virtue of not intervening
on that data has sort of revealed to us that structure and so we see interesting things like if you
basically if you train on the original data and evaluate on the holdouts that from the counter
factually revised data you go from 90% accuracy to like 55% accuracy and vice versa train on
the revised data you get like 90% accuracy but evaluate it on the original data and you go down
like 55% accuracy if you train on both data sets together so we call the counter factually
augmented data you end up with a model that gets like 88% accuracy on both which is pretty good
it basically means you know you pay a small price for not relying on these sort of like spurious
correlates but surprisingly small you know you're getting like just you're just paying a couple
percentage points in accuracy and you're basically getting good performance on both the original
data and the counter factually augmented data so the name of the paper is learning the difference
that makes a difference with counter factually augmented data and I think that this sort of you
know leads us towards what I'm excited about is that I think the conversation about these data set
artifacts, spuriousness, whatever has been a little bit derailed by a sort of failure to recognize
that one this this this is not just that sort of IIDML problem it's asking us about something
beyond predicted accuracy and perhaps even beyond what's identified in the in the observational data
but by soliciting this this intervened upon data we're we're actually able to tease apart
and so we actually see some interesting things so we can go empirically places that we can't go
theoretically and one thing that we see is that if you train the model on the counter factually
augmented data not only do you do well on both but all those words that you thought should not be
related like horror romance both live some like a lot of these words that just like why is that
associated with sentiment yeah they actually fall out of the model so they they've ceased to become
like high high co-efficient features and like the linear model but then we see for both linear models
LSTMs birth models if we train on the counter factually augmented data we also do better out of
domain so we go outside movie reviews and we train those models and we run the value of those
models on on a different sentiment task like Yelp restaurant reviews or something like that
that are not movies and what is romance or horror mean in the context of a restaurant review
it's not clear right but those the models that were trained on the counter factually augmented data
empirically generalize significantly better not you know in the iid notion of generalization
but in that across domain sense that we're talking about earlier right so it gives us some
suggestion that there you know there's something substantial here and you know I think the next big
leap is to sort of figure out how can we just like on one hand I think we've learned something
really interesting conceptually and we've created a data set and resource that we've released
in the public that hopefully other people can do interesting things with on the other hand is
an extremely laborious process so so the huge part of the work that makes possible was my student
had to basically single handedly wrangle a workforce of like 800 crowd workers in order to
and then actually manually inspect them to make sure that all of these revisions were coherent and
made sense and that people were actually doing the past we were asking a lot more from them and
just like check the right radio button we're saying revise a paragraph of prose which is usually
hard with crowd crowdsource crowd workers right you know but I think we learned a lot of things one
is that I think you get a lot of benefit from really actively monitoring the process you get a
lot of benefit from paying them well and that there's a huge like really significant factor that's
determined by the quality of the instructions that you give them so we we kind of recognize over
that and I think getting the instructions right and reaching them crystal clear was a big factor that
made this the results better than if we had given them some hard work of ours instructions all right
we're getting short on time so before we move on to your predictions you also mentioned another
paper that you're excited about can you give us a quick summary of the fair ML from a non ideal
perspective paper sure so this is and I'm going to ruin his name in a horrible way such that
you know I'll never talk to you again but this is work with a postdoc student is got a Persian
surname Sina Fuzzleport so sorry Sina for your name but this is work that we've done together
that basically we've been looking at a lot of the work in fair machine learning both from the
sort of physical approach and the causal approach and coming up against a certain set of frustrations
and the main set of frustrations is sort of that and we this is sort of what we talked about last
time is that what what they're sort of missing is the right set of ingredients to even make a
determination about what is the just thing to do that you could very easily for a lot of these
things construct two different scenarios where you could describe them both in terms of you know
choose your covariance here's your label here's your prediction and here's a sensitive feature
and you know what seems like the Justin to do is very different you know if we're say on
hiring and like one case is why do we think that there's something different about the way you
should approach incorporating demographic in the hiring when you're considering say whites and
Asian applicants versus white and black applicants in the United States and the reason why is because
that there's a very different question of justice is because in one of them the current situation
is a product of very well documented very well understood history of discrimination in justice
and the other case you have a sort of over representation that sort of occurs not because of
discrimination but maybe in spite of and so that absent this coherent causal story of the data
that you're addressing and also maybe a coherent causal story of what is the impact of a proposed
policy intervention that you know you don't really have the right ingredients to offer someone
a off-the-shelf solution to say oh this is the way to be fair to be just so basically in this
context of formulating these this sort of argument about so we can't just sort of hand people
tools and tell them that they are these like oh like plug your classifier into this and it'll
magically become fair that it doesn't have the right set of inputs scene I made a really nice
connection to a lot of the literature that arose in the context of political philosophy
in the context of segregation and integration and there it was actually were especially influenced
by a work called the imperative of integration by a political philosopher named Elizabeth Anderson
and you see throughout this book she makes this distinction really clear between the ideal and
the non-ideal approach in philosophy and basically the ideal approach sort of asserts what does the
like in the most naive form in the ideal approach you sort like what does the perfect world look like
and you sort of identify some discrepancies between the world you live in and the ideal approach
and this is your justification and so historically you know a sort of naive of
application of ideal approach might be something like saying that we must have a colorblind policy
for everything because in the perfect world everyone would be colorblind and the non-ideal approach
is more concerned with well what are we don't live in a perfect world so what is the landscape
of injustices and and basically what you know why are we in a situation where in the who are the
agents what the moral responsibility to do something about it and then what are the strategies
that are actually going to be effective given the dynamics of the kind of messed up world that we
live in and so you know in the context of like say the naive ideal approach for a raceblind policy
the non-ideal approach would say something like well given we don't live in that ideal world so
given that people have whether or not we even believe that race is real given that on the count
of perceived race people have been discriminated against in their these mechanisms in place
what then is sort of your responsibility to act and so the I think that you know sort of maybe
argument against this casting of like ideal approach is that it's almost like so
uh it's also naive that like you'd say well maybe no one actually believes it so literally and
that actually you know in some ways it's you know besides maybe like someone reaching for like
us like a like a naive argument against affirmative action or something like that but actually in a
lot of ways it's actually precisely in uh the fair machine learning approaches that we're seeing
precisely like this kind of you know like like each of these parodies is something that would hold
in our ideal world where um whatever is the demographic didn't matter and the proposed approach
you just latch on to one of them and then minimize the disparity. So is the idea is is this
principally a critique of the fair ML kind of the state of the the world in fair ML or
is are you more trying to point to tools and that have been discovered or presented in the
political philosophy world and kind of getting to non-ideal approaches and drawing analogies for
fair ML. Yeah so I wouldn't say so much that the purpose of the paper is to offer a critique. I
think there are some sort of well-known problems with any of these individual parity metrics but
I think that's really more to one maybe present something of a more unifying view and maybe
most importantly just to make a connection to a large existing body of work that I think a lot of
people there are people in the space are aware of but I think a lot of people in the space are
not and I think this is something that maybe you know by by by stepping back from the like
fair ML kind of I've utilized classification context and looking at actually cases that have been
probed you know really much more critically and I think that like cases like segregation
integration offer a sort of policy context where people have really thought deeply about the
problem and by reading a book like I think Anderson's imperative of integration you see sort of what
what is the actual work that goes into making a coherent policy argument the fact that you know
that this includes among other things a sort of an account for the disparity a sort of like
normative framework that tells you who's responsible for intervening which is not necessarily
congruent with a set of people that are responsible for the problem in the first place
that actually is a pragmatic one that takes into account the best evidence for for what the
actual impact of proposed interventions would be and is it you know are they actually not just
beneficial in a sense of like the entries to your confusion matrix then you're like idealized
view of the world but actually do they sort of lead to the kind of social change that we sort of
are are seeking I think that hopefully by making this connection between this sort of previous
body of work and philosophy and what's happening right now in Fair ML I think if anything maybe we
allow the hope is made to give people an opportunity that sort of learn from sort of people's
past mistakes rather than having to make them all ourselves got it got it so maybe let's switch gears
and talk a little bit about your predictions for 2020 or if you dare the next decade since we're
entering a new one number one prediction and I think this is already well underway so maybe
it's a conservative prediction but I think that what we're seeing and we'll continue to see and
become more obvious is is a kind of great commodification that the technology probably talk about like
exponential progress and technology so I think actually in a lot of ways certain aspects of ML
progress have started to stagnate a little bit because that's how things go we don't we don't just
blow up exponentially we we make a little progress we have a little bit of a rupture and then things
slow down again and tell the next big idea where things are growing it sort of horizontally is that
I think we used to be you went to nerfs or ICML and you saw the same four companies that were
basically massively represented it which is you know Google Microsoft Facebook you know some Amazon
some Apple maybe and now what you're starting to see is a lot of like boring companies
showing up the nerfs and sending you know and I say it's not as like a put down but more like this
isn't like just like sole province of the extremely like tech elite but that this technology I think
is going to start to become boring technology in the way that like any successful technology does
right so I think there was a time when you built the first object oriented program you were doing
something riveting that was you know academic kind of happening and there's a time now where
basically every single person building a website is building an object oriented program and I think
that we're seeing a lot more of this of sort of banks government just sort of all throughout the
economy that there's sort of a larger sort of base of people that are sort of participating in
these technologies and these jobs and it's becoming much more widely diffused at the same time I think
you know a lot of this there was this moment right where if you could train a neural network you
had a route to possibly like a 400k year job or something absolutely ludicrous you know that
a number of companies and I think that now at the very top you know I think there's not as many
people that could could run a research lab that could you know I think they're still there's
still parts of the economy that are as hot or hotter than ever but I think that a lot of this
just kind of can train neural network to do X is getting commodified and it's just going to you
know the there's a lot more people could do it and a lot more companies will be doing it and I
think it's going to start becoming just more kind of commonplace and obvious I think at the same
time hand in hand with that and this ties into the maybe like theme of the conversation is that I
think that that's sort of like that's the direction I think for this established sort of just
predictive models as on one hand yeah get boring become widely diffused at the same time maybe
a little bit of a stagnation on progress on the other hand I think a lot of the creative people
are going to be pushing more and more sort of beyond the limits of just this sort of train test
prediction and I think that'll you know one direction is people thinking more seriously about
generative models I think one direction is people thinking much more seriously about causality
people trying to come up with more expansive or ambitious ideas about robustness so we've been
sort of my authentically fixated on this idea of perturbations within the L2 ball or within the
L infinity ball like for adversarial examples but starting to get a bit more ambitious with
kinds of invariances and kinds of robustness that we want to build into models and I think
hand in hand with causality and the papers we talked about earlier I think is also starting to
ask research questions that situate these models in the context of the like wider decision-making
process that they're actually part of so I think this kind of integration of machine learning
and economics is sort of going to be an exciting area and I think that's really if I have to look
you know over the next five ten years what I think is going to blossom I think that is that sort of
I can't say that I see completely the technical router that I have all the right tools to make it
blossom but I think in terms of like what needs to in order for this technology actually to be
deployed in the kinds of ways that we imagine it should be that's the kind of research that needs
to start happening and so I think we're going to start seeing the field kind of setting it
sides a little bit a little bit wider yeah do you have a sense for you you mentioned not being
able to see clearly what the technical pieces of that but do you have any kind of sense for what
that needs to look like certainly some of the things we've talked about in the context of fairness
some of these fat star papers feedback loop papers are in this vein is there kind of a broader way
to characterize what happens when these two fields start to you know collide more frequently
I think what really needs to happen is that where I think where we're sort of in trouble right now
is that we have the pure predictive modeling world which is sort of conceptually involved
rich but is able to deal with really rich real world better so if we pretend all we care about
is prediction then we're like limited to a very kind of you know really flat set of conceptual
questions we can ask but we're able to address them concerning really rich spaces of interesting
data then on the other hand I think we have a much richer conceptual worlds that are offered by
the language of causality the language of the economic modeling that get in towards a much more
you know deeper and critical consideration of these multi-agent environments or or even just
you know the causal structure of the world that are really allow us to frame like philosophically
coherent questions that are much more expansive than what we could say and to sort of supervise
learning business as usual but the downside there is that we don't have tools that we can take
to real data so it's like do we want the sort of impoverished tools that we could really take the
data or do we want the really rich tools that we can just use to run thought experiments or
you know even toy data experiments and and I think bridging this gap and if I you know if I
had all the answers I certainly wouldn't be telling you you know I'd be
be writing this family the archive of everyone else yeah no but I don't I don't know I don't
want to pretend to to know what that looks like I don't think it's sufficiently like respectful
to the difficulty of the task but that's that's that's what I try to look for right now
it one of the things that I try to do coming back from noreps and this time of year is try to
identify a few kind of key ideas or thoughts that were notable out of that event but also kind of
broadly you know gaining traction over the year and a couple that come to mind for me this year
were causality and generative models and they certainly came up quite a bit in our conversation
today you know do you think similarly in terms of those those particular do you have others
you know if if those are at the top of your list like why do you think that is the case now
well I don't know about you know I think those those two are a bit different like I mean there's
other contexts amongst people talking about generative models that that aren't you know the
the sort of graphical models we talked about in the context of causality of those those are
our generative models but I think the reason why they're pressing now is just because we're
actually using this technology right so it's like we sort of have a technology that addresses
a narrow set of concerns that it produces sort of like artifacts that are enough to get us excited
enough to get us excited about deploying the technology but not enough to actually really
address the needs of like the deployment environment if that makes sense we we basically like we
were running this stuff in the lab forever we have these tools that that do well at this sort of like
guess the answers on the test set and now we're deploying tools based on it we're not actually like
ready for prime time in terms of being able to address address the needs of those real world
deployment environments I think what's happening is people are starting to go to those stakeholders
this starting to come up against the limits of like what's wrong like why why it's insufficient to
use neural network like like why hold out test performance isn't enough to make decisions in a
medical decision-making scenario like why it's not enough to make clinical decisions just to have
good prediction accuracy and so I think the more people start using this technology like those
issues have to come up because they were there in the first place we just weren't thinking
seriously about it but we sort of like put ourselves in this bind like I think like that that's I
think the drivers that we suddenly are I think it's some amounts of sobriety as we start coming up
again sort of like failure after failure I think same for just like in the problem robustness
out of distribution and causality are quite quite related to each other and I think a lot of the
conditions under which we try to ensure robustness correspond to causal stories like we talked about
before but I think you know when we start many people you look at what are people spending billions
of dollars on in a machine learning space one is you know medical decision-making one is self-driving
cars right so if you're building self-driving cars and you have no assurance that given training
on the enormous statistic like through 2019 or something like that you have no assurance that
they're gonna not crash in 2020 and that any you know small things like Mazda comes out with a new
paint job that your cars are gonna start killing people that's you know obviously a problem so I
think we've already sort of you know I think I think the reason why they're coming out is to be
already like sort of sign the contracts like we've already like hitched our reputations towards
delivering these products and and we're discovering a little bit too like that that these other
sort of things that are that you know these other competencies are that that our machinery
doesn't provide or actually necessary to do the things we told people we would deliver for them
I think like the real driver is just coming up against nature also awesome well Zach all good
things must come to an end so it goes for 2019 as well as this wonderful conversation reflecting
on 2019 thanks so much for taking the time to chat with us your perspective on these papers and
the field in general really appreciate it thanks for having me Sam awesome thank you all right
everyone that's our show for today for more information on today's guest or for links to
any of the materials mentioned check out twimmol ai.com slash rewind 19 be sure to leave us a
five star rating and a glowing review after you hit that subscribe button on your favorite
podcast catcher thanks so much for listening and catch you next time
