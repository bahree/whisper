Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Last week I spent some time at CES, the consumer electronics show in Las Vegas, exploring
the vast sea of drones, cameras, paper thin TVs, robots, laundry folding closets and other
smart devices.
You name it?
It was there.
Of course, I was also able to sit down with some really interesting folks working on some
pretty cool AI-enabled products.
Head on over to our YouTube channel to check out some behind-the-scenes footage from my
interviews and other quick takes from the show.
And beyond the look up for our AI and consumer electronics series right here on the podcast,
coming soon.
The show you're about to hear is part of a series of shows recorded at the rework Deep Learning
Summit in Montreal back in October.
This was a great event, and in fact, their next event, the Deep Learning Summit San Francisco,
is right around the corner on January 25th and 26th, and will feature more leading researchers
and technologists like the ones you'll hear on the show this week, including Ian Goodfellow
of Google Brain and Daphne Kohler of Calico Labs and more.
Definitely check out the event and use the code TwimmelAI for 20% off of registration.
In this show, I speak with Greg Dymus, Senior Computer Systems Researcher at Baidu.
Greg joined me before his talk at the Deep Learning Summit where he spoke on the next
generation of AI chips.
Greg's talk focused on some of the work his team was involved in that accelerate Deep Learning
training by using mixed 16-bit and 32-bit floating point arithmetic.
We cover a ton of interesting ground in this conversation, and if you're interested
in systems-level thinking around scaling and accelerating Deep Learning, you're really
going to like this one.
Of course, if you like this one, you're also going to like TwimmelTalk number 14 with
Greg's former colleague, Shubo Sengupta, which covers a bunch of related topics.
If you haven't already listened to that one, I encourage you to check it out.
And now on to the show.
Hey everyone, I am here at the Rework Deep Learning Conference in Montreal, and I've got
the pleasure to be seated across from Greg Dymus from Baidu, and he's a Senior Researcher
there.
Greg, welcome to this week in Machine Learning and AI.
Thanks for having me.
Awesome.
Why don't we get started by having you tell us a little bit about your background and
how you got interested in ML and AI?
Sure, absolutely.
My background has traditionally been in high performance computing.
I've been really excited about building really fast processors for important applications
that enable new applications that people can use.
You know, it's actually kind of a strange story how I got to AI.
I used to be an AI skeptic.
Okay.
That's always a good place to start.
Yeah.
I always felt like AI would be valuable, but I just felt like there's no way that simple
algorithms like Stochastic gradient descent could ever solve these complex, you know, highly
multi-dimensional optimization problems.
And then at one point, I remember sitting at Nvidia Research and hearing a talk from
Jan LeCoon and just realizing, oh, I was wrong.
I was totally wrong.
And immediately after that, I joined by your research, Andrew was founding the Silicon
Valley AI lab and it seemed like a great opportunity to learn more about AI.
And man, it's been such a crazy ride to get to this point.
I bet.
So what was your path to get to that point that you even had an opinion that involves
Stochastic gradient descent?
Oh, sure.
I mean, well, let's see.
I like building things that are useful for people.
I feel like computing in general has enabled many, you know, new capabilities like the
internet and like videos and, you know, so many things that we take for granted every
day, but it really make our lives better.
And I was always really passionate about making computers even better.
Okay.
It's kind of this belief that although you might not know it going into it, if you make
a faster computer or more efficient computer, someone will find a way of building something
amazing on top of that.
And so I spent a lot of time looking at applications.
What are the things that you can use computers to do?
And AI was always there.
Okay.
Just the feeling with AI has, you know, for me before deporning was that it would just
be too hard that a lot of existing theory was kind of steering you down and had all of
these really difficult challenges.
And, you know, I'd seen a lot of people, very smart people spend a lot of time trying
to tackle those problems and not quite getting all the way there.
So do you recall what was it about Jan's talk that kind of made the light bulb go off
and made you realize that it's to cast a green and to send was the answer?
Sure.
So you can look at these hierarchical feature representations and covenants.
So when people, you know, look at images, you can tell, it's the world is hierarchical.
You can break a chair down into pieces.
It has arms and legs.
You can break those pieces down, you know, recursively.
And there's a lot of existing work that provides some evidence that vision algorithms will
do similar things for recognition tasks.
That wasn't ever really a question.
And people were able to build by hand, you know, things like feature detectors and these
hierarchical systems that worked reasonably well.
They were just very difficult to build.
And the interesting thing about Jan's talk was that, you know, systems could do it automatically.
I thought before that that you would get stuck in these, you know, intractable optimization
problems where even if a solution exists, you know, it's one out of some enormously large
number, like, you know, two to the power of 10 to the power of 30, like something amazingly
big.
Like, you know, you think about different sizes of numbers.
Sometimes I think of the number of atoms in the universe as being a big number.
This is far, far bigger than that.
And you were thinking that that number represents the represents what?
How many things you'd have to search through to find a good search base for your, okay?
Yeah, it's like the needle in an enormous haystack, more atoms than there are in the universe.
How could you ever possibly hope to search through it efficiently?
Wow.
Actually, with these very simple algorithms, but, you know, reliably, I've seen since
then for one application after another, for image recognition, for speech recognition,
for synthesis, for language understanding, it works very reliably.
Did you happen to catch any of Jeff Hinton's talk about this?
What do you think about his kind of post SGD capsule?
Well, it's not post SGD, actually, the starting point is that SGD is really the only
thing that we know works, right?
But it's more post kind of the traditional model of the neuron.
Yeah.
I almost think of it as like post-convenants, and, you know, one thing we've realized recently
is just there is a lot of complexity in modeling, that while we like to think of deep warning
as a general purpose learning algorithm, as you sort of playing it to different applications,
like my experience has been spending a lot of time applying it to speech recognition.
And you do get some benefits from more data and from some general purpose aspects of the
learning algorithm to the extent that it's robust to different speakers or different
variations in different environments, but you also get a lot of benefits from specialization.
So finding the right neural network architecture seems like it matters a lot.
And as we look into details for different applications, as we spend time tuning your
own network architectures for different applications, you see very different structures emerge.
So it wouldn't surprise me at all if there is a more efficient, more general purpose structure
for vision than covenants.
Yeah.
One of the really helped me see that was a blog post by Stephen Merritti a while ago at
May, be almost a year ago at this point, but he talked about network architecture being
the new feature engineering.
Hinton Salk was post-comments, but he did start it off by talking about, like calling
into question the basic neuron structure, but he didn't necessarily, he did kind of pivot
to talking about the network architecture at a higher level, right?
Was there a piece in there where he suggested what might be the kind of successor to the
traditional neuron architecture?
I think it's about this concept of capsules, which might be groups of cooperating neurons.
And then the way that they cooperate together might be more, more complex.
Let's see, I feel like from a computational perspective, it's actually hard to get away
from the formulation of neurons that we have as the basic building block where even if there
is something that's algorithmically more efficient or more well matched to the problem, the computational
building blocks that we have have been so highly tuned that if you made a very substantial
change, it might be better kind of at the algorithm level, but it might be very inefficient
on the type of computer that we know how to build today.
It breaks this whole ecosystem that we've built up around this traditional way of building
neurons and networks and solving them.
Yeah, so much of the existing technologies are built on top of harder support and also
algorithm support for linear algebra, like dense linear algebra.
It's actually kind of surprising to me how effective that's been given that the primitives
are so old and that they're so simple, it's very surprising to me that those building
blocks have gotten us as far as they have.
So we took a little digression, I guess, before we got to kind of what you're up to at
Baidu.
Sure.
And Baidu, really the Silicon Valley AI lab is about building new breakthrough technologies
in AI that especially have connections or enabled new products.
We focus on things that we can't currently do today and there are really multiple ways
we end up attacking this.
The thing that I focus a lot on is just the idea of scale that as we have faster computers
that can train larger or complex neural networks that it's not the only way, but that's a very
reliable way of improving accuracy or enabling new capabilities.
We've seen this in vision to a large extent.
It's actually kind of interesting when we started working in Baidu, there is a question
whether you could apply this outside of vision.
We spent a lot of time looking into speech recognition.
I think looking back on that, it works very reliably.
You can definitely apply deporting outside of vision to many different applications.
Sometimes now instead of thinking what is the new application that you can apply deporting
to, I sometimes wonder, are there any applications that are not well matched that we won't be
able to make significant progress on by just applying the simple recipe of deep architectures,
large data sets, large scale computer?
I haven't found one yet.
We talked a little bit about before we got started.
We talked a little bit about the fact that you worked with one of our previous guests,
Shibos and Gupta, who was at Baidu and is now at Facebook, is that right?
Yeah, he's playing Dota.
Nice. He and I spoke pretty extensively about the speech translation, the specific name
of the project with the Baidu speech, the deep speech, right?
In the course of that conversation, we talked a little bit about some of the scalability
challenges that your team ran into in tackling that problem.
Here at this conference, you're talking about, you have to talk tomorrow, in fact, about
some even further work that you've done.
Can you tell us about what you're planning to talk about?
Sure, definitely.
This is definitely along the lines of scaling deep neural networks.
We pretty consistently find that if you throw more data at the problem, it isn't the only
way, but it is a very effective way of reducing error rates and improving accuracy.
So oftentimes, when you keep throwing data at the problem, you eventually run into some
limitation.
Sometimes you run out of data, and sometimes you run out of patience to wait for your
system to train.
There's one example that I think drives this point at home that we once had a model that
ran, I let it run on a large cluster, run about 64 GPUs for about six months.
We were still getting improvements in accuracy at the time I decided to pull the plug.
How are you, if you're still running your training model for, you're saying that kind of your
incremental error is decreasing as you ran it?
Yeah, this was a state of the art model, so it's actually improving the state of the art
as it runs every minute, it's getting a little bit better than any model that we've had
before.
And then after six months, you really go back and look at that and say, well, I could
let it run for another few years, but I'd like to use it now.
So we're always looking for ways to improve speed.
There's actually, oh man, there's something that's related to that that I can't talk
about yet.
It's kind of a weird situation to sometimes be in where like you know why something happens
or you know that something will keep happening, but you can't talk about it.
One of the things that I think I know is that for many applications, we will continue
to see improvements in the state of the art from faster computers.
I can't tell you why, but I'm pretty sure, I'll tell you why soon.
Okay.
Is this the, are we talking about a theoretical result or a, okay, yeah, I think so.
We're nodding yes for those who can't see.
I'm nodding yes.
Yeah.
This is one of those things that'll definitely be surprising to people when we can finally
talk about it, but sorry, I can't talk about it today.
But you, so you just have to take my word for it that we need faster computers.
Okay.
And the talk tomorrow is going to be about a way that we can make computers faster for
deep warning.
Okay.
Yeah.
I spend a lot of my time thinking about this, like what's the best that you could do?
How fast could you possibly make a computer even our understanding of physics on our existing
technology?
I think one thing the industry is realizing is that we spend a lot of time focusing on general
purpose computation, so building computers to run windows or your browser.
But if you specialize, if you build a computer that's good, it only a few things and not
everything, you can do a lot better.
So we're exploring right now, how do you build computers that are good at AI for good
and deep warning?
And is this different than what others in the space are doing, like TPUs and things of
that nature?
Let's see.
So this is about a very specific technique.
It might be one technology that might go into a chip like a GPU or a TPU.
Okay.
Right now, many of these designs are using a lot of the same technologies.
Okay.
This is a new one.
And this one has a pretty high upside.
This one has a maybe order of magnitude upside.
Okay.
Well, before we dive into that, can we take a second to kind of characterize the thing
that GPUs and TPUs are doing that's kind of gotten them the benefit and then we'll
dive into, you know, this approach and what makes it different?
Sure.
Definitely.
So, let's see.
I feel like one of the, well, there are two, okay, there are a lot of differences.
One thing that's worth keeping in mind is that modern processors incorporate probably
thousands, probably even more optimizations.
So these are technologies that will improve their performance in some way.
They might be circuit level, they might be architecture level, they might be in the software
stack.
It's very hard because real designs are composed of, you know, you pick out your favorite
out of this pool of thousands of technologies and that becomes the new processor that you
build.
These distinctions like TPU, GPU, CPU, they're very high level and they gloss over all of
those details.
Okay.
So, I think what's more important than the name is what it does, how fast is it actually,
like what is the result that you get from it?
How fast does it run a model that you care about?
We've seen things that were called CPUs being commonly used for training maybe 10 years
ago.
There was a transition where people started using GPUs.
The important thing about GPUs was optimization for parallelism, that there is abundant
parallelism in neural network computations.
Some of the things that are, you know, a few, like a couple out of that list of thousand
things that are being added into the next generation are optimizations around locality
and low precision.
So the technology I'm going to talk about tomorrow is focused on low precision, and there's
a big difference when a lot of previous technologies have been discussed or proposed for low precision,
it's mostly been focused on inference and not training.
And this will be one of the first results, and especially the, as far as I know, largest
skill result that focuses on using low precision for training, I think the high level conclusion
is it finally works.
It was enormously difficult.
Wow.
It was actually kind of a weird surprise that when you try doing low precision for training
versus inference, we didn't really know that we would see this.
We started looking into this, but it just turns out that for some reason, inference is
so much easier than training, that even, you know, very drastic reductions in precision,
like moving from double precision down to, you know, even 8-bit or possibly even lower
fixed point representations.
It works just fine across many different models.
But if you try and do the same thing for training, things fail.
It's actually kind of a funny point to me that we kind of made this implicit transition.
CPUs commonly support a high performance double precision.
CPUs don't, GPUs have historically optimized around single precision, so 32-bit floating
point instead of 64-bit floating point.
It turns out it's kind of expensive to do this in a GPU, to do 64-bit in a GPU, whereas
it's pretty cheap in a CPU, because you don't have to replicate this thing, this unit
very many times.
Right.
On a GPU, you have to replicate it a lot.
So if you replicate something big a lot, it becomes expensive.
It's kind of surprising that the whole industry, you know, when I started watching people train
deep neural networks, they might write scripts in, you know, MATLAB or, you know, call CPU
libraries directly.
And those things, by default, use double precision.
When the industry switched to GPUs, they switched from double precision to single precision.
And we got so lucky.
It turns out that it didn't really matter.
But I think that was just by luck.
And we tried doing the next step.
We tried moving from single precision to half precision, so moving from 32-bit floating
point to 16-bit floating point, things started failing all over the place.
And what caused those failures?
There were a lot of them.
Let me try and draw a couple of big categories.
One was just differences in range.
So one of the points of having floating point, as opposed to fixed point, is that you have
a very large dynamic range.
You might, you know, be able to do an operation like an ad of a number that's, you know,
where one number is a billion.
And the other number is, you know, 10 to the minus 5.
And that works.
And so you need your range to extend from the smallest numbers that you want to deal
with to the biggest numbers that you want to deal with.
And it turns out that if you look at all of the operations that go on in four-propagation
and backpropagation, the nonlinearities in the SGD algorithm, there's actually a pretty
large dynamic range.
Aren't we typically normalizing to try to get rid of some of that?
Yeah, it's interesting.
I'll come back to that point.
It was an interesting thing related to that point.
Yeah, let me come back to that.
But I feel like the number one reason why when we just, so the first experiments we did
were just convert all of the 32-bit numbers to 16-bit precision numbers and try using
exactly the same algorithm.
And also, and by due, you know, because we were working on speech recognition, we started
doing this for recurrent neural networks.
It turns out that was one of the harder examples.
We started with one of the harder cases it turned out.
And so we would see all sorts of failures.
And what makes RNN particularly harder?
I think it has to do with accumulated errors.
So as you keep doing this repeated application of a matrix multiplication with the same weights,
you're thinking about, or over time, this just encourages extreme values, either extremely
small values or extremely large values.
So sometimes people call this the vanishing gradients or exploding gradients problems.
And for speech recognition, we see very long time series.
We might see hundreds of iterations of an RNN or maybe thousands.
And we saw, you know, large accumulated errors over time.
One of the biggest sources we came across was when you're actually combining gradients with
the gradient update with the master copy of the weights.
So when you have this model, it turns out it seems like SGT just makes these repeated
small updates to a model.
And so if you look at it from a range perspective, there's a large difference in magnitude
between the magnitude of the gradients and the magnitude of the weights.
And so when you try and do operations on those, you know, numbers that have very different
magnitudes, you get loss of information or you get errors.
And that was one of the biggest problems we had with training in half precision.
It seemed like, yeah, moving for some reason, the errors introduced from like the, in floating
point, things work out well if the numbers are in different magnitudes, but not by too much.
And so it turns out that the difference for single precision versus double precision
was okay, but it ended up being borderline for multiple applications when we were looking
at the difference between single precision and half precision.
So we had to introduce some changes in order to deal with that.
One of the questions that came up in this previous conversation with Shubo, in which we talked,
we touched on some of this stuff, I think pretty tangentials, like the end of our conversation,
I think if I remember correctly, but we're talking about a reduced precision and I think
I asked the question like, you can reduce the precision in multiple places.
You can reduce the precision in your weights, you can reduce the precision in your outputs.
When you're talking about reduced precision, or you're talking, it sounds like you're
talking about reduced precision everywhere, just running on reduced precision infrastructure
on a reduced precision mode and not being particularly discriminating in terms of where
you reduce the precision.
Is that what you're referring to?
Yes, we're trying to keep it simple.
We feel like if it ends up getting very complex, then it's difficult for people to know how
you would actually apply this to a real model.
You get back into your architecture, feature engineering, complexity issues.
We definitely didn't want to introduce this as another hyper parameter, or maybe this
only works for a few layers, but it doesn't work in these places.
You have to make this hard choice of deciding which ones to convert, which ones not.
We wanted it just to be kind of like a switch, and you would turn on the switch, and you
would get the performance improvement.
I think we finally got to that point, but for this kind of reason, there were a lot of problems
along the way.
I mentioned the difference in magnitudes between the updates and the weights as a source
of errors.
The other big one was just accumulated errors in long dot products.
It turns out that taking weights convert, quantizing them to 16-bit, and then doing multiplications
of activations with those weights didn't introduce too many errors.
But in neural networks, especially in recurrent neural networks, as layers get big, you end
up with these long dot products, and so you're doing a running sum, kind of like over each
row, or all of the inputs of a neuron.
Each operation has an accumulated error, so everyone in the sequence is going to add some
amount of error.
And now we're not talking about error in the kind of machine learning modeling, since we're
talking about floating point error.
Yeah, we're talking about just, you know, you really wanted to do this multiply operation.
You didn't get the exact result.
We had to clamp it to a value that's representable by the computer.
And so each time you do that, you introduce quantization error.
And normally, as long as you have enough bits, the quantization error is small enough that
it doesn't really affect the final result too much.
Exactly what too much means is very application dependent, and into complex systems like neural
networks, it's really hard to know how much error is too much error other than just trying
it on a real application.
But we found for real applications, like for speech recognition or for translation, the
error introduced by doing ads in 16-bit was too much error.
Models would diverge or models would achieve significantly worse accuracy than the 32-bit
baselines.
And so we went back to that and we tried a whole bunch of things, like we tried hierarchical
reductions and a bunch of things that ended up just being complicated.
And eventually we went back and looked at the circuits and came to the conclusion that
it wasn't that expensive just to put in a 32-bit adder.
So you have a bunch of 16-bit multipliers and then you have a few 32-bit adders.
And if you look at the performance improvement that you get from that, it ends up being
most of the performance improvement that you would have got if you would have built 16-bit
multipliers and 16-bit adders.
So yeah, we ended up with a mixed precision format.
You end up doing multiplication in 16-bit but then the addition in 32-bit.
And there are a lot of other things.
We ended up looking at there's still some other failure cases but those are really the
two big things.
As long as you keep the master copy of weights in 32-bit and as long as you do all the
additions in 32-bit, you can do all the multiplications and you can represent all the activations
and intermediate copies of weights and weight gradients in 16-bit.
To take a step back and make sure I understand why we're doing this, are we talking about
performance and computational costs, are we talking about kind of unit compute costs
for this chip by having narrower buses and things like that?
Are we talking about training time performance?
What are the factors that are driving us to say we want to do this and not just we can
do it and reduce precision, we want to do this and reduce precision.
Sure.
Yeah, why do we want to do this and reduce precision?
It's really so we can build more efficient hardware.
Within without this technique you can just do a comparison.
If you're building the same processor within without this technique, there's a fair amount
of performance at play.
It might be something like four to eight X difference in really both sides of it, total
performance or energy per operation, which would translate into efficiency.
By going to reduce performance or by going to reduce precision, we can increase some
composite of performance and energy consumption by four to eight X, like nearly order of
magnitude.
Yeah, we could finish my six month model in maybe just a single month.
And is it, I guess I'm trying to get at this question, I don't know if the question
makes sense, but like, is it something inherent about the lower precision or is it the fact
that the lower precision allows us to use new compute architectures that are faster in
other ways?
Oh, sure, definitely.
So do you get this performance improvement on existing computers?
You get some performance improvement because you're moving around less data, but it might
be closer to two X, it really depends on whether you're compute bound or bandwidth bound,
but the maximum might be more like two X.
But if you build another computer, if you build a new processor that was optimized around
this idea, you could do even better, you could realize the four to eight X.
Okay.
So low precision fundamentally allows you to do kind of train these neural nets by moving
around less data, right, 16 bits instead of 64, for example.
So you get some advantage in doing that, even if you're just in low precision mode on a
general purpose computer, but it also allows you to build chips that are specific to running
in low precision, and that gives you, that's where you get the big opportunity to bump up
your speeds.
Yes.
Exactly.
Okay.
And so you were here talking about the actual chip.
Is that correct?
Oh, yeah.
So we're going to talk about the Volta GPU from Nvidia.
This is a, yeah, this is a collaboration within video.
It's worth noting, you know, this hardware has been shipping for a while.
But the side of it that we're talking about now is the validation that we've done on
it.
So we've actually shown that you can train models in low precision.
We've looked at, you know, over 15 large scale, complete end to end deporting applications.
So it's really easy to build hardware that gets great performance numbers, but isn't
able to run any real algorithms.
So from the point of view of low, of low precision, the Volta is like its general purpose, right?
It's not a chip that specifically designed for low precision.
Oh, it did actually have, they're called tensor cores.
Right.
It was the name for them is a tensor core that is this operation I'm talking about.
Okay.
It's a specialized unit that does 16 bit multiplication floating point with 32 bit floating point
addition.
That unit was designed as a result of the study.
Got it.
Got it.
And now, if I remember correctly, when this was announced, they made a big deal about not
the floating point side of things, but like N8 performance and things like that.
How does that all fit in?
Sure.
Definitely.
So I kind of alluded to this maybe in the beginning that inference just is easier for
some reason than training.
So that's all the inference.
It's like we can do N8 on inference side and it is easy and it just works.
And it's faster.
Exactly.
Got it.
Okay.
I don't know.
I don't know that this whole topic has been really fully explored yet.
Maybe someday in the future, we might see someone who gets in date training to work.
But as far as I know, I've never seen it.
I know there are a lot of, there's a lot of work on, you know, very reduced precision,
like even down to binary.
But one thing that's worth noting about these approaches is that they either have accuracy
losses, so you trade precision for accuracy on the complete application or they only apply
to inference and not to training.
Mm-hmm.
Okay.
Got it.
So, reduced precision, you did some validation that shows that essentially running in this
mode is kind of a generalized approach you can take.
Now, you know, things that you need to do or switches that you need to flip when you're
training your model in order to get it to work accurately or to work correctly.
Sure.
So, one switch that you need to flip is you need to decide to do this.
Okay.
You need to decide to represent things in 16-bit and do your matrix multiplications or
convolution operations in this mixed 16-bit, 32-bit format.
That's somewhat of a global switch, you can just turn that on for the entire program.
Okay.
The other thing that you need to do that we found as essential is you need to master
a copy of the weights.
Mm-hmm.
So, in your optimization algorithm, like your implementation of SGD, you need to have a
separate copy in 32-bit of all the weights.
And only when you're doing a Ford propagation or back propagation, do you convert from that
into 16-bit?
Okay.
But, both of those changes we found are the first ones really easy.
The second one can be encapsulated inside of the optimization algorithm.
So at least when you're designing a network, you don't have to think about this.
Okay.
You know, I can envision how I might do this if I was writing the, you know, if I was
implementing SGD myself to the higher level frameworks and toolkits all know how to
do this or is that, you know, yet forthcoming.
So it's straightforward to do this in most frameworks, but there needs to be developers who are
working on those frameworks who will actually add support for this.
Okay.
You know, when we did this in the framework that we have in BIDU, it's something like a,
you know, 15 lines of code change.
Yeah.
So it's really minor, but you still have to do that.
Otherwise, you won't get access to the improved performance.
Right.
Okay.
Anything else you talked about in your, or I keep saying it in past tense.
Anything else you're going to talk about in your talk that you want to mention?
I guess the last thing is that this is one piece.
This is one technology that gives us a large improvement in performance.
I think we're aware of a lot of them that haven't been realized yet.
So you know, I mentioned before, the hardware industry for a long time has been kind of creeping
along.
It's actually very difficult to realize large improvements in sequential performance.
But at least for parallel programs like graphics applications and things that would run on GPUs,
performance has been increasing, you know, following something like the popular form of
Moore's law, so exponential growth.
For AI, if you're only thinking about running deep neural networks, you can probably do
a lot better than that in the short term.
So we might not have to wait 10 years to get 1,000 X faster.
It might happen in just a few years.
I'm going to mention some of the other ways that haven't been implemented yet, but that
we know about it are likely to happen in the future.
Can you rattle those off?
This podcast will not be published before you're talked tomorrow.
Sure.
One of them is array parallelism.
I think this is one of the other big ones is array parallelism.
Okay.
I had a Forbes article about this for us talking about locality, the importance of locality.
If you build processors around the idea of locality and parallelism, not just parallelism, you
end up with something that it looks, I call it like an array processor rather than a vector
processor.
Okay.
You're thinking about the core instruction that you're doing instead of adding or multiplying
two vectors together, you're adding or multiplying two arrays together.
So you see things like this in designs like the TPU.
I feel like the thing that is wrong with those designs is that they don't find the knee
of the curve that this is beneficial, but you don't have to go all in on it to get most
of the benefits and you actually are trading off.
So you don't find the knee of the curve, what exactly does that mean?
It means working on a raise is a good idea, but they don't have to be enormous arrays.
Okay.
And you actually are trading off flexibility for performance when you're making the arrays
bigger.
So you shouldn't make them enormous.
You should make them big enough to get most of the savings and energy.
In terms of order of magnitude, we're talking about like little teeny ones, like convolutional
kernel sizes, are we talking about, you know, something bigger than that?
Or...
Yeah, it's like more like 16 by 16 than 256 by 256, that makes sense.
Okay.
Yeah.
Interesting.
Any others on that list that come to mind?
One of the ones that doesn't work yet, but I think is very promising, is sparsity.
Okay.
We're talking with sparse representations rather than dense representations.
And it might seem like that's incompatible with the one that I just said, the array parallelism.
We haven't shown this yet, but I suspect that they're not incompatible.
Well, what I'm hearing putting the two together is that, you know, we're living in a world
that thinks about all this stuff as composite vector operations.
And by thinking about this at the level of matrices, you know, there are opportunities
there.
Yes.
Yeah.
That's a good way of thinking about it.
Interesting.
Well, I really enjoyed this chat.
Thank you so much for taking the time.
Good to be here.
Great.
Thanks, Craig.
Thank you.
All right, everyone.
That's our show for today.
Thanks so much for listening and for your continued feedback and support.
Thanks to you, this podcast finished the year as a top 40 technology podcast on Apple
podcasts.
My producer says that one of his goals this year is to crack the top 10.
And to do that, we will need your help.
Please head on over to the podcast app, rate the show.
Hopefully we've earned your five stars.
Leave us a glowing review and share it with your friends, family, co-workers, Starbucks
baristas, Uber drivers, everyone.
Every review and rating goes a long way.
So thanks in advance.
For more information on Greg or any of the topics covered in this episode, head on over
to twomolei.com slash talk slash 97.
Of course, we'd be delighted to hear from you, either via a comment on the show notes page
or via Twitter at at Twomolei.
Thanks once again for listening and catch you next time.
