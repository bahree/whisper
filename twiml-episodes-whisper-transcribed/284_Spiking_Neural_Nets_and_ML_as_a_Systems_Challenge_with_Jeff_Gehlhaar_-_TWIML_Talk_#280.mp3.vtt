WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.200
I'm your host Sam Charrington.

00:31.200 --> 00:36.280
I'd like to start out by sending a huge thanks to Qualcomm for their support of the podcast

00:36.280 --> 00:39.520
and for sponsoring today's episode.

00:39.520 --> 00:44.160
As you're here in my conversation with Jeff, Qualcomm is taking a systems approach to helping

00:44.160 --> 00:50.720
the industry address the challenges associated with AI on mobile devices and at the edge.

00:50.720 --> 00:55.000
In support of their Snapdragon chipset family, which powers some of the latest and greatest

00:55.000 --> 01:00.360
Android devices, Qualcomm provides their own suite of software tools and is also actively

01:00.360 --> 01:05.720
supporting a variety of partner and industry projects, including the Android Neural Network

01:05.720 --> 01:12.720
APIs, TensorFlow Lite, the TinyML initiative, and the Open Neural Network Exchange or Onyx

01:12.720 --> 01:14.480
ecosystem.

01:14.480 --> 01:21.120
To learn more about Qualcomm's AI research, platforms, developer tools, and ecosystem support,

01:21.120 --> 01:26.400
visit twimbleai.com slash Qualcomm.

01:26.400 --> 01:29.600
A quick community update before we dive in.

01:29.600 --> 01:33.800
Many of you are aware that we've been hosting a couple of paper reading meetups in conjunction

01:33.800 --> 01:34.800
with the podcast.

01:34.800 --> 01:40.240
Well, I'm excited to share that Matt Kenny, Duke staff researcher and longtime listener

01:40.240 --> 01:44.880
and friend of the show, has stepped up to help take this group to the next level.

01:44.880 --> 01:50.160
The paper reading meetup will now be meeting every other Sunday at 1 p.m. U.S. Eastern

01:50.160 --> 01:56.760
time to dissect the latest and greatest academic research papers in machine learning and AI.

01:56.760 --> 02:00.760
If you want to take your understanding of the field to the next level, please join us

02:00.760 --> 02:09.640
this Sunday, July 4th, or check twimbleai.com slash meetup for more upcoming community events.

02:09.640 --> 02:13.840
We've also got a couple of study groups currently running, with one group working through

02:13.840 --> 02:18.920
the fast.ai deep learning from the foundations course, formerly known as deep learning for

02:18.920 --> 02:26.280
coders part two, and another working through the Stanford CS224N deep learning for natural

02:26.280 --> 02:28.800
language processing course.

02:28.800 --> 02:32.640
These study groups just started and will be working on these courses through October

02:32.640 --> 02:37.640
and November respectively, so it's not too late to join in.

02:37.640 --> 02:45.880
Sign up on the meetup page at twimbleai.com slash meetup.

02:45.880 --> 02:48.480
Hi everyone, I am on the line with Jeff Galhar.

02:48.480 --> 02:53.480
Jeff is VP of technology and head of AI software platforms at Qualcomm.

02:53.480 --> 02:56.240
Jeff, welcome to this week in machine learning and AI.

02:56.240 --> 02:57.240
Thank you very much.

02:57.240 --> 02:59.840
I'm pleased to be here and thank you for having me, Sam.

02:59.840 --> 03:01.640
It's great to have you on the show.

03:01.640 --> 03:05.600
Let's jump right in and chat a little bit about your background.

03:05.600 --> 03:10.760
You have spent 30 years at Qualcomm as far as that, right?

03:10.760 --> 03:14.240
That's right, not in a linear sort of fashion.

03:14.240 --> 03:19.840
I like to say I'm on my second tour of duty and we can talk about how I came and went.

03:19.840 --> 03:26.080
But yes, I've been here a long time since the long arc of Qualcomm's innovation capacity.

03:26.080 --> 03:31.800
So tell us a little bit about some of the things you've done at the company and how you've

03:31.800 --> 03:37.000
come to get involved in their AI software efforts.

03:37.000 --> 03:38.560
Yeah, perfect, perfect.

03:38.560 --> 03:44.760
So I started as a young engineer a few years after the company got started and so I've

03:44.760 --> 03:51.160
had the pleasure of seeing the company go through ups and downs and challenges and innovate

03:51.160 --> 03:58.640
in first and CDMA before that and other communication vehicles and then move on to 3G and

03:58.640 --> 04:01.760
4G and now 5G wireless standards.

04:01.760 --> 04:07.240
And I spent a lot of my career working in wireless, both in hardware and software.

04:07.240 --> 04:12.040
I worked on ASICs and semiconductors, but most of my career has been in systems and software

04:12.040 --> 04:13.160
work.

04:13.160 --> 04:19.720
And then as a part about Devastrature, the Qualcomm went through in the late 90s.

04:19.720 --> 04:26.680
I left Qualcomm, it was end of two or one and ended up landing at a small startup.

04:26.680 --> 04:30.560
Actually, I didn't know that they were sort of doing what we would today consider sort

04:30.560 --> 04:34.080
of machine learning with support vector machines and so on, but that's what they were doing

04:34.080 --> 04:41.160
and joined them in ran engineering for that company, did commercial work for the government

04:41.160 --> 04:43.920
and for Fortune 500 companies.

04:43.920 --> 04:47.680
And that was my first exposure to sort of AI and machine learning, let's say in the

04:47.680 --> 04:49.000
modern age.

04:49.000 --> 04:53.320
And so I learned a lot there and then came back to Qualcomm and with that company we sold

04:53.320 --> 04:58.160
that company and came back to Qualcomm and then again rejoined wireless and spent a good

04:58.160 --> 05:04.600
chunk of the last, you know, 15 years, 16 years I've been back in wireless.

05:04.600 --> 05:10.600
And then I got a chance to, I was asked to help co lead a project that had gotten started

05:10.600 --> 05:12.320
in spiking neural networks.

05:12.320 --> 05:15.440
So we had made an investment in a small company.

05:15.440 --> 05:17.960
We did a joint research program with this company.

05:17.960 --> 05:19.280
What was that company?

05:19.280 --> 05:24.320
The company has brain corporation, it's still going well here in San Diego.

05:24.320 --> 05:27.920
Qualcomm Ventures took an investment in them and they've since gone on to do sort of

05:27.920 --> 05:31.200
automated robotics and vision systems.

05:31.200 --> 05:36.120
But we did early work with them on the pretext of and in conjunction in some sense with

05:36.120 --> 05:40.760
the DARPA Synapse program, who was actually sort of at the end of the Synapse program,

05:40.760 --> 05:46.440
but we were taking a lot of those similar ideas like IBM was at the time as well.

05:46.440 --> 05:52.440
And really tried to see if we could build sort of biologically accurate systems, vision

05:52.440 --> 05:58.440
systems in this case, in the way that we think of deep learning today, but sort of predating

05:58.440 --> 06:02.880
sort of this Krochevsky kind of, you know, aha moment.

06:02.880 --> 06:09.640
And when that happened, when in 2011, 2012, you know, it kind of became obvious that these

06:09.640 --> 06:12.840
deep neural networks had, you know, hit upon something, right?

06:12.840 --> 06:16.920
It resurrected, you know, back prop and hit upon something.

06:16.920 --> 06:22.960
We moved the program away from spike in neural networks as all kinds of challenges.

06:22.960 --> 06:32.800
And it subsequently, subsequently, we evolved into a deep learning and kind of program.

06:32.800 --> 06:36.920
That was during my time in Qualcomm research.

06:36.920 --> 06:42.560
What has happened in the last few years is that the work that we did there, I left whenever

06:42.560 --> 06:46.240
our commercial group now run our commercial AI software.

06:46.240 --> 06:50.520
And what is, we think of now as Qualcomm AI research has grown and continued based on

06:50.520 --> 06:53.320
that sort of initial work.

06:53.320 --> 06:55.360
And now we're sort of partners.

06:55.360 --> 06:59.800
They work on long-term research and bring innovations that we then bring to products.

06:59.800 --> 07:06.440
And we work with our SOC channels and our customers that are using our SOCs to help them, you

07:06.440 --> 07:10.120
know, do AI on our chips.

07:10.120 --> 07:15.440
Okay. And for folks that are interested in hearing a bit more about what's happening

07:15.440 --> 07:22.160
from our research perspective at Qualcomm, they might be interested in checking out the

07:22.160 --> 07:30.960
recent interview with Max Welling that is Twomel Talk 267 from back in May.

07:30.960 --> 07:36.200
But that was a great conversation, as I'm sure this one will be.

07:36.200 --> 07:39.920
You mentioned the spiking neural networks work.

07:39.920 --> 07:44.400
I don't think Max and I got into any of that kind of stuff.

07:44.400 --> 07:49.320
And it's come up, you know, maybe superficially on the show once or twice.

07:49.320 --> 07:53.640
But I don't think I've ever got anyone to kind of share, you know, who's worked in it

07:53.640 --> 07:56.400
to share a little bit of background on that.

07:56.400 --> 07:58.680
Is that something you can maybe spend a few minutes on?

07:58.680 --> 07:59.680
Sure.

07:59.680 --> 08:00.680
Sure.

08:00.680 --> 08:02.880
I have to dust off some old memories, but I'm going to get to do.

08:02.880 --> 08:09.440
So okay, so the basic idea was, you know, in some sense, this basic intuition that

08:09.440 --> 08:12.760
neural networks are made of, you know, stacks of neurons and they're interconnected

08:12.760 --> 08:13.760
somehow.

08:13.760 --> 08:18.560
And they're the some learning mechanisms where the strength of the connections is, you

08:18.560 --> 08:23.680
know, somehow related to the experience, if you will, that the neurons get just like

08:23.680 --> 08:26.400
an artificial neural network in some sense.

08:26.400 --> 08:31.320
But the gentleman who started Braincore Eugenia Sikavic had written a book about how to mathematically

08:31.320 --> 08:39.400
model the basic 20 or so synaptic behaviors, as we understand them in mammals, I'd say.

08:39.400 --> 08:44.440
And the visual system is the most well sort of studied part of our cortex.

08:44.440 --> 08:51.360
And so that was the premise of, and what I would say is that was kind of our seed moment

08:51.360 --> 08:58.000
for Qualcomm to get started in what has now sort of become a clearly a revolution computation.

08:58.000 --> 09:03.600
And so it was a bat, it was a really early stage bat that the next computing evolution

09:03.600 --> 09:06.760
would be in this general direction.

09:06.760 --> 09:11.920
And we thought, hey, look, well, the brain does this amazing thing in 20 billion neurons

09:11.920 --> 09:16.840
and 20 watts, let's say roughly speaking, gee, could we build some kind of computing machine

09:16.840 --> 09:18.760
that kind of works like that.

09:18.760 --> 09:25.280
And so we did, and we had some early success, and we built a large scale like training and

09:25.280 --> 09:27.360
simulation kind of environment.

09:27.360 --> 09:32.600
And we did work in FPGA's to actually build hardware, made a design hardware for such

09:32.600 --> 09:34.120
a machine.

09:34.120 --> 09:38.120
And what it really, I think if you boil it down to where it didn't work, it worked in

09:38.120 --> 09:39.120
a lot of ways.

09:39.120 --> 09:41.960
But where it didn't work is back prop.

09:41.960 --> 09:49.280
There was no sort of fundamental principled way to train these networks in a way that produced

09:49.280 --> 09:50.280
stable output.

09:50.280 --> 09:53.600
And so you see that maybe a little bit with GANs, you see them maybe with the struggles

09:53.600 --> 09:55.880
of people having RNNs and related things.

09:55.880 --> 09:57.480
But this was on a massive scale.

09:57.480 --> 10:02.280
You could train small scale networks to do very, very simple things like be tricked into

10:02.280 --> 10:06.600
the optical illusions that human visual cortex systems can be tricked into.

10:06.600 --> 10:09.720
But it's an interesting use case.

10:09.720 --> 10:10.720
Yeah.

10:10.720 --> 10:13.840
Well, it was kind of a concept of like where we kind of on the right track.

10:13.840 --> 10:18.520
And so you could do a very simple like, think about an intention mechanism.

10:18.520 --> 10:24.320
If you stare at an object, your visual system will start, you know, looking for changes

10:24.320 --> 10:28.920
in lines and edges and changes in light, and you could get the system to kind of do that

10:28.920 --> 10:29.920
same sort of thing.

10:29.920 --> 10:33.480
If it's stared at an object long enough, the object would sort of vanish because you

10:33.480 --> 10:39.400
don't have any, you know, psychotic eye movement or anything to sort of stimulate the neurons.

10:39.400 --> 10:43.800
But as you tried to build like a real thing to say a visual system or an object recognition

10:43.800 --> 10:48.040
system at that kind of scale, they just were super hard to train.

10:48.040 --> 10:52.360
And so there were limits and they're basically, you know, fundamental limits that systems

10:52.360 --> 10:56.320
like ANNs with back prop basically solve for us today, right?

10:56.320 --> 11:00.000
And what is the spiking refer to and spiking neural networks?

11:00.000 --> 11:01.000
Yeah.

11:01.000 --> 11:07.640
So literally the sort of the idea that in a synapse, right, in a neuron, in a biological

11:07.640 --> 11:10.080
neuron, it is, you know, gets all this input.

11:10.080 --> 11:13.800
Think about it like a sort of sigma delta kind of time coded input.

11:13.800 --> 11:15.800
You get a little bit of stimulus, a little bit of stimulus.

11:15.800 --> 11:22.120
And when it reaches some kind of threshold, depending on the kind of neuron, it fires.

11:22.120 --> 11:27.600
It sends a signal down to the next neuron or next neurons, however it's connected.

11:27.600 --> 11:29.520
And it's got some kind of decay function, right?

11:29.520 --> 11:35.080
So you can think about these not being like a step function, most of them aren't.

11:35.080 --> 11:37.720
They've got some kind of exponential sort of decay function.

11:37.720 --> 11:41.840
So when we think of spiking, one of the intuitions was for low power.

11:41.840 --> 11:48.680
It was like only stimulate the neurons that are implicated in some transaction, right?

11:48.680 --> 11:53.800
And modeling them biologically in a biologically accurate way in terms of how real neurons

11:53.800 --> 11:59.960
collect input and then, you know, fire, right, discharge their output to the next downstream

11:59.960 --> 12:00.960
neuron.

12:00.960 --> 12:05.440
Because of course your brain is not, really neurons are not active all at the same time.

12:05.440 --> 12:08.600
So that's when we think of spiking, what we really mean is, you know, what is that impulse

12:08.600 --> 12:13.120
function when you reach the, you know, exit threshold, if you will, of the neuron, what

12:13.120 --> 12:14.120
is that impulse?

12:14.120 --> 12:15.520
You know, what is the output?

12:15.520 --> 12:18.240
And at what point does it reach that output, right?

12:18.240 --> 12:19.240
Yeah.

12:19.240 --> 12:20.240
Yeah.

12:20.240 --> 12:25.200
And have you followed the work in this field, it still comes up.

12:25.200 --> 12:29.720
So I imagine there's, you know, there's some progress that's been made.

12:29.720 --> 12:33.080
Do you have a sense for where we are today?

12:33.080 --> 12:37.720
Well, I do follow it a little bit when it comes up in various, you know, journals and

12:37.720 --> 12:39.320
blogs and so on.

12:39.320 --> 12:43.800
I still feel like we're at a place where in some sense we don't have a fundamentally

12:43.800 --> 12:50.800
principled way to train these kinds of systems, and I guess over time, do we have that for

12:50.800 --> 12:52.640
regular deep neural networks?

12:52.640 --> 12:53.640
Well, okay.

12:53.640 --> 12:58.680
So we could put some parameters around that, but we have backprop, so we have a structured

12:58.680 --> 13:02.680
way of propagating these errors of tuning, right?

13:02.680 --> 13:07.680
The strengths of biases, if you will, of all these connections.

13:07.680 --> 13:12.520
But I think that part of it is that what I think artificial neural networks have shown

13:12.520 --> 13:20.880
us so far, you don't need anything as complicated as actually modeling the biology of a neuron

13:20.880 --> 13:23.880
to achieve some pretty impressive results.

13:23.880 --> 13:27.400
In the same way that airplanes, this is probably an old analogy, but in the same way that airplanes

13:27.400 --> 13:33.080
don't flap their wings, you know, it's likely that we can build, they're still very complicated

13:33.080 --> 13:38.480
systems, but systems that don't work quite like the brain works and achieve maybe similar

13:38.480 --> 13:39.960
results, right?

13:39.960 --> 13:47.040
And so most of the work I think in spiking has moved in the direction of things like hardware

13:47.040 --> 13:52.240
architectures that are event driven, there are quite a number of efforts in that area,

13:52.240 --> 13:58.600
and using those kinds of architectures to basically map artificial neural networks onto substrates

13:58.600 --> 14:05.040
that are more energy efficient or more computationally efficient in some way by applying like an

14:05.040 --> 14:10.280
attention mechanism, right, by applying a different way of computing these things by using analog

14:10.280 --> 14:12.120
circuitry in a lot of cases.

14:12.120 --> 14:18.440
So different strategies where the angle is more about low power or more about, you know,

14:18.440 --> 14:23.720
activating only the parts of the network that need to be activated at a given point in time.

14:23.720 --> 14:27.600
So that was I think quite a digression in your bio.

14:27.600 --> 14:28.600
Yeah.

14:28.600 --> 14:34.640
There you go, you want the arc of how we end, how I ended up here and that's how I ended

14:34.640 --> 14:35.640
up here.

14:35.640 --> 14:36.640
Awesome.

14:36.640 --> 14:37.640
Awesome.

14:37.640 --> 14:43.920
And so today maybe dive a little bit deeper into your current role and some of the things

14:43.920 --> 14:47.680
that Qualcomm is working on from a software perspective.

14:47.680 --> 14:48.680
Sure.

14:48.680 --> 14:49.680
Sure.

14:49.680 --> 14:54.520
So as part of this going back a little in the bio as part of this early research we were

14:54.520 --> 14:59.320
doing, we started on things like face detection and object detection and this led us to buy

14:59.320 --> 15:03.640
some companies and set up some additional research offices and so on.

15:03.640 --> 15:10.880
And part of that effort is we realized in this, you know, predates a lot of other movers

15:10.880 --> 15:16.480
here, predates TensorFlow Lite and so on, we realized that in order for us even to experiment

15:16.480 --> 15:23.760
internally to do this on Qualcomm Snapdragon SOCs, we needed some kind of toolkit to, you

15:23.760 --> 15:29.200
know, run all these kernels on our GPU, for example, or on our DSP.

15:29.200 --> 15:35.400
And so we started and we built what now you can go to the developer network and download

15:35.400 --> 15:43.160
the Snapdragon neural processing SDK and but an early version and we targeted at internal

15:43.160 --> 15:48.520
use cases mostly and for evaluating our research and it started to become clear that our customers

15:48.520 --> 15:52.720
were coming to us and saying, Hey, you know, I've got an AI thing I want to try to run.

15:52.720 --> 15:56.880
And I saw your demo, I didn't realize you could do this and this and this on an edge device

15:56.880 --> 15:59.880
on a mobile device on a phone.

15:59.880 --> 16:03.760
Can we have that toolkit basically you have a toolkit and so that led us to commercialize

16:03.760 --> 16:10.920
a toolkit and so today my role is to lead a global organization that commercializes our

16:10.920 --> 16:17.040
AI software stacks not just that toolkit, but our overall AI software stacks in order

16:17.040 --> 16:25.000
to make it our hardware basically our high performance SOCs accessible to internal and external

16:25.000 --> 16:30.600
customers who want to run, you know, AI powered solutions on our SOCs.

16:30.600 --> 16:37.800
And so that what that really has taken a form of is that we are architecture and our chips

16:37.800 --> 16:44.320
is heterogeneous, not every core or every use case lends itself to, you know, every piece

16:44.320 --> 16:47.320
of hardware depending on what your use case is.

16:47.320 --> 16:54.040
And so we provide high performance solutions principally targeting our GPU, our hexagon

16:54.040 --> 17:01.040
DSP and in our flagship parts are HTA or tensor accelerator core.

17:01.040 --> 17:06.880
And then we provide access, high performance access to those hardware blocks either through

17:06.880 --> 17:15.680
our SDK or through Android NN API, which is, you know, Google Android's newest sort of

17:15.680 --> 17:19.040
neural network API and framework.

17:19.040 --> 17:25.080
And recently at Google IOW we announced direct work with the TensorFlow Lite team to power

17:25.080 --> 17:31.680
the backend of TensorFlow Lite with our hexagon neural network accelerator library.

17:31.680 --> 17:37.800
So those are examples of where we're building these high performance AI software blocks

17:37.800 --> 17:46.000
and then exposing them via various ecosystem strategies based on what the ecosystem needs

17:46.000 --> 17:48.160
for those use cases.

17:48.160 --> 17:54.560
You know, when you kind of take a step back and look at this landscape with similar functionality

17:54.560 --> 18:06.120
being exposed via these varying APIs, you mentioned a bunch of them, the Android NN APIs, TensorFlow

18:06.120 --> 18:12.920
Lite, you've got your own stack that a developer can interact directly with as well as, you

18:12.920 --> 18:17.760
know, that supports these other things like what's the best way to make sense of all that

18:17.760 --> 18:25.160
if you're a developer or machine learning engineer that needs to deploy stuff out to a device,

18:25.160 --> 18:26.920
how do you know what you should be using?

18:26.920 --> 18:33.360
Yeah, that's a great question and we face a lot of those kinds of questions.

18:33.360 --> 18:38.880
You know, it's hard to make a universal recommendation, but I could give a little bit of guidance.

18:38.880 --> 18:44.560
I would say that maybe 18 months ago the question, if you'd asked a question similar to it,

18:44.560 --> 18:51.160
which training framework do I use, because the question was really about this kind of explosion

18:51.160 --> 18:54.160
of training frameworks that had happened, right?

18:54.160 --> 18:55.160
We do still have questions.

18:55.160 --> 18:59.960
Now are we talking about kind of TensorFlow versus PyTorch versus something else?

18:59.960 --> 19:06.360
Sure, it was PyTorch, which didn't exist at the time, by the way, 18 months ago.

19:06.360 --> 19:17.960
So a cafe to TensorFlow, now PyTorch, MXNet, CNTK from Microsoft, and then if you think

19:17.960 --> 19:23.760
about it, you think even globally, then in Asia, there's a list, there's Chainer and

19:23.760 --> 19:33.160
there's Parrot and there's all kinds of proprietary mace from Xiaomi, proprietary and quasi-proprietary

19:33.160 --> 19:35.760
frameworks that are out there.

19:35.760 --> 19:41.560
And so one of the things that we tried to do early on was to pick a few, we couldn't support

19:41.560 --> 19:50.160
them all and provide converters from those frameworks to our SDK so that customers didn't

19:50.160 --> 19:52.560
have to, in some sense, make some of these choices.

19:52.560 --> 19:58.160
They could train in TensorFlow if they wanted, they could train in cafe, and then you

19:58.160 --> 20:04.760
can convert your network from those frameworks into something we understand and it can accelerate

20:04.760 --> 20:05.760
for you, right?

20:05.760 --> 20:08.760
And so part of it was just making it easier.

20:08.760 --> 20:14.560
Now in the process, Onyx came along, in part to address this issue of, look, I've got

20:14.560 --> 20:18.360
all these training frameworks and I don't have a way to move between them and I sort of

20:18.360 --> 20:23.200
get locked in with the operators or the techniques that that training system TensorFlow

20:23.200 --> 20:25.560
PyTorch cafe use.

20:25.560 --> 20:32.880
And then I have exactly this problem, oh, I want to run on this SOC, but I started in TensorFlow

20:32.880 --> 20:35.880
and they don't support that operator in TensorFlow.

20:35.880 --> 20:43.280
And so what would really, the advice I give people is, look, there are a lot of training

20:43.280 --> 20:48.840
frameworks, but there's maybe three or four Macs that are really robust and popular.

20:48.840 --> 20:56.440
A lot of the toolkits are going to find their way through one of those into either RSDK

20:56.440 --> 20:59.800
or let's say through TensorFlow Lite onto our SOC.

20:59.800 --> 21:06.240
And so, you know, the funnel is getting narrower, you will have a rich choice of training environments.

21:06.240 --> 21:10.040
A lot of them are doing these eager mode, very Python-like kinds of environments.

21:10.040 --> 21:14.840
And when you're ready to deploy to your edge device, your phone, your IoT device, your

21:14.840 --> 21:17.520
automobile, there'll be an on-ramp, right?

21:17.520 --> 21:25.280
And then our job is to make that network run as fast as possible, you know, power, constraint,

21:25.280 --> 21:28.360
kind of environment that a lot of these applications face.

21:28.360 --> 21:37.120
So in other words, the story today is more like, look, choose one of the popular frameworks

21:37.120 --> 21:46.240
that maps to the experience that you want to have as a developer, you know, ALA, the

21:46.240 --> 21:51.960
differences between TensorFlow versus PyTorch versus something else.

21:51.960 --> 21:58.920
And, you know, we'll build essentially middleware that ensures that, you know, that framework

21:58.920 --> 22:04.160
that you're using can take advantage of, transparently to you, you know, all the things that

22:04.160 --> 22:06.520
underlying chipset is capable of.

22:06.520 --> 22:08.240
Yeah, that's a great way to put it.

22:08.240 --> 22:15.200
That's exactly kind of our perspective is to reduce the friction for our users, customers,

22:15.200 --> 22:20.440
partners, to go from, you know, where they're comfortable onto high-performance solutions

22:20.440 --> 22:22.080
on our devices.

22:22.080 --> 22:26.480
Yeah, forget the timeframe you threw out there, but, you know, historically, it was really

22:26.480 --> 22:33.520
all about kind of this training experience is the follow-on to that that today it's

22:33.520 --> 22:36.240
more about inference?

22:36.240 --> 22:39.880
Well, okay, so I'll make a twist on this.

22:39.880 --> 22:45.440
Yes, it's about inference in the sense that there are, you know, billions of devices

22:45.440 --> 22:51.040
that people want to run some kind of AI-powered inference solution, you know, whether it's

22:51.040 --> 23:00.440
a camera, whether it's audio, like translation, like, you know, Google Home, like, you know,

23:00.440 --> 23:03.800
the explosion of audio use cases we're seeing.

23:03.800 --> 23:09.160
I would characterize that training on device and I will characterize this a little bit more

23:09.160 --> 23:14.560
subtly, what I'll call personalization of the experience is something that I think will

23:14.560 --> 23:17.720
become important and we're starting to see and we're starting to make investments in

23:17.720 --> 23:18.720
that direction.

23:18.720 --> 23:20.040
We have done it in the past.

23:20.040 --> 23:21.360
We know it's possible.

23:21.360 --> 23:26.600
I want to personalize like how my gallery is organized or something in my phone, okay,

23:26.600 --> 23:27.600
right?

23:27.600 --> 23:28.600
Pretty straightforward.

23:28.600 --> 23:35.760
I think the next evolution of personalization will be about things like really contextualizing

23:35.760 --> 23:41.880
the experience of my device to me, to my preferences, to how I use the device.

23:41.880 --> 23:47.240
But we're not there yet, but I would characterize, in my view, the training aspect to be less

23:47.240 --> 23:54.880
about, you know, big data and, you know, massive data sets and more about taking like, if

23:54.880 --> 24:00.960
you will, a vanilla experience and then personalizing that vanilla experience out of the box

24:00.960 --> 24:06.560
experience into something that feels a lot more customized, right, to your experience,

24:06.560 --> 24:07.560
right?

24:07.560 --> 24:11.520
On this personalization point, this is a really interesting point that I've been curious

24:11.520 --> 24:12.520
about for a while.

24:12.520 --> 24:15.840
Do you have a sense for kind of how this is done today?

24:15.840 --> 24:21.960
You know, for example, you know, think about an app like, you know, Gmail that is doing

24:21.960 --> 24:29.480
it the predictive replies or even now as you're typing, like predicting the end of your sentences.

24:29.480 --> 24:35.800
You know, presumably they built out some language model there and it's able to do, you

24:35.800 --> 24:42.200
know, it's able to predict based on, you know, tons and tons of data that they've got

24:42.200 --> 24:43.200
of people's emails.

24:43.200 --> 24:48.840
And that's a whole separate issue about like the privacy issues associated with that,

24:48.840 --> 24:49.840
et cetera.

24:49.840 --> 24:56.760
But, you know, but it also is kind of appears to be personalized to me in the sense that

24:56.760 --> 25:04.800
I think it kind of replies in the way that I tend to reply as opposed to like just some

25:04.800 --> 25:07.600
generalization of what everybody does.

25:07.600 --> 25:14.640
And so how does that, you know, how are folks achieving that today, you know, with machine

25:14.640 --> 25:16.800
learning models on devices?

25:16.800 --> 25:17.800
Well, okay.

25:17.800 --> 25:22.680
So I'd say that, you know, you've really hit on what I can still consider sort of a big

25:22.680 --> 25:26.200
data use case.

25:26.200 --> 25:30.560
And I hate to speculate exactly how Google is doing this, you know, on a behind the scenes

25:30.560 --> 25:32.040
in Gmail.

25:32.040 --> 25:36.760
I think the personalization on device is relatively nascent.

25:36.760 --> 25:40.560
I think this is one of these emerging, you know, things to watch and to come back in

25:40.560 --> 25:43.840
a year or 18 months and talk about it where it's moved.

25:43.840 --> 25:49.840
We're seeing some, I'll put them in the personalization category and maybe people don't think

25:49.840 --> 25:56.600
about it this way, but nascent things like, um, like the ability for you to have fingerprint

25:56.600 --> 25:58.080
or face unlock, right?

25:58.080 --> 26:02.920
That's a personalized thing, you have a generic feature face unlock, but it unlocks for

26:02.920 --> 26:06.040
your face or your fingerprint, right?

26:06.040 --> 26:07.520
Speaker identification, right?

26:07.520 --> 26:13.000
So the ability for a device to know that you're Sam and I'm Jeff, because I've, we've

26:13.000 --> 26:17.120
said 10 keywords or five keywords and it picks up builds a pattern, right?

26:17.120 --> 26:22.320
It's doing that with, let's say, building some kind of classifier by using the features

26:22.320 --> 26:28.040
of the built-in model and then running a classification pass over this, right?

26:28.040 --> 26:32.680
These kinds of things we know are possible, we've done them ourselves on device.

26:32.680 --> 26:36.840
What I think the next revolution will be much deeper integration.

26:36.840 --> 26:44.040
What is the, it's not just a single app, it's like what is the sort of experience of, um,

26:44.040 --> 26:47.960
of the device really looking at how you interact with a lot of different kinds of data and

26:47.960 --> 26:53.800
then drawing some personalization experiences out of that, right?

26:53.800 --> 26:58.880
So that ain't going back to the privacy thing so that in the long arc of it, these things

26:58.880 --> 27:03.440
don't require, you know, massive amounts of cloud data, for example, right, to, in order

27:03.440 --> 27:06.040
to get that sort of more intimate experience.

27:06.040 --> 27:12.920
Yeah, I feel that there's like, I'm struggling with the right language to describe this problem

27:12.920 --> 27:13.920
space.

27:13.920 --> 27:17.680
And maybe it's because it's so new we don't have it yet, but, you know, if you or if anyone

27:17.680 --> 27:21.800
else is listening to this, you know, does have it, you know, getting touched, but I guess

27:21.800 --> 27:28.760
it strikes me that there are different classes of use case here, like the face ID and voice

27:28.760 --> 27:36.760
ID strikes me as like, uh, it's a relatively simple use case, uh, you know, where you,

27:36.760 --> 27:42.160
you kind of train this classifier and it's, I don't know, the word monolithic is, it comes

27:42.160 --> 27:43.160
to mind.

27:43.160 --> 27:47.360
You know, but it seems like there are, there's, these other use cases, maybe more like

27:47.360 --> 27:53.640
Gmail, where what you want to do is kind of akin to some kind of hierarchical model where

27:53.640 --> 28:00.720
you've got like the, it's almost like a edge transfer learning kind of thing where you've

28:00.720 --> 28:05.280
got the met the master model, but then you want to fine tune that model based on, you

28:05.280 --> 28:08.880
know, a set of data that is available on the device.

28:08.880 --> 28:13.960
And maybe, maybe blurring a bunch of lines here because my Gmail data is all in the cloud.

28:13.960 --> 28:19.920
Um, uh, but the independent of where the, the data is, there's like a data set that's

28:19.920 --> 28:26.640
purse, that's very personal and then a model that's based on a bunch of people's data.

28:26.640 --> 28:30.320
So Google talked a bit about this kind of approach.

28:30.320 --> 28:34.800
It's not a new idea, but I think, you know, it may be closer to being put into practice.

28:34.800 --> 28:41.880
They discuss a little bit at Google IO about, for example, um, G board, um, and, uh, this

28:41.880 --> 28:44.880
I, you know, the federated learning and how do you do federated learning?

28:44.880 --> 28:49.120
And so what I think you just described in some senses a federated learning use case,

28:49.120 --> 28:50.120
right?

28:50.120 --> 28:53.720
And the example they gave, I thought was, was very, uh, very interesting, you know, you

28:53.720 --> 28:58.160
could think about it like crowdsourcing and it is that, uh, but then, you know, applying

28:58.160 --> 29:04.200
machine learning to it, which is if some new term gets hot and a lot of users start

29:04.200 --> 29:10.760
tweeting a hashtag or some new, you know, term gets coined because of news or politics

29:10.760 --> 29:12.040
or who knows what?

29:12.040 --> 29:16.160
Then, you know, in today, you might have to type that, I don't know, pick a number 50 times

29:16.160 --> 29:20.720
or something before your learning keyboard goes, you know, this guy types his word a lot

29:20.720 --> 29:21.720
over and over and over again.

29:21.720 --> 29:23.040
Maybe I should remember it, right?

29:23.040 --> 29:24.040
Right.

29:24.040 --> 29:26.200
And if you could crowdsource in some sense, right?

29:26.200 --> 29:33.120
If you could like update the predictive model by sourcing and we've done some research

29:33.120 --> 29:38.640
here in privacy preserving, uh, federated learning, um, in a privacy preserving kind

29:38.640 --> 29:45.800
of way, then you can, ideally, you know, climb that sort of hill in terms of the importance

29:45.800 --> 29:48.080
of new terms much more quickly, right?

29:48.080 --> 29:49.600
Maybe in hours or days.

29:49.600 --> 29:52.360
Oh, yeah, that's super interesting, right?

29:52.360 --> 29:57.480
And I think that that kind of application is going to be, and now this is, we're drifting

29:57.480 --> 30:02.640
away from personalization into privacy distributed learning and so on, but there are interesting

30:02.640 --> 30:07.760
applications, I think in like healthcare, for example, uh, similarly, um, you know,

30:07.760 --> 30:14.960
my individual experience, um, is like a tiny, tiny fraction of the collective experience

30:14.960 --> 30:21.120
of a lot of users using the same medicine or with the same symptoms or, you know, whatever

30:21.120 --> 30:22.120
it is, right?

30:22.120 --> 30:28.320
If I can aggregate that data in a privacy preserving, you know, assured kind of way, then

30:28.320 --> 30:33.680
I get a benefit if I can share my experience with others and I can get back personalized

30:33.680 --> 30:36.880
recommendations or I can be told, hey, look, your symptoms are a lot like this other

30:36.880 --> 30:41.800
person over here and we just diagnosed as other person with such and so, um, you know,

30:41.800 --> 30:48.000
you can think about other, you know, life-improving sorts of experiences that rely essentially

30:48.000 --> 30:53.120
on the same property, which is an aggregation of a lot of individual pieces of data and

30:53.120 --> 30:58.120
you could get a personalized in some sense, you know, experience out of that.

30:58.120 --> 31:03.240
So we're talking about inference before, yeah, I got a little feel sorry about that.

31:03.240 --> 31:06.040
No, no, it's awesome.

31:06.040 --> 31:12.480
But I think that led us to, led us to personalization.

31:12.480 --> 31:18.520
One of the questions that I had, I was at the launch for the Cloud AI100 back in April,

31:18.520 --> 31:25.520
which is for those that don't know or don't recall Qualcomm's kind of foray into data

31:25.520 --> 31:37.400
center inference chips or systems and I'm curious is, you know, to what degree, you know,

31:37.400 --> 31:44.480
your teams are starting to think about how all of this, you know, software stack applies,

31:44.480 --> 31:48.920
you know, similarly or differently to the data center devices.

31:48.920 --> 31:50.840
Sure, sure, yeah.

31:50.840 --> 31:58.120
So work with that team a little bit, I think the, that's a complicated, it's a complicated

31:58.120 --> 31:59.120
answer.

31:59.120 --> 32:05.720
So again, like the question we discussed earlier about what does it mean to, you know,

32:05.720 --> 32:10.080
what is a recommendation for when somebody wants to run inference on an edge device and

32:10.080 --> 32:12.680
do they start in TensorFlow or PyTorch?

32:12.680 --> 32:19.600
All those questions and then some come up in the data center because of course you have

32:19.600 --> 32:25.960
a really expensive capital, you know, investment in a data center and a lot of these kinds

32:25.960 --> 32:31.360
of customers are committed to PyTorch or they're committed to TensorFlow or they're committed

32:31.360 --> 32:34.760
to a proprietary framework.

32:34.760 --> 32:42.480
And so we have very analogous issues in order to enable that server chip in the data center

32:42.480 --> 32:43.880
and to provide our customers.

32:43.880 --> 32:45.840
Again, the goal is the same.

32:45.840 --> 32:51.760
You pick your training system, let's say TensorFlow and we want to provide a high performance,

32:51.760 --> 32:57.160
in this case, inference in the first generation product at, you know, high density, lower

32:57.160 --> 33:01.520
power, all the things that you heard about in the pitch and how do we do that kind of

33:01.520 --> 33:03.800
in a frictionless sort of way.

33:03.800 --> 33:06.920
And so let me make it maybe a few points here.

33:06.920 --> 33:13.120
One is the problems are analogous and so we can share our experiences and we do between

33:13.120 --> 33:20.040
how we've seen the edge inference dynamics evolve as the marketplaces become somewhat

33:20.040 --> 33:26.520
more mature and, you know, what kinds of questions and needs customers will have as they bring

33:26.520 --> 33:28.200
their problems to these devices.

33:28.200 --> 33:33.200
So that's one area where I think it's again, it's very analogous and our goal will be

33:33.200 --> 33:39.560
very similar which is provide the lowest friction path that we can for people to bring their

33:39.560 --> 33:43.280
cloud inference tasks to our devices.

33:43.280 --> 33:48.880
But linking a lot of stuff that Qualcomm says does does well and we're invested in and

33:48.880 --> 33:55.120
thinking about Qualcomm as a company that does really well with end end very high complexity

33:55.120 --> 33:56.600
system problems, right?

33:56.600 --> 34:02.840
When you think about a wireless system, it's not just the device, the chip in the handset,

34:02.840 --> 34:08.240
it's the standards, it's the radio, you know, protocols, it's what happens at the base

34:08.240 --> 34:11.920
station versus what happens at the edge device, the whole thing.

34:11.920 --> 34:17.080
When you kind of zoom out and you say, how is this going to fit with AI, then you have

34:17.080 --> 34:23.640
a really, really very complicated and very interesting situation that I think few companies

34:23.640 --> 34:28.800
in the world are really well positioned to sort of look at the whole thing and that's

34:28.800 --> 34:33.600
if I've got an inference chip in the cloud or at the, let's call it heavy edge of the

34:33.600 --> 34:38.560
radio access network and I've got a bunch of edge devices, whether it's their phones

34:38.560 --> 34:45.080
or their IoT devices or their cars and I connect it all with 5G or Wi-Fi or both, now

34:45.080 --> 34:46.320
what happens, right?

34:46.320 --> 34:49.560
And so these are the kinds of problems we're starting to think about, how do we enable

34:49.560 --> 34:55.320
customers to apportion their use case between their edge device, their, you know, maybe edge

34:55.320 --> 34:58.760
compute and the cloud, they're going to have different latencies.

34:58.760 --> 35:02.600
The closer you are to the edge of the 5G network, you have very, very low latencies, super

35:02.600 --> 35:03.600
high bandwidth.

35:03.600 --> 35:09.120
So maybe you can do things like AR and VR split rendering, very easily, right?

35:09.120 --> 35:14.920
So you have lighter weight, you know, headsets that take advantage of massive compute, not

35:14.920 --> 35:16.480
that far away.

35:16.480 --> 35:21.320
Maybe similarly you can do this kind of stuff with, you know, voice translation and other

35:21.320 --> 35:22.920
kind of hard use cases.

35:22.920 --> 35:28.680
You do a certain amount of it on device and then you send the rest of it over 5G to another

35:28.680 --> 35:32.520
inference solution that's not so far away in terms of latency.

35:32.520 --> 35:36.600
These are the kinds of problems we're starting to think about to kind of go beyond just,

35:36.600 --> 35:42.040
okay, how do we enable our customers to run on our devices in the cloud and in the edge,

35:42.040 --> 35:46.400
but then how do we help them start to assemble, you know, real systems that take advantage

35:46.400 --> 35:49.200
of all of these building blocks that we have to offer them?

35:49.200 --> 35:52.920
Yeah, as we've talked about on the software side, it's not just Qualcomm, like there's

35:52.920 --> 35:56.560
tons of initiatives.

35:56.560 --> 36:03.520
One of them that we talked a little bit about before we started rolling here was TinyML.

36:03.520 --> 36:04.960
What's going on with that?

36:04.960 --> 36:08.800
What is it and what are the goals there?

36:08.800 --> 36:14.480
We had a first ever, hopefully annual, I think it'll be annualized, a conference, code

36:14.480 --> 36:20.080
shared between Qualcomm and Google, a couple months back, the TinyML conference and you

36:20.080 --> 36:24.720
can look at for it and LinkedIn and there's some, you know, websites and so on that we

36:24.720 --> 36:26.540
can link you to.

36:26.540 --> 36:34.520
And the idea there is to think about really low power, really simple devices, right?

36:34.520 --> 36:40.200
And so for example, working with Pete Warden at Google, he's made sort of this recent push

36:40.200 --> 36:49.580
on sort of microcontroller-sized devices to run ML and again, at the TensorFlow summit

36:49.580 --> 36:54.160
a few months back, he showed a real simple, you know, microcontroller-powered board that

36:54.160 --> 36:58.780
could do a wake word detection and so on and very, very small footprint.

36:58.780 --> 37:04.280
So this is a like a logical extension of the work that we're already doing, you know,

37:04.280 --> 37:08.380
moving, you know, all the way from the server we were just talking about out to mobile phones

37:08.380 --> 37:14.640
and then out even farther to, you know, microcontroller-sized devices, whether it's a sensor, temperature

37:14.640 --> 37:20.160
sensor, you know, some of the kind of really simple device, you know, what does it take

37:20.160 --> 37:26.740
to do machine learning there, can you run TensorFlow ultra light there, can you do it in, you

37:26.740 --> 37:30.380
know, 30, 40, 50K of memory, you know, how many MIPS does it take?

37:30.380 --> 37:35.780
So this requires innovation in the software frameworks, you know, how do you actually,

37:35.780 --> 37:40.580
you know, build a library that's that small that can do something meaningful and Google

37:40.580 --> 37:43.820
and others, they're working with, are making progress there.

37:43.820 --> 37:45.460
What kind of hardware should you have?

37:45.460 --> 37:50.700
So you think about, again, these really simple low power, maybe you want to device, it'll

37:50.700 --> 37:56.740
run a year or two years or three years on a battery, right, can you do that?

37:56.740 --> 37:57.980
This is the direction that we're doing.

37:57.980 --> 38:05.020
We're partnering with, you know, academia and with people like Pete Warden and his team

38:05.020 --> 38:11.020
at Google and starting to explore this and over the course of time, this will lead us

38:11.020 --> 38:16.060
to hardware innovations and software innovations to solve, you know, machine learning problems

38:16.060 --> 38:19.380
on these really, really small devices.

38:19.380 --> 38:25.420
It also brings me to another point I wanted to mention that kind of going back to my roots

38:25.420 --> 38:32.180
on around Qualcomm AI research and the product side is a lot of our research focus is on

38:32.180 --> 38:33.500
these kinds of problems.

38:33.500 --> 38:38.860
How do you compress a network, quantize a network, you know, compactify it, if you will,

38:38.860 --> 38:44.380
in some way, and how do you make the software systems, the frameworks, what you call the

38:44.380 --> 38:48.420
middleware, I think it was a good term, how do you make those lean and small so they don't

38:48.420 --> 38:51.740
take a ball on a memory and they provide a lot of computation.

38:51.740 --> 38:56.300
And then how do you innovate on the hardware so that it all goes together as a system?

38:56.300 --> 39:01.660
So I've got a compression scheme that leverages something I'm doing on a hardware that takes

39:01.660 --> 39:06.500
advantage of, you know, how my frameworks are set up and then how do I go from TensorFlow

39:06.500 --> 39:12.140
or PyTorch through that whole chain of events to get on to a device to do a meaningful

39:12.140 --> 39:17.300
amount of AI in a power constrained kind of situation?

39:17.300 --> 39:23.820
I guess you said something that suggested the middleware being on device or taking up device

39:23.820 --> 39:31.300
resources versus being part of the development stack, you know, that's running on the, you

39:31.300 --> 39:36.460
know, the developer workstation is to what degree is that the case that, you know, as

39:36.460 --> 39:44.700
you're layering all of these elements, you know, they're all contributing to kind of

39:44.700 --> 39:47.780
the resource taking, you know, using resources on device.

39:47.780 --> 39:49.980
Yeah, that's a great question.

39:49.980 --> 39:52.900
So it's an area we focus on a lot.

39:52.900 --> 39:58.900
And again, I want to highlight for your listeners, I think a really interesting kind of

39:58.900 --> 40:03.380
area of innovation, something to keep an eye out on, is going back to this question

40:03.380 --> 40:09.100
of an inference, it has been mostly the case that the pipeline has looked something like

40:09.100 --> 40:14.300
you training your favorite framework, and this is true, not just of our frameworks, but

40:14.300 --> 40:18.980
the other SSEs in the marketplace have very similar strategies, you training your favorite

40:18.980 --> 40:23.140
framework, you do some amount of quantization, compression, optimization, call it what

40:23.140 --> 40:29.140
you want, and you deploy to your edge device and that middleware, that runtime, whether

40:29.140 --> 40:35.580
it's TensorFlow Lite or it's a proprietary one, has to take that graph, that model, and

40:35.580 --> 40:37.020
interpret it in some way.

40:37.020 --> 40:42.860
So whether it arrives in an open format like TensorFlow Lite's format or it's a proprietary

40:42.860 --> 40:47.220
format, you have to read it, you have to deploy it to your hardware or accelerated whatever

40:47.220 --> 40:51.220
run it over your blast library, whatever your acceleration mechanism is.

40:51.220 --> 40:55.780
And so basically you're interpreting this graph on the fly, which is really handy if

40:55.780 --> 41:01.260
you're an app developer, let's say, and you want to provide an in-store app that has

41:01.260 --> 41:06.860
machine learning, very, very hard to predict, you know, to build an app that's tailor-made

41:06.860 --> 41:12.580
for Snapdragon, this chip or whatever, you really want a pretty general purpose toolkit,

41:12.580 --> 41:16.940
and that's what something like Android and an API provides.

41:16.940 --> 41:20.820
But if I want to hit a microcontroller or I want to hit a purpose built device, like

41:20.820 --> 41:25.660
it's a home speaker, I know what it will do, it will listen to wake words, it'll do translation

41:25.660 --> 41:29.100
or whatever, and it'll talk to the internet for the things they can't do, talk to the

41:29.100 --> 41:30.100
cloud.

41:30.100 --> 41:37.020
I have a very, you know, price-conscious sort of customer, and the bomb is really important.

41:37.020 --> 41:38.540
I want high-performance.

41:38.540 --> 41:39.540
Bomb-gillematerials.

41:39.540 --> 41:40.540
Billematerials, right?

41:40.540 --> 41:41.540
Yeah.

41:41.540 --> 41:44.660
Yeah, how much it costs them to manufacture it, and how much a consumer is willing to spend

41:44.660 --> 41:49.220
to buy, you know, ten of them or five of them at their house, let's say.

41:49.220 --> 41:55.620
And there, what we're heavily involved in now, let's say, particularly for our hexagon

41:55.620 --> 42:03.740
on DSP, is compiler technology, making our middleware a lot more modular so that you can

42:03.740 --> 42:11.340
pull out of the accelerator library just the parts you need for your use case, and compilers,

42:11.340 --> 42:13.980
and making these things all work in conjunction with each other.

42:13.980 --> 42:21.940
So to long answer your question, we're, I think the future world will be a little bit separated.

42:21.940 --> 42:26.540
You'll have these cases where I won't know in advance what my model needs to do.

42:26.540 --> 42:31.900
This is the app store use case, and I need to provide middleware that's as lean as possible,

42:31.900 --> 42:35.100
but yet provides a very generic experience.

42:35.100 --> 42:40.540
So I can essentially run any network that a customer might deploy to my device, and another

42:40.540 --> 42:48.980
world of these from microcontroller up to some, you know, larger device where I have the

42:48.980 --> 42:53.300
network in advance, I know exactly what it needs to do, and so therefore I can spend a lot

42:53.300 --> 42:56.820
of time compiling and optimizing it like you would with a piece of software.

42:56.820 --> 43:03.500
I can distill the network in some way, both architecturally like compressing it or quantizing

43:03.500 --> 43:09.700
it, but then my middleware then is assembled, if you will, based only on the operators that

43:09.700 --> 43:12.300
that network requires for that use case, right?

43:12.300 --> 43:17.700
And so I end up with a very compact, but still high performance experience, right?

43:17.700 --> 43:22.700
So these are other areas in which we're making investments and working with the ecosystem

43:22.700 --> 43:27.700
to innovate, and you can expect, over the course of time, for us to provide tooling that

43:27.700 --> 43:33.620
will allow our customers to essentially compile their networks into libraries that are

43:33.620 --> 43:37.180
sort of custom built for their use case.

43:37.180 --> 43:43.860
Do you envision a future where maybe, you know, the answer is that this has been happening

43:43.860 --> 43:54.620
for years, but as opposed to the step beyond kind of compiling down to standard chipsets

43:54.620 --> 44:02.860
or SOCs might be compiling kind of the neural network down to some kind of HGL that can

44:02.860 --> 44:10.780
be implemented via FPGA or some other kind of hardware description that can then be produced

44:10.780 --> 44:17.780
so that the Silicon itself is optimized for these super cost-sensitive high-volume applications

44:17.780 --> 44:19.340
that you described earlier?

44:19.340 --> 44:20.340
Yeah, sure.

44:20.340 --> 44:29.060
I think, so I got a good answer for you, which is your listeners over to the TVM stack

44:29.060 --> 44:30.260
from University of Washington.

44:30.260 --> 44:33.540
It's now a patchy project, so you can find it there.

44:33.540 --> 44:38.140
The idea there is, in short, there's a lot of really good innovation happening there,

44:38.140 --> 44:43.900
but one of their outputs is, for example, a capacity to generate essentially custom hardware

44:43.900 --> 44:48.460
for the TVM program, if you will, that it's compiling.

44:48.460 --> 44:56.140
So that is already starting to happen, and people like Microsoft today already deploy FPGAs

44:56.140 --> 45:01.180
into their data centers, I don't know exactly what models they run, but targeting specific

45:01.180 --> 45:02.180
models.

45:02.180 --> 45:06.740
Again, so it's happening, and I think we can well expect for certain use cases for

45:06.740 --> 45:08.260
this to continue to happen.

45:08.260 --> 45:15.140
The trade-off, of course, is that you're making a strong commitment to essentially do a

45:15.140 --> 45:16.140
piece of hardware.

45:16.140 --> 45:21.820
Now, an FPGA gives you flexibility, but you're really committing yourself to a particular

45:21.820 --> 45:27.460
instantiation of a network in a market that is highly dynamic, right?

45:27.460 --> 45:28.460
Yeah.

45:28.460 --> 45:29.460
People are thinking that it works all the time.

45:29.460 --> 45:31.300
So that's the trade-off, right?

45:31.300 --> 45:35.220
And so that's the balance we're trying to strike is designing, for example, our tensor

45:35.220 --> 45:37.900
accelerator hardware.

45:37.900 --> 45:41.060
We've got a first generation of the marketplace now, and you can well expect it to continue

45:41.060 --> 45:46.580
to evolve, where we're trying to find that point where we provide very high performance

45:46.580 --> 45:49.820
experiences and the right amount of flexibility.

45:49.820 --> 45:52.820
So going back to what I was saying earlier, OK, I'm an app developer.

45:52.820 --> 45:55.660
I want maximum flexibility on my platform.

45:55.660 --> 46:01.500
I want to run any neural network that has hundreds of operators in it.

46:01.500 --> 46:02.900
OK, that's one use case.

46:02.900 --> 46:06.540
And I want to take my same basic design and workflow.

46:06.540 --> 46:11.980
And I want to design a home speaker, let's say, or a thermostat or whatever.

46:11.980 --> 46:14.620
And I got a much more limited set of use cases.

46:14.620 --> 46:18.580
But I also have more limited computer, more limited memory.

46:18.580 --> 46:19.580
Can we serve both?

46:19.580 --> 46:26.300
And so the compiler, the distiller, will serve the IoT, or embedded, or purpose-built

46:26.300 --> 46:28.140
device use case.

46:28.140 --> 46:32.740
And the same accelerator building blocks that we're building and the same accelerator hardware

46:32.740 --> 46:38.060
will also be available for your smartphone, for your automobile, whatever, where it's not

46:38.060 --> 46:41.580
so clear that the exact use case is kind of boiled down.

46:41.580 --> 46:46.940
And so instead of focusing on, say, FPGA's in a home speaker, I think the direction will

46:46.940 --> 46:51.660
be how do we use tiny ML techniques, how do we use compression, how do we innovate in

46:51.660 --> 46:58.700
the hardware so that we can provide this high performance experience, and a design sort

46:58.700 --> 47:05.020
of approach that meets the sort of memory, power, constraints, cost point for these use

47:05.020 --> 47:06.020
cases?

47:06.020 --> 47:10.940
Well, more thing that I wanted to be sure to cover, and it's related to everything that

47:10.940 --> 47:15.620
we've been talking about, Qualcomm's been pretty active in the Onyx community.

47:15.620 --> 47:22.300
How does that play with all of the other ecosystems that we've been discussing?

47:22.300 --> 47:27.340
And we can take a step back and describe for folks that don't know it Onyx, and maybe

47:27.340 --> 47:30.260
kind of an update on what's been going on with it.

47:30.260 --> 47:31.260
That's good.

47:31.260 --> 47:32.260
That's a good foundation.

47:32.260 --> 47:39.460
So Onyx, I don't know, started maybe roughly a year and a half ago, something like that,

47:39.460 --> 47:45.300
set forth by Microsoft, Facebook, and Amazon, and we were asked at the very early stages

47:45.300 --> 47:51.460
to join, and we were, I think the first mobile SSC vendor to join, and the first mobile

47:51.460 --> 47:59.300
SSC vendor to make Onyx converters into our accelerator framework shipping product.

47:59.300 --> 48:06.180
So, real deep sort of interaction with Onyx, but what is the sort of idea?

48:06.180 --> 48:10.260
The idea was to provide an open standard, if you will.

48:10.260 --> 48:14.980
It's an open source project, interchange format, so that going back to your question about

48:14.980 --> 48:20.140
what do we recommend to our customers in terms of how to bring inference to devices?

48:20.140 --> 48:27.060
Well, again, going back maybe 12 or 18 months, the issue was that all these training frameworks

48:27.060 --> 48:31.460
had different paradigms, different file formats, different ways of interchanging data.

48:31.460 --> 48:36.940
So Onyx's kind of first kind of principal idea was, okay, let's define an interchange format,

48:36.940 --> 48:43.700
so I can move easily from PyTorch to Cafe, or PyTorch to TensorFlow, and so on.

48:43.700 --> 48:48.300
And so by and large, I think it's achieved that translation between these frameworks that

48:48.300 --> 48:52.740
have different sets of operators and different assumptions is always a bit tricky.

48:52.740 --> 48:58.340
But we do have customers that come to us and want to run Onyx produced models.

48:58.340 --> 49:02.340
And the nice thing is that it's helped with this funnel I talked about earlier.

49:02.340 --> 49:07.140
A lot of frameworks, chain, or others, have adopted Onyx support.

49:07.140 --> 49:13.500
And so now the fact that we support Onyx as one of the on-ramps to our SOCs means that

49:13.500 --> 49:18.820
we've actually opened up the number of training frameworks our customers can use.

49:18.820 --> 49:22.940
So long as they can come through that Onyx kind of gateway.

49:22.940 --> 49:32.860
And so we're seeing quite a bit of interest in using that as an on-ramp to our products.

49:32.860 --> 49:35.860
And we're also chairing the Onyx Edge working group.

49:35.860 --> 49:42.620
And so the idea there is to define in broad strokes what this goes also back to the data

49:42.620 --> 49:43.620
center discussion.

49:43.620 --> 49:49.580
We had the set of operators you want in a data center generally a lot richer than the

49:49.580 --> 49:53.940
set of operators that you need in practice on an edge device, whether that's a mobile

49:53.940 --> 49:55.420
device or IoT device.

49:55.420 --> 50:02.820
And so part of the work of the edge working group is to define sort of a subset, a strict

50:02.820 --> 50:09.300
and defined subset of the full set of Onyx operators that we define as sort of a conformance

50:09.300 --> 50:10.540
set for edge.

50:10.540 --> 50:14.980
So if you said I'm Onyx Edge 1.0 although we don't really coin this term but think about

50:14.980 --> 50:21.060
it that way, compliant, then I support these 65 operators and they work in this way and

50:21.060 --> 50:23.860
we can write a sort of compliance test around it, right?

50:23.860 --> 50:29.620
And then interoperability becomes a lot more meaningful because today one of the issues

50:29.620 --> 50:34.940
when somebody says that their Onyx compliant is okay which of the 130 plus operators

50:34.940 --> 50:40.380
whatever the number is today, do you actually support and is that combination meaningful

50:40.380 --> 50:43.940
for use cases that you want to deploy to the edge?

50:43.940 --> 50:44.940
Right.

50:44.940 --> 50:45.940
So that's our involvement.

50:45.940 --> 50:53.740
Again, the big theme here is we want to be able to talk about what does compliance mean,

50:53.740 --> 50:58.260
what does it mean to support AI at the edge?

50:58.260 --> 51:02.900
How do you make it reduce the friction for our customers to do that, right?

51:02.900 --> 51:08.420
We've covered a lot of ground, a lot of very fun digressions, I would say.

51:08.420 --> 51:10.900
Anything else that you wanted to make sure we covered?

51:10.900 --> 51:16.780
Look, I just appreciate the opportunity to be on the show and to on your podcast and

51:16.780 --> 51:20.340
to reach out to your audience.

51:20.340 --> 51:28.340
I'm really excited about where things are going, a bit of a pitch I think maybe for the

51:28.340 --> 51:32.580
company and proud of the company and what we do.

51:32.580 --> 51:38.380
This is a big systems problem, AI, I view it as a real paradigm shift in computing

51:38.380 --> 51:43.500
and it's a big systems problem, but that I mean that I don't think you can just look

51:43.500 --> 51:51.020
at just the edge or just a piece of silicon or just a piece of middleware and deduce

51:51.020 --> 51:54.300
from that, what are the implications across the whole thing?

51:54.300 --> 52:00.260
This is a pervasive kind of technology shift we're undergoing and it's I think fundamentally

52:00.260 --> 52:05.420
going to change how we build computing systems and how we build all the systems that

52:05.420 --> 52:09.620
were in all automated devices that exist today and then all the ones that are going to

52:09.620 --> 52:11.220
be invented.

52:11.220 --> 52:17.140
When we think about this, when your listeners interact with your podcast, I'd encourage

52:17.140 --> 52:22.380
them to be thinking about you're going to have various experts and those experts rightfully

52:22.380 --> 52:25.660
so focus on a slice of this.

52:25.660 --> 52:30.420
When we think about in total an end to end system, I think it's really important if

52:30.420 --> 52:35.780
we're going to like our focus is on high performance, low power, let's say to put it in a sound

52:35.780 --> 52:41.620
bite, that requires huge amounts of innovation in the algorithms and architecture of networks

52:41.620 --> 52:46.900
in the hardware, in the middleware like we talked about, that's a big problem and that's

52:46.900 --> 52:51.740
basically the problem that we're trying to tackle that I'm deeply involved in and excited

52:51.740 --> 52:57.340
to see where this goes, takes a lot of people, we have to work with our ecosystem partners

52:57.340 --> 53:02.300
to make it happen, but really excited at the rate of progress we're seeing and just really

53:02.300 --> 53:06.780
excited about what's going to come as we enable more of these capabilities for people to

53:06.780 --> 53:07.780
innovate on.

53:07.780 --> 53:10.540
Well Jeff, thanks so much for being on the show.

53:10.540 --> 53:15.180
Oh, absolutely, thank you for the invitation, should do it again in some period of time

53:15.180 --> 53:20.500
and we can reflect back on what are these predictions came true and which ones fell flat?

53:20.500 --> 53:22.220
Absolutely, absolutely.

53:22.220 --> 53:23.220
Thanks so much.

53:23.220 --> 53:28.860
Okay, thank you, you have a good day.

53:28.860 --> 53:31.380
Alright everyone, that's our show for today.

53:31.380 --> 53:35.220
If you like what you've heard here, please do us a favor and tell your friends about the

53:35.220 --> 53:36.380
show.

53:36.380 --> 53:40.540
And if you haven't already hit that subscribe button yourself, make sure you do, so

53:40.540 --> 53:44.580
you don't miss any of the great episodes we've got in store for you.

53:44.580 --> 53:48.620
Thanks again to Qualcomm for their sponsorship of today's episode.

53:48.620 --> 53:53.200
Check them out at twomla.com slash Qualcomm.

53:53.200 --> 53:56.580
As always, thanks so much for listening and catch you next time.

