1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:33,960
I'm your host Sam Charrington, a quick update on our deep learning study group.

4
00:00:33,960 --> 00:00:38,760
As you know, we're huge fans of the fast.ai courses and we recently had our second group

5
00:00:38,760 --> 00:00:44,720
of deep learning learners complete the fast.ai deep learning for Kodo's course back in December.

6
00:00:44,720 --> 00:00:48,080
I'm excited to announce that we'll be hosting our third group of students taking the

7
00:00:48,080 --> 00:00:52,800
part one course starting this Saturday morning, February 2nd.

8
00:00:52,800 --> 00:00:57,040
This study group will run seven weeks finishing just in time for participants to jump right

9
00:00:57,040 --> 00:01:02,560
into the deep learning for Kodo's part two course, which is set to start in mid-march.

10
00:01:02,560 --> 00:01:10,160
For details on the study groups or to get registered, visit twimbleai.com slash meetup.

11
00:01:10,160 --> 00:01:14,160
While it nerfs this past December, I had the pleasure of attending the second annual

12
00:01:14,160 --> 00:01:18,840
Black and AI workshop in dinner, which brings in participants from all over the world to

13
00:01:18,840 --> 00:01:23,080
showcase their research, share experiences, and support one another.

14
00:01:23,080 --> 00:01:26,760
I was fortunate enough to spend the day at the workshop and I'm excited to share with

15
00:01:26,760 --> 00:01:31,200
you over the course of this month conversations with just a few of the great members of this

16
00:01:31,200 --> 00:01:32,600
community.

17
00:01:32,600 --> 00:01:38,080
To keep up with the series, visit twimbleai.com slash Black and AI 19.

18
00:01:38,080 --> 00:01:44,720
To get this series kicked off, we're joined by Randy Williams, PhD student at the MIT Media

19
00:01:44,720 --> 00:01:45,720
Lab.

20
00:01:45,720 --> 00:01:49,720
At the Black and AI workshop, Randy presented her research on pop-bots, an early childhood

21
00:01:49,720 --> 00:01:57,120
AI curriculum, which is geared toward teaching preschoolers the fundamentals of artificial intelligence.

22
00:01:57,120 --> 00:02:01,840
In our conversation, we discussed the origins of the project, the three AI concepts that

23
00:02:01,840 --> 00:02:06,840
are taught in the program, and the goals that Randy hopes to accomplish with her work.

24
00:02:06,840 --> 00:02:10,360
This was a fun conversation, it was super thought-provoking.

25
00:02:10,360 --> 00:02:12,360
Enjoy.

26
00:02:12,360 --> 00:02:17,920
All right, everyone, I am on the line with Randy Williams.

27
00:02:17,920 --> 00:02:21,840
Randy is a PhD student at the MIT Media Lab.

28
00:02:21,840 --> 00:02:24,480
Randy, welcome to this week in machine learning and AI.

29
00:02:24,480 --> 00:02:26,680
Hi, thank you Sam for inviting me.

30
00:02:26,680 --> 00:02:30,920
I'm happy to be here to talk to everyone about my work.

31
00:02:30,920 --> 00:02:31,920
Absolutely.

32
00:02:31,920 --> 00:02:37,800
So we had an opportunity to meet at NURBS recently, in fact, you presented at the Black

33
00:02:37,800 --> 00:02:48,120
and AI workshop there, and I was really fascinated by the work you're doing in teaching preschool

34
00:02:48,120 --> 00:02:51,920
children about artificial intelligence.

35
00:02:51,920 --> 00:02:58,360
What sparked your interest in doing that, in teaching, you know, young children about

36
00:02:58,360 --> 00:02:59,360
AI?

37
00:02:59,360 --> 00:03:06,200
Thanks, the workshop that was definitely incredible, and I was happy to share my work with the

38
00:03:06,200 --> 00:03:07,920
people there.

39
00:03:07,920 --> 00:03:11,480
So I am a PhD student at the Media Lab.

40
00:03:11,480 --> 00:03:15,800
I've been working on this project for about three years, and when it started, it wasn't

41
00:03:15,800 --> 00:03:19,920
about AI, and it wasn't necessarily about preschool children either.

42
00:03:19,920 --> 00:03:27,120
It was about computational thinking and how do we help students who might not have access

43
00:03:27,120 --> 00:03:30,200
to fancy robotic toolkits or to teachers?

44
00:03:30,200 --> 00:03:32,640
How do we help them start to learn about these things?

45
00:03:32,640 --> 00:03:39,920
How do we spread the influence of the whole craze about CS to different populations?

46
00:03:39,920 --> 00:03:44,120
So I personally am from, well, Prinshire just County, Maryland, but my family is from

47
00:03:44,120 --> 00:03:48,360
Baltimore, and I went to school in Baltimore, and while I was in undergrad, I spent a lot

48
00:03:48,360 --> 00:03:54,320
of time working with inner-city children and, you know, doing maker spaces or doing

49
00:03:54,320 --> 00:03:59,480
workshops, that kind of thing, and what was really awesome about it was how engaged students

50
00:03:59,480 --> 00:04:04,000
were with, you know, learning these different things, but it also made me a bit sad that,

51
00:04:04,000 --> 00:04:09,240
you know, so many of my classmates in school, they were like, oh, yeah, I've been programming

52
00:04:09,240 --> 00:04:12,560
since I was seven, and, you know, these students, you know, they were like high schoolers,

53
00:04:12,560 --> 00:04:14,760
and they're just getting started with this.

54
00:04:14,760 --> 00:04:20,280
So I just felt really strongly that, you know, there needs to be more done to help everyone

55
00:04:20,280 --> 00:04:24,280
have a chance to learn about these things early on, and it was really difficult in Baltimore

56
00:04:24,280 --> 00:04:28,520
was that there just weren't a lot of teachers, there weren't a lot of people who knew about

57
00:04:28,520 --> 00:04:32,920
the field to come and, you know, teach the kids and share the expertise.

58
00:04:32,920 --> 00:04:36,480
So within the group that I started working in, in the MIT Media Lab, it's called the

59
00:04:36,480 --> 00:04:41,600
Personal Robots Group, and my professor is in Theore Brazil, and she's really passionate

60
00:04:41,600 --> 00:04:46,880
about how AI robotics can help us flourish as human beings, and so a lot of our work

61
00:04:46,880 --> 00:04:50,240
has been about education and how robots play a role in that.

62
00:04:50,240 --> 00:04:55,320
So I started out just building a robot that could help children learn how to program and

63
00:04:55,320 --> 00:05:00,320
sort of, like, be the fun, interactive learning companion to help them, you know, figure things

64
00:05:00,320 --> 00:05:04,960
out and push them to solve new problems and things like that, so the absence of a trained

65
00:05:04,960 --> 00:05:10,480
teacher, how can a robot help children learn about these kinds of things?

66
00:05:10,480 --> 00:05:13,000
And it was really fun.

67
00:05:13,000 --> 00:05:17,360
I started thinking about, you know, like, so what makes the most sense, you know, having

68
00:05:17,360 --> 00:05:21,200
some experience in Baltimore, I was like, well, no one's going to go buy a $200 robot

69
00:05:21,200 --> 00:05:24,920
to do this, so how do we make something less expensive?

70
00:05:24,920 --> 00:05:28,680
So the pop-up project that I'm working on is mostly based around a mobile phone, so

71
00:05:28,680 --> 00:05:34,320
the mobile phone is the intelligent robot that children program, and then I was also thinking,

72
00:05:34,320 --> 00:05:40,040
well, how do we break away from computer sciences, you know, solving mases or doing puzzles

73
00:05:40,040 --> 00:05:44,880
and really open it up to different interests that students might have?

74
00:05:44,880 --> 00:05:49,400
So the robot, you know, can become a character, you can make it look like whatever you want,

75
00:05:49,400 --> 00:05:53,440
it can control the lights around your room and things like that, so it's sort of like

76
00:05:53,440 --> 00:05:58,720
opening doors for, you know, art and theater and music even.

77
00:05:58,720 --> 00:06:04,760
And then somewhere along the way, I was having a conversation with Cynthia one day about

78
00:06:04,760 --> 00:06:09,320
the project in the direction, and she was like, you know, Randy, it work is great, but

79
00:06:09,320 --> 00:06:12,440
you should really think about AI, because AI is the next big thing, and no one's really

80
00:06:12,440 --> 00:06:17,560
doing like AI education, and, you know, as a student, I'm like, okay, well, I've taken

81
00:06:17,560 --> 00:06:22,520
AI classes in college, I don't really know how I'm going to teach children AI, but yeah,

82
00:06:22,520 --> 00:06:25,800
sure, you know, let's go ahead and try it.

83
00:06:25,800 --> 00:06:32,600
So I quickly pivoted and how did I end up with preschool children?

84
00:06:32,600 --> 00:06:39,280
Well, there were a lot of like robot kits for like the seven to ten year old age, and

85
00:06:39,280 --> 00:06:43,600
I just, I guess, enjoy not sleeping and like solving really crazy problems.

86
00:06:43,600 --> 00:06:45,360
So I was like, I'm going to go like right below that.

87
00:06:45,360 --> 00:06:50,640
I'm going to do the five to seven year olds, and that's, it worked out, I guess.

88
00:06:50,640 --> 00:06:56,760
So we have a five to seven year old preschool AI toolkit that I'm working on.

89
00:06:56,760 --> 00:06:57,760
That's awesome.

90
00:06:57,760 --> 00:06:58,760
That's awesome.

91
00:06:58,760 --> 00:07:03,840
So is the Nielab, is that its own department, or are you affiliated with computer science

92
00:07:03,840 --> 00:07:11,840
or robotics department, and kind of what I'm also curious about here is, have you also

93
00:07:11,840 --> 00:07:18,440
received any formal training in education, and how do you kind of think about the interdisciplinary

94
00:07:18,440 --> 00:07:19,840
nature of your project?

95
00:07:19,840 --> 00:07:20,840
Yeah.

96
00:07:20,840 --> 00:07:23,480
So the Nielab is this weird crazy place.

97
00:07:23,480 --> 00:07:25,400
I work in a robotics lab.

98
00:07:25,400 --> 00:07:30,280
The people next to me, they do devices that go underneath your skin to help monitor

99
00:07:30,280 --> 00:07:33,640
your health, and the people on the other side of me do neurobiology.

100
00:07:33,640 --> 00:07:39,480
So it's a department with pretty much anyone who has a crazy idea that doesn't fit into

101
00:07:39,480 --> 00:07:43,240
like normal science or engineering department, you know, that's the place that you go to

102
00:07:43,240 --> 00:07:44,720
do work.

103
00:07:44,720 --> 00:07:49,800
So as a result, I get to do this project, this very interdisciplinary, I get to think

104
00:07:49,800 --> 00:07:58,040
about art, I get to go to schools and do education work, I get to do robots, and you know, I also

105
00:07:58,040 --> 00:08:01,960
have lots of resources around the lab where people do all of these things and can sort

106
00:08:01,960 --> 00:08:06,320
of help contribute to the project and help me grow my ideas.

107
00:08:06,320 --> 00:08:12,120
So I don't actually have formal training and education, however, a lot of the work

108
00:08:12,120 --> 00:08:16,200
that I'm doing is built on this program called Scratch.

109
00:08:16,200 --> 00:08:22,240
So Scratch is this website where children ages seven and up can go and learn about CS,

110
00:08:22,240 --> 00:08:29,040
and it was, I don't want to, so currently the leader of the lab is Mitchell Resnick, but

111
00:08:29,040 --> 00:08:35,320
I believe it started by, like, Seymour Pappert, like, you know, logo turtles.

112
00:08:35,320 --> 00:08:40,120
If you don't know what that is, it's these turtles where, you know, Seymour Pappert

113
00:08:40,120 --> 00:08:44,600
back in the 60s was like, all kids should know how to program, and all kids should, you

114
00:08:44,600 --> 00:08:45,600
know, be able to do this.

115
00:08:45,600 --> 00:08:49,120
And this is when like computers weren't even very popular, so everyone was like, dude, you're

116
00:08:49,120 --> 00:08:50,120
crazy.

117
00:08:50,120 --> 00:08:53,160
But, you know, he's like, yeah, I'm going to do it, and so he started building this

118
00:08:53,160 --> 00:08:57,960
programming language for children, and you know, generations later, there's this online

119
00:08:57,960 --> 00:09:02,800
portal where literally children, millions of children all over the world are learning

120
00:09:02,800 --> 00:09:04,240
how to program.

121
00:09:04,240 --> 00:09:07,440
So even though I didn't have the background, the CS education background to know how to

122
00:09:07,440 --> 00:09:11,680
do it, I got to work with Mitch, and I took his class, and I learned from his students

123
00:09:11,680 --> 00:09:14,160
and can start to pull those things in.

124
00:09:14,160 --> 00:09:17,600
I also didn't necessarily have a background in robotics.

125
00:09:17,600 --> 00:09:22,120
I did computer engineering and undergrad, but mostly, like, building little devices,

126
00:09:22,120 --> 00:09:25,720
not things that were, like, big and interacted with people.

127
00:09:25,720 --> 00:09:31,000
So I learned a lot from my group, and then everyone at the media lab, they're kind of just,

128
00:09:31,000 --> 00:09:32,280
you know, like artists.

129
00:09:32,280 --> 00:09:36,080
So I'm not an artist, I'm an engineer, but I became an artist, and it was able to pull

130
00:09:36,080 --> 00:09:37,080
that in.

131
00:09:37,080 --> 00:09:42,120
So really, what's awesome about interdisciplinary work is that you get to pull in from all

132
00:09:42,120 --> 00:09:46,040
of these different fields, you know, talking to developmental psychologists, can kids

133
00:09:46,040 --> 00:09:47,040
understand AI?

134
00:09:47,040 --> 00:09:48,920
Are they ready to do that yet?

135
00:09:48,920 --> 00:09:52,440
What are the right ways to translate the information so that it makes sense to them?

136
00:09:52,440 --> 00:09:57,040
I've really actually been inspired by all the people I've gotten to work with.

137
00:09:57,040 --> 00:10:01,960
So when you think about teaching AI to preschool kids, obviously, we're not trying to teach

138
00:10:01,960 --> 00:10:08,520
gradient descent algorithm or anything like that, like, how do you, how do you kind of

139
00:10:08,520 --> 00:10:12,360
break down, or maybe take it from the other direction?

140
00:10:12,360 --> 00:10:18,160
What are your goals in trying to teach AI to children at this age level?

141
00:10:18,160 --> 00:10:24,680
Yeah, I would say my primary goal is to give children agency and the world around them.

142
00:10:24,680 --> 00:10:30,200
So before I even, you know, put out this little kid and started, like, actually testing

143
00:10:30,200 --> 00:10:35,240
with children and building things, I did a whole series of studies with other people

144
00:10:35,240 --> 00:10:38,480
in my group around what do children think about AI?

145
00:10:38,480 --> 00:10:41,120
So we would have them interact with toys.

146
00:10:41,120 --> 00:10:44,120
If you look at kids' toys now, they're like amazing, they're really cool.

147
00:10:44,120 --> 00:10:49,200
So they have, like, these little robot things called Cosmo, which, like, they move around

148
00:10:49,200 --> 00:10:53,280
and they can play games against you and they're super cute and you can program them and

149
00:10:53,280 --> 00:10:54,280
stuff too.

150
00:10:54,280 --> 00:10:58,920
But, you know, it's like real AI that is being marketed to children.

151
00:10:58,920 --> 00:11:03,040
Then there's all this controversy about this Barbie doll, hello Barbie, that they talk

152
00:11:03,040 --> 00:11:08,400
to you and every kid in the world, I think, has had a conversation with Siri or Alexa,

153
00:11:08,400 --> 00:11:11,360
you know, not every kid in the world, but quite a few have.

154
00:11:11,360 --> 00:11:16,280
So it's interesting to see how in our time, you know, computers were just kind of, like,

155
00:11:16,280 --> 00:11:20,200
coming to be, and the internet was just coming to be, and children are growing up in a

156
00:11:20,200 --> 00:11:24,880
world now where it's like, yeah, AI is kind of a thing, like, it's normal to see that.

157
00:11:24,880 --> 00:11:30,120
So I was like, okay, so when a child interacts with this thing, that's not a life, not

158
00:11:30,120 --> 00:11:34,120
a human, but can talk to them and seems kind of smart, you know, what are they thinking?

159
00:11:34,120 --> 00:11:36,440
What's going on in their head?

160
00:11:36,440 --> 00:11:39,560
And oftentimes, they're, like, kind of just figuring it out.

161
00:11:39,560 --> 00:11:42,000
They're like, okay, it talks to me.

162
00:11:42,000 --> 00:11:43,000
Like, you know, let's poke it.

163
00:11:43,000 --> 00:11:48,040
We'll see if it can answer questions about dinosaurs or sloths, or does it know what's

164
00:11:48,040 --> 00:11:53,800
in the grocery store down the street, things like that, and then they're like, well, can

165
00:11:53,800 --> 00:11:54,800
I break it?

166
00:11:54,800 --> 00:11:59,320
Hey, do you want this apple, you know, asking these kinds of things to computers, and

167
00:11:59,320 --> 00:12:04,240
just just do what it will say, or, hey, do you have a boyfriend, do you have a girlfriend?

168
00:12:04,240 --> 00:12:08,840
Funny things like that, but even worse, so they kind of didn't really understand how

169
00:12:08,840 --> 00:12:09,840
it worked.

170
00:12:09,840 --> 00:12:14,720
And to me, that's like an opportunity and a challenge for today, because we don't really

171
00:12:14,720 --> 00:12:19,160
want children to have toys that they can't pick apart and understand.

172
00:12:19,160 --> 00:12:22,120
You know, they're like, I like so well, answer my question.

173
00:12:22,120 --> 00:12:23,120
Does she not like me?

174
00:12:23,120 --> 00:12:28,040
And it's like, no, Alexis, you know, NLP algorithm, just isn't programmed for young children's

175
00:12:28,040 --> 00:12:32,240
voices, because it was made by adults who thought only adults would be using it.

176
00:12:32,240 --> 00:12:35,440
So, you know, that's what's going on, but if you say that to a kid, they'll look at

177
00:12:35,440 --> 00:12:36,960
you like, what are you saying?

178
00:12:36,960 --> 00:12:39,200
I have no idea what's going on.

179
00:12:39,200 --> 00:12:43,520
So the goals of the curriculum are really to help children break those kinds of ideas

180
00:12:43,520 --> 00:12:44,520
down.

181
00:12:44,520 --> 00:12:51,400
Like, oh, Alexa isn't working because Alexa was trained a certain way, and if you try

182
00:12:51,400 --> 00:12:55,040
and have Alexa do things outside of the way that she was trained, then she's not going

183
00:12:55,040 --> 00:12:56,040
to get it.

184
00:12:56,040 --> 00:12:59,480
Like, that's kind of the right level that I think any child should have.

185
00:12:59,480 --> 00:13:02,920
It makes me think a little bit of a couple of examples.

186
00:13:02,920 --> 00:13:11,680
One that comes to mind is when children like, you know, who are raised on iPads, see magazines

187
00:13:11,680 --> 00:13:18,040
and start me capping at a magazine like wanting it to do something, or the other example

188
00:13:18,040 --> 00:13:23,320
that comes to mind is knowing how to really effectively search Google, it's a powerful

189
00:13:23,320 --> 00:13:24,320
skill.

190
00:13:24,320 --> 00:13:28,720
But, you know, both of these things, I think, illustrate, you know, like mental models

191
00:13:28,720 --> 00:13:34,880
that are created over time about this thing that you're interacting with, that, you

192
00:13:34,880 --> 00:13:38,720
know, in the case of an iPad, it's just like this piece of glass, but you kind of develop

193
00:13:38,720 --> 00:13:43,960
this model about like how, you know, flat things work, I guess, or, you know, in the case

194
00:13:43,960 --> 00:13:48,360
of a search box, like, you know, how do you can really effectively use, you know, this

195
00:13:48,360 --> 00:13:50,560
world that's behind the search box?

196
00:13:50,560 --> 00:13:56,400
It is part of your work here, trying to shift the mental model that kids have about AI.

197
00:13:56,400 --> 00:14:02,120
I actually really love that framing, and I might have to use that in the future, you know,

198
00:14:02,120 --> 00:14:09,480
like, we're in children's mental models about AI, yeah, so that is literally what I'm

199
00:14:09,480 --> 00:14:10,480
doing.

200
00:14:10,480 --> 00:14:16,320
So, a part of the actual pop-up study that I did, so I have children interact with the AI,

201
00:14:16,320 --> 00:14:20,360
and not just interact with it, I also have them, like, building algorithms from scratch,

202
00:14:20,360 --> 00:14:21,360
not the whole thing.

203
00:14:21,360 --> 00:14:24,960
They're not writing the programming, but pretty much they have a lot of control over the

204
00:14:24,960 --> 00:14:26,360
way the algorithm works.

205
00:14:26,360 --> 00:14:30,920
So, we do that, and at the end of the day, they have this finished AI product that seems

206
00:14:30,920 --> 00:14:36,240
intelligent, and before and after, they're learning about AI, ask them, what do you think

207
00:14:36,240 --> 00:14:37,960
about this thing that's in front of you?

208
00:14:37,960 --> 00:14:38,960
Do you think it's alive?

209
00:14:38,960 --> 00:14:39,960
Is it a person?

210
00:14:39,960 --> 00:14:40,960
Is it a toy?

211
00:14:40,960 --> 00:14:41,960
Is an adult?

212
00:14:41,960 --> 00:14:42,960
Is it a child?

213
00:14:42,960 --> 00:14:43,960
Is it smarter than you?

214
00:14:43,960 --> 00:14:44,960
Are you smarter than that?

215
00:14:44,960 --> 00:14:49,480
And I'm asking all of these questions, because I expect a mental model change to

216
00:14:49,480 --> 00:14:55,120
happen when children are learning about AI, and I'm wondering, you know, how can we make

217
00:14:55,120 --> 00:15:01,520
sure that, you know, there's so many privacy and security, like, safety concerns around

218
00:15:01,520 --> 00:15:04,120
having something that's always recorded you in the house.

219
00:15:04,120 --> 00:15:08,960
So, what are children's mental models, and how does learning about AI impact that?

220
00:15:08,960 --> 00:15:12,160
How does it change the way that they want to interact with these things in the future?

221
00:15:12,160 --> 00:15:17,920
And what were some of the results you saw in these before and after surveys?

222
00:15:17,920 --> 00:15:20,800
Mostly very strange things.

223
00:15:20,800 --> 00:15:28,120
So before, well, the studies that I did, like, two years ago, I interviewed four to ten

224
00:15:28,120 --> 00:15:33,160
year olds about AI, and the older kids, the eight to ten year olds, they were, like,

225
00:15:33,160 --> 00:15:34,160
solid.

226
00:15:34,160 --> 00:15:37,800
They knew exactly what they thought about AI, and, you know, they're like, it's not a

227
00:15:37,800 --> 00:15:41,400
person, it's not quite a toy, it's somewhere in the middle, like, they knew what they were

228
00:15:41,400 --> 00:15:42,400
doing.

229
00:15:42,400 --> 00:15:45,800
But the younger children, they kept telling me, I don't know, I don't know, I don't

230
00:15:45,800 --> 00:15:46,800
know.

231
00:15:46,800 --> 00:15:48,920
And I was getting frustrated, you know, it's a research, you're saying, like, I don't

232
00:15:48,920 --> 00:15:53,720
know, it's not going to get me a paper set into a conference.

233
00:15:53,720 --> 00:15:57,560
And so what I found before was pretty much the same children were like, I'm not really

234
00:15:57,560 --> 00:16:02,600
sure what this thing is, like, you know, we're all over the place.

235
00:16:02,600 --> 00:16:05,440
People saying, yes, people saying, no, people saying, oh, it said my name.

236
00:16:05,440 --> 00:16:09,320
So I guess it's pretty smart, or it didn't say my name, or it doesn't know my favorite

237
00:16:09,320 --> 00:16:10,320
song about the train.

238
00:16:10,320 --> 00:16:11,320
So it's not smart.

239
00:16:11,320 --> 00:16:16,120
You know, just things that were very hard to understand, which is interesting because

240
00:16:16,120 --> 00:16:19,400
these parents are also looking at this, and they're like, I'm not sure, but my child thinks

241
00:16:19,400 --> 00:16:22,200
about it, like, so you know, they keep calling it their best friends.

242
00:16:22,200 --> 00:16:25,720
So, you know, what is this relationship?

243
00:16:25,720 --> 00:16:29,520
But afterwards, there were some interesting differences.

244
00:16:29,520 --> 00:16:35,560
So I split my age grouping into, like, pre-K and kindergarten children.

245
00:16:35,560 --> 00:16:40,200
And so after they had learned about AI, the pre-K children were like, oh, now I understand

246
00:16:40,200 --> 00:16:41,200
it.

247
00:16:41,200 --> 00:16:42,640
So yeah, I would say this thing's pretty smart.

248
00:16:42,640 --> 00:16:46,080
And the kindergarten would children, like, oh, I don't think it's smart.

249
00:16:46,080 --> 00:16:47,080
anymore.

250
00:16:47,080 --> 00:16:48,080
Like, I thought it was smart before.

251
00:16:48,080 --> 00:16:52,160
But now that I get how it works, it's like, nope, like, okay.

252
00:16:52,160 --> 00:16:58,840
And then I also created, like, assessments, like, very simple, multiple choice questions

253
00:16:58,840 --> 00:17:04,440
to ask how much do children really understand about the activities that I've given them,

254
00:17:04,440 --> 00:17:07,520
and how much do they understand about the AI concepts?

255
00:17:07,520 --> 00:17:12,320
So the ones who did the best of the AI concepts were like, you know, not that I played with

256
00:17:12,320 --> 00:17:13,320
this thing.

257
00:17:13,320 --> 00:17:14,320
I'm like, yeah, it's kind of like a person.

258
00:17:14,320 --> 00:17:16,600
Like, it thinks sometimes in ways that I think.

259
00:17:16,600 --> 00:17:18,560
And yeah, it could be smarter than me, too.

260
00:17:18,560 --> 00:17:19,560
It could learn.

261
00:17:19,560 --> 00:17:21,920
It can get better and better.

262
00:17:21,920 --> 00:17:26,440
But interestingly, the children who didn't understand the activities very well were the

263
00:17:26,440 --> 00:17:27,440
opposite.

264
00:17:27,440 --> 00:17:28,440
They're like, nope, not smarter than me.

265
00:17:28,440 --> 00:17:29,600
It's just a toy.

266
00:17:29,600 --> 00:17:35,840
You know, it's fun, but it doesn't seem to be as alive or as human to me.

267
00:17:35,840 --> 00:17:43,040
And so I'm still undecided about what conclusions I want to draw from that.

268
00:17:43,040 --> 00:17:45,360
But I definitely think it's interesting that there is a big difference.

269
00:17:45,360 --> 00:17:49,200
Like, the children who understood AI versus the children who didn't understand AI saw

270
00:17:49,200 --> 00:17:52,880
the technology in very different ways.

271
00:17:52,880 --> 00:17:58,160
So at the very least, it sounds like teaching children about AI does cause something interesting

272
00:17:58,160 --> 00:18:01,240
to happen, something interesting, and hopefully not negative.

273
00:18:01,240 --> 00:18:07,840
So it seems like a good motivator to continue with the work and continue to explore this.

274
00:18:07,840 --> 00:18:13,920
I can't help but think that teaching adults about AI would have the same positive effects.

275
00:18:13,920 --> 00:18:21,080
You know, you think about kind of some of the mass media coverage of AI and some of

276
00:18:21,080 --> 00:18:25,320
the, you know, hysteria is that you read about.

277
00:18:25,320 --> 00:18:30,840
They often kind of belive this, you just lack of understanding.

278
00:18:30,840 --> 00:18:36,640
And have you thought about creating an adult version of your curriculum?

279
00:18:36,640 --> 00:18:43,240
So I haven't thought about it myself, but sort of, you know, even to discuss with you,

280
00:18:43,240 --> 00:18:49,880
we're mentioning about the more that people understand the less scared they might be.

281
00:18:49,880 --> 00:18:55,440
I think that the way that, you know, AI conversations are just playing out in society right

282
00:18:55,440 --> 00:18:56,440
now.

283
00:18:56,440 --> 00:18:57,640
It's kind of like a little bit of both.

284
00:18:57,640 --> 00:19:03,080
So there are some people, I guess, like myself and people in my lab who are like, you

285
00:19:03,080 --> 00:19:05,840
know, look at AI, look at how much good it can do.

286
00:19:05,840 --> 00:19:10,840
We can use it to build, you know, this good thing, that good thing, this good thing.

287
00:19:10,840 --> 00:19:15,920
And I would say that we're experts, but then there are also people who are extremely

288
00:19:15,920 --> 00:19:16,920
wary of AI.

289
00:19:16,920 --> 00:19:22,720
So they're like, they see this technology coming and they see all the negative ways that

290
00:19:22,720 --> 00:19:23,720
it can be used.

291
00:19:23,720 --> 00:19:31,200
And they're like cautious to like, you know, completely repulsed by it, like, no, even

292
00:19:31,200 --> 00:19:34,600
if AI can do good things, we shouldn't build it because in the wrong hands, it can just

293
00:19:34,600 --> 00:19:37,680
be too powerful and too destructive.

294
00:19:37,680 --> 00:19:44,560
And so I think that any adult education, AI, I think would probably have an interesting

295
00:19:44,560 --> 00:19:48,440
time trying to wrestle with that, you know, not trying to push people in any particular

296
00:19:48,440 --> 00:19:52,800
direction, but to let them come up with their own interpretation of how they think the

297
00:19:52,800 --> 00:19:54,600
technology should continue.

298
00:19:54,600 --> 00:19:57,320
Yeah, that's a really interesting point.

299
00:19:57,320 --> 00:20:00,600
I definitely get where you're coming from.

300
00:20:00,600 --> 00:20:04,680
You know, it's almost like the thing that you're afraid of, is it really the thing you

301
00:20:04,680 --> 00:20:05,680
should be afraid of?

302
00:20:05,680 --> 00:20:06,680
That's true.

303
00:20:06,680 --> 00:20:11,400
You know, it's not that there's nothing to be afraid of or not even afraid, but worried

304
00:20:11,400 --> 00:20:12,400
about.

305
00:20:12,400 --> 00:20:19,200
There are, there are genuine concerns, but, you know, it's, it's not necessarily kind of

306
00:20:19,200 --> 00:20:22,360
this terminator scenario in the next five years.

307
00:20:22,360 --> 00:20:23,680
I can tell you, it's a roboticist.

308
00:20:23,680 --> 00:20:29,160
There will be no terminator, because robots love to break, like, they're just not going

309
00:20:29,160 --> 00:20:30,160
to work out.

310
00:20:30,160 --> 00:20:34,760
There could be a, what is the movie, her, like, the thing where it's like all of mine,

311
00:20:34,760 --> 00:20:41,080
like, that's a bit more likely, but I wish you guys no terminators are going to come.

312
00:20:41,080 --> 00:20:47,880
Let's maybe talk about the curriculum that you developed as a part of pop bots.

313
00:20:47,880 --> 00:20:52,720
What are the core AI concepts that you're trying to teach these children?

314
00:20:52,720 --> 00:20:59,720
So the three that I started with were knowledge based expert systems, supervised machine

315
00:20:59,720 --> 00:21:02,360
learning and generative AI.

316
00:21:02,360 --> 00:21:06,280
And I started with these because they seem to be the most relevant to what children

317
00:21:06,280 --> 00:21:07,280
were experiencing.

318
00:21:07,280 --> 00:21:12,440
So a lot of their simple toys were using these, you know, kinds of concepts in them, and

319
00:21:12,440 --> 00:21:17,080
I could easily make connections between, when you toyed as this, this is what's happening

320
00:21:17,080 --> 00:21:18,080
underneath it.

321
00:21:18,080 --> 00:21:22,800
And to me, that seemed like the most important things to teach children at first.

322
00:21:22,800 --> 00:21:25,600
So, you know, knowledge based systems.

323
00:21:25,600 --> 00:21:30,560
Let's go through each of those and maybe you can provide an example of the way a child

324
00:21:30,560 --> 00:21:39,520
might experience that in their kind of everyday toys and how you introduce that in the curriculum.

325
00:21:39,520 --> 00:21:44,280
Knowledge based systems for rule based expert systems or expert systems, they have multiple

326
00:21:44,280 --> 00:21:49,040
names, often come up in natural language processing.

327
00:21:49,040 --> 00:21:56,000
Now a lot of NLP also uses, you know, deep learning, but, you know, in the past, you know,

328
00:21:56,000 --> 00:21:59,720
back in the good old days, it came up a lot in natural language processing as well as

329
00:21:59,720 --> 00:22:05,040
medical diagnosis and even the video game characters that, you know, aren't the main characters

330
00:22:05,040 --> 00:22:10,360
but the ones that, I don't know, if you play a lot of RPG games when you're battling

331
00:22:10,360 --> 00:22:15,600
the random townsmen, like those kinds of characters were all controlled by knowledge based

332
00:22:15,600 --> 00:22:20,720
expert systems or rule based systems, and so it was easy to, you know, talk to children

333
00:22:20,720 --> 00:22:26,360
about, it's like, oh, when you're playing tic-tac-toe against your smart computer or when

334
00:22:26,360 --> 00:22:30,920
you're doing that video game or when you're talking to Alexa, there's probably a bit of

335
00:22:30,920 --> 00:22:35,880
this going on underneath and the way that we did the activity was we played rock paper

336
00:22:35,880 --> 00:22:36,880
scissors.

337
00:22:36,880 --> 00:22:41,800
So, rock paper scissors, you need knowledge, like there are rules, which is, you know,

338
00:22:41,800 --> 00:22:47,840
arts, scissors, paper beads, rock, et cetera, and so children would literally program these

339
00:22:47,840 --> 00:22:50,560
rules into the robot.

340
00:22:50,560 --> 00:22:54,760
All of the interfaces are completely picture-based because I was working with children who were

341
00:22:54,760 --> 00:22:58,560
so young that they couldn't read, so it was an interesting design challenge to say,

342
00:22:58,560 --> 00:23:04,760
okay, powerful AI, absolutely no words whatsoever, and like no math and stuff like that.

343
00:23:04,760 --> 00:23:11,640
So children pretty much put, like, pictures of a paper hand and then an air will like greater

344
00:23:11,640 --> 00:23:15,760
than sign and then a rock, which means paper beads rock.

345
00:23:15,760 --> 00:23:20,320
And so they put in all the rules and now the robot has its knowledge base and it can

346
00:23:20,320 --> 00:23:22,960
use that to make decisions about what to play.

347
00:23:22,960 --> 00:23:27,980
So when the child's actually playing against the robot, then the robot will kind of keep

348
00:23:27,980 --> 00:23:32,080
packing their moves and it'll use that to say, oh, well, I think that you're going

349
00:23:32,080 --> 00:23:37,680
to play paper next and you told me that scissors beats paper, so I'm going to play scissors

350
00:23:37,680 --> 00:23:43,400
and then it's revealed, you know, did the child actually play a paper and a lot of times,

351
00:23:43,400 --> 00:23:46,920
you know, after a couple rounds of the game, yeah, the robot's pretty good at guessing

352
00:23:46,920 --> 00:23:52,160
and they're like, oh my god, the robot got so smart, but what's really cool about that

353
00:23:52,160 --> 00:23:57,400
is it didn't start off smart, it actually starts off losing a lot and as it keeps going,

354
00:23:57,400 --> 00:23:58,400
it gets smarter.

355
00:23:58,400 --> 00:24:04,120
The children start to see how this intelligence didn't disappear, it was learned over time.

356
00:24:04,120 --> 00:24:08,120
Also the children gave the robot the rules to the game, so the children actually played

357
00:24:08,120 --> 00:24:13,880
at part in helping the robot become intelligent, what's even more funny is that the kids

358
00:24:13,880 --> 00:24:17,360
will be like, I'm going to cheat and they'll like switch all the rules around backwards

359
00:24:17,360 --> 00:24:21,440
and the robot will be like, well, I guess that paper beats scissors, so does that mean

360
00:24:21,440 --> 00:24:26,240
that I win and the child's like laughing their head off like, ha, ha, ha, I tricked you.

361
00:24:26,240 --> 00:24:31,680
But that's actually an interesting point because, you know, expert-based rule systems, one

362
00:24:31,680 --> 00:24:35,720
of the ethical issues is what if the rule that you're teaching isn't correct?

363
00:24:35,720 --> 00:24:40,120
So children even get to explore that idea like, okay, if I have a car that's driving by

364
00:24:40,120 --> 00:24:43,640
itself and I teach it the wrong rules, what happens?

365
00:24:43,640 --> 00:24:48,120
And you can see like this look of realization, like, you know, move over the child's face

366
00:24:48,120 --> 00:24:50,480
and they're like, oh my god, that would be so bad.

367
00:24:50,480 --> 00:24:54,120
I'm like, yes, like you're understanding, like, you know, the impact, the real world,

368
00:24:54,120 --> 00:25:00,840
the impact of AI. And then also, children will teach the robot how to react to winning,

369
00:25:00,840 --> 00:25:05,240
losing and getting a tie. So, you know, like, the robot can be a sore winner and like,

370
00:25:05,240 --> 00:25:09,840
every time it wins, it's like, ha, ha, win, you lose. And like, it makes the sparring

371
00:25:09,840 --> 00:25:14,880
sound. So that's like, one iteration that you can have it win humbly. It's like, oh,

372
00:25:14,880 --> 00:25:18,760
that was a good game. So children are also, you know, it's kindergarten, it's free

373
00:25:18,760 --> 00:25:25,120
school. Children also need to have some way that, you know, social interaction, social

374
00:25:25,120 --> 00:25:29,560
learning is coming into this as well. So that's a part of it too.

375
00:25:29,560 --> 00:25:38,080
I'm curious in this first part, the rules-based systems, when you, when the child was programming

376
00:25:38,080 --> 00:25:44,160
the robot, teaching the robot how to respond via these rules, were you also introducing

377
00:25:44,160 --> 00:25:49,120
some notion of probabilistic systems or responses?

378
00:25:49,120 --> 00:25:54,840
So in the sense that the robot was learning over time, what the child was most likely

379
00:25:54,840 --> 00:26:01,200
to do next, yes. It was a little bit tricky, because probability doesn't come up for

380
00:26:01,200 --> 00:26:09,400
a while in, like, early education, but we would make these rule trackers, or these game

381
00:26:09,400 --> 00:26:14,160
trackers, rather, where the children would write down, you know, what moves they put. And

382
00:26:14,160 --> 00:26:19,280
the robot would say, well, I think you're going to put paper next, because you put paper,

383
00:26:19,280 --> 00:26:23,440
like, three times out of the five times that we just played, and the child can look back

384
00:26:23,440 --> 00:26:27,520
at what they did, and they can start to see, oh, yeah, like, one, two, three, like, those

385
00:26:27,520 --> 00:26:34,040
are the times that I picked paper. So yeah, that's the sense in which I envisioned it,

386
00:26:34,040 --> 00:26:39,920
yeah. Yeah, it's like, it's not like super deep, but yeah, that's about how far we can

387
00:26:39,920 --> 00:26:46,360
go. But you kind of raised that as, like, that was made explicit in the curriculum, thinking

388
00:26:46,360 --> 00:26:55,960
about, you know, these, you know, percentages or frequency types of numbers. Yeah, absolutely.

389
00:26:55,960 --> 00:27:00,160
And the entire time the robot is, like, saying these things, it's pointing out what its

390
00:27:00,160 --> 00:27:05,320
knowledge is, and it's explaining why it's making decisions, so that the child can understand

391
00:27:05,320 --> 00:27:09,760
it. And I think my hope is that, you know, when they get older, if they have another AI

392
00:27:09,760 --> 00:27:14,520
class, they can revisit these ideas and actually learn about probability, and it all starts

393
00:27:14,520 --> 00:27:19,360
to make sense. So, like, even reinforcing those ideas later.

394
00:27:19,360 --> 00:27:23,920
And so just a point of clarification, you are referring back to this idea of a robot.

395
00:27:23,920 --> 00:27:28,360
Is this an entirely software based robot just on the smartphone, or is there a hardware

396
00:27:28,360 --> 00:27:34,400
component as well? Ah, yes, I should have explained that. So the robot is a mobile phone with

397
00:27:34,400 --> 00:27:40,000
this social robot technology that we developed in our lab, so it can talk. It has all these

398
00:27:40,000 --> 00:27:46,840
really cute, fun animations. I can listen to you. It has a camera, but around it, I've

399
00:27:46,840 --> 00:27:53,320
built a Lego body. So there's two different bodies that I'm working on now. One uses

400
00:27:53,320 --> 00:27:59,440
Lego We Do, which is like this $200 motor kit, and you can use normal Legos, and then you

401
00:27:59,440 --> 00:28:04,160
could add these motors to it. So now the robot can move and dance, but because it's Lego,

402
00:28:04,160 --> 00:28:08,440
you can change the way that it looks. So sometimes it's a car. Some children really wanted

403
00:28:08,440 --> 00:28:13,000
to play with one that's spun around, so they can sort of have total control over the way

404
00:28:13,000 --> 00:28:16,800
that their robot is. So they're programming it, they're building it, like all of it is

405
00:28:16,800 --> 00:28:21,960
brought down to the child's level. And then the other one, I imagine, is Arduino. So

406
00:28:21,960 --> 00:28:27,200
now I'm thinking about slightly older kids and even more fun things to build, so I'm

407
00:28:27,200 --> 00:28:31,320
building an Arduino platform for it too. So then back to these concepts, we just talked

408
00:28:31,320 --> 00:28:36,480
about the rules-based types of systems. Do you mention supervised machine learning as

409
00:28:36,480 --> 00:28:42,560
well? Yeah. So supervised machine learning comes up in YouTube kids, which surprisingly

410
00:28:42,560 --> 00:28:48,200
a lot of children were interacting with. So I would ask them, how does YouTube know

411
00:28:48,200 --> 00:28:54,040
which movie you want to watch next? And they're like, oh, it just picks whatever is random.

412
00:28:54,040 --> 00:28:57,200
And I'm like, no, it's not random. It usually picks things that are kind of like the video

413
00:28:57,200 --> 00:29:00,880
you just watch. And they're like, oh, yeah, I guess you're right. And so we can talk

414
00:29:00,880 --> 00:29:06,720
about supervised machine learning. So that's when you label some things, for example, as

415
00:29:06,720 --> 00:29:12,040
good and bad. So in the case of YouTube kids, children are labeling things as things

416
00:29:12,040 --> 00:29:16,640
that I want to watch by watching it. And then they can also give extra feedback, thumbs

417
00:29:16,640 --> 00:29:20,960
up, thumbs down, so that the robot, sorry, not the robot. YouTube's algorithm can learn

418
00:29:20,960 --> 00:29:28,360
better. But it's often used for like recommender systems. So it can be YouTube, it can be Netflix.

419
00:29:28,360 --> 00:29:32,640
Children don't have email yet, but sometimes they can kind of get what I mean if I throw

420
00:29:32,640 --> 00:29:37,600
that in there. But for children what we do is we sort foods into healthy and unhealthy

421
00:29:37,600 --> 00:29:43,040
groups. So rock paper scissors is nice because I have street rules. But if you want to

422
00:29:43,040 --> 00:29:47,000
teach a robot about which foods are healthy and unhealthy, you know, I have children think

423
00:29:47,000 --> 00:29:49,760
about like, how many foods are there? Like how many foods would you have to teach the

424
00:29:49,760 --> 00:29:53,640
robot about? And after they kind of like hit the dirty, they backs out and they're like,

425
00:29:53,640 --> 00:29:57,760
oh my god, that's so many foods. So I'm like, okay, there's a better way. We can give

426
00:29:57,760 --> 00:30:03,240
the robot a few examples and it can learn to make guesses on its own. So sort of on the

427
00:30:03,240 --> 00:30:07,880
back end, something I do beforehand, the robot has this database where it has like the

428
00:30:07,880 --> 00:30:13,120
color of foods with food groups and how much sugar it has all of these different features.

429
00:30:13,120 --> 00:30:18,480
And then it's going to use a K-nearest neighbors algorithm to sort of say, well, this food

430
00:30:18,480 --> 00:30:24,080
has this many features similar to this other food. So maybe these two are nearest neighbors

431
00:30:24,080 --> 00:30:31,520
as opposed to this other food. So a good example is like bananas would be more similar to

432
00:30:31,520 --> 00:30:38,960
lemons than chocolate. So what I have children do, they have this like list of 20 foods and

433
00:30:38,960 --> 00:30:44,600
I say we're going to label two of them. So they label, you know, either both of them good

434
00:30:44,600 --> 00:30:49,080
or both of them bad. They take whatever food they want. And then we're say, okay, now let's

435
00:30:49,080 --> 00:30:53,760
ask the robot to guess like whether this food is healthy or not. And so they start to see

436
00:30:53,760 --> 00:30:58,560
like, okay, if I tell the robot that strawberries and tomatoes are healthy and then I ask about

437
00:30:58,560 --> 00:31:01,800
chocolate, it's going to think chocolate's healthy too because I haven't given it any

438
00:31:01,800 --> 00:31:05,640
bad examples. So I have to do better. I'm like, okay, let's teach the robot that chocolate

439
00:31:05,640 --> 00:31:09,800
is not healthy. So now we have strawberries and tomatoes and the good chocolate and the

440
00:31:09,800 --> 00:31:15,880
bad. Let's ask it about ice cream. And the robot can say, well, ice cream is probably

441
00:31:15,880 --> 00:31:19,160
closer to chocolate than it is to the other things because they both, you know, are in

442
00:31:19,160 --> 00:31:23,680
the sweet section. They have a lot of sugar. So it's chocolate unhealthy too. And boom,

443
00:31:23,680 --> 00:31:28,520
like magically the robot seems intelligent, seems like it's learning. It ends up being that

444
00:31:28,520 --> 00:31:34,600
after we teach the robot about like five foods, then it can sort of guess the other 15 foods

445
00:31:34,600 --> 00:31:39,680
that remain. But it all depends on how good the training set is. So I don't use the

446
00:31:39,680 --> 00:31:44,960
words training set with a five year old. But the idea is still the same. It's like, so

447
00:31:44,960 --> 00:31:49,760
we only told it about these five foods and it learned about these 15 foods. Like would

448
00:31:49,760 --> 00:31:53,600
of all the five foods have been good would it do a good job? What if they were all bad?

449
00:31:53,600 --> 00:31:58,640
Would that do a good job? And they can start to seeing how the robot, you know, needs

450
00:31:58,640 --> 00:32:02,400
certain examples of certain quality, like if we only teach it about red foods and we

451
00:32:02,400 --> 00:32:06,680
ask it about blue foods, it's probably going to be a bit confused because like you haven't

452
00:32:06,680 --> 00:32:12,800
given it a good enough training set. That one's usually really fun. And then again, of course

453
00:32:12,800 --> 00:32:16,440
children want to trick the robot. It's like, well, I like chocolate. So I'm putting chocolate

454
00:32:16,440 --> 00:32:20,400
on the good side and like, okay, well, we can do that, of course. How does that impact

455
00:32:20,400 --> 00:32:25,640
the robot? And then we can discuss that as well. And then the last activity is genitive

456
00:32:25,640 --> 00:32:32,400
AI. And this was one that I thought was really important because AI doesn't just follow

457
00:32:32,400 --> 00:32:37,560
rules and it doesn't just classify things and make rules. Sometimes it is creative and

458
00:32:37,560 --> 00:32:43,720
it can be used in art. This particular activity is about music. So first children give the

459
00:32:43,720 --> 00:32:49,760
robot parameters about different emotions and how they would sound as music. So happy

460
00:32:49,760 --> 00:32:56,040
music sounds, you know, kind of fast and upbeat. And it also sort of goes up in core progression.

461
00:32:56,040 --> 00:33:02,080
So like rather than going like during, or it's going to go up. So they teach the robot

462
00:33:02,080 --> 00:33:06,640
that by sliding these two bars, like core progression up and music fast. And then they

463
00:33:06,640 --> 00:33:12,840
do sad. And they're like core progression down music, maybe a little bit slow and then

464
00:33:12,840 --> 00:33:18,440
excited. So core progression up, music fast or scared, core progression down, but music

465
00:33:18,440 --> 00:33:24,280
fast. And they teach the robot about what different emotions should sound like a song.

466
00:33:24,280 --> 00:33:30,160
And then I drive all the teachers in the room crazy. We start playing music with the robots.

467
00:33:30,160 --> 00:33:37,160
So children have this piano where they can play a song. And then the robot will take whatever

468
00:33:37,160 --> 00:33:42,400
song they make and will remix it according to the different emotions. So it's just like

469
00:33:42,400 --> 00:33:48,160
really noisy. But a lot of fun is like children are like, you know, playing songs and hearing

470
00:33:48,160 --> 00:33:54,080
the robot play their song back and sort of like going back and forth with this turn taking.

471
00:33:54,080 --> 00:34:00,600
And then, you know, we ask questions like, so did the robot song sound like your song?

472
00:34:00,600 --> 00:34:04,560
So if you tell it not to change anything, yes, if you tell it to go faster, it'll just

473
00:34:04,560 --> 00:34:08,280
change the progression a little bit. If you tell it to go slower, it'll, you know,

474
00:34:08,280 --> 00:34:12,240
make it a bit slower. Sometimes it'll add new notes. If you tell the core progression

475
00:34:12,240 --> 00:34:17,960
to go up, but you play a down court progression. And then how does that impact the emotions?

476
00:34:17,960 --> 00:34:22,200
It's like, oh, well, it seems like it's kind of picking randomly, but all the happy songs

477
00:34:22,200 --> 00:34:28,280
kind of start to sound the same. So it's really cool to watch children sort of do this

478
00:34:28,280 --> 00:34:32,240
less structured. Like, you know, it's not bright, long answer. It's how does it sound

479
00:34:32,240 --> 00:34:36,840
and they're making music. And then they get up and they play a class orchestra. And then

480
00:34:36,840 --> 00:34:43,240
we turn the tablets off with the rest of the activity after all of that stimulation.

481
00:34:43,240 --> 00:34:51,120
Yeah. So to be clear in this, this third concept, how are you getting at the AI element

482
00:34:51,120 --> 00:34:59,600
of what's happening? So examples of this AI come up. Like, if you look at some of Google's

483
00:34:59,600 --> 00:35:05,240
AI experiments, they have this piano where you can play music along with a computer. But

484
00:35:05,240 --> 00:35:09,720
the question is, how does that computer know what to play? And so in this activity, children

485
00:35:09,720 --> 00:35:14,880
are actually setting parameters for how the computer should change the song to make it

486
00:35:14,880 --> 00:35:22,120
sound a particular way. So if I give you, if they give the robot rather an input with

487
00:35:22,120 --> 00:35:27,000
three notes, C-D-E, and they tell it to make it faster and happier than the robot should

488
00:35:27,000 --> 00:35:33,240
return something according to the parameters faster. And maybe it'll go up C-E-G, like

489
00:35:33,240 --> 00:35:39,120
even higher than the child's output. So they start to see how they can use AI to create

490
00:35:39,120 --> 00:35:47,080
and to create new things. And then you mentioned previously the, some of the surveys you did

491
00:35:47,080 --> 00:35:54,560
before and after, was that before and after children go through this curriculum? Are there

492
00:35:54,560 --> 00:35:59,440
additional observations that you made about their experiences and what they've learned

493
00:35:59,440 --> 00:36:04,240
about having gone through this curriculum beyond what we've already talked about? Yeah. So

494
00:36:04,240 --> 00:36:09,960
there was the before and after about how children feel about AI and about robots. I also did

495
00:36:09,960 --> 00:36:16,880
a before and after about how children feel about engineers. So one of my, I guess, pet

496
00:36:16,880 --> 00:36:22,560
peas as an engineer is that, you know, there's a lot of emphasis on science and mathematics

497
00:36:22,560 --> 00:36:27,200
and STEM, but often technology and engineering is a lot harder to do and so it gets less

498
00:36:27,200 --> 00:36:32,440
attention. So as I went through the curriculum, I was hoping that children would have a better

499
00:36:32,440 --> 00:36:37,880
sense of what engineers do and why engineering is fun. And unfortunately, they did it. And

500
00:36:37,880 --> 00:36:45,440
I think that, you know, a lot of that is because it was a very new concept to them. So to

501
00:36:45,440 --> 00:36:51,600
tell a hilarious story, the first thing that I went in, I was like, okay, who here knows

502
00:36:51,600 --> 00:36:57,360
who an engineer is and, you know, in this classroom of 20, like two children raised their hand.

503
00:36:57,360 --> 00:37:00,840
And I was like, okay, that's not good. We have to do better than that. And so I pointed

504
00:37:00,840 --> 00:37:05,000
to one of the children and I said, okay, you tell me, what is an engineer? Tell us all.

505
00:37:05,000 --> 00:37:09,960
He's like, an engineer is someone who drives a train. And I was like, like, we're really,

506
00:37:09,960 --> 00:37:17,680
really far further than I even thought we were. So, you know, part of doing this research

507
00:37:17,680 --> 00:37:22,080
that I find, you know, like personally enjoyable is that I get to say, well, I'm an engineer.

508
00:37:22,080 --> 00:37:25,720
I'm an engineer because I build things and I build things to help people and starting

509
00:37:25,720 --> 00:37:32,280
to have children, you know, think about that as a different new career path. I kind of

510
00:37:32,280 --> 00:37:36,400
wish going back that I had built more of that into this curriculum that I built. So,

511
00:37:36,400 --> 00:37:39,360
you know, these activities were fun and they were playing games. But at the end of the

512
00:37:39,360 --> 00:37:43,600
day, they didn't get to see how the things they were building could be useful or how they

513
00:37:43,600 --> 00:37:47,960
could help other people or how they could bring joy to other people. And I think that's

514
00:37:47,960 --> 00:37:51,360
why you know at the end when I was like, okay, who wants to be an engineer? I still got

515
00:37:51,360 --> 00:37:56,720
crickets because it's like, all right, we need to do a better job of helping children

516
00:37:56,720 --> 00:38:01,640
sort of like see themselves as this, but also see the value of it in society. And then

517
00:38:01,640 --> 00:38:06,920
also just, like I said, I did a sort of AI assessment. So how much did children learn

518
00:38:06,920 --> 00:38:12,160
about these things? The AI assessment was like 10 questions, all about the different

519
00:38:12,160 --> 00:38:16,680
activities. And some of them are kind of tricky. It was like, I think I mentioned before,

520
00:38:16,680 --> 00:38:20,440
if you only teach the robot about good foods, where will it take you to chocolate goes?

521
00:38:20,440 --> 00:38:24,320
For five-year-old, you're like, of course, everyone knows chocolate's unhealthy, but it's

522
00:38:24,320 --> 00:38:31,320
very difficult for them to see like, okay, wait, but this AI algorithm only knows foods

523
00:38:31,320 --> 00:38:36,080
that I taught it, only foods that I've labeled. So it'll always use those labels to make

524
00:38:36,080 --> 00:38:40,200
its guesses. So it's really cool to see like a lot of children start to get those things

525
00:38:40,200 --> 00:38:46,560
right because it kind of like blew the developments of psychology literature, you know, out of

526
00:38:46,560 --> 00:38:49,160
the water. They were like, I'm not sure if children could do this kind of reasoning yet.

527
00:38:49,160 --> 00:38:51,640
I'm like, no, they did it. It was awesome.

528
00:38:51,640 --> 00:38:52,880
That's fantastic.

529
00:38:52,880 --> 00:38:58,360
Yeah. So yeah, anyways, 10 questions. Some of them a bit tricky. And I think the median

530
00:38:58,360 --> 00:39:05,160
score was like 70%. So I mean, obviously, we shouldn't assess children to have a lead.

531
00:39:05,160 --> 00:39:09,400
That's probably not healthy for them, but they understood a good amount of what was presented

532
00:39:09,400 --> 00:39:15,440
in front of them. And I think that's really encouraging and important. It's, I mean, the

533
00:39:15,440 --> 00:39:20,600
same with like early computer science education. It was very easy to say children can't understand

534
00:39:20,600 --> 00:39:26,240
this is too complex, but the way that something is designed, it can be made accessible to

535
00:39:26,240 --> 00:39:33,360
children. So, you know, right now I'm working on a deep reinforcement learning activity.

536
00:39:33,360 --> 00:39:37,960
So pretty much we're going to build agents that can play snake. So first, we're going

537
00:39:37,960 --> 00:39:43,280
to hand code it. And then we're going to use a simple neural net where we like give

538
00:39:43,280 --> 00:39:47,400
it a bunch of examples. And then we're going to have it do deep learning. And I'm building

539
00:39:47,400 --> 00:39:52,040
very confident that these children can understand it because, you know, if you just are able

540
00:39:52,040 --> 00:39:57,800
to break something down enough, they can get it. So if you're a deep RL person, I'm going

541
00:39:57,800 --> 00:40:03,160
to have some five year olds coming for your job. Pretty. So get ready.

542
00:40:03,160 --> 00:40:11,440
Nice. Nice. This is awesome work. Where do you go from here? So this was really the center

543
00:40:11,440 --> 00:40:17,800
of your master's thesis. Where do you see it going beyond that?

544
00:40:17,800 --> 00:40:22,680
There's so many different things that I want to do and so much work to be done. So some

545
00:40:22,680 --> 00:40:28,400
really things that are going on that there are others in my lab who I was kind of like

546
00:40:28,400 --> 00:40:33,640
the person to go and try this and see if it works or not. So now that it works, other people

547
00:40:33,640 --> 00:40:40,040
in my lab are also trying out their own experiments. One of my lab mates, Athena is doing this work

548
00:40:40,040 --> 00:40:44,400
around, you know, when children are learning with AI, how can that impact their creativity?

549
00:40:44,400 --> 00:40:48,040
So they're not just learning about AI anymore. They're also learning to be more creative

550
00:40:48,040 --> 00:40:54,000
and to be explorative as they're learning, which will have like huge benefits for them,

551
00:40:54,000 --> 00:41:00,520
you know, beyond just learning a particular skill. Another student in my group, Blakely,

552
00:41:00,520 --> 00:41:04,920
not student in my group, like I'm their professor. Another one of my lab mates, Blakely is working

553
00:41:04,920 --> 00:41:11,120
on AI ethics curriculum. So really helping children be able to understand the ethics behind

554
00:41:11,120 --> 00:41:15,920
every AI decision so that they can critically evaluate the things that are on them. But

555
00:41:15,920 --> 00:41:19,720
also when they're building things, you know, why teach children to build something if

556
00:41:19,720 --> 00:41:22,760
they don't know how to build it ethically. Like at the same time, they should be thinking

557
00:41:22,760 --> 00:41:27,120
about both of these things. So I'm really excited about that work. Personally, I've gotten

558
00:41:27,120 --> 00:41:31,040
a lot of feedback from teachers like, this is great. I have no idea about anything

559
00:41:31,040 --> 00:41:35,320
that they had. So can you teach me? So I'm trying to figure out the problem of how can

560
00:41:35,320 --> 00:41:40,520
we actually make this something that teachers can like use and feel empowered to use and

561
00:41:40,520 --> 00:41:46,520
not scared of so that it can really get into classrooms and get into the spaces and

562
00:41:46,520 --> 00:41:53,160
also more that I used to work in. And then I think also just making more cool activities.

563
00:41:53,160 --> 00:41:57,600
In my dream of dreams, this will become like this big online platform and children everywhere

564
00:41:57,600 --> 00:42:02,840
can learn about AI and ways that are meaningful to them. So you know, there has to be a lot

565
00:42:02,840 --> 00:42:08,160
more content behind it beyond these three activities. What other things can children learn

566
00:42:08,160 --> 00:42:13,720
and what are other metaphors that make sense for them, right? So yeah, that's all the

567
00:42:13,720 --> 00:42:24,400
things I'm going to do. Yeah. Awesome. Awesome. Are there things that you have identified

568
00:42:24,400 --> 00:42:29,680
that you need? Meaning if there's, you know, some potential partner out there that,

569
00:42:29,680 --> 00:42:35,640
you know, someone in our listening community might, you know, be connected to anything

570
00:42:35,640 --> 00:42:42,360
come to mind in that regard? I mean, yes. I can plug for things. So I think one thing

571
00:42:42,360 --> 00:42:47,320
that would be really awesome is to have an AI person, somebody feels comfortable with

572
00:42:47,320 --> 00:42:54,600
AI who's really passionate about teaching this, who I can sort of help get started with

573
00:42:54,600 --> 00:42:58,680
their own activities. So I'm doing that with teachers right now and I think the biggest

574
00:42:58,680 --> 00:43:02,600
problem is that they're not comfortable with AI and there's a lot of work that I have

575
00:43:02,600 --> 00:43:07,720
to do to get them there. So I'm wondering how might it be different if I take an AI person

576
00:43:07,720 --> 00:43:14,080
and start to give them tools to be teachers? That would be really cool. Also, if you kind

577
00:43:14,080 --> 00:43:19,920
of just want to chime things out with your kids and experiment and ask questions, some

578
00:43:19,920 --> 00:43:25,360
of the papers that are linked in the website that we have have actual AI resources that

579
00:43:25,360 --> 00:43:29,840
parents can go on and chime right now. I'm just like short activities based on scratch.

580
00:43:29,840 --> 00:43:35,680
So unfortunately, you have to have a kid between the ages of seven or maybe over there.

581
00:43:35,680 --> 00:43:39,480
But there are already things that exist that people can try that I highly recommend them

582
00:43:39,480 --> 00:43:43,720
try and give a feedback on. Well, Randy, thanks so much for taking

583
00:43:43,720 --> 00:43:50,080
the time to share what you're working on with us is really cool stuff and I'm looking

584
00:43:50,080 --> 00:43:55,520
forward to seeing how it evolves. Oh, thank you. I really appreciate the opportunity.

585
00:43:55,520 --> 00:44:04,240
Thanks. All right, everyone. That's our show for today. For more information on Randy

586
00:44:04,240 --> 00:44:11,320
or any of the topics covered in the show, visit twimmelai.com slash talk slash 225.

587
00:44:11,320 --> 00:44:17,200
For more information on the black and AI series, visit twimmelai.com slash black and AI

588
00:44:17,200 --> 00:44:36,840
team. As always, thanks so much for listening and catch you next time.

