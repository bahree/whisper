Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast we're featuring a series of shows that highlight just a few of
the great innovations and innovators at the intersection of three very important and
familiar topics, data science, the Python programming language and open source software.
To better understand our listeners' views on the importance of open source and the projects
and players in this space, I'm conducting a survey, which I'd be very grateful if you
took a moment to complete.
To access the survey, visit Twimbleai.com slash Python survey.
Please hit pause now and we'll wait for you to get back.
That's twimbleai.com slash Python survey.
Before we dive into the show, I'd like to send a huge thanks to our sponsor for this
series IBM.
Speaking of open source, IBM has a long history of engaging in and supporting open source
projects that are important to enterprise data science, projects like Hadoop, Spark,
Jupiter and Cubeflow to name just a few.
IBM also hosts the IBM data science community, which is a place for enterprise data scientists
looking to learn, share and engage with their peers and industry renowned practitioners.
Here you'll find informative tutorials and case studies, Q&As with leaders in the field
and a lively forum covering a variety of topics of interest to both beginning and experience
data scientists.
Check out and join the IBM data science community by visiting IBM.com slash community slash
data science.
All right, everyone.
I am on the line with Luciano Recende.
Luciano is an open source AI platform architect at IBM and part of the center for open source
data and AI technology there, also known as code eight.
Luciano, welcome to this weekend machine learning and AI.
Hi, Sam.
Thanks for having me.
So Luciano is great to have you on the show and I'm looking forward to chatting with you
about a project that you spent a lot of time working on in particular Jupiter notebook
and the Jupiter notebook ecosystem.
But before we dive into that, I'd love to hear a little bit more about your background
and how you got started working at this, the intersection of open source and data and
AI.
Great.
Let me start.
So like I've been working with open source for a long time.
I've been contributing to projects that are patching in other areas on the open source
AI ecosystem for over 10 years and more towards getting into this Jupiter ecosystem part.
A couple of years ago, we started seeing a more and more adoption of the notebooks as
a way for the interactive development for data science and recently for AI as well.
And when talking to external or internal customers, we had a need to basically scale that
platform for a large number of concurrent users.
So like you go to a financial institution and they might have like 500 data scientists
that come in the morning will launch their notebook and they need to be able to do all
the crunch of the data that they have to start getting insights from that.
And we didn't have a very good solution for that.
We also were looking to building the IPM platform, the Watson Studio and we needed a similar
solution.
And that's how we started the Jupiter Enterprise Gateway.
Basically, what we do in Jupiter Enterprise Gateway, we are a lightweight, multi-tenant,
scalable and secure gateway that enables Jupiter notebooks to start sharing resources across
an Apache Spark cluster or a Kubernetes cluster.
And we focus a lot on the enterprise and cloud use gays for that.
I'm starting to hear quite a bit of interest on the part of enterprises and folks that
are working in building out platforms and infrastructure for data science and machine
learning teams about the idea of kind of standing up Jupiter notebook instances for their teams.
As you kind of think about the way Jupiter is being used in large organizations, what
are some of the big challenges that folks run into?
So Jupiter notebooks, they are evolution from Python console.
And because of that, we see it some limitations that came from that past instances.
And Jupiter by default, it's a single user kind of like environment and all the kernels
which are the actually run time of the data executes the code, it's default to run as a
local process.
And what we see is that it doesn't scale to the needs that we need on enterprise or large
deployments.
Then on on the Jupiter ecosystems, we started seeing a lot of like projects that are kind
of like built as an add-on to Jupiter and start solving some of those limitations.
And the idea is that you can start building itself service platform for data science and
AI.
So when we look, for example, Jupiter gives you the ability to start providing multi-user
authentication for those notebooks, and then Jupiter Enterprise Gateway starts to scale
the runtime and distribute that into either a Sparkless or Kubernetes cluster.
And that's the direction that we are seeing things going to the Jupiter ecosystem at this
days.
Okay, yeah, that was going to be one of my questions.
The differences between Jupiter Hub and Jupiter Enterprise Gateway, is it all about kind
of where the back end kernels are run or are there other differences?
So as I mentioned, we need a way to start launching an environment for multiple users,
so provide a multi-tenant support.
And what Jupiter Hub does, it's kind of like a launcher of a Jupiter server.
So each user that requests an environment gets a Jupiter server runtime for doing
its computation.
Then when you get that environment, you then start using multiple notebooks.
Those notebooks require a kernel to execute to the code.
And then that's when Jupiter Enterprise Gateway comes into mind so that you can have a very
lean Jupiter server environment launched by Jupiter Hub.
And the runtime then, it goes across a cluster and you can start having multiple notebooks
at the same time.
But if you close a notebook, you let go those resources and make it available to other
data scientists on the platform.
It also enables you to start sharing more expensive resources, like for example, you're
doing AI and you need GPUs.
We provided a way to containerize that and then be able to associate the number of GPUs,
for example, or the resources per kernel that it gets launched.
And because those are decoupled, those come and go as you're using them and not necessarily
the whole time that your notebook server is up.
Okay, so it sounds like then the typical organization will use both Jupiter Hub as kind
of the front end and then Enterprise Gateway as a way to distribute the backend kernels.
But they're not alternative products, they're complimentary.
They are definitely complimentary.
They work very well together.
Yes.
So you mentioned both Spark and Kubernetes as services that you can use to provide the
backend distributed computing framework for these kernels.
What's the current mix that you see of those two options?
Is one more popular than the other?
I think it really depends on the type of workloads that you're processing.
What I see Spark and use it is more for analytics where you have a lot of data and you might
start processing crushing those data and then applying some kind of like a machine learning
on top of that.
And with the recent release that we have, the Enterprise Gateway 2.0, we support not
only if you have like a legacy Spark deployment based on like a HDFS and I do distribution,
but we also support Spark on Kubernetes.
So if you're doing analytics, you're most likely going to use Spark runtime and that's
why we initially started supporting that.
With more of the AI workloads, what we have seen is a lot of like AI platforms for training
models and deploying models all based on Kubernetes and the reason I think for that is that
Kubernetes provides you kind of like the right sandbox with containers and that enables
you to, for example, start a Python kernel, but if you're doing a TensorFlow model, you'll
just kind of say, okay, I want to Python kernel based on a TensorFlow image and you start
mixing and matching your environment with the kernel that you need and then you can have
a very specific environment and close it in the container and another AI engineer might
completely have a completely different one and Kubernetes manage and although deployment
anything that needs to happen on that, and we manage the lifecycle and give the connectivity
to notebooks to be able to actually execute and train your models on a Kubernetes environment.
Now, I've had several conversations with organizations, Airbnb is one that comes to mind
that have invested pretty heavily in customizing Jupyter Notebook and in some ways, maybe they've
duplicated what you've done or went to build it because what you've done either wasn't
ready on their time frame or didn't offer features that they needed and I'm wondering
in general, where do you see folks investing heavily in customizing Jupyter to build it
into their workflows? What are some of the key gaps maybe in what's currently available
or some of the ways that organizations might want it to be more tightly integrated?
What I've been seeing in the industry is a lot of customization around your internal
environment and integration with some of the tools.
You might need to get some of the outputs of your notebooks to a given platform that
is available only internally and that's a lot of the things that I've been seeing.
When you go, I think with Jupyter Lab, which is a new UI for the Jupyter Notebook, those
kinds of customizations and ability to start bringing up widgets are going to become
much more flexible and easy to deploy or customize and then in terms of runtime, I think
what I've been seeing is a lot of like trying to accomplish multi-tenant by deploying multiple
environments for different users, but it's a duplication of that environment and not like
a decoupling the actual execution of the kernel with the Jupyter Notebook and that's
how Jupyter and Fifthgate became to do.
Can you elaborate on that last part when you say it's duplication and what sense?
So Jupyter Hub, as you were saying, was available a lot prior than enterprise data and initially
what I've seen people doing before enterprise data is that they will start a Jupyter
server for each user and then that instance of the Jupyter server will have to have all
the resources required for any type of workload that the user needs to do.
That is an okay solution, but all those resources get locked for a long time and in that case,
I think a lot of those resources might be in an idle position and not be able to be reused
by other members of your platform or other data scientists that wants to do some computations.
You also mentioned integrating with internal systems and security and things like that
as a big motivator for folks.
The Jupyter Notebook and this ecosystem, APIs, do they lend themselves to folks doing this
kind of integration, is it difficult?
Where is it on the scale of built for this and easy to do or really difficult to do and
requires a lot of jumping through hoops?
That's a good question.
Let me look into a different perspective here.
Imagine your data scientists and you're running your workloads with Jupyter.
You will not see any difference in terms of user experience if you're using enterprise
gateway on the backhand or not.
If you are a kind of like DevOps and you are providing that environment for your data
scientists, pretty much what you need to do is customize the kernel definition and instead
of saying that it's just a local kernel, you will switch to a different lifecycle manager
and say, this is a remote kernel, we call it process proxy, we have different lifecycle
managers for each resource manager so we have one for yarn, we will have one for Kubernetes
and that is extensible so people have been coming in and providing others as well and
depending on the one that you choose, the resource manager that you choose, you can then
add additional information needed.
So for example, if you choose yarn, you might want to pass like your Spark home information.
If you choose Kubernetes, you might not need anything specific or depends on how it goes
into terms of configurations.
But once that is properly configured, we handle most of the hard work internally on the
tool by automating all the lifecycle like the starting the kernel remotely figuring out
where it actually mounted, providing the configuration back to Jupyter so that the end-to-end
communication can be connected.
So it's to certain extent not very hard to get an environment going and we also have
been doing lots of tools like SQL scripts where you can deploy enterprise gateway from scratch
or from an existing environment for both Spark and Kubernetes run types.
So the work to create this remotely executable or invocable kernel, was that already in existence
in the Jupyter ecosystem or is that part of the work that enterprise gateway needed
to do in order to exist really?
Yes, so the kernels in itself is nothing new that we created.
The kernels were available before in Jupyter and today I think we have a list of like over
100 kernels available.
The kernels, they will abstract the languages that you are using.
So for example, if you are doing Python, you have a Python kernel, if you're doing
Scala, you have a Scala kernel.
What enterprise gateway brings is the ability to decouple those that originally were running
as local process to be run remotely.
That's the main thing that enterprise gateway brings.
And around that came a lot of requirements such as security, how to handle multi-tenant
so that the users that are actually requesting the kernel are the ones that are running the
kernels on the ground times as well.
And we started leveraging the capabilities that we had to handle all of those additional
requirements in enterprise gateway.
So occasionally here from folks that are using Kubernetes for these types of workloads that
they run into challenges with the kind of out-of-the-box Kubernetes scheduler.
Is that something that you've seen for running these kernels?
We haven't experienced any particular issue with the schedule at the moment.
We also haven't heard from any users.
I think most of the users at the moment are going more for the Kubernetes environment and
we haven't heard anything specifically to that from them yet.
I guess that one of the other things that I hear from folks about the way they want to
use Jupyter is to integrate it with, for example, more tightly with Git repositories and shared
file systems and kind of have these resources easily accessible by data scientists and machine
learning engineers, kind of wherever they spin up their notebooks.
Is that something that you see as well?
And is it a piece of what's offered by the current notebook ecosystem?
That's definitely something that I hear a lot.
I think the Jupyter notebook and as well the new UI Jupyter lab have the ability to have
the extensions built on top of that.
Particularly on the Jupyter lab, I've seen, for example, integration with Git repositories
and you can easily or transparently get things back and forth based on how you're progressing
with your notebook.
I think another thing that I also hear a lot, and currently I don't think it's available
or at least not easily available on the open source, is the ability to start doing collaboration.
That is something that, like sharing and having multiple people looking to the notebook
and maybe trying to collaborate on that.
I think it's one thing that really needs to come and be available on the open source
of the Jupyter ecosystem and that's not available today.
Yeah, I think Google Collab is an offering that comes to mind that does a pretty nice
job at showing what a collaborative notebook experience might look like.
And I can imagine that's kind of creating a lot of demand for folks that want to use
something similar, but in a native Jupyter type of an environment.
That is correct.
I mean, I've seen commercial solutions that start offering those, but in the open source
or more like on the academia, we haven't seen an open source solution that handles that.
Okay.
You mentioned that there are hundreds of kernels for Jupyter, which I actually find kind
of surprising.
I mean, when I click down that kernels button, it only ever says, I thought, two or three.
I'm wondering, to what extent my experience is reflective broadly or in general, how much
of the Jupyter ecosystem is kind of Python centric and is Python users and how much of
it folks that are trying to do other things beyond Python?
I think that we at least commercially like on more enterprise environments, what we have
seen is Python, the number one, R, the second, and those are mostly data scientists.
Then some data engineers, particularly the ones working on analytics workloads, they
also use the Scala for particular integrating with Spark.
And you mostly see Python because Python, like if you have an environment like an account
or any other way, Python most likely will be natively available for you.
The other kernels are, you can easily install all those, they will just after installation
start appearing as one of the options into your Jupyter notebook environment.
And particularly in academia, then you start seeing Julia, C++ and other kernels that are
very used on that community.
So that's why you have at least like over a hundred kernels available today.
Okay.
And for folks that are, can you maybe are there specific companies or examples that you
can share about what folks have managed to do with the enterprise gateway?
One of the examples that I can give you internally is IBM Watson Studio.
In order to be able to scale the notebook environment that we provide as part of Watson Studio,
we use enterprise gateway to not only remote to the kernels, but also to give you a choice
on where you want to run those kernels.
We have different run times that we support and you can select those run times as where
you want to run where you want to have the execution of the notebook runtime.
Another example, and this is just from strata, we were at strata yesterday and PayPal was
giving a very interesting presentation on how they were using enterprise gateway to be
able to view the internal platform for data scientists and AI.
So these are two examples that comes to mind.
There are probably more, but particularly because we are open source, we only hear from
them when they have an issue that gets reported or maybe like to a conference or something
like that.
Is it currently at a production ready?
State sounds like PayPal at least in IBM or using it in production.
How mature would you say it is?
So we have two code streams that we are maintaining at Jupyter Enterprise Gateway.
We have what we call the OneX release, which supports Spark, is mostly focused on analytics
workloads and that has been stable and being used internally by products and have been
available in the open source for a long time.
Probably over a year or around close to a year and a half.
And we have maintaining that in terms of like any new bugs that we see around that or any
dependencies that needs to be updated for security reasons or anything like that.
In parallel, we then are working on our tool release, which two days ago, we launched
the tool RC1 release candidate one.
And that brings a lot of innovation that we are doing on Kubernetes environment, having
said that based on the questions and issues that we are getting, we have seen a lot of people
experimenting, going more towards that direction and using the tool already to view their analytics
environments on top of Kubernetes.
So hopefully that will also be available as a GA in the next couple of weeks, based on
whatever feedback we get from the RC1.
And is the Enterprise Gateway functionality independent of the kernel that's being used
or there's some limitations or dependencies there?
So it is supposed to be independent.
What we have done, we created a layer called kernel launcher that gives us the ability
to enhance the kernels functionality, particularly for startup and shutdown.
So we have tested a lot with Python, DR, and Scala kernels, which are the most requested
and the most used for us.
And things that we do is like, for example, if you are starting a kernel and that kernel
is connecting to Spark, we create, for example, a parallel thread to start the Spark context.
So it gives you a better and quicker user experience for the startup of the kernel.
We need the ability to provide remote signaling so you can hit stop the kernel and things like
that.
And we need to flow that externally to a remote kernel and the launcher does that as well.
But that kernel launcher can be easily ported or used for a different kernel.
So the addition of new kernels shouldn't be very hard to incorporate into Enterprise
Gateway.
Here's about the requirements that users have when architecting environments for Enterprise
Gateway.
One of the things that comes to mind, for example, is when you're running your kernel locally,
you've got typically data locally.
If you're starting to remote your kernel, then the kernels operating against data that's
remote, I'm wondering if there are network or bandwidth considerations or any kinds of
issues that an organization might want to think about as they're architecting an Enterprise
Gateway environment and generally how straightforward that process is or their tools that are
available for capacity planning, things like that.
I completely agree with the type of considerations that you're bringing up.
I just, from what I'm seeing, these are not specific to when you're trying to deploy Enterprise
Gateway.
And it's a request more in general.
So if you're doing analytics, the data will be most likely in a Spark cluster, but in
a distributed file system like HDFS or some kind of like object storage environment.
And you have like shuffling of data going back and forth, independent if you're using Enterprise
Gateway or not.
So that should have been part of like planning the deployment of Spark in particular.
If we consider the Kubernetes environment, Kubernetes we most likely have multiple microservices
being together to bring up a solution and you have similar type of constraints regarding
data flowing and any other things like that.
The easiest way that I've been seeing in more like a Kubernetes environment is we give
the ability to do mattings into volumes into the specific container and by doing that,
you can have the user environment available in that kernel image and that helps you alleviate
most of these constraints by maybe providing data, maybe providing environment with specific
packages that you might need for a given user on that mount and that should solve some
of these problems.
So it sounds like there's nothing particularly exotic or unknown or that needs special attention
here.
It's kind of the typical considerations when architecting a distributed environment for
these kinds of workloads.
Exactly.
Granted that this capability that we're talking about particularly with Kubernetes is something
that was just announced a couple of days ago or released a couple of days ago.
But are there any other kinds of issues that you hear a lot about that folks run into
that maybe you wish more people would think about before they embark on this path?
Maybe a little bit unrelated but in the context of the Geoctery ecosystem, what I see a lot
of discussions of what is the notion of a notebook and how it incorporates on your, let's
say, model training pipeline and things like that.
And most cases that I've seen, people would use the notebook for kind of like evaluation,
they can look at the data and play with your model.
But we'll actually convert the notebook into a Python file, for example, when they are
kind of like ready to do a big training or start getting close to go to production.
In my mind, the notebooks can become a little bit more kind of like first class citizens
on those pipelines for model training and actually be one of the steps, actually be the
notebook in itself and not necessarily go to a pipeline as a converted Python file.
And I think once we have that, it's going to be much more productive, the data scientists
can come up with something that then goes directly into the pipeline, produces the model and
then can recreate the model later on if needed without any kind of like a collaboration maybe
with a system engineer to convert to a Python or anything like that?
And what needs to happen in order to get there from a kind of Jupyter ecosystem perspective?
We have been seeing tools that it starts to enable that.
I think it's mostly in a state of mind of like how people see the notebooks and they initially
were a lot used for kind of like experimenting with data, trying to understand the data
and trying to build models, but not necessarily maybe because those tools were not available
yet go into production.
And I think today we have seen like the Netflix coming out with paper, meal, a couple
of other schedulers also coming out and supporting notebooks.
My hope is that I will start seeing more of that being part of the pipeline for model
development.
All right, Luciano, this has been really great.
I'm wondering if you've got any final thoughts or tips or ideas for folks that are using
Jupyter, maybe as individual data scientists and want to use it more broadly in their
enterprises?
Oh, definitely.
I think one of the things that I've seen and it's interesting that a lot of people still
use and know about the Jupyter, what I call like classic UI, but not aware of like Jupyter
Lab.
Jupyter Lab gives you a lot of like flexibility and a better UI to do your model development
and that gives you ability to start having like time in those multiple tabs all together
and provide a much more integrated development environment for the model development.
Awesome.
If folks should definitely check that out.
Well, Luciano, thanks so much for taking the time to chat with me.
Okay, thanks, Sam.
All right, everyone.
That's our show for today.
If you like what you've heard here, please do us a huge favor and tell your friends about
the show.
And if you haven't already hit that subscribe button yourself, make sure you do so you
don't miss any of the great episodes we've got in store for you.
As always, thanks so much for listening and catch you next time.
