Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. A big thanks to everyone who made it out to our meet-up
this week. It was our biggest event yet with nearly 50 participants online to discuss
the YOLO Object Detection System. If you missed it, you can check it out at twimlai.com-meetup.
While you're there, you can also sign up to learn deep learning with us this summer.
I'll be working through the fast AI practical deep learning for coders course starting in
June and I'm organizing a study and support group via the meet-up.
I've been blown away by the interest in this. This is a great course and fast AI co-founder
Jeremy Howard encouraged our group on Twitter noting that groups that take the course together
have a higher success rate. So let's do this.
Three simple steps to join. One, sign up for the meet-up at twimlai.com-meetup, noting
fast AI and know what you hope to learn box. Two, using the email invitation you'll receive,
join our Slack group. And three, once you're there, join the fast AI channel. This is going
to be great.
Alright, in this episode, I'm joined by Taran Southerne, a singer, digital storyteller
and YouTuber whose upcoming album, IMAI, will be produced completely with AI-based tools.
Taran and I explore all aspects of what it means to create music with modern AI tools
and the different processes she's used to create her singles, break free, voices in my
head, and more. She also provides a rundown of the many tools that she's used in this
space, including Google Magenta, Watson Beat, Amper, Lander, and more. This was a super
fun interview that I think you'll get a kick out of.
And now on to the show.
Alright, everyone. I am on the line with Taran Southerne. Taran is a digital storyteller,
a YouTuber, and a singer whose new album, IMAI, is composed and produced entirely with
artificial intelligence. Taran, welcome to this week in Machine Learning and AI.
Thank you so much for having me.
How do we get started by having you tell us a little bit about your background and what
sparked your interest in AI-generated music?
Sure. I've done a lot of things in my background as an artist. I started out in Los Angeles
as an actress and a writer for television. And then a few years into that, I started
making YouTube videos, mainly just as a way to appease my boredom. And it became very
clear after a few years of doing that that there was actually a way to make a living.
I had a number of friends who were also in the YouTube world who had been early pioneers
on the platform and had built up very large audiences and were living the dream, making
whatever content they wanted to make. And I thought, I want to do that. So I started
making weekly videos in 2012 and did that for about four years. I made a weekly video,
comedy, sketches, music videos, you name it pretty much anything and everything. And it
was a really great experience. I also started producing videos for other companies as well
and built up a digital production company. So over the course of ten years, I made approximately
1500 videos, which is a lot of content. And I think, yeah, it's a tremendous amount.
And I think why this is relevant to what we're talking about now is as a YouTuber or content
creator in the digital world, for better or for worse, your success is ultimately defined
more by the quantity of creation than the quality. And so as a result, you start finding
a lot of tools to help increase your production and optimize your content to look as good as
possible and be as good as possible in the shortest amount of time possible. And so you
almost become like a hacker of sorts in terms of how you edit, how you shoot. You're always
trying to find like the quickest but best path forward to make the best possible content
in a very, very quick period. So as a YouTuber, I learned everything. I learned how to edit,
how to light, how to do makeup, how to write, how to market, how to Photoshop photos, and
how to do all of those things in an extremely short period of time. And with usually with
simpler tools than perhaps what professionals used in each of those categories. And so
ultimately around 2015, 2016, I grew very tired of the hamster wheel of content. And so I
decided to throw in the towel and jump ship into a new area, something that would just really
excite me and interest me. So I found myself drawn to VR and wanting to learn how to make
VR content and do something new and different that didn't require me to make content super
fast. And through that, I began working on a project utilizing artificial intelligence.
I really wanted to explore how humans were using technology. And so due to a grant from Google,
I was working on this experiential VR piece. And I really wanted to incorporate AI as
much as I could. So I was just researching every tool that there was out there. And in my
similar to my YouTube roots, I wanted to find tools that are user friendly, easy to use,
don't require me to learn a bunch of code. And that's how I ended up stumbling upon some of
the current tools that I'm using on my album. And it really was just one of those happy accidents
where I wanted to create something new and different. I liked the idea of not having a roadmap.
And within a few weeks of working with some of the AI tools that I'd stumbled upon, I realized,
wow, I can actually make some cool music with this. Maybe I should turn this into a separate
project all on its own. And that's exactly what I ended up doing with the album.
Oh, wow. On your website, you describe yourself as the first artist to work with artificial
intelligence as the sole composition and instrumentation tool on a music album. What exactly does that mean?
Good question. And I think that the definitions get a little bit hazy when you're looking at how AI
is currently used for music today. And quite frankly, I'm not the first album to incorporate AI.
In fact, Flow Machines just released an album two months ago. I haven't even released my album yet.
I've released two singles publicly. I've got a third one coming out in two weeks and then
a fourth one coming out in June. And then I release the album in September.
What was, I suppose, unique about the first song I released and the second song is September
of last year in January of this year, respectively, was that all of the instrumentation that you hear
in that music was spit out from the AI tool that I used to compose with it. Whereas a lot of the
other AI tools or AI songs that you'll hear currently on the market today, those were software,
those were built from software that essentially created a MIDI track, which then a human used
to transpose into instruments of their choosing. So they could essentially extract the notes
from the MIDI track, separate them according to how they want those notes to be heard and
rearrange those notes, reconstruct those notes. I mean, there was a lot of, there's a lot of human
interaction with those, with those musical pieces and then transpose it into the instrumentation,
which is not a bad thing. It's a great, I mean, it's just, it's another tool that humans can use,
but it requires a certain level, I think, of musical acumen on the part of the user to be able to
make those kinds of choices. Whereas the first two songs I released, you could, you could say that
the tool I used was like the AI for beginners tool, or you don't need to know anything about music.
All you need to do is make choices like a, like a film editor would make choices with, with film
footage. So I think, I think that's actually the best possible example of what I'm talking about
is the tool I use for those two songs ampere. Very easy user interface, front facing, you make choices
like what BPM you want the music to be, what genre, what types of instruments you want to be included,
you can specify them, and then it spits out a finish track. Now from that finish track, you can
download the stems and you can remix them, you can make them, you can take some instruments and cut
them out, et cetera, but essentially those instruments were inputted into the AI system with the
thousands of different notes of variation, different ways of playing the instruments and then the
AI is actually making those choices for you. So, so I was not the one choosing those instruments
or making any of those sounds unlike a lot of the AI other, other AI projects that you might see
out there. Hmm, you mentioned stems. What are those? Stems are like the individual pieces of,
it's like the individual file for each instrument. So when you download a finished song, you've got
an MP3, but a stem would be separating out the trombones, the drums, the guitar. So that way
in a mix, you can actually have them at different volumes come in and out, have more variation of
sound. That's what gives you that full sound when you're listening to the radio. So you mentioned
some of the parameters that you kind of tune with this, the beats per minute and some others.
What's the, how much control do you feel like you have as an artist given this approach to creating
music? The question is how much time you have. Yeah, I mean, you really are, when people ask
whether or not working with AI reduces the creativity of the artist, I don't think so at all,
I think it just changes the role of the artist. So, for instance, in my case, I don't come from
a musical background, I can sing, I write melodies, vocal melodies, I can hear harmonies in my head,
but if I had to actually put my hands on a piano or a guitar, I wouldn't know what in the world
to do with it because I just don't have that tactile memory and I haven't taken the time to learn
and it's very hard to learn those things and this is an adult. When I'm working with these tools,
I'm basically getting a bunch of raw data. I can then pick and choose from that raw data
for as much time as I have in a day. I can listen to hundreds, if not thousands, of pieces of music,
discern which piece I find interesting and then from there, I can iterate on that one piece of
music as many times as I'd like. I can change the instruments, I can change the BPM, I can change
the key, I can change how, you know, whether that's a mixolodian key or a major or I can change
the genre so that it's got more of an empowering and themic feel rather than like a sad
ballad type of feel. And so I can make all of these choices and those choices end up changing
the piece dramatically. It's almost as if you handed three editors, a bunch of raw film footage
and you told them all to make a movie and they're all going to make a very, very different kind of
movie and it ultimately just depends on how much time you have. But I just think the work becomes
a lot more editorial based. But it is, I think these kinds of tools will see a ton of creative
input and it's just, it's a matter of time to see how people will utilize them and what other
things will come out of their use. Where did the music creation part of the process fit in
relative to creating the song and other aspects of the songwriting process? Did you already have
the song and you created the music for it? Did you create the song for the music? Did somehow the
AI tool, did you sing the song and the AI tool map to the song as song or did you change the way
you sung the song to the tool? How does that whole workflow come together for you?
So it's different for every song and I'll give you two different examples. The two songs that you
heard that have already been released break free in life support. I wrote those songs to the music
once the music was almost complete. Basically, I got to a point where I was very happy with
one of the exported songs probably after dozens of iterations within Amper. And from there,
the song had a certain feeling to it and I was really writing to that feeling and imagining
imagining the world that I wanted to create based on what the music was sort of doing inside of me.
And so the first song I wrote, I wrote a song about what it might be like to be an AI in the future
who's so intelligent that she doesn't know whether she's AI or human. And the song had a lot of
synth elements and kind of quirky machine-like sounds. So it felt like it fit. And then once I was
done with the lyrics, then I went back to the song and I said, is there any part of this that
doesn't really work or that needs to be changed? And so I went back in and iterated. So it was a
little bit of a back and forth process, but definitely the lyrics and even the vocal melodies,
those were all born out of the music that had essentially been finished. At least the melodies were
in a place that I was really happy with. And then when I'm working with the tool like IBM Watson,
it's quite a bit different. I wrote a song recently with a couple hundred people, which sounds absurd,
but and it kind of is, but it was really fun, utilizing some blockchain technology. And we basically,
we had people submitting song lyrics in a certain category. We decided we wanted to write a song that
was essentially the blockchain anthem for this new generation of people who kind of want to do
things differently, want to break the rules, want to build something from scratch again,
and just write something that kind of had that impact for them. And so we did that. And that was
just based on submissions of lyrics from everyone. And then I decided to start ingesting everyone's
favorite anthem into IBM Watson. So I actually asked the group, what are your favorite anthemic
songs from the 1700s, 1800s, 1900s? I don't care, just give me an anthem. And we did once I
transpose those songs into middies, I was able to feed it into Watson and then Watson was able to
learn from those anthems and create something new out of it. And I found that to be really fun,
like giving the software inspiration collectively from the group that represented something to them.
And so the result is a mixture of sort of an anthemic synth pop song. And I still wrote the lyrics
had been done, but I really made sure that whatever IBM, whatever Watson was spitting out,
kind of fit the lyrics. So there were a few songs that had to throw out even though I liked them,
just because they just weren't good foot fits. And so I think the process can go both ways.
What does it mean to feed those MIDI files into Watson? For example, which of the Watson products did
you use? I used Watson Beat, which you can currently download on GitHub. It runs entirely
in terminals. So it's not the most user friendly. It's not like Amper where anyone can just go in
and use it. You have to have some some basic knowledge of code. And there's a you know, there's a
user guide in the GitHub station that you can use to also find it. But you can feed a number of
different things into the system. You can feed it data, but you can also utilize some of the existing
MIDI tracks that they have available to you. Like you can make a song based on the
the learnings of Mary had a little lamb or other royalty free songs. And then you can set
parameters similar to how you can do with Amper around BPM, key, the instrumentation at what point
section two or section three or section four of the song begins and ends. There's actually a lot
of differentiation that you can utilize with Watson. And what's the what's the relationship
fundamentally between the the MIDI files, the data that you're feeding into this tool and what
it's producing at the on the output? And where does really artificial intelligence come in as
opposed to just some kind of computer generated aggregation of stuff that you've provided?
Right. Well, I'm sadly not the programmer using it. So I can only tell you what I have been told.
And I would love for other AI programmers and researchers to look further into some of these
tools and and actually analyze, you know, their integrity in terms of of what type of network they
are and and how they work. But from what I understand with Google magenta with both Google magenta
and IBM Watson, they study the patterns in the music that it's being fed. And so from those
patterns, they can discern what types of music should be coming next. It sounds like a lot of
its statistical analysis running, you know, if you have a G chord, then 70% of the time you'll move
to a D. If this is a anthemic pop song versus if this is a sad ballad, you know, 30% of the time
you'll move to a D. And then it will sort of transpose and learn from there. That's the general
model. But with magenta, you can feed it a lot more music in a grouping. And same with Ava,
which is another, it's an orchestral AI where they still feed it up to 10,000 songs at one time.
So there's a lot more generative analysis. Okay. So the idea is that you're feeding
these tools, some input data and it's going to produce output that's similar to
whatever the input tunes that you're giving it. But within, you know, some sort of parameters or
constraints that you've. That's right. Because you're applying new constraints or parameters,
like new styles, new genres on top of an existing set of music that gives it some sense of statistical
patterning. And so your inspiration for this was coming out of the YouTube world and kind of going
for volume or quantity over quality. Is that do you see that as being really the only role for
this type of creativity or is there? Definitely not. Yeah. So I mean, like using AI is not fast or easy
to make something good takes a significant amount of time. I think what I intended with my
statement was to demonstrate that there's a whole new group of people out there who I think
as a result of the internet era, really, we've just seen a whole new group of people who
do in order to, to wear a hundred hats, which they typically do now in blogging and in video
creation, they have to find really great tools to do so. And I'm not a fan of like the fast, quick
and dirty way of doing things because to me, I really want to make great art. And I really want
to tell great stories and doing things super quickly doesn't necessarily work. But being able to
have complete control over your over the entire process can be really important for artistry.
And so I know in my case, like if I want to make a song on my own, I've got to call up a music producer.
I probably have to pay them a couple thousand dollars if they're good. And I've got to drive halfway
across the 405 in Los Angeles to get there and record with them. And there's nothing wrong with
that model, but that model is inherently prohibitive. It keeps a lot of people from being
able to create art. It certainly, there's a, you know, high barrier to entry in terms of financial
resources and or skill sets that are required to do that. And so now I can use these tools to actually
make music on my own. That's really exciting. And I think that that same kind of hacker DIY
mentality has, has proliferated across, you know, all types of content creation because of,
because of the internet and because of the tools that we have at our disposal. And everyone just
wants to, everyone wants to create. I mean, if we look at like photography alone, right? I mean,
thanks to filters and Instagram. It's like everyone's a photographer. And that might be a bad thing
for photographers. So, but it's also a really good thing for people that want to express themselves
and have a hand in the creative process. So who are the photographers?
Thousands of people who post on Instagram and think they're photographers. You know, or who
who are able to make decent photographs, who may not have been able to do, I mean, look at all,
even like it's crazy to think about the, the, the huge boom in, in makeup over the last five,
six years because of YouTube makeup tutorials. It's one of the largest categories of videos
online. And so all these girls, they, they have learned new ways of creating makeup looks. And so
they're like makeup artists in their own right. And I'm sure that it's been pretty hard on the makeup
artist community. But so all of these things are, are both suffering and benefiting from these
new tools that allow for us to create quickly, easily. But I do think that everywhere you find
these new tools, you also find artists who are using them and spending hours and hours and hours
hours to make something awesome with them. So like, even for me, I would not say that my time
has been reduced and being able to make this album. You mentioned so far,
Amber Watson Magenta and Eva Eva, are there other tools that you've come across or used in the
process of creating this album and AI tools in particular? There are, there's a, there's a tool
called Lander, which allows, it's an AI tool that masters the music, which is essentially a
process of taking a finished mix and then pulling out certain high and low decibels within the range
for like a radio or a surround stereo mix. That's typically a process that's expensive and
and you need like a very specialized audio engineer to do. There's also a company called AI Music
out of the UK, which is doing some very interesting things. I'm not sure if they've launched yet,
but it will, I believe allow people to, I saw an early demo of it, which will allow people to
sing into a microphone and the AI can analyze the melody and basically play music in tandem
while you are singing. So you kind of have a live duet partner. So it's so to speak. And then
there are also tools through the, through both Google and, and IBM that do other things in the
music sphere like AI duet or incense, which allows you to create new sounds by combining sounds
like a cat and a harp together. Now you can have a new instrument called a carp. I mean, there's
all kinds of wacky things that have a variation of of uses, but I'm trying to utilize as much as I
can on the album in fun ways, but I'm sure that we'll just see more and more time goes on.
So also juke deck, juke deck is another UK based company that's great for jingles and very
quick kind of little diddies that you might use for a corporate video or something like that.
What are the things that you've learned, maybe the top two or three things that you've learned
that, you know, might be useful for someone who wants to experiment with AI-generated music?
Top two things I've learned. Well, I would say you have to, oh my goodness, I've learned a lot.
I think it just depends on the software that you're using. There aren't a lot of people making
music right now with AI software, so it takes a lot of trial and error. There aren't any great
user guides. I think understanding the limitations of each program is important because you can really
devise a strategy based on those limitations and make what you feel is the best possible thing
that you can make with that software. I would also say that it's good to go in with a general
idea of what kind of style and tastes you might have in the music sphere. So, for instance,
when I use Amper, I really focus on cinematic and symphonic electronic sounds.
I really like kind of soundtrack, movie soundtrack sounding music, and so that was my
my north star in creating. Otherwise, you just end up with way too many options,
and it's hard to actually boil something down that you like. Was that related to that particular
project or a particular strength of Amper? I think it was both strength of Amper for sure.
I went through a bunch of their styles and I found that the cinematic style was one of my favorites,
whereas other styles might have been weaker in terms of my musical preferences. Then I also just
felt like I wanted this project to have an epic feel to it. I wanted there to be that movie
magic feeling to it, but I also love electronic music, so I had to combine those two things.
There are a bunch of interesting parallels here with Data Science, which is the typical
conversation topic on this podcast. One of the things that you mentioned was
just the iterative nature of creation with these tools, and that's certainly the case for folks
that are trying to solve business or engineering types of problems with these AI tools,
and you also mentioned just understanding the limitations of the tool, and I guess that's
important with any use of any tool, particularly technology tools. Are there any other observations
like that that you've come across? I think that's primarily it. What I found is that the AI
is not just giving me data. It's giving me new sources of inspiration. What I try to do
is stretch outside my comfort zone, because if I were to collaborate with someone here in LA,
for instance, let's say I was able to find a couple thousand dollars to collaborate with a record
producer here. The likelihood of that person being a pop producer who's trained in radio,
pop hits that understands that formula is very high, because that's what you're going to find.
Whereas if I'm working with an AI, depending on what software or program I'm working with,
and the parameters that the engineers have set, and depending on what type of data I feed it,
I could get something totally random. That's exactly what I found with all of these programs,
to varying degrees, is that there's a lot of randomness that comes out of collaborating with
this software. I love that because I think what it does is it forces me to think outside of my own
box, and it gives me a new collaborative partner that's not in obvious, that's not going to go in
an obvious direction. I guess I would just encourage people to think about these new tools as really
new and unique sources of inspiration, and it might be the thing that's very strange or offbeat
that is actually the most brilliant seed of an idea. We're clearly just at the beginning of
all of this. Where do you see it going based on your experience? Creating this album,
working with these tools, do you have a sense of the direction this will all take?
Oh my goodness. Yeah, a little bit. In the same way, when I started making YouTube videos,
I thought to myself in a few years, everyone is going to be a YouTuber. Not everyone, but I
thought a lot of people are going to become content creators, because making content is fun,
and be because people love to express themselves, and they want to be seen. Before YouTube was around,
you couldn't just be seen. There were huge gatekeepers. You had to live in LA, have an agent,
be going out on additions. That was the only way that you could actually have your material seen,
and all of a sudden you had this platform where millions of people could listen to you, whether or not
you deserve to be watched. It was another question, but I think with these new AI tools,
you'll see the same thing. Specifically with music, I think we'll see a lot of new artists,
a huge democratization of the music creation process. It's just becoming so much easier
to make something that sounds great without all of the super expensive tools that you needed 10
years ago. They had to go into a studio to use. Some people will say that's not a good thing,
and maybe that's the case. I think there's an argument on the other side as well,
that everyone deserves their shot, and we are all artists that deep down want to create,
and make stuff. That's part of the human experience. I think we'll see a democratization
in that regard. Honestly, what I'm most excited about is the fact that out of this democratization
will inevitably come new forms of art that we can't even conceive, whether that's figuring out
how to create music that shifts someone's mood through biotracking devices, or is immersive
musical art in the form of not just sounds, but three-dimensional visuals, augmented reality,
virtual reality. I think we'll see a whole new crop of artists come up that are not just
music artists or visual artists or video artists, but create an entire experience for someone.
I think that that's really what we have to be excited about is the fact that every time we get
really good at one thing, e.s. humans usually figure out something new as a result of that,
and that becomes part of our culture. It just occurred to me that this would be a good time
to throw this out there for our audience, really. Maybe you can give them some tips if anyone
wants to take on this challenge. My producer and editor are not huge fans of the royalty-free
intro music that I use here at the podcast and have used for the past couple of years. We really
should have an AI-generated track, AI-generated intro theme song. If anyone in the audience wants to
take that on, you are certainly welcome to do that. Taren, do you have any tips as to where they
should start? Where would you say your audience sits on the spectrum of AI capabilities?
I'd say they're pretty capable. They tend to be fairly technical and fairly excited about AI.
Awesome. Well, if they have any musical experience whatsoever with the DAW workstation, like logic
or protocols or garage band even is sufficient, I would recommend they use either Google Magenta
or IBM Watson's tool, just because they'll have more control over the inputs and parameters,
and they can actually go in and change the code if they so choose. If they are very AI proficient,
but musically not proficient, then maybe Ampers a good place for them to go, just to play around,
and they could certainly make you something very quickly. So I would recommend any of those
tools to start. And I might add that if anyone else stumbles upon anything new, just because I'm
head down with the album right now, please send it my way. They can just tweet it to me. I'm always
excited to learn about new interfaces. And I'm also simultaneously running a contest for the last
song on my album. I'm putting it out there so we can also extend this to your audience as well
that anyone who writes a composes a track that I like using one of the tools that are out there
with AI, I will co-write the song with them and they will be a co-writer on that track on my album.
So taking submissions up until June 30th. Awesome. So two AI music contests announced right here
the Twimmel intro contest and Taren Sutheran's last track on her album, AI Music Contest. And
if we have any overachievers in the audience, what you should be doing is writing a composition
that can serve as both the Twimmel intro track and Taren's last track on her album and maybe
she'll write the track about the podcast or something. Awesome. Awesome. Well Taren, it's been really,
really great to get to chat with you about this. Thanks so much for taking the time. Thanks Sam,
you too. Take care. All right everyone, that's our show for today. For more information on Taren
or any of the topics covered in this episode, head on over to twimmelai.com slash talk slash 139.
Thanks so much for listening and catch you next time.
