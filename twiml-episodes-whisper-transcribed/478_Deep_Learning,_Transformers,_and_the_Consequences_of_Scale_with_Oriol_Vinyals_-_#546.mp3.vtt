WEBVTT

00:00.000 --> 00:12.880
All right, everyone. I am here with Ariel Vinyals.

00:12.880 --> 00:17.520
Ariel is the lead of the deep learning team at DeepMind.

00:17.520 --> 00:20.240
Ariel, welcome to the Twomo AI podcast.

00:20.240 --> 00:23.120
Hey, Sam. It's great to be here. I'm a big fan of the show.

00:23.120 --> 00:26.720
Thanks so much. I'm really looking forward to chatting with you.

00:26.720 --> 00:32.480
This conversation is long overdue. I'd love to get started

00:32.480 --> 00:35.120
by having you share a little bit about your background

00:35.120 --> 00:37.920
and introduce yourself to our audience.

00:37.920 --> 00:42.560
Yeah, absolutely. I mean, this could go a long time,

00:42.560 --> 00:44.960
so I'll try to keep it maybe at a maybe more

00:44.960 --> 00:48.240
recency bias, I guess. But yeah, I've been in the field of

00:48.240 --> 00:51.680
machine learning deep learning since it was quite not as

00:51.680 --> 00:55.600
popular as it is today. So it's been definitely a fun journey.

00:55.600 --> 00:59.120
I'm happy to maybe link this back at the end of our conversation

00:59.120 --> 01:04.000
a little bit. But you know, in the nutshell, I've been

01:04.000 --> 01:07.920
maybe the main passion I've had always is with sequence modeling.

01:07.920 --> 01:11.360
So I started in speech recognition where a lot of deep learning

01:11.360 --> 01:16.000
early days things were going on. And then I transition from

01:16.000 --> 01:20.400
doing my PhD in California in Berkeley to joining Google

01:20.400 --> 01:24.320
Brain in very early days, worked a lot on actually natural

01:24.320 --> 01:29.200
language processing. We had maybe one of the works that

01:29.200 --> 01:32.400
you know, links forward to many of the research that I'm doing

01:32.400 --> 01:35.120
these days is the sequence to sequence work on machine

01:35.120 --> 01:39.600
translation that we did back in the day. And then you know,

01:39.600 --> 01:43.120
my passion for sequences kind of was developing.

01:43.120 --> 01:46.800
At some point it was a good time to actually go back to Europe. I'm

01:46.800 --> 01:51.600
originally from Spain. So I moved to London in 2016 where I joined

01:51.600 --> 01:55.200
deep mind. And since then, you know, I learned a lot actually

01:55.200 --> 01:58.480
about the reinforcement learning. One of the, you know, most fun projects

01:58.480 --> 02:01.680
I've had, you know, a pleasure to work with a large team of people is the

02:01.680 --> 02:05.200
Alpha Star project, you know, involving creating an agent to play

02:05.200 --> 02:09.200
Starcraft. But actually in that creation, a lot of the sequence

02:09.200 --> 02:13.840
modeling background came back as we use a lot of, you know, LSTM's

02:13.840 --> 02:16.560
transformers, all these kind of models that are very popular

02:16.560 --> 02:21.920
thanks to their performance in NLP. And these days, I'm mostly just

02:21.920 --> 02:27.120
focusing on, you know, leading the deep learning team and trying to

02:27.120 --> 02:31.360
just do the usual things that we like to do in deep learning, which is

02:31.360 --> 02:39.440
try to unlock, you know, state of the art or new horizons and benchmarks

02:39.440 --> 02:42.640
in many modalities and as many modalities as we can.

02:42.640 --> 02:48.480
As deep learning has matured, you know, folks often get a lot more

02:48.480 --> 02:53.040
specialized, you know, leading the scope of deep learning,

02:53.040 --> 02:58.720
writ large seems like a very big scope.

02:58.720 --> 03:03.520
Yes, indeed. I mean, it was fun because the way the way it all started, right,

03:03.520 --> 03:08.400
it was literally a lot of like proving yourself in very specific fields,

03:08.400 --> 03:13.920
even very specific applications in those fields. So the very first

03:13.920 --> 03:18.160
results that caught people's attention were actually on speech

03:18.160 --> 03:22.080
recognition. So that started actually mostly in, you know,

03:22.080 --> 03:25.920
thanks to the Toronto group led by, of course, Jeff Hinton.

03:25.920 --> 03:29.760
I got to learn that through my internships at Microsoft Research,

03:29.760 --> 03:33.920
where a lot of action was happening. And then, you know, maybe the most

03:33.920 --> 03:37.440
obviously notable moment that most people would refer to is the image

03:37.440 --> 03:41.040
net moment that really happened almost three, four days, you know,

03:41.040 --> 03:46.000
days, no years, these days, time passes very differently.

03:46.000 --> 03:50.080
But yeah, three or four years after that breakthrough,

03:50.080 --> 03:54.640
the image net moment occurred. And then from there on, it's been an adventure

03:54.640 --> 03:59.040
of basically trying to find where are the frontiers

03:59.040 --> 04:03.680
of what deep learning models can or cannot do. And, you know,

04:03.680 --> 04:07.680
through the years, it turns out that by applying more or less the same

04:07.680 --> 04:13.120
methodology, you know, through gradient descent, new

04:13.120 --> 04:17.440
architectures, but refining ideas, adding the right kind of

04:17.440 --> 04:21.840
inductive biases to our models, et cetera, you end up

04:21.840 --> 04:26.080
then transferring to more and more areas that involve, of course, natural

04:26.080 --> 04:31.120
language, machine translation, then now all sorts of like

04:31.120 --> 04:34.960
language modeling, multimodal language, vision,

04:34.960 --> 04:39.520
generative models. One of the recent successes, of course, of the year was

04:39.520 --> 04:43.040
alpha fold, in which essentially all these tools just were applied to

04:43.040 --> 04:47.520
these very different domains. So expanding to more and more domains

04:47.520 --> 04:51.920
of science in general has been now, you know, it's the day to day and the way

04:51.920 --> 04:55.600
to think, perhaps when you're, you know, thinking more deep learning

04:55.600 --> 04:59.360
in 2021 and beyond. I'd love to have you elaborate a little bit on that

04:59.360 --> 05:06.880
when I kind of think about the field, I tend to think about it

05:06.880 --> 05:12.240
in terms of, you know, there's this, you know, set of work that's been like

05:12.240 --> 05:18.000
applying deep learning to new application areas. There has been, you know,

05:18.000 --> 05:23.920
applying a deep learning approach to, you know,

05:23.920 --> 05:28.960
kind of technical field. That's not a great way to put it, but like,

05:28.960 --> 05:32.720
okay, we've got graph machine learning. Like, how do we do that in deep learning?

05:32.720 --> 05:35.520
You know, we've got reinforcement learning. How do we do that with deep learning?

05:35.520 --> 05:40.000
That kind of thing. And then there's been, you know, a lot of energy, just how

05:40.000 --> 05:47.280
do we make deep learning more computationally efficient and, you know,

05:47.280 --> 05:52.880
make it easier to train that kind of thing? Are you working across all of those

05:52.880 --> 05:57.120
areas or do you even, you know, think about it similarly? How do you kind of,

05:57.120 --> 06:02.720
you know, think about that taxonomy? Yeah, I mean, it's one thing that I tend to,

06:02.720 --> 06:07.120
I found it useful throughout my career. And this, I think, the level of

06:07.120 --> 06:14.240
generality has increased over the years. But the, what you try to do mostly if you're

06:14.240 --> 06:19.680
a deep learning researcher, having been in the field for abilities to try to

06:19.680 --> 06:26.320
just identify commonalities across, you know, modalities or problem settings.

06:26.320 --> 06:32.160
And in a way, I have this sort of argument. And I've given more, obviously,

06:32.160 --> 06:37.200
more detailed technical talks at, you know, like recently about what I call the

06:37.200 --> 06:42.560
deep learning toolbox, right? And I think that is a reasonably, you know,

06:42.560 --> 06:47.440
you can see quite a few examples on one of, you know, some of the major successes

06:47.440 --> 06:52.240
recently on applying these toolbox approach. And by a toolbox, I mean,

06:52.240 --> 06:57.120
we have architectures, um, tricks of the trade on how to train the models like,

06:57.120 --> 07:00.960
you know, like optimization methods, how to use the hardware more efficiently.

07:00.960 --> 07:06.160
Like this is all part of this gigantic toolbox. And then when you're faced with a new problem,

07:06.160 --> 07:10.160
right? If you're truly like embracing sort of the deep learning approach,

07:10.720 --> 07:15.920
you're applying almost always the same first principle, which is you learn everything.

07:16.960 --> 07:22.000
Everything is learned and to end from the output back to the input by training a set of

07:22.000 --> 07:28.240
learnable weights through gradient descent in general. And then you just pick a mix and match

07:28.240 --> 07:33.920
from the toolbox. The precise elements beat, oh, I have a sequence. Okay, I'm going to use a

07:33.920 --> 07:39.520
transformer. It's a very long sequence. Maybe I'll use an LSTM. I have vision. I use

07:39.520 --> 07:43.680
compolutions, et cetera, et cetera. So you're mixing and matching all these components.

07:43.680 --> 07:48.640
And then when faced with a new problem, like folding proteins, which obviously has some

07:48.640 --> 07:52.720
similarities, but in the end is like at the input, you have a sequence of letters, right?

07:52.720 --> 07:56.560
The amino acids that form the protein. And then the output are like these

07:56.560 --> 08:01.840
sequence and ordered set almost of 3D coordinates, right? So when you face with this problem,

08:01.840 --> 08:07.360
you say, okay, I have some data that maps these inputs to these outputs that people in the

08:07.360 --> 08:11.280
community has built over the years, very expensive to do because you need

08:11.280 --> 08:16.160
crystallography and some methods to generate this training data. But then with the deep learning

08:16.160 --> 08:21.680
mindset, you just go at the problem and then iterate over ideally a very nice, you know,

08:21.680 --> 08:28.240
training, validation split, as usually is done in machine learning. Then you start playing with

08:28.240 --> 08:34.560
details that matter a lot actually to unlock performance to attack this particular new modality,

08:34.560 --> 08:40.400
right? So I think it's the right framing to think about what is the next modality? What is a

08:40.400 --> 08:45.200
challenge? Is it a very large graph that we currently are at the, you know, we cannot quite do,

08:45.200 --> 08:50.320
because yeah, we have graph neural nets, but do they scale properly and so on? And that is

08:50.320 --> 08:56.640
generally how then you see a lot of papers at Newribs and other conferences kind of tackle

08:56.640 --> 09:01.760
these new challenges. And what's beautiful about this is that even though there is a theory of

09:01.760 --> 09:07.280
deep learning, that's a field I've gotten a bit into like just because it's just fascinating.

09:07.280 --> 09:12.960
But in general, how the field has advanced is there is a problem out there. You take it, you cannot

09:12.960 --> 09:18.320
change it, but you apply this first principle of end-to-end learning, learnable model, powerful

09:18.320 --> 09:25.280
model, and then hopefully you enable something that that field was not, you know, maybe looking

09:25.280 --> 09:29.920
at at the time. Although nowadays many people have heard about deep learning and it's quite

09:29.920 --> 09:34.720
permitting everywhere. It's mostly you don't have to prove yourself in the maybe it's the same way

09:34.720 --> 09:41.360
that we had to by going one field at a time almost as we were doing in 2008, 2009, up to maybe

09:41.360 --> 09:48.800
2014-15, where things really start taking over and people start paying attention, given the successes

09:48.800 --> 09:56.000
where too many may be to ignore. Two, three years ago, a lot of work was happening

09:56.640 --> 10:07.040
in the area of just getting the basic machinery, working, tweaking the optimizer,

10:07.040 --> 10:14.720
tweaking learning rates, all this kind of stuff. Are you still involved in that kind of work?

10:14.720 --> 10:18.720
Do you think that's accelerating as more people are coming in or slowing down as we've gotten the

10:18.720 --> 10:25.200
basic machinery working? Yeah, it's a good question. I mean, going to kind of all the modalities,

10:25.200 --> 10:32.560
right? The ultimate modality if you extrapolate is, well, all the data is just a sequence of

10:32.560 --> 10:38.160
bytes, right? You can always represent any data structure, input or output as a sequence of

10:38.160 --> 10:44.160
bytes. And that is a very kind of romantic, almost way to think about machine learning,

10:44.160 --> 10:49.360
at least supervised learning problem. Kind of a grand unified theory of deep learning or something?

10:49.920 --> 10:56.880
Yeah, yeah, exactly. So in that sense, what has happened is that sequence models have evolved

10:56.880 --> 11:03.840
enough that we have transformers, which might not be the last iteration over the ultimate model,

11:03.840 --> 11:09.760
right? That will rule all the modalities with the same sort of applying the same model,

11:09.760 --> 11:15.920
the same formula, right? The toolbox maybe just reduces to this one model. We're not quite there yet,

11:15.920 --> 11:23.440
but that is one way to see it. And indeed, as the field is advancing, I think the details

11:23.440 --> 11:29.600
are refined enough that more people can just access these toolbox without being maybe

11:29.600 --> 11:36.800
necessary an expert and see successes reasonably in a reasonable, simple way. Obviously, this goes

11:36.800 --> 11:42.880
hand in hand with the fact that our software has also tremendously advanced with all the frameworks

11:42.880 --> 11:49.040
that exist currently and open source, and et cetera, that exist to kind of lower the barrier

11:49.040 --> 11:55.520
of entry to the field. But I would say that, yes, it's easier to get the details right

11:56.400 --> 12:02.000
more so than it was three years ago. Although, I mean, I think there's still quite a lot of research

12:02.000 --> 12:09.200
to be done indeed. Just kind of hinging off of the, you know, this point that you made around,

12:09.200 --> 12:19.440
you know, sequences and transformers and how we're, you know, almost there. Do you have a feeling

12:19.440 --> 12:25.520
or a bet on kind of what, you know, do we get there? What, you know, is transformers the thing

12:25.520 --> 12:30.880
that gets us there? Is it something that's a slight evolution of transformers? You know, what

12:31.600 --> 12:37.200
what dimensions do you think get us there if you think we get there? Yeah, I think transformers

12:37.200 --> 12:43.840
have evolved, you know, in a very natural, cool way from obviously what has come before transformers,

12:43.840 --> 12:51.840
LSTMs, attention mechanisms, et cetera. But indeed, there are also a lot of limitations and

12:52.800 --> 12:58.320
maybe one way, like there's a very computational sort of framing of machine learning, which is to say,

12:59.520 --> 13:04.400
well, we want to have any modality to any modality. Our model needs to not make many assumptions

13:04.400 --> 13:09.760
over the underlying data. So it kind of adapt to all the tasks that we might be interested in

13:09.760 --> 13:15.040
tackling. I think transformers do quite well here. An obvious challenge that if you look at the

13:15.040 --> 13:19.840
probably the amount of papers tackling this, this probably going to be a few, is the fact that

13:19.840 --> 13:25.760
transformers is still requires kind of, it's very symmetric in the way that it looks at all the data.

13:25.760 --> 13:32.560
I mean, if you think of language modeling, for example, it just looks at every single piece of text,

13:32.560 --> 13:37.680
right? And when you're reading a book, that's not how you're kind of ingesting this information

13:37.680 --> 13:42.800
as you read chapter after chapter. So there's some beautiful work, I think, still to be done in

13:42.800 --> 13:50.880
in the memory mechanism being a bit more hierarchical. Maybe that's lose inspiration from how we,

13:50.880 --> 13:56.320
you know, we think we work in terms of ingesting information and, you know, compressing the information

13:56.320 --> 14:01.440
we ingest. We don't remember every single detail. That being said, computers might not necessarily

14:01.440 --> 14:07.280
need to operate like we do, right? So it is possible that, well, okay, like that's not we cannot do

14:07.280 --> 14:12.320
that, but well, the machines can can can do it. So maybe that is fine. But I think computationally,

14:12.320 --> 14:18.720
there are there are definitely challenges that may make transformers have definitely some limits

14:18.720 --> 14:24.400
on the amount of information they can process effectively in parallel as they ingest these sequences

14:24.400 --> 14:31.120
of information or bytes. It sounds like you're suggesting a kind of a higher level attention

14:31.120 --> 14:40.640
mechanism that more more broadly shapes the way that the transformers learning from the data that

14:40.640 --> 14:45.760
it's presented. Yeah, I think, I mean, there again, and this exists, it just that then it's the

14:45.760 --> 14:52.960
matter of what details how the optimization can be made to work. But I think the indeed some more

14:52.960 --> 14:59.520
forms of hierarchical memory from course to find lots of these ideas existed in computer vision

14:59.520 --> 15:05.680
for years as well. So I think there's going to be a good mix of ideas and iterative processes

15:05.680 --> 15:13.520
until maybe the next model that feels maybe more efficient for for other tasks we might not even

15:13.520 --> 15:17.680
be thinking about. That's that's what I was saying. There's a you need to push the envelope. So

15:17.680 --> 15:23.440
usually it goes hand in hand with a new task that we cannot even dream of doing right now because

15:23.440 --> 15:29.680
yeah, the limitations of the state of the art models. But I think hierarchy in memories, one of my

15:29.680 --> 15:35.840
beds, definitely some work, you know, we've done and we might mean the whole research community is

15:35.840 --> 15:40.880
doing that I think is I'm definitely keeping an eye on. You know, also related to

15:42.000 --> 15:50.720
transformers and the impact that we've seen there is the work specifically happening around

15:50.720 --> 15:57.280
large language models. What kind of work are you doing there? Yeah, I mean large language models

15:57.840 --> 16:05.120
is a very fascinating field that you could you could think there's been a huge paradigm shift

16:06.000 --> 16:12.080
or maybe there's been non-depending on how far you zoom in, zoom back in the past, right? Because

16:12.720 --> 16:19.680
you know, if you look even even in the 50s, Shannon like was already intrigued by the fact that

16:19.680 --> 16:25.280
well, if you have, you know, statistical pieces of text, you could you could actually generate

16:25.280 --> 16:31.840
text from end-gram models. And then, you know, you have to go forward to maybe the neuro-language model

16:31.840 --> 16:38.800
error until we start to scaling this up with the data and the models and GPUs, etc. to start

16:38.800 --> 16:44.560
unlocking indeed state-of-the-art machine translation, which already feels a bit magical because

16:44.560 --> 16:50.080
in machine translation, we're asking the model to translate sentences that definitely have not been

16:51.280 --> 16:54.960
in, are not in the training set. As we know, most of the sentences we add are

16:55.760 --> 17:00.320
they're probably unique. Otherwise, we would not be talking to one another because we could always

17:00.320 --> 17:07.280
predict what is some going to say, what is Oriol going to say. So, but what had, but thanks to

17:07.280 --> 17:13.360
these kind of mix of components, I think the last definitely 10 years, last five years, and then

17:13.360 --> 17:19.360
transformers being the last maybe ingredient that has been added to this mix toolbox, you know,

17:19.360 --> 17:26.880
build that I was mentioning. We've gotten to language models that, yeah, we can sample from and

17:26.880 --> 17:32.560
we can query, but it starts to feel like, oh, this is, if we were talking to someone, right,

17:32.560 --> 17:37.280
behind the scenes, like going back to obviously Turing and Turing test ideas, although maybe that

17:37.280 --> 17:43.040
is not that useful these days, but just thinking like that, you have this pre-trained language model

17:43.040 --> 17:50.240
that you can start treating like an entity that can obviously utter any text, and that's powerful

17:50.240 --> 17:55.760
if it's good enough. And I think that's what changed. We were doing this all along, I would argue,

17:55.760 --> 18:02.240
in NLP for many years, many people have obviously tried to model language. So the basic principle

18:02.240 --> 18:07.280
exists. I mean, you could always query the model to, you know, ask it a question and see how it

18:07.280 --> 18:13.760
reacts. Does it know about color of the sky, et cetera? But recently, thanks to scale, there's

18:13.760 --> 18:20.640
what it's true is that the performance has been so good. And like the change from like the,

18:20.640 --> 18:26.720
you know, millions to billions of parameters has triggered like now, okay, a new dream almost of

18:26.720 --> 18:32.880
what else could we do if we kept scaling on the one side? And also like, look, it's unbelievable

18:32.880 --> 18:38.080
that this that felt like maybe at that end or like sure, we could use language models in a more

18:38.080 --> 18:44.320
complex system that was doing, you know, the traditional chatbot building with rules and so on.

18:44.320 --> 18:50.880
Now, this actually feels like it's in a state that for some applications and with lots of

18:50.880 --> 18:58.400
caveats actually can actually be used and feels like a very powerful tool. And indeed, I mean,

18:58.400 --> 19:05.680
open AI being probably the one that has pioneered this more notably has shown some demonstrations,

19:05.680 --> 19:13.600
right? So at DeepMind, we are definitely looking into this. I mean, the space of large language

19:13.600 --> 19:19.120
models is extremely exciting. And actually, if you look a bit at the history and perhaps

19:20.720 --> 19:25.600
things like you see in projects such as the one I was involved with, which is a StarCraft,

19:25.600 --> 19:31.280
the way I always thought about StarCraft coming from where I come from is that it's just modeling

19:31.280 --> 19:37.600
sequences of words. These words don't are not like English words. They're like instructions on,

19:37.600 --> 19:42.640
you know, that you send to the game engine, like move this piece here and it's a very rich language.

19:42.640 --> 19:49.280
It's an API like language. But you could already start seeing the power of these methods beyond

19:49.280 --> 19:56.160
language to like decision making and agents. And I think this is there's a lot of interesting

19:56.160 --> 20:02.720
parallels that we are already witnessing by like, you know, maybe a like human level, like go

20:02.720 --> 20:07.680
actually the very first steps in AlphaGo where indeed as well similar to modeling,

20:08.560 --> 20:13.920
precise modeling of the probability of the next word in Go, the next word is just a two-dimensional

20:13.920 --> 20:20.720
like position of where you would put the next, you know, piece. But in general, this principle has

20:20.720 --> 20:27.680
been there since ever and then the all that it took is for the performance to be at the level of,

20:27.680 --> 20:34.400
wow, you can really play a game or reply to almost anything you can ask these models and you will

20:34.400 --> 20:40.640
get somewhat sensible replies sometimes enough that this speaks now the interest of many more

20:40.640 --> 20:47.680
and the field is indeed expanding a lot which is super welcome. So you mentioned Starcraft

20:47.680 --> 20:57.920
once again and you are you've got a workshop paper at Nurebs that is a follow-up to the Alpha

20:57.920 --> 21:07.840
Star work. That was maybe 2019 the Alpha Star. Can you maybe give us a refresher on that work and

21:07.840 --> 21:14.720
then talk about what you are presenting at Nurebs this year? Sure, yeah, I mean the I think DeepMine

21:15.360 --> 21:21.360
as a company obviously had its own different stages where, you know, the very early beginnings

21:21.360 --> 21:27.360
on Atari just showing that deep reinforcement learning has to prove itself, right? It's a bit like

21:27.360 --> 21:34.320
deep learning but now a bit more specific. So a way to prove itself is actually indeed to master

21:34.320 --> 21:39.280
ever more complicated domains or environments as we call them in reinforcement learning instead

21:39.280 --> 21:45.200
of being datasets you are optimizing a reward but ultimately actually a lot of the deep learning

21:45.200 --> 21:53.840
kind of main components or ideas apply. So you saw this kind of kind of one at a time, right? Almost

21:53.840 --> 22:00.480
as a curriculum of increasingly, you know, domains like from Atari to Go and Chess and then,

22:00.480 --> 22:05.120
you know, ultimately I would say Starcraft being, you know, much more complex in many different

22:05.120 --> 22:10.240
dimensions as a as a game or a video game. That's what we kind of were doing at DeepMine and maybe

22:10.240 --> 22:16.640
that's the Alpha Star project that regarded Starcraft 2 which is a popular real-time strategy game

22:16.640 --> 22:22.400
was the end of that sort of sequence of demonstrations of, well, these deep parallel principles

22:23.440 --> 22:28.560
really apply to all the domains that we have found interesting in these sense of

22:28.560 --> 22:34.080
games that are complex. They require like a synchronous thinking, partial observability,

22:34.080 --> 22:39.600
all the right kind of interesting properties that may be the same way computer vision went

22:39.600 --> 22:44.320
from emnis which was very interesting until, you know, it became soft and then we moved over,

22:44.320 --> 22:50.720
right? So this is kind of a parallel. I like to kind of visualize almost in this complexity versus

22:50.720 --> 22:57.760
the kind of game that DeepRL was stacking. And Alpha Star essentially did this through an approach

22:57.760 --> 23:04.000
that was not purely deep reinforcement learning. It actually employed imitation learning or offline

23:04.000 --> 23:10.000
reinforcement learning thanks to the massive amount of games that when humans play one another

23:10.000 --> 23:15.520
I recorded anonymously by the company that makes the game. So there's a huge wealth of sequences

23:15.520 --> 23:20.720
of observations and actions, right? That as I said, you could see as a bit of a language

23:20.720 --> 23:28.080
just that expresses moves in the game, right? And then that was kind of the first seed of Alpha

23:28.080 --> 23:34.560
Star, how we reach ground master level at the game was, well, let's take a look at these sequences

23:34.560 --> 23:39.600
that we have at scale. There's actually millions of sequences. So we have a lot of data which is

23:39.600 --> 23:45.760
great in deep learning the more data, the better. And then we learn to imitate which essentially

23:45.760 --> 23:52.000
is applying the same principles as language modeling. So given all the words that we've seen and to

23:52.000 --> 23:56.560
until some point, we're trying to predict the next work or in the case of StarCraft, we're trying

23:56.560 --> 24:02.480
to predict the next move, which is just kind of a complex object like move this unit

24:02.480 --> 24:07.280
onto this position in the map, right? But it's actually very similar to modeling language.

24:07.840 --> 24:14.080
And that first agent was reasonably good. It was actually better than most humans in terms of

24:14.080 --> 24:20.240
the median performance. But then we took it to the next level by then initializing a self kind of

24:20.240 --> 24:27.280
play system, a multi agent system in fact, of many agents that developed different skills and

24:27.280 --> 24:33.920
different strategies in the game. And they played each other for many years actually, like there's

24:33.920 --> 24:40.160
like each agent kind of plays a StarCraft over like 150 years or so. That's what we did in the

24:40.160 --> 24:46.400
nature paper. And that achieves from this median level or above median level to really like the top,

24:46.400 --> 24:51.920
the very top top level of play that then we verified and we published, as you said, indeed in the

24:52.880 --> 25:00.480
in the end tail of 2019. So that was very cool. And obviously our goal was can we just really crack

25:00.480 --> 25:05.200
this game, right? So we took this kind of hybrid approach of initializing the model to imitate

25:05.200 --> 25:11.520
humans and then taking off by just doing self play. And in a way, imitate how humans have discovered

25:11.520 --> 25:17.920
the game by playing online against each other. Recently, we were both motivated by first actually

25:17.920 --> 25:24.560
trying to pose StarCraft or imitating human moves as a challenge for those who study offline

25:24.560 --> 25:30.320
around, which is this area of reinforcement learning where you're not allowed to interact with

25:30.320 --> 25:35.520
the environment. You you can observe kind of agents or people interacting with environments,

25:35.520 --> 25:41.360
but you're not allowed because maybe it's not practical to perhaps go there and to plug your agent

25:41.360 --> 25:46.400
in the environment and try to see if you get a reward. So that's a fascinating area. And what we

25:46.400 --> 25:52.800
thought is look, we have one of the richest offline around data sets, right? Millions of trajectories

25:52.800 --> 25:59.840
from humans of all levels playing the game would it would be great to first try to open this as a

25:59.840 --> 26:04.800
challenge for the community. So one thing we're working on very hard these days actually is to,

26:04.800 --> 26:10.960
you know, finalizing open sourcing. So, you know, anyone can just go download the the code and

26:10.960 --> 26:17.040
just train their own agent based on these imitation principle only. So we're only looking at the

26:17.040 --> 26:21.760
the very kind of first beginning of the whole Alpha Star agent. But at the same time, we were

26:21.760 --> 26:26.960
wondering, look, can we push the performance of these agents? We didn't care to do it at the time

26:26.960 --> 26:32.240
because we knew that multi-agent and self-play and reinforcement learning were going to be successful

26:32.240 --> 26:39.680
at making the agent better. But could we do that? Only imitating human moves. And that is kind of

26:39.680 --> 26:45.440
almost the same if you think of language modeling that you're only imitating next word prediction.

26:45.440 --> 26:50.160
You're never training these models further, but by themselves are quite good. And the answer is

26:50.160 --> 26:58.800
you can push performance. We definitely have beaten like the agent that we published, the initial

26:58.800 --> 27:03.920
agent that was learned to imitate. With some further refinements, we use Mu0, there are details

27:03.920 --> 27:11.360
in the paper that folks can go and read. But with some ideas, very key ideas using further than

27:11.360 --> 27:18.480
just imitating the next move. So from the offline RL community, we're able to beat that first Alpha

27:18.480 --> 27:23.680
Star agent that was very good and see that, of course, what yielded the nature of publication

27:23.680 --> 27:31.120
performance by, I think, over 90% of the time. So there is performance to be unlocked, and this

27:31.120 --> 27:37.360
is only the beginning. We tried a few ideas, but I think this is a very fruitful resource for those

27:37.360 --> 27:42.320
who are interested in this field of offline reinforcement learning to maybe go and tackle

27:42.320 --> 27:47.120
this challenge and hopefully with the source code available. And just the fact that it's such a fun

27:47.120 --> 27:53.440
game to observe and a cool domain, people might go and obviously then take it on and try to

27:54.400 --> 28:00.080
get to a next level of performance just with imitating human moves, which is fascinating to me at

28:00.080 --> 28:10.720
least. Do you think of at least in this context the offline setting and imitation as synonymous?

28:10.720 --> 28:18.000
It seems like they largely are, but that you could also envision other types of processing of

28:18.000 --> 28:24.800
the data set that has some benefit beyond just the imitation itself. Yeah, so I think, I mean,

28:24.800 --> 28:28.960
there's a lot of names. I mean, people call this supervised learning, behavioral cloning,

28:28.960 --> 28:35.040
imitation learning. I think what's powerful about, if you restrict yourself to, I will not interact

28:35.040 --> 28:40.720
with the environment, but I am able to see like what agents interacting with the environment achieved.

28:40.720 --> 28:47.120
I think that the powerful and what offline are really poses is imitation is one part, right? So

28:47.120 --> 28:53.120
imitating the actions very well is what language modeling regards with and it's very important.

28:53.120 --> 28:59.680
But there is also the reward that we observe in a game like StarCraft, who won the game? And you can

28:59.680 --> 29:05.200
also predict the winner, right, offline. You can say, look, I mean, we have this game to players

29:05.200 --> 29:09.600
playing. We have every single action they took. Let's try to imitate that to understand that

29:09.600 --> 29:15.440
very well, like we understand language. But also there is a fact that one of them won. And if we

29:15.440 --> 29:21.040
can model that precisely with a value of action, which is obviously a crucial element of most RL,

29:22.000 --> 29:28.720
then we can even in an offline setting try to find an action that is not just human like, but

29:28.720 --> 29:34.080
maximizes the probability according to our own estimate of the value and maximizes the

29:34.080 --> 29:40.160
probability of winning. And in fact, one of the best performance agents we showed uses Mu0,

29:40.160 --> 29:48.000
which is offline. We never use self play, but Mu0 basically tries to model based on

29:48.000 --> 29:53.920
simulating actions that you may take in the future and then pick those that maximize reward

29:53.920 --> 29:59.520
or value, but according to your own estimates, right. And that extra step we didn't take at the

29:59.520 --> 30:05.120
time. But many people in offline RL obviously are studying not only actions, but also estimating

30:05.120 --> 30:12.720
the reward or value. We found that already enabled this level of performance that we didn't

30:12.720 --> 30:18.000
unlock at the time we were doing the project. But I'm sure there is more on how you train the values,

30:18.000 --> 30:22.560
how do you train the actions, how, what the losses are. There's a lot of toolbox actually

30:22.560 --> 30:29.280
components to be discovered perhaps. And, you know, the usage of benchmarks is critical to advancing

30:29.280 --> 30:34.880
this. So there are quite a few benchmarks in offline RL already. I think StarCraft poses an

30:34.880 --> 30:40.880
interesting one given its complexity in action space and the fact that it's partial observable

30:40.880 --> 30:45.280
and some properties that make, you know, this a unique environment like many others.

30:45.280 --> 30:51.920
So what degree are you seeing, you know, the learnings from the work that deep mind and others

30:51.920 --> 30:58.880
are doing around games kind of translate to, you know, real world, non-game scenarios.

30:58.880 --> 31:05.520
Yeah, that's a great question because I think we're seeing quite a lot like, for instance,

31:05.520 --> 31:12.240
like actually like thinking of Alpha Star and then Alpha Fold. A lot of the work we did in Alpha

31:12.240 --> 31:19.040
Star early days, right. We, you know, transformers just had come up. So we started investing

31:19.040 --> 31:24.880
or seeing maybe good performance with transformers. Maybe the first project that we saw that was Alpha

31:24.880 --> 31:32.000
Star actually. And then transformers and self-attention and some further tools that were developed

31:32.000 --> 31:37.200
specifically thinking about the protein folding problem were developed. And, you know, there's

31:37.200 --> 31:41.440
loose inspiration, right, by the fact that, hey, we know that there's this group that found this

31:41.440 --> 31:46.560
model that, you know, a different research group that the great part of research is you take at

31:46.560 --> 31:52.160
learnings from not only your own company, but of course, any other research institution. And,

31:52.720 --> 31:56.720
you know, loosely speaking, right, there is always, you can find always these connections, right,

31:56.720 --> 32:03.760
that from one project learnings, then it goes to others and so on. From games and reinforcement

32:03.760 --> 32:11.280
learning, actually, I see a shift now with many great examples of just applying the same

32:11.280 --> 32:17.440
areal techniques to other places. There's some that we are applying in language, right,

32:17.440 --> 32:23.280
machine translation, for example. It's difficult, like we have a few works. I'm not sure they're

32:23.280 --> 32:29.920
quite there in terms of breaking the state of the art, but certainly, you know, there, you know,

32:29.920 --> 32:34.480
because you have this, again, going back to maybe the beginning of the conversation,

32:34.480 --> 32:40.080
these deep learning approach that the tools must be generic, then anything you discover in any

32:40.080 --> 32:45.280
specific domain, because you tried not to be very domain specific, then they will naturally

32:45.280 --> 32:49.680
translate to other domains, ideally, right. And perhaps one of the things that

32:50.800 --> 32:55.760
in StarCraft, maybe it's a very simple idea, but I think this one has a lot of potential when

32:55.760 --> 33:02.000
you mix this idea of imitating humans with reinforcement learning refinement, is that we added

33:02.000 --> 33:08.800
distillation laws, which is a pressure of the areal model to actually look still even if it wants to

33:08.800 --> 33:13.680
change the actions because reinforcement learning is a different laws and it's doing different things.

33:13.680 --> 33:19.440
You make some pressure for the model to always sort of imitate a little bit the policy that

33:19.440 --> 33:25.440
it started from that imitates humans. And that was critical there. And in fact, in a lot of now

33:25.440 --> 33:31.200
language model applications, for instance, this principle, I see it kind of sprinkled around,

33:31.200 --> 33:36.720
like because it's quite natural to not want the model to diverge. And this comes from obviously

33:36.720 --> 33:43.040
all their work on model distillation, knowledge distillation, et cetera. So it's, as I said,

33:43.040 --> 33:47.600
there's always these tools that ideally you purpose for some particular reason initially,

33:47.600 --> 33:53.840
but the consequences or the applications later on along the line, they're going to be unprecedented.

33:53.840 --> 33:59.680
And I mean, Transformers is a great example of having been developed for machine translation

33:59.680 --> 34:06.240
now suddenly, they're folding proteins. Even the authors of these papers, it's just sometimes

34:06.240 --> 34:10.560
hard to believe probably, like, how is this happening? And it's a bit random and you need a

34:10.560 --> 34:15.600
bit of lag and the right titles in the papers. And there's a lot of interesting actually randomness

34:15.600 --> 34:20.800
in the field, which makes nearly such an interesting conference, actually, and of all the other

34:20.800 --> 34:30.320
conferences in ML. But yeah, it's cool to think that these are basically in the end tools that we

34:30.320 --> 34:37.600
want to apply generally anywhere. And many times we see these successes transfer over very

34:37.600 --> 34:43.920
surprising areas that even the original inventors did not anticipate. One of the historical challenges

34:43.920 --> 34:50.160
with reinforcement learning, deep reinforcement learning in particular is the sample inefficiency.

34:50.160 --> 34:56.880
You know, that gives rise to topics like fuchsad or related to topics like fuchsad and one-top

34:56.880 --> 35:04.240
learning. You've got a poster at Nureps that is looking at multimodal fuchsad learning

35:04.240 --> 35:10.720
with language models. Is that in the RL context or separate from RL?

35:10.720 --> 35:18.000
That's not in the RL context. But it is definitely in the imitation learning context. And then it's

35:18.000 --> 35:25.120
it's proposing a way to try to leverage a large amount of data and a big language model that we

35:25.120 --> 35:32.720
trained, right? Only on language. But then can this language model with a little bit of extra training

35:32.720 --> 35:39.200
be then tuned to be able to talk about images that it takes as its inputs, right? And I mean,

35:39.200 --> 35:44.320
this is fascinating because it links to fuchsad learning, which is an area that actually has

35:44.320 --> 35:50.000
one of the very first works I did when I joined in mine. There was all these metal learning

35:50.000 --> 35:55.680
papers coming from from the groups and I was very impressed. So one of the works we did was working

35:55.680 --> 36:01.760
on fuchsad learning. What happened with the work is that we propose a new method that I think

36:01.760 --> 36:06.640
it's reasonably simple. But but actually the benchmark proposing that paper is what made,

36:06.640 --> 36:12.480
you know, this this paper more maybe widely known for the benchmark it proposed, which is called

36:12.480 --> 36:17.520
Mini ImageNet. And you know, Mini ImageNet is the task of fuchsad learning where you know,

36:17.520 --> 36:22.160
you just have a few images from one class you've never trained and you know, the the goal is to

36:22.160 --> 36:27.040
classify it better at that chance, right? And you know, there's been a lot of work following up

36:27.920 --> 36:32.320
nicely defined clear benchmark for the community. And the cool thing here is that

36:32.880 --> 36:39.280
without even training at all to do Mini ImageNet or fuchsad learning for images, these language

36:39.280 --> 36:45.360
models, right? Just through imitating just generic knowledge that I found on the internet about

36:45.360 --> 36:51.600
people talking to one another and so on. And with a little bit of fine tuning to understand correlations

36:51.600 --> 36:56.640
between images and language, namely captioning, image captioning is what we pre-trained these models

36:56.640 --> 37:02.320
with. But by freezing the language component, you can then reuse this model to not only do image

37:02.320 --> 37:08.000
captioning, but also Mini ImageNet was one of the tasks that we present in the paper. And it's

37:08.000 --> 37:13.680
impressive that it's not only doing fuchsad learning better than chance, but it's never trained

37:13.680 --> 37:18.960
with the purpose of doing fuchsad learning. You're in a way borrowing the capabilities of fuchsad

37:18.960 --> 37:25.200
learning from the language modeling. And then with a little bit of vision mixed in, you're able to do

37:25.200 --> 37:30.800
fuchsad tasks that you were not even thinking when you were designing the training setting. And like

37:30.800 --> 37:35.200
what you were doing at the time, we were doing at the time with fuchsad learning for image classification,

37:35.200 --> 37:39.600
where we were training the models to do fuchsad learning for image classification. And then they

37:39.600 --> 37:44.400
were good at that. But now it's like, almost you don't train them to do this particular task,

37:44.400 --> 37:50.480
but they actually generalize over different tasks that involve language and vision in different

37:50.480 --> 37:59.120
degrees, of course, of accuracy, not very high. But this is actually a lot of work that I see in

37:59.120 --> 38:03.600
the vision and language intersection is going this direction. And it's quite exciting to see all

38:03.600 --> 38:09.040
the amazing work that appears as well at Neurib's and computer vision conferences like. But it

38:09.040 --> 38:14.880
doesn't have RL, although you could always use RL to fine tune the models further. But in general,

38:14.880 --> 38:20.800
that has not been adopted yet too much, I guess, in these communities. We spoke earlier about

38:22.080 --> 38:31.440
transformers as kind of this, you know, innovation frontier, you know, for lack of a better term,

38:31.440 --> 38:38.880
and kind of where a lot of the activity is in the toolkit. And, you know, this poster is an example

38:38.880 --> 38:47.120
of, you know, transformers, you know, plus, you know, transformers trained with language. And

38:47.760 --> 38:53.040
in parallel, there's this broader conversation about foundation, the foundational models in the

38:53.040 --> 38:59.360
community. You know, what's your take on that? Do you think that, you know, language is going to

38:59.360 --> 39:04.640
provide this, you know, substrate that we'll be able to do a lot of non-language things with. And

39:04.640 --> 39:10.800
that's going to be a, you know, broadly applied tool in the toolbox. Yeah, I mean, I honestly think

39:10.800 --> 39:19.840
looking at what's been going on. And even taking some inspiration from not only like transformers,

39:19.840 --> 39:24.640
but also ideas around unsupervised learning, self-supervised learning, another big area that's

39:24.640 --> 39:31.120
been exploding lately, it does feel that, you know, again, going back to the very beginning of

39:31.120 --> 39:36.320
deep learning, right? You have this principle, right? Like the traditional deep learning, maybe,

39:36.320 --> 39:41.520
let's see what the next generation deep learning could look like. But traditional one is this,

39:41.520 --> 39:46.320
as I said, you take the data set of input outputs and you have these toolbox, you mix and

39:46.320 --> 39:51.120
match components, and then off you go, you train your model and it does reasonably well at many

39:51.120 --> 39:57.200
tasks. But this is quite unsatisfying, right? If you ask many of those who have been in the field

39:57.200 --> 40:02.320
for a long time, and obviously the ones that are not probably also find it annoying, the annoying

40:02.320 --> 40:08.480
bit is the ways are randomly initialized, which is beautiful in a way. But it's also like quite annoying.

40:08.480 --> 40:13.040
I mean, this, this is very wasteful, right? We're, we're starting to train from scratch,

40:13.920 --> 40:19.440
and we're throwing away a lot of energy used to train, you know, fine weights that we had for

40:19.440 --> 40:26.080
a different task that's somewhat related. And so this weight reusability, I think is what these

40:26.080 --> 40:31.280
foundational models and this way of thinking is getting at, which is, look, not only we want to

40:31.280 --> 40:37.680
take tools, we might want to take the tools with the weights associated with them, which poses a very

40:37.680 --> 40:44.720
interesting challenge of, can we initialize parts of the network with maybe a language modeling

40:44.720 --> 40:54.160
component, but practice and sadly this is at the frontier, it's still not very clear that we found

40:54.160 --> 41:01.200
the way to use this idea of pre-trained weights in a successful way, meaning that we start from

41:01.200 --> 41:05.680
the pre-trained weights rather than random weights, and we achieve better performance. Although

41:05.680 --> 41:11.520
more exceptions to start to appear in the field, such as the one from self-supervised learning,

41:11.520 --> 41:18.800
in which you train features that look at image statistics that seem superfluous maybe initially,

41:18.800 --> 41:24.400
but these features happen to be quite good at classifying or doing all sorts of image tasks if

41:24.400 --> 41:31.920
you use them as pre-trained weights. But I do believe that these probably has to be part of an

41:31.920 --> 41:39.600
answer for the next generation deep learning that not only reuses the tools, but reuses the weights,

41:39.600 --> 41:45.760
but it is extremely tricky. Again, the field has been looking at this problem not, you know,

41:45.760 --> 41:51.600
definitely before deep learning was deep learning, but it is very natural to think about that, and

41:51.600 --> 41:58.640
also again, maybe looking a bit at how we learn, we don't always like start from a fresh brain when

41:58.640 --> 42:05.600
we learn something new, right? There's always an accumulation of learning capabilities, and that

42:05.600 --> 42:11.120
feels like a big gap in the way we do deep learning. Despite the fact that you train these networks

42:11.120 --> 42:16.160
from scratch, and they argue that protein folding or translation and the components are the same,

42:16.160 --> 42:24.160
but the weights, no, right? And that, I think I believe this will be as we do research and find

42:24.160 --> 42:29.760
ways to make these weights useful and beating the performance without, you know, training from

42:29.760 --> 42:36.240
scratch, et cetera. I believe this is definitely a way forward, and, you know, it's exciting because,

42:36.240 --> 42:41.600
I mean, we tried, I mean, we, definitely we tried many, many things, and, you know, it's, it's

42:41.600 --> 42:46.800
hard. It's not, it's, you know, it feels intuitive, and it's one of these things that neural nets don't

42:46.800 --> 42:52.080
seem to want to do too well without, like, a lot of effort. So there must be a way, and I mean,

42:52.080 --> 42:56.400
finding it is definitely in the future. I'm sure many people are excited about this.

42:56.400 --> 43:05.600
So another thing I wanted to ask about is a panel that you're going to be on, also at NERPs,

43:05.600 --> 43:10.800
that is talking about a topic that's somewhat related to what we've talked about thus far,

43:10.800 --> 43:20.000
but is specifically focused on, you know, the consequences of the level of scale that we've achieved,

43:20.000 --> 43:24.400
and that we, that, you know, the way we're approaching machine learning with transformers is

43:24.400 --> 43:31.200
requiring. Tell us a little bit about that panel and some of the questions that it is exploring.

43:31.200 --> 43:37.200
Yeah, I mean, I think the panel and the question about scale, there's a more profound question,

43:37.200 --> 43:42.800
which links to actually in the whole field, or what it, what it, what it, what makes research good

43:42.800 --> 43:49.280
or interesting. And I find that that is a fascinating evolving topic, right? Definitely.

43:49.280 --> 43:58.080
I actually kind of recalled one of my, I think my very first NERPs paper has a theorem and a proof,

43:58.080 --> 44:03.280
because at the time, you know, you had to put a theorem and a proof in a NERPs paper for it

44:03.280 --> 44:09.520
to be accepted, right? And it's just that's what you had to do. The paper actually talks a bit about

44:09.520 --> 44:14.640
actually deep learning with SBMs, which was a very, you know, obviously popular model at the time,

44:14.640 --> 44:21.360
super vector machines. But, you know, the question, and I think a challenge is, um, scale is,

44:21.360 --> 44:27.760
is definitely permeating into research. And there is, I think one, one of the main aspects that

44:28.800 --> 44:36.320
will be discussing the panel, or we discussed in the panel is the fact that many research works

44:36.320 --> 44:42.800
or papers might be required or asked naturally by reviewers. Hey, like, can you try this idea at

44:42.800 --> 44:48.560
scale, right? If you look at reviews, there's many reviews that are actually public. I really applaud

44:48.560 --> 44:55.280
the usage of systems like open review that has a more transparent way to show how the review process

44:55.280 --> 45:00.560
work. And it's quite great for people that are new in machine learning, perhaps the most. But

45:00.560 --> 45:06.880
you often see that, well, does this model scale or not, right? And, you know, that is that a fair

45:06.880 --> 45:12.000
question to ask. Should we always aim to scale up our methods? I wish, as we show, like,

45:12.000 --> 45:16.880
results at the conference, like, near ribs. And I think the answer is, I mean, absolutely not.

45:17.600 --> 45:24.800
But the question is, if a paper claims, um, claims to have discovered a new tool or a new

45:24.800 --> 45:30.080
advance on an existing tool, um, the real question, perhaps, and that, that's where it gets maybe

45:30.080 --> 45:36.640
a bit tricky is in which data sets or in which benchmarks are you trying this idea on? And I think what

45:36.640 --> 45:42.560
the community has evolved towards is that, let's say, in computer vision, if you don't show something

45:42.560 --> 45:49.920
works on image net, um, it's probably a bit inconclusive based on, let's say, other very popular

45:49.920 --> 45:57.040
data sets that are smaller and does a bit easier to to run on, like, CIFAR or MNIST, um, right? And

45:57.040 --> 46:02.320
there's still a lot of good work and insights discovered on those data sets. But it is natural

46:02.320 --> 46:08.000
that you scale up, right? These experiments to image net. Otherwise, I mean, the field is full,

46:08.000 --> 46:12.720
like, it's very big right now. There's another big change is that there are lots of good papers

46:12.720 --> 46:19.520
very well written. And there's a bit, you know, challenging signal signal to noise ratio to understand

46:19.520 --> 46:24.800
or slice out what is a meaningful contribution or what needs more work in terms of showing

46:24.800 --> 46:30.240
empirically that some method works. And here, scale plays a big role. I think the role is that

46:30.240 --> 46:38.800
it's fair to ask maybe to run on image net certain models. And then a very interesting follow-up

46:38.800 --> 46:46.320
question is can everyone in the world scale to that scale of data set, right? I mean, image net is

46:46.320 --> 46:52.000
not, um, perhaps the most large one, you know, if you think of it of hardware and we can train

46:52.000 --> 46:56.320
image net. I mean, some there's some results that can train image net in 10 seconds, of course,

46:56.320 --> 47:01.680
using a lot of parallelism. So it feels like in this sense, it's more accessible, but I mean,

47:01.680 --> 47:07.840
the real question is, is it truly accessible? And that creates, I think, a challenge and part of

47:07.840 --> 47:15.760
the panel, right? I'm discussing about accessibility to scale and is research at scale the only or is

47:15.760 --> 47:22.080
is there interesting research at scale? I think the answer is yes. But is the only research we should

47:22.080 --> 47:28.720
look for at scale? And I think absolutely not. And one very beautiful example that I can think of

47:28.720 --> 47:34.800
again related to transformers is that at the time we were working on machine translation,

47:35.520 --> 47:42.000
at Google, we had this sequence, the sequence paper in which we use an 8,000 dimensional long,

47:42.000 --> 47:47.760
like LSTM basically to do machine translation. And we achieved almost a state of the art numbers

47:47.760 --> 47:52.880
with a technique that was completely different than what people had been doing at the time,

47:52.880 --> 47:58.400
which got I guess people's attention. And it was perfected further. And most of, if not all,

47:58.400 --> 48:02.720
the machine translation systems now, they ran with some sort of neural network underneath.

48:03.440 --> 48:11.040
But Montreal had less GPUs. It's not that we use a lot of GPUs. We use a GPUs for like 20 days.

48:11.040 --> 48:15.600
So it was a painful experiment to run, right? You had to wait 20 days and it's a GPUs. They all

48:15.600 --> 48:21.120
fit in one machine. But of course, it's reasonably large scale. But I think Montreal took a different

48:21.120 --> 48:27.920
approach. And instead of scaling up to 8,000 layers, 8,000 units, they only could use 1000 because

48:27.920 --> 48:33.280
maybe they run it on a single GPU. But what they did, which was I think that's maybe the most

48:33.280 --> 48:39.440
transformative thing. Nobody intended is to invent attention. So the attention paper came maybe

48:39.440 --> 48:46.080
perhaps because a scale was not as available to a group in Montreal as it was for us at Google.

48:46.640 --> 48:53.680
And I think this is just a lesson that we need to learn that sometimes by just being resource

48:53.680 --> 48:57.840
constrained, very good research has happened, right? I mean, I'm not saying that then all

48:57.840 --> 49:04.000
please be happy and never try to scale up. It's not like that. But indeed, there are examples and

49:04.000 --> 49:08.800
it should inspire creativity and a different way of thinking, which ultimately might create

49:08.800 --> 49:13.520
a pattern shift. And absolutely as a community, we need to be very careful, of course, at

49:13.520 --> 49:20.400
discarding ideas because they don't scale. And it would obviously had been a mistake to discard

49:20.400 --> 49:25.280
that idea that created attention, which of course, followed up with self-attention transformers

49:25.280 --> 49:32.400
and who knows what comes next. But that attention principle maybe was the key tool that we keep

49:32.400 --> 49:39.520
seeing being quite useful. And it was invented by a lack of like scaling up to achieve better

49:39.520 --> 49:44.800
results. They had to invent these very intuitive attention mechanism, which for translation made

49:44.800 --> 49:48.880
a lot of sense. You look at, you know, you look at the sentence and you attend over the words

49:48.880 --> 49:54.080
that look like, oh, yeah, I'm going to translate this word. And that beautiful principle now is

49:54.080 --> 49:59.840
folding proteins, right? It's like unbelievable. But it is what it is. Yeah. So that's one aspect

49:59.840 --> 50:08.000
in the panel. And I think maybe another big challenging question, I think, is I don't think

50:08.000 --> 50:13.920
we have many answers. Although there were some interesting parallels with different fields,

50:13.920 --> 50:22.320
like, for instance, the large collider that exists at CERN, the question about, I mean,

50:22.320 --> 50:29.200
companies obviously have access to resources like compute. And I think it is our duty to do the

50:29.200 --> 50:36.400
best we can in terms of research, publishing, and scaling up as responsibly as we can. But the

50:36.400 --> 50:42.320
real question is, how can you make that kind of research available, right? To more people. And

50:43.760 --> 50:51.120
you know, a very interesting thought is should, you know, who should kind of invest on the

50:51.120 --> 50:57.760
maybe building a computer for academic research. CERN is a very interesting example

50:57.760 --> 51:02.800
because the physics community kind of came together and decided, well, we need to build this

51:02.800 --> 51:07.200
device. It's very expensive. It actually uses a lot of energy. I mean, it's quite an interesting

51:07.920 --> 51:13.040
place to be actually. I was very lucky to be very recently in the bizarre real world, you know,

51:13.040 --> 51:19.440
experience that I had during the pandemic. But that is an interesting model. And there are

51:19.440 --> 51:23.520
challenges. It's not like, oh, we should just build a super computer like CERN and, you know,

51:23.520 --> 51:28.880
just have people access it in the same way that you do access CERN by writing grants. And

51:28.880 --> 51:34.000
it's a very interesting system, actually. But the problem there is that happened with a lot of

51:34.000 --> 51:39.520
consensus that, you know, physics is a, for many years, like they said, oh, we need to test these

51:39.520 --> 51:45.360
things and to do that, we need this device. So a very, another question we're discussing

51:45.360 --> 51:50.080
the panel is, of course, it might be too early is scaling up is something that, I mean, it has

51:50.080 --> 51:55.120
happened in machine learning, actually, forever. It's not new. You look at the history of data sets.

51:55.120 --> 52:01.280
There's a scaling up trend, of course, of everything, thanks to more laws and so on. But the

52:01.280 --> 52:08.240
question is, do we know, do we feel, you know, assured enough that it's investing, like, let's say,

52:08.240 --> 52:14.320
public money from different governments that maybe could form a coalition similar to CERN,

52:14.320 --> 52:20.000
is that the right moment to do it, right? Do we know that's the way to go? And that's a good

52:20.000 --> 52:26.160
good question. My answer to that is scaling up will be part of the solution, but it's not the

52:26.160 --> 52:31.520
solution, right? In terms of building intelligence, I think it's inconceivable to me that

52:33.520 --> 52:38.960
we need to scale up just because of the amount of the sheer amount of learning at the planetary,

52:38.960 --> 52:44.400
like, species level that happened, if you think of the amount of parallelism, years, etc., that,

52:44.400 --> 52:51.200
you know, it got us to be intelligent beings. So just the existing proof tells me that scaling

52:51.200 --> 52:57.360
probably will be part of the solution, but it's not the only thing that will be, like, required,

52:57.360 --> 53:03.840
right? So, anyways, the panel, I guess, no more spoilers, but it touches all these very interesting

53:03.840 --> 53:08.800
questions and a lot of them, of course, are inspired indeed by what we were discussing before on

53:08.800 --> 53:14.640
scaling up language models and their foundational model capabilities that they seem to exhibit,

53:14.640 --> 53:21.440
and, you know, if that is true, like, when is the time to think very carefully about how to,

53:21.440 --> 53:27.280
you know, get access to the community, similar to the hardware that was being made accessible

53:27.280 --> 53:32.320
with CERN or another example is obviously the Hubble telescope, right? Very, you know,

53:32.320 --> 53:37.600
huge endeavors from to build those things. So, yeah, very interesting questions. I don't know if

53:37.600 --> 53:42.160
you have any thoughts or any solutions to these either, or what do you think? You know, my,

53:44.320 --> 53:50.320
what occurred to me was when you talked about kind of humans as the existence proof of the

53:50.320 --> 53:57.120
requirement for scale, it prompted me to think about, I think we've demonstrated that

53:58.160 --> 54:03.680
scale is required to advance knowledge, but it's not clear that, you know, that that's the same

54:03.680 --> 54:12.480
as advancing intelligence. And, you know, certainly, you know, there's, you know, there's a degree

54:12.480 --> 54:19.200
to knowledge that becomes kind of common knowledge and, you know, we take for granted,

54:21.280 --> 54:28.640
but yeah, it's not obvious the impact of, you know, our scale is a species on our intelligence

54:28.640 --> 54:35.040
per se. I mean, I guess you could argue that our scale as a species has, you know,

54:35.040 --> 54:40.720
facilitated, you know, for example, nutrition, like, you know, we've, we've industrialized farming

54:40.720 --> 54:45.280
and we become stronger and that's, you know, made our brains bigger and, you know, that's,

54:46.560 --> 54:54.320
that has advanced intelligence significantly, but I'm not sure that that's the same kind of scale

54:54.320 --> 55:01.200
argument. Yeah, I really like, by the way, your, your, yeah, your knowledge. I think definitely

55:01.200 --> 55:06.960
from a knowledge standpoint, I mean, we, we store all the knowledge in different formats,

55:06.960 --> 55:13.520
right? Like, over time, without which, you know, things would be much harder. So even in that sense,

55:15.840 --> 55:22.320
accessing all the knowledge and learning how to access that feels like a natural thing we need

55:22.320 --> 55:28.400
to investigate from a machine learning standpoint. And what maybe, you know, these language models

55:28.400 --> 55:35.040
are getting at the very first stages of they know, or they have the knowledge in an imperfect way,

55:35.040 --> 55:41.200
et cetera, but they have the knowledge that exists in a particular corpus. And, but then, yeah,

55:41.200 --> 55:46.560
from there to intelligence, I agree. Like, that's what I'm saying. I think it's part of the puzzle,

55:46.560 --> 55:53.680
but definitely not the whole puzzle indeed. Yeah, awesome, awesome. Well, Ariel, it has been

55:53.680 --> 55:59.280
wonderful catching up, chatting a little bit about all the things you're working on,

55:59.280 --> 56:07.680
particularly with regard to nirips. You are also involved in a new to machine learning panel.

56:07.680 --> 56:12.160
We're not going to have a chance to talk about that, but for those who are new to the field,

56:12.160 --> 56:17.760
I encourage you to seek out that panel. And I'm sure there'll be lots of interesting tidbits to

56:17.760 --> 56:23.520
learn from there. Thanks so much, Ariel, for taking the time to chat. Excellent. Sam, it's been a

56:23.520 --> 56:28.800
pleasure over, long overdue and looking forward to him back in a few years. And we'll see if we change

56:28.800 --> 56:35.200
the discussion topics or it will, it will be like about the same, the same line of thinking about

56:35.200 --> 56:40.080
scaling up and so on. But the field is fascinating. So looking forward to our next chat. Absolutely,

56:40.080 --> 56:42.080
thanks so much. Thank you.

