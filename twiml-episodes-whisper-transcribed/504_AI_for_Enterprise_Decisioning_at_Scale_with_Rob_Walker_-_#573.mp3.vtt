WEBVTT

00:00.000 --> 00:11.200
All right, everyone. Welcome to another episode of the Twinkle AI podcast. I am your host, Sam

00:11.200 --> 00:17.200
Charrington. And today I'm joined by Rob Walker, VP of Decisioning and Analytics and General

00:17.200 --> 00:23.280
Manager of one to one customer engagement at Pegasystems. Before we get going, be sure to take

00:23.280 --> 00:29.600
a moment to hit that subscribe button wherever you're listening to today's show. Rob, it's been

00:29.600 --> 00:34.800
almost three years to the day since the last time we spoke. Welcome back to the podcast.

00:35.520 --> 00:39.520
Yeah, thank you. Yeah, glad to be here. Incredible three years ago, my God.

00:40.560 --> 00:46.960
It has been a crazy three years. Why don't we have you introduce yourself or reintroduce

00:46.960 --> 00:54.720
yourself to our audience? It's been a while. Tell us how you got involved in AI and we'll jump in

00:54.720 --> 01:01.680
from there. Yeah, no, happy to, happy to do that. Yeah, well, AI has been a real passion of me

01:01.680 --> 01:08.240
for a long time. I'm not prepared to admit how long it is. But I did a PhD in AI a long time ago.

01:08.240 --> 01:14.720
Before it was really fashionable, I was just very intrigued by the promise. But obviously,

01:14.720 --> 01:19.360
at that point, it was, well, we have neural networks, not the deep learning quite yet,

01:19.360 --> 01:24.320
but definitely the learning. But it was also expert systems and rules. But that got me really into

01:24.320 --> 01:30.400
that, into that, into that space. So I did my PhD then work for a large consultancy firm

01:31.280 --> 01:35.840
around mostly predictive analytics. What we probably would now call data science and

01:35.840 --> 01:41.600
consultancy around that. And then really, really, really wanted to, you know, set up my own,

01:41.600 --> 01:47.920
my own company in an area called Decisioning, which was not quite a verb, is not quite a

01:47.920 --> 01:54.720
verb I think. So maybe we coined it, but it's pretty popular right now. And that sort of is like

01:54.720 --> 01:59.920
an applied way of, you know, of using data science and predictive models. So I've always been

01:59.920 --> 02:04.400
intrigued by sort of, you know, apart from the science of it, but also, you know, how to make it

02:04.400 --> 02:12.960
like really, really practical. So that company I sold and then through another acquisition,

02:12.960 --> 02:19.920
you know, got, got to be part of, of, of, of Bega, where I'm responsible for, you know,

02:19.920 --> 02:24.880
that part of the, of the business. So basically, long wind away saying, I've never done anything else,

02:24.880 --> 02:32.000
then sort of AI, then applied, applied AI. So, yeah, I'm always excited to, to talk about that.

02:32.960 --> 02:39.360
Awesome. Well, let's, let's talk a little bit more about the use of AI and ML in the context of

02:39.360 --> 02:45.440
customer engagement and customer decisioning. What are the types of problems that you find yourself

02:45.440 --> 02:50.160
helping people solve? Yeah. And, and, and this is actually sort of even, even way back. I've

02:50.160 --> 02:54.240
always tried, maybe that's the consultancy background. I've always been very careful, I think,

02:54.240 --> 02:58.480
to align this with outcomes because AI and machine learning is obviously used and very, very

02:58.480 --> 03:04.480
broad. Even in customer engagement, it's still relatively broad because it can go from creating

03:04.480 --> 03:12.320
auto generating text or images that you may want to use in marketing to stuff that, that we do,

03:12.320 --> 03:18.640
which is really around determining the next best action or the next best experience, next best

03:18.640 --> 03:26.240
conversation. There's a lot of next best now in the market. But have AI in combination with

03:26.240 --> 03:33.840
business rules, determine what you should be talking about with a particular customer to make

03:33.840 --> 03:41.920
that super relevant and, and very, you know, contextual. So the, the business outcomes are usually

03:41.920 --> 03:46.240
around, you know, mutual value creation, right? So it's obviously, you know, it can be about

03:46.240 --> 03:51.440
revenue. But in these times, you know, during the pandemic, actually, especially, but also in,

03:51.440 --> 03:58.080
you know, in other sort of economic downturns, it's also about maybe coaching or counseling,

03:58.080 --> 04:04.000
customers, you know, towards, you know, financial resilience. It's really not just about selling,

04:04.000 --> 04:10.320
next best action is, is really about, like, you know, across the whole customer life cycle,

04:10.320 --> 04:17.280
you know, whether it's nurturing retention, risk management, sales or, or actually determining

04:17.280 --> 04:21.920
in real time that nothing is really relevant enough. So we shouldn't, you know, steal the moment

04:21.920 --> 04:27.280
from the customer to talk about something. But, but it is, it's really about, so where the

04:27.280 --> 04:34.480
machine learning and AI comes in is in part to determine what's best in next best action, right?

04:34.480 --> 04:40.800
So that, that metric, because that sounds very simple, but in practice, it means that,

04:41.360 --> 04:48.800
if you combine that to another big thing that's really important to, to, you know, to our vision,

04:48.800 --> 04:54.560
is the sort of the one-to-one approach, right? So it's not working off averages and segments,

04:54.560 --> 04:59.920
but it is really like, you know, for this particular customer, you know, Sam Charrington is,

05:00.640 --> 05:08.640
you know, calling or browsing or swiping on his mobile app. Now, what do we do based on what

05:08.640 --> 05:13.040
we know about him, based on what he's currently doing, maybe what he's even doing another, you know,

05:13.040 --> 05:18.640
another devices or other systems, what should we be really talking about? And then the moment you

05:18.640 --> 05:25.520
do something and react to it, we need to, you know, recalculate that in the moment. And the

05:25.520 --> 05:30.400
relevance, I think, is a, is a key thing. And that's where we use the AI for. Like, what are you

05:30.400 --> 05:36.320
likely, you know, going to be favorably disposed to, or what is top of mind for you? Yeah. Yeah.

05:36.880 --> 05:44.560
How do you distinguish this idea of next best action and determining it from a recommender system,

05:44.560 --> 05:50.480
you know, which we're all familiar with and interact with, with all the time, both from a kind of

05:50.480 --> 05:55.760
high level use case perspective, as well as, you know, from a technical perspective.

05:55.760 --> 06:01.200
It's interesting, because the, is there some media distinction there?

06:01.760 --> 06:07.120
No, there is a media distinction, because we actually work in a few, you know, in some from

06:07.120 --> 06:15.120
eco, e-customer or e-commerce clients, where both are being used, right? And, and typically,

06:15.120 --> 06:21.200
I mean, in next best action is really at a customer level, right? Whereas the recommender system

06:21.200 --> 06:26.240
is an input to that on, you know, doing a basket analysis and those kind of things. And then saying,

06:26.240 --> 06:32.000
hey, people like you have built, have bought this based on transactions. And that's only part of

06:32.000 --> 06:37.680
it, right? Because for instance, the next best action would, on top of that recommendation, say,

06:37.680 --> 06:42.080
yeah, that's really, you know, what that person would be interested in, what that customer is

06:42.080 --> 06:48.320
interested in, but it's too expensive. Or we believe that if he took out a loan on that,

06:48.320 --> 06:54.800
then he wouldn't repay it. Or he is actually not interested in this at all, because there's a

06:54.800 --> 06:59.440
surface issue that we should really solve. And that's what, you know, is actually the most pressing

06:59.440 --> 07:07.360
thing right now. So it's an input to a next best action. And it's an inside that is useful,

07:07.360 --> 07:12.240
but can easily be overruled or augmented by, you know, other more important insights.

07:12.240 --> 07:17.600
Got it. So what I'm hearing there is that recommender systems tend to be kind of broader,

07:17.600 --> 07:25.760
maybe product and offering based. And this idea of next best action is very personalized

07:25.760 --> 07:31.200
to the, to the individual. Yeah. And very personalized to the individual. And I think that is the,

07:31.200 --> 07:36.800
that is sort of, I think, the important thing. And also looks at a lot of different things.

07:36.800 --> 07:42.000
Well, as an example, right, the recommender system might say, hey, this is like, you know,

07:42.000 --> 07:49.120
your 4k television that you would likely be interested in because people like you would have,

07:49.120 --> 07:55.280
would have bought that, right? But then on top of that, you know, you would, you would really have to look

07:55.280 --> 08:00.400
at like, you know, can you afford it? Did you already maybe, you know, buy that particular

08:00.400 --> 08:04.880
television? You already have it. So you may be interested because you already actually bought it.

08:04.880 --> 08:10.480
And we should do something else, maybe an accessory to that television. There is like all of these

08:10.480 --> 08:17.120
things get into, you know, come into consideration. So to us, it's just one of the many, many

08:17.120 --> 08:24.000
propensities you would get, product propensities. That's the, that's the outcome. And in the case

08:24.000 --> 08:29.600
where we work with like retailers, actually, next best action is often used sort of at the category

08:29.600 --> 08:35.280
level. So it is using, hey, like, hey, we have this, we have this person, we think based on everything

08:35.280 --> 08:39.520
she's been doing, this is the category she's interested in, which may be, you know, high

08:39.520 --> 08:46.800
definition televisions. And then which particular brand and model of television might be the output

08:46.800 --> 08:52.320
of a recommender system. And technically, because she also asked technically, the other thing

08:52.320 --> 08:57.840
is that the recommender system doesn't really have to, you know, refresh all the time, right? So

08:57.840 --> 09:01.920
in these scenarios where, you know, next best action works with the recommender system,

09:01.920 --> 09:08.480
the recommender system, it's fine to, you know, overnight churn and create 10 million different

09:08.480 --> 09:14.000
probabilities at the product level, at the skew level, right? Whereas next best action will just

09:14.000 --> 09:20.080
grab that from the database, but then adds all the contextual insights and behavioral insights

09:20.080 --> 09:24.400
to decide, yeah, this is the thing we need to actually talk about. Sounds a little bit like the

09:24.400 --> 09:31.920
fusing of a CRM and a recommender and, you know, all of the possibilities that are created by

09:31.920 --> 09:37.760
machine learning. It's a great combination. And the, yeah, it's, but it is really at the, because

09:37.760 --> 09:43.040
a recommender system is really at sort of the skew level recommendation, which in a customer

09:43.040 --> 09:48.400
experience, customer engagement context is really only one of the things you should be looking, you

09:48.400 --> 09:53.280
know, you should be looking at if you want to be, you know, customer centric. Got it, got it. And

09:53.280 --> 09:58.000
so if you're building a system like this, what are some of the things that you need to be thinking

09:58.000 --> 10:05.200
about from a machine learning perspective to deliver these types of next best actions?

10:05.200 --> 10:09.760
Yes, next, well, next best action recommendations. Yes, it's like it's fine. The recommender system

10:09.760 --> 10:14.560
is just a particular class, you know, of algorithms that just an input. It's very confusing.

10:14.560 --> 10:18.400
But I think the, the important thing, and this is obviously an open door, but I just want to sort of

10:18.400 --> 10:24.320
say it's still, you know, it's not like, you know, doing AI or machine learning for, you know,

10:24.320 --> 10:29.760
AI shake, right? So we really, really start, and I've always started from the business side,

10:29.760 --> 10:34.240
right? So what are the outcomes in the customer engagement you want to achieve? Is it, is it

10:34.240 --> 10:40.160
revenue growth or is it revenue growth with like constraints on the risk if you're a bank,

10:40.160 --> 10:44.640
you know, you can, you know, you can hand out as we've seen a lot of mortgages, but, you know,

10:44.640 --> 10:51.040
what does your risk profile look like? So, so you look at sort of the outcomes and KPIs,

10:51.760 --> 10:56.880
and once you have established sort of those broad business goals, then you can sort of see the

10:56.880 --> 11:02.000
sort of the scaffolding of, you know, the best metric in next best action, right? And the best,

11:02.000 --> 11:09.040
then say, if this is the outcome, what kind of thing are we, for instance, if this is about

11:09.040 --> 11:14.640
retention initially, right? And we want to retain customers that are, you know, dear to us,

11:14.640 --> 11:21.840
and profitable to us, but, you know, we see sort of, you know, the, the bells of churn,

11:21.840 --> 11:26.800
and we need to, you know, we need to see if we can, you know, save them. From that metric,

11:26.800 --> 11:32.640
you then have to decide, okay, what kind of rules apply? So that's human judgment or, or, or,

11:32.640 --> 11:38.560
or economic factors and profitability factors, right? Like, hey, we want to retain you, Sam,

11:38.560 --> 11:44.400
right? But how much is that worth to us? What can we actually do that would be, you know, that,

11:45.040 --> 11:51.120
that we have in stock in terms of, you know, convince you to stay longer with us, for instance?

11:51.120 --> 11:57.920
So that's, that's past one. And then you're sort of start at the machine learning AI level,

11:57.920 --> 12:03.360
where now we need to figure out, you know, what is on your mind, right? What you would be interested

12:03.360 --> 12:10.640
in, what you would likely say yes to, right? And, and, and, and, and then one level below that is

12:10.640 --> 12:14.880
another level of machine learning and AI. And that is like, okay, once we have decided that we

12:14.880 --> 12:19.840
know what to talk to you about right now, right? And that can change on a dime, but probably talk

12:19.840 --> 12:24.400
about that later when we talk about like real time and those kind of things. But once you have,

12:25.360 --> 12:29.760
once we've decided this is what we need to talk to you about, now the question is how are we going

12:29.760 --> 12:34.880
to do that? Like, are we using a visual? What's the script? How should we say it? All of these kind

12:34.880 --> 12:40.080
of things. And again, that's where AI and machine learning will come up, you know, based on,

12:40.080 --> 12:46.880
you know, collective learnings on what's best. That's sort of the, the approach very much top

12:46.880 --> 12:52.960
down from the outcomes and then all the way down from, you know, outcomes and KPIs to rules,

12:52.960 --> 12:57.920
to predictions to the data that is then required to actually make those predictions.

12:57.920 --> 13:03.600
And is that last part relatively new? You mentioned kind of creating the script. Are you

13:04.400 --> 13:10.960
now at the point where you're looking at generative AI to kind of assemble the script from a set of

13:10.960 --> 13:18.080
data points? Yeah. We, we, we pick actually sort of more, you know, script or content because I

13:18.080 --> 13:22.960
think that is a particular use case of AI, which is very interesting, you know, to generate,

13:22.960 --> 13:27.120
you know, what is the perfect sentence? What's the perfect image? So we do sort of, you know,

13:27.120 --> 13:34.800
some creative optimization, but it is of a stock, right? Because I think, especially generating it,

13:34.800 --> 13:39.280
right, not picking the best one, but generating it, very interesting. It's not what we do,

13:39.280 --> 13:45.520
but we work with systems like that and say, Hey, we give you the context. We know exactly what we

13:45.520 --> 13:50.640
want to talk about. If, you know, if you can pick or construct more interesting, you know, the

13:50.640 --> 13:58.160
right background image or the right, you know, banner text, then that's great. We don't, we don't

13:58.160 --> 14:03.360
specifically do that. We just pick it from a library of, of actions, which can be thousands of

14:03.360 --> 14:08.560
things. A couple of things that you mentioned jumped out of me. One is this idea of starting from

14:08.560 --> 14:15.440
the, the outcome. Another is part of that. You mentioned that use a combination of machine learning

14:15.440 --> 14:21.440
and heuristics. I'd love to get your take on kind of how those are co-existing in your

14:21.440 --> 14:26.640
engagements nowadays. I think we've kind of, you know, there's been this pendulum swing of,

14:26.640 --> 14:31.280
hey, we're going to do everything using statistical machine learning to, hey, you know, we've got all

14:31.280 --> 14:35.920
this domain knowledge and these rule-based systems. Can we find ways to fuse them together?

14:35.920 --> 14:40.080
I'm curious how this plays out and the types of problems that you're solving.

14:40.080 --> 14:46.800
Yes. And it's, I think it's, it's, it's, it's a very interesting, I think topic to, to discuss

14:46.800 --> 14:53.040
a little bit because the, the combination of sort of, yeah, heuristics or, or even human judgment

14:53.040 --> 14:58.880
in most cases, but mostly, you know, heuristics on, on top of machine learning, I think that

14:58.880 --> 15:06.640
combination is important for, for a few reasons. I think one, there are just things that honestly

15:06.640 --> 15:13.680
require, you know, human policies at least, right? For instance, an AI will not tell you that,

15:13.680 --> 15:18.560
unless that's what it's trying to do, but will not tell you that there is a competitive pressure

15:18.560 --> 15:24.880
or there is an announcement of an acquisition in your industry and you need to really, you know,

15:24.880 --> 15:29.040
prepare for that and change tack. You know, so that's the kind of heuristic that you really want

15:29.040 --> 15:35.040
to change quickly, but you don't necessarily want the AI to do those kind of things for you,

15:35.040 --> 15:42.080
indefinitely not unsupervised. And that's the second topic, is that in the industries,

15:42.080 --> 15:48.160
we do most in, which is, you know, retail banking, insurance, communications, healthcare.

15:48.160 --> 15:56.000
It's typically very important that you can also, to some degree, and in many cases,

15:56.000 --> 16:02.640
to a pretty significant degree, explain what is happening, right? So if you had like AI,

16:02.640 --> 16:08.720
especially, you know, the, the fancy algorithms, try to figure out everything that would not be

16:08.720 --> 16:16.160
an acceptable way of, of doing it, even if it worked, but in combination, so the heuristics on

16:16.160 --> 16:23.120
top of thousands of insights that are generated by AI machine learning, we've learned that just from

16:23.120 --> 16:29.600
a business outcome perspective, that's pretty, you know, um, parallel performance.

16:29.600 --> 16:33.440
And can you talk a little bit about how you make that work? Is it kind of

16:34.160 --> 16:41.360
fusing rules engines with algorithms? It is, um, although that makes it sound a little bit like,

16:41.360 --> 16:47.200
you know, it's like a technical integration. Um, and the thing is, um, we've done this for a very

16:47.200 --> 16:51.360
long time. I mean, you know, we spoke last time three years ago, but I've been in this business,

16:51.360 --> 16:57.360
as I said, much longer than I care to admit. Um, and, and, and we've really been very opinionated

16:57.360 --> 17:03.680
now, uh, in a good way, opinionated sounds maybe a little bad, but it's like, um, we know sort of,

17:03.680 --> 17:09.760
you know, what these large brands or, you know, in particular industries, what's best practice,

17:09.760 --> 17:16.080
right? So it's, it's really like more of, um, of, um, of a declarative way of saying, hey,

17:16.080 --> 17:21.920
let's, let's take you through this process of defining the outcomes, then defining rules,

17:22.800 --> 17:28.720
like, for instance, eligibility rules, right? I mean, whatever the AI thinks, you cannot actually

17:28.720 --> 17:33.920
sell a car to a four-year-old, right? Even if the person would be really, really interested to buy

17:33.920 --> 17:39.360
like a, whatever, there's like all sorts of, all sorts of rules that maybe, you know, inventory,

17:39.360 --> 17:43.760
there's risk and margin rules, there's all sorts of these kind of rules that you need. So that's

17:43.760 --> 17:50.320
what you start to define outcomes, then eligibility rules, suitability rules, maybe some competitive,

17:50.320 --> 17:56.320
you know, priorities that you, that you, that you see. And then you start looking at sort of the,

17:56.320 --> 18:03.200
the full action library. So what are all the actions across, you know, nurturing and onboarding

18:03.200 --> 18:07.840
and selling and retention and, and, and, and, and, and, and, and, and, and, and, and risk management

18:07.840 --> 18:14.800
like collections across all of the actions in those categories. Um, how are we going to arbitrate,

18:14.800 --> 18:20.240
given the top of that pyramid, pyramid that I just described, right? And then automatically,

18:20.240 --> 18:24.320
and automatic is here, one of the key words because there are thousands of these actions. And if you

18:24.320 --> 18:30.960
have the treatments on how to, how to present those actions, it gets into even bigger numbers.

18:30.960 --> 18:36.960
For each of those, you need a predictive model to predict this is the interest in the moment,

18:36.960 --> 18:42.400
right? And, and it's not like you have to import or something or find, you know, a thousand

18:42.400 --> 18:46.800
algorithms in the organization, the way that typically because that would obviously be, you know,

18:46.800 --> 18:54.480
fine, but un, unmanageable. Um, so the way that really works is that we look at the data science

18:54.480 --> 19:00.720
output that's already available, right? They may have a risk model or many risk models or attrition

19:00.720 --> 19:06.480
models or propensity models for certain very important products, which we would actually import

19:06.480 --> 19:14.160
into that framework. So we can execute it, but we almost always compliment that, um, with a massive

19:14.160 --> 19:21.040
machine learning capability to compliment data science, right? So data science, typically with the

19:21.040 --> 19:27.520
kind of, um, brands we work with, uh, would still monitor that. So it's like, they shouldn't really

19:27.520 --> 19:33.680
care whether this is a model they created in, you know, um, you know, Python or whatever it is,

19:33.680 --> 19:39.040
or it's a self-learning model that beg I has supplied to them, you know, they, it's, it's their

19:39.040 --> 19:44.880
responsibility, but it just gives them, uh, uh, much, much higher throughput, right? Because these

19:44.880 --> 19:49.680
models learn on the fly, they look about competitive actions that look about seasonal

19:49.680 --> 19:56.960
winfluences, well, all of these kind of things, um, and, and, and, and, and just back to your question,

19:56.960 --> 20:01.920
so they stop down approach, declarative from the outcomes, then the rules that every business

20:01.920 --> 20:05.760
will understand, like eligibility and profitability and all of these kind of things,

20:05.760 --> 20:10.560
then to all the actions and their propensity models that are generated automatically,

20:11.200 --> 20:16.800
that sort of, you know, I think the state of play currently in the, in the industry, it's not a

20:16.800 --> 20:23.680
patchwork of like costly integration to do this. It's just that's the motion for next best action.

20:23.680 --> 20:30.560
And when you go into a large brand of those propensity models already typically exist or

20:31.280 --> 20:37.200
are there, you know, are there holes that need to be filled or new ones that need to be created

20:37.200 --> 20:42.880
in order to, to fulfill the, the framework that you're trying to create? Yes, two, two, two things

20:42.880 --> 20:49.520
about that. So, uh, usually, um, um, so first of all, from, from a, from a decisioning perspective,

20:49.520 --> 20:55.920
that's at Europe again, um, um, we're, we're really trying to be, uh, agnostic about where the

20:55.920 --> 21:00.480
algorithms come from, right? Because there's like, there's great open source tools,

21:00.480 --> 21:05.600
the big cloud platforms will have, you know, offer increasingly more analytics. So,

21:05.600 --> 21:11.440
I don't really care about any of that, where that comes from. I just care about like, can I have

21:11.440 --> 21:19.360
um, an algorithm behind every single action and treatment? And can I execute it in the moment?

21:19.360 --> 21:23.760
That's very important. So, I, I don't want to retrieve a probability from a database, right?

21:23.760 --> 21:30.240
Because if, if a customer says no, right, then I want to re-evaluate a thousand different

21:30.240 --> 21:36.720
predictive models where that no may have some influence and then re-decide what the next best

21:36.720 --> 21:42.880
action, what the next best action is, right? So, so we usually, and, and I'm, when I say usually,

21:42.880 --> 21:49.680
actually, I mean always, complement the, the existing models, um, the data science models with

21:49.680 --> 21:56.080
this machine learning capability. Um, but one thing I would like to, to add is that on top of that,

21:56.080 --> 22:00.400
and this is, I think what a lot of brands are also asking for is around like, you know, this whole

22:00.400 --> 22:06.880
responsible AI, right? We also want to make sure that every algorithm, whether the data science

22:06.880 --> 22:15.040
department build it or our machine learning generated it for them, um, we'll have to adhere to,

22:15.040 --> 22:21.120
you know, sort of the tenets of responsible AI. Like, is it a fair model? Is it a robust model?

22:21.120 --> 22:27.840
Is it transparent? If you need it to be transparent in a particular, um, as said, is it empathetic,

22:27.840 --> 22:33.280
which is sort of, in my mind, really about like, is it relevant? But anyway, those, those tenets

22:33.280 --> 22:40.000
of responsible AI are much easier to enforce if you have like a central AI policy that you can

22:40.000 --> 22:47.440
apply to, you know, your next best action strategy, wherever the algorithm or originated from.

22:48.000 --> 22:53.280
I think that's important. And, and, and lastly, Sam, just before we move on, it's also like,

22:53.280 --> 23:01.040
because it's this combination of heuristics and AI, we can't just look at an AI algorithm

23:01.040 --> 23:05.680
and say, oh, it's biased, you know, it's, well, if it's biased, it's biased and, you know,

23:05.680 --> 23:11.680
we have a problem. If it's not biased, that doesn't mean that your overall outcome is unbiased.

23:11.680 --> 23:16.640
If you apply your rules to it and, and a hundred other models and then arbitrate,

23:16.640 --> 23:24.560
is the final outcome of that, the final recommendation is that fair, right? That's, that's more

23:24.560 --> 23:31.120
of sort of the challenge that we take on. Okay. Uh, I want to circle back to, uh, responsible

23:31.120 --> 23:36.080
AI. One of the things you mentioned in kind of drawing this distinction between

23:37.440 --> 23:42.320
data science and machine learning or these, uh, propensity models and, and kind of the

23:42.320 --> 23:49.600
decisioning on top is that you don't want to have predictions, uh, in some database because

23:49.600 --> 23:57.200
you want to be able to evaluate them real time across these thousands of models. Uh, talk a

23:57.200 --> 24:04.400
little bit about the doing that at scale. Yeah, that's hard. That's actually, it sounds hard.

24:05.040 --> 24:09.120
It is, it is, it is hard. Yes. So I think part of a little part of, of, of,

24:09.120 --> 24:12.800
certainly, of, of my career, but also, you know, the, the teams that have been, that have been

24:12.800 --> 24:18.640
building this, um, it's, it's all about that scale. Can you do that, um, at scale and can you

24:18.640 --> 24:23.520
do it fast enough, right? Because remember, this is customer engagement, right? So you are, it

24:23.520 --> 24:29.280
means that, that if, if, if, if, if, you know, if, if you were, you know, saying no to something,

24:29.280 --> 24:35.600
we need to within fraction of a second, we need to take that no into account, recalculate 1200

24:35.600 --> 24:41.680
propensities, apply all the rules on top of it and then say, oh, but in that case, you know,

24:41.680 --> 24:48.640
and then, you know, have the next best action ready, right? So that's, um, from a technology

24:48.640 --> 24:54.800
perspective, I think is, is, is quite a feat. Um, and I think it's often confused and, and maybe

24:54.800 --> 25:00.240
even downplayed a little bit in, not, it sort of in the market like, well, do we really need real time?

25:00.240 --> 25:06.320
Um, and, and there are really two different things. One is like, do we have all the data real time?

25:06.320 --> 25:10.800
And that can be a challenge. You don't meet all the data, you know, real time. I don't need to know

25:10.800 --> 25:16.320
your birthday in real time, you know, that's fine. Um, but the, the thing that you're doing in a

25:16.320 --> 25:20.720
particular channel, right? What I'm browsing, what I'm clicking, what I'm not clicking, my mood,

25:21.360 --> 25:27.440
all, all of these things that we can now figure out increasingly accurately, um, all change the

25:27.440 --> 25:34.320
context, right? And I want to have the, the most up to date, really up to the second context,

25:34.320 --> 25:40.800
added to sort of a more static profile, and then apply all of these algorithms to it, right?

25:40.800 --> 25:45.840
If I don't, um, then if you said no, then, you know, the results will be pretty much the same.

25:45.840 --> 25:50.640
Well, I would probably suppress it in a rule. I would say, if he said no, let's not offer this

25:50.640 --> 25:57.280
again in three weeks, right? But in fact, you'll know informs a thousand predictions, right? That

25:57.280 --> 26:01.600
again, you know, maybe some of them will not change. A lot of them may change a little bit. So

26:01.600 --> 26:07.280
may change a lot, right? And then the next best action, that kind of relevance is, I think,

26:07.280 --> 26:12.640
that's sort of definitely our claim to fame, but, you know, relevance drives conversions,

26:12.640 --> 26:19.280
drives drives, drives customer satisfaction, drives a conversation like, you know, interaction.

26:19.280 --> 26:24.160
And that's why it's important to not retrieve a score from a database that is, even if it's,

26:24.160 --> 26:30.960
you know, a minute old, that's already not great. Like it would be like, we were, we would be talking

26:30.960 --> 26:52.160
with like a minute delay. It would be hard.

27:00.960 --> 27:13.840
Yes, it's the way it usually works. So first of all, we do support, you know, both on-premise

27:13.840 --> 27:20.800
and cloud model, but pretty much all we do is, you know, it's all cloud, either our cloud or,

27:20.800 --> 27:26.400
you know, the client's cloud, the private one, whatever, all of these options exist. And it's

27:26.400 --> 27:32.560
actually more, more of a, you know, a data and data governance challenge than it is a technical,

27:32.560 --> 27:40.160
than it is a technical, technical challenge. But what we, what we do is like we do pre-aggregate

27:41.040 --> 27:45.600
all the things we can pre-aggregate. And again, that's completely automated. There's no building.

27:45.600 --> 27:50.720
It all comes from declaring what your business challenge is that you're trying to solve, you know,

27:50.720 --> 27:56.800
from that pyramid that I described from the outcome down, but also all the data transformations,

27:56.800 --> 28:02.080
you know, are also happening in real time. But for instance, what we don't need to calculate in

28:02.080 --> 28:10.480
real time is like, for instance, if you are, you know, with like, you know, like a telecom operator,

28:10.480 --> 28:19.360
right, we don't need to re-calculate your, you know, your text messaging or I am volumes

28:19.360 --> 28:23.440
over the month, is that going up? Is that going down? You know, those kind of things you can,

28:23.440 --> 28:29.280
if that's a, if that's a material predictor, right, you can, you can pre-calculate all of that.

28:29.920 --> 28:35.040
And there are many of these kind of trending things that we, that we would sort of pre-aggregate

28:35.040 --> 28:39.440
to make sure that in the moment we don't have to do that. And we can just take your monthly

28:39.440 --> 28:49.600
effort and your weekly effort and your three monthly effort as pre-calculated, assuming that. In the

28:49.600 --> 28:56.400
machine learning and ML ops ecosystem, there's this notion of a feature store which sounds a little

28:56.400 --> 29:04.320
bit like what you're describing. Do you relate this idea to a feature store? Yes, yes, I would,

29:04.320 --> 29:10.560
I would definitely, you know, call it that. We call it, you know, an extended analytics record,

29:10.560 --> 29:16.720
but that's pretty much, pretty much the same thing. So it's the usual profile data then,

29:16.720 --> 29:22.960
then as much of a transactional data that you have or that, you know, the brand cares to share,

29:22.960 --> 29:29.280
you know, could be part of, you know, of a customer data platform or in, you know, big storage,

29:29.280 --> 29:36.160
big data platforms. And, and then, and then some of these features that you can pre-calculate

29:36.160 --> 29:42.400
and are not two-time sensitive, that's, that would be added to that, to that, to that record.

29:42.400 --> 29:47.920
But not sort of, I just want to make clear because it so nicely top down, you don't have to sort

29:47.920 --> 29:52.880
of boil the ocean. I see a lot of, a lot of organizations that think, hey, we're going to build

29:52.880 --> 29:58.560
our CDP or our data, you know, big data platform and we're going to do that first because we need

29:58.560 --> 30:04.720
that to do analytics and we need to do the decisioning on top of it. So that will have to wait.

30:04.720 --> 30:09.920
That's actually, although it sounds sort of common sense, but that's actually the wrong way of

30:09.920 --> 30:14.080
doing it because if you go from, well, it's not the wrong way of doing it. It's a very expensive

30:14.080 --> 30:19.360
way of doing it, right? Because you can use AI and machine learning to sort of figure out, hey,

30:19.360 --> 30:26.240
first of all, given that we're optimizing business outcomes, maybe the data that we have or can get

30:26.240 --> 30:33.600
relatively quickly, we'll get us already 80% there, right? So you have like the funding and,

30:33.600 --> 30:40.240
and, and, and, and, and, and, and the business case to then iterate and, you know, make your data

30:41.040 --> 30:46.240
better or, you know, increase the number of features that you would, in the feature store.

30:46.240 --> 30:52.960
All of these things are much better informed if you actually know what you have rather than

30:52.960 --> 30:58.720
trying to be complete and, and AI sort of, it's almost a side effect, but not really AI and machine

30:58.720 --> 31:05.360
learning can report very accurately on, hey, this data works, this data works with this kind of data,

31:05.360 --> 31:09.840
or there is a correlation between this and this, so you actually don't even need to store this,

31:09.840 --> 31:15.680
or certainly not, you don't have to make it available in real time, you know, so from, from

31:15.680 --> 31:20.800
thousands and thousands and thousands of customer attributes, for instance, the predictive models

31:20.800 --> 31:27.360
may use a couple of hundred, maybe even tens, you know, in, in, in some cases, and it's really good

31:27.360 --> 31:32.560
to know that, otherwise you will invest a lot of time and money in making the data perfect.

31:32.560 --> 31:38.960
Are these next best action types of problems? Typically, at least the machine learning parts

31:38.960 --> 31:45.520
supervise, like is there a requirement to label a lot of data? It's, it's usually really matched

31:45.520 --> 31:51.760
against, you know, it's supervised because it's running against particular outcomes that we already

31:51.760 --> 31:57.440
know, right? So we are trying to predict, are you going to what's right? Are you going to default

31:57.440 --> 32:01.680
on your loan? Are you going to buy this product? Are you going to make us, you know,

32:01.680 --> 32:07.840
is the value of the relationship going to go up and down? And, and, and that's essentially,

32:07.840 --> 32:13.840
you know, what, what you learn about. So it's, it's in that sense, it's supervised learning,

32:13.840 --> 32:18.560
it's supervised, very highly used from historical data, as opposed to, you know,

32:18.560 --> 32:23.040
have a label, a particular transaction. No end, no end labeling, no, no, no, this is all,

32:23.040 --> 32:27.680
yeah, no good question. Yeah, so that's all, this is all at the, at the, at the very large

32:27.680 --> 32:31.600
scale. I think part of this, it's only part, but part of this is trying to sort of, you know,

32:31.600 --> 32:37.760
industrialize this whole process, right? Because otherwise, it would just be, be daunting,

32:37.760 --> 32:42.880
right? So that's sort of the design phase is actually relatively easy, you know, if you know,

32:42.880 --> 32:46.320
what you're doing, you go from the outcomes and the rules and the data, it's, you know,

32:46.320 --> 32:51.120
and then you don't boil the ocean, but you start at maybe 80% of the value, which will be in

32:51.120 --> 32:57.760
a astronomical figure, usually given, you know, the brands we work with, at least. And then,

32:57.760 --> 33:03.040
you know, you can get from 80 to 85 to 90, maybe to 95, and maybe you don't even worry about the

33:03.040 --> 33:13.120
last, the last 5% because, you know, it may not even be, be worth squeezing the data for, for, for, for, for, you

33:13.120 --> 33:18.800
know, that last bit. Yeah, maybe one last question on this. I think you spoke to this already

33:19.520 --> 33:25.440
from an explainability perspective and kind of the opacity of, quote unquote, fancy algorithms,

33:25.440 --> 33:31.760
which I took to mean deep learning. You know, deep learning gets a lot of attention, but a lot

33:31.760 --> 33:36.720
of the bread and butter problems within enterprises are kind of these tabular data problems that,

33:37.760 --> 33:45.200
that is struggles with our, I'm curious the types of models that you, that you see I'm assuming,

33:45.200 --> 33:51.680
you know, kind of a lot of classical tree-based types of things. Are you also seeing places for

33:51.680 --> 33:59.040
deep learning within the, this next best action problem? Yeah, some, and, and, and, but as you say,

33:59.040 --> 34:04.720
because I get the question a lot like, hey, we want to do deep learning, but again, it's often for

34:04.720 --> 34:10.800
deep learning sake, right, because it's, it is, first of all, it is opaque, so you have to be very

34:10.800 --> 34:16.720
careful in practice of what you let it predict, you know, if this is the background color, okay,

34:16.720 --> 34:22.480
or the position of your billboard in the city, maybe, right, but whether you get like a mortgage or

34:22.480 --> 34:27.600
not or be approved for a loan, there's no way any bank would apply a deep learning model to do

34:27.600 --> 34:33.520
that, but from our perspective, as I said before, we are really agnostic and then allow, so we,

34:33.520 --> 34:37.680
so if you, if you, for instance, if we will work with the bank and the bank says we have like,

34:37.680 --> 34:45.520
you know, an incredible model to predict risk, right, then, then you could actually, so we don't

34:45.520 --> 34:52.080
really care, except we allow them to apply sort of an AI policy. So, transparency is a good one,

34:52.080 --> 34:56.880
there's more, like bias and things like that, but that's part of sort of the AI policies,

34:56.880 --> 35:03.920
not our policy, that's not our place, but we allow these organizations to define an AI policy

35:03.920 --> 35:09.840
that we will automatically check against the algorithms, right, which means that a deep learning

35:09.840 --> 35:17.360
model would not be compliant, so we would flag it as incompliant or not compliant if it is about,

35:17.360 --> 35:24.000
like, you know, the probability to take on a loan or recommend a loan, but we would probably

35:24.000 --> 35:30.160
approve it if it's about the position of a particular ad on the web page. Got it. It's hard to

35:30.160 --> 35:39.520
visualize the user interface of a AI policy system, although maybe it's, it's similar to, you know,

35:39.520 --> 35:44.640
it's, it's another set of rules. Yeah, it is like, well, it's more like the way we look at it,

35:44.640 --> 35:48.960
it is more like, like, like, like a filter, so you have like all of these models that you are,

35:48.960 --> 35:53.840
if you have your libraries, so that's part of part of, you know, of model ups, it's like you have

35:53.840 --> 36:00.640
all of all of the models, and they get tacked by like, you know, their explainability or the

36:00.640 --> 36:07.440
type of algorithm, and then, and then we sort of see, well, this is allowed, and this is part of

36:07.440 --> 36:12.640
the policy, you just define and say, hey, for this particular use case or this particular, you know,

36:12.640 --> 36:20.080
decision, we really have to insist on a very transparent models like a decision tree or regression,

36:20.080 --> 36:27.680
or just rules, whereas here, you know, go a lot, right? So even if we don't really understand it,

36:28.400 --> 36:34.480
but we would flag, before taking something into production, we would flag, you know, the

36:34.480 --> 36:42.320
oh for all logic, which is, you know, the heuristics plus the models as being out of compliance. Now,

36:42.320 --> 36:48.080
transparency is just one aspect of responsible AI, what are some of the other ways that you're helping

36:48.080 --> 36:54.720
customers manage those challenges? Yeah, well, so the way we define responsible AI, yeah,

36:54.720 --> 36:59.680
transparency is I think is clear, and we discussed the other one is I think it's also obvious,

36:59.680 --> 37:05.360
you know, fairness, so is there a bias? But again, it's not a bias in the model, well, as well,

37:06.000 --> 37:12.240
but, but no bias or very acceptable bias, maybe if there is anything acceptable about it,

37:12.240 --> 37:18.240
but in the model, doesn't actually mean that your final decision will be biased, right? It's a very

37:18.880 --> 37:24.480
subtle combination of predictive insights with like, for instance, eligibility or suitability

37:24.480 --> 37:30.720
or profitability rules may actually produce results you didn't expect, right? And that's what we

37:30.720 --> 37:36.880
in that AI policy test against. So you can have 100 models, you can have, you know, a thousand

37:36.880 --> 37:42.320
different predictive models underneath all of the rules, and we check if the next best action

37:42.320 --> 37:48.240
distribution is actually fair, right? And I think that's the kind of thing I think that's that's

37:48.240 --> 37:54.240
important. And then we have sort of softer things that we call it empathy in our responsible AI that's

37:54.240 --> 37:59.440
like, it's maybe not even completely correct because it's, you know, you can be responsible

37:59.440 --> 38:04.080
without being empathetic, but we get and I get when I talk to executives a lot of things like,

38:04.080 --> 38:11.200
well, we don't want to be a clinical brand that, you know, has this robot AI deciding what to do

38:11.200 --> 38:19.760
and it's not warm enough or it doesn't really reflect our brand values and that's another

38:19.760 --> 38:26.080
aspect. So we also calculate sort of the softer elements like, for instance, your level of empathy,

38:26.080 --> 38:34.480
your relevance, like are you, for instance, withholding relevant messages or actions to a particular,

38:35.120 --> 38:42.720
you know, subgroup of your customer base. And if so, why is that? Is that like, because it can

38:42.720 --> 38:51.840
be eligibility rule, like back to the car selling the cars or, you know, to teenagers, for instance.

38:51.840 --> 38:56.080
So in that case, that's okay that you're doing that. They may be interested and the models may

38:56.080 --> 39:02.000
indicate they're interested, but you still can't do it. But it can be a lot more subtle, right?

39:02.000 --> 39:07.040
Maybe it's like a policy way that you've set or you put in competitive pressure or you, you said,

39:07.040 --> 39:12.000
well, the profitability is so important. Like, for instance, we need to get this message out

39:12.000 --> 39:18.240
because we need to sell, you know, 10,000 of these and we are completely spamming our customer

39:18.240 --> 39:23.360
base and nobody's interested. We would flag that kind of meta analysis to say, that's not very

39:23.360 --> 39:29.840
empathetic and you need to decide on, you know, what your brand is. And also what the cost is

39:29.840 --> 39:37.040
of doing those kind of things. And the last one, Sam, is around robustness. And this is a question

39:37.040 --> 39:42.000
I also get a lot. And that is like, if you have your predictive models and they're static predictive

39:42.000 --> 39:46.720
models, so, you know, you've been building them or your data science group has been building these

39:46.720 --> 39:51.760
models and you execute them in real time, the model itself obviously doesn't need to change.

39:51.760 --> 39:55.600
And that's fine. So you can test it, you can simulate it. All of these things are fine.

39:56.160 --> 40:01.680
But if it's a self-learning model, especially if you have thousands of them, you want to make

40:01.680 --> 40:07.680
sure that none of them goes rogue, right? Like, hey, it's it's being exposed to real time or real

40:07.680 --> 40:14.800
life, I should say, real life behavior. And suddenly it's doing weird, weird things. And especially

40:14.800 --> 40:19.120
if you combine that with like, you know, maybe a deep learning algorithm, you know, that can go,

40:19.120 --> 40:25.360
hey, why are you really quick? So that's another aspect that we make sure like, okay, we are testing

40:25.360 --> 40:31.760
these self-learning models. Are they are they drifting? Are they drifting a little bit too quick?

40:31.760 --> 40:35.600
Is there a concern? And there's thousands of them. So you can't manually look at that. So you

40:35.600 --> 40:43.200
need to flag that for a data science or a governance body like the model of his in a large organization

40:43.200 --> 40:47.840
and say, hey, there's something weird going on. It still works. You're not you're still fair.

40:47.840 --> 40:53.280
You know, you're still making value, creating value. But this model is doing weird things and you

40:53.280 --> 40:58.720
need to, you know, look at why that is and we then, you know, help this that discovery process.

40:58.720 --> 41:03.760
Awesome. Awesome. Well, I know that in addition to customer engagement, one of the other things that

41:03.760 --> 41:09.120
you're excited about is your upcoming conference, Pega World. And before we wrap up, I wanted to

41:09.120 --> 41:13.760
give you a chance to share a bit about that. Is that a place where folks can come to learn more

41:13.760 --> 41:19.040
about the kinds of things we've been talking about? Absolutely. So, yeah, Pega World, old virtual.

41:19.040 --> 41:24.080
So everybody can sign up. It's it's it's free. Maybe the last time or one of the last time,

41:24.080 --> 41:29.280
we do it virtually, virtually only. But for now, it's virtual. Everybody can sign up.

41:29.280 --> 41:36.240
It is May 24th in the morning in the in the US. So it won't take up all of your all of your day.

41:36.240 --> 41:43.680
And you will hear, you know, companies like, like, for instance, T-Mobile talk about, you know,

41:43.680 --> 41:50.080
much of this and their vision and how they've been, you know, operating and and and and productizing

41:50.080 --> 41:54.240
next best action in that are obviously, you know, breakdowns and deep dives. It's very interesting.

41:54.240 --> 41:58.400
And we'll talk about a lot of these kind of things. But you can also, as I said, hear it from,

41:59.360 --> 42:04.000
you know, the brands themselves. Awesome. Well, Rob, great to see you once again. And thanks so

42:04.000 --> 42:09.760
much for joining the show and sharing a bit about what you've been up to recently and digging into

42:09.760 --> 42:17.360
this more detail. And thanks for having me, Sam, as always. Okay. Yeah. Bye-bye.

