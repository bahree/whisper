1
00:00:00,000 --> 00:00:25,760
Hey everyone, hope you all had a wonderful holiday.

2
00:00:25,760 --> 00:00:30,520
For the next few weeks we'll be running back the clock with our second annual AI Rewind

3
00:00:30,520 --> 00:00:32,120
series.

4
00:00:32,120 --> 00:00:36,960
Join by a few friends of the show, we'll be reviewing the papers, tools, use cases,

5
00:00:36,960 --> 00:00:43,120
and other developments that made us splash in 2019 in key fields like machine learning,

6
00:00:43,120 --> 00:00:49,480
deep learning, NLP, computer vision, reinforcement learning, and ethical AI.

7
00:00:49,480 --> 00:00:55,680
Be sure to follow along with the series at twomolai.com slash rewind 19.

8
00:00:55,680 --> 00:01:00,080
As always, we'd love to hear your thoughts on this series, including anything we might

9
00:01:00,080 --> 00:01:01,160
have missed.

10
00:01:01,160 --> 00:01:06,600
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via

11
00:01:06,600 --> 00:01:11,560
a comment on the show notes page you can find at twomolai.com.

12
00:01:11,560 --> 00:01:13,920
Happy New Year, let's get into the show.

13
00:01:13,920 --> 00:01:22,120
Alright everyone, we are here for AI Rewind, our second annual walk through the top trends

14
00:01:22,120 --> 00:01:28,480
and developments in machine learning and AI, and this time I am with Amir Zameer.

15
00:01:28,480 --> 00:01:33,760
Amir is an assistant professor of computer science at the Swiss Federal Institute of Technology

16
00:01:33,760 --> 00:01:35,760
or EPFL.

17
00:01:35,760 --> 00:01:42,960
In fact, Amir at this current moment for another three days is a postdoc at Affiliated

18
00:01:42,960 --> 00:01:49,800
with Stanford and UC Berkeley, where he was when we first spoke with him back in July

19
00:01:49,800 --> 00:01:55,000
of 2018, Amir, welcome back to the twomolai podcast.

20
00:01:55,000 --> 00:01:56,000
Thanks.

21
00:01:56,000 --> 00:01:57,000
Great to be here.

22
00:01:57,000 --> 00:02:02,680
Yeah, I'm really excited to dig into this conversation about what's new, your take

23
00:02:02,680 --> 00:02:07,040
on 2019 from a computer vision perspective.

24
00:02:07,040 --> 00:02:13,360
And so to kind of get us started with that, why don't we just take broad brushstrokes?

25
00:02:13,360 --> 00:02:15,080
What's your take on 2019?

26
00:02:15,080 --> 00:02:17,000
Thanks for having me.

27
00:02:17,000 --> 00:02:23,240
It was another exciting year, in my opinion, for fields with an AI and including computer

28
00:02:23,240 --> 00:02:26,680
vision, you know, complex conferences are expanding.

29
00:02:26,680 --> 00:02:28,920
So there's more talent coming in, more energy.

30
00:02:28,920 --> 00:02:38,120
I think CPR 2018 was 6,000 roughly, the number of attendees in 2019 was about 10,000.

31
00:02:38,120 --> 00:02:40,400
So we will see, and that was six months ago roughly.

32
00:02:40,400 --> 00:02:44,760
So we will see how it's going to be in 2020, but that expansion in size.

33
00:02:44,760 --> 00:02:49,960
How does that growth rate compare to NERPs, which is the one that gets a lot of headlines?

34
00:02:49,960 --> 00:02:50,960
Right.

35
00:02:50,960 --> 00:02:55,400
NERPs was, I came back from NERPs like roughly two weeks ago.

36
00:02:55,400 --> 00:03:00,120
I believe it was 13,000, but again, they are like six months apart.

37
00:03:00,120 --> 00:03:04,240
So, you know, we will see how CPR will be.

38
00:03:04,240 --> 00:03:10,640
But I think roughly the same size, maybe NERPs, somewhat bigger because it generally includes

39
00:03:10,640 --> 00:03:15,360
many different areas, not just vision, not just NLPMs, so on.

40
00:03:15,360 --> 00:03:19,920
Many vision people are there, myself included, but yeah, but they are a huge conference.

41
00:03:19,920 --> 00:03:22,840
It's big enough that you won't see your friends anymore.

42
00:03:22,840 --> 00:03:28,080
And are the paper submissions as CVPR growing as quickly as NERPs?

43
00:03:28,080 --> 00:03:29,080
Yes.

44
00:03:29,080 --> 00:03:30,080
Yes.

45
00:03:30,080 --> 00:03:31,080
Yeah.

46
00:03:31,080 --> 00:03:32,080
I don't know the exact same year.

47
00:03:32,080 --> 00:03:38,720
I'm an area chair, so I should know this number, but all in, all in what I'm sure about

48
00:03:38,720 --> 00:03:43,200
and is that there is a really sharp expansion.

49
00:03:43,200 --> 00:03:48,280
And, you know, to some extent that presents a problem for us academics because we have

50
00:03:48,280 --> 00:03:54,080
to find reviewers for this load of like papers and so on.

51
00:03:54,080 --> 00:03:55,080
But that's fine.

52
00:03:55,080 --> 00:03:57,480
That's a good problem to have.

53
00:03:57,480 --> 00:03:59,080
The overall outcome is positive.

54
00:03:59,080 --> 00:04:02,760
I'll be at some variances.

55
00:04:02,760 --> 00:04:08,520
And, you know, the growth was in a way, this was proportional because now, for the past

56
00:04:08,520 --> 00:04:12,160
like four or five years, there was a huge interest.

57
00:04:12,160 --> 00:04:15,240
And so we have a lot more like young talent in the field.

58
00:04:15,240 --> 00:04:20,600
For the same reason, they're young and new, so we don't have as many seasoned reviewers

59
00:04:20,600 --> 00:04:22,480
to value the papers for us.

60
00:04:22,480 --> 00:04:27,200
So to some extent, the review quality has a little bit of variance in it compared to

61
00:04:27,200 --> 00:04:28,360
few years ago.

62
00:04:28,360 --> 00:04:32,640
But again, like I said, that's a good problem to have because the field is just generating

63
00:04:32,640 --> 00:04:37,400
more results, so a little bit of variances, in my opinion, acceptable.

64
00:04:37,400 --> 00:04:42,920
So the field is growing dramatically as our other areas in ML and AI.

65
00:04:42,920 --> 00:04:44,520
What else is happening in vision?

66
00:04:44,520 --> 00:04:45,520
Right.

67
00:04:45,520 --> 00:04:51,800
In terms, more technically, I think, you know, we see a few trends that they are not too

68
00:04:51,800 --> 00:04:56,920
exclusive to 2019, but I think some of them are like maturing up in 2019.

69
00:04:56,920 --> 00:05:02,720
One metal trend that I see for sure is that we see a lot of mixes of areas like vision

70
00:05:02,720 --> 00:05:06,320
plus something else, like vision pros, graphics.

71
00:05:06,320 --> 00:05:11,520
I think a nominal example of that are all these like image synthesis pipelines, either

72
00:05:11,520 --> 00:05:14,560
GAN-based and whatnot.

73
00:05:14,560 --> 00:05:19,800
But the pipelines that essentially generate an image, that's, in a way, it's a graphics

74
00:05:19,800 --> 00:05:24,440
problem because graphic is about like generating something, good looking, that you put under

75
00:05:24,440 --> 00:05:26,480
your screen rendering things.

76
00:05:26,480 --> 00:05:28,400
And vision was the inverse of the problem.

77
00:05:28,400 --> 00:05:31,440
You already have an image and you want to understand it.

78
00:05:31,440 --> 00:05:38,760
But these two areas got blended together, and I think the first time in vision that I

79
00:05:38,760 --> 00:05:44,560
saw a reasonably working example was the paper picks the picks a few years ago.

80
00:05:44,560 --> 00:05:49,280
And after that, it became really popular and cycle GAN and so on.

81
00:05:49,280 --> 00:05:53,440
Those other papers actually came primarily from the vision community and of course the graphics

82
00:05:53,440 --> 00:05:55,720
community worked a lot on it too.

83
00:05:55,720 --> 00:06:01,400
So mixes of areas, vision pros, graphics, I guess we'll discuss it partially when we

84
00:06:01,400 --> 00:06:03,360
get into more details.

85
00:06:03,360 --> 00:06:05,440
Vision plus robotics is expanding.

86
00:06:05,440 --> 00:06:10,320
I think it's one of the areas to watch for sure, vision plus like adversarial robustness

87
00:06:10,320 --> 00:06:11,320
literature.

88
00:06:11,320 --> 00:06:17,760
I think that's something that we, not many of us, saw it coming, but in a way it actually

89
00:06:17,760 --> 00:06:24,960
makes sense that how the algorithms, basically there was a line of research going forward

90
00:06:24,960 --> 00:06:28,840
on making machine learning systems more robust and that there are examples where like

91
00:06:28,840 --> 00:06:37,480
poking concerns to people and turns out that if you have more robust algorithms for processing

92
00:06:37,480 --> 00:06:44,800
visual data, they are more useful sort, just processing non adversarial literature and

93
00:06:44,800 --> 00:06:47,240
non adversarial content as well.

94
00:06:47,240 --> 00:06:52,640
Like if you have an image synthesis pipeline, it works better if it was robustified even

95
00:06:52,640 --> 00:06:56,000
though they, even if you don't mess with the input anymore.

96
00:06:56,000 --> 00:06:57,760
We can discuss it in more details and go forward.

97
00:06:57,760 --> 00:07:04,320
But I think generally the trend of mixing different areas with vision is increasingly popular

98
00:07:04,320 --> 00:07:08,000
and I think there's actually a healthy reason to this.

99
00:07:08,000 --> 00:07:16,320
I think it's a realization of the fact that vision is a service to some downstream goal.

100
00:07:16,320 --> 00:07:21,520
It's a very powerful skill, but we don't usually observe the world for the purpose of just

101
00:07:21,520 --> 00:07:24,800
absorbing, like just understanding what's going on.

102
00:07:24,800 --> 00:07:29,440
We usually have an intent in mind, like we understand the world with, you know, when I,

103
00:07:29,440 --> 00:07:35,320
when I get up in the morning, when I open my eyes, I intend to get out of bed safely

104
00:07:35,320 --> 00:07:38,320
and navigate myself out of the bedroom.

105
00:07:38,320 --> 00:07:44,920
So the vision is a very practical skill and so we cannot really make that independent,

106
00:07:44,920 --> 00:07:48,280
the research that we do on vision of these downstream skills.

107
00:07:48,280 --> 00:07:54,760
So vision plus X is, to me, I see that as realization of that fact, especially in

108
00:07:54,760 --> 00:07:56,680
the context of robotics.

109
00:07:56,680 --> 00:08:01,840
The reason we mix vision and robotics together is that our robots need to have a complex

110
00:08:01,840 --> 00:08:05,640
understanding of the world acquired through the cameras.

111
00:08:05,640 --> 00:08:13,080
So whatever the vision pipeline outputs, it should be in a way curated to best support

112
00:08:13,080 --> 00:08:15,240
the downstream goal of a robot.

113
00:08:15,240 --> 00:08:19,360
I particularly don't care, for instance, if I have a robot in my home and it can detect

114
00:08:19,360 --> 00:08:23,480
all the objects and do all sort of like complex things.

115
00:08:23,480 --> 00:08:28,400
I don't really care how the vision pipeline works if the downstream goal of a robot's

116
00:08:28,400 --> 00:08:32,760
whatever it is, make the bed or do laundry or whatever it that is.

117
00:08:32,760 --> 00:08:37,760
If that works just fine, the vision can be as simple as it wants to be.

118
00:08:37,760 --> 00:08:44,840
And so it is really an end-to-end intertwined pipeline and I'm actually happy to see that

119
00:08:44,840 --> 00:08:54,000
these areas are mixing together because we can now do a more meditated design in our

120
00:08:54,000 --> 00:08:58,760
research and do vision in a way that it's more useful to our downstream goals.

121
00:08:58,760 --> 00:09:01,440
There are some caveats in this story, for instance, art.

122
00:09:01,440 --> 00:09:06,040
When I observe a painting, I'm watching it, I'm looking at it, but I'm appreciating it.

123
00:09:06,040 --> 00:09:12,160
I don't necessarily intend to do something with it, but generally speaking vision is a

124
00:09:12,160 --> 00:09:19,760
very practical skill and makes of areas in my opinion is a realization of that.

125
00:09:19,760 --> 00:09:26,040
Is there also an implication that vision has reached a level of maturity or meaning

126
00:09:26,040 --> 00:09:32,600
the core vision tasks have reached a level of maturity or performance that we can now

127
00:09:32,600 --> 00:09:38,040
even consider moving onto a real world, types of things and incorporating in these other

128
00:09:38,040 --> 00:09:45,360
areas, like we've solved enough of core vision to then mix it with these other fields?

129
00:09:45,360 --> 00:09:53,000
Yes and no, I would be really hesitant to say that we have solved enough core vision

130
00:09:53,000 --> 00:09:56,160
problems or we have solved them like fine.

131
00:09:56,160 --> 00:10:02,680
I actually think, let's say, the simplest example, probably the longest running problem

132
00:10:02,680 --> 00:10:08,400
in vision is, let's say, object detection. We are not a point that we can say that we

133
00:10:08,400 --> 00:10:13,960
can, with a high confidence, we can detect an object under varying lighting conditions

134
00:10:13,960 --> 00:10:17,880
and different contexts and so on and so forth.

135
00:10:17,880 --> 00:10:22,800
But at the same time, a huge amount of progress has been made, whether there's a way to make

136
00:10:22,800 --> 00:10:28,200
them useful, I think the answer is yes, and that's why many people that are not vision

137
00:10:28,200 --> 00:10:36,800
experts are actually using vision pipelines and we see APIs as well, and Microsoft or Google

138
00:10:36,800 --> 00:10:44,920
their APIs where you can do somewhat niche problems, but they're sufficiently reliable, something

139
00:10:44,920 --> 00:10:47,560
like face detection.

140
00:10:47,560 --> 00:10:53,520
So the fact that we have APIs that non-experts can use at the command line level, that basically

141
00:10:53,520 --> 00:11:01,440
means, yes, a certain level of maturity has been reached, but that does not mean that

142
00:11:01,440 --> 00:11:08,520
we can solve the problems now that seemingly are simple, because we have huge problems

143
00:11:08,520 --> 00:11:14,120
on that, like specifically 3D, like 3D perception, we have a lot of sensors for it and these

144
00:11:14,120 --> 00:11:18,200
sensors are expensive, such as lighter and so on and so forth.

145
00:11:18,200 --> 00:11:24,640
But understanding the content of an image in terms of 3D is far from like perfection,

146
00:11:24,640 --> 00:11:26,720
so there's a lot more work to do.

147
00:11:26,720 --> 00:11:34,360
We've got the field growing in size, we've got this mix of areas being explored, any other

148
00:11:34,360 --> 00:11:37,480
general trends that you're seeing in vision?

149
00:11:37,480 --> 00:11:42,400
I think in general, we see less and less fixed pattern recognition problems, like I said,

150
00:11:42,400 --> 00:11:46,160
object detection or segmentation and so on, that's going forward too and that's going

151
00:11:46,160 --> 00:11:47,560
forward to strong.

152
00:11:47,560 --> 00:11:53,040
We see progress, but the attention and energy from what I see is shifting towards these

153
00:11:53,040 --> 00:11:56,480
new horizons that are opening up right now.

154
00:11:56,480 --> 00:12:03,720
And I think there's generally also interest in unsupervised or self-supervised learning

155
00:12:03,720 --> 00:12:09,840
has been growing and it continued to grow and I think there were some good progress in

156
00:12:09,840 --> 00:12:16,440
the past year, but that's not something that I anticipate being solved anywhere in the

157
00:12:16,440 --> 00:12:17,440
near future.

158
00:12:17,440 --> 00:12:22,520
And we'll continue to see that as a significant area of research.

159
00:12:22,520 --> 00:12:29,680
Let's transition to some of the specific areas that you've identified to dig in deeper.

160
00:12:29,680 --> 00:12:37,760
We asked you to identify just a few papers that you thought represented the kind of progress

161
00:12:37,760 --> 00:12:42,480
that we've made in vision as a broad field in 2019.

162
00:12:42,480 --> 00:12:44,240
You found that particularly difficult to do.

163
00:12:44,240 --> 00:12:50,800
Can you talk a little bit about why and the areas that you've identified to discuss with

164
00:12:50,800 --> 00:12:51,800
us?

165
00:12:51,800 --> 00:12:56,640
There are a number of areas that I think it's worth discussion.

166
00:12:56,640 --> 00:13:00,680
Like I said, when you have a conference with 10,000 people and that's just CVPR.

167
00:13:00,680 --> 00:13:05,280
We have ICCV and ECCV as well, at least as top-tier vision conferences.

168
00:13:05,280 --> 00:13:10,520
I mean, you can imagine that there's a lot of research going on, thousands of papers

169
00:13:10,520 --> 00:13:11,520
coming out.

170
00:13:11,520 --> 00:13:20,160
It's hard to just pick out one or two or five papers because there's just more good work

171
00:13:20,160 --> 00:13:22,480
than those numbers and a handful of papers.

172
00:13:22,480 --> 00:13:26,480
But I think in terms of we can summarize the trends.

173
00:13:26,480 --> 00:13:34,840
One trend that I specifically see and I expect to grow is vision for robotics.

174
00:13:34,840 --> 00:13:40,760
So generally interesting robotics is growing, especially in what usually to go refer to

175
00:13:40,760 --> 00:13:46,880
as like robot learning these days, like reinforcement learning platforms or generally mix of learning

176
00:13:46,880 --> 00:13:50,480
based robotics with the classic robotics.

177
00:13:50,480 --> 00:13:53,040
And vision is part of this story.

178
00:13:53,040 --> 00:13:59,240
Like robot is a large intertwined framework and it involves multiple aspects like sensory

179
00:13:59,240 --> 00:14:05,120
hardware, perception, planning, control, and so on.

180
00:14:05,120 --> 00:14:10,720
And one common question is like what makes vision vision like robotics robotics then?

181
00:14:10,720 --> 00:14:14,560
These days, especially with the rise of robot learning where like one of the frequent claims

182
00:14:14,560 --> 00:14:17,480
is learning directly from raw pixels.

183
00:14:17,480 --> 00:14:21,760
So one question is like why we need like a specific vision algorithm at all?

184
00:14:21,760 --> 00:14:25,560
All we can do is like learning directly from pixels.

185
00:14:25,560 --> 00:14:34,600
So first like I draw the line between vision and let's say control, acknowledging that

186
00:14:34,600 --> 00:14:42,040
this sort of intertwined pipeline and clearly entwent, sensory observation comes in, goes through

187
00:14:42,040 --> 00:14:45,600
vision, goes through planning, goes through control, goes through hardware, and the robot

188
00:14:45,600 --> 00:14:47,360
in the end does something.

189
00:14:47,360 --> 00:14:49,800
So it's an end to end pipeline clearly.

190
00:14:49,800 --> 00:14:58,600
But to me as a vision researcher, what makes vision research is a robot has some sensory

191
00:14:58,600 --> 00:15:04,000
hardware like a camera and these sensors observe something and have some output.

192
00:15:04,000 --> 00:15:09,800
And the algorithm that just processes this output and extract some useful abstractions

193
00:15:09,800 --> 00:15:15,440
out of those high dimensional sensory data, that's the perception problem.

194
00:15:15,440 --> 00:15:21,680
So if the processing pipeline that black box sits closer to the sensors, it's more about

195
00:15:21,680 --> 00:15:25,480
understanding the world because those sensors are sensing the world.

196
00:15:25,480 --> 00:15:30,360
So the prior, the type of prior is that those black boxes user about like let's say the

197
00:15:30,360 --> 00:15:37,200
world is 3D or there is some dynamics in the world, there is like motion smoothness

198
00:15:37,200 --> 00:15:38,200
and so on.

199
00:15:38,200 --> 00:15:45,320
So perception uses these priors to to tame these high dimensional signal and then hands

200
00:15:45,320 --> 00:15:49,520
that over to the rest of the pipeline, let's say planning and so on.

201
00:15:49,520 --> 00:15:56,760
So the closer to the end we get, then comes the problem of control, like having these abstractions

202
00:15:56,760 --> 00:16:01,720
and having planned something, you want to issue an action that the robotic hardware executes

203
00:16:01,720 --> 00:16:03,120
for you.

204
00:16:03,120 --> 00:16:08,400
So to me, vision researchers are in the beginning of this end to end pipeline.

205
00:16:08,400 --> 00:16:12,800
They are more concerned about the world, they use the priors about the world and turn

206
00:16:12,800 --> 00:16:15,600
that into processing pipelines.

207
00:16:15,600 --> 00:16:22,720
And robotic researchers get these more abstractions and turn are closer to the agent, they understand

208
00:16:22,720 --> 00:16:27,080
the hardware, the robot better, so they control the robot so that they execute something

209
00:16:27,080 --> 00:16:32,360
that in a way that after doing multiple of these iterations, the outcome is achieved.

210
00:16:32,360 --> 00:16:39,080
So like I said, this is immediately brings these multiple fields together, vision plus

211
00:16:39,080 --> 00:16:46,600
robotics and it calls for us vision researchers to develop this vision box in a way that it

212
00:16:46,600 --> 00:16:49,800
can support the rest of the pipeline the best.

213
00:16:49,800 --> 00:16:56,040
It's not an open loop system, it's not like we receive a sensory observation like images

214
00:16:56,040 --> 00:17:01,240
and produce something, an object, frames and object detection and then we just say we don't

215
00:17:01,240 --> 00:17:05,560
care about what's going on after that, it is really important what comes after that.

216
00:17:05,560 --> 00:17:11,560
For instance, uncertainty estimation, it's really important for control theory people.

217
00:17:11,560 --> 00:17:16,520
So it makes sense that as vision researchers, we become aware of that and provide some form

218
00:17:16,520 --> 00:17:22,720
of uncertainty estimation that is tied with our detection and so on and so forth.

219
00:17:22,720 --> 00:17:30,760
So I think there are a number of clear developments in the field acknowledging this vision for

220
00:17:30,760 --> 00:17:33,040
robotics type of research.

221
00:17:33,040 --> 00:17:39,640
You drew a distinction early on in this between, and I want to get this distinction, I don't

222
00:17:39,640 --> 00:17:43,600
think I'm going to get this distinction correct, so please correct me, but it was something

223
00:17:43,600 --> 00:17:47,920
along the lines of vision versus learning from pixels.

224
00:17:47,920 --> 00:17:57,160
Right, well vision is learning from pixels, I see, so I understand, so yeah, so there's

225
00:17:57,160 --> 00:18:02,160
actually tabular also learning, it's a common word, it's not actually new, it's called

226
00:18:02,160 --> 00:18:07,400
like, I think it's Latin, it means like clean slate, so it's a system that let's say a robot

227
00:18:07,400 --> 00:18:13,680
that does tabular also learning in terms of vision, what it does is that, okay, I have the

228
00:18:13,680 --> 00:18:18,960
sensory output just raw pixels, and I'm going to learn directly from raw pixels, let's

229
00:18:18,960 --> 00:18:27,480
say, for instance, using a model free enforcement learning policy, and then you define your goal

230
00:18:27,480 --> 00:18:32,600
in terms of some reward function that rewards you when you do something right and penalizes

231
00:18:32,600 --> 00:18:38,120
you when you don't do that right, and then you hope that by direct interaction with the

232
00:18:38,120 --> 00:18:46,680
world, many, many, many data points, the system will learn how to do the job right, and that's

233
00:18:46,680 --> 00:18:53,280
generally what we see in the enforcement learning literature, especially like in model free

234
00:18:53,280 --> 00:18:59,200
enforcement learning, but there, the distinction is that learning directly from raw pixels,

235
00:18:59,200 --> 00:19:06,360
basically means the state of the world are these raw pixels, and by the way, I'm generalizing

236
00:19:06,360 --> 00:19:15,480
a lot here to get the general concept right, what vision is about when we are processing

237
00:19:15,480 --> 00:19:19,920
these raw pixels, instead of just using them raw, viewing them as like a 2D matrix that

238
00:19:19,920 --> 00:19:23,880
is coming from the camera, so that's what we know, vision is about like having some

239
00:19:23,880 --> 00:19:31,000
priors about the world, and instead of using this signal raw, we use those priors to extract

240
00:19:31,000 --> 00:19:35,440
some abstractions that are easier to understand, they're more interpretable, they're more efficient

241
00:19:35,440 --> 00:19:40,200
and so on, so an example, like I said, is the fact that the world is 3D, so when you look

242
00:19:40,200 --> 00:19:47,960
at an image, it's a projection 2D projection of a 3D world onto 2D plane, so it basically

243
00:19:47,960 --> 00:19:53,360
loses the 3D information, so if a robot is trying to make use of this 2D projection,

244
00:19:53,360 --> 00:19:59,600
it has to somehow recover that information, so either we hope that by these like millions

245
00:19:59,600 --> 00:20:05,720
and millions and millions of interaction, this robot actually understands 3D reconstruction

246
00:20:05,720 --> 00:20:09,320
to some extent at least to be able to solve the problem, because we know that some of

247
00:20:09,320 --> 00:20:13,360
these problems, especially in navigation, they need 3D perception, like you need to know

248
00:20:13,360 --> 00:20:18,680
how far the obstacle is to be able to avoid it, so there's no escape from that problem,

249
00:20:18,680 --> 00:20:23,760
at least at the course level, so either the hope is that by not providing these priors

250
00:20:23,760 --> 00:20:30,960
directly, and learning directly from pixels, the system by many millions of interactions

251
00:20:30,960 --> 00:20:37,040
it learns that, or some people like vision researchers, they use these priors about the

252
00:20:37,040 --> 00:20:43,000
world, they extract these abstractions out of their off pixels, and that's what the

253
00:20:43,000 --> 00:20:48,600
rest of the system brings them to their enforcement and learn policy uses, so you can learn

254
00:20:48,600 --> 00:20:54,360
them, you can view basic vision as identifying these like facts of the world, the fact that

255
00:20:54,360 --> 00:20:58,840
the world is 3D, the fact that there's like motion and smoothness, turn them into processing

256
00:20:58,840 --> 00:21:04,680
pipelines, so when the raw image comes in, it first goes through these abstractions, and

257
00:21:04,680 --> 00:21:09,320
then those abstractions are what the rest of the system, like the robotic system sees,

258
00:21:09,320 --> 00:21:13,720
so they're more interpretable, they're easier to tame, and you don't have to like redo

259
00:21:13,720 --> 00:21:22,880
this process every time that you turn a robot on during the learning phase, because these

260
00:21:22,880 --> 00:21:26,560
are just facts about the world that are generally true, it doesn't matter whether you're navigating

261
00:21:26,560 --> 00:21:31,520
or you're manipulating or you're finding an object or you're just going to GPS coordinate,

262
00:21:31,520 --> 00:21:38,520
you need some understanding of about 3D from the world, so it makes sense to turn that

263
00:21:38,520 --> 00:21:44,040
into more like the standalone problem where vision people solve and then plug that into

264
00:21:44,040 --> 00:21:45,040
the bigger pipeline.

265
00:21:45,040 --> 00:21:52,120
Now, when I talk to folks that are coming from the opposite direction or the other direction,

266
00:21:52,120 --> 00:21:59,280
the pixel-based learning and reinforcement learning, one of the things that they say

267
00:21:59,280 --> 00:22:05,520
is hot this year is model-based reinforcement learning, for example, where they're trying

268
00:22:05,520 --> 00:22:11,680
to learn a higher level model in the process of learning from the pixels to do some of

269
00:22:11,680 --> 00:22:16,880
the things that you're describing, and I guess the question that I'm asking is, does this

270
00:22:16,880 --> 00:22:24,960
mean that vision and the pixel-based approaches are converging or is it saying something else?

271
00:22:24,960 --> 00:22:30,280
So those models in the model-based RL, are they the same kind of abstractions that you're

272
00:22:30,280 --> 00:22:33,680
describing that are core to vision?

273
00:22:33,680 --> 00:22:40,560
Right, I mean, model-based RL or generally model-based everything, it's too general to the

274
00:22:40,560 --> 00:22:45,360
point that it can include anything in everything, so for instance, model-based, yeah, you can't

275
00:22:45,360 --> 00:22:49,920
have a model-based on the 3D of the world and that would become model-based something,

276
00:22:49,920 --> 00:22:57,520
so this doesn't necessarily mean there is a completely orthogonal approach to solving

277
00:22:57,520 --> 00:23:02,440
this problem that is going to replace vision or anything, generally when you have a model

278
00:23:02,440 --> 00:23:08,920
you're encoding some priors, either you're learning these priors or you're encoding them

279
00:23:08,920 --> 00:23:13,400
into the system, and like I said vision is more or less about that too, we define some abstractions

280
00:23:13,400 --> 00:23:18,200
that we believe are generally true about the world and we focus on solving those, up until

281
00:23:18,200 --> 00:23:23,560
a few years ago we were doing it completely independently of who uses the output like a robot,

282
00:23:23,560 --> 00:23:30,200
but now, like I said, with these areas being mixed, we are developing them in a more end-to-end

283
00:23:30,200 --> 00:23:36,040
manner, with the awareness that we need to develop them in a way that they're best useful to

284
00:23:36,040 --> 00:23:41,800
the rest of the pipeline, but yeah, in the end it's all the model, and there are people

285
00:23:41,800 --> 00:23:46,760
call different things in the model, there's like dynamics model, which is about prediction

286
00:23:47,560 --> 00:23:55,960
of the outcome of a certain action, so that helps with efficiency and so on, but for generally

287
00:23:55,960 --> 00:24:00,520
being able to solve this problem, you need to have a model and we are all in the business

288
00:24:00,520 --> 00:24:09,960
of developing that. So what are some of the specific papers that kind of exemplify this trend for

289
00:24:09,960 --> 00:24:17,400
you? Yeah, so for vision for robotics, like I said, the vision community primarily focused on

290
00:24:17,400 --> 00:24:24,600
the navigation, there are good reasons for that again, I think it's one of the most important

291
00:24:24,600 --> 00:24:28,680
things that are probably enlightening here is to look at the data, like where we get the data

292
00:24:28,680 --> 00:24:34,920
for for being able to solve, develop a vision model for an agent that is active in the world,

293
00:24:34,920 --> 00:24:40,120
like a robot that does navigation. So it's a big question, it's a very different from say

294
00:24:40,120 --> 00:24:45,080
an offline data set that sits in the hard drive of a computer, like I mentioned that, you take a

295
00:24:45,080 --> 00:24:51,480
picture once and that's it, you're annotated, you have no control over this pixel any longer,

296
00:24:51,480 --> 00:24:56,280
you cannot say like let me move a little bit in this image, like let me look at the same object

297
00:24:56,280 --> 00:25:01,640
from left or right and so on. So this pixel is like pre-recorded and is offline basically.

298
00:25:02,200 --> 00:25:07,800
Now by definition, active agent has some degree of freedom, so it can do something,

299
00:25:07,800 --> 00:25:14,040
it can move around for instance if it's an navigation agent. So there's a gap between

300
00:25:14,040 --> 00:25:21,560
like static and offline data sets and the type of data that's developing vision for robotics

301
00:25:21,560 --> 00:25:27,960
and balls. We basically need to have some online pipeline for generating data and that's usually

302
00:25:27,960 --> 00:25:34,840
why people use simulators. Again, till a couple of years ago, the simulators were primarily based on

303
00:25:36,120 --> 00:25:42,280
synthetic data. These are synthetic data meaning that a designer would sit down and model,

304
00:25:42,280 --> 00:25:47,880
let's say an apartment for you by putting together some cat models of chairs and tables and so on

305
00:25:47,880 --> 00:25:52,440
and they would be a computer graphics pipeline that renders that into pixels for you and we would

306
00:25:52,440 --> 00:25:57,480
use that as a source of data and that would solve the problem of being active because of course,

307
00:25:57,480 --> 00:26:04,520
you can just shift a little bit in this scene and re-render it. So it would give the possibility

308
00:26:04,520 --> 00:26:11,240
of a degree of freedom to the agent that is using the data but the main problem was that the data

309
00:26:11,240 --> 00:26:16,520
was not done from the real world. So there would be no guarantee that this would generalize and

310
00:26:16,520 --> 00:26:21,960
generally speaking since computer graphics is not completely solved yet. So the type of pixels

311
00:26:21,960 --> 00:26:27,640
that we get as a result of this are not fully photorealistic and even if they were photorealistic,

312
00:26:27,640 --> 00:26:32,920
the underlying semantics are coming from a designer and the designers have biases too.

313
00:26:33,960 --> 00:26:39,080
So if you look at the, let's say some of these models that existed in such data sets like Sun

314
00:26:39,080 --> 00:26:45,080
CG and so on, it's the moment you look at an image it's immediately clear that they are coming from

315
00:26:45,080 --> 00:26:50,360
a simulator they're not from the real world and what gives it out is not just the fact that the

316
00:26:50,360 --> 00:26:55,320
pixels are not like photorealistic is the fact that these models are usually too clean. No designer

317
00:26:55,320 --> 00:27:00,760
would sit down and design say a very messy bedroom and so on and so forth. So there was a

318
00:27:00,760 --> 00:27:07,480
there was a photorealism gap but there was a bigger actually semantic gap. So since a couple of

319
00:27:07,480 --> 00:27:12,200
years ago what changed and I think in my opinion was actually a big change in the community was

320
00:27:12,200 --> 00:27:17,800
that data sets came out that they're based on scans of real world buildings and then see if

321
00:27:17,800 --> 00:27:22,920
here are 16 we have one paper and called building parser does that I think I believe the first time

322
00:27:22,920 --> 00:27:31,640
that multiple large buildings were scanned using like commercial scanning pipelines in full in 3D.

323
00:27:31,640 --> 00:27:36,040
So you would have a mesh of one building I remember at that time you had I think six buildings

324
00:27:36,040 --> 00:27:41,320
of a Stanford scan so you could load the mesh in your computer and look at it and it's the entire

325
00:27:41,320 --> 00:27:49,560
building is at your disposal. So that later became a underlying model for many simulators and now

326
00:27:49,560 --> 00:27:55,880
the simulators instead of using the the designer to develop things or design things for you now

327
00:27:55,880 --> 00:28:03,720
there's actually a real building that is serving as the underlying data and so multiple datasets

328
00:28:03,720 --> 00:28:12,920
came out of that one of them is called like Stanford 2D 3DS Matterport 3D and Facebook replica

329
00:28:12,920 --> 00:28:20,440
and so on there was actually the last one in this time wise in Gibson in 2019 there was a

330
00:28:20,440 --> 00:28:25,720
2018 and so if you are we have the paper and Gibson that had brought that to really larger scale

331
00:28:25,720 --> 00:28:31,560
there was about 600 buildings that they were scanned. So your robot can virtually visit 600

332
00:28:31,560 --> 00:28:38,600
buildings interact with it in as far as navigation is involved of course and and learn from it and

333
00:28:38,600 --> 00:28:45,560
sings 100 is a lot if you go to a new building every week and visit all corners of it because

334
00:28:45,560 --> 00:28:51,720
these are scanners like scan everything if you go to building a new building every week it takes 10

335
00:28:51,720 --> 00:28:57,560
years to get to 600 buildings so there's a lot of visual data certainly more than what like humans

336
00:28:57,560 --> 00:29:06,360
probably observe by the end of age two that's where like vision is sufficiently developed so

337
00:29:06,360 --> 00:29:11,400
that wasn't excused anymore essentially lack of data. Given the the constraint that you mentioned

338
00:29:12,440 --> 00:29:18,120
on you know how long it takes to scan these buildings was the the data crowdsourced?

339
00:29:18,840 --> 00:29:24,920
Yes to to some extent we we spoke with we had like hammer man some of them we would send them

340
00:29:24,920 --> 00:29:30,040
to actually scans buildings for us some of them would have a scans already so it would acquire it

341
00:29:30,040 --> 00:29:36,680
from them but yeah we didn't actually scan them it was like people's that would scan them for

342
00:29:36,680 --> 00:29:43,240
different reasons then we would acquire them and there's actually since the scanning pipeline

343
00:29:43,240 --> 00:29:50,680
series scanning pipelines are now sufficiently mature there is actually a support chain for them

344
00:29:50,680 --> 00:29:57,320
primarily coming from real estate market you know most of the I'm not actually sure if it's

345
00:29:57,320 --> 00:30:01,400
most of the houses but a good percentage of the houses if you want to sell them well you have

346
00:30:01,400 --> 00:30:07,480
to scan them in 3D so you can put up a website for them so the buyer can navigate in it before

347
00:30:07,480 --> 00:30:14,120
they actually come see it and so on so that's actually a good resource of data for us and that

348
00:30:14,120 --> 00:30:20,040
actually created a supply chain of camera man that you can probably any city at least in the

349
00:30:20,040 --> 00:30:25,400
United States and in Canada you can hire a camera man that already has the camera and they can

350
00:30:25,400 --> 00:30:31,640
go with scan and within a few hours they can send them all this habitat competition this is a

351
00:30:31,640 --> 00:30:38,760
benchmark that Facebook is proposing for agents that are navigating spaces can we talk about what

352
00:30:38,760 --> 00:30:45,960
the kind of how do they quantify performance in this environment right yeah it's built

353
00:30:45,960 --> 00:30:52,600
Facebook has a team of software engineers and researchers that developed a habitat platform

354
00:30:53,080 --> 00:30:59,240
like you said to a large extent it was developed on top of Gibson and a few other works that

355
00:30:59,240 --> 00:31:06,920
exist in the community like it said in the past like Minos and so on so the way it works in general

356
00:31:06,920 --> 00:31:15,640
is that the tasks are specific like point math or point navigation the agent is dropped

357
00:31:15,640 --> 00:31:23,000
in a new building completely unseen and it's provided at a random location in a new building

358
00:31:23,000 --> 00:31:28,360
and it's provided with a coordinate to go to so the agent could be randomly spawn in a bedroom

359
00:31:28,360 --> 00:31:33,000
and the coordinate that it's provided as the goal location is somewhere in the living room

360
00:31:33,000 --> 00:31:42,520
and all it sees is a stream of RGB or RGB data and it has to now plan its way around the obstacles

361
00:31:42,520 --> 00:31:47,480
and safely navigate itself to that particular location which could be like tens of meters away

362
00:31:47,480 --> 00:31:52,600
so it's a hard problem to solve especially that this is a new building it's not like you could

363
00:31:52,600 --> 00:31:58,680
spend time scan this building something like a slam pipeline and then run ASDAR and something

364
00:31:58,680 --> 00:32:06,360
like that to plan like post plan trajectory is just like it like imagine as a human when you go to

365
00:32:06,360 --> 00:32:10,760
your friends house when they just bought a house they have never been there and once you enter

366
00:32:10,760 --> 00:32:16,600
the living room I want to use the bathroom you can probably think about okay the bathroom would be

367
00:32:16,600 --> 00:32:21,480
probably I need to be looking for some hallways or doors and maybe a little bit of search

368
00:32:22,120 --> 00:32:26,120
you would find your way to the bathroom but you would not be just randomly wandering around

369
00:32:26,120 --> 00:32:30,280
this building for hours and hours till maybe by chance you would find a bathroom

370
00:32:30,280 --> 00:32:37,960
right so so these agents actually the task is something similar to that completely unseen

371
00:32:37,960 --> 00:32:43,960
building a spawn in a random location and provided with a target coordinate just travel to it so

372
00:32:43,960 --> 00:32:51,240
that was the first competition of habitat in 2019 and there actually a lot of entries I don't know

373
00:32:51,240 --> 00:32:56,200
the exact number but they were enough to actually make a good competition there were two tracks

374
00:32:57,160 --> 00:33:02,840
and they were not about the task but they were about the type of data so if the agent all it sees

375
00:33:02,840 --> 00:33:10,520
is RGB that would be akin to having just an RGB camera on a real robot that would be RGB only

376
00:33:10,520 --> 00:33:15,560
track and there was another track with this RGBD that would be like having a 3D sensor to like connect

377
00:33:16,360 --> 00:33:21,240
now we are at a point that we have a challenge with like tens of teams enter and there is a good

378
00:33:21,240 --> 00:33:27,720
amount of energy in the community that is spent in this area so it's to me it's actually a very

379
00:33:27,720 --> 00:33:35,000
healthy progress towards vision plus robotics in this context and this like I said the robots are

380
00:33:35,000 --> 00:33:41,480
limited to navigation now and the reason for it is that our data platforms right now can support

381
00:33:41,480 --> 00:33:50,440
only navigation not manipulation technically speaking it's hard to scan a building and now go make

382
00:33:50,440 --> 00:33:56,120
a change in it and be able to render it these buildings are scanned aesthetically so we don't have

383
00:33:56,120 --> 00:34:03,880
a good like support pipeline yet as of now for capturing both dynamic content and interactive

384
00:34:03,880 --> 00:34:10,760
content there's work going in this direction the community too but I think it's I would summarize

385
00:34:10,760 --> 00:34:17,800
them to be still in a scouting stage and they're in fancy so we'll see how that plays out in a few

386
00:34:17,800 --> 00:34:23,160
years but navigation is something that you know it's a reasonably stable we have good data platform

387
00:34:23,160 --> 00:34:30,440
um for it and we can look at the output and and to be honest like I was personally impressed

388
00:34:30,440 --> 00:34:39,400
by the the performance of the the winning teams in both tracks it's it's it's actually a hard problem

389
00:34:39,400 --> 00:34:45,320
to drop an agent in an unseen building and give it a goal that is like tens of meters away

390
00:34:46,040 --> 00:34:50,200
navigating the path is hard because you need to identify a lot of things you need to know for

391
00:34:50,200 --> 00:34:54,200
instance where the doorway is to get yourself out of the room and the woman you're out of the room

392
00:34:54,200 --> 00:35:01,000
you don't want to hit like the walls or many obstacles in the way and then maybe the target is behind

393
00:35:01,000 --> 00:35:08,040
let's say a chair and so on so there's a lot of like a fine-grained planning that goes into successfully

394
00:35:08,040 --> 00:35:12,840
doing that but the success rates are actually higher than what I expected so hopefully in the next

395
00:35:12,840 --> 00:35:17,240
year we'll see this becoming more mature and the task of force need to be more realistic

396
00:35:17,240 --> 00:35:23,800
um point navigation is is probably the simplest one uh we need to be able to navigate towards

397
00:35:23,800 --> 00:35:28,760
uh like semantic network towards and that semantic navigation let's say if you task an agent with

398
00:35:28,760 --> 00:35:33,480
finding a key the key doesn't have a specific coordinate it can be anywhere but it's not in arbitrary

399
00:35:33,480 --> 00:35:39,240
locations too are there any other uh kind of highlights uh in the vision for robotics

400
00:35:40,520 --> 00:35:47,160
yeah I think I think I think this was habit that was uh and and Gibson and so on

401
00:35:47,160 --> 00:35:53,640
were actually a good uh representative of the progress uh in terms of the discussion that

402
00:35:53,640 --> 00:35:59,640
we had earlier is uh like if you can learn everything from raw pixels what we didn't need for like

403
00:35:59,640 --> 00:36:06,840
vision uh uh they were actually multiple papers that came out in this area that they had

404
00:36:06,840 --> 00:36:13,400
focused the studies on why actually it is critical to have vision pipelines when you're trying to

405
00:36:13,400 --> 00:36:22,040
do robotic tasks uh one of them was uh a paper that we did um um we published in um last year I believe

406
00:36:22,040 --> 00:36:27,960
in December 2018 on archive it's called mid-level vision representations improve generalization

407
00:36:27,960 --> 00:36:33,800
sample efficiency of vision water policies and an update that we published in in coral uh in

408
00:36:33,800 --> 00:36:41,240
November that's basically uh mid-level vision for uh for navigation and there's also another paper

409
00:36:41,240 --> 00:36:48,440
that had a very similar flavor concurrently came out that uh was actually published in um um

410
00:36:48,440 --> 00:36:53,240
I don't remember actually living in an an aerobotic strontal and that how it actually does

411
00:36:53,240 --> 00:36:59,800
combat a vision matter for action which is a very direct statement of of the question and the

412
00:36:59,800 --> 00:37:05,160
the conclusion to both of these is that yes it's really important and critical to have vision

413
00:37:05,160 --> 00:37:11,960
pipelines and the and the reason is again those priors if you don't supply your um your robotic

414
00:37:11,960 --> 00:37:16,040
pipeline with these priors about the world like the world is treaty there are objects and there's

415
00:37:16,040 --> 00:37:21,080
permanence and so on and so forth yes there's a way that they can learn it but it will take

416
00:37:21,080 --> 00:37:25,960
tremendous amount of data like millions and millions of interactions to recover those facts

417
00:37:26,760 --> 00:37:33,720
so either we we don't provide those priors and we directly use raw pixels and but the

418
00:37:33,720 --> 00:37:40,760
consequence of that is being inefficiency as we know let's say tabularasa or l is very inefficient

419
00:37:41,400 --> 00:37:45,560
that's why most of the time it's focused on simulators because you cannot do as much interaction

420
00:37:45,560 --> 00:37:51,640
with the real world or it will take years or we make ourselves prone to not generalization

421
00:37:52,360 --> 00:37:58,120
you could solve the problem for limited space for where you did learning that's when the algorithms

422
00:37:58,120 --> 00:38:03,880
find the shortcuts but then the moment you go to a new building because the same shortcuts that

423
00:38:03,880 --> 00:38:09,480
they used for learning there was based on bicep data so they wouldn't generalize so both of these

424
00:38:09,480 --> 00:38:16,120
papers and they strongly showed that supplying these priors which is the job of computer vision

425
00:38:17,640 --> 00:38:22,600
significantly improves efficiency so you can learn faster with less data and also it improves

426
00:38:22,600 --> 00:38:28,200
generalization so what you learn in one building it won't be specific that one building anymore

427
00:38:28,200 --> 00:38:36,840
so I think this was actually a very strong conclusion and in a way response to to to demand for a

428
00:38:36,840 --> 00:38:44,280
study that it is really important for robotic pipelines to to use like vision algorithms and priors

429
00:38:44,840 --> 00:38:50,600
so I think those that should basically summarize are at least in my opinion the important

430
00:38:50,600 --> 00:38:57,480
progress in vision for robotics in past year the next area you had in mind was 3d vision which

431
00:38:57,480 --> 00:39:05,080
we've actually talked quite a bit about in the robotics context yes exactly so I think that the

432
00:39:05,080 --> 00:39:12,200
motivation is clear the world is especially 3d but when we look at an image we are looking at

433
00:39:12,200 --> 00:39:22,040
actually a 2d projection of it so basically that calls for the problem of recovering the 3d

434
00:39:22,040 --> 00:39:28,440
structure as a human you understand the world in 3d just because the retina in our eyes receives

435
00:39:28,440 --> 00:39:34,200
a 2d projection that doesn't mean we are unable to recover that underlying 3d but we do that through

436
00:39:34,200 --> 00:39:42,600
multiple mechanisms such as a stereo we have eyes so a stereo is a solution to recovering 3d but

437
00:39:42,600 --> 00:39:47,000
even when you cover one of your eyes you can still see the world in 3d basically it means some

438
00:39:47,000 --> 00:39:53,320
brick ignition or learning based is in process so we need so recovering this 3d

439
00:39:53,320 --> 00:40:03,000
structure from a 2d projection such as an image is a strong problem in vision community especially

440
00:40:03,000 --> 00:40:10,920
in the past year I think I see a rise I think that was a demonstrated by the fact that the

441
00:40:12,280 --> 00:40:17,560
paper award nominations and both to see if you are an ICCV include actually papers and 3d

442
00:40:18,600 --> 00:40:25,240
and also I think the problem has actually another angle to and 3d vision now we have 3d sensors

443
00:40:25,240 --> 00:40:31,320
because 3d is important we need processing pipelines for this 3d data so this is different from

444
00:40:31,320 --> 00:40:37,480
the first problem they mentioned images 2d you want to recover 3d and then you will do something

445
00:40:37,480 --> 00:40:42,840
with that recovered 3d sometimes we have the 3d from a sensor like a lighter or connect and so on

446
00:40:43,560 --> 00:40:51,000
but the processing pipeline that views for images do not directly work on 3d data

447
00:40:52,120 --> 00:40:56,680
they either work or they are very inefficient or they just don't produce as much good results so

448
00:40:56,680 --> 00:41:03,880
that's actually the second category in 3d vision having pipelines for processing 3d data

449
00:41:05,400 --> 00:41:13,400
both of them I believe they expand it notably in 2019 and I see a rising trend over there so

450
00:41:13,400 --> 00:41:20,680
I expect to see more and more of it in the years to come I think they're like in the first

451
00:41:20,680 --> 00:41:27,160
like we can actually maybe talk about a specific example in each of these categories does that sound

452
00:41:27,160 --> 00:41:33,400
to okay yeah absolutely yeah so I think in terms of like generally when we talk about the first

453
00:41:33,400 --> 00:41:39,000
category of problems recovering 3d from a 2d image first question is that where we get the data

454
00:41:40,520 --> 00:41:45,240
you have to have an image and have the underlying 3d data if you want to do it in a

455
00:41:45,240 --> 00:41:50,040
so police supervise learning manner to all to learn let's say your network that receives an image

456
00:41:50,040 --> 00:41:58,680
and a spit out the 3d so where we get the data so 3d sensors are are there but they're less common

457
00:41:58,680 --> 00:42:08,680
than RGB so we have a lot more RGB content than RGB plus 3d out there it work around that is common

458
00:42:08,680 --> 00:42:20,680
and now is that people get RGB data let's say a YouTube video or multi-view cameras and then

459
00:42:20,680 --> 00:42:26,760
recover 3d using like classic methods like the structure from ocean slam and so on and then

460
00:42:26,760 --> 00:42:35,640
once that is recovered using classic methods they use the recovered 3d as a supervise supervision

461
00:42:35,640 --> 00:42:43,320
for doing RGB to 3d from like individual frames like monocular that has been working for a few

462
00:42:43,320 --> 00:42:49,720
years and that has been shown like effective to a reasonable extent one thing that makes that

463
00:42:49,720 --> 00:42:57,480
hard is dynamic content because like those classic 3d reconstruction methods usually work based

464
00:42:57,480 --> 00:43:03,240
on point correspondences and then you have moving objects into scene it becomes basically an

465
00:43:03,240 --> 00:43:07,480
ill-posed problem is that is it a treaty of the scene that is governing this motion or is

466
00:43:07,480 --> 00:43:12,120
actually there's like let's say a human that is moving in the scene and so on so and humans are

467
00:43:12,120 --> 00:43:17,080
actually specifically an important part because humans are a very important object in the world in

468
00:43:17,080 --> 00:43:24,120
general so it was one paper that I personally found like pretty cute and past year it was learning

469
00:43:24,120 --> 00:43:30,040
the depth of moving people by watching frozen people so it was it had a very interesting like

470
00:43:30,040 --> 00:43:36,760
workaround there was the mannequin challenge that became popular if you remember it was about like

471
00:43:36,760 --> 00:43:44,520
people just standing stationary for a period of time and somebody would film them so that is

472
00:43:44,520 --> 00:43:50,520
actually interesting because they did a lot of data collection for us implicitly so people

473
00:43:50,520 --> 00:43:56,520
just stood the stationary and somebody walked between them so we actually had they collected a

474
00:43:56,520 --> 00:44:02,360
snapshot of the world where it was frozen and people were specifically there because it was

475
00:44:02,360 --> 00:44:09,480
mannequin challenge so there was a moving camera but no dynamic content but whereas the rest of

476
00:44:09,480 --> 00:44:13,960
the times it's really hard if you think about it to find people when they are stationary and then

477
00:44:13,960 --> 00:44:18,760
they don't move unless they are sleeping when somebody is awake there's usually some at least

478
00:44:18,760 --> 00:44:25,640
micro motion so mannequin challenge was great that turned into a source of data that this paper used

479
00:44:25,640 --> 00:44:34,040
so now they they apply the methods that would not probably normally work for dynamic content

480
00:44:34,040 --> 00:44:40,920
in people and the mannequin challenge data and then use that as supervision for now they can do

481
00:44:40,920 --> 00:44:46,360
depth of moving people whereas they actually learned it on frozen people so they kind of tricked

482
00:44:46,360 --> 00:44:52,760
the system by learning from frozen people but when you when you do it framed by frame then it

483
00:44:52,760 --> 00:44:59,240
basically doesn't know that the person is moving so that that was an interesting so an interesting

484
00:44:59,240 --> 00:45:07,240
paper and I think it was a representation of the fact that still when we are doing 3D vision

485
00:45:08,520 --> 00:45:15,320
data is a big problem there and this paper actually addressed the data by but it's nice

486
00:45:15,320 --> 00:45:22,760
interesting challenge to happen past year yeah it's a fairly it's quite creative but it also

487
00:45:22,760 --> 00:45:29,160
breaks the question you know for you know where will we get the data sets required to generalize

488
00:45:29,160 --> 00:45:34,920
this kind of approach right yeah so like I said there are aspects of the world that are static

489
00:45:35,640 --> 00:45:43,640
like I said for instance like in Gibson data these are human like primarily these are like

490
00:45:43,640 --> 00:45:52,760
residential places and and people just go and scan them and by default there's little motion

491
00:45:52,760 --> 00:45:58,440
unless there's a human in the same so humans very often are one of the main sources of like

492
00:45:58,440 --> 00:46:02,920
motion and there's some other forms of like motion too like if there's a fan on the ceiling that

493
00:46:02,920 --> 00:46:08,840
is like rotating there's no human there but it's there's motion there so but they're less common

494
00:46:08,840 --> 00:46:14,760
so and also like I said humans are around the most important objects in the world so it makes

495
00:46:14,760 --> 00:46:21,480
sense to actually have processing pipelines that that solve them and then we'll take it from there

496
00:46:21,480 --> 00:46:27,320
yes I guess we need to be either more creative or we can hope for algorithms that come out and

497
00:46:27,320 --> 00:46:33,800
handle the dynamic create a content better to be able to generalize it to to the more general setting

498
00:46:33,800 --> 00:46:42,120
yeah and and specific to this model is the idea that the model that's created with this technique

499
00:46:42,120 --> 00:46:48,520
could be kind of pulled out and used in a transfer learning kind of fashion and other pipelines

500
00:46:48,520 --> 00:46:56,920
or that you would incorporate this process into other pipelines and or none of the above

501
00:46:56,920 --> 00:47:02,840
is it just more of a proof of concept well I think the transfer in a way that happened was

502
00:47:02,840 --> 00:47:09,640
transferred from frozen people to moving people so the data was purely from mannequin challenge

503
00:47:09,640 --> 00:47:15,000
in which nobody's moving right by by the definition of mannequin challenge but once you learn from

504
00:47:15,000 --> 00:47:21,400
there that doesn't mean that when you're using the learn the learn model it has to be applied on

505
00:47:21,400 --> 00:47:29,720
on frozen people too so think about it this way what they let's say if you learn a frame by frame

506
00:47:29,720 --> 00:47:35,400
processing pipeline out of the mannequin challenge so it receives one image of a human

507
00:47:36,520 --> 00:47:43,720
and it can recover the 3D now during the actual data in the in the data set the people are frozen

508
00:47:43,720 --> 00:47:50,200
so five five frames in a row the person is frozen so it's not moving but it's a frame by frame

509
00:47:50,200 --> 00:47:55,000
process processing the frame first frame is independent of the second frame and third frame

510
00:47:55,000 --> 00:48:01,160
so at the test time you can actually apply it on a video where people are moving and it can just

511
00:48:01,160 --> 00:48:07,400
perfectly find recovered the 3D because it basically bridges no connection between the first

512
00:48:07,400 --> 00:48:13,560
frame and second frame so you can you can now apply that on moving people actually they did

513
00:48:13,560 --> 00:48:20,680
show it in the paper that they learn on frozen people and they applied and moving people

514
00:48:20,680 --> 00:48:26,680
and that's the consequence of the fact that you know whenever you do say single frame processes

515
00:48:26,680 --> 00:48:32,680
you can the frames are independent that's actually an old trick you have we have seen it before

516
00:48:32,680 --> 00:48:40,360
in many papers and it goes from there so so that's the type of transfer I'm not aware of any sort of

517
00:48:40,360 --> 00:48:46,120
like other transfer learning being in play here but if we have a pipeline that it can recover

518
00:48:46,120 --> 00:48:52,440
the 3D of people reliably well I think that itself is a pretty important problem and worth

519
00:48:53,240 --> 00:49:01,240
worth attention so that was the on an example from recovering 3D from 2D images like I said the

520
00:49:01,240 --> 00:49:08,760
other batch of problems and 3D is processing the data that is 3D let's say an output of a light

521
00:49:08,760 --> 00:49:14,200
or sensor or connect let's say to extract semantics out of it you can do object detection given

522
00:49:14,200 --> 00:49:19,080
an image or you can do object detection given a lighter scan and you'd hope that if you have

523
00:49:19,080 --> 00:49:23,000
something like lighter you would have more information than what an image gives you so you

524
00:49:23,000 --> 00:49:30,280
should be able to do a better job so that has been progress in general on this front to in in

525
00:49:30,280 --> 00:49:38,360
terms of efficiency and accuracy of extracting semantics given 3D data whether it's lighter

526
00:49:38,360 --> 00:49:47,240
or or connect and whatnot there are multiple papers actually there's I believe a Stanford

527
00:49:47,240 --> 00:49:54,760
there's a pipeline it's called Minkowski engine that is focused on an efficient 3D processing

528
00:49:55,880 --> 00:50:01,160
there's point that too that came a couple of years ago but it has been like matured since then

529
00:50:01,160 --> 00:50:10,040
a few months ago in an ICCV and so there was this paper on deep half voting for 3D object

530
00:50:10,040 --> 00:50:14,920
detection in point cloud that's actually another representative I believe it was one of the

531
00:50:14,920 --> 00:50:21,400
award nominations too so 3D sensors typically give you something like a point cloud and then you

532
00:50:21,400 --> 00:50:27,000
want to do various type of semantic extractions on top of that an object detection is probably

533
00:50:27,000 --> 00:50:33,720
most nominal example you want to put a bounding box around the objects that you see and so this

534
00:50:33,720 --> 00:50:41,480
paper actually had some pretty good results on that and the idea in general was and voting so

535
00:50:41,480 --> 00:50:48,600
when you look at let's say the point cloud is as a name says the cloud of 3D points and each

536
00:50:48,600 --> 00:50:55,080
point can vote for what kind of object that it belongs to like all the points that belong to a

537
00:50:55,080 --> 00:51:01,160
chair that can vote for for being a chair and for like some parametrized like our president

538
00:51:01,160 --> 00:51:07,640
of points let's say a center of the chair and um this paper actually used this voting pipeline

539
00:51:07,640 --> 00:51:16,680
and a voting voting concept and report actually good results in terms of object detection something

540
00:51:16,680 --> 00:51:25,000
similar to that actually happened in 2D sector 2 I believe it was called center net

541
00:51:25,000 --> 00:51:29,880
on something or something similar to that that they're like pixels and an image vote for the center

542
00:51:29,880 --> 00:51:37,240
of the object that they belong to so that's in a way it's different from the the the preceding

543
00:51:40,120 --> 00:51:46,040
angle that was basically labeling each pixels and its own like such as in segmentation and so on

544
00:51:46,040 --> 00:51:51,880
or finding finding exact like it's it wouldn't be a voting pipeline that's that's a distinction

545
00:51:51,880 --> 00:52:00,120
between them so um I think that was also just just to have a nominal example of papers that do 3D

546
00:52:00,120 --> 00:52:05,480
processing for extracting semantics and I and I see that um to be again a rising movement

547
00:52:06,200 --> 00:52:13,320
to develop more efficient pipelines with an vision community for recovering 3D or processing 3D

548
00:52:13,320 --> 00:52:20,120
and you know doing more work rounds um for lack of data there let's move on to the next category

549
00:52:20,120 --> 00:52:26,600
now they're area that I I believe in 2019 continued to observe progress with self supervised learning

550
00:52:26,600 --> 00:52:32,680
in general we can I think summarize that things are working sufficiently where for fixed mapping

551
00:52:32,680 --> 00:52:38,120
problems if you have enough data meaning that if you have an image you want to like extract something

552
00:52:38,120 --> 00:52:43,240
out of it let's say objects and so on if you have enough data it's a matter of probably designing

553
00:52:43,240 --> 00:52:48,600
your architecture and struggling with with hybrid parameters a little bit and training it for long

554
00:52:48,600 --> 00:52:55,080
enough you'll get some sufficient results the question that have been concerning a lot of vision

555
00:52:55,080 --> 00:53:00,840
researchers several since like several years ago after deep learning probably immediately after

556
00:53:00,840 --> 00:53:04,680
deep learning way it came was that whatever you're going to do if you don't have enough data

557
00:53:05,320 --> 00:53:12,040
and that gave rise to multiple research directions um self-suppose learning being one of them

558
00:53:12,040 --> 00:53:19,160
and I think the general uh the general question and the the goal there is that you want to

559
00:53:19,160 --> 00:53:26,680
rely on less and less on human labeled data you have a lot of data raw data in hand

560
00:53:26,680 --> 00:53:33,000
but just rely less and less on human labeled data and use raw data to identify trends in there

561
00:53:33,000 --> 00:53:39,560
and then kick a start your um learning pipeline to the point that the the the reliance on

562
00:53:39,560 --> 00:53:47,160
and human labels is just only for the last mile and perhaps exceptions so self-suppose learning

563
00:53:47,160 --> 00:53:54,120
has been going forward for a number of years um that you can actually look at the workshops

564
00:53:54,120 --> 00:54:00,520
within vision and machine learning community um to and you'll see that every year there's actually

565
00:54:00,520 --> 00:54:08,280
a popular workshop in that um in ICML last year in 2019 the we also had this workshop on on

566
00:54:08,280 --> 00:54:16,120
self-suppose learning hopefully we'll have it again in ICML 2020 in CBPR 18 uh we had the beyond

567
00:54:16,120 --> 00:54:22,280
supervised so there's a lot of basically focus on this problem in the community and a lot of

568
00:54:22,280 --> 00:54:29,320
good researchers have focused on it um so it's not as specific to 2019 as a trend but in 2019 I think

569
00:54:30,200 --> 00:54:35,800
it was for the first time that we saw that uh the image net problem essentially being solved

570
00:54:35,800 --> 00:54:43,960
with less labels or uh it was to be more specific it was the first time that without using labels a

571
00:54:43,960 --> 00:54:51,400
self-supervised pipeline was able to achieve as uh good results that uh uh data as a

572
00:54:51,400 --> 00:54:57,960
fully supervised pipeline would receive and that was actually a good point um I believe there were

573
00:54:57,960 --> 00:55:06,280
like two or three papers that reported that uh success the one that I particularly liked was the

574
00:55:06,280 --> 00:55:12,360
there's a paper based on contrastive predicting coding um so this paper the contrastive

575
00:55:12,360 --> 00:55:17,000
predicting coding that the title of the paper is data-efficient image recognition with contrastive

576
00:55:17,000 --> 00:55:22,680
predictive coding um so the contrastive predicting coding is not a new idea it has existed

577
00:55:22,680 --> 00:55:29,960
existed before but I think they really rendered it into a mature pipeline to the point that

578
00:55:29,960 --> 00:55:36,280
it's stable now and it can at as far as this image net data set is concerned you can actually

579
00:55:36,280 --> 00:55:41,800
get good image net classification results without having to use the image net labels which is

580
00:55:41,800 --> 00:55:47,800
uh which is actually interesting and a good proxy for for progress here the contrastive

581
00:55:47,800 --> 00:55:53,720
predicting coding is one of the ideas that have worked towards this purpose is actually not a complex

582
00:55:53,720 --> 00:56:00,600
concept in general you can let's say you you you learn the regularities in the visual world to be

583
00:56:00,600 --> 00:56:06,520
able to learn good features out of it let's say if you start if I show you the like top half of an

584
00:56:06,520 --> 00:56:11,080
image you will have an idea of what the bottom half would be like if in the top half if you see

585
00:56:11,080 --> 00:56:15,640
like a head of a dog probably you would say in the bottom half there would be the body of a dog

586
00:56:15,640 --> 00:56:20,600
so now when would you be able to make that prediction he would be able to make that prediction when

587
00:56:20,600 --> 00:56:26,120
you know how the world looks like you know that dogs are not just these like heads that are like

588
00:56:26,120 --> 00:56:33,160
floating in the air right there's usually a body to support it so so the raw image if you have a

589
00:56:33,160 --> 00:56:40,520
lot of them is enough for for learning that so so that's basically uh like in a very simplified

590
00:56:40,520 --> 00:56:47,560
way the idea behind like predictive coding you show part of the data and you learn your system

591
00:56:47,560 --> 00:56:53,160
to predict the rest of the data and this requires no labeling because the data is like raw and

592
00:56:53,160 --> 00:56:59,000
available to you you just mask it during learning and you get the on your network to predict the

593
00:56:59,000 --> 00:57:05,560
mask part so that this turns out to be actually a good way of learning features and the paper that

594
00:57:05,560 --> 00:57:11,400
I mentioned actually uses this idea in a more advanced way to not rely on human defined labels

595
00:57:11,400 --> 00:57:17,480
that's in contrast to let's say something like AlexNet that basically just started from images

596
00:57:17,480 --> 00:57:26,280
from the iteration zero of learning it would map pixels into human defined labels human

597
00:57:27,000 --> 00:57:32,280
human defined labels of course those thousand classes but actually in like every a single

598
00:57:32,280 --> 00:57:40,120
instance of an image was labeled by playing annotator so um so that was actually there I think that

599
00:57:40,760 --> 00:57:47,560
what you need to be aware of for the year to come is that this was shown for image net and this

600
00:57:47,560 --> 00:57:55,240
does not mean that we can do this for all problems and vision and under all settings so the caveat

601
00:57:55,240 --> 00:57:59,160
essentially is that this is a specific setup there's a certain architecture there's a linear

602
00:57:59,160 --> 00:58:04,120
layer and so on this doesn't mean that this will continue to happen regardless of these design choices

603
00:58:05,000 --> 00:58:10,600
and uh but still we didn't have this even even this prior to this year so I think it's

604
00:58:10,600 --> 00:58:17,000
that we can there's something to celebrate here and the more importantly uh even if it was

605
00:58:17,000 --> 00:58:23,160
architecture agnostic this is about image net and image net is not the world it's very niche part

606
00:58:23,160 --> 00:58:28,520
of the world it's a very dog biased part of the world because there are like many dog classes

607
00:58:28,520 --> 00:58:32,680
in there and then for instance from other species and animals is like a less of them

608
00:58:32,680 --> 00:58:39,320
so of course this is about the bias that the design of any dataset involves um the fact that we

609
00:58:39,320 --> 00:58:44,200
showed this on as a community on image net that doesn't mean that we can show it for everything

610
00:58:44,200 --> 00:58:51,480
so we can we can call it a success when for any image recognition problem we would have a

611
00:58:51,480 --> 00:58:56,840
self-supervised learning pipeline in which we would not rely have to rely on human labels too much

612
00:58:56,840 --> 00:59:00,520
and be able to solve that problem let's say if somebody does that for a depth estimation

613
00:59:01,160 --> 00:59:07,400
or object segmentation and so on then it would be a lot more convincing but that was definitely

614
00:59:07,400 --> 00:59:12,760
a progress for 2019 and so's with us learning too now we've seen uh uh

615
00:59:13,800 --> 00:59:23,000
2019 and and 2018 as well has brought um self-supervised learning to the fore on the NLP

616
00:59:23,000 --> 00:59:31,240
uh within the realm of NLP in a big way with models like Bert and Elmo and others you know

617
00:59:31,240 --> 00:59:38,280
is there a relationship between the you know this being a focus area vision in the vision space

618
00:59:38,280 --> 00:59:43,640
do you think you think that we're kind of pulling from you know inspiration from you know one to

619
00:59:43,640 --> 00:59:49,880
the other no no it's actually a great point it's both ways um like predictive coding in a way

620
00:59:49,880 --> 00:59:57,480
actually has similarities to the models that that work well in NLP too so um like a language model

621
00:59:57,480 --> 01:00:03,720
in general is something similar to that like you give the beginning of of a sentence where

622
01:00:03,720 --> 01:00:08,040
there is character words but you give it the beginning and it turns to send to predict what comes

623
01:00:08,040 --> 01:00:13,400
next and it can push it really it's a not just the next word that comes but many words that

624
01:00:13,400 --> 01:00:18,840
that come after that or you can turn it into like fill in the blank problem so it's all about like

625
01:00:18,840 --> 01:00:23,560
masking part of the data and filling it in whether it's the next part of the data or some like

626
01:00:23,560 --> 01:00:32,200
proceeding part of data um but the the common just here is that uh we use raw data to be able to

627
01:00:32,760 --> 01:00:38,360
we define a problem that is primarily based on masking part of the data and get the system to learn

628
01:00:38,920 --> 01:00:45,640
to to fill that gap and that is just a good feature and then what comes out of this like

629
01:00:45,640 --> 01:00:49,800
pipeline is a feature whether it's learned on text yeah it becomes something like

630
01:00:49,800 --> 01:00:55,480
birthed if it's learned on images it becomes something like the um predictive coding pipeline

631
01:00:55,480 --> 01:01:01,400
that I mentioned earlier and they become good features that are just sufficiently at least

632
01:01:01,400 --> 01:01:06,040
as for us let's say for images uh image that was involved sufficiently universal so could you

633
01:01:06,040 --> 01:01:11,720
use those features to to read different things out of it such as like image not classification

634
01:01:11,720 --> 01:01:16,520
so yeah there are similarities actually and I see that going both ways there are many people

635
01:01:16,520 --> 01:01:24,360
that that that work on both um the self-survised like models for both image and and text

636
01:01:25,080 --> 01:01:32,760
another category I think is worth mentioning um and it it's uh it's interesting to me the connection

637
01:01:32,760 --> 01:01:40,680
between either cellular postmas um research and and vision so um it's basically an interesting

638
01:01:40,680 --> 01:01:46,200
finding so in general the robustness issue is that you know you would have an image recognition

639
01:01:46,200 --> 01:01:51,640
pipeline let's say object detection and turns out that if you can make very small changes in the

640
01:01:51,640 --> 01:01:57,720
input uh something that humans even won't see like adding a few pixels or making a change like

641
01:01:57,720 --> 01:02:03,960
very tiny tiny changes in the pixels there's a way to actually mess with the uh object detection

642
01:02:03,960 --> 01:02:11,640
pipeline uh whereas the humans wouldn't see that change at all so that kind of made uh um

643
01:02:11,640 --> 01:02:16,840
it raised a big question and it made a lot of people concerned also it's it's

644
01:02:16,840 --> 01:02:21,560
besides like the concerns and safety and so on uh it was actually an interesting question

645
01:02:21,560 --> 01:02:28,120
intellectually that why is it that uh image recognition pipeline that seemed to be working well

646
01:02:28,120 --> 01:02:33,640
there's a there's such a big loophole that we can mess with the pixels in a very imperceptible

647
01:02:33,640 --> 01:02:43,160
way um that that uh changes the output such drastically um so there has been progress in in the

648
01:02:44,120 --> 01:02:49,720
in the community of adversarial robustness community um towards like methods that just

649
01:02:49,720 --> 01:02:56,440
robustifies the neural networks we respect to these changes um a lot of work came out of Alexander

650
01:02:56,440 --> 01:03:04,760
Matthews group at MIT and they showed actually good progress towards like high frequency adversarial

651
01:03:04,760 --> 01:03:10,520
patterns and they basically train the augment the the the training of a system in a way that

652
01:03:11,720 --> 01:03:17,560
the the outcome is in vain and respect to that kind of knowns is and then therefore you would

653
01:03:17,560 --> 01:03:22,760
expect more robustness and indeed the system does become more robust at least with respect to that

654
01:03:22,760 --> 01:03:30,840
particular adversarial pattern now I think what was interesting uh in retrospect it's common sense

655
01:03:30,840 --> 01:03:39,000
but it it had to be shown that if you robustify every image recognition pipeline you would expect

656
01:03:39,000 --> 01:03:45,240
it to have a better understanding of the world and therefore it should actually work even better

657
01:03:45,240 --> 01:03:50,440
for applications that are not within the context of adversarial robustness so let's say if you

658
01:03:50,440 --> 01:03:55,880
um if you have a image recognition pipeline that with a little bit of changes in the pixels around

659
01:03:55,880 --> 01:04:01,640
the dog it would just misclassify a dog for an ostrich and it just basically means it didn't

660
01:04:01,640 --> 01:04:07,480
quite understand what a dog is um so it was prone to these kind of mistakes now if you have a robust

661
01:04:07,480 --> 01:04:12,280
system that it doesn't make that system anymore it means it better understood what a dog is so

662
01:04:12,280 --> 01:04:17,160
it better understood the manifold of real world images and what's possible in the real world and not

663
01:04:17,160 --> 01:04:25,240
so those this paper actually in Europe's just uh two weeks ago um from Alexander Matri's group that

664
01:04:25,240 --> 01:04:32,760
basically showed that if you have a robust image classifier image classifier that is like trained

665
01:04:32,760 --> 01:04:38,280
with this robustness mechanism you can use it to synthesize better looking images compared to

666
01:04:38,280 --> 01:04:45,000
the same exact classifier just without being trained with uh with adversarial robustness the title

667
01:04:45,000 --> 01:04:50,120
paper I believe was image synthesis with a single robust classifier and they basically

668
01:04:50,120 --> 01:04:58,520
show it that uh side by side you can look at uh synthesized image coming out of a neural network

669
01:04:58,520 --> 01:05:02,200
you know in image synthesis pipeline we always use some sort of pre-trained neural network

670
01:05:02,920 --> 01:05:11,080
an image network and what not uh there's a requirement um um in the in the system so for that

671
01:05:11,080 --> 01:05:18,360
uh pre-trained neural network if you use a robust classifier versus a non very same exact data same

672
01:05:18,360 --> 01:05:24,040
exact architecture the robust classifier just renders much better looking uh images for various

673
01:05:24,040 --> 01:05:29,560
kind of synthesis problems like in painting or transferring sketches to images or super

674
01:05:29,560 --> 01:05:35,240
resolution and so on so this was an interesting uh finding it like I said it makes sense because of

675
01:05:35,240 --> 01:05:41,400
force and more robustness I mean to understood you would hope that you'd understood the uh uh the

676
01:05:41,400 --> 01:05:46,200
manifold of real world images better so therefore you should be able to do better synthesis too

677
01:05:46,200 --> 01:05:51,240
but you finally have a paper that that actually showed that too which I personally find interesting

678
01:05:51,240 --> 01:05:56,040
and it's another example of the uh mix of different areas with the envision and this time

679
01:05:56,760 --> 01:06:04,680
adversarial about its next research the the generalization result makes me think a little bit of

680
01:06:04,680 --> 01:06:10,280
the parallel to multi-task learning where we've seen over the past you know relatively recently

681
01:06:10,280 --> 01:06:14,520
I think past year or two a lot of work's been going into multi-task learning that showed that

682
01:06:15,480 --> 01:06:21,960
just the addition of another task and the training process helps the networks generalize and

683
01:06:21,960 --> 01:06:31,480
perform better in a sense often in this uh in the uh adversarial uh robustness research

684
01:06:31,480 --> 01:06:38,600
the robustification of the network is another task and so these are kind of similar results

685
01:06:38,600 --> 01:06:44,760
or or related potentially results right to to some extent that's true like essentially the

686
01:06:44,760 --> 01:06:49,640
robustness is as a result of augmenting the loss with the additional terms that you wouldn't

687
01:06:49,640 --> 01:06:57,000
normally have and you can view as multi-task learning uh versus single-task learning as augmenting

688
01:06:57,000 --> 01:07:03,880
the loss of a single task framework with more losses like more tasks that it would normally have

689
01:07:03,880 --> 01:07:10,600
so there's a there's that like regularization effect that uh though I have to say that in multi-task

690
01:07:10,600 --> 01:07:17,240
learning that observation has been challenged quite few in quite few different settings it

691
01:07:17,240 --> 01:07:23,480
really depends for instance what task that you learn together to be able to uh to be able to

692
01:07:23,480 --> 01:07:29,560
actually see the benefits of multi-task learning um uh ironically actually the publish one paper

693
01:07:29,560 --> 01:07:35,080
on this area which is exactly called which tasks should be learned together in multi-task learning

694
01:07:35,800 --> 01:07:43,320
and that that the the title says at all that there's a question there to ask like it's just being

695
01:07:43,320 --> 01:07:49,720
multi-task is not necessarily better uh there's like multiple other parameters in play such as what

696
01:07:49,720 --> 01:07:55,880
tasks should be learned together and there's also like the uh an equal amount of research going into

697
01:07:55,880 --> 01:07:59,880
let's say even if you know what tasks that you want to learn together what the architecture should be

698
01:07:59,880 --> 01:08:04,360
how the sharing exactly should happen should it be like self-parameter sharing hard parameter sharing

699
01:08:04,360 --> 01:08:09,080
some sort of progressive sharing where you decide what parts of the parameters and layers should

700
01:08:09,080 --> 01:08:15,320
be shared and which parts should be dedicated to to networks to to different tasks so um to be able

701
01:08:15,320 --> 01:08:22,280
to get the benefits of multi-task learning um I personally believe that a lot more research has to

702
01:08:22,280 --> 01:08:28,680
be done and the examples that we see are probably when we got lucky and they worked out doesn't

703
01:08:28,680 --> 01:08:34,040
mean we completely understood the problem yet so you know there you have we've kind of gone through

704
01:08:34,040 --> 01:08:44,040
these four key areas you see us making some advances uh in the vision field in 2019 and those are

705
01:08:44,040 --> 01:08:49,960
primarily from kind of the academic perspective how you seeing these trends play out in the real world

706
01:08:50,600 --> 01:08:55,960
yeah I mean uh I think if by the real world you mean like who has used them or in terms of like

707
01:08:55,960 --> 01:09:04,520
commercials um I think probably I see two areas um image synthesis like I said it became uh

708
01:09:04,520 --> 01:09:10,600
it entered the new maturity level like we see apps that come out to actually use these kind of

709
01:09:10,600 --> 01:09:17,080
learning-based synthesis that was a thing a child of the marriage between vision community and graphics

710
01:09:17,080 --> 01:09:25,320
community and these apps that like transfer you from like young to old or like smiley to not

711
01:09:25,320 --> 01:09:32,040
smiling and so on so forth and then there's uh companies like Nvidia I know that they're active

712
01:09:32,040 --> 01:09:38,920
on that and it had the consequences of uh you know the photo fakery concerns like deep fakes and so on

713
01:09:38,920 --> 01:09:43,800
that if things become so well then how do you know when something is real and when it is not

714
01:09:43,800 --> 01:09:48,760
but it's safe to say that that technology has been proven effective and has been adopted

715
01:09:49,640 --> 01:09:57,560
so that's one area that I see um it it entered a maturity level that was uh uh commercialized

716
01:09:57,560 --> 01:10:03,400
and also used by ordinary people and their phones and so on um another interesting area

717
01:10:03,400 --> 01:10:11,640
continues to be autonomous driving um um again a car an autonomous car is actually a robot it's a

718
01:10:11,640 --> 01:10:18,200
it's a big robot it has sensors it has perception that's planning is control though it's a specific

719
01:10:18,200 --> 01:10:24,600
one it's one for navigation designed to move on the roads that you know marked up and so on so

720
01:10:24,600 --> 01:10:33,000
forth so in a way um um it's an example of like robotics vision plus robotics and and again I'm

721
01:10:33,000 --> 01:10:37,480
as commenting and the perception aspect because there's a lot of things that going to getting

722
01:10:37,480 --> 01:10:46,840
something like autonomous driving um car to work um uh we sell progress um now tesla continues

723
01:10:46,840 --> 01:10:52,200
to release new features like a smart summon that you can call the car to to come to you

724
01:10:52,200 --> 01:11:00,280
um in a parking lot and the waymo I heard um during nerfs that they just released

725
01:11:00,280 --> 01:11:07,000
a an app that works in Arizona then it can actually order caps that are completely autonomous

726
01:11:07,000 --> 01:11:12,440
there's no no person in the cap from what I know I haven't used it I haven't been to Arizona

727
01:11:12,440 --> 01:11:18,680
since I heard this but I'm looking forward to doing it um so waymo did that and it's actually

728
01:11:18,680 --> 01:11:23,160
find it impressive you'll see how it it plays out but it can actually have these autonomous caps now

729
01:11:24,200 --> 01:11:29,800
um and different companies have different approaches like you know we know the tesla is um

730
01:11:30,680 --> 01:11:37,640
you know Elon Musk and and he has been vocal against like using heavy 3d sensors like LiDAR

731
01:11:38,840 --> 01:11:44,440
waymo has the safety first approach even at the expense of like coming having more sensors on it

732
01:11:44,440 --> 01:11:49,160
so different companies have different approaches and like I said we have seen progress

733
01:11:49,160 --> 01:11:56,600
such as the waymo release or tesla features but one summary I have is that again the last mile

734
01:11:56,600 --> 01:12:02,920
has been proven to be more difficult than the optimistic anticipations like a few years ago and

735
01:12:02,920 --> 01:12:09,400
many people had like predicted this like rod brooks and so on um if you look at the predictions about

736
01:12:09,400 --> 01:12:13,880
like when autonomous driving would be here let's say five years ago or three years ago let's say

737
01:12:13,880 --> 01:12:20,920
if you would hear by 2020 it would be uh available but it was then then 2018 and 19 they would

738
01:12:20,920 --> 01:12:26,680
push it like yeah a little bit more into the future like five years and you know in 2019 we heard

739
01:12:26,680 --> 01:12:33,240
this question uh multiple time answered but we don't know exactly when it will be here which is

740
01:12:33,240 --> 01:12:38,440
representation of the fact that again the last mile has been like proven to be harder than the

741
01:12:38,440 --> 01:12:45,320
optimistic anticipations but um but I'm watching it actually carefully because it's an interesting

742
01:12:45,320 --> 01:12:51,640
area there's a lot of investment in it there's a lot of talents going in it um so if something

743
01:12:51,640 --> 01:12:57,720
wants to work I believe autonomous driving will be one of the first examples of as the application

744
01:12:57,720 --> 01:13:04,200
of say vision plus AI machine learning and so on and uh so we'll see how it goes do you think the

745
01:13:04,200 --> 01:13:12,200
last mile problem applies specifically or only to autonomous driving whereas uh do we see this

746
01:13:12,200 --> 01:13:19,880
generally across uh bringing research-based uh innovations in the vision domain into real world

747
01:13:19,880 --> 01:13:25,160
applicants? No I absolutely see it as uh as a latter it's not about autonomous driving anymore

748
01:13:25,160 --> 01:13:34,040
any any real world system has a lot of unanticipated issues that you probably won't even think

749
01:13:34,040 --> 01:13:42,200
of before you reach there and um many of these pipelines that we have right now they have caveats

750
01:13:42,200 --> 01:13:48,680
let's say if you have a sensor that it it works fine it might continue to work fine during summer

751
01:13:48,680 --> 01:13:56,040
but during winter when it is snowed how do you know um how it's going to react or like let's say

752
01:13:56,040 --> 01:14:00,280
something like autonomous driving or even like say indoors navigation if you have a robot home

753
01:14:00,280 --> 01:14:05,160
that like navigates safely from one point to another let's say when that happens for household

754
01:14:05,160 --> 01:14:12,760
robotics? I'm pretty sure that you would be surprised by um by the way the system works when

755
01:14:12,760 --> 01:14:17,800
you take it to a different country when the houses look different or things that are that are okay

756
01:14:17,800 --> 01:14:23,880
here they're not okay there and and so on so forth so now I definitely believe that any kind of

757
01:14:23,880 --> 01:14:29,560
real world like adoption of of this research that we're doing especially in terms of machine

758
01:14:29,560 --> 01:14:37,000
learning where there's a factor of uninterpretability as of now um the last mile will be proven

759
01:14:37,000 --> 01:14:45,320
proven harder so let's shift gears from looking backwards into 2019 to looking forwards to

760
01:14:45,320 --> 01:14:53,320
the future maybe give us your top predictions for computer vision for uh 2020 or or beyond I've

761
01:14:53,320 --> 01:14:58,120
been challenging the folks that I've been doing are AR rewind series this year since we're at the

762
01:14:58,120 --> 01:15:05,720
end of a decade to project uh a decade to the future and no one takes me up on it. The problem

763
01:15:05,720 --> 01:15:11,000
with making predictions is that there's a pretty good chance that um for turning out to be wrong

764
01:15:11,000 --> 01:15:17,080
like if if a decade ago you asked anybody about you turning I'm pretty sure the chance of that being

765
01:15:17,080 --> 01:15:23,320
the prediction um would be even though it just like it's in 2010 it just happened two years after

766
01:15:23,320 --> 01:15:29,320
that in in in in computer vision but yeah I don't think I'm not sure if anybody saw that coming

767
01:15:29,320 --> 01:15:34,520
but maybe some people say they saw it coming but they were probably very very small percentage

768
01:15:35,080 --> 01:15:41,880
but um I think you know I can extrapolate a little bit at least if I don't even make you know

769
01:15:42,840 --> 01:15:48,120
two controversial like predictions one thing that I um in terms of since we just managed

770
01:15:48,120 --> 01:15:52,440
the commercial discussion I think at least I can say that from the commercial development perspective

771
01:15:52,440 --> 01:15:57,640
I'll continue to watch autonomous driving progress I really view that as a good proxy for

772
01:15:57,640 --> 01:16:05,080
progress in a well-focused and well-invested area so lack of talent lack of data lack of money

773
01:16:05,080 --> 01:16:11,800
is not really an excuse so it we are really down to solving the problem the physical problem over there

774
01:16:12,440 --> 01:16:18,760
and and and one of the other reasons that I particularly watch autonomous driving uh

775
01:16:18,760 --> 01:16:25,160
progress is that perception wise ironically autonomous driving to some extent is is a simplified

776
01:16:25,160 --> 01:16:29,960
case compared to the general perception case there are there are complications but there's

777
01:16:29,960 --> 01:16:34,840
the reason it's simplified compared to general perception is that like you know autonomous driving

778
01:16:34,840 --> 01:16:40,120
they're like lane markers and there's signs and there's some code of conduct and there's a lot of

779
01:16:40,120 --> 01:16:46,680
data um so the lane markers are you know designed to tell you where you are there as like when you're

780
01:16:46,680 --> 01:16:52,120
just walking in in the woods there's no lane marker so it's the same navigation problem but you

781
01:16:52,120 --> 01:16:58,360
have to it's the lost kind of a lot less information there's a lot less sort of design ahead of time

782
01:16:58,360 --> 01:17:04,440
that's gone into making the problem perceptually simpler so if if we can solve let's say a lane

783
01:17:04,440 --> 01:17:09,720
detector for autonomous driving a big problem is solved like for a general say navigation problem

784
01:17:09,720 --> 01:17:15,160
in unstructured setting there's no lane to detect so you have to really do something more than that

785
01:17:15,160 --> 01:17:23,400
so in some senses in some senses autonomous driving's perception is simplified so I'm hoping to first

786
01:17:23,400 --> 01:17:28,920
see the simplified problem solved before we even like dream of solving the more uh complex and

787
01:17:28,920 --> 01:17:35,160
unstructured problems but of course there's like like other big issues over there like

788
01:17:35,160 --> 01:17:39,640
there's safety risk the speed that's autonomous driving is high that basically means you have very

789
01:17:39,640 --> 01:17:46,120
limited amount of time for making decisions you have to make make decisions like way ahead of time

790
01:17:46,120 --> 01:17:52,760
compared to the normal setting when the speed is like uh low and then sensitivity is high because

791
01:17:52,760 --> 01:17:59,400
there's like a risk of fatal accidents and varying conditions cars go everywhere and like during

792
01:17:59,400 --> 01:18:05,240
the night during the day during the snow foggy and so on so forth and sensors get constantly

793
01:18:05,240 --> 01:18:10,840
interfered by sun and whatnot then you're capped by the price um so there is like that's not to

794
01:18:10,840 --> 01:18:16,360
basically play down to complications but at least let's say purely focusing in detection aspect

795
01:18:16,360 --> 01:18:20,840
there are some things in autonomous driving that are simplified so I continue as a vision researcher

796
01:18:20,840 --> 01:18:26,440
to watch autonomous driving progress and to see where that goes and when it is that you're going

797
01:18:26,440 --> 01:18:33,640
to finally seal it and say that you safely have autonomous driving system and uh so that's a good

798
01:18:33,640 --> 01:18:41,160
proxie proxie proxie for me that I watch another aspect that I think it's outside this niche

799
01:18:41,960 --> 01:18:47,160
like more specific applications like autonomous driving if we like they're yeah they're like other

800
01:18:47,160 --> 01:18:55,080
applications that we could watch there's actually a lot of focus on like a startups and robotics

801
01:18:55,080 --> 01:18:59,080
that of course they need to solve vision to some of them are focused on warehouses some of them

802
01:18:59,080 --> 01:19:07,720
focus on indoor spaces but if you want to think about like more like general settings

803
01:19:07,720 --> 01:19:14,760
about vision commercially speaking I look forward to more like democratization of vision pipeline

804
01:19:14,760 --> 01:19:21,240
so there are APIs right now so it's a very democratic way of of using vision like if you're

805
01:19:21,240 --> 01:19:28,760
somebody that has no expertise in vision you can actually use Microsoft Azure or Google Cloud

806
01:19:28,760 --> 01:19:34,040
to do certain tasks like face detection or some object detection but compared to general

807
01:19:34,840 --> 01:19:42,920
what vision can do that's a very small part of the potential so another good proxy for progress

808
01:19:42,920 --> 01:19:50,280
is that how much these democratic tools uh vision how far they go how much you go beyond like face

809
01:19:50,280 --> 01:19:55,560
detection and and so on and like I said there's a good reason that these APIs have these now because

810
01:19:55,560 --> 01:20:00,200
they are the things that work good then many other things that we have they just don't work

811
01:20:00,200 --> 01:20:05,080
good enough to be at the API level and that's why I'm saying like the more I see

812
01:20:05,080 --> 01:20:11,480
brought to the API level that works reliably and sufficiently well that's a good progress

813
01:20:12,760 --> 01:20:17,080
proxy for our progress that we can see and examples are like 3D again

814
01:20:18,280 --> 01:20:23,480
there's no good API at least from what I know that would recover the underlying 3D structure

815
01:20:23,480 --> 01:20:30,120
from from given images for you there's like are you predicting that for 2020?

816
01:20:33,080 --> 01:20:40,680
Well maybe let's like up mystically yeah I don't know whether we are going to get to the

817
01:20:40,680 --> 01:20:47,320
API level probably not but I think we're the progress in the field points to the direction that

818
01:20:47,320 --> 01:20:51,720
people would have started like thinking about bringing that to the API level hopefully based

819
01:20:51,720 --> 01:21:00,360
on the progress they'll make in in 2020 so yeah we'll I'm also observing these and looking forward to

820
01:21:00,360 --> 01:21:05,320
more like democratization of the tools that we have envisioned because that's really when

821
01:21:05,320 --> 01:21:11,320
as vision researchers we will let the product of a work to be used by people that are not vision

822
01:21:11,320 --> 01:21:18,680
researchers yeah yeah so that's something from the from the commercial perspective from the

823
01:21:18,680 --> 01:21:24,280
technical perspective I think actually the trends that I anticipate would be more or less

824
01:21:24,280 --> 01:21:30,440
in extrapolations of the trends that we be discussed I do believe that vision plus robotics

825
01:21:30,440 --> 01:21:38,040
will continue to go forward as strong the way it will change is that we will we will actually

826
01:21:38,040 --> 01:21:44,360
start working on more specific sorry more general problems like right now let's say in this

827
01:21:44,360 --> 01:21:49,480
example of like habitat challenge and Gibson to discuss things very primarily about navigation

828
01:21:49,480 --> 01:21:53,240
even with the navigation it was about one case of navigation I go to this particular

829
01:21:53,240 --> 01:22:01,000
coordinate but looking at the more general setting the find objects for me or solve the tasks

830
01:22:01,000 --> 01:22:08,440
that or like a sequential game like find object a and then go to an exit like an example it says

831
01:22:08,440 --> 01:22:14,040
the rescue robot we send the robot to a building that is on fire this robot has not been in this

832
01:22:14,040 --> 01:22:20,520
particular building before but you task it with find the victim and go to the closest fire exit

833
01:22:20,520 --> 01:22:28,120
so it's a really useful application just that but we are not there yet and so in terms of vision

834
01:22:28,120 --> 01:22:35,640
plus robotics I expect the proportion of researchers interested in this topic to to expand

835
01:22:36,200 --> 01:22:43,640
and the technical part the problems to become more challenging more realistic and hopefully work

836
01:22:43,640 --> 01:22:48,920
on the more like more general setting not limited settings that we have been focusing on so far

837
01:22:51,160 --> 01:22:59,720
I multitask learning came up in our discussions and I think that's an area that I hope to see

838
01:22:59,720 --> 01:23:07,160
progress in it generally you know there's very little things about vision that is single task

839
01:23:07,160 --> 01:23:13,560
like your job with an image doesn't end the moment to detect objects in it or the moment to detect

840
01:23:13,560 --> 01:23:20,680
the depth of it the moment to detect the vanishing points for any practical use for most practical

841
01:23:20,680 --> 01:23:28,040
uses you usually need multiple of things such task at the same time so the problem is like

842
01:23:28,040 --> 01:23:34,920
essentially multitask to be useful so being able to bring multitask learning to

843
01:23:34,920 --> 01:23:44,600
a more reasonable state where we can solve multiple problems efficiently and reliably and

844
01:23:44,600 --> 01:23:53,160
consistently is an important problem we did see some progress in 2019 but we are far from

845
01:23:53,160 --> 01:23:59,560
reaching that level to the point that we can supply a multitask vision pipeline with one image

846
01:23:59,560 --> 01:24:06,600
and in the uploaded produces a breadth of different abstractions extracted in a way that

847
01:24:06,600 --> 01:24:12,680
that is done independently and sorry that is done efficiently of course you can't have like under

848
01:24:12,680 --> 01:24:18,840
the hood you can have like end different independent pipelines for end different tasks that's inefficient

849
01:24:18,840 --> 01:24:24,280
because there's a lot of redundancy between them so you don't want to do that being able to do

850
01:24:24,280 --> 01:24:31,320
that efficiently do not sacrifice the accuracy over there all these end outputs should have a

851
01:24:31,320 --> 01:24:36,760
reasonable quality and something that I'm personally excited about these days and hopefully

852
01:24:38,200 --> 01:24:43,640
soon we'll release this stuff on it is on consistency so the output of a multitask system should

853
01:24:43,640 --> 01:24:49,160
be consistent your objects and your computer cannot be inconsistent with each other so how we

854
01:24:49,160 --> 01:24:57,320
actually learn do the learning in a way that the outcome is has some guarantees for being consistent

855
01:24:58,120 --> 01:25:02,440
and one thing that also I think we have overlooked for too long especially again in the

856
01:25:02,440 --> 01:25:08,840
conflicts of multitask learning is uncertainty for each of these outputs sure we are providing

857
01:25:08,840 --> 01:25:14,520
some predictions but you can't do much with the prediction especially as a practitioner

858
01:25:14,520 --> 01:25:20,040
if it's not associated with some confidence metric this is a dog in an image but with what

859
01:25:20,040 --> 01:25:25,240
confidence if it's if you're 99% confident and versus you're 50% confident your decision

860
01:25:25,240 --> 01:25:30,360
probably changes if there's a tiger within like two meters of you with 99% confidence

861
01:25:30,360 --> 01:25:35,720
I'm gonna run if it's 1% confidence probably I'm not gonna run I'm gonna do something else

862
01:25:35,720 --> 01:25:44,600
so there's there's a real real value in an extra acting uncertainty from the visual signal

863
01:25:45,880 --> 01:25:51,720
surprisingly that's not actually a big part of the computer vision research right now

864
01:25:51,720 --> 01:25:56,680
and I think that's part of the part of the reason that's happening is that division plus x

865
01:25:57,800 --> 01:26:05,480
movement is recent so the moment you want to provide vision outcome towards some downstream

866
01:26:05,480 --> 01:26:10,040
goal whether that's a roboticist or some and so on so forth you'll realize that they need a

867
01:26:10,040 --> 01:26:13,880
little bit more than whether you're providing them and one of the things that they they definitely

868
01:26:13,880 --> 01:26:20,680
need is uncertainty estimation so you bring this up in the context of multitask is the idea that

869
01:26:21,720 --> 01:26:32,360
the the consistency desire and the uncertainty desire can be formulated as additional tasks or

870
01:26:32,360 --> 01:26:37,320
objectives as part of your training right the reason I brought up in the context of multitask learning

871
01:26:37,320 --> 01:26:44,360
is that multitask learning itself provides an opportunity for a new way of quantifying uncertainty

872
01:26:44,360 --> 01:26:49,640
but uncertainty is still useful and it can be done in a single task setting too and in the machine

873
01:26:49,640 --> 01:26:56,440
learning community there's actually work on it more than vision community that systems that

874
01:26:56,440 --> 01:27:02,680
supply a confidence score besides the prediction that they're making even that's a single task

875
01:27:02,680 --> 01:27:07,640
there's like research being done and that and so on but it has been again proven harder than

876
01:27:07,640 --> 01:27:16,120
anticipated because neural networks are found to be making confident mistakes so because of various

877
01:27:16,120 --> 01:27:24,840
different things that's most of them are actually theory they're papers in just past icml there was

878
01:27:24,840 --> 01:27:30,440
probably more like a track that why this is happening that's basically a question rather than

879
01:27:30,440 --> 01:27:34,760
something that you already know the answer to it but we have made this observation that neural

880
01:27:34,760 --> 01:27:40,680
networks make confident mistakes so that's a problem so basically means that even if you have a

881
01:27:40,680 --> 01:27:49,720
measure of uncertainty chances are that might not be reliable because if you extract that uncertainty

882
01:27:49,720 --> 01:27:54,280
out of a single task system there's no redundancy over there or there's no unzombling

883
01:27:54,280 --> 01:27:59,800
that uncertainty estimation might not be very may accurate too so multitasking there's a way

884
01:27:59,800 --> 01:28:06,120
to solve that because essentially under the hood there are multiple processing pipelines for

885
01:28:06,120 --> 01:28:12,680
different abstractions and the consistency across them is actually a good proxy for how accurate

886
01:28:12,680 --> 01:28:22,440
each of them are and there has been research on it and hopefully I think soon maybe within a few

887
01:28:22,440 --> 01:28:29,000
weeks we will also release new material on that but from what I see the method point is that we

888
01:28:29,000 --> 01:28:35,560
definitely need to to output uncertainty estimates out of the visual processing pipelines that we do

889
01:28:35,560 --> 01:28:41,880
and and more specifically on multitask learning and so on what I see is that there is a there's

890
01:28:41,880 --> 01:28:47,640
a good opportunity to solve that problem at least within the multitask learning framework and

891
01:28:47,640 --> 01:28:53,720
we'll see how 2020 turns out awesome awesome any additional predictions for us I think that's

892
01:28:53,720 --> 01:29:01,960
that that's enough to embarrass myself in the year from now you heard it here first autonomous

893
01:29:01,960 --> 01:29:10,200
driving in 2020 that's what you said right well I didn't for directors but I hope

894
01:29:10,200 --> 01:29:19,400
nice nice at the very least Amir and an autonomous vehicle cab in Arizona yes that's

895
01:29:19,400 --> 01:29:24,920
at least a thing there and I'm looking forward to trying it awesome well Amir thanks so much

896
01:29:24,920 --> 01:29:33,400
for taking the time to share with us your again your take on 2019 and predictions or

897
01:29:33,400 --> 01:29:43,160
anticipations for the the year ahead thanks for having me all right everyone that's our show

898
01:29:43,160 --> 01:29:49,240
for today for more information on today's guest or for links to any of the materials mentioned

899
01:29:49,240 --> 01:29:56,760
check out twimmelai.com slash rewind 19 be sure to leave us a five star rating and a glowing review

900
01:29:56,760 --> 01:30:02,600
after you hit that subscribe button on your favorite podcast catcher thanks so much for listening

901
01:30:02,600 --> 01:30:12,600
and catch you next time

