All right, everyone, I am here with Irene Chen. Irene is a PhD student at MIT. Irene, welcome to the podcast.
Thank you so much. I'm a huge fan of the podcast, so I'm absolutely thrilled to be here today.
Awesome. Awesome. I'm really looking forward to digging into our conversation. As a listener, you know that we are going to start with a little bit about your background. How did you come to work at the intersection of machine learning and healthcare?
It's a great question. So my training is an applied math. I did my undergrad at Harvard and the applied math program there is essentially for people who can't pick just one field.
So technically, my training is applied math with an application in computer science and economics, which just meant that I got to take different classes and a bunch of different fields and get a feel for how quantitative sciences can be applied pretty broadly.
After I graduated, I went to Dropbox and I worked for two years, and I think the combination of sort of the research focus I took in undergrad and then seeing algorithms at scales, seeing how technology develops, seeing how companies make pretty big decisions about when to deploy something and when not to deploy something.
Maybe I realized that I really wanted to study how machine learning, how AI can influence sort of the toughest problems, things we hadn't even figured out yet. So I went back to school. I got, now I'm in my PhD at MIT focusing on healthcare, which is a new part of my training.
But luckily MIT has tremendous amount of classes and I've been able to take classes at Harvard Medical School and really dig into how to bridge the gap between machine learning and these questions of healthcare.
And is the degree program that you're in now still applied math or computer science or is it a what a healthcare degree, whatever that might be.
Oh, so it's, it's an ECS electrical engineering computer science, and then there's a great certificate program through HST. So that's Harvard Science Technologies that allows you to get sort of a like a four class certificate through basically Harvard Medical School where you can take, take in pathology, physiology, and currently I'm doing a remote clinical preceptorship where you get to hear from all these clinicians about so the questions that keep them up at night.
So it's a really good bridge between the computer science, the math, the technical stuff that I love and I'm very familiar with, and then the medical field, which is a new area for me and deciding how to best bridge that gap and make sure that collaborators and anyone we work with is engaged and are really wanting to work with us.
Nice, nice. So what are the questions that keep practitioners up at night?
There are a lot of questions, certainly. I think from the machine learning side, machine learning on working on healthcare data is like everything you know about machine learning, but then you add a question mark to it.
So the very classic machine learning scenario, maybe the one of the first problems that you do is you get a bunch of pictures of cats and dogs and you want to classify them. This picture is either a cat or a dog.
When you think about healthcare, it's not so easy to say, oh, this person has diabetes or doesn't have diabetes and now we want to classify who has diabetes or not.
It turns out, you know, whether or not someone has diabetes, that label can be wrong, right? Maybe that person hasn't been diagnosed with diabetes yet. Maybe that person doesn't like doctors and hasn't had come in it that very much.
Yeah, so all of a sudden the labels are in question. And then also the picture you get, you know, you don't get a clear picture of a cat, you don't get a clear picture of a dog. Instead you get all of the longitudinal visits that person has ever had with one healthcare system, but oh, wait, they moved.
So actually we don't know what happened to them or we have people who have lots of tests, lots of information from them, but we are not sure why there's huge fluctuation. So what's going on? Maybe they were on medications that make them have different readings and we would expect.
So the data we get is really noisy and confounded by a bunch of different things and the labels we have are also combined by different things. And then also, you know, to make everything even worse, healthcare is a very high stakes field.
So if you misclassify a cat or a dog, maybe you insult someone's pet, beloved pet, but if you misclassify someone getting diabetes, that could be really detrimental later on as they can contemplate treatment plans and figure out how to manage a chronic disease.
So the uncertainties in healthcare coupled with the stakes make it really confusing for practitioners to try to figure out.
That's exactly right, Sam. And then I think on a technical level, you know, we think of our data as maybe a huge matrix where each row is a patient and then all the columns are, you know, for one visit all the data we've collected or the next visit what's going on.
And already you can start to imagine some things might be wonky. One of which is that not everyone has all of the data measure, not everyone comes in once a year or once every six months and gets the full slate of blood tests and everything. So all of a sudden, you have very sparse data.
Most people have zeroes for all of these fields and then also maybe you don't have a ton of data for all the talk about big data in healthcare. A lot of times when you dice it down to one chronic disease, there might be only a few thousand people who fit your inclusion criteria.
So all of a sudden the data we're working with is like pretty dicey, pretty sparse and pretty hard to do, you know, classic machine learning on. So a lot of the work that I do is developing machine learning methods that can handle these sort of more longitudinal long term analyses and figure out how we make these best set predictions for stratification.
It sounds like from the examples that you're giving your focus is more on like population health and healthcare delivery from a systems perspective as opposed to, you know, particularly diagnostic approach medical images or something like that.
Maybe let's take a step back and have you talk broadly about your research and, you know, what are some of the problems that you're thinking about.
Yeah, so my research focuses on developing new machine learning methods specifically for healthcare and then through the lens of questions of equity and inclusion.
So a great example of this would be, you know, you're working in hospital and you want to build a really good risk stratification algorithm.
So you mentioned that, you know, right now there's really a lot of work in scoped acute tasks that you could deploy to hospital.
For example, when someone comes into the ICU, if we could predict the patient mortality during the hospital stay, that would be really beneficial to clinicians because they can allocate resources, they can, you know, talk to the patients, they can develop a treatment plan.
The problem is that when you develop a healthcare algorithm like this.
There are questions about fairness and bias that I might arise because the data that we're training on may have sort of systemic health disparities baked in.
So all of a sudden, and this is something I found fairly early in my PhD, you might develop a, you know, a supervised training, a supervised learning algorithm that tries to predict based on the first 48 hours of a patient stay, who's going to live or die in the hospital during the rest of their hospital stay.
What I found is that the algorithm that I had developed was less accurate for some racial groups than for others, and this is not great.
This is problematic for a bunch of different reasons and least of which it makes the engineer, the person who developed the model practitioner, the person who developed algorithm very confused and they don't know why and what's going on.
So a lot of my research looks at how can we think about these machine learning for healthcare algorithms, from the risk revocations, or the very scope of acute tasks, all the way to thinking about like longitudinal chronic disease work and thinking about how can we ask questions about how these algorithms are affecting all populations and how can we design new models that work across the entire patient population.
Not just the people that maybe are overrepresented in the data or have already benefited from the healthcare system, maybe want to focus this on people for whom there have already been helped disparities enacted.
In the case of the model that you described, what was happening that caused your model to have such disparate results based on the ethnicity of the patient.
Yeah, so for about a year, my PhD became the mystery of what's going on with this model.
Certainly I wasn't there sprinkling and bias being like, my turn to make this evil algorithm.
And if you talk to a bunch of different people, a lot of people, especially when they think about questions of fairness, have different hypotheses.
So some people thought, oh, it's because certain racial groups are smaller compared to other racial groups, and therefore there's not enough data.
We need to go out and collect more data for say the Asian population, which had 2% in this data set compared to the white population, which was 70%.
That would explain why the Asian population is having higher errors.
Or other people would say, actually, it's because the data we collect is just noisier for some groups. Some groups might have, say, historical mistrust of the data of the healthcare system.
Maybe the measures we don't collect as many measurements as often, and therefore we can't get any better.
They're sort of already baked in issues that we as algorithm makers aren't to blame.
And therefore we should just sort of say like, actually this is what it is and we can't blame this person.
So as it turns out, this algorithm was a little bit of both of those two. So one of them is that the data set could have been bigger.
The data sets for certain populations, we're not measuring in the same way.
This patient population is too small for some groups, that explains some of it, and there are tools that we produced to be able to estimate what was going on there.
And the other half is that actually there are some groups, and there are some conditions that we don't know as much, or we're not collecting the right information,
we're not able to differentiate for those patients who lives and dies in the hospital based on the information we collect.
And that's sort of a, like, I wish Sam that I had a very neat answer for you, but the truth is that we only right now have a set of tools to be able to cross out hypotheses, suggest new ones, and then go back to the clinical collaborators and say,
like, okay, we're thinking of deploying this tool. Here are all the caveats. Do you think this would still be useful or not? Let's discuss.
You mentioned earlier, I believe you said risk stratification.
Can you elaborate on on that and what you mean?
Is stratification in the sense of, you know, we might stratify a data set to address imbalances and class imbalances in the data set, or is it something, another use of the term?
Risk stratification here applies to the idea that you, if you have some sort of adverse event, say patient mortality, you can stratify patients by their risks.
So you can predict, essentially, we're not talking about essentially a binary classification problem, but you can have the probabilities, for example, and say if one person is 98% likely to die versus 2% likely to die, maybe we should be allocating resources or focusing more attention on the higher risk patient versus the lower risk patient.
Already, you might think, well, are these scores calibrated meeting? Does 98% really mean 98% or are things being a little weird here?
And that's also an important question, especially when we're not just looking at classification, we're not just looking at zeros and ones, we're looking at probabilities as well and being able to see if we're ranking the patients appropriately, and if 98 is correct or maybe it should have been closer to 70.
Right, right. And then even if you've got correct numbers, good numbers, you've got this totally different question of where do you draw the lines, which may or may not be a machine learning problem.
Right. Yeah, and I think that's maybe the thing that I have learned the most in the PhD is that yes, machine learning is fantastic and also terrible and also has all of these complexities. But in fact, it's sort of one piece that fits into this entire clinical care pipeline.
And ultimately, you need to figure out how this risk ratification tool, for example, would be used by clinicians. Is it a score that they sort of just look at on the patient record and sort of assess what's going on there? Is it a like a alarm that goes off when the score goes above something and everyone drops everything and runs over?
Is it something that we're just sort of passively putting in the background and maybe if they want to check, they can look it up. And if they don't, for example, if they're unsure about patients, then they would check the score versus they don't.
Or and then they sort of proceed on their normal clinical delivery anyway. I think understanding where in the health care system, a score like this would be used can better help us understand, you know, what kind of machine learning methods we should be developing in the first place.
So in your focus of your research, is it primarily on developing tools to help either data scientists in this field or clinicians or is it more broadly kind of understanding the, you know, the nature of these disparities and how they're introduced in the system or, you know, something, you know, yet altogether different.
Or some combination, I like to think of the machine learning pipeline from like all the way to the beginning where you're like collect, you know, you figure out what do we want to study in the first place.
And then you're collecting the data and then maybe you're, you know, deciding what kind of prediction task whether the X is in Y is going to be based on the data if you've collected.
And then we have the algorithm development, which we, of course, spent a lot of time on and then we have deployment and then monitoring what's going on with the machine learning algorithm in the clinical context.
And my sort of thesis is that we can think about each step and think about questions about ethics and equity and inclusion at each possible step. And so that's sort of what I've been focusing my research on.
Which is that everything we do, everything we touch and machine learning for healthcare should be thought about how we can make sure we include the entire patient population.
So the example about risk stratification and then let's say auditing an algorithm for bias, that comes at the very end.
You've basically almost got an algorithm going your step away from pushing the button and having it in a hospital.
Uh oh, what's going on there? How could we debug what's gone wrong fairness wise. I have other work that sort of moves up the development moves up the pipeline and saying when you're developing the algorithm, what kinds of considerations can we take into account.
Maybe different people have different access to healthcare and we should sort of build that into the model so that we're not accidentally wrong for people that don't come into the hospital as much.
And therefore we're not effectively penalizing them. And then I have also work sort of at the very very beginning thinking about what problems are even solving with machine learning.
So I recently started a project looking at domestic violence predicting which patients are high risk going back to risk stratification for being victims of domestic violence and thinking about this is a task that we don't really give much attention to clinicians are not really trained to assess this.
And it's a tough problem because we don't even know what the labels are going back to the questions about cats and dogs, you know, who is a victim who is not or survivor as some people like to call them.
And then how do we figure out the right clinical data set to be able to collect for that and how do we bring that through the entire pipeline.
So I would say my I love my research and I love that it can focus on different parts of this pipeline hoping and the end as we build out machine learning for healthcare as we start to get more into hospitals or better understanding how diseases progress or anything, you know, anything in between that we're able to better think about all of the patient population and who we might be forgetting along the way.
I'd love to hear more about this domestic violence project that's kind of at the beginning of the of this life cycle.
What's your ultimate goal there.
We're building a early detection program for so intimate partner violence is a subset of domestic violence, and that's what we're focusing on. So this is violence between present and past intimate partners. And our goal is to be able to build assess evaluate and then eventually deploy a detection algorithm at the clinical level.
So in the emergency department or at the rate, you know, at the radiologist level, if someone comes in multiple times, or if they have a series of markers, for example, you know, a whole fracture.
Exactly. So they have series of markers or biomarkers that are sort of high risk that they would be able to be a flag or some sort of alert that would go off that will allow the clinician or any kind of healthcare practitioner to start a conversation.
Currently, the state is that into a partner violence is pretty widespread and urgent concern that is not. It's not clear what there is to do. A lot of survivors.
They are reluctant to come forward because of stigma about the situation or they don't have the resources or they distrust healthcare professionals are sort of a lot of factors that go into play.
The idea is if that we are able to do early detection, then we can help sort of speed up some of these this process where eventually a healthcare professional could broach the subject, provide resources, or at least monitor what's going on.
This is work that's done in collaboration with some fantastic colleagues at Brickham and Women's Hospital in downtown Boston. And we are sort of we've how we validated and a healthcare algorithm, a machine learning algorithm.
And now we're sort of working on enlarging our data set to even more patients and validating what's going on there before hopefully deploying it at somewhat large scale, at least in a hospital to start with.
And so within the context of applying machine learning to reduce inequality and healthcare is the idea with this particular project that the population itself is underserved and the application of machine learning here is what reduces inequality.
Also, or primarily focused on kind of at the micro level biases within the detection and mitigating issues there.
I would say that nothing is off the table. I like to think of it as kind of glass half full or half empty, you know machine learning is this incredible tool. And so glass half full would say now we have this tool that can allow us to close inequities to detect conditions that haven't been detected in populations that haven't been maybe seen in the healthcare system as as readily or as alertly as they should be.
And so that is tremendous opportunity to mine, you know, all of this big data, these electronic health records, these imaging studies, you know, any kind of wearables, all of this data, we can now feed into machine learning algorithm and both build prediction models that can be deployed and also learn about these conditions and figure out what's going on.
The glass half full version plus have empty version as you may know and from all your interviewing is that oftentimes machine learning is like robot going wrong. It's knife that cuts you while you're trying to cut something else and there are so many things that can happen when you blindly learn and algorithm and potentially deploy it.
So you're able to mitigate some of the things that happen there and better understand what's going on is sort of the glass half empty approach where you're constantly fighting different algorithms and figuring out what's going on.
I would say actually they're both in the same picture so that like just as you can't build any algorithm without seriously considering the clinical landscape that you're developing for and the data that you're collecting and how it's being collected.
So that leads in to figuring out what are the questions that we should most readily attack that have questions of ethics and equity that you should be able to focus in the first place.
So you're completely right that the domestic violence project, for example, is very exciting because it packets the glass half full it's the pitch is this is what machine learning was promised for us.
So as we deploy that we're also seriously concerned about maybe you know different socioeconomic statuses people might manifest differently and we might be omitting different people or if we're training what you know how are we determining a label for intimate partner violence.
Are we saying people who come forward and say I need help I want help if we use those labels, then we actually might be completely missing a whole other population that are not coming forward and are not being seen by health professionals in the same context.
So thinking that through that carefully is as of utmost important is and is a topic for ongoing work right now to be honest.
And you know this may be part of this ongoing work, but when you think about yeah, how do you think through in an example like this case, the you know the implications of the prediction itself right.
You know, if someone comes in for treatment and they get a positive prediction here, you know that they their injury was potentially associated with some kind of active domestic about domestic violence that you know potentially sets off some chain of events that impacts their life.
You know, how do you from a research perspective like how do you unwind all of that is that in the scope of of your work.
It's not I did not imagine it would be in the scope of my work, but I actually think it has to be in the scope of everyone's work, which is that no machine learning exists in a vacuum.
And so you know the Matthew the Matthew part of me would say something like oh well then the loss function should be weighted towards making sure that we have high you know no false positives and only false negatives.
But then also the public health part of me wants to say well false negatives aren't so good either right you want to make sure that you're actually just accurate all the time, why don't we just be perfect all the time.
And I think thinking through these trade-offs figuring out the clinical protocol the when the score comes up and figuring out what happens afterwards and therefore how should we tune this threshold of specificity and sensitivity is a very key question and could be more important even than what kind of algorithm you know is a linear model non linear model is it a you know self attention for rest distillation fancy machine learning model.
In fact you know the very end you don't want to take the you know take the fall the way to the you know one yard line and then completely miss so I think thinking through these questions are incredibly important.
And you're completely right that you know figuring out what what happens after the flag goes you know what happens after the alert goes off or for whom does the alert go off and it's right or it's wrong.
Thinking through all those questions is sort of a perfect merging of the clinical club the clinical domain who has been thinking about these questions for a long time and the machine learning domain where we have the tools the computational tools to be able to parse out is this air if we have errors are they coming from lack of data aka variance or are they coming from noisy measurements aka noise or they coming from the model that we're using aka bias and so that is a lot of my focus my research.
But we can never ever ever forget that it comes from the other side of what happens after the model comes out there's been some interest so it's not my line of work but there's been some interesting work about how doctors interact with machine learning and the results are.
Interesting in that you know oftentimes the more experienced a doctor is in their own professional career the less likely they are to be swayed by the AI algorithm correctly or incorrectly maybe they're just disregarding they do you know they already have a high prior in their own medical knowledge and therefore don't need the algorithm the same way.
Whereas younger thinking of a study that specifically looks at dermatologists younger dermatologists that are earlier in their career they might be more swayed by machine learning algorithm in this study they made them the machine learning algorithm be correct or incorrect and then the younger dermatologist would be easily swayed either way because they don't have a strong clinical knowledge to expertise built up just yet.
And so I think I think thinking through all of that is really important as well as we as we know hopefully ultimately is the goal to be able to work alongside doctors in a more concrete way.
Are there any techniques that you've used or recommend when you're kind of dealing working across this interface between the data scientists the researcher and the machine learning researcher in particular and the clinician to for example tune the sensitivity of a particular algorithm.
You know what language are you speaking you know they are you speaking you know false positive rates and the like to them are you showing them examples what have you found to work given the different languages that these two groups are most comfortable in.
But I would say the best thing you can do is have as tight of a feedback loop between clinicians and machine learning practitioners as possible.
So my advisor is David saw an attack at MIT and I think he's done a fantastic job in the lab of building clinicians who actually sit in the lab not currently because we're all remote but before there was a debt you know there were desks and clinicians which is sit there and they would do their work and we would do our work.
So something happened oh wait this blood test is giving me weird things or no does this model look reasonably correct is this seem like something that's working we could just swivel over and talk to the clinicians who are right there.
And I think that's you know such an underrated part of clinical collaborations which is that you need you need to be able to talk to each other you need to be able to you know have small back and forth about you know for on the same direction.
And I would say that the mental model a lot of people have is that you know you build your machine learning model and you throw it over the wall and a clinician says like yes or no and then throws it back over the wall.
And anything we can do to you know lower that wall as low as possible so that we're just kind of tossing a bean bag back and forth would be you know advantageous.
Another thing I would say is that better understanding from the machine learning side of what works and doesn't work for the clinician so you know do.
When we talk about this patient mortality model you know one of the things that surprised me was talking to the clinician and he said you know I know I know which patients are going to die or we're aren't going to die you know this model isn't telling me anything I don't already know.
And being able to think oh wait our baseline is not a logistic regression and seeing whether or not linear model fit on these co these features will you know predict this outcome our baseline is does the clinician know like is this even helpful is the problem we're solving even useful and having thinking that through is incredibly important and that to me is a way of bridging sort of the divide of interdisciplinary work.
You you worked on a paper focused on probabilistic approaches to machine learning for health care can you talk a little bit about that work.
Yeah so one of the fun things that I get to do in addition to sort of going super deep into one topic and trying to push the frontier of knowledge a little bit one of the things that I find really important is taking a step back and saying actually what is the field doing right now what are the things that we can take into account and through that last summer I wrote this review of looking at probabilistic machine learning for health care.
One of the things that comes up often is how do we build in these levels of say uncertainty estimates or how do we express how a data is distributed and a lot of this comes back to basic ideas of probabilities.
And so thinking about how we can express you know it's not just zero one it's everything in between into concepts like fairness if we're making a prediction or if we're thinking about how a data is distributed being able to have the expressivity expression expression degree the degree of expression for the using probabilities is incredibly important.
Probably the most something that resonates a lot with clinches is having uncertainty estimates you know the first time I used my iPhone and I asked Siri a question and she said you know I don't know actually gave me a lot of confidence in Siri for the first time when I used her and she you know she was able to express hey I thought about this or I searched my database and actually rather than give you a bad answer or make a guess I'm just going to tell you I don't know.
And I think having expressing that mathematically is probably you know on the back and if I had to imagine it had to be something along the lines of if the probability of you know why given X is above some level then we say it and if it's below some level or if they're equal across all of the classes or something like that then we say we don't know.
And similarly for clinical decision making it shouldn't just be zero ones this person has this disease this person doesn't have a disease you could also give an uncertainty estimate and thinking through how we can use probabilistic machine learning in that respect.
But I think maybe the larger point that I want to emphasize is that I really enjoyed writing that review of probabilistic machine learning in health care I actually have another review article about ethical machine learning in health care and and I've written a few commentaries about different parts of the field and I think that that's something that I encourage you know all academics to sort of take us to back and say.
Thinking super in depth about one topic all day long for several weeks slash months slash years in a row is an incredibly rewarding process and is why a lot of people sign up for the PhD but it is also our duty as you know people who are so privileged to be able to think about these things all day to be able to step back and synthesize it and be able to share that back with people who maybe aren't so nearly focused all day in the one area as well.
Yeah yeah I wonder if to maybe wrap us up you have some key takeaways for folks that are either interested in this area but don't you know aren't currently working in you know health care and and machine learning or are you know but want some pointers for thinking about kind of ethics and inclusion.
And you know what are the kind of the top line things that you've learned in your journey that you think folks need to hear about.
I would say two main pieces of advice one of them is that the field is way more accessible than I ever thought it would be in that there's a lot of open access large medical so electronic health record data sets out there.
The largest one is called mimic I believe they just released mimic four so the fourth version and this is data that's collected from Beth is really in this medical center to the hospital in downtown Boston and last I checked it's like tens of thousands of adults and also I think like up to almost 10,000 children.
First it was in the intensive care units just sort of their entire stay everything that happened the notes which is incredible the clinical notes of the doctors actually wrote all the lab tests and then I think they recently also added the emergency department as well so everyone who went through the emergency department by this really gives medical center and what they've done which is tremendous is they've allowed this data set to be accessible to researchers.
Who are can prove that their researchers for for with some sort of credentialing but is pretty light and effectively you know people use it all the time people a lot of classes teach out of it.
Students are able to download the data set and make that model you know and build a prediction model about who is going to live and die in the ICU based off of the first 48 hours of clinical notes.
This is like a very real task that now finish that students in all kinds of introductory classes can now take so I would say there's a lot of open medical data sets out there.
If you're interested hop right in get I believe Andrew beam has a webpage actually so type in Andrew beam open medical data sets and he just lists dozens of data set if you're interested in mammography if you're interested in colon cancer if you're interested in PCOS there's sort of all kinds of data sets.
Available so I would say first piece of advice jump right in get your hands dirty see what happens see if you like it see if you like the data cleanings if that's kind of annoying to you see how you enjoy it.
The second piece of advice I would say is that this field is incredibly collaborative I've just waxed poetically about my clinical collaborators and you know how much I miss being able to swivel over and and annoy them but I would I think I when I started my PhD I had this notion.
Especially at a very you know prestigious place like MIT where you feel like everyone is stressed out all the time I have this notion that they're sort of a genius who sits alone in the room and does you know just spits out papers alone just sort of just manages to to create you know
really just by themselves and my experience in the PhD has been anything from that even you know the lone genius is actually reading papers by other people and is able to sort of build on talk of them.
And in the PhD you can actually you know really tighten that loop and instead of waiting to read someone's paper you can talk to people and say hey I have this cool idea what do you think and they can say I have this cool idea and you're able to collaborate through that.
So something that I've really enjoyed is both the collaborations with people in my lab my lab is awesome clinical machine learning group at MIT.
But also people at conferences people at clinicians random people who read my papers and email me Twitter direct messages I think that being able to tap into a whole ecosystem of very excited people that span machine learning people.
And I think that's the good part is being able to come in to the room figure out where who knows what they're talking about learn from them and then be able to shape your own ideas.
So you know I am only still a PhD student but I'm very excited that I get to be part of this community in this community really means like machine learning people health care people anyone else who is vaguely interested in the implications of what's going on.
Now we're expanding to like HCI people human commuter interaction people and so thinking about all of those communities coming together is what gets me up in the morning honestly and it keeps my powers my research as I you know race towards finishing it.
Are you close.
The hope is that next year I'll graduate so I'll try back with you it's a year and we'll see see where I am but that's the hope but I've had such an amazing comment at MIT and I you know I when I was in 2016 when I started the fairness field was really not a thing machine learning and health was like this tiny workshop at nirups the main machine learning camp conference.
And now the machine learning workshop is like the biggest workshop at nirups fairness is has its own set of conferences you know two three four conferences and I can't imagine what's going to be in another five 10 years like I'm so excited.
That's awesome I mean thanks so much for sharing a bit about what you're up to.
It was completely my pleasure Sam thank you so much for having me on.
Thank you.
Thank you.
