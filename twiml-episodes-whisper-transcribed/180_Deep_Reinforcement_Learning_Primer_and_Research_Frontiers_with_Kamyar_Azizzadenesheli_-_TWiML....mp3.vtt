WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.600
I'm your host Sam Charrington.

00:31.600 --> 00:36.760
In today's episode we're joined by Kamyar Azizad Anisheli, PhD student at the University

00:36.760 --> 00:42.440
of California Irvine and visiting researcher at Caltech where he works with Anima Anankamar

00:42.440 --> 00:46.400
who you might remember from Twimble Talk 142.

00:46.400 --> 00:51.200
We begin with a reinforcement learning primer of sorts, in which we review the core elements

00:51.200 --> 00:56.680
of RL, along with quite a few examples to help newcomers get up to speed.

00:56.680 --> 01:02.560
We then discuss a pair of Kamyar's recent RL-related papers, efficient exploration through Bayesian

01:02.560 --> 01:11.640
deep-cune networks, and sample efficient deep RL with generative adversarial tree search.

01:11.640 --> 01:16.000
In addition to discussing Kamyar's work, we also chat a bit about the general landscape

01:16.000 --> 01:19.120
of reinforcement learning research today.

01:19.120 --> 01:24.040
So whether you're new to the field or want to dive into cutting-edge reinforcement learning

01:24.040 --> 01:27.360
research with us, this podcast is here for you.

01:27.360 --> 01:31.680
If you'd like to jump ahead to the research discussion, the primer portion of this show

01:31.680 --> 01:34.080
is about 30 minutes long.

01:34.080 --> 01:39.120
We'll have a more specific timestamp noted in the show's description, so check your podcast

01:39.120 --> 01:45.680
app or the show notes at twimlai.com slash talk slash 177 for that.

01:45.680 --> 01:47.680
A quick note before we get into the show.

01:47.680 --> 01:52.000
One day is Labor Day here in the States, and we won't be publishing a show then.

01:52.000 --> 01:56.720
But this is an extra long episode, which will tide you over until we do publish the next

01:56.720 --> 01:59.760
show, which will be Thursday, September 6th.

01:59.760 --> 02:03.000
Alright, onto the show.

02:03.000 --> 02:10.200
Alright everyone, I am on the line with Kamyar Aziza Denisheli.

02:10.200 --> 02:16.280
Kamyar is a PhD student at the University of California Irvine, as well as a visiting

02:16.280 --> 02:22.080
student researcher at Caltech, where he works with Anima Anankumar, who was a guest of

02:22.080 --> 02:24.360
ours back in May of this year.

02:24.360 --> 02:27.360
Kamyar, welcome to this week in machine learning and AI.

02:27.360 --> 02:28.680
Yeah, thank you, Sam.

02:28.680 --> 02:31.360
Thank you for the introduction.

02:31.360 --> 02:36.000
Why don't you give us a sense of your research interests and some of the work you're doing

02:36.000 --> 02:39.320
at Caltech and Irvine?

02:39.320 --> 02:48.160
Right, my research interests is mainly in the area of a specific area of machine learning,

02:48.160 --> 02:51.680
which is called reinforcement learning.

02:51.680 --> 02:58.760
This is my main focus on my main research interests, but at the same time, I do a lot of

02:58.760 --> 03:03.280
other stuff in the field like TensorFlow methods.

03:03.280 --> 03:10.600
I do optimization, I do generative models, study of generative models.

03:10.600 --> 03:20.480
I do a study of safety and furnace in machine learning, because we build up many theory

03:20.480 --> 03:28.880
in machine learning and we build up many practical, like, evolutionary methods, revolutionary

03:28.880 --> 03:29.880
methods.

03:29.880 --> 03:35.920
The question is, can we use them in real world and the question and answer is, hey, you

03:35.920 --> 03:41.440
need to make sure that your algorithm is robots, your algorithm is safe, it's fair.

03:41.440 --> 03:46.720
There are a lot of questions you can ask when you take your machine learning algorithm

03:46.720 --> 03:48.920
and deployed in real world.

03:48.920 --> 03:54.280
Those are not a part of machine learning field that I'm interested in.

03:54.280 --> 04:00.320
We've talked about reinforcement learning on this podcast a number of times, both from

04:00.320 --> 04:12.080
a theoretical perspective, as well as applications like game playing, AlphaGo, things like that.

04:12.080 --> 04:19.600
For this conversation, I thought we would dig into a couple of recent papers of yours

04:19.600 --> 04:24.840
looking at deep reinforcement learning, but also spend some time up front to refresh

04:24.840 --> 04:30.920
ourselves on some of the foundational research in this area.

04:30.920 --> 04:39.160
Why don't we get started by having you walk us through some of the core elements of

04:39.160 --> 04:43.160
reinforcement learning like deep Q networks, for example.

04:43.160 --> 04:45.440
Great, that's an amazing thing.

04:45.440 --> 04:50.680
I really like to, whenever I want to talk about it and explain reinforcement learning,

04:50.680 --> 04:58.040
I get super excited because it's an amazing framework, an amazing setting that almost

04:58.040 --> 05:07.200
it has a lot of intuition from human behavior and also it has super nice theoretical analysis.

05:07.200 --> 05:14.440
The modeling is crazy, great, and I really love it and I really like to explain to others.

05:14.440 --> 05:20.400
It's like, let's assume that I can give you one example, let's assume that you have

05:20.400 --> 05:27.640
a vacuum cleaner in your house or in your apartment or your place and you just leave it

05:27.640 --> 05:34.680
alone and this vacuum cleaner should know how to clean your place.

05:34.680 --> 05:41.400
So what it does is like, vacuum cleaner goes around if it finds something and if it's

05:41.400 --> 05:45.800
like cleaning that place, when you come back home, you tell the vacuum cleaner, hey,

05:45.800 --> 05:46.800
you did a good job.

05:46.800 --> 05:50.760
And if you, the other day you should come back home and like, you see the vacuum cleaner,

05:50.760 --> 05:56.360
you didn't do anything, you tell the vacuum cleaner, hey, you did not do a good job.

05:56.360 --> 06:00.880
It's like, you punish it, I'm not like, you do not punish it, but it's like, this is

06:00.880 --> 06:01.880
the term we use.

06:01.880 --> 06:09.800
So you give some sort of reinforced feedback to the robot, the agent is like going around

06:09.800 --> 06:15.920
the environment or let's say, it even makes it easier in which might make more sense

06:15.920 --> 06:24.120
is like, a baby, a newborn baby, if the newborn baby is getting closer to like, to fire,

06:24.120 --> 06:30.880
the baby feels like some harm and also parents like tell the baby that, hey, do not do it

06:30.880 --> 06:36.080
or like, if the baby does something good, parents, they give the baby like a reward which

06:36.080 --> 06:40.480
is like, let's say, candy, hopefully not candy, but something like, rewarding to the baby

06:40.480 --> 06:47.160
and baby learns that the thing that he or she did was good things over time, the baby

06:47.160 --> 06:51.440
interacts with the environment, which is like the whole world and parents.

06:51.440 --> 06:56.960
And based on the decision the baby makes, like going toward the fire or like getting

06:56.960 --> 07:02.680
higher grade in the, in school, or I don't know, learning how to like, when, let's say

07:02.680 --> 07:08.720
my father of a baby and I'm teaching my baby like, how to ride a bike, if my baby does

07:08.720 --> 07:14.480
a good job, I give a lot of rewards for my baby is like, my kid is like, has this incentive

07:14.480 --> 07:16.480
to learn this task fast.

07:16.480 --> 07:22.520
So it's like, the baby here or the kid here is like the oral agent, it's interacting

07:22.520 --> 07:28.240
with the environment and based on this signal, this feedback system learns how to do

07:28.240 --> 07:32.720
a best job and find the optimal behavior.

07:32.720 --> 07:38.000
So it's a general framework why I really like reinforcement learning because it's like

07:38.000 --> 07:42.960
almost all the time we have reinforcement in problems because you're learning, human

07:42.960 --> 07:47.760
learns, human runs like reinforcement learning somehow is not a, it's not a exact statement

07:47.760 --> 07:50.360
but somehow you can think of it.

07:50.360 --> 07:55.520
So it's reinforcement is actually, or we, in short, we call it RL, it's actually really

07:55.520 --> 08:02.440
interesting framework that has a lot of root in psychology, a lot of root in neuroscience

08:02.440 --> 08:07.240
and also theoretically we found it from graph theory, which was quite interesting.

08:07.240 --> 08:13.000
So from theory, we analyzed many things and some point we realized that oops, is actually

08:13.000 --> 08:18.680
the way human behaves, there was like the nice intersection between like psychology

08:18.680 --> 08:22.760
and neuroscience and like machine learning, which was quite interesting.

08:22.760 --> 08:29.520
So now, this is a motivation why reinforcement is important and why we work on reinforcement

08:29.520 --> 08:35.120
learning because if you can understand the way human learns and the way mathematically

08:35.120 --> 08:41.280
we can make systems to learn, then we can design a robot which can clean a house or

08:41.280 --> 08:48.800
which can like, I can find, I can build a robot arm which helps me to like build something

08:48.800 --> 08:55.480
or like get grass or something, or if I do not have legs, it can help me to walk or

08:55.480 --> 09:02.520
I can come up with the robot, this can help my doctor and my robot is given a patient

09:02.520 --> 09:08.160
comes to my like a clinic, I ask doctor, hey, what is the best prescription you can give

09:08.160 --> 09:14.120
to this patient and also I ask the robot, hey, what do you think?

09:14.120 --> 09:20.400
And the robot has seen many other like trials over the world, so the doctor cannot see

09:20.400 --> 09:26.440
all the possible experiences that other doctors they have seen, but if I can have a robot

09:26.440 --> 09:31.640
which interacts with all the doctors around the world and I give that robot to my doctor

09:31.640 --> 09:37.640
and my doctor can see what my robot thinks and what my doctor is on his own or her own

09:37.640 --> 09:42.640
is thinking about that new patient and then combine this information and give the better

09:42.640 --> 09:46.080
prescription to the patient and patient gets better over time.

09:46.080 --> 09:51.400
So there are a lot of interesting like application and like necessary and interesting application

09:51.400 --> 09:57.680
of reinforcement learning in like, in real world, but the way the model is, there are

09:57.680 --> 10:04.880
many ways to model and the setting, I just want to talk about one of them because mostly

10:04.880 --> 10:11.160
using deep reinforcement learning, let's assume I am playing a game, okay, when I play

10:11.160 --> 10:17.000
a game, let's say, I don't know, you're familiar with, let's say GTA, okay, and then I've

10:17.000 --> 10:18.000
taught her.

10:18.000 --> 10:23.040
Yeah, granted, I thought her or yeah, this is one example that I can imagine many people

10:23.040 --> 10:28.520
have seen it because either they were like punishing their kids for playing this game

10:28.520 --> 10:36.680
like for 24 hours a day or the kids they played themselves is like, you have this agent

10:36.680 --> 10:43.000
going around the world, its own world and like, need to make a decision to optimize its

10:43.000 --> 10:45.280
final goal, okay.

10:45.280 --> 10:51.600
So the decision the agent makes at each point is like walking to left or right or like going

10:51.600 --> 10:57.240
to the barber shop, this or going to gym, this kind of stuff, these decisions at the time

10:57.240 --> 11:02.400
gives it like some reward, each of these decisions has like long term effect.

11:02.400 --> 11:12.400
If my guy in Grand Theft Auto goes to like gym, it gains more power, can run faster, okay.

11:12.400 --> 11:17.680
So this running faster doesn't have meaning at the time, but the thing is this running

11:17.680 --> 11:23.960
faster feature is useful for the agent for later in like few days later when the agent

11:23.960 --> 11:26.920
is going to run away from the cops, something like that.

11:26.920 --> 11:31.960
So the action you're making, the decision you're making now has long term effect.

11:31.960 --> 11:38.280
That effect of this action is like appeared like later, let's say if I am like sitting

11:38.280 --> 11:45.680
in my office, I'm going to grab a coffee, if I grab my wallet now, it doesn't have any

11:45.680 --> 11:49.640
meaning until I get to the coffee shop and I want to pay, okay.

11:49.640 --> 11:53.720
The action I made at the very beginning has a long term effect or if like I go, I want

11:53.720 --> 12:00.320
to invest in a bank, the bank tells me like, hey, if you put money now, you're going

12:00.320 --> 12:02.400
to get returned like over a year.

12:02.400 --> 12:05.680
So the action I'm going to make has like a reward and this reward is like accumulating

12:05.680 --> 12:08.680
over the whole future.

12:08.680 --> 12:15.560
So it's like this reinforcement has this temporal effect, like if I make some decision now,

12:15.560 --> 12:20.240
I see the effect of this decision like way later in the future, which is like makes the

12:20.240 --> 12:24.520
problem hard and also makes the problem interesting because the real world works like this.

12:24.520 --> 12:30.480
If I break my arm now, I might have serious problem like the whole life, my whole there is

12:30.480 --> 12:31.480
my life.

12:31.480 --> 12:37.040
It's not just I see something claim at that time, I see same thing like that effect for

12:37.040 --> 12:38.040
long.

12:38.040 --> 12:47.800
And so one of the big challenges and reinforcement learning is the idea that you've got this huge

12:47.800 --> 12:54.400
in many cases environment, like if we're thinking about this whole, the analogy to humans,

12:54.400 --> 12:58.120
we've got the entire world of things that you can do.

12:58.120 --> 13:05.800
And so how do we explore, how do we decide how we explore all of these possible actions

13:05.800 --> 13:12.880
and states that we could end in end up that that's actually amazing question and the challenge

13:12.880 --> 13:19.480
for reinforcement learning that we are trying to deal with like for many years is like, so

13:19.480 --> 13:26.600
now I give you the world and you need to explore this world, you don't know what each action

13:26.600 --> 13:27.600
is doing.

13:27.600 --> 13:33.440
So, you don't know if you follow some sequence of actions where you're going to end up.

13:33.440 --> 13:37.360
That's exactly the interesting question that you need to explore.

13:37.360 --> 13:41.600
Let's say you go to coffee shop and I'm bringing this example of coffee shop, I don't

13:41.600 --> 13:48.680
know, it makes it more interesting, you go to coffee shop and there is like latte, you

13:48.680 --> 13:53.800
have like let's say five options, there is latte, there is Americano, there is like

13:53.800 --> 13:58.720
cappuccino and brew coffee and also let's assume that in your local coffee shop they also

13:58.720 --> 14:03.960
sell vodka and you go to like coffee shop in eight in the morning, let's assume that

14:03.960 --> 14:08.520
you are not Russian or like Polish and you go eight in the morning to the coffee shop

14:08.520 --> 14:14.720
and you want to get coffee, you for sure not going to order like vodka, right?

14:14.720 --> 14:20.040
But you don't know that you like latte the most or you like like cappuccino the most.

14:20.040 --> 14:25.400
What you do, you need to know which one you think is the best one now and like you drink

14:25.400 --> 14:29.720
let's say you choose latte but tomorrow when you go you are like, I don't know what is

14:29.720 --> 14:34.280
the taste of like cappuccino, let's try cappuccino and for the first time you try the cappuccino

14:34.280 --> 14:35.280
for first time.

14:35.280 --> 14:41.000
Third time you go you order maybe brew coffee and the fourth time you go you are like, okay,

14:41.000 --> 14:47.040
I think latte was good so I'm going to go with latte today, so you get latte and the

14:47.040 --> 14:53.920
day after you say, well I'm not pretty sure how how like cappuccino tastes compared

14:53.920 --> 14:59.200
to latte, so next time you try like cappuccino, so over time you do this type of expression

14:59.200 --> 15:06.800
you are trying different things but you do not like try vodka, so you try some actions

15:06.800 --> 15:14.880
that they make most sense, right, so this is the way we want the RL agent or robot to

15:14.880 --> 15:20.920
do exploration, we don't want to have a RL agent to explore everything uniformly which

15:20.920 --> 15:27.680
is not possible, I don't want to have a GCA organic auto agent to just go left, left

15:27.680 --> 15:33.080
right, like without any reasoning of like what it does, I want to have the agent which

15:33.080 --> 15:38.760
does exploration, in such a way that it maximizes the amounts of information it gains from

15:38.760 --> 15:45.560
the environment while trying to, I mean by information I mean it can build better understanding

15:45.560 --> 15:52.520
of the world, while do not forgetting that the agent is there to collect like more like

15:52.520 --> 15:59.840
rewards, it's like those money in grant us also getting, so my agent wants to, my GCA agent

15:59.840 --> 16:07.400
wants to go around and understand how the world works, while it wants to maximize the understanding

16:07.400 --> 16:13.800
of the world or minimize the uncertainty around the world while trying to maximize the

16:13.800 --> 16:21.520
reward of the game, maximize the score of the game or like get the game to be done, if

16:21.520 --> 16:27.160
you have a kid playing like GTA, the kid doesn't explore the whole year and then start

16:27.160 --> 16:33.080
playing games, the agent while playing games like try to learn and build a model and see

16:33.080 --> 16:38.680
who is friend, what is good, going to gym is good, at the very beginning the kid might

16:38.680 --> 16:42.840
not know that going to gym is good, maybe if you go there they kill you, so something

16:42.840 --> 16:48.280
that, so there are a lot of things, characteristics of the environment that you need to learn,

16:48.280 --> 16:54.280
you need to explore, but you need to explore it carefully, so this way you do exploration

16:54.280 --> 16:59.000
is like actually the key factor in reinforcement learning that most of my works are based on

16:59.000 --> 17:06.200
like how do you do exploration efficiently, and so one of the simple concepts that comes

17:06.200 --> 17:11.880
up in reinforcement learning is this idea of explore, exploit and you've talked a little

17:11.880 --> 17:19.640
bit about that trade off via examples and then one of the algorithms that encodes that

17:19.640 --> 17:24.600
is this idea of epsilon greedy, what is that?

17:24.600 --> 17:30.360
epsilon greedy is actually super powerfully interesting like algorithm and it's super

17:30.360 --> 17:35.720
simple as well, what it does is that again I'm going to talk about coffee shop, you're

17:35.720 --> 17:41.000
at the coffee shop, you have these five options like I told you, you have lots of cappuccino,

17:41.000 --> 17:47.240
americano and like blue coffee and vodka, okay, so now let's assume that so far based on

17:47.240 --> 17:52.040
the, let's say you've seen that you have been in the coffee shop for many times and you know

17:52.040 --> 17:58.520
that you feel that you are a fan of latte demos, you like them all latte demos, so your greedy

17:58.520 --> 18:05.560
decision, which is the decision which maximizes your satisfaction is choosing latte, okay?

18:06.200 --> 18:11.960
So epsilon greedy exploration and exploitation strategy says hey, with probability like,

18:11.960 --> 18:19.160
let's assume epsilon is like 0.1, it says with probability 1 minus 0.1 which is like 1 minus epsilon,

18:19.160 --> 18:25.080
I'm going to choose the most greedy decision which maximizes my satisfaction which is latte,

18:25.080 --> 18:31.080
so with probability 90%, I'm going to choose latte and also I say hey, I'm also not

18:31.080 --> 18:38.600
pretty sure about other other decisions, I'm not sure, very sure about like cappuccino,

18:38.600 --> 18:44.520
so what it does with probability 1 minus epsilon which is like 10%, the agent randomizes

18:44.520 --> 18:49.720
over all these five actions, I think it says like with probability, but with probability 10%,

18:49.720 --> 18:53.720
it's randomizes over all the actions means that sometimes it's like choosing cappuccino,

18:53.720 --> 18:59.560
sometimes choosing americano and sometimes choosing latte when it does exploration and also

19:00.200 --> 19:05.880
it sometimes chooses vodka, so it's like the probability that you choose vodka is exactly equal

19:05.880 --> 19:13.000
to the probability that you choose brute coffee or like cappuccino, which is actually the part

19:13.000 --> 19:19.960
that epsilon greedy sales because you know that vodka is not good, why you choose it again and again,

19:19.960 --> 19:27.240
so this is one part that in one of my papers we address and we resolve this issue which is quite

19:27.240 --> 19:35.320
interesting, so epsilon greedy is like the powerful algorithm, it does choose like the best action

19:35.320 --> 19:40.040
with high probability, the best action I mean the best action that agent so far knows is the best,

19:40.040 --> 19:45.800
might not be, might not be the best, and with probability 1 minus epsilon, sorry with probability

19:45.800 --> 19:51.960
epsilon is gonna just uniformly choose other action, even though the agent might know that some

19:51.960 --> 19:58.200
actions are really bad, but it does choose them, so this is the epsilon greedy and it's been

19:58.200 --> 20:05.800
used in like area of deep, it was a prominent like algorithm for making tradeoff between

20:06.520 --> 20:13.080
exploration and exploitation in area of deep reinforcement learning, and one of the

20:13.080 --> 20:20.360
famous algorithm which uses this is deep q network, so I can just briefly tell like what deep

20:20.360 --> 20:27.800
q network does is, so let's go back again to the grandest auto game, deep q networks are one of

20:27.800 --> 20:36.680
the algorithms for deep reinforcement learning that really popularized the space and

20:37.560 --> 20:44.280
launched a lot of the efforts to solve video games and things like that, it was created at deep

20:44.280 --> 20:50.920
mind, is that right? Yeah, deep q network was created at deep mind and it was like one of the

20:50.920 --> 20:58.520
main reason why like there are many researchers are working on deep reinforcement learning because

20:58.520 --> 21:06.760
this first paper made it somehow possible to go beyond like a small grid work, small games

21:06.760 --> 21:12.680
that theoreticians we were like working on the small work and like this paper was the first paper

21:12.680 --> 21:19.080
that we were able to apply reinforcement learning method on the games like Atari's and like the

21:19.080 --> 21:24.200
games, the video games, the games that we were not even thinking that possible to solve

21:24.200 --> 21:30.200
using deep reinforcement learning algorithm at the time, but this paper was like a kind of

21:30.200 --> 21:34.840
revolutionary paper that brought a lot of attention to the field of reinforcement learning and

21:34.840 --> 21:41.560
people start working on this field, a lot of practitioners and like scientists they start

21:41.560 --> 21:48.840
working on these games and on these algorithms because this third paper made it somehow, it's not

21:48.840 --> 21:54.840
I'm not claiming that this was the it was like the huge gap between the previous works and this

21:54.840 --> 22:01.560
work, there were like many works before like deep q networks they were able to do many things on

22:01.560 --> 22:07.640
Atari games, but this one was the simplest one which had like simple idea and before it

22:07.640 --> 22:13.800
neural networks in order to solve Atari games, so it was quite interesting. And so what's the

22:13.800 --> 22:20.360
relationship between epsilon greedy and deep q networks? So the very deep q network works is like

22:20.360 --> 22:27.880
let's get back to the grand test auto, the agent walks around the city and for each action it

22:27.880 --> 22:33.960
knows that if the agent choose that let's say action going forward or shooting that person,

22:33.960 --> 22:41.080
it has the follow up return, like if it does something it might win the game or like it might get

22:41.080 --> 22:49.720
some reward or some money, some score, so each action has like like upcoming and like forwarding

22:49.720 --> 22:55.880
reward, so it's like each action has value, how good is that action? If I kill that person,

22:55.880 --> 23:03.400
sorry for my language, but if I go to the gym, how much value it has for me, how much I get

23:03.400 --> 23:12.600
full fit if I do this action now, so what deep q network does for when it plays game is like

23:12.600 --> 23:19.320
given a state of the game, even given the image of the game, which is like the frame of the game,

23:19.320 --> 23:26.120
it decides how good is each action, it's like compute the value of each action, okay?

23:26.120 --> 23:31.560
For the time if I play sequence, which is like a submarine is like in the sea and it's

23:31.560 --> 23:36.280
getting out of oxygen, it needs to go on the top of the sea and like get some oxygen,

23:36.280 --> 23:43.000
if I see I'm running out of oxygen, I know that going up has the most value for me,

23:43.000 --> 23:51.000
it's like I'm staying around the bottom of the sea has lowest value, so what deep q network does,

23:51.800 --> 23:58.760
deep q network finds their value associated with each action, okay? The way it constructs this

23:58.760 --> 24:04.120
value is like the agent needs to interact with the environment, the way the agent interacts with

24:04.120 --> 24:10.600
the environment collect samples and explore the environment is excellent, so what deep q network does

24:11.560 --> 24:21.000
builds the model, which somehow gives the value of each decision, and with probability like

24:21.880 --> 24:28.200
one minus epsilon, it goes with the best decision, the agent so far thing is the best,

24:28.200 --> 24:34.360
and with probability epsilon, it randomizes over the all the action, okay? So if I run deep q

24:34.360 --> 24:41.640
network on myself, when I go to the coffee shop, it probably one minus epsilon, I'm gonna choose

24:41.640 --> 24:46.760
latte, and with probability epsilon, I'm gonna randomize over the all the action, okay? So this is

24:46.760 --> 24:53.080
like the way deep q network works, it learns how good is each action, it learns the value associated

24:53.080 --> 24:59.720
with each action, how good is making the like action up at the current time step, how good is

24:59.720 --> 25:05.240
this action? So it learns this function, and when it learns this, while the way it learns this

25:05.240 --> 25:11.400
function is like collect samples to learn this function better and better. And so we can think of

25:11.400 --> 25:20.600
deep q networks as essentially an algorithm for accounting for the various values of these actions

25:20.600 --> 25:28.280
over a series of steps, is that fair? It's fair, it's fair that it's like counting the

25:28.280 --> 25:36.120
amount of reward, it's gonna receive in the future, and somehow in stuff memorizing all of them,

25:36.120 --> 25:43.960
it learns a function which approximates this count. And so in the best case, there are many

25:43.960 --> 25:51.720
theoretical analysis, we show that it doesn't do it exactly, but for this conversation, we can

25:51.720 --> 25:59.400
assume that it's done. Does deep q networks specify a particular type of neural network architecture,

25:59.400 --> 26:07.560
or can it be implemented with multiple different types of networks? Oh, deep q network is actually

26:07.560 --> 26:15.960
an algorithm, and it can be used for, and you can anyone can design his or her own like our

26:15.960 --> 26:22.680
neural network architecture. Neural network here are like the machinery used to solve this problem,

26:22.680 --> 26:29.800
but deep q network it's own is like a generic algorithm, doesn't care that you're using neural

26:29.800 --> 26:38.040
network, you're using Canon machine or using linear models, I mean, the objective is being used

26:38.040 --> 26:43.880
there is generic and can be applied on the on the variety of different models and can be applied

26:43.880 --> 26:49.320
on variety of different architectures, but the first paper used this objective function

26:51.240 --> 26:59.160
on using like deep neural networks, so that's why we call it deep q network. So the thing is like

26:59.160 --> 27:05.800
it can be applied on any architecture design for the deep network. I can design my own

27:05.800 --> 27:11.960
deep, I design also like for different tasks, I design different neural network architectures,

27:11.960 --> 27:20.040
but I still use deep q networks like machinery, and I'll go down to optimize and learn this value

27:20.040 --> 27:26.680
of each decision. So it's kind of generic algorithm, it's not just for a specific neural network,

27:26.680 --> 27:34.120
it's like it works, I mean, it's generic and it's applicable to a variety of almost all the

27:34.120 --> 27:41.720
neural networks. Are there specific neural networks that folks tend to use with deep q networks?

27:41.720 --> 27:49.560
Yeah, for Atari games, when we are like dealing with Atari games, we are using a specific neural network,

27:49.560 --> 27:55.480
mainly people have tried different neural networks, but mainly we use the same neural networks that

27:55.480 --> 28:02.280
give mine paper back in a day, not back in a few years ago, there's a lot. I mean, nowadays the

28:02.280 --> 28:07.640
time is, when you say back in the day, you mean like five years ago in the field of AI.

28:09.880 --> 28:16.760
So far, like most of the researchers, they use the architecture designed in the original deep

28:16.760 --> 28:25.800
q network paper, which is interesting because we use the same architecture and we design better and

28:25.800 --> 28:31.720
better algorithms on the top of deep q network. So we have a variety of extensions to deep q network,

28:31.720 --> 28:36.920
we have double deep q networks, we have my work, which is like Bayesian deep q network,

28:37.640 --> 28:43.240
is like we use the same architecture, but we are developing better and better algorithms.

28:43.240 --> 28:50.920
And once again, what makes an algorithm better and better in this context is its ability to

28:51.720 --> 28:58.120
make better decisions about what elements of the space to explore or what decisions to make

28:58.120 --> 29:04.040
in any particular juncture so that it's more sample efficient. That's a concept that comes up

29:04.040 --> 29:11.800
a lot in here. How efficiently are we using our time in the environment to train an algorithm?

29:11.800 --> 29:21.080
That's a super interesting point that you brought up is I want to learn the optimal behavior for

29:21.080 --> 29:26.680
let's say, playing games in minimum number of interaction with the game, right? I don't want to

29:26.680 --> 29:35.000
like play game for 25 billion years in order to, by game, I mean, let's say game pond or game

29:35.000 --> 29:40.440
and sequence. I don't want to play this game for like 25 billion years in order to be able to

29:40.440 --> 29:47.480
play years, I mean, like time for the for the for the entire game. So I don't want to play,

29:47.480 --> 29:52.440
I want to like play these games for like, I don't know, half an hour. By hour, I mean,

29:53.640 --> 29:58.360
by this time I'm talking is like the time that actually you need to play that game.

29:58.360 --> 30:06.040
It's not like the time that your RL agent is going to use is going to be the number of

30:06.040 --> 30:11.240
interactions with the environment you have. You don't want to have the number of times you

30:11.240 --> 30:15.560
play the game. You don't want to be like 20 billion in order to stop the game. You want to

30:15.560 --> 30:21.080
play this game 100 times and be able to learn it or you want to play this game like 200 times

30:21.080 --> 30:27.080
and being able to learn. So the sample complexity is really issue in reinforcement learning and

30:27.080 --> 30:35.080
the goal mainly is to design an algorithm which makes the optimal balance between exploration

30:35.080 --> 30:40.440
and exploration and minimize the sample complexity or some other notions that we call regret,

30:40.440 --> 30:45.320
which is like, you don't want to lose a lot before getting to the good performance.

30:47.000 --> 30:55.160
Another thing is that these environments, they can effectively have like local optima,

30:55.160 --> 31:02.840
meaning you could going back to your Starbucks coffee shop example, you can kind of get into a

31:02.840 --> 31:11.000
rut where you settle on the latte and that's what you choose. But you don't know that in one

31:11.000 --> 31:18.280
particular day, you happen to order the cappuccino, the person behind you also orders the cappuccino

31:18.280 --> 31:24.440
and you find out that your soulmates and like we're destined to be together. If you didn't order

31:24.440 --> 31:30.280
the cappuccino, you would have never have had that experience. The games that these environments are

31:30.280 --> 31:37.320
so dynamic that unless you're really careful about the way that you explore them, you miss opportunities.

31:38.120 --> 31:42.600
Yeah, it's like your tour ride, it's like these environments are super complicated,

31:42.600 --> 31:50.920
like the dynamic is like really complicated and that's a part that makes the whole our life

31:50.920 --> 31:57.320
really hard and also interesting. So like if the setting might have a lot of

31:57.320 --> 32:06.440
weird and complicated situation and it makes me to explore all of them. But if I'm gonna for the

32:06.440 --> 32:11.240
first time I go to coffee shop, I don't care that much that there is a person behind me is gonna

32:11.240 --> 32:16.440
be my soulmate. But if I go to coffee shop and I learn that which coffee actually I like,

32:16.440 --> 32:22.280
then I'm certain then I start to explore other stuff. I see who is behind me. So for example,

32:22.280 --> 32:26.600
first time I came to United States, I was not able to speak English very well. So when I went to

32:26.600 --> 32:33.560
coffee shop, I was like just focused to get my coffee and pronounce things correctly and like

32:33.560 --> 32:39.480
say my name correctly and be ever and pay correctly. But nowadays when I go to coffee shop,

32:39.480 --> 32:45.720
I just while ordering coffee, I talk to the person behind me. So over time when I get more

32:45.720 --> 32:50.440
confident and confident about the state of the environment, the way I need to make a decision,

32:50.440 --> 32:56.760
I start to do more complicated exploration. So it's like a very human does and I was like when

32:56.760 --> 33:02.840
the first came to United States as a person who was not speaking English before coming here.

33:02.840 --> 33:08.440
So I had this experience that when I start going to coffee shops, I was not even care who is behind me.

33:08.440 --> 33:15.560
I was like I was just focused to talk to the lady or the person, the guy there and just make my

33:15.560 --> 33:21.480
mission accomplished. I wanted to just order my coffee and make this that task done correctly.

33:21.480 --> 33:28.360
And so you've got these two different metrics for what makes a good algorithm. One is it sample

33:28.360 --> 33:35.720
complexity, sample efficiency. The other is the degree to which it fully explores the environment.

33:36.280 --> 33:42.120
There are many times but for simplicity, we all let's call it like sample complexity,

33:42.120 --> 33:46.680
like how many samples I need to come over the good strategy. Okay, so in other words,

33:46.680 --> 33:52.040
what I'm here you say is you can kind of boil all of that stuff down into sample complexity

33:52.040 --> 33:59.800
at the end of the day, whether it's the algorithmic, like the computational element of it,

33:59.800 --> 34:05.480
whether it takes a long time to converge on anything or not, whether it takes a long time to

34:05.480 --> 34:10.600
figure everything out or not. But all of this stuff is ultimately related to the sample complexity.

34:10.600 --> 34:15.720
Yeah, all of them are happening like the goodness of the sample in the sense that I get

34:15.720 --> 34:23.240
lower uncertainty about my world. And also how much that knowing that sample is going to help me

34:23.240 --> 34:29.480
to come up with a better like strategy or better decision. This combination drives me to come

34:29.480 --> 34:36.200
with a sample complexity. So with all that in mind, maybe you can walk us through a couple of

34:36.200 --> 34:41.240
your papers on this topic. You know, one of them is the one that you mentioned earlier,

34:41.240 --> 34:46.440
the Bayesian DeepQ Networks. What are you trying to do there? Yeah, the Bayesian DeepQ Network is

34:46.440 --> 34:53.960
actually quite interesting. I really like that work. It has some theoretical like guarantee

34:53.960 --> 35:02.120
about sample complexity, but also it has interesting behavior in like in real but on Atari game.

35:02.120 --> 35:08.920
Let's go back again on the coffee shop example. Epsilon really what we're saying is like it's

35:08.920 --> 35:13.640
going to choose latte with high quality and randomized over all the actions, all the other

35:13.640 --> 35:21.080
decisions uniformly. So Epsilon duty is also going to choose the vodka with the same number of times

35:21.080 --> 35:28.280
that it's choosing other like cappuccino or good coffee. But the thing is what it's happening

35:28.280 --> 35:34.840
is like it doesn't care that how confident you are about the other action. It just cares how much

35:34.840 --> 35:41.560
information you know about the best decision, which is like latte. So even for DeepQ Network,

35:41.560 --> 35:47.240
DeepQ Network is like computing the value of each action. If the action related to the vodka,

35:47.240 --> 35:55.160
it has a really low value, the Epsilon duty action is going to choose that. So what we do in DeepQ

35:55.160 --> 36:04.280
Network is we say hey, instead of just estimating the by we do in Bayesian DeepQ Network is we are

36:04.280 --> 36:10.440
saying that instead of estimating the value of each action, how good is each action, also

36:10.440 --> 36:16.360
estimate how confident how confident you are about the value of that action. Let's say I'm really

36:16.360 --> 36:23.640
confident that value of like the latte is like high, but let's say I'm not confident that the value

36:23.640 --> 36:29.480
of the cappuccino is high. So I have estimation of the value of the cappuccino. I know how cappuccino

36:29.480 --> 36:35.800
should be good. And I also know that I'm not that certain. So it worth trying. So if I know that

36:35.800 --> 36:41.640
the value of the cappuccino is like high, but it's not as high as latte, but I'm really uncertain

36:41.640 --> 36:49.160
about this value, like how good is the cappuccino, then I would like to try it. And also if I know the

36:49.160 --> 36:53.880
value of the vodka is really low, and also I know that with high confidence, I know that the value

36:53.880 --> 37:01.800
of vodka is low, I'm not going to ever like try it, right? So what DeepQ Network does,

37:01.800 --> 37:08.440
Bayesian DeepQ Network is like I'll algorithm on the top of DeepQ Network, but it says in sub

37:08.440 --> 37:14.840
estimating just the value of each action, also estimate how confident you are about each action.

37:14.840 --> 37:20.280
And then you want to make a decision, see how good is the value of that action and how much

37:20.280 --> 37:27.320
you're confident. And if you're not confident about the action with high value, let's try that one.

37:27.320 --> 37:32.760
Okay, so it's like what it does is like the exploration doesn't happen in epsilon giddy setting.

37:32.760 --> 37:39.880
The exploration happens in a setting that it tries to maximize a combination of the uncertainty

37:39.880 --> 37:46.680
and expected like a goodness of that action. If the action, the agent thinks it's good,

37:47.320 --> 37:52.360
but the agent is certain about that, but there's another action, you choose a slice, it's

37:52.360 --> 37:57.160
worse, but you're really uncertain about it, you want to try that one. Let's squeeze this one.

37:57.160 --> 38:02.920
You go to coffee shop, you go, you always get lottery, and you never got like cappuccino,

38:02.920 --> 38:08.040
but your grandma every day talks about cappuccino, okay? When you go to coffee shop,

38:08.040 --> 38:13.880
you have, you, you believe that the cappuccino has high value because your grandma is always

38:13.880 --> 38:18.760
talking about it, and but you never tried it. So you have a gigantic uncertainty about it,

38:18.760 --> 38:26.920
okay? But still you love lottery. So at this situation, you better to choose like the cappuccino

38:26.920 --> 38:34.040
because you think is a really good drink is not as good as lottery, you think, but you're not sure

38:34.040 --> 38:38.760
that's how good it is. Maybe it's way better than lottery, but you don't know, okay? But the

38:38.760 --> 38:43.640
expectation, your grandma, the way your grandma explains it to you, you think that it's a good drink,

38:43.640 --> 38:48.120
it's comparable to lottery, but it's, but it's still you like lottery a bit more because,

38:48.120 --> 38:53.560
but based on your grandma's explanation, you don't know that cappuccino is how good it is,

38:53.560 --> 38:58.760
but you roughly speaking, you know that it's not, you believe that it's not better than lottery,

38:58.760 --> 39:05.000
but you are not certain about it. So you just tried that one. So it's like kind of balanced

39:05.000 --> 39:14.040
between like uncertainty over the your leap about the value of each decision and makes the

39:14.040 --> 39:20.440
decision based on the expected value of each action and also on certain your over that

39:20.440 --> 39:29.800
expected value. And so are you in the epsilon greedy, you are choosing your primary, the,

39:29.800 --> 39:34.760
the one that you believe has the most value at a probability of one minus epsilon and then

39:35.480 --> 39:41.320
say you've got your five choices, the probability of you choosing one of those other ones is

39:41.320 --> 39:48.280
epsilon over four, right? Yeah. And epsilon over five. I randomize over the whole thing.

39:48.280 --> 39:53.560
Are you randomize over the whole thing? And so that was my question with, with the approach

39:53.560 --> 40:01.320
we're describing, we're waiting, are you, are you doing this confidence waiting over just the other

40:02.360 --> 40:08.360
four or over all of all of you just kind of evaluating each of them based on this confidence

40:08.360 --> 40:15.240
weighted metric. Exactly. So I just for each action, I have estimated value and also I have

40:15.240 --> 40:22.360
uncertainty. And I, what I do, I, so somehow it's like, I have a belief about each action.

40:23.160 --> 40:28.760
I am like, I know how good it's going to be, but this goodness is not a number. It's like

40:28.760 --> 40:35.800
somehow a distribution over each action. So I'm like, when I say I believe that this should be,

40:35.800 --> 40:41.640
okay, a means of high probability, I think it's good, right? So for each action, I have a

40:41.640 --> 40:47.560
distribution about how good each action it is. And the mean of this distribution is going to be

40:47.560 --> 40:53.960
my expected like, expected like value. And also this distribution has some variance,

40:53.960 --> 40:58.840
it's going to be somehow my uncertainty. Okay. So for each action, I have a belief and my

40:58.840 --> 41:04.360
belief is somehow a distribution over the, over the goodness of each action. Or it's going to be

41:04.360 --> 41:11.400
uncertainty over each action. And thinking about it in terms of distributions is where the

41:11.400 --> 41:17.080
concept of basing comes in the body. Exactly, exactly. So this distribution is like my belief,

41:17.080 --> 41:23.400
my posterior belief about each action. So if I know the posterior belief about each action,

41:23.400 --> 41:29.320
what I can do for each action, I can sample out of this belief. I get some, like, let's assume

41:29.320 --> 41:38.280
that for Lasse, I am my expected, expected value is like five. And my variance is like one,

41:38.280 --> 41:46.360
is like, I'm somehow uncertain that this is between four to six. Okay. So what I do, I sample

41:46.360 --> 41:52.920
a number between four and six. And I assume that the value of the, the Lasse is that number.

41:52.920 --> 42:00.280
I do the same thing for other, other actions like for Lasse, for Lasse, I know that my grandma told

42:00.280 --> 42:05.720
me the value of the lot, sorry, for, for Kappuccino, my grandma told me the value of the Kappuccino

42:05.720 --> 42:10.840
is like four. But I'm like my uncertainty is like the variance of my... She thinks it's higher

42:10.840 --> 42:16.360
than the Lasse. So maybe like eight. No, no, she tells me that the Kappuccino is good. But for me,

42:17.000 --> 42:21.880
she thinks that Kappuccino is the best. But for me, I think the Lasse is the best.

42:21.880 --> 42:28.040
Ah, okay. Okay. So for me, Kappuccino is like four. I never tried it. My grandma told me,

42:28.040 --> 42:33.000
I put four for Kappuccino. But the variance for Kappuccino is like 10. So the Kappuccino can be

42:33.000 --> 42:39.800
between 14 to minus like six. Got it. Got it. So if I sample, there is a high chance that the

42:39.800 --> 42:46.440
Kappuccino is going to get a number above like five. If the Kappuccino gets a number of five,

42:46.440 --> 42:52.600
I'm going to, I'm going to choose Kappuccino. So this is the way I do exploration. This is called

42:52.600 --> 43:01.640
Council Sampling. There was a guy back in like 1930s and he developed this idea of sampling.

43:01.640 --> 43:08.600
If you have a belief about the environment, you sample out of that and just do just act based on

43:08.600 --> 43:14.440
that sample. Also in psychology, the literature is called, they call it, they do not call it

43:14.440 --> 43:21.320
Council Sampling. They call it Beijing Sampling because like they are from psychology backgrounds

43:21.320 --> 43:26.840
so they they define their own term. But they have a cool setting that they say even human,

43:26.840 --> 43:33.560
that's why I bring up this coffee shop example. It's like they say in psychology that human also

43:33.560 --> 43:39.880
does Beijing Sampling. The human come up with a belief about the world, a belief about each

43:39.880 --> 43:46.760
decision and sample out of that belief and do the thing, the human thing. It's like the human

43:46.760 --> 43:53.240
randomizes over its behavior. It doesn't go always with treaty. We do not always get a lot

43:53.240 --> 43:59.240
of it. Sometimes randomize. The way we do randomization is sampling through our belief.

43:59.240 --> 44:03.880
This is playing in psychology. I'm not making the discreet. I'm just fitting the thing they say.

44:04.440 --> 44:10.440
I'm not sure how exactly to articulate this but it strikes me that there's one of the important

44:10.440 --> 44:17.240
assumptions in here is this idea that, I mean, I guess the whole idea of sampling, that you can

44:17.240 --> 44:24.120
just pick a number and even though relative to the distribution, that number could be an extreme

44:24.120 --> 44:29.400
outlier. We're still just going to use this number as to make our decision like how,

44:29.880 --> 44:33.960
what tells us that we can do that? So that's an interesting question.

44:35.160 --> 44:43.240
So if I draw samples and one sample is suddenly it's extremely high and like the question is should

44:43.240 --> 44:49.480
I go with that or not? So I guess maybe to start to answer my own question, are we just kind of

44:49.480 --> 44:54.600
reverting to law of large numbers here? If we do this enough, we're basically sampling around,

44:54.600 --> 45:00.680
you know, we'll kind of converge to our distribution. It's slightly related to that but it's not

45:00.680 --> 45:09.000
exactly that. Here it's about concentration of the measure mainly. It's like if I have uncertainty

45:09.000 --> 45:15.320
about the latte and cappuccino and others, if I sample them a lot, then I am going to be sure

45:15.320 --> 45:22.520
how good they are, right? If I'm, let's say in the like not realistic world, I'm able to drink

45:22.520 --> 45:29.480
latte 10 billion times and I'm allowed to drink cappuccino 10 billion times, then I can say

45:29.480 --> 45:36.920
which one is better, right? So it's like over time, my belief concentrates over its actual value

45:36.920 --> 45:43.880
because latte for me has a goodness. I don't know what is that goodness and I need to try it many,

45:43.880 --> 45:49.880
many times to understand what is that goodness and if you allow me to drink latte billions of times,

45:49.880 --> 45:57.800
then I can over time I can like shrink down my uncertainty about latte and at some point I say,

45:57.800 --> 46:03.720
hey, latte is actually a bit, even though might not be, but for other people, for me, let's say

46:03.720 --> 46:09.720
is like when I drink latte, a lot and cappuccino and a lot, over time I am certain about how

46:09.720 --> 46:17.800
which one is good. So it's not just law of large numbers because we're not just sampling a lot

46:17.800 --> 46:25.320
from this distribution to learn its parameters. We're also updating the distribution,

46:25.320 --> 46:30.920
the parameters of the distribution as we go along to reflect our increased confidence.

46:30.920 --> 46:36.440
Yeah, and here is like the distribution, my belief is my distribution,

46:36.440 --> 46:43.720
right? So this distribution at the very beginning has a like fact tail, but over time when I collect

46:43.720 --> 46:49.640
more samples, I'm going to make sure that, so what is this distribution? This is the distribution

46:49.640 --> 46:55.720
over the value of latte, right? So the value of latte in expectation is a fixed number,

46:55.720 --> 47:01.400
so it's five, right? So, but I don't, I'm not sure about it, so I am uncertain, so this uncertainty,

47:01.400 --> 47:06.760
so this distribution represents my uncertainty and over time when I collect more samples,

47:06.760 --> 47:12.200
I'm reducing my uncertainty and this uncertainty is going to be shrink down and over time if you

47:12.200 --> 47:18.920
give me, if you allow me collect more samples, I'm going to shrink down my uncertainty to zero

47:18.920 --> 47:24.680
and claim that I exactly know what is the value of the latte. So it's going to be like a measure

47:24.680 --> 47:32.760
of uncertainty, like if I, if I get more experience, if I try same thing many times, I get more,

47:32.760 --> 47:38.520
I get more certain about it and then I reduce somehow the variance of my belief about that,

47:39.320 --> 47:47.480
that action, that decision. So it's going to reduce that the uncertainty over the world for me,

47:47.480 --> 47:54.520
so and it's been like a study that if you use tons of sampling, you actually balance between,

47:54.520 --> 48:01.080
nicely balanced between exploration and exploitation and you actually get a like ordered off

48:01.080 --> 48:09.320
and all like sample complexity. So a part of your work then is applying this, this Thompson sampling

48:09.320 --> 48:18.440
algorithm to the deep Q networks and does the analytical results follow as well into the reinforcement

48:18.440 --> 48:24.840
learning realm? Yeah, it does. So I'm not applying Thompson sampling on deep Q networks,

48:24.840 --> 48:31.880
I'm extending deep Q networks to have this Bayesian property. Deep Q networks, as I said,

48:31.880 --> 48:39.080
they just compute the value of each action, but Bayesian deep Q networks use the same machinery as

48:39.080 --> 48:45.880
deep Q networks, but in self computing, the value of each action, it also, not only is

48:45.880 --> 48:52.360
estimated value of each action, it also estimate the uncertainty of that value. So it's like

48:52.360 --> 48:59.160
collecting more information. And on the other hand, for exploration, Bayesian deep Q network

48:59.160 --> 49:06.120
doesn't use epsilon greeting, but it used tons of sampling, which is like super interesting.

49:06.120 --> 49:13.800
I mean, the first time I applied this algorithm, this method on the Atari game, it was like,

49:13.800 --> 49:19.960
when I was showing this result to my colleagues, no one was like, like believing that was going on,

49:19.960 --> 49:27.800
because it was doing super great, like 1000 times better in the performance, like 100 times better

49:27.800 --> 49:34.680
in sample complexity. It was like crazy, really good. It can learn the game that like deep Q network

49:34.680 --> 49:40.440
learns in like, I don't know, 100 million times that it can learn it like less than 5 million,

49:40.440 --> 49:45.480
or less than 2 million for some games. It's crazy, really cool. It was like

49:46.280 --> 49:52.120
interestingly, interesting observation and made us to think more about exploration. If you

49:52.120 --> 49:58.040
like the literature in the deep Q networks, and learning almost like more than 99 percent of them,

49:58.040 --> 50:02.840
they use epsilon greeting. They're few of them, they are using more sophisticated exploration

50:02.840 --> 50:07.640
algorithm. And here we show that if you just end up doing architecture design, if you just

50:08.280 --> 50:12.280
come up with better exploration strategy, you're going to gain a lot.

50:13.160 --> 50:20.040
Has that result stood the test of time? Is this still state-of-the-art for certain games,

50:20.040 --> 50:27.560
or has it been extended by other folks? Where does it sit in the landscape of extensions to

50:27.560 --> 50:34.440
deep Q networks? Interesting, interesting question. After deep Q networks, there have been many,

50:34.440 --> 50:41.080
many extensions to design better cost functions, design better, like sampling, the design

50:41.080 --> 50:47.560
better handling of the memory. There are a lot of extensions that they advanced deep Q networks,

50:47.560 --> 50:56.280
but they're almost all using epsilon greeting. So we are not comparing against those advanced

50:56.280 --> 51:01.800
architecture. We are just saying, just take the simple deep Q network, and instead of doing epsilon

51:01.800 --> 51:09.160
greeting, do top of that. And we show that it does way better, and also we show that it does

51:09.160 --> 51:14.760
better than many, many other algorithms, many, many advanced algorithms, but we did not compare

51:14.760 --> 51:22.120
against those algorithms that they are advancing the architecture or advancing some other like

51:22.120 --> 51:28.120
v-warding functions, because the point of its work was like, hey, if we just change the exploration

51:28.120 --> 51:33.080
algorithm, what's going to happen? If those algorithms, those algorithms, they are advancing deep

51:33.080 --> 51:39.640
Q network. If they, if I applied this top of sampling on those algorithms, I'm going to get

51:39.640 --> 51:45.240
like better performance. Got it, got it. So you kind of left it as an exercise to the reader to

51:45.240 --> 51:52.520
take their favorite extension to deep Q networks, and try this as a way to get even better sample

51:52.520 --> 51:58.600
complexity. Yeah, it's like just this is like the way we say to do exploration and exploitation

51:58.600 --> 52:05.000
instead of epsilon greeting. If you have a sophisticated like deep reinforcement learning algorithm,

52:05.000 --> 52:10.040
you're, if you're using epsilon greeting, if you apply this one, this approach, which has a

52:10.040 --> 52:16.760
theoretical analysis, and a theoretical guarantee, you hope to get better performance.

52:16.760 --> 52:22.920
What you've done here is you've proposed an alternative to epsilon greeting based on

52:22.920 --> 52:30.920
Thompson sampling, so based on uncertainty waiting. Presumably, there are other ways that you could

52:30.920 --> 52:39.560
change the exploit mechanism further. How well explored is that space? Have a lot of people

52:39.560 --> 52:45.720
proposed different algorithms for dealing with exploitation? That's a super interesting question.

52:46.520 --> 52:53.960
Is from land of theory, like the main question we are asking on everyday is to come up with

52:53.960 --> 52:59.480
the better exploration and exploitation algorithm. From the theory land, we developed a lot of

53:00.120 --> 53:06.440
nice, amazing, and sweet, I would say algorithms, they, they balance this exploration

53:06.440 --> 53:14.200
for different settings. So, theoretically, this area is nicely studied. There are a lot of

53:14.200 --> 53:25.000
rooms to explore more and to prove and study new things and find the optimal way of exploration.

53:25.000 --> 53:33.640
But the thing is extending those methods. Even I have seen like a theoretical way of doing

53:33.640 --> 53:41.880
exploration, exploitation in the theory land, but it's not easy to extend those ideas and those

53:41.880 --> 53:48.920
settings to deep neural networks. So, from the theory, we have a lot of algorithms and we have

53:48.920 --> 53:55.800
a massive amount of studies that we know how to do exploration, exploitation to get some sort of

53:55.800 --> 54:02.680
order optimal, like safer complexity. But the problem is that those algorithms are not easily

54:02.680 --> 54:09.320
extendable to deep reinforcement learning. Is it simple to explain or give an example of an

54:09.320 --> 54:17.000
algorithm that isn't easy to apply or how or why they tend to break? So, one of the major

54:17.800 --> 54:23.720
portion of the theory land in reinforcement learning is like model-based reinforcement learning.

54:23.720 --> 54:34.520
What we do is like we literally learn the model dynamics. For each state, we store the

54:34.520 --> 54:41.160
transition from each state to another state. If I'm receiving, I'm going to coffee shop,

54:41.160 --> 54:48.440
I get a latte and after that, I'm going to go back. So, we need to store all these possible

54:48.440 --> 54:55.160
transitions, going from one situation to another situation. We need to store all these things

54:55.160 --> 55:05.720
and do our optimization on the gigantic space of possibility. It's not possible to store

55:06.440 --> 55:14.920
all these things in the memory and do your computation. It's not possible. Also,

55:14.920 --> 55:23.560
it's like most of the algorithms we designed for the area from the theory land,

55:24.040 --> 55:32.040
they are not always. Most of the time, we were really concerned about the worst case scenario.

55:32.040 --> 55:37.080
We want to compute, we want to come up with the algorithm, which no matter what is the model,

55:37.080 --> 55:43.800
is going to perform the best. So, it makes almost all the algorithms we developed so far,

55:43.800 --> 55:49.240
makes all of them such that they are the best for worst case scenario. Because we want to

55:49.240 --> 55:54.840
provide a theory. Why this is a case is a case? Because we are going to provide theoretical

55:54.840 --> 56:01.560
guarantee. When I say this algorithm for short works, it means that for no matter what is the world,

56:01.560 --> 56:06.680
what is the environment, it should work. But that doesn't mean it's practical for

56:07.240 --> 56:13.560
any given sense. Exactly. Those are the algorithms that they give you bounds and they give you

56:13.560 --> 56:20.600
theoretical proof that they are going to work. But the thing is, if you know that your environment

56:20.600 --> 56:27.400
is not that bad, it's not adversarily chosen or it's not the worst case scenario ever can happen

56:27.400 --> 56:32.360
to your algorithm, then you can come up with a better and more sensible algorithm.

56:32.360 --> 56:39.560
Reinforcement learning is kind of young. We need many, many more people to spend time and do

56:39.560 --> 56:43.400
and build up that theory to kind of stop reinforcement learning. So we are like the community of

56:43.400 --> 56:49.640
deep reinforcement. The community of the theoreticians in reinforcement learning is not as big as

56:51.160 --> 56:55.400
I cannot even say like it's like the proportion is almost getting to zero. If you look at the

56:55.400 --> 57:00.600
number of people they do theory of like reinforcement learning compared to the people they do

57:00.600 --> 57:05.080
empirical study of reinforcement learning. This ratio is like super small. You cannot even see

57:05.080 --> 57:11.720
theoreticians. Which is like, I'm not, I mean, it's almost nice and also is a concerning. We should

57:11.720 --> 57:17.880
do both. And so just to make sure I understand the first of the two impediments you mentioned,

57:18.520 --> 57:27.400
it is that in practically speaking with deeply reinforcement learning models like deep Q, they make

57:27.400 --> 57:35.400
simplifications that lend themselves to, you know, practical concerns, computational concerns.

57:35.400 --> 57:45.240
And so for example, they are looking at, they're kind of rolling up the value into a single

57:45.800 --> 57:53.400
state, if you will, or element, if you will. Whereas some of these algorithms that are, you know,

57:53.400 --> 57:59.560
you could argue are more optimal, you know, they may be looking at a wider, you know, wider

57:59.560 --> 58:05.000
set of observations of the state that you couldn't really operate on practically. Is that what

58:05.000 --> 58:12.040
you're kind of getting at? So the more sophisticated algorithm they design, more sophisticated

58:12.680 --> 58:19.480
cost function. So they are like, they are designing the advancing the objective function that

58:19.480 --> 58:26.040
deep Q network model is solving. So it's like, they are advancing that. And the idea behind

58:26.040 --> 58:31.880
deep Q network was not giving the best algorithm. They, the Q network came out to show that it is

58:31.880 --> 58:39.160
possible to play games. It doesn't claim that it's the best. And the aim of that work from deep

58:39.160 --> 58:44.840
mind was not heavier doing the best reinforcement learning algorithm. They wanted to just want to show

58:44.840 --> 58:51.880
that, hey, it is possible to use current algorithms to in order to beat human or like, in order to

58:51.880 --> 58:58.200
learn simple games. And a big part of that possibility was the computational feedback. Yeah, they didn't

58:58.200 --> 59:03.320
yeah, they didn't care that much about like, like, like sample complexity. They didn't want to minimize

59:03.320 --> 59:08.920
the number of samples that the agencies, they wanted the main goal was, hey, we want our

59:08.920 --> 59:15.480
else from like other algorithms and we are going to ask from other like human. That was the

59:15.480 --> 59:21.160
goal behind the nature paper they had was like, hey, this is the case that you can come with the

59:21.160 --> 59:27.480
algorithm, which does better than human in certain games. Or let's say for alpha, go like the

59:27.480 --> 59:34.840
goal game, you do not care about the damage about the number of samples that your like goal players,

59:34.840 --> 59:40.840
like your agency, you just at the end wants to like, like, win against the best goal player.

59:40.840 --> 59:45.880
Right. Right. So it's like, this is really important. This is really interesting to do, to be

59:45.880 --> 59:52.440
make it feasible for first time. But after that, you need to design the algorithm to do the thing

59:52.440 --> 59:57.560
that reinforcement learning actually requires. It's like reducing number of samples, like learning

59:57.560 --> 01:00:02.760
better policy. So they made it feasible. The rest is going to be like, first make it better.

01:00:02.760 --> 01:00:09.400
By making it better, it's like coming with better sample complexity, coming with a better value

01:00:09.400 --> 01:00:16.280
estimator. In the second work that I want to talk is like, you can easily show that the QN

01:00:16.280 --> 01:00:22.280
objective is actually biased objective function. So you better off not to do it. If you want to be

01:00:22.280 --> 01:00:28.760
really serious in reinforcement learning, you better like do something else than get QNF. But if

01:00:28.760 --> 01:00:33.960
deep QNF is the only thing you can do, we just go with it. If you can't, I mean, if you're able

01:00:33.960 --> 01:00:40.040
to do better, it's better off not to deep QNF because the cost function is easily biased and can

01:00:40.040 --> 01:00:44.360
it can screw you off. So elaborate on that. You said the second cost function.

01:00:45.160 --> 01:00:49.640
No, in the second work that I get chance to talk about here.

01:00:49.640 --> 01:00:56.280
In that second paper, walk us through the results. So you started with looking at the cost

01:00:56.280 --> 01:01:02.040
function for deep QNF works. So I started to looking at the cost function at deep QNF work,

01:01:02.040 --> 01:01:10.280
and I have many friends from theory lands. They have many amazing papers and astonishing papers

01:01:10.280 --> 01:01:18.120
that are analyzed back in like 10 years ago or 20 years ago that they analyze this cost

01:01:18.120 --> 01:01:23.080
function that one of them has been used by deep QNF work. And they analyze this constantly.

01:01:23.080 --> 01:01:30.760
They show that hey, they are biased. And also there is another work we show that for any function

01:01:30.760 --> 01:01:37.560
approximation method, there is a subset of problems that if you run this functional approximation

01:01:37.560 --> 01:01:44.040
method, they either diverge or are biased. So it's like, there is no hope that you can get

01:01:44.920 --> 01:01:52.760
algorithm, which is like, you think that it can do something reasonable almost everywhere.

01:01:52.760 --> 01:02:01.320
But the hope is that they do not break in the simple setting. So if you look at the deep QNF work

01:02:01.320 --> 01:02:08.120
objective, it's like it minimizes two terms where you just care about one of them. The second term,

01:02:08.120 --> 01:02:15.560
you don't want to minimize. But it's naturally minimizes. And also it has some other issues of like

01:02:15.560 --> 01:02:24.920
like it somehow looks at, and stuff like like averaging the errors, it just always looks at the

01:02:24.920 --> 01:02:31.160
maximum error and like back propagate that maximum error, which is not the thing you want. If you

01:02:31.160 --> 01:02:36.280
have a Gaussian random variable, you don't want, you don't care about it max, you care about

01:02:36.280 --> 01:02:43.000
like it's mean. But there's the thing that deep QNF work setting passes through its back

01:02:43.000 --> 01:02:49.560
propagation is actually the max of the sample, which is not the thing you want. So it's kind of like

01:02:49.560 --> 01:02:57.640
biased in many, many different senses. And the question is, if it's biased, and if we know that

01:02:57.640 --> 01:03:03.560
any function approximation method can be biased or diverges, can we do anything for that? That was

01:03:03.560 --> 01:03:08.680
the question I was asking myself when I start to work on the second work, which we call it

01:03:08.680 --> 01:03:15.080
Generative Adversarial 3 Search, or in short we call it Gats. Probably you have watched the movie

01:03:15.080 --> 01:03:22.680
Gats B. So the name has some relation to that guy as well. But yeah, the algorithm is called Gats

01:03:22.680 --> 01:03:29.080
and it's trying to address the fact that if I use functional approximation for deep reinforcement

01:03:29.080 --> 01:03:35.240
learning, and if my functional approximation is biased, can I do anything with respect to that

01:03:35.240 --> 01:03:42.760
or not? And presumably you found that you could do something using a generative adversarial network

01:03:42.760 --> 01:03:49.080
based approach. Yeah, that's an interesting idea. So there's a line of research in psychology,

01:03:49.960 --> 01:03:55.880
which says when human wants to make decision, it forecasts what's going to happen in the future.

01:03:56.600 --> 01:04:04.360
Or people say imagine I would, I would like avoid using word imagine, but I can't, I would say

01:04:04.360 --> 01:04:11.800
human like when I want to go to coffee shop from my office, I just imagine or forecast or

01:04:11.800 --> 01:04:18.760
predict what's going to happen in the future. I can think of going there talking to the coffee man,

01:04:19.720 --> 01:04:26.040
an order and pay and think about what I'm going to get and come back. I can imagine and think

01:04:26.040 --> 01:04:34.280
about it. So this is a line of research in psychology which says sometimes when human

01:04:34.280 --> 01:04:41.400
makes decision, human builds the model of the environment, model of the world, and in that world

01:04:41.400 --> 01:04:47.960
does some analysis. It's not always human does analysis based on just observation. When human

01:04:47.960 --> 01:04:53.400
interact with the environment with the world, it has some model of that some abstract model of

01:04:53.400 --> 01:05:00.840
the environment and the world in his or her brain, okay? I mean, baby, it doesn't make sense, but

01:05:00.840 --> 01:05:04.600
I think to me it makes sense because when I'm going to do something and when I want to plan for

01:05:04.600 --> 01:05:12.440
future or for my like like a trip of I want to do a road trip, I plan everything and I think of

01:05:12.440 --> 01:05:17.720
what's going to happen, what are the situations, all these scenarios, I check all of them in my brain,

01:05:17.720 --> 01:05:22.840
I do not start my road trip and then see what's going to happen, I just plan everything.

01:05:22.840 --> 01:05:29.960
So this is a thing that we they say, of course there are like many decisions we do not plan for,

01:05:29.960 --> 01:05:35.080
like let's say there's a mosquito, it just bites us, like we do not plan to just scratch that

01:05:35.080 --> 01:05:42.040
part of body that happens. But part of it happens through this planning, through like the model of

01:05:42.040 --> 01:05:48.680
the world that we have in our brain. So we were saying like let's get some idea from this part

01:05:48.680 --> 01:05:55.800
of psychology and also see what theory tells us. Theory tells us that functional approximation

01:05:55.800 --> 01:06:06.120
like is can be bias or diverse. So now if I have let's say bias like value function, okay. And if I'm

01:06:06.120 --> 01:06:13.640
going to use it and also if I'm able to learn the model dynamics, the dynamic, the way model works,

01:06:13.640 --> 01:06:20.120
I can build the model of the world right in my brain. Or somehow the agent can based on

01:06:20.120 --> 01:06:25.960
interaction the agent has with the world here by world, I mean let's say Atari game, given interaction

01:06:25.960 --> 01:06:31.240
that the agent has with Atari games going left, right up shooting left kind of things,

01:06:31.240 --> 01:06:38.040
it can learn that given the current situation, if it does some sort of sequence of like decisions,

01:06:38.040 --> 01:06:46.920
what's going to happen in the future. So in this work, we used this an interesting framework

01:06:46.920 --> 01:06:54.680
out there, which is called generative adversarial networks. These methods, they're like used to

01:06:54.680 --> 01:07:01.640
generate images out of like random noises. So they are like the kind of generative model. And we

01:07:01.640 --> 01:07:09.320
use this machinery to be able to make ourselves able to like come up with a model of the world.

01:07:09.320 --> 01:07:16.120
So now given the samples, we designed an algorithm and a neural network which is like able

01:07:16.120 --> 01:07:23.800
given like current frame of the Atari game is able to tell you what is going to be the next 20

01:07:23.800 --> 01:07:30.120
frames. If you follow up, down, down, up some sequence of action, it's able to tell you what

01:07:30.120 --> 01:07:35.160
actually is going to happen in the future, which is quite interesting. So if I am able to see

01:07:35.160 --> 01:07:41.640
like 20 steps from now, okay, then I can say, hey, whenever I'm going to make a decision, I just

01:07:41.640 --> 01:07:50.360
see all possible all possibilities from now in 20 steps in my brain. And then in the lift node,

01:07:50.360 --> 01:07:56.520
so it's going to be I'm going to construct a tree. I say, if I am at the current time step,

01:07:56.520 --> 01:08:03.320
if I am my agent, my like my Atari agent is at some location in the game, if it choose action

01:08:03.320 --> 01:08:08.040
up, very it's going to go, if it choose action down, very it's going to go, if it goes forward,

01:08:08.040 --> 01:08:14.280
way it's going to go. So I can ask my this generative model, because it's generative dynamic model,

01:08:14.280 --> 01:08:22.440
as short like GDM. I can ask GDM, hey, if I if I am at this kind of state and if this and my action

01:08:22.440 --> 01:08:29.000
is going up, what's going to happen? And GDM tells me you go to the new place and I ask GDM, hey,

01:08:29.000 --> 01:08:34.280
now if I am in just new place, if I go action down, where I go. So I can build up a tree for

01:08:34.280 --> 01:08:40.440
different possible actions and see where I go. It sounds like, I mean, in a regular

01:08:41.640 --> 01:08:49.960
DRL type of problem, they're also kind of evaluating the expected reward for each of the

01:08:49.960 --> 01:08:56.600
possible actions, but that the notion of expectation is like you're you've lost a lot of

01:08:56.600 --> 01:09:03.960
information. And so is the idea that by using GANs to project the future state instead of or

01:09:03.960 --> 01:09:11.960
an expectation, you retain more information? Okay, that's a great point is if I run my DRL algorithm

01:09:11.960 --> 01:09:19.640
to compute this value, what I'm claiming here is that value is can be really, really off.

01:09:19.640 --> 01:09:27.240
It means that can be arbitrary bad. Okay, so if I run DeepQ network for, let's again,

01:09:27.240 --> 01:09:37.000
pond for a given situation, the pond agent, if I run, if I run DeepQ network, it tells me

01:09:37.000 --> 01:09:45.560
going up gives you return of like value of like 10. Okay, if I run Deep, like double DeepQ

01:09:45.560 --> 01:09:52.120
network, which is another algorithm, it gives me 5. It's like the DeepQ network, the value

01:09:52.120 --> 01:09:57.400
was estimating was like off square factor of 5, by another factor, by bias of 5, at least.

01:09:58.200 --> 01:10:03.640
So it's like my point is like any deep RL algorithm can be arbitrary bias.

01:10:04.600 --> 01:10:10.440
And now the question is like if it's bias, it means that as you said, it's supposed to tell me

01:10:10.440 --> 01:10:17.400
what is the future reward, right? But if this estimator is like wrong, then the question is what

01:10:17.400 --> 01:10:24.840
we can do. So what we can do is like we train a generative model in order to be able to like

01:10:24.840 --> 01:10:32.040
generate future. And if I am able to generate future, if I'm like, if I'm, if I have an agent

01:10:32.040 --> 01:10:39.320
is playing in the playground and it's a ditch somewhere. And if I imagine, if I like, if the agent,

01:10:39.320 --> 01:10:44.920
if I use this generative model and see what's going to happen in the future, I see if I choose

01:10:44.920 --> 01:10:50.440
action up, up, up, I'm going to fall in that ditch, right? So I do not go. So it's like,

01:10:50.440 --> 01:10:55.640
somehow I'm seeing what's going to happen in the future. Yeah, I think maybe the question I'm

01:10:55.640 --> 01:11:04.440
that I'm asking is what is it about GANS maybe that allows us to be any more accurate looking

01:11:04.440 --> 01:11:11.800
into the future than the machinery used for, you know, for predicting a future reward?

01:11:11.800 --> 01:11:22.120
Okay, so it's like, so we theoretically show that it's actually exponentially can improve the error

01:11:22.120 --> 01:11:28.040
in the, in the Q function. Okay, so it's, theoretically, it's like exponential improvement,

01:11:28.680 --> 01:11:34.360
but practical, I can tell you something. So the way we compute the reward,

01:11:35.000 --> 01:11:41.000
the way we compute the value and return is through discount factor, right? So there's a discount

01:11:41.000 --> 01:11:48.120
factor. It's like the reward I get that kind of time step worth more than reward I receive in

01:11:48.120 --> 01:11:54.600
like next 10 times steps, right? There's a discount factor. So now, if I have my, if my Q function

01:11:54.600 --> 01:12:03.160
has a bias of five, okay? And if I am able to do rollout in my generative model up to depth of

01:12:03.880 --> 01:12:10.520
up to depth of 20, then the Q value I'm going to use at depth of 20 is going to be

01:12:10.520 --> 01:12:15.640
discount factor, power of 20, discount factor is less than one. So the power of 20 is going to be

01:12:15.640 --> 01:12:21.560
super small times that five. So the effect of the bias is not going to appear. So actually,

01:12:21.560 --> 01:12:26.280
this machinery has been used for AlphaGo as well. What AlphaGo does AlphaGo is like,

01:12:27.080 --> 01:12:35.080
runs Monte Carlo 3 search on the board game for some depth, let's say depth of age. And when it goes

01:12:35.080 --> 01:12:42.360
to the like leaf node of that tree, so it builds a tree and then the leaf node of that tree

01:12:42.360 --> 01:12:49.800
use the learn Q value, okay? If the Q value is biased, since I'm like rolling out for like depth

01:12:49.800 --> 01:12:56.920
of age, the effect of that bias is going to exponentially go down. I can imagine I need to draw

01:12:56.920 --> 01:13:05.560
three in length. No, I think it makes sense. So your the argument is that with the deep Q networks,

01:13:05.560 --> 01:13:15.560
you are predicting the future reward at a given point based on a given action. But by using

01:13:15.560 --> 01:13:23.880
GaNS to project into the future, what the board is going to look like, you can then discount

01:13:23.880 --> 01:13:30.600
out the errors by the cause of the you're predicting further into the into time.

01:13:30.600 --> 01:13:36.600
That one thing is like the first part of your statement. When I use deep reinforcement

01:13:36.600 --> 01:13:43.080
and algorithms to compute to see what is the cumulative reward of future at the given point

01:13:43.080 --> 01:13:50.680
and given action, you said, my statement here is any deep reinforcement and algorithm use

01:13:50.680 --> 01:13:57.320
this error error in this estimation can be arbitrary wrong. It can be arbitrary B. So the

01:13:57.320 --> 01:14:05.880
actually you think that you are actually learning the expected return of that state and action

01:14:05.880 --> 01:14:13.880
given that the specific point that a specific action. But I can prove of the show that there are

01:14:13.880 --> 01:14:21.080
problems that this estimator can be arbitrary wrong. So now I'm saying that if I had a perfect

01:14:21.080 --> 01:14:26.280
estimator, there was no need. I mean, there isn't there isn't that much of need for having a

01:14:26.280 --> 01:14:35.080
generative model or GaNS. But I know that this error is big for like for deep Q networks,

01:14:35.080 --> 01:14:41.640
these errors are gigantic that sometimes like shockingly gigantic. People have studied this

01:14:41.640 --> 01:14:46.760
by bias. Because the bias in the estimator, people have studied this biases and they observe

01:14:46.760 --> 01:14:53.640
that these biases are like huge. It's like hundreds of percentage are like bigger than the actual value.

01:14:54.360 --> 01:15:00.840
So it means that the estimator you get, when you compute the expected return condition

01:15:00.840 --> 01:15:07.160
on the specific point and the space and the action, that estimation can be arbitrary wrong.

01:15:07.160 --> 01:15:13.480
Okay, so if it's arbitrary wrong, you cannot do anything. But if it's like wrong but not super

01:15:13.480 --> 01:15:20.920
wrong, the question is can we do anything? And one of the answer is like doing this generative

01:15:20.920 --> 01:15:28.600
adversary or research is like if it's wrong, I can build a tree. I can look at the future and I can

01:15:28.600 --> 01:15:35.240
say I want to look at the future and see where I go after like 20 times steps. And then I go to

01:15:35.240 --> 01:15:43.160
at the 20 times steps, time step, I'm going to use the this bias value function I learned.

01:15:43.160 --> 01:15:49.880
Okay, but the thing is whatever the value of that 20 step is, I'm going to multiply it by

01:15:49.880 --> 01:15:57.000
discount factor to power of like 19 or 20 something that. So whatever is going to be the error,

01:15:57.000 --> 01:16:03.080
that error is going to be a squash down exponential in depth, which is the thing I really like to have.

01:16:03.080 --> 01:16:11.720
Are you using the GAN to project the state only at 20 or is it 2019, 18 all the way down to one

01:16:11.720 --> 01:16:16.680
and they're discounting each of them? I literally build the whole tree and then scan all of them.

01:16:16.680 --> 01:16:26.120
Okay, and so it's other word, I literally build the model of the world. I literally build the

01:16:26.120 --> 01:16:31.400
NDP. If there exists NDP, I literally build the Markov decision process there.

01:16:31.400 --> 01:16:37.560
Right. So another way to think about this is like, you're basically trying to do what AlphaGo

01:16:37.560 --> 01:16:44.120
did with Go, but for a game like Go, you've got like fixed positions and fixed states. And while

01:16:44.120 --> 01:16:51.160
there are a lot of them and we can't enumerate them efficiently, we know what they are given a

01:16:51.160 --> 01:16:57.160
particular state of the game now and a particular move and we can enumerate a tree.

01:16:57.160 --> 01:17:04.840
For an Atari game, they're more continuous. They're not like these very discrete moves and

01:17:04.840 --> 01:17:11.640
positions. So how do we, how might we apply this idea of getting the tree at a bunch of future

01:17:11.640 --> 01:17:16.600
states? Well, we can use GANs to do that. Okay, there are three points I need to make it.

01:17:16.600 --> 01:17:24.360
First of all, first of all is AlphaGo does this tree search on the board game, right?

01:17:24.360 --> 01:17:31.240
And AlphaGo agent has the board game, has the model. But for Atari, I do not have the model.

01:17:31.240 --> 01:17:37.800
Yep. So I need to actually learn the model. So there's one difference from like this GANs

01:17:37.800 --> 01:17:45.160
and AlphaGo has actually the model of the environment. But for GANs, I need to learn the model

01:17:45.160 --> 01:17:52.760
of the environment. Second is for Monte Carlo tree search, in order to work, you do not need to have

01:17:52.760 --> 01:18:00.360
discrete like state space. They also work for continuous like state space. So the GAN tea

01:18:00.360 --> 01:18:06.120
like the holds before the continuous state space, it doesn't need that. But the thing which makes

01:18:06.120 --> 01:18:13.880
it easy for Atari again is like state space, hypothetically it's like continuous, but the transition

01:18:13.880 --> 01:18:19.720
is deterministic somehow. It's like if I play sequence, if I choose action left, I go to left,

01:18:19.720 --> 01:18:25.640
I'm not jumping somewhere else. So it's like, if we assume that this is a

01:18:27.000 --> 01:18:31.880
hypothesis, this hypothesis is that for Atari games, the state of space is continuous,

01:18:32.840 --> 01:18:37.240
but still the transition is deterministic. It's like going from one state to another,

01:18:37.240 --> 01:18:43.800
I mean, condition of current state and current action, I know which state I'm going to end up.

01:18:43.800 --> 01:18:51.080
So this is, it makes the life of the GANs easier, but the GANs is also able to handle the

01:18:51.080 --> 01:18:57.160
stochastic state transition. If I am at the state at some frame, if I choose action one action,

01:18:57.160 --> 01:19:01.800
if there is distribution of going to next state, GANs is able to handle that issue as well,

01:19:01.800 --> 01:19:07.800
but generally Monte Carlo tree search or opera confidence bound tree search, these algorithms,

01:19:07.800 --> 01:19:17.480
they do not require neither deterministic state transition nor finite state space. So they

01:19:17.480 --> 01:19:24.280
are able to handle continuous one as well. And so one thought that occurs to me is both the

01:19:24.280 --> 01:19:29.880
question as well as thinking through the implications of the question. You know, this process of

01:19:29.880 --> 01:19:37.720
generating this tree using GANs, what are the computational implications of this? And

01:19:38.680 --> 01:19:45.240
you know, assuming they're or imagining that they're significant, you know, I wonder if

01:19:45.800 --> 01:19:51.320
maybe it makes sense to do this like to bootstrap the deep reinforcement learning algorithm,

01:19:51.320 --> 01:19:58.280
but then once we gain more confidence, switch to something that's more like the approach we

01:19:58.280 --> 01:20:02.360
talked about before, you know, the Bayesian deep q networks or something like that.

01:20:02.360 --> 01:20:08.840
Sweet. You brought an interesting question. It's like, how how hard is to do this

01:20:09.560 --> 01:20:17.640
Monte Carlo tree search? How bad is the competition cost of doing this? The competition cost is bad.

01:20:19.080 --> 01:20:26.840
It makes the algorithm slow. Yeah, okay. Yeah. And but if I can get order of magnetic

01:20:26.840 --> 01:20:33.160
to sample complexity, then if I'm going to have a self-driving car, I would more compute to be

01:20:33.160 --> 01:20:42.360
able to not kill anyone. Okay. So here, the this GANs paper, that GANs work adds like a lot of

01:20:42.360 --> 01:20:48.360
competition cost. Like I think makes it like five times more compared to normal DQN,

01:20:49.320 --> 01:20:56.520
normal deep q network. But the thing is it's supposed to give you first of all better

01:20:56.520 --> 01:21:04.360
performance, second of all, like better sample complexity. Okay. So the general thing is we use

01:21:04.360 --> 01:21:11.720
Atari GANs as a test bed in order to see what our algorithms are doing. We are not using Atari

01:21:11.720 --> 01:21:18.040
GANs, some people they do, but I personally do not use Atari GAN, Atari GANs as a competition

01:21:18.040 --> 01:21:24.280
to say, hey, my numbers are better than yours. Or like my agent gets better scores than

01:21:24.280 --> 01:21:31.560
your agent. So mine is better. I'm not using like Atari GANs as to do this type of research,

01:21:31.560 --> 01:21:38.520
which is like probably you might not even call it research. I use so I try to develop algorithms

01:21:38.520 --> 01:21:42.680
and test them on Atari GANs to see what is their behavior. Yeah, sure. Generative

01:21:42.680 --> 01:21:49.800
adversaries research are really bad in the sense of competition costs, but they are supposed to

01:21:49.800 --> 01:21:56.280
give you better sample complexity and also better estimation of the value function.

01:21:56.920 --> 01:22:03.000
Can you define maybe formally sample complexity for me, because for you to say that

01:22:03.640 --> 01:22:09.160
something takes five times as long, but has better sample complexity to me,

01:22:10.280 --> 01:22:15.400
it sounds like a contradiction in terms. Oh, I see. So in reinforcement learning,

01:22:15.400 --> 01:22:20.920
there are like many components that you might be interested. One is like how much

01:22:21.560 --> 01:22:30.920
like wall clock hours you need to spend to find the good policy. So in this study, you don't care

01:22:30.920 --> 01:22:37.480
how many times you interact with the environment. You can have form of CPUs and GPUs,

01:22:37.480 --> 01:22:41.720
all of them are playing games and like they just give all the feedback to you and you come

01:22:41.720 --> 01:22:47.080
with a better algorithm. So if you just care about wall clock time, you don't care about

01:22:47.080 --> 01:22:53.320
the sample complexity. You just want to get to high performance in less number of hours

01:22:53.320 --> 01:22:59.640
of your wall clock. This study is about just engineering study, it's just about computation.

01:22:59.640 --> 01:23:06.040
So how much time you want to spend per day to be able to learn a better policy. This is an

01:23:06.040 --> 01:23:12.120
interesting line of study, but it's not my specialty and I'm not focusing on this one. There is another

01:23:12.120 --> 01:23:20.200
one is, so now there's another one is like says what objective function for DRL algorithms,

01:23:20.200 --> 01:23:30.600
I can define such that I get the best policy and also get the best, more reasonable policy

01:23:30.600 --> 01:23:37.560
and also reasonable value estimation. So there's another line of study that people have done a lot

01:23:37.560 --> 01:23:43.080
of research on it to come up with a better cost function or objective function. TQN uses the

01:23:43.080 --> 01:23:48.200
most naive objective function because the goal of deep-tune network paper was like hey,

01:23:48.200 --> 01:23:54.360
to show that it's feasible and it's possible to ask for human or be able to run something on

01:23:54.360 --> 01:24:01.560
a target. So there's not a line of research tries to come with a better objective function in order to

01:24:01.560 --> 01:24:07.080
estimate better policy and also estimate better value function. Even in this case, you don't care

01:24:07.080 --> 01:24:13.480
about the number of, in this setting, you don't care about, in the extreme case of this setting,

01:24:13.480 --> 01:24:18.360
you don't care about computation cost and also you don't care that much about the sample complexity.

01:24:18.360 --> 01:24:25.080
You are like, the people run the algorithms for like billions of time steps, okay? But there's

01:24:25.080 --> 01:24:30.760
another third line which is the most interesting part for me is like, can I get the same performance

01:24:30.760 --> 01:24:36.200
in stuff in one billion times of interaction with environment in like one million time steps?

01:24:37.080 --> 01:24:42.600
So one million times, so this is like literally I'm gonna, so if I have two kids, they play in the

01:24:42.600 --> 01:24:50.200
Atari's and they are playing that, that game, I just see which one is learning faster, but learning

01:24:50.200 --> 01:24:56.440
faster means that in the amount of hours they put to play game. If one kid is playing that game

01:24:56.440 --> 01:25:04.680
like like from 18 a morning to 9 at night and then does better than the other kid which plays

01:25:04.680 --> 01:25:12.040
it one hour per day, then I just, the way I compare the smallest of these two kids somehow might

01:25:12.040 --> 01:25:18.040
be like how many hours they play exactly the game, how many hours they interact with them with

01:25:18.040 --> 01:25:23.160
the game, right? How many times they press up and down? This is going to be the notion of something

01:25:23.160 --> 01:25:31.480
type of complexity. How many times my RL agent like interacted with environment in order to get to

01:25:31.480 --> 01:25:39.240
the optimal policy? So in other words, in this method you may take five times as long to figure

01:25:39.240 --> 01:25:46.440
out each step on a wall clock, but ultimately you're able to converge to a better policy

01:25:46.440 --> 01:25:54.440
taking less steps. Less steps in the game. In the game, right? Because actually the main goal of

01:25:54.440 --> 01:25:59.880
reinforcement learning is like when I deploy reinforcement learning for like autonomous vehicles,

01:25:59.880 --> 01:26:09.080
I would rather to use the whole AWS like compute to do not give anyone, right? So I'm in one

01:26:09.080 --> 01:26:15.160
content, it's for sure a complete compute, but main content is like developing a better algorithm

01:26:15.160 --> 01:26:22.840
which has less number of samples required to come with the optimal behavior. So this is like

01:26:22.840 --> 01:26:27.800
notion of sample complexity for this setting. I'm not using the exact theoretical notion because

01:26:27.800 --> 01:26:32.680
the exact theoretical notion has another meaning, but here somehow it's like how many samples

01:26:32.680 --> 01:26:40.200
I need to, if deep QNetwork solves a game in 200 minutes samples, I mean, get some performance

01:26:40.200 --> 01:26:46.680
in 200 minutes samples, am I able to reach the same performance in less than one million time

01:26:46.680 --> 01:26:53.160
steps and less than one million samples? The answer in the Bayesian deep QNet for paper is like

01:26:53.160 --> 01:27:01.240
custom games yet. You can reduce the number of interaction you need to make with the environment

01:27:01.240 --> 01:27:07.720
100 times less in order to get to the same performance as deep QNetwork, getting after 200

01:27:07.720 --> 01:27:13.160
minutes samples. This is a notion of sample complexity and this is the main part of theory that

01:27:13.160 --> 01:27:19.960
I'm also seeing in reinforcement learning in theory land that we mainly care about to reduce

01:27:19.960 --> 01:27:26.440
the number of samples we need in order to get to the nice and useful and reasonable performance.

01:27:27.080 --> 01:27:34.680
And so along the same lines, can you directly compare the sample efficiency of, you said,

01:27:34.680 --> 01:27:44.120
you know, deep Q might be 2 billion, Bayesian, you might be 1 billion. If you throw in the GANs or

01:27:44.120 --> 01:27:51.320
GATs approach, can you compare it directly in that same kind of metric? Yeah, of course, I can

01:27:51.320 --> 01:27:58.360
definitely compare it in the same metric. I have, like, I have compared it in just for one game

01:27:58.360 --> 01:28:05.960
in the paper, but as I said, seems like the composition complexity of GATs is a bit high,

01:28:05.960 --> 01:28:14.680
it's a bit beyond the power of academic research labs to do massive experiments. So if I was a

01:28:14.680 --> 01:28:20.120
deep mind, I would definitely have all the game. But since I am, like, using my advisors,

01:28:21.160 --> 01:28:28.760
AWS credit, probably I rather not burn the whole, the GPU clusters we have just for this

01:28:28.760 --> 01:28:36.440
board. So I just try to show a few games. Before those few games, like, how did it compare to the

01:28:36.440 --> 01:28:43.560
other couple of methods? Compared to deep Q network, for that game we tried, it reduced the sample

01:28:43.560 --> 01:28:49.880
complexity by half, like it converts to the same performance is like half of the samples required

01:28:49.880 --> 01:28:57.000
for deep Q network. But in order to make a empirical claim, definitely we need to try more than

01:28:57.000 --> 01:29:03.080
for sure one game. Right. So we are trying to come up with more experiments for more games

01:29:03.640 --> 01:29:09.320
and to come up with better analysis, better empirical analysis. We are not going to compare against

01:29:09.960 --> 01:29:19.400
EDQM or Bayesian deep Q network because the gap is exploring deep Q network inside of it.

01:29:19.400 --> 01:29:26.200
So because I have deep Q network and I'm building something on the top of deep Q network,

01:29:26.200 --> 01:29:31.240
I'm going to compare against deep Q network. I could use Bayesian deep Q network and use

01:29:31.240 --> 01:29:37.800
Gats on the top of that which is totally like this one line change in the code. I could be in the Gats

01:29:37.800 --> 01:29:44.040
on the top Bayesian deep Q network and compare against Bayesian deep Q network. This comparison is

01:29:45.000 --> 01:29:51.800
meaningful. But if I compare Gats with deep Q network and compare it with the Bayesian one,

01:29:51.800 --> 01:29:56.600
it's not that reasonable. The comparison is because we are comparing to somehow orthogonal

01:29:56.600 --> 01:30:01.800
effects. You gave the example. You said that like for a given game, you would expect that

01:30:02.520 --> 01:30:09.640
the Bayesian deep Q networks is roughly half the number of samples as regular deep Q networks.

01:30:09.640 --> 01:30:19.080
No, no, it's like 100 times some games better. So 100 X better. And so it was this that is the Gats

01:30:19.080 --> 01:30:24.520
that was half. Yeah, for one game. So for one, right, for the one game that you

01:30:26.040 --> 01:30:34.280
but I'm just saying something that Bayesian deep Q network just we studied to come with a

01:30:34.280 --> 01:30:41.000
better exploration strategy for deep reinforcement learning problem. At Gats, we are coming,

01:30:41.000 --> 01:30:48.120
we came up with the algorithm to reduce the error and bias in the learned value function. So

01:30:48.120 --> 01:30:53.480
these are kind of orthogonal effects. So the thing is I'm not going to compare these two together

01:30:53.480 --> 01:31:00.520
because these are two studies, not two orthogonal things. And I personally don't care that much

01:31:00.520 --> 01:31:05.240
which one does better because both of them can become blind together and come with new algorithm.

01:31:06.040 --> 01:31:10.680
Yeah, these are like we are in deep reinforcement learning. We are like far from claiming that we

01:31:10.680 --> 01:31:16.520
can use them in real world. So what we do, we are now going to hold the like majority of the

01:31:16.520 --> 01:31:23.560
fields. You are trying to study and come up with better and better algorithms. And these are

01:31:23.560 --> 01:31:32.520
algorithms are like studying because reinforcement learning like M4 is too big. And we need to do a

01:31:32.520 --> 01:31:39.000
lot of study to come up with to study different effects of different components in the setting.

01:31:39.000 --> 01:31:44.920
So we are not competing like if like in the vision community, we have ImageNet. Everyone

01:31:44.920 --> 01:31:53.720
tries on ImageNet and compare the results of one algorithm to another one. But here we are not doing,

01:31:53.720 --> 01:32:00.440
I mean many people they do compare their numbers but I personally do not do compare my numbers to say

01:32:00.440 --> 01:32:09.960
this is better than the other. We are just studying the effects of different components in the

01:32:09.960 --> 01:32:17.160
environment. And we are in the baby stage of like reinforcement learning. We are like getting

01:32:17.160 --> 01:32:25.480
things to see what what what each change would do. We have very very far from to get to real

01:32:25.480 --> 01:32:29.960
competition to say these algorithms better than the other ones. So in other words, you know,

01:32:29.960 --> 01:32:37.000
it's just kind of exploring ideas about you know what levers we even have available to us to tweak

01:32:37.000 --> 01:32:42.360
if we cared about maximizing performance. But we're so early on there's really nothing to maximize

01:32:42.360 --> 01:32:48.280
performance against. There's no ImageNet or something that we're you know someone who's building

01:32:48.280 --> 01:32:53.000
a self-driving car, you know, they might want to take all of these different approaches and

01:32:53.000 --> 01:32:57.000
you know, apply them together. But that's a lot of work and that's their issue, right?

01:32:57.000 --> 01:33:01.880
Yeah, they're going to combine everything together. You can imagine like like coming up with

01:33:01.880 --> 01:33:09.240
better like exploration and strategy like console sampling. It's obvious that someone should have

01:33:09.240 --> 01:33:15.080
tried this one like five years ago, right? But no one has tried it because we are like very very

01:33:15.080 --> 01:33:19.640
like this is a fundamental component in the reinforcement learning algorithms. Like the exploration

01:33:19.640 --> 01:33:26.120
is like one of the most important and actually I can say even main component of the reinforcement

01:33:26.120 --> 01:33:31.160
learning algorithms. But no one has tried this one. Why? Because we are in the early stages. We are

01:33:31.160 --> 01:33:38.840
still don't know and don't get to know like what are the fundamental components of the algorithm

01:33:38.840 --> 01:33:45.080
is. Like for ImageNet, we know mainly that we need to have this convolution layers. We need to have

01:33:46.040 --> 01:33:53.560
like softmax outputs. We can use cross entropy loss. These are the common thing that everyone uses

01:33:53.560 --> 01:33:58.440
and it works very well. But in reinforcement learning, we don't have this all components

01:33:58.440 --> 01:34:04.280
all together. We don't know what is the best way of doing exploration and exploration together.

01:34:04.280 --> 01:34:09.400
We don't know what is the best function approximation we should use. We don't know what is the best

01:34:09.400 --> 01:34:15.240
memory we should use. We don't know what we don't expect. We are like really really far from

01:34:15.240 --> 01:34:21.560
making. So we're just reading that even before the baby stage, we are not even got born. We were not

01:34:21.560 --> 01:34:28.840
born. Awesome. Well, Camyor, you've been very, very generous with your time. Thanks so much for

01:34:28.840 --> 01:34:35.320
taking the time to chat with you about all this stuff. It was a lot of fun and I think folks will

01:34:35.320 --> 01:34:40.440
enjoy this and learn a lot. I really appreciate it. Awesome. Thank you. Oh, thank you. Have a great day.

01:34:43.960 --> 01:34:49.160
All right, everyone. That's our show for today. For more information on Camyor or any of the

01:34:49.160 --> 01:34:55.560
topics covered in this episode, head over to twimmolai.com slash talk slash 177.

01:34:56.600 --> 01:35:01.000
If you're a fan of the podcast, we'd like to encourage you to visit your Apple or Google

01:35:01.000 --> 01:35:07.000
podcast app and leave us a five star rating and review. Your reviews help inspire us to create

01:35:07.000 --> 01:35:14.040
more and better content and they help new listeners find the show. As always, thanks so much for listening

01:35:14.040 --> 01:35:22.280
and catch you next time.

