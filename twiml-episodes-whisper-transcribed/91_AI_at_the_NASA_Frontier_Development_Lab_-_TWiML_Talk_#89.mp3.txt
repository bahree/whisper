Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast, we're featuring a series of conversations from the Nips conference
in Long Beach, California.
This was my first time at Nips and I had a great time there.
I attended a bunch of talks and of course learned a ton.
I organized an impromptu roundtable on building AI products and I met a bunch of wonderful
people including some former Twimble Talk guests.
I'll be sharing a bit more about my experiences at Nips via my newsletter, which you should
take a second right now to subscribe to at twimblei.com slash newsletter.
This week through the end of the year, we're running a special listener appreciation contest
to celebrate hitting one million listens on the podcast and to thank you all for being
so awesome.
Tweet to us using the hashtag twimble1mil to enter.
Everyone who enters is a winner and we're giving away a bunch of cool Twimble swag and
other mystery prizes.
If you're not on Twitter or want more ways to enter, visit twimblei.com slash twimble1mil
for the full rundown.
Before we dive in, I'd like to thank our friends over at Intel Nirvana for their sponsorship
of this podcast and our Nips series.
While Intel was very active at Nips with a bunch of workshops, demonstrations and poster
sessions, their big news this time was the first public viewing of the Intel Nirvana
Neural Network Processor or NNP.
The goal of the NNP architecture is to provide the flexibility needed to support deep learning
primitives while making the core hardware components as efficient as possible, giving
neural network designers powerful tools for solving larger and more difficult problems
while minimizing data movement and maximizing data reuse.
To learn more about Intel's AI products group and the Intel Nirvana NNP, visit Intel
Nirvana.com.
In this episode, I'm joined by Sarah Jennings, Timothy Sebrick and Andres Rodriguez to discuss
NASA's Frontier Development Lab or FDL.
The FDL is an intense eight week applied AI research accelerator, focusing on tackling
knowledge gaps useful to the space program.
In our discussion, Sarah, producer at the FDL, provides some insight into its goals
and structure.
Timothy, a researcher at FDL, describes his involvement with the program, including
some of the projects he worked on while on-site.
He also provides a look into some of this year's FDL projects, including planetary defense,
solar storm prediction and lunar water location.
Last but not least, Andres, senior principal engineer at Intel's AI products group joins
us to detail Intel's support of the FDL and how the various elements of the Intel AI
stack supported the FDL research.
This is a jam-packed conversation, so be sure to check out the show notes at twimlai.com
slash talk slash 89 for all the links and tidbits from this episode.
And now on to the show.
Alright everyone, I'm here in Long Beach at the Nip's conference, and I have the pleasure
of being joined by Sarah Jennings, who is the producer and NASA's Frontier Development
Lab, Timothy Seabrook, NASA Frontier Development Lab researcher via the City Institute at University
of Oxford, and Andres Rodriguez, senior principal engineer at Intel's AIPG.
And I am really excited to have a chance to talk a little bit about this program that
you're all involved in and share it with the folks listening to the podcast.
So welcome everyone.
Thank you.
Happy to be here.
Absolutely.
Why don't we get started by just kind of going around the table and doing intros.
We'll start with you Sarah.
Great, so I'm the producer for NASA Frontier Development Lab, which means I get the really
cool job of working with the researchers for different challenges over the summer throughout
our program.
And previously I've worked in emerging tech and innovation at X-Prize, where I did prize
design and operations, and I've done a lot of work with the private space industry
as well.
Awesome.
Awesome.
Andres?
Yeah, thanks for having me here.
As engineering and intelligent engineering tell me with various customers to understand their
workloads and build solutions for them.
And I work with a lot of other Intel teams to provide to build a solution for our customers.
The solutions may be anywhere from designing models for their particular problems.
So deep learning models or optimizing their algorithms so that they run efficiently on Intel
architecture.
I've been working on AI for about 13 years in an exciting area to be especially over the
past four years, five years to see the tremendous growth and interest for the community in this
area.
Absolutely.
How about you, Tim?
So, as you mentioned previously during my PhD at the University of Oxford, I come from
quite a multi-disciplinary background.
So I've studied under engineering, computer science and communication systems, and that's
given me quite a broad view of technology and also of the machine learning field.
So right now I'm focusing and I'm super interested in verification of learning and multi-agent
systems so that hopefully we can have a bit of specification about these learning agents
so that they might be able to be deployed and used in public grounds because of the minute
you know, it can be a little bit risky to put something out that learns without knowing
what it could learn.
So that's my focus there.
Yeah, I was super happy to go along to the Frontier Development Lab over the summer being
based at NASA.
That was a great opportunity and a great experience and I'm happy to be continuing involvement
with that going forward.
And what was your role with the program?
I was a researcher there.
I applied through the normal means, the application process, got to meet James and Sarah first
online and then in person.
So there were a number of projects that were involved at the Frontier Development Lab and
I was involved in one of those which was the exploration of lunar water and volatiles.
Okay.
Actually, I'll talk a little bit, I guess a little bit about that now.
We can dig into that in a second.
For now probably makes sense to get a little bit more context on FDL as a whole and what
the mission is, Sarah, what can you tell us about FDL?
Yeah, so FDL is an applied artificial intelligence research accelerator and it was kind of
started with the premise of bringing back the Apollo era of interdisciplinary teams paired
with rapid innovation and so during that time they would bring together the comms team
with the life support team and the propulsion team all in the same room so that they could
work on advancing things faster.
And so that was really where this started from.
It started in 2016 with asteroid grant challenge where they mostly focused on planetary defense
problems and now we're moving on to your three because the program has been so successful.
So we bring together researchers from AI fields and we pair them with planetary scientists
and we work on challenges that are of interest to NASA.
Hmm.
And now I've heard this description before and every time planetary defense is said I
think of like defending us against aliens.
What does that mean to you?
Yeah, so the challenges that we focused on were asteroids and long period comms for this
past year.
Okay.
So it's mainly detection and understanding them a little bit better.
So if there was an asteroid that was coming towards us we'd have a little bit more of understanding
of mitigation strategies and things like that.
Okay.
So it actually is protecting us from things that are hurling towards us from space.
And so tell us about the program, what's the structure of the FDL program and how, you
know, how's it organized, who's involved?
Yeah, so it is a public private partnership.
So it's with NASA Ames and hosted at the study institute and then we are able to bring
a lot of our private partners and which allows us to bring in researchers and mentors from
around the world.
It is an eight week program that takes place out at NASA Ames and Silicon Valley where
we bring in about 24 researchers and put them on teams of four.
For that eight week program it starts out with a bootcamp where everybody gets an understanding
of all the problems and then it goes into brainstorming different approaches to work
on solutions for them.
Okay.
And then they do rapid prototyping during that time.
But then after the program ends it doesn't, the work doesn't end.
We do a lot of continuity work and Tim will talk a little bit more about what he's doing
with Intel later.
Okay, and the researchers that you've referred to, they're all or predominantly PhD students
or it is a very.
It varies.
We had a mix of PhDs, postdocs, and individuals in the industry that actually took sabbaticals
from their job.
Oh wow.
And we had an astronomer from Brazil, so it's quite a combination.
It's a really high caliber of people that we bring in.
Oh, very cool.
Very cool.
So Tim, why don't you give us an overview of the types of research that the different teams
worked on?
Sure, so I'm sure you're aware there's a lot of excitement around the field of deep learning
at the moment.
Really?
I'm not sure if you noticed.
Yeah, so I guess to start off with expanding what Sarah mentioned was, it seemed like the
goal of the Frontier Development Lab was really to empower talented scientists who have
been working the field for a long time with these new technologies and enable them to
achieve more than they've been able to previously.
So we were very much looking, and the bootcamp was incredibly helpful to be talking to
NASA scientists and really understand the state of the game when it comes to space and
what's been done and what needs to be done next.
Was that an area that you had any experience with going in?
I mean, as a hobbyist, you know, we're all nerds at the Frontier Development Lab.
We all have a little bit of space, a bit of Star Trek, a bit of Star Wars and stuff
like that.
I don't know, yeah, as a hobbyist from Space News, I was fairly aware of things, but not
to the depth of NASA scientists, of course, have been working a field for 40 years.
Throughout that bootcamp week, we really did come to understand what was going on and sort
of did have to explore where we could contribute.
So we had the artistic freedom to explore and do the prototyping and work out where our
skills as individuals aligned with the needs of the group.
So each individual team came up with their own custom solution to the problems they
were facing.
So a lot of it was, so for my team, it was image classification, similarly for long period
comments, looking at cameras situated around the world.
Sarah, do you know what the name of the camera's project?
Cam's project.
Okay.
Yeah, so what the goal of that is is to detect objects coming through the atmosphere and
identifying when it isn't object and it's not just a bat or a firebug that's gone in
front of the lens and doing that autonomously because people have been doing that for 40
years by hand, which is incredibly arduous.
The asteroid shape modeling team were using variational auto encoders to try and transfer
Doppler shift images, which are incredibly unintuitive into a 3D model so that the shapes
of asteroids and the distribution of the shapes of asteroids can be better understood
the team was working on invasion techniques to ensure that there was, to encode the knowledge
of the NASA scientists into a prior distribution so that the shapes that came out made sense.
So the weather prediction teams, they were looking at predicting when a coronal mass
ejection might come out, which would be quite catastrophic if it does, when a coronal
mass ejection.
So when a magnetic band within the sun sort of snaps and it ejects a huge plume of
heart plasma through the solar system and the Earth luckily has a magnetic field that
protects us from most of that, but if we get hit by a 1 or 2 or several waves then it
can be quite catastrophic and damage satellites of course, it could be dangerous for people
on space stations.
How often does this kind of thing happen?
So there was the Carrington event in...
The Carrington event.
Carrington event.
Yeah, that was the...
No relation to Carrington.
My last name.
Yeah, that was the last major one, but they haven't happened quite frequently.
So I think it was around late 19th century, so there wasn't too much electricity around
by then, so it wasn't too damaging to that.
It went largely under my notice, apart from in Canada I believe, it was where it was
noticed the most.
So that caused some great northern lights to show up, so people were really loving that,
but it would be a little bit more damaging now.
But yeah, I believe there are smaller events happening quite a lot, and I mean, we're
a small dart in a solar system, so a lot of the time they'll miss us, which is fortunate.
Yeah, I mean, in day-to-day life, they can affect things like GPS, mobile phone signals,
things like that.
Okay.
So we really need to have a prediction of when one of these events might happen, and
the way you do that is by monitoring the hyperspecial images of the sun's surface
what you can see.
So we had the team there was using LSTM, which is a type of neural network, often used
for neural linguistic programming.
It's long short-term memory, that's what the LSTM stands for, so that takes into account
the long-term history of the sun's activity as well as the short-term, so it is quite
useful in financial predictions as well.
We also had the other teams, so the terrestrial interaction, they had a hard time.
Terrestrial interactions?
So the terrestrial.
Terrestrial interaction.
So that happens when the solar activity does hit the earth.
And how that might affect electrical companies, how that might affect markets and things
like this.
And it's really difficult to get a hold of the law of that data.
I mean, and then just for that problem or for all of these problems that we've discussed?
Well, luckily there is a huge wealth of public data for most of these things, particularly
when it comes to looking at the moon.
It's up there we've got, it's fortunately NASA's a public entity.
So all the work they do, well, a lot of the data that they do, they do, they do, they
are a lot of the data that they do is available to the public, which is great for hobbyists.
So yeah, the solar terrestrial interaction team was pulling as much data as they could
from disparate sources, from insurance companies, from claims of damage that have been done
to transformers and things like this and trying to find a model to describe, particularly
what the cost of damage could be if one of these events happens, so they ended up using
a wide variety of techniques and machine learning and, for instance, and funding together
and see where they could come out with.
And I guess last but not least the work that the team I was on was working on.
We had a good team.
There's Deep Mar Backus, who is a, he does a lot of mapping problems and a lot of, yeah,
a lot of synthesis of maps and working on those.
Eleni Beowichek, who does computer vision for Mars Rovers.
We had Nate Amusa, who was one of those, for those from industry, who comes from a strong
understanding of computer vision.
Myself and also we were supported by Tony Dubovowski, who is a planetary scientist, particularly
interested in creative formations, which was incredibly useful.
And so what was your project?
So the project was to find a way to facilitate the investigation and the quantification
of how much water or volatile is refocused on water is on the lunar surface.
And there's potentially quite a lot.
I was surprised to learn this because you know, there's a lot of discussion about, maybe
there's water on Mars, maybe there's water on one of the moons of Jupiter or Saturn.
But it seems there's a lot, quite a lot closer to home.
Sarah, I'm curious from your perspective, why is, you know, in one sense, there's, you
know, it's obvious that we'd be interested in exploring, you know, water on the moon,
but how, you know, what are, how does NASA think about the reason why this is important
and maybe you can also touch on some of the, you know, how do we know there's water
on the moon?
Yeah.
So I can touch a little bit on that without overstepping, but so the reason NASA is focused
on the moon currently is because with this new administration we're focused on going
back to the moon.
So the reason why later resources is important and Tim can also share a little bit more about
that is that those resources are very expensive to move from the earth into space.
And so if you already have those resources there, then you could utilize them for fuel
or for other things potentially like settlements and things where you don't have to import those
materials there.
Mm-hmm.
So that's another, another way there.
Okay.
And I heard an anecdote about the first discovery of water on the moon, like how do we,
Oh.
Yeah.
So it was the lacrosse mission.
So the Apollo, some Apollo 15 rock samples showed water in them, but, you know, there was
disputed whether or not they've been contaminated and things like this.
Right.
So there was seeking for further evidence.
So there were a couple more missions, one from NASA, one from the Indian Space Agency,
which involved sending an impactor into a crater on the South Pole that never sees sunlight.
So I can explain a little bit maybe why that was the reason, but why that was chosen.
So the impactor wouldn't hit the surface, it was basically crashing a ship into a vehicle
of some sort into the side of the crater.
Yeah.
And the ejecto plume that came out, all the dust that was kicked up, spectral reading
was taken of that and showed, I believe, about 4.6 percent water in a form of ice particles.
I imagine.
Yeah.
I'm not sure because the impact of the energy might have vaporized some of it, but yeah,
in its stable form it would have been ice, yeah.
So yeah, the reason that they were targeting permanently shadowed craters is because I
mean, if you consider that of the last 4 billion years, every meteor and comet that's impacted
on the moon has had the potential to contain some rare earth metals and some water.
All of that's been deposited.
Some of it will evaporate, some of it will escape the atmosphere, but some of it will get stuck
in the bottom of the craters that are at 40 Kelvin.
So anything that goes in there freezes and gets stuck there potentially forever.
Okay.
So it's considered that might be, you know, the great wealth, the bank of resources that
was available on the moon.
And the role of your team was to basically figure out how much of this stuff might exist.
And the role of our team was to work out how we could contribute to the accessing of
it, right?
Using machine learning.
Right.
So yeah, we started off.
So right now, as I understand, an objective would be to exactly investigate, understand
how much there is, understand if it's economically viable to set up a base there or to send
further missions, the way you do that is with rovers, is with in situ resource measurements.
So we were looking firstly, looking at traverse planning, rovers have to stay in sunlight
all the whole time with a solar powered or for as long as possible anyway.
They have to stay in direct communication with the earth a lot of the time, so the moon's
quite close.
So usually it's still human operated.
When there's a rover up there, it's only a second or two delay.
So we were looking at that problem and then we realized that, you know, there were some
other work that needed to be done before that to facilitate that sort of advancement.
And what was that other work?
So in doing the traverse planning, we realized that it's the maps weren't in the highest
quality that we had come to expect, you know, maybe lately we thought the moon's just above
us that the maps should be complete, right, right, regions where there is permanent shadow,
visual images are difficult to to grab and also as the, you know, reconnaissance orbiter,
which is where we took our data from, as that's orbited the moon, thousands and thousands
of time and built a laser altimeter sort of height map readings, a stitching and the composite
images that are formed have created artifacts which to a rover and a rover's eyes look like
20 foot clips and 20 foot digits.
So when you're trying to...
They aren't actually there.
That may not be.
Yeah, actually there.
Yeah, but a lot of them aren't.
So you can just smooth over because, I mean, a rovers an expensive thing to put on a moon
you don't have to fall down one of these digits, if it is real, but also you can't do an automated
plan until you've worked out which ones are your own which ones aren't.
Okay, so that became the focus of our continued work.
Okay.
And there was also an element of your work that involved trying to count the number of
craters on the moon.
So where did that come in?
The lunar surface.
I suppose every time a picture of the lunar surface has been taken, there's no GPS,
right?
So you don't know exactly where that photo is.
But thankfully, craters being so abundant, form a sort of fingerprint that allows you
to uniquely identify what you're looking at.
And by matching that in a visual image with the elevation model, then you can compare
what the artifacts might be in the elevation model with the visual images.
And then if it's in one but not the other, then you know it doesn't exist.
If it's in both, then you know, okay, that is a ditch that I need to watch out for.
And maybe going back to more fundamental kind of space question, like how many craters
are on the moon?
Oh gosh.
More than we can count.
More than we can count.
Are these craters that were all created with the creation of the moon itself or are these
craters a result of impact like comments or other things?
So I believe there's 10 notable fresh impacts per day.
So we looked at around 40 to 100,000 craters.
And that was around 1,000th of a percent of the lunar surface.
Wow.
Yeah.
They're coming every day.
But I think a lot of it was historical.
A lot of it has been in the past and it's come down a bit.
But it's still going.
There's still impacts every day, yes.
Okay.
Is the earth impacted that much by craters?
Is it our magnetic field as you were describing earlier that protects us from that?
Or do we get our fair share of those crater impacts as well?
Like certainly the earth is in pocket with craters like the moon is.
So we do get a lot of things coming through the atmosphere.
You might see a shooting star every now again that's a small meteoroid coming through
the atmosphere.
The atmosphere is another protective shield for us.
You know, the friction that the meteoroid comes under as it's entering causes it to burn
up a lot of the time or break apart.
So we do have that minor protection.
Yeah, the planetary defense team, they're really looking at bigger asteroids that could
be more deadly.
So I think it was 10 years ago we thought there were maybe 16,000 of these.
Now we know there's hundreds of thousands of potentially dangerous objects floating
around the space.
Luckily we've gotten much better at modeling where these are and understanding which ones
could be dangerous and it looks like we're in a clearer at least for now.
So to kind of take a step back, you're trying to ultimately understand the available resources
on the moon in order to do that.
One of the techniques involved was trying to understand actually was trying to plan out
the rover mission or provide information that would be used in planning a rover mission.
In order to do that, we need to understand how the maps all fit together.
And in order to do that, we need to be able to identify the craters.
Yeah.
So this was really a keystone that would open up the possibility for future works to go
ahead.
And so what were the techniques and challenges involved in identifying the craters?
I mean, main challenges came to even understanding the problem.
I mean, it took a lot of, thank God we had the prototype in process.
So we only came to understand that problem by trying to do the diverse plans.
When it came to it, I think we were five weeks in program already.
There were three weeks left.
So we had a lot of late nights labeling all the data.
So for any machine learning algorithm, you need to give it training examples, right?
You need to teach it as though it's going through school.
Right.
So we had to collect images, representative images of craters and come to an
agreeance of what the crater looks like so that we could feed that to our algorithm.
Okay.
That was surprisingly difficult actually.
We had five people on the team and several others were helping us label these images.
And to come to a consensus on which should be considered to be good representations of
craters and which weren't and which were too ambiguous was a long process actually.
Meaning full-fledged craters versus, you know, dips in the terrain or something like that.
So one interesting thing we came across was that actually hills look exactly like craters
depending on the angle of the light.
So that was a little bit, after you were looking through thousands of these, everything started
looking like a hill and you weren't sure anymore.
You have to take a break every now and again to, you know, let me perception rest again.
So yeah, it was the sheer number of training examples that we had to label ourselves and
then.
And so how many of these craters did you personally label?
I looked through 40,000.
Wow.
There was a bottle of scotch nearby that definitely helped me through that process.
Yeah.
So we got through that.
And then once we got to that training data, then it went through the iteration process of
developing a computational network classifier, and thankfully we had Intel, Nirvana, and
the technology there to help us go through that process quickly and we had the support
of the Intel engineers to give us advice and support on how to use their framework most
effectively.
Was there anything that came up as kind of unique about training a model in this situation
that you wouldn't expect in other, you know, applications of CNNs?
I would imagine one of the things that is unique compared to what you usually find in academia
you're focusing on two aspects.
One is it's a binary, is this a crater or not?
You're not trying to classify between thousands or hundreds of classes.
And the second one is you are also doing detection.
You don't, you're looking for the craters in large images.
So the project essentially has these two steps.
First, can you classify craters versus non-creators and second, can you then detect them in a large
image?
I don't know.
If you have other things to add to.
Yeah.
So one of the particular problems with the crater detection was that there's just so many
of them and they're all overlaying on each other and that's quite a difficult image classification
problem.
So if you're doing facial recognition, for example, you don't expect to see a face in
a face in a face in a face or like a cluster of faces altogether.
So that was definitely a challenge for the image classifier.
Andres, maybe give us some perspective on why does Intel support a program like this?
How does Intel think about engaging with programs like FTL?
Intel wants to advance science and this is a great opportunity for Intel to partner
both with researchers, with NASA engineers, with the NASA program to advance science.
And in addition, Intel wants to democratize machine learning.
So it was a great opportunity to work with engineers that many of them had not to not have
a background in machine learning and give them the knowledge and the tools to use machine
learning for a particular problem.
And how did your team in particular get involved with the projects?
So we were invited, one of our team managers received an invitation for Intel to participate
and we were very excited to join the opportunity.
I went to the kickoff meeting and got to meet the researchers in the program and invited
a couple of my colleagues to come and participate.
One who couldn't be here unfortunately, his name is Najib Hakim, who was the main Intel
engineer, his principal engineer, with a strong background in machine learning and deep
learning.
And he provided a lot of the mentorship and guidance that the engineers needed, along
with Ravi, Panchamarthi and Hanlin, they provided additional assistance, engineering assistance
to the teams.
You mentioned a moment ago the kind of the unique challenges of this problem with regard
to clusters of craters and craters and all that kind of stuff.
How did you overcome those problems?
So thankfully the problem that we were trying to solve was the co-registration improvement
of maps.
So so long as we could get a reliable fingerprint for an image that could be matched in the
elevation model, then that would satisfy our goals.
So when there was particularly rough areas, then it didn't matter so much that maybe the
quality of our image classified suffered in those regions because we were able to make
up for it elsewhere.
So it would definitely be a problem for research question almost and when it comes to building
an application in a short period of time, sometimes we had to make do with what we had and
do it quickly, rather than trying to buy it off more than we could chew an identity
up, not contributing anything.
Right.
Right.
And so how do you envision, I understand there's an ongoing role for you in particular in
this project with FDL, how does it move forward?
So I mean, identifying the issues that we had with the time constraints, we all invited
individually to continue to contribute if we would like to.
I think for many of us, it's, you know, this really stirred up our passion and our excitement
about the whole field.
So since the end of the project, we've been working to, you know, tidy up the code, make
it publicly available, and we've been working alongside the folks at Intel to improve
the algorithm to be able to detect clusters of creators and creators within creators using
our newer techniques, single-shot detector, it's a modipox.
Which allows you to identify lots of creators in the same image.
So moving forward, I suppose we're looking forward to a fronted development of 2018, we're
going to be taking applications for that currently up until March.
And really what we're looking for is to see what ideas people come with and how the
ways that they can contribute to the continuation of this project.
Andreas, were there any kind of unique properties of the Intel AI stack that lent themselves
to solving this problem or that, you know, helped in helping these research teams advance
their research?
Yeah, until we have the full solution stack anywhere from starting at the bottom with
our hardware, we have our general purpose CPU, such as young processors, we're also developing
a deep learning accelerator, specific target for the learning world, that was not available
in 2017 when these researchers were working on this problem, we hope that they'll be able
to use it in the following summer, but we have our CPUs in addition to that, we have libraries
that are optimized to run deep learning functions efficiently on our CPUs, you might have heard
of the Intel math kernel library, we in addition we have deep learning frameworks that we've
optimized, such as cafe, TensorFlow, MxNet, and we have an in-house framework called NEON,
which was the framework that was made available, FTL for the researchers to use, NEON's unique
in the sense that it's a framework that is highly optimized, but Intel for both CPUs and
CPUs, you can get very high throughput and high utilization, regardless of the hardware
backend that you use, and in addition to that, we actually have a Nirvana offering called
the Intel AI cloud, it used to be called the Intel Nirvana cloud, and this cloud where
our partners can log in and easily load their jobs and they have the tools to easily experiment
with various models and various hyperparameters, meaning various knobs that you need to often
tweak in order to get your models to officially train.
So we spend some time getting the engineers and have the alab to speed on how to use the
tools, but once you spend a few days or even less a couple of days or day, you can start
training your own models, so you usually start by training a simple model like Lynette,
which is an all model that was developed a couple decades ago, and you can do that
your first couple of hours, you train that model on image recognition, and then you can
actually modify it a little bit to start getting a baseline on how well does this work
on our crater images, and do I need to add more layers, do I need to add more units or
neurons in its layer, once you can do crater classification then you can move into detection,
which as Tim was talking about, initially we use single shot detection algorithm or SSD,
and there are newer algorithms that are better for this type of workloads, where you have
small craters, some large craters, and you don't have a diversity of classes, you're
trying to classify, and so that's going to be the next step of classifiers or detectors
that we're going to be using.
Awesome, awesome.
This sounds like an amazing program, maybe we can close out by having you Sarah tell
us like for folks that want to learn more about participating in the 2018 program, is there
a site they can go to, or what's the process there?
Yeah, so if you're interested in the program, you would go to frontchairdevelopmentlab.org,
and like Tim said, we do have our applications open, we're currently doing our planning
for next year, and so our challenges should be launched here soon as well.
Okay, and Tim from you, any final thoughts or advice for folks that might be interested
in pursuing, either getting involved in FDL or doing similar research?
Yeah, I'd say, just go for it, you know, I grew up in the UK, I never imagined for a
second that I could be working alongside NASA scientists or living on a NASA facility.
Similar program was absolutely excellent.
If you're wanting to get involved in machine learning, start reading online, take small
goals, keep assuring them.
It's very easy to be like, should I learn this machine learning algorithm today, or
should I watch another season of my favorite TV show, put short-term goals, keep going
for it, and you'll be able to contribute in this new and exciting field as well.
Awesome, awesome.
And how about from you, any parting thoughts?
Yeah, in total doing a lot of exciting work to advance AI, and you can check out what
we're doing at internalrunner.com, in addition of developing this new, the Blurnexelerator,
we've optimized the framework, so if you were to do deep learning on CPUs, today you'll
see significant gains in performance, and if you were, if you had tried to do deep learning
on CPUs a year ago, or even six months ago, so it's an exciting time.
We want to democratize the use of deep learning, and most facilities have Intel CPUs, so we
hope that they can be put to good use, such as we did with FDL.
On that note, for the hobbyist, Intel does have an AI bootcamp that provides a lot of
resources available for people to learn how to use Intel Nirvana, oh, Intel AI, I think
it's called now, and yeah, get involved and get their hands dirty with the final points
of learning machine learning.
Awesome, as well as the dev cloud offering, which I did an interview, I forget which episode
it was, but we'll put that in the show notes as well, folks can sign up for access to that
too.
Thanks.
Awesome.
Well, Sarah Andres, Tim, thanks so much for taking the time to chat.
Thank you.
Thank you for trying that out.
All right, everyone, that's our show for today.
Thanks so much for listening, and for your continued feedback and support.
For more information on Sarah, Timothy, Andres, or any of the topics covered in this episode,
head on over to twimlai.com slash talk slash 89.
To follow along with the nip series, visit twimlai.com slash nips 2017.
To enter our twimla 1 mil contest, visit twimlai.com slash twimla 1 mil.
Of course, we'd be delighted to hear from you either via a comment on the show notes
page or via a tweet to at twimlai or at Sam Charrington.
Thanks once again to Intel Nirvana for their sponsorship of this series.
To learn more about the Intel Nirvana NNP and the other things Intel's been up to in
the AI arena, visit intel Nirvana.com.
As I mentioned a few weeks back, this will be our final series of shows for the year.
So take your time and take it all in and get caught up on any of the old pods you've been
saving up.
Happy Holidays and Happy New Year.
See you in 2018.
And of course, thanks once again for listening and catch you next time.
