Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charington.
Just a couple of quick announcements today related to the Twimble Online Meetup.
First, the video from our December meetup has been posted and it's now available on our
YouTube channel and at twimbleai.com slash meetup.
It was a great meetup, so if you missed it, you'll definitely want to check it out.
But you definitely don't want to miss our next meetup either.
On Tuesday, January 16th at 3 o'clock Pacific, we'll be joined by Microsoft Research's
Timnett Gebru, who will be presenting her paper using deep learning and Google Street View
to estimate the demographic makeup of neighborhoods across the United States, which has received
national media attention for some of its findings.
Timnett will be digging into those results as well as the pipeline she used to identify
22 million cars and 50 million Google Street View images.
I'm anticipating a very lively discussion segment as well to kick off the session, so make
sure to bring your AI resolutions and predictions for 2018.
For links to the paper or to join the meetup group, visit twimbleai.com slash meetup.
Alright, onto today's show.
In this episode, we hear from Kenneth Stanley, professor in the Department of Computer Science
at the University of Central Florida and senior research scientist at Uber AI Labs.
Kenneth studied under twimble talk number 47 guest, Risto Mikulainen at UT Austin, after
geometric intelligence, the company he co-founded with Gary Marcus and others, was acquired
in late 2016.
Kenneth's research focuses Neuroevolution, which applies the idea of genetic algorithms
to the challenge of evolving neural network architectures.
In this conversation, we discuss the Neuroevolution of Augmenting Topologies, or Neat, paper
that Kenneth authored along with Risto, which won the 2017 International Society for Artificial
Life's Award for Outstanding Paper of the Decade 2002-2012.
We also cover some of the extensions to that approach he's created since, including
Hyper-Neat, which can efficiently evolve very large neural networks with connectivity
patterns that look more like those of the human brain and that are generally much larger
than what prior approaches to neural learning could produce, as well as novelty search,
an approach that, unlike most evolutionary algorithms, has no defined objective, but
rather simply searches for novel behaviors.
We also cover concepts like complexification and deception, biology versus computation,
and some of his other work, including his book, and Nero, a video game, complete with
real-time neural evolution.
This is a meaty, nerd alert interview that I think you'll really enjoy.
And now on to the show.
Alright, everyone, I am on the line with Kenneth Stanley.
Kenneth is a professor in the Department of Computer Science at the University of Central
Florida, as well as a senior research scientist at Uber AI Labs.
Kenneth, welcome to this week in Machine Learning and AI.
Thanks very much.
Real happy to be here.
Fantastic.
Why don't we get started by having you tell us a little bit about your background?
Sure.
I've been interested in artificial intelligence since I was a little kid, maybe around
eight years old, went on to major in computer science because of that, and carried that interest
into graduate school, where I was at the University of Texas at Austin, where I did my PhD,
and there I became interested in particular in neural networks, artificial neural networks,
which are what are now the basis of deep learning, which everybody's talking about.
And also what's called evolutionary computation, which means kind of Darwinian type of principles
being applied inside of computer algorithms.
And so the intersection of those two things is what's called today, neural evolution.
So it means like evolving neural networks or like evolving brains, you could think of
it as in a computer.
And I guess my particular interest is just how brains evolved, you know, these amazing
astronomically complex things that are in our heads.
I was always fascinated by how an unguided process, seemingly an intelligent process like
evolution could just produce something.
So astronomically complex and amazing as our own brains.
And so as a neural evolution researcher, I've been trying to figure out how can you actually
make algorithms that would evolve something of similar scale and complexity?
Was there anything in particular that you came across at the age of eight or so that
got you interested in AI?
Yeah, yeah.
So at the age of eight, that's when my family bought a computer.
It was like a Commodore 64.
Yes.
And it was, it was also I, my parents put me in a programming class.
And that was on a TRS 80, which is a very old computer system.
And.
Flash 80.
Yeah.
Exactly.
And I guess for some reason, like, as a little kid, it just really made an impression on
me that I could tell the computer to do anything.
Like I had this feeling like there was like infinite freedom in the things that I could
get the computer can do to do.
If only I could just figure out how to tell it what I wanted.
Right.
And I felt like if I could just tell it how to have a conversation with me, then it would
basically be my friend or like talk to me.
And I was really, really interested in just getting the computer to have a conversation
with me, like a casual conversation, like how are you doing, what's your name, that kind
of thing.
And at first, I would write really simple programs and basic, the basic computer language
that would have like little conversations like this, like I'd say, what's your name?
I'd say Ken, like basically in typing and they would say, hi, Ken.
And I was very impressed that we could have this kind of conversation and that I got it
to do that.
But I quickly hit a wall where I couldn't get it to like really do anything interesting,
you know, just a very star scripted thing.
And at the time, like around age eight, I thought there's some way to do this that I just
need to read a book or something like there's something that would just tell me how to get
it to have a real conversation with me.
And I didn't realize that this is like one of the greatest problems like facing humankind,
like how to get a computer actually being intelligent, like a real person.
And it took me a while actually for it to strike me that this is actually like an extremely
hard problem.
And there's not just like some manual you can read that can get the computer to do that.
So I probably, within a couple of years, I realized this is like a huge problem and then
I was really interested in hooked and like, wow, this is actually hard and like there's
got to be a way to do this.
And I guess I would just stay captivated by that problem like forever.
But I guess I changed the shift a bit in my interest because if you look at that and
you look at it from the lens of like today's subfields of artificial intelligence, you
probably call that natural language processing or something like that.
And I kind of shifted away from that over time to more like lower level stuff, like control,
like neural stuff.
That was like what initially hooked me into it and got me into the AI.
Interesting.
And you mentioned that you studied at UT Austin.
I did an interview with Risto, Michaeline and did you study with him there?
Yeah.
So I guess it's just a coincidence that Risto is my advisor or was my advisor during the
PhD.
I worked with him for years there.
Yeah.
Awesome.
Awesome.
Can you tell me a little bit about your primary research focus?
Sure.
So my primary research focus is in an area called Neuroevolution.
And it's an area that is probably less well known in the general public like you hear tons
of stuff about deep learning today, but you don't hear so much about neural evolution.
It's certainly related to deep learning because both of them are about in effect neural networks.
But Neuroevolution has this twist, which is that we're interested in neural networks,
which are for those who don't know basically these rough abstractions of what happens in
brains.
Like, you know, the word neural comes from neurons and neurons are in our brain.
So neural networks are roughly motivated or inspired by brains in nature, although they're
not at all accurate models of them.
But then in neural evolution, we're combining that with evolutionary principles, which really
means kind of like breeding.
Like if you think about it, like it's like if you had a neural network that does something
good, like say drives a robot and makes it able to do attacks, like say, walk, like it
gets your biped robot to walk, then like, neural evolution is kind of like you're breeding
those brains.
So you're saying, okay, I have a bunch of brains.
These are artificial brains.
We'll call them neural networks though, because artificial brains exaggerates like how
cool they are.
They do their artificial neural networks, and we would then look at like, well, how well
do they get the robot to walk, like a whole bunch of them, and they call that a population.
And then like we choose the ones that do better, some will do worse, and some will do better.
And those that do better will have children, which basically means like new neural that
will be born as offspring of the old ones that we chose, or we call that selection,
we selected those.
And our hope is that the offspring of those better ones will sometimes be even better
than their parents.
And we keep on playing this game, which is just breeding, so like it's not hard to understand,
like some areas of AI are kind of complex, and are to understand at first.
But intuitively, this is easy, because this is just like breeding horses or breeding dogs.
They just choose the ones that are better in respect to whatever criteria you have, and
then just breed them, and hope that things get better over time.
And so a nerve evolution is basically about breeding these artificial things, rather than
real organisms, which are these artificial neural networks, and thereby getting them to
get better over generations.
And what is interesting about it to me is that, like, well, it's like a simple concept
in principle, at least like the initial outline that I gave is quite simple just in terms
of breeding.
Like under the hood, there's like real mysteries here, because this is really the process,
you know, that produced you and me, and like the high level of intelligence that we have,
going all the way back to single-celled organisms, and it's quite amazing to believe that, like,
there is some kind of path through that space just through breeding that can lead to something
like us from something so humble and simple.
And to get algorithms to do that is an enormous challenge, and not fully understood right
now.
And that's where kind of the research comes in in the field.
Interesting.
Interesting.
And then you're also, again, a senior research scientist at Uber AI Labs.
What can you tell us about Uber AI labs and how that came about and what the charter
is there?
Right.
So there was no Uber AI labs around nine months ago, but I was one of the co-founders of
a startup company called Geometric Intelligence.
My co-founders were included Gary Marcus, Zubin Garmani, and Doug Beamus.
Some of them are really quite well known and have very respected researchers themselves.
And we were doing in Geometric Intelligence proprietary machine learning research and
developing new technologies and building a team that we were hoping to be a world-class
research team.
And what happened was that Uber acquired us nine months ago in December.
And when Uber acquired us, they had partly one of their aspirations was to start in
AI lab like a real research lab in industry that researchers the cutting edge of artificial
intelligence because Uber believes and believes at the time that artificial intelligence
is a critical competitive component of the industry where Uber needs to be staying at the
cutting edge.
And Uber has, and had before, a lot of competence already in machine learning.
So it's not like there was nobody here that were plenty of people here who were very
qualified in the field.
But they didn't have something that was really a fundamental research lab and where they're
sort of just really pushing on the cutting edge of AI itself as opposed to just applying
it to internal problems.
Like, for example, Uber has a team focused already that was focused on autonomous driving.
And so they already had that in place, but that's an applied aspect of artificial intelligence.
And so the AI lab that was founded off of the company that we started, which we founded,
was really intended to be focused more in advancing the algorithms themselves.
And so what Uber got was basically all at once, like all of these researchers who had
this capacity to push forward the field of AI.
And so you can kind of think about it roughly in analogy with similar types of research
labs at big tech companies, like maybe like something like DeepMind, which was originally
acquired by Google or something like Facebook AI research, or just also Google Brain and
Google.
So there's some rough analogy there between us.
And then we're much smaller though, because we're newer.
But we have the kind of similar mandates in terms of researching the cutting edge of AI.
And I should say that actually we're going to, we are going to engage with the outside world
in the academic community.
You'll be hearing from Uber AI labs and we're going to be publishing and we understand
that like just we cannot be a successful AI lab if we are not engaged with the outside
world.
So we will be publicizing and publishing some of our work so people can see what we're
doing and so that we can communicate with other other researchers and scientists across
the world.
Okay.
Great.
Great.
So a little bit about the intersection between your work and evolutionary AI and the kind
of things that Uber is doing around self-driving cars.
Yeah.
So I can't get into specifics about what Uber is doing with their self-driving cars for
a obvious reason.
But I can say that Uber AI labs is diverse.
I mean, that was one of the original inspirations behind geometric intelligence or the predecessor
to Uber AI labs was to have a diverse group that isn't just in one particular fad which
you might say deep learning is although it's obviously an important one that's making
a lot of important contributions.
But our philosophy was that, you know, we need to not have all our eggs in one basket.
And so Uber AI labs itself is like that too and that we have a lot of diversity in terms
of the expertise and areas that we cover.
And so among those, we clearly are world class in neural evolution, which is the field that
I just described where I've focused at most of my career.
And so this is a particular direction within AI and machine learning that offers some
unique insights and angles on certain types of problems that other areas might have a
different take on.
So in terms of like autonomous driving, I mean, it's clear that the idea of the evolution
of complexity and how really high level intelligence can be evolved in terms of complex, large
deep artificial neural networks has a connection in principle to how you could get a really
sophisticated controller for a vehicle or something like that.
And so the insights of the field of neural evolution, both directly, which means like using
neural gene itself as an algorithm and indirectly in terms of insights that we gain as a side
effect of doing experiments in that area, can impact how we would create algorithms that
might control things like autonomous vehicles.
But I should also note that it's not that it's not the case that the only application
or even necessarily the main application of AI at Uber is in that area.
I mean, Uber has AI problems across the gamut of all of their business components.
So there's a lot of different applications that are under consideration when it comes
to like AI labs and what AI helps does.
Sure.
So can you talk a little bit about how your research focus kind of compares and contrast
with what RISTO is doing down at UT Austin?
Yeah, sure.
So I mean, actually, there's a lot of overlap because I mean, I'm his advisor, so I've
taken a lot of the original teachings that he gave me as a basis of my career and obviously
collaborated with him for years to publish some of the, in the end, it turned out to be
some of the seminal papers in the area, both together.
And so I think we're not actually so different in terms of like the fields that we're interested
in where we may differ is more just in like what particular algorithms have we contributed
to inventing sensory parted ways when I basically graduated with the PhD.
And so he's focused on his own set of innovations and I've focused on my own and there's some
divergence there.
But we really ultimately tend to be very close because like when I've invented new things
like I don't know is it, and I'm still at the University of Central Florida as a professor
RISTO would sometimes build on those things in vice versa.
So we're very intertwined and it's not a surprise since we started out in same area.
Absolutely, absolutely.
And so folks that are interested in maybe some of the background on, you know, you talked
about the kind of breeding process that are really high level, RISTO and I spent quite
a bit of time digging into that in more detail, you know, so folks that are interested
in that might want to refer back to to that podcast since you've graduated and now that
you're kind of driving your own research agenda, like what are some of the specific algorithms
that you've published research on and, you know, how do they build on kind of that, the
core ideas of genetic or evolutionary computing or algorithms?
Yeah, sure.
So, so a neural evolution, which is this idea of evolving neural networks, like one interesting
thing is that when, what we're, at least for me, what I find really interesting is not
just optimization, like a lot of people in machine learning think in terms of optimization,
which means just like how do you get this structure to get better and better and better
with respect to a task?
But I'm also interested in what you might call complexification, which means like how
do we get increasing complexity, like the thing that really fascinates me is like how in
nature things got more complex, like insanely more complex.
Not just like a little bit of incremental increases in complexity, but like from a single
cell to organism to something that has in our brain a hundred trillion connections among
a hundred billion cells, approximately, or a hundred billion neurons, and that's just
amazing to me that like some kind of unguided process could build something like that.
This is not something that was engineered and so I'm sort of always have my eye on like
what is it that allows really high level astronomical levels of complexity to emerge from
this kind of process, kind of automated process.
And so the interesting thing in neural evolution is that every time it seems like we have an
advance where we kind of figure out something about how do you get increasing complexity to
happen inside of an algorithm, and we've made some advances, including the first thing
that I did in grad school, which was this algorithm called neat or neural evolution of
augmenting topologies, which I did with Risto, which was basically an algorithm about how
can we have the neural networks that are evolving in the computer, increasing complexity over
the course of the algorithm running in the computer.
And it was because I had this real fascination with increasing complexity that led to us introducing
this algorithm that increases complexity, but then what's interesting is that every time
we make an advance like that, it sort of uncover some like deeper underlying question, because
it turns out that like the explanation for why it was possible to get from one cell to
trillions is really, really subtle and nuanced and complicated.
And when you say that, are you speaking biologically or from a computational context?
Right.
Good question.
Yeah.
So actually, those things constantly get intertwined in my mind, like whether I'm speaking
biologically or computationally, because the way I look at it is kind of like the biology
and computation aren't really necessarily different things, like in effect, like if you read
a biology textbook, you know, you feel like you're reading about biology, but like in effect,
it's also about computers because, or at least algorithms, you know, because you're talking
about a principled process that basically follows some certain kinds of rules.
Just these analog computers that we really don't understand very well.
Yeah, you could think of like the universe as a big analog computer, we don't really
understand.
And so like, I mean, but like evolution is a very algorithmic thing, you know, you're talking
about there are individuals and those individuals reproduce.
And then the thing that, and who gets to reproduce is based on a formula, which is, which is
obviously complicated, but basically some, some individuals reproduce some, some don't.
And this can be formalized as basically like a program.
You could imagine writing the rules of the system.
And this is what inspired the field of evolutionary computation.
I mean, people saw the theories evolution in biology and thought like, you know what?
This is actually not that hard to write down as a program and actually make evolution happen
artificially inside of a computer.
And it turned out though that like, if you just read a textbook and then, you know, learn
these principles that sound like good explanatory principles for like how evolution works.
Like if you read a biology, text was like, well, they know how it worked.
That's an explanation.
It turns out that explaining something is easier than actually implementing it, which is
basically something that we found across the field of artificial intelligence.
You know, you can read about, you can read a neuroscience textbook and say, this is
how brains work.
Of course, biology will acknowledge we don't know everything, but this is what we understand
now.
It's a comprehensive explanation, but it's far, far away from like telling you how to
actually build a brain.
I don't know how to build a brain just because we have some understanding of how brains
work.
It's the same with evolution.
Like, we don't know how to build a true evolutionary system at the scale and magnitude
of what happens on Earth, even though we know a lot of the details about what goes on.
And the missing details, like the gap between what we understand and what we can actually
build, that's where the research is and that's where like a lot of fascinating insights
occur.
Like to me, I think that to some extent, like when we make advances in artificial intelligence,
we're actually learning something about biology in a sense because we're realizing that
the gap in our knowledge, like what we didn't understand, are actually filled by something
that we didn't expect or that wasn't in the textbook about how things work.
And it's true that sometimes we may be doing things that are not actually the same as biology,
but at least they're revealing gaps in our knowledge of biology because like if in some
sense, if we actually knew everything about how things works, then we could just program
it in, but we clearly don't.
And so it's kind of like, I think AI has like a higher bar in a way than biology where
in biology, like you can explain something or statistically analyze it, but then I actually
actually have to build it, which is much, much harder.
So it sort of forces us to grapple with the problems of the gaps in our knowledge and
biology.
Now some people in AI would just sort of like say not like that way of looking at things
because some people in AI don't care about the biology and they just want to build intelligent
things and they don't really care, do these things correspond or not with biology.
That's not the goal.
The goal is just to build intelligent things, we aren't like adhering to biology or
not.
I tend to be more biologically inspired, but I also agree that like I don't really,
honestly, ultimately care whether what I build is exactly the way it works in biology
or not, but I just find it interesting and inspiring that biology has achieved things
that are just so amazing, I mean like human level intelligence, and I find it fascinating
that we just don't know how, and like trying to probe those gaps in my understanding,
I find leads to over and over again, really deep insights in artificial intelligence
because it's like we suddenly realize, oh wait a second, actually there's an explanation
here which is much different than what we thought it might be.
And so after a graduate school, like there was a succession of those that I went through,
we would realize that, you know, there's something missing still after like for example
the need algorithm, which actually became the most used algorithm in this sort of niche
field of neuro evolution, but we realized, you know, there's limitations on what need
can ever do.
And so this will, wow, can we get around those limitations?
How did nature get around those limitations?
So like one example is that like in need, there's this artificial DNA, which encodes the
neural network, so we have to do evolution, so we have like an artificial DNA, which we
call a genome, well it would have one gene per connection in this brain that's evolving.
And like this is clearly not going to scale, even though like this, this brain can keep
expanding, but like if you wanted to get 100 trillion connections, this is what we have
in our brain right now in biology, we would need 100 trillion genes in need.
And there is no way that's ever going to happen.
100 trillion genes is just astronomically insanely large.
And like for example, our genome in biology only has 30,000 genes, or 3 billion base pairs,
another way of thinking about it.
So we had to invent new algorithms, and this is after grad school and after need that
could encode much, much larger structures, we called these indirect encodings, and this
led to something called hyperneet eventually, which is a new kind of genetic encoding that
is much more compact than the original need.
And so hyperneet was something that I did after I left UT Austin, and so where I did that
independently of Risto, and led to the ability to evolve much bigger in effect neural networks.
And then I think one of the biggest things probably that has had a lot of impact in the
field after that was something called novelty search, which is a result of discovering
that in some cases the best way to get something in a search process, in evolution to kind of
a search process, like you're searching through space of possibilities, is to not be trying
to get it.
And this was a really counterintuitive and paradoxical insight, but really important I think for
realizing how things are achieved.
So in other words, if you say that you're trying to breed for something, like say we want
to get human level intelligence, then that actually may doom you from the start.
Like sometimes the only way to get to something is to not be trying to get it.
And this is a hard kind of a bitter pill to swallow, but something that, what is the mechanism
of frying that keeps you from being able to get it?
Yeah, so the mechanism there is something called deception.
And actually this is something that applies way, way outside just neural evolution.
This is a general principle for everything in life.
Is that deception?
It's called deception, yeah.
It's basically the situation when if you are observing that things are getting better,
so it's like you have some metric for what it means to be doing well, like performance
metric, like let's say, how well are you able to walk?
And so you have some metric that says, well, how well am I walking?
And so normally, like if I was trying to get something to walk, I would select things,
meaning I would breed things that are apparently better walking compared to their predecessors.
And I would call that their fitness.
And so that's what I mean by trying.
I keep on intentionally picking things that seem to be better.
And this is a very intuitive idea, like everybody for a long time felt like this is obviously
the way to get things to evolve is to pick things that are better.
But it turns out that if you're in a deceptive situation, which it turns out unfortunately
you often are in, that you can be moving in the wrong direction, even though your metric
for performance is going up.
And that's because like the world is really, really complicated.
So it can appear that you're improving in some way when you're actually not.
And so for example, like when it comes to walking, like lunging forward like a maniac and
falling down like a few feet from where you started may appear to actually be an improvement
in your ability to travel, you know, because basically you're getting farther than your predecessors
by throwing yourself on your head like five feet in front of you.
But this is actually not a good stepping stone towards really good walking behavior.
In fact, like a good stepping stone might be discovering the concept of oscillation.
Like that's what your legs do.
They kind of oscillate when you walk.
Well, it could be that when you initially discover oscillation, you fall on your face.
And so it actually looks like you're not improving.
And so, but because your metric is basically how far did you go, it causes you to basically
be blind to the underlying discovery that's actually essential to making the progress that
you need to make in the long term.
And this problem of deception is just like universal across all kinds of endeavors.
Not just neurovolution.
It's like, is this analogous to almost like a kind of a local maxima kind of issue?
Yeah, I mean, it's basically the same thing.
It's related to local maxima or local optima or premature convergence.
Sometimes people would call it to getting stuck on a local optimum.
But I think that the insight that we have that's different from just saying, okay, well,
we just rediscovered local optimum because we already knew about local optima.
Exactly.
It's just how utterly profound the problem is.
Then like you cannot just like, I mean, people think, well, there's ways of getting around
local optimal.
You know, I mean, you can do your tricks.
We have diversity.
We have randomness.
Docasticity.
There are things we can do to kind of jiggle things around a little so we don't just get
stuck on a peak, which is what kind of we think of local Optimus like getting stuck on
a peak in a big space.
That like, that's just not going to cut it in certain types of problems because they
are just so absolutely complex that almost no matter what you do, deception is going to
kill you.
And we showed this when we introduced this algorithm called novelty search that in some
problems that it was like shockingly terrible with deception could do to you in these spaces.
And what was profound was that we showed that in certain problems like this where deception
is a really big problem.
And I would claim that deception is a really big problem in like almost any interesting
problem.
And we kind of demonstrate that later if we want to get into it.
But when it is a serious problem, then we showed that with this novelty search algorithm
that we introduced, which was basically not trying to solve a problem, but rather it
was just driven by selecting things that are more novel.
So not things that are better, but just more novel, that this would actually be better
at solving a problem that was deceptive than an algorithm that was actually explicitly
being driven by selecting things that were better.
So the lesson it showed is it can be better sometimes to not be trying to solve the problem
than to actually try to solve the problem in terms of getting a better solution.
And this obviously really counterintuitive in paradoxical and upsetting maybe even because
it's like embarrassing in a way for anybody who's like saying, okay, I've got this really
good optimization algorithm to lose to an algorithm doesn't even know what kind of problem
it's trying to solve.
And that's sort of what novelty search is.
It's a divergent search algorithm, so basically it's just trying to find things that are different
than what it's found before.
It sounds a little bit like, you know, explore, exploit where you're explore is kind of optimizing
for newness.
Yeah, yeah, yeah, it is related to this kind of exploration, exploitation dichotomy that
a lot of people talk about machine learning, but it's also different, I think.
So like there's an additional element of insight here beyond that, which is really important,
which is that when we think of exploitation versus exploration, like often we think of
exploitation as following some gradient, which means information towards something that
we are trying to get to.
So in other words, we're using information to move in a direction that's intelligent.
But interestingly, exploration we tend to think of as sort of random moves that are
sort of ignoring the informed gradient.
So it's like, let's just go somewhere and see what happens.
And that's what we think of exploration.
But what novelty search showed is that there is a principled kind of exploration that is
not random, that actually exploration is something that's also very informed.
And so in the novelty search case, you're informed by where you've been, because novelty
is basically a comparison between where I am and where I've been before.
So it's anything but random.
It's a very informed gradient.
It's just that it's the gradient of novelty instead of the gradient of the objective.
And this is actually a very information rich gradient, because if you think about it, you
know a lot about where you've been.
In fact, you know more about where you've been than you know about where you're trying
to go, because the whole problem with where you're trying to go is you don't know about
it.
Otherwise, you would just go there.
So novelty is actually more informed, I'd say, than the objective gradient.
And for this reason, it's an extremely interesting gradient to follow, like the gradient of novelty.
Because you're being pushed away from where you've been before.
And it turns out that you will be inevitably pushed towards higher complexity.
So it's really tied into this idea of increasing complexity.
Because if you think about it, as soon as you exhaust all the simple things you can do
in the world, like the only choice you have if you want to continue to create novelty is
to do something more complex.
And so ultimately, there's an inevitability that like with novelty search that you're
going to be pushed towards increasing complexity.
So I think of it as almost like an information accumulator, like in order to continue to do
novel things in the world, you have to accumulate information about the world.
So for example, like you could imagine if you were trapped in a room and I told you
like to just do novel stuff, like for a while you could just run around randomly and you'd
like bump into walls and everything you do would be novel.
But eventually you'd bump into all the walls in the room.
And so at some point you're going to have to learn how to not bump into walls.
And when you do that, you're going to have to learn what a wall is and how to sense
a wall and how to navigate walls.
And eventually you have to learn how to open a door because you have to get out of the
room eventually to do something new.
Right.
And eventually you're going to have to get off planet earth and go to Mars.
And clearly like doing that requires like learning extremely deep and complicated facets
of how the universe works, like physics.
And so you're going to be forced to become an expert on the domain where you find yourself
if you're going to be pushed towards doing more and more novel things.
And so knowledge actually is a very deep and interesting kind of a process.
And that's why sometimes it alone will do better than actually trying to solve the problem
you're trying to solve.
If you think about like evolutionarily, like if you think about like how could we get
to human intelligence from a single cell, it'd be crazy to do selection based on the
intelligence of single celled organisms.
Like we wouldn't start out by applying IQ tests to single celled organisms.
That would just kill the population.
I mean because none of them are intelligent at all.
And so it's funny, but in a sense, the reason that we got to where we are today is because
we were not trying to get there.
Like if we had started out where selection was based on intelligence, then everything
would have died or we would have gotten nowhere and we wouldn't have gotten to where we
are today.
So we see this issue of deception come up over and over again.
Like it turns out that like there was a turning point long ago, eons ago, where symmetry,
bilateral symmetry was discovered.
These are our ancestors.
There's these bilateral, these symmetric flatworms.
There's no indication that they'd say anything to do with being more intelligent, but actually
it does in some kind of like really, really long term sense.
Like that was an important discovery that led ultimately, or stepping still in the leads
ultimately to human level intelligence, but you wouldn't be able to predict that on
the basis of doing an IQ test.
And yet we needed to lock that in.
So in some sense, we could recognize that was interesting from a novelty perspective because
it was a very new innovation, but we cannot recognize it from a performance perspective
because at that long, long ago point in time, it's not an indicator at all from the point
of view of performance, like if the ultimate indicator is intelligence.
And this is another kind of example of deception and why many things are not going to be possible
to discover if we just set them as a goal and just select based on those things.
And this is a principle not just for evolution, but for life too.
You know, like there are many inventions that would not have been invented if they had
been our goal to invent them, which is again the paradox coming up.
Like computers, for example, were the first computer for based on vacuum tubes, but the
people who invented vacuum tubes were not trying to invent computers.
Like if you had gone back to the 1800s and told all the researchers working on vacuum
tubes who were interested in electricity, that like actually there's something more interesting,
like a computer, and maybe you should just invent that.
Like forget this boring vacuum tube stuff.
You would neither have vacuum tubes nor computers.
So like once again, we needed people to be exploring very diverse ideas without having
their eyes on the prize.
If you think of the prize as like a computer, in order to eventually get the prize.
And so there's a paradigm right there.
And so this concept is so general and connected to this novelty search idea that we wrote this
whole book about it called Why Greatness Cannot Be Plan.
After a long time researching novelty search, and a long time for me talking in various
forums and venues about novelty search, and I realized that like the principles are really
general about this paradox, this is what I call the objective paradox, that like it's
actually relevant to all society, like how we run our institutions.
Like we give money to people based on them making progress with respect to an objective,
like this is what granting agencies do, like in the sciences.
And it's actually not principled in the long run.
Like we have, there are other processes that need to be recognized and respected if we
really want to be able to achieve really, really ambitious ends.
And so that's why we wrote this book basically to introduce these principles of deception
and divergent search and the objective paradox to the general public.
We are hoping that maybe this would actually provoke a discussion of these things in a larger
sense because of the fact that it affects many of the kind of attempts at innovation that
we as a society are engaged in.
So it turned out to have really broad implications across culture and society.
Interesting.
And then one of the papers that I noticed is one called Galactic Arms Race, is that an
extension of this pork or is that a different direction?
It's related.
Yeah, it's related.
So like we, as we started to understand this idea of, we call it sometimes divergent
search.
Like searches that are not aimed at a particular goal, but rather which are diverging
through the space of what's possible.
There are kind of searches that show you all the cool stuff that you could find, not
just one thing.
Evolution on Earth is kind of like that.
It's not like one thing it's trying to do, it wasn't trying to get human level intelligence.
It's kind of illuminated all of the possible cool stuff that's out there in nature, all
of the diversity of nature.
And so we started to realize these algorithms are really cool that do stuff like that, perhaps
for applications in the real world.
In Galactic Arms Race, the application is a video game.
And our idea there was like maybe we could put one of these divergent search algorithms
in a video game so it would generate the content in the game.
And you'd get more and more cool content just like flowing into the game from nowhere.
Like no human has to actually design or invent it.
And in the case of Galactic Arms Race, it was the weapons of the ships that you fly.
Like people are familiar in video games like with playing games where like you have to pick
up new types of lasers or weapons or guns or something like that.
Right.
So we said let's let evolution invent the weapons.
But with a kind of a novelty search like process where it's not like aiming for like
the optimal weapon, it's just diverging through the space of weapons.
But with some information about how humans are actually using them.
So it's informed by the humans in the game and in real time inventing new weapons for
the humans to try.
And so there's an interaction called interactive evolution between what humans do and what
evolution does.
And it caused like all these cool weapons to be invented things that I don't have never
seen in any other game that were just invented by the computer itself.
And it's kind of I think a really nice exposition of like the potential of like divergent
search or novelty like searches to create kind of open worlds where things are just continually
generated.
And sometimes we call this open-ended evolution that are interesting and hopefully without
end.
What's an example of a type of weapon that was invented in this game?
Okay.
Yeah.
There's a couple good ones.
So like one was I, so there's funny we started naming these things after the fact
because they don't actually have names because they're invented by the computer.
Like one we call the tunnel maker which would basically generate like two streams of particles,
these are all particle weapons that would sort of like very slowly shoot on the left and
right side of your spaceship.
So basically it created a protective tunnel that you could fly through.
And then in the middle of that tunnel there was another faster stream that was actually
used for shooting things.
So you would be creating basically like a shield that would like shoot out from your
sides that you could then fly through.
There was another one that we called a lasso which would just look like, it looked like
a cowboy's lasso, you know, just like shot out and like created like this spiral around
the enemy and then like closed in on it.
And it was really surprising that this thing was invented and it was kind of interesting
because I actually, it's not a great weapon in an objective sense like the lasso one because
like I think it's much better probably just to shoot straight at something and kill it.
But like the players loved it because of the aesthetics.
It's just so interesting and fun like to have the lasso weapon and to kind of show off
because it was a multiplayer game so people could see each other's lassoes that it became
popular.
And the game just kind of went with it, you know, the game didn't say this is objectively
worse or objectively better.
It just saw that people were interested in lassoes who created more lassoes and diverse
lassoes and we had all these lasso weapons proliferate in the world because people liked
them whether they're, you know, optimal and some objective sense or not.
Is there an argument that says that the, you know, the, you know, issues around, you
know that you identified in novelty search and, you know, getting led down the wrong path
they example, I guess you gave us with, you know, a robot trying to learn how to walk
and kind of, yeah, using a motion that kind of allows it that kind of doesn't lead it towards
walking and eventually let's fall on its face.
I guess the thought is are, you know, it can all of us be boiled down to just not being
able to express enough sophistication in our objective function or not being able to
express our objective function in the right time frame or something like that.
Yeah.
Actually, there's an element of truth to that view that like, yeah, like if we knew
enough about the world, we could just write the objective function to take into account
how the world actually works.
But the problem is that like in practice, that's just impossible because like you ultimately
would have to know every single thing about all the stepping stones that you would have
to go through to write the objective function to take that into account.
So it's like, say there's like, you know, a million steps between here and a human level
AI.
So it will obviously if I wrote a fitness function where your score is literally how far
you are along that path.
Then of course, this is like the ideal objective function is going to work out fine.
But the whole point, the whole problem that we're facing just begs the question of how
are we going to figure out what the stepping stones are so we're back to square one again.
And so in practice, like you're probably not going to be able to do that in even like
a relatively simple problem because the whole problem of searches we don't know the stepping
stones, if we did, we wouldn't be doing search because we would just build the thing because
we would know all the steps to get it right.
So this paradox is basically unavoidable, you know, like if the problem's not interesting,
then we do know the stepping stones that we don't need to do these things.
But the problem's not interesting.
But if the problem is interesting, it's interesting because we don't know the stepping stones.
Like that's what makes it an interesting problem.
And so almost any interesting problem is going to be confronting this paradox.
Now that doesn't mean that there aren't some cases where search will work.
Obviously it will with an objective sometimes, there's no doubt about it.
In fact, deep learning has exposed that like in really high dimensional spaces between
spaces of many, many parameters, like many weights in their own network, that there's
less deception than we thought.
Like and this has been a surprise for everybody including me.
And so sometimes we still can just push sort of a brute force through the objective function
because high dimensional spaces have some very odd properties and succeed at solving some
problems.
So we shouldn't conclude from what I'm saying that like all objectives are completely useless.
They do work in some cases.
But I think that it's still the case that in very, very complex problems, we are going
to be facing deception.
We are not going to know how to write the correct objective function to go through all
those stepping stones, which are basically reflecting eons of progress to get to some
of these really ambitious ends that we have.
And so it's an element.
It's not like everything should be done this way, but it's an ingredient that's added
to our toolbox now, which is going to be important in concert with sometimes explicit
objectives.
And so it gives us kind of a powerful new tool.
And this has actually led to a field called quality diversity where we combine quality
measures with kind of diversity measures and try to do both at once in order to make
a principle attempt to leverage what we know about both of those kinds of searches.
Hmm.
Super interesting stuff.
Kenneth, I really appreciate you taking the time to speak with us about neural revolution
and your research.
Is there anything else that you'd like to leave us with?
Well, I just, I guess just to say that take a look at neural evolution, like it's actually
becoming now more recognized in deep learning that, you know, we have actually a lot of synergy
with deep learning because we're also doing neural networks.
And so both fields, I think, are realizing today that we have something to offer each other
perhaps, you know, like a revolution can evolve architectures and deep learning can apply
really powerful learning algorithms to those new complicated architectures for just
as one example.
And our evolution can contribute to reinforcement learning in new ways because of the way that
fitness can be a different kind of driver of progress than, say, the typical gradient
based approach.
And so in the end, we get a possible really powerful synergy.
And so I think it's worth looking at how these two things can possibly feed into each other
going forward.
Awesome.
And what's the best way for folks to learn more about what you're doing?
I'd point people to, I mean, I'm guessing you probably have some links associated with
the energy.
We can include a link.
And I know you've got a page on the UCF site.
Is that the best one?
Yeah.
I point people to my home page, my research group home page, both their UCF and also I
can provide a link to Uber and I labs where we actually are hiring too.
So people are just interested in jobs in general.
That's another opportunity there.
So I'll also point to that.
Fantastic.
Well, thanks so much, Kenneth.
Yeah, thanks.
It's been a pleasure.
All right, everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
Thanks to your support.
This podcast finished the year as a top 40 technology podcast on Apple podcasts.
My producer says that one of his goals this year is to crack the top 10.
And to do that, we need you to head over to your podcast app.
Keep the show.
Hopefully, we've earned your five stars and leave us a glowing review.
And more importantly, share the podcast with your friends, family, co-workers, the Starbucks
Barista, your Uber driver, everyone who might be interested.
Every review, rating and share goes a long way.
So thanks in advance.
For more information on Kenneth or any of the topics covered in this episode, head on
over to twimmolai.com slash talk slash 94.
Of course, we would love to hear from you, either via a comment on the show notes page
or via Twitter to at Sam Charrington or at Twimmolai or at Twimmolai.
Thanks once again for listening and catch you next time.
