1
00:00:00,000 --> 00:00:15,920
Welcome to the Twimble AI Podcast, I'm your host Sam Charrington.

2
00:00:15,920 --> 00:00:22,680
Hey, what's up everyone?

3
00:00:22,680 --> 00:00:26,680
If it's been a while since you've checked out the Twimble online community, now would

4
00:00:26,680 --> 00:00:29,960
be a really good time to stop by and get reacquainted.

5
00:00:29,960 --> 00:00:33,680
We've got a ton of interesting things happening for folks looking to grow their machine learning

6
00:00:33,680 --> 00:00:35,680
and AI knowledge.

7
00:00:35,680 --> 00:00:38,280
Here's a sample of what's new.

8
00:00:38,280 --> 00:00:42,720
The new Kaggle Slash Projects Meetup, led by Michael, Christine, Philip, and Mahul,

9
00:00:42,720 --> 00:00:45,040
is off to a very strong start.

10
00:00:45,040 --> 00:00:49,800
They'll be meeting on Saturdays at 10am Pacific and focused on competing in Kaggle competitions

11
00:00:49,800 --> 00:00:56,200
together, and more generally supporting one another and working on ML and AI projects.

12
00:00:56,200 --> 00:01:00,040
If natural language processing is more your interest, we've got a study group for the

13
00:01:00,040 --> 00:01:04,160
Fast.ai NLP course starting December 14th.

14
00:01:04,160 --> 00:01:08,960
The course will cover NLP applications like topic modeling, classification, language

15
00:01:08,960 --> 00:01:11,000
modeling, and translation.

16
00:01:11,000 --> 00:01:15,240
If that sounds interesting but you don't want to wait, the Fast.ai deep learning from

17
00:01:15,240 --> 00:01:20,160
the foundation study group is starting Lesson 12 this Saturday, which is essentially

18
00:01:20,160 --> 00:01:23,240
a three week NLP crash course.

19
00:01:23,240 --> 00:01:25,440
You're welcome to join in.

20
00:01:25,440 --> 00:01:29,560
We've also got group leaders organizing study groups for the Fast.ai deep learning for

21
00:01:29,560 --> 00:01:35,320
coders, i.e. the Part 1 course, which is a great place to get started with deep learning,

22
00:01:35,320 --> 00:01:41,040
as well as Andrew Ng's deeplearning.ai course, which takes a more traditional and math oriented

23
00:01:41,040 --> 00:01:42,600
take on the topic.

24
00:01:42,600 --> 00:01:46,800
Finally, if you're more interested in learning how to use and deploy machine learning and

25
00:01:46,800 --> 00:01:52,080
AI in the Enterprise, I'll be leading a study group focused on the IBM AI Enterprise

26
00:01:52,080 --> 00:01:57,000
workflow certification, which is a sequence of courses hosted over on Coursera.

27
00:01:57,000 --> 00:01:59,120
That group will start in January.

28
00:01:59,120 --> 00:02:04,120
The way to get started with any of this is to join the Twimal community at twimalai.com

29
00:02:04,120 --> 00:02:06,120
slash community.

30
00:02:06,120 --> 00:02:09,720
Submitting that form will get you invited to our Slack, and once you're there, you can

31
00:02:09,720 --> 00:02:12,680
join the appropriate study group channels.

32
00:02:12,680 --> 00:02:17,680
Hope to see you online.

33
00:02:17,680 --> 00:02:22,840
All right, everyone. I am on the line with Mike Tong. Mike is the founder and CEO of Diffbot.

34
00:02:22,840 --> 00:02:26,240
Mike, welcome to the Twimal AI podcast.

35
00:02:26,240 --> 00:02:27,800
Thanks for having me, Sam.

36
00:02:27,800 --> 00:02:30,560
Yeah, I am looking forward to this chat.

37
00:02:30,560 --> 00:02:36,360
We first talked about cover Diffbot in a newsletter.

38
00:02:36,360 --> 00:02:40,960
When you announce, I think it was the company's knowledge graph project, and the headline

39
00:02:40,960 --> 00:02:46,400
of that newsletter was all the world's knowledge on tap, and so it's taken us a while, but

40
00:02:46,400 --> 00:02:50,080
we will finally dig into what that was all about.

41
00:02:50,080 --> 00:02:53,720
But before we do, let's explore your background a little bit.

42
00:02:53,720 --> 00:02:56,240
You were a patent lawyer at one point.

43
00:02:56,240 --> 00:02:57,240
Yes.

44
00:02:57,240 --> 00:03:01,920
Among many random jobs I've held, quick background on me.

45
00:03:01,920 --> 00:03:03,880
I'm a machine learning researcher, right?

46
00:03:03,880 --> 00:03:09,960
So I studied at Berkeley electrical engineering, and then at Stanford, I started grad school

47
00:03:09,960 --> 00:03:20,040
in AI, and I've worked as a software engineer before at Microsoft, at eBay, and Yahoo.

48
00:03:20,040 --> 00:03:26,040
I was the founder of a startup that was sold to Cisco, called Click.TV, that was like a video

49
00:03:26,040 --> 00:03:31,080
search engine, and I was a founding engineer of a startup called Defined that was a product

50
00:03:31,080 --> 00:03:32,080
search engine.

51
00:03:32,080 --> 00:03:37,040
I was sold to Facebook, and in between all that, while I was in grad school, doing research

52
00:03:37,040 --> 00:03:40,880
and AI, I held a side job as a patent attorney.

53
00:03:40,880 --> 00:03:44,440
So as a patent prosecutor, helping people write patents.

54
00:03:44,440 --> 00:03:53,240
So I was the patent prosecutor for Panasonic out here in the Bay Area, and that's sort

55
00:03:53,240 --> 00:03:56,320
of how I bootstrapped the company and paid the bills at the very beginning.

56
00:03:56,320 --> 00:04:02,160
So I got really good at writing patents, could basically pull all nighters and jam out

57
00:04:02,160 --> 00:04:04,640
a patent over the weekend and make like 20K.

58
00:04:04,640 --> 00:04:08,320
So I have like my rent covered for a few months here in the Bay Area.

59
00:04:08,320 --> 00:04:09,320
That's awesome.

60
00:04:09,320 --> 00:04:14,000
So does that mean that before the engineering degrees, you had a law degree?

61
00:04:14,000 --> 00:04:18,840
No, so the thing about patent law is, you know, first of all, it's federal, right?

62
00:04:18,840 --> 00:04:20,640
So it's Washington, DC.

63
00:04:20,640 --> 00:04:25,880
So you don't have to have a state bar, but you get the patent bar.

64
00:04:25,880 --> 00:04:31,440
So all I did was I sat for the patent bar, took that exam, passed it, and could represent

65
00:04:31,440 --> 00:04:33,800
clients and helping them get patents.

66
00:04:33,800 --> 00:04:34,800
That's awesome.

67
00:04:34,800 --> 00:04:41,000
I wonder if we'll have any takers, you know, listeners that try to pick up this side hustle.

68
00:04:41,000 --> 00:04:42,800
Sounds like a nice one.

69
00:04:42,800 --> 00:04:46,120
Well, I really doubt tell with my interest, right?

70
00:04:46,120 --> 00:04:51,000
Because I mean, patents are kind of, especially the claims part of the patent is almost

71
00:04:51,000 --> 00:04:54,880
like a programming language in itself, it's a legal programming language.

72
00:04:54,880 --> 00:04:58,920
And to write the description part of the patent, you have to be technical, right?

73
00:04:58,920 --> 00:05:03,160
So you have to be an engineer and I was, since I had a CS and electrical engineering

74
00:05:03,160 --> 00:05:07,720
training, I was able to translate that into a patent language.

75
00:05:07,720 --> 00:05:11,840
And then think of like alternative embodiments of the invention, right, that the inventor

76
00:05:11,840 --> 00:05:12,840
came up with.

77
00:05:12,840 --> 00:05:13,840
Nice.

78
00:05:13,840 --> 00:05:14,840
Nice.

79
00:05:14,840 --> 00:05:23,000
Yeah, my sister sat out to Lori is a patent, an IP lawyer at Intel.

80
00:05:23,000 --> 00:05:28,960
And we have really interesting conversations about the public aspects of what she does.

81
00:05:28,960 --> 00:05:31,640
It definitely sounds like interesting work.

82
00:05:31,640 --> 00:05:37,240
But you know, on to machine learning AI and Diffbot, tell us a little bit about the

83
00:05:37,240 --> 00:05:38,240
company.

84
00:05:38,240 --> 00:05:47,480
You know, as I mentioned, I was at Stanford in grad school and, you know, I was, you know,

85
00:05:47,480 --> 00:05:51,520
kind of procrastinating from writing my thesis, you know, and thinking of like what kind

86
00:05:51,520 --> 00:05:55,080
of area of AI research to specialize in.

87
00:05:55,080 --> 00:06:01,280
But I realized that, you know, there's essentially three key drivers to improving AI, right?

88
00:06:01,280 --> 00:06:03,640
There's people that work on the hardware.

89
00:06:03,640 --> 00:06:07,080
There's people that work on improving the software and algorithms.

90
00:06:07,080 --> 00:06:09,760
And there's people that work on data, right?

91
00:06:09,760 --> 00:06:15,120
And there's large public companies that focus on essentially Moore's law, like making the

92
00:06:15,120 --> 00:06:20,080
hardware faster, like NVIDIA and, you know, where your relative works at Intel, right?

93
00:06:20,080 --> 00:06:22,360
They're making the chips faster.

94
00:06:22,360 --> 00:06:28,280
There's tons of people now making the algorithms better, right, including what I was supposed

95
00:06:28,280 --> 00:06:33,600
to be doing as a grad student, as well as, you know, things like TensorFlow and PyTorch,

96
00:06:33,600 --> 00:06:35,000
like these actual frameworks.

97
00:06:35,000 --> 00:06:39,680
Yeah, I was going to say, I'd argue that there's at least a fourth category of tools that

98
00:06:39,680 --> 00:06:44,880
support the folks that are using the algorithms, but not to take away from your point.

99
00:06:44,880 --> 00:06:45,880
I get it.

100
00:06:45,880 --> 00:06:46,880
Totally.

101
00:06:46,880 --> 00:06:48,680
Yeah, the tools, I mean, I would include that in software, right?

102
00:06:48,680 --> 00:06:53,360
It's the algorithms itself plus a software, but yeah, you could separate that out.

103
00:06:53,360 --> 00:06:57,680
But the third category data, I feel like there isn't like sort of like an everything

104
00:06:57,680 --> 00:06:59,200
store of data, right?

105
00:06:59,200 --> 00:07:05,480
If you're building an AI application, you generally have the data as part of your current

106
00:07:05,480 --> 00:07:06,480
process, right?

107
00:07:06,480 --> 00:07:10,920
Or you start rolling up your own sleeves and gathering information, right?

108
00:07:10,920 --> 00:07:16,440
So this became really clear, actually, at that time, I was at Stanford because that's around,

109
00:07:16,440 --> 00:07:21,040
you know, just down the hall, Fei-Fei Li was coming up with the ImageNet, right?

110
00:07:21,040 --> 00:07:25,640
Which is a very large set of annotated images, right?

111
00:07:25,640 --> 00:07:29,880
And, you know, I think it's like about a million images classified into about a thousand

112
00:07:29,880 --> 00:07:33,360
since set categories of WordNet.

113
00:07:33,360 --> 00:07:38,240
And that dataset is really what kicked off the deep learning revolution, right?

114
00:07:38,240 --> 00:07:41,720
Just that amount of labeled structured data.

115
00:07:41,720 --> 00:07:46,520
And so, you know, a lot of neural networks were invented way before, right?

116
00:07:46,520 --> 00:07:52,280
Like in the 70s and 80s, and we've made some tweaks to improve how fast they train

117
00:07:52,280 --> 00:07:57,040
and our new architectures and so forth, but it was really that dataset that made computer

118
00:07:57,040 --> 00:08:01,720
vision go from something that was basically a research grade, you know, task into something

119
00:08:01,720 --> 00:08:03,320
that's production grade, right?

120
00:08:03,320 --> 00:08:06,960
Something that's a little bit better than random to something that's approaching human

121
00:08:06,960 --> 00:08:10,480
level of classification accuracy.

122
00:08:10,480 --> 00:08:13,840
So at that time, I was thinking about, you know, how could you build like an ImageNet

123
00:08:13,840 --> 00:08:17,560
for language or general concepts, right?

124
00:08:17,560 --> 00:08:22,080
And the way that they built ImageNet, basically using Google Image Search and Amazon Mechanical

125
00:08:22,080 --> 00:08:29,400
Turk, you would require a calculated about 50 manures with a team of about 20 people to

126
00:08:29,400 --> 00:08:34,120
build a similar kind of dataset for language because there's languages way more complex

127
00:08:34,120 --> 00:08:35,120
than vision, right?

128
00:08:35,120 --> 00:08:41,600
Like human beings alone have language and like all animals have computer vision.

129
00:08:41,600 --> 00:08:43,000
But then there's way more concepts, right?

130
00:08:43,000 --> 00:08:47,760
So if you think about a number of concepts even on Wikipedia, there's about a million or

131
00:08:47,760 --> 00:08:54,240
so a quarter of magnitude pages on Wikipedia and so just having about a thousand labeled

132
00:08:54,240 --> 00:09:00,320
examples of each of those concepts, you quickly stack up like how much it would take.

133
00:09:00,320 --> 00:09:03,960
So I started thinking about, well, where is all this knowledge?

134
00:09:03,960 --> 00:09:06,280
It exists on the public web.

135
00:09:06,280 --> 00:09:11,440
It's the web is the largest, you know, resource of public knowledge we have as a species.

136
00:09:11,440 --> 00:09:15,680
But the problem is the information is stored in all of these documents.

137
00:09:15,680 --> 00:09:17,680
So it's not structured data, right?

138
00:09:17,680 --> 00:09:19,000
It's not machine readable.

139
00:09:19,000 --> 00:09:24,200
And so if only we could create an algorithm that could actually read and understand all

140
00:09:24,200 --> 00:09:29,160
of those pages on the web and convert it into a coherent machine readable structure, then

141
00:09:29,160 --> 00:09:33,920
we would have solved this problem, essentially using AI.

142
00:09:33,920 --> 00:09:37,040
And so that got me to thinking about the idea behind Diffbot.

143
00:09:37,040 --> 00:09:42,360
So the mission of our company is to build the world's first complete map of the human knowledge

144
00:09:42,360 --> 00:09:48,200
and make it machine readable so that other companies can build all kinds of smart experiences

145
00:09:48,200 --> 00:09:49,920
are on top of it.

146
00:09:49,920 --> 00:09:54,320
And so we can have that future that we all won't watch with intelligent agents all around

147
00:09:54,320 --> 00:09:55,320
us, right?

148
00:09:55,320 --> 00:09:58,080
That can benefit from structured information.

149
00:09:58,080 --> 00:09:59,080
So how to do that?

150
00:09:59,080 --> 00:10:04,400
It's kind of a big, a big task and we don't have the resources, you know, bootstrapping

151
00:10:04,400 --> 00:10:07,600
to crawl the whole internet from day one.

152
00:10:07,600 --> 00:10:15,400
In fact, one of the first things that I thought as you kind of laid out that mission is, you

153
00:10:15,400 --> 00:10:20,760
know, Google and Microsoft are both out there talking about their knowledge graphs and

154
00:10:20,760 --> 00:10:27,920
how they're kind of the very core of what they're able to do in many cases, machine learning

155
00:10:27,920 --> 00:10:29,760
and AI and beyond.

156
00:10:29,760 --> 00:10:30,760
Yeah.

157
00:10:30,760 --> 00:10:35,720
A lot of those come from, you know, their experience is building the Google and being search

158
00:10:35,720 --> 00:10:37,440
engines.

159
00:10:37,440 --> 00:10:43,760
You know, massive, massive investments in pulling all that together, sounds like how can we

160
00:10:43,760 --> 00:10:44,760
do it?

161
00:10:44,760 --> 00:10:45,760
Exactly.

162
00:10:45,760 --> 00:10:49,680
That's a lot of people ask us for sure.

163
00:10:49,680 --> 00:10:52,040
So how Google really coined that word knowledge graph.

164
00:10:52,040 --> 00:10:57,880
So how they did it is basically the history behind that, they acquired a company called

165
00:10:57,880 --> 00:10:58,880
MetaWeb, right?

166
00:10:58,880 --> 00:11:04,440
They had a project called Freebase and Freebase was essentially that.

167
00:11:04,440 --> 00:11:09,560
It's Freebase was basically imported all of the information from Wikipedia, those info

168
00:11:09,560 --> 00:11:11,240
boxes on Wikipedia.

169
00:11:11,240 --> 00:11:16,120
And then they had like a crowd editor that basically kind of allowed you to edit Freebase,

170
00:11:16,120 --> 00:11:17,600
like random people on the internet, right?

171
00:11:17,600 --> 00:11:19,160
So kind of crowdsourcing the problem.

172
00:11:19,160 --> 00:11:20,160
Mm-hmm.

173
00:11:20,160 --> 00:11:21,160
And Google acquired.

174
00:11:21,160 --> 00:11:22,160
Yeah.

175
00:11:22,160 --> 00:11:23,160
Sorry.

176
00:11:23,160 --> 00:11:24,160
Go ahead.

177
00:11:24,160 --> 00:11:25,160
Yeah.

178
00:11:25,160 --> 00:11:29,920
It's interesting that I think I envisioned something much more, I don't know, glamorous

179
00:11:29,920 --> 00:11:36,840
is the right word, but you know, something more learned than something that started with

180
00:11:36,840 --> 00:11:37,840
Wikipedia.

181
00:11:37,840 --> 00:11:44,360
Like they kind of used the page rank graph and figured out what the concepts were and

182
00:11:44,360 --> 00:11:47,880
you know, did something, I guess exotic is what I envisioned.

183
00:11:47,880 --> 00:11:48,880
Yeah.

184
00:11:48,880 --> 00:11:51,880
I mean, people just, they have the assumption that, okay, they're Google, they have infinite

185
00:11:51,880 --> 00:11:52,880
resources.

186
00:11:52,880 --> 00:11:55,040
So everything is machine learning, right?

187
00:11:55,040 --> 00:11:56,040
You see, right?

188
00:11:56,040 --> 00:11:59,440
The knowledge channels and everything like that, right?

189
00:11:59,440 --> 00:12:05,080
But the reality is, when you're searching on Google, only things that generally have a

190
00:12:05,080 --> 00:12:09,600
Wikipedia page have a knowledge panel, at least that was originally when it launched,

191
00:12:09,600 --> 00:12:13,840
like you type in someone who's famous and you'll get a knowledge panel, right?

192
00:12:13,840 --> 00:12:15,760
But you type in like a regular Joe, right?

193
00:12:15,760 --> 00:12:19,640
Or you type in one of your relatives, your friends, your colleagues, they don't get a knowledge

194
00:12:19,640 --> 00:12:20,640
panel, right?

195
00:12:20,640 --> 00:12:21,640
Look, why is that?

196
00:12:21,640 --> 00:12:22,640
Because no one's added it.

197
00:12:22,640 --> 00:12:28,480
You know, it's actually great, even though there's pages about them for sure, right?

198
00:12:28,480 --> 00:12:35,400
And so what the, at these large companies, many of which are customers actually go more

199
00:12:35,400 --> 00:12:42,000
into that, you know, if we have time, but they basically start out with Wikipedia and then

200
00:12:42,000 --> 00:12:44,720
allow there's ability to edit and curate it.

201
00:12:44,720 --> 00:12:48,840
And then this knowledge graph basically becomes like a file format within the company that

202
00:12:48,840 --> 00:12:51,200
many teams contribute to actually, right?

203
00:12:51,200 --> 00:12:54,920
So a lot of these data sources, they're licensed from third parties, right?

204
00:12:54,920 --> 00:12:59,680
Like the sports scores and things like that, the weather feeds and stats, other pieces

205
00:12:59,680 --> 00:13:05,320
of information that go into it are built by a specific department at the company.

206
00:13:05,320 --> 00:13:09,240
So there'll be like a recipes department that focuses on the recipe section of the knowledge

207
00:13:09,240 --> 00:13:10,240
graph.

208
00:13:10,240 --> 00:13:14,880
And they'll have an entire army of curators and stuff that is uncurrating those particular

209
00:13:14,880 --> 00:13:15,880
sections.

210
00:13:15,880 --> 00:13:21,960
But the different knowledge graph is the only one that is fully built by an autonomous

211
00:13:21,960 --> 00:13:22,960
system, right?

212
00:13:22,960 --> 00:13:27,640
We don't have the resources to hire thousands of curators and labelers to curate all

213
00:13:27,640 --> 00:13:30,200
different aspects of knowledge.

214
00:13:30,200 --> 00:13:34,880
So the biggest difference is between us is a, well, first of all, our knowledge graph is

215
00:13:34,880 --> 00:13:35,880
much larger, right?

216
00:13:35,880 --> 00:13:39,760
Because it's based on actually a different technique of crawling all the pages on the web and

217
00:13:39,760 --> 00:13:41,400
building it.

218
00:13:41,400 --> 00:13:47,120
So it does have like those average Joe entities and startups and smaller companies in it.

219
00:13:47,120 --> 00:13:51,400
It has about 10 billion entities in the knowledge graph and about a trillion edges.

220
00:13:51,400 --> 00:13:57,680
Thirdly, it's, secondly, sorry, it's, you know, it's available for use, right?

221
00:13:57,680 --> 00:14:01,720
So it's not just for like consumer search where, you know, it's good for like the Kanye

222
00:14:01,720 --> 00:14:03,800
West or Taylor Swift query, right?

223
00:14:03,800 --> 00:14:08,320
But it's good for the kinds of entities that you would interact with in the business world,

224
00:14:08,320 --> 00:14:10,040
your suppliers, your vendors, right?

225
00:14:10,040 --> 00:14:12,640
Your customers, people you're trying to recruit, right?

226
00:14:12,640 --> 00:14:16,600
So I always like to say, you're not usually trying to recruit, you know, like Donald Trump

227
00:14:16,600 --> 00:14:20,440
or like higher tide, you know, sell something to Tiger Woods, these kind of entities that

228
00:14:20,440 --> 00:14:23,640
are in these consumer knowledge graphs, right?

229
00:14:23,640 --> 00:14:27,920
But those entities that you would deal with in the business world are actually in ours.

230
00:14:27,920 --> 00:14:32,400
So ours, I would argue, is much more useful to building real things.

231
00:14:32,400 --> 00:14:34,080
And then you can use it.

232
00:14:34,080 --> 00:14:35,520
So that's a very important point too.

233
00:14:35,520 --> 00:14:39,360
Like the Google Knowledge Graph, you can't actually pay like to access it, right?

234
00:14:39,360 --> 00:14:44,000
And that has to do with, you know, for strategic and business reasons, they don't want people

235
00:14:44,000 --> 00:14:48,640
to just build a skin on top of Google and, and they just have like a, you know, a competitive

236
00:14:48,640 --> 00:14:51,840
product offering.

237
00:14:51,840 --> 00:14:56,200
So they haven't focused on that as their main revenue model.

238
00:14:56,200 --> 00:15:02,680
One of the use cases that I often hear Google and Microsoft talking about the contribution

239
00:15:02,680 --> 00:15:06,480
of their knowledge graphs is with virtual assistants.

240
00:15:06,480 --> 00:15:13,640
Do you find folks using Diffbot as a kind of foundational component for building that

241
00:15:13,640 --> 00:15:17,640
kind of virtual assistant bot experience?

242
00:15:17,640 --> 00:15:18,640
Yes.

243
00:15:18,640 --> 00:15:23,200
I can't share too many details, right, about the companies that use it in that way.

244
00:15:23,200 --> 00:15:27,120
But they include big companies as well as startups.

245
00:15:27,120 --> 00:15:32,400
The main problem that everyone's trying to solve in this category is basically a virtual

246
00:15:32,400 --> 00:15:36,800
assistant needs to have knowledge in order to be intelligent, right?

247
00:15:36,800 --> 00:15:41,640
Most of the time you ask Alexa or Siri a question, you know, if it hasn't been something that's

248
00:15:41,640 --> 00:15:44,960
been pre-programmed and it's not going to be able to answer that, right?

249
00:15:44,960 --> 00:15:48,480
You have to almost talk like a robot to communicate with these systems.

250
00:15:48,480 --> 00:15:52,400
You have to talk in certain templates, right?

251
00:15:52,400 --> 00:15:55,920
And if they could solve actually the problem of knowledge, they'd be able to answer, right?

252
00:15:55,920 --> 00:15:58,080
Almost any question that's, that's askable.

253
00:15:58,080 --> 00:15:59,440
That's like a public fact.

254
00:15:59,440 --> 00:16:04,000
That is definitely one of the applications that we see of the knowledge graph.

255
00:16:04,000 --> 00:16:09,720
In general, though, we have, you know, over 400 companies that currently use Diffbot.

256
00:16:09,720 --> 00:16:13,640
There's, that's example of a consumer application, like an intelligent assistant.

257
00:16:13,640 --> 00:16:18,800
We have a lot of the major search engines like DuckDuckGo, Yandex, Bing, where we're

258
00:16:18,800 --> 00:16:22,280
powering like their knowledge panels that you see, you know, so we're powering parts

259
00:16:22,280 --> 00:16:24,840
of their knowledge experience.

260
00:16:24,840 --> 00:16:29,880
And then we have a lot of consumer apps like Instapaper, Snapchat, we power like the articles

261
00:16:29,880 --> 00:16:30,880
for you and that.

262
00:16:30,880 --> 00:16:35,080
There's like a wedding planning app, Zola, where people build like a wedding registry.

263
00:16:35,080 --> 00:16:37,640
And then there's a whole bunch of business process applications.

264
00:16:37,640 --> 00:16:39,800
So you can use a knowledge graph to find sales leads.

265
00:16:39,800 --> 00:16:42,120
You can use it to find people to hire.

266
00:16:42,120 --> 00:16:47,120
You can use it to enrich your current CRM to better understand your customer insights,

267
00:16:47,120 --> 00:16:48,120
right?

268
00:16:48,120 --> 00:16:51,720
If you're a brand, you can use it to track like all the places online that are selling

269
00:16:51,720 --> 00:16:56,680
Nike's and monitor if anyone's selling fake Nike's or have counterfeit goods and things

270
00:16:56,680 --> 00:16:57,680
like that.

271
00:16:57,680 --> 00:17:03,960
So there's a whole bunch of BI and market intelligence applications too that we're seeing

272
00:17:03,960 --> 00:17:04,960
now.

273
00:17:04,960 --> 00:17:05,960
This new product.

274
00:17:05,960 --> 00:17:06,960
Interesting.

275
00:17:06,960 --> 00:17:10,280
And so what's the typical user developer experience?

276
00:17:10,280 --> 00:17:15,400
So if you're a developer wanting to use DiffBots products, there's basically three main ways

277
00:17:15,400 --> 00:17:16,640
you can use it.

278
00:17:16,640 --> 00:17:18,880
The first is our extraction APIs, right?

279
00:17:18,880 --> 00:17:24,040
So you can pass in an individual URL from the web to our endpoint and then our machine

280
00:17:24,040 --> 00:17:27,800
learning will classify that URL and then extract it into an entity, right?

281
00:17:27,800 --> 00:17:31,920
So if you pass in like a product page, it'll say this is a product page.

282
00:17:31,920 --> 00:17:36,640
It's a type product and here's the price and here's the image and name of the product

283
00:17:36,640 --> 00:17:41,480
and description and SKU and weight and all those product facts, right?

284
00:17:41,480 --> 00:17:48,120
Actually, let's just pause on that because I've got a little bit of experience trying

285
00:17:48,120 --> 00:17:49,760
to extract data from pages.

286
00:17:49,760 --> 00:17:55,880
I'm sure a lot of people that are listening have tried to do this and it is historically

287
00:17:55,880 --> 00:17:56,880
very, very hard.

288
00:17:56,880 --> 00:18:02,200
I mean, you have, first of all, trying to do if you're doing it based on regular expressions

289
00:18:02,200 --> 00:18:05,840
or X-Path, there's all different kinds of ways to do it.

290
00:18:05,840 --> 00:18:07,640
They're all super fragile.

291
00:18:07,640 --> 00:18:10,080
The sites change and they break all the time.

292
00:18:10,080 --> 00:18:11,040
They break all those roles.

293
00:18:11,040 --> 00:18:12,040
Yeah.

294
00:18:12,040 --> 00:18:13,040
Right.

295
00:18:13,040 --> 00:18:16,560
So what you're saying sounds like magic, like, you know?

296
00:18:16,560 --> 00:18:17,560
Yeah.

297
00:18:17,560 --> 00:18:19,560
So we launched that on hacker news, right?

298
00:18:19,560 --> 00:18:24,200
I'm sure some of your listeners are familiar with that site and that's what a lot of

299
00:18:24,200 --> 00:18:25,200
developers say.

300
00:18:25,200 --> 00:18:27,320
This is basically like magic.

301
00:18:27,320 --> 00:18:31,800
As an alternative, you'd have to use something like impar.io or scrapy, right?

302
00:18:31,800 --> 00:18:35,560
And like you said, create all these patterns and then maintain them.

303
00:18:35,560 --> 00:18:39,600
That's actually fine if you just want to get information from one site on a one time

304
00:18:39,600 --> 00:18:40,600
job, right?

305
00:18:40,600 --> 00:18:41,600
Right.

306
00:18:41,600 --> 00:18:42,680
And maybe take a few minutes to configure that.

307
00:18:42,680 --> 00:18:46,680
But it's a problem when you want an ongoing process, right?

308
00:18:46,680 --> 00:18:49,880
And you want to get information at large scale, like from thousands of sites, right?

309
00:18:49,880 --> 00:18:54,120
It starts to become tractable to maintain that, like 15% of your roles will break each

310
00:18:54,120 --> 00:18:55,120
week, right?

311
00:18:55,120 --> 00:18:56,120
Right.

312
00:18:56,120 --> 00:18:57,120
Right.

313
00:18:57,120 --> 00:19:01,600
And so with the machine learning based approach, it's robust to any changes, like you

314
00:19:01,600 --> 00:19:05,720
said in the design or the layout and you don't have to create any roles.

315
00:19:05,720 --> 00:19:09,160
You just pass in the URL and we don't have to create any roles because you can literally

316
00:19:09,160 --> 00:19:10,440
pass in a URL from anywhere.

317
00:19:10,440 --> 00:19:13,160
So it's like we can't, right?

318
00:19:13,160 --> 00:19:18,280
And then the other thing that's quite distinctive too is it works across any language on the web

319
00:19:18,280 --> 00:19:19,280
too.

320
00:19:19,280 --> 00:19:23,440
So you can pass in a page, like a Japanese e-commerce page, which has totally different

321
00:19:23,440 --> 00:19:24,440
design conventions, right?

322
00:19:24,440 --> 00:19:28,960
Or like a German article page and it'll parse it perfectly as well.

323
00:19:28,960 --> 00:19:32,120
So a lot of people use it because of that aspect.

324
00:19:32,120 --> 00:19:37,280
So that was our first product, really popular as an alternative to writing your own custom

325
00:19:37,280 --> 00:19:38,800
web scraper.

326
00:19:38,800 --> 00:19:41,160
The second way developers can use it is called crawlbot.

327
00:19:41,160 --> 00:19:44,760
So that's crawling an entire domain essentially, right?

328
00:19:44,760 --> 00:19:49,480
So it'll start from those C-D-R-Ls and then it'll, that's how you can get essentially

329
00:19:49,480 --> 00:19:51,360
the entire database from a site, right?

330
00:19:51,360 --> 00:19:56,440
So you can say, I want all of the products from Target, Macy's, J-Crew, Dan or Republic

331
00:19:56,440 --> 00:20:02,600
Home Depot, and then you get the entire product catalog, right, of those sites synchronized.

332
00:20:02,600 --> 00:20:05,400
And then the third way is, of course, the knowledge graph.

333
00:20:05,400 --> 00:20:09,680
And the way that you interface with that as a developer is with the Diffbot query language,

334
00:20:09,680 --> 00:20:13,280
which is basically kind of like a structured semantic search.

335
00:20:13,280 --> 00:20:17,600
So you can almost search the web as if it was like a huge, you know, a structured semantic

336
00:20:17,600 --> 00:20:19,480
database.

337
00:20:19,480 --> 00:20:25,080
And we also have like a UI that allows you to help you build those queries, sort of for

338
00:20:25,080 --> 00:20:26,640
the less technical users.

339
00:20:26,640 --> 00:20:32,040
And then for like business users, we integrate, right, the knowledge graph into the tools

340
00:20:32,040 --> 00:20:33,040
they already use, right?

341
00:20:33,040 --> 00:20:37,920
So you can export it as CSV, and so you can open it in Excel, or you, we plan to build

342
00:20:37,920 --> 00:20:44,720
integrations directly into things like Tableau, Salesforce, Excel, the actual tools that you

343
00:20:44,720 --> 00:20:50,400
might be, you know, your actual daily driver, right, where you're doing your data manipulation,

344
00:20:50,400 --> 00:20:53,160
being able to tap into the knowledge graph directly from those.

345
00:20:53,160 --> 00:20:58,320
And so when you're writing these queries, you're using machine learning on the backend

346
00:20:58,320 --> 00:21:05,920
to do the crawling and kind of understand the pages and please elaborate on what specifically

347
00:21:05,920 --> 00:21:08,280
you're doing there.

348
00:21:08,280 --> 00:21:15,720
But are you also applying some kind of machine learning and interpreting the query itself?

349
00:21:15,720 --> 00:21:20,680
So, you know, if I put in a term, it's not just the literal term, but maybe hitting

350
00:21:20,680 --> 00:21:24,920
some embedding or, you know, abstraction that's kind of trying to figure out what I'm looking

351
00:21:24,920 --> 00:21:25,920
for.

352
00:21:25,920 --> 00:21:30,560
Yeah, so there's, like I was saying earlier in this call, like our almost our entire company

353
00:21:30,560 --> 00:21:32,680
is one big machine learning problem.

354
00:21:32,680 --> 00:21:37,040
There's about actually like a 50 or 60 separate machine learning problems that we study

355
00:21:37,040 --> 00:21:38,040
at the bottom, right?

356
00:21:38,040 --> 00:21:43,920
So we're about a, now a 35 person company and like 80% of our company is machine learning

357
00:21:43,920 --> 00:21:45,920
researchers.

358
00:21:45,920 --> 00:21:52,980
So when we crawled the web, that's largely, our VP of Search is Matt Wells, he was the

359
00:21:52,980 --> 00:21:55,360
founder of a search engine called Gigablast, right?

360
00:21:55,360 --> 00:22:00,720
So that's how we're able to, as a small start up, crawl the full web.

361
00:22:00,720 --> 00:22:03,480
But what we, what differs is we render the whole web.

362
00:22:03,480 --> 00:22:08,960
So we're actually running the web inside real, real browsing engines and playing the web,

363
00:22:08,960 --> 00:22:10,720
almost like a video game.

364
00:22:10,720 --> 00:22:15,160
We serialize from every page, essentially all the pixels on the page, the geometry of

365
00:22:15,160 --> 00:22:20,560
the page, all the visual styles and layout and the internal state of the virtual machine,

366
00:22:20,560 --> 00:22:23,440
the JavaScript and CSS layout engine.

367
00:22:23,440 --> 00:22:27,360
And those are essentially just like a long, you know, string of numbers.

368
00:22:27,360 --> 00:22:32,280
And that's where our algorithms use those numbers to classify the type of the page, right?

369
00:22:32,280 --> 00:22:35,480
So this page look like a article page.

370
00:22:35,480 --> 00:22:41,000
It has a very different look and layout from a article page or a product page or like

371
00:22:41,000 --> 00:22:42,400
a person page.

372
00:22:42,400 --> 00:22:46,000
And then we use machine learning to extract the particular fields after we've classified

373
00:22:46,000 --> 00:22:47,000
it, right?

374
00:22:47,000 --> 00:22:52,040
So on a product page, look for the things that look like the price of the product, look

375
00:22:52,040 --> 00:22:54,880
for the things that look like the image of the product, right?

376
00:22:54,880 --> 00:22:58,760
And then analyze the actual image to determine what's the color of the product, what material

377
00:22:58,760 --> 00:22:59,760
is it made from?

378
00:22:59,760 --> 00:23:00,760
Right?

379
00:23:00,760 --> 00:23:06,760
Is it a, you know, red sports car inside at which model of cars it, right?

380
00:23:06,760 --> 00:23:09,400
Is it a brown sweater, right?

381
00:23:09,400 --> 00:23:13,400
Like what kind of fabric and swatch pattern and such and so forth.

382
00:23:13,400 --> 00:23:16,880
And then we're analyzing the text of the page as well.

383
00:23:16,880 --> 00:23:24,160
So, you know, inside an organization's description, it might include like what is the category

384
00:23:24,160 --> 00:23:27,320
of that organization when it was founded, where it's based, right?

385
00:23:27,320 --> 00:23:29,840
Who are the main officers of the company?

386
00:23:29,840 --> 00:23:35,160
So to do that, you need to do various kinds of natural language processing.

387
00:23:35,160 --> 00:23:40,520
So we have folks that have developed the state of the art in entity linking, working at

388
00:23:40,520 --> 00:23:43,240
Diffbot to find the entities in the text.

389
00:23:43,240 --> 00:23:47,520
We do what's called a relation extraction to find the relations between those entities.

390
00:23:47,520 --> 00:23:49,560
And we also do machine translation, right?

391
00:23:49,560 --> 00:23:55,040
Because the text could be a non-English to start out with like Arabic or Chinese, right?

392
00:23:55,040 --> 00:24:00,000
And then once we've extracted these things, we then need to be able to link it across pages,

393
00:24:00,000 --> 00:24:01,000
right?

394
00:24:01,000 --> 00:24:06,120
There could be one page about Sam Charrington and then another one on a different page

395
00:24:06,120 --> 00:24:07,120
on the web.

396
00:24:07,120 --> 00:24:08,680
But we know they're the same real world person, right?

397
00:24:08,680 --> 00:24:12,280
You don't want to have two entries in the knowledge graph for that, right?

398
00:24:12,280 --> 00:24:16,480
So we need to use machine learning to link together those extractions across multiple

399
00:24:16,480 --> 00:24:17,720
pages.

400
00:24:17,720 --> 00:24:22,200
And then we work on a problem called knowledge fusion, which is given all that evidence,

401
00:24:22,200 --> 00:24:25,080
what is the probability of truth of each of those statements?

402
00:24:25,080 --> 00:24:28,760
And then we write the really highly confident facts, like as triples, into the knowledge

403
00:24:28,760 --> 00:24:29,760
graph.

404
00:24:29,760 --> 00:24:34,400
And that's kind of like end-to-end kind of soup to nuts, how it goes from a page into

405
00:24:34,400 --> 00:24:38,360
an entity, right, inside this AI-sensized system.

406
00:24:38,360 --> 00:24:42,640
We build a new knowledge graph, like around every four days.

407
00:24:42,640 --> 00:24:47,440
And then on the query side, of course, we have structured querying, right, like such as

408
00:24:47,440 --> 00:24:52,560
the default query language, there can be ambiguity, right, in parts of the syntax.

409
00:24:52,560 --> 00:24:58,400
Like if I say I'm looking for machine learning engineers that live in Mountain View, well,

410
00:24:58,400 --> 00:25:01,880
it needs to interpret whether that's Mountain View, California, or Mountain View, Arkansas,

411
00:25:01,880 --> 00:25:02,880
right?

412
00:25:02,880 --> 00:25:05,040
There's another city over there that's called Mountain View.

413
00:25:05,040 --> 00:25:08,000
There's also another Mountain View, you know, outside of the US.

414
00:25:08,000 --> 00:25:11,280
But we all know which one, you know, we're likely referring to.

415
00:25:11,280 --> 00:25:16,080
So it needs to take in the stats, right, to interpret that statement.

416
00:25:16,080 --> 00:25:19,680
And then we're also doing at the research level, natural language question answering, right,

417
00:25:19,680 --> 00:25:23,880
which is more in line with the kind of earlier question about assistance.

418
00:25:23,880 --> 00:25:24,880
Yeah.

419
00:25:24,880 --> 00:25:28,600
You know, what's fascinating about this is that, you know, when you talk about the extraction

420
00:25:28,600 --> 00:25:36,920
problem and identifying the pictures and identifying the, you know, what's probably the price,

421
00:25:36,920 --> 00:25:45,120
it sounds both super, you know, simple, really from the straight forward, but also like

422
00:25:45,120 --> 00:25:48,840
terribly, terribly complex at scale.

423
00:25:48,840 --> 00:25:54,360
The trick is getting it to, it's easy to get 80% accuracy, but to get to the level back

424
00:25:54,360 --> 00:25:59,200
or see needed by commercial customers, you know, and at an at scale across, you see a

425
00:25:59,200 --> 00:26:02,840
lot of weird stuff when you crawl the whole internet, right, there's all kinds of, of

426
00:26:02,840 --> 00:26:07,520
wacky stuff going on in the long tail of the web, but to get it to work perfectly there

427
00:26:07,520 --> 00:26:13,440
as well, right, that's where it's really hard and we'll be working on this problem for

428
00:26:13,440 --> 00:26:14,440
many years.

429
00:26:14,440 --> 00:26:23,560
Now, often when a company is tackling these kinds of problems, you know, whether it's information

430
00:26:23,560 --> 00:26:29,800
extraction or a natural language processing, there's, you know, we're using machine learning,

431
00:26:29,800 --> 00:26:37,640
but also whether, you know, for exceptions or, you know, the kind of under the covers

432
00:26:37,640 --> 00:26:42,880
thing that's doing the heavy lifting is, you know, some old school, you know, rejects

433
00:26:42,880 --> 00:26:46,640
or rule rules base or something like that.

434
00:26:46,640 --> 00:26:52,440
I can't imagine that, you know, again, scaling working in this context, do you kind of totally

435
00:26:52,440 --> 00:26:57,600
issue that, you know, those types of approaches or do you, do you do them and have you found

436
00:26:57,600 --> 00:26:59,920
a way to fuse them in a way that works?

437
00:26:59,920 --> 00:27:03,320
I mean, I guess, you know, we established a Google kind of does this.

438
00:27:03,320 --> 00:27:08,720
So, you know, it can be done at scale, but perhaps not with the team of 35.

439
00:27:08,720 --> 00:27:09,720
Yeah.

440
00:27:09,720 --> 00:27:14,760
So actually, if you are a user of this bot and you have access to developer APIs, when

441
00:27:14,760 --> 00:27:19,880
you log into our developer dashboard, there is an ability to override what's extracted

442
00:27:19,880 --> 00:27:21,320
by our AI.

443
00:27:21,320 --> 00:27:27,200
So essentially, if, for example, you know, our attraction works pretty well.

444
00:27:27,200 --> 00:27:32,200
It has over 95 percent, you know, like precision and recall.

445
00:27:32,200 --> 00:27:36,920
But if it made a mistake for whatever reason, you have an ability to actually say, hey, no,

446
00:27:36,920 --> 00:27:42,000
this was the actual price of the product and override that with a rule as a customer.

447
00:27:42,000 --> 00:27:46,040
And so you kind of crowdsource the corrections?

448
00:27:46,040 --> 00:27:47,640
Yeah, exactly.

449
00:27:47,640 --> 00:27:51,760
That basically allows someone who's non-technical with like a visual interface, right, to kind

450
00:27:51,760 --> 00:27:52,920
of correct it.

451
00:27:52,920 --> 00:27:56,320
That basically, you know, it works now for you.

452
00:27:56,320 --> 00:28:00,200
And then the second thing it does is basically, it takes that rectangle, right, that they

453
00:28:00,200 --> 00:28:03,000
clicked on and then it adds it to our training set, right?

454
00:28:03,000 --> 00:28:05,720
So it improves the global model for everybody.

455
00:28:05,720 --> 00:28:12,760
So it doesn't create a rule that is, you know, an override for everyone just for that

456
00:28:12,760 --> 00:28:13,760
user.

457
00:28:13,760 --> 00:28:14,760
Okay.

458
00:28:14,760 --> 00:28:15,760
Yeah.

459
00:28:15,760 --> 00:28:16,760
Interesting.

460
00:28:16,760 --> 00:28:17,760
Yeah.

461
00:28:17,760 --> 00:28:18,760
Yeah.

462
00:28:18,760 --> 00:28:19,760
That is basically trained in that.

463
00:28:19,760 --> 00:28:20,760
It's kind of like your spam, right?

464
00:28:20,760 --> 00:28:25,200
Like you might mark stuff as spam or not spam in your own inbox and it affects you.

465
00:28:25,200 --> 00:28:28,400
But then it also helps, you know, your email provider's global model.

466
00:28:28,400 --> 00:28:34,680
So I'm curious when you set out to start the company or, you know, thinking about the

467
00:28:34,680 --> 00:28:42,520
evolution of the company, is it like a machine learning, research project organization

468
00:28:42,520 --> 00:28:48,760
that turned into a commercial entity or was it a commercial entity that, you know, eventually

469
00:28:48,760 --> 00:28:53,240
found in order to really do this, you have to be a really heavy research organization.

470
00:28:53,240 --> 00:28:58,880
Well, I mean, the purpose was, you know, as I said, it was before, it's a try to build

471
00:28:58,880 --> 00:29:02,080
the first complete map of human knowledge, right?

472
00:29:02,080 --> 00:29:07,880
And it just turns out that a corporate structure, I found is the best way to organize labor,

473
00:29:07,880 --> 00:29:08,880
right?

474
00:29:08,880 --> 00:29:15,200
But at the, for the first couple of years, yes, it was just pretty much me sitting in a

475
00:29:15,200 --> 00:29:18,160
dark basement, like working on the math problems, right?

476
00:29:18,160 --> 00:29:23,160
Because I really wanted to make sure that the technology actually worked before trying

477
00:29:23,160 --> 00:29:28,240
to do things like hire a bunch of people or scale it or raise money or things like that,

478
00:29:28,240 --> 00:29:30,240
right?

479
00:29:30,240 --> 00:29:37,240
Because I think all too often with a lot of AI projects, like they might work well at

480
00:29:37,240 --> 00:29:41,680
a researcher prototype phase, but then they're not at the level of quality that someone would

481
00:29:41,680 --> 00:29:44,840
actually pay to use it, right, in the business world, right?

482
00:29:44,840 --> 00:29:49,520
And then so they, they'll have problems down the line trying to commercialize it, right?

483
00:29:49,520 --> 00:29:54,840
So another unique aspect of our company, even though it is primarily an AI research company,

484
00:29:54,840 --> 00:29:56,400
is that we're profitable.

485
00:29:56,400 --> 00:30:01,520
So a lot of other groups that do AI research are either subsidized by another part of the

486
00:30:01,520 --> 00:30:02,520
company, right?

487
00:30:02,520 --> 00:30:08,120
Like, like, most of the major tech companies, right, they aren't themselves a profit center,

488
00:30:08,120 --> 00:30:13,840
or they're funded by, you know, like a philanthropist, like, like one of those nonprofit,

489
00:30:13,840 --> 00:30:16,360
you know, AI research companies.

490
00:30:16,360 --> 00:30:17,840
Formerly nonprofit?

491
00:30:17,840 --> 00:30:18,840
Yeah.

492
00:30:18,840 --> 00:30:24,680
Well, open AI, well, also the Alan Institute, I think, is another example where it's funded

493
00:30:24,680 --> 00:30:29,120
by, you know, primarily Paul Allen, but that, I think it's great though, right, that money

494
00:30:29,120 --> 00:30:32,480
is going into advancing the research that we definitely benefit, right, from those

495
00:30:32,480 --> 00:30:33,480
results.

496
00:30:33,480 --> 00:30:40,440
But yeah, so then over time though, as, you know, this kind of web scraping machine learning

497
00:30:40,440 --> 00:30:48,280
AI became popular, we needed to hire people to help keep the servers up, and to grow the

498
00:30:48,280 --> 00:30:49,280
data center.

499
00:30:49,280 --> 00:30:52,280
We crawled the web out of two data centers here in the Bay Area, and so that's when I

500
00:30:52,280 --> 00:30:57,760
started kind of tapping into my Stanford network, eventually got connected with Andy

501
00:30:57,760 --> 00:31:04,000
Backelstein, who was the first investor in Google, so he led our angel round and defbot

502
00:31:04,000 --> 00:31:09,280
and invested twice as much in our company, and then partnered with Skydatin, who is the

503
00:31:09,280 --> 00:31:15,320
founder of both Earthlink as well as cloud kitchens, and then in the series they partnered

504
00:31:15,320 --> 00:31:21,560
up with some of the folks behind Tesla and SpaceX, and folks at Tencent.

505
00:31:21,560 --> 00:31:28,160
I mean, you mentioned that there's kind of 50 machine learning problems or challenges

506
00:31:28,160 --> 00:31:33,600
as you kind of look across the things that you need to do to deliver this offering or

507
00:31:33,600 --> 00:31:35,280
solve this problem.

508
00:31:35,280 --> 00:31:42,600
Are your researchers also, do you publish, do you go to conferences like Noreps and kind

509
00:31:42,600 --> 00:31:45,360
of contribute to the community in that way?

510
00:31:45,360 --> 00:31:51,520
Yeah, so basically the area of machine learning, machine learning is a big tent, right?

511
00:31:51,520 --> 00:31:56,680
But the area that we care about is information extraction, right, from unstructured information,

512
00:31:56,680 --> 00:32:02,360
whether it's text or images or document layout, right, and also knowledge fusion.

513
00:32:02,360 --> 00:32:09,040
So this particular corner of AI, the people that are working at Defbot are in general way

514
00:32:09,040 --> 00:32:12,760
more qualified than me, so I'm probably the least qualified person in the company, and

515
00:32:12,760 --> 00:32:16,080
that they've probably developed a state of the art system in that area, right?

516
00:32:16,080 --> 00:32:23,640
So in unstructured relation extraction, open relation extraction, the folks that developed

517
00:32:23,640 --> 00:32:28,360
the state of the art system there now, in during their PhD work on that problem at Defbot

518
00:32:28,360 --> 00:32:34,040
and have scaled it to the size of the Defbot knowledge graph, one of the previous CTO

519
00:32:34,040 --> 00:32:39,520
of DPPDIA, which is another well-known knowledge graph joined Defbot to focus on knowledge fusion.

520
00:32:39,520 --> 00:32:45,280
So we benefit from these top researchers, and then another thing I should mention is

521
00:32:45,280 --> 00:32:50,720
we give free access to our knowledge graph to about a dozen or so different academic

522
00:32:50,720 --> 00:32:57,880
AI research groups, and the thinking is that those professors and PhD students and grad

523
00:32:57,880 --> 00:33:04,440
students should be able to stay within academia and do fruitful research in this area of knowledge

524
00:33:04,440 --> 00:33:09,720
graph and large-scale information extraction without having to join a big company.

525
00:33:09,720 --> 00:33:14,360
So we kind of see them and give them access to our knowledge graph, and they've used that

526
00:33:14,360 --> 00:33:16,000
to produce some interesting results.

527
00:33:16,000 --> 00:33:23,920
So we had a collaboration like that that had a paper at NERIPS, and we've also done more

528
00:33:23,920 --> 00:33:26,480
active, more recently, some of our own publishing.

529
00:33:26,480 --> 00:33:34,160
So at the last EMNLP, we released a data set called KnowledgeNet that allows you to, in

530
00:33:34,160 --> 00:33:37,440
the research world, build your own end-to-end knowledge graph.

531
00:33:37,440 --> 00:33:43,680
It's the largest knowledge graph construction data set that's been released so far, and

532
00:33:43,680 --> 00:33:47,640
it's very high quality compared to previous data sets.

533
00:33:47,640 --> 00:33:54,960
And we also released like a baseline system in the open source that is a reference system

534
00:33:54,960 --> 00:33:58,760
on that knowledge-based construction task.

535
00:33:58,760 --> 00:34:03,840
And so that's also being used by a lot of other AI centers right now.

536
00:34:03,840 --> 00:34:08,760
So I think in the earlier years, we're just pretty heads down on getting stuff to work,

537
00:34:08,760 --> 00:34:17,880
and now we're trying to have a more capacity to publish and help other research groups,

538
00:34:17,880 --> 00:34:22,920
and kind of help with knowledge sharing, and help see more research in this area.

539
00:34:22,920 --> 00:34:29,120
So one of the bottlenecks to productive research in this area is, like if you, there hasn't

540
00:34:29,120 --> 00:34:32,960
been a lot of progress in knowledge fusion, for example, in academia, because you need

541
00:34:32,960 --> 00:34:37,160
access to a really large database as a knowledge graph to study that problem, right?

542
00:34:37,160 --> 00:34:41,720
And so hopefully we're unblocking one of the bottlenecks to more research going on in

543
00:34:41,720 --> 00:34:43,760
the state of yarn in academia.

544
00:34:43,760 --> 00:34:49,800
Can you talk about those couple of challenges that you mentioned, one, knowledge fusion,

545
00:34:49,800 --> 00:34:51,120
and what's there?

546
00:34:51,120 --> 00:34:55,720
And you also mentioned the knowledge-based construction task.

547
00:34:55,720 --> 00:35:00,880
How's that problem framed, and what are the, you know, the success metrics there?

548
00:35:00,880 --> 00:35:02,080
Totally, yeah.

549
00:35:02,080 --> 00:35:08,880
So knowledge-based construction, so like a very classical academic shared task for that

550
00:35:08,880 --> 00:35:15,440
is run by a tech KBP, so that's like, I think, was originally organized by the National

551
00:35:15,440 --> 00:35:20,040
Institute of Standards, but the input to that problem is basically text, right?

552
00:35:20,040 --> 00:35:25,720
So like, newswire articles and things like that, and the output to that problem is, is

553
00:35:25,720 --> 00:35:26,720
a knowledge graph.

554
00:35:26,720 --> 00:35:31,080
So it's like, what are the entities mentioned, and all those documents, and what is the

555
00:35:31,080 --> 00:35:33,880
relationship between those entities, right?

556
00:35:33,880 --> 00:35:36,920
Like is it company A, acquired company B?

557
00:35:36,920 --> 00:35:41,360
Those would be two different entities mentioned, and then the relationship would be like acquisition,

558
00:35:41,360 --> 00:35:42,360
right?

559
00:35:42,360 --> 00:35:45,040
Or person A is the founder of company B.

560
00:35:45,040 --> 00:35:49,320
Those are all examples of relations or triples.

561
00:35:49,320 --> 00:35:57,920
So knowledge fusion is probably not a very, you know, probably, it's not very well publicized.

562
00:35:57,920 --> 00:36:03,160
A lot of people have heard about this research problem, but what it is is basically how do

563
00:36:03,160 --> 00:36:09,800
you fuse multiple sources of data, right, into a singular resource or database, right?

564
00:36:09,800 --> 00:36:17,600
So on the web, you can think about, you know, there's many different kinds of sites and

565
00:36:17,600 --> 00:36:22,360
variety of different kinds of levels of quality of information on the web, right?

566
00:36:22,360 --> 00:36:26,120
And you know, you might trust, for example, something you read on Wikipedia more or so

567
00:36:26,120 --> 00:36:32,720
than on a blog that's hosted in Ukraine that just, you know, was created a month ago, right?

568
00:36:32,720 --> 00:36:36,160
Or something posted on social media, for example, right?

569
00:36:36,160 --> 00:36:38,400
And also there's the time aspect, right?

570
00:36:38,400 --> 00:36:44,560
So what was true at one point in time may not be true at another point in time, like people

571
00:36:44,560 --> 00:36:50,080
change jobs, people switch roles, relationships change, right?

572
00:36:50,080 --> 00:36:51,080
Products change.

573
00:36:51,080 --> 00:36:53,160
There's new stuff coming out all the time.

574
00:36:53,160 --> 00:36:56,920
And the recency of information is critical, right, to any kind of business application,

575
00:36:56,920 --> 00:36:57,920
right?

576
00:36:57,920 --> 00:37:04,280
So what Knowledge Fusion does, we've created essentially algorithms at Diffbot, kind

577
00:37:04,280 --> 00:37:07,080
of equivalent to what we call knowledge-based trust.

578
00:37:07,080 --> 00:37:13,720
So think about PageRank, but not for site authority, but for trustworthiness of the facts,

579
00:37:13,720 --> 00:37:15,360
like from that origin, right?

580
00:37:15,360 --> 00:37:16,360
Okay.

581
00:37:16,360 --> 00:37:22,160
So we almost have an algorithm that propagates truth, right, through this graph that learns

582
00:37:22,160 --> 00:37:27,000
on its own to know that Wikipedia, if something is published there, is more trustworthy, right,

583
00:37:27,000 --> 00:37:29,720
than a random social media post, right?

584
00:37:29,720 --> 00:37:30,720
Because why?

585
00:37:30,720 --> 00:37:38,480
Because Wikipedia has a higher track record in previous iterations of knowledge-based trust

586
00:37:38,480 --> 00:37:40,920
of producing facts that agree with other sources, right?

587
00:37:40,920 --> 00:37:47,400
So there's kind of like a consensus algorithm going on and cross-checking going on.

588
00:37:47,400 --> 00:37:51,720
Within Knowledge Fusion, there's a whole kinds of different ways of approaching the problem.

589
00:37:51,720 --> 00:37:53,880
There's ontological inference.

590
00:37:53,880 --> 00:38:01,720
So for example, if you see on a page, Mike Tung, who's me, lives on the planet Venus, right?

591
00:38:01,720 --> 00:38:05,320
Our algorithms would ideally say that that's not very likely to be a true fact.

592
00:38:05,320 --> 00:38:06,320
Why?

593
00:38:06,320 --> 00:38:08,920
Because other pages say Mike Tung works at Diffbot.

594
00:38:08,920 --> 00:38:14,560
Diffbot is based in Menlo Park, Menlo Park is in California on the planet Earth, right,

595
00:38:14,560 --> 00:38:17,520
which is millions of miles away from Venus, right?

596
00:38:17,520 --> 00:38:19,120
Those are all entities in our knowledge graph, right?

597
00:38:19,120 --> 00:38:25,880
So that logical chain of reasoning would assign very low weight to that fact being true, right?

598
00:38:25,880 --> 00:38:30,760
And also the fact it's not being corroborated by other trustworthy sources, right?

599
00:38:30,760 --> 00:38:36,040
So this kind of mechanical calculation of how likely something is to be true is part

600
00:38:36,040 --> 00:38:40,960
of Knowledge Fusion, which fuses it together to estimate a probability of truth.

601
00:38:40,960 --> 00:38:44,560
One question that I've got, you know, as you describe this knowledge-based construction

602
00:38:44,560 --> 00:38:53,240
task and the example that you gave of the kind of knowledge-based that one might want

603
00:38:53,240 --> 00:39:01,120
to extract from newswire articles, et cetera, I guess I'm trying to work through the relationship

604
00:39:01,120 --> 00:39:06,600
between a knowledge-based that you've got, you know, that's kind of a global knowledge-based

605
00:39:06,600 --> 00:39:11,680
and a knowledge-based that, you know, I want to create around my documents.

606
00:39:11,680 --> 00:39:14,520
Should I be trying to, you know, if I've got a problem and I want

607
00:39:14,520 --> 00:39:19,440
to, you know, say, you know, I'm at a large enterprise and I've got kind of stores of internal

608
00:39:19,440 --> 00:39:24,880
knowledge and I want to, you know, create some kind of knowledge graph based on that.

609
00:39:24,880 --> 00:39:32,200
Should I be building a knowledge graph from scratch that is not aware of kind of the

610
00:39:32,200 --> 00:39:39,200
broader global knowledge graph or knowledge-based or should I somehow be, you know, not to overload

611
00:39:39,200 --> 00:39:44,800
the worth fusing, but, you know, kind of fuse the knowledge that, you know, service

612
00:39:44,800 --> 00:39:49,200
like yours might make available to me about the broader world, you know, maybe treat

613
00:39:49,200 --> 00:39:54,200
that as some kind of framework or ontology or something to get me started.

614
00:39:54,200 --> 00:39:57,240
How do you see folks kind of dealing with those questions?

615
00:39:57,240 --> 00:40:00,560
Yeah, so that's a really good question.

616
00:40:00,560 --> 00:40:04,760
So, I mean, there's all kinds of vendors in this burgeoning space, right, of knowledge

617
00:40:04,760 --> 00:40:12,440
graph. I think it's recently been added as one of kind of like the things that attract

618
00:40:12,440 --> 00:40:17,000
in like the hype cycle, gardener's hype cycle.

619
00:40:17,000 --> 00:40:21,680
There's people that provide actually knowledge graph databases, right, like graph databases.

620
00:40:21,680 --> 00:40:26,120
There's people that are kind of provide consulting services to help your organization build

621
00:40:26,120 --> 00:40:28,640
their own knowledge graph.

622
00:40:28,640 --> 00:40:33,600
Our focus is on just structuring the public knowledge, right, of the public entities and

623
00:40:33,600 --> 00:40:40,000
that's a big enough space for us, but what we find though is that a lot of the entities

624
00:40:40,000 --> 00:40:44,640
that companies care about are public knowledge, they are public entities, right, like all

625
00:40:44,640 --> 00:40:51,760
of the accounts inside your CRM, like an inside your customer database, those aren't specific

626
00:40:51,760 --> 00:40:52,760
to you.

627
00:40:52,760 --> 00:40:57,160
There are companies that exist out there in the real world, right, so are the people,

628
00:40:57,160 --> 00:41:02,560
like we have a vast majority of the people on earth inside our knowledge graph.

629
00:41:02,560 --> 00:41:08,400
So however, the key thing is being able to connect the internal knowledge graph to the

630
00:41:08,400 --> 00:41:13,280
external one and benefit from it, right, so be able to import the facts that we know

631
00:41:13,280 --> 00:41:18,440
in the Diffbag Global Knowledge Graph into your internal stores, right, so that's where

632
00:41:18,440 --> 00:41:25,640
the integrations are key, right, so there's a API that we have called Enhance that actually

633
00:41:25,640 --> 00:41:32,160
allows you to, let's say imagine, for example, you're a small business, you have some

634
00:41:32,160 --> 00:41:37,200
leads inside a database that you collect from your website, you might know, okay, the first

635
00:41:37,200 --> 00:41:43,720
name, email address, and company that one of your customers works for, you can essentially

636
00:41:43,720 --> 00:41:48,600
look that up in our knowledge graph, just only using those three facts, what we call a

637
00:41:48,600 --> 00:41:53,680
impartial entity, and match our entity in our knowledge graph, and then you'll gain

638
00:41:53,680 --> 00:41:59,280
basically like 200 or 300 additional facts like about that entity, so Diffbag can be used

639
00:41:59,280 --> 00:42:06,560
as a tool to both correct your internal data and to keep it up to date with new information.

640
00:42:06,560 --> 00:42:11,600
So if you think about enterprise knowledge, a lot of people's effort is spent just keeping

641
00:42:11,600 --> 00:42:17,560
that database up to date, right, you have like a big vendor database and how many of these

642
00:42:17,560 --> 00:42:21,640
vendors are still in business, or is this the current mailing address of this company

643
00:42:21,640 --> 00:42:27,040
anymore, right, it's a huge headache, right, and a lot of effort is spent in many functions

644
00:42:27,040 --> 00:42:33,200
across the whole enterprise, just maintaining the currency and accuracy of all this information,

645
00:42:33,200 --> 00:42:37,120
and that's the kind of work that we hope to alleviate human beings from having to do in

646
00:42:37,120 --> 00:42:40,880
the future by basically tapping into this global knowledge base.

647
00:42:40,880 --> 00:42:47,480
Awesome, lots of good stuff in this conversation, you know, I guess one quick question I have,

648
00:42:47,480 --> 00:42:55,000
just pulling up the Diffbot page and looking at, or the Diffbot site and looking at pricing,

649
00:42:55,000 --> 00:43:03,160
it doesn't look like there's a kind of developer free tier, that kind of thing, so maybe folks

650
00:43:03,160 --> 00:43:06,320
listening to this shouldn't get excited and say, oh, I'm going to go try this out, you've

651
00:43:06,320 --> 00:43:12,840
got a free trial, but you're not necessarily taking that kind of freemium type of a model,

652
00:43:12,840 --> 00:43:20,640
is that correct, or is there something available for folks that want to play around, hobby

653
00:43:20,640 --> 00:43:22,720
projects, that kind of thing?

654
00:43:22,720 --> 00:43:27,240
So we call our business model knowledge as a service, right, so it's a subscription

655
00:43:27,240 --> 00:43:32,280
to access information called the knowledge graph, like you said, we do have a two-week

656
00:43:32,280 --> 00:43:37,720
free trial for trial access, if you need to use it longer than that for certain projects,

657
00:43:37,720 --> 00:43:42,720
like I mentioned, we do provide free access to certain kinds of groups, if it's a student

658
00:43:42,720 --> 00:43:49,560
project, or if it's like academic research for things like that, I think we have pretty

659
00:43:49,560 --> 00:43:58,560
friendly pricing for startups, starting at $2.99, it'll basically be the same cost as

660
00:43:58,560 --> 00:44:04,320
your EC2 server, probably that you host your application on, or less if you're like a startup,

661
00:44:04,320 --> 00:44:08,920
for the larger companies, large enterprise, usually those kind of companies like to pay

662
00:44:08,920 --> 00:44:14,320
annually with annual contracts, so those are basically gone through sales rather than

663
00:44:14,320 --> 00:44:15,320
the website.

664
00:44:15,320 --> 00:44:21,040
Cool, well Mike, thanks so much for taking the time to chat with me, share a bit about

665
00:44:21,040 --> 00:44:23,440
what you're up to, definitely enjoy it.

666
00:44:23,440 --> 00:44:27,480
Yeah, likewise, it's been pleasure talking to you, Sam, thanks for having me on.

667
00:44:27,480 --> 00:44:28,480
Thank you.

668
00:44:28,480 --> 00:44:35,440
All right, everyone, that's our show for today, to learn more about this episode, visit

669
00:44:35,440 --> 00:44:46,400
homolei.com, as always, thanks so much for listening and catch you next time.

