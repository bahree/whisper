1
00:00:00,000 --> 00:00:16,520
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,520 --> 00:00:21,640
people doing interesting things in machine learning and artificial intelligence.

3
00:00:21,640 --> 00:00:23,940
I'm your host Sam Charington.

4
00:00:23,940 --> 00:00:28,920
The show you are about to hear is the first of a series of shows recorded in New York City

5
00:00:28,920 --> 00:00:31,520
at the O'Reilly AI New York event.

6
00:00:31,520 --> 00:00:35,960
But before we get to the show, I've got a ton of updates and announcements for you.

7
00:00:35,960 --> 00:00:39,880
First off, I want to give a huge thank you to everyone who came out to our very first

8
00:00:39,880 --> 00:00:42,480
Twimble Happy Hour in New York City.

9
00:00:42,480 --> 00:00:46,560
It was a great mix of folks and attendance, including listeners from New York, O'Reilly

10
00:00:46,560 --> 00:00:49,840
AI attendees, and NYAI members.

11
00:00:49,840 --> 00:00:52,000
It was an awesome, awesome night.

12
00:00:52,000 --> 00:00:57,240
I want to especially thank Mariam and the rest of the team at the NYAI meetup for helping

13
00:00:57,240 --> 00:01:02,800
us pull this entire thing together and for clarify for supporting it.

14
00:01:02,800 --> 00:01:07,520
The O'Reilly AI conference itself was great, and of course my favorite part was getting

15
00:01:07,520 --> 00:01:09,400
to meet so many listeners.

16
00:01:09,400 --> 00:01:15,040
I especially enjoyed meeting Twimble's super fans Bill Barion and Beth Ann Noble.

17
00:01:15,040 --> 00:01:19,800
They're both long time listeners of the show and highly engaged members of this community

18
00:01:19,800 --> 00:01:23,240
and it was just amazing to get a chance to hang out with them.

19
00:01:23,240 --> 00:01:27,640
I did a ton of interviews at the show and I'm pleased to present them to you for your

20
00:01:27,640 --> 00:01:29,880
binge listening pleasure.

21
00:01:29,880 --> 00:01:34,440
We have got your commute covered for the entire week with this series.

22
00:01:34,440 --> 00:01:38,240
The series is brought to you by our friends at Intel Nirvana.

23
00:01:38,240 --> 00:01:43,040
I talked about Intel's acquisition of Nirvana systems when it happened almost a year ago,

24
00:01:43,040 --> 00:01:47,320
and I was super excited to have an opportunity to sit down with Nirvana co-founder Naveen

25
00:01:47,320 --> 00:01:52,400
Rao, who now leads Intel's newly formed AI products group.

26
00:01:52,400 --> 00:01:56,920
Naveen and I talked about how Intel plans to extend its leadership position in general

27
00:01:56,920 --> 00:02:03,840
purpose compute into the AI realm by delivering silicon design specifically for AI and to end

28
00:02:03,840 --> 00:02:09,800
solutions including the cloud enterprise data center and the edge and tools that let customers

29
00:02:09,800 --> 00:02:13,680
quickly productize and scale AI based solutions.

30
00:02:13,680 --> 00:02:19,360
I also spoke with Hanlon Tang and algorithms engineer on that team about two such tools

31
00:02:19,360 --> 00:02:25,440
announced at the conference version 2.0 of Neon until Nirvana's deep learning framework

32
00:02:25,440 --> 00:02:30,960
and Nirvana graph a new project for expressing and running deep learning applications as

33
00:02:30,960 --> 00:02:37,680
framework and hardware independent computational graphs Nirvana graph in particular sounds

34
00:02:37,680 --> 00:02:42,480
like a very interesting project not to mention a smart move for Intel and I'd encourage

35
00:02:42,480 --> 00:02:48,080
folks to take a look at their GitHub repo at github.com slash Nirvana systems as well

36
00:02:48,080 --> 00:02:51,960
as their main site at intel Nirvana.com.

37
00:02:51,960 --> 00:02:56,120
One of the things announced at the conference is that Intel and O'Reilly will be partnering

38
00:02:56,120 --> 00:03:02,000
on the AI conference going forward starting with the San Francisco event in September.

39
00:03:02,000 --> 00:03:07,840
They've also changed the name of the event to the AI conference to celebrate all this.

40
00:03:07,840 --> 00:03:12,880
We at twimmel are going to start our the AI conference ticket giveaway early and run

41
00:03:12,880 --> 00:03:14,920
it through the end of the month.

42
00:03:14,920 --> 00:03:19,680
To enter just let us know what you think about any of the podcasts in the series or post

43
00:03:19,680 --> 00:03:26,080
your favorite quote from any of them on our series page on Twitter or via any of our social

44
00:03:26,080 --> 00:03:27,760
media channels.

45
00:03:27,760 --> 00:03:35,280
Make sure to mention at twimmel AI at intel AI and at the AI comf so that we know you want

46
00:03:35,280 --> 00:03:41,760
to enter full details can be found on the series page at twimmel AI dot com slash O'Reilly

47
00:03:41,760 --> 00:03:43,720
AI and why.

48
00:03:43,720 --> 00:03:47,680
By the time this series drops I'll have just returned to the states from my trip to

49
00:03:47,680 --> 00:03:48,680
Europe.

50
00:03:48,680 --> 00:03:52,760
I'll be back on the road later this month to check out the wrangle conference in San Francisco

51
00:03:52,760 --> 00:03:54,240
on July 20th.

52
00:03:54,240 --> 00:03:59,880
You may remember that I recorded the very first twimmel talk show with Claire Cortell at

53
00:03:59,880 --> 00:04:05,240
wrangle and cloud error was my very first sponsor for the podcast so I'm really looking forward

54
00:04:05,240 --> 00:04:06,720
to getting back there.

55
00:04:06,720 --> 00:04:10,720
I definitely hope to catch up with some twimmel listeners while I'm out there so please

56
00:04:10,720 --> 00:04:12,440
check the event out.

57
00:04:12,440 --> 00:04:16,920
If you'd like to attend I've arranged for a special discount for twimmel listeners using

58
00:04:16,920 --> 00:04:22,280
the code PC VIP that's good for 20% off of registration.

59
00:04:22,280 --> 00:04:27,400
Finally a couple of shows ago I mentioned the idea of starting a paper reading group

60
00:04:27,400 --> 00:04:31,640
and it turns out a bunch of you are interested so let's make it happen.

61
00:04:31,640 --> 00:04:37,000
If you'd like to give some input on the details visit twimmel AI dot com slash meetup

62
00:04:37,000 --> 00:04:42,200
and join the discussion in the comments actually I've got one more finally.

63
00:04:42,200 --> 00:04:46,440
Long time listeners will know that I've been talking about doing a newsletter since the

64
00:04:46,440 --> 00:04:51,080
dawn of time well friends and I were chatting the other day about how we've both been

65
00:04:51,080 --> 00:04:55,840
putting off launching our newsletters and ended up challenging each other to just do

66
00:04:55,840 --> 00:05:01,500
it so look for it next week and if you're not already signed up please do so at twimmel

67
00:05:01,500 --> 00:05:04,040
AI dot com slash newsletter.

68
00:05:04,040 --> 00:05:09,800
Okay apologies for the long intro but now a bit more about this series in addition to

69
00:05:09,800 --> 00:05:14,920
my conversations with Navine and Hanlon this series is packed with more interviews that

70
00:05:14,920 --> 00:05:20,760
I know you'll love including my conversation with Doug Eck of Google brains project magenta

71
00:05:20,760 --> 00:05:25,880
in which we discussed the intersection of AI and art in general as well as Google's

72
00:05:25,880 --> 00:05:30,840
recently announced performance RNN project which was demonstrated for the first time

73
00:05:30,840 --> 00:05:38,200
at the O'Reilly AI conference Ben Vagoda of Gamalon in which we discuss probabilistic programming

74
00:05:38,200 --> 00:05:44,600
this one I think is nerd alert worthy raises a day of matroid about how his company is scaling

75
00:05:44,600 --> 00:05:52,760
video object detection and Rana Elkayubi of affectiva about how her company uses emotional AI

76
00:05:52,760 --> 00:05:59,080
to allow brands to better measure the effectiveness of customer experiences all right enough meta

77
00:05:59,080 --> 00:06:05,160
let's jump right into the first episode of our O'Reilly AI New York series after the bumper

78
00:06:05,160 --> 00:06:10,760
you'll hear my brief interview with Navine and immediately after that my interview with Hanlon

79
00:06:10,760 --> 00:06:12,760
enjoy

80
00:06:19,080 --> 00:06:26,440
so hey everyone I am here with Navine Rao Navine is the vice president of Intel AI product group

81
00:06:26,440 --> 00:06:32,680
and we're here on location at the O'Reilly AI conference where he just delivered a keynote

82
00:06:32,680 --> 00:06:37,080
they mean how are you great it's I think you went over pretty well short and sweet

83
00:06:37,080 --> 00:06:41,320
got to announce a few important things around some of the open source projects we have going on

84
00:06:41,320 --> 00:06:46,760
as well as our direction of end to end AI great why don't you tell us a little bit about the

85
00:06:46,760 --> 00:06:52,760
announcements you made yes so one of them was about Intel Nirvana Graph this is a almost an

86
00:06:52,760 --> 00:06:58,120
abstraction where for hardware basically collapsing primitive some different deep-boarding frameworks

87
00:06:58,120 --> 00:07:03,640
into a common representation that we can then optimize for different types of hardware platforms

88
00:07:03,640 --> 00:07:09,880
CPUs GPUs our new architecture FPGAs that kind of thing so it really lessens the burden on

89
00:07:09,880 --> 00:07:14,600
optimizing each framework for every new hardware platform out there I think this is something we

90
00:07:14,600 --> 00:07:20,440
want to drive forward as a standard in industry and the other one is we release neon 2.0 which is

91
00:07:20,440 --> 00:07:25,960
our reference standard framework for deep-learning and this supports Intel architectures CPUs the

92
00:07:25,960 --> 00:07:30,520
latest CPU that's going to be launched from Intel and will be supported by this framework and

93
00:07:30,520 --> 00:07:36,920
optimize highly okay great and I've got a conversation scheduled with one of the technical folks in

94
00:07:36,920 --> 00:07:43,800
I didn't tell Hanlon for later on so we'll dig into some of that but I wanted to also just kind

95
00:07:43,800 --> 00:07:49,480
of get a pulse from you on it's been almost exactly a year since the acquisition it's been

96
00:07:49,480 --> 00:07:54,920
about ten months yeah how's it going and what have you been up to what's been consuming your time

97
00:07:54,920 --> 00:08:00,200
besides from the announcements that you just made yeah it's been a ride actually so when we came

98
00:08:00,200 --> 00:08:06,680
in to Intel we were 50% startup now where we formed an entire new division devoted to AI

99
00:08:06,680 --> 00:08:12,840
kind of seeded from that 50% startup and it's it's much bigger than that now so it's actually

100
00:08:12,840 --> 00:08:17,480
been quite exciting to you know bring together the resources that we have it until and actually

101
00:08:17,480 --> 00:08:23,160
drive a bigger picture a broader portfolio of products and solutions to the industry the way we

102
00:08:23,160 --> 00:08:27,320
think has to change a bit as a startup you're trying to be scrappy you're trying to get that

103
00:08:27,320 --> 00:08:31,720
next deal now we can think in a much bigger way right we can say well what can we do they'll have

104
00:08:31,720 --> 00:08:37,160
a maximum impact across the entire industry sales channels and you know relationships it Intel

105
00:08:37,160 --> 00:08:43,160
has with enterprise is just enormous 6000 salespeople can be unwished which is just a different way

106
00:08:43,160 --> 00:08:49,640
of thinking entirely from a startup absolutely absolutely if I can kind of dig right in one of the

107
00:08:49,640 --> 00:08:54,360
you know what I think of like the elephant in the room when I think about Intel is you know at

108
00:08:54,360 --> 00:09:00,040
the the chip level and video was kind of at the right place at the right time for AI with their

109
00:09:00,040 --> 00:09:06,920
GPUs and they a lot of people think that they've got a big head start in the market and you know

110
00:09:06,920 --> 00:09:12,360
I wonder what's kind of how does Intel think about that and what's the plan I mean there's no doubt

111
00:09:12,360 --> 00:09:16,520
they're executing extremely well they're doing a great job they've adapted their architecture

112
00:09:16,520 --> 00:09:21,080
for these kinds of problems pretty well you know head start sure y'all who had head start

113
00:09:21,080 --> 00:09:28,600
approval to so there's a there's a lot of examples to the contrary there and you know I welcome

114
00:09:28,600 --> 00:09:32,920
the competition I think we're at a point now where obviously there's not going to be there's

115
00:09:32,920 --> 00:09:38,360
never going to be one provider for these things Intel really owns the host processor in the data

116
00:09:38,360 --> 00:09:42,680
center the harm the huge software investments that have been made in in terms of building the

117
00:09:42,680 --> 00:09:47,560
internet right things where you can really scale out infrastructure make it reliable like when

118
00:09:47,560 --> 00:09:52,040
you hit a website it works every time because of all the software investment builds on top of

119
00:09:52,040 --> 00:09:58,280
Intel architecture so we're leveraging that and actually most AI solutions that are deployed

120
00:09:58,280 --> 00:10:03,560
in the data center actually running on Intel right so we have we have those things we're enabling

121
00:10:03,560 --> 00:10:08,280
them with our software stack today some of the announcements I made are relevant to that we're

122
00:10:08,280 --> 00:10:14,440
adapting our main product lines for these purposes and we're also going straight for AI as a

123
00:10:14,440 --> 00:10:19,240
as a preferred workflow essentially for acceleration so you'll be seeing some announcements in the

124
00:10:19,240 --> 00:10:25,560
next you know six months to a year around our our silicon in that space as well okay you guys have

125
00:10:25,560 --> 00:10:30,680
made some pre-announcements in terms of the broad picture broad brush roadmap can you walk us

126
00:10:30,680 --> 00:10:36,840
through you know what we should expect to see sure so that was really based on the roadmap we had

127
00:10:36,840 --> 00:10:42,840
from nirvana as a company so we are developing the silicon that we were developing at nirvana you

128
00:10:42,840 --> 00:10:46,840
know it's going to be prototypes this year and we're really taking the learnings from that building

129
00:10:46,840 --> 00:10:52,840
a real new architecture for this kind of workload is not simple all right takes a few iterations

130
00:10:52,840 --> 00:10:57,080
so we're not really announcing products beyond that in terms of roadmap but we are basically going

131
00:10:57,080 --> 00:11:02,360
to have products out in 2018 19 20 an entire roadmap that we're not talking about performance

132
00:11:02,360 --> 00:11:08,840
just yet but I mean we do have some really important and exciting things on horizon from

133
00:11:08,840 --> 00:11:15,240
the silicon engineering side as well so AI is is a great showcase for those capabilities

134
00:11:15,240 --> 00:11:20,600
because density of compute and power per operation matter right right so that's something that

135
00:11:20,600 --> 00:11:25,560
into it's right in until schoolhouse and then speaking of density a big part of the nirvana

136
00:11:25,560 --> 00:11:33,560
story was around cloud how are you guys thinking about the role of cloud with regards to AI so yeah

137
00:11:33,560 --> 00:11:38,040
that's a really interesting question so part of it is actually we're continuing our hosted cloud

138
00:11:38,040 --> 00:11:42,200
service so we're talking about nirvana cloud we look at that as very very much a quick way of

139
00:11:42,200 --> 00:11:46,360
getting going on a solution for an enterprise in addition to that we want to bring those

140
00:11:46,360 --> 00:11:51,640
capabilities on prem for enterprise customers you don't necessarily want to move data on their

141
00:11:51,640 --> 00:11:56,760
premises and so that's kind of the products will be you'll be seeing in the next year or so then

142
00:11:57,640 --> 00:12:02,840
obviously a broader industry cloud service providers are a huge customer of Intel also Amazon

143
00:12:02,840 --> 00:12:08,120
Google Microsoft the big ones so they're all developing their AI platforms and we're supporting

144
00:12:08,120 --> 00:12:13,000
that effort it's basically a different kind of customer for us but we look at is basically they

145
00:12:13,000 --> 00:12:17,560
intercept at different points in the stack right and so I think you're going to see a variety of

146
00:12:17,560 --> 00:12:24,840
solutions ranging from fully in the cloud to hybrid on prem cloud and completely on prem okay

147
00:12:24,840 --> 00:12:31,800
okay great great one of the announcements you made was around the nirvana software stack how does

148
00:12:32,440 --> 00:12:37,880
can you talk about how that relates to some of the other frameworks that are out in the marketplace

149
00:12:37,880 --> 00:12:45,960
TensorFlow for example has gained a lot of traction I think my impression was that nirvana's stack

150
00:12:45,960 --> 00:12:50,920
was initially positioned as an alternative to something like a TensorFlow is that still the case

151
00:12:50,920 --> 00:12:56,360
and how do you see the kind of landscape there yeah so when we first started nirvana actually there

152
00:12:56,360 --> 00:13:01,880
was no TensorFlow right there are a few fragments of frameworks we put neon out at that time and it

153
00:13:01,880 --> 00:13:07,320
is still an alternative to TensorFlow it's kind of works at the same semantic level okay we are

154
00:13:07,320 --> 00:13:10,680
keeping that development going as a reference standard people can obviously build on and we're

155
00:13:10,680 --> 00:13:15,000
supporting it that's good for us because it allows us to bring the latest optimizations that we

156
00:13:15,000 --> 00:13:19,720
have for hardware to the open source community quickly we're not beholden to anyone else who

157
00:13:19,720 --> 00:13:24,840
owns the database basically right right so we can get those out intel nirvana graph is about

158
00:13:24,840 --> 00:13:29,800
supporting everybody else's frameworks so if you go to intel nirvana.com you can actually see

159
00:13:29,800 --> 00:13:35,080
how we're plugging TensorFlow into intel nirvana graph and allowing it to be optimized on various

160
00:13:35,080 --> 00:13:40,200
hardware platforms so we want to play in the community that way but we can control the ecosystem

161
00:13:40,200 --> 00:13:44,680
from the neon side and provide the latest innovations there and it'll take a little bit

162
00:13:44,680 --> 00:13:49,640
longer for the trickle down into the rest of the open source community okay what are some of

163
00:13:49,640 --> 00:13:55,640
the specific ways today that the hardware innovations are surfacing in into the neon framework

164
00:13:56,280 --> 00:13:59,480
I mean these are some of these things are we can't talk about just yet but we're going to do

165
00:13:59,480 --> 00:14:05,000
parallelism and distribution okay of work was we have some novel constructs and the way we handle

166
00:14:05,000 --> 00:14:09,160
memory and things like that okay it's not to say we couldn't make it work in other frameworks but

167
00:14:09,160 --> 00:14:13,720
we'd have to really fork it and do things a little bit differently so we can get those new concepts

168
00:14:13,720 --> 00:14:18,200
out and I think now what's what's cool about being part of such a big company is that we can

169
00:14:18,200 --> 00:14:23,000
actually shape how the rest of industry sees this so we get those things out I think researchers

170
00:14:23,000 --> 00:14:28,360
are playing with it and we start seeing changes happening in all the frameworks okay I'm sure

171
00:14:28,360 --> 00:14:33,480
it'll be interesting I've seen similar path happened with the intel investment in cloud error

172
00:14:33,480 --> 00:14:38,280
and how they push a lot of the security and encryption innovation and other things like that

173
00:14:38,280 --> 00:14:45,080
into the Hadoop ecosystem it would be interesting to watch so I think we're about at our time

174
00:14:45,080 --> 00:14:50,200
anything else you'd like to mention to the listeners well I think you know the partnership with

175
00:14:50,200 --> 00:14:55,480
O'Reilly it is very exciting for us I think we're at a time and industry we're seeing adoption

176
00:14:55,480 --> 00:15:01,720
happen quickly and so O'Reilly has been on the strata Hadoop site has been really a big

177
00:15:01,720 --> 00:15:07,160
player in that and so I see a parallel happening with AI as well and so I hope to see this this

178
00:15:07,160 --> 00:15:12,760
grow for O'Reilly and we'll be part of it and you just announce a strategic partnership where

179
00:15:12,760 --> 00:15:19,720
you guys are the exclusive partner for the O'Reilly AI conference going forward not exclusive we'll

180
00:15:19,720 --> 00:15:24,680
still take on other partnerships of course with them but we are the main headline sponsor yes okay

181
00:15:24,680 --> 00:15:31,160
analogous to the cladera and exactly strata data now you got a conference okay awesome awesome

182
00:15:31,160 --> 00:15:36,760
while looking forward to seeing you in September and at the O'Reilly AI San Francisco looking forward to

183
00:15:36,760 --> 00:15:40,600
as well awesome thanks to being all right thank you

184
00:15:45,720 --> 00:15:50,680
hey everyone I am here with Hanlon Tang Hanlon is a senior algorithms engineer with Intel

185
00:15:50,680 --> 00:15:57,400
Nirvana Hanlon gave a talk here yesterday at the O'Reilly AI conference and we're here to talk

186
00:15:57,400 --> 00:16:02,440
about his talking what he's been up to how you doing Hanlon good how are you I'm doing great I'm

187
00:16:02,440 --> 00:16:06,760
doing great why don't we start by actually having you talk a little bit about your background and

188
00:16:06,760 --> 00:16:13,000
how you got into AI and algorithms yeah of course I guess it mainly started when I was in graduate

189
00:16:13,000 --> 00:16:18,200
school I was doing research and computational neuroscience and that's really where the connection

190
00:16:18,200 --> 00:16:23,560
between understanding how the brain works and attempting to transfer some of that knowledge

191
00:16:23,560 --> 00:16:30,200
into silicon and computer systems really took hold so after graduate school I joined Nirvana

192
00:16:30,200 --> 00:16:35,800
which is a deep learning startup and through that it began to sort of apply the research that I did

193
00:16:35,800 --> 00:16:42,200
in graduate school to some of the applications that Nirvana is developing the now of course

194
00:16:42,200 --> 00:16:47,880
as Intel we have the opportunity to scale that out quite significantly across all fronts hardware

195
00:16:47,880 --> 00:16:55,000
software algorithms great great and you gave a talk here yesterday at the conference that's right

196
00:16:55,000 --> 00:17:01,560
I think I mainly focused on how do we do that exact same process that I had just described

197
00:17:01,560 --> 00:17:08,440
of no taking research these sort of algorithms and models that you see in the scientific literature

198
00:17:08,440 --> 00:17:13,080
and then begin to apply them and deploy them into production settings okay our sort of unique

199
00:17:13,080 --> 00:17:17,320
challenges that you face when trying to do something like that why don't we have you walk us

200
00:17:17,320 --> 00:17:23,720
through that I know a lot of myself and a lot of our listeners will read papers and walk through

201
00:17:24,520 --> 00:17:29,960
you know the latest cutting edge research and try to understand how to implement it but putting

202
00:17:29,960 --> 00:17:36,040
it in a production is a whole other issue so how did you frame that up in your talk I think I

203
00:17:36,040 --> 00:17:43,320
mainly focused around three key aspects so the first one being the lack of data so I think we've

204
00:17:43,320 --> 00:17:49,080
often heard that there is a flood of data in the world today and certainly with fortune 500

205
00:17:49,080 --> 00:17:55,560
companies and government agencies is a large corpus of data but all that data has to be funneled

206
00:17:55,560 --> 00:18:02,440
through a very small pipe of manual annotations because existing methods we need a human to actually

207
00:18:02,440 --> 00:18:07,720
go through and put you know boxes around all of the cars for thousands of images before a model

208
00:18:07,720 --> 00:18:14,200
can learn to do it so we're data rich but labeled data poor that's right and be able to navigate

209
00:18:14,200 --> 00:18:20,360
that environment with either in heavy investments in data or some of the newer techniques and

210
00:18:20,360 --> 00:18:25,720
generating synthetic data from what you already have is sort of quite critical on building applications

211
00:18:25,720 --> 00:18:33,000
that perform well because deep learning particularly requires a large amount of data to reach

212
00:18:33,000 --> 00:18:40,680
the level of performance that sort of exceeds what humans can do so on your first point then with

213
00:18:40,680 --> 00:18:47,640
regards to data you know we there's clearly you know there are ways to take this on manually by

214
00:18:47,640 --> 00:18:53,640
you know just investing in labeling data but on the synthetic data side what's happening in that

215
00:18:53,640 --> 00:18:59,640
part of the what kind of activity is happening there so one great example is from Intel labs

216
00:18:59,640 --> 00:19:09,400
where they have used video games to generate some realistic imagery by sort of getting graphics

217
00:19:09,400 --> 00:19:14,920
artists and such to build out a video game environment to use that sort of build a synthetic data in

218
00:19:14,920 --> 00:19:22,440
order to train many of the autonomous driving applications okay or alternatively there have also

219
00:19:22,440 --> 00:19:29,240
been advances in using generative out of serial networks to also generate realistic imagery that

220
00:19:29,240 --> 00:19:35,000
could be used during the training process oh interesting I think I've seen examples of the

221
00:19:36,040 --> 00:19:45,960
using video game data to train autonomous driving programs at the time I thought the results

222
00:19:45,960 --> 00:19:53,160
that I saw suggested that for whatever reason the results didn't transfer very well did you

223
00:19:53,160 --> 00:19:59,560
guys in the lab research that you're referring to find some ways to address that I think that's

224
00:19:59,560 --> 00:20:06,680
still an active area of research is how to generalize but they did find that if you're able to augment

225
00:20:06,680 --> 00:20:12,040
your existing rural data set with the synthetic data set you do get better performance overall

226
00:20:12,040 --> 00:20:17,320
because the transferability problem exists for real data sets as well or you may collect large

227
00:20:17,320 --> 00:20:23,080
amounts of data in one city but not able to generalize to other cities or other environments okay

228
00:20:23,080 --> 00:20:30,520
oh interesting yeah the other sort of aspect that I highlighted was building a feedback loop

229
00:20:30,520 --> 00:20:37,080
into the into your systems to have annotation occur on the edge and what I mean by that is if

230
00:20:37,080 --> 00:20:43,720
you're building say aviation security application when you have sort of detectors at the scanning

231
00:20:43,720 --> 00:20:49,800
sites looking for you know dangerous objects and baggage you also want to build in a system for

232
00:20:49,800 --> 00:20:56,360
the agents to provide feedback on how the algorithm is doing and in that way you build a sort of cycle

233
00:20:57,000 --> 00:21:03,800
of collecting data and monitoring data in production right and we've seen that to be quite critical

234
00:21:03,800 --> 00:21:12,440
because the world and the can change underneath you so objects that may be more popular during

235
00:21:12,440 --> 00:21:18,040
the summer maybe less popular in the winter so being able to monitor those changes of the

236
00:21:18,040 --> 00:21:23,400
distribution to object that you expect to see and modify the algorithm appropriately is quite

237
00:21:23,400 --> 00:21:28,840
critical that's an interesting point I know a lot of startups are you know founded basically

238
00:21:28,840 --> 00:21:35,720
around this idea of collecting data and allowing consumers to to basically annotate models for

239
00:21:35,720 --> 00:21:40,280
them annotate data for them but I can imagine enterprises building these systems and putting them

240
00:21:40,280 --> 00:21:46,920
out and not kind of closing the loop that's right is relatively easily to close the loop when

241
00:21:46,920 --> 00:21:53,480
it's just a web interface where the closing the loop is quite simple but in autonomous driving

242
00:21:53,480 --> 00:21:59,240
or aviation security or many of these are the applications where physically the inference occurs

243
00:21:59,240 --> 00:22:04,120
on the edge you actually have to build in the networking and the storage and the memory and also

244
00:22:04,120 --> 00:22:09,160
the sort of components that Intel has in order to close that loop in many of these many of these

245
00:22:09,160 --> 00:22:16,600
scenarios okay all right so you talked about data as one of the first elements of being

246
00:22:16,600 --> 00:22:22,200
able to put these systems into production what else did you talk about the other point that I

247
00:22:22,200 --> 00:22:27,960
really wanted to highlight was around model selection uh-huh it's a difficult challenge these days

248
00:22:27,960 --> 00:22:33,080
because for any particular tasks such as object localization you will find many models in the

249
00:22:33,080 --> 00:22:39,240
literature so faster RCNN single shot detection models RFCN and there are always newer ones coming

250
00:22:39,240 --> 00:22:45,640
out you know all the time so Intel recently has pva net as well and how do you make a decision

251
00:22:45,640 --> 00:22:52,200
as a data scientist of what models to choose and I guess what we've seen is that many customers

252
00:22:53,000 --> 00:22:59,400
may sort of just choose the latest model and run with that where sometimes you have to make very fine

253
00:22:59,400 --> 00:23:07,960
grain speed accuracy trade-offs around your particular use case so a particular model may

254
00:23:08,680 --> 00:23:14,200
be more performant but also take longer to train in which case your iteration cycle is slower

255
00:23:14,200 --> 00:23:20,360
right or on your training costs are higher yes and your training costs are higher or some models

256
00:23:20,360 --> 00:23:26,120
may perform better than another model on sort of an aggregate performance metric but perhaps one

257
00:23:26,120 --> 00:23:31,080
model will perform better at small objects compared to large objects and so be able to make that

258
00:23:31,080 --> 00:23:37,320
fine-grained determination not just on sort of average metric level but also splitting it up into

259
00:23:37,320 --> 00:23:42,440
the individual categories depending on what you're interested in is what we've found to be valuable

260
00:23:42,440 --> 00:23:49,400
for many of our enterprise customers do you find that that understanding the the various

261
00:23:49,400 --> 00:23:59,400
trade-offs is it to what degree is it dependent on a very specific use case and specific data set

262
00:23:59,400 --> 00:24:08,280
and I guess the broader question is is it possible to kind of come up with some standard metrics

263
00:24:08,280 --> 00:24:14,840
around you know in a given category like object detection or speech recognition and you know

264
00:24:14,840 --> 00:24:19,960
rate the different algorithms according to you know some set of standard metrics I haven't seen

265
00:24:19,960 --> 00:24:24,360
anything like that but it would certainly be helpful to folks that are you know coming into a

266
00:24:24,360 --> 00:24:29,480
space like object detection and trying to figure out where to get started. There are certainly

267
00:24:29,480 --> 00:24:36,280
ways to do that so there was a recent paper by many of our colleagues at Google on doing exactly

268
00:24:36,280 --> 00:24:42,600
that okay measuring the various types of object detection models on performance and speed

269
00:24:43,800 --> 00:24:49,720
and that is sort of valuable work to help guide many of our customers and that determination

270
00:24:49,720 --> 00:24:55,720
somewhat general across different use cases with an object detection however for your particular

271
00:24:55,720 --> 00:24:59,880
use case you need to dive much deeper than that it's not enough just to look at the overall

272
00:24:59,880 --> 00:25:04,120
mean average precision which is the metric that they use you then have to split it out by

273
00:25:04,120 --> 00:25:12,200
particular pedestrians or motion large objects small objects and that determination is much more

274
00:25:12,200 --> 00:25:18,600
use case specific. And then in this paper that you're referring to did they also consider

275
00:25:19,640 --> 00:25:25,720
practicalities like training time and you know training cost things like that. They did not I

276
00:25:25,720 --> 00:25:31,080
think they were mostly focused on the inference side of the equation so that's certainly valuable

277
00:25:31,080 --> 00:25:38,200
work moving forward. Interesting. So what else did you cover in your talk? I think those were the

278
00:25:38,200 --> 00:25:46,680
three sort of main points. I made around data closing the loop and model selection okay. I guess

279
00:25:46,680 --> 00:25:54,200
if I were to add a fourth one that I had mentioned was knowing your model provenance. So going back

280
00:25:54,200 --> 00:26:01,560
to the object detection example there that model and many of those models are designed for specific

281
00:26:01,560 --> 00:26:06,920
data sets you know as a grad student you build a model around specific benchmark data sets to help

282
00:26:06,920 --> 00:26:14,360
guide you on how you are doing. And so in particular two data sets Pascal VOC and MS-Coco have been

283
00:26:14,360 --> 00:26:20,840
very popular for object detection. But those data sets typically have maybe five to ten objects

284
00:26:20,840 --> 00:26:26,600
per image. If we're trying to transfer that same object detection model to do a different

285
00:26:26,600 --> 00:26:31,960
application such as satellite imagery. Suddenly you have a scenario where the data sets statistics

286
00:26:31,960 --> 00:26:37,960
don't match the benchmark data sets of the model was designed for. In satellite imagery you have

287
00:26:37,960 --> 00:26:44,200
hundreds of objects in a particular image. You have rotational symmetry because an aerial image

288
00:26:44,200 --> 00:26:49,240
can be rotated and retain many of the similar properties. You have boxes around buildings that

289
00:26:49,240 --> 00:26:54,120
are no longer you know that are rotated. And so now you need to additionally predict an angle

290
00:26:54,120 --> 00:27:01,640
in addition to the coordinates. And so we've been actively developing ways to adapt existing models

291
00:27:01,640 --> 00:27:07,560
to that application. So even though both paths are object detection knowing where the model came

292
00:27:07,560 --> 00:27:12,440
from and what it was designed for can help you sort of maximize the performance on your particular

293
00:27:12,440 --> 00:27:18,440
application where the statistics may be completely different. Yeah this comes up all the time in

294
00:27:18,440 --> 00:27:25,240
the context of the research community and the practitioner community kind of you know quote-unquote

295
00:27:25,240 --> 00:27:30,600
overfitting on image net right. And it sounds like some similar things are happening in the object

296
00:27:30,600 --> 00:27:35,320
detection realm where there are some standard data sets that you know folks are building models

297
00:27:35,320 --> 00:27:42,200
on and then trying to apply all over the place. Knowing the providence of your models is one thing

298
00:27:42,200 --> 00:27:48,920
it sounds like you're also then coming up with techniques that allow you to generalize and adapt

299
00:27:48,920 --> 00:27:54,760
those models to new situations. In the case of satellite imagery for example how exactly do you

300
00:27:55,400 --> 00:28:01,160
what's kind of the underlying techniques that you're using to enable the model to adapt to

301
00:28:01,160 --> 00:28:08,760
a different use case. It's really doing surgery on the model itself. So in this particular case

302
00:28:08,760 --> 00:28:14,600
existing object detection models in the literature mostly just predict the xy coordinate of the

303
00:28:14,600 --> 00:28:20,840
upper left corner and then a width and a height of a box. And so we are working on modifying that

304
00:28:20,840 --> 00:28:27,560
for traditionally predict rotation angle for example or dealing with multispectral satellite data

305
00:28:27,560 --> 00:28:34,840
where it's not just red green blue in the image but IR near IR as well. So there are also these

306
00:28:34,840 --> 00:28:41,240
other spectrums that we can begin to build a model to to take advantage of. Okay so we're

307
00:28:42,120 --> 00:28:51,880
here we're talking about evolving the models themselves as opposed to we're not training a model

308
00:28:51,880 --> 00:28:59,080
on the original data set and then using some techniques with a train model to give it better

309
00:28:59,080 --> 00:29:05,800
inference in a new scenario. We're talking about how do we build a new model that is better adapted

310
00:29:05,800 --> 00:29:11,560
to this situation. Yes exactly. I mean don't get me wrong these existing object localization

311
00:29:11,560 --> 00:29:16,360
models are very powerful. Oh absolutely. You know five years ago as a grassland I would never

312
00:29:16,360 --> 00:29:22,360
have imagined a world where these sort of applications can be done and not only be performed well

313
00:29:22,360 --> 00:29:28,040
but at real time speed. Right. It's quite incredible. And so we really is sort of standing on the

314
00:29:28,040 --> 00:29:34,760
shoulders of sort of these papers but then entering further for particular use cases where you need

315
00:29:34,760 --> 00:29:41,000
to make some some changes. Okay and then I think in your presentation you also talked a little bit

316
00:29:41,000 --> 00:29:48,760
about the Nirvana stack and what was happening in that area. It's an exciting time for us. We've

317
00:29:48,760 --> 00:29:56,760
spent the last two and a half years building a full stack for for deep learning and I think as

318
00:29:56,760 --> 00:30:02,920
Nirvana and now as Intel we still firmly believe in that philosophy that you need the full stack in

319
00:30:02,920 --> 00:30:08,360
order to get maximal performance and ease of use out of what we're building. So it's everything

320
00:30:08,360 --> 00:30:15,800
from the custom silicon to our software work up to a cloud or platform service and finally to

321
00:30:15,800 --> 00:30:22,280
to applications and now that we're part of Intel we have an opportunity to supercharge those

322
00:30:22,280 --> 00:30:31,480
efforts in a sense and so as Naveen had mentioned we've released Neon 2.0 which Neon previously was

323
00:30:31,480 --> 00:30:37,240
known you know when we were start up as one of the fastest frameworks on GPUs due to the custom

324
00:30:37,240 --> 00:30:42,440
assembly kernels that we wrote and some of the algorithmic innovations that we introduced with

325
00:30:42,440 --> 00:30:48,200
winner grad convolution and now we're you know have we've been working very hard with other

326
00:30:48,200 --> 00:30:55,000
engineers of Intel to also optimize Neon for Intel architecture and so by integrating into Intel's

327
00:30:55,000 --> 00:31:01,160
math kernel library we can achieve quite significant speed ups so on an image classification model

328
00:31:01,160 --> 00:31:07,640
such as Google net inference is about 98 times faster than previously so these are very serious

329
00:31:07,640 --> 00:31:14,360
optimizations that Intel has done for other frameworks as well such as TensorFlow and MXnet

330
00:31:14,360 --> 00:31:18,920
and we're excited to work with them to now bring many of these optimizations into Neon as well

331
00:31:18,920 --> 00:31:26,040
okay for those that aren't familiar with Neon can you walk us through the design philosophy and how

332
00:31:26,040 --> 00:31:31,240
it differs from some of the other frameworks that are out in the market of course so Neon we

333
00:31:31,240 --> 00:31:41,320
really designed for enterprise use cases where speed was quite important and many of our customers

334
00:31:41,320 --> 00:31:47,000
don't really need the model of the week they want a stable and fast and optimized object

335
00:31:47,000 --> 00:31:52,600
localization model or speech recognition model and that's really what we provide to many of our

336
00:31:52,600 --> 00:31:59,880
customers in addition we pay a lot of attention to data loading because the you know the folks

337
00:31:59,880 --> 00:32:04,440
that we talked to Ado Riley you know those that run companies that build applications they don't

338
00:32:04,440 --> 00:32:11,160
train their models on image net or on MS cocoa or on pen tree bank they train on their own custom

339
00:32:11,160 --> 00:32:17,960
data sets right and so we put a lot of effort into designing modular data loaders that are fast

340
00:32:17,960 --> 00:32:23,880
but also flexible okay and easily provide a data API for users to switch between different

341
00:32:23,880 --> 00:32:29,080
models so if I'm doing an object detection model we have a couple that we've implemented there's

342
00:32:29,080 --> 00:32:34,360
a common data API for loading that kind of data so that you can test different models

343
00:32:34,360 --> 00:32:43,560
are relatively easily okay are there particular use cases beyond the ones that you've mentioned

344
00:32:43,560 --> 00:32:51,080
that you've found to be the sweet spot for Neon relative to other frameworks I think we found

345
00:32:51,080 --> 00:32:58,440
use cases across a variety of domains so not just an image that I mostly focused on but also in

346
00:32:58,440 --> 00:33:04,760
speech recognition where we've developed a model based on by do state of the art deep speech to

347
00:33:04,760 --> 00:33:09,400
model okay a natural language processing which many of our financial customers are using

348
00:33:09,960 --> 00:33:18,280
I think one of our sort of MOs is to keep track of the quickly changing literature for new

349
00:33:18,280 --> 00:33:23,800
models coming out that bring new level of capabilities and then porting them into Neon and then

350
00:33:23,800 --> 00:33:29,720
optimizing them for speed and for stability and for ease of of data loading and that's really

351
00:33:29,720 --> 00:33:37,880
where we find the value to provide to many of the folks that that we work with one of the the

352
00:33:37,880 --> 00:33:46,040
challenges for enterprises putting these types of machine learning models into production is

353
00:33:46,040 --> 00:33:52,200
monitoring their performance over time and then building a feedback loop that allows them to

354
00:33:52,200 --> 00:33:57,240
improve and enhance their models does Neon have anything in particular to offer in that scenario?

355
00:33:58,680 --> 00:34:07,160
Yes so in Neon we built in callbacks okay that allow the model to report back its progress either

356
00:34:07,160 --> 00:34:14,280
during the training process but also actually mostly during during the training process we don't

357
00:34:14,280 --> 00:34:21,160
have anything currently for sort of specially built for monitoring in inference okay but that's

358
00:34:21,160 --> 00:34:29,240
certainly a good idea that we can look into okay so in addition to the Neon 2.0 announcement you

359
00:34:29,240 --> 00:34:34,680
also announced the Nirvana Graph product can you talk to us a little bit about that?

360
00:34:35,240 --> 00:34:40,520
The Nirvana Graph project really started even before we were acquired okay add Nirvana

361
00:34:41,320 --> 00:34:46,840
when we realized that many of the newer models coming out attention-based models for example

362
00:34:46,840 --> 00:34:53,800
were much easier expressed in a computational graph and that's really the core of the efforts

363
00:34:53,800 --> 00:34:59,880
that we're doing where we've really rethought the backend of Neon and with the Nirvana Graph

364
00:34:59,880 --> 00:35:07,160
effort we built and we're in the process of designing a Nirvana Graph interview representation

365
00:35:07,960 --> 00:35:13,720
which different frameworks can then hook into okay and then on the back and different hardware

366
00:35:13,720 --> 00:35:19,480
back ends will take the graph emitted from Nirvana Graph and then apply their hardware specific

367
00:35:19,480 --> 00:35:25,240
optimization passes until we eventually build an executable execution graph that can run

368
00:35:25,240 --> 00:35:31,240
on different devices so at Intel we're fortunate to have a variety of hardware targets

369
00:35:31,240 --> 00:35:40,600
Zian Zian Phi, Future Lake Crests, Movedias, FPGA's and having a common tool chain to allow folks

370
00:35:40,600 --> 00:35:47,480
to train on one hardware device and deploy on another one or train on a heterogeneous mixture

371
00:35:47,480 --> 00:35:54,520
of hardware devices we think will really change how models are being developed and make

372
00:35:54,520 --> 00:36:00,680
it much easier for industry to transfer those models you know between these different devices.

373
00:36:01,480 --> 00:36:06,600
Can you talk a little bit more about graph as a paradigm for building out these models?

374
00:36:06,600 --> 00:36:12,520
You mentioned attention based models as an example how what's the relationship between

375
00:36:12,520 --> 00:36:19,000
you know a graph based view of the world and attention models and how does a graph

376
00:36:19,000 --> 00:36:21,560
framework support building that kind of model better?

377
00:36:22,680 --> 00:36:29,720
Fundamentally neural networks are graphs of computations and representing that directly

378
00:36:30,440 --> 00:36:35,880
in the Nirvana Graph framework and allowing folks to interact with the graph in building these

379
00:36:35,880 --> 00:36:42,040
models will allow much more flexibility than what we had originally envisioned with NIA.

380
00:36:43,720 --> 00:36:52,440
And so it's the idea that for an arbitrary network my different layers represent nodes in the

381
00:36:52,440 --> 00:36:57,800
graph or what does a node in the graph represent? So a node in a graph represents a fundamental

382
00:36:57,800 --> 00:37:07,960
operation adding or exponential function or matrix multiply. And so layers which is a higher

383
00:37:07,960 --> 00:37:14,760
level concept each layer implements a graph of operations. And when we stream together layers

384
00:37:14,760 --> 00:37:21,080
are essentially adding components and nodes to the graph itself and that's during the construction

385
00:37:21,080 --> 00:37:27,240
phase and after that construction is complete we now have a complete view of the exact

386
00:37:27,240 --> 00:37:33,160
operations that you need to do to train your model. And from that we can then apply optimization

387
00:37:33,160 --> 00:37:39,720
passes to reduce unnecessary computations to optimize memory usage and also taking to account

388
00:37:39,720 --> 00:37:48,520
the specific data layout requirements of different hardware. So how does the developer experience

389
00:37:48,520 --> 00:37:56,440
change in using the graph project? You call it a project or product? I would call it a project.

390
00:37:56,440 --> 00:38:03,400
What I'm an engineer? Not a product person. So I'm not quite sure what the exact

391
00:38:03,400 --> 00:38:10,760
semantics are. So it's clear that thinking about a neural network as a computational graph

392
00:38:10,760 --> 00:38:19,640
and having that explicitly laid out allows us to perform optimization just like a compiler

393
00:38:19,640 --> 00:38:25,000
might or like a query planner and a database might. Is this something that's happening at the

394
00:38:25,000 --> 00:38:29,800
developer level or is it kind of shimmed in underneath an experience that the developer might

395
00:38:29,800 --> 00:38:36,920
already be using like Neon or TensorFlow or something else? If you're a developer that principally

396
00:38:36,920 --> 00:38:43,160
uses components that have already been written out so layers like convolution and pooling

397
00:38:44,120 --> 00:38:52,440
then your experience will be relatively similar. If you have to develop new layers or apply

398
00:38:52,440 --> 00:38:59,400
some custom computation then you're able to access this node level directly and develop on that

399
00:38:59,400 --> 00:39:08,200
so you can compose the ops yourself into a graph. And in many cases we find that the latter

400
00:39:08,200 --> 00:39:13,800
brings a lot of value because not everyone is just sort of applying the vanilla models and layers

401
00:39:13,800 --> 00:39:18,600
that have already come out there. And part for the reasons that we previously talked about they

402
00:39:18,600 --> 00:39:26,520
don't apply as well to some problem sets or data sets then a model that's been augmented to

403
00:39:26,520 --> 00:39:31,560
meet the specific needs of that data set. Right and in many cases it's not just the different

404
00:39:31,560 --> 00:39:35,640
layers that they're composing together but also the way they're composing them together.

405
00:39:35,640 --> 00:39:42,200
So I have a bunch of layers coming in and I want to fork that into outputs that get broadcast to

406
00:39:42,200 --> 00:39:48,120
multiple streams or I may have multiple streams of data coming in whether it be images and

407
00:39:48,120 --> 00:39:54,920
video and text I want to concatenate that together. So the graph also allows that to be much easier

408
00:39:54,920 --> 00:40:00,040
than before because if we know the graph we know how to do the forward and the backward passes

409
00:40:00,040 --> 00:40:05,000
during the training regime. Whereas before and neon you would have to explicitly guide the model

410
00:40:05,000 --> 00:40:12,440
towards okay I have this topology do the forward pass this way and then pass you know the outputs

411
00:40:12,440 --> 00:40:17,480
across this fork and then collect the gradients etc. So the graph takes care a lot of that

412
00:40:17,480 --> 00:40:22,920
so you can it's much more composable is one of the key points okay oh super interesting

413
00:40:22,920 --> 00:40:31,720
it. How can folks learn more about the neon and the graph projects? I definitely encourage listeners

414
00:40:31,720 --> 00:40:38,520
to check out our GitHub page. Okay. Nirvana systems. There is a repository for neon and also

415
00:40:38,520 --> 00:40:45,720
for n graph or the Nirvana graph. Okay. Additionally if you go to www.intelnervana.com we have a

416
00:40:45,720 --> 00:40:51,800
couple blog posts that introduce the sort of the framework itself how to use it links to the model

417
00:40:51,800 --> 00:40:56,680
zoo where we have a lot of pre-trained models that folks and easily get started with. And we

418
00:40:56,680 --> 00:41:02,120
definitely encourage the community to contribute as well. The Nirvana graph effort is still in the

419
00:41:02,120 --> 00:41:08,840
early stages we're sort of releasing our sort of latest commits quite often but definitely if users

420
00:41:08,840 --> 00:41:13,480
look into that and see a feature that they like that they're missing you know we definitely welcome

421
00:41:13,480 --> 00:41:19,160
external contributions. Awesome. Awesome. Anything else you'd like to mention? I think that's it.

422
00:41:19,160 --> 00:41:24,280
I'm excited to be here at Eteralli talking to you and I'd be happy to continue the conversation

423
00:41:24,280 --> 00:41:27,880
in future future meetings. Fantastic. Thank you. Thank you.

424
00:41:31,400 --> 00:41:37,400
All right everyone that is our show thanks so much for listening and for your continued support

425
00:41:37,400 --> 00:41:44,360
comments and feedback. A special thanks goes out to our series sponsor Intel Nirvana. If you didn't

426
00:41:44,360 --> 00:41:49,880
catch the first show in this series definitely check it out. If you didn't catch the first show

427
00:41:49,880 --> 00:41:55,800
in this series where I talked to Naveen Rao the head of Intel's AI product group about how they plan

428
00:41:55,800 --> 00:42:00,520
to leverage their leading position and proven history and silicon innovation to transform the

429
00:42:00,520 --> 00:42:06,840
world of AI you're going to want to check that out next. For more information about Intel Nirvana's

430
00:42:06,840 --> 00:42:13,960
AI platform visit intel Nirvana.com. Remember that with this series we've kicked off our giveaway

431
00:42:13,960 --> 00:42:20,520
for tickets to the AI conference. To enter just let us know what you think about any of the podcasts

432
00:42:20,520 --> 00:42:26,280
in the series or post your favorite quote from any of them on the show notes page on Twitter

433
00:42:26,280 --> 00:42:33,800
or via any of our social media channels. Make sure to mention at Twimlai at Intel AI and at

434
00:42:33,800 --> 00:42:41,640
the AI Conf so that we know you want to enter the contest. Full details can be found on the series

435
00:42:41,640 --> 00:42:48,040
page and of course all entrants get one of our slick Twimo laptop stickers. Speaking of the

436
00:42:48,040 --> 00:42:54,920
series page you can find links to all of the individual show notes pages by visiting twimlai.com

437
00:42:54,920 --> 00:43:09,880
slash O'Reilly AI and Y. Thanks so much for listening and catch you next time.

