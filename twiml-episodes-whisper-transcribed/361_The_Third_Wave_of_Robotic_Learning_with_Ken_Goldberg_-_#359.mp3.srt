1
00:00:00,000 --> 00:00:15,600
Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.

2
00:00:22,600 --> 00:00:28,840
Alright everyone, I am on the line with Ken Goldberg. Ken is a professor of engineering at UC Berkeley.

3
00:00:28,840 --> 00:00:33,320
Ken, welcome to the Twimal AI Podcast. Thank you, Sam. Pleasure to be here.

4
00:00:33,320 --> 00:00:39,320
Yeah, it is great to finally get you on this show. We've been talking about this for a bit.

5
00:00:39,320 --> 00:00:45,560
You know, I meant to ask you before we started. Last time you were, you mentioned you were working on a book.

6
00:00:45,560 --> 00:00:52,760
Maybe we'll, did I get my remembering that right? Well, I think I've been thinking about that for a while,

7
00:00:52,760 --> 00:01:01,000
but I'm also thinking about it and more right now, an article. Okay. Okay. Okay. Well, we'll, we'll get to the article.

8
00:01:01,000 --> 00:01:08,200
I think I first came across you and some of your work in the context of DexNet.

9
00:01:08,200 --> 00:01:15,080
I saw that at a Siemens Innovation Fair last year, and I think we exchanged some tweets and stuff like that.

10
00:01:15,080 --> 00:01:22,840
But, you know, I would really love for you to introduce yourself to the audience and share a little bit about your background

11
00:01:22,840 --> 00:01:27,640
and how you came into working in robotics and AI. Okay, great.

12
00:01:27,640 --> 00:01:35,240
I, well, first since you mentioned Twitter, I should mention my Twitter handle, which is at Ken underscore Goldberg.

13
00:01:35,240 --> 00:01:40,840
And I've been trained very well by my daughter to post there at least once a day.

14
00:01:40,840 --> 00:01:47,080
So, I've got, I've actually found it very interesting channel. So, so I am posting technical things as well as,

15
00:01:47,080 --> 00:01:52,440
as updates about things that I'm finding out, which is that I'm learning about, which is, I find very useful.

16
00:01:53,240 --> 00:02:00,280
So, my background is that I was, I went to University of Pennsylvania and then went to Carnegie Mellon for PhD.

17
00:02:00,280 --> 00:02:05,640
I was at USC for four years and then moved to Berkeley, where I've been for now 25 years.

18
00:02:05,640 --> 00:02:12,120
I, here I run a lab. The, we, we called the Auto Lab for Automation Science and Engineering.

19
00:02:12,120 --> 00:02:19,160
And we have approximately 30 students doing research in there. And we're doing work.

20
00:02:19,160 --> 00:02:23,480
There's, there's, there's postdocs, graduate students and a good number of undergrads.

21
00:02:23,480 --> 00:02:30,760
And we're also associated with other labs like the Berkeley AI Research Lab and the RISE Lab and Citrus and other programs at Berkeley.

22
00:02:30,760 --> 00:02:37,000
Our particular lab is interested in, in, in doing research on, on robotics.

23
00:02:37,960 --> 00:02:42,200
Basically, an algorithmic approach is to robotics and specifically in the last few years,

24
00:02:42,200 --> 00:02:48,840
we've been focusing on learning methods for, for imitation learning, deep learning and reinforcement learning

25
00:02:48,840 --> 00:02:55,160
for control of robots and applications from grasping, as you mentioned, which is a primary one

26
00:02:55,160 --> 00:03:01,640
that I've been working on for, for 35 years, to surgery, surgical assistance,

27
00:03:02,920 --> 00:03:10,440
assistance to human surgeons for, for robotics and home robots to, especially for seniors and,

28
00:03:10,440 --> 00:03:17,080
in, who are, who prefer to live at home. And the last area is very new and we can talk about it later

29
00:03:17,080 --> 00:03:23,560
is, uh, is agriculture. And we have a new approach to polyculture farming that we're exploring

30
00:03:23,560 --> 00:03:30,840
using deep learning. So one thing that I thought was really interesting, uh, in looking at your bio

31
00:03:31,640 --> 00:03:38,200
is in spite of the fact that you are a highly accomplished roboticist, you start your, your

32
00:03:38,200 --> 00:03:43,480
bio starts with Ken Goldberg as an artist. Uh, and so your art clearly must be very important to you.

33
00:03:43,480 --> 00:03:49,880
I actually saw some sketches behind you. Um, and I'm curious, uh, you know, I'm, I'm curious about

34
00:03:49,880 --> 00:03:54,360
Ken as an artist and, you know, how, if, if it all that ties into your work, it's not the usual

35
00:03:54,360 --> 00:04:00,360
fare of, uh, this podcast, but, um, then I saw some of us, you were a filmmaker as well. Um,

36
00:04:00,360 --> 00:04:05,400
is that your art? Like, tell us about that. Okay. Well, actually, I've wanted to be an artist

37
00:04:05,400 --> 00:04:11,400
when I was a kid and I, I, I, I, I basically, my mother said, listen, you can be an artist

38
00:04:11,400 --> 00:04:18,840
after you become an engineer. So, um, she, she, she, she was very wise. And I think it was, it

39
00:04:18,840 --> 00:04:23,800
was a, it was a good choice for me because I actually love both. Art is something that I, I take

40
00:04:23,800 --> 00:04:29,160
very seriously. I think it's often underrated by, by many people, especially engineering engineers,

41
00:04:29,160 --> 00:04:34,520
who think of it as, as lightweight. It's actually just the opposite. Trying to produce something

42
00:04:34,520 --> 00:04:41,080
that's meaningful in the art world is extremely difficult and demanding. So I've spent a lot of

43
00:04:41,080 --> 00:04:47,800
time studying art. I have made a series of installations and projects that almost always involve

44
00:04:47,800 --> 00:04:55,000
technology in some way, but they're also commenting on the role of technology in society. So probably

45
00:04:55,000 --> 00:04:59,960
best known piece is a project is a project called the telegarden that my students and I set up

46
00:04:59,960 --> 00:05:08,600
in the very early, very big early years of the internet. So it's 1995 that we, we connected a

47
00:05:08,600 --> 00:05:15,800
industrial robot arm to the web interface at the time, which was mosaic browser. And we built

48
00:05:15,800 --> 00:05:19,320
an interface that would allow you from your screen, from anywhere, from your laptop.

49
00:05:20,680 --> 00:05:25,080
There were no cell phones at the time, but you could, you could log in and control this thing.

50
00:05:25,080 --> 00:05:30,200
I think I remember this. Right. Yeah, it was a very fun project. We thought, well, it's kind of

51
00:05:30,200 --> 00:05:35,960
curious, you know, what, what, who would use it if anyone? And we got thousands of people coming in

52
00:05:35,960 --> 00:05:41,240
and, and, and moving the robot, but the, the part of what was made in artwork was the context

53
00:05:41,240 --> 00:05:46,120
because it was sitting inside a garden, a real physical garden, so you could plant and water

54
00:05:46,680 --> 00:05:52,440
seeds remotely. And then we got tens of thousands and we estimate that over the, the time that

55
00:05:52,440 --> 00:05:57,560
project was, was that robot was available online, which was approximately nine years. It was

56
00:05:57,560 --> 00:06:03,240
visited and over a hundred thousand people participated in the, in the project. That's awesome.

57
00:06:03,240 --> 00:06:07,080
That's awesome. And again, kind of the technology and art coming together.

58
00:06:07,080 --> 00:06:13,080
Right. So that was the thing Sam, because one of the ideas was that I, I don't think I would have

59
00:06:13,080 --> 00:06:19,320
pursued that if I had just stuck with my research plans at the time, but because this came out

60
00:06:19,320 --> 00:06:24,280
and it offered a way to reach at the time when I saw as a potentially very broad audience,

61
00:06:24,760 --> 00:06:30,600
I started putting effort into this and then I, there was a fantastic team of students who worked

62
00:06:30,600 --> 00:06:37,880
on it and then we, we, I was thrilled with the, the idea that you could take a robot and you could

63
00:06:37,880 --> 00:06:44,440
put it into the hands essentially of a potentially millions of people. And then there were,

64
00:06:44,440 --> 00:06:48,600
there was a proof of concept. There was all the user interface questions. There, it turned out

65
00:06:48,600 --> 00:06:53,160
that there were lots of interesting theoretical questions that came out of that. So after that

66
00:06:53,160 --> 00:07:00,120
project, we did a series of subsequent projects and then had an NSF grant to develop versions of

67
00:07:00,120 --> 00:07:05,480
this. We have a patent related to this. So yeah, it really grew into a whole new direction of

68
00:07:05,480 --> 00:07:11,880
research that that really started with art. Awesome. Awesome. And so tell us a little bit about your

69
00:07:11,880 --> 00:07:17,000
research interests nowadays more broadly. So we're still doing art and I can come back to that.

70
00:07:17,000 --> 00:07:24,200
There's a new project. But the, the, the lab right now has been, been very, very focused on robot

71
00:07:24,200 --> 00:07:30,120
learning. And especially as, as I know your, your listeners are very aware, there's been a huge

72
00:07:30,120 --> 00:07:37,960
revolution in the past decade. And so we've been, we were interested in this before the, the advances

73
00:07:37,960 --> 00:07:43,960
in deep learning started. But now it's really has become a huge focus for us. So in particular,

74
00:07:43,960 --> 00:07:50,360
we have this, we've been working in robot grasping for many years. And then when deep learning came

75
00:07:50,360 --> 00:07:56,520
out, we saw an opportunity to apply it. I can tell you that story if you, if you like, how we do it?

76
00:07:57,160 --> 00:08:02,920
Maybe start from the perspective of the grounding on the challenges associated with grasping.

77
00:08:02,920 --> 00:08:10,280
Like we see these pictures of, you know, whether they're, you know, robot hands or more industrial

78
00:08:10,280 --> 00:08:18,520
types of robots or prostheses. And, you know, they can grasp like we've seen, we've all seen,

79
00:08:18,520 --> 00:08:24,440
you know, pictures of that. But maybe it's harder than it looks or, you know, maybe the,

80
00:08:24,440 --> 00:08:28,360
we want to have the opportunities that, you know, we've not yet figured out.

81
00:08:28,360 --> 00:08:32,680
Oh, good. Okay. So I can, I can answer that partly. I, I've realized that only in the last

82
00:08:32,680 --> 00:08:37,160
few years, the part of the reason I believe I went into this field was that I myself as a kid was

83
00:08:37,160 --> 00:08:43,320
a, was incredibly clumsy. I still, I still am. I mean, you know, anyone would throw me a ball. I

84
00:08:43,320 --> 00:08:48,520
would drop it. And so, you know, I was a last kid getting picked for any sports games or anything

85
00:08:48,520 --> 00:08:54,120
like that. And it was just that I, I think that me, unconsciously made me interested in,

86
00:08:54,120 --> 00:08:58,760
in trying to figure this thing out. Like how, how do you grasp things? And many years later,

87
00:08:58,760 --> 00:09:03,560
when I was in undergraduate, I joined a laboratory at the University of Pennsylvania. And they were

88
00:09:03,560 --> 00:09:08,600
studying various aspects of tactile sensing and, and, and I built a very simple hand

89
00:09:08,600 --> 00:09:13,880
with another student. And we started really exploring this, this question of how do you grasp things?

90
00:09:14,440 --> 00:09:18,040
And it is fundamentally difficult for robots. I like to say that robots

91
00:09:18,840 --> 00:09:24,760
remain incredibly clumsy today. They're much better than they were. But industrial

92
00:09:24,760 --> 00:09:30,280
arms, if you give them novel objects, they will drop them with a fairly high frequency.

93
00:09:31,560 --> 00:09:36,520
And this is a problem because what you really want is you want, um, you want to be able to pick

94
00:09:36,520 --> 00:09:41,000
up anything that's put in front of you. And the application, the big application that's

95
00:09:41,000 --> 00:09:46,360
growing enormously right now is e-commerce. So you want to be able to take objects,

96
00:09:46,360 --> 00:09:50,840
you know, every order is different. So you want to take things from bins and pack them,

97
00:09:50,840 --> 00:09:57,480
lift them out of the bin, grasp them and put them into boxes or bags for shipment. And that

98
00:09:57,480 --> 00:10:03,800
turns out to be a bottleneck right now for robotics. And Amazon's been holding competitions to,

99
00:10:03,800 --> 00:10:08,120
for teams to figure out how to do this for a few years now, right? Right. So they had a very

100
00:10:08,120 --> 00:10:12,120
interesting competition called Amazon picking challenge and picking is the word for grasping

101
00:10:12,120 --> 00:10:18,680
out of a bin. And they had, and it pushed the field forward in a very, very constructive way.

102
00:10:18,680 --> 00:10:23,160
And there were, there was progress made. But however, they stopped doing that about three years ago.

103
00:10:23,160 --> 00:10:28,600
Oh, really? Okay. And the, but what the good news is that it did bring a number of researchers

104
00:10:28,600 --> 00:10:34,760
into this, got a number of researchers interested. And now there's really a great, great number of

105
00:10:34,760 --> 00:10:40,040
researchers working on this. Now we had been working on it for, as I mentioned, 35 years. I mean,

106
00:10:40,040 --> 00:10:46,760
since I was an undergrad. And so about seven or eight years ago, we were looking at new ways

107
00:10:46,760 --> 00:10:53,160
of grasping and, and, and we were working with Google and talking with them about what we,

108
00:10:53,160 --> 00:11:00,120
we called the Dexterity Network. And the idea there was to use an analogy with machine vision.

109
00:11:00,920 --> 00:11:04,680
And you're familiar with ImageNet. And I'm sure you've talked about it on the, on the podcast.

110
00:11:05,400 --> 00:11:13,320
So ImageNet really transformed machine learning by having a very large data set of labeled images.

111
00:11:14,360 --> 00:11:19,160
And it seemed that you get to a critical mass. Enough labeled images that then you can train

112
00:11:19,160 --> 00:11:23,960
a system and it could start to generalize to new images. So the question for us is, could we do

113
00:11:23,960 --> 00:11:31,560
something analogous in grasping by assessing a very large data set of three-dimensional objects,

114
00:11:32,200 --> 00:11:38,920
three-dimensional models, CAD models, and then labeling them with grasps and specifically robust

115
00:11:38,920 --> 00:11:47,160
grasps. So then we can start to learn from those examples. A robust graph meaning robot actually

116
00:11:47,160 --> 00:11:51,160
has the object and it's not in some precarious position. It's, it's fingers are in the right

117
00:11:51,160 --> 00:11:55,080
place, so to speak. Right. And actually, let me, I'm glad you asked that question because that,

118
00:11:55,080 --> 00:12:01,480
that's actually very important. So by robust, we mean robust to the following uncertainty in sensing,

119
00:12:02,680 --> 00:12:08,360
control, and physics. So this is a fundamental reason why robots are still clumsy.

120
00:12:09,320 --> 00:12:14,040
Because of this uncertainty, what do I mean? Well, let's start with, with perception.

121
00:12:14,040 --> 00:12:20,200
Just even if you have the highest resolution camera available, and you look at a scene,

122
00:12:20,200 --> 00:12:25,720
you still don't know the precise geometry of where everything is in that scene. And there has

123
00:12:25,720 --> 00:12:30,760
now been advances in depth sensors, which I can talk about because I'm a big believer in those.

124
00:12:30,760 --> 00:12:36,120
I think those are really game-changing, but they still don't completely solve this problem. Because

125
00:12:36,120 --> 00:12:43,000
if there's anything reflective or transparent on the surface that causes the light to react

126
00:12:43,000 --> 00:12:47,960
in unpredictable ways. And so you, you don't get, it doesn't register as an exact correct

127
00:12:47,960 --> 00:12:53,720
position of where that surface really is. And you primarily referring to LiDAR and connect

128
00:12:53,720 --> 00:12:58,360
types of sensors, or is there something else? No, that's what I'm referring to. And those are,

129
00:12:58,360 --> 00:13:05,720
there's now a whole new genre of such cameras coming out that are lower in cost, higher resolution,

130
00:13:05,720 --> 00:13:11,400
higher frame rate, more reliable for various reasons. And so I'm, I'm very excited about that.

131
00:13:11,400 --> 00:13:15,160
But it doesn't solve the problem. It actually facilitates making progress.

132
00:13:16,360 --> 00:13:20,280
So the perception is still a huge problem. I'm looking down at the scene, even right now at my

133
00:13:20,280 --> 00:13:25,800
desk, and I have a sense of where things are. But a robot doesn't actually really figure out

134
00:13:25,800 --> 00:13:31,240
exactly where things are in space. Second of all, the control. And by that I mean that robot,

135
00:13:31,240 --> 00:13:37,240
we move its gripper. And is that, I mean, that even that is an interesting question, right? Because

136
00:13:37,240 --> 00:13:44,200
certainly if it's a single two dimensional image, then yeah, we understand why the robot doesn't

137
00:13:44,200 --> 00:13:50,600
can't figure out where things are. But now when you're introducing in stereoscopic images and

138
00:13:50,600 --> 00:13:58,440
LiDAR and all these things, like we should be able to give the robot enough data point that it

139
00:13:58,440 --> 00:14:04,360
should be able to figure it out. But there's, you know, there's still something missing, is it,

140
00:14:04,360 --> 00:14:14,280
is it that we have kind of surpassed our capacity from a kind of compute per unit time perspective,

141
00:14:14,280 --> 00:14:22,040
or is it that, you know, locating objects on a flat surface requires some fundamental human

142
00:14:22,040 --> 00:14:26,280
intelligence that we haven't transferred to robots yet, or is it something totally different?

143
00:14:26,280 --> 00:14:31,000
A little bit of all of those. I mean, one of it is that you would think, right, if you put

144
00:14:31,000 --> 00:14:35,880
multiple cameras and then you use stereo and depth sensors, that that would, would help you.

145
00:14:35,880 --> 00:14:40,520
But remember, every time you add another modality, another sensor, you actually add more complexity.

146
00:14:41,160 --> 00:14:45,400
And you end up with more cases where you have contradictions between what these different

147
00:14:45,400 --> 00:14:51,640
sensors are telling you. So in fact, what you get is these, the results are that you, you actually

148
00:14:51,640 --> 00:14:57,720
have system will often have two aspects of its own sensory apparatus telling you two different

149
00:14:57,720 --> 00:15:02,600
things, right? And that's a problem. So then it doesn't know what to trust. So that,

150
00:15:02,600 --> 00:15:08,200
throwing more, more, more sensors does not solve the problem. The other is you mentioned computation.

151
00:15:08,200 --> 00:15:13,480
So that I actually think you're, you're right, we are, we are, computation is not necessarily the

152
00:15:13,480 --> 00:15:19,720
bottleneck. Right now we have very fast computing and we can distribute it over many, many, many

153
00:15:19,720 --> 00:15:25,880
processors. But the, it is challenging when you have to build statistical models because of the

154
00:15:25,880 --> 00:15:29,640
residual uncertainty, you don't actually know. So you want essentially to say, here's where I

155
00:15:29,640 --> 00:15:34,200
think it is. And here's my confidence level on top of that. So there's a really that you're

156
00:15:34,200 --> 00:15:38,680
actually out, you're dealing with a statistical model of the environment. And that is actually fairly

157
00:15:38,680 --> 00:15:44,760
high dimensional. And you have to, there is some computational complexity there. The, the other

158
00:15:44,760 --> 00:15:49,560
thing though is that humans and animals, by the way, seem to cope very well with the problem like

159
00:15:49,560 --> 00:15:55,800
grasping and interacting with the physical world. Because we bring to it a sort of inherent

160
00:15:55,800 --> 00:16:01,800
understanding, a deeper understanding about the, the nature of objects. And so this is very subtle.

161
00:16:01,800 --> 00:16:07,800
We don't really, I can't describe this exactly. It's, it's, it's, it's intuitive to us how to make

162
00:16:07,800 --> 00:16:14,520
things up. But it's very hard for us to, to, to formalize that intuition and put, give that to a robot.

163
00:16:15,560 --> 00:16:20,520
So that, so the sensing is still a problem. And anyone who says, oh, that's a solve problem,

164
00:16:20,520 --> 00:16:24,840
doesn't really know the problem. Right. And this is true, by the way, for self-driving cars,

165
00:16:24,840 --> 00:16:29,880
and every, every other application where you want to perceive the environment. And by the way,

166
00:16:29,880 --> 00:16:35,320
it matters for sensing, for, for grasping, very critically, because for a car, maybe an inch,

167
00:16:35,320 --> 00:16:40,360
one way or another, doesn't matter. Although it does, if you're on the edge of a cliff. But for

168
00:16:40,360 --> 00:16:47,400
grasping, one inch makes all the difference. Even a half, even less. Yeah. Exactly. A millimeter,

169
00:16:47,400 --> 00:16:52,360
or less, can make the difference between holding something and dropping it. So that's why it

170
00:16:52,360 --> 00:16:56,840
matters to get this exactly right. It's very subtle. And that's all the sensing. That's in the

171
00:16:56,840 --> 00:17:01,480
control aspect I was starting to say, which is that the robot has to now get its manipulator and

172
00:17:01,480 --> 00:17:07,160
its gripper and its fingertips to the precise position and space consistent with what it's,

173
00:17:07,160 --> 00:17:13,720
what it believes is happening in from its sensors. For a robust grasp. So, so wait, I'll come back

174
00:17:13,720 --> 00:17:21,080
to robots. Right. We're almost, the third one, the third one is, is, is physics. So one of the

175
00:17:21,080 --> 00:17:24,520
things that we don't know often know, if I look at an object, I don't know exactly where the center

176
00:17:24,520 --> 00:17:30,920
of massive that object is. Very importantly, I don't know its frictional properties. And friction

177
00:17:30,920 --> 00:17:37,720
is still an immense unknown for science. I like to say that we can predict the motion of a,

178
00:17:37,720 --> 00:17:42,920
of an asteroid, a billion miles away, far better than we can predict the motion of pushing a

179
00:17:42,920 --> 00:17:48,760
pencil across a table. Because you actually can't predict the latter. Is that, is that, is that,

180
00:17:48,760 --> 00:17:54,120
that's hard to believe? Like, try it. Put a pencil down. I thought I did that in McKenna's class

181
00:17:54,120 --> 00:17:58,520
many years ago. Oh, yeah. For a fiction of static friction, a fiction of dynamic friction.

182
00:17:58,520 --> 00:18:04,840
I apply a force and that tells me how far I go. No, those are, what are you talking about, Ken?

183
00:18:04,840 --> 00:18:08,920
Exactly. Exactly. Cool arm friction, right? The law of cool arm friction. Well,

184
00:18:08,920 --> 00:18:13,560
unfortunately, it doesn't actually work that way. So it's a, it's a reasonable approximation.

185
00:18:13,560 --> 00:18:18,680
But look at just pushing, take a pencil, put it on your, on your desk and put your finger in the,

186
00:18:18,680 --> 00:18:22,840
hold it's horizontally and then put your finger in the middle approximately, start pushing it.

187
00:18:22,840 --> 00:18:27,560
At a certain point, it's going to rotate off your finger. And you don't know where that's going

188
00:18:27,560 --> 00:18:30,680
to happen. And if you do it again, it'll do it somewhere different. Every time you do it,

189
00:18:30,680 --> 00:18:35,000
it's somewhere different. Why? What's going on? Because it matters. Because really, what's going

190
00:18:35,000 --> 00:18:39,720
on under there is a chaotic system. And it has to do with the tackle. It's a little dirty. Right.

191
00:18:39,720 --> 00:18:47,080
One lot of out of true, one microscopic grain of sand of anything under there is going to

192
00:18:47,080 --> 00:18:51,640
cause it to behave extremely differently than if that, that sand wasn't there. So, and we can't

193
00:18:51,640 --> 00:18:58,280
blend itself to some kind of statistical probabilistic distribution of how the pen's going to tumble.

194
00:18:58,280 --> 00:19:03,160
Right. But here's a thing is that in fact, that's another example where the statistics is not

195
00:19:03,160 --> 00:19:06,360
necessarily going to help you because it's not going to be a nice Gaussian distribution of where

196
00:19:06,360 --> 00:19:13,640
it's going to end up. It actually will end up in possibly a very, a very multimodal distribution.

197
00:19:14,840 --> 00:19:21,080
So, it's very, it may be a non-parametric distribution of where the pen will end up in space.

198
00:19:21,640 --> 00:19:26,440
And it's very hard to model or predict. It's going to change every time there's a little bit of

199
00:19:26,440 --> 00:19:31,960
moisture or dust on your table. And you can't even if you just want a robot to push pens.

200
00:19:31,960 --> 00:19:38,760
You're out of luck, out of luck. Pencil pushers, okay. Robots can't do it. So, one of the things

201
00:19:38,760 --> 00:19:44,440
that's really interesting is that these three elements together conspire to make robotics

202
00:19:44,440 --> 00:19:49,000
grab more robot grasping extremely difficult. The uncertainty in the perception, uncertainty in

203
00:19:49,000 --> 00:19:54,680
control, and uncertainty in physics. Those are, those are fundamental. Now, what I, let's come back

204
00:19:54,680 --> 00:19:59,960
to robust, what I mean by robust. What I mean to that by that is that can I look for a grasp

205
00:19:59,960 --> 00:20:05,480
that'll be robust, that'll be insensitive to my uncertainty in those three elements.

206
00:20:06,200 --> 00:20:10,360
What I mean by that is I want to grasp that even if my perception is slightly off,

207
00:20:10,360 --> 00:20:13,880
even if my control is slightly off, and even if the physics is slightly off,

208
00:20:14,680 --> 00:20:18,360
I'll still be able to pick the object up successfully. That's a robust grasp.

209
00:20:19,560 --> 00:20:26,440
So, an example, you know, if you pick up a glass of wine, for example, you put your hand under it,

210
00:20:26,440 --> 00:20:32,360
you sort of, you put your fingers apart, you scoop up around the stem, and then you lift.

211
00:20:32,360 --> 00:20:37,640
Now, that's robust grasp, because even if the glass isn't quite where you thought it was,

212
00:20:37,640 --> 00:20:41,800
even if your hand isn't quite where you thought it was, and even if the thing is slippery,

213
00:20:41,800 --> 00:20:46,920
you're still going to be able to pick it up. That's a robust grasp. So, it turns out that for most

214
00:20:46,920 --> 00:20:52,360
objects, there are grasps that are more or less robust, and what we're trying to do is get a robot

215
00:20:52,360 --> 00:20:58,440
to learn that quality, that robustness. And we can generate that by using the physics that you're

216
00:20:58,440 --> 00:21:04,920
talking about, that you learned, and actually it goes all the way back to centuries of beautiful

217
00:21:04,920 --> 00:21:10,440
mechanics of understanding the physics and forces and torques, wrenches and space,

218
00:21:11,160 --> 00:21:15,800
that characterize what happens if we know everything, then what we do is perturb that statistically.

219
00:21:16,600 --> 00:21:21,000
And we say if we can, if it's robust, it works for all these statistical perturbations with

220
00:21:21,000 --> 00:21:26,040
high probability, then we say it's a robust grasp. Now, before we make our way back to Dexnet,

221
00:21:26,920 --> 00:21:31,480
which is where we start here, you brought up an interesting point about the kind of marriage

222
00:21:31,480 --> 00:21:37,640
of physics and statistics. When I talk to some of your frequent collaborators,

223
00:21:38,280 --> 00:21:43,640
Sergey Levine, Peter Abile, come to mind, the impression I get from them, maybe less so now,

224
00:21:43,640 --> 00:21:47,880
than years ago, was that hey, we should throw away all the physics and just get the centers and

225
00:21:47,880 --> 00:21:54,200
let the computers learn how all the physics is going to work. When I talk to more traditional

226
00:21:54,200 --> 00:22:00,280
roboticists, they are more interested in preserving everything that we've learned in the past

227
00:22:00,280 --> 00:22:07,080
couple of centuries via physics. I get the, well, you tell me where are you in that kind of

228
00:22:07,080 --> 00:22:15,640
with feed in both worlds? Good. I like that you ask that. I call the old physics, the classic

229
00:22:15,640 --> 00:22:21,720
physics, the first wave of robot grasping. That's really dominated and still very, very

230
00:22:22,760 --> 00:22:31,080
common in the robotics conferences and journals. It's very well grounded. It's beautiful mathematics,

231
00:22:31,080 --> 00:22:37,560
beautiful theory. That's the first wave. The second wave is really robot learning,

232
00:22:37,560 --> 00:22:42,600
which Peter, Sergey, and many others are very excited about today, which is purely data-driven

233
00:22:42,600 --> 00:22:48,760
approaches that say forget about the physics, but let's just learn it. Let's just learn it from

234
00:22:48,760 --> 00:22:56,840
observation purely. Actually, I'm an advocate of what I call the third wave and that is to synthesize

235
00:22:56,840 --> 00:23:03,320
those two, to use the physics where it's appropriate and use learning where it's appropriate.

236
00:23:03,320 --> 00:23:09,640
That those combination is exactly what we need, figuring out where that combination is is the

237
00:23:09,640 --> 00:23:19,000
challenge. That's really the story behind Dexnet. Dexnet you assembled this data set of CAD,

238
00:23:19,000 --> 00:23:26,680
models, and grasps, and then what did you do with it? What we did was we applied the first wave,

239
00:23:26,680 --> 00:23:35,000
the physics and the statistics to lots and lots of simulated models. We just basically

240
00:23:35,000 --> 00:23:41,560
simulated and said, are these robust to simulations and perturbations of each of the candidate

241
00:23:41,560 --> 00:23:47,800
grasps? We checked for many, many combinations and found grasps that are robust. We used that

242
00:23:47,800 --> 00:23:52,600
whole first wave, all that beautiful theory there. Then we took the examples that were generated

243
00:23:53,160 --> 00:23:59,880
and used those to train a deep learning system to be able to generalize to new examples. One

244
00:23:59,880 --> 00:24:05,800
thing that's important is that we did this with three-dimensional models. I occurred to be that

245
00:24:06,520 --> 00:24:12,520
if we're going to try and model the perception part, what we really care about is the three-dimensional

246
00:24:13,080 --> 00:24:18,840
arrangement of the surfaces and the points in space. I don't care about the color of things or

247
00:24:18,840 --> 00:24:25,560
the texture on things. In fact, that's a distraction. We asked, could we do this with just pure depth

248
00:24:25,560 --> 00:24:31,320
sensors? Another nice thing about depth sensing is that you can simulate it very nicely.

249
00:24:32,200 --> 00:24:37,160
Simulating color images actually turns out to be very hard, and we can talk more about why,

250
00:24:37,160 --> 00:24:43,480
but in simulation, you can say, you know the points in space perfectly. There you know everything,

251
00:24:43,480 --> 00:24:48,920
by the way, there's no uncertainty at all, but what you can do is now add a little bit of noise

252
00:24:48,920 --> 00:24:53,240
to what you would say is, what would a depth sensor see if it looked at this scene?

253
00:24:53,240 --> 00:25:01,000
Now you can say, okay, but I know we're talking about the depth sensor. The simulation or the

254
00:25:01,000 --> 00:25:06,920
imagery? This is a depth sensor. The depth sensor is creating an image, but it's a depth image,

255
00:25:06,920 --> 00:25:16,120
if you would. That's all it sees. It's throwing out what we call RGB color information, and it's

256
00:25:16,120 --> 00:25:21,800
only using the depth information. Now I have an arrangement of points in space, and then I know what

257
00:25:21,800 --> 00:25:28,440
a grasp, the successful grasp, when that arrangement of points corresponds to a successful grasp or

258
00:25:28,440 --> 00:25:36,680
not, because I'm using the physics and statistical model of the sensor. How do you even represent that,

259
00:25:36,680 --> 00:25:44,520
is that you have for a cylinder you've got four ideal points on one side and one ideal point on

260
00:25:44,520 --> 00:25:51,960
the other side, and you do like some kind of distance metric of the fingers to those points or something, or like...

261
00:25:51,960 --> 00:25:57,960
How do you do that? So representing that sounds hard. Well actually, so if I have a cylinder,

262
00:25:57,960 --> 00:26:02,280
I can look at two points on opposite sides of the cylinder. I can generate friction cones around

263
00:26:02,280 --> 00:26:08,040
those, and I can just check if those two friction cones intersect, then actually by the first

264
00:26:08,040 --> 00:26:15,400
way of grasping, I can check if that grasp is going to succeed, or not. So that's fairly...

265
00:26:17,000 --> 00:26:22,840
That's very fairly understood. If I knew everything, then I could do a check that would tell me

266
00:26:22,840 --> 00:26:27,080
if it succeeds or not. But since I don't know everything, what I would do is I say, well here,

267
00:26:27,080 --> 00:26:31,160
if I change it ever so slightly in all these different ways, if it still works, well then it's

268
00:26:31,160 --> 00:26:37,400
robust. Now what we do is we start with the perfect model of these objects, then we say,

269
00:26:37,400 --> 00:26:42,520
okay, we know what the good grasps are, because we've done... The robust grasps are for those

270
00:26:42,520 --> 00:26:48,600
perfect models, and then we say, let's make a... Let's pretend we actually have a fairly noisy sensor,

271
00:26:48,600 --> 00:26:53,640
which we do in practice, and so we can simulate that noise, just add noise to all the little points

272
00:26:53,640 --> 00:27:00,840
that you would detect with your depth camera. So now you have a noisy pattern of points in space,

273
00:27:00,840 --> 00:27:08,200
and you know what the true robust grasp was for that pattern of points. So that is the input to...

274
00:27:08,200 --> 00:27:13,320
That's one example of a robust grasp, and you run that through, and you actually have already

275
00:27:13,320 --> 00:27:18,280
computed the probability of success. So the output is just a scalar number from zero to one,

276
00:27:18,280 --> 00:27:24,120
which is the quality. We call it the probability that that grasp will succeed. That's one example.

277
00:27:24,120 --> 00:27:29,240
Now we generate millions of these examples, and we can do this very fast. Actually, you can generate

278
00:27:29,240 --> 00:27:35,240
the examples overnight. Then we say, okay, now we have a nice data set. It's not quite as big as

279
00:27:35,240 --> 00:27:40,680
ImageNet, right? But it's pretty nice-sized, and by the way, I have to give it both positive and

280
00:27:40,680 --> 00:27:45,080
negative examples, so I have to give it a whole range of qualities, so I can learn that whole function.

281
00:27:46,760 --> 00:27:53,480
Now what I do is... Now I'll put that out into the field, where I'm taking new depth images,

282
00:27:53,480 --> 00:28:00,920
from a real camera, of objects it's never seen before, and candidate grasps for that object,

283
00:28:00,920 --> 00:28:05,400
and then it'll tell me which one, basically, it can quickly evaluate the grasps quality.

284
00:28:05,960 --> 00:28:11,160
And then what I do is I try a number of different grasps, again, synthetically, on that depth map,

285
00:28:11,960 --> 00:28:17,560
and it tells me this is the one with highest quality. So now what I do is execute that one,

286
00:28:17,560 --> 00:28:25,800
that is defined, we consider that the optimal grasp, the one with highest quality, and we execute it,

287
00:28:26,680 --> 00:28:33,880
here's the thing, it works remarkably well, far better than we thought. So that was a big surprise

288
00:28:33,880 --> 00:28:39,720
for us. That idea that you could train it very fast, you could generate lots of examples,

289
00:28:39,720 --> 00:28:43,960
you could train it on that work relatively fast, again, another overnight, but then the result

290
00:28:43,960 --> 00:28:51,640
was it did generalize in surprising ways. Now it's not perfect, and it's not perfect, I will say,

291
00:28:51,640 --> 00:28:57,000
and it's really important, and by the way, I really believe it's our duty as roboticists to

292
00:28:57,000 --> 00:29:06,360
point out the limitations and limits of our work. So we're able to get up to the 90 plus success

293
00:29:06,360 --> 00:29:12,040
rate, 90 plus percent success rates, but it depends on the nature of the objects. So if the objects

294
00:29:12,040 --> 00:29:22,120
are all fairly well-behaved, like cylinders and cuboids, then it's fairly easy to do well,

295
00:29:22,120 --> 00:29:27,640
but it's when you have more complex geometries that the system, many systems have trouble,

296
00:29:27,640 --> 00:29:33,880
and this is where our system was relatively good. Again, getting above 90, but not close to 100.

297
00:29:35,240 --> 00:29:41,080
And so the generalization that you're referring to, specifically generalization and shapes,

298
00:29:41,080 --> 00:29:46,600
as opposed to grippers, sensors, and any of other pieces of the system.

299
00:29:48,040 --> 00:29:52,440
Good questions to them. So that's actually a great point. We have to retrain this. If you change

300
00:29:52,440 --> 00:29:59,880
the gripper, I have to generate a new data set for that for your gripper, and then retrain a

301
00:29:59,880 --> 00:30:09,080
new neural network. But the framework should apply. And if you change the sensor, same story.

302
00:30:09,080 --> 00:30:15,720
I have to change the generation of the samples, get a new training set, retrain the network,

303
00:30:15,720 --> 00:30:22,120
and then use that. So in fact, interestingly, just a quick story. When we first announced

304
00:30:22,120 --> 00:30:29,240
DexNet, we got picked up in some news reports, and we were contacted by industry.

305
00:30:29,240 --> 00:30:38,280
And they quickly taught us that actually we want to use this, but we use suction cups,

306
00:30:38,840 --> 00:30:45,800
not grippers. And we sat and thought about that, and never forget we were sitting in the

307
00:30:45,800 --> 00:30:49,320
lap, and suddenly it was like a light bulb went off, and we said, wait a second, we can take the same

308
00:30:49,320 --> 00:30:55,480
exact framework and apply it to suction. And so instead of a two-point grasp, we're looking for

309
00:30:55,480 --> 00:30:59,960
a single point grasp. And that's the suction point, and you want that to be robust.

310
00:31:00,920 --> 00:31:04,440
So kind of flat, kind of bigger than the suction cup itself, that kind of thing.

311
00:31:04,440 --> 00:31:10,040
That's right. But we can formalize that in a paper. We actually started looking around for

312
00:31:10,040 --> 00:31:15,800
literature, like how do you define the quality of a suction grasp? And amazingly, there was a gap

313
00:31:15,800 --> 00:31:22,520
in the literature there. Wow. I mean, we looked and we could not find it. And to this day, I don't

314
00:31:22,520 --> 00:31:27,880
know if someone who's really nailed that question. But we developed a model that looks at the,

315
00:31:27,880 --> 00:31:36,600
basically, the points around the suction cup, how far they deviate from planar and a spring model

316
00:31:36,600 --> 00:31:42,440
that would give you an estimate of how well the seal would be achieved. And then we also

317
00:31:42,440 --> 00:31:48,760
looked at the object in general and how what wrenches would be created as you lifted that object,

318
00:31:48,760 --> 00:31:54,440
because it has to do with the center of mass and the angle of the surface you're making contact

319
00:31:54,440 --> 00:31:58,440
with. So you could come up with the exactly the same physics that you were talking about earlier,

320
00:31:58,440 --> 00:32:04,440
again, a first wave type of approach. But then we perturbed it by all the same ideas. So we don't

321
00:32:04,440 --> 00:32:08,280
know the exact position of the surface. We don't know the exact position of the suction cup.

322
00:32:09,400 --> 00:32:15,240
We don't know the exact physics of the suction and the frictions. But then we did the same thing.

323
00:32:15,240 --> 00:32:20,040
So then we have a model of suction cups. And again, it worked remarkably well.

324
00:32:20,760 --> 00:32:30,440
Nice. Nice. And so Dex that was a few years ago, a couple like 17 or when, well, it was 2017

325
00:32:30,440 --> 00:32:38,200
that Jeff, and I really want to give credit to Jeff Moller. This was his PhD work. And he,

326
00:32:38,200 --> 00:32:46,200
most of the ideas, I mean, he was, he's still, he's now running a startup called the ambidextrous

327
00:32:46,200 --> 00:32:54,200
laboratories. And Jeff is a brilliant student and really a great engineer. He worked out a lot of

328
00:32:54,200 --> 00:33:02,200
this and deserves a huge amount of the credit. So my role was essentially guiding him toward

329
00:33:02,200 --> 00:33:08,920
toward taking a, basically teaching a, a TA in a class that was on, on the mechanics of,

330
00:33:08,920 --> 00:33:14,760
of grasping because that's where he picked up the physics aspect of it. And then in our discussions

331
00:33:14,760 --> 00:33:18,520
we basically on Dexnet, et cetera, that we started thinking, okay, how can we start put this

332
00:33:18,520 --> 00:33:24,840
together? But he had the idea of applying it to heaps of objects, which I, I was skeptical

333
00:33:24,840 --> 00:33:32,680
that that was going to work. And then it was very surprised what it did. Awesome. Awesome. So one of

334
00:33:32,680 --> 00:33:42,280
the, the things that we kind of hinted on earlier, it's related to the, you know, first wave,

335
00:33:42,280 --> 00:33:46,520
second wave, third wave. I don't know if this is something that you'll want to comment on. But,

336
00:33:46,520 --> 00:33:52,520
you know, I think a lot of the way we think about robotics or maybe just some of the way we think

337
00:33:52,520 --> 00:33:57,960
about robotics is influenced by like the general, why don't I always forget their name, not general

338
00:33:57,960 --> 00:34:06,440
dynamics. Boss dynamics. The boss dynamics. Yeah. Videos. Right. As a roboticist, like, you know,

339
00:34:06,440 --> 00:34:13,320
in particular, roboticist that is, you know, thinking about things from the perspective of learning

340
00:34:13,320 --> 00:34:20,600
and AI, like what's your take on those? My perspective is that actually Mark Raybert was my first

341
00:34:20,600 --> 00:34:26,600
research advisor, Carnegie Mellon. Oh wow. And I was, I used to hang out in that lab where they

342
00:34:26,600 --> 00:34:31,080
were doing the, the running machine, which was at that, like, at that time, just a single Pogo

343
00:34:32,120 --> 00:34:39,160
actuator. And it was, it was, it was beautiful research. I think what I always, one thing that attracted

344
00:34:39,160 --> 00:34:44,200
me to Mark was that he would always spend time showing the bloopers and show the thing running

345
00:34:44,200 --> 00:34:48,920
and then he would show it wiping out. And it was really nice because he would always say,

346
00:34:48,920 --> 00:34:54,600
well, look at, you know, it's, it works, but not always. And somehow, what I'm worried about,

347
00:34:54,600 --> 00:35:04,040
and is that, for example, boss dynamics, when it showed, showed the, the robot doing the backflip

348
00:35:04,040 --> 00:35:08,920
that got, you know, millions of views, at most, at least, even the backflip. Yeah. Now,

349
00:35:08,920 --> 00:35:13,320
that's remarkable. And it's beautiful. But if you stay, if you watch to the end of that video,

350
00:35:13,320 --> 00:35:19,240
it goes black for a little while. And if you stick with it, it then goes to a, it shows you a blooper

351
00:35:20,360 --> 00:35:26,520
where it just totally wipes out. Yeah. In fact, is that's very common. All right. So, but they,

352
00:35:26,520 --> 00:35:31,560
they, they, they sort of put that delay. And most people didn't see that. So when I give talks,

353
00:35:31,560 --> 00:35:36,840
I always, I always show that blooper. I say, this is what our world is. Right. This is so important

354
00:35:36,840 --> 00:35:44,600
to convey. So I have a huge respect for Mark and his, his, his team at Boston Dynamics. But I

355
00:35:44,600 --> 00:35:50,920
think that they, at least the way that the videos sometimes are portrayed or that, hey, we're,

356
00:35:50,920 --> 00:35:55,960
we've solved these problems. And robots are now capable of, you know, all kinds of agility,

357
00:35:55,960 --> 00:36:01,160
like, like, as good as, as a gymnast. And the reality is that it's not, we're nowhere near that.

358
00:36:01,160 --> 00:36:07,720
Yeah. Yeah. Well, which is interesting. So you're saying that we're not near the agility. There's

359
00:36:07,720 --> 00:36:14,600
a whole, another level of it or layer, which is, at least as I understand it, like the self-directed

360
00:36:14,600 --> 00:36:20,440
ness, it's not like someone said, robot, do a backflip. And it took 400 times, but it did 10 backflips.

361
00:36:21,400 --> 00:36:27,320
It's more, much more choreograph than that. Someone is plotting out the, the points, you know,

362
00:36:27,320 --> 00:36:32,040
in which it's doing the backflip, that kind of thing. Like, can you, is that your impression as well?

363
00:36:32,040 --> 00:36:36,440
Like, what's your take on that? You mean in terms of how well is it crafted or fine-tuned?

364
00:36:37,480 --> 00:36:46,520
Mean more how, how autonomous is it, I think? Oh, right. I'm getting out like, or how is it,

365
00:36:46,520 --> 00:36:51,240
you know, the flip side of that being how orchestrated, orchestrated is it?

366
00:36:51,240 --> 00:37:01,240
Flip side. Good. Yeah. It's a, no, the, the, the, the, it's very, I would say, let's see,

367
00:37:01,240 --> 00:37:04,600
it's a little difficult to say, but I would say orchestrated is probably the right word.

368
00:37:04,600 --> 00:37:11,560
There's a lot of results like, you know, where you look at something like OpenAI's hand doing a

369
00:37:11,560 --> 00:37:18,280
Rubik's cube. And what's equally important to point out is that despite this huge and very

370
00:37:18,280 --> 00:37:23,880
impressive engineering effort, that it still drops that cube very often, and that the motions are

371
00:37:23,880 --> 00:37:31,560
very, the imprecise and inefficient. And we're, we're making progress, Sam. I mean, that's the thing

372
00:37:31,560 --> 00:37:37,560
I want to say. I don't want to, I don't want to cast a negative shadow on, on, on research. But

373
00:37:37,560 --> 00:37:43,400
I also think it's really important to understand that it's not nearly as fast as the, the, the public

374
00:37:43,400 --> 00:37:52,120
beliefs and the press often leads people to believe by, by highlighting the successes and not

375
00:37:52,120 --> 00:37:57,320
talking about the failures or the limits. Yeah. So that's what I was getting to where I believe

376
00:37:57,320 --> 00:38:03,160
that there's a lot of, I do believe our field is, is somewhat overhyped. I do. I think AI in

377
00:38:03,160 --> 00:38:10,200
general and I think robotics, we have, we are making wonderful progress, but let's put it in

378
00:38:10,200 --> 00:38:17,640
context. And so don't over raise the expectations, because I do worry about a robotics winter,

379
00:38:18,600 --> 00:38:21,960
where we're going to, where people say, wait a second, you told us you're going to have self-driving

380
00:38:21,960 --> 00:38:25,720
cars and now we're still waiting for them. And this whole thing is a bad idea. So we're just going

381
00:38:25,720 --> 00:38:30,440
to cancel all the projects. That is a real danger. You worry about them more than a robot uprising.

382
00:38:31,160 --> 00:38:35,000
Far more. I'm not worried about a robot uprising. I totally not.

383
00:38:35,000 --> 00:38:43,160
So now you're working on some, some other interesting things since the, or beyond the,

384
00:38:43,160 --> 00:38:50,440
the grasping work, a couple of things that I came across were telemedicine and some activity

385
00:38:50,440 --> 00:38:54,760
there, agriculture. And tell us about some of the, the more recent things you're up to.

386
00:38:54,760 --> 00:39:00,600
Okay. Well, one area that I've been interested in for many years is, is, is, is robot assisted

387
00:39:00,600 --> 00:39:05,400
surgery. And you may have seen sometimes hospitals who advertise that you have a robot, they're doing

388
00:39:05,400 --> 00:39:09,880
surgical, you know, they have a surgical robot. Well, what they mean by that, it's intuitive,

389
00:39:09,880 --> 00:39:16,280
is the, is they, by far, the, the biggest, biggest company in this space is they have a system

390
00:39:16,280 --> 00:39:21,400
that a human can operate a human surgeon, but that reflects the actual motions of the human

391
00:39:21,400 --> 00:39:25,640
in the body of the patient. And this has some great advantages because you mean like haptic

392
00:39:25,640 --> 00:39:30,360
feedback kind of actually that's, it doesn't have haptic feedback, which is interesting, but it can,

393
00:39:30,360 --> 00:39:35,000
basically take the motions I make with my hands, right? And surgeon is operating with, think of

394
00:39:35,000 --> 00:39:41,160
it as VR. And then looking down the reflection one way from the doctor to the exact surgical studio

395
00:39:41,160 --> 00:39:45,720
as opposed to back. Exactly. In fact, it's very, the only thing coming back is video from inside

396
00:39:45,720 --> 00:39:50,360
the body. And they do this very minimally invasive, you know, just a couple of small holes in the

397
00:39:50,360 --> 00:39:56,280
abdomen, which is very important for recovery and reduces infection and speeds up healing,

398
00:39:56,280 --> 00:40:00,680
because often times the big problem is you have a big scar and that's, it, it has a lot of side

399
00:40:00,680 --> 00:40:06,920
effects. So, so this is a great, a huge breakthrough in surgery. It's, it's fantastic, but the,

400
00:40:06,920 --> 00:40:10,120
but it's important to keep in mind that it's been completely controlled by a human every step

401
00:40:10,120 --> 00:40:15,480
of the way. So it's like a, a very, very expensive puppet. Now, one thing that's interesting,

402
00:40:15,480 --> 00:40:21,160
though, is it has the capability of offering a little bit of autonomy. And I think of this as,

403
00:40:21,160 --> 00:40:26,680
you know, the levels of, of autonomy in driving, self-driving car right to zero, which is where,

404
00:40:26,680 --> 00:40:32,200
you know, most cars are all the way up to fully autonomous where you can, you know, you're,

405
00:40:32,200 --> 00:40:37,080
there's someone in front driving and you're asleep in the back. So, CMM level five.

406
00:40:37,080 --> 00:40:44,040
Yes. Exactly. The five levels. So what, what I believe in, and I think this is very true for,

407
00:40:44,040 --> 00:40:48,840
for cars as well, is that we're actually making nice progress around level two or three.

408
00:40:48,840 --> 00:40:52,760
That we can assist drivers under certain conditions in, for example, there are a nice,

409
00:40:52,760 --> 00:40:59,080
nice weather and a freeway and it's well controlled, right? And then you can actually give up control

410
00:40:59,080 --> 00:41:04,760
and it's getting very good at doing that. And that's got a lot of benefits, saving, safety,

411
00:41:04,760 --> 00:41:14,200
and reducing tedium. So what's an example of in the remote surgery field? So it would be where,

412
00:41:14,200 --> 00:41:19,640
for example, you are a surgeon is performing a complex procedure like a transplant and has to

413
00:41:19,640 --> 00:41:26,840
perform lots and lots of sutures. So suturing is very tedious and it's just, you have to keep doing

414
00:41:26,840 --> 00:41:34,600
this over and over and over again and closing up after a surgery and that, here's the thing is

415
00:41:34,600 --> 00:41:40,920
that the uniformity consistency is very important in suturing, just as it is. And we've been doing

416
00:41:40,920 --> 00:41:45,080
sewing machines for a really long time. Exactly. Thank you. That's exactly what I was going to say.

417
00:41:45,080 --> 00:41:48,840
Great. Because sewing machines are very nice in uniform. They do a beautiful job. And the same

418
00:41:48,840 --> 00:41:55,960
is true for surgery. If you have a uniform stitch, then what happens is the healing can be spread

419
00:41:55,960 --> 00:42:01,240
out, the tension between the different stitches get, the sutures get spread out. The wound closes

420
00:42:01,240 --> 00:42:06,360
and heals better, less scarring, all kinds of benefits. And my father-in-law, who was a surgeon,

421
00:42:06,360 --> 00:42:12,760
told me that the difference between an average surgeon and a great surgeon is their suturing

422
00:42:12,760 --> 00:42:21,480
skill. So if you could facilitate that, you could reduce the tedium and increase the recovery time,

423
00:42:22,200 --> 00:42:29,000
healing, it would be a great thing. But it's a very hard problem. So we're trying to work on

424
00:42:29,640 --> 00:42:33,240
that as well. Right now, we're looking into breedment, which is just to be able to pull out

425
00:42:33,240 --> 00:42:39,640
dead or damaged tissue out of a particular body cavity. Okay. So if you had

426
00:42:41,320 --> 00:42:45,880
necrology, there's a certain area where they have some dead tissue or under certain illumination,

427
00:42:45,880 --> 00:42:52,360
you might see cancerous tissue, and you're just basically pulling it out with surgical tweezers.

428
00:42:52,360 --> 00:42:58,680
That's very tedious also. But that's something that I do believe will be able to provide surgical

429
00:42:58,680 --> 00:43:03,240
assistance for. Okay. So there's all kinds of interesting problems there. With uncertainty, I

430
00:43:03,240 --> 00:43:07,880
mentioned earlier, it's even magnified with surgical robots because of the cables that drive them,

431
00:43:07,880 --> 00:43:12,760
so there's a lot of uncertainty in the control, and there's uncertainty in the perception,

432
00:43:12,760 --> 00:43:17,560
and there's uncertainty in the physics too. So, but that's another area that I'm excited about,

433
00:43:17,560 --> 00:43:23,800
and I think we're going to make some progress in next few years. And agriculture? Agriculture. Okay.

434
00:43:23,800 --> 00:43:29,320
So now we've been kind of talking about precision agriculture, and there have been some successes,

435
00:43:29,320 --> 00:43:34,600
like Blue River, which was acquired by deer that was, I don't know, would you consider that

436
00:43:34,600 --> 00:43:41,640
robotics? It's like the device mounted on a tractor that is using computer vision to check out

437
00:43:41,640 --> 00:43:47,320
the weed that's is passing over and spraying them with fertilizer to burn them out or pesticide or

438
00:43:47,320 --> 00:43:52,680
something. Right. No, I would definitely because one of the things that what you're trying to get

439
00:43:52,680 --> 00:43:58,760
down to is controlling things at the plant level, per plant level. Normally in agriculture,

440
00:43:58,760 --> 00:44:04,440
things are done on the field level. You just sort of have a setting of your combine that sort of

441
00:44:04,440 --> 00:44:10,520
said, well, it's about right, and then it runs over the whole field roughshod. And what happens

442
00:44:10,520 --> 00:44:18,040
is you waste a lot of the food. You also, it misses a lot of things. The setting is average,

443
00:44:18,040 --> 00:44:22,600
so it's it's either too high or too low for for the actual crops, because they vary a lot.

444
00:44:23,160 --> 00:44:26,520
And you try to make the crops as consistent as possible. That's really the name of the game

445
00:44:26,520 --> 00:44:32,360
in agriculture. And you can do that genetically in other ways, right? So then your machine can be

446
00:44:32,360 --> 00:44:39,000
more efficient. But it uses a lot of pesticides and other genetics and other other things to make

447
00:44:39,000 --> 00:44:44,760
that to control that food. So the food is corresponding, the plants are corresponding to the machine.

448
00:44:44,760 --> 00:44:49,400
Yeah. One thing that's really interesting is maybe if you don't have to do that,

449
00:44:49,400 --> 00:44:55,160
but if you can have the machine correspond to the plant, adjust to the plant. So that's where you

450
00:44:55,160 --> 00:45:01,080
want to be able to move through the field and detect what's going on. Notice where exactly where

451
00:45:01,080 --> 00:45:06,280
the weed is showing up and then being able to zap that weed rather than just tapping the whole

452
00:45:06,280 --> 00:45:11,080
field with a big dose of pesticide. So this is this is definitely robotics. It's being able to

453
00:45:11,080 --> 00:45:16,280
precision get in and do that and do that fast and it cost effective. That's really exciting.

454
00:45:16,280 --> 00:45:20,600
It's also another area of that is plant phenotyping where you're driving robots around in a field where

455
00:45:20,600 --> 00:45:24,840
you're actually changing parameters and using it to look at the plants and see how they grow and

456
00:45:24,840 --> 00:45:31,960
to be able to pick out the more successful combinations of parameters of pesticides and fertilizers,

457
00:45:31,960 --> 00:45:38,280
et cetera. So there's a lot of interesting work going on. We are, Mike, are you describing like

458
00:45:38,280 --> 00:45:45,640
I'm envisioning a dynamic AB testing or multivariate testing on your field that is all being

459
00:45:45,640 --> 00:45:51,320
affected by some robot? Right. Yeah. There are researchers who are doing this. So they plant

460
00:45:51,320 --> 00:45:58,200
the fields with all slightly varying levels of seed spacing and seed type and they're basically

461
00:45:58,200 --> 00:46:03,960
running a huge large scale experiment, but they have to measure what each of the plants are doing

462
00:46:03,960 --> 00:46:08,680
in terms of how they're growing. It's very tedious. So robot is great for doing that.

463
00:46:09,800 --> 00:46:14,840
And so what's your lab doing in this space? So our lab is looking at, we started by looking at

464
00:46:14,840 --> 00:46:20,920
precision irrigation and this is a project with colleagues at Merced and UC Davis and the idea

465
00:46:20,920 --> 00:46:26,440
there was to use drones to essentially fly over a field and then identify where there was too

466
00:46:26,440 --> 00:46:33,080
little water or too much and then adjust these small emitters all around the field, drip irrigation

467
00:46:33,080 --> 00:46:37,480
emitters to compensate. One thing we found was that we couldn't, it was very hard to do experiments

468
00:46:37,480 --> 00:46:44,200
because it's just that you have to fly out, usually you have to go into some remote area, you have

469
00:46:44,200 --> 00:46:49,880
to fly, you have to measure the ground conditions and it was just taking forever. So we asked, could we,

470
00:46:49,880 --> 00:46:56,280
what if we try a little miniature scale version of the farm and what if we build a farm in our lab?

471
00:46:56,280 --> 00:47:04,520
So to make a long story short, we've recently built one and it's one and a half meters by three

472
00:47:04,520 --> 00:47:09,160
meters. So it's fairly small. It's not in the lab, it's in the greenhouse, that's two blocks away

473
00:47:09,160 --> 00:47:15,960
from the lab at Berkeley and it is, it has got over it a robot, this is a Gantry type robot, X,

474
00:47:15,960 --> 00:47:24,520
Y and Z, and it's made by a company named Farmbot. Now so we are not innovating in the,

475
00:47:24,520 --> 00:47:29,880
in the, in the hardware, they're doing great job with that. It's a commercial company, by the way,

476
00:47:29,880 --> 00:47:33,560
you can, I recommend them very highly, it's about $3,000. They've been added for a while,

477
00:47:33,560 --> 00:47:38,120
it's kind of set up like a 3D printer, but for gardening, right? Yeah, yeah, that's a good way

478
00:47:38,120 --> 00:47:43,640
to put it. It's a really nice system, beautiful and they, they package it, it comes like a, it's like

479
00:47:43,640 --> 00:47:47,880
an opening in Apple product, you know, everything is totally organized and it's going to set it up,

480
00:47:47,880 --> 00:47:53,240
it's not trivial to put together, but it's very, very, it's very effective. So we have that,

481
00:47:53,240 --> 00:47:58,280
and then what we've been doing though is adding cameras to be able to automatically monitor the

482
00:47:58,280 --> 00:48:04,360
state of the garden, and what we're adding to it that, that, that Farmbot doesn't necessarily

483
00:48:04,360 --> 00:48:10,440
recommend is we're doing polyculture, gardening. So we want to be able to handle the case where I

484
00:48:10,440 --> 00:48:15,960
have lots of different plants growing in close proximity, and where there's a certain amount of

485
00:48:15,960 --> 00:48:22,920
lack of structure in the garden. So I'm not trying to keep everything separate as, as, as monoculture,

486
00:48:22,920 --> 00:48:28,520
and almost all like real farming is done, but I want to let things grow in wherever they happen

487
00:48:28,520 --> 00:48:33,640
to grow. And there's benefits to that because you can reduce pesticides and increase resilience and

488
00:48:33,640 --> 00:48:41,320
reduce water and all those nice, nice factors. So nutrients, so nutrients, but it takes more labor

489
00:48:41,320 --> 00:48:47,880
to be able to do it. So can we, can we automate? Now I think it's an open question. It's a very hard

490
00:48:47,880 --> 00:48:53,400
question because you have a very high dimensional state space again of all the plants that you're

491
00:48:53,400 --> 00:48:57,800
looking at trying to make decisions about the action space. In this case, if you look at a particular

492
00:48:59,800 --> 00:49:04,440
sector of the garden, you want to decide, do I just skip it, do nothing today, or do I water,

493
00:49:04,440 --> 00:49:11,480
or do I prune, or plant, so it's a fairly small state space. But can we learn over time? Now the

494
00:49:11,480 --> 00:49:16,200
other problem is this, this, where this differs from something like grasping is there's a very

495
00:49:16,200 --> 00:49:21,400
long time constant. So if I go and try, if I grasp something, I fairly quickly to check that,

496
00:49:21,400 --> 00:49:27,640
but in a garden, it takes weeks to see the effect of my actions. Right. And if you thought we were

497
00:49:27,640 --> 00:49:33,800
bad at simulating grasping cylinders. Right. Right. And try simulating a bunch of different plants

498
00:49:33,800 --> 00:49:38,520
growing your close proximity. It's very hard. Now there are people that have developed simulators

499
00:49:38,520 --> 00:49:43,880
for individual plants. And that's actually a whole, you know, subfield. But when we talk to them

500
00:49:43,880 --> 00:49:47,880
about, well, what about if I have two plants next to each other, they're like, oh, we don't do that.

501
00:49:49,080 --> 00:49:53,000
They do it in graphics, of course, but graphics, of course, is a perfect example where like,

502
00:49:53,000 --> 00:49:56,760
you know, Jurassic Park or something, you can make amazing things are very convincing visually,

503
00:49:56,760 --> 00:50:04,520
but they're not realistic. So we've been developing a simulator that's very, very, very simple,

504
00:50:04,520 --> 00:50:09,800
but allows us to test exactly this robustness aspect that you and I were talking about earlier,

505
00:50:09,800 --> 00:50:15,720
which is I want to be able to have variation in everything and then see if a, if a particular

506
00:50:15,720 --> 00:50:23,160
policy for planting and pruning and watering is robust to those variations. Okay. So that's

507
00:50:23,160 --> 00:50:29,480
me we're running and we and just come full circle. Sam, it's a, we also considered an art project

508
00:50:30,360 --> 00:50:36,360
because the, now we call it not the telegarden, but this is the alpha garden. Okay. And it's a,

509
00:50:36,360 --> 00:50:42,280
it's basically it's an ongoing project. We launched it at a gallery in New York that's doing an

510
00:50:42,280 --> 00:50:50,280
exhibition on called the question of intelligence. And so we, this opened about a month ago, it's a,

511
00:50:50,280 --> 00:50:57,160
and it's online. If you go to alpha garden.org and you can see images and you can essentially

512
00:50:57,160 --> 00:51:04,200
explore the garden. You can't plant yourself, but it's made to be able to observe what's happening.

513
00:51:04,200 --> 00:51:08,760
And we're by the way on the first season and it will have multiple seasons. Every season is

514
00:51:08,760 --> 00:51:15,320
about two or three months. Okay. Interesting. Interesting. One more question for you. We were talking

515
00:51:15,320 --> 00:51:21,560
before we got started and this is maybe going back to health and medicine domain. It's the

516
00:51:21,560 --> 00:51:26,680
middle of March. Kind of we're in the middle of the, the middle of the beginning of dealing with

517
00:51:26,680 --> 00:51:36,120
COVID-19. And you mentioned some kind of thoughts on how robots might play in testing and assessments.

518
00:51:36,120 --> 00:51:41,880
Yeah. So I'm working on, well, I'm not working on it, but I've been following some threads where

519
00:51:41,880 --> 00:51:47,240
one of them was that because we expect there, there, I mean, it's likely that there could be

520
00:51:47,240 --> 00:51:54,760
some real crisis in hospital population where we may have some spike in occurrences of the virus.

521
00:51:54,760 --> 00:52:00,680
In the next, in the next four to six weeks or even sooner. And it will overwhelm the hospitals.

522
00:52:00,680 --> 00:52:04,440
So we're trying to flatten that curve and avoid that. But if it happens, one of the things is that

523
00:52:04,440 --> 00:52:08,760
people will want to suddenly report their, you know, how they're, they're feeling temperature

524
00:52:08,760 --> 00:52:13,640
and they have problems breathing. So what happens is everybody wants to test all of a sudden.

525
00:52:13,640 --> 00:52:17,400
We only have limited number of tests. So what do we do? Well, people are just self-reporting

526
00:52:17,960 --> 00:52:23,560
as their, the White House just announced everyone should log in and, you know, sign up if you want

527
00:52:23,560 --> 00:52:28,040
to test. Well, everybody's going to overstate their symptoms because they want to be tested.

528
00:52:28,680 --> 00:52:31,880
So an alternative to that, which is really interesting is to use cameras.

529
00:52:32,600 --> 00:52:36,920
Everybody has a camera built into their phone or their, their laptop. And you use the camera to

530
00:52:36,920 --> 00:52:42,040
basically take a video of your face. And it, and this is where the research is, is can we

531
00:52:42,040 --> 00:52:50,280
process that, that video to extract the pulse rate and the oxidation of your, oxymetry of your,

532
00:52:50,280 --> 00:52:56,920
of your body? Is that enough to give us signal to detect a viral infection? Well, it's at 19.

533
00:52:56,920 --> 00:53:00,200
No, but it would be helpful, especially over in temperature, by the way. It would also

534
00:53:00,200 --> 00:53:04,040
might be able to be correlated with that. So you say you have 103, but I've seen video,

535
00:53:04,040 --> 00:53:09,160
and that doesn't, it's not consistent, right? So one of the things, this is a very complex question.

536
00:53:09,160 --> 00:53:12,680
You're right. It's not enough to just look at a video and say that person has it or doesn't.

537
00:53:12,680 --> 00:53:16,760
Well, also, and I can get a pulse oxymeter on Amazon for a box, right?

538
00:53:16,760 --> 00:53:20,680
And now you can, but it may be hard to get if this really gets, you know,

539
00:53:20,680 --> 00:53:24,600
if it really starts gripping up. Plus, you might not have it with you, right? You're out somewhere

540
00:53:24,600 --> 00:53:27,800
or something like that. So, but if you just, you always have your cell phone. So if you could point

541
00:53:27,800 --> 00:53:32,280
that at yourself, you get like a nice read. And you could send that. This would be very helpful for,

542
00:53:32,280 --> 00:53:37,880
for, for, for patient intake, essentially, it's the problem. So the problem is though, can we

543
00:53:37,880 --> 00:53:43,480
analyze the noisy camera that is on your phone, which is not as good as a high quality video camera

544
00:53:44,360 --> 00:53:48,360
that they used in past experiments? So people have been able to extract heartbeat fairly

545
00:53:48,360 --> 00:53:53,000
reliably, but they're using a high quality camera and it's calibrated, et cetera. So I hope

546
00:53:53,000 --> 00:53:59,240
a question is, can we do that with, with, with commodity cameras? So that's one interesting,

547
00:53:59,240 --> 00:54:03,320
interesting question that's, that's coming up right now. And I just want to say, I think that

548
00:54:03,320 --> 00:54:07,880
roboticists will have some, hopefully, will be able to contribute to helping address this kind

549
00:54:07,880 --> 00:54:14,680
of problem by facilitating telemedicine robots that can be used, you know, to, to admit and screen

550
00:54:14,680 --> 00:54:19,560
patients, et cetera. So you can imagine the, you know, the pictures that we've seen of drive through

551
00:54:19,560 --> 00:54:25,560
testing facilities, right? At some point, in some future, you know, some of that could be automated

552
00:54:25,560 --> 00:54:29,400
through robots. Right. So the whole idea being you should be able to walk through with those,

553
00:54:29,400 --> 00:54:33,240
you buy it, you get it, maybe like the kit of a swab and a few things, but you kind of you,

554
00:54:33,240 --> 00:54:38,920
you kind of walk through this, you know, and the camera helps me basically make sure it's consistent.

555
00:54:40,120 --> 00:54:43,720
Last thing I know we're writing out of time, but can I just mention this idea that I have about,

556
00:54:44,280 --> 00:54:49,480
about complementarity? Please. All right. So very quickly, I want to adjust this issue,

557
00:54:49,480 --> 00:54:55,080
you talked about earlier Sam about the, the fears about robots taking over and becoming our overlords.

558
00:54:55,080 --> 00:55:00,760
And I hear you, I mean, I think that's very, very prevalent and it's even in very major publications,

559
00:55:00,760 --> 00:55:06,360
like the New York Times and New Yorker, but the reality is that we're, we're very far from that.

560
00:55:06,360 --> 00:55:10,840
And what I think it's really, we're, we're important to keep in mind is that robots have great

561
00:55:10,840 --> 00:55:16,600
potential, robots and AI systems have great potential to enhance us as human workers. And they can

562
00:55:16,600 --> 00:55:23,800
reduce drudgery. So for example, that they can help do many things in almost every job. There's

563
00:55:23,800 --> 00:55:27,160
some aspect of the job you don't like doing. And that's usually the most boring drudgery part.

564
00:55:27,720 --> 00:55:33,640
So can we get robots to do that? That frees up humans to do what we do best, which is

565
00:55:33,640 --> 00:55:39,080
interact with each other to grasp and manipulate complex objects, write all these things that we're

566
00:55:39,080 --> 00:55:44,440
doing. And that, that's what I call complementarity. So it's an idea of looking at robots, not as some

567
00:55:44,440 --> 00:55:49,480
threat that's going to take over, but it's actually something that won't work with us in, in new ways.

568
00:55:49,480 --> 00:55:55,480
And I usually use the picture of, of, of, from Star Trek, of Captain Kirk and Spock,

569
00:55:56,680 --> 00:56:01,640
because they exemplify complementarity. Spock is kind of like a robot.

570
00:56:02,760 --> 00:56:10,760
It's all about logic and, and, and Kirk is all about, you know, intuition and, and, and humanity.

571
00:56:10,760 --> 00:56:14,760
So the, but what's interesting if you look at the show is that it's very much always the two of

572
00:56:14,760 --> 00:56:20,360
them working together. And that message, I think, is extremely relevant today. Oh, we've seen that

573
00:56:20,360 --> 00:56:27,160
in industry, over the past few years, there's been a move in the robotics space to develop these

574
00:56:27,160 --> 00:56:34,920
co-bots and to kind of optimize on the interactions between the human and the robot, both from kind of

575
00:56:34,920 --> 00:56:40,120
a safety perspective, but also kind of training through imitation and some other things.

576
00:56:40,120 --> 00:56:45,480
Right. And, and you're using the human is in the loop, both for the training side, but also after

577
00:56:45,480 --> 00:56:52,120
that, that the, the human could be enhanced. So in a warehouse and setting, you would have, you're

578
00:56:52,120 --> 00:56:56,040
not going to, people say, well, you know, we're going to wipe out all these jobs. Well, no, I really

579
00:56:56,040 --> 00:57:01,960
don't think that's going to happen for, you know, any foreseeable future. We just can't get enough

580
00:57:01,960 --> 00:57:08,840
humans to work on these jobs. So in e-commerce, for example, which is exploding right now, because of,

581
00:57:08,840 --> 00:57:16,440
of, of, of the COVID virus. And, and we'll only continue to increase that, we, and also people

582
00:57:16,440 --> 00:57:22,600
are not wanting to come into these big factories or warehouses to, to work. And so we need robots

583
00:57:22,600 --> 00:57:28,360
to assist. But there's going to be still a need for humans, because it's all kinds of corner cases.

584
00:57:28,360 --> 00:57:34,280
And we can facilitate human making, making intelligent decisions, but we're, we're, we're not fully

585
00:57:34,280 --> 00:57:42,760
replacing the human. Do you think we iron out the social issues, um, you know, the relationship

586
00:57:42,760 --> 00:57:50,520
between kind of income well-being and value creation and time on job and all these kinds of things

587
00:57:50,520 --> 00:57:56,680
to create a path to this feature you're describing where, you know, the robots can get rid of the

588
00:57:56,680 --> 00:58:02,440
drudgery, but people can still make a living or, you know, be alive. Right. Well, you know, I mean,

589
00:58:02,440 --> 00:58:08,040
I think it's super important to be, to be very sensitive to that. And I think you're, you're right.

590
00:58:08,040 --> 00:58:14,680
I mean, let's take, you know, the idea of, um, drivers. Right. Uh, it's been so much

591
00:58:15,400 --> 00:58:21,640
said about the imminent, you know, self-driving car. And, and as many people who worry about that,

592
00:58:21,640 --> 00:58:28,200
they make their living, um, driving, whether it's trucks or taxis or buses or, or, or, or ride, um,

593
00:58:28,200 --> 00:58:34,760
services. But I, and there's a lot of fear out there, but I want to say that's, we're not going

594
00:58:34,760 --> 00:58:42,360
to replace those drivers. They just, we're not going to put a, um, a truck driver into driving,

595
00:58:42,360 --> 00:58:47,800
you're not going to be having a robot driving a truck through a city or a taxi for, for, for,

596
00:58:47,800 --> 00:58:55,080
for 30, 50 years. It's just, it's, it's such a hard problem. So the drivers, you know, what the

597
00:58:55,080 --> 00:59:03,240
driver is, you know, the driver's value is no longer staying awake for long stretches on a

598
00:59:03,240 --> 00:59:08,520
pretty straight road. It's now getting this machine that got him from one end of this long road

599
00:59:08,520 --> 00:59:14,040
to the next in and out of the city and, you know, offloading the goods and all that kind of stuff.

600
00:59:14,040 --> 00:59:19,000
Right. Exactly. Exactly. So we can support the driver, assist the drivers. And we're already

601
00:59:19,000 --> 00:59:24,600
seeing that with, uh, with something like Google Maps, right? Driving in a city is a lot less tedious

602
00:59:24,600 --> 00:59:28,840
and stressful than it used to be because of Google Maps. Still stressful, but it's, you know,

603
00:59:28,840 --> 00:59:32,680
it's better. So, you know, where can we start thinking about other ways that we can do that to

604
00:59:32,680 --> 00:59:38,840
facilitate them? We don't have to do the boring things. And that's what I, I do think there's a,

605
00:59:38,840 --> 00:59:43,400
there's a lot of positive potential. We're going to see a lot more of that. But at the same time,

606
00:59:43,400 --> 00:59:49,160
let's be very conscious of who is put at disadvantage in all of these technologies.

607
00:59:49,160 --> 00:59:55,000
Uh, well, Ken, thanks so much for taking the time to chat with us. Share a bit about what you're

608
00:59:55,000 --> 00:59:59,960
up to. It's delightful speaking with you once again. Oh, it's a pleasure, Sam. Thank you so much for

609
00:59:59,960 --> 01:00:08,680
having me on your show. All right, everyone. That's our show for today. For more information on

610
01:00:08,680 --> 01:00:16,600
today's show, visit twomolai.com slash shows. As always, thanks so much for listening and catch you

611
01:00:16,600 --> 01:00:26,600
next time.

