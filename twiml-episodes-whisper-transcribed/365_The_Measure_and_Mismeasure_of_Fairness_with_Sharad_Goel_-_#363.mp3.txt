Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.
Alright everyone, I am on the line with Sharred Goyle. Sharred is an assistant professor in
the Management Science and Engineering Department at Stanford University. Sharred, welcome to the
Tumel AI Podcast. Hi, thanks for having me. Absolutely, absolutely. So we are going to dive into a
conversation about your paper, the measure and mismeasure of fairness, a critical review of fair
machine learning. But before we do that, you've got an interesting background. You have joint
appointments or courtesy appointments actually to be more precise in the computer science, sociology,
and the law school at Stanford in addition to your primary appointment at Management Science and
Engineering. How did you come to work at the confluence of these many disparate areas?
Yeah, I mean, it's definitely an unusual intersection that I'm working in. I hope more people
start working in this area. I guess I had a slightly non-traditional path to university. I did my
PhD in math. I did a postdoc in math and then I realized maybe it wasn't exactly for me,
that kind of traditional academic math path. And I went off to industry first at Yahoo and
then Microsoft and I spent seven years doing, I guess we were doing data science, but it wasn't
really called data science at that point. And I did that for a while and I started really becoming
interested and applied statistical, applied machine learning problems, this so-called big data,
distributed computing, and especially how they were applied to social science questions. And so
again, at that point, I was, you know, this was kind of the people were still trying to figure out
what's going on with Facebook, Twitter, and these were kind of the early days for network analysis.
And there was a sense that there was something that computer scientists could bring to these
questions, these social scientific questions, but it wasn't exactly sure what that was. And so I was
fortunate enough to be in some of these places that were driving these conversations. So that was
my first kind of foray into this intersection of computer science and social science. And then
at the end of my time in industry, I started becoming even more interested in policy questions. I was
living in New York at the time and I became quite interested in policing practices. Stop and frisk
was in the news as it is actually reoccurring now in the news. There are a lot of high profile court
cases. There was a high profile mayor's race and the police chief ultimately was replaced.
And so I, you know, was coming at it from the perspective of what, you know, what can we say that
goes beyond the traditional legal and policy analysis to these questions that I thought were,
you know, super important I wanted to get involved, but I wasn't quite sure what that involvement
would look like. And what we ended up doing is, you know, there's this one line from a court case
that struck me saying that lots of people were being stopped unjustifiably, but we would never
know how many people were, that was affecting. And this was in reference to what are called
fourth amendment constitutional violations where someone has stopped without what's called a
reasonable suspicion of criminal activity. And the traditional way that this is looked at by the
courts is that you interview the people involved, the individuals who are stopped, the police
officers involved, any witnesses. And then you have to come to, you know, here I've been
storing, then you have to come to a decision. And, you know, in New York, at the height of stop and
frisk, you're talking about half a million people stopped a year. And so there is no way that you
could go and interview people in all of these cases. And so kind of the first project that I worked
on that was at the intersection of machine learning and policy was trying to develop a method
to estimate how many people were being stopped in violation of this constitutional protection
against unreasonable stops and seizures. And so we ended up, and ended up being a pretty
straightforward project, but the result I think was striking, at least to me, we estimated that
something like 40% of people were being stopped with less than 1% chance of having a weapon on them.
You know, it's not like totally clear what the bar is for reasonable suspicion. I mean, the courts
have, I would say, adamantly refused to put a probabilistic threshold on that kind of phrase,
reasonable suspicion. But I think a lot of people, including myself, would say 1% is pretty low.
And if you have 40% of people who are being stopped without even a 1% chance of having
contraband of having a weapon on them, it really raises the question of whether or not it meets
that constitutional threshold. That was, I think, an interesting and revelatory to me that these
machine learning tools could be used in these contexts that were very different from what I was
traditionally doing, like, you know, maximizing click-through rates, you know, just to give an example.
And so it was very different domain and something that I became quite interested in pursuing.
And at that point, I more or less at the same time moved to Stanford and then started really
tried to understand what these computational statistical and specific machine learning methods
could add to our broader conversations about policy and social issues.
So that experience awakened you to an interest in the application of machine learning and data science
to these social challenges. How did, you know, what were some of the first things that you did to
really dig into that to understand how to apply them?
Yeah, so literally the first project that I started when I came to Stanford was a collaboration
with a journalist here who was interested in understanding police practices across the country.
So again, this was very close to the work that we were doing in New York or revolving stop and
frisk. And the basic question was to see if we could quantify some of the potential
discrimination that was happening in police encounters. And so again, it's like if you kind of
backtrack to, you know, five, seven years ago, there was, there were a lot of conversations
happening around police practices. But to the most, for the most part, these were kind of compelling
stories that people would tell about their own experiences. But there was a lack of data that we
could bring to this problem. And I say this not it all to minimize the value of these stories
that people were telling, but to highlight some of the challenges that we were facing at the time,
where, where we just didn't have any, we didn't have any broad systematic way of looking at
policing across the country. We had these disparate stories, these incidents, these news reports,
but we didn't have a way of bringing this all together. And so we started on this project.
And, you know, I was like admittedly pretty naive. I was like, okay, this is great. This is
super important. And you know, we're going to spend like a year, we're going to try to collect
all this data and try to make, make some sense of it. And then hopefully that'll add to this conversation.
Now I think this is six years into the project. We're still going. It's been, it's been
extremely difficult. We filed hundreds of requests, public records requests, FOIA requests,
with state agencies, with municipal police departments, we have a team of maybe, you know,
something like 10, 11 people who have worked on this project, probably over 10,000 hours of work,
just compiling the data. And so this is what we call the Stanford Open Policing Project.
Yeah. There are really kind of two big challenges in that project. The first is the data collection
and the standardization, which was extraordinarily difficult. So you can imagine you get
information from, or even even, even getting information from a department or any form is
extremely difficult. But when you get that information, it comes to you in paper records,
it comes to you in CDs, it comes to you in all sorts of, you know, different formats. And then
coercing that, coalescing that into something coherent is extraordinarily difficult. So that was
the first challenge. I mean, formats aside, how do you even capture the salient aspects of a
police stop? It's, it's, we didn't know how to do it. And so we took a very narrow
view of this. And we said, we're going to clock as much information that we can, we can,
but really we're going to focus on searches, police searches. Because that's something where we
felt that we could develop a machine learning toolkit for understanding how these, how these
searches are carried out in the extent to which there might be discrimination in that particular
police action. And so we spent several years both collecting, cleaning the data and then also
developing statistical methods to audit police search behavior. And what we found, again, perhaps
unsurprisingly, but I do think it's sort of, it was, I do hope that we're bringing something
new to that conversation is saying that we, that we did find, I would say pre compelling statistical
evidence that the threshold for searching individuals across the country, black and Hispanic
individuals across the country, was lower than the threshold for searching white individuals.
And so these are conditions on being stopped. So people will be stopped for all sorts of reasons,
maybe speeding, maybe reasons that are, that are less directly related to a traffic violation.
For example, a broken tail light, but there are, you know, various reasons that people are stopped.
And then after someone has stopped, we can ask what, what is it that props a search? And so a
search might happen somewhere between one and 10% of the time, depending on where you are. And
what we're finding pretty consistently is that that threshold for searching, black and Hispanic
drivers tended to be substantially lower than the threshold for searching white drivers. And
that's what we took as evidence of bias in that particular interaction. And you asked, what is it
that prompts a search? Did you have characteristics in your data that was able to answer that question?
What specifically prompts a search like are the officers required to report, you know, what their
suspicion was? Yeah. So it, again, it depends given the plurality of diversity of jurisdictions
that we're looking at. You know, it varies a little bit from jurisdiction to jurisdiction.
But broadly, I would say there are kind of two reasons that people are conducting searches.
Buying the kind of predominant reason is suspicion of drugs. So relatively,
relatively low-level drugs, you know, think marijuana, small amounts of marijuana.
And then the second is officer safety. So think of a weapon. And so these are the two big
reasons for carrying out a search. What was it ultimately looking at kind of the relative success
rates of finding the thing that they were looking for once they started the search? That's exactly
right. And so done. I mean, the simplest thing, and there's actually this kind of, this was called
the hit rate test or the outcome test. It goes back to Gary Bakker, the Nobel Laureate in economics.
And it's very clever idea. It just says, let's look at the rate at which an officer recovers
contraband. So let's say you're looking for drugs. How often do you actually recover drugs?
So let's take an extreme example. Let's say that when an officer is searching for drugs,
they end up in they search a black motorist. They find drugs 1% of the time. But on the other hand,
let's say when they search a white driver for drugs, they find drugs 99% of the time. So intuitively,
that suggests that they're only carrying out that search of a white driver when they're really
sure they're going to find something. But for a black driver, they're willing to carry out that
search on much less suspicion. And so that's evidence of a double standard. And again, we would
think of that as a discriminatory search threshold. And so it's a great starting point.
This kind of hit rate analysis or outcome analysis. What we discovered is that the test itself,
while informative, has all these kind of funny statistical anomalies. And so it could be,
and this is what we found in some jurisdictions, that the hit rate for finding a contraband on
black drivers was sometimes higher than the hit rate for finding contraband and white drivers.
And so the typical, background style analysis would say that the officers are discriminating
against white drivers in that situation. And we thought that was pretty unusual. And so it's like,
maybe there were, maybe there's situations where there wouldn't be any discrimination. But we thought
it was, you know, a little bit unusual that there's actually active discrimination against white
drivers in these types of police interactions given everything we know. And so we dug into this test
and we found out that they're, even though it's quite intuitive, there are all sorts of things
that can throw it off. And so we designed a new test, what we call the threshold test.
A word of some examples of the things that would throw it off. Yeah, so it's really good for
just not stopping any white drivers. Yeah, so it's really good question. And so this is what
economists call the problem of informationality. So let me give you an example of this.
If I could hit pause on that, because I don't want to forget this, you described the test.
And I forget the name of the Nobel laureate, but the suggestion was that this was a nuance test
or maybe even it contributed to him winning the Nobel Prize. But it seems obvious. Like, what am I
missing? What is baked into this that is beyond what you might expect? So I think what's great
about this test is that in hindsight, it's, you're right, it is obvious. You know, it feels obvious.
I would say it wasn't the way that people were thinking about it at the time. So the way that most
people, it would say still the way that most people think about detecting discrimination is looking
at rates of action. So looking at search rates. And so you might say, let's look to see, is it the
case that black drivers are being searched more often than white drivers? And in fact, we see
that that's true. We see that black driver that searched twice as often on average across the
country than white drivers. You don't have a baseline or a ground truth or anything to compare.
Exactly. And so the standard way that people address this issue is starting to adjust for
various factors. So they're saying, okay, let's adjust for where you are. Let's adjust for the
time of day. Let's adjust for, you know, maybe the type of car you're driving. Let's adjust for
all these things. But ultimately, you can't really adjust for the fact that maybe the officer
sees something that's not in the data. And that's what is prompting the search. And so back
around this very, very clever insight in your right that in retrospect, it's totally obvious.
But the time it was quite clever of saying, let's turn this problem on its head. Let's not look at
the rate at which we're searching people. Let's look at the rate at which those actions are successful.
And that gets around what statisticians call omitted variable bias. Now we don't have to know
exactly why did the officer or the bank manager or the hiring manager decide to take action.
Let's just look at whether or not their decision is ultimately successful.
Great. And then you were about to introduce another statistical term statistical anomaly.
Yes. Yes. Yes. And so this is a funny term. It's called the problem of informationality.
And for marginality. It's not, it's not that common. It's not even that common in economics.
It's, you know, we sort of stumbled upon it where we're going through this literature.
And here's the idea that our notion of discrimination
intuitively is based on a double standard. Whether or not officers are applying a double standard
when they decide to search individuals from different race groups.
Now, hit rates, the search success rate doesn't directly tell you the standard that was being
applied. It just tells you how often those decisions were successful.
And so the funny thing is, let's say, and we actually see that this is happening some jurisdictions,
let's say that officers are discriminating by applying a lower standard
when searching black drivers relative to white drivers. But let's say that there's a group of
black drivers who, if you search them, you're very likely to be successful. So,
so in some cases, you might actually see the contraband sitting on the passenger seat,
or you might see some strong evidence that suggests, if I search this person, there's like a 90%
chance I'm going to find something. Now, because there is this group of really high likely
to find contraband drivers, and if they're not equally distributed across race groups,
that can boost your hit rates even though you're applying a lower standard on average
when searching black drivers in this hypothetical. And so it's a very kind of subtle phenomenon.
And it's not only kind of subtle to see that it could be happening. In theory,
it's subtle to see if it's happening in practice. And so I would say we made two contributions
to that literature. And the first was to find evidence in the data that this is something you
really do have to worry about. And it was triggered by the fact that we were seeing all these cases
where Becker's test would have concluded that you're discriminating against white drivers.
And just kind of naive analysis, even though super intuitive, would suggest something that we
really had strong reason to believe wasn't really happening in the world. That we didn't think
that officers were discriminating against white drivers. It seemed odd, not impossible,
but it definitely got us scratching our heads when we saw that pattern. And then the second
contribution was developing this, what we call the threshold test, that tries to get around
this problem of informationality and directly infers that standard of evidence itself,
not just the hit rate, which is a proxy for the standard. It's a lot. I know. So it's definitely.
There's part of this description suggests to me, you know, some kind of statistical approach
where you, instead of assuming a single distribution, you assume two distributions or multiple
distributions, depending on how many of these classes that you assume and then trying to,
you know, I don't know, fit a stop to a distribution or something like that.
That's exactly right. And so the idea, this was Becker's insight, is that it's possible
in theory that drivers are carrying contraband at different rates. Like we don't know,
but if I see elevated search rates for one group, I at least have to be open at the possibility
that one group is carrying contraband more often than another group. And that might be a
non-discriminatory explanation for why they're being searched at higher rates. And so the idea for
the threshold test was let's try to find a way to explain the results that are consistent with
elevated search rates, but also consistent with the hit rates that we're seeing that would explain
all of these different things at the same time. And statistically what we're trying to do is find
distributions of risk that match all of these observable features in our data.
Drillin to that, the observable feature, does the data that you receive report on
beyond that the individual was stopped and a search cause? Does it report some kind of feature
that allows you to separate the stop into multiple classes? Yeah, it does. I mean, again,
when we're lucky, when we're not lucky, it's all lumped together. But when we're lucky, we can
get rid of, I would say, what are sometimes called non-discretionary searches. And so this might
be an impound. Like once a vehicle's been impounded, then an officer will carry out a search
just kind of part of the process. And so we want to throw those out because those aren't really
directly predicated on suspicion of anything in particular. It's just part of the procedure.
When someone is arrested, then often a search is conducted afterwards. It's called a search
instant to an arrest. Again, not really predicated on specific suspicion. It's just what the
procedure is. And so we try to throw out all of those. And when we leave what we think of as
discretionary searches that are really directly tied to suspicion of, of contraband. Are you trying
to classify those searches that are left based on some feature that says, oh, well, in this search,
there was a gun sitting on the passenger seat. And so this was one of these high-risk situations
versus the other or is the idea that you don't actually have that information in the data. And so
you're, this threshold is being used to try to associate the different stuff. So that's it. So
so in theory, the test is supposed to be able to distinguish between those without the information
directly. Now in practice, of course, it's trickier. And what we're finding that gave us a little
bit of confidence in some places like North Carolina, we had particularly good data. And we were able
to, you know, we were we asked the test to try to infer what was going on without explicitly giving
it that information. And it turned out though that some of this information was in the data also.
So we can look at these stops that were being flagged as potentially high risk. And we say, oh,
in fact, the officer marked that this was, quote, unquote, a plain view search, meaning that they
saw evidence of something in plain view or contraband and plain view. And in fact, the test
was able to infer that without us directly providing that information. And so this gave us some
confidence that even in places where we don't have that information, we could apply it and it would
try to automatically infer what was going on. And I saw note that the data set that you've collected
of these stops is now over 100 million traffic stops. Yeah, we are I think over what period of
time is it? That's an insane number. Yeah, we've actually released over 200 million
traffic stops from across the country. You know, it varies by jurisdiction, but going back about 10
years. Everything is publicly available now. And so if you go to openpolicing.stanford.edu,
you can just go and download everything. We've made it super easy or when we think it's super easy
to use. We've cleaned it all up. We've standardized it. A lot of researchers, policymakers,
law enforcement agencies, reporters, lots of different groups, community activists, a lot of
different groups are using the data to kind of audit their own jurisdictions to help improve
local practices. And so we're super supportive of all of those uses of the information. We
analyze in about of those 200 million stops, about 100 million were in good enough shape that
we could statistically analyze, but we have released everything. And so we hope that other people
continue working on it. We've already seen some positive outcomes from releasing the data.
One thing that I was particularly happy to see is in collaboration with Los Angeles Times,
they did their own analysis using the open policing data and using the threshold test and using
other statistical tests, which they complemented with on the ground reporting. And about a week
after they released their reporting, the LAPD dramatically reduced their stop practices.
Are there questions that you wish people would try to answer based on this data? Do you have
or things that you wish people would do to help support the project in terms of,
you know, we've got a pecking order of, you know, these are the 10 things that are wrong that,
hey, if someone just magically did this, yeah. I mean, yeah, I mean, I think they're like kind of, they're
sort of maybe three big things that I would like people to do. I mean, one is making everything
super local. And so our analysis was broad by necessity across the country. We have, I think,
something like 100 jurisdictions represented. And we can't go in and look at the idiosyncrasies
of any given jurisdiction. And so we are counting on local journalist, policy makers,
community activists to look at the data for their communities and try to understand what is going
on and try to, you know, push law enforcement agencies to improve practices. And so that's
something that we can do. And we've tried to set a template for doing this nationally. But
we really, we know if there are anomalies in any specific jurisdiction, we just can't,
we don't have the bandwidth to deal with that. And so that's where we've seen kind of the most
direct consequence is when people like in Los Angeles, when they go in and they really make it
their own. And especially if they're kind of complementing the data analysis with reporting,
we think that that can make a big difference. The second big thing is we need to figure out
how to make sure this project moves forward. We've spent something like six years doing this.
But I'm not sure how much we can do going forward. This was kind of an enormous kind of commitment
of resources. And I would love to see this project continue and even expand. But I don't know
what the best way is to make that happen. And so I hope that other people will take up the cause
and again, make this project their own and help, you know, filing these records, cleaning the data.
We released all of our code. We've tried to make this easier for other people to do themselves,
but it's hard. It's very difficult to do. But I am hoping that there'll be someone who can
can help take this up. And then the last thing is really on the ML sat side is that there is a lot
of work that needs to go in to still understanding what doesn't mean to have an equitable police interaction.
And part of this is statistical. Part of this is policy. But even when we understand some of the
policy issues, it's still not clear how to measure them statistically. And so we, you know,
we did the threshold task, which I think is a good first step or maybe a good second step into
this problem of understanding potential bias and search decisions, but it's really an early
foray into this area. And I would love to see more people create more nuanced ways of measuring
potential discrimination in that action and that search action, and then more broadly in every
type of policing action that's happening. So that's maybe a good opportunity to segue to
the paper, the measure and mismeasure of fairness, a critical review of fair machine learning,
is there a connection between these two works beyond the kind of application of statistics
and computational approaches to policy questions and problems? Yeah, it's very directly related.
So all of the insights pretty much that we got in the five, six years that we were working on
understanding bias and human decisions, we translated quite directly to understanding bias and
allergic decisions. And even at the technical level of this problem of informationality,
the exact same issue comes up when we're talking about bias and allergic decisions. And so it was
really just a direct continuation of that work. We didn't know it at the time. And so we were like,
oh, this is interesting. And kind of over the course of thinking about this problem,
we're like, actually, these are like deeply connected. And then we kind of put all the pieces
together. We're like, oh, you know, really it's the same problem. It's just two sides of the same
coin. All right. Well, can you walk us through the kind of the goals of this particular paper?
Yeah. So the background of our paper is that there are many in the computer science community,
in particular, who were interested in, for lack of a better word, methodizing fairs. And so what
does this mean? That means that you have an algorithm. And we kind of all, or at least now,
I would say many of us have this idea that algorithms are not just through a neutral objective
objects. But they, you know, they have implications. And we want to ensure that these algorithms
are reusing in these high stakes context. For example, judicial decision making,
healthcare decisions, policing, all of these high stakes context. We want to make sure that they're
fair, whatever that means. And I think one way that I, you know, I've been trained to think,
and I think a lot of other computer scientists have been trained to think, is let's formalize this
idea in terms of math. And I understand that kind of that impulse to do it. But what we saw is
that the leading mathematical definitions of fairness that we proposed were missing. I think a lot
of what was happening on the ground and how we interpret what these algorithms are ultimately
doing. And it wasn't, at least in my view, it wasn't moving us in a direction where we could
really start evaluating the equity of these types of algorithms. And in some cases, definition,
in fact, I would say the most popular mathematical definition of fairness, what's what's called
equal false positive rates, equal error rates. In many cases, this can lead us astray and cause one
to create algorithms that many people would consider to be patently unfair.
Well, let's maybe go through the different approaches that folks take and then we'll talk about
their failures and how folks should be thinking about this. So you started, you were just mentioning
classification parity. Yeah, so classification parity. And so again, I should point out these are
kind of words that we have made up in the field is still evolving. And so there isn't necessarily
a common vocabulary around this yet. But this idea of classification parity or equal error rates
means that let's say that we have an algorithm and I'll give you a concrete example. So an algorithm
that might try to assess the risk of some adverse outcome. For example, someone failing to appear at
a scheduled quartet and we might say, well, how often does the algorithm get this wrong? So how
often does the algorithm flag someone as high risk who actually would ultimately show up to their
quartet? And so this is called a false positive. And so you can say we, you can say that algorithm is
fair if it's a case that false positive rates are equal across race groups or across gender groups
or across whatever your favorite group classification is. So that's one popular definition. And another
popular definition is saying that an algorithm is fair if it doesn't use protected characteristic
once making its decision. So it doesn't use, so the algorithm doesn't have access to, for example,
someone's race or gender. In some ways, this is just blinding the algorithm. And so it's again
an intuitively quite appealing thing that I can say, okay, well, if I didn't tell the algorithm
about someone's race or gender, then how can it be discriminating? So that's another, you know,
quite popular definition that people use. They challenge with that one being correlations and
zip code and race, for example. Yeah, so it's super challenging. So one kind of common
objection to that blindness notion of fairness is that there's lots of stuff that is correlated
with race. That's not race itself, like where you were, you know, where you grew up, where you live,
all of these aspects that really feel that they might be a proxy for race. Now the problem is
everything is correlated with everything else. And so it's not even clear where to draw the lie.
Like what constitutes a proxy and what, you know, what is kind of an innocuous correlation.
So now I think it's been pretty well understood for a long time. One thing that's a little bit
less well understood is that in some cases, you might even want to have, you might want to include
the protected characteristic in your algorithmic decision. And so this strikes people as unusual.
Let me give you an example in the decision or in the in the algorithm. In both. You know,
there are approaches that people are proposing that include the protected attribute in the algorithm
so that the algorithm or the model can ensure anticorrelation with that attribute. But that's
slightly different than including it in the decision. So I guess I think these are a little
bit related. So it's like a hard to totally distinguish the algorithm from the decision. So let
me give you an example where you might use this directly affirmative action. So here's a case
where a lot of people including myself would say that it's completely ethical, equitable to use
a protected characteristic namely race when you're making that decision about who to admit to a
college. You know, I mean, hopefully this example points out that there are cases where we very
actively want to consider someone's group membership in order to make better decisions.
Now let me give you another example which is even more subtle and it's more controversial. I
actually don't know what the answer is, but I think it's an instructive example. So what we found
and we've looked across the country is that if you take a man and a woman who have similar criminal
history or similar age, and so these are the traditional things that people use when they're
trying to assess risk, women are less likely to re-event than men. And so again, men and women
have similar backgrounds, similar criminal history, similar age, women are less likely to re-event
and re-event in the future than men. And so if you have a blind algorithm that doesn't consider
gender, you're going to systematically overestimate the risk of women and you're going to systematically
underestimate the risk of men. What the implication of this is is that if I now just decide to
take some action, for example, detain an individual who's deemed high risk, I'm going to end up
detaining women who are deemed high risk who actually statistically know are not high risk.
They just happen to look like the men who in fact are high risk. And so this is a tricky situation
because statistically it's an easy fix. Statistically, I just say that I know all our
sequel women are less likely to re-event than men. And so that should go into my decision-making
process. That should go into my statistical algorithm. And some jurisdictions actually do this.
Wisconsin does this, for example. But the flip side is that there might still be, and I think
this is true, that there's kind of a social norm that's being violated. And we don't like to
focus on these differences across groups. And there's a real cost to sending the signal that
there might be average differences across groups, even when we just for certain things. And that's
not necessarily saying that this is inherent. It's not saying that it's biological or that it's
going to last forever. You know, maybe it's related to all sorts of things, whether or not you're
a primary caregiver or who knows what it's related to. But it might be a reasonable proxy for
figuring out who is higher risk and who is not. But do we want to use that attribute? Do we
fundamentally want to use gender when we're making these types of assessments? And I don't know what
the answer is. And different people, I would say, have recently come to different conclusions.
But the reason I say there's no easy answer is that if you decide to use gender, well, you're
using gender, and now we're advertising to everybody, now we think this is an important factor to
consider. And if you don't use gender, then you're ultimately going to be detaining women at a higher
rate than is really necessary to ensure public safety. And so it's very difficult. Most jurisdictions
that I know have decided not to use gender. But I don't know. I don't know if that's the right answer.
And again, I think it's just a challenging question that to me points out why these kind of
hard and fast rules like saying we can't use protected characteristics are really
trying to sweep some of the hard policy questions under the rock.
And so is there a third traditional definition that you look at calibration?
So another definition that people use, they're really three, the one, the first is these
error equal error rates, the second is anti-classification, and then the third is what's called
class, is called calibration, where it says that if I give individuals, two individuals,
or two groups of individuals the same risk score, the outcomes should happen similar rates.
And so if I say that here's a group of white individuals that are classifying as medium risk
and a group of black individuals that I'm classifying as medium risk, then they both should ultimately
re-offend its similar rates. And if that's not true, then it suggests that something is missing
from my algorithm that maybe that I somehow created it in a way that isn't appropriately
making use of the data that's available to me. And in my mind, this is an important property
to have, but it's a pretty low bar. And so there are many algorithms out there that would have
this property that they are calibrated, but they're not actually algorithms that we would really
want to use in practice. And so even something like redlining, kind of this historical
example of what we now recognize to be a highly discriminatory way of making lending decisions,
that if I classify a neighborhood as quote unquote high risk, that that might be true on average,
but still the fact that I'm classifying neighborhoods at high risk as opposed to
looking at the specifics of an individual and determining, oh, you happen to live in a neighborhood
that on average is defaulting at higher rates in another neighborhood, but you in particular
for creditworthy, that strikes people, I think correctly, is being inappropriate.
And so sending an example of an algorithm, which is calibrated, but is not really getting
at the heart of what we think of as equity. And so what the paper is doing is it's looking at these
these three definitions. And yeah, I think we've talked through and introducing these definitions,
some of the unique challenges of each of them and kind of why they're difficult,
but the paper is taking it a step further if I'm understanding it correctly and saying,
is it saying that it's not that one of these is bad and the other is good, but that they're all bad
and that maybe help me understand what is the paper go from here. So we definitely take in,
in some ways, like burn the house down approach to this. I don't want to be the last minute,
I don't want to be too pessimistic. I advocate against taking a formal mathematical view
of fairness. I say that even though my training is in math, all my degrees are in math.
So that's what is coming down to us. It's like we're going to talk about and introduce these
approaches, but they all are problematic. And the answer is not another mathematical approach.
It is taking a step back from the math and why it's that.
Yeah, so I think that's exactly right. And I think you said it well, but in my mind, it's not
that we need more math. It's that we need to understand what is it that we're trying to accomplish.
And so if I were to give you a piece of legislation, you know, just traditional legislation,
I wouldn't say, you know, give me a mathematical definition that I could apply to the text of the
legislation and then tell you whether or not it's fair. We sort of recognize that that's,
and nothing is technically difficult to do, but that's missing the point of what we mean by equity
in these broader policy contexts. And I think when we use order to algorithm, it encourages us
to think in those mathematical terms, which while it has some value, I think it still misses
the goal of any policy intervention, which is trying to improve outcomes down the road. And so
that's where I would like the work to go of trying to understand how can we determine
what the consequences of any particular algorithmic decision-making system are. And to the
extent that we can formalize that great, but in many cases, I guess I'm personally pessimistic
that there's going to be some universal or a small set of definitions that one could apply
that lets us audit an algorithm and comes out with the green chap mark or the red ax that says,
this is fair and this is not fair. Do you have a kind of a keystone example where
all of the traditional approaches to fairness fail for various reasons? And we probably won't be
able to come up with another measure, but if we take a simple approach, ABC, that will
get us closer to what we want as a society. So I think criminal risk assessment system in
an area that has received a lot of attention. And so almost all criminal risk assessments are
going to violate the equal air rate principle. If you don't use gender, so adopting this anti-classification
point of view, it will violate calibration. And if you do use gender, it will violate anti-classification,
but it will be calibrated. And so it's hard to know what the right answer is, but you definitely can't
satisfy, you'll probably likely violate several of these formal fairness criteria at the same time.
Now what you do here, I think this is super tough. And the way that I think about the problem here
is again, the risk assessment algorithm in some sense is just trying to give you predictions about
what's going to happen to this individual. Is this person likely to miss court or not miss court?
Now when I think about fairness, I think about what you do with that information. So one thing
that we're trying to do is identify people who are high risk of missing court and then arranging
for them to have door to door or free door to door ride chair service from their home to court.
Because we recognize a lot of the time that people miss their court date is because they don't have
easy access to public transportation. They don't have another way of getting to court. And so taking
the exact same information, this like risk, this assessment that you're likely to miss court,
and saying we're going to provide a supportive service to you. And my mind makes it much more
equitable than if we were to take a punitive approach and say we're going to lock you up,
which I don't think addresses the court. To your point, even if your punitive system is based on
a much better message. Exactly. Even if you even if you use the exact same information,
you know, one scenario, I would say your action is not justifiable or I would not think of it as
ethical. And the other situation where you're providing supportive services, I think it becomes
much more tolerable. And so it's really in this sense, whether or not a decision is equitable,
can be justified is not so much how accurate your algorithm is, but what you use that information
to do. Well, Sarah, thank you so much for taking the time to kind of talk through your research
really interesting stuff. And I appreciated the opportunity to learn about what you're up to.
Thanks. It's been a great conversation.
All right, everyone. That's our show for today. For more information on today's show,
visit twomolai.com slash shows. As always, thanks so much for listening and catch you next time.
