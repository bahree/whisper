WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.640
I'm your host Sam Charrington.

00:31.640 --> 00:37.120
In this, the second episode of our NURRIP series were joined by David Spiegelhalter, chair

00:37.120 --> 00:42.280
of the Winton Center for Risk and Evidence Communication at Cambridge University and president

00:42.280 --> 00:45.080
of the Royal Statistical Society.

00:45.080 --> 00:50.280
David, who was an invited speaker at NURRIP's, presented on making algorithms trustworthy,

00:50.280 --> 00:55.760
what can statistical science contribute to transparency, explanation and validation.

00:55.760 --> 00:59.600
In our conversation, David and I explore the nuance difference between being trusted

00:59.600 --> 01:04.600
and being trustworthy and its implications for those building AI systems.

01:04.600 --> 01:10.160
We also dig into how we can evaluate trustworthiness, which David breaks into four phases, the inspiration

01:10.160 --> 01:16.760
for which he drew from British philosopher Onora O'Neill's ideas around intelligent transparency.

01:16.760 --> 01:17.760
Enjoy.

01:17.760 --> 01:23.480
Alright everyone, I am here in Montreal for the NURRIP's conference and I've got the pleasure

01:23.480 --> 01:26.280
of being seated with David Spiegelhalter.

01:26.280 --> 01:32.600
David is chair of the Winton Center for Risk and Evidence Communication at Cambridge, as

01:32.600 --> 01:38.040
well as president of the Royal Statistical Society and he was one of the invited speakers

01:38.040 --> 01:42.440
here at NURRIP's, talking on making algorithms trustworthy.

01:42.440 --> 01:45.240
David, welcome to this week in machine learning and AI.

01:45.240 --> 01:49.880
Thank you for having me, it's our pleasure.

01:49.880 --> 01:54.200
Before we jump into the topic of your talk, please share a little bit of your background

01:54.200 --> 02:00.040
and how you get involved in statistics, machine learning and kind of the confluence of

02:00.040 --> 02:01.040
the two.

02:01.040 --> 02:06.320
Exactly, well I'm a statistician as you can tell and I was around in one of the last summers

02:06.320 --> 02:12.120
of AI in the 1980s and I was very interested in computer-aided diagnosis such as it was

02:12.120 --> 02:16.960
then an interest in statistical approaches to that, using simple Bayesian methods or

02:16.960 --> 02:23.120
logistic regressions, the standard stuff and that was an exciting time and I got very

02:23.120 --> 02:29.280
interested in this new idea of Bayesian networks and graphical models and so in the 1980s

02:29.280 --> 02:33.360
I really worked and developed this thing called the Louds and Spiegelhalter algorithm that

02:33.360 --> 02:38.080
was for exact propagation in Bayesian networks and we did a lot of work in that and then

02:38.080 --> 02:44.200
I went into Bayesian graphical modeling, developing a bug software for Bayesian Markov-J

02:44.200 --> 02:54.040
Monte Carlo analysis and so on and worked all the time in this intersection of machine learning

02:54.040 --> 02:56.640
and AI and statistics.

02:56.640 --> 03:00.560
For the last 10 years I've been much more to do with communication and I've got a job

03:00.560 --> 03:05.600
that involves communicating statistics and risk and evidence and now we've got a center,

03:05.600 --> 03:10.280
a strange center in the maths department at Cambridge with a great gang of psychologists

03:10.280 --> 03:15.640
and communication specialist XBBC people, web designers, I'm very interested in producing

03:15.640 --> 03:21.120
trustworthy material that communicates numbers and statistics and risks and predictions

03:21.120 --> 03:22.120
and so on.

03:22.120 --> 03:23.280
Okay, that's really interesting.

03:23.280 --> 03:29.720
I was wondering what the meaning of risk and evidence communication was, almost anything

03:29.720 --> 03:31.320
to do with numbers.

03:31.320 --> 03:34.680
It's better than public communication and statistics.

03:34.680 --> 03:35.680
Right.

03:35.680 --> 03:36.680
Okay.

03:36.680 --> 03:37.680
Fantastic.

03:37.680 --> 03:42.440
And so you're here at NURBS talking about making algorithms trustworthy.

03:42.440 --> 03:43.440
What does that mean?

03:43.440 --> 03:46.000
Yeah, the issue of trust is very important.

03:46.000 --> 03:50.640
I've been very influenced by this wonderful philosopher in the UK and Nora O'Neill who studied

03:50.640 --> 03:56.560
Kant and has come up with this very important idea which has been very influential that

03:56.560 --> 04:00.680
organizations and developers of systems shouldn't be trying to be trusted.

04:00.680 --> 04:03.440
There's the wrong objective to try to be trusted.

04:03.440 --> 04:08.280
Well, they should be doing what we all should be doing is trying to be trustworthy in other

04:08.280 --> 04:13.520
ways to earn that trust because that is within our control to be trustworthy.

04:13.520 --> 04:18.800
And this idea of being trustworthy has a big impact in the UK, the National Statistics

04:18.800 --> 04:22.800
Code now puts trustworthiness as its number one objective.

04:22.800 --> 04:28.240
Why is that nuance important between being trusted and being trustworthy because being trusted

04:28.240 --> 04:32.040
is something you want, but other people can only offer it up to you.

04:32.040 --> 04:36.720
You trust worthy of something within your control, and that means really analyzing what

04:36.720 --> 04:38.240
it means to be trustworthy.

04:38.240 --> 04:39.240
Okay.

04:39.240 --> 04:44.840
And so what does that mean from a statistical perspective or how can statistics inform trustworthiness?

04:44.840 --> 04:50.800
Well, I think that in the talk I break trustworthiness of an algorithm or any sort of system into

04:50.800 --> 04:55.600
two components that the system itself should be trustworthy, the claims it makes should

04:55.600 --> 04:56.600
be trustworthy.

04:56.600 --> 05:00.840
You should be able to rely on them or if you can't rely on them, it can tell you how

05:00.840 --> 05:02.520
confident it is.

05:02.520 --> 05:06.960
The other thing is that what is very important is that the claims made about the system are

05:06.960 --> 05:10.760
trustworthy by the developers, by the commercial entity or whatever.

05:10.760 --> 05:14.480
So you've got not only believed the system, but you've got to believe what's said about

05:14.480 --> 05:15.480
the system.

05:15.480 --> 05:20.320
Now, what that leads you into very quickly is the importance of evaluation.

05:20.320 --> 05:25.520
And in my talk I draw an analogy with the highly developed evaluation phases that are

05:25.520 --> 05:29.280
used in drug development and pharmaceuticals, which statisticians I've worked in that area

05:29.280 --> 05:30.560
for decades.

05:30.560 --> 05:33.320
And they're just very briefly four phases.

05:33.320 --> 05:35.800
Phase one is safety on a few healthy people.

05:35.800 --> 05:41.000
Phase two is proof of concept done on some selected people to try to optimize the dosage.

05:41.000 --> 05:45.560
Phase three are the big control trials in which you actually compare it with a comparator

05:45.560 --> 05:50.000
and that allows you to market the drug and phase four is post-marketing surveillance.

05:50.000 --> 05:54.880
And what I did was draw an analogy with developing algorithms that are going to go into practice

05:54.880 --> 06:02.480
that phase one is just the digital testing that people do on a set of test cases.

06:02.480 --> 06:07.040
Phase two is laboratory tests where you actually compare it, say with doctors if you've got

06:07.040 --> 06:10.800
a medical system and do the user center design for the interface.

06:10.800 --> 06:14.560
And phase three is where the field tests where it actually goes out there and you actually

06:14.560 --> 06:18.360
evaluate what is impact is, which might be beneficial, but it could be harmful.

06:18.360 --> 06:20.520
You never know what side effects you might have.

06:20.520 --> 06:24.040
And phase four then is the post, once the thing is out there, monitoring to make sure it's

06:24.040 --> 06:27.520
not degrading and that it's not making mistakes.

06:27.520 --> 06:31.440
And so I suppose what I'm saying is that on the whole when I read about evaluations,

06:31.440 --> 06:35.280
they rarely get past phase one, they just sort of accuracy on test cases.

06:35.280 --> 06:40.480
Some of them moving into phase two comparison of diagnostic systems with medical, with experts

06:40.480 --> 06:41.480
and things like that.

06:41.480 --> 06:47.160
Almost nothing about phase three, what actually is the benefit impact when these things are

06:47.160 --> 06:50.280
put into practice in society and properly evaluated.

06:50.280 --> 06:55.280
And I think that the, in order for claims about a system to be trustworthy, then you need

06:55.280 --> 06:57.800
a much more rigorous evaluation.

06:57.800 --> 07:03.840
In order for claims about a system to be trustworthy, you need to have a much more rigorous evaluation.

07:03.840 --> 07:05.840
My sense is that we're very far from that today.

07:05.840 --> 07:06.840
Exactly.

07:06.840 --> 07:10.960
And I suppose what I'm saying, this field is developed so wonderfully.

07:10.960 --> 07:16.120
It's so, the stuff at the conference is so amazing, but it's still, for all that fantastic

07:16.120 --> 07:22.000
technical capacity at a very early stage, because when these things start moving into society,

07:22.000 --> 07:24.880
you find, you know, people are saying, hey, come on, you know, I want to divide it.

07:24.880 --> 07:28.160
It's not, you know, it's not immediately obvious that this is going to be a good thing

07:28.160 --> 07:29.640
in all areas.

07:29.640 --> 07:34.680
And so I think, you know, this area is due to mature into something which does rigorous

07:34.680 --> 07:35.680
evaluations.

07:35.680 --> 07:36.680
It's interesting.

07:36.680 --> 07:42.520
So one of the controversies that last year's Neurup's, then Nips was kind of a call for

07:42.520 --> 07:48.760
increased theoretical rigor around deep learning in particular, but, you know, our current

07:48.760 --> 07:52.360
approaches to AI in general.

07:52.360 --> 07:57.880
This is a call for rigor also, but a very different one, one from more of a statistical

07:57.880 --> 07:58.880
perspective.

07:58.880 --> 08:03.720
And it's about a rigorous test of what does it mean to actually implement this?

08:03.720 --> 08:05.080
Do you need both?

08:05.080 --> 08:10.200
Because you need the rigorous sort of internal analysis in order to demonstrate what it says

08:10.200 --> 08:11.760
is trustworthy.

08:11.760 --> 08:17.160
So because the part of the trustworthiness, of course, this is where we get to explanation

08:17.160 --> 08:21.160
is to be able to say why it's come up with its conclusion, to be able to justify that

08:21.160 --> 08:22.160
conclusion.

08:22.160 --> 08:25.760
And from the other statistical perspective, I take very strongly because statisticians

08:25.760 --> 08:29.840
are obsessed with uncertainty, getting the error bars right, you know, with as much

08:29.840 --> 08:33.160
concerned with the uncertainty as we are about the point estimate.

08:33.160 --> 08:34.920
And so that's what we bring.

08:34.920 --> 08:40.320
And I think again, if a claim is going to be made, and especially when it's made with

08:40.320 --> 08:43.760
some uncertainty or lack of confidence, you've got to understand what that means.

08:43.760 --> 08:51.120
You've got to rely not on the claimed confidence of what is, what is say, what an algorithm

08:51.120 --> 08:52.120
comes up with.

08:52.120 --> 08:59.240
And you've talked to you provide examples of this, the kinds of claims that you envision

08:59.240 --> 09:04.320
this kind of model being applied to and, you know, what you'd expect to see or what you've

09:04.320 --> 09:07.600
seen in kind of passing a claim through these filters.

09:07.600 --> 09:12.080
Well, in the talk, I just give various examples of different phases of how some statistical

09:12.080 --> 09:16.680
ideas can come in, just at the early phase when, you know, you're comparing algorithms

09:16.680 --> 09:19.720
on your database to see decide which is the best one.

09:19.720 --> 09:23.800
You know, I talk about ranking algorithms and how you use some bootstrap methods on the

09:23.800 --> 09:24.800
test set.

09:24.800 --> 09:28.120
You can get a probability that any algorithm is actually the best rather than just producing

09:28.120 --> 09:29.440
a simple league table.

09:29.440 --> 09:32.960
Again, there's been a lot of statistical work on league tables and essentially taking them

09:32.960 --> 09:37.520
apart because just because something happens to rank best on one particular set of data,

09:37.520 --> 09:39.880
it does not mean it's the best algorithm.

09:39.880 --> 09:42.880
Incidentally, for the football team, just because the football team is top of the league,

09:42.880 --> 09:46.120
it doesn't mean it's the best team because there's always luck involved and we're rather

09:46.120 --> 09:48.760
good at trying to put numbers on luck.

09:48.760 --> 09:50.840
So there's that aspect of the phase two.

09:50.840 --> 09:55.760
And again, the, you know, recent critique of systems that have made comparisons with doctors

09:55.760 --> 10:00.360
saying diagnostic systems, which are actually being slightly, you know, quite pulled apart

10:00.360 --> 10:02.640
because of their lack of statistical rigor.

10:02.640 --> 10:06.240
And, you know, it's very good they got to that stage, but actually they're not doing

10:06.240 --> 10:07.240
them very well.

10:07.240 --> 10:11.360
You're not doing to the standard of rigor that one would expect.

10:11.360 --> 10:15.120
And for phase three, I talk about an ultra-alars involved in the diagnostic system.

10:15.120 --> 10:20.080
It's a terrible system, but it actually helped when it was put into practice.

10:20.080 --> 10:23.360
And it's because it wasn't because what the system, the computer was saying, it's because

10:23.360 --> 10:28.600
it just, it changed the culture of data collection and encouraging people to make early diagnoses

10:28.600 --> 10:30.560
and being more confident about their work.

10:30.560 --> 10:34.960
So there's all sorts of unintended ways that systems might benefit, but also unintended

10:34.960 --> 10:38.800
ways in which they might harm.

10:38.800 --> 10:45.960
And so, you know, I went through those applications, but then I went on to this idea of transparency,

10:45.960 --> 10:49.200
which is, you know, is an element of trustworthiness.

10:49.200 --> 10:53.200
And this philosopher, Anora Neal, has got some great things to say about transparency.

10:53.200 --> 10:56.560
She thinks transparency can be really dangerous.

10:56.560 --> 11:02.280
It's not an end in itself, especially in the sense of disclosure in that, you know, you

11:02.280 --> 11:05.760
can be very transparent, and yet nobody can understand what's going on, if you, you

11:05.760 --> 11:08.920
could release the code or something like that, that's very transparent, but people

11:08.920 --> 11:09.920
is hopeless.

11:09.920 --> 11:10.920
Right, hopeless.

11:10.920 --> 11:15.560
So she's really pulled apart transparency and so she's making this appeal for intelligent

11:15.560 --> 11:19.600
openness, which means that any information you give, and this is a really good checklist,

11:19.600 --> 11:23.440
any information you give should be accessible, so people are going to be able to get to it.

11:23.440 --> 11:24.440
It's going to be intelligible.

11:24.440 --> 11:25.960
They're going to understand it.

11:25.960 --> 11:26.960
It's going to be usable.

11:26.960 --> 11:27.960
It's going to meet their needs.

11:27.960 --> 11:31.680
And it's going to be accessible, which means somebody needs to be able to check the working.

11:31.680 --> 11:35.760
Not everybody, but somebody out there, you know, needs to be able to check the working

11:35.760 --> 11:36.760
if necessary.

11:36.760 --> 11:41.360
Now, when you're using deep learning methods, you know, that's really quite a hard challenge

11:41.360 --> 11:42.360
to counter.

11:42.360 --> 11:46.000
I did mention what I thought was some very nice work being done by Google DeepMind,

11:46.000 --> 11:50.400
with in London, with more fields hospital and analysing eye scans, in which they've,

11:50.400 --> 11:55.560
you know, delivered trained this network, so to provide intermediate steps, segmentation

11:55.560 --> 12:00.400
map, so that it doesn't just go straight to a diagnosis, got a probabilistic diagnosis,

12:00.400 --> 12:04.000
but it's actually putting in the intermediate steps, which seems a really cool thing.

12:04.000 --> 12:07.600
Obviously, and that's because that project seems to be very strongly influenced by the

12:07.600 --> 12:08.600
clinicians themselves.

12:08.600 --> 12:09.600
You want that.

12:09.600 --> 12:13.800
That's the way in which they used to thinking about it, and they wanted to map their way

12:13.800 --> 12:14.800
of thinking.

12:14.800 --> 12:19.320
And because many people are claiming now that you don't necessarily have to make a trade

12:19.320 --> 12:25.840
off in performance in order to get a much more interpretable model, that it actually,

12:25.840 --> 12:29.320
you know, there's vast numbers of models, vast numbers of options, it gives very similar

12:29.320 --> 12:33.000
performance, especially, and that, especially, there's actually a lot of the differences

12:33.000 --> 12:36.520
in performance and largely illusory, that was what I talked about earlier.

12:36.520 --> 12:37.520
Okay.

12:37.520 --> 12:43.720
And so, you know, actually, you know, the struggle to, you know, among the great space of

12:43.720 --> 12:50.440
models you can use to choose one that actually enables a much more transparent, much better

12:50.440 --> 12:55.320
explanation, makes it more trustworthy, because people can see the reasoning.

12:55.320 --> 13:02.760
You ran off several of the qualities that's noranil outlined, but they're, they're all

13:02.760 --> 13:08.000
very subjective and, and seem to be in some ways that are with this statistical rigor

13:08.000 --> 13:09.000
that we're talking about.

13:09.000 --> 13:12.880
No, yeah, but no, no, exactly, but that's, I think I, I spent all my time with this

13:12.880 --> 13:13.880
psychologist.

13:13.880 --> 13:14.880
Yes.

13:14.880 --> 13:18.600
So, I've been very influenced by this, and I'm not even going to try to define exactly

13:18.600 --> 13:23.000
what explainable or interpretable or transparent means.

13:23.000 --> 13:29.760
But you can be quite rigorous about your evaluation of some of these aspects.

13:29.760 --> 13:34.440
For example, in the interfaces for the systems we build, we evaluate three things in which

13:34.440 --> 13:38.840
they have an impact on people, they're cognitive, do they understand it, the behavioural, what

13:38.840 --> 13:42.920
does it do to their, their behaviour and their intentions, and the affective, how does

13:42.920 --> 13:46.280
it affect their emotions, and we want to measure all of those, and they can be completely

13:46.280 --> 13:47.280
different.

13:47.280 --> 13:51.120
So, it's very important to get that feeling of what, you know, what do people get from

13:51.120 --> 13:52.120
it?

13:52.120 --> 13:57.680
And for example, through surveys, yeah, yeah, so we, the psychologists would do, you

13:57.680 --> 14:02.080
know, actual direct-to-face-to-face interviews on people, this is the phase two evaluations

14:02.080 --> 14:06.360
within a laboratory, we get patients in, get them to talk through a system, actually

14:06.360 --> 14:10.880
do some eye tracking as well, see how they're using it, using it, and then evaluating

14:10.880 --> 14:12.200
on these things.

14:12.200 --> 14:16.840
The metrics are quite difficult to do, you know, the satisfaction with which a decision

14:16.840 --> 14:20.840
has been made is quite a tricky thing to evaluate, but you need to try to be able

14:20.840 --> 14:21.840
to do it.

14:21.840 --> 14:26.240
You know, these rather loose things, it is worth the effort of trying to measure them

14:26.240 --> 14:27.760
as accurately as possible.

14:27.760 --> 14:32.560
I used as an advocate, an idea of a system we put a front end on could predict, which

14:32.560 --> 14:36.600
is for women merely diagnosed with breast cancer, who are trying to decide what other therapies

14:36.600 --> 14:42.040
to have apart from surgery, which is based on a fairly, you know, basic statistical analysis

14:42.040 --> 14:47.600
of the database of 4,000 cases, and it produces survival curves up to 15 years for women,

14:47.600 --> 14:49.480
and then looks at what would be the effect.

14:49.480 --> 14:54.920
These are personalized to various attributes of the tumor and the woman, and then, you

14:54.920 --> 15:00.840
know, it says how that survival would change if you take particular therapies, and those

15:00.840 --> 15:05.720
the effect of the therapies, all that data is based on clinical trial, causal data from

15:05.720 --> 15:08.120
randomized clinical trials.

15:08.120 --> 15:09.640
And that's fine.

15:09.640 --> 15:14.240
Our idea is that the system, which is currently used by doctors, will be used by doctors

15:14.240 --> 15:18.200
talking to patients, it already, and even by patients themselves and support groups,

15:18.200 --> 15:22.080
but we're using exactly the same system for all these different groups, and that means

15:22.080 --> 15:27.160
having very good explanation facilities, both for the terms, but also ways of portraying

15:27.160 --> 15:29.480
the risks to patients, and this is serious stuff.

15:29.480 --> 15:33.720
This is the chance people are going to be alive in 10 years' time, but with very careful

15:33.720 --> 15:40.640
use of wording and imagery and even the colour and accepting that we can do it.

15:40.640 --> 15:45.040
And the point about this is that for explanation like that, is that two things, one size does

15:45.040 --> 15:46.800
not fit all.

15:46.800 --> 15:50.560
There are people who've got different needs, they've got different levels of understanding

15:50.560 --> 15:52.680
about numbers and graphics.

15:52.680 --> 15:57.120
And so what you need is both multi-layered explanation, with a very simple level at the

15:57.120 --> 16:01.200
top, through to much deeper level, we put the maths in, if you want to, you can see a PDF

16:01.200 --> 16:04.280
with all the maths in, you can get the code if you really want it.

16:04.280 --> 16:08.520
So you've got all those layers of explanation vertically, but also horizontally.

16:08.520 --> 16:13.040
So when we're explaining 15-year survival, we can provide bar charts and survival curves

16:13.040 --> 16:18.480
and icon arrays and tables and text, etc., all of those options, depending on what people

16:18.480 --> 16:20.000
prefer to see.

16:20.000 --> 16:23.640
So you've got both vertical and horizontal explanation choices.

16:23.640 --> 16:28.040
There's no correct way to do it, but you can try to evaluate all of it.

16:28.040 --> 16:31.040
And there are only some people who want to see the stuff at the bottom, but they should

16:31.040 --> 16:35.080
be able to see it, because that's part of the accessible openness.

16:35.080 --> 16:40.360
That example is a compelling one, I find that, you know, often time we're dealing with

16:40.360 --> 16:48.320
physicians, there's a presumption of trust or trustworthiness that, you know, may work

16:48.320 --> 16:52.440
for a lot of people, but sometimes you want a little bit more data, and they're not

16:52.440 --> 16:53.440
always prepared to...

16:53.440 --> 16:54.440
Things are changing.

16:54.440 --> 16:56.000
People are making much, not everybody.

16:56.000 --> 16:57.000
You know, I don't know.

16:57.000 --> 16:58.880
I just completely have them off top of my head.

16:58.880 --> 17:03.280
I'd say half people who are quite prepared still go along with a very paternalist point

17:03.280 --> 17:04.280
of view.

17:04.280 --> 17:05.280
You know, thank you very much.

17:05.280 --> 17:06.280
Tell me what to do.

17:06.280 --> 17:07.280
Right.

17:07.280 --> 17:08.280
Just tell me what to do.

17:08.280 --> 17:09.280
I don't want to know anything else.

17:09.280 --> 17:14.240
You know, are asking questions and actually wanting to exercise some of their own rights.

17:14.240 --> 17:15.240
I don't know.

17:15.240 --> 17:19.320
I've got friends who have used the system that we've been working on in order to challenge

17:19.320 --> 17:21.320
their doctors and saying, okay, I'm not...

17:21.320 --> 17:22.320
It's only a tiny benefit.

17:22.320 --> 17:24.320
I know I'm going to get terrible side effects.

17:24.320 --> 17:25.320
I'm not going to have it.

17:25.320 --> 17:26.320
Yeah.

17:26.320 --> 17:29.080
And they've using that to challenge us and power them and power them.

17:29.080 --> 17:30.080
I think this is very valuable.

17:30.080 --> 17:36.640
Not only that, but in the UK now, there's a much stronger legal structure on what must

17:36.640 --> 17:41.440
be explained to people in order to get informed consent for treatment, and in others, all

17:41.440 --> 17:43.960
this should be explained to people.

17:43.960 --> 17:48.040
So we're providing it as actually some of the tools to allow doctors to carry out their

17:48.040 --> 17:49.040
work better.

17:49.040 --> 17:50.040
Okay.

17:50.040 --> 18:00.040
There's a thread in the AI community around taking ideas from adjacent fields like electrical

18:00.040 --> 18:05.920
engineering, the idea of data sheets or model cards, some folks have called them and

18:05.920 --> 18:11.640
basically different ways of documenting the characteristics or biases of different

18:11.640 --> 18:14.080
AI data sets or systems.

18:14.080 --> 18:20.720
And it sounds like a part of what you're doing is a similar idea, but applying ideas from

18:20.720 --> 18:26.720
clinical trials and the statistical methods associated with clinical trials and medical

18:26.720 --> 18:33.440
and pharmaceutical field to the way we talk about and communicate around AI systems and

18:33.440 --> 18:34.440
machine learning models.

18:34.440 --> 18:39.200
And yes, and not just the pharmaceutical area, but there's been involved for years in

18:39.200 --> 18:45.520
building prognostic systems for people and then both evaluating them and putting them

18:45.520 --> 18:46.520
into practice.

18:46.520 --> 18:50.320
And one of the crucial things about the evaluation that people would get really obsessed about

18:50.320 --> 18:55.480
in a sort of pedantic way that statisticians tend to operate in is that the probabilities

18:55.480 --> 18:56.720
must be meaningful.

18:56.720 --> 19:02.600
If you say 70% probability for something or 70 out of 100 chance, then it's got a meaning

19:02.600 --> 19:07.680
that 70 out of the number of times you say that, it should happen in 70% at the time.

19:07.680 --> 19:12.480
The undercalibrated probabilities, in other words, the uncertainty, the accuracy of the

19:12.480 --> 19:19.520
uncertainty, it is important as the accuracy of the main number, it is a very statistical

19:19.520 --> 19:25.000
idea and yet it's very important because otherwise you get these grossly over-confident things

19:25.000 --> 19:30.600
like, oh, I'm 99% sure that this is the diagnosis, that is grossly misleading, that really

19:30.600 --> 19:31.840
is terrible.

19:31.840 --> 19:37.680
So that's, I think, another very important way, thing that can be brought from statistics,

19:37.680 --> 19:42.400
which has worked a lot on how to evaluate the calibration of probabilities or test statistics

19:42.400 --> 19:48.240
to use and so on in order to check that element of trustworthiness of the claim.

19:48.240 --> 19:49.240
Right, right.

19:49.240 --> 19:53.800
This all calls to mind, at least in the US, I don't know if it's somewhere in the UK,

19:53.800 --> 19:58.360
when you're advertising pharmaceuticals, there's like you have your 30 minute ad and then

19:58.360 --> 20:02.480
you're long, that's red.

20:02.480 --> 20:03.480
Exactly.

20:03.480 --> 20:06.280
Well, you shouldn't have to do that.

20:06.280 --> 20:08.080
That's just some, you know, regulation.

20:08.080 --> 20:09.320
What you want should, we know.

20:09.320 --> 20:13.160
But at the same time, you know, that's kind of a summary of a data sheet.

20:13.160 --> 20:16.200
Yeah, that's not trustworthy communication.

20:16.200 --> 20:20.920
That's like having to sign, you know, we're getting some software and those terms and conditions,

20:20.920 --> 20:23.080
16 pages of terms and conditions.

20:23.080 --> 20:24.080
Right.

20:24.080 --> 20:26.160
Yes, that is not intelligent openness.

20:26.160 --> 20:33.880
No way is that accessible, usable, incomprehensible, accessible, is break breaks every rule.

20:33.880 --> 20:36.160
And it's terrible at sort of communication.

20:36.160 --> 20:37.160
Right.

20:37.160 --> 20:43.200
It's there to obey a law, but it is a complete sham in terms of good communication.

20:43.200 --> 20:44.200
I agree.

20:44.200 --> 20:45.200
I agree.

20:45.200 --> 20:51.160
At the same time, it is one step better than, you know, yes, or no, the computers, yes,

20:51.160 --> 20:52.480
yes, the computers exactly.

20:52.480 --> 20:53.480
Exactly.

20:53.480 --> 21:00.640
And of course, the worst systems of all are proprietary systems that are used in courts

21:00.640 --> 21:04.960
to decide about recidivism, risk, or bail, right, like that, are shocking because they're

21:04.960 --> 21:05.960
proprietary.

21:05.960 --> 21:06.960
They're totally done transparent.

21:06.960 --> 21:09.760
You've got no idea what information is being used in them.

21:09.760 --> 21:15.200
I mean, that's absolutely shocking again, that breaks every rule, you know, everything

21:15.200 --> 21:19.240
I'm trying to, I'm talking about is broken by that kind of system.

21:19.240 --> 21:21.560
So key takeaways from your talk.

21:21.560 --> 21:22.560
Oh.

21:22.560 --> 21:23.560
Yeah.

21:23.560 --> 21:27.280
Well, that's basically basic statistical ideas, you know, and their experience and other

21:27.280 --> 21:28.280
words.

21:28.280 --> 21:29.280
I've got a lot to offer.

21:29.280 --> 21:31.480
A lot to offer.

21:31.480 --> 21:35.280
But also, I can't, you know, I'm not just taking ideas from, you know, from statistics.

21:35.280 --> 21:40.680
I'm taking ideas from philosophy and psychology and empirical testing and things that really

21:40.680 --> 21:45.160
in this maturing disciplines, unbelievably important discipline, I think could take

21:45.160 --> 21:47.040
a lot more account off.

21:47.040 --> 21:48.040
Great.

21:48.040 --> 21:53.600
Very much in line with some of the key themes that I'm hearing at this year's Neurobs,

21:53.600 --> 21:55.160
you know, it's, in fact, two of them.

21:55.160 --> 22:01.000
One is the importance of, you know, fairness, transparency, et cetera.

22:01.000 --> 22:05.280
And the other is kind of the importance of interdisciplinary approaches and you're kind

22:05.280 --> 22:07.400
of bringing those right to you with that.

22:07.400 --> 22:10.880
And there's some wonderful work going on, you know, this morning I really featured the

22:10.880 --> 22:16.080
FATML, you know, social impact statements, you know, lists that they've got partly because

22:16.080 --> 22:19.200
they do not identify transparency as an objective.

22:19.200 --> 22:24.360
They've, they've learned themselves that transparency just means to an end, you know, it's,

22:24.360 --> 22:29.440
it's no good just being transparent, unless you obey a Neel's ideas of what transparency

22:29.440 --> 22:30.440
means.

22:30.440 --> 22:37.320
Well, we'll definitely provide a pointer to Nora Neel and your work as well, of course.

22:37.320 --> 22:38.320
Yeah.

22:38.320 --> 22:40.120
The talk's up on Facebook as well, it just looks to.

22:40.120 --> 22:41.120
Oh, fantastic.

22:41.120 --> 22:42.120
Fantastic.

22:42.120 --> 22:44.520
Well, David, thank you so much for taking the time.

22:44.520 --> 22:48.160
No, the real pleasure, real pleasure, thank you very much for asking me.

22:48.160 --> 22:53.760
All right, everyone, that's our show for today.

22:53.760 --> 22:59.840
For more information on David or any of the topics covered in this episode, visit twimmelai.com

22:59.840 --> 23:02.440
slash talk slash 212.

23:02.440 --> 23:09.320
You can also follow along with our NURP series at twimmelai.com slash NURP's 2018.

23:09.320 --> 23:16.320
As always, thanks so much for listening and catch you next time.

