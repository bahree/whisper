.
.
.
.
.
All right everyone this is Sam Charrington host of the Tumel AI podcast and today I'm coming to you live from the future frequency podcast studio at the AWS
Reinvent Conference here in Las Vegas and I am joined by Amad Mostak. Amad is founder and CEO of Stability AI. Amad welcome to the podcast.
Thanks so much Harmi. Super excited to talk to you. You are of course the founder and CEO of Stability. Stability is the company behind stable diffusion, which is a large multimodal model that has been getting a lot of a lot of fanfare I think.
And I'd love to jump in by having you share a little bit about your background.
Yeah, I think it's been super interesting. I think several diffusions kind of a specific text image model. I think it's that large, but we can talk about that a bit later, which is one of the fun parts.
As for me, I started off mass computer science at uni and enterprise developer and then we came a hedge fund manager and one of the largest video game investors in the world and then artificial intelligence.
I was doing that. There was a lot of fun and then my son was diagnosed with autism and they said there was no cure treatment. So I quit switched to advising hedge funds and built an AI team to do literature review all the autism literature and then buy molecular pathway analysis of neurotransmitters to repurpose drugs to help them out.
And it kind of worked. He went to mainstream school and super happy. It's awesome. It's kind of cool. Good trade. Good trade.
Then I went back to the hedge fund world once more wars as I was boring. So then decided to make the world a better place. So first off took the global ex price for learning. That was 15 million dollar prize from Elon Musk and Tony Robin.
So the first app to teach kids literacy and numeracy about internet. My co-founder and I have been deploying that around the world and now we're teaching kids and refugee camps literacy and numeracy in 13 months and one hour a day.
And about AI the craft out of that in 2021. I designed and led the United Nations one of the United Nations AI initiatives as COVID-19.
Collective and mental intelligence against COVID-19 launched a Stanford that by the WHO in ESCO and the World Bank.
And that was really interesting because we're trying to use the world's made the world's knowledge free on COVID-19. So there's a 500,000 paper data set freely available to everyone and use AI to organize it because it's really confusing.
During that lots and lots of interesting tech kind of came through but I realized these foundation models are super powerful. You can't have them control by any one company.
It's bad business and it's not the correct thing ethically. So I thought let's widen this and create open source foundation models for everyone because I think it can really advance humanity. And again, I think it'll be great to see these things collaborate.
So we can have an open discussion about it and also have the value created from just these brand new experiences.
That's awesome. And when did you get started down that that part of the journey?
About two years ago, stability has been going for about 13 months now. Yeah, when I think about the, you know, a lot of stable diffusion goes back to this latent diffusion paper, which was, you know, not even a year ago.
It's not only a year ago. I think the whole thing kind of kicked off with the clip released by open AI in January of last year.
So I said COVID during that time while doing my COVID thing. Okay.
My daughter came to me and said, Dad, you know, all that stuff you do, taking all that knowledge and squishing it down to make it useful for everyone. Can you do that with images like we can. So a bit of system for her based on VQ gang and clip.
So image generating model and then clip is an image to text model where she created like a vision board of everything she wanted a description what she wanted to make.
There is 16 different images and then she said how each one of those is different and change the latent. And then generate another 60 another 60 another 60 and then eight hours later.
She made an image that she went on to sell as an NFT for three and a half thousand dollars.
Wow. And donated the proceeds to India code relief. Okay.
I thought it was awesome. She's seven years old. Wow.
And then I was like, this is transformative technology.
The image is the one it's out language. You're already at 85% we're going to get a 95% image. We're at 10%. We're not visual species like the easiest way for us to communicate is what we're doing now.
We're having a nice chat, you know, then text is the next part of an image like the images or PowerPoints are impossible.
Let's make it easy. This tech can do that. So we started funding the entire sector, Google call up notebooks models, all these kind of things.
Late in diffusion was done by the conference lab at the University of Munich, who are led on the stable diffusion one as well.
Amazing lab led by Bj√∂rn Jomrod and led by Robin Rombach, who was one of our lead developers here at Stability.
And then there was work by Catherine Krausen, Rivershave Wings, a Twitter handle on clip condition models and things like that.
And the whole community just came together and built really cool stuff.
Then he had entities like mid-journey where we just gave grants for the beta that started operationalizing it.
And it's all come together now to the finality of stable diffusion that was released on August 3rd.
So that was led by the conference lab and then we ourselves a stability runway ML, a Luther AI community that we kind of helped run and lie on.
We came together to put out 100,000 gigabytes of image lab text pairs, two billion images turned into a two gigabyte file that runs natively on your Macbook.
That can create anything. It's kind of insane. I think it's yeah.
And the speed of which it all came together is mind modeling.
Yeah, like our model was to have a core team and then site contributors and partners from academia and then these communities that we kind of built and accelerated.
So like tens of thousands of people from open by ML were doing protein folding work to a Luther with language models to harmonize with audio.
And it turned out that's a really good system to just iterate and experiment with these things and exactly the right time.
And now it's progressed. So like when we started with stable diffusion and launched it in August, 5.8 seconds for a generation on an A 100.
As of yesterday, 0.86 seconds.
As of two weeks from now, it'll be 20 times faster with our new still models.
So you're getting to 24 frames a second high resolution image creation from basic blobs a year ago.
I don't think we've ever seen anything that fast. And the uptake has been crazy. So I believe on Monday, the number of GitHub stars was stable diffusion overtook Ethereum and Bitcoin.
So we're taking Kafka, everything else thing. I'll overtake kind of PyTorch and TensorFlow and like a month or two.
And that's since inception, like over the last month, I think Master Don has had 6000 GitHub stars over the last week.
Stable diffusion to has had 6000. Yeah, and stable diffusion to was just released this month.
Yeah, we were last month. It was a week ago. Yeah, last month. These times I lay stable. So stable diffusion one.
We kind of use the Lyon data set to create the image model and then we use open a eyes clip L 14 to kind of condition it.
So we combine the text model and the image model.
We're stable diffusion to we instead use something called open clip run by kind of the Lyon charity, whereby we had an open data set for both because open added amazing work open sourcing clip.
But we didn't know what data was inside it. So it learned all these concept. I'm like, how does it know that?
And so when we launched able to fusion kind of as a collaboration, we had all these questions about attribution about what's in the data set, say for work, not say for work.
But you can't control that if you don't control half the data set right off the learning. So still diffusion to have that, but I also had a better texting code a model.
So now basically it's heading towards photorealism. Yeah, you can get photorealistic outputs from it if you can process.
Yeah, and again, kind of insane. Like you just see these things generated in a second. You're like, it can be completely like artistic or completely photorealistic.
These people do not exist. This landscape or this interior does not exist. I don't think we've ever actually seen anything like this because the majority of him out he doesn't believe they can visually create.
Just like before the Gutenberg press, you couldn't write or read.
But now hundreds of thousands of developers think we've had like 380,000 developers sign up for a thing on hugging face. And now using this to create ridiculous things. And now that it gets to real time, what does that even look like when people can just seem to see communicate visually.
Like we could literally in a few months a year, definitely this podcast, you could generate a live video almost on it.
But all the topics that we're talking about, which is insane. One of the examples that you like to use is killing PowerPoint. So we've got the text that's where you usually start and you go through this long process to make it pretty or engaging aesthetic, right?
Yeah, because you know what these models do, like this attention based models, like it's interesting. So with my son with his autism, autism is kind of a social interaction disorder.
It's caused, in my opinion, largely by a gap of glutamate imbalance in the brain. So Gabba calms you down when you pop a value and glutamate excites you.
And obviously in our industry, a lot of people kind of have people, they know on the spectrum or they're just highly there because it lends itself sometimes where there's a dual wedge thing.
Because of all that stuff, what happens is that there's too much glutamate. It's like, you know when you're tapping your legs, there's too much going on in your brain.
Imagine that was like that all the time. You couldn't think straight.
Yeah. And so you can't form the connections of like a cup means cup your hands or a cup or a world cup in your brain.
That's why there's a lot of cases where they can't communicate properly.
Addressing those factors and calm it down, then you basically start teaching them just like when you have a stroke.
A cup means it's a cup means that a cup means that and they can start talking or you know progress.
With these attention-based models, you've moved from kind of giant extrapolation of data to paying attention to the most important parts between words and pixels, which is kind of crazy for the denoising process of diffusion.
The latent is that it built up there where it has all the concepts of a cup means that if you have a cup in a sentence, it understands what that is in that context, a world cup or cup in the hands.
And then can do these images, which is kind of insane. So it works like that part of the human brain.
I think that's what's so exciting. That's what lets you have the compression of knowledge.
Like so 100,000 gigabytes into two gigabytes is like we're pie-pype from that Silicon Valley HDO show, right?
I think it doesn't make sense. Yeah. You know, like a lot, but that's because...
A hundred thousand gigabytes. A hundred terabytes.
Was our input data and the output files two gigs. Yeah.
And it's not optimised yet. We reckon we can get that to 400 megabytes.
Oh wow. A 400 megabyte file that now works on an iPhone that can generate any image in seconds by description.
Yeah.
And you can go the other way as well. You can take an image and turn it into text.
And that text encoding is only a few lines that can generate a high resolution masterpiece.
It's insane.
That's nuts.
And I think we were kind of a bit misguided by not mismigod, but you know, the focus was on scale is all you need 540 billion parameter, trillion parameter, large language models.
Stable diffusers 890 million parameters.
How does that kind of work about large earlier?
It's not large. It's actually quite small.
And this is kind of pointing something to the future because like, you know, OpenAI took GPT-375 billion parameters.
And they instructed it. So reinforce with only human feedback by getting annotators to use it.
And then seeing which neurons kind of lit up these kind of latent space things.
Instruct GPT had equivalent performance. I think they probably use a larger version of that at 1.3 billion parameters.
Because kind of you don't need all the information of the world completely to do stuff.
You just need some of it.
Image models though are surprisingly small.
Like the largest we've seen was the 12 billion parameters. Are you Dali model?
But now, like I said, we're 900 million parameters and we've had great success with our 400 million parameter models.
Our 4 billion parameter models are better. Actually, the largest is party, which was from Google at 220 billion.
We don't know what an optimal data set is, what optimal parameter size is for these particular non-text models.
Text models themselves. Text is quite a dancing coding. I think we'll tend larger.
But combining these models is going to be super interesting as we move forward.
It's a lot of the efforts as far have been on shrinking the model to make the performance better, to make it smaller, faster.
Do you see a pull towards larger models?
Or do you think it's a different paradigm altogether where there's not going to be that kind of drive to make the model bigger and bigger?
I think there'll be a mixture of things. Again, like what we saw with the deep-mind chiller paper was that the scaling laws weren't necessarily appropriate.
So that showed that a 67 billion parameter model trained on 5 epochs would outperform 175 billion parameter model.
But actually what it really showed if you dig into the details is that data is what you need.
And what does that data look like? We haven't done the proper data augmentation in other studies.
But this is also like, you can think of these models like, stable diffusion one was a pre-crecious kindergarten.
And we talked about the whole internet, so it occasionally turned a little bit off in some of the outputs.
Yeah, stable diffusion too, you're getting to like grade school now.
Let's still see what we're pre-crecious. And you know, we made it safe for work and then it's safer for work and a whole bunch of other things.
Did you do the data sets? We're still not feeding it the right information.
Once we know what the information to feed it will make it even better.
And I don't think that translates to large. I think it turns to more efficient.
And I think one of these things is the accessibility.
Because we optimize stable diffusion kind of as a group and collective to be available on low energy devices, not just like 3090s or A100s.
You can download it on your MacBook right now.
And MacBook M2, as of today, can generate an image in 18 seconds of any type.
In a couple of weeks, it'll be less than a second.
So you can have PyTorch, you can have jacks or whatever.
And you can just stop coding.
And so that opens it up to so many people. It's a new type of programming primitive.
You know, this hashed file that can create anything.
That went into the connection between programming and stable diffusion.
So if you think about it, you're creating an experience for your programming, right?
And so if you use the diffuses library from hugging phase, it's like a couple of lines you can be using stable diffusion in a code base.
And again, it can run your MacBook with no internet.
So what type of experiences can you do when you have this verifiable file, words go in, images come out.
It opens up a whole world of its possibilities.
It's like an ultra library in a way.
Like the library condense in AI model.
And we're not really used to that.
Like, you know, we've had birds and kind of some of these other things.
But nothing that has this massive range, shall we say.
Like two billion images, a snapshot of the internet can press down.
Yeah.
You're kind of thinking more broadly like we.
A lot of the conversation about stable diffusion today is about art.
And kind of, you know, the creation part of that process.
I'm thinking more broadly about practical applications.
And this is maybe getting into something I wanted to speak about later, just where you see the company going.
You know, talk about some of the other things that are disrupted beyond just, you know, making pretty pictures, arts and crafts, right?
Yeah, man.
I think art is like, we think about it as like, if my artist never make money, right?
Unless they do, you know, like my seven-year-old daughter.
She's obviously, you know, one of the OGs now in general is about.
I actually asked her, why don't you make any more art anymore? And she's like, well, dad, there's this thing called supply and demand.
If I reduce the supply and you can make this whole industry, the bond from my stuff will go up so the value can't be like, you're paying for your own university.
Creative industry is worth hundreds of billions of dollars a year.
Video games, 170 billion, like movies or 80 billion.
This will all be disrupted by this technology.
If you think about the creation process, like one of our directors, he was doing a shoot with a famous actress.
And with a rating, it's going to be $113,000.
It gets flyer out and do all this and get all these other people just for three days.
He fine-tuned a stable diffusion model.
Did it in three hours, 2,000 shots.
Go to realistic.
Meaning being entire shoot was generated as opposed to?
Yeah, like all the shots.
Because there's going to be a shoot to kind of put her in different things to go into the movie kind of process.
So concept artists are using this to become more efficient.
There's a group corridor digital.
They created a Spider-Man everyone's home, which is like a two and a half minute trailer,
in the spider-verse style by having a spider-verse model.
But they train on like 100 images.
And you can't tell it's like, wow, this is an amazing animation.
No, it isn't.
They just interpolated every single frame and used stable diffusion to kind of do image damage.
It's the craziest thing.
It would cost millions of dollars before.
It did it like a few days.
I think media is going to be the first to be disrupted here, because that creation process is hard.
And now it's easy.
I would think industrial design, for example, wouldn't be too far back behind.
Like auto desk.
You know, they spend a lot of time thinking about ways to use machine learning help designers.
Yeah, they've got amazing kind of data sets.
You know, you've got the cameras of the world that have every single click on design.
It can make all of those easier, because the system learns.
There's a foundational model in some ways, because also like a base foundation.
You can then train on your own things.
And it learns physics and all sorts of other stuff, which is a bit creepy.
But it can learn about that specific type of design that you might want to do.
We work with car manufacturers right now, who want to have custom models based on their entire back catalog.
And they want to iterate and combine different concepts.
And then they automatically stick together these cars and combines them.
You know, we also didn't just release the model.
We also released an in-painting model.
So you can delete parts of a picture and have seamless edits based on your text conditioning on that.
You've got an image to image model.
We can define it into any style.
We have a four soon to be eight times up scaler.
That's like enhanced and enhanced on a TV show, you know.
And all of these are going sub-second now in terms of the speed of iteration on them.
So I think creative is the first.
But then I said some of this design kind of things.
Then it goes into more visual communication, like I said, slides.
If you go to image model, combine with language model, combine with code model, you never need to make a presentation again.
And understand what aesthetics are.
Like one of the things we do with stable diffusion thing is that we create a discord bot where everyone rated the outputs of stable diffusion 1.1.
And then we use that to filter down our giant 2 billion image data set into the most aesthetically pleasing things using clip conditioning on that.
And then we trained on that and it became more aesthetic and pleasing.
Bit weird in some ways, but again, these feedback leaves become very, very interesting.
Because to get the wide range of viability on these image models, language models, audio models, others.
The human and the loop factor is essential.
Because your typical training data is quite diverse.
But you want to customize it to the needs and wants of the humans or the sector or the specificness of that.
There are other models out there.
You mentioned the journey a few times, you mentioned Dali, we've talked about performance as a kind of a target differentiator.
What are some of the other ways that you see stable diffusion kind of defining itself relative to the other things that are popping up?
Open source will always lack closed source.
Because they can always just take open source and upgrade it, especially foundation models, right?
I think data is kind of a key thing.
There's been a recurring theme that's come up in our conversation a lot.
This is the idea of the human and the loop and data, refining the data versus evolving the model, the whole data-centric AI idea.
Yeah, so it's kind of a data-centric thing where if you look at people adapt to these models right now, they're doing few shot learning.
Or they're doing basic fine tuning.
Because there's no point in training your own model because it's freaking moving so fast.
We'll have stable diffusion version three in a few months. We had a 20 time speed up yesterday on the model.
It's insane, these kind of moves.
I don't think we've ever seen anything quite this exponential.
But what happens then is that if you go via an API, there's only so much you can do.
That's what all of these companies do.
Or if you go via an interface like the mid-genius and like that or a Dali.
If you've got the model yourself and you can play, you can experiment, you can adapt it.
So the language models from the Elite, the community, GPT Neo, JNX.
They're GPT level models, but only up to 20 billion parameters.
They've been downloaded 20 million times by developers.
And they need to tell anyone, they just get on with things.
And so one of the interesting things for me is that the positioning is the tooling around this.
Because once you've got those primitives, you can build stuff around just like you've seen loads of community web UIs and other interfaces to interact with stable diffusion.
And for our own company, it's a very simple thing.
This is like a database on steroids.
You're thinking about it.
Like it's a database that comes pre-filled with interesting stuff.
And that's how much people are using it right now.
But soon when we upgrade it a few bits and it comes mature.
It's a database, it's a database, it's a kind of a magic bot.
Bot's database of images and your query is your prompt.
Exactly. It's a data store.
Except for a super efficient data store.
100,000 gigs to two.
And it can do all sorts of wonderful things.
So right now everyone's using like the pre-baked version.
Like the Laura Mipson version, right?
But then in a few years, everyone wants their own custom ones.
So obviously, it's very simple.
Take the exabytes of data from content companies.
Convert them into these models and make them useful.
Because we think content is turning intelligent.
And it goes beyond media companies to bio, farmer, and others.
And we're probably the only foundation model company building cutting edge AI.
That's willing to work with people and go to the data.
So models to the data, I think, are a very interesting thing.
Based on open frameworks, so you don't have to lock in
of some of these other ecosystems.
You'll be like, I'll trade a model for you,
but you have to be locked into my thing.
Yeah. Yeah.
One of the things that you mentioned in passing
is that you've seen the model learn physics.
What does that mean?
So like, if you type in a lady looking across a still lake,
it will do her reflection in the water.
You know, raindrops, it gets correct and things like that.
And as you train it more, it learns more and more concepts
of how things interact, which again is a bit insane.
Like, you can show it the sides.
You can train it on like a experimental call like a cyber truck.
You can see how much effort has gone into
the visualization community trying to get that stuff right.
Exactly. So like, you can show it parts of like a cyber truck.
Yeah.
And it doesn't know a cyber truck, say for instance.
What the back of the cyber truck looks like and it will guess
and it'll probably get it right.
It knows the essence of truckness.
Yeah. So rather than having these very specific models
that learn stuff, you can now have something
that can do just about anything in terms of lighting
and, you know, they've got prompt to prompt
where you can say, make this picture sadder
or, you know, turn them out into a clown or a stormtrooper
and automatically does that.
Because it understands the nature of these things
and the physics and balancing of that,
which again is kind of insane.
This has been implications for the rendering industry
and other things because this is a far more efficient renderer
that can do image-to-image
and transform something into something else.
Nobody's quite sure how it works.
And I've got theories.
And this is one of these things
with these foundation models like
they're just an alien type of experience
when you first really start pushing it.
Most people are surface level when you start pushing through it
you're like, it's really curious that you can do this.
It doesn't have agency.
It's too key what file.
But the fact that you can have that compression
of those concepts
is really interesting.
Would that always be a fundamental limiter
meaning, you know, if you want a quick and dirty approximation,
use something like stable diffusion.
But if you want a precise rendering, you know,
you have to turn to traditional techniques.
I think it's going to be, I always say it's part of a process.
Yeah.
Architecture, you shouldn't try to do zero sure everything.
That's what people tend to pull into a travel.
Like, yeah, I just wanted to know, like, have kind of
K&Ns or knowledge graphs
or retrieval augmented systems or kind of whatever.
Put it as part of a process pipeline.
But definitely a quick and dirty,
it does very, very, very well.
Better than anything.
But then I think that also this way we have our
impainting and all these other models.
It's going to be part of multiple models
doing multiple things for multiple purposes.
Sometimes there might be a giant model
once you get to a certain stage.
Other times, you might just want to have a quick and dirty
256x256 iteration loop.
And so what we've seen as well.
Like, we're stable diffusion too.
We actually flattened the late in space through
DDP and all bunch of other things.
So it's more difficult to prompt.
Stilt diffusion one was quite easy to prompt.
Stilt diffusion two is more difficult,
but it's got much more fine grain control.
But where we're going, we're not going to use prompts.
Because it will learn from...
What do you see taking the place of prompts?
Well, I think it will just be a case of like,
you will have your own embedding store.
That points to points in the late in space
and then pulls up like the things that you like most commonly.
So it learns and then kind of there's that interaction
two things. So, you know, embedding is being
a multiplier representation of kind of what's in there.
So I think that people's own context is important.
And AI models have, we understand people's
AI person context in that.
Or companies or other things.
And again, this is fine tuning effect where you can
with a two gigabyte file.
Actually have your own model.
And then why do you need to prompt training on
art station 3D, obtain render and all these things.
When it learns that that's what you want to have.
That's this type of style that you like.
Right.
Having said that.
I think prompting is just very difficult.
Likewise, we've been trying to prompt me for 16 years
if she has a quite managed.
You've touched on a couple of things.
Open source versus API.
And very briefly.
This idea of kind of customization.
And I think, you know, based on stuff that I've heard you talk
about in the past, like.
You're very strongly opinionated around the.
The model that through which you're kind of delivering the technology.
Beyond just the technology itself.
Can you talk a little bit about.
Your thoughts there and kind of what's driving.
You were driving that.
So I think.
This is incredibly powerful technology.
I think it's one of the big epoch changes in humanity.
Because you have a model that can do anything and approximate.
There's two types of thing in type one and type two.
Logical thinking and then principle based thinking.
This kind of get to principle based thinking.
We still don't have AI that does good fashion reasoning with logic.
This can take leaps.
That's what we said.
Like quick and dirty approximation.
You can do that.
Your type of end you get like a hundred different images of.
Like a book or a vase or something like that.
You can then iterate and improve.
Just very different experience.
So I think was like, again, put this out as foundation models.
Like again, benchmark models that people can then develop around.
Because the pace of invasional outpace anything that's closed.
But also addresses things like the digital divide.
And my route is nothing.
So like with open AI and Dali too.
They introduced the anti bias filter, which automatically for non-gendered word added a gender and a race.
So we type in suma wrestler or do Indian female suma wrestler.
Yeah.
Which I suppose could exist.
Probably not many of them.
That's probably not an intent.
It's kind of limited.
Whereas with our model, what happens we release it.
And then a team manager pan created a Japanese texting code.
That's our alternative.
So salary man rather than meaning man with lots of salary, men and race have man.
In a county's local context, these local elements, these local fine tunes.
I think we're essential.
And also widening the discussion.
Because a lot of the stuff that occurs with these big powerful models is that we won't release them.
Because we're scared about what's going to happen.
Because the no, no, no, no, no, no.
So that's fair.
That's an opinion.
That shouldn't mean that it shouldn't be available to other people because of the power of this technology.
Because otherwise, they'll just go to corporate first and it won't be available.
That's why the fact it could uplift them creatively and communicatively and other things.
One way to think about it, if I'm really mean in some discussion sometimes, is like,
why don't you want Indians or Africans to have this technology?
Because there's no comeback.
You can't say that more education is needed or it's too dangerous and they're not responsible.
Because the reality is this is technology of the humanity.
And there's an echo of what happened with cryptography.
We can't let cryptography be open and the government pacified it as a weapon here in the US.
Because bad guys might use it.
But we use it now to protect ourselves as well.
Open source will always be more secure than close source if the community rounds together.
Because what do we run our infrastructure on here at AWS?
It's not really Windows.
It's Linux.
Our databases are mySQL and things like that.
Because the community can come together and build stronger systems and more effective systems.
But it's crazy how far this is going.
And so it's difficult lines to throw a toe down.
Yeah.
You mentioned that more recent versions of sable diffusion include, like, safe work filters, that kind of thing.
So it sounds like something that you're thinking about and care about and not just putting out without any kind of controls.
Yeah.
So the original version, look, again, it was led by the Confis Lab.
And we said very specifically, you guys get to decide and we will advise.
Because it wasn't academic endeavor, you know, even if the people like one of them works for us and then works for runway and et cetera.
Is the nature of the thing.
And so we're very respectful of kind of entities that we collaborate with because it can be a minefield, right?
You know, trying to whitewash anything.
So it's released under a creative ML open rail license, which is a new type of license from Hungry Face that said you have to use ethically.
Add a safety filter because the decision was made by the developers not to filter the data.
So it could be a baseline from which we could then figure out biases and other things.
And that removed a lot of nudity and kind of other things, especially because it was accidentally creating it.
Sable diffusion too was trained on a highly safe for work data set.
So it's massively more safe.
It doesn't have a filter because of the need one.
It has some drawbacks such as one of the things that we saw during the fine tuning after stele diffusion one is that people trained on not safer work images.
Internet is for that whatever they fine tune it.
They took lots of images that were not safe for work.
Right.
So obviously there was a standard effect of that because again, they're free to kind of use it as a community.
But the side effect is that when you actually used it for safe for work prompts, it did amazing humans.
Like photorealistic because it learned about anatomy from these not safer work images.
It was quite funny.
So stele diffusion two out the box is a bit less good at anatomy because we removed a lot of those things.
Not much.
And again, we're adding it back in safely.
We really care about that.
The other thing that we care about a lot is, you know, we read this community as big.
We're creating millions, hundreds of millions of artists.
So artists are part of the community.
They were asking, can we opt out of the data sets?
Some are actually asking, can we opt in because we're not in the data set.
And so we worked with spawning and lying on others on opt in an opt out mechanisms.
Because I think that's the right thing to do.
Like, I think that it's ethical to use web scrapes to create models like this.
Especially because the diffusion process doesn't create copies or photo mashes.
It actually learns principles.
It's like a human.
For the same time, people don't want to have their data in the data set.
They should opt out.
If they want to end, they should opt in.
In fact, we've had thousands of artists sign up for the system.
It's been 50-50. Opt in an opt out.
I think it's really interesting and not maybe what some people would expect.
Yeah, interesting, interesting.
Maybe shifting gives a little bit to stability as a company as an organization.
I've heard it described as, you know, very, you know, variously an art studio.
Kind of looks and feels a little bit like a research lab.
Feels a little bit like a funder of things, a provider of GPUs and instances.
How do you describe what it is?
I mean, stability AI is a platform company.
So we're trying to build a layer one for foundation model AI.
And we think the future will be open source on this.
So our research lab is, you know, researchers who have loads of freedom.
And they can, in their contracts, open source and they create.
And there's a revenue share for when we run the models on the API.
Even if the researchers don't work at stability, they still get cut checks.
Which thing is very interesting when we're doing things.
Yeah.
We've got a product team that takes the open source stuff, just like anyone can,
and productizes it into things like Dream Studio.
We have Dream Studio Pro coming up, which is a full enterprise level piece of software
with like 3D keyframing, animation, video, audio, everything.
We're going forward to deploy a team whereby for our top customers,
with the most content that we've transformed in foundation models,
we're basically embedding teams inside there and saying,
you don't need to build a foundation model team.
We're your team because we do all the modalities from the text to language to audio.
I can add some of this super appealing to people that we've got infrastructure team
that is supporting our five, six thousand to eight one hundred's
and the infrastructure to scale API's to billions in support with Amazon and others as I am.
Can you talk a little bit about some of the ways that you engage with enterprises?
Like, what are the kinds of things that they want to help doing with these models?
So, the pace of ML research is literally exponential,
with a 23 month wobbling.
It looked crazy.
They can't keep on top of this.
And there's very few people that publish papers on market.
Yeah.
Okay.
It's always nice when you see a casual exponential.
In AI to help with that.
Yeah.
Yeah.
But like, when you look at this,
they're realizing they need to be on top of this technology now.
And they come to us as kind of almost consultants.
They take a parentier type model.
Yeah.
Where we're like, we'll fine tune some models for you
and we'll make them usable through Dream Studio.
But you shouldn't train your own models now.
Because the models aren't going to mature for another year.
When that time comes, we will train the models for you.
We will fine tune them for you.
We will create custom models for you.
That's our highest touch engagement with a couple of dozen entities.
And when you're telling them, they shouldn't train models.
Are you talking about from scratch from scratch?
Sure.
They should.
Okay.
They will be able to eventually.
But right now, it's not a sensible thing to train a model from scratch.
Yeah.
I stable the fusion to 200,000 a 100 hours to like 600k you spent on it?
Yeah, 600k.
Well, we actually spent less because of our discounts.
But I can't say what I discount.
You know what I mean?
You can figure out the retail.
Yeah, retail.
Retail-style diffusion to about 800,000 hours.
Retail-open clip.
Because we had to make the clip model about $5 million.
So, you know, these things add up.
Yeah.
Quite a large bill.
So, I think that when you kind of look at all of these,
now's not the right time to do big trainings for big companies.
Because again, the model architecture is just increasing on ridiculous rate.
But there's going to level off.
You can't keep improving forever.
And then that's the right time to train up your own models.
So, it'll be better than these fine-dune models.
But then you have models with multimodalities.
You know, this is part of the dear reason we've kind of partnered with SageMaker.
Because people need to get used to this technology now.
And they'll have all these different primitives,
these different models they can mix and match to create brand new things going forward.
And SageMaker makes it kind of easy to do that.
And it makes it easy to address the tail.
Because, apart from the top couple of dozen companies,
we just want to have a SaaS solution for everyone else to be able to access,
use and modify these models.
And following up a little bit on the SageMaker and the AWS announcement,
kind of red as, you know, you selected AWS.
From my understanding, you've been using AWS for some extent all along.
Yeah, so AWS built the core cluster.
And now, you know, we reached this point.
So, it was originally a 4,100 cluster,
which on the public top 500 listed,
about number 10, super computer in the world, which is kind of insane.
So, AWS did a great job building that.
But then we have to decide what's next,
like the managing the resilience through some of these other things.
We build our next cluster.
Amazon came and they said,
let's use the SageMaker service to offer a high level of resilience optimization.
So the SageMaker crew, for example,
took our language model, GP Neo X,
again, 20 million downloads of this family.
Yeah.
They went and took the efficiency of a 512-A 100 trading
from 103 telephlops with GPU to 163,
by optimizing it for Amazon, EFA, Fabric,
and pipeline powerism, and cost attention, and kind of all these things.
And that was amazing things.
So they're helping us optimize our entire stack,
from inference to training, through to having resilience.
So when GPUs fail, they come back up.
And the final part of it was just how do you make this accessible?
Through SageMaker and the services,
the ecosystem they built around that.
Now, we're going to make our models available on everything.
Right.
Right? So, like today, they became available on the MacBook M1,
with native neural engine support, one of the first models ever to have that.
You know, that's massive.
We've got it working on Qualcomm.
We're going to work on iPhones, all these things.
But Amazon is a really great partner,
because they're infrastructure players,
one of the biggest cloud drivers in the world.
And so that's why we kind of picked them as our preferred partner.
Also, you know, we're super grateful in that.
We wouldn't be here if they hadn't philosophically an enormous cluster,
and really believed in us.
Because we're only a 13-month-old startup.
Yeah.
So everything's been in the cloud the entire time?
All the entire time.
Yeah.
So we had a machine learning ops team of four people,
managing 4,000 A-100s.
Wow.
Now we're up to nearly 6,000.
Was that team managing that cluster, you know,
kind of bare metal with your own tooling,
or how much of the Amazon tooling have you?
So it was easy, too.
And then the Amazon had a system called parallel cluster,
with Slurm, that was used to kind of manage it.
And so we've been working for the last four, five, six months,
just constantly improving it together.
And again, it's open source.
Yeah.
If you go to the stability AI GitHub,
you can literally download all our configurations
to run your own parallel cluster on that.
Okay.
And again, this is part of what we really like.
The fact that the stack is open source.
And anyone can take it and they can build their own clusters.
Maybe not quite to the size that we did,
unless you're feeling really punchy.
Yeah.
But still, I think these knowledge and these things should be shared.
Because you find that large model training isn't really an art.
Is it really science?
It's more of an art.
Like one of the most interesting reads you can do
is the Facebook OPT175 logbook for the 175 billion parameter model.
They just try stuff and it often fails.
And there's the occasional weird thing,
like really was the Azure kind of customer support
on the 23rd of December.
It deleted the entire cluster.
And you're like, man, I feel for you guys.
It's kind of that.
But like I said, this is not just an easy click and play kind of thing.
These models are difficult to train.
Yeah.
The smallest hardware thing can throw it out.
They can be just weird stuff.
We're making it up and figuring out as we go along.
Because remember, transformer architectures
are literally only five years old.
Yeah.
It's thinking about open source and that direction broadly
that the company is taking.
One of the challenges that comes up in open source
as it matures is this idea of governance.
How do you think about, maybe it's early,
talking about governing a community that's just months old.
But do you have thoughts on how the community,
you know, governs itself over time?
Yeah.
So again, it's complicated one, right?
AI governance.
And does it policy led?
Is it community led?
Who are the voices at the table?
Because there's some important things.
This is such powerful technology.
It's going to be essential, I believe, to the future of humanity.
So like, for example, Luther AI is two and a bit years old.
That's our language model community.
15,000 people and 12 of us.
We're kind of incubating at the moment.
We're going to spin it out into its own separate 5 and 1 C3.
Because it shouldn't be else influencing the direction
of open source large language models, right?
It should be a collective effort.
But now we're really going through the governance thing
and looking at different examples.
And the Linux Foundation is an excellent example of that.
So PyTorch has just been given to the Linux Foundation.
And so Rin talks with them, a whole bunch of others to say,
what are best practices here?
And what should really look like given the power
of these, some of the decisions need to make about that?
As stability itself, we're setting up a subsidiaries in every country
such that, first off, a temp set of equity in those
goes to the kids using our tablets.
Because I think they should influence it.
Because that's the next generation.
And this AI will be important to them.
But then we want this to be independent entities that run the AI
for India or Vietnam or kind of Malawi, et cetera.
Because we need to train up a next generation of people
to make those decisions for their own country.
Because right now, what we have is a situation
where you've got a few people in San Francisco
making decisions in the most powerful infrastructure
of the world for everyone.
Because that's not to deny ourselves.
This AI is infrastructure.
It's essential for where we're going to go.
And it shouldn't be controlled by any person or entity.
Like, I'm very supportive of the whole ecosystem.
The one time I, by almost the very direct.
So I spoke out against open AI.
Because for Dali too, they banned Ukrainians from using it.
They removed any Ukrainian entities from that as well.
And this is doing the time when they're being oppressed.
I said, basically, you have excluded and removed and deleted
and oppressed people.
And that is ethically and morally wrong.
But is their prerogative as a private company?
And if it wasn't for us, there would be no alternative.
And so I literally took Ukrainian developers,
and houses were destroyed, and brought them to the UK.
And so this is part of the thing as well.
If you have control of this artificial intelligence
given to an unregulated entity like these big companies,
they can't help themselves but behave in certain ways
and they can't release it.
More than that, they tend to optimize.
So I did a lot of counter-extremism work,
advising multiple governments.
The YouTube algorithm got hijacked by extremists
because the most engaging content was extreme.
Again, that's not YouTube's fault.
That's full of great people.
Add driven AI companies.
They will use this technology to create the most amazing
manipulative ads.
Against this not their fault, it's kind of what they are.
So regulation needs to come in appropriately.
Governance needs to come in appropriately.
But we need to educate and widen the discussion on this.
And the only way to do that is open source.
Otherwise, it will never happen.
And so you will have AI basically being a colonial tool
in some ways with very Western norms.
When this is essential infrastructure,
like I said, I believe for everyone.
And I think the common retort to that
is it needs to be controlled because it's so powerful,
so dangerous.
Yeah, so who are you to control it?
I mean, this is a thing.
Like, I've had it, it likes into a nuclear weapon.
I'm like, it's a nuclear weapon that can allow humans
to create visually.
And so you're restricting it.
And when you get, come and answer a question.
Like, I've asked this, I've never had a question.
Why don't you want Indians to have this for Africans?
And the only answer is, because they need more education,
so educate them more.
Because they can't use it responsibly,
and you can, it's racist.
Like I think fundamentally, if you think about digital divide,
we've seen this with technology being restricted
from minority groups and from the rest of the world frequently.
It's fundamentally racist because we think we know better
in the West.
When it's reality, we don't, because people can take this
and extend it.
And people are generally good.
People are not bad, and if people are bad,
as a society, we build systems to regulate that.
So even if they create deep fakes,
we build our social networks and those type of curation mechanisms.
You know, we build authenticity schemes like content
or authenticity.org that we back.
That sounds like the core of your answer
is that the ecosystem will solve the problem.
The bad actors come in.
They use these tools to cause whatever
having their cause, and then, you know, we'll find faces.
The bad actors have the tools already.
They have tens of thousands of A-100s and people.
I mean, since you're the proof point of this, right?
Open AI was keeping Dolly closed behind, you know,
APIs and wait lists and things like that.
And, you know, you came up, I don't know where,
and released something.
And look, 4chan has had this technology for three months.
What if they created nothing, right?
Yeah.
You know, like this isn't going to topple humanity,
and have more and more people know about it,
so we can bring this discussion.
You know, we took a lot of flak.
We had a lot of benefits.
Yeah.
We brought this discussion into the open,
into policy and other fields as well.
Like, again, it's my hope that now,
we access a forcing function.
So I reckon Dolly III will be open sourced.
You know, it's like the open source whisper.
And I think this will be fantastic.
Let's bring it out into the open,
because, again, this is foundational infrastructure
for extending our abilities.
It should not be closed.
Yeah.
But I don't believe that it should be free forever,
and, like, even, actually, it's not open source,
because it doesn't confirm with rule zero of open source
in a pure open source way.
The creative analysis is not,
because we say you must use it ethically.
You know, do we have to move it to open source?
Yes.
Under CC by a five, or MIT license,
just like our other models,
like our Korean language model,
the polygote one from the Luther
or open-clapal things like that.
Yeah.
But, again, this needs to be an open discussion, I think,
rather than who is deciding it.
I don't know.
Right.
If regulators want to come and regulate it,
again, that's a democratic decision.
And so I'm a big supporter of democracy
and, kind of, these things.
But let's use our institutions on processes
rather than trying to make these decisions ourselves
in closed rooms.
Mm-hmm.
Awesome.
Awesome.
Well, Emma, thanks so much for taking the time to chat.
It's been wonderful.
Speaking with you, I'm learning a bit more about what you have to.
It's a pleasure, and I hope you have a seat as well.
Nearly done.
Nearly done.
Thanks so much.
Take care.
