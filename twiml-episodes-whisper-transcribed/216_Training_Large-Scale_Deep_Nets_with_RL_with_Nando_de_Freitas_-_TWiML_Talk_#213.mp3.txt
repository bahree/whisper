Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. Today we close out our NURP series and our 2018 conference
coverage with this interview with Nando DeFratus, team lead and principal scientist at Deep
Mind and Fellow at the Canadian Institute for Advanced Research. In our conversation,
we explore Nando's interest in understanding the brain and working towards artificial
general intelligence through techniques like meta learning, fuchsot learning and imitation
learning. In particular, we dig into a couple of his team's NURP's papers, playing hard
exploration games by watching YouTube and one shot high fidelity imitation, training
large-scale deep nets with reinforcement learning. Enjoy.
Alright everyone, I am here with Nando DeFratus. Nando is a team lead and principal scientist
at Deep Mind as well as a Fellow of the Canadian Institute for Advanced Research or CIFAR
as you might know it. Nando, welcome to this week in machine learning and AI.
Thank you. Thank you very much for having me here. It's such a pleasure.
Absolutely. Absolutely. It's great to finally get a chance to speak with you. We've been
trying to coordinate something for a while now, but fortunately NURP brings us all together.
I'd love to hear a little bit about your background and your path into ML&AI.
Yes, so I got into ML&AI a very long time ago. We're going back to prehistoric times here.
So actually it was around, it was in 1994, when I was an underground at the University of
it, Father's Rund in Johannesburg, South Africa. And Professor, my control professor, introduced
me to this thing called Neural Networks. I was fascinated by neural networks and tried
to learn as much about them as possible, 100 maps by propagation, and ended up implementing
something that eventually went into hardware and it became a controller for pneumatic
control valves. With that work, I eventually even made it to an international conference.
That was, I still remember 26-hour trip from my home to my first conference in Washington.
It was extremely intimidating because I was the skit from Africa. And there I was meeting
all these big shots and people that I thought I would never be able to ever hang out with.
They were like heroes to me. Some folks like David Hackerman, Underbarco and a few others,
like very prominent people in the field. It was a great experience. It was intimidating.
And when I went back to South Africa, that made me wanted to do a masters. And so I continued
my work with neural networks. And eventually through that, I was able to apply to universities
and I got lucky, got a scholarship to Cambridge, and I went there, and I continued working
on based neural networks. And that was my PhD. And it became very statistical, ended up going
to Berkeley. And by just fluke, my supervisor there, still at Russell, was in computer science.
So I somehow became a computer scientist. And eventually after two years there, I moved
to UPC where I was a professor of machine learning, introducing some of the undergrad courses
that you probably have seen on YouTube. And it was there that I first started participating
in the SIFER program. And this was organized chiefly by Professor Jeff Hinton. And the
goal back then was to understand the brain. And this was so amazing because it was suicide
in those days to put a grant with that title. You would not get tenure. This was like a crazy
thing to do. But fortunately, some of the key leaders in Canada believed in that ambitious
vision. They're still there and they continue inspiring us. And that's what we did. We embraced
that vision. Our research sort of steered toward that at the time, more different, more
difficult path. And now it seems it's very easy and retrospect. What we do. Yeah, and
that's kind of how I got into deep learning and how I got here.
Have you since returned to applications in control systems?
Yeah, I continue doing a lot of that. So a lot of the knowledge that I gained from my
undergrad in my masters in control, things about certainly the automatic thinking about
our differential equations, knowing how to sort of map things of legacy domain and so on.
But as well as being able to do things like PID controllers, those are still the things
that we use today. Like when we try to control robots and we might use neural networks
these days to sort of set up parameters, automatic, of different PID controllers. But you
know, all that background has been very useful.
Yeah, those are some of my favorite courses from grad school, PID controllers and hysteresis
and all these different things. But I've had the impression that that stuff is, you know,
works so well and there's such a need for more deterministic response for a lot of
the applications of this that there hasn't been much in terms of neural network applications
for that. But it sounds like you've been doing that for a while.
Yes, no, there's quite a few people who work with this. So control and the sort of combination
of classical control techniques with the new, sort of the reinforcement learning techniques.
Yeah, it's actually a very fruitful area of research. And I think a lot of the work that
folks like Peter Abel at Berkeley do, like Michael Thunderpan at UBC, Emma Chauderof at
the University of Washington, it all sort of involves a sort of a bit of a marriage between
the classical techniques and the new techniques. And I think there's still a lot we can learn
from classical control as well in terms of robustness and constraints. Because as we build
controllers, we want to make sure that they're safe, that they we can verify them and so
on. So these, these continue being challenges that we have to address in order to deploy
the technology.
So tell me a little bit about your research interests. What are, it sounds like you kind
of definitely a lot of different things, but how do you kind of characterize the general
scope and direction of your research?
I think fundamentally it's still the same that it's always been. It's trying to understand
the brain, trying to understand what it is to be me, what it is for you to be you and
for us to be engaged in this discussion. That's the real goal. And of course we have to
sort of break this problem in order to be able to attack it. We have to think of like what
question, how to formulate the question. And often that's what leads to things like
RL, you know, you can specify tasks and cost functions. It's a language to talk about
the problem. We can sort of think of it in terms of cognitive abilities that we have
like memory or imagination, curiosity and so on.
So there's many ways to attack the problem of understanding the brain. And I think this
is a huge problem. And there's so much yet to be done. And I've picked a few directions.
I see my colleagues have picked out the directions. And I think it's going to involve many
perspectives for us to eventually come up with an understanding at many levels of abstraction
of what it is in the sense to be human. Do you think that our understanding of the brain
has benefited our understanding of machine learning or the practice of machine learning
more or less than machine learning has benefited our understanding of the brain? That's an
interesting question. So it is very clear that the understanding of the brain has helped
us build a get here. It's a couple years, but actually four years ago, I went to Japan
and I had the honor of meeting Professor Fukushima, who was the author of the New Cognitron,
which is essentially the convolutional neural network architecture that we all use these
days. Janakun added backpropagation to that architecture and that kind of is what brought
us here. That was a success story and vision and so on. When I spoke to Professor Kitagawa,
he mentioned that we're inspired him to do that work was the work of Hubel and Vizel
and Neuroscience. When he worked at his institute, I forget the name of the institute, but they
put neuroscientists and computer scientists together. It was through this combination
of different disciplines working together that he found out about these results in Neuroscience
and decided to code this model. So I think the neuroscientists had a much greater
potential. I'm told a set of writers was also influenced by cortical models of cells and
so on in order to propose LSTM, which also one of the key components of machine learning.
Has it helped the other way around? That's a much harder question. I mean, clearly with
the success of backpropagation and so on, a few researchers at MIT, for example, they
went on and they were starting to use that new knowledge to explain how the visual cortex
might be working and so on. I also think more recently, folks are starting to contract
coding and so on. Trying to look at the research that has happened, not just in machine learning,
but the combination of statistics and econometrics and machine learning and biostatistics, looking
at things like causal modeling in order to go back and try to understand, to try to formulate
the theory of how it is that we should approach neuroscience and what sort of findings we're
looking for. So I think to some extent, it also has penned out the other way.
Yesterday, at the poster sessions, I was checking out one of the posters that was making
kind of a strong correlation between what's happening in the brain and image processing
and CNNs and I always thought of neuro networks in general and CNNs as kind of quote-unquote
inspired by the brain, but not really exhibiting a very strong correlation. But they were talking
about layers as like V1, V2, V4, whatever, and IT, which presumably are related to a model
of how we think the visual information is processed in the brain. I didn't realize that there
was that strong correlation between these two fields.
Yeah, there appears to be. Certainly, for example, it's SIFAR. So every year, before
New Europe, there's another workshop that takes place. It's a two-day workshop called the SIFAR meeting.
And actually, I just came from it. There, we always have a combination of invite people
working in neuroscience as well as people working in computer science and engineering physics
and some people from different areas, from causality, et cetera. And we try to sort of use
everyone's insights to sort of advance what we're mainly to choose what to research areas
to explore next. So I think neuroscience has always informed what we do in machine learning
and machine learning, I think, also informed neuroscience. So certainly, at least at that
meeting, that's the case.
So the driving motivation is to try to understand the brain, so that kind of reverse part of
the feedback loop. And one of the recent areas that you've been exploring to help get there
is been imitation learning. And in particular, you contributed to a paper here that's focused
on third-party imitation learning. Let's just dive in. Tell us about that paper. What's
the motivation there?
Okay, so I'm excited about imitation because imitation is something that we all do. A lot
of the things we learn are through a process of imitation. And in particular, I'm interested
in a form of imitation called fused shot imitation, which is sort of follows from, you can sort
of think of it in terms of meta learning. So the idea of meta learning is evolution is
a learning process that happens at a very slow scale and it produces biological machines,
you know, us. Machines are capable of learning very rapidly with very few data. We don't
experience that much data throughout our lifetimes. And so we kind of want to do the same thing.
We want to sort of learn machines that can imitate very rapidly. So we trained them for
a very expensive process, but then we can deploy this machine and someone can show, demonstrate
something to this machine and then the machine is able to do that. Now then comes another
question of when you demonstrate, your demonstration might be with different objects, so different
hands. And so you need to now deal with the third person here. What we mean by that is
that you might have learned to do the task using, I don't know, a robot hands. And now you
have to watch human hands doing the task and then you have to, the robot has to imitate.
So very much like when children can imitate a dog by using their foot to scratch behind
their ear. Right. And they're able to remap from one type of body to another body quite
easily. And the contrast is, I guess first person imitation learning, which is, for example,
you're looking at the actions that someone is playing in playing a game and learning
based on those actions as opposed to just watching them, quote unquote, from a distance.
Correct. So you precisely said in first person in Atari, for example, if you're playing
Atari, first person, you would collect data from playing the game in Atari. We tried
to fit the model to it. In third person, what we do is we actually go to YouTube and we
look at people playing the games. And it's third person because there's a bit of, there's
a domain gap. You know, in YouTube, sometimes this video is, well, the video will definitely
be of a different resolution, different color. It will have ads inserted. It has all sorts
of other artifacts. And so we need to use, be able to remap from that thing in YouTube
to our domain so that we can play a play the game. And the inspiration for this for me
was like watching a child, my nephew. Spending a lot of time playing, what's that thing
that gets played? Minecraft. So he spent a lot of time watching Minecraft. And I remember
telling my brother, you're not worried about this and my brother's like, oh no, he's
just actually very good. He does really well in school when he does this sort of thing.
And what he does is he watches these videos and then he goes and plays. And so I realized,
well, that makes sense. This is what we should do. And so then I, you know, I work with these
very gifted scientists and engineers who took the idea and went and implemented this
for in the context of Atari. I think this sort of an initial step, which is more than anything
just a demo of what could be done. I think there's a lot of footage in YouTube for all sorts
of things. And so the hope would be that we eventually could train, say, Robert Manipulate
is or in simulation or with real robots to be able to watch YouTube and be able to even
though there's a domain gap because it's, you know, it's, they're watching different
human hands say tying knots. And then they would have to then tie knots themselves. That's
not actually easier. Like, tying shoelaces, for example, is something that even six-year-old
human struggle. Interesting. I think I've come across videos that or examples of research
that is trying to do some of that kind of thing, but not with actual instantiated robots,
but like animations. So like, watch a video of dancing and try to get the stick figure
to dance or things like that. It seems like there are a lot of parallel activities kind
of going after the same type of problem. Correct. A lot of this can be done in simulation
in a sense. It's easier to do it in simulation. Robots are basically machines that complicate
to this. And they're not produced by a typically a man. They're like cars, except that with
cars, we have these big companies that produce these nicely engineered machines. And robots
are more like cars when they started appearing in the world, you know, they sort of make shift
Frankenstein type things put together. And so they break down a lot of the time. It's
very hard to do robotics. The software is somewhat lagging and so on. But nonetheless,
it's interesting. Manufacturing keeps being one of the automation and manufacturing
keeps being one of the driving forces behind, you know, control theory. In fact, and control
and machine learning. And so has your work in this area with playing the Atari games?
Have you is that also still in simulation or do you have some robotic thing playing the
game? So that's in simulation. But we have been doing other works also doing imitation.
And in particular, doing this few short imitation, where we do, we use simulation. So physics
accurate simulation through an engine called Majoko. It's kind of what the OpenAI GM uses.
And then we also use real robots to do these sort of things. And so we've been very interested
in this sort of few short imitation, which is basically imitation with a few data. The
challenge here is and the challenge being the challenge of what I call few short metal
learning is to get a machine and being able to sort of demonstrate something new to the
machine with novel objects with novel motion and have the machine be able to do the same
thing. So if you think so, I want to go beyond building machines like classifiers, like
a CNN or and instead, they're learning machines. They're machines that I can give to someone
at a factory. And that person can then modify the machine via demonstration or via some
formal language or natural language and get the machine to do something new or do it slightly
different. So I always think of the next generation of AI machines as adaptable machines or tools
that learn. So it's very much like when we give tools to animators. I think to people in
factories and so on, what we need to do is give them tools that they can adjust and adapt
and become much more productive. So they don't have to do all this sort of low level coding
in order to or change hardware, etc. In order to deal with the fact that maybe the
lids of the water bottles have changed in size. So when we're learning as humans, one
of the things that we get to take advantage of is this relatively vast amount of quote unquote
common sense that I guess you can kind of think of as like transfer learning in a sense.
In the context of a few shot learning is there in your research in particular, is there
a starting place? Is there some base that we've taught the agent via you tell me or we
just starting from scratch with a few examples and saying go figure this out like what's
the base? Yeah, so you're absolutely right. Transition is very important. So we probably
need to unpack what I mean by few shot learning. So few short I mean it has to learn from
few data. And then meta learning is there's this assumption that the learning is going
to happen at least in two phases. There's going to be a very expensive phase which involves
many tasks. We want to build machines that can do many things or one thing. So we train
it to do many things and there's different ways to do this. But we should be thinking
about this as evolution and evolution leads to what some people call priors. It's an initial
state in your brain when you're born, you're born with a faculty of language. You start
with some sort of prior. There's still a lot of discussion on how to best do meta learning
in practice. And so there is this long process and then there is also another process which
is the fast learning which just with a few data you should be able to learn to do something
new with new objects or with new behaviors. So this is very important for robotics because
unlike simulation robots cannot afford to do or repeat the task a trillion times until
they get it right. And so it's important that they learn rapidly. So the name of the game
then is maximized generalization while minimizing the amount of time needed to learn. Which
of the two phases or both? Actually in both phases. In particular in the second phase you
sort of assume that you will be given very few data and you will have very little compute
power and you will have to come up with a solution very quickly. And of course if you haven't
in the first phase constructed a representation that allows for abstraction so that you can
go to so you can sort of easily move to new tasks. Then this is not going to work. So it's
important that whatever representation you come up with that there be sort of these
causality people call this like Burner-Scholkhoff and a recently Professor Yoshio Benji here
in Montreal they call this sort of the independent mechanisms. We also often refer to this as concepts
abstractions. And so by having these concepts abstractions or programs I did some work before
on something called neuro program interpreters that I'm so very excited about which is trying to
understand these modular components of things in the world like behaviors, programs, objects,
and so on. So if we have that and at the same time we don't want to hard code this we want to
learn this so that I think that's important. Then we have some much more hope that we will be
able to generalize to novel objects and so on. So we'll be able to reuse some of the knowledge.
So we'll know if we had knowledge for example of physics then whether there's new task
involves dropping something then it doesn't matter whether it's an apple or a watermelon or
or a piece of chalk. If you open your hand it will drop. You can predict it because you have an
understanding of the laws of at least at an intuitive level you understand the laws of physics.
So we need to we definitely need abstraction. So it's a few shots with the learning
but in order to get generalization we need abstraction. We need to have a causal understanding
of the mechanisms in the world. Kind of taking a step back we've got few shot meta learning,
imitation, all of these are part of you know they're brought together to try to solve this problem
is it is meta learning kind of like decore challenge and if you figure that out then the other two
are easy or do they each kind of contribute their own unique challenges to the overall picture.
I think they're all different perspectives of looking at the same problem which is like
oh god that's the brain do it. So the meta learning perspective is very useful but of course
it still leaves open the question of how do we choose the representation.
Like in likewise we do some reinforcement learning when we do imitation sometimes sometimes
we do supervised learning but whether you do reinforcement learning supervised learning or whether
you choose to think about the problems in terms of reinforcement learning and off-policy learning
or you or you instead choose to think about them in terms of causality and contractuals.
It's kind of a it's a matter of taste. It still leaves the question of what other
representations. So we still have to figure out what are these sort of independent representations
and how to combine them in order to solve tasks. So I think all these different perspectives on the
problem are very useful. The multi-agent perspective is also very useful because the most interesting
thing at Neurip's is not like the objects around it's the people. So people are the most
interesting in our environment. A lot of these abstractions that I'm talking about that are
only possible with people. We can talk about a podcast because even though podcasts are things that
or perhaps a better example is we can think about today Wednesday. There's no such a thing as
Wednesday in the world Wednesday is a construct of our minds. It's an abstraction that we humans find
useful to communicate and so it's eventually we do need to the multi-agent perspective.
And another sort of thing that sort of comes into this sort of important in third person
imitation is I'm able to say observe you and you have a slightly different body than mine
but and I have the sort of third person view of you. So I'm looking at you and I can see your
whole body and parts of your body that you can't see. I also have my own sort of perspective
and I know what I'm feeling for example in my fingers and I know how cold this room is and I
apologies and always. So the interesting thing is that I see if you think of I have a third person
of view of you and I have a first person view of me and through that I can have a first person
view of you. So I can more or less predict how you're finding the temperature in the room. I can
probably predict what you're feeling in your fingers right now and at the same time I can also do
the third person view of me I can sort of step out of my head and imagine me sitting here in this
chair or renting a tube and so we then end up with this two by two matrix and eventually I think
this being able to sort of step outside of my mind and to know that it's me here talking to you
to be aware of that I think it's very profound and as part of understanding what is intelligence
and how how do our brains work because this is about knowing how I relate to the world. How do I
know what I know it's to know what I don't know to know what I don't know it's we're getting to
this question of awareness to be aware of your knowledge it's something none of our machines
have at present kind of done and and I mean in fact if we go in this direction we are not too far
from there I say having a stab at understanding consciousness because that's to to some extent
in non-metaphysical sense but to a computational we're moving toward a computational definition
of consciousness here and there's a few scientists in working say on attention scheme of theories
and so on that are starting to actually believe that this it might be possible for us to start
talking about we already talk about like like for example a neural network policy with R&N
the internal state is a subjective state and some of the work that I've already done with
Misha Deniel one of the researchers I collaborate with shows that the models have an internal
representation of the world even after they've no longer in the presence of that world you know
sort of they've they've touched an object and the hand lifts even a while later they're still a
memory of the shape of that object in the in the internal representation of the recurrent neural
network and so if we go on step further to there being awareness in that neural network of what it
knows that it that it knows that there was this object and it had a representation when we start
combining this with you know with abstractions with counter five thought thinking and so on I
think we are getting very close to intelligence the problem is we still don't know how to get
these abstractions in order to be able to do third person imitation and in order to be able to
get good awareness models of the world so when you're pursuing a line of research like the third
person imitation learning can you walk us through you know the various phases of a project like that
how does it evolve so this one in particular is it's fun you go collect videos of that's our
in YouTube and because the first thing you need is data yeah so one initial state is coming up
with representations that whether it's the video in YouTube or whether it's the the video that you
can generate by playing the game in your own emulator in the lab the representations in both the
main to such that they that they are equivalent so we call this sort of self super so one of
micro operators which is the two that are equivalent the representation so it's just you the
ideas of build neural networks that will whether whether they look at a YouTube video or the actual
video in the emulator that I have they will produce similar representations okay so the questions
how do we train this because we won't have super we don't have supervision and it's not an RL
problem it's something that we're now referring to a self-supervision okay and so my one of my
collaborators you survived are went to the YouTube videos and he came up with a technique contrast
of learning now that allows us to be able to learn these representations so see for example
tries to predict whether a video frame appears before or after a recent video frame or he tries to
classify how many steps ahead is one segment of video from another segment okay or whether the audio
video just an important part of Atari that we never get to see much of whether the audio signal
car happens at the same time or not as the video signal and through and through just these very
basic checks you can sort of formulate a you know training labels essentially you can train a
neural network the typical techniques that we use there the sort of contrasting what is right versus
what is wrong is basically what we call contrast of learning or and there's been many techniques for
doing this max margin maximum likelihood in fact it's a form of thing this and in fact this is
what kind of led to also Gantt it was that this kind of research of being able to so try to
have a classifier that is telling you what's real what's not real and so you collect the video
and I'm still not very clear on that okay so we're representations right so one representation is
the one you've built up from the YouTube videos what's the other so we because we're using videos
of that are very different for different players with different I know the difference of all
the videos will all have different appearance but we through learning the representations like
this we're able to get an in to embed the videos into vectors okay that are the same for all
different variations of the video and once you have such an embedding embedding on a frame-by-frame
basis or segment or entire video or something else typically on a frame-by-frame basis but you
could sort of do this also conceivably you could try to do this of a sequences also okay
and with this now you essentially now if you have a trajectory in YouTube now you can map this
to a trajectory in latent space in this sort of embedding space and now this is essentially
the trajectory that you need to follow when you play Atari so then we use this trajectory
as a reward signal and then we just do reinforcement learning and here Toby Favme the other
collaborator just went and tried a couple reinforcement learning agents and then it sort of learns to
it's using the trajectory as the reward signal and it tries to follow it by taking actions in the
game so essentially all we've done is we've used the data that we've served in the world solving
the task in a slightly different setting we call it with a domain gap and and we've
we first learn the features and then once we have those features we can use those features to
construct the reward function and then we just try and just maximize the discounted sum of
returns in order to solve the game okay you mentioned that the videos that you come across have
you know different levels of quality and I thought you said something like different you know
colors or levels of noise or whatever do you do any pre-processing like domain adaptation or
out as anything to augment the the data in some way yes so in fact the self-supervised learning
is a form of domain adaptation that's what we're trying to find features are sort of common for
all these domains in Atari the gap the domain gap is not that big so it's somewhat
easy for us to do some processing of a video scale and so on okay what we will need to do is
really address and there's been a few papers but this is still I think largely an unsoft problem
is be able to just watch a few videos of someone in YouTube doing something like pouring liquids
and then be able to get a robot whether a simulation or real robot to also pour liquids
that is very hard to do and in terms of mapping human hands to robot hands
think the task there's a few nice efforts recently Chelsea Finn has done some good work in this we
we've sort of been looking at this as well open AI has also some works but I think we still have
a long way to go before solving the problem and so what's your sense for what the next step is the
next piece of that puzzle so I think it's one of the things that I'm going to be betting on will
be through coming up with better representations and currently there's two sort of hypothesis
here one is just make the networks bigger and so the big networks is something we explored recently
on a paper we actually just put on archive recently with an algorithm called metamimic
the idea of metamimic is it closely imitates the demonstration like we in fact call this
high fidelity limitations so you're trying to do something precisely like a human we do
okay so and the idea is we're going to learn to do what humans do precisely for many many tasks
so far we have only done it for one task with variation in the task but the intention is to do
this for many tasks if you can learn to do one short imitation so if imitate many tasks then
a test time we show a new task with new objects and a new check whether we can still imitate
so we've in effect test generalization now what we found when doing when training metamimic
is we had to keep the net making the networks bigger in order to improve generalization on the
test set typically our real researchers haven't focused much on generalization and that this has
changed recently over the last few years and I'm actually glad to see our colleagues all sort of
embracing these generalization tests and what we're finding is here it becomes really important
to make the networks big however we didn't know whether we could train the massive network so
we trained for perception for things like vision and so on to also do control but in in this work
we found out that it is indeed possible if you have enough sort of these demonstrations it's
possible to train like in our work is the largest ever-trained neural networks to do RL
and by orders of magnitude larger than any pre-existing network most of what we did before with
Atari and so usually was with two layers neural networks and in fact professor Imot Todorov even
complained about this at one stage I think if we are going to be doing control from pixels
or from pixels and ego emotion and so on it's very important to use large network architectures
and to learn how to model them properly and how to train them so the reason why we chose to go
this way with metamemic is because if you observe children when humans are solving a task
if they introduce irrelevant steps in between children will solve the task but they will also
do the irrelevant task whereas a few experiments have shown that I think with chimpanzees and
volunteers that they will go and solve the task immediately they will not do the relevant steps
okay so humans have this propensity to over what psychologists call over imitate
okay and essentially this is the strategy we're following at metamemic so build a very big network
that will imitate precisely millions of things okay and that will allow it to sort of build into
this model the capacity to when shown a new thing that I had not seen before it can just from one
single demonstration repeat what has been done okay we also found that these models can also
be further trained with RL to solve tasks more efficiently so that's one way to go okay that I'm
gonna explore just very big networks and of course there's a lot of engineering because it's
in the big networks we have to put a lot of thinking into normalization and so on so it's not
when we say big networks it should not be understood that this is brute force engineering
on the other hand is one requires very precise engineering in order to train these big networks okay
this certainly has been my experience with some of our work on liberating as well as some of
the brilliant work that your mind has put out on big gants and using gants as generative models
recently on the other hand I also want to sort of come up with the representations that are
more compositional so that's like the neural programming interpreters work that I did with
Scott read a few years ago and that some folks like in Stanford like folks in Stanford have used
to build things like neural task programmers that are able to actually do very sophisticated
control just by exploiting modularity sort of program modularity as well as to you know
sort of being able to break tasks into some tasks and so on yeah that's going to be a continued
effort and I think I'm also thinking that in terms of building many causal models many small
modules that you can then combine and if the small modules are doing the right thing and they have
a good understanding of I mean a good understanding in the sense that the causal representations
are world and so by combining them and they're in ways which we still have to devise
and we should be able to get much larger models that can still represent the world and
and you know and be able to generalize much better because these models will be compositional
they will be able to have this combinatorial reuse of the components when you describe the first
of those two directions building out these much bigger networks one of you may reference to
kind of staying in the pixel domain do you see efforts to are there what are the alternative
approaches that are being explored if any there's two different philosophies there is the learning
from scratch philosophy that tries to learn just from pixels yeah and I often find too much from
just pixels okay in fact if you're doing grasping and so on it's important that you don't just
have a monocular image but you have stereo okay so death is a very important cue in order to
grab things and you just have to try to grab things from a single video monocular video to see
how hard it is even for humans to do it so there's that there's of course touch half-text and
there's the you know the fact that we have little hairs in our ear that allow us to know the
accelerations of our head and so on so we have a lot of internal gyroscopic information
there's some cares less about kind of multi-sensory and more like is there a direction to you know
I think the weights and activations in these networks kind of form concepts but we don't necessarily
force them to form concepts that are meaningful to us or abstract in any way and I'm curious if
that is a direction that people are pursuing that makes sense indeed so it's indeed there's two
orthogonal things here one is so sort of indeed go for multi-sensory perception and then learn
these networks to conduct actions and this is the big open question like for example in robotics
with like these competitions with flying drones in a way that's an easy problem because the drone
doesn't have to touch anything so there's no contact force says this should be an easy control
it's an easy control problem if you know the state of the world what makes it hard is that they have
to do this if they if they have to do this from pixels navigate from pixels and if there's people
in the background that are moving and lighting changes and so on there's then becomes incredibly
hard and it's still by far and an open problem so perception has not been solved right and that's
essentially what this tells us or perhaps we're not using the right sensors we should and so
mm-hmm now the the other side of this of the argument is also what other folks are doing is
they're sort of saying we shouldn't be learning from scratch all the time you should fight in
fact use that we have already learned some modules some perceptual modules or some action modules
some controllers and sort of learn to reuse the components and that indeed I think is a very
promising area if you certainly if you've learned controllers to achieve tasks it's possible
to reuse them and here controllers you can think of for them as also like sub-programs and we can
combine them what we haven't done well yet is being able to learn a representation of something
like an object and then being able to exploit it that representation keep it in new
you'll be able to harness it for future to build future representations or to keep learning
continually more and more complex representations and be able to manipulate to be able to manipulate
those objects that is still an open problem we don't quite know how to build and this type of
model architectures and there's two philosophies as people who try to model this by learning everything
from scratch and then there's people who of course try to inject much more domain knowledge
and they argue with each other as to who is right but we've seen a lot of these debates
in Twitter especially but I think the verdict is still open it's good that there's different
perspectives on this well Nando any advice or words of wisdom or pointers for folks that are
interested in kind of digging in a little bit deeper into this area losing thoughts I guess come
to come to new reps and I clear and then it's sort of get get to know watch this podcast
and there's also brilliant courses online like Andruing and so on so you where you can sort of
get into this sort of material and get a much better understanding of the material we also
organize summer schools we as in many researchers volunteer throughout the year to teach summer
schools all over the world so here in Canada through the SIFA program lately I've been involved
with teaching a summer school in Africa called the Deep Learning in Dapa which is a great initiative
because and I think that is a parting thought that I want to bring into this AI is such a very
powerful techniques AI will be very influential in our world not just the tech world but AI
will shape politics AI will shape our economies and so on it is essential it is of paramount
importance that everyone has access to AI if currently there is very strong biases
women are underrepresented in AI there's a huge under representation of certain races in AI
of certain continents in AI and we need to address these unbalances we need to address them for
two reasons one then we want to build tools that are fair we want to make sure that AI is for all
of us not just for the few and the other reason is because some of the people that have been
marginalized from this actually have a lot to contribute we often look at Africa as there's
this wall as there hasn't participated in the machine learning conferences this year at NIPS we
have at least two papers that I know of that came from Africa so by helping a bit going and
volunteering there we can sort of start reaping the benefits of their contributions and there's
lots of things that came from Africa Gaussian processes were invented at my university by Professor
Krig and he used to be called Krigging EC2 if you actually look at the history of AWS Africa played
a big role in it you know this is where people in South Africa working on this that have given us
the infrastructure that not pretty much holds all the data from all banks and nation states and so on
it's important to also address the problems in Africa that we haven't even thought about
because we haven't gone there and things like translation in South Africa there's 11
official languages in other countries in Africa there's like 50 languages and when people tweet they
usually use three languages mixed so translation is a huge problem and so when you go there you learn
this and then you realize oh it's important to sort of start working more on unsupervised machine
translation and so on there's also other problems one and I'm really going off track here there's
a very long apologize for having gone this long another problem that I thought was very
interesting in going to Africa two years ago was when I learned about Mum Connect so with Mum Connect
Mum's essentially get enrolled when they go to hospitals into this messaging service and throughout
Africa people use these very cheap old phones these two have here and they can still text and
communicate quite efficiently in fact that the service plans are not as expensive as here in Canada
so in many ways they have leapfrogged out inefficiencies and what they can do is the doctors can send
the messages reminders of what to do after the child is born and so on and they can also if they
have questions they can text the doctors so for example they could text the doctor and this is
like an example of what happens like I've given water to my child all day and all night but the
child keeps throwing up the water at that state the doctor can simply say boil the water
provide that sort of basic advice that will save a life and in fact most if we want to have a
huge contribution to the health of people in our planet those little things other things that
sort of matter the most and you can learn about them when you go and include people from other
communities and then you see the wonderful things that have already done to address this problem
so they actually can contribute a lot of insights into the development of a machine learning tools
well Nando thanks so much for taking the time the child it's great to get to speak with you
thank you very much thank you for listening
all right everyone that's our show for today for more information on Nando or any of the topics
covered in this show visit twimmalei.com slash talk slash two thirteen you can also follow along
with our nirip series at twimmalei.com slash nirips 2018 as always thanks so much for listening
and catch you next time
