WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.040
I'm your host, Sam Charrington, Hey Twimble listeners, if you're a fan of our AI

00:34.040 --> 00:39.680
platforms coverage and especially if you download our first AI platforms ebook, Kubernetes

00:39.680 --> 00:44.800
for machine learning, deep learning and AI, then today is your day.

00:44.800 --> 00:49.440
While we've been working tirelessly on Twimblecom, I have also been working on Book 2 in the

00:49.440 --> 00:53.440
series, the definitive guide to machine learning platforms.

00:53.440 --> 01:00.080
And I am happy to say that it is finally available to download now.

01:00.080 --> 01:04.880
We created the book as a resource for ML, AI and data science leaders and innovators

01:04.880 --> 01:09.120
to help guide their efforts in scaling and industrializing machine learning and deep learning

01:09.120 --> 01:11.320
in their organizations.

01:11.320 --> 01:16.440
In this book, we address questions such as why invest in increasing your organization's

01:16.440 --> 01:20.160
capacity to deliver machine learning and deep learning models.

01:20.160 --> 01:23.720
What are the key barriers to delivering ML and DL models?

01:23.720 --> 01:28.560
How of organizations like Facebook, Airbnb and LinkedIn overcome these challenges and

01:28.560 --> 01:32.240
how can their learnings be applied to your organization?

01:32.240 --> 01:35.840
What are the state of the art, machine learning platforms and tools and how can you put

01:35.840 --> 01:37.480
them to use?

01:37.480 --> 01:42.280
How can you develop an AI platform strategy to support your organization's goals?

01:42.280 --> 01:47.960
To access the definitive guide to machine learning platforms, visit Twimbleai.com slash

01:47.960 --> 01:50.200
AI platforms.

01:50.200 --> 01:53.480
And now onto the show.

01:53.480 --> 01:56.960
All right, everyone.

01:56.960 --> 01:59.240
I am on the line with Olivier Bachem.

01:59.240 --> 02:02.240
Olivier is a research scientist at Google Brain.

02:02.240 --> 02:05.600
Olivier, welcome to this week at Machine Learning and AI.

02:05.600 --> 02:06.600
Hi, Simon.

02:06.600 --> 02:07.600
Thanks for having me here.

02:07.600 --> 02:08.600
Yeah, absolutely.

02:08.600 --> 02:14.920
I'm excited to learn more about the environment that you worked on.

02:14.920 --> 02:17.920
It's called Google Research Football.

02:17.920 --> 02:19.240
We'll dive into that.

02:19.240 --> 02:23.480
But before we do, I'd love to explore your background a little bit.

02:23.480 --> 02:27.760
You did your PhD in large-scale clustering problems.

02:27.760 --> 02:32.240
Tell us a little bit about that, how you got to that and what you've been working on

02:32.240 --> 02:33.240
more recently.

02:33.240 --> 02:34.240
Yes.

02:34.240 --> 02:39.720
As you said, I did my PhD at Dieter Suryk on large-scale clustering.

02:39.720 --> 02:46.760
So one thing, I can't be the main question that I consider my PhD is, how can we go and

02:46.760 --> 02:51.600
apply clustering methods such as k-means clustering to very large data sets?

02:51.600 --> 02:54.720
And I essentially worked on two major topics.

02:54.720 --> 03:00.080
One was called, or is called core sets where the idea is you have a very large data set.

03:00.080 --> 03:04.960
That you want to run your machine learning or your clustering algorithm, not on a full

03:04.960 --> 03:09.680
data set, but on a smaller set because it might just take too long if you have billions

03:09.680 --> 03:11.040
of data points.

03:11.040 --> 03:16.760
And now core sets, they're a subset of your original data that you can find very efficiently.

03:16.760 --> 03:22.160
But that when you train on this smaller subset, you get solutions that are proofably competitive

03:22.160 --> 03:24.800
as if you would have trained on the full data set.

03:24.800 --> 03:31.320
That was one topic, and the second one was related to k-means clustering as well, where

03:31.320 --> 03:36.760
many people use an algorithm called k-means++, which is a smart way to initialize a clustering

03:36.760 --> 03:37.760
algorithm.

03:37.760 --> 03:42.960
And we worked on ways to make it much faster by, again, only looking at parts of the data

03:42.960 --> 03:46.840
or maybe looking at it once or twice instead of multiple times.

03:46.840 --> 03:47.840
Cool.

03:47.840 --> 03:53.440
And when you talk about the core sets and having provably competitive results is that an

03:53.440 --> 04:00.280
exercise in structuring your sample subset so that it has similar statistical properties

04:00.280 --> 04:02.440
as the larger data set.

04:02.440 --> 04:07.920
It is, to some point, it is also about finding the data points that are very important.

04:07.920 --> 04:12.440
Turns out in many machine learning problems, but in particular in clustering, you can

04:12.440 --> 04:17.000
have billions of data points, but some data points, maybe in regions where you have very

04:17.000 --> 04:23.320
few, not a lot of data, and now you want to capture this data because it can contribute

04:23.320 --> 04:27.440
a large part to the area of your model, whereas in regions of the space where you have a

04:27.440 --> 04:31.200
lot of data, you want to take several data points and kind of group them together.

04:31.200 --> 04:35.960
So I would say that's the intuition behind the approach, but then you have to kind of

04:35.960 --> 04:39.320
do this properly in order to actually get the article guarantees.

04:39.320 --> 04:40.320
Okay.

04:40.320 --> 04:44.320
This sounds like in that regard it's somewhat related to active learning.

04:44.320 --> 04:46.120
Well, yes and no.

04:46.120 --> 04:49.600
I think the yes part is it's about finding important points here.

04:49.600 --> 04:55.560
The key idea is you want to do this, I mean, here we're doing this essentially in an unsupervised

04:55.560 --> 04:56.560
setting, right?

04:56.560 --> 05:00.000
If you're doing clustering, you don't get labels, but it's more about actually which data

05:00.000 --> 05:02.600
do you want to look at more closely?

05:02.600 --> 05:03.600
Right.

05:03.600 --> 05:06.600
And I make conceptually as opposed to in practice.

05:06.600 --> 05:07.600
Yes.

05:07.600 --> 05:08.600
Yeah.

05:08.600 --> 05:14.360
And so that was your PhD, what sparked your interest in doing a PhD in machine learning at

05:14.360 --> 05:15.360
all?

05:15.360 --> 05:21.440
Yeah, so I have a bit of a non-traditional background, I did my undergrads in economics.

05:21.440 --> 05:23.880
I worked in different companies.

05:23.880 --> 05:31.040
I did a masters in quant finance, worked in some startups to the statistics master.

05:31.040 --> 05:36.040
And during my masters in statistics, I ran across these cool algorithms where you could

05:36.040 --> 05:39.200
solve statistical problems in a very applied way.

05:39.200 --> 05:44.240
I thought this was pretty cool and immediately after my masters, I thought it would be cool

05:44.240 --> 05:46.240
to do a PhD in the Explorer.

05:46.240 --> 05:47.240
All right.

05:47.240 --> 05:48.240
Cool.

05:48.240 --> 05:51.680
And what are some of the things that you've been working on at Google Brain?

05:51.680 --> 05:52.680
Yes.

05:52.680 --> 05:58.080
So at Google Brain, I've done quite a lot of different projects, which I thought was really

05:58.080 --> 06:02.080
cool, but I would broadly group them into three different categories.

06:02.080 --> 06:09.200
So in the last year, I've mainly led a big effort on research into learning what's

06:09.200 --> 06:11.800
called disentangled representations.

06:11.800 --> 06:14.680
Tell us a little bit more about that, what are disentangled representations?

06:14.680 --> 06:21.280
Yes, so the key idea is that in many settings in machine learning, you see a lot of high-dimensional

06:21.280 --> 06:28.200
observations, such as images or video or audio, but that these observations are not truly

06:28.200 --> 06:33.560
high-dimensional, but that they are the result of a lower-dimensional set of ground-truth

06:33.560 --> 06:36.000
factors of variation.

06:36.000 --> 06:40.160
And now, when you do representation learning, you essentially want to capture a learning

06:40.160 --> 06:45.760
function that takes such a high-dimensional observation and captures some form of information

06:45.760 --> 06:47.280
about that observation.

06:47.280 --> 06:53.560
And disentanglement or disentanglement representations is one property that you might want to get

06:53.560 --> 06:57.640
when you're doing representation learning, which is you want to capture these different

06:57.640 --> 07:01.760
factors of variation in your actual representation.

07:01.760 --> 07:07.000
So one example would be if you have an image with an object in the middle, right, or somewhere

07:07.000 --> 07:11.440
on the image, you could say, for example, one of the factors of variation might be there

07:11.440 --> 07:16.320
is an object at some position, with some size, with some color, and you want to capture

07:16.320 --> 07:20.360
each of these properties independently in representation.

07:20.360 --> 07:26.320
And can you give us a sense of within the realm of disentanglement representations, what's

07:26.320 --> 07:30.960
kind of the state of the art, what are the key tests and research?

07:30.960 --> 07:36.080
Yeah, that's actually a pretty cool question, because this is essentially the question I

07:36.080 --> 07:40.200
asked myself about the year ago, when I started working on this topic, there was a lot of

07:40.200 --> 07:47.640
papers on this topic doing new methods, new metrics, and we started a project actually

07:47.640 --> 07:49.960
benchmarking all of these methods.

07:49.960 --> 07:55.800
It's called a result in a paper called challenging common assumptions in the unsupervised learning

07:55.800 --> 08:00.320
of disentanglement representations, where we actually found that when you actually train

08:00.320 --> 08:05.120
a lot of these models, and we did a large scale study with train more than 10,000 such

08:05.120 --> 08:12.600
disentanglement models, we found that a lot of things happen that are quite surprising.

08:12.600 --> 08:18.840
Now the approach that we took is going to be, we thought of it, okay, what's the approach?

08:18.840 --> 08:23.560
If we are a new researcher, we get a new dataset, we want to find a disentanglement representation,

08:23.560 --> 08:25.680
what are the questions we actually have to answer?

08:25.680 --> 08:29.160
And as you mentioned, I guess the first question you want to answer is which model should

08:29.160 --> 08:30.160
I use, right?

08:30.160 --> 08:31.520
What is the state of the art model?

08:31.520 --> 08:37.480
The second question is how would you select model parameters, hyper parameters to actually

08:37.480 --> 08:42.440
train your model, and thirdly, you might not just train one model, but many different models,

08:42.440 --> 08:46.560
and you might have to select the model that you actually want to use.

08:46.560 --> 08:52.040
And what we kind of found is actually very interesting that the model that you use is

08:52.040 --> 08:53.040
not that important.

08:53.040 --> 08:57.360
It seems that all the methods that were proposed, and when you do a large scale evaluation

08:57.360 --> 09:03.960
on many dataset with many metrics to measure disentanglement, you kind of find out that all

09:03.960 --> 09:09.600
of them can give you very good disentanglement representations at the same time.

09:09.600 --> 09:18.120
So yeah, having said that, if you, what we also found is, it's very hard to select hyper

09:18.120 --> 09:23.000
parameters, because one of the hard parts in learning disentanglement representations

09:23.000 --> 09:28.000
is that when you train these models and you evaluate them, you know what the ground truth

09:28.000 --> 09:29.760
factors of variations are.

09:29.760 --> 09:33.960
But many of these methods that you actually want to use, the call assumption, the whole

09:33.960 --> 09:38.000
point of using them is that you don't know the ground truth factors of variation.

09:38.000 --> 09:41.200
So when you do the training, you have to be kind of strict, you're not allowed to look

09:41.200 --> 09:45.880
at the labels, because that would become cheating in a sense, because in a settings where

09:45.880 --> 09:49.840
you actually want to use such a model, you don't have access to these labels.

09:49.840 --> 09:53.920
So one of the hard parts is it's very hard to select good hyper parameters, because you

09:53.920 --> 09:57.240
cannot compute the disentanglement scores when you're off to it's going to use it.

09:57.240 --> 10:02.080
It turns out models are quite sensitive to hyper parameters, which may not be, may not

10:02.080 --> 10:03.080
be surprising.

10:03.080 --> 10:07.720
There is some hope that you can transfer hyper parameters across different data sets,

10:07.720 --> 10:12.720
but that brings you kind of to the last point, right, how do you actually select a model

10:12.720 --> 10:16.320
when you've trained many models with the same hyper parameters, what turns out there

10:16.320 --> 10:21.520
is even for a given model and the given set of hyper parameters, you can get vastly different

10:21.520 --> 10:22.520
results.

10:22.520 --> 10:26.880
You can, if you train 100 models, you're going to get some that are very, well, doesn't

10:26.880 --> 10:27.880
tell.

10:27.880 --> 10:30.120
You have some that are completely entangled.

10:30.120 --> 10:33.880
And the hard part is without looking at the labels or without looking at the models

10:33.880 --> 10:39.600
yourself as a human, it's very hard to say which of these models you should be using.

10:39.600 --> 10:44.560
We looked at different strategies on how to do this and maybe to illustrate with a kind

10:44.560 --> 10:50.080
of comparison with the best thing that we could find is even if you kind of do a smart

10:50.080 --> 10:56.560
strategy of finding good hyper parameters, you're going to beat a random model across

10:56.560 --> 11:01.280
all the models I've trained only slightly more than at the coin flip.

11:01.280 --> 11:04.360
So slightly more at 50, than 50%, so I mean 60%.

11:04.360 --> 11:07.760
Well, when I hear you describe that work, I hear a couple of different things.

11:07.760 --> 11:14.440
One, I hear this like core focus on the disentanglement versus entanglement, which I'm

11:14.440 --> 11:21.440
kind of conceptualizing as like an orthogonality of these different, I don't know what you'd

11:21.440 --> 11:25.640
even call them, not models, but spaces representation.

11:25.640 --> 11:27.640
Yeah, yeah, yeah, thank you.

11:27.640 --> 11:35.520
Also in the example you gave where you've got an image and the disentangled representations

11:35.520 --> 11:40.040
are, you know, an object and color and size and things like that.

11:40.040 --> 11:47.760
It almost starts to sound like a holy grail of AI or AGI where you're like, there's true

11:47.760 --> 11:48.760
intelligence there.

11:48.760 --> 11:54.120
And the question that comes to mind is, you know, to what degree are these disentangled

11:54.120 --> 12:01.120
representations generally that explainable or aligned with our intuition and how we as

12:01.120 --> 12:07.000
humans would interpret what's happening in an image or are they just some orthogonal

12:07.000 --> 12:11.400
established set of representations that don't necessarily map to the way that we would

12:11.400 --> 12:13.680
interpret an image.

12:13.680 --> 12:16.120
So I think there's maybe two points.

12:16.120 --> 12:21.400
So the first point, this is also one that we're making a paper is that if you look at a task

12:21.400 --> 12:26.040
because it's an unsupervised learning task, it is actually in defined and we have an

12:26.040 --> 12:32.080
impossibility results, which kind of shows that no algorithm can do, I mean, this is my

12:32.080 --> 12:36.640
interpretation, no algorithm, kind of to learn disentanglement representations can do

12:36.640 --> 12:43.000
so consistently for all possible world models of the world or all possible processes that

12:43.000 --> 12:44.000
generate your data.

12:44.000 --> 12:47.720
And I think that brings us kind of to that second point, right?

12:47.720 --> 12:57.160
This essentially means that you require inductive biases onto kind of the models which match

12:57.160 --> 13:01.000
the data that you want to use the models for, right?

13:01.000 --> 13:06.080
And I guess one of the hopes or like the key idea while a lot of people are excited about

13:06.080 --> 13:10.960
disentanglement representations is that there are such good inductive biases, right?

13:10.960 --> 13:15.000
And if you were to find a disentanglement representations, and this is kind of follow-up

13:15.000 --> 13:20.760
work that we have looked into, it seems that you get quite some benefits in terms of the

13:20.760 --> 13:26.000
representations you get, we found that for non-trivial downstream tasks, when you look

13:26.000 --> 13:32.120
at tasks that require reasoning, abstract reasoning about the world that you will do

13:32.120 --> 13:37.200
better if your representation is disentangled versus if you looked at, or at least we

13:37.200 --> 13:42.240
saw that this was correlated with your representation being entangled.

13:42.240 --> 13:46.600
I think that's kind of the hope that some of your algorithms or your models that you train,

13:46.600 --> 13:51.600
they will generalize better to stuff that you have not seen before and things like this.

13:51.600 --> 13:54.640
And I think that's why a lot of people are excited about this.

13:54.640 --> 13:59.600
But I completely agree it matters how you define what is interpretable or not, right?

13:59.600 --> 14:00.600
Yeah, yeah.

14:00.600 --> 14:01.600
Okay.

14:01.600 --> 14:06.560
Well, for the sake of time, we'll skip some of the work you've done on generative

14:06.560 --> 14:07.560
model.

14:07.560 --> 14:13.040
That's the other thing that you spend a bunch of time on at brain and dive into the research

14:13.040 --> 14:16.720
that was recently published on Google Research Football.

14:16.720 --> 14:22.040
Tell us a little bit about that project and its motivation and how it came about.

14:22.040 --> 14:24.640
Sure, sure.

14:24.640 --> 14:33.800
So basically, I think the whole project started in late summer last year.

14:33.800 --> 14:39.560
It basically started as a kind of cool idea in our team where we have, I mean, one thing

14:39.560 --> 14:46.800
you have to see about Google Brain at least in Zurich where I've been, it's very collaborative.

14:46.800 --> 14:51.680
We share a lot of ideas over informal sessions like when you go for lunch, when you go

14:51.680 --> 14:58.400
for coffee, but also in kind of within a team, and remember, we had this idea that we could

14:58.400 --> 15:08.000
use reinforcement learning to play video or kind of video games that model soccer, right?

15:08.000 --> 15:14.280
I guess a lot of people have played in the youth such video games and we started discussing

15:14.280 --> 15:20.440
about this in order to pros, whether to cons, and out of this, we started kind of the

15:20.440 --> 15:23.080
game, a bit of traction within the team.

15:23.080 --> 15:28.640
And we started investigating, would this be possible, would it be feasible?

15:28.640 --> 15:35.960
And we came across this open source football game called Gameplay Football, and we thought,

15:35.960 --> 15:38.400
okay, let's see what we can do with this.

15:38.400 --> 15:45.240
And at the same time, we, as we wanted to do, or some people in our group, were doing

15:45.240 --> 15:49.560
research on reinforcement learning, we started noticing a lot of benefits that would be

15:49.560 --> 15:58.320
had if we would build this into a fully-flanched modern reinforcement learning environment.

15:58.320 --> 15:59.320
So we started, yeah?

15:59.320 --> 16:04.120
Oh, no, I was just going to ask the obvious question, which is, so what are some of those

16:04.120 --> 16:05.120
benefits?

16:05.120 --> 16:12.320
Yeah, so I mean, the first benefit that I personally see is its open source.

16:12.320 --> 16:19.400
So this gave us a lot of flexibility to modify the needs for a modern reinforcement

16:19.400 --> 16:20.400
learning environment.

16:20.400 --> 16:26.200
So it's primarily built for research, and that gives us a whole bunch of benefits.

16:26.200 --> 16:31.800
And the first benefit, and I mean, that is also inherent to the game of football, is

16:31.800 --> 16:34.800
that it is an on-tribal problem, right?

16:34.800 --> 16:40.720
It's actually, if you look at it, it's quite hard to play, and let me if we iterate on

16:40.720 --> 16:43.840
or kind of explain what we're actually doing here.

16:43.840 --> 16:47.600
So this is in the context of reinforcement learning, where you have to teach an agent

16:47.600 --> 16:53.720
to interact with an environment, here the agent has to control, and this is in the basic

16:53.720 --> 16:58.920
form of this environment, has to control the active player on one of the teams, and it

16:58.920 --> 17:04.360
has to do actions such as, okay, I'm going to walk to the right, to the left, to the top

17:04.360 --> 17:09.760
or the bottom, and it has to pass around, shoot as you would, as a human if you were controlling

17:09.760 --> 17:13.200
a team in a football video game.

17:13.200 --> 17:18.000
And now, one of the benefits is that this is actually kind of, if you think of the game

17:18.000 --> 17:21.760
of football, where you start in the middle of game with a kickoff, you have to pass around,

17:21.760 --> 17:26.240
you have to play around your opponent's defense, you have to come up with a strategy, and

17:26.240 --> 17:31.520
you have to score, that is actually a non-tribal task, and it's challenging task.

17:31.520 --> 17:37.920
In particular, if you have different difficulties of opposing teams, as already one of the advantages

17:37.920 --> 17:44.920
we saw, as you change the difficulty of the strength of the opposing team, you can change

17:44.920 --> 17:49.800
the difficulty of the learning task.

17:49.800 --> 17:53.320
Similarly, you can make it much easier, you can say, well, let's say that pitches empty

17:53.320 --> 17:58.480
and they just have to score a goal against no goalkeeper, that is even much easier, right?

17:58.480 --> 18:04.240
So one of the benefits that we saw was that it's very easy to adjust the difficulty of

18:04.240 --> 18:07.600
the game, while still not being trivial, right?

18:07.600 --> 18:12.000
And that is kind of in contrast to maybe other environments that may be rather easy to

18:12.000 --> 18:17.760
solve, or just very hard computation expensive to run stuff on.

18:17.760 --> 18:23.200
The same time, as it's open source, we were able to build a lot of features into the

18:23.200 --> 18:26.800
game that help with doing enforcement learning research.

18:26.800 --> 18:32.960
So one thing we could do is we can turn on and turn off soasticity, we can make the game

18:32.960 --> 18:38.800
deterministic, then it turns out, or this kind of still an open question research is whether

18:38.800 --> 18:44.400
some algorithms generalize from a setting, from an environment that is deterministic to

18:44.400 --> 18:46.640
a sarcastic one, right?

18:46.640 --> 18:49.920
The game is focused on controlling the player that has the ball.

18:49.920 --> 18:54.720
The players that don't have the ball, as well as the opposing team, is there some kind

18:54.720 --> 18:58.880
of traditional, quote unquote, game AI that's controlling those?

18:58.880 --> 18:59.880
Yes.

18:59.880 --> 19:05.880
Maybe the one thing to keep in mind here, so the way we structure this is we build what

19:05.880 --> 19:13.200
we call the football engine, which is a simulation of a game of football, and it supports a lot

19:13.200 --> 19:14.200
of different features.

19:14.200 --> 19:20.240
One is it supports a built-in rules-based AI, as you would probably find in a professional

19:20.240 --> 19:21.960
video game.

19:21.960 --> 19:26.160
And at the same time, it supports a lot of features for research.

19:26.160 --> 19:32.000
And we chose that, OK, so if you have to control all the players in your team, your toss becomes

19:32.000 --> 19:33.000
much harder.

19:33.000 --> 19:36.800
We did some additional experiments and turns out if you have to control more players, this

19:36.800 --> 19:37.960
becomes harder.

19:37.960 --> 19:44.720
So in the basic version on what we call football benchmarks, which is in this setting, we consider

19:44.720 --> 19:49.200
the opposing players and your teammates to be part of the environment and they play with

19:49.200 --> 19:52.280
the built-in rules-based AI.

19:52.280 --> 19:58.920
In that case, you don't control them, right, and these are basically traditional reinforcement

19:58.920 --> 20:04.200
learning problems, and we chose three of them as benchmark problems where people can

20:04.200 --> 20:05.960
compare different algorithms.

20:05.960 --> 20:10.840
But at the same time, the football engine that we built supports actually controlling all

20:10.840 --> 20:16.800
of the players, or you can control all the players of one side, or we can even do a mix.

20:16.800 --> 20:22.040
I think in the paper we, with this experiment, where we control between one and three players.

20:22.040 --> 20:27.040
So that is completely flexible, and that's, again, one of the benefits of having a modern

20:27.040 --> 20:30.720
reinforcement learning environment that is open source.

20:30.720 --> 20:37.680
You mentioned that you chose three scenarios for your benchmarking.

20:37.680 --> 20:40.000
How are those three scenarios distinguished?

20:40.000 --> 20:46.440
Yes, so what we decided is, so we built these football benchmarks, which we considered

20:46.440 --> 20:52.480
the current benchmarks, problems in this environment, and it's essentially you play a game

20:52.480 --> 20:59.080
of 11 versus 11 football against three different of these rule-based opponents, and the only

20:59.080 --> 21:04.840
thing changes is the difficulty of how well these different opponents, how well the opponent

21:04.840 --> 21:05.840
plays.

21:05.840 --> 21:11.120
Okay, so some parameter to the rule-based AI that's driving them.

21:11.120 --> 21:12.120
Yes.

21:12.120 --> 21:16.800
Exactly, I'm kind of how quickly it reacts to you, yeah, how well it plays.

21:16.800 --> 21:21.200
As you would, if you play a, you know, a football game yourself, you can choose, you know,

21:21.200 --> 21:25.760
between an easy medium and a hard, here it's essentially the same thing.

21:25.760 --> 21:32.920
But the key motivation for actually doing this is also to accommodate that different researchers

21:32.920 --> 21:35.960
may have different computational budgets.

21:35.960 --> 21:43.120
So it turns out, if we are running currently with the algorithms that we tried and we found

21:43.120 --> 21:48.280
that if you run on medium and hard problems, it becomes quickly very challenging and you

21:48.280 --> 21:54.520
might have to go to distributed implantations of the funnagrums, you need quite a lot of

21:54.520 --> 21:59.600
computation resources, but at the same time, you also want to provide benchmark problems

21:59.600 --> 22:06.360
where you could go with a single machine, maybe a single GPU, and you could actually test

22:06.360 --> 22:10.360
your ideas and compare against other algorithms, right?

22:10.360 --> 22:14.800
Yeah, and I guess, yeah, there's different areas, right, of reinforcement learning, such

22:14.800 --> 22:21.440
as learning with a few samples, I think this is one of the very hard problem reinforcement

22:21.440 --> 22:23.040
languages, also very relevant.

22:23.040 --> 22:29.000
And even if you don't have a lot of computational resources, you can still go and benefit from

22:29.000 --> 22:30.000
this environment.

22:30.000 --> 22:31.000
Okay.

22:31.000 --> 22:37.440
Yeah, I mean, one thing also, we also added what we call the football academy.

22:37.440 --> 22:44.320
And this is essentially, as I said before, like the having that flexible game engine allows

22:44.320 --> 22:47.840
us to really kind of change what is happening on the pitch.

22:47.840 --> 22:52.640
And one inspiration we took when football teams trained right, they might do some drills,

22:52.640 --> 22:53.920
and we can actually model this, right?

22:53.920 --> 23:00.600
We can say, well, let's not have 11 versus 11 players, maybe let's play 3 versus 2 counter

23:00.600 --> 23:01.600
attack.

23:01.600 --> 23:07.360
And we define a set of different, such academy scenarios as we call them, they start with

23:07.360 --> 23:09.480
very easy ones where you're close to the goal.

23:09.480 --> 23:13.160
There is no goalkeeper and you just have to score, so you have to eventually learn how

23:13.160 --> 23:19.040
to walk into the goal or how to kick the ball into the goal to more difficult problems

23:19.040 --> 23:26.040
where you play with 11 players versus goalkeeper or, as I said before, counter attack scenarios

23:26.040 --> 23:31.280
that kind of accommodate very different game situations, but also which come with very

23:31.280 --> 23:33.600
different difficulty levels.

23:33.600 --> 23:39.280
And maybe the key thing here is the easy ones, right, where you just have to score on

23:39.280 --> 23:43.920
easy, you can like, if you're on a single machine, you can get results with current algorithms

23:43.920 --> 23:50.560
within minutes, or let's say 10, 20 minutes instead of having to wait a full day and block

23:50.560 --> 23:56.000
a full machine before you can tweak your algorithm or check your implementation.

23:56.000 --> 24:00.240
So I think that's also like one of the key advantages of the environment, you can go

24:00.240 --> 24:05.560
and start with very simple cases and kind of see where does it break without always

24:05.560 --> 24:09.160
having to wait a very long time in between.

24:09.160 --> 24:14.040
You can also define your own scenarios, so there is a lot of flexibility and I'm pretty

24:14.040 --> 24:15.040
excited about it.

24:15.040 --> 24:21.920
I suspect this is going to be well beyond the scope of what you have looked at so far,

24:21.920 --> 24:28.840
certainly at this paper, but have you explored kind of the idea of transfer learning or

24:28.840 --> 24:33.720
curriculum learning, like training an agent on these academy scenarios and how that might

24:33.720 --> 24:37.320
impact performance on the game as a whole?

24:37.320 --> 24:44.200
Yes, so in the last few months, we've been very busy pushing out or kind of pushing out

24:44.200 --> 24:49.600
this environment and fixing bugs and running the experiments we added in this paper.

24:49.600 --> 24:53.880
It has certainly been on our mind and one of the motivations actually for the football

24:53.880 --> 24:59.240
academy was to enable curriculum learning.

24:59.240 --> 25:05.880
We didn't include results here, we kind of, in the paper we include benchmark results

25:05.880 --> 25:11.640
for the different academy scenarios to show how hard they are and this role, as I mentioned

25:11.640 --> 25:18.280
before, where you have a different variety of tasks, which are different difficulties,

25:18.280 --> 25:22.000
but you can use this to go and build your own curriculum, right, and you can see does

25:22.000 --> 25:29.880
thus fare better at actually playing the 11-versa-lem game than if I just start to play the 11-versa-lem game.

25:29.880 --> 25:36.120
You've talked about some of the benefits of your environment in terms of it being open-source

25:36.120 --> 25:42.040
and some of the features that you've built in, but I'm curious taking a step back are there,

25:43.560 --> 25:55.800
any thoughts in terms of some unique properties of football slash soccer as a task for reinforcement

25:55.800 --> 26:02.360
learning agents, there are a ton of RL simulation types of environments, everything from the tower

26:02.360 --> 26:11.480
challenges, things to very domain-specific to the more general, teach some agent to walk

26:11.480 --> 26:19.400
kind of environment, plus Atari games, why is football potentially interesting or more

26:19.400 --> 26:22.280
interesting than some of these many other tasks?

26:22.280 --> 26:31.080
Yes, so I think the key benefit of football is twofold, but the first one is I think the

26:31.080 --> 26:37.880
task as we have it now is challenging even for modern reinforcement learning algorithms.

26:37.880 --> 26:43.560
If you look at, for example, the hard scenario, even playing against the fixed rules-based AI

26:43.560 --> 26:52.600
with the general learning algorithm, we need to run a lot of several hundred millions of steps

26:52.600 --> 26:57.960
in a distributed setting where we have several hundred machines actually running this game, right?

26:57.960 --> 27:04.920
So I think having a challenging task is very important. Now, when you do research here,

27:04.920 --> 27:08.760
every research is kind of free how to choose the approach, but one of the hopes is that

27:08.760 --> 27:17.240
if you take a general learning algorithm, which is not specific to football, that it would also

27:17.240 --> 27:26.040
transfer to other under-settings, right? Now, in terms of how does it compare to different

27:26.040 --> 27:33.080
environments? I think the key part where I mentioned before is that we can adjust the difficulty,

27:33.080 --> 27:38.680
this allows you to do research on a variety of different difficulty levels. I think that's

27:38.680 --> 27:47.480
different to prior environments, such as Atari, which I guess now people would consider rather

27:47.480 --> 27:51.640
on the easy side. On the other hand, there has been reinforcement learning research on other

27:51.640 --> 27:59.480
video games, such as Starcraft 2 or Dota 2, which is rather under computationally expensive side.

27:59.480 --> 28:04.360
Here you get kind of everything, right? At the same time, it's also open source, so you don't

28:04.360 --> 28:12.840
need access to a potentially a closed source binary. The other part, and I think that's also one

28:12.840 --> 28:19.640
thing we are very excited about. There is essentially two extensions that we put into the environment,

28:19.640 --> 28:27.320
which is it allows to play multiplayer in a sense that football is inherently a multiplayer game

28:28.440 --> 28:33.480
where you play against an opponent, which brings a completely new dynamic when you have

28:33.480 --> 28:40.360
somebody else acting in the same environment with adversarial goals to you. Similarly, the

28:40.360 --> 28:47.560
environment supports actual research into multi-agents where you could think of every single player

28:47.560 --> 28:51.720
has to make their own decision on what it's going to do, and now you have two teams, right?

28:51.720 --> 28:56.440
And they have to start cooperating. Players within the team have to start cooperating

28:56.440 --> 29:00.920
with each other to play against somebody else. And I think this is just a very broad

29:00.920 --> 29:08.120
range of topics that you can investigate. Similarly, we built into the engine that you can go

29:08.120 --> 29:14.200
and train on different representations or different types of observations, so you can either go

29:14.200 --> 29:20.200
and train about pixels, which may be computation expensive, or we can go at other representations

29:20.200 --> 29:27.640
of the observations, such as the actual positions of the players, right? Not all the data is images,

29:27.640 --> 29:34.200
and you can, again, do something which you cannot do in other environments.

29:35.240 --> 29:41.880
So the representations can be, in addition to just looking at the images, you can get the

29:42.840 --> 29:50.040
kind of more inherent state. What about on the control side? What are the options that you have

29:50.040 --> 29:56.760
on control of the player with the ball? Yeah, so there has been other reinforcement learning

29:56.760 --> 30:05.480
environments, which are more, which are also doing football. This is DeepMind Soccer and Robocup,

30:05.480 --> 30:11.560
the 3D simulation. These are more on the side of doing continuous control, where it's really about

30:11.560 --> 30:18.840
low level control of either the robot in Robocup or the simulator robot in Robocup that you control,

30:18.840 --> 30:27.400
or essentially a kind of players in this DeepM Soccer, where you really have to specify how much

30:27.400 --> 30:32.520
do you turn, how much do you go forward and back, which is much more focused on low level control.

30:32.520 --> 30:37.720
Here, the idea is more, you want to learn how to do these more high level control,

30:37.720 --> 30:43.560
like things like strategy, tactics, become much more important, here you control.

30:43.560 --> 30:48.360
Okay, as you would with a gamepad, essentially you can move right, left, top, bottom,

30:48.360 --> 30:55.880
you can sprint, you can pass, you can try to press when you're in defense, and so on.

30:56.840 --> 31:01.000
Again, the environment is open-source, so this can be modified, right?

31:01.560 --> 31:07.720
So it sounds like the way that you control the character is one of the key distinction

31:07.720 --> 31:12.120
between this environment and some of the others that are out there for this game.

31:12.120 --> 31:19.160
Are there other distinctions? Yeah, I think that is one of them, it's in particular to with regards

31:19.160 --> 31:24.120
to the other environments that do football. That is, for me, the key difference, I think to the

31:24.120 --> 31:29.640
other ones, the actual tasks that you're doing, the fact that you can go from different types of

31:29.640 --> 31:36.360
observations, that you can adjust the difficulty, that is open-source, these are all differences.

31:36.360 --> 31:40.760
I think one of the points here is also, I think there's also, in reinforcement, always a certain

31:40.760 --> 31:46.040
investment required in going and using an environment, and what I like here is, in one

31:46.040 --> 31:50.120
environment, you can essentially do a lot of different types of research.

31:51.080 --> 31:57.320
Maybe share a little bit about where your group goes from here. This has been published,

31:58.040 --> 32:03.400
out on GitHub. When did it go live? When was the environment published and the paper published?

32:03.400 --> 32:14.600
Yeah, so we put this kind of right. We finished a sprint just before ICML, where we first pushed

32:14.600 --> 32:21.640
us to GitHub with an accompanying paper, which describes the environment. Since then, we had a

32:21.640 --> 32:27.000
demonstration at ICML, at the Google booth, where we showed this to people, we got a lot of feedback,

32:27.000 --> 32:33.960
we've worked on the environment actively. Since then, we added a lot of cool features that I'm

32:33.960 --> 32:39.160
excited about, this multiplayer feature, where you can do self-play, for example. We've added

32:40.040 --> 32:47.400
multi-agent support. We've done optimizations of the game engine. It runs now through

32:47.400 --> 32:53.080
ExFaster, and we've just recently updated the paper and put it on archive with more experiments,

32:53.080 --> 32:59.480
where we investigate the first experiments into three different research directions,

32:59.480 --> 33:05.480
which we think are very promising in this environment. One of them is what happens if you do

33:05.480 --> 33:09.400
self-play, what happens second is what happens if you do this multi-agent setting where you

33:09.400 --> 33:15.960
control more players, and third one is what happens if you use different representations of the

33:15.960 --> 33:24.920
actual environment or the observations that the agent interacts with. From this point,

33:24.920 --> 33:30.520
does your group continue to work on the environment itself? Or do you start to

33:32.040 --> 33:37.240
branch off into doing more experimentation work on the environment? Where do you see your work going

33:37.240 --> 33:45.320
with this? Yes. I think there is, as you said, two things that we want to do. The first thing is

33:45.320 --> 33:52.920
we want to make sure it is a useful research or enforcement learning environment. We are

33:52.920 --> 34:00.120
definitely going to continue adding features, make it easier to use the environment. I think

34:00.120 --> 34:05.960
the potential that it has, especially with the multiplayer component or where you have to compete

34:05.960 --> 34:11.480
against other agents, we are definitely looking into options how to make this more accessible.

34:11.480 --> 34:16.360
At the same time, we are also super excited to do research on this environment. Yes.

34:16.920 --> 34:22.520
And also, I think one thing that we should highlight here, this has been a big team effort

34:22.520 --> 34:29.880
in the team here. So it has been over 10 people that have worked actively on this project, right?

34:31.320 --> 34:35.720
Now, I think it's also maybe a time where different people are going to explore different things

34:35.720 --> 34:42.520
in this environment. And so, Olivia, maybe what are three things that you personally learned

34:42.520 --> 34:50.680
in working on this project? Yes. So for me, this has been quite a cool experience. I didn't really

34:50.680 --> 34:56.200
do reinforcement learning research before I came to this project. So for me, I learned a ton

34:56.200 --> 35:03.640
about all different environments that are out there, about different algorithms. And I also learned

35:03.640 --> 35:10.520
that this kind of funny anecdote that even now these algorithms that we have, they're very good

35:10.520 --> 35:17.640
at exploiting kind of loopholes in the game engine. And like one example that people in our team

35:17.640 --> 35:22.680
found and the photo is pretty funny is if you have 11 players playing football and trying to

35:22.680 --> 35:28.360
score against a single goalkeeper, what is kind of the best strategy to do? And turns out in

35:28.360 --> 35:34.440
the version that we have the game that we had at that time, what the agent learned is it would

35:34.440 --> 35:42.200
go take the ball, kick it outside of the playing field so that the goalkeeper has to go to a throw-in

35:42.200 --> 35:45.880
where he doesn't have any teammates, so he's going to throw it somewhere where the other players

35:45.880 --> 35:51.240
have it, and now the players will score. And I think that really it is a pretty cool

35:52.360 --> 35:56.040
example that we found. How long did it take the agent to figure that out?

35:56.040 --> 36:01.480
I don't remember the details, but it's kind of like once it found this, it's a super smart strategy,

36:01.480 --> 36:06.280
right? And it's really exploiting kind of a loophole. We had a similar one where essentially when

36:06.280 --> 36:11.560
you train these agents in the beginning, the opposing team scores a lot and you're kind of learning

36:11.560 --> 36:19.960
not to admit any goals, right? And one thing I learned is for example that once the opposing

36:19.960 --> 36:24.840
player, for example, kicks the ball besides the goal, your goalkeeper when he's doing the kickoff,

36:24.840 --> 36:28.920
he can just wait indefinitely because the engine didn't include any mechanism to

36:28.920 --> 36:33.960
pray with that, and that is a good way to minimize the opponent's goaling goals if the game doesn't

36:33.960 --> 36:40.520
continue. I think these things are super funny, and I think it's also highlights one of the things

36:40.520 --> 36:48.200
which we didn't really talk about yet, but which is that doing research in this football environment

36:48.200 --> 36:53.800
also makes that research to some degree more accessible, right? A lot of people they care about

36:53.800 --> 37:01.720
football, they like watching football is very intuitive, and that's why I also, we hope that

37:01.720 --> 37:07.240
actually having a football environment also helps with actually people maybe demystifying a bit

37:07.240 --> 37:12.120
what is happening in this research so that people can see, oh, you know, this is a random agent

37:12.120 --> 37:17.080
that's doing this, right? And then suddenly it starts learning how not to admit goals, and then

37:17.080 --> 37:23.160
you see how it learns to score. I think people can very much identify with it, students that

37:23.160 --> 37:28.840
are doing master programs that want to learn about reinforcement learning. It's super cool,

37:28.840 --> 37:35.480
you can go with the environment, learn how to pass or kind of score against an empty goal,

37:35.480 --> 37:40.440
then two versus one, and so on. So I think that's maybe one of the benefits that we didn't really

37:40.440 --> 37:45.640
talk about, which is that it is a very accessible environment, right? You don't like like a lot of

37:45.640 --> 37:52.360
people know about football. And of course, you just recently published this, but are you aware of any

37:52.360 --> 38:01.720
efforts or plans to incorporate this into kind of learning curricula? So deep learning and reinforcement

38:01.720 --> 38:07.560
is very fast-moving field. So we chose the approach of releasing the environment and publish,

38:07.560 --> 38:14.280
putting online the paper, so this has not been published yet. It's on the web, everybody can

38:14.280 --> 38:19.640
use it, but we're definitely trying to publish this, but it has not been published yet. Got it. And

38:19.640 --> 38:28.600
the second question for curriculum learning, we did some very initial experiments. To be

38:28.600 --> 38:38.600
clear, more talking about DC application, well, you spoke to seeing applications of this environment

38:38.600 --> 38:46.200
in the education of humans and growing their own knowledge of reinforcement learning,

38:46.200 --> 38:51.880
and I was just curious if you were aware of any plans to formalize that and maybe build it into

38:53.240 --> 38:59.880
some online course or some school-based course to teach students about reinforcement learning

38:59.880 --> 39:05.800
using this metaphor that they're very familiar with. So I'm not aware of this, but I definitely

39:05.800 --> 39:11.960
hope that this happens. We've seen quite a lot of positive feedbacks on things like Twitter,

39:11.960 --> 39:17.320
and where people got very excited about that. So I definitely hope that helps people access

39:17.320 --> 39:25.240
research. Again, also given that it's open-source, it's very easy to use or to get started. If people

39:25.240 --> 39:30.040
are interested in them, even if your listeners are interested in, we go to our GitHub page,

39:30.680 --> 39:37.880
GitHub.com Google-research-football. It's very easy to get started. You can install the

39:37.880 --> 39:43.080
environment, there is the examples, which show you how to train a basic agent. So I'm all for

39:43.080 --> 39:50.280
making it as accessible as possible. You talked about the agents that you've played with finding

39:50.280 --> 39:57.160
these corner cases to exploit the environment. How do you address that? It strikes me that in some

39:57.160 --> 40:06.600
cases, the solution of that is maybe putting in more constraints that are like the human game of

40:06.600 --> 40:11.320
football or better modeling that environment. In other cases, you might need to do something

40:11.320 --> 40:20.680
that doesn't make sense in the context of the actual sport. It creates this unique computer

40:20.680 --> 40:27.960
football sport. Does that resonate at all? How have you addressed those kinds of corner case

40:27.960 --> 40:35.400
issues? We ran a lot of experiments. As you said, it is actually pretty hard to build a good

40:35.400 --> 40:41.320
reinforcement learning environment. I think now we are pretty confident that it works pretty well,

40:42.440 --> 40:49.080
but that doesn't exclude that you go, come with a new learning algorithm. It's really good.

40:49.080 --> 40:55.640
It finds another corner case. I think we then have to address this on a case-by-case basis.

40:55.640 --> 41:02.760
I think it also boils back to the underlying motivation. I think the motivation is not

41:02.760 --> 41:10.840
necessarily playing as well as possible in this environment. It's a primary goal, but the goal

41:10.840 --> 41:15.400
is to do research. If there are things that hind the research in this environment,

41:16.360 --> 41:22.280
then I guess we have to change it. On the other hand, I think there's also the general feeling

41:22.280 --> 41:28.280
our team. There is a benefit to having stability on the environment side, so that research and

41:28.280 --> 41:35.480
results stay comparable across different papers. We definitely don't want to change the

41:35.480 --> 41:40.760
environment too often, but if there is very obvious payload cases, which you can very exclude,

41:40.760 --> 41:47.400
then we might have to do this. It sounds like then that for the most part, these exploitable

41:47.400 --> 41:55.400
corner cases, you've considered them as bugs as opposed to inherent qualities of the game

41:55.400 --> 42:04.920
that you're trying to model. Does that make sense? Yes. I think the example of the goalkeeper

42:04.920 --> 42:08.760
having to go during the throw-in that really defeats the purpose of these scenarios we've

42:08.760 --> 42:14.200
defined. Similarly, the example that I gave where the goal doesn't kick it away. If you do this

42:14.200 --> 42:24.120
in a real game, you would get a delay of game penalty. Something would happen. I think that

42:24.120 --> 42:30.840
really shows, maybe this was really a bug in the implementation. In the other hand,

42:30.840 --> 42:35.960
it also boils back to whenever you're building an environment like this. You have to take some

42:35.960 --> 42:41.640
choices. We've done this with the best faith effort, and we think now it is ready to be used

42:41.640 --> 42:47.960
for research. Olivier, thanks so much for taking the time to share this project with us.

42:47.960 --> 42:58.120
Yeah, thanks for having me here. All right everyone, that's our show for today. If you like what

42:58.120 --> 43:03.160
you've heard, please do us a favor and tell your friends about the show, and if you haven't already

43:03.160 --> 43:08.280
hit that subscribe button yourself, make sure you do so you don't miss any of the great episodes we've

43:08.280 --> 43:21.800
got in store for you. As always, thanks so much for listening, and catch you next time.

