1
00:00:00,000 --> 00:00:23,000
All right, everyone. I am here with Oxshot call. Oxshot is the head of data and machine learning at Redfin, where he oversees the data engineering data platforms and ML teams.

2
00:00:23,000 --> 00:00:26,000
Oxshot, you've been there since 2016.

3
00:00:26,000 --> 00:00:33,000
Yeah, that's correct. Awesome, awesome. Well, welcome to the podcast. I'm looking forward to digging into our chat.

4
00:00:33,000 --> 00:00:36,000
Yeah, absolutely. Thank you so much for inviting me.

5
00:00:36,000 --> 00:00:52,000
Yeah, I know a bunch of folks at Redfin. I worked at a company called Plum Tree previously. So I know Bridget and Glenn and I imagine a bunch of other plum tree vians are still at the company.

6
00:00:52,000 --> 00:01:00,000
So I'm really looking forward to digging into how machine learning is used at Redfin.

7
00:01:00,000 --> 00:01:07,000
Can you start us off by maybe giving us a little bit of history of ML at the company?

8
00:01:07,000 --> 00:01:22,000
Sure, yeah, absolutely. So I can talk a little bit about Redfin first. Redfin is a real estate brokerage founded by software developers and we use technology to make everything about buying or selling a house better, more efficient.

9
00:01:22,000 --> 00:01:27,000
We've saved our customers a lot of money, like more than a billion dollars in fees over the years.

10
00:01:27,000 --> 00:01:37,000
And we have tons of really unique and interesting data about real estate transactions and people who are interested in buying or selling homes.

11
00:01:37,000 --> 00:01:41,000
And we rely quite heavily on AI in many parts of the business.

12
00:01:41,000 --> 00:01:47,000
So there are tons of use cases that we have been investing in machine learning for quite some time.

13
00:01:47,000 --> 00:02:00,000
It's like the redfin estimate things like redfin recommendations. There are other use cases also like hot homes. So yeah, we have unique data. We have interesting data and we have some really interesting use cases.

14
00:02:00,000 --> 00:02:04,000
How long is the machine learning team been in place at the company?

15
00:02:04,000 --> 00:02:14,000
I think it's been it's been there like longer than I have. So I was I joined in 2016 and I think machine learning team existed probably like two years before that.

16
00:02:14,000 --> 00:02:17,000
Almost seven, eight years now, I think.

17
00:02:17,000 --> 00:02:25,000
Okay. And since you've been leading the team, you tell us a little bit about the evolution of ML there.

18
00:02:25,000 --> 00:02:38,000
Yeah, absolutely. So I think when we started like when I initially joined the team, we basically had this model where there was one team sort of one consolidated team that did applied machine learning in the company.

19
00:02:38,000 --> 00:02:52,000
Machine learning use cases would be built out by this team. It's a team of really special people. It's a team of really smart engineers. They are you know data scientists and engineers rolled into one. So they are machine learning engineers.

20
00:02:52,000 --> 00:03:02,000
And they were not just building machine learning use cases and applications like the redfin estimate, for example, they were also building the underlying infrastructure.

21
00:03:02,000 --> 00:03:12,000
So the model was that you would give somebody a problem and they would go in go in and solve it, developing the infrastructure along the way.

22
00:03:12,000 --> 00:03:16,000
And this work, you know, reasonably well for the company for quite some time.

23
00:03:16,000 --> 00:03:24,000
But we had come up to the point where this was not really scaling. So we had gotten to the place where redfin was involved in multiple businesses.

24
00:03:24,000 --> 00:03:34,000
It was not just a brokerage. We also had an instant buying business. We have a market business. So we have all these tons of you know use cases across different domains.

25
00:03:34,000 --> 00:03:41,000
And so it doesn't didn't make quite so much sense for just one team to be sort of the choke point of machine learning at the company.

26
00:03:41,000 --> 00:04:00,000
So what we've been trying to do more recently is really develop that infrastructure, make it standardized, make it easy to use and really democratize machine learning within the company and allow people teams across the business across different domains to hire people who have that machine learning talent or to grow that talent.

27
00:04:00,000 --> 00:04:16,000
And then use the platform that this team has built to you know tackle use cases in different domains. So that's that's what we are trying to get to where the best practices are established and there is a standardized platform that everyone can use.

28
00:04:16,000 --> 00:04:20,000
But it's not just one team that's actually building the applications.

29
00:04:20,000 --> 00:04:38,000
Got it got it. So then your team is providing that kind of tooling and infrastructure and the individual application teams or business teams have the data scientists and the machine learning engineers that are actually building the models.

30
00:04:38,000 --> 00:04:56,000
Yeah, it's it's so certain applications are still owned by this sort of centralized team. So the redfin estimate is an example, redfin recommendations is an example. So there are certain use cases which sort of cut across multiple domains and multiple businesses, which is you know handled by this team.

31
00:04:56,000 --> 00:05:06,000
But the platform itself is available for anybody to use within the company and we are encouraging people to build use cases using the platform.

32
00:05:06,000 --> 00:05:16,000
Awesome, awesome. Well, let's maybe talk about the two central use cases to get started redfin estimate.

33
00:05:16,000 --> 00:05:18,000
Tell us about that.

34
00:05:18,000 --> 00:05:33,000
Yeah, absolutely. So the redfin estimate is a machine learning driven calculation of the market value of any given home. And we have the redfin estimate for pretty much every home that's both on market and off market within the US.

35
00:05:33,000 --> 00:05:41,000
The redfin estimate factors in more than 500 data points about the market, the neighborhood, the home.

36
00:05:41,000 --> 00:05:47,000
And you know, other things about so things like how many bedrooms and bathrooms it has.

37
00:05:47,000 --> 00:05:55,000
What size the home is as well as like broader market data about how home prices have been trending in the county and the state.

38
00:05:55,000 --> 00:06:06,000
It looks at, you know, what comparable homes in that neighborhood have sold for recently. So it puts all of this together to come up with a value for what a particular home should be worth.

39
00:06:06,000 --> 00:06:16,000
And yeah, it's been, you know, it's been a really tremendous tool for redfin. So traditionally, it was something that we were using mainly as a growth driver.

40
00:06:16,000 --> 00:06:28,000
So people were really interested in knowing the value of their home as you can imagine. It's one of the biggest assets that most people own. And so they would come to redfin to see what the value of their home was.

41
00:06:28,000 --> 00:06:37,000
But more recently as redfin has gotten into the instant buying business called redfin now, where you know redfin actually buys homes and makes improvements and then sells them.

42
00:06:37,000 --> 00:06:48,000
So that business is also heavily reliant on the redfin estimate. And so redfin estimate has started to have a significant impact on both our user growth and also our revenue.

43
00:06:48,000 --> 00:06:55,000
You mentioned that 500 data points kind of go into this calculation.

44
00:06:55,000 --> 00:07:13,000
So tell us a little bit about the data sources that those data points come from. Imagine the MLS is one major source of data for you. But I can't think of 500 points of data in the MLS. So there's got to be others.

45
00:07:13,000 --> 00:07:25,000
The MLS has just a tremendous amount of data actually. The MLS is the multiple listing service, which is, you know, the database of of all homes which are coming on the market.

46
00:07:25,000 --> 00:07:34,000
There's also properties data so that like there are county records which have data about what a particular home sold for the history of the price of that home.

47
00:07:34,000 --> 00:07:51,000
And it has information about, you know, the MLS has information about amenities that that home has access to. There are other sources of amenities data that we use like, you know, how close is the closest Starbucks to a particular home, for example.

48
00:07:51,000 --> 00:08:05,000
But are there accessible recreation centers that are close to the home. So things like that also factor in then we have sources of all kinds of geographical data so that we have, you know, things like flood zones, for example.

49
00:08:05,000 --> 00:08:16,000
And of course, we also have tons of data about, you know, user engagement. So redfin.com is one of the biggest is actually the biggest brokerage site in the US.

50
00:08:16,000 --> 00:08:29,000
So we get more traffic than any other brokerage site. And so we also have data about what areas people are actually looking at for buying home. So we know what the competitive landscape is in a particular area.

51
00:08:29,000 --> 00:08:40,000
So that also factors into, you know, one of those, some of those 500 data points that we have about a particular home as we make the estimate.

52
00:08:40,000 --> 00:08:55,000
I think it's probably pretty common for us to think of in the case of like a Google search that the user interactions with the site kind of informed, you know, which popular, which content is most relevant.

53
00:08:55,000 --> 00:09:09,000
And that those would shift the ranking. I don't know that I would have guessed that the user interactions with a site like redfin would impact, you know, my home value on that site or the estimated home value on that site.

54
00:09:09,000 --> 00:09:20,000
It's I guess it's kind of prompting questions about like how do you monitor for abuse of that if people can kind of, you know, send bots to the site and manipulate home values.

55
00:09:20,000 --> 00:09:22,000
Is that is that a thing that you think about?

56
00:09:22,000 --> 00:09:30,000
Yeah, certainly something that we think about and we have, you know, lots of mechanisms in place that prevent some of that bot activities.

57
00:09:30,000 --> 00:09:46,000
So, you know, our website has ways to detect it. We have ways in which we, we would, you know, for example, we would prevent somebody who's coming in from, you know, AWS, for example, or like a VPN using a VPN.

58
00:09:46,000 --> 00:09:53,000
We would be able to detect that this is a machine instead of like this traffic is coming from machine instead of a person, potentially.

59
00:09:53,000 --> 00:10:03,000
And we have means of eliminating that data, but you know, it's, it's only one factor in a lot of different factors like this, this traffic data, the impact it has on the, on the price of a home.

60
00:10:03,000 --> 00:10:22,000
It's, there's also, you know, like after a home has been on the market for a certain amount of time, even if it's getting tons of traffic, like it is likely that the competitive landscape for that particular home is not like, you know, at the price at which it is listed is maybe, you know, maybe not the right price.

61
00:10:22,000 --> 00:10:29,000
So we, we can take that into account as well to make sure that the right for an estimate remains accurate for listed homes.

62
00:10:29,000 --> 00:10:31,000
Yeah, yeah, no, I think that's a great point.

63
00:10:31,000 --> 00:10:44,000
The ability to use machine learning to incorporate a ton of data points, you know, potentially reduces the sensitivity to an individual data point.

64
00:10:44,000 --> 00:10:56,000
I think probably a lot of folks, you know, listening to this interview have kind of done the, the, you know, build a home price estimate, you know, yeah.

65
00:10:56,000 --> 00:11:03,000
Build a home price estimate model based on the Kings County data, like that was a classic for a really long time.

66
00:11:03,000 --> 00:11:19,000
Yeah, where you're looking at like the square footage and the number of bedrooms and bathrooms, but, you know, we're well beyond that, can you talk a little bit about the types of models that you are using to produce the estimate.

67
00:11:19,000 --> 00:11:26,000
Is it, you know, deep learning, for example, versus, you know, more traditional types of models.

68
00:11:26,000 --> 00:11:35,000
Yeah, I think it's, we use a combination of like classical models and deep learning models, you know, for various things that we do in the business.

69
00:11:35,000 --> 00:11:46,000
For the redfin estimate, like we have models which predict off market homes, we have other models which are predicting the price of an on market home, a listed home.

70
00:11:46,000 --> 00:12:03,000
And as you can imagine, the, the ones for the listed home, it's more of a real time model, the, so, so yeah, we use a combination of many different techniques, you know, random forest, we use a great boosting, we use deep learning in some cases.

71
00:12:03,000 --> 00:12:13,000
So yeah, it's a, it's a fairly complex problem, it's a fairly complex model and we use a multitude of techniques to make the estimate as accurate as we can.

72
00:12:13,000 --> 00:12:31,000
And one of the things I've seen in this space is kind of a heavy use of a hierarchical or ensemble types of models, like you have a bunch of data points that, you know, for example, predict something akin to a walk score and then you feed that into a high level model, are you doing that kind of thing as well.

73
00:12:31,000 --> 00:12:55,000
Yeah, we do have, we do have ensemble models as well as a part of the overall picture, we get data from tons of different sources, we have, we have like, you know, we also have multiple estimates that we, you know, combine sometimes to get to the perfect value for different use cases again.

74
00:12:55,000 --> 00:13:01,000
We certainly, you know, have a bunch of different techniques that we use across across the business.

75
00:13:01,000 --> 00:13:08,000
And then beyond the estimate, the other model that you mentioned was the other big one is home recommendations recommendations.

76
00:13:08,000 --> 00:13:09,000
Okay.

77
00:13:09,000 --> 00:13:18,000
And that's the recommendation that user would see in their feed or other homes you might like when they're viewing a home on the site.

78
00:13:18,000 --> 00:13:32,000
Yeah, absolutely. So we both have a feed on redfin.com. So the first page that most people see is a, is a feed that is a list of recommended homes that redfin thinks that they might be interested in.

79
00:13:32,000 --> 00:13:39,000
We also send out emails and push notifications about homes that redfin thinks somebody might be interested in.

80
00:13:39,000 --> 00:13:52,000
And the home recommendations tool is like a really valuable and popular feature for people who are searching for homes on redfin. And this is, it's really tremendously helped us grow our business.

81
00:13:52,000 --> 00:14:13,000
Just just to, you know, throw a few numbers out there. When, when I joined redfin initially like six five years ago, red recommendations was driving about eight percent of our overall traffic. And it is now close to a quarter of our traffic, even while our overall traffic has grown by, you know, three times of what it was five years ago.

82
00:14:13,000 --> 00:14:32,000
So that, that gives you a sense of just the importance of recommendations to the business. Also some really interesting things we've seen is that are that people are more likely to click on homes that we recommend to them, then they are on homes that match their own saved search.

83
00:14:32,000 --> 00:14:43,000
It's like the recommendation system is actually better at predicting what homes people will click on and what homes people will tour than saved searches.

84
00:14:43,000 --> 00:14:51,000
So save searches, of course, like when somebody goes to redfin.com and tells us this is the price range I'm interested in. This is the area I'm interested in.

85
00:14:51,000 --> 00:15:08,000
But yeah, recommendations work even better than that. So it's just really amazing to see. And it's come to the point where now people expect us to know what they're interested in without them having to tell us their preferences explicitly, which is just a change in consumer behavior.

86
00:15:08,000 --> 00:15:25,000
So it's people are they just they just want you to have a recommendation system that's that that works very well for them. And also one thing we've noticed is that recommendation systems, at least in the interactions that we've seen, they keep getting better as people engage more with them.

87
00:15:25,000 --> 00:15:33,000
So the earlier you get started as a business on setting something up is the more value you get out of it in the long term.

88
00:15:33,000 --> 00:15:55,000
One thing I've noticed this is not something that redfin does is something that one of your competitors does is that the recommendations seem to promote a price creep like a budget creep like you identify, you know, you might say in your search that, you know, you're interested in homes within a certain price range.

89
00:15:55,000 --> 00:16:22,000
And it starts showing you those and then gradually it starts showing you more and more expensive homes. I'm wondering, you know, with that as kind of a general example of, you know, I don't know, responsible AI or maybe a broader question is in what ways does responsible AI play into the way you think about models at redfin.

90
00:16:22,000 --> 00:16:26,000
And how do you approach the topic?

91
00:16:26,000 --> 00:16:37,000
Yeah, I think that's a really, really good question. We, I think I think what you're what you're talking about here is also how does the market itself get defined by these algorithms, right?

92
00:16:37,000 --> 00:16:45,000
So if we start to recommend homes in a particular neighborhood, do those homes become more valuable and things like that.

93
00:16:45,000 --> 00:17:03,000
So I think that's that's certainly a really interesting topic and how we, how we generally approach this, like, you know, the idea of building models which don't have biases or building models which don't, you know, create biases in the market.

94
00:17:03,000 --> 00:17:24,000
We are very careful about what data we add to the models. So we don't want to, you know, whatever the model learns based on what data you give to it. And so what we try to do is we try to explicitly not put certain types of data into both our recommendations models and our estimate models.

95
00:17:24,000 --> 00:17:36,000
So for example, crime data, it's not something that we would put into the model because we, you know, we believe that it might include might create some inherent biases in the data, which we don't want it to have.

96
00:17:36,000 --> 00:17:45,000
And as far as like price creep is concerned, like sometimes there is always an element of exploration and exploitation when you're doing machine learning.

97
00:17:45,000 --> 00:18:07,000
So it's possible that, you know, I don't want to speak to what my competitors are competitors are doing first, but there might be some element of OK, we'll for, you know, X percent of times we might suggest a home that is outside the price range that a customer might have specified just to see if that is actually interesting to the customer to the user.

98
00:18:07,000 --> 00:18:28,000
And that way the model learns about their preferences. And sometimes, you know, certainly for me, I can tell you my own story like the region, the area that I was looking at to buy a home myself was I was looking in the Seattle area and like there were this and my main criteria in my mind was like, can I commute this was

99
00:18:28,000 --> 00:18:44,000
pandemic, so can I commute to work. And I didn't even know that there were these areas where I could actually commute to work outside of Seattle, but I could actually commute to work within the same time frame that I had in my mind.

100
00:18:44,000 --> 00:19:04,000
And the recommendation system was able to figure that out and, you know, give me make suggestions. And so I ended up buying a home in the greater Seattle area as opposed to within Seattle just because, you know, my own preferences did not, you know, taking to account this additional information that was outside of my view.

101
00:19:04,000 --> 00:19:20,000
I'm curious the degree and the ways in which the pandemic impacted machine learning at Redfinn clearly impacted the real estate market clearly impacted the company as a whole, how about from an ML perspective.

102
00:19:20,000 --> 00:19:40,000
Absolutely, so this is a really interesting topic, the real estate market as a whole was impacted in really, at least for us like unpredictable ways, you know, it was not something that people expected to see that the price growth would happen at such a rapid base.

103
00:19:40,000 --> 00:19:58,000
So just to give you some background on the broader market, we have seen earlier this year in May of this year, 26% growth in price, home prices across the country year over year, over the, like in certain markets, it's it was as high as 40% year over year growth.

104
00:19:58,000 --> 00:20:12,000
And even in August of this year, it was still about 16% year over year nationwide and 60% of homes were being sold, you know, within two weeks of being listed, so it was a really competitive market.

105
00:20:12,000 --> 00:20:28,000
And also, we saw that one really interesting thing that we saw was that 63% of home buyers were making offers on homes, even without stepping foot inside them. This is a complete like change from how things were before the pandemic.

106
00:20:28,000 --> 00:20:44,000
And as you can imagine, this made it really important for for us to have an experience on the website where people could get the get the sense of what it was like to be inside a home without physically being inside a home.

107
00:20:44,000 --> 00:20:58,000
So we already read from already showcased 3D scans on the website quite prominently. And we already created 3D scans for every read from this thing, but to really help our customers understand the physical layout of homes, even better.

108
00:20:58,000 --> 00:21:06,000
We started automatically tagging all floor plan images using computer vision. So that was one very clear connection to machine learning.

109
00:21:06,000 --> 00:21:20,000
We invested quite heavily, we started generating a lot of image tags and generally starting to come up with ways in which we could extract unstructured data and presented on the website in a structured form.

110
00:21:20,000 --> 00:21:24,000
And that was a big investment that we made.

111
00:21:24,000 --> 00:21:42,000
Another really interesting thing that we saw was that because the market was so crazily competitive, you had to like if you were really interested in buying a home, you had to get as early as possible. So you wanted to be one of the first few people who put in their beds on a home.

112
00:21:42,000 --> 00:21:59,000
And in order to identify homes, which are likely to sell really quickly, we have this feature called hot homes on redfin, which is very much a machine learning driven feature. So certainly, you know, speaks to the topic of machine learning and the pandemic.

113
00:21:59,000 --> 00:22:16,000
The hot home feature, as you can imagine, it takes into account things like user interaction, but user interaction takes a little bit of time. So a home comes on the market, we observe how many people are clicking on it, touring it, things like that for a couple of days.

114
00:22:16,000 --> 00:22:23,000
And then the hot homes would tag a home as a hot home, the hot homes model would tag the home as a hot home.

115
00:22:23,000 --> 00:22:36,000
What we wanted to do was to make it even faster. So we wanted to have a model that would tag a home as a hot home as soon as it listed. So even, you know, without having that user interaction data, we wanted to be able to tag it.

116
00:22:36,000 --> 00:22:56,000
We wanted to build a model, which is days on market model. So it's like it predicts how many days on, like a particular home is likely to stay on market based on its price based on the home characteristics based on the neighborhood, things like that. And then we reused that days on market model to start predicting hot homes early.

117
00:22:56,000 --> 00:23:12,000
So we called it the early hot homes model and it as soon as a home lists in many markets now across the nation, we are able to tag a home as a hot home and give redfin users an edge in, you know, being able to get in early and start bidding on on homes.

118
00:23:12,000 --> 00:23:33,000
So we added to this earlier, the impact of, you know, redfin on the broader market. And I'm wondering, you know, the extent to which and how you might go about studying the impact of identifying a home as a hot home on the market.

119
00:23:33,000 --> 00:23:53,000
We definitely, we like the model is also trying always to, you know, be as accurate as possible, like we look at, you know, precision recall at the time when we are training the model, but we also keep an eye on precision and recall, like we have, we have ways of measuring error for these kinds of things.

120
00:23:53,000 --> 00:24:20,000
So if we predict a home is a hot home and it doesn't actually sell within 14 days, then we want to make sure that our model knows about that and we want to, you know, train our model, retain our model with that new data. So we are all of our models, including the estimate, we are measuring the error that we see compared to the real world results of, you know, what, what actually happens to that home.

121
00:24:20,000 --> 00:24:40,000
And the pandemic hit to, to what extent was, you know, the your response to that, you know, either, hey, we always retrain our models, you know, on a periodic basis, weekly, whatever, and they just will automatically adjust versus, you know, you just talked about the error.

122
00:24:40,000 --> 00:24:53,000
You know, you're monitoring the error, those, you know, the pandemic through the errors out of bounds, and then you selectively retrain versus just kind of more of a top down, hey, the world is upside down, we need to look at everything differently.

123
00:24:53,000 --> 00:25:10,000
Yeah, yeah, so I think it was a combination of both those things. So as you said, as you said, you know, we do retrain our models on a regular basis, but we found that because the market was upside down and crazy, like completely crazy over the last 18 months.

124
00:25:10,000 --> 00:25:27,000
We did decide to selectively retrain certain models on a greater frequency, even though, you know, retraining models is a costly thing, so it adds additional operational cost, but we did decide that retraining our models more, particularly the red finestimate more frequently

125
00:25:27,000 --> 00:25:47,000
was required to keep up with the dynamic fast-paced market that we have been experiencing through the pandemic. So I think that's what I was eruding to in terms of the effect on the red finestimate, like we have been retraining it much more frequently, and we have seen significant improvements in accuracy based on based on that change.

126
00:25:47,000 --> 00:26:14,000
And going back to our conversation about models and maybe tying into the previous question about the hot homes feature to what degree have you explored causal models to identify or take into account, you know, linkages between things you do on the site and the impacts they have in market.

127
00:26:14,000 --> 00:26:22,000
Yeah, I think we have been looking into into causal models. It's a it's a hot topic in machine learning right now.

128
00:26:22,000 --> 00:26:34,000
I think we are exploring some ways in which they can be used to improve the accuracy of our hot homes predictions.

129
00:26:34,000 --> 00:26:46,000
So it's still like it's an ongoing effort like we are we are always trying to we are always, you know, working on improving the models that we have in production and that is one of the areas we're exploring.

130
00:26:46,000 --> 00:26:54,000
So you mentioned the there's maybe some interesting background and the pandemic impact on the estimate.

131
00:26:54,000 --> 00:27:21,000
Yeah, I think we talked a little bit about it just now where we have, you know, we started retraining our models more frequently. We also saw that, you know, certain certain certain regions had a significant certain or ad actions is regions, I should say certain types of homes had, you know, more of of of a change in price compared to other.

132
00:27:21,000 --> 00:27:41,000
For example, single family homes became a lot more valuable compared to, you know, multi family homes or condos or, you know, places where people were scared of, you know, being being surrounded by other people, which, you know, the pandemic tends to bring out that kind of behavior.

133
00:27:41,000 --> 00:28:01,000
We saw that because the home, the types of home that sort of the type of the home has such a significant impact, we started looking into other types of features that would, you know, focus more on home like on the characteristics of a home.

134
00:28:01,000 --> 00:28:16,000
We saw that the waiting of some of those features started to get more and more during the pandemic in the model. So that was again, something more of an observation, I think, then something that we, we had to step in and change.

135
00:28:16,000 --> 00:28:26,000
But that was another thing that we found was interesting like it was the pandemic was revealing certain preferences, which are different from how they were before the pandemic began.

136
00:28:26,000 --> 00:28:40,000
I'm curious about some of the, some of the smaller use cases at the company that are more driven by the specific teams as opposed to these two larger ones, what are some examples of those.

137
00:28:40,000 --> 00:29:00,000
There's a new team that we have had for I think like the last year and a half, two years, and it's called the document intelligence team. It's a separate team from the main applied machine learning team, but they are focusing very almost exclusively on how to extract information from real estate transaction documents.

138
00:29:00,000 --> 00:29:23,000
So we have tons of this really interesting data that very few other companies have because we are a part of the transaction, the real estate transaction, and we're part of like tons of real estate transaction. So we have lots of deals happening, we have a lot of PDFs, we have a lot of this, you know, unstructured data sitting in these in these files.

139
00:29:23,000 --> 00:29:37,000
We wanted to have a mechanism for extracting useful information from those files, and we also wanted to have, you know, build tools that would help our real estate agents, you know, deal with these documents in a really efficient way.

140
00:29:37,000 --> 00:30:06,000
So we created this like a bunch of these tools that would do a CR on these documents that would extract useful information from these documents that would, you know, perform all kinds of checks to make sure that the data that was in those in those forms was accurate, because if that data is not accurate, the transaction, the deal can fall through and, you know, that or get delayed and that's not ideal for any, for any customer who's, you know, in the process of spending sometimes, you know, millions of dollars.

141
00:30:06,000 --> 00:30:25,000
So that's been one really interesting use case, another really interesting use case has been the, there is this, there are all these tools that we build for, for our agents, which help them become more efficient.

142
00:30:25,000 --> 00:30:41,000
And there is one particular tool, it's called the comparative market analysis. So that's a smaller use case, I guess, compared to these other two big ones, but that's also a really valuable use case for the agents who actually, you know, need to work with with customers directly.

143
00:30:41,000 --> 00:31:00,000
When a, when a listing agent is talking to a person who wants to sell their home, after they finish talking to them, the next step is usually that they would write up a comparative market analysis, which is a document that describes these are the other homes in your neighborhood, this is how much they have sold for

144
00:31:00,000 --> 00:31:18,000
this is, this is what the landscape looks like. So this is what you should price your home at. And that document, it's called the CMA. So agents typically spend, you know, couple of hours on building out that CMA and they share it with the customer after they have that initial consultation.

145
00:31:18,000 --> 00:31:31,000
But because we have the, the, like this investment in machine learning at, and we are able to generate that CMA automatically, even before a real estate agent actually talks to a customer.

146
00:31:31,000 --> 00:31:42,000
So that gives them an advantage that saves them a lot of time and that again, you know, brings in a lot, lot of efficiency into the whole business of running a brokerage.

147
00:31:42,000 --> 00:31:56,000
It's the degree to which you've explored language models for text generation, I can imagine, you know, creating listings, things like that.

148
00:31:56,000 --> 00:32:02,000
Is that something that you've looked into much, or are you, are you using it anywhere in the business?

149
00:32:02,000 --> 00:32:16,000
I think we are, we are not using generative models so much in the business right now. One area where I'm a little bit of a, of a skeptic, I guess, is, is conversational AI.

150
00:32:16,000 --> 00:32:23,000
So I think that's one of the places where a lot of people do get excited about using generative models.

151
00:32:23,000 --> 00:32:33,000
Like I, I, I think like we are not really at that place yet where conversational AI is good enough to be included in high value transactions like real estate.

152
00:32:33,000 --> 00:32:41,000
So I don't think I would want to, like if I was buying or selling a house, I don't think I want to talk to a chatbot agent, for example.

153
00:32:41,000 --> 00:32:56,000
So, so what we do instead of, you know, building chatbots for customer service or for, you know, real estate agents, we use, we, we instead focus on improving the efficiency of humans in the loop.

154
00:32:56,000 --> 00:33:11,000
We increase the, we build tools for them to be able to do their work more efficiently, we build tools for them to be able to explain how the right for an estimate works for example.

155
00:33:11,000 --> 00:33:20,000
So we, like there's accuracy and there's explainability and sometimes those two things are at odds right so we have tools for helping explain, you know, what goes into the estimate.

156
00:33:20,000 --> 00:33:29,000
And if somebody comes and tells us like, you know, cause our customer service agent and asks, why is the, why is my home priced at this price.

157
00:33:29,000 --> 00:33:33,000
Our customer service folks can tell them these are all the inputs that go into the model.

158
00:33:33,000 --> 00:33:40,000
And if any of this data is that Edwin has about your home is inaccurate, you can tell us and we can, we can make make a correction.

159
00:33:40,000 --> 00:33:58,000
So that's mostly been the focus of our machine learning investments as opposed to conversationally. I want to maybe shift gears and talk a little bit about the platform and tooling that you've set up to, to support modeling across the business.

160
00:33:58,000 --> 00:34:08,000
Can you give us a broad brush landscape of the way that you think about ML ops and tooling in general.

161
00:34:08,000 --> 00:34:23,000
Yeah, absolutely. So as I said, we have made recently a big investment in, you know, building machine learning tooling, making a really easy to use machine learning platform that people across the business can use.

162
00:34:23,000 --> 00:34:38,000
Even if they don't have already have like a big deal of expertise in machine learning or the right talent, they can actually go, teams can go and hire that talent and start using this easy to use platform. We call our platform red eye.

163
00:34:38,000 --> 00:34:58,000
And it's, it's, it is not, it's, we are not trying to build the like a ground up, like, you know, base level machine learning platform where we're not trying to sort of recreate Sage maker, for example, what we're trying to do is bring together all of these already existing tools that exist outside of, you know, right,

164
00:34:58,000 --> 00:35:11,000
like that exists within AWS within GCP. And we want to bring all of those in, we want to create tools that make it easy for those those things to be used with threat and data.

165
00:35:11,000 --> 00:35:22,000
And we want to establish standard best practices around, you know, how to do experimentation, how to do prototyping, how to actually push a model that has been prototyped into production.

166
00:35:22,000 --> 00:35:36,000
And that's been the focus of the red eye initiative. The specific tools that we use, like, you know, we have started using air flow, we have started using ML flow quite extensively.

167
00:35:36,000 --> 00:35:57,000
Because we have our own sort of feature library, we, you know, we call it internally, we call it Ohara, but it's, it combination of a feature library and data catalog so people can easily discover the features that have already been engineered by, you know, their colleagues at the company for other kinds of machine learning use cases.

168
00:35:57,000 --> 00:36:11,000
So all of this in combination is, you know, what we hope is going to really make it very easy for every engineer at the company to start working within the ML ecosystem.

169
00:36:11,000 --> 00:36:14,000
And yeah, that's, that's what our vision is.

170
00:36:14,000 --> 00:36:23,000
And is the feature library kind of bottoms up homegrown or is that based on open source or commercial offering?

171
00:36:23,000 --> 00:36:45,000
And I think it's, it's mostly homegrown. So we, it is like we, we have all this data that, you know, we have all these high tables park tables data that goes into that, like, it was all kind of spread across and not very well organized and we decided to organize it in a way that makes sense and that can be make makes that data reusable.

172
00:36:45,000 --> 00:36:54,000
So we haven't invested heavily yet in sort of open source data catalogs or data organizations because we don't haven't found the need for it yet.

173
00:36:54,000 --> 00:37:01,000
But we have been exploring some of those things and certainly, you know, open to investing in them if they make sense for our use cases.

174
00:37:01,000 --> 00:37:19,000
When you think about the platform that you're building, who is the target persona or who are the target personas are the users there more kind of classical, you know, data scientist, not particularly

175
00:37:19,000 --> 00:37:38,000
infrastructure oriented, you know, aren't native Docker users that kind of thing are they more the ML engineers that are more comfortable with those kinds of things. How do you, oftentimes, you know, platform teams are trying to balance, you know, who they're building for and those personas.

176
00:37:38,000 --> 00:37:56,000
Yeah, yeah, I think you said something, yeah, you very insightfully, you know, explain the various personas and that is exactly how, you know, things are at redfin also we have people who are data scientists who are maybe, you know, more comfortable with R than with Python.

177
00:37:56,000 --> 00:38:12,000
They might be folks who are not, you know, quite as familiar with some of these some of these tools which machine learning engineers use. So our platform is geared towards serving both those types of customers.

178
00:38:12,000 --> 00:38:32,000
So we want users who are, you know, we want we have power users who are machine learning engineers and the platform supports their use cases, you know, they can build like really large scale models, they can go in and, you know, they can use all kinds of different tools, deep learning tools sometimes.

179
00:38:32,000 --> 00:38:43,000
We have the same platform serving our complex use cases like recommendations and and the estimate and we also have the same platform serving.

180
00:38:43,000 --> 00:39:11,000
We are trying to build something which caters to both of these kinds of customers, but two big personas are folks who are more bi focused who are, you know, a data scientist maybe who work within R and we allow for them to be able to build models that can go into production or build, you allow for them to build models that can have API is built in front of them that can, you know, be productionized.

181
00:39:11,000 --> 00:39:15,000
And we also cater to more hardcore machine learning engineers.

182
00:39:15,000 --> 00:39:39,000
And you mentioned air flow and ML flow is some of the tools is the implication that your platform today is more focused on kind of the experimentation and model building phase as opposed to, you know, other organizations emphasize the, you know, push one button and a production instance of a predictive services spun up.

183
00:39:39,000 --> 00:39:52,000
Yeah, yeah, so I think it's, I'd say we are trying to cater to both like air flow is not just for experimentation air flow can also be used in production like it's an orchestration platform that.

184
00:39:52,000 --> 00:40:18,000
And powers many of our production workflows tradition like previous to this we had our own homegrown, you know, orchestration system orchestration platform and that was it was getting like really cumbersome and hard to manage like we had really complex graph of services which were all involved in finally serving the red fin estimate or the home recommendations.

185
00:40:18,000 --> 00:40:25,000
We had like these lambda functions, you know, talking to each other across multiple services, it was very complex, it was, it was hard to maintain.

186
00:40:25,000 --> 00:40:36,000
And so we decided we would standardize over air flow and it's we use it as I said, you know, both for building experimentation pipelines and we also use it for building production pipelines.

187
00:40:36,000 --> 00:40:39,000
ML flow of course is, you know, very helpful in experimentation.

188
00:40:39,000 --> 00:40:48,000
But air flow is useful for both. I alluded to kind of darker and Kubernetes earlier, is that a part of the tool set there.

189
00:40:48,000 --> 00:41:02,000
Yeah, absolutely. So we as a company, we are investing quite heavily in Kubernetes, like not just for machine learning, but you know, across the business, we are building a bunch of platforms that run on top of Kubernetes.

190
00:41:02,000 --> 00:41:17,000
So we actually explored cube flow before we, you know, initially when we were starting out, we explored cube flow, it, it was a little bit more complex than what was needed for, I think, for, you know, supporting the platform that we have.

191
00:41:17,000 --> 00:41:28,000
It comes with like you, you have to have machine learning engineers who are also Kubernetes experts, if you use cube flow right now, like I think that's where the state is of the world is.

192
00:41:28,000 --> 00:41:40,000
So we are running air flow in Kubernetes, but not using cube flow, maybe you know, we'll come back to it in a couple of months or years and see if it makes sense for for us then.

193
00:41:40,000 --> 00:41:53,000
But at this moment, like we are using Kubernetes, but we are sort of, you know, running a hosted version of Kubernetes and we are most of our energy goes into maintaining air flow on top of Kubernetes as opposed to maintaining cube flow.

194
00:41:53,000 --> 00:42:04,000
So we have referenced the cloud and cloud services several times in our conversation, SageMaker, GCGK, that kind of thing.

195
00:42:04,000 --> 00:42:15,000
How do you think about the role of the cloud and delivering the platform for your users and more generally.

196
00:42:15,000 --> 00:42:28,000
Deploying building and deploying machine learning, yeah, yeah, I think it's it's it's really like it's almost impossible to do machine learning right now without some kind of reliance on cloud services.

197
00:42:28,000 --> 00:42:45,000
So you know, traditionally, like before SageMaker and auto ml existed, people would use emr on AWS, even before that people were like deploying hive on, you know, EC to like EC to instances on on raw EC to instances.

198
00:42:45,000 --> 00:43:02,000
So things have, you know, continuously progressed over the last few years to make it easier and easier for somebody who wants to build a machine learning model to to get to the point where they have an actual model running in production or running, you know, for an experiment prototyping in the cloud.

199
00:43:02,000 --> 00:43:25,000
And like I think what we do what we have seen what we usually see is that it's there's like always this, like if you use some of these more advanced more, you know, out of the box services like auto ml or SageMaker, it is much faster to get to to production, but it is also much more expensive.

200
00:43:25,000 --> 00:43:42,000
So it ends up you usually make this trade off at the end, like you try a bunch of things out, you see what works, you try things out on auto ml, you try things out on SageMaker, you get to a point where you have a model that works that gives you good results, and then you start to optimize on the operational cost.

201
00:43:42,000 --> 00:43:59,000
And then you see, okay, does it make sense to invest some time now into, you know, building something more bespoke, see if that saves enough operational cost to justify the cost of the of actually, you know, investing and building that model.

202
00:43:59,000 --> 00:44:15,000
That's how it's been for us, like, you know, we use a combination of all of these services, some of our staff students on the MR. And, you know, it is, it is actually cheaper to do that sometimes than to use SageMaker.

203
00:44:15,000 --> 00:44:33,000
And then you get that up when you were referring to the document intelligence tools that you were building, I was thinking, well, you know, all the major club vendors have kind of these document AI services that will do OCR and

204
00:44:33,000 --> 00:44:59,000
you described it, it sounded like you were going for a level of integration or, or kind of custom, you know, feature that may not be supported by them, but also, you know, at some point, your costs is going to be expensive, kind of operating using one of those higher level services versus building it out at a lower level infrastructure.

205
00:44:59,000 --> 00:45:13,000
Yeah, yeah, absolutely. So it's always, there's always that, you know, you have to make that trade off. And, yeah, we certainly like, I don't think like, you know, we are not the document intelligence team is not creating new OCR libraries.

206
00:45:13,000 --> 00:45:29,000
They are using in a standard OCR mechanism. So, so yeah, I think they, there's always a straight off, they might have started with, you know, looking at the document AI document intelligence services that some of these platforms provide.

207
00:45:29,000 --> 00:45:47,000
Eventually, they evolved to a point where they were using either a combination of those services with some more homegrown stuff or sometimes moving to a model, which would be operationally a lot cheaper, especially if they are deploying a model and like doing influence at scale, where they are using something more this book.

208
00:45:47,000 --> 00:46:05,000
Do you see the platform going at Redfin and even more broadly machine learning going at Redfin? There are like a really ton of really interesting use cases where we are hoping to spend, you know, more of our energies.

209
00:46:05,000 --> 00:46:30,000
I'm talking more about like machine learning at Redfin now. So there is, as I said, there are all these really interesting unstructured data that we have access to in terms of, you know, there is this language that there is all this stuff in the properties in when a home comes on the market like marketing remarks have a ton of really interesting data images have a certain ton of interesting data.

210
00:46:30,000 --> 00:46:43,000
So we expect that we will be using computer vision in a quite an extensive way to improve the accuracy of the estimate into improving the relevance of our recommendations.

211
00:46:43,000 --> 00:47:01,000
We hope that at some point we can start to start to figure out home conditions even better than maybe humans can based on all this unstructured data that we have about about properties. So that's one area in which I see a big investment like computer vision.

212
00:47:01,000 --> 00:47:27,000
Another area is, you know, there is this really a lot of interest in making the process of touring homes a lot easier. So there is a lot of coordination that happens when somebody wants to tour a home, you have to talk to, you know, multiple people, including the obviously the person who wants to do the home, the agent, whether that home is available or not needs to be figured out.

213
00:47:27,000 --> 00:47:35,000
So there are certain use cases there which, you know, would be maybe good good places for machine learning to be used.

214
00:47:35,000 --> 00:47:53,000
And I think more broadly like the business of of mortgage is something that is probably going to have some really interesting, you know, use cases where we would we can expect to see some some investment within the company.

215
00:47:53,000 --> 00:48:00,000
So yeah, there are just a ton of open areas within lots of different domains.

216
00:48:00,000 --> 00:48:13,000
Also, including the instant buying business where I'm expecting that we'll see a lot of new investment on the platform side, we are getting close to the point where we have a platform that is quite easy to use.

217
00:48:13,000 --> 00:48:23,000
We are investing quite heavily in, you know, building labs. So labs as in like documentation and making it easy for people to understand how to use that platform.

218
00:48:23,000 --> 00:48:34,000
So that's another area where we are going to spend a ton of our time just to make it, you know, really easy to use, make it possible for someone who maybe has a basic understanding of machine learning and has, you know, taken the courses.

219
00:48:34,000 --> 00:48:44,000
But hasn't really started to apply that knowledge within the right in context. So that's another area where we, you know, expect we'll be doing some work.

220
00:48:44,000 --> 00:48:51,000
Awesome, awesome. So big focus on onboarding engineers and data scientists on to the platform.

221
00:48:51,000 --> 00:48:53,000
Yep, yeah, absolutely.

222
00:48:53,000 --> 00:48:55,000
Great, great.

223
00:48:55,000 --> 00:49:04,000
Well, Aksha, thanks so much for joining us. It was wonderful to learn about machine learning at Ripon.

224
00:49:04,000 --> 00:49:33,000
That really interesting conversation.

