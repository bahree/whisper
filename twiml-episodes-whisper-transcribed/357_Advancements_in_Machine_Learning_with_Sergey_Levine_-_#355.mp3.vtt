WEBVTT

00:00.000 --> 00:16.000
Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.

00:16.000 --> 00:27.920
Alright everyone, I am here in Vancouver at the 33rd NURPS conference and I've got the

00:27.920 --> 00:33.200
pleasure of being seated with a repeat Twimal guests. Sergey Levine. Sergey is an assistant

00:33.200 --> 00:38.480
professor in the Department of Electrical Engineering and Computer Science at UC Berkeley.

00:38.480 --> 00:46.480
Sergey, welcome back to the Twimal AI Podcast. Thank you. So your team has been continues to be

00:46.480 --> 00:53.200
very productive and prolific. You've got a dozen papers submitted here or accepted here at the

00:53.200 --> 00:59.680
main conference as well as a number of workshop papers and I would love to use this opportunity to

00:59.680 --> 01:05.840
kind of get caught up with you and hear about what you've been excited about recently. I think it

01:05.840 --> 01:12.320
was July, well not I think, I just checked actually. It was July of 2017 that we last spoke on

01:13.440 --> 01:18.400
deep robotic learning. What have you been up to? What are you excited about?

01:18.400 --> 01:23.920
I guess a lot has happened since then. Broadly speaking, a lot of what my lab has been trying to do

01:23.920 --> 01:30.560
since then is really to try to make it, try to move towards sort of a future where we could have

01:30.560 --> 01:34.240
machines that are out there in the real world learning continues through their own experience.

01:34.240 --> 01:39.520
And while we're doing a lot of very different things, in many ways much of this work is

01:39.520 --> 01:42.480
centered around the components that we believe are necessary to make that happen.

01:43.520 --> 01:47.760
So I could tell you, for example, about some of the things in this conference that I'm pretty

01:47.760 --> 01:51.840
excited about it that I think are giving us sort of a non-trivial step towards that direction.

01:51.840 --> 01:58.400
Absolutely. So one of those things is basically techniques for combining model free reinforcement

01:58.400 --> 02:04.480
learning with planning. So this is something that I think is actually a really big deal.

02:04.480 --> 02:08.400
You know, conventionally when we think about rational decision making, we think about kind of

02:08.400 --> 02:12.160
a planning that is that process, you know, if you're imagining how you're going to get to the

02:12.160 --> 02:16.560
airport, you think, well maybe you'll like get the train or get the taxi, how do you pay for the

02:16.560 --> 02:20.560
taxi, how do you pay for the train, you know, what do you do when you get there, and there's a

02:20.560 --> 02:24.320
sequence of steps that you have in your mind. But the thing that's always been kind of puzzling is

02:24.320 --> 02:29.440
that those steps are, they're a little bit abstract. So you don't plan how you're going to move

02:29.440 --> 02:33.360
every muscle in your body in order to get on the train, you plan through individual steps.

02:33.920 --> 02:39.600
And that has always been the big challenge in marrying planning and learning.

02:39.600 --> 02:45.200
You can't plan over the low level kind of instantiation of your behavior and it seems like

02:45.200 --> 02:49.440
learning doesn't by itself give you these kind of planning type behaviors. So we have actually

02:49.440 --> 02:55.440
two papers in the main conference that study different facets of this problem, essentially using

02:55.440 --> 03:00.960
learning to learn behaviors that achieve goals and then planning over the instantiation of those

03:00.960 --> 03:06.640
goals to achieve much more temporarily extended outcomes. And I'm pretty excited about that because

03:06.640 --> 03:11.360
that gives us some ideas about how reinforcement learning can, it sounds almost provide the abstractions

03:11.360 --> 03:17.920
over which planning can be done. What's the relationship between planning and model-based

03:17.920 --> 03:23.360
approaches where you're integrating in some, you know, prior knowledge about how a task should be

03:23.360 --> 03:29.840
performed? So planning is typically regarded as a very model-based thing. And I think that one of

03:29.840 --> 03:33.680
the, one of the things that's kind of an interesting shift away from the conventional way of thinking

03:33.680 --> 03:37.360
is that conventionally people think that, well, planning is this thing that you do, well,

03:37.360 --> 03:40.960
first people think that it's something that you do that you publish and kind of dusty,

03:40.960 --> 03:46.080
old robotics venues. But they think it's like this thing that you do on top of a very

03:46.080 --> 03:49.760
physics-based, manually designed model, you know, you open up the first textbook right out the

03:49.760 --> 03:54.480
equations. And I think, you know, my group is not the only one working on this. I think this

03:54.480 --> 03:57.840
is actually something that we've seen come up more and more over the past year or two is that

03:57.840 --> 04:04.160
people are thinking, well, can you learn abstractions that you can plan over that are not just purely

04:04.160 --> 04:08.000
predictive models? So these abstractions don't try to predict, you know, if you take this very low

04:08.000 --> 04:12.320
level action, here's what will happen immediately at the next point in time. But they're kind of

04:12.320 --> 04:17.120
higher level abstractions. They're like, if you execute this intention, how will the world change

04:17.120 --> 04:21.520
in some representation that sort of lifts you away from the low level physical grounding?

04:21.520 --> 04:25.200
And by lifting you off from that low level physical grounding, you actually get a much

04:25.200 --> 04:30.240
easier planning problem. So in effect, learning serves to simplify the planning by putting it into

04:30.240 --> 04:37.200
the right abstract representation. And why is that? If the planning still needs to be done at a

04:37.200 --> 04:43.920
low level theory to actually, you know, move the robot or get us from point A to point B.

04:44.640 --> 04:49.200
Yes, exactly. So in effect, the learning kind of takes care of the low level details.

04:49.760 --> 04:55.360
So if you have a model-free learned behavior for, let's say, you know, walking to the door,

04:55.360 --> 04:59.120
then you don't have to plan how you're going to do that. You can plan at the level of, you know,

04:59.120 --> 05:03.920
I want the door open and then kind of, you know, let your body do its thing to get you there.

05:03.920 --> 05:07.520
And I think that's kind of the intuition behind how these methods work. They let the learning

05:07.520 --> 05:12.640
take care of these low level details and then remove them from consideration for the higher level planning.

05:12.640 --> 05:16.160
So it's a kind of a hierarchical approach. I was just going to ask that it sounds like you're

05:16.800 --> 05:25.920
proposing a hierarchical approach in which, as opposed to assuming low level physics-based models,

05:25.920 --> 05:34.240
for example, you are able to, you're maybe learning in both levels of the hierarchy, but the hierarchy

05:34.240 --> 05:42.480
itself simplifies the, doesn't simplify both or just the higher level problem. So the hierarchy

05:42.480 --> 05:46.080
simplifies the higher level problem, but it doesn't, in a sense, simplify the lower level problem

05:46.080 --> 05:50.720
because now the learning component doesn't need to take care of being able to achieve very

05:50.720 --> 05:56.160
temporarily extended goals. So often in reinforcement learning, one of the hardest things is if you have

05:56.160 --> 06:01.440
a task where you don't realize that you've reached a successful outcome until you've performed a

06:01.440 --> 06:06.960
very long sequence of behaviors that are by themselves unrewarding. So in effect, by chunking out

06:06.960 --> 06:13.280
the problem into these intermediate sub-goals, which you can, which the algorithm can basically

06:13.280 --> 06:17.040
invent, so basically make some, you know, here are some places I could go for each place that I've

06:17.040 --> 06:22.720
reached repeatedly. Let me just compartmentalize that into a little skill. So that doesn't require

06:22.720 --> 06:25.920
achieving any temporarily extended goal, like getting to the airport, and then the higher level

06:25.920 --> 06:29.680
planner can go in and say, oh, you know how to reach all these sub-goals. Let me figure out how

06:29.680 --> 06:34.640
to sequence them so that you accomplish your end goal. The notion of hierarchical learning like

06:34.640 --> 06:39.280
this is, is not by any means new, but one of the things that has been sort of coming to fruition

06:39.280 --> 06:44.880
recently is that people have figured out how to abstract away both the behaviors and the states,

06:44.880 --> 06:50.320
right? A planning problem consists of states and the actions, right? So the actions get abstracted

06:50.320 --> 06:54.720
away as these skills and the states get abstracted away via representation learning. And if you have

06:54.720 --> 06:59.440
both of those, then you can get a substantially simplified problem for the higher level planner,

06:59.440 --> 07:03.280
and you can actually get some of these things to work on interesting like image-based tasks,

07:03.280 --> 07:08.320
for example. One of the interesting nuances that came out for me in the way you described that

07:08.320 --> 07:14.880
is when we think about the analogy that you used, I want to get to the airport and I need to take

07:14.880 --> 07:22.480
the bus and the train and the taxi, whatever, that is maybe let's say a top-down kind of plan. I'm

07:22.480 --> 07:29.440
thinking, you know, I had, I start by identifying the intermediate states that's going to get,

07:29.440 --> 07:37.360
that are going to get me to the end goal. But what I thought you just said was that perhaps the

07:37.360 --> 07:44.000
intermediate states can be learned in the process of, you know, you're still specifying the end

07:44.000 --> 07:48.880
goal maybe and intermediate states are learned and, you know, I'm almost thinking like that,

07:48.880 --> 07:55.040
you know, they become kind of a compact or a better optimized representation of memory, you know,

07:55.040 --> 08:00.880
there's a lot of work happening here to try to, you know, in RL to incorporate different memory

08:00.880 --> 08:06.000
schemes so that we're not throwing away what we've learned in the process. This is a little bit.

08:06.000 --> 08:10.640
So it's, so there's always for a hierarchical scheme, there's a top-down in the bottom-up

08:10.640 --> 08:14.880
component and I think it's conventionally actually something that's been a lot, a lot more

08:14.880 --> 08:19.600
common is to think about it mostly as a top-down process. So there's a single task that you want to

08:19.600 --> 08:24.560
accomplish, it's a very complex task and you'll go in and, and sure chunk that task up into pieces.

08:24.560 --> 08:29.520
Yeah. But what has really worked out much better and this is what we use actually in our work

08:29.520 --> 08:34.400
in the main conference too, is that the discovery of the behaviors works very well when it's bottom-up.

08:34.400 --> 08:38.720
Essentially, if you go and try to do something in the world and in the process you've accomplished

08:38.720 --> 08:43.200
accidentally, maybe something else, like maybe, you know, I'm trying to get to the kitchen,

08:43.200 --> 08:47.200
but I happen to have figured out how to reach the bedroom. I'm not thrilled about that, but I can

08:47.200 --> 08:51.600
sort of file that away as something I've learned to reuse later. And this bottom-up discovery of

08:51.600 --> 08:55.520
skills ends up working very well for these kinds of hierarchical methods because you can discover

08:55.520 --> 09:00.080
those skills before you actually know how to solve some really complex tasks. And so is this

09:00.080 --> 09:06.560
analogy to memory one that's useful or? Yeah, I think so. I think in effect, everything that you

09:07.760 --> 09:11.280
experience in the course of solving whatever tasks you have to solve in the world,

09:11.280 --> 09:15.200
even if it's not useful to that task, you sort of file it away in your memory as behaviors that

09:15.200 --> 09:20.880
you can draw on later. And in practice, the way we represent this is not quite so discrete.

09:20.880 --> 09:24.880
It's actually a continuous space of all the possible goals you could reach and you can think of

09:24.880 --> 09:28.800
this as basically filling out that space. So for all the possible states you've seen, can you get

09:28.800 --> 09:33.600
a behavior that reaches them and it's continuous representation not a discrete one? What did you

09:33.600 --> 09:38.560
call this particular kind of line of research? We don't really have a clear name for it, although

09:38.560 --> 09:43.040
maybe we should. So we don't call it hierarchical reinforcement learning because the higher level

09:43.040 --> 09:47.440
here is not really reinforcement learning. So I've been calling it kind of hybridizing planning

09:47.440 --> 09:52.560
on learning. To me, the important bit about is that the learning provides the abstractions to

09:52.560 --> 09:57.680
the planning. So maybe I should come up with a name for it that reflects that. Okay, so that's

09:59.280 --> 10:03.360
that's a couple of the papers that you had. Yeah, so we have two papers on this. We have

10:04.240 --> 10:08.880
one called search on the replay buffer by Benjamin Eisenbach, which applies kind of a non-parametric

10:08.880 --> 10:13.440
graph search approach to it. And then we have another one called learning with goal-conditioned

10:13.440 --> 10:19.600
policies by Serucia Nasseriani, which applies more of a continuous trajectory optimization approach

10:19.600 --> 10:24.160
for the higher level. So where you continue to see optimize over goals. And so are they it sounds

10:24.160 --> 10:30.800
like they're both they're alternate approaches to the higher level optimization problem as opposed

10:30.800 --> 10:39.120
to kind of picking off small pieces of. So the lower level skills actually in both papers are

10:39.120 --> 10:44.560
built by learning goal-conditioned policies with RL. And that's a technique that has been explored

10:44.560 --> 10:48.560
in prior work too. So that's in some sense. It's very important, but it's not really the new part.

10:48.560 --> 10:55.920
Cool. So that's two of the top papers. Yeah. What else? What else? There was another thing

10:55.920 --> 11:00.560
that I could tell you about that is maybe a little bit outside the norm for what we typically do

11:00.560 --> 11:05.120
because it's not quite so focused on autonomous robotic learning, but it's a topic that I think

11:05.120 --> 11:08.880
was very interesting to work on. And maybe something that could be interesting to your listeners too.

11:08.880 --> 11:13.520
So we had a presentation. This was actually an oral presentation by a student who was actually

11:13.520 --> 11:18.640
a visiting student with us named Pim Dehan. And what he worked on was understanding the role of

11:18.640 --> 11:23.600
causality and imitation learning. So you know causality is obviously a big topic. I'm sure you've had

11:23.600 --> 11:28.240
guests on your show talk about it. Not enough though. Not enough. It's something that it took me a

11:28.240 --> 11:32.720
while to understand why this was important. And for that project we really wanted to isolate.

11:32.720 --> 11:38.960
Well, let's pause there. What did you learn after the while? You know, what was your realization

11:38.960 --> 11:45.600
about why it's important? Yeah. So I guess maybe to answer this I should first say why I was not

11:45.600 --> 11:49.680
sure that it was important before. Please. And the reason that I think this is actually a fairly

11:49.680 --> 11:56.000
typical doubt that comes from a reinforcement learning research. And to kind of go back,

11:56.000 --> 12:02.320
you know, inside the onion one more layer maybe, a summary of causality and kind of, yeah,

12:02.320 --> 12:09.200
you know, how do you describe you know causality? Well, so a causal model is a model that

12:09.200 --> 12:14.640
associates causes with effects. Whereas in correlative model is just a model that tries to notice

12:14.640 --> 12:19.600
that, you know, when two things seem to occur together, you can guess in your data what the value

12:19.600 --> 12:23.360
of one of those things will be from the value of the other one. But if the relationship there is

12:23.360 --> 12:27.680
not causal, that doesn't necessarily mean that when you see new things that same relationship will

12:27.680 --> 12:34.240
still hold. So to use an example, it's maybe a bit more relevant for our work. If you imagine

12:34.240 --> 12:39.520
a driving scenario, you have a car, the car in front of you stops that causes you to stop. But if you

12:39.520 --> 12:44.160
stop, the car in front of you will not necessarily stop. It just so happens that if you look at data,

12:44.160 --> 12:48.880
almost always, if your car stops, the car in front of you stops, but the causality goes the other

12:48.880 --> 12:53.040
way, right? So there's a correlation in the data because people don't just stop for no reason,

12:53.040 --> 12:57.680
but you can't exploit that correlation to affect change in the world. And that's tremendously

12:57.680 --> 13:00.880
important for robotics, of course, because the whole point is to do things that affect change in

13:00.880 --> 13:05.360
the world. But you didn't think that was important for a while. I didn't think that was important

13:05.360 --> 13:09.840
for the following reason, which is that if you train predictive models and you use those models to

13:09.840 --> 13:14.960
act, then maybe your models will make mistakes. But when you go and use them to act, if you actually

13:14.960 --> 13:18.560
try stopping to get the guy in front of you to stop and you notice that it isn't working,

13:18.560 --> 13:22.320
then you will update your model. You'll learn that. Yeah, you'll patch up that whole. And that's

13:22.320 --> 13:25.920
why I said earlier that oftentimes we're enforcing learning researchers. Maybe don't worry about

13:25.920 --> 13:30.720
these things so much because once you're in this sort of closed loop scenario where you're constantly

13:30.720 --> 13:36.240
updating your model, maybe those correlations very quickly get fixed up and turn into causal

13:36.240 --> 13:39.120
relationships because otherwise you make mistakes and you have to fix those mistakes.

13:40.560 --> 13:46.240
But we did figure out a particular setting where this is very important that is actually a setting

13:46.240 --> 13:51.040
where the issue even comes up in practice, like in actual practical things that are deployed there

13:51.040 --> 13:55.360
in the world right now, which is imitation learning. So imitation learning is a very simple and very

13:55.360 --> 14:00.640
powerful tool that people use today for things like autonomous driving and many other about

14:00.640 --> 14:06.000
applications where you collect data from a person operating a machine, maybe driving a car,

14:06.000 --> 14:10.000
and then you just treat that as a supervised learning problem. So predict the action of the person

14:10.000 --> 14:14.480
took from the observations, which seems like a great idea like for driving you can get lots and

14:14.480 --> 14:18.800
lots of data doing this. But there are all sorts of correlations that happen like that stopping

14:18.800 --> 14:23.680
example that I gave. Another correlation could be that maybe you turn on the turning signal when

14:23.680 --> 14:28.560
you're about to turn and then you turn. But of course the turn signal is not what caused you to turn.

14:28.560 --> 14:34.080
What caused you to turn is your intention to make a right turn. And that's perhaps a bit risky,

14:34.080 --> 14:38.480
you know, typically your camera and your car will be mounted outside the car, but if it happens

14:38.480 --> 14:42.000
to be inside the car and you can see your turn signal that it might decide, well, I'll just wait

14:42.000 --> 14:47.920
for someone to turn on the turn signal before I turn, which makes sense of course. So there are

14:47.920 --> 14:52.080
all sorts of these correlations that can happen. And the way it shows up in practice is that when

14:52.080 --> 14:58.640
people add more inputs to their system, maybe they'll have a camera, they'll also add a history,

14:58.640 --> 15:02.720
or they'll add a LiDAR, although that's some other sensor. They sometimes find that their

15:02.720 --> 15:06.800
imitation learning system actually does worse. And why does it do worse? Well, because all those

15:06.800 --> 15:11.680
additional sensors that they're adding add more potential correlations, which might result in

15:11.680 --> 15:20.000
this causal confusion. So one of the things that we did with PIM is that we worked on formalizing

15:20.000 --> 15:25.040
this problem, identifying why it happens, where it happens, reproducing it kind of in a little

15:25.040 --> 15:30.640
peach tradition, some toy domains, and then coming up with some simple causal discovery techniques

15:30.640 --> 15:35.600
that could be used to partially address it. So our solution is not perfect, it doesn't fix it

15:35.600 --> 15:40.400
every time. But to me, the thing that's pretty exciting about this work is that it actually

15:40.400 --> 15:45.040
allows us to make a little bit more formal something that people have observed anecdotally

15:45.040 --> 15:49.040
before, which is this phenomena that adding more information to your imitation learning system

15:49.040 --> 15:53.120
makes it do worse. And I'm pretty happy with that work because I think it's also a technique

15:53.120 --> 15:56.720
that people are using in the real world, and if they're not aware of this issue, they might get

15:56.720 --> 16:02.000
into a bit of trouble. Can you give us a sense for the technique itself, how what it's doing?

16:02.000 --> 16:06.480
Yeah, it's actually kind of neat. So this is something that PIM and Dinesh, the LiDAR

16:06.480 --> 16:14.640
authors on this, came up with. The idea is that you can think about discovering causal graphs.

16:14.640 --> 16:17.280
So if you have a bunch of variables, let's say you have a discrete set of variables,

16:18.480 --> 16:22.400
the variables you're predicting are the actions, the variables that you're observing are the

16:22.400 --> 16:28.480
observation variables of states, and some of those state variables are actual causes of the actions,

16:28.480 --> 16:33.360
and some of them are are are spurious, some of them you should not pay attention to, like the

16:33.360 --> 16:38.640
turn signal or whatever. Now the trouble is that in a if your observations are things like

16:38.640 --> 16:42.960
images, you can't just treat every pixels a possible cause. You know, there's a lot more structure

16:42.960 --> 16:48.080
underneath. So what they did is they actually combined a representation learning phase where you

16:48.080 --> 16:52.800
actually take the your observations and you turn them into a smaller set of independent latent

16:52.800 --> 16:58.880
variables with a causal graph discovery phase. So they have one model that learns this representation,

16:58.880 --> 17:03.760
these disentangled variables, and then they have a second phase where they train a model that

17:03.760 --> 17:09.840
takes in those variables and a mask that represents the graph. So the graphs always map inputs to

17:09.840 --> 17:14.480
outputs. So basically you can represent all possible graphs with a vector of bits that says which

17:14.480 --> 17:21.040
edges there and which edges absent. So is the graph learned or the graph is the graph imposed?

17:21.040 --> 17:25.440
It's actually a little weird. It's the model actually simultaneously represents all possible

17:25.440 --> 17:29.680
graphs, which seems really hard because they're exponentially many graphs. But of course the

17:29.680 --> 17:34.560
wonderful thing with neural nets is that they do generalize. So it turns out that you don't have to

17:34.560 --> 17:40.240
have it train on every possible graph to get it to generalize meaningfully to all or most possible

17:40.240 --> 17:46.080
graphs. So you basically randomly delete or remove edges and then train it on on these randomly

17:46.080 --> 17:50.400
selected graphs and then hope that it generalize to the others. And so long as those training graphs

17:50.400 --> 17:55.680
are drawn from the right distribution then you should get generalization if you sample enough graphs.

17:55.680 --> 17:59.440
Now that doesn't let you discover the right graph. It just lets you compactly represent all the

17:59.440 --> 18:05.120
possible graphs. The discovery then requires intervention. So to discover which of those graphs

18:05.120 --> 18:08.640
is correct you have to somehow break the correlations in your data. So you do that by actually

18:08.640 --> 18:13.360
attempting the task but a very very small number of times just so you can figure out which of those

18:13.360 --> 18:19.440
graphs is the right one. And those attempts can have one of two forms either you attempt the task

18:19.440 --> 18:23.920
and you ask for additional human supervision so you say well oh if I were in this intersection

18:23.920 --> 18:28.240
how should you drive or you assume that you have access to a reward function so you try

18:28.240 --> 18:32.720
yourself and then you look at the rewards. Of course in both cases you could learn the task

18:32.720 --> 18:37.440
entirely from scratch if you have enough interventions but the point is that if you have these

18:37.440 --> 18:42.080
pre-trained candidate graphs you can discover the right one with a very very small number of

18:42.080 --> 18:50.720
interventions. Okay I'm trying to wrap my head around all this. It sounds like if I'm understanding

18:50.720 --> 19:00.240
what's happening that the training effort is like some exponential multiplier on the training

19:00.240 --> 19:06.560
effort for not trying to figure out this causality. In other words it's not a lot harder because

19:06.560 --> 19:13.840
you have to it's almost like you are coming up with this graph and then applying like a dropout

19:13.840 --> 19:20.080
kind of thing where you're breaking these connections and you've got to you know in the graph is

19:20.080 --> 19:26.400
potentially you know you have a kind of end by and fully connected thing like it I'm imagining it

19:26.400 --> 19:32.560
exploding. Yeah well so the reason that this ends up being not exponentially difficult is that

19:32.560 --> 19:36.960
there's a bit of regularity so there's a particular variable and you include that variable or exclude

19:36.960 --> 19:42.880
that variable. While in principle the behavior of the resulting model depends a lot on all the other

19:42.880 --> 19:49.120
variables in practice you can generally get a pretty good idea for the for whether that variables

19:49.120 --> 19:54.640
of cause or not just from whether it by itself is excluded or not. So if you exclude for example the

19:54.640 --> 20:01.760
extra turn signal input and things just kind of work okay maybe the right graph is like no turn

20:01.760 --> 20:06.560
signal and some of these other things but just from the fact that you included the turn signal in

20:06.560 --> 20:10.320
these five graphs and excluded in these five graphs that's a really good hint that the turn

20:10.320 --> 20:15.680
signal is should not be included. So while in theory the whole thing is exponentially bad in

20:15.680 --> 20:19.920
practice for most problems that's not usually the case and you can identify these individually

20:19.920 --> 20:25.120
so that's why we can get away with substantially less than exponential time training for this

20:25.120 --> 20:29.600
even though in principle there are exponentially many graphs. Of course you can construct pathological

20:29.600 --> 20:34.800
problems where this would not work. So if there's some very very specific set of edges that works

20:34.800 --> 20:38.480
and everything else fails then through random sampling you probably won't discover that.

20:38.480 --> 20:46.320
Right right. You mentioned that you formulated a toy problem to illustrate this presumably that's

20:46.320 --> 20:52.400
also the problem that you presented your experimental results in the context of tell us a little

20:52.400 --> 20:56.560
bit about those problems. It's actually a very simple toy problem we just took an Atari game

20:56.560 --> 21:01.600
and we drew on the screen a little number that indicates what action was taken at the last

21:01.600 --> 21:06.080
time step. You think this would be completely innocuous because the action you already took that

21:06.080 --> 21:12.560
action and just draw it on the screen but for good Atari playing strategies usually the current

21:12.560 --> 21:16.800
action is strongly correlated with the previous one. So if you're moving the pong paddle down you

21:16.800 --> 21:21.120
probably move it down for several steps. So there's a strong temporal correlation which means that

21:21.120 --> 21:26.240
drawing that previous action as a digit on the screen gives the imitator a very strong hint about

21:26.240 --> 21:32.400
the next action. But of course that's not a causal relationship because the action you took before

21:32.400 --> 21:37.040
is not the cause of your next action. So it turns out that just adding that digit to the screen for

21:37.040 --> 21:42.800
an Atari game completely breaks your ability to imitate an optimal Atari player which is kind of

21:42.800 --> 21:51.840
a disturbing thing to me. Why is that because the agent kind of over indexes on the thing that you

21:51.840 --> 21:57.920
drew or yeah it's because basically learning machines like to be lazy if they can be lazy. So very

21:57.920 --> 22:02.880
lazy strategies to just say hey I know that the next action strongly correlates with the previous

22:02.880 --> 22:08.160
one not always but it's a pretty good clue. So it's very easy for me to read this digit. It's very

22:08.160 --> 22:12.240
hard for me to figure out what's going on in the game. So I'll just start off by looking at that

22:12.240 --> 22:17.280
little hint that you gave me and I can get some decent performance out of that and I'm so happy

22:17.280 --> 22:20.720
with that that I'll kind of not really pay as much attention to everything else and never bother

22:20.720 --> 22:26.560
learning the real task. So it of course doesn't pay attention exclusively to that thing but it

22:26.560 --> 22:32.160
pays so much attention to it that once that relationship is broken once the action is not

22:32.160 --> 22:36.000
actually coming from an optimal policy once it has to play for itself the performance just tanks.

22:36.880 --> 22:43.520
And so since you're optimizing on the ultimate ultimate outcome it's kind of like

22:43.520 --> 22:52.160
in a sense crude sense like a multitask thing it's like find an optimal policy given

22:54.560 --> 23:00.880
the previous action. Well there's an important distinction which is that imitation learning

23:00.880 --> 23:06.720
is not explicitly trying to find the optimal policy. So this is one of the things about

23:06.720 --> 23:11.360
annotation learning that's a little bit of a shortcoming but it's why it's so simple is that

23:11.360 --> 23:14.560
imitation doesn't care about what's good or what's bad it just cares about what the

23:14.560 --> 23:18.960
demonstrator did. Which is great because you can train it from data without all this complexity

23:18.960 --> 23:23.280
of reinforcement learning but it's not so great because of course it can't understand what's

23:23.280 --> 23:29.360
optimal and what's not. Did you do it and can I copy it? I'm still trying to kind of narrow down

23:29.360 --> 23:35.120
what exactly we've done by drawing the number because we already knew the thing that the agent

23:35.120 --> 23:40.000
is trying to imitate is what was done. It seems like it almost changed the problem.

23:40.000 --> 23:46.400
Yeah. But the subtlety there is that the data that you trained the agent on all came from your

23:46.400 --> 23:52.480
expert demonstrator. So while when the agent is playing the game itself that previous action

23:52.480 --> 23:57.680
indicator doesn't really give it any additional information. When it's trying to copy the expert

23:57.680 --> 24:01.920
it does actually give it a bit of additional information because it tells it that the action

24:01.920 --> 24:05.760
that the expert took in the previous state which is probably pretty similar to the current one.

24:05.760 --> 24:15.520
So it is in the otherwise formulated imitation learning problem the agent doesn't have

24:15.520 --> 24:21.520
access to the action itself. It has access to the observation of. Yeah. So it's supposed to

24:21.520 --> 24:25.440
predict the action. So in the same way that an image class far predicts the label. Yeah.

24:25.440 --> 24:30.160
You're noticing predicts the action from you. Yeah. What other interesting things do you

24:30.160 --> 24:35.920
presenting here? So there is one more thing that maybe would be interesting to discuss a little

24:35.920 --> 24:41.120
bit which is some work by a student named Michael Jan or studying model-based reinforcement learning.

24:41.120 --> 24:46.000
Okay. So model-based reinforcement learning is something that we often think about as being

24:46.000 --> 24:51.040
kind of a more efficient but perhaps somewhat more complicated solution to reinforcement learning

24:51.040 --> 24:54.400
problems as opposed to model-free reinforcement learning. Model-based reinforcement learning

24:54.400 --> 24:59.920
first learn how to predict the future and then you use that predictive model to actually

24:59.920 --> 25:04.720
act in the world to accomplish some tasks. And what Michael wanted to understand is can we

25:04.720 --> 25:11.120
actually analyze the degree to which model-based reinforcement learning provably results in a

25:11.120 --> 25:15.760
better policy at each iteration. So there's been a lot of analysis in model-free RL,

25:15.760 --> 25:20.240
particularly for policy gradients. So we did some work for example back in 2014 with John

25:20.240 --> 25:24.480
Schollman on the Trust Research and Policy Optimization algorithm TRPO where we did analyze

25:25.440 --> 25:28.720
that under some circumstances you provably get an improvement in your policy.

25:28.720 --> 25:32.080
Admittedly under some assumptions that are pretty strong like having infinite samples,

25:32.080 --> 25:37.280
but it's a beginnings of a theory. But such a thing did not exist for model-based RL and one of

25:37.280 --> 25:42.160
the things that Michael and the other students on the paper were able to figure out is that you

25:42.160 --> 25:48.080
can actually write down a similar kind of theoretical proof that given enough samples model-based

25:48.080 --> 25:53.920
reinforcement learning will produce an improvement at each iteration and how much an improvement

25:53.920 --> 25:58.720
it achieves depends on the error in the model obviously and also how much you change your policy.

25:58.720 --> 26:03.360
So if you collect some data and then change your policy be totally different now on that totally

26:03.360 --> 26:06.960
different policy the model will probably be very inaccurate so that's no good. So you need to change

26:06.960 --> 26:11.840
the policy by bounded amount and change the model by bounded amount. That's all fairly straightforward.

26:11.840 --> 26:16.480
But there's another term that showed up in the in the bound that we had which is the number of

26:16.480 --> 26:21.680
time steps for which you utilize the model. So if I predict five steps in the future with my model,

26:21.680 --> 26:25.840
I'll get lower error than if I predict 50 steps in the future. That's again pretty obvious.

26:26.480 --> 26:30.000
What was not obvious is that if you actually look at that bound you know the bound is derived

26:30.000 --> 26:35.760
using standard proof techniques that people have used in other RL model free RL work.

26:36.400 --> 26:40.640
If you actually try to find the optimal value for that horizon you end up with the optimal value

26:40.640 --> 26:45.440
being zero. So it says you know the proof says the model will result in a model they don't use.

26:45.440 --> 26:50.480
Yeah. Yeah. The proof says you will improve but the most improvement is obtained by just not

26:50.480 --> 26:55.280
using your model. That doesn't seem to make any sense that defies our intuition. It also defies

26:55.280 --> 27:00.800
our experimental results. So to a degree this is illustrating some shortcomings in the current

27:00.800 --> 27:04.080
theoretical tools that we have. But then one of the things that we did is we tried to

27:05.760 --> 27:12.240
that number the number of time steps is multiplied by some coefficient which theoretically you know

27:12.240 --> 27:16.640
whenever you do theory you derive sort of the most pessimistic coefficients because you want to

27:16.640 --> 27:20.640
always be true. But then we tried to measure that empirically we tried to actually train some models

27:20.640 --> 27:25.200
train some policies and measure those error terms and see what do they look like in reality.

27:25.200 --> 27:29.760
In reality they're not nearly as pessimistic and in reality models seem to generalize a lot better

27:29.760 --> 27:33.200
than the theory would suggest. So then we did something that you shouldn't really do when

27:33.200 --> 27:37.600
you're doing theoretical work which is that we sort of eyeballed what the actual relationship

27:37.600 --> 27:42.000
and the experiments looks like kind of basically did a linear fit. Subtune that term for the

27:42.000 --> 27:45.680
theoretically motivated term in the bound and then we do actually get a trade off that says

27:45.680 --> 27:50.560
that with the actual observed error patterns you should not use zero length roll outs.

27:50.560 --> 27:53.920
You should also not use very long roll outs. You should limit how many time steps you use your

27:53.920 --> 27:59.280
model for and it turns out that if you actually do that if you use your model for only short periods

27:59.280 --> 28:03.680
you actually get a much better algorithm than anything that was done in previous model based

28:03.680 --> 28:07.360
our all work at least at the time when we published the paper since then of course much better

28:07.360 --> 28:13.520
things have come out. So if you arrived at an analytical relationship between the number of

28:13.520 --> 28:20.480
time steps in some characterization of the problem. Well yes except that this is often the case

28:20.480 --> 28:25.520
for kind of theoretical analysis is that there is a relationship and you can derive the optimal horizon

28:25.520 --> 28:29.200
except that it's derived in terms of quantities that are in practice very difficult to measure.

28:29.200 --> 28:35.360
So probably the actionable conclusion is that you should use a length that is not too long

28:35.360 --> 28:40.400
and not too short probably if you're solving a real problem you're going to select that length

28:40.400 --> 28:50.320
empirically. And so does the takeaway is that the the later analytical result kind of nullifies

28:50.320 --> 28:56.560
the initial analytical result but in practice it kind of works the way we would expect anyway.

28:56.560 --> 29:01.920
So that was a lot very hand wavy. How do you how do you how do you attribute value to the you know

29:01.920 --> 29:07.120
these sets of results? Well I think it works the way we expect it to a degree. I do think that

29:07.120 --> 29:13.120
there were some actionable conclusions from it like for example we did end up with sort of after

29:13.120 --> 29:17.040
combining the empirical observation in the theory we ended up with with an algorithm that uses

29:17.040 --> 29:20.800
pretty short rollouts from the model probably you know shorter than what people typically thought

29:20.800 --> 29:25.920
would be suitable and that did end up working very well. I think it also tells us a little bit about

29:25.920 --> 29:29.680
where we should look to next if we're going to develop better theoretical tools so tells us that

29:29.680 --> 29:34.000
you know you can do some of this analysis but the current tools give you a slightly nonsensical

29:34.000 --> 29:39.360
result there's some term if you substitute in the will you empirically measure that term to be

29:39.360 --> 29:43.040
into the theory you get a much more reasonable conclusion so that maybe give us some guidance

29:43.040 --> 29:47.680
where we can look to next. And I think that does it also say that we should all shift our efforts

29:47.680 --> 29:54.240
to model based RL as opposed to model free RL because it's provably better? Not necessarily so

29:54.240 --> 29:58.240
what does the result say? It says that with model based RL it could improve your policy further

29:58.240 --> 30:02.640
than you could with just model free RL but how much further depends on the error in your model?

30:02.640 --> 30:06.560
So in some cases you can get models with this is very obvious statement in some ways in some

30:06.560 --> 30:10.000
case you can get models with very low error in some cases with very high error if you can get a

30:10.000 --> 30:14.560
model with low error then it seems like you should use it if you can get a model but in some case

30:14.560 --> 30:19.280
that's that's very hard to do so if you have very complex let's say image observations maybe you

30:19.280 --> 30:23.760
really just can't get a model that's accurate enough for more than you know what a single time

30:23.760 --> 30:28.960
somewhere even no time steps into the future so then you're still out of luck. And so one thing

30:28.960 --> 30:39.760
that I often do is maybe conflate the idea of model based as like incorporating physical world

30:39.760 --> 30:45.600
models with what you're talking about which sounds different like you know learn models almost

30:45.600 --> 30:53.600
another type of hierarchical RL. Yeah so for this discussion I was just talking entirely about

30:53.600 --> 30:59.200
learned models right that said the the results and the analysis doesn't really distinguish between

30:59.200 --> 31:03.280
them and there's a very blurry line between them so if you if you do have a physics-based model let's

31:03.280 --> 31:08.320
say you have a CAD model of your robot and a physics simulator and maybe you don't know the masses

31:08.320 --> 31:12.640
of some of the links you can view the process of identifying those masses as a learning process

31:12.640 --> 31:16.240
so you're learning a very small number of parameters we typically work with much more generic

31:16.240 --> 31:20.480
models so basically neural nets that predict the future so there you have a very large number of

31:20.480 --> 31:25.760
parameters but in principle the same lesson should apply to both. Does this particular research

31:26.880 --> 31:31.920
parameterize the relative benefit in terms of number of parameters or is that interesting for you?

31:32.720 --> 31:36.880
We typically don't worry too much about the number of parameters because we're more concerned

31:36.880 --> 31:41.360
with sort of how well we can get this thing to work and how efficiently in terms of data.

31:41.360 --> 31:44.480
This is this is maybe like a little bit of a philosophical point you know I kind of trust

31:44.480 --> 31:48.800
the systems people and the architecture people I think they'll build better chips for us and

31:48.800 --> 31:53.600
we'll be able to have lots of parameters and also in general in deep learning research

31:53.600 --> 31:58.240
one of the things we found time and time again is that when you have a huge number of parameters

31:58.240 --> 32:02.480
things seem to just work better and right now people actually starting to understand why

32:02.480 --> 32:05.920
they're starting to understand that over parameterization is actually a very good thing for

32:05.920 --> 32:11.920
optimization so it seems like trying to keep the number of parameters down is not such a rewarding

32:11.920 --> 32:16.960
process so we're more concerned with data because data comes at a cost and final performance.

32:16.960 --> 32:23.760
Okay so this particular result then to kind of summarize is not necessarily

32:24.800 --> 32:28.800
an about face or shift in perspective from the last time we talked that was very much

32:29.920 --> 32:35.840
in the camp of kind of an end-to-end learned approach with a highly parameterized model you

32:35.840 --> 32:41.440
know a deep neural network as your core model you know towards hey this result says that we should

32:41.440 --> 32:48.080
be incorporating more physics-based models into our RL. Yeah it doesn't necessarily say that

32:48.080 --> 32:52.560
although of course if you do have the luxury of having some knowledge about physics you'll probably

32:52.560 --> 32:56.320
have a more accurate model which means that you'll be able to get more improvement at every step

32:56.320 --> 33:01.280
of this process but yeah it's really all about how much error you have. And you've also got some

33:01.280 --> 33:07.120
work being presented here on off-policy RL? Yeah so this is this is an area that I've been

33:07.120 --> 33:11.840
pretty passionate about for a while because I think it's it's important in robotics it's also

33:11.840 --> 33:17.040
very important in many fields outside of robotics so basically when we think about reinforcement

33:17.040 --> 33:21.520
learning we typically think about it as a very active process so if you open up the you know the

33:21.520 --> 33:25.840
Sutton and Bartow textbook you see this diagram the agent interacts with the world and then it

33:25.840 --> 33:29.440
improves its policy and then interacts with the world some more it's fundamentally an online

33:29.440 --> 33:35.920
active thing but if we want models that generalize very well we want to be able to train those

33:35.920 --> 33:39.760
models on large amounts of data because that's where you get your generalization so if you want

33:39.760 --> 33:44.880
you know to have very good computer vision systems as part of your RL policy you need to see lots

33:44.880 --> 33:50.640
of images and it's very hard to have lots of data and to have this fundamentally active online

33:50.640 --> 33:55.120
learning process. If you need to collect data every time you improve your policy and you need

33:55.120 --> 33:58.880
image net size data sets you essentially need to collect something the size of image net

33:58.880 --> 34:02.960
improve your policy then throw it out and then collect it again and that's just not a very scalable

34:02.960 --> 34:10.480
proposition. So what we've been working on a fair bit is this problem of off policy or offline

34:10.480 --> 34:13.440
reinforcement learning is sometimes they're called fully off policy it's also sometimes referred

34:13.440 --> 34:17.200
as battery reinforcement learning people can't seem to agree on the name but the basic idea is

34:17.200 --> 34:21.840
that you have some data and in the most extreme version you're not even collect allowed to collect

34:21.840 --> 34:26.800
any more data the data is all you've got you just have to extract the best policy you can out of it.

34:26.800 --> 34:30.400
In reality we'll probably do something a little in between we'd use the data and then interact

34:30.400 --> 34:33.920
with the world a little bit but let's just say we were not allowed to interact at all that's kind

34:33.920 --> 34:40.000
of the the most rigid formulation. It turns out that a lot of standard RL methods like Q

34:40.000 --> 34:44.960
learning for example while in principle applicable to that setting in practice perform very very

34:44.960 --> 34:49.200
poorly and people have tried this before and they look at these learning curves where you're

34:49.200 --> 34:53.760
using fully off policy data your policy seems to get better and then it gets worse and they look

34:53.760 --> 34:57.680
at it and say well maybe I have an overfitting problem like that that looks like overfitting

34:57.680 --> 35:01.760
why don't I add more data they add more data same thing happens it's like okay what what is this

35:01.760 --> 35:05.440
an overfitting problem that doesn't go away as you add more data. Turns out that it's not an

35:05.440 --> 35:09.600
overfitting problem turns out that what's happening is that the structure of the Q learning algorithm

35:09.600 --> 35:14.800
itself actually causes it to perform very poorly if it's not allowed to interact with the world on

35:14.800 --> 35:19.760
its own and it actually ties a little bit almost to that causality point that I mentioned before yeah

35:19.760 --> 35:26.240
see in Q learning you you're making counterfactual queries people often don't don't realize this

35:26.240 --> 35:32.080
where does the counterfactual query come up it comes up when you calculate a target value right

35:32.080 --> 35:36.560
because in Q learning you're saying you took this action you got this reward and you're going to

35:36.560 --> 35:40.000
land in this state but then you're going to run a different policy not the one that was used in

35:40.000 --> 35:44.800
the data so ask your Q function how good that new policy would be you don't get to actually run

35:44.800 --> 35:49.520
that policy you just have to ask your Q function so that means plug in a different action

35:49.520 --> 35:53.440
and that action that you plug in is not the action that was taken in the data and people say okay

35:53.440 --> 35:58.480
that's okay the Q function will generalize and it will generalize if the distribution matches

35:58.480 --> 36:02.560
the distribution it was trained on so you can plug in a different action that's okay but you can

36:02.560 --> 36:06.880
plug in an action that comes from a different distribution and when you optimize your policy

36:06.880 --> 36:11.200
of course your policy is going to find a different distribution in fact if your policy can find

36:11.200 --> 36:15.760
an action for which your Q function makes a mistake and erroneously predicts a very high value

36:15.760 --> 36:19.360
it will find that because that's what you're asking you to do so essentially your policy ends up

36:19.360 --> 36:24.880
exploiting your Q function essentially comes up with like an adversarial action it fools your

36:24.880 --> 36:28.240
Q function to thinking that it's a good one and then because you don't interact with the world

36:28.240 --> 36:31.520
because you don't actually end up trying that action you never learn that it's actually bad

36:32.240 --> 36:36.080
in the extreme case you could imagine there's some action that was never ever taken in the data

36:36.640 --> 36:40.560
your Q function will make some completely nonsensical prediction for it and if that prediction

36:41.200 --> 36:46.000
is large it's a large number then your policy will just start taking that action yeah

36:46.000 --> 36:50.880
so it's not an overfitting problem it's actually this kind of counterfactual out of distribution

36:50.880 --> 36:55.440
action problem and once you recognize it for what it is then you can actually study

36:56.000 --> 37:01.280
possible solutions and so why does the existence of this counterfactual problem

37:02.000 --> 37:08.080
manifests itself more acutely in offline well because in online learning you would still make

37:08.080 --> 37:12.400
that mistake but then you would go and take that action and then you would add it to your training

37:12.400 --> 37:16.240
action but if you're not allowed to interact with the world then you don't get the opportunity to do

37:16.240 --> 37:20.560
we're actually not the only ones to recognize this there was so there was also some wonderful work

37:20.560 --> 37:26.320
by a student named Scott Fujimoto who also had a paper that studies kind of a similar type of problem

37:27.280 --> 37:32.320
one of the things that seemed to work out really well in our work is the particular formulation

37:32.320 --> 37:37.440
for the constraint that you can use to alleviate that issue and that turns out to work actually very

37:37.440 --> 37:43.200
well for a lot of offline problems so we have the evaluation this conference is kind of on standard

37:43.200 --> 37:48.640
benchmark tasks but now we're looking to see if we can use it for actual robotics tasks another

37:48.640 --> 37:52.560
thing that I think is super exciting about this line of work is that once you have this fully

37:52.560 --> 37:58.960
data driven way of doing RL you could also imagine applying RL to domains where traditionally

37:58.960 --> 38:03.680
online active collection is very very hard like for example medical applications you don't want to

38:03.680 --> 38:07.600
run a reinforcement learning agent interact with the real patients but maybe you can get some logs

38:08.720 --> 38:14.800
you know maybe applications for e-commerce for educational support agents you know decision-making

38:14.800 --> 38:19.040
support that sort of thing these are all areas where you can get data but it's very costly and

38:19.040 --> 38:25.120
dangerous to actually have active interactions the example you use in the in describing the

38:25.120 --> 38:34.480
off policy offline RL was something like an image net if image net is the data that the data set

38:34.480 --> 38:40.000
that you're working on what's an example of the kind of the problem formulation or the thing

38:40.000 --> 38:45.440
that you're trying to learn yeah so I was maybe a little quick in saying that so talking about a

38:45.440 --> 38:51.040
data set generally I meant image net size got it so so you know image net is a giant data set that

38:51.040 --> 38:55.120
we know enables generalization it's not I mean it's not an RL data sets an image classification

38:55.120 --> 38:58.800
got it got it but if you imagine that you need similar generalization RL you probably need a

38:58.800 --> 39:06.240
similar scale so in the order of you know millions of samples sure sure and so are there what are

39:06.240 --> 39:12.160
the data sets you know for which you are a experiment or with which you're experimenting with

39:12.160 --> 39:18.720
off policy yeah so in the paper that we have in the main conference we you know this is just kind

39:18.720 --> 39:24.560
of standard benchmark tasks so we basically took regular RL benchmarks like the the open AI

39:24.560 --> 39:30.160
gym benchmarks and we made them off policy so that's that's not a that's not an application that's

39:30.160 --> 39:35.760
just a you know a little benchmarking procedure and how do you do that do you just have an agent go

39:35.760 --> 39:40.240
do a bunch of stuff and then erase a bunch of the data or yeah we just we just have we just have

39:40.240 --> 39:44.640
some existing agent interact with a task save the data to disk and then pretend as though we're

39:44.640 --> 39:50.000
given that data okay but that's just for testing um one of the things that we've been doing since

39:50.000 --> 39:54.640
then which is not part of this paper but it's actually something we'll be presenting at a workshop

39:54.640 --> 39:59.920
is actually trying to collect such a data set for real uh we actually called it RoboNet uh and

39:59.920 --> 40:04.240
this was actually kind of a joint effort with a number of different universities so uh we had

40:05.280 --> 40:10.640
some folks from from Stanford Chelsea Finn and her lab some folks from University of Pennsylvania

40:10.640 --> 40:16.480
uh some folks from Carnegie Mellon contribute data to this uh large data set of robotic interactions

40:16.480 --> 40:20.720
and one of the things that we did that was a little unusual is that it's actually a data set

40:20.720 --> 40:25.440
collected from multiple different robots so these are all robotic arms and they're all performing

40:25.440 --> 40:29.200
kind of similar tasks relocating objects on table moving things around but they're actually

40:29.200 --> 40:35.360
different robots and one of the things that uh we studied in that work is well first you know

40:35.360 --> 40:39.600
can we collect this data can we make it available to the community but also can we train a model

40:39.600 --> 40:44.080
that actually is robot agnostic so you could imagine whatever robot arm you have you take the

40:44.080 --> 40:49.120
small plug it in and maybe we'll uh control that robot we didn't quite achieve that so we couldn't

40:49.120 --> 40:53.680
get a single model that actually generalizes zero shot to new robots but maybe somebody else we

40:53.680 --> 40:58.480
get to work later but what we did achieve is we managed to use it as effective pre-training so you

40:58.480 --> 41:03.440
can take all the robots but except for one of them pre-trained and then get a new robot get a

41:03.440 --> 41:08.480
little bit of data and then fine tune and that works so far and you know maybe with a data set out

41:08.480 --> 41:12.880
there and available perhaps people can take it and see if they can move towards zero shot

41:12.880 --> 41:18.480
generalization and of course since it's a fixed data set it's kind of an ideal fit for

41:18.480 --> 41:25.200
fully off-ball CRL research. Well Sergei thanks so much for taking the time to chat with us give

41:25.200 --> 41:30.560
us an update on what you're up to sounds like a bunch of really interesting stuff. Thank you.

41:30.560 --> 41:40.720
We'll have lots of homework to do after this podcast. Thank you. All right everyone that's our show

41:40.720 --> 41:48.960
for today. For more information on today's show visit twomolai.com slash shows. As always thanks

41:48.960 --> 42:04.880
so much for listening and catch you next time.

