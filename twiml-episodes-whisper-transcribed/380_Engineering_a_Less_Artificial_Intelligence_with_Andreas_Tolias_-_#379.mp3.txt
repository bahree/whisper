Welcome to the Tumul AI Podcast.
Hey, what's up everyone?
One of the questions we asked in our recent machine learning development and deployment
survey was the extent to which respondents' organizations had established a platform
to facilitate the delivery of machine learning models.
Nearly 85% reported being somewhere on this journey, from not yet having established
a platform but working on it to having one or more environments already in place.
With so many organizations investing in building out their machine learning tooling, it's
no wonder that the top-voted topic at last year's Tumul Khan Unconference was a discussion
on whether you should build or buy your data science platform.
I'm looking forward to taking on this topic with Kenny Daniel, the CTO of Algorithmia
in an upcoming webinar on Tuesday, June 9 at 10am Pacific Time.
I'll be sharing some more insights from our survey, and Kenny and I will be discussing
what goes into building a machine learning management platform.
How to make the business case for MLOPS at your company, and how to evaluate off-the-shelf
machine learning management solutions.
I hope you can join us.
To learn more or register, go to www.tumulai.com slash Algorithmia.
And now on to the show.
Hey everyone, I am here with Andreas Tolyos.
Andreas is a professor of neuroscience at the Baylor College of Medicine.
Andreas, welcome to the Tumul AI podcast.
Thank you.
How are you making out down there in Houston?
It's good, we're trying to survive, trying to maintain our social distancing, but so
far so good.
Good, good, good.
Why don't we get started by having you share a little bit about your background and
in particular how you came to work in the nexus of neuroscience in AI.
Did you come at it from the medical side or in biology side or from the computational
side or a little bit of both?
Yeah, mostly from the neuroscience side, the scientific side.
Although I did my PhD at MIT in systems and computational neuroscience, and I always
had an interest in how we can use our understanding of the brain to, you know, or test our understanding
of the brain by mimicking its capabilities in behavioral, such as visual perception,
maybe motor control and other things and decision-making.
And so I've always had this interest in how do we bridge neuroscience and AI together.
And at the same time, how do we use tools from machine learning, which are just, you
know, other form of statistical tools to analyze the huge amount of data that we are right
now are capable of getting from the brain.
So the sort of the information goes both ways in the sense of we use tools from machine
learning and AI to study the brain and by understanding the brain, we hope to advance
this field of AI.
So that's sort of my background in tweet, but I myself, most of my training is in systems
neuroscience and neurophysiology.
You mentioned that we have access to lots of data coming from the brain.
Where is that coming from? Is this like MRI type of information or that many, many, I
would say in the last 10 years, maybe a little bit longer, we have the capability because
of new technologies and also, you know, things that we can parallelize, record a huge amount
of data from the human brain, starting from the human using a MRI, but now also like
maybe high density, also human recordings from neurosurgical patients.
But in particular, also in the animal, we in different animal species, we have new technologies
to record large, very large numbers of neurons, either using electrophysiological methods
or imaging, in particular two photon imaging and calcium imaging has enabled us to record,
you know, there are now, data sets are in order of 10,000 neurons recorded simultaneously
from the brain of an animal doing a task.
So, and these data are huge in terms of both size and complexity and, you know, we cannot
just look at them to understand them, we have to analyze them and one of the methods that
we use to analyze them is actually machine learning and in particular, my team and my collaborators
and also other people around the world are using deep learning to model the brain or model
this data.
So, on the one hand, you can think of it like we have the technologies to record a huge
amount of data, we have the tools now from machine learning and in particular, deep learning
to model this data and build predictive models of this data.
And given that we have these predictive models, what we can do is that we can analyze the
model in silico of the brain and run an unlimited almost number of experiments that we could
not have done in the brain by itself in the first place.
For the paradigm, we call inception loops that where we start from an in vivo experiment
will build an in silico model of that system we want to study.
In our case, it was the visual system of the mouse or the macac.
And then having this in silico model, we can ask questions that would have been very
hard to ask without the model in the first place.
So, we can ask the model, for example, to find whether the optimal stimuli that these
neurons that were recorded lie.
And then we call it a loop because then we go back in vivo and test the predictions
of the model and to falsify them or verify them.
And that we can also, in principle, run this in multiple loops where we can improve the
model.
And so, that's kind of where deep learning, one aspect where deep learning, which is
a subset of machine learning, is helping us analyze and understand the brain.
So we've used this and we had a paper come out last year where we've shown that a very
old, you know, understanding of the mouse visual system in this case, we found new principles
of how it's organized.
And in particular, we found that this early visual areas like primary visual cortex had
preferences that seemed much more complex than people had previously thought.
So that's one area where deep learning and AI has helped our understanding of the brain.
You mentioned earlier, you kind of used modeling the data and modeling the brain interchangeably.
You know, is that always the case that when you're modeling the data that you have somehow
gotten that kind of describes things that are going off in a brain that you're building
a model of the brain itself?
Or does that data always capture?
I think of the latter, like modeling the brain is kind of modeling its structure, whereas
the data could be kind of absent of that structure.
Yeah, well, it's both, right?
So the brain, modeling the brain, you can model it at different levels, right?
You can, if we start on the top, you have the behavioral level.
Okay.
So you model behavior without necessarily trying to model their presentations or faithfully
model their presentations that are happening inside the brain, either neurons.
So you could let a build a model of human behavior and given a certain input to predict what
how humans are going to behave and cognitive neuroscientists and psychologists have been
doing this for many, many years, many cases very, very successfully.
So you could let a build a model that tries to predict, as I said, to human behavior
if you want, but with, but big, agnostic or not trying to faithfully capture the neural
representations.
Then if you go a level down, you could say, okay, I'm going to build a model that captures
the behavior, but I'm going to, I'm going to ask the model to also capture the neural
representations of how that behavior came about.
And in this interesting debate in the field because there are some people or ideas out there
that say that once we impose that constraint, the behavioral constraint, then maybe of these
models without even trying to hard, they will start capturing these neural representations.
Alternatively, there's many ways to solve a problem and the brain happens to be one specific
solution.
And for example, we know that, you know, we take the field of let's say object recognition.
There are other species that can do object recognition like birds, but they don't necessarily
have the same architecture or the same structure as our brain.
So they're, but they have some similarities and differences, so that becomes interesting.
So you could say, I'm going to model the brain and David Mar is famous for it because he came
with these different levels of analysis where you can model it at the behavioral level
or we can model it more at the representational level, which is by recording these thousands
of neurons and trying to build the model that is not only achieving the right behavioral
goals, but it's achieving them through the right representations.
And then you could even go at the incorporate another level where is the one you were talking
about, which is the structure you're saying the brain has a specific wet wear architecture
and is relying on this specific architecture to achieve those representations, which in
turn will achieve those behaviors.
So if I want to fully understand the system in some ways, and in particular, if I want
to be able to fix it if it breaks down, you know, psychiatric diseases, I have to go
the way to the implementation level details.
Like for example, I have to take into account that the brain has, you know, different
layers, let's say, you know, the cortex has different layers, different cell types, because
there's evidence from, you know, work done in animal models of diseases and in human diseases
that a lot of these diseases, mental diseases affect specific circuits and specific parts
of the brain or specific cell types because of often they are genetic underpinning.
So if you want to build a model to understand how the brain achieves the behavior that
it achieves, let's say I was talking now, you know, I saw you a couple of weeks ago, we
still remember your face stuff like that.
If we want to understand it and be able to fix it in case it breaks down, we need to go
down to the implementation level details.
It's not enough to stay at the very high level because not fixing.
Okay.
So you've got these different kind of levels of modeling the brain and, you know, essentially
we're taking, you know, data from the brain and using them to build these models at various
levels to kind of ultimately inform our understanding of how, you know, either, I guess it depends
on what level you're at.
How humans work or how the brain is working and we're using machine learning and deep
learning to kind of further that understanding, but you've also talked about going the other
way where we're using what we know about the brain to enhance our understanding of machine
learning and deep neural networks.
And you recently wrote a survey paper on that work, engineering a less artificial intelligence.
Can you kind of introduce that work to us?
Yes.
So what, you know, with my colleagues and I we wrote is a perspective on how or what some
ideas or some approaches for neuroscientists and AI scientists and machine learning engineers
to try and use neuroscience to advance AI and in particular, deep learning.
So just to give you to put this into perspective, in this paper first, we give an introduction
into very brief historical introduction into what is now called deep learning.
In particular, underscoring the point that there are many approaches to solving AI or
to from an engineering point of view, there's probably many different ways to achieve AI.
But what seems right now, one of the most promising ones, which is old in the sense that
people have been thinking about is since the 50s or 60s and 70s and so on, is by building
what's called artificial neural networks.
So it's saying that our brain, its key computational ingredients in neuron and the signups, that
is plastic.
And that's how we learn, right?
So if I have a neural network and it performs some input output non-linear transformation
and I change the signups, is I can achieve computation.
So the idea is that that's how the brain does it.
So let's try and mimic that into a revert, you know, engineer systems that do something
like that.
This has been the big revolution or success in the last 10 years in deep learning.
That's how these networks are trained to play chess, go, do voice recognition, how Alexa
works, Siri.
You have some sort of, at least part of it is some aspect of a neural network with a lot
of engineering put into it to optimize it and make it work.
And what we've learned also in the last 10 years, because we have very strong, you know,
very good, you know, fast computers and we can parallelize this, is that if you have
a scenario, and I'll give you, let's say, an example, let's say you chess and you allow
to computers to play against each other and they essentially play millions or maybe tens
of millions of games that two humans could never play.
And in this type of well-defined problems, humans can not out-compete computers.
And if these problems seem very, very difficult to us, like playing chess, go, you know, things
that are or doing very fine discriminations, as long as the computer has the ability to
be exposed to a lot of training examples.
And because it's running on silicone, it never gets tired.
You know, you can parallelize it and run, you know, on a super computer and it can play
day and night for, you know, equivalent to probably thousands of years of what it would
take a human.
It's game over, right?
They cannot, no human, I mean, it's very hard for humans to compete where the computers
or deep learning fails right now is when you change the testing distribution, even slightly
from the training distribution, okay.
And my colleague Matthias Betke, who is a coder on that paper, has done a lot of interesting
work on this and I'll give you one and others, of course, but I'll give you a simple example.
And one of the poster trials of deep learning is object recognition.
People have trained, and the famous example is image net.
So there was a heroic effort where people sat down, they took maybe a million images,
they labeled them into a thousand classes or more, you know, and then they trained in
your own network to say, this is a bird, this is a dog, this is a cat, this is a car,
this is a bus, this is a spoon and so on.
And when you train these, you find that these networks become pretty good.
I mean, not as good maybe as a human, but in some classes actually even better, like
things that, you know, what type of bird it is maybe, and because most humans are not
experts, a network can learn that even better.
But they're very good, okay, and we can use this and, you know, Google, if you go and
do a Google search, they use algorithms like this.
But if now the network has only seen clean images with no noise added into it.
If you add noise onto these images, even random noise, that to a human, it would not deteriorate
that perception at all.
We would still see this as a cat or as a dog, this network is completely confused.
And they quickly got a chance performance.
And one of the sort of puzzling thing that was discovered early on in the early days
of deep learning is something called the serial attacks.
So if you take this image, if you take an image of a dog in the network, you can change
a few pixels in a specific way that you can fool the network, thinking that right now,
that now it's a cat, and this is called the serial attack.
And these by itself, and these changes are imperceptible to a human.
So they are so small changes.
So the fact that these networks can get confused and be very confident, that something that
it's imperceptible to a human, which you can think of it as, if I change a few pixels
in an image, there's no way that the physical objects out there are changes, right?
Because ultimately intelligence is about inferring what's the correct causes of the images
that we experience, let's say in vision, right?
So if I am in the jungle and I see a lion, I need to perceive it as a lion and notice
my child, right, that I may be hiking with.
So the fact that these things can get fooled so easily is a testimony that they're doing
something qualitatively very different than the way human and animal brains work, okay?
To summarize it, it has to do with the way they get trained, and they have to do with
the learning algorithm in particular, you know, but they're basically, they are, you
can think of it, they are trained by a brute force approach, where for an network to learn
that this is a cat, it has to, the way it learns it, that this is a cat, it's definition
of a cat inside its hidden or latent representations, seems to be very different than our representations
because of this example.
And that is why it seems to fail.
Now there are similarities in this very nice work, that by a bunch of my colleagues, both
in neuroscience and in AI, that show there is quite a lot of similarities between our
representations in neural networks, but they also find, and we, and others find a lot of
differences.
And I think there is a qualitative difference, that is strong enough, that makes them so
prone to things like adversarial attack.
Dollar runs for one set, there is the known nodes, the known unknowns, and the unknown unknowns.
These networks are very bad in the unknown unknowns, if they're faced with a completely
new situation, they do not know what to do with it, whereas we humans are much more flexible,
you know, and that is what gives us, right, and animals are also like that, there's also
working animal behavioral work that they can do it.
So that's what gives biological brains a superiority in terms of their ability to
generalize outside their training distribution.
And the paper, the perspective that we wrote is trying to make a proposition if you want
or what are the areas that neuroscientists can study the brain to extract what we call
an inductive or also a model bias, to then try and advance this field, you know, try
and make machines smart.
And we have some propositions, and it actually goes along, again, these three levels of David
Mark, including a relearning rule, we know that right now the successes in deep learning
have been with what we call supervised learning, some with reinforcement learning, but we humans
and animals rely a lot on self supervision, active learning or active learning where we manipulate
objects in the world, we do cause a leave you on manipulations in the world to try and
understand how the vision works in the world so we can perceive it.
And you know, so you could imagine things at the behavioral level where you change, you
build the CERABOT or you build some agent that tries to mimic a child the way they learn
at this behavioral level and people working on that, or you can even go a step further
down into this representational level and try and understand whether the principles,
the way information is represented in the brain.
And then you can go into the learning rule itself and also put even more architectural
constraints.
Now, another thing that is very interesting is like when we are bored, in particular other
animals, you know, a horse is bored and you can immediately start galloping or not galloping
but walking, right?
So we, we've learned at multiple times because it could have the evolutionary timescale.
So we do not come out as a blank slate necessarily like we have some capabilities and different
animals have different capabilities depending on their, you know, environment.
So that's kind of the overall idea is like how can we learn what we call the model bias
or the inductive bias from the brain, although those are not exactly the same but they are
related, to then transfer that information into a neural network of a device that we
be able to generalize outside this training distribution better.
And this is still a major challenge.
I would say in the last ten years, one area that deep learning has advanced very little
is this outside of the training set generalization and the way that people do this by a brute force
approach right now.
Sure.
Data augmentation and domain adaptation and things like that.
Kind of throwing more data into the training set, whether it's real or artificial.
Exactly. And that may work again, if you are an engineer that works in Google or Uber
and you want to build autonomous driving, you know, they are approaching the problem
by that, okay, I'm going to like collect data and all kinds of conditions, rains, snow,
clouds, you know, whatever, right.
And then and I think although it may end up working in the end, it's not very energetically
efficient because you're not really understanding three intelligence.
You're basically just getting humans to level more and more and more data until you cover
the whole distribution.
You're never going to be surprised.
But that's not I think the most exciting area of AI, the most exciting area of AI is to
figure out how to do this with a few examples and by trying to understand how the brain can
achieve this.
Yeah.
Yeah.
You were careful to point out that there's difference between inductive bias and model
bias.
Can you elaborate on that difference and how it plays out in this scenario?
Yeah.
So the inductive bias or a learning bias comes from the, you can think of let's say two
networks that have a different architecture and different long linearities and they will
have a different inductive bias, okay, because they will in a simpler way, you can think of
it.
I'm trying to fit some data.
And if I have to rely if my architecture gives rise to a particular order of a polynomial
or another order of a polynomial, then that's an inductive bias.
Then once you learn from the data, you also learn a model bias because if in a good example
is what you just said, when you do data documentation, you know, you're basically learning maybe
a better model bias because you're getting trained now on an adversarial training is one
example, right?
Where you're now getting trained on many different modifications or augmentations of this data.
And now you are better at generalizing and people have shown that the best way to defend
against adversarial attack is by, you know, training in a adversarial scenario, which is
basically a better model bias, even if inductive bias, the architecture may be exactly the
same.
The brain is very interesting because it does have a very strong inductive bias because
as I said, the horse is bored and it has some, that network has not experienced the world.
There is information is genetic code that wires it and then you get a network that does
something.
So, you know, and then the experience fine tunes it and improves on it, but it has a very
strong inductive bias.
As a proof that they are easy, we were to understand the how to read out this inductive bias.
You know, then we will be able to build smarter machines.
Now, there is another complexity to this that the learning algorithm may be tightly interwined
with this architecture.
So you may have the perfect architecture, but if you use a different objective function
on different learning algorithm, you will not get now the right performance or the right
model bias.
So, that's how these things are related.
And so, how are we doing on this task of learning from the brain and applying it to deep
learning?
Yeah.
I don't get the sense that a lot of the most important things that we've learned about
making deep learning work like, you know, drop out and, you know, learning rate tricks
and things like that came from biological inspiration.
Yeah.
That is true.
And this is, so it's very interesting because I think that's a very interesting question
also from a, you know, the larger scale if you want and I'll elaborate.
So if you, if you try to understand the brain, right, and you say, you know, in our case,
we try to understand vision and visual perception, right?
In some ways, a good test of it would be able to build a system based on what we think
we understood that does vision, right?
Because if I say, I understand how object vision works in the brain, I've done some experiments
in, you know, in humans, animals, whatever, and I've studied these principles, then my
long-term goal or as a field should be, can we now assemble this principle into a system
that mimics the behavior of the system that I'm trying to understand, right?
And that is essentially where the AI's goal is.
You're saying from a neuroscience perspective, you know, we've got these models of, you
know, cones and layers and all these things, you know, forget about this deep learning
stuff.
We should just take our models and kind of implement them and they should be better
in theory.
In theory, in theory, we understand it.
I mean, if we really understand, once we fully understand how vision works, right,
we should be able to reverse engineering, we should be able to take all these principles
and put them together and it should perform the task that the human visual system does.
I mean, that's sort of the, it's a very stringent test over hypothesis, right?
You have to test them like that.
Now, I'm not saying it's going to be achieved in our lifetime, maybe, but who knows, or
it could take many, many years, but that's the goal of neuroscience, right?
Ultimately, not necessarily now, but we are far away from that goal, right?
I mean, it's very, you know, there's very, there's only sort of toy examples where we've
taken these principles and we've shown that we can achieve robust vision, you know, that
we can achieve the zero-bust optical recognition.
Is that based on gaps in our understanding or our ability to implement what we understand?
No, I think it's based on two things.
I think, and this is a more pessimistic view.
It's based on our gab of understanding principles.
I mean, right now, we've been doing more, and this is maybe, you know, it's a very difficult
problem, right?
And there's very, there's a lot of technology, I said earlier, that gives us incredible
capabilities to understand the brain.
But new principles are much harder to understand, fundamental principles.
And then we get lost in details and we humans don't know how to, I mean, you know, we don't
just copy it because you see there's a difference also between understanding and, I mean, if
we could take the brain and copy it piece by piece, we would, maybe we would achieve what
I just said, but it would still not be satisfying from a scientific point of view because
it wouldn't be to understand it, right?
If we just categorize the cones, the receptors, the wiring, and we just like copy it over,
and it works, what have we understood is like, I don't know, like, it's like me copying
a foreign language, I faithfully copied it by understood nothing.
So we don't want to do that either.
So that's why it's more complicated, right?
We need to understand the principles that guide these organizations.
And then we may not even have to worry, hopefully we'll not worry about all the details, you
know, the way we understand the principles of flying and we build aeroplanes.
That's kind of the idea.
We don't try and copy every single feather of a bird.
Now, on the other side, my children has been much more successful in building systems
that work, right, like, you know, if you use an AI system, it's built by machine learning
engineers and not by neuroscientists.
And you're right that a lot of it is brute force engineering and often, or most cases,
is also there's no understanding.
It's not like people that do the engineering of the machine learning side.
I mean, there is some understanding, but it's very, it's a lot of black magic, you know,
it's like a black box approach, you tune things, and then it makes it work.
We do understand some principles.
And in some ways, that's sort of maybe how the brain works too, like, for example, a deep
learning has the deep layers, it has the bug propagation, it's a principle.
So I can, we can write down a recipe of, you know, a few things that then when you train
it, it will do stuff.
But you're right that we haven't been very successful in translating neuroscientific
understanding to machine learning in a direct way.
It's mostly an inspirational way, like if you talk to AI people, they say, oh, yeah, I
use reinforcement learning as an inspiration.
I use, we call them deep networks comes from neuroscience because we know that in the
brain, there's many layers, you know, the first deep learning model was completely inspired
by Fukushima, who was a neuroscient, I mean, not a new computational neuroscience person
or inspired person that, so there are fundamental ingredients in deep learning, but a lot of
it is right now brute force engineering, okay, with a lot of ideas.
So I do think that the dialogue is very fruitful.
I mean, we ourselves have some ideas on how we are trying to do this, and we had some
papers that are mentioned in that review too at the representational level, but it hasn't
been, is not a, you know, I think the big breakthroughs let's say in this field of taking,
of understanding neuroscience principles, A, and B, translating them in a way that you
show a qualitatively different performance in a machine learning case has not been demonstrated.
Yeah.
I mean, I hope it will be and hopefully soon, but I think that's going to be a major
breakthrough because it will give us, if you want the bridge on how to go from one to
the other, but it's not easy, I agree with you.
And a lot of it, I think, comes from our inability to extract principles from neuroscience.
Such as your paper, help us understand where we should be looking, you know, framework
for the different places where we might be likely to find this kind of insight.
Yeah.
I mean, there we are again, drawing on three levels plus the learning on the behavioral level,
or on the cognitive level, you know, you can, and a lot, I mean, you know, this is not
a new idea and a lot of people in the field in machine learning are very interested in
this and exploring it is when you, you try and change the learning objective if you want.
I mean, let me give you an example, instead of let's say just learning object recognition
by just giving labels, you could maybe give a few labels, but you could have an agent manipulate
the objects, right?
And maybe move them around and maybe break some every now and then.
So you get an idea of the physics in the world on the other level.
And this is an area that we might team and my collaborators were doing a lot of work on
this is trying to understand it at the representational level.
We know that let's say the Maccag or the human visual system was in the order of 30 visual
areas.
And although we know a lot about it, we also lack a fundamental, let's say something
that everybody agrees on, what is the role of all these even 30 areas?
Why do we have 30 and not five or why, you know, there's this idea of eventual and
dorsal stream, but then some people say, well, that's not quite true, maybe it's like
what more and more than, you know, like it's more of an action stream versus a perception
stream.
So to understand how the brain does it, we have to record from individual neurons and
we have to have ways to and we've developed some metal based on the learning the system
identification, the assumption loops to try and decipher what are the representations
of the tuning functions of this neuron.
And these are some mathematical principles that dictate these tuning functions or do they
look like they are not interpretable.
And I think one key thing here that we find both in neuroscience and in AI is they do
have interpretable AI or interpretable.
And that's a struggle both for neuroscientists and machine learning people.
One theory in the field in both sides is that if you do not have interpretable a colleague
of mine actually has a paper he's writing where he's trying to relate interpretability
with robustness.
So this idea that if we do not have interpretable systems and the representations are not
interpretable, then a is going to be very hard to trust them necessarily.
And this is an issue of deploying AI technologies, let's say, for radiology readings.
Although others say, what do you care about?
The human level interpretability is not possible even in the brain and we've kind of been
fooling ourselves by saying, we understand this area, does this and this area, does that
that is humanly interpretable.
But I think that's an interesting conversation and debate that we will see more of it
of interpretability.
And I think if there is such a thing as interpretability and we understand it in the brain, then
it's going to be easier for engineers to like say, okay, now we know that thing exists.
Let's try and figure out how we can put it in the brain.
And that's at the representational level where we'll call from yours and this is what
with most of the perspective paper focuses on this representational level.
If we need to figure out interpretability in the brain before we're able to get to interpretability
or explainability in machine learning models, I think we're probably in trouble.
Yeah.
That is true.
But it's interesting though, because the tools that people in machine learning are developing
to gain interpretability, it's exactly the tools that we and others are trying to implement
in the brain.
So that's where there's a very strong link again between the two fields.
Are there examples that come to mind of successes that we've seen already in, you know, pulling
over understanding that we've gained on the machine learning deep learning side to the
neuroscience side?
Yeah.
So we had a paper in NIPs last year, Julie was the first author and what Julie did is he
took recordings in this case from the mouse visual system and built, we built a model actually
this inception loop model.
And then we generated a similarity matrix, basically we showed, let's say, 1000 natural
images to the model and we computed the similarity matrix between image I and image J in the
models, the brain models neural space.
Okay.
And then we took a neural network and we tried instead of just teaching it to say cats
and dogs, we also tried to make its representations to look more similar in terms of these representational
similar metrics to the ones we measure in the brain.
So in some ways, you can think of it like if you have a deep neural network, you have
some laws on the top layer, which is what's the class, but all this stuff of this latent variables
are let free to do whatever they want, right?
So we tried to put some constraints in these middle layers and say, let's try in one of
these layers to make it look a little bit more like the brain of a mouse.
Okay.
And what we found was that that model became more robust to perturbations, it was interesting
because it became more robust to high frequency noise perturbations, which we are now studying
and we can explain in a simpler way that it may be like, you know, the mouse is more sensitive
to low spatial frequencies, but this is an at least a proof of concept that, you know,
even if this case, it may be a simpler scenario that you should, if you know, that you could
learn something at the representational level in a system and then translate it over.
We have the tools to sort of move it over because if we have a model of a brain, there
is a deep neural network and it's trained on data and then we have a neural network model
that tries to do object recognition.
They're both models, they're both built of the same ingredient, which is neurons with
synapses in silicon, so we can try and now transfer information from one to the other.
Now that's still not, even if we're in my personal taste, even if we were to do this and
we would achieve, you know, something incredible, let's say, so that the serial attack, it would
still not be very satisfying if we don't understand why it's solving.
You know, the mere co-being operation itself, you know, it may work, but for me personally
to be satisfied, I would like to understand why it's working and which again goes back
to the some aspect of interpretability in some ways.
It almost kind of points to this, at least you're your limited experiment, you know, kind
of calls to mind this future where instead of pulling a, you know, building a network
up from convolutional layers and pooling layers where, you know, I'll, you know, use a mouse
layer and a flatworm layer and a cat layer and that kind of thing.
Yeah, yeah.
And I think the other thing is to show the utility of these systems, we need to come also
with the right machine learning benchmark test.
And right now, the focus in machine learning in the last 10 years has been, again, to create
benchmark tests that were very difficult in the 90s and in the 80s for computers, like
optical cognition, right?
But there hasn't been as much or there's more now, but where you train, you know, you
want to build a visual system that has all the possible visual tasks, right?
It's a segmentation, optical cognition, motion tracking, action recognition, it can work
with video, you can work with study game, like the way biological system works, right?
And from a, any male sort of to do with who is working on the problem, like if you have
engineers, especially working in a company that they want to build autonomous driving, they
don't care if that system, that static optical cognition, because they'll never be tested
on static images, right?
You know, all that same system is not the one that's going to talk to you to the driver,
right?
So, but from a, if you're interested from the AI, more scientific, AI as a science and
not as an engineering field, you want to understand which is the same as neuroscience in some
ways, what are the principles that enable the same network to perform all this task and
also do transfer learning from one task to the other very fast?
And one of the areas that seeing some interesting research on the machine learning side is the
idea of multitask learning.
So, you know, we can train these networks to do two things at the same time instead of
one or end things at the same time instead of one and there, you know, that somehow has
this kind of regularizing, you know, a thing that makes them perform better.
Is there any kind of biological inspiration to that or biological parallel to that?
Yeah.
Actually, we discussed that in the paper also multitask training and when we see the world,
we don't just do object recognition, we know the curvature of an object, the distance
of the object, we can grasp it, we have some idea about the texture, you know, we can,
to multi-scale perception, like we can look at the individual, you know, eyebrow on someone's
face and also recognize their face, we can, you know, do, you know, figure out if someone
is sad, so we do a multitask, multi-futural embedding and if you want, the same brain is
doing all these things.
So that's one of the areas where we think the brain is very different than a machine.
And I think there are people that are saying, okay, maybe we should like, we train one
everywhere on all the tasks, maybe we'll generalize also better, it will be more robust,
it will be, you know, so that's sort of an interesting direction that we and others
are interested in, you kind of, you're absolutely right, that's very important.
The problem again becomes how do you train this because if you rely on human labeling,
all these data, then you need to like have humans labeled the color, detect, and then
you're sort of again limited by, you know, yeah, but you could like approach it in a brute
force approach at least in toy examples to show that this is the right direction.
Is there any one particular direction in kind of this entire space, biological systems
to deep learning, deep learning to biological systems that you're most excited about?
The good question, I think that I'm mostly, I think right now in the next few years,
the most exciting directions are at the cognitive or behavioral level and at the representational
level because of, you know, like, and it may be a practical thing because we have good
baseline models that relay, they have a cost function, what they're training and they
have representations that we can measure, right?
And I'm mostly excited about these two higher levels in terms of building models that are
going to get information at best or inspiration, you know, more, in a less maybe ambitious
way from the brain to advanced AI.
If you go down to the implementation level, although I think that's, you know, it may happen
at some point, there's too much complexity down there and we don't understand why there
is this complexity.
It could be implementation, it could be biological way we are constraints, for example, energy
constraints like, you know, the brain has chemistry and it doesn't have silicon and then
synapses need neurotransmitters because of biology and then you build this complexity.
So trying to like understand, you know, without a clear understanding and let me, the classical
argument is even the spike, right, neurons have spikes, neural networks don't, they're
not spiking, you know, so people are still debating and there hasn't been let's say golden
success in spiking networks that can do something that, you know, the standard neural networks
cannot do.
So I would say the first sky or two levels, at least right now in the next few years
are more promising in my mind to see these transfer of knowledge.
Well, Andreas, thanks so much for sharing with us what you're working on, really great
to speak with you.
Thank you for having me and have a nice day.
Thanks Andreas.
Alright everyone, that's our show for today.
For more information on today's show, visit twomolai.com slash shows.
As always, thanks so much for listening and catch you next time.
