Welcome to the Tumel AI Podcast.
I'm your host Sam Charrington.
Hey, what's up everyone?
Before we get to today's show, I want to send a huge shout out to our friends at Waitin
Biasis.
Last week, we premiered my conversation with WNB Founder Lucas B.Wald on our YouTube
channel.
It was a great conversation about managing artifacts in the machine learning life cycle,
and you can find it at www.tumelai.com slash artifacts.
Waitin Biasis is a lightweight toolkit for machine learning practitioners.
Reviews their original experiment tracking tool for quite a while in the Tumel community
as part of various study groups, and folks have found it to be super useful.
Now that they've added data set versioning and model management capabilities, you've
got a one-stop shop for managing and visualizing your complete machine learning pipeline.
If you'd like to learn more, Waitin Biasis is extending a special offer to Tumel listeners
and viewers, including unlimited private projects and priority support.
For more details or to start tracking your models today, visit WNB.com slash Tumel.
And now on to the show.
Enjoy.
All right, everyone.
I am here with Sherry Rose.
Sherry is an associate professor at Harvard Medical School.
Sherry, welcome to the Tumel AI podcast.
Thank you for having me.
It is great to have a chance to chat with you.
I'm looking forward to digging into your background and your research and the things you're
doing related to COVID to help out there.
Let's start at the beginning.
How did you become interested in machine learning and the intersection of that and healthcare?
I always was very interested in science and mathematics and physics, and I didn't really
have a good sense of how you could use that to solve problems when I was going to college.
And it was during college that I was exposed to this summer program called the Summer Institute
for Training and Biostatistics.
And it really sounded like what I was interested in, which was bringing quantitative reasoning
and thinking to problems in health and public health.
And I realized very quickly that I needed more than my bachelor's degree in statistics
in order to really solve a lot of those problems.
And I didn't actually get any training in machine learning in my bachelor's degree.
I graduated in 2005, and the curriculum definitely did not include it at that point.
And so when I went to graduate school at UC Berkeley in biostatistics, that's where I saw
the benefit of having really general frameworks in which to solve problems.
And that's when I started working on non-parametric machine learning and having these kind of
big picture ways to attack big problems in population health.
And that was for me, that's been both machine learning in non-parametric models for prediction,
but also causal inference.
And the driver for me was really the ability to use these flexible tools to solve hard
problems in health care and medicine.
It must have been helpful having that undergrad in stats.
It's been very helpful.
I actually started as a mechanical and aerospace engineering major.
And I did not feel very invigorated by the coursework there.
And I also was a little frustrated that I was often the only woman in the classes.
And there was a lot of reasons why I didn't feel like the right fit for me.
I ended up taking my second semester in college statistics course, and I immediately saw
how statistics could be used for solving lots of different problems.
And engineering can as well.
But for me, the statistics was really how I saw bringing all of my interests together.
You mentioned non-parametric machine learning.
What is that and how does that relate to both the broader field as well as the health
care field?
Yeah.
So when I talk about non-parametrics, I mean it in the very broad statistical sense.
A non-parametric model is a larger model space where we're making many fewer assumptions.
And whereas with parametric models, more standard parametric models, we might be making really
strict assumptions about the functional form, the underlying unknown functional form of the
data.
With non-parametrics, I want to really have a large model space, so I have a much better
opportunity to uncover the truth with my machine learning estimator.
So meaning like you're not assuming a normal distribution which has a couple of parameters
and meaning and a standard deviation, it can be anything.
Definitely not.
Definitely not.
That would be a limiting assumption in your work?
Absolutely.
And most of the data that I work with does not conform to those types of strict assumptions.
Talk a little bit more about the scope of your research interests and where you apply
machine learning.
It sounds like you are interested both in the kind of the systematic issues, the health
care system with the relationships between the providers and the payers, as well as clinical
issues.
Absolutely.
Our services research were really interested in the whole broad scope of the health care
system that includes costs, quality, access to providers and services, and also a health
outcomes following care.
So that clinical piece often comes into the health outcomes following care.
And some of the major areas that I've worked in intersect with the health spending aspects,
the financing aspects like mental health and telemedicine and cardiovascular treatments,
all of these things intersect within this system that relies on the cost, the quality and
the access to providers.
So it's a really having a research program that encompasses both pieces of that can allow
you to ask and answer questions in more integrated ways.
It's difficult, but I find that if you understand those underlying systems and try and bring
them into your work when you're looking at clinical work, it can really help you.
You inform better answers.
And when you are looking at those kinds of questions, are you primarily trying to understand
or influence?
Great question.
So a lot of the work that I do, we are trying to understand some kind of phenomena in the
system, but influence, yes, in a sense, that we're trying to inform policy.
So understanding the comparative effectiveness of multiple different types of treatments,
I would like to understand which treatments have better health outcomes.
But if we find a particular treatment has very bad outcomes, we want to inform policy
to the FDA or to the relevant stakeholder in order to potentially have that treatment removed
from market.
And we're talking towards the end of April, many of us have been in some form of another
of lockdown due to COVID.
You mentioned that your dog may start barking in time.
He may, he may.
My neighbor just, I think my neighbor is finished cutting the grass now.
This is just the times, but it sounds like your work intersects with COVID as well.
Can you talk about that intersection a little bit?
Absolutely.
A large focus of my work, because I'm so integrated in starting with the substantive problem
and bringing either existing machine learning tools or developing new machine learning tools
to answer those questions, it really, there has to be the strong grounding in data.
And the coronavirus pandemic has really illuminated for a lot of people how much we need to care
about data.
And I, I mean, we have misclassification, we have missingness in the types of data that
we're collecting for coronavirus, both for cases and mortality counts.
And these are things that are very, very common in most of the electronic health data that
we use in the healthcare system, where a lot of my work has focused on dealing with some
of these types of issues.
I mean, we use billing claims, we use clinical records, registry data, and on and on.
And these data types were not designed for research.
And so we need to be really aware of the issues in these types of data.
And some of the newer forms of data, like wearable and implantable technology that people have
been very excited about, measuring physical activity.
We're now using in the coronavirus pandemic, you know, smartphone location data to try
and understand how people are social distancing or with potentially with contact tracing.
And then digital types of data like Google search trends and Twitter data, which has been
used for different types of research questions in the past.
Now Google is developing and has released this location history website where they're
showing about, you know, how we can understand social distancing.
And so a lot of the data related work that I've been focused on is very relevant to the
pandemic and understanding our data sources and trying to bring rigorous flexible methods
to them.
Specifically, I had been working the last two years with my now former postdoctoral fellow
who's an infectious disease expert.
My am a jummer who's now faculty at Boston Children's Hospital and Harvard Medical School.
We had been looking at news media data, CDC data and electronic health data to understand
the generalizability of these data sources for both infectious disease and chronic disease.
And now this has become very, very relevant to the coronavirus pandemic.
We had been one of the conditions we've been studying was, was flu-like illnesses and
understanding, you know, what electronic health data sources like billing claims and electronic
health records, what we can really understand from these data sources.
And we've seen people, many people now start modeling and making projections about cases
and death counts.
What we're going to start seeing next, once people start having access to different types
of electronic health resources is trying to use this data to understand, you know, to
predict outcomes, maybe to predict clinical courses or to try and do causal imprage,
which is even more difficult.
And it's very, very important that people understand the limitations of these data sources.
And so that's one of the things that we're working on and hopefully the first paper from
that work will be able to release in the next coming weeks.
But this is something that's relevant for the coronavirus pandemic, but has been, you
know, a problem going back, you know, decades is using data that people don't understand.
And that's been at the forefront of my work is really making sure, especially, you know,
with the theme of one of the themes of this podcast machine learning, a lot of people
get very excited about machine learning and they throw a tool at data without understanding
the data.
And we're now in the midst of something where it's really crucial that people do not do
that.
Yeah.
We just had a panel discussion, a tum live discussion earlier this week on responsible
data science in the fight against COVID-19 and talked quite extensively about this issue.
And, you know, a lot of the panel initially grew out of the reactions I was seeing to,
you know, folks jumping in, wanting to, you know, help out, produce dashboards and models.
And then you'd have this kind of counter reaction of folks saying, hey, you know, you're not
an epidemiologist, stay in your lane kind of thing, which I kind of object to, to a large
degree because, you know, a, people want to help and be, you know, people, you know, want
to learn and, and, you know, everyone's bringing something.
But at the same time, you know, the stakes are high.
And even if you're an expert, it's easy to get things wrong because the data is, you
know, as you mentioned, everyone's reporting different things.
The data is messy.
You know, talk a little bit more about the kinds of things you're, you're seeing in the data.
And you mentioned you have a paper coming out is the, the objective of this paper to try
to kind of quantitatively, qualitatively provide measures for data quality as applied to
some of these use cases or what exactly are you trying to do with this paper?
Yeah, this, this first paper is one of the main things that, that people will see.
And this is true of most health conditions, but particularly among infectious diseases
if that is that in billing claims in electronic health records, you will see under counts
of infectious disease conditions.
And so using this data and not understanding all of the different reasons why we might
under count a particular health condition is, it would be very problematic.
So one of the things that we will do in this work is try to quantify and we've got multiple
years of data and so we can show trends over time, quantifying this under counting in electronic
health data for influenza-like illnesses.
And but this is even true of chronic diseases.
We see with chronic diseases that one of the, in order to be counted in an electronic
health database, you have to have an encounter with the health care system.
And we know that there's many reasons why people may not have an encounter with the health
care system.
We know that people in rural communities whose hospitals have closed may not have an encounter.
We know that people who do not have insurance or are low income may have additional barriers
to getting to, to having care.
Is the issue that the records that we do have under count because folks, there are folks
out there that, you know, contract COVID and don't interface with the health care system
or is it that even of those folks that are interfacing with the health care system,
they're systematic under counting for some reasons.
It's both.
It's both.
So not ever, so not, we won't see people who don't have an encounter with the health
care system.
And even people who do have an encounter with the health care system, they may not be coded.
So there's now an ICD-10 code, which is a billing code for health conditions.
There will be people who have coronavirus who will not be coded as having coronavirus,
even though they have an encounter with the health care system.
And there will be many reasons for this, including the fact that we don't have enough testing.
But there's lots of reasons why somebody might not have a code.
They might get coded for something else.
They might get coded for flu instead of coronavirus.
They might get coded for just a higher level.
And then we're going to have people who are coded for coronavirus who don't actually have
it.
So there's going to be in this classification in both directions.
So you're supposed to wait for a positive test.
But again, because they're so little testing, a physician might be inclined to code somebody
for coronavirus if they're a suspected case without a confirmed test, or because they're
such a delay in getting the test results back.
And so when you're trying to characterize this type of undercounting, does machine learning
come into play there?
And if so, what are the tools that you're using?
So right now, this is not using, this is not a prediction question, it calls a inference
question yet.
This is a data quality question.
This is a data quality question.
And so there's data science techniques that go into this, for example, when we need to
use different types of data aggregation methods in order to extract data from PDFs when
we're comparing to maybe CDC reviews.
One of the things that has been part of this project for the last two years has been,
we wanted to understand the impact of news deserts on infectious disease outbreaks.
And we have a lot of different types of data in order to understand communities where their
local newspapers have closed.
And it will be interesting over the long term to see how that, whether that will even
matter for the coronavirus given that it's a global pandemic.
And that's not necessarily how people are becoming aware of the pandemic.
And that's not necessarily the way that we're counting cases anymore, with much smaller
outbreaks, local news media is really vital in informing the community and also for researchers
to use those local news reports to get another source of case counts.
So not just informing the community of citizens, but informing the medical community, there's
not a back channel of, hey, be on the lookout for this disease.
Or if there is, it's inefficient, is that what you're saying?
Well, I guess I'm highlighting that the local news media data is another resource.
It's another way that we find out about outbreaks when they're smaller outbreaks.
And it's another way for researchers to, again, there's no gold standard.
There's limitations in infectious disease outbreaks in the data from the CDC, from an
electronic, you know, billing claims resource from local news media data.
And so one of the overarching goals of this, this project that had been ongoing was really
to understand the generalizability of all these data sources and try to quantify how we
could leverage multiple data sources to get at accurate case counts.
And then more broadly, when you're applying machine learning to these types of problems,
I'm curious about the tools that you end up using. You mentioned causality, causal
inference.
I saw in maybe the publications page on your site, which of course we'll link to in the
show notes picture where you have your, it looks like you are trying to relate the, you
know, costs of different interventions.
So you've got some baseline formula and then, you know, mental health as a broad category
of intervention, maybe, and then substance use.
You can tell me what you're actually saying in this diagram.
And then you've got increases and decreases of compensation, actually, so that may not
be cost.
But it kind of strikes me that you're looking at, it's your applying causal inference
and you're looking at interventions.
Yeah, so I'd have to confirm which diagram that was, but I believe it's probably from
one of the studies where we're examining the impact of different types of payment models
in the healthcare system.
Okay.
And to understand the impact of, you know, changing how spending occurs and the different
types of what we call spending models, you know, we have payment systems in the U.S.,
where sometimes we have sort of what we call a bundle payment, a certain specific amount
for a particular type of procedure and other types of spending models where we have, you
know, fee for service, every single thing that you do, whether it's a lab or a procedure,
it has a particular dollar amount attached to it.
And so then the incentive, of course, is if you're getting a fee for service is to have
many, many services.
And so a number of the studies that I've worked on is trying to understand the impact
of these types of spending models, but one thing that I will highlight that I think
a lot of people who don't work in health economics may not realize is that the impact of changing
either, you know, improving how we allocate funding in the healthcare system or the
impact of different funding models has the potential to improve human health.
So it's not an exercise in trying to save money at the cost of human health.
It's really about more efficiently serving people such that we can improve human health
in one of the areas where we've seen this is in mental health care, where over, you
know, the second half of the decade, sorry, the second half of, you know, 1950 to 2000,
we saw that the vast majority of improvements in mental health treatment really came from
changes in the financing of health care and improvements, things that led to improvements
in access.
And so it wasn't necessarily new treatments.
But as far as the types of methods that I work on, so a lot of the work that I do in causal
inference is focused on, on sampling methods.
So bringing together multiple algorithms so we don't have to rely on a single algorithm.
So especially when different types of algorithms become the new flashy tool.
So when I was in grad school, it was random forest and now it's, you know, deep learning
and neural networks and everyone's like, should I do a regression or should I do a random
forest and I say you can do both and more by incorporating, you know, rigorous ensemble
techniques by using multiple algorithms in a priori specified metrics.
And then within causal inference, you know, we bring these ensembles into so-called double
robust estimation.
So we don't just use information from an outcome regression.
We also use information for estimating what the, you know, the probability that you
would have been treated and so a functional form for that, a flexible functional form
for that.
And so I've co-authored two books on this topic and really a lot of my research has focused
on bringing double robust methods and machine learning together and again integrating that
into health services research and really trying to develop tools to specifically answer
questions in health services research.
You mentioned rigorous ensemble methods.
What does that mean for an ensemble method to be rigorous and how does one achieve the
requisite level of rigor?
Yes.
So I threw in rigorous because anytime I mention ensemble techniques, I'm always a little
bit concerned that for people who might be unfamiliar and I mentioned in a priori metrics
that's being rigorous, but we really have to decide upfront.
If we're going to run multiple algorithms, we really need to specify upfront what they
will be, how they will be evaluated.
We need to incorporate cross validation.
Meaning as opposed to I'm working on a Kaggle competition, I'm going to throw every model
I can think of against this dataset and see which produces the best accuracy on my test
set.
Well, you could do that if you make sure to choose beforehand that accuracy is going to
be your metric and you incorporate cross validation.
We can come back to why leaderboard accuracy is bad.
Single metric, single metrics is another one of the thing, you know, I get data quality
is a soapbox for mine, single metrics is another, but I threw in the word rigorous because
I, a problem that you see a lot is people run an algorithm and then they tweak it and
they run it again or they run multiple algorithms, but they run them in sequence and then they
try another thing.
And so, yeah, it's the machine learning version of P hacking.
And once you start touching the data that's your estimator and I really feel like you
need to, you need to be really upfront about what your estimator is going to be, what
your metrics are going to be and, you know, a single metric is often not going to be sufficient.
You can get a really high accuracy and have incredibly poor true positive rate.
And I mean, and a lot of our, a lot of the problems that we're dealing with, you know,
and a lot of the, unfortunately, a lot of the papers that we see in, you know, clinical
medicine and health services research and health outcomes, now that people are starting
to bring machine learning to this space, a lot of the papers are very much that.
Oh, we got a, we got an accuracy of 0.97 versus the, the parametric regression was 0.95.
And that, that type of a difference, you haven't explained whether it's meaningful, you haven't
looked at any other metrics, you've usually only used a single data set and a single medical
center, and suddenly they make these big broad claims that it can be useful in clinical
practice, which is, is dangerous, is, is frankly dangerous.
And when you put it in a clinical journal, the standards need to be really, really high
for clinical research and, and making claims with machine learning.
And that's, a lot of the work is not what I would call rigorous.
And this would be also be very, very true of the coronavirus when we're putting these
papers out there, they, you know, this, the speed cannot, cannot be an excuse for lack
of rigor.
One of the things you said was that when you apply a model to your data, that's your estimator.
Oh, something along, like, along those lines.
I mean, on that, what does that mean?
Yes.
So start touching your data, that's, that's, that's an estimator.
So you need to incorporate all of that uncertainty into.
What does it mean for that to be an estimator?
So for example, in the, the P hacking scenario, if you run a regression and then you, you
know, change it, run it again, change it, run it again.
So now you've run it three times.
Well, that, those three time, that whole thing is your estimator.
It's not the last regression that you ran, but normally what people would do is they
would publish that last regression they ran with the standard errors based on having only
run that single regression, which means the standard errors aren't correct.
And so the second you start touching your data, the second you run any kind of algorithm
that you need to build that into your estimator.
So if you do a multiple stages of estimation, you need to account for that.
And that's why, you know, for a prediction problem, that's why you should just state
all of your estimators up front, run them all together and, you know, we have both finite
sample and asymptotic properties that allow us to, you know, know that this will have good
statistical properties.
But this, this iterative cherry picking of running algorithms can really disperius results.
In the machine learning community, we often describe the fundamental process as one that
is inherently iterative, where engineering features and trying out models and, you know,
that's, that's just the job.
And how do you reconcile that with what you're describing here?
I think you need to be really clear when your analysis is hypothesis generating.
So if you really are doing exploratory data analysis, be very clear about that.
If you're doing feature selection, if you're trying to discover relationships, if you're
doing something that's unsupervised learning and you're trying to discover groups, we'll
be very clear about that.
If you're then going to use those groups to define some causal intervention, again, you
need to incorporate the uncertainty in how those groups were defined.
So I think the transparency of what you're doing and what the goal is is, is absolutely
paramount.
And if you get into a place where you're then trying to do causal inference, then again,
you can't have this sort of cherry picking iterative style because then our, the reliability
of the results are going to be very flawed.
Causal inferences sometimes presented as a, a tool that solves the kind of problems
you're describing kind of by its very nature.
It's more rigorous in, in some way than kind of the general, the other stuff that people
do, but it sounds like there are lots of pitfalls and opportunities to do it wrong.
Yeah, I think that the same way that there are some people who think that, you know, electronic
health record data is the gold standard data and has no issues, which is false.
There may be some people who think causal inferences, this, this, this, this area that is,
does not have magic and pixie dust and it really, it really isn't.
So again, you need the causal inferences difficult, the underlying research question should
be driving whether it's causal inferences or not.
If you're interested in a prediction question or an effect question, that should really
be guiding what types of techniques you need to use, but that you can have really terrible
causal analyses, the same way you can have really terrible prediction analyses or clustering
analyses.
And the transparency about what your assumptions are, what the limitation of the, of your
data are, all of that is just something that needs to be up front.
And it's something that I, I mean, that we recently had a, a workshop in the fall back
when we, we gathered in groups of people at the, at the national academies.
And I was really advocating for us, we need to have as a research community, you know,
a baseline of Stanford machine learning research, especially in, in, in the clinical medicine.
And when we, when we have prediction research, when you causal inference research in these
clinical journals, the standards need to be incredibly high and at a minimum, we need
to be very, very transparent.
We need to share code.
We need to be very explicit about what the causal assumptions are and when they might not hold.
And creating a culture around this that's much less about having magic flashy results
and much more about genuine discovery and having incredibly clear, probably appendices about
these issues.
But having that be the standard.
Code sharing is, I think just starting to catch on and become a, I don't know if it's even
one would say an accepted practice at the NURPS conference for the past couple of years.
They've, there's been a repeatability effort that has encouraged researchers to submit
papers with code.
I've got to imagine, perhaps less so in the, you know, more traditional sciences, statistics,
medicine, or I don't speak out of turn.
So I'll just say this is my impression.
I am a journal co-editor for one of the journals and statistics that the journal biostatistics.
We are standard is that you have to share code when you publish.
And a number of statistics journals have this now.
So I think we might be a little bit ahead of the machine learning community.
There might be a little bit more buy-in, but that's just my perception.
I would say in the clinical journals, it's very early stages to have that be something
that people think is reasonable and expected to do.
And some of this has been helpfully driven by funders where they require that every project
you might do in the clinical space that you have to share code.
And I think that this is something else that I brought up at that National Academy's
workshop was we need funders, we need journals, the researchers will do it if they're forced
to do it.
And we need to get allies on our side to make that happen because it's not necessarily
going to keep erroneous results from being published, but again, it's that transparency.
And with high impact work, with all of the work we were doing before the coronavirus and
now that we have the coronavirus, when data can be reasonably shared and not all data
can be shared, so a lot of electronic health data cannot be shared because of privacy
considerations.
But many of the data sources with coronavirus can be shared and if it can be shared, it
should be shared.
The same way your code should be shared and that should be the standard.
We started talking about single metrics and leaderboard efforts.
I felt like you wanted to jump in there, we didn't like jump in there.
We did touch on it a bit in my frustration of what seems to be published in not just
clinical journals, but other journals.
And I say this in a critical sense, I mean, if you go back five years and look at some
of my papers, I was publishing a single metric as well.
I know you recently had a podcast about algorithmic fairness.
And so one of the other areas that I work in is algorithmic fairness.
And so not only do we need multiple metrics of global fit, like your accuracy or your
AUCs and your R-squareds, they tell us different things, but we also really need to center
group fit and understanding how particular algorithm might further marginalize already
marginalized groups.
And in the healthcare system, we have many different groups that we already know are
marginalized, individuals with mental health and substance use disorders, individuals in
rural communities, black and African-American individuals, there's a lot of groups that
we need to make sure that if we're saying in a clinical paper, this algorithm should
be deployed.
And again, I already mentioned the fact that there's lots of other issues in that paper,
a single medical center, not using cross validation, et cetera, et cetera.
But they also haven't looked, they're saying use this when they haven't studied the harms
of that algorithm.
What are the potential harms?
And one of the ways that we can assess that is with metrics of group fit.
So-
The current state of that in clinical practice?
Almost nonexistent.
I mean, the concept of studying algorithms for issues of fairness is starting to make
a dent, but when you look at published papers, the vast, vast, vast majority of published
papers in clinical journals do not even consider it.
What's your sense for what it will take to resolve that?
Is it, you know, just time or what are the things that you're doing in the worlds that
you influence and the journals that you mentioned to try to drive the community towards considering
those kinds of metrics?
I'm excited to see more people working on algorithmic fairness in the health space.
This is something that, historically, a number of the conferences in fairness have not had
any papers on health.
And so I had previously submitted some-
Yes.
And so it was only maybe two years ago that I saw the first paper.
It's like, oh, they took a paper in health.
And one of my papers that was recently published in a statistics journal in biometrics on
fair regression for health care spending, that paper was rejected from one of the top
fairness conferences.
And so I had been trying to kind of bring these issues to some of the existing fairness conferences.
I'm glad to see that the last two years, there's more health work at these conferences.
There's also some newer conferences incorporating both people working in health and people working
in fairness.
So I do think that there are a number of people now in the fair, there's a growing consortium
of people who care about fairness specifically in the health space.
And I'm excited to see everyone in that community who's really trying and actively working to
make sure that it gets more attention, especially given the fact that the health care system
is just one of the biggest levers in the country that can have an impact on social policy.
And so if we bring algorithms into this huge system without really vetting them for
these issues, we can make people who are already incredibly marginalized, much less healthy.
And so I'm speaking at a conference this summer, which I'm assuming will be virtual.
One of these conferences, a chill conference.
And I'm really thankful to the organizers who are really pushing forward this endeavor.
And so I'm hopeful, but I'm also concerned.
The paper that you mentioned, fair regression for health care spending, let's talk a little
bit about that.
What are the goals and what is fair regression?
How does one achieve fair regression?
So there were two main goals of this paper.
So the methodological goal was that a lot of the work in algorithmic fairness and both
from methods and definitions really focused on binary outcomes.
And there was very little work in continuous outcomes.
And so a large portion of my work is in health care spending, which is a bounded continuous
outcome.
And so we developed some new fair regression methods for continuous outcomes and also
compared them to the few methods that are existed.
But what are the two groups, fairness or some other type of?
Yes.
Relative to various measures of group fairness and also global fit.
And so what fairness means for health care spending is that the formulas that we use
to pay health plans are called risk adjustment formulas.
And these aim to distribute health care funds based on health.
But unfortunately, health plans can discriminate against groups, including those that are defined
by certain health conditions, I mentioned one group earlier that I've worked on a
lot, individuals with mental health and substance use disorder.
So if these groups are currently costly to the insurer.
So any group that is costly incentivizes the health insurer to discriminate against
them through various mechanisms, like changing which providers are available to enrollees
or the cost of the copay of certain prescription drugs.
So our goal was to try and create regressions formulas that were fairer in the sense that
under compensated groups would be less likely to be discriminated against.
So if we could redistribute the funds within the formulas such that individuals with mental
health and substance use disorders were not massively under compensated, then the insurers
would have less of an incentive to try and change their plans to harm those enrollees.
And we found these regressions performed incredibly well.
We could improve fairness for groups by over something like 98% while global fit was reduced
by maybe 4%.
Just a very small loss of global fit for incredible improvements in group fairness.
As some of my continued work in that space, that paper was written with a PhD student in
a zinc.
We then after that paper collaborated with our economist colleague, Tom McGuire, where
we tried to look at not just a single group of multiple groups and bringing together fair
regression with other types of interventions in the healthcare system.
And because we were concerned, okay, if we help, if we improve under compensation for
one group, what happens to other groups?
And we found that by improving fairness for multiple groups, we picked four groups that
we knew were under compensated and we knew that insurers might have an incentive to discriminate
against them.
By improving fairness for those four groups, we improved fairness for 88% of additional
groups that we didn't even bring into the loss function.
And we were also able to reduce the number of variables that were needed in the formula
by something like 60%.
What's your intuition for why that works?
Why does that happen?
Well, so what it ends up doing is it takes funds away from, so there are people in the
healthcare system that are over compensated because the formula expects them to have more
expenses that they don't have.
So it actually redistributes funds in a very smart way.
So people who are healthy and are currently over compensated, it moves that money to
people who are being under compensated.
And so we're really excited about this.
You mentioned at the beginning of our discussion about am I trying to, is it about understanding
for the influence?
And I've been working on fairness methods and risk adjustment methods for a long time
now.
And we're really getting into a place where we're able to bring all of these advances together
and make recommendations at a certain point to not just in the US, there's many different
health systems in the world that use risk adjustment formulas.
And I'm excited about the potential, again, to have such a potential big impact on a risk
adjustment system that can have a tremendous influence on improving health and social
policy.
That's great.
Before we wrap up any other, any parting thoughts or things that other things that you're excited
about in the space that you're working?
Oh, great question.
I think my parting thoughts would be about reading research in general, but especially
during a pandemic, read it with a critical eye, read things before you share them.
I don't know if I'm naive or it's because I'm an elder millennial, but I think read things
before you share them should really be the baseline.
But please read things before you share them and be careful about, ask questions when
you're reading whether it's a coronavirus paper or it's a flashy machine learning paper
in a clinical journal.
What data source did they use?
Did they talk about any limitations of the data source?
What kinds of metrics did they use?
If they only used accuracy or AUC or R-squared, that's a red flag.
What's the true positive rate?
What's the false positive rate?
What are their conclusions?
Are they overstated?
How many, what populations did they study?
What does it mean for generalizability?
Ask all of these questions, whether it's machine learning research or not.
Just be interested, be excited, but be skeptical and thoughtful.
What I love about your comment is that it doesn't require some kind of secret decoder
ring, years of statistical stats courses or what have you.
It's just asking, being a critical reader and consumer of all the news and articles
and journal papers that you're reading and just thinking broadly about the stuff that
the claims that they're making.
Absolutely.
Awesome.
Well, Sherry, thanks so much for taking the time to share a bit about what you're up
to.
Well, thank you so much for chatting.
Great.
Thank you.
All right, everyone, that's our show for today.
For more information on today's show, visit twomolai.com slash shows.
As always, thanks so much for listening and catch you next time.
