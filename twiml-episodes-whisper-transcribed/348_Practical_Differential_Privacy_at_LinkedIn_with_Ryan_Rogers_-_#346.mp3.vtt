WEBVTT

00:00.000 --> 00:15.360
Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.

00:23.360 --> 00:28.160
Before we get going, I'd like to send a huge thanks to LinkedIn for sponsoring today's show.

00:28.160 --> 00:35.200
LinkedIn Engineering solves complex challenges at scale to create economic opportunity for every member

00:35.200 --> 00:41.680
of the global workforce. AI and ML are integral aspects of almost every product the company builds

00:41.680 --> 00:47.760
for its members and customers. LinkedIn's highly structured data set gives their data scientists and

00:47.760 --> 00:53.520
researchers the ability to conduct applied research to improve member experiences. To learn

00:53.520 --> 01:01.360
more about the work of LinkedIn Engineering, visit engineering.linkedn.com slash blog. And now onto the show.

01:03.680 --> 01:09.760
All right, everyone. I am here at the 33rd NURPS in Vancouver and I am with Ryan Rodgers. Ryan

01:09.760 --> 01:15.920
is a senior software engineer in data science at LinkedIn. Ryan, welcome to the Tumel AI Podcast.

01:15.920 --> 01:18.640
Yeah, thanks so much for having me. I'm a huge fan of the show.

01:18.640 --> 01:25.520
Awesome. Awesome. I am glad to glad to hear that and glad to have you here. You're pretty active

01:25.520 --> 01:32.160
in federated learning, differential privacy. That's in fact what we're going to be talking about.

01:32.160 --> 01:35.760
You know, tell us about your background and how you got interested in those topics.

01:35.760 --> 01:41.840
In my grad school during my PhD, I was very interested in sort of the fields of

01:41.840 --> 01:48.560
game theory with computer science. And I was working with Michael Kerns and Aaron Roth there.

01:49.200 --> 01:54.480
And I started learning more and more about differential privacy from Aaron Roth. He wrote

01:54.480 --> 02:00.800
the textbook on it. And so he was a great teacher and we started working a lot on the topic.

02:00.800 --> 02:06.880
And then toward the end of my PhD, like the last year, Apple announced that they were doing

02:06.880 --> 02:12.800
differential privacy in iOS 10. So that was like really exciting to see that like, wow,

02:12.800 --> 02:18.880
the research that I'm doing can be actually useful to companies that are interested in privacy.

02:19.600 --> 02:25.360
So it seemed like a logical next step to just after my PhD to go immediately to Apple.

02:25.360 --> 02:30.880
And it kind of continued some of the research that I had done during my PhD, but now at Apple.

02:30.880 --> 02:38.160
And so they're kind of worked on what's called the local model of differential privacy where

02:39.280 --> 02:45.200
any records that are submitted are privatized on device prior to being sent to Apple in that case.

02:45.840 --> 02:51.040
So use cases like, you know, finding popular emojis that people are typing, popular new words,

02:51.040 --> 02:58.480
that people are typing on the keyboard, things like that. And then toward the end of my time at Apple

02:58.480 --> 03:03.920
started exploring more private federated learning, local differential privacy in that case,

03:04.720 --> 03:10.080
machine learning tasks. And then I started learning about more and more about some of the work

03:10.080 --> 03:15.040
that's happening at LinkedIn with differential privacy. And it was really exciting to me.

03:15.040 --> 03:19.120
It was like kind of a different model of differential privacy that was going on there.

03:19.840 --> 03:26.320
Something that's called the central model of differential privacy where data is collected and

03:26.320 --> 03:32.480
things that we want to do with the data set is privatized. So rather than in the local model,

03:32.480 --> 03:37.840
which can be restrictive in some settings, the global model can kind of open up a whole new

03:37.840 --> 03:43.200
class of problems that you can do. So that was really exciting to me. So after about two years

03:43.200 --> 03:48.480
at Apple, I then moved to LinkedIn. And that's where I am now working on differential privacy there.

03:48.480 --> 03:54.000
Awesome. Awesome. Yeah. We first started covering differential privacy on the podcast, maybe a couple

03:54.000 --> 04:02.800
of years ago with a series of shows, which included Aaron. And so much has happened since then.

04:02.800 --> 04:10.800
I think it's like the census is now rising differential privacy. So many more organizations

04:10.800 --> 04:14.320
are talking about how they're incorporating it into kind of the way they do business

04:14.320 --> 04:20.080
whereas before differential privacy and in particular, differentially private machine learning.

04:20.080 --> 04:27.120
Oh, and also there's a TensorFlow library now. Yeah. I hope it stores projects now.

04:27.120 --> 04:32.320
A lot of open source stuff. Exciting time to be in the field. Yeah. So in 2016, when Apple

04:32.320 --> 04:37.120
announced they were doing differential privacy, that spiked interest in the area. And now,

04:37.120 --> 04:41.600
like Census Bureau announcing that they're doing differential privacy is even more recognition

04:42.400 --> 04:48.880
into the area. So it's really important to get practical things to be differentially private

04:48.880 --> 04:55.680
and for more people to get engaged with the topic. Yeah. Yeah. So you have a spotlight talk here

04:55.680 --> 05:02.480
at the conference on paper that you contributed to practical, differentially private top-case

05:02.480 --> 05:09.440
selection with pay what you get composition. Yeah. Yeah. I feel like we can spend the interview

05:09.440 --> 05:14.240
like just dissecting what all those words mean in this context. Right. Yeah. What's the best way

05:14.240 --> 05:19.280
to go through that? So yeah. This was a really, really fun project with my colleague at LinkedIn,

05:19.280 --> 05:27.200
David Durphy. So when we first started working on a project at LinkedIn, we wanted to kind of

05:27.200 --> 05:32.160
use them off the shelf techniques from differential privacy. It's been an active research field for

05:32.160 --> 05:39.600
close to 15 years. So we were kind of handling a very basic task like what are the top 10 articles

05:39.600 --> 05:46.080
that members are engaging with across LinkedIn. And so for that type of query, we were just like,

05:46.080 --> 05:51.760
okay, well, we can take existing algorithms and use that and it'll be differentially private.

05:51.760 --> 05:58.080
Unfortunately, these existing techniques require you to have extra information about what you're

05:58.080 --> 06:02.560
searching for. For instance, if I want to know top 10 articles that are being engaged with among

06:02.560 --> 06:08.320
all data scientists in the Bay Area, I would need to also provide all the possible articles

06:08.320 --> 06:13.040
that are actually out there, which is kind of contradictory to how a data scientist would

06:13.040 --> 06:18.720
actually want to run that query. You would want to ask the data set, hey, what are the top 10

06:18.720 --> 06:22.800
articles? Because I don't know what the articles are. As opposed to which of these

06:22.800 --> 06:27.520
many articles? Yeah. Exactly. So my algorithm would have to know all the possible articles that are

06:27.520 --> 06:31.920
out there, which could be millions. Maybe I don't even know about it, without all the possible articles.

06:31.920 --> 06:39.600
You know, you would presume that those articles are in the data set is the requirement to know

06:40.400 --> 06:46.640
all of those in the data set is that the issue there that it breaks the privacy aspect of it itself.

06:46.640 --> 06:52.640
Exactly. So like even articles that nobody clicked on in differential privacy, you would have to

06:52.640 --> 06:59.040
think of this hypothetical world, this hypothetical data set, where one new member might be added to

06:59.040 --> 07:05.600
the data set, which might actually click on these other articles that didn't exist in the real

07:05.600 --> 07:10.080
data set that you're running things in. So only in the analysis to prove that it's actually

07:10.080 --> 07:15.360
differentially private, do you have to consider like all possible articles that can show up?

07:15.360 --> 07:21.840
Okay. So that same contradictory to like how data science is one to work with it. So we're like,

07:21.840 --> 07:26.560
okay, there's got to be a better way of doing this. More of like a black box approach. Ask

07:26.560 --> 07:35.680
top 10 articles, top 10 skill sets among these users, something like that. You wouldn't have to feed it

07:35.680 --> 07:39.680
also. Like here are all the articles that you should search for. Here are all the skill sets to look

07:39.680 --> 07:45.760
for. So we're talking about an approach. The essence of this is that the differential privacy

07:46.480 --> 07:53.520
at its core is providing a theoretical guarantee of privacy, but all the theory in these theoretical

07:53.520 --> 08:00.320
guarantees was based on this having every the knowledge of everything. Exactly. And in practice,

08:00.320 --> 08:06.000
that's in practical. Exactly. Yeah. So whenever you would like to run these things, like you just

08:06.000 --> 08:11.040
want the data set to tell you these sort of things, sort of like exploratory data analytics.

08:11.680 --> 08:18.000
So before like, yeah, you would have to give exactly those things beforehand. But then with these

08:18.000 --> 08:25.360
new algorithms, you can just ask top 10 articles, top 10 countries, top 10 skill sets, and not have

08:25.360 --> 08:29.760
to provide, oh, here are all the articles, here are all the countries, here are all the skill sets

08:29.760 --> 08:35.200
that you need. And furthermore, still have your differential privacy guarantees. It's not that you

08:35.200 --> 08:39.760
couldn't ask your database, you know, select top 10 from whatever it's that you can still get

08:39.760 --> 08:44.880
your differential privacy. Yeah. Subjected differential privacy is always the constraint with these

08:44.880 --> 08:52.720
these issues. Yeah. So the practical aspect of these algorithms are that you don't have to feed it

08:52.720 --> 09:01.280
the what we call the domain of the data set into these algorithms. And they could work on top of

09:01.280 --> 09:07.360
existing infrastructure, say that you already have built at your company some very efficient

09:08.320 --> 09:13.520
top-cave solver, but you didn't we weren't considering differential privacy to begin with.

09:13.520 --> 09:18.160
So you're taking this highly distributed data set that's located in lots of different areas,

09:18.160 --> 09:24.640
and you built this system that can run queries incredibly fast. So now when you want to introduce

09:24.640 --> 09:28.880
differential privacy, you don't want to just totally bypass this existing infrastructure.

09:28.880 --> 09:34.000
Right. You want to be able to leverage that still with all of your queries. So that's where we

09:34.000 --> 09:38.960
wanted to apply our differential privacy algorithms, sort of as a layer on top of this existing

09:38.960 --> 09:44.160
infrastructure. So we wouldn't slow things down. You could still get the efficiency that you asked for

09:44.160 --> 09:49.200
while still getting privacy guarantees. So that's why what was so appealing about these

09:49.200 --> 09:54.240
algorithms, like the mathematics behind it is incredibly difficult, but the algorithms themselves

09:54.240 --> 10:02.320
are fairly straightforward. So in essence, like you would ask a top-k problem, and rather than

10:02.320 --> 10:07.840
asking top-k from the original existing system, you would ask like maybe top 2k, maybe a little bit

10:07.840 --> 10:13.440
more fetch a little bit more data, get those results, and then add noise to those results,

10:13.440 --> 10:20.880
and only release at most k results from that. And so that's essentially the algorithms that we

10:20.880 --> 10:25.680
propose, but proving that it's differentially private is a lot more difficult.

10:25.680 --> 10:30.960
It's surprising that your multiplier just needs to be 2 or on that order.

10:30.960 --> 10:38.880
Yeah, so that's kind of a parameter that you can tune. So depending on how much slower your

10:38.880 --> 10:44.160
computation be while still being usable. So ideally, you would just go to this as existing

10:44.160 --> 10:49.200
infrastructure and say, give me all the data. But that might not be practical in some sense,

10:49.200 --> 10:55.840
because we think of the problem as privacy as a constraint to the problem, but also run time,

10:55.840 --> 11:01.120
also data storage. So we want to be mindful of all those things in terms of a practical

11:01.120 --> 11:06.560
system. So if you can run things, if things can take a little bit longer, go and fetch as much

11:06.560 --> 11:12.160
data as you possibly can. But if you can't, then maybe just a factor of 2 is enough, but utility

11:12.160 --> 11:20.960
will improve if you fetch more results. And so then the paper is primarily establishing this,

11:20.960 --> 11:30.240
this kind of the relationship between the number that you're requesting and this multiplier

11:30.240 --> 11:35.760
and your ability to still guarantee differential privacy. Yeah, so there are kind of two components

11:35.760 --> 11:42.320
to the paper. First is here are some algorithms that we consider very practical that you can put

11:42.320 --> 11:47.280
a layer on top of existing systems and get differential privacy. It might slow things down a

11:47.280 --> 11:52.880
little bit because we have that extra parameter of fetching more data if you need. The other aspect

11:52.880 --> 12:00.720
of it is we also provide a way to sort of budget how much privacy you're allowing if somebody can

12:00.720 --> 12:06.080
interact with the system multiple times. So if I ask like a top-k query followed by another

12:06.080 --> 12:10.800
query followed by another query, you're gaining more and more information about the data set. And so

12:10.800 --> 12:16.640
you want to sort of track how many accesses somebody has made about your data set while still

12:16.640 --> 12:23.360
preserving some privacy. So what second part specific to the top-k framing of this problem,

12:23.360 --> 12:29.200
because it's one of the things I recall from previous conversations is that a big part of

12:29.200 --> 12:33.680
you know doing this in practice is like trying to figure out what this epsilon needs to be.

12:33.680 --> 12:38.240
Exactly. And so it's always a problem. It's not just in this top-k sense. Yeah,

12:38.240 --> 12:41.600
so the kind of is impacted by the multiple dips at the

12:41.600 --> 12:47.280
time. Exactly. So this privacy loss parameter, this epsilon, that's something that we need to track

12:47.280 --> 12:51.280
as we're interacting with it. So differential privacy, one of the great features about it is

12:51.280 --> 12:57.200
what's called composition so that if I run one differentially private query followed by another

12:57.200 --> 13:01.360
one, it's still a differentially private, although it's slightly worse parameters because I've

13:01.360 --> 13:06.560
gained more information about it. So you can sort of budget for if I am comfortable this much

13:06.560 --> 13:11.600
privacy loss over my entire interaction with the system, then I'm just going to track how many

13:11.600 --> 13:17.840
interactions they make and just shut off access after a certain amount of time. But what's the

13:17.840 --> 13:23.520
new contribution of this work is what we call pay what you get composition, which our algorithms,

13:23.520 --> 13:29.680
you can think of it as like maybe a deficiency of them, is if you ask for top 100 results,

13:29.680 --> 13:35.600
it might return only 20 results. And the reason why is because there might be lots of ties in your

13:35.600 --> 13:42.800
results so that it doesn't know how to rank them. And so rather than releasing a full 100 results,

13:42.800 --> 13:48.640
it might return only 20. And so typically with differential privacy, your privacy budget would

13:48.640 --> 13:52.960
have to deplete by the number of things that you requested, not how many things you actually got.

13:52.960 --> 14:00.720
So if I ask for 100 things, my privacy budget would deplete by 100, even if I got only 20 results.

14:00.720 --> 14:08.080
So we provide a new composition technique that allows you to only pay for the number of results

14:08.080 --> 14:15.360
that you get so that if I have a total budget of maybe 100 accesses, I can ask like a top 100.

14:15.360 --> 14:21.120
And then if I only get 50 results, now my budget only dropped by 50, I can ask another query.

14:22.160 --> 14:28.640
And maybe I only get like 25 results. And so I can keep asking until a full 100 is exhausted. Whereas

14:28.640 --> 14:35.600
before, if I asked for a top 100 and only 10 were returned, I would be done with my interaction.

14:35.600 --> 14:44.960
Are there constraints imposed on the way the presented results of a tie are selected?

14:44.960 --> 14:53.680
Yeah, so the ties are an issue for the algorithm because we have to introduce a threshold as well.

14:53.680 --> 14:59.680
Which kind of intuitively for privacy constraints, not even thinking about differential privacy,

14:59.680 --> 15:05.760
but privacy in general, if counts are sort of below some value, you would think of those as being

15:05.760 --> 15:11.280
really sensitive. Maybe only a few people actually use that or viewed that article or type of

15:11.280 --> 15:18.720
URL. So we inject some threshold and it might be the case that there's like several articles

15:18.720 --> 15:24.160
that are actually at or below that threshold value. And so if that's the case, then if there's

15:24.160 --> 15:29.440
sort of lots of small counts, we're only going to release fewer results. Okay, okay.

15:30.880 --> 15:37.120
You mentioned that the first part of the paper is presenting several algorithms. How are the

15:37.120 --> 15:45.120
different algorithms differentiated? Yeah, so that's a great question. So the privacy guarantees

15:45.120 --> 15:50.800
that we can give with these algorithms are what's called user level. And so this is in distinction

15:50.800 --> 15:57.440
with some of other practical works of what's called event level differential privacy. And so if you

15:57.440 --> 16:03.920
think about it, event level differential privacy is a lot maybe an easier constraint to the problem

16:03.920 --> 16:09.360
because it's protecting the privacy of any one action a user makes, maybe like one click or one

16:09.360 --> 16:15.520
page view. But with user level privacy, what you want to do is you want to protect all actions

16:15.520 --> 16:21.520
that a user could possibly make over the entire thing. So if a user is actually engaging with

16:21.520 --> 16:28.080
multiple articles, then you want to protect the privacy of all of those articles that that user

16:28.080 --> 16:34.720
viewed. So that because we're after a user level privacy guarantee, what that means is that we have

16:34.720 --> 16:42.240
to sort of know in advance whether a user can impact counts in lots of different places. Maybe they

16:42.240 --> 16:49.920
can view every possible article that's ever on LinkedIn. And in that case, we call that unrestricted.

16:51.120 --> 16:55.120
So we don't know in advance how many things they can touch. And so we have a particular algorithm

16:55.120 --> 17:00.320
that will give you user level privacy. But for something like top 10 countries that have a particular

17:00.320 --> 17:08.320
skill set, you know in advance that any one user could only be in one country. So if you take

17:08.320 --> 17:13.760
that user out of the data set or add them, you know only a single count can change. And so the

17:14.800 --> 17:20.560
algorithm that we have in that case is slightly different. And so in either case, we're ensuring a

17:20.560 --> 17:27.200
user level privacy guarantee. But that's sort of the distinction to ensure user level privacy. We

17:27.200 --> 17:32.240
need to know in advance either how many things they can possibly touch, how many counts they can

17:32.240 --> 17:36.560
change. And if you don't know how many things they can change, then you're going to use a sort of

17:36.560 --> 17:40.400
default algorithm, which is well, they could change everything. So this is the algorithm that you

17:40.400 --> 17:46.160
should run. So practically speaking, who are the users in these in the scenarios? This

17:47.120 --> 17:53.120
Sam as the user using the LinkedIn app, you know, web or mobile, where I don't tend to see

17:53.120 --> 17:59.840
counts or something like that. Or it's just like an internal LinkedIn user who has access to do,

17:59.840 --> 18:04.000
you know, reporting on articles and things like that. But you still want to protect the privacy

18:04.000 --> 18:10.240
of the user base. Right. So the existing like differential privacy system at LinkedIn

18:11.360 --> 18:18.720
sort of handles data analytics or reporting for users so that we can provide like add analytics,

18:18.720 --> 18:26.400
content analytics, profile view, statistics, that sort of thing. So it's sort of we're providing

18:26.400 --> 18:32.880
products for people to get sort of reporting information. And because of that, we want to be mindful

18:32.880 --> 18:40.240
of the member's privacy, their data. So we might call them like private actions that users are doing.

18:40.240 --> 18:45.440
Things that they don't want to like deliberately share with people outside, unlike, you know,

18:45.440 --> 18:51.520
a share of an article or a like or a comment. And so these aren't things that I might, you know,

18:51.520 --> 18:57.280
be exposed controls that might be, you know, surface via the feed, but more like the, you know,

18:57.280 --> 19:02.000
the business tools add interfaces, things like that. Exactly. Yeah. You know, there's more of a risk

19:02.000 --> 19:09.280
that I might run some report. And you're trying to make sure that I'm not able to get to

19:09.920 --> 19:15.360
individually identifiable. Exactly. I want to provide only aggregate statistics about how

19:15.360 --> 19:23.120
people are interacting across LinkedIn, but still maintain privacy. Even in the aggregate statistics,

19:23.120 --> 19:27.440
there's been studies showing like even with aggregates, you can get individual level

19:28.880 --> 19:34.000
information. So that's kind of why differential privacy is so great.

19:34.960 --> 19:40.000
So the different types of algorithms that you mentioned in the first part of the paper

19:40.000 --> 19:47.440
is it both the event base and the user base for this kind of problem or you're saying the event

19:47.440 --> 19:52.000
base is already solved and you're doing different flavors of user base. Yeah. So what we're going

19:52.000 --> 19:57.760
after is user level privacy. And so because of that, we have to be mindful of how many things one

19:57.760 --> 20:02.960
user can change. If we're going for event level, we know that only a single click can only change

20:02.960 --> 20:09.120
a single count. Right. So in that setting, like we also provide an algorithm for that case. So even

20:09.120 --> 20:14.640
though we're after user level privacy guarantees, we have an algorithm that would also give you

20:14.640 --> 20:20.800
a event level, which is also new. Does this paper describe something that you've been able to

20:20.800 --> 20:28.400
put into production at LinkedIn or is it more kind of ongoing research that is driving towards

20:28.400 --> 20:33.360
something that's usable for you? Yeah. So with the existing differential privacy system,

20:34.400 --> 20:38.080
that's providing noise on aggregate statistics and providing a event level privacy.

20:38.080 --> 20:44.400
And so with these extra tools, extra algorithms, we wanted to sort of incorporate that into our

20:44.400 --> 20:52.160
existing suite of tools so that we can also provide differentially private results for exploratory

20:52.160 --> 20:59.120
data analytics, kind of like the top K. And we could provide user level privacy guarantees.

20:59.120 --> 21:04.080
So this is sort of ongoing work that we're doing and the differential privacy efforts at LinkedIn

21:04.080 --> 21:09.760
is we're pushing forward. You mentioned the kind of the compositionality of

21:09.760 --> 21:12.960
composability, I forget which words he is. Yeah, one of those.

21:12.960 --> 21:14.640
Composition, because of the possibility. That's great.

21:15.360 --> 21:23.280
Elements of differential privacy. Does the user level controls to those,

21:23.280 --> 21:28.080
like sit on top of the event-based controls, or are they a totally separate system or approach

21:28.080 --> 21:36.240
to this? Yeah, so with the composition, we have to think of not just algorithms that are providing

21:36.240 --> 21:42.880
a differential privacy, but we also have to have sort of a tracking of how many accesses are they

21:42.880 --> 21:48.480
making with my system, and then deducting things based on whatever queries they're asking.

21:48.480 --> 21:54.480
So you can kind of think of these of two components. There's sort of the algorithm side of things,

21:54.480 --> 22:00.080
and then there's also the budget tracking. So that these together can provide a very global

22:00.880 --> 22:08.640
differential privacy system. You also are active in kind of the federated learning and the use

22:08.640 --> 22:14.400
of differential privacy and federated systems has LinkedIn published anything in that area?

22:14.400 --> 22:22.240
No, so that was my my prior work at Apple. Yeah, so with that, we were very interested in

22:22.240 --> 22:27.600
federated learning because there's lots of iPhones that are out there. It'd be great if we could

22:27.600 --> 22:34.000
just push training on device because there's lots of sophisticated hardware on the iPhone, so

22:34.000 --> 22:38.960
why don't you just push all the intensive work to the peripheral devices.

22:39.840 --> 22:44.720
What's great about is that you're not submitting a data back, but you're sending the weights of

22:44.720 --> 22:49.920
your neural net that you were training on device, which those themselves are computed on the data

22:49.920 --> 22:55.440
so they contain information about what data they were computed on. And so the work that I did

22:55.440 --> 23:02.160
there, we worked on sort of local privatization of those model weights prior to submission,

23:02.160 --> 23:08.960
so that on the server we can aggregate them and update the model with only these privatized weights

23:08.960 --> 23:15.040
and iterate on that process. And it's actually some of the work that I that I did was actually

23:15.040 --> 23:22.800
featured in the expo at Sunday or Sunday, which is why you're able to tell us about it. Exactly.

23:22.800 --> 23:29.360
Yeah, so Apple just talked about it. Yeah, so they presented some of the work at WWDC this year

23:29.360 --> 23:34.480
and they then talked about the private federated learning system. Some of the use cases, I think

23:34.480 --> 23:41.760
we're like with Hey Siri and some other ones too, so yeah, really cool work. Yeah, where do you see

23:41.760 --> 23:48.800
some of the big kind of, you know, frontiers for differential privacy, you know,

23:48.800 --> 23:52.800
at LinkedIn and beyond in the industry, you know, given the progress that's been made over the

23:52.800 --> 23:58.800
last few years? Yeah, so I think within differential privacy, you can kind of break it up as local

23:58.800 --> 24:04.080
and central. And then within those two models, you can break up the task as data analytics and

24:04.080 --> 24:10.880
machine learning. So with all of those, like I think we need to still push for practical

24:10.880 --> 24:15.360
deployments. And all the stuff we talked about previously is data analytics. We're not talking

24:15.360 --> 24:19.520
about differential private machine learning. Yeah, so in the central model, I was only talking about

24:20.400 --> 24:25.040
data analytics with the federated learning and the local model that's more machine learning

24:25.040 --> 24:30.960
with the emojis one that's more data analytics and the local model. So really, like the book is not

24:30.960 --> 24:36.880
closed on any of these things, like we need to keep pushing in these areas and some of the great

24:36.880 --> 24:41.520
work that's going on in the open sourcing of these projects, I think is a great step forward

24:41.520 --> 24:46.880
because it will bring more people to the forefront of this field and really push the frontiers of

24:46.880 --> 24:53.120
it. Because I think it is the future of how we think about data use. We need to be mindful of,

24:53.120 --> 24:58.160
you know, what algorithms we're picking, how we can track, how users data is following.

24:59.040 --> 25:06.400
And so some of the open source efforts, like there's the work from Microsoft collaborating with

25:06.400 --> 25:14.560
Harvard, pushing this differential privacy open source platform. And so we're actively collaborating

25:14.560 --> 25:20.720
with with that team because LinkedIn is a within Microsoft now. So that's a project.

25:20.720 --> 25:27.520
Is that focused on centralized central? Yeah, so yeah, so I believe like it wants to cover like

25:27.520 --> 25:34.480
all of potential use cases, but initially it will be central model working on top of data stores.

25:34.480 --> 25:39.600
A bit hesitant to try to poke at the details because it's difficult to poke at, you know,

25:39.600 --> 25:44.480
the details of kind of a theoretical paper. But, you know, I wonder if you can kind of give us

25:44.480 --> 25:51.520
a sense for, you know, what was the hard part here? And, you know, where you started and,

25:51.520 --> 25:57.920
and, you know, what that, what the effort was like. Yeah, so some of the difficulties with it were,

25:57.920 --> 26:05.040
there's like sort of standard differential privacy algorithms, like adding liposinoise,

26:05.840 --> 26:09.920
is a very standard differential privacy algorithm. And then there's something called the

26:09.920 --> 26:14.320
exponential mechanism, which you typically don't think of adding noise, you kind of think of

26:14.880 --> 26:19.680
random sampling things instead. And so one of the really nice connections that we made in our

26:19.680 --> 26:24.960
paper was that actually exponential mechanism can be implemented by adding something called

26:24.960 --> 26:31.440
gumbled noise rather than like liposinoise. So, and gumbled noise actually pops up a lot of

26:31.440 --> 26:39.840
machine learning. It's something that you would do to report like the category that has the

26:39.840 --> 26:44.480
highest weight. So typically how you would think about doing this is you would have weights for

26:44.480 --> 26:52.000
different like, you know, dog, cat, orange, whatever, for a particular image classification task.

26:52.000 --> 26:55.600
And so you'd have weights, they would all be positive, but they wouldn't be like a probability

26:55.600 --> 26:59.600
distribution. They wouldn't all add up to one. So what you would do is you'd do a soft max

26:59.600 --> 27:05.680
layer where you would take all those weights, take each of the weight and divide by the sum of the

27:05.680 --> 27:09.920
exponentials of the weights. Now you have a probability distribution and you would sample

27:09.920 --> 27:15.360
according to that probability distribution to get like them. This is the label for that particular

27:16.320 --> 27:21.760
task. Now, typically you don't do this in practice. What you do is you stop at the

27:21.760 --> 27:27.520
weights and you add gumbled noise to all of those weights and report the category that had the

27:27.520 --> 27:33.920
highest noisy value. And this is called the gumbled max noise trick. And so it turned out that

27:33.920 --> 27:39.440
we could use that with the exponential mechanism to get a differentially private algorithm.

27:39.440 --> 27:45.680
Now, one of the limitations of it is because it's differential privacy, gumbled noise itself,

27:45.680 --> 27:51.440
you cannot release the counts and satisfy the differential privacy condition. You can only release

27:51.440 --> 27:58.960
the element that was the max. So you don't know why it was the max. It was the max because the

27:58.960 --> 28:04.800
value was 45 or something. You only know that this was the most popular one. This one was the one

28:04.800 --> 28:12.240
with the highest weight. And so with that connection of exponential mechanism with gumbled noise,

28:12.240 --> 28:20.240
we were able to take existing algorithms, typically how you would solve top K, you would use the

28:20.240 --> 28:26.160
exponential mechanism K different times. You could now do this in one shot by just adding gumbled

28:26.160 --> 28:32.800
noise to everything and reporting the K values that are in the top. And so that connection made

28:32.800 --> 28:38.080
made things a little bit easier in our analysis. We analyzed things according to the exponential

28:38.080 --> 28:44.240
mechanism, but we were using gumbled noise in the actual implementation which made it a lot more

28:44.240 --> 28:53.280
efficient and practical in the title. And so just the use of gumbled noise and kind of a noise

28:54.160 --> 29:00.640
oriented approach as opposed to the exponential approach to that allow the differential privacy

29:00.640 --> 29:08.960
guarantees to be able to kind of use the existing noise-based privacy guarantees, but with a

29:08.960 --> 29:15.360
different distribution. Yeah, so the connection of noise with exponential mechanism was really

29:15.360 --> 29:21.200
helpful because we wanted to incorporate a threshold. We wanted to say if counts were too small,

29:21.200 --> 29:25.680
we didn't want to report them. So we wanted to incorporate a threshold, but incorporating a

29:25.680 --> 29:30.320
threshold when you're randomly sampling things like we didn't know where a threshold could be

29:30.320 --> 29:35.120
implemented. But now when you're talking about adding random noise to values, now it makes sense

29:35.120 --> 29:39.760
to add a threshold to those things so that you would only report things that were above some

29:39.760 --> 29:44.880
threshold value. Whereas with exponential mechanism, if you were just to look at it, you wouldn't

29:44.880 --> 29:49.280
know where to necessarily say like, oh, a threshold really needs to be this value.

29:50.080 --> 29:54.960
Awesome. Any other kind of interesting things we should be kind of looking forward to coming out

29:54.960 --> 30:01.040
of the differential privacy community? Yeah, so I'm really excited about the open source efforts

30:01.040 --> 30:08.960
in the area. I think this is kind of the future of private data analytics. It's really important

30:08.960 --> 30:13.920
to be transparent with how you're doing things. Otherwise, if you're just touting that you're

30:13.920 --> 30:18.960
private and you're not revealing what it is, then is it really private how do people verify

30:18.960 --> 30:24.720
these certain things? So transparency is really important and this will bring more people to the

30:24.720 --> 30:29.680
community. So that's really got to be the movement forward. I think there's going to be there's

30:29.680 --> 30:36.320
great open problems in central DP with data analytics with the machine learning. There's lots

30:36.320 --> 30:42.160
of open problems. I think that still needs to have a lot of consideration from people in industry

30:42.880 --> 30:48.960
to make them practical rather than trying to go after just pure theory, try to make things

30:48.960 --> 30:55.280
actually work in practice and at scale. And in the local model, I think there's lots of work

30:55.280 --> 31:00.400
to do as well there. There's also very interesting work sort of in between the two,

31:01.280 --> 31:05.840
taking sort of a little bit of local with a little bit of central to get like a hybrid model.

31:06.560 --> 31:13.760
So I think that's an incredibly interesting push forward to think of like various models of privacy.

31:13.760 --> 31:20.000
It sounds like it would be a lot harder in that case to kind of provide these, you know, general

31:20.000 --> 31:27.200
privacy guarantees. Yeah. So even in the local model like the guarantees are local as opposed to

31:27.200 --> 31:32.960
you know, local relative to some some centralized thing. Yeah. So I think for that you kind of take

31:32.960 --> 31:38.240
a step back and you're like, well, where's the privacy in my system because maybe the privacy

31:38.240 --> 31:43.680
parameters the same and all of these things. But one just feels more private than the other. And

31:43.680 --> 31:49.360
it's because you're sort of shifting this trust barrier. In the local model like the trust is like

31:49.360 --> 31:54.560
on your device, like, yeah, I know that I'm privatizing it before I'm sending it. And the central

31:54.560 --> 32:02.000
model, the trust boundary is not a data submission, but more wherever I want to build an application,

32:02.000 --> 32:09.200
that's where my privacy trust barrier is going to be. And the model in between, there's like, well,

32:09.200 --> 32:14.640
maybe there's some things internal to my company that I want to protect the, you know,

32:14.640 --> 32:20.880
malicious internal employees from accessing these data sets. So I want to add, introduce some

32:20.880 --> 32:27.200
noise at that level, but maybe not as much as what I would be comfortable with as if I want to

32:27.200 --> 32:33.600
just publicize this model and have deploy this on all devices out there. So it's sort of the trust

32:33.600 --> 32:39.680
boundary, like you can sort of think of various models of privacy, whereas the privacy parameter

32:39.680 --> 32:44.640
might be the same in all of them, you're shifting the trust. All right. Thanks so much for taking

32:44.640 --> 32:48.240
the time to share with us, you know, a little bit about what you're up to. Absolutely.

32:48.240 --> 32:50.400
Thanks for really appreciate you having me. Thank you.

32:54.880 --> 32:59.840
All right, everyone. That's our show for today. For more information on today's show,

32:59.840 --> 33:13.360
visit twomolai.com slash shows. As always, thanks so much for listening and catch you next time.

