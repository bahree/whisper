1
00:00:00,000 --> 00:00:10,560
All right, everyone. Welcome to another episode of the Twimble AI podcast. I am, of course,

2
00:00:10,560 --> 00:00:15,840
your host, Sam Charrington. And today I'm joined by Richard Socher. Richard is co-founder

3
00:00:15,840 --> 00:00:22,040
and CEO at U.com. Before we get into our conversation today, please be sure to take a moment

4
00:00:22,040 --> 00:00:27,440
to head over to Apple Podcasts or your listening platform of choice. And if you enjoy the show,

5
00:00:27,440 --> 00:00:34,560
please leave us a five-star rating and review. Richard, it's been almost exactly two years since

6
00:00:34,560 --> 00:00:39,680
we spoke you were at Salesforce. Then welcome back to the podcast. It's great to be back, man.

7
00:00:39,680 --> 00:00:46,080
A lot has changed in those two years. A lot has changed. I think the last time we spoke to you were

8
00:00:46,080 --> 00:00:55,760
like hold up in some exotic bunker, you know, early in the pandemic. But we made that conversation

9
00:00:55,760 --> 00:01:00,400
work. And I'm super excited to get caught up with you. You've been a busy guy for the past couple

10
00:01:00,400 --> 00:01:08,480
of years. It's been, yeah, busy bays. But exciting can complain. Awesome. Well, for those who

11
00:01:09,360 --> 00:01:14,560
don't know you, I'd love to have us start with you sharing a little bit about your background

12
00:01:15,280 --> 00:01:20,320
before we dive into what you've been up to recently. Sounds great. Yeah. Hello, everyone. I'm Richard.

13
00:01:20,320 --> 00:01:27,440
Where do I start originally from Germany that my PhD at Stanford in deep learning for

14
00:01:27,440 --> 00:01:34,480
natural language processing and computer vision. And thought it would be great to have more people

15
00:01:34,480 --> 00:01:41,600
use deep learning for natural language processing, which was still quite received with quite a

16
00:01:41,600 --> 00:01:48,240
lot of skepticism back then. And so I started teaching at Stanford deep learning for natural

17
00:01:48,240 --> 00:01:57,360
language processing in 2014. And well, needless to say, nowadays is kind of obvious that like that

18
00:01:57,360 --> 00:02:03,040
is the right technology after four years for a variety of reasons. But one being that everyone

19
00:02:03,040 --> 00:02:08,480
was then teaching deep learning as the main approach for natural language processing. I stopped

20
00:02:08,480 --> 00:02:16,080
teaching. My main job was doing a startup meta-mind enterprise AI platform. We got acquired by

21
00:02:16,080 --> 00:02:22,800
Salesforce where I became a chief scientist. And then build out the research group there. And

22
00:02:22,800 --> 00:02:32,160
eventually also a lot of the AI product engineering teams that I let. And then couldn't quite

23
00:02:32,160 --> 00:02:36,960
shake this idea off. I actually during the last couple of days of my PhD, I'd implemented a first

24
00:02:36,960 --> 00:02:42,160
little prototype of a new search engine that was going to summarize the web for people and be

25
00:02:42,160 --> 00:02:48,160
just much more useful and quicker in Google. At the time I thought, oh man, all my smart friends

26
00:02:48,160 --> 00:02:53,680
are going to Google. No one's ever complaining about it, where as far as I can hear, it's just

27
00:02:53,680 --> 00:02:58,400
maybe two or three days of an idea. But I couldn't quite shake the idea off over the last eight,

28
00:02:58,400 --> 00:03:05,760
nine years. And after many amazing wonderful years with the great teams at Salesforce, I decided I

29
00:03:05,760 --> 00:03:12,640
think I need to do this. I think the world needs a better search engine for a lot of macro reasons

30
00:03:12,640 --> 00:03:22,400
as well as sort of user reasons. And so I decided in the summer of 2020 to start this with Brian

31
00:03:22,400 --> 00:03:31,200
McCann, one of my amazing collaborators and co-workers, both at Stanford and at Salesforce. And yeah,

32
00:03:31,200 --> 00:03:37,520
couldn't be happier with Brian and the team we've built at U.com. When you talk about the meta

33
00:03:37,520 --> 00:03:43,600
and user reasons for a new search engine, what exactly does that mean? What's the motivation for

34
00:03:43,600 --> 00:03:49,840
U.com? Yeah, so, and the high level reason, it's kind of crazy that the entire economy is moving

35
00:03:49,840 --> 00:03:55,840
online. And you have the single gatekeeper in the beginning of most people's online journey that

36
00:03:55,840 --> 00:04:01,520
mostly wants to sell you to the highest bidding advertiser and you and your queries. At the same time,

37
00:04:02,320 --> 00:04:07,520
we are in an information age and there's information overload. 20 years ago, it's hard to get access

38
00:04:07,520 --> 00:04:13,520
to information. But nowadays, it's actually almost too easy to get access to a lot of not that

39
00:04:13,520 --> 00:04:18,400
useful information and you need AI to help you deal with this flux of information, help you

40
00:04:18,400 --> 00:04:23,920
summarize all the things that are going on and get quickly to what you want to actually achieve

41
00:04:23,920 --> 00:04:28,720
and do. And you give that intent usually to a search engine, but now 60 percent of all Google

42
00:04:28,720 --> 00:04:33,680
queries are zero clicks, meaning they don't leave the Google ecosystem anymore. They stay within

43
00:04:33,680 --> 00:04:40,160
YouTube, the maps. And they try to suck you into these engagement loops rather than trying to be

44
00:04:40,160 --> 00:04:45,760
as useful as they can, summarize and then get you on your way, either somewhere else onto the internet

45
00:04:45,760 --> 00:04:51,280
or just that you execute on the intent that you have. So if I search for

46
00:04:51,280 --> 00:04:59,040
how to do a sort array by value or something in Python, I just want the code snippet. And that's

47
00:04:59,040 --> 00:05:02,240
what, you know, in your dot com, one of the many, many features that we just give you. There's a

48
00:05:02,240 --> 00:05:06,080
code snippet and a copy and paste button because we know that that's probably what you want. Instead

49
00:05:06,080 --> 00:05:10,000
of a list of 10 links and you go there and you don't have good string matching and whatnot.

50
00:05:10,000 --> 00:05:16,080
So those are just just one of many examples that's helpful for the user, connect to the macro. Then

51
00:05:16,080 --> 00:05:23,200
there's sort of outside of AI and machine learning reasons. And just that when every company has

52
00:05:23,200 --> 00:05:29,520
to pay this tax to exist on that first page, which is, you know, by paying for ads, it creates

53
00:05:29,520 --> 00:05:35,280
some really odd incentives that we've, that I've observed now, multiple people kind of tell me

54
00:05:35,280 --> 00:05:42,160
a story where they got organically up in the Google ranker. And then, you know, they make,

55
00:05:42,160 --> 00:05:48,640
start to make millions of dollars because the content was just good. And so Google ads team comes

56
00:05:48,640 --> 00:05:52,880
over to sales teams, say, hey, do you want to, you know, buy some ads to get even more traffic?

57
00:05:52,880 --> 00:05:55,760
And you're like, no, we're good. We're getting so much traffic. They basically

58
00:05:57,040 --> 00:06:03,680
disappeared and went to page nine. And then I'm like, sorry, yeah, we'll buy the ads. And then

59
00:06:03,680 --> 00:06:10,160
magically they come back to page one with the ads too. That's kind of one of the, one of the many

60
00:06:10,160 --> 00:06:16,240
reasons to do it. And then in some crazy way, also now, people are actually complaining more and

61
00:06:16,240 --> 00:06:23,200
more about just the ranking and the relevance. You have all these SEOed microsites that kind of look

62
00:06:23,200 --> 00:06:28,800
at try to reverse engineer the algorithm that Google decides for everyone what to see and read and

63
00:06:28,800 --> 00:06:35,040
consume and buy. And, and they're trying to reverse engineer it. And so you get all these like

64
00:06:35,040 --> 00:06:39,520
really odd microsites that have a bunch of sort of language model samples on there that are

65
00:06:39,520 --> 00:06:47,120
known to resonate well. You look for like good machining tools for like building a roof or something.

66
00:06:47,120 --> 00:06:51,040
And at the bottom, there's some weird Wikipedia article about California on the page that comes

67
00:06:51,040 --> 00:06:55,200
up highest on Google. It's because they know that stuff ranks well in the algorithm. So there's

68
00:06:55,200 --> 00:06:59,920
all this reverse engineering. And I think part of the problem there is that, and it's partially

69
00:06:59,920 --> 00:07:06,880
an AI problem, but it's also a systemic problem and how you approach AI is that Google decides

70
00:07:06,880 --> 00:07:11,760
and wants to be able to decide for everyone what they read, consume and buy, because they have

71
00:07:11,760 --> 00:07:17,520
so much power by then showing you mostly ads, which is also just becoming 90% of any page of the

72
00:07:17,520 --> 00:07:21,440
results that, you know, when you actually want to buy something and have them monetize. It's got

73
00:07:21,440 --> 00:07:30,160
very bad experience. It's degraded surprisingly over the years. Yeah. And so I think it's important

74
00:07:30,160 --> 00:07:39,440
to help people still get stuff done, but not just try to do that through ads. And by giving them

75
00:07:39,440 --> 00:07:45,280
also some control. So we have, of course, a large neural network too to rank what we actually

76
00:07:45,280 --> 00:07:50,480
think about the more is the apps that you're looking for, like big content islands, like Reddit,

77
00:07:50,480 --> 00:07:54,400
things like that. Stuff where, you know, social signals that people actually care about rather than

78
00:07:54,400 --> 00:08:00,960
SEOs, microsites. And, but we actually give people control and say, Oh, I like this source.

79
00:08:00,960 --> 00:08:06,480
I don't like the source. And that way, if you try to manipulate this too much by playing sort of,

80
00:08:06,480 --> 00:08:12,000
you know, SEO games, people will just download you and you'll disappear. And so I think giving

81
00:08:12,000 --> 00:08:17,760
people control over the AI that influences so much of their information diet is yet another

82
00:08:17,760 --> 00:08:21,600
reason I could go on forever. There's so many reasons why, but yeah, I mean, it's also one of the

83
00:08:21,600 --> 00:08:27,360
most exciting AI applications that's so important in an information age. It tackled summarization,

84
00:08:27,360 --> 00:08:32,480
which is one of the big, hard, unsolved problems in AI and so on. It's just the most exciting thing.

85
00:08:32,480 --> 00:08:38,640
I couldn't not do it. Awesome. Awesome. But when you, you know, starting to build a search engine

86
00:08:38,640 --> 00:08:44,320
in 2020, do you start with something that's fundamentally similar to a page rank type of an

87
00:08:44,320 --> 00:08:51,600
approach or are you approaching it very differently? Yeah, it's a good question. We are actually approaching

88
00:08:51,600 --> 00:08:56,880
it quite differently. We have not sort of replicated the list of blue links. We're getting that

89
00:08:56,880 --> 00:09:02,320
from an API. But what we actually are doing is using large neural networks to understand what

90
00:09:02,320 --> 00:09:07,360
the intent actually is and then try to give you the most useful application. And we want to be a

91
00:09:07,360 --> 00:09:12,560
much more open platform for, you know, building these applications out so that different people can

92
00:09:12,560 --> 00:09:18,480
actually implement and contribute to that first page of the internet and make it very, very seamless.

93
00:09:18,480 --> 00:09:26,320
And so we have essentially relied much more on the content and the semantics. And then, of course,

94
00:09:26,320 --> 00:09:31,280
we can nowadays already extract a lot of popularity signals that you use to need page rank four

95
00:09:31,280 --> 00:09:36,560
because a lot of like recipe sites and codes and a bit sites like stack overflow. They actually have

96
00:09:36,560 --> 00:09:41,600
votes on how popular something is. So you can extract that. That's part of the signal together with

97
00:09:41,600 --> 00:09:48,160
the natural language processing signals. So on top of the kind of core search engine, you also

98
00:09:48,160 --> 00:09:53,040
recently announced a couple of extensions. Well, at least the code is new is right new also.

99
00:09:53,600 --> 00:09:58,080
Yeah, yeah, it's a lot of new stuff's coming out. You know, we're thinking about how can we be

100
00:09:58,080 --> 00:10:03,440
more useful? And of course, if you build a search engine nowadays, the really tricky balance is

101
00:10:03,440 --> 00:10:09,440
how many of your resources do you spend on just catching up to Google versus how many resources

102
00:10:09,440 --> 00:10:13,520
do you spend on doing something that's different that Google doesn't yet do, right? And I'm sure

103
00:10:13,520 --> 00:10:18,080
in the beginning, when people saw Google flights, they're like, oh, why does a search engine

104
00:10:18,080 --> 00:10:21,920
help you book flights? And we get some of the things now when we write an essay for you, like you

105
00:10:21,920 --> 00:10:27,360
search how to write an essay about World War Two or the American Revolution and this and that,

106
00:10:28,160 --> 00:10:34,000
it'll just write you an essay and you can modify it and then generate new ones and it helps you

107
00:10:34,000 --> 00:10:38,800
with blank page problems. I'm like, okay, how do I start? You can run and write a blog post about,

108
00:10:38,800 --> 00:10:45,920
you know, your Airbnb project or whatever it is. And so we think like that is something unique

109
00:10:45,920 --> 00:10:52,160
that AI can also bring. We have these exciting large language model applications now twice

110
00:10:52,160 --> 00:11:00,320
in the search engine. One is in code completion. So if you look for, you know, how to do something in

111
00:11:00,320 --> 00:11:07,520
Python or other languages, the code complete app comes up and it's just a full on like GitHub

112
00:11:07,520 --> 00:11:13,360
co-pilot-like model that essentially just auto-generates the code that you're looking for. And then,

113
00:11:13,360 --> 00:11:16,560
of course, you have, you know, stack overflow apps with copy and paste buttons. We have simple

114
00:11:16,560 --> 00:11:21,680
tutorials and, you know, to sort of get you started with things and you have hugging face and

115
00:11:21,680 --> 00:11:28,880
pytorge kinds of official documentation to and it's all just right within their first page with

116
00:11:28,880 --> 00:11:34,560
code snippets and copy and paste buttons to be very easy. And then on the writing natural language,

117
00:11:34,560 --> 00:11:40,240
probably like natural language side, we have this you write app under you.com search right where

118
00:11:40,240 --> 00:11:46,800
you basically can have an API just write an essay or blog post for you. Got it. Got it. So kind

119
00:11:46,800 --> 00:11:52,880
of stepping back, you've got AI throughout this platform. You've described a couple of applications

120
00:11:52,880 --> 00:11:58,720
that are, you know, I don't know if you think of them as part of the search engine or kind of a

121
00:11:58,720 --> 00:12:05,040
Jason or on top of the search engine, but then you're using AI as part of surfacing relevant

122
00:12:05,040 --> 00:12:11,600
results to folks. You know, thinking about that that core, maybe dig a little bit deeper into some

123
00:12:11,600 --> 00:12:18,960
of the ways that you're using machine learning to deliver relevant results to folks. The large

124
00:12:18,960 --> 00:12:23,440
language models come into play there or is it more on the summarization side? Yeah, it's a great

125
00:12:23,440 --> 00:12:30,560
question. There are a bunch of summary features too. And AI is sort of both in terms of our product,

126
00:12:31,520 --> 00:12:36,320
sort of very deeply. And we can get into that like in 10 classifications, slot filling,

127
00:12:37,200 --> 00:12:46,080
ranking of different applications and so on. It's also a user persona that we know a lot about

128
00:12:46,080 --> 00:12:51,120
and care a lot about. And just people who program AI applications. And you know,

129
00:12:51,120 --> 00:12:57,600
hence we have all these specific tutorials and documentation sets for hugging face for instance

130
00:12:58,720 --> 00:13:06,000
for PyTorch and a bunch of other things. We have GitHub issues. We crawled all of GitHub so you

131
00:13:06,000 --> 00:13:11,360
can find all the different issues about your code that you're working on directly all in the

132
00:13:11,360 --> 00:13:16,400
search engine. When you when you're basically trying to understand a query, you want to understand

133
00:13:16,400 --> 00:13:21,440
like what programming language someone is using or we like if you say I want directions from

134
00:13:21,440 --> 00:13:26,160
San Francisco to LA. It's kind of a standard example. That's a good one for slot filling where you

135
00:13:26,160 --> 00:13:32,720
want to extract sort of certain things from the query directly and input them into an app. And then

136
00:13:32,720 --> 00:13:40,000
basically let people immediately kind of have that filled out the slots of you know the from

137
00:13:40,000 --> 00:13:46,240
direction to direction. So you get the time that it takes you to drive somewhere in the directions

138
00:13:46,240 --> 00:13:53,280
for that. So that that is an example of slot filling that we have to do for 150 or so apps that

139
00:13:53,280 --> 00:13:59,440
that we have. If you look for Dow jobs or Web 3 jobs for instance, then we kind of extract that's

140
00:13:59,440 --> 00:14:04,320
the kind of job type you're looking for. And we have a bunch of apps that basically show you

141
00:14:04,320 --> 00:14:11,040
job listings for these kinds of categories. So slot filling is a is a pretty big part of the

142
00:14:11,040 --> 00:14:14,480
search engine. And then of course you have the intent kind of knowing this is just the weather

143
00:14:14,480 --> 00:14:21,200
intent. And that leads then to influencing and kind of into and training a large a large

144
00:14:21,200 --> 00:14:26,240
ranking model that basically ranks all these different apps in terms of their priority for

145
00:14:26,240 --> 00:14:35,040
helping you get something done. And so when you're are the the intense or the apps are those kind

146
00:14:35,040 --> 00:14:40,640
of top down you know created by you or project manager, hey we're going to need our travel app,

147
00:14:40,640 --> 00:14:44,880
we're going to need our directions app, we're going to need you know this list of you know hundreds

148
00:14:44,880 --> 00:14:51,520
of things or are they kind of bubbled up from the queries that people are making themselves.

149
00:14:51,520 --> 00:14:59,040
Yeah great question. You bring up two good points. One is namely privacy. We care a lot about

150
00:14:59,040 --> 00:15:04,480
privacy. I think it's a really important important right. And so we have a private mode in which we

151
00:15:04,480 --> 00:15:08,560
don't track anything so we don't know really what people are doing. We don't have the queries.

152
00:15:08,560 --> 00:15:13,520
But we also have a personalized mode. And so some people actually want to give us feedback

153
00:15:14,080 --> 00:15:19,600
and tell us what they want. So we've got we have a very active community of thousands of people

154
00:15:19,600 --> 00:15:24,080
that give a lot of like feature requests and you know it's kind of tricky sometimes because

155
00:15:24,080 --> 00:15:29,040
research touches really everything right. Everything we do often online like starts with search.

156
00:15:29,840 --> 00:15:36,560
And then we also yeah can look at sort of churn queries like things that you know people

157
00:15:36,560 --> 00:15:42,640
tried to do that they couldn't do and then they leave forever. And so sports was a good example

158
00:15:42,640 --> 00:15:48,480
of that that just kept bubbling up people wanting to see sports results. And so we now have

159
00:15:48,480 --> 00:15:54,000
actually releasing this week a bunch of you know results for sports that are live coming in from

160
00:15:54,720 --> 00:16:01,360
different APIs. So you can kind of see what's going on. So yeah it's a mix of direct feedback and

161
00:16:01,360 --> 00:16:05,680
then something direct feedback. Can you talk a little bit about the way that the ways that you're

162
00:16:05,680 --> 00:16:11,040
using summarization? You've referenced that a couple of times. Yeah yeah I think summarization

163
00:16:11,040 --> 00:16:16,880
in the limit is actually one of the hardest and most interesting and impactful and LP applications

164
00:16:16,880 --> 00:16:23,040
of our time right now because we're in the an attention economy and in the information age we need

165
00:16:23,040 --> 00:16:29,280
better summarization. But really if you think about it if you're deeply engrained in a space and

166
00:16:29,280 --> 00:16:34,000
you only got a summary for a new paper it's very different to when you've never read anything in

167
00:16:34,000 --> 00:16:40,800
that in that space right. I sort of because of progenous protein generation model I've been working

168
00:16:40,800 --> 00:16:48,640
on at Salesforce. Now I've been reading a lot more bio papers and like there's just so much

169
00:16:48,640 --> 00:16:54,560
lingo that you don't know that if you were to try to summarize and make it even shorter you

170
00:16:54,560 --> 00:16:59,680
would not understand anything. And so the summary might need to simplify things but also add some

171
00:16:59,680 --> 00:17:06,480
explanations for some things. And then you think about things where you're an expert like

172
00:17:06,480 --> 00:17:12,320
oh the Elmo paper was basically like the contextual vector code paper but instead of translation

173
00:17:12,320 --> 00:17:17,280
they use language modeling. And then Bert was essentially like Elmo but instead of you know

174
00:17:17,920 --> 00:17:23,120
an LSTM model they use a transformer model. And so that's like a one cent in summary if you know

175
00:17:23,120 --> 00:17:27,520
exactly what all of these things mean already you're like boom I get it and you know their models

176
00:17:27,520 --> 00:17:33,120
are larger is probably also a good add-on to that summary. But if you don't know what a language

177
00:17:33,120 --> 00:17:39,760
model is what a word vector is what you know what a neural network is on an LSTM and Bert and

178
00:17:39,760 --> 00:17:44,880
all of these things are then these are non-useful summaries for you. So long story short summary

179
00:17:44,880 --> 00:17:51,600
super interesting super hard AI problem but for us you know we're to say okay well we don't know

180
00:17:51,600 --> 00:17:57,760
that much about what what a user knows. How can we start with something that's useful for pretty

181
00:17:57,760 --> 00:18:04,080
much everyone where we started is basically with coding like if you look for this here's like the

182
00:18:04,080 --> 00:18:09,840
most relevant code snippet that's in some ways a you know sort of multimodal if you think of

183
00:18:09,840 --> 00:18:15,200
programming as a different modality to natural language multimodal kind of summary. And then

184
00:18:15,200 --> 00:18:20,000
another one is just pros and cons. So if you look for best headphones for instance you want to

185
00:18:20,000 --> 00:18:26,640
just extract whether the main pros and cons of this particular headset. So we kind of extract those

186
00:18:26,640 --> 00:18:32,960
from professional reviews review sites you can very quickly skim a bunch of results and know

187
00:18:32,960 --> 00:18:37,520
what are the pros and cons of the different headphones. Another one is recipes we heard a lot of

188
00:18:37,520 --> 00:18:42,560
people complain about having to read the whole life story of someone if they just want to get like

189
00:18:42,560 --> 00:18:48,080
a lasagna or chocolate chip cookie recipes. So we extracted like here the ingredients here the 10

190
00:18:48,080 --> 00:18:53,280
steps to actually make the cookie and then you're done. So those are examples that you already see

191
00:18:53,280 --> 00:18:58,720
in the product on u.com that you can just kind of see a useful summary that pretty much for everyone

192
00:18:58,720 --> 00:19:05,440
is going to be universally useful. And are summaries the way that you're using them are they

193
00:19:06,000 --> 00:19:14,000
tending towards more generative summaries versus extractive summaries or do you use both in

194
00:19:14,000 --> 00:19:18,800
different places? You have to you have to kind of use both our values or trust facts and kindness

195
00:19:18,800 --> 00:19:24,560
and if you think about it as much as I love these language models you can't quite trust their facts

196
00:19:24,560 --> 00:19:29,200
yet, right? They they make stuff up, right? You can just say write me an article about how Hillary

197
00:19:29,200 --> 00:19:34,160
can one the election and it will write you a perfectly reasonable sounding article how that

198
00:19:34,160 --> 00:19:42,000
that happened, you know? And so the veracity and just like factual correctness of large language

199
00:19:42,000 --> 00:19:48,480
models is still you know iffy sometimes. So you can't just let them generate you might get some

200
00:19:48,480 --> 00:19:55,040
some pretty not so great tasting cookies if you just do that. So you have to you have to start

201
00:19:55,040 --> 00:20:01,200
with with some clear extractions but then you can kind of simplify things and get rid of redundancies

202
00:20:01,200 --> 00:20:09,520
for multi-document summarization and things like that. Your example reminded me of one of the

203
00:20:09,520 --> 00:20:20,320
issues that came up with Google sometime this past year where I think the you know just one of

204
00:20:20,320 --> 00:20:25,440
these examples I'd be typing a query what should you do if you know someone's having a seizure

205
00:20:25,440 --> 00:20:34,000
and like it extracted the things the list of things not to do. That's right. And you know I'm curious

206
00:20:34,000 --> 00:20:41,440
if you can talk about you know in the in the context of using similar technology and trying to

207
00:20:41,440 --> 00:20:47,600
to present information you know in a similar kind of summarized way like how do you you know

208
00:20:47,600 --> 00:20:52,720
when you're starting from the ground up build guard rails you know so that you you can achieve

209
00:20:52,720 --> 00:20:57,680
this level of trust that you're aspiring to. Yeah it's a good question. I actually think you

210
00:20:57,680 --> 00:21:04,640
you have to keep your users and yourself and the use cases in mind and the more important

211
00:21:04,640 --> 00:21:10,320
the use case is the more important it is to really have some human oversight you know I love AI

212
00:21:10,320 --> 00:21:18,400
I think I can change anything from cow diets to reduce methane to you know agriculture to medicine

213
00:21:18,400 --> 00:21:23,280
and creating new protein structures and all of these different things but when it comes to like

214
00:21:23,280 --> 00:21:28,400
life-threatening information you still need to have some human oversight so for instance when

215
00:21:28,400 --> 00:21:34,160
you when you ask how to commit suicide we just have a handcrafted message that your life matters

216
00:21:34,160 --> 00:21:38,720
and you know don't do it here's a suicide hotline you don't want some AI to kind of do that

217
00:21:39,520 --> 00:21:44,400
of course there are some interesting sort of chatbot applications to have longer conversations

218
00:21:44,400 --> 00:21:49,760
with someone who is thinking about this and I think like companies like replica where full

219
00:21:49,760 --> 00:21:54,080
disclosure I'm an investor too you know I have done a phenomenal job kind of being essentially

220
00:21:54,080 --> 00:21:59,680
a journal that talks back to you and seems to care about you and kind of helps you work through

221
00:21:59,680 --> 00:22:04,160
issues so there are some exciting and interesting applications in that space as well but yeah

222
00:22:04,160 --> 00:22:10,480
we are basically thinking about the more impactful the application is the more careful we have to

223
00:22:10,480 --> 00:22:18,720
be in just letting AI run run off and do its thing. You know when you were when you were working on

224
00:22:18,720 --> 00:22:26,960
metamine I'm imagining that you know you had to be pretty far out in the you know it's called

225
00:22:26,960 --> 00:22:35,680
the research domain to you know get these raw tools to do the kind of things that you wanted them

226
00:22:35,680 --> 00:22:43,360
to do you know how how how much is that change or not now like are you able to kind of are

227
00:22:43,360 --> 00:22:52,720
using mostly off the shelf stuff or you you know thinking are you required to do a lot of you

228
00:22:52,720 --> 00:23:01,760
know research oriented things you know novel architectures or novel training methods or that kind

229
00:23:01,760 --> 00:23:07,520
of thing. Yeah it's a great question I think it's actually gotten so much easier to build AI

230
00:23:07,520 --> 00:23:13,120
companies and just functionally like useful algorithms it's it's been really incredible to see

231
00:23:13,120 --> 00:23:20,640
like back in metamine we still built those CNNs and C++. And so it sounds like the your general

232
00:23:20,640 --> 00:23:25,600
take is that there you have a lot more ability to kind of pull things off the shelf and

233
00:23:28,000 --> 00:23:34,640
and you get the you have raw horsepower there that you're not kind of needing to push the

234
00:23:34,640 --> 00:23:42,800
innovation frontier in that way. Yeah so yeah that I think you can build a lot more you can have

235
00:23:42,800 --> 00:23:47,360
a lot of impact with the applications in fact we've done so much research that you can just

236
00:23:48,160 --> 00:23:53,120
have I think currently more impact in just applying that research to all these different domains

237
00:23:53,120 --> 00:23:57,920
and problems that you see in the world and workflows that are not yet automated as much as they

238
00:23:57,920 --> 00:24:05,200
can. Then you could I think in pure research right now it's kind of been in terms of pure research

239
00:24:05,200 --> 00:24:10,080
and sort of major ideas it's been mostly executing on make the models larger

240
00:24:11,120 --> 00:24:17,200
efficient to optimize and you know paralyze on current hardware and get more training data

241
00:24:17,200 --> 00:24:22,000
and collect that training data in reasonable ways and think about the bias and so on.

242
00:24:22,000 --> 00:24:28,080
And the models haven't really changed and honestly even if like there's a different kind of model

243
00:24:28,080 --> 00:24:32,000
we just need a very large general function approximated that's efficient and has enough

244
00:24:32,000 --> 00:24:38,400
nobs to tune and then you can kind of optimize that whole that whole setup. My hunch is if it wasn't

245
00:24:38,400 --> 00:24:45,280
for the particular hardware that we're using we would probably be we could use any other large

246
00:24:45,280 --> 00:24:50,320
model too. I think LSTMs aren't inherently worse they're just like less easy to optimize and

247
00:24:50,320 --> 00:24:57,520
paralyze and then train henceforth on our current hardware. So that's one aspect of the answer.

248
00:24:57,520 --> 00:25:03,680
The second one is we definitely have to still innovate because there is no hugging phase model

249
00:25:03,680 --> 00:25:09,920
to build a neural network ranking mechanism that takes into consideration your intent your slots

250
00:25:09,920 --> 00:25:14,320
and then 150 of the apps that you know we build for the first time ever and no search engine ever

251
00:25:14,320 --> 00:25:18,960
had before and then ranking them properly. So you do still have to innovate and build brand new

252
00:25:18,960 --> 00:25:25,120
models. Now for now we can't really publish those. I actually hope that at some point we are

253
00:25:26,160 --> 00:25:31,120
safe enough as like in terms of the existence of the company that we can be much more open in

254
00:25:31,120 --> 00:25:36,080
the future but yeah so the answers of course we still have to massively innovate because of

255
00:25:36,080 --> 00:25:41,120
these hard and interesting new problems that we're tackling where we also don't have that much

256
00:25:41,120 --> 00:25:46,160
data right. So large language models enable us to know that if you look for you know a Chinese

257
00:25:46,160 --> 00:25:51,600
restaurant near me or East Asian restaurants in my area or close to where I am and all of these

258
00:25:51,600 --> 00:25:56,960
things they all mean the same thing. That was you know something that in the past you couldn't

259
00:25:57,600 --> 00:26:03,520
know and you would have to get a ton of training data and then you know be able to actually map

260
00:26:03,520 --> 00:26:09,920
them all to a similar kind of you know natural language sorry API language to then triggers.

261
00:26:09,920 --> 00:26:15,760
You essentially have to translate human language into the language of computers and APIs.

262
00:26:15,760 --> 00:26:21,280
So so there has to be a lot of innovation on our side for that and then maybe a last note since

263
00:26:21,280 --> 00:26:28,240
we brought up Meta Mind is that back then Meta Mind tried to do I think so many different things

264
00:26:28,240 --> 00:26:33,520
like help you kind of label your data and drag and drop it into the browser. That's you know

265
00:26:33,520 --> 00:26:39,040
scale and crowd flower now and scale is like an eight billion dollar company or something.

266
00:26:39,040 --> 00:26:43,280
That that piece of itself that was just like one of the many features of Meta Mind and then

267
00:26:43,280 --> 00:26:48,800
you know deploying it and scaling that deployment and helping you do error analysis and then

268
00:26:48,800 --> 00:26:56,240
just making it a simple Python interface to actually run your AI classifier or model in production.

269
00:26:56,240 --> 00:27:02,240
All of these things now have companies that are valued in the hundreds or millions or billions

270
00:27:02,240 --> 00:27:07,040
of dollars each of these one like separate features that we had implemented Meta Mind from scratch

271
00:27:07,040 --> 00:27:13,680
pre you know having anything like PyTorch or TensorFlow and so it's just fascinating how a good

272
00:27:13,680 --> 00:27:20,560
tooling is gotten for AI and that was sort of my tangent on like we're investing in both vertically

273
00:27:20,560 --> 00:27:25,680
integrated but also sort of tooling companies that AI expenditures and and because of that it's

274
00:27:25,680 --> 00:27:30,880
gotten a lot easier for people to to have impact in those applications. You reference the

275
00:27:30,880 --> 00:27:38,000
particulars of the hardware that you're using is that say that you're using kind of non-traditional

276
00:27:38,000 --> 00:27:45,520
exotic things or just that you know there's a affinity between GPUs and and transformers that

277
00:27:45,520 --> 00:27:52,400
allows you to scale. Yeah we definitely are using very standard hardware because we need to scale

278
00:27:52,400 --> 00:28:00,160
we need to be able to pull up a data center in in Europe or in Asia and in different different

279
00:28:00,160 --> 00:28:06,800
places so we don't really want to rely on any non-standard hardware right now and even GPUs are

280
00:28:06,800 --> 00:28:12,480
often there's a shortage so we have to sometimes map some of the GPU models and see if we can

281
00:28:12,480 --> 00:28:18,800
make them fast enough on the CPU just so that we can have more data centers and have you know less

282
00:28:18,800 --> 00:28:26,400
lag time when they're not enough GPUs available in certain Geos so yeah it's it's an interesting

283
00:28:26,400 --> 00:28:35,200
interesting challenge for sure. And how how are you finding the level of maturity from an engineering

284
00:28:35,200 --> 00:28:40,960
perspective to allow you to achieve that that kind of scale? I guess it's just about the people

285
00:28:40,960 --> 00:28:48,320
like we've gotten really lucky and I'm having hired like an incredible AI and engineering team

286
00:28:48,320 --> 00:28:53,360
and also DevOps you know just like like spawning out all these machines with a click of a button

287
00:28:53,360 --> 00:28:58,720
you have a whole new data center and reduce latency for people in a different geo it's it's

288
00:28:58,720 --> 00:29:06,320
yeah mostly about the people and I'm imagining you've invested significantly in kind of building

289
00:29:06,320 --> 00:29:12,560
out a enabling platform that allows you to kind of develop and train models quickly and get them

290
00:29:12,560 --> 00:29:18,160
in a production quickly was that a big focus? Yeah yeah it's definitely definitely something we're

291
00:29:18,160 --> 00:29:22,320
also relying on you know things like weights and biases full disclosure I'm also an investor and

292
00:29:22,320 --> 00:29:29,280
them to help with with experiments and running those and you know there are a lot of a lot of good

293
00:29:29,280 --> 00:29:34,400
tools that you can use now but ultimately to actually spawn out the whole system and like have

294
00:29:34,400 --> 00:29:39,840
a new end-to-end you know search engine that runs with a bunch of different machines and so

295
00:29:39,840 --> 00:29:45,520
on they all communicating that is still something that we had to build ourselves from scratch and then

296
00:29:45,520 --> 00:29:51,040
we also want to make it easy to create a new application so now we just have it so that you have

297
00:29:51,040 --> 00:29:56,880
a new JSON-like data dump and then boom of like a config file and you have a new application within

298
00:29:56,880 --> 00:30:03,360
u.com and so I'm excited to in the future essentially let anyone kind of build that and have

299
00:30:03,360 --> 00:30:10,960
search capabilities over all that data so I think automating search over new kinds of data sets

300
00:30:10,960 --> 00:30:18,560
that has been you know an interesting and tough challenge that we tackled. We spoke a little

301
00:30:18,560 --> 00:30:26,640
bit earlier about the code module code application and a lot of ways the kind of the use case that

302
00:30:26,640 --> 00:30:31,200
you described of hey you know I'm searching Stack Overflow really I just want this code snippet

303
00:30:32,480 --> 00:30:38,480
I think I mentioned that in a conversation with Greg Brockman about codex you know how that's

304
00:30:38,480 --> 00:30:44,400
you know ultimately what we want you know what we need you know in terms of the process of

305
00:30:44,400 --> 00:30:52,560
building that module can you maybe compare contrast with what you've seen others do or I'm you know

306
00:30:52,560 --> 00:31:01,040
codex copilot that kind of thing. Yeah I think you know I think there's basically I think about

307
00:31:01,040 --> 00:31:06,320
this in kind of two levels either you're trying to solve a problem that people have solved before

308
00:31:06,320 --> 00:31:12,240
and at that point you just want the direct code snippet as is or you're trying to combine it

309
00:31:12,240 --> 00:31:17,280
and and have you know sort of a new combination of problems that no one has yet quite solved

310
00:31:17,280 --> 00:31:23,200
like this before and at that point you need like codex or like our code complete and you

311
00:31:23,200 --> 00:31:30,160
.com to just give you the answer and generate something novel and and it's kind of incredible how

312
00:31:30,160 --> 00:31:34,720
these models aren't just kind of able to deal with things inside the convex hull but really in

313
00:31:34,720 --> 00:31:40,560
the hypercube of like you know different combinations and combinatorial that I mean combinatorial

314
00:31:40,560 --> 00:31:46,000
combinations of things they have seen in the training data to generate and then combine them so

315
00:31:47,360 --> 00:31:54,320
that's kind of how I think about these two levels of generation. It's a ground-up model that you

316
00:31:54,320 --> 00:32:00,240
built as opposed to an API that you're using. No yeah we're also using API for the code generation.

317
00:32:00,240 --> 00:32:08,560
Oh for the code generation you are. For the models that you're building what are some of the

318
00:32:08,560 --> 00:32:13,520
training data sources that you rely most heavily on? I guess there's you know sort of large-scale

319
00:32:14,640 --> 00:32:21,760
internet available data that is there and then we have to crawl a ton too. So that is probably

320
00:32:21,760 --> 00:32:28,080
the biggest one and it's been something that I think a lot of machine learning leaders can relate to

321
00:32:28,080 --> 00:32:32,880
which is everyone wants to come in and build cool models and you know but then they realize

322
00:32:32,880 --> 00:32:36,160
man that's really hard so they download hugging face models and that's just kind of work out

323
00:32:36,160 --> 00:32:43,840
of the box and then the biggest thing is that you know everyone wants to not very few ML engineers

324
00:32:43,840 --> 00:32:49,600
want to spend a lot of time on data which you know let Andrew to say oh let's just have data

325
00:32:50,320 --> 00:32:54,800
competitions like we're ever can get the most interesting data set for for this problem and that's

326
00:32:54,800 --> 00:32:59,680
something for us that that often meant we had to spend a lot of time crawling and eventually we

327
00:32:59,680 --> 00:33:04,480
just hired some people who are actually excited about crawling data and getting us that data that

328
00:33:04,480 --> 00:33:09,200
we need to then be able to actually train summarizing models summarization models and so on

329
00:33:10,640 --> 00:33:16,560
later on and so yeah it's been a continuous challenge to crawl and it's one of the many places

330
00:33:16,560 --> 00:33:21,360
that the monopoly of Google comes into because there's some sites that say only Google's allowed

331
00:33:21,360 --> 00:33:25,440
to crawl us no one else's and they're like well you know however we're gonna be Google if we

332
00:33:25,440 --> 00:33:31,200
can't do that and so there yeah all kinds of interesting challenges both on the technical side but

333
00:33:31,200 --> 00:33:39,280
also the sort of systemic side okay you mentioned earlier when I asked about page rank I thought

334
00:33:40,000 --> 00:33:47,760
your response was saying that you weren't crawling for kind of the the index but rather you

335
00:33:47,760 --> 00:33:54,880
are consuming that via an API so that's the index but you are crawling for some of these other

336
00:33:54,880 --> 00:34:00,400
applications your building is that the idea yeah sorry I was yeah there's some ambiguity there so

337
00:34:00,400 --> 00:34:06,880
we actually think that the list of like a blue link of lists a blue list of links isn't going

338
00:34:06,880 --> 00:34:14,560
to be as important anymore as you know the actual larger content islands like Reddit like medium

339
00:34:14,560 --> 00:34:20,960
like Twitter and so on and then in order to be able to summarize things you also can't really

340
00:34:20,960 --> 00:34:25,520
do that on the fly these large models are not fast enough people want things in hundreds of milliseconds

341
00:34:25,520 --> 00:34:32,800
and took us a long time to for 90% of queries now be faster than you know Dr. Go and other competitors

342
00:34:32,800 --> 00:34:38,080
in the search engine space and almost as fast as Google at least when you're close to our

343
00:34:38,080 --> 00:34:43,200
data centers we don't have as many all over the world of course but a long story short

344
00:34:43,920 --> 00:34:49,280
we are actually crawling a ton of data in order to build these apps and make them fast enough

345
00:34:50,080 --> 00:34:55,040
there are also some several times where we thought we could rely on an API from someone else

346
00:34:55,040 --> 00:35:01,280
but then just the scale and burstiness of search and when you get tens of thousands of queries

347
00:35:02,240 --> 00:35:06,480
in a few hours like you just know API was able to deal with that we have to build

348
00:35:07,280 --> 00:35:12,960
and have that content ourselves index it be able to do interesting vector search operations

349
00:35:12,960 --> 00:35:17,360
and things like that with the data all in a few hundred milliseconds to then be able to

350
00:35:17,360 --> 00:35:23,280
surface the right kind of content very quickly so yeah we're kind of slowly crawling the web

351
00:35:23,280 --> 00:35:29,680
through the most important content islands like Stack Overflow like GitHub like you know

352
00:35:29,680 --> 00:35:36,080
PyTorch or HuggingFace documentation or all of Medium which is also pretty large so

353
00:35:36,080 --> 00:35:40,720
there are a bunch of interesting things that we are you know sort of we have to crawl ourselves

354
00:35:40,720 --> 00:35:44,960
just to be able to have the speed and the AI capabilities that you have to run offline

355
00:35:44,960 --> 00:35:53,360
the the goal is to produce a better general search engine but you've also specialized in some

356
00:35:53,360 --> 00:35:58,400
ways that makes it a super interesting search engine today for more technical folks

357
00:36:00,000 --> 00:36:05,200
yeah how do you think about like one you hit the knee of the curve that it's like better for

358
00:36:05,200 --> 00:36:11,040
for everyone yeah well we've learned so far is that we're better for developers already

359
00:36:11,040 --> 00:36:17,760
like a lot of people I posted with a couple of features on a Twitter thread for our you know you

360
00:36:17,760 --> 00:36:24,880
code kind of special search and it blew up like crazy hundreds of thousands like 300 400

361
00:36:24,880 --> 00:36:30,000
thousand impressions thousands of likes and so it resonates a lot with that crowd now what we've

362
00:36:30,000 --> 00:36:34,800
learned is that I sort of jokingly say it turns out developers are people too and they want to know

363
00:36:34,800 --> 00:36:39,600
what the weather outside is and what the sports results are and how to travel and like all of these

364
00:36:39,600 --> 00:36:46,080
things and so you know where to buy food and maybe order food and so we have to basically if you

365
00:36:46,080 --> 00:36:50,160
want to be the best search engine for developers and be your default and be there every day and in

366
00:36:50,160 --> 00:36:57,840
your nav bar through you know Chrome extensions and things like that we have to be able to do

367
00:36:57,840 --> 00:37:03,360
everything else in search too which is tough for a small startup but we've now gotten to a point

368
00:37:03,360 --> 00:37:09,520
where once we launch sports results there's maybe only the travel category where we're not as good

369
00:37:09,520 --> 00:37:16,400
for everyone else and then most other things we actually are you know we have answers for

370
00:37:16,400 --> 00:37:22,000
movies and and things like that and you know there are still some APIs like the movie API that

371
00:37:22,000 --> 00:37:27,120
is a little bit slow so it takes like two or three seconds to load rather than you know less than

372
00:37:27,120 --> 00:37:34,800
one second and we've gotten complaints about that also but yeah there's some proprietary data that

373
00:37:34,800 --> 00:37:40,720
we could probably just crawl I guess the law just kind of changed a little bit because LinkedIn

374
00:37:40,720 --> 00:37:46,960
lost a big lawsuit that you know they tried to prevent folks from from crawling data but I'm

375
00:37:46,960 --> 00:37:53,760
sorry short a lot of pralings happening and the speed is speed is always super important and we

376
00:37:53,760 --> 00:38:01,040
are we are having to build a lot of that in house of course congrats on on the on the launch of

377
00:38:01,040 --> 00:38:09,520
v.com and and you.com code and write before we part ways I did want to circle back on a couple of

378
00:38:09,520 --> 00:38:16,320
the things that we spoke about that you started at Salesforce it sounds like you're you're still

379
00:38:16,320 --> 00:38:22,560
working on those the protein generation one we spent a fair amount of time talking about that

380
00:38:22,560 --> 00:38:28,800
the last time we spoke and we'll include the link to that in the the show notes we didn't I don't

381
00:38:28,800 --> 00:38:34,320
think spend much time talking about the AI economist I think the timing didn't quite work out to

382
00:38:34,320 --> 00:38:42,720
dig into that so would love to have you share a bit about that project and I think you have some

383
00:38:42,720 --> 00:38:49,680
recent news there new recent updates that's right this week in machine learning we we actually

384
00:38:49,680 --> 00:38:55,520
got the science advances paper out about the AI economist and maybe just at a very high level

385
00:38:55,520 --> 00:39:04,160
what is the AI economist was word by Stefan Stefan Chang and Alex Trot and and a few others

386
00:39:04,160 --> 00:39:11,120
at Salesforce and myself and basically the idea is using reinforcement learning for some of the

387
00:39:11,120 --> 00:39:18,000
most important applications that we can think about it for humanity period instead of having our

388
00:39:18,000 --> 00:39:24,880
L play games that are kind of interesting but ultimately themselves not very useful why don't we

389
00:39:24,880 --> 00:39:31,200
see if we can build a very realistic simulation and we're far from that in terms of realism right

390
00:39:31,200 --> 00:39:40,320
now this will have to scale up over time too but I think it's a brand new area of AI that can have

391
00:39:40,320 --> 00:39:45,760
a huge amount of impact and so the high level ideas you have a two level reinforcement learning

392
00:39:45,760 --> 00:39:53,920
problem where you have an RL agent that sets that sets taxes and subsidies for a bunch of other

393
00:39:53,920 --> 00:39:59,200
RL agents that which themselves are just trying to optimize their own utility as in they try to

394
00:39:59,200 --> 00:40:05,040
maximize resources they can collect money they can make houses they can build blocking of other

395
00:40:05,040 --> 00:40:11,040
people from resources by you know building houses around them things like that and are basically

396
00:40:11,040 --> 00:40:17,840
to some degree more selfish and you know but may also eventually identify patterns to collaborate

397
00:40:17,840 --> 00:40:24,240
towards their own selfish goals and maximizing their own utility functions and so the interesting

398
00:40:24,240 --> 00:40:30,720
thing is now you can give that top level RL agent that sort of the AI economist the ability

399
00:40:30,720 --> 00:40:37,680
to subsidize or tax different income groups differently in order to maximize a specific objective

400
00:40:37,680 --> 00:40:44,000
that you've given that AI so the idea here is that you can now say oh I want to help the middle class

401
00:40:44,000 --> 00:40:50,480
or I want to maximize productivity of this economy or I want to maximize equality in this economy

402
00:40:50,480 --> 00:40:55,760
or a combination of all of these things that you wait and you say okay I care about the one we

403
00:40:55,760 --> 00:41:00,560
chose in the end was productivity multiplied with equality which is one minus the genie index

404
00:41:00,560 --> 00:41:04,880
it's essentially thinking about how how equal you want to be you don't like you know in the

405
00:41:04,880 --> 00:41:09,680
limit you don't want to be like everyone is extremely equal but extremely poor right that's

406
00:41:09,680 --> 00:41:14,960
that's not helpful too so you have this like overall productivity in there as well and so that

407
00:41:14,960 --> 00:41:19,840
that is kind of the high level and so what that means is that you know if you if you take that

408
00:41:19,840 --> 00:41:27,040
idea and you really scale it out and you make the simulation more realistic you increase the size

409
00:41:27,040 --> 00:41:34,880
of the number of agents to hundreds of thousands and you actually put in sort of historic data

410
00:41:34,880 --> 00:41:40,880
into this that you know to start the model that in the future of a politician says oh I'm doing

411
00:41:40,880 --> 00:41:46,800
like these following five things to help the middle class or to help this particular group of people

412
00:41:46,800 --> 00:41:55,040
whoever it is like worldwide right then you can run that suggestion across and compare it

413
00:41:55,040 --> 00:42:04,960
and contrast it with millions and millions of years of simulated taxation where you basically

414
00:42:04,960 --> 00:42:10,960
try to identify what the fairest or best or most sustainable or most equal or most productive

415
00:42:10,960 --> 00:42:17,840
way is to tax that entire system that touches on highly philosophical things like you know

416
00:42:17,840 --> 00:42:24,560
communism capitalism socialism market market economy and so on and combinations of these

417
00:42:24,560 --> 00:42:30,080
systems on the one side but it's very concrete like it'll you know could change and be another

418
00:42:30,080 --> 00:42:36,720
input into economists models to be more realistic it's kind of crazy but you know there's there's

419
00:42:36,720 --> 00:42:42,560
models that are being used right now like the ps formula very famous Berkeley professor in economics

420
00:42:43,600 --> 00:42:50,480
who has this provably correct way to tax different income brackets but it's provably correct in a

421
00:42:50,480 --> 00:42:58,400
one-step economy turns out people iterate right like turns out time moves on and and and so this

422
00:42:58,400 --> 00:43:04,320
AI economist model can actually recover the provably correct solution for a one-step economy

423
00:43:04,320 --> 00:43:10,560
but then as the models learn as the agents adapt as the time continues like that model just

424
00:43:10,560 --> 00:43:16,960
is so much more powerful and realistic than any of the linear models and one-step models that

425
00:43:16,960 --> 00:43:24,000
we're currently using that it's just hard for me to not see how that won't change all of economics

426
00:43:24,000 --> 00:43:29,600
which in a grand scheme of things has it been an area that hasn't been impacted by I nearly as

427
00:43:29,600 --> 00:43:34,800
much as I think it could or should and if you think about how much bloodshed there has been human

428
00:43:34,800 --> 00:43:41,120
history to identify what the right model is of taxation and representation and things like that

429
00:43:41,120 --> 00:43:47,360
like it's just so powerful to be able to try to offload that into a simulation get millions of

430
00:43:47,360 --> 00:43:53,360
years of taxation going and then you know learn from that and see if we can use some of these things

431
00:43:53,360 --> 00:43:58,240
and of course you know like we don't want like an iDictator either like we need it as like another

432
00:43:58,240 --> 00:44:07,200
data point as a model that helps us make more you know more accurate like decisions but ultimately

433
00:44:07,200 --> 00:44:13,040
people still want to decide what the objective is so that's still like a very important one

434
00:44:13,040 --> 00:44:16,880
and we have to sanity check it of course before you implement these things so I'm not like

435
00:44:16,880 --> 00:44:21,520
absolutist like this has to be like a new religion or anything but like it I think it's just

436
00:44:21,520 --> 00:44:26,880
such a powerful tool and and I have high hopes that just like what we've seen in the

437
00:44:26,880 --> 00:44:31,760
linguistics and natural language processing or we've seen in computer vision or we've seen in

438
00:44:31,760 --> 00:44:36,960
robotics or we've seen self-driving all of these different application areas of AI

439
00:44:36,960 --> 00:44:43,440
that economics could be another such application area sounds like a model that would be really good

440
00:44:43,440 --> 00:44:50,640
at the Sims yeah it's it's not actually crazy to think that that is like a pretty good simulation

441
00:44:50,640 --> 00:44:55,600
now of course the problem is that it doesn't capture like sort of as realistic utility functions

442
00:44:55,600 --> 00:45:00,560
that people have like in the Sims like like people might not get as tired and then like just don't

443
00:45:00,560 --> 00:45:04,240
want to work anymore because they want to sleep and things like that so you want to you know

444
00:45:04,240 --> 00:45:08,800
adjust and for most people like you know they're sort of logarithmic happiness curves to like

445
00:45:09,360 --> 00:45:13,440
making like an order of magnitude you need to make an order of magnitude more money often in order

446
00:45:13,440 --> 00:45:18,400
to be like a little bit happier and then it sort of levels out logarithmically they're all kinds

447
00:45:18,400 --> 00:45:23,440
of interesting things that we have found in in psychology but what's fascinating too is that you

448
00:45:23,440 --> 00:45:28,240
can actually say well I think people are this and that like I think people are going to want to

449
00:45:28,240 --> 00:45:33,520
work more I want to work less you can actually make that very explicit in the beginning of the

450
00:45:33,520 --> 00:45:39,840
simulation and then see how those assumptions about how people you know define their own

451
00:45:39,840 --> 00:45:46,880
utilities will actually influence the optimal political model or not political some degree you know

452
00:45:46,880 --> 00:45:54,480
the few kind of group all of these different policies into one cluster but you know just generally

453
00:45:54,480 --> 00:46:03,040
sort of taxation and financial is the work on behavior economics you know things like predictably

454
00:46:03,040 --> 00:46:10,720
irrational all of that you know does it does it say that you kind of akin to what you're saying

455
00:46:10,720 --> 00:46:15,760
that everyone has their own utility functions and they're not as uniform as traditional economics

456
00:46:15,760 --> 00:46:23,920
might like or you know is there is it more that you know there's just an emotional irrational

457
00:46:23,920 --> 00:46:29,200
component and if that is the case like how do you even model something like that yeah it's a

458
00:46:29,200 --> 00:46:35,600
great question so you can you can have a prior distribution and then you sample different like

459
00:46:35,600 --> 00:46:40,160
you know you sample from that prior distribution that you have for instance for utility functions

460
00:46:41,120 --> 00:46:45,520
and then you know based on that sample and based on how you define your prior distribution

461
00:46:46,080 --> 00:46:52,720
you can get different sets of agents that that come out of it and so so that's that's one one

462
00:46:52,720 --> 00:47:00,640
aspect and then and then there's some things where the irrationality has not been captured yet

463
00:47:00,640 --> 00:47:05,360
as as realistically in that simulation just the idea that you know sometimes people do

464
00:47:06,000 --> 00:47:10,160
something that they know is actually sub-optimal for them but they think because of fairness they

465
00:47:10,160 --> 00:47:16,720
want to still do it and and so those are are not yet modeled in that simulation that we ran

466
00:47:16,720 --> 00:47:24,160
but at the same time those are L agents that have neural networks and can try to adjust their behavior

467
00:47:24,160 --> 00:47:29,920
to others and so on still much more realistic than anything that economists use nowadays which is

468
00:47:29,920 --> 00:47:37,280
like often linear models and one step kind of like probably correct formulas and if you have

469
00:47:37,280 --> 00:47:43,920
you published the models themselves or the simulation environment yes it's actually extremely important

470
00:47:43,920 --> 00:47:48,720
you know imagine someone is like I know what this is right for everyone and I had a I said it

471
00:47:48,720 --> 00:47:53,520
let's all trust it that's of course a terrible idea so you have to open source these models you

472
00:47:53,520 --> 00:47:59,200
have to open source all the assumptions you made that went into the simulation and the simulation

473
00:47:59,200 --> 00:48:03,360
itself that could be some pretty insidious bugs right if you said oh this is how everyone's

474
00:48:03,360 --> 00:48:08,480
gonna tax get taxed and then there was a bug and you're like oops so you know a lot of people need

475
00:48:08,480 --> 00:48:13,760
to do this and and that's you know one thing I loved about Salesforce Research 2 can still love

476
00:48:13,760 --> 00:48:20,960
that you know for these kinds of important human kinds of applications we we did open source

477
00:48:20,960 --> 00:48:26,320
all that model and there there's some really exciting ongoing projects now that you know you

478
00:48:26,320 --> 00:48:32,320
can use this also to avoid things like tragedy of the comments where it's like old example of

479
00:48:32,320 --> 00:48:36,480
if all the sheep farmers put all their sheep into one field and the field just gets completely

480
00:48:36,480 --> 00:48:40,800
destroyed and no one has a field anymore if any sheep so you have to kind of partner up and

481
00:48:40,800 --> 00:48:44,320
make sure you don't use your resources too much it's something that I think we're going to hit

482
00:48:44,320 --> 00:48:50,000
worldwide in terms of sustainability and deforestation and things like that and water

483
00:48:50,720 --> 00:48:56,480
so we all have to kind of avoid tragedy of the comments in like sort of worldwide yeah

484
00:48:57,200 --> 00:49:04,800
given the meta factors that we've talked about you know with regard to the way revenue models

485
00:49:04,800 --> 00:49:11,760
impact you know search engine behavior like how does u.com become a viable company

486
00:49:12,720 --> 00:49:18,080
if it's not going to be ad-based and fall into the same traps that you know we saw with Google

487
00:49:18,080 --> 00:49:24,320
yeah great question so the main goal is to have these applications that we're building actually

488
00:49:24,320 --> 00:49:29,680
provide enough value that people would want to pay for them you write as one example you know

489
00:49:29,680 --> 00:49:36,240
costs a lot of money to run a large language model as it writes a blog post for you or an essay

490
00:49:36,240 --> 00:49:42,640
like you can pay for that and that's one thing I also think that private ads can be used

491
00:49:42,640 --> 00:49:46,480
especially in our private mode where we don't log anything we don't know what's going on at all

492
00:49:47,280 --> 00:49:50,880
and we can't really monetize it in any way other than through private ads and what I mean by

493
00:49:50,880 --> 00:49:56,400
private ads is just ads that are dependent on the query and that's a luxury that you can have

494
00:49:56,400 --> 00:50:01,440
as a search engine because people give you an intent of what they want to do if your social

495
00:50:01,440 --> 00:50:06,240
network they don't really tell you like I want to buy an air purifier right now they just talk

496
00:50:06,240 --> 00:50:10,880
to their friends and be like oh I'm peep sneezing or coughing and maybe I have dust mites in my home

497
00:50:10,880 --> 00:50:15,040
and and then you kind of have to like spy on them if you want to sell the ads to like

498
00:50:15,040 --> 00:50:21,840
know what they might want to buy in the future but as a search engine even if you don't know anything

499
00:50:21,840 --> 00:50:29,760
about the user if you just look at a query and then you based on that query give an ad I think that

500
00:50:29,760 --> 00:50:34,400
is is better and it's kind of what we've seen ducked up go doing too you know they care about

501
00:50:34,400 --> 00:50:40,400
privacy as well and they have these private ads that basically are not user dependent they're

502
00:50:40,400 --> 00:50:48,960
only query dependent and the advertisers don't really know which user is seeing the ad and you know

503
00:50:48,960 --> 00:50:53,520
just it's just basically really only based on the query and so I think that that can be kind of

504
00:50:53,520 --> 00:50:57,600
a backup but I really hope that we can build something that's useful enough that people are

505
00:50:57,600 --> 00:51:03,040
going to want to pay money for certain things well Richard it has been wonderful catching up

506
00:51:04,160 --> 00:51:08,480
again congrats on the recent launches and all the amazing work that's gone into

507
00:51:09,280 --> 00:51:15,040
building what you built over the couple years the past couple years and looking forward to

508
00:51:15,040 --> 00:51:19,840
catching up again soon thanks so much great questions and yeah been a pleasure chatting with you

