1
00:00:00,000 --> 00:00:15,440
Hello and welcome to another episode of Twinmultaugh, the podcast where I interview interesting

2
00:00:15,440 --> 00:00:20,040
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,040 --> 00:00:22,520
I'm your host, Sam Charington.

4
00:00:22,520 --> 00:00:27,480
The show you're about to hear is part two of our O'Reilly AI New York series sponsored

5
00:00:27,480 --> 00:00:29,160
by Intel Nirvana.

6
00:00:29,160 --> 00:00:32,880
I'm super grateful to them for helping make this series possible.

7
00:00:32,880 --> 00:00:37,440
And I'm excited about the cool stuff they launched at the O'Reilly AI New York Conference,

8
00:00:37,440 --> 00:00:42,640
including version 2.0 of their neon framework and their new Nirvana graph project.

9
00:00:42,640 --> 00:00:46,000
Be sure to check them out at intelnervana.com.

10
00:00:46,000 --> 00:00:50,040
If you haven't already listened to the first show in this series, where I interview Naveen

11
00:00:50,040 --> 00:00:56,240
Raugh, who leads Intel's newly formed AI products group, and Hanlon Tang and algorithms

12
00:00:56,240 --> 00:01:02,200
engineer on that team, it's Twinmultaugh number 31, and you definitely want to start there.

13
00:01:02,200 --> 00:01:06,080
My guess for this show is Doug Eck of Google Brain.

14
00:01:06,080 --> 00:01:10,720
Doug did a keynote at the O'Reilly conference on magenta, Google's project for melding

15
00:01:10,720 --> 00:01:12,680
machine learning and the arts.

16
00:01:12,680 --> 00:01:18,200
Doug and I talk about the newly announced performance RNN project, which uses neural networks

17
00:01:18,200 --> 00:01:21,880
to create expressive AI-generated music.

18
00:01:21,880 --> 00:01:25,800
The demonstrations of this project are truly incredible and I encourage you to check them

19
00:01:25,800 --> 00:01:28,800
out via the links we're placing in the show notes.

20
00:01:28,800 --> 00:01:30,800
All right, on to the show.

21
00:01:30,800 --> 00:01:42,760
All right, hey everyone, I am here with Doug Eck at the O'Reilly AI conference.

22
00:01:42,760 --> 00:01:47,760
Doug is a research scientist on the Google Brain team, who's principally focused on the

23
00:01:47,760 --> 00:01:49,440
magenta project.

24
00:01:49,440 --> 00:01:50,760
Doug, welcome to the podcast.

25
00:01:50,760 --> 00:01:52,240
Hey Sam, thanks for having me.

26
00:01:52,240 --> 00:01:53,240
Excited to be here.

27
00:01:53,240 --> 00:01:58,200
Absolutely, so this podcast, while I focus on interviews now, I originally focused on

28
00:01:58,200 --> 00:02:04,000
covering news in the space and I covered the magenta project when it launched.

29
00:02:04,000 --> 00:02:11,520
And I remember this vividly because my daughter was in a summer program or leadership program

30
00:02:11,520 --> 00:02:16,440
or something like, I live in St. Louis and I had to drive her down state and I spent a

31
00:02:16,440 --> 00:02:20,880
bunch of time on the trip back listening to something about, I might have been listening

32
00:02:20,880 --> 00:02:26,960
to my notes or articles that I had doing like text-to-voice about the magenta project.

33
00:02:26,960 --> 00:02:31,760
And so I'm excited to finally get a chance to talk to you about it and learn more about

34
00:02:31,760 --> 00:02:32,760
it.

35
00:02:32,760 --> 00:02:35,240
And yeah, so welcome once again.

36
00:02:35,240 --> 00:02:40,680
I'm happy to be here a little over a year later from our initial launch and still

37
00:02:40,680 --> 00:02:41,680
add it.

38
00:02:41,680 --> 00:02:42,680
Absolutely, absolutely.

39
00:02:42,680 --> 00:02:48,760
And I think this is the first time from an interview perspective that we're really

40
00:02:48,760 --> 00:02:55,320
getting into the intersection of AI and art, which is an area that I've been wanting to

41
00:02:55,320 --> 00:02:57,800
talk a little bit more about as well.

42
00:02:57,800 --> 00:03:00,760
So very excited to have you on.

43
00:03:00,760 --> 00:03:07,360
Earlier today, you delivered a keynote at the conference and we'll jump into that.

44
00:03:07,360 --> 00:03:12,880
But first, why don't you walk us through your background and how you got into AI?

45
00:03:12,880 --> 00:03:14,640
So how did I get into AI?

46
00:03:14,640 --> 00:03:15,880
Good question.

47
00:03:15,880 --> 00:03:23,000
My undergraduate was in English literature, creative writing, and I finished that, and

48
00:03:23,000 --> 00:03:27,880
it turns out some of your listeners may not know this, but it's hard to get a job when

49
00:03:27,880 --> 00:03:28,880
you're undergraduate.

50
00:03:28,880 --> 00:03:29,880
It's in English literature.

51
00:03:29,880 --> 00:03:31,880
You actually have to work at something.

52
00:03:31,880 --> 00:03:35,120
So there's David Foster Wallace and then there's me.

53
00:03:35,120 --> 00:03:40,280
I became a database programmer, which actually I love coding and worked for a while as just

54
00:03:40,280 --> 00:03:41,280
a coder.

55
00:03:41,280 --> 00:03:45,360
Coding databases in Albuquerque, New Mexico and playing music, the pinnacle of my music

56
00:03:45,360 --> 00:03:50,400
career happened in Albuquerque as well, playing for dozens of fans in coffee houses all around

57
00:03:50,400 --> 00:03:51,400
the street.

58
00:03:51,400 --> 00:03:52,400
Is that nice?

59
00:03:52,400 --> 00:03:53,400
I don't know.

60
00:03:53,400 --> 00:03:55,720
I just was doing what I could.

61
00:03:55,720 --> 00:03:57,880
I was passionate about it.

62
00:03:57,880 --> 00:03:59,200
And I drifted back into grad school.

63
00:03:59,200 --> 00:04:02,480
I think I was just, you know, for the intellectual challenge of it and it made more sense

64
00:04:02,480 --> 00:04:03,640
to stay with computer science.

65
00:04:03,640 --> 00:04:08,040
So actually, I think it was one of the better decisions that I made in my life.

66
00:04:08,040 --> 00:04:11,880
I just not easily took the joint, you know, the overlap of two things I was passionate

67
00:04:11,880 --> 00:04:13,240
about, which is computing and music.

68
00:04:13,240 --> 00:04:18,200
And I just said, well, I like music and I like computers.

69
00:04:18,200 --> 00:04:20,280
So what can you do with computers and music?

70
00:04:20,280 --> 00:04:23,320
And you know, it's 24 and that was what I was thinking.

71
00:04:23,320 --> 00:04:24,880
And I said, hey, let's do AI.

72
00:04:24,880 --> 00:04:26,520
Let's do AI music.

73
00:04:26,520 --> 00:04:30,080
So I wanted to work with a guy named Doug Hofstetter at Indiana University.

74
00:04:30,080 --> 00:04:31,080
Okay.

75
00:04:31,080 --> 00:04:33,720
I ended up actually not doing my PhD under his direction, but took courses with him and

76
00:04:33,720 --> 00:04:35,880
ended up working with some other advisors there.

77
00:04:35,880 --> 00:04:42,200
Just kind of dove into a PhD in music and music cognition and computer science and kind

78
00:04:42,200 --> 00:04:47,160
of kept at it and eventually, you know, eventually ended up at Google, having been a faculty

79
00:04:47,160 --> 00:04:48,680
member for a while at University of Montreal.

80
00:04:48,680 --> 00:04:49,680
Okay.

81
00:04:49,680 --> 00:04:53,880
But really, it was kind of following a passion, just doing what I thought I was good

82
00:04:53,880 --> 00:04:54,880
at.

83
00:04:54,880 --> 00:04:55,880
Oh, that's awesome.

84
00:04:55,880 --> 00:04:56,880
That's awesome.

85
00:04:56,880 --> 00:05:03,000
So maybe tell us a little bit about your keynote today and along the way, weave in magenta

86
00:05:03,000 --> 00:05:06,080
or you can start with magenta and then go into the keynote, whatever kind of makes

87
00:05:06,080 --> 00:05:07,080
the most sense.

88
00:05:07,080 --> 00:05:08,080
Sure.

89
00:05:08,080 --> 00:05:13,800
So today's keynote had a particular focus that I think is important, which is that we

90
00:05:13,800 --> 00:05:19,280
can't do machine learning for music or art without the music in the art.

91
00:05:19,280 --> 00:05:23,080
So machine learning is about learning how to solve a problem.

92
00:05:23,080 --> 00:05:26,520
You know, you build an algorithm that itself can learn how to solve a problem.

93
00:05:26,520 --> 00:05:31,840
So I guess it stands to reason that if you want to build machine learning algorithms

94
00:05:31,840 --> 00:05:36,800
that can make music or make art or be tools for musicians and tools for artists, they have

95
00:05:36,800 --> 00:05:38,240
to see the right data.

96
00:05:38,240 --> 00:05:40,520
They have to see the world in the right way.

97
00:05:40,520 --> 00:05:42,600
And so I talked about two projects.

98
00:05:42,600 --> 00:05:45,080
One of them came out today on the magenta blog.

99
00:05:45,080 --> 00:05:47,680
Please check it out, g.co slash magenta.

100
00:05:47,680 --> 00:05:54,400
It's a recurrent neural network trained to make piano music perform a score that it writes.

101
00:05:54,400 --> 00:05:59,600
And crucially, it's trained on real piano performances, captured and midi.

102
00:05:59,600 --> 00:06:02,200
And there's captured and midi really isn't that important.

103
00:06:02,200 --> 00:06:06,160
We still know where, you know, all of the keys were pressed and how long they were pressed

104
00:06:06,160 --> 00:06:09,960
and how hard, you know, the velocity, et cetera.

105
00:06:09,960 --> 00:06:12,720
For your listeners who aren't familiar with midi, think of that as just kind of a way

106
00:06:12,720 --> 00:06:17,600
to store the events that happen when you play, you know, a synthesizer or electric keyboard

107
00:06:17,600 --> 00:06:20,040
or an appropriate piano.

108
00:06:20,040 --> 00:06:26,000
And it turns out that when you train on this data versus a bunch of musical scores that

109
00:06:26,000 --> 00:06:29,440
is with no performance, timing, you know, just the score.

110
00:06:29,440 --> 00:06:33,080
The resulting output of the models is dramatically different.

111
00:06:33,080 --> 00:06:36,760
And to my ear, at least, much more human sounding, almost delicate sometimes in terms of how

112
00:06:36,760 --> 00:06:39,600
the model figures out how to play the piano.

113
00:06:39,600 --> 00:06:41,200
So I thought that was a really nice story.

114
00:06:41,200 --> 00:06:47,000
And the scores that you played during the keynote were incredible.

115
00:06:47,000 --> 00:06:48,600
I mean, the after.

116
00:06:48,600 --> 00:06:53,280
Like there was the before and the after before is just the model train on the score.

117
00:06:53,280 --> 00:06:58,800
But the after was incorporating in, I guess, the more subtle effects, like, I don't know,

118
00:06:58,800 --> 00:07:01,000
attack and delay, I guess, of the ways I'm thinking of it.

119
00:07:01,000 --> 00:07:05,720
Like, just the force with which they pressed the nodes and that kind of thing and timing,

120
00:07:05,720 --> 00:07:07,080
subtle timing differences.

121
00:07:07,080 --> 00:07:12,520
And yeah, I never heard anything like that from a computer-generated program.

122
00:07:12,520 --> 00:07:13,560
Yeah, it's quite nice.

123
00:07:13,560 --> 00:07:18,600
I mean, I haven't done a thorough enough literature search to know if, you know, I haven't

124
00:07:18,600 --> 00:07:19,600
heard anything like it either.

125
00:07:19,600 --> 00:07:22,600
Hopefully people will come up and say, hey, we did this cool work before and we'll say,

126
00:07:22,600 --> 00:07:23,600
great, we'll credit it.

127
00:07:23,600 --> 00:07:24,600
We'll learn from it.

128
00:07:24,600 --> 00:07:25,600
You know, this is a research project.

129
00:07:25,600 --> 00:07:28,440
We're trying to always give credit where it's due, but I haven't heard anything quite

130
00:07:28,440 --> 00:07:29,440
like this.

131
00:07:29,440 --> 00:07:32,680
And I think you're right, you know, the way to think about it is, you know, Chopin wrote

132
00:07:32,680 --> 00:07:34,160
a piece of music, right?

133
00:07:34,160 --> 00:07:38,040
Yeah, we still love listening to different pianists interpret that music.

134
00:07:38,040 --> 00:07:42,080
And especially with that kind of music, the interpretation actually matters.

135
00:07:42,080 --> 00:07:47,200
You know, if you don't believe it, it's really entertaining to listen to a truly robotic

136
00:07:47,200 --> 00:07:50,360
performance of a piece of music that just doesn't work.

137
00:07:50,360 --> 00:07:54,840
And I think, you know, in modern pop and rock and jazz, it's even more so the case.

138
00:07:54,840 --> 00:07:59,800
Like think of Jimmy Hendrix playing the star-spangled banner.

139
00:07:59,800 --> 00:08:05,680
Like we're not listening to that because of the score, right, because this melody is

140
00:08:05,680 --> 00:08:06,680
so good, right?

141
00:08:06,680 --> 00:08:07,680
Right.

142
00:08:07,680 --> 00:08:11,080
It has cultural significance and it's this carrier for the sounds that Jimmy's making

143
00:08:11,080 --> 00:08:12,080
with his guitar.

144
00:08:12,080 --> 00:08:13,080
Yeah.

145
00:08:13,080 --> 00:08:16,840
So like this idea that the expressive timing matters is, you know, if you kind of unpeel

146
00:08:16,840 --> 00:08:18,800
the onion a little bit, it's pretty clear.

147
00:08:18,800 --> 00:08:22,280
And it's really fun to see what a model can do when it finally has that data.

148
00:08:22,280 --> 00:08:27,880
Well, you made a comment that I thought was really, really interesting and that was that,

149
00:08:27,880 --> 00:08:36,960
you know, what this work gets you closer to is looking at the keyboard or other instruments

150
00:08:36,960 --> 00:08:42,480
in other cases, but as, you know, sensors that are capturing sensory motor control from

151
00:08:42,480 --> 00:08:47,200
humans all the way up, you know, from fingers to muscles to, you know, neural transmission

152
00:08:47,200 --> 00:08:48,600
and that kind of stuff.

153
00:08:48,600 --> 00:08:50,440
Can you elaborate on that a little bit?

154
00:08:50,440 --> 00:08:51,440
Yeah.

155
00:08:51,440 --> 00:08:52,440
And did I capture that?

156
00:08:52,440 --> 00:08:53,440
You did.

157
00:08:53,440 --> 00:08:54,440
I think you did, yeah.

158
00:08:54,440 --> 00:08:58,000
So first in terms of just the model, yeah, what's being captured is how hard the finger

159
00:08:58,000 --> 00:09:03,600
hit the key and there are lots of just kind of the data picks up on, you know, human constraints

160
00:09:03,600 --> 00:09:05,800
that I think are important to music, right?

161
00:09:05,800 --> 00:09:11,120
And I've always been fascinated by the connections between dance and music and between motor control

162
00:09:11,120 --> 00:09:12,880
and general and music.

163
00:09:12,880 --> 00:09:15,960
There's lots, you know, piles of books written about why is music here?

164
00:09:15,960 --> 00:09:21,200
Is it maybe creating co-presence between people or it's about motor control synchronization

165
00:09:21,200 --> 00:09:26,120
and it's about just having fun, but in any case, it's clear that there's the motoric aspect

166
00:09:26,120 --> 00:09:27,120
is really important.

167
00:09:27,120 --> 00:09:29,160
The same thing is true for drawing, right?

168
00:09:29,160 --> 00:09:32,680
The constraints of the hand, what the hand can do, I think, is really important whether

169
00:09:32,680 --> 00:09:35,480
it's holding a paintbrush, you know, or a pencil.

170
00:09:35,480 --> 00:09:39,880
I guess arguably if you move to Photoshop here in a different world, I'm not sure that

171
00:09:39,880 --> 00:09:42,800
it matters how your hand holds the mouse, but maybe there, too.

172
00:09:42,800 --> 00:09:47,240
And it's really cool that we're able to train on the data that drives that.

173
00:09:47,240 --> 00:09:48,640
I could say more about this if you want.

174
00:09:48,640 --> 00:09:49,640
Sure.

175
00:09:49,640 --> 00:09:54,120
The other thing that I would add is a lot of people work in machine learning right now

176
00:09:54,120 --> 00:09:57,080
are working with images and with audio.

177
00:09:57,080 --> 00:09:58,080
I mean, we are, too.

178
00:09:58,080 --> 00:10:00,000
And I think it's really important.

179
00:10:00,000 --> 00:10:04,280
But I'm always concerned by, or not concerned by, but intrigued by the idea that when we

180
00:10:04,280 --> 00:10:09,960
as people, when we make new artifacts, whether it's like a new painting or a new piece of

181
00:10:09,960 --> 00:10:15,720
music, we tend to actually not do it pixel by pixel or, you know, we don't have the control,

182
00:10:15,720 --> 00:10:20,040
even our voices, we don't really have control over the waveform, right?

183
00:10:20,040 --> 00:10:23,880
We have a buzzer in our throat called, you know, called vocal cords and then we're shaping

184
00:10:23,880 --> 00:10:24,880
our vocal tract, right?

185
00:10:24,880 --> 00:10:30,560
It's very, very in machine learning terms, it's, it's a pretty low dimensional control

186
00:10:30,560 --> 00:10:31,760
surface, right?

187
00:10:31,760 --> 00:10:34,200
So we're not dealing with like millions of parameters.

188
00:10:34,200 --> 00:10:37,480
We're able to like move some muscles around and make something vibrate.

189
00:10:37,480 --> 00:10:41,720
With playing a piano, we build this thing out of wood and metal and then fundamentally

190
00:10:41,720 --> 00:10:43,760
we bang it with our fingers, right?

191
00:10:43,760 --> 00:10:47,920
And so, you know, I think, I think trying to do machine learning for art and music, trying

192
00:10:47,920 --> 00:10:51,920
to move into these spaces that are really low dimensional and by that, I mean, you've

193
00:10:51,920 --> 00:10:55,920
only got 88 keys and that may seem like a lot until you think of the number of pixels

194
00:10:55,920 --> 00:11:02,320
there are in an image or the number of moving numbers there are in a one second of CD quality

195
00:11:02,320 --> 00:11:03,320
audio.

196
00:11:03,320 --> 00:11:05,560
It's a relatively low number of parameters to work with.

197
00:11:05,560 --> 00:11:10,480
And I think that that ties to these really beautiful ideas about, you know, what is meaning

198
00:11:10,480 --> 00:11:15,240
but at some level, compression, like just pulling the important bits out of a hard problem

199
00:11:15,240 --> 00:11:16,720
and it's easier to get there.

200
00:11:16,720 --> 00:11:18,240
I'm getting too philosophical, but...

201
00:11:18,240 --> 00:11:22,760
No, I mean, it's an interesting conversation and it reminds me a little bit of not much

202
00:11:22,760 --> 00:11:27,480
of an audio file admittedly or not at all an audio file really admittedly.

203
00:11:27,480 --> 00:11:28,480
But...

204
00:11:28,480 --> 00:11:30,480
Big headphones, I mean, come on.

205
00:11:30,480 --> 00:11:35,640
But, you know, you get people argue over, you know, CDs versus vinyl records and like

206
00:11:35,640 --> 00:11:40,320
the, you know, the get into these debates about the richness of the vinyl records or transistors

207
00:11:40,320 --> 00:11:45,600
versus tubes and that kind of thing and it does, you know, when you think about it in

208
00:11:45,600 --> 00:11:51,200
this context, it does kind of think of, you know, the machine learning, the input that

209
00:11:51,200 --> 00:11:54,920
we're giving to machine learning models in a lot of ways is kind of reductionists and

210
00:11:54,920 --> 00:12:01,040
do we, you know, how do we make sure we don't reduce out the essence of the thing, right?

211
00:12:01,040 --> 00:12:03,040
Does that make any sense?

212
00:12:03,040 --> 00:12:04,040
Yeah, it does.

213
00:12:04,040 --> 00:12:09,600
I mean, so first, I think that we are always at risk of doing that and what makes the

214
00:12:09,600 --> 00:12:14,680
problem retain, you know, retain for me its beauty is that, you know, we're always there.

215
00:12:14,680 --> 00:12:18,360
Like, I'm not interested in a machine learning algorithm where I can just push a button and

216
00:12:18,360 --> 00:12:19,880
have it do its work.

217
00:12:19,880 --> 00:12:23,440
I mean, I think it's cool, like the samples we posted today were basically pushing a

218
00:12:23,440 --> 00:12:25,920
button, but that's more or less to understand how the model works.

219
00:12:25,920 --> 00:12:30,840
I think these get interesting when you close the loop and you have musicians able to work

220
00:12:30,840 --> 00:12:36,800
with them and use them as ways to kind of expand possibilities or create some, some

221
00:12:36,800 --> 00:12:39,600
line and then you add another line, et cetera, like that.

222
00:12:39,600 --> 00:12:44,080
I think that becomes, becomes less reductionist, but yeah, look, let's be real.

223
00:12:44,080 --> 00:12:45,080
What are these bottles doing?

224
00:12:45,080 --> 00:12:46,600
They're basically warping data.

225
00:12:46,600 --> 00:12:49,960
They're doing some, they're taking some numbers in and they're transforming them and

226
00:12:49,960 --> 00:12:51,840
they're pushing some numbers out.

227
00:12:51,840 --> 00:12:55,240
Hopefully you've done the math right and you can, you can roll the dice and sample from

228
00:12:55,240 --> 00:12:57,880
these and get lots of different really interesting instances.

229
00:12:57,880 --> 00:13:02,120
But yeah, I mean, reductionist is a pretty fair term.

230
00:13:02,120 --> 00:13:08,680
Were we close to being able to do like a music style transfer, like, you know, I've got

231
00:13:08,680 --> 00:13:13,760
this rough idea of, you know, score or melody or, yeah, I don't know what the input would

232
00:13:13,760 --> 00:13:18,120
be, but, you know, show pan eyes this for me.

233
00:13:18,120 --> 00:13:23,560
The demo that you played was kind of a vocative of that kind of idea for me.

234
00:13:23,560 --> 00:13:29,240
If we rely on the representation of MIDI, where what we're manipulating are the notes

235
00:13:29,240 --> 00:13:32,160
and when they happen and how many of them there are and we're adding and subtracting notes

236
00:13:32,160 --> 00:13:37,920
and we're performing them, yeah, I think we can, we can imagine with some work being able

237
00:13:37,920 --> 00:13:40,160
to do a style transfer over something.

238
00:13:40,160 --> 00:13:46,960
However, can we take an Adele tune and make it sound like it was done by, I mentioned

239
00:13:46,960 --> 00:13:47,960
Jimmy Hendrix, right?

240
00:13:47,960 --> 00:13:48,960
Right.

241
00:13:48,960 --> 00:13:49,960
It's a very different kind of style transfer.

242
00:13:49,960 --> 00:13:53,720
Or can we, can we take it and Adele tune and make it sound like bebop jazz from the

243
00:13:53,720 --> 00:13:54,720
audio?

244
00:13:54,720 --> 00:13:55,720
Right.

245
00:13:55,720 --> 00:13:56,720
We're very, very far from that.

246
00:13:56,720 --> 00:14:00,720
I think, to be honest, I think that's more getting at the flavor of what style transfer

247
00:14:00,720 --> 00:14:02,600
for images does.

248
00:14:02,600 --> 00:14:07,120
And the reason I bring it up is that there's a nice trick in style transfer for images.

249
00:14:07,120 --> 00:14:11,520
You know, for example, taking a painting and making it look more like Picasso, which is

250
00:14:11,520 --> 00:14:16,360
that it's, it's looking at little patches in the image and the patches are varying sizes,

251
00:14:16,360 --> 00:14:17,360
but it's always local.

252
00:14:17,360 --> 00:14:18,360
Right.

253
00:14:18,360 --> 00:14:22,760
Literally, you can imagine just like moving a little spotlight over, over the image.

254
00:14:22,760 --> 00:14:25,640
And then doing transforms at some layer of granularity.

255
00:14:25,640 --> 00:14:29,480
So you can like get thicker brush strokes versus thinner ones, right?

256
00:14:29,480 --> 00:14:33,440
But in music, you know, music unfolds in time.

257
00:14:33,440 --> 00:14:38,560
And it's not always the case that it's the nearest sounds that are the most important.

258
00:14:38,560 --> 00:14:41,440
There's not this locality, this spatial locality in music.

259
00:14:41,440 --> 00:14:45,480
And I think, I think that would make like a style transfer so that something sounded more

260
00:14:45,480 --> 00:14:49,240
like, you know, an electric guitar quite, quite a bit harder.

261
00:14:49,240 --> 00:14:53,200
I don't, certainly the tools that are used for image style transfer don't quite make sense.

262
00:14:53,200 --> 00:14:57,440
Which is why, by the way, we don't see lots of groups have tried this.

263
00:14:57,440 --> 00:15:00,400
We don't see compelling audio based style transfers.

264
00:15:00,400 --> 00:15:01,400
Okay.

265
00:15:01,400 --> 00:15:02,400
We're getting there.

266
00:15:02,400 --> 00:15:03,400
We'll get there.

267
00:15:03,400 --> 00:15:04,400
I mean, someone will figure it out.

268
00:15:04,400 --> 00:15:06,400
But it's just, it doesn't sort of fall out for free from the, from the image side.

269
00:15:06,400 --> 00:15:07,400
Right.

270
00:15:07,400 --> 00:15:08,400
Right.

271
00:15:08,400 --> 00:15:13,240
So why don't you walk us through the, kind of the technical underpinnings, the architecture

272
00:15:13,240 --> 00:15:17,240
of the, the project that we just talked about.

273
00:15:17,240 --> 00:15:21,880
The project performance RNN, we give things names just to have something to talk about.

274
00:15:21,880 --> 00:15:26,840
It's not an important name, but performance RNN is a recurrent neural network called

275
00:15:26,840 --> 00:15:34,360
LSTM, long short-term memory, that is listening to, to lots and lots of performances, in this

276
00:15:34,360 --> 00:15:36,000
case, panel performances.

277
00:15:36,000 --> 00:15:41,400
What it's seeing is actually a very simple encoding of that performance, a note turned

278
00:15:41,400 --> 00:15:48,920
on that had this velocity, a note turned off, let's advance time, let's advance the clock.

279
00:15:48,920 --> 00:15:53,560
So it's almost like, imagine you've got a paper punch in your cutting holes, you punch,

280
00:15:53,560 --> 00:15:56,600
punch, punch, punch, and then you move forward and you punch, punch, punches, a very, very,

281
00:15:56,600 --> 00:16:00,000
very simple way to reduce to represent the data, but it's not losing any of the data.

282
00:16:00,000 --> 00:16:02,920
You can reconstruct the entire performance from there, right?

283
00:16:02,920 --> 00:16:06,080
The recurrent neural network is, is trying to solve an interesting problem.

284
00:16:06,080 --> 00:16:08,200
It's listening, so to speak.

285
00:16:08,200 --> 00:16:10,080
It's processing this information.

286
00:16:10,080 --> 00:16:12,800
It's trying to predict what's coming next.

287
00:16:12,800 --> 00:16:18,760
And in our case, it can predict, hey, generate another note, turn off a note, or move the

288
00:16:18,760 --> 00:16:20,200
clock.

289
00:16:20,200 --> 00:16:24,120
And every time it gets it right, every time it predicts correctly what it's being trained

290
00:16:24,120 --> 00:16:27,000
on, you know, it gets a good job.

291
00:16:27,000 --> 00:16:32,200
And every time it gets it wrong, then we adjust the weights of the network.

292
00:16:32,200 --> 00:16:36,720
So these are weighted connections between computational units or nodes in the network, so that

293
00:16:36,720 --> 00:16:38,920
it does better next time.

294
00:16:38,920 --> 00:16:44,640
And so it's really playing kind of a funny telephone game, you know, where you, maybe that's not

295
00:16:44,640 --> 00:16:45,640
the right word for it.

296
00:16:45,640 --> 00:16:49,080
I had an analogy for this, but like, it's kind of a weird thought.

297
00:16:49,080 --> 00:16:50,080
You know, you're listening.

298
00:16:50,080 --> 00:16:54,200
You've heard six notes of a melody, and you're trying to guess what the seventh note is.

299
00:16:54,200 --> 00:16:56,360
Even people aren't going to get it right all the time, because there's lots of possible

300
00:16:56,360 --> 00:16:58,240
ways, which things can go.

301
00:16:58,240 --> 00:17:01,880
So in the end, these models, they don't memorize specific tunes.

302
00:17:01,880 --> 00:17:07,000
Instead, they kind of figure out patterns of what should come next given these previous

303
00:17:07,000 --> 00:17:08,000
notes.

304
00:17:08,000 --> 00:17:09,480
And in fact, they learn about chords.

305
00:17:09,480 --> 00:17:10,480
They learn about arpeggiation.

306
00:17:10,480 --> 00:17:13,920
They learn about scales, because those happen in the data.

307
00:17:13,920 --> 00:17:14,920
Okay.

308
00:17:14,920 --> 00:17:19,320
And when we want to make a piece of music, we do take a clever, very simple trick that's

309
00:17:19,320 --> 00:17:24,160
been around for 20 years, you start with a note or two, and then you predict what should

310
00:17:24,160 --> 00:17:25,160
come next.

311
00:17:25,160 --> 00:17:29,040
And that's like a, not an exact answer.

312
00:17:29,040 --> 00:17:32,080
The model says these are the possible things that can come next.

313
00:17:32,080 --> 00:17:34,240
You choose one based upon those probabilities.

314
00:17:34,240 --> 00:17:37,520
And then you feed it in, like you feed, it's, you feed the networks output back in as

315
00:17:37,520 --> 00:17:39,920
input, and you kind of keep going called auto regression.

316
00:17:39,920 --> 00:17:42,240
And in that way, you end up like composing a new score.

317
00:17:42,240 --> 00:17:43,240
Okay.

318
00:17:43,240 --> 00:17:48,520
How many order of magnitude musical samples are you training this on?

319
00:17:48,520 --> 00:17:53,240
And are you training them on full scores or snippets or does that matter?

320
00:17:53,240 --> 00:17:54,240
It does matter.

321
00:17:54,240 --> 00:17:58,320
And the end, for technical reasons, you end up kind of chunking things, but we have ways

322
00:17:58,320 --> 00:17:59,320
to do that automatically.

323
00:17:59,320 --> 00:18:04,680
So in terms of when the model receives the input, it processes an entire score.

324
00:18:04,680 --> 00:18:09,360
I think it's on the order of 20 or 30,000 pieces of music in this case, it's not huge.

325
00:18:09,360 --> 00:18:13,680
They're all performances, and they're all from solo piano.

326
00:18:13,680 --> 00:18:14,680
Okay.

327
00:18:14,680 --> 00:18:20,120
So, and, and did, did you commission them or co, no, these are, so these are all pieces that

328
00:18:20,120 --> 00:18:24,800
were, you know, like Chopin, you know, it's all public old, you know, old classical music

329
00:18:24,800 --> 00:18:28,680
and their performances that come from a number of sources, all kind of freely out there

330
00:18:28,680 --> 00:18:29,680
on the web.

331
00:18:29,680 --> 00:18:33,280
And you can, you can track those down if you follow the magenta blog where we have it.

332
00:18:33,280 --> 00:18:34,280
Okay.

333
00:18:34,280 --> 00:18:36,120
It's probably not interesting to go and aware the data source has actually come from,

334
00:18:36,120 --> 00:18:39,600
but they're, but behind the dollar, are people having performed these for sometimes

335
00:18:39,600 --> 00:18:40,600
for competitions?

336
00:18:40,600 --> 00:18:41,600
Okay.

337
00:18:41,600 --> 00:18:45,480
One of the sources is that you can use is, is Yamaha has a particular kind of piano that's

338
00:18:45,480 --> 00:18:49,440
like a player piano called the disclivier, and they've had competitions where very, you

339
00:18:49,440 --> 00:18:51,560
know, top to your pianist come and play.

340
00:18:51,560 --> 00:18:54,680
And at the same time, the piano records all of the movements of the hammers, so you have

341
00:18:54,680 --> 00:18:56,080
the trace left behind of what they played.

342
00:18:56,080 --> 00:18:57,080
Oh well.

343
00:18:57,080 --> 00:18:58,720
So we're training, we can train that data like that.

344
00:18:58,720 --> 00:18:59,720
Okay.

345
00:18:59,720 --> 00:19:00,720
Oh, really interesting.

346
00:19:00,720 --> 00:19:07,680
So you've got this, you know, 20, 30,000, and were they all specifically Chopin or

347
00:19:07,680 --> 00:19:12,040
no matter how much variation, large variation across the classical tradition.

348
00:19:12,040 --> 00:19:13,040
Okay.

349
00:19:13,040 --> 00:19:14,880
And we have other, we can collect other data sets.

350
00:19:14,880 --> 00:19:19,080
In fact, you know, it's perfectly reasonable for someone to actually just collect a few

351
00:19:19,080 --> 00:19:22,280
hours of playing of their own, and that's going to be enough to train a model.

352
00:19:22,280 --> 00:19:26,640
Not one hour, but if you, if like one of your listeners is say like a jazz pianist and

353
00:19:26,640 --> 00:19:30,640
does jazz improv, you know, you could, you could train a model on just a few hours of

354
00:19:30,640 --> 00:19:35,360
improv, and then they'd have their own, like the model would encode their own playing style,

355
00:19:35,360 --> 00:19:36,360
which is kind of cool.

356
00:19:36,360 --> 00:19:37,360
Right.

357
00:19:37,360 --> 00:19:38,360
Right.

358
00:19:38,360 --> 00:19:40,440
And then when you sample from the model, you can kind of hear some, some of your own aspects

359
00:19:40,440 --> 00:19:41,440
of playing.

360
00:19:41,440 --> 00:19:42,440
Wow.

361
00:19:42,440 --> 00:19:49,160
So what degree was the network architecture for this particular neural network?

362
00:19:49,160 --> 00:19:54,680
How is it unique from other network architectures that are used for, you know, other LSTM based network

363
00:19:54,680 --> 00:19:59,200
architectures that are used for like, you know, the kind of thing you do to, you know, we've

364
00:19:59,200 --> 00:20:01,920
seen the projects where we're getting scripts auto generated.

365
00:20:01,920 --> 00:20:02,920
Right.

366
00:20:02,920 --> 00:20:03,920
That's good.

367
00:20:03,920 --> 00:20:04,920
Yeah.

368
00:20:04,920 --> 00:20:05,920
Yeah.

369
00:20:05,920 --> 00:20:06,920
Yeah.

370
00:20:06,920 --> 00:20:08,320
The network is actually pretty much a generic LSTM.

371
00:20:08,320 --> 00:20:10,960
What's somewhat novel is the representation of the data.

372
00:20:10,960 --> 00:20:13,120
So what the model is trying to predict.

373
00:20:13,120 --> 00:20:19,240
There have been other people who have predicted like the duration of, of a note in terms of

374
00:20:19,240 --> 00:20:23,440
its relative duration in the score, like, I'm going to generate a middle C, and it's

375
00:20:23,440 --> 00:20:25,720
going to be a 16th note.

376
00:20:25,720 --> 00:20:29,160
And we've simply extended that to say, well, forget about whether it's a 16th note.

377
00:20:29,160 --> 00:20:33,520
Instead, let's just move the clock forward some number of milliseconds.

378
00:20:33,520 --> 00:20:36,000
And that way, we'll just learn about real time.

379
00:20:36,000 --> 00:20:37,000
Right.

380
00:20:37,000 --> 00:20:41,600
So if that's not clear, that means we don't need a metronome to figure out what a quarter

381
00:20:41,600 --> 00:20:42,600
note is.

382
00:20:42,600 --> 00:20:43,600
Right.

383
00:20:43,600 --> 00:20:46,760
If that's not clear, like, what a quarter note is is that it's a quarter of a measure.

384
00:20:46,760 --> 00:20:48,960
And what a measure is is defined by the beat.

385
00:20:48,960 --> 00:20:52,560
And if the beat's fast, quarter note takes less time to be played, right?

386
00:20:52,560 --> 00:20:57,120
Here we're just saying, we're just going to look at the time of the pieces at unfolds

387
00:20:57,120 --> 00:20:58,480
in the performance.

388
00:20:58,480 --> 00:21:00,480
And so the model doesn't know what a quarter note is at all.

389
00:21:00,480 --> 00:21:02,840
It just generates notes and moves the clock.

390
00:21:02,840 --> 00:21:04,440
And that, I think that's unique.

391
00:21:04,440 --> 00:21:08,240
Again, we just put this blog posting out and we're in the middle writing a paper around

392
00:21:08,240 --> 00:21:09,240
it.

393
00:21:09,240 --> 00:21:11,520
And it's pretty hard and research to do something that's completely new.

394
00:21:11,520 --> 00:21:13,760
You always find that someone did something similarly, right?

395
00:21:13,760 --> 00:21:17,560
I mean, there's almost nothing new under the stars and sun.

396
00:21:17,560 --> 00:21:18,560
Nice.

397
00:21:18,560 --> 00:21:24,000
Well, we'll definitely include the audio that you shared in your keynote in the show notes

398
00:21:24,000 --> 00:21:25,800
for folks to check out.

399
00:21:25,800 --> 00:21:28,040
It was really, really compelling.

400
00:21:28,040 --> 00:21:32,760
We also talked about a project based on the Google Draw experiment.

401
00:21:32,760 --> 00:21:33,760
Yeah.

402
00:21:33,760 --> 00:21:35,280
Tell us a little bit about that one.

403
00:21:35,280 --> 00:21:37,480
That was an AI experiment called Quick Draw.

404
00:21:37,480 --> 00:21:40,640
It was done by the Creative Lab folks in New York at Google.

405
00:21:40,640 --> 00:21:45,680
And what they did was they, we already had some image classifiers that we can use that

406
00:21:45,680 --> 00:21:49,960
can identify like tens of thousands of different kinds of images.

407
00:21:49,960 --> 00:21:54,280
You see, you point your mobile device and it recognizes through the camera that that's

408
00:21:54,280 --> 00:21:57,880
a lamp or that's a dog or that's a cell phone.

409
00:21:57,880 --> 00:22:01,920
And so someone had a clever idea of saying, well, what if could it identify a sketch of

410
00:22:01,920 --> 00:22:04,840
a dog or a sketch of a cell phone, it turns out, yeah, I can't.

411
00:22:04,840 --> 00:22:08,960
So you get to play Pictionary against an image classifier.

412
00:22:08,960 --> 00:22:11,320
Actually, you're playing Pictionary with an image classifier.

413
00:22:11,320 --> 00:22:12,960
Remember Pictionary is collaborative, right?

414
00:22:12,960 --> 00:22:18,200
And so you're given a prompt like, hey, you have 20 seconds to draw a dog and you try

415
00:22:18,200 --> 00:22:19,200
to draw a dog.

416
00:22:19,200 --> 00:22:23,560
And if the image classifier can guess it, then you get a point, right?

417
00:22:23,560 --> 00:22:28,320
So at some point, we decided we wanted to use this data for machine learning.

418
00:22:28,320 --> 00:22:30,280
So we changed the messaging on the site.

419
00:22:30,280 --> 00:22:32,520
So it said, hey, we're going to, you know, if you want to play the game, we're just going

420
00:22:32,520 --> 00:22:35,640
to keep anonymized drawings around, we're going to learn from them, right?

421
00:22:35,640 --> 00:22:36,640
We're going to train.

422
00:22:36,640 --> 00:22:37,640
So so we kept that data.

423
00:22:37,640 --> 00:22:41,360
We gave it back to the community to use for other, for artistic purposes and people have

424
00:22:41,360 --> 00:22:43,640
done tons of crazy things with this data.

425
00:22:43,640 --> 00:22:45,120
What are some of those things?

426
00:22:45,120 --> 00:22:47,000
So this, by the way, this is an art machine learning.

427
00:22:47,000 --> 00:22:52,000
This is just the people's drawings, analyzing how different people in different cultures

428
00:22:52,000 --> 00:22:56,160
draw circles, turns out like some cultures draw circles clockwise and other cultures

429
00:22:56,160 --> 00:22:59,640
draw them counterclockwise and you kind of clustered things.

430
00:22:59,640 --> 00:23:05,680
The way that people draw chairs, Asian culture is apparently draw chairs often, I'm going

431
00:23:05,680 --> 00:23:12,040
to get these backwards because yeah, they usually draw chairs in, no, it's in three, whether

432
00:23:12,040 --> 00:23:16,120
you do it in 3D or 2D, whether a chair is just like an H almost, right?

433
00:23:16,120 --> 00:23:17,360
Which is what I would draw.

434
00:23:17,360 --> 00:23:18,800
And I think that's actually an Asian pattern.

435
00:23:18,800 --> 00:23:19,800
Okay.

436
00:23:19,800 --> 00:23:25,760
Yeah, someone else outside of Google took the data, the circle stuff is like a blog posting

437
00:23:25,760 --> 00:23:28,520
from a couple of weeks ago, encourage your listeners to find it.

438
00:23:28,520 --> 00:23:32,480
Just a Google like quick draw circles and it has you draw a circle and tells you some

439
00:23:32,480 --> 00:23:36,400
things about the circle you drew and analyzes the circles from around the world.

440
00:23:36,400 --> 00:23:40,680
And so I don't know, you just kind of get this kind of fun, I mean, it's not world changing,

441
00:23:40,680 --> 00:23:41,680
but it's interesting, right?

442
00:23:41,680 --> 00:23:43,360
It's a kind of sociology behind it.

443
00:23:43,360 --> 00:23:44,680
Anyway, I digress.

444
00:23:44,680 --> 00:23:50,400
We had this data and David Ha, who is the primary person on this paper, decided to train

445
00:23:50,400 --> 00:23:53,840
a recurrent neural network to try to reproduce the strokes.

446
00:23:53,840 --> 00:23:55,680
And we have the strokes as they appeared in order.

447
00:23:55,680 --> 00:23:59,640
So, you know, if you were trying to draw a garden, if you drew a flower first and then you drew

448
00:23:59,640 --> 00:24:03,040
the grass, then it would be reproduced in that order.

449
00:24:03,040 --> 00:24:04,720
This was a slightly different model.

450
00:24:04,720 --> 00:24:08,880
This was two different recurrent neural networks and an intermediate representation.

451
00:24:08,880 --> 00:24:11,240
But maybe that's too far a field too.

452
00:24:11,240 --> 00:24:20,000
The upshot is you can now generate new instances of like dogs or cats or, you know, several

453
00:24:20,000 --> 00:24:25,000
hundred classes and kind of try to get a better understanding of what people are doing

454
00:24:25,000 --> 00:24:28,080
when they draw them and also have a way to explore the space.

455
00:24:28,080 --> 00:24:29,080
Hmm.

456
00:24:29,080 --> 00:24:31,080
So what exactly does that mean?

457
00:24:31,080 --> 00:24:37,240
You've got a kind of encoder, decoder encoder set up where the decoder is learning, how

458
00:24:37,240 --> 00:24:41,600
people are drawing these things and you've got an encoder that is trying to, you know,

459
00:24:41,600 --> 00:24:46,120
give it a thing that it's trying to create that thing or...

460
00:24:46,120 --> 00:24:49,240
I think you reverse it to reverse it to reverse it to reverse it.

461
00:24:49,240 --> 00:24:55,440
Yeah, so the one way to think about it is that the work that I described with the performance

462
00:24:55,440 --> 00:24:58,440
RNN, you could think of as a decoder.

463
00:24:58,440 --> 00:25:01,440
It's not encoding into some different representation.

464
00:25:01,440 --> 00:25:05,240
It's basically just taking the score and trying to predict the next note.

465
00:25:05,240 --> 00:25:07,400
This model does something slightly different.

466
00:25:07,400 --> 00:25:13,520
It takes the strokes and tries to encode them into a vector, into a sequence of numbers.

467
00:25:13,520 --> 00:25:20,680
And then only from that sequence of numbers does it try to reproduce or decode the drawing.

468
00:25:20,680 --> 00:25:22,920
So it's getting these strokes in.

469
00:25:22,920 --> 00:25:28,040
It's pushing some information via recurrent neural network into this kind of intermediate

470
00:25:28,040 --> 00:25:32,160
representation called our latent space if you want the technical term.

471
00:25:32,160 --> 00:25:36,200
And then it's trying to then recreate the drawing by decoding it using another recurrent

472
00:25:36,200 --> 00:25:37,680
neural network.

473
00:25:37,680 --> 00:25:43,000
And the crucial, the reason that we use this intermediate step of this latent space is

474
00:25:43,000 --> 00:25:48,080
that if we've constructed it correctly, we can do a really nice job of generating new

475
00:25:48,080 --> 00:25:51,280
samples with lots of variants for the same problem.

476
00:25:51,280 --> 00:25:52,280
I don't know.

477
00:25:52,280 --> 00:25:53,280
A library on that.

478
00:25:53,280 --> 00:25:54,280
Yeah.

479
00:25:54,280 --> 00:25:55,880
No idea how technical they get.

480
00:25:55,880 --> 00:26:02,040
So for example, we could take the drawing of a face that has like almost like a triangle

481
00:26:02,040 --> 00:26:07,520
inverted triangle or like a pointy chin, right, and we could run that through the encoder.

482
00:26:07,520 --> 00:26:09,440
And that would give us some vector.

483
00:26:09,440 --> 00:26:15,080
And then we might take another face that's a big round face, we know with a big round

484
00:26:15,080 --> 00:26:17,880
nose and run that through the encoder.

485
00:26:17,880 --> 00:26:21,360
And that would give us another vector in this latent space, right.

486
00:26:21,360 --> 00:26:24,960
But now we have these two vectors that are not very big, maybe a few dozen numbers or

487
00:26:24,960 --> 00:26:26,600
maybe a hundred numbers.

488
00:26:26,600 --> 00:26:29,240
And we could just take the average of those two.

489
00:26:29,240 --> 00:26:34,200
And that should, if we've trained our model right, give us the face that is somewhere

490
00:26:34,200 --> 00:26:39,360
towards, yeah, kind of between this triangular face and between this circular face, right.

491
00:26:39,360 --> 00:26:43,760
And if you do that enough, you get to like the meaning of a face, right?

492
00:26:43,760 --> 00:26:44,760
Is that the idea?

493
00:26:44,760 --> 00:26:45,760
Yeah.

494
00:26:45,760 --> 00:26:49,880
I mean, you capture, yeah, I mean, in a hand-woven philosophical way, you get the platonic

495
00:26:49,880 --> 00:26:50,880
face.

496
00:26:50,880 --> 00:26:51,880
Right.

497
00:26:51,880 --> 00:26:55,320
In a less ridiculous way, at the very least you get like, what's happening is that latent

498
00:26:55,320 --> 00:26:58,920
space is forced, is encoding those aspects of the face that are the most important to

499
00:26:58,920 --> 00:27:01,960
remember if you want to be able to cover the variance of faces.

500
00:27:01,960 --> 00:27:06,640
And so, yeah, it's captured something really important, some of the really important aspects

501
00:27:06,640 --> 00:27:07,640
of faces.

502
00:27:07,640 --> 00:27:11,920
So in that latent space, as you move around that space and maybe you generate some random

503
00:27:11,920 --> 00:27:16,360
vector in that space, you should still be able to decode some kind of face, right.

504
00:27:16,360 --> 00:27:17,840
Let's call it a variational model.

505
00:27:17,840 --> 00:27:18,840
Okay.

506
00:27:18,840 --> 00:27:22,600
So I don't know if that captures the gist of the tech.

507
00:27:22,600 --> 00:27:26,600
I think that one of the take home messages is the goal, the reasons why we're caring

508
00:27:26,600 --> 00:27:31,880
about having this embedding space, this latent space is that we can use it as a way to

509
00:27:31,880 --> 00:27:35,920
give artists more control over a model like that.

510
00:27:35,920 --> 00:27:41,880
So for example, you could take, you could draw a face and encode it through the encoder.

511
00:27:41,880 --> 00:27:45,120
And then now you have this vector and you could perturb that vector, you could move that

512
00:27:45,120 --> 00:27:50,320
vector around, you could use this as a starting point for some, you know, some other work

513
00:27:50,320 --> 00:27:51,640
with the model.

514
00:27:51,640 --> 00:27:54,280
You could just email that vector to someone else and they could reproduce it, they could

515
00:27:54,280 --> 00:27:55,600
know what you think about.

516
00:27:55,600 --> 00:28:01,040
Now granted, this is all in the context of drawings that took 20 seconds to make using

517
00:28:01,040 --> 00:28:02,320
a mouse on a computer.

518
00:28:02,320 --> 00:28:07,960
So our expectations are not that, you know, people will go, oh, I can make, you know, the

519
00:28:07,960 --> 00:28:10,480
next shagall painting or whatever with this.

520
00:28:10,480 --> 00:28:16,880
But that in principle, you have this space where, you know, we can encode some drawing

521
00:28:16,880 --> 00:28:21,400
into some space that is kind of really good at storing drawings.

522
00:28:21,400 --> 00:28:24,600
And then from there, we can carry that around and we can decode not just that original

523
00:28:24,600 --> 00:28:28,800
drawing, but tons of drawings like it, and then maybe, you know, be able to move forward

524
00:28:28,800 --> 00:28:33,040
with, you know, possibilities for moving around that space for reasons like for animation

525
00:28:33,040 --> 00:28:34,040
and things like that.

526
00:28:34,040 --> 00:28:35,040
Okay.

527
00:28:35,040 --> 00:28:36,040
It's interesting.

528
00:28:36,040 --> 00:28:40,400
One of the other, one of the interviews I did yesterday that'll be coming out at the

529
00:28:40,400 --> 00:28:46,120
same time in this O'Reilly series is, we're talking about word to veck.

530
00:28:46,120 --> 00:28:51,680
And it's interesting to kind of think about all of the various, we talked about several

531
00:28:51,680 --> 00:28:58,040
applications of embeddings in the context of, you know, word to veck and related things.

532
00:28:58,040 --> 00:29:00,520
But, you know, this is definitely drawing to veck.

533
00:29:00,520 --> 00:29:01,520
Right.

534
00:29:01,520 --> 00:29:02,520
Yeah.

535
00:29:02,520 --> 00:29:10,480
So you encode your samples into this vector or vector space and then you use that to,

536
00:29:10,480 --> 00:29:15,280
you can use that as, you know, almost like a filter for creating things through the decoding

537
00:29:15,280 --> 00:29:16,280
process.

538
00:29:16,280 --> 00:29:21,720
I think one of the examples you showed was, and contextualize this for us, but you had

539
00:29:21,720 --> 00:29:24,440
folks draw like an eight-legged pig.

540
00:29:24,440 --> 00:29:29,200
You encoded that and then decoded it using the kind of the pig vector.

541
00:29:29,200 --> 00:29:30,200
Yeah.

542
00:29:30,200 --> 00:29:31,200
Is that the way to think about that?

543
00:29:31,200 --> 00:29:32,200
Yeah.

544
00:29:32,200 --> 00:29:34,040
So you're trading this autoencoder model only on pigs.

545
00:29:34,040 --> 00:29:35,040
Yep.

546
00:29:35,040 --> 00:29:36,040
So it knows nothing but pigs.

547
00:29:36,040 --> 00:29:37,040
Yeah.

548
00:29:37,040 --> 00:29:38,040
Oh, it's pigs all the way down.

549
00:29:38,040 --> 00:29:39,040
Right.

550
00:29:39,040 --> 00:29:41,880
And, you know, we make these latent spaces pretty small and inject some noise.

551
00:29:41,880 --> 00:29:43,160
We don't want them to overfit.

552
00:29:43,160 --> 00:29:44,920
It's boring if they just memorize the data.

553
00:29:44,920 --> 00:29:45,920
Mm-hmm.

554
00:29:45,920 --> 00:29:46,920
Right.

555
00:29:46,920 --> 00:29:50,920
So this little vector can barely, barely manage to generate a pig, but it does pretty

556
00:29:50,920 --> 00:29:52,280
good job of generating pigs.

557
00:29:52,280 --> 00:29:53,280
Mm-hmm.

558
00:29:53,280 --> 00:29:54,280
Yeah, and you give it a pig with eight legs.

559
00:29:54,280 --> 00:29:56,760
Well, it never sees pigs with eight legs.

560
00:29:56,760 --> 00:30:00,800
So it encodes through the encoder, the recurrent norm that work.

561
00:30:00,800 --> 00:30:05,120
It encodes some numbers that were driven by those, those strokes, including the eight

562
00:30:05,120 --> 00:30:06,120
legs.

563
00:30:06,120 --> 00:30:07,120
Mm-hmm.

564
00:30:07,120 --> 00:30:12,120
But when it decodes it, what's remembered by that embedding is that there were four.

565
00:30:12,120 --> 00:30:13,120
Mm-hmm.

566
00:30:13,120 --> 00:30:14,120
Right.

567
00:30:14,120 --> 00:30:15,120
I think it's kind of nice.

568
00:30:15,120 --> 00:30:19,360
Also, the other example that I talked about in the keynote was if you have the same pig

569
00:30:19,360 --> 00:30:26,200
class, the same pig auto encoder, and you take a nice drawing of a semi-truck, you know,

570
00:30:26,200 --> 00:30:27,200
it decodes, guess what?

571
00:30:27,200 --> 00:30:28,200
It's a pig truck, right?

572
00:30:28,200 --> 00:30:32,480
Like, it kind of turns into a pig because that's what the pig model knows about is pigs.

573
00:30:32,480 --> 00:30:34,080
And it's at one level, that's a weakness, right?

574
00:30:34,080 --> 00:30:36,840
It's like, well, but there are so many other things in the world.

575
00:30:36,840 --> 00:30:39,520
But of course, you can train more models and you can train models that are conditional

576
00:30:39,520 --> 00:30:42,000
so they can represent more than one thing.

577
00:30:42,000 --> 00:30:48,480
It's that this latent space is really capturing some important aspects of pigness, right?

578
00:30:48,480 --> 00:30:53,720
And people have someone else in open source, not us, took those very embeddings and then

579
00:30:53,720 --> 00:30:54,720
did something fun.

580
00:30:54,720 --> 00:30:59,640
They started looking for examples of pigs in the quick draw dataset that were far away

581
00:30:59,640 --> 00:31:01,400
from that embedding.

582
00:31:01,400 --> 00:31:06,240
So you imagine you embed a bunch of pigs, so you get the average embedding, you know,

583
00:31:06,240 --> 00:31:10,120
you don't have to embed all of them, but 10,000 of them get the average, right, to

584
00:31:10,120 --> 00:31:11,120
take the average.

585
00:31:11,120 --> 00:31:15,040
Even from there, you embed a pig and you take its distance, how far away is it from that

586
00:31:15,040 --> 00:31:16,440
average, right?

587
00:31:16,440 --> 00:31:20,640
And then, like, what's fun is they just, they just found really poorly drawn pigs, or

588
00:31:20,640 --> 00:31:24,760
they occasionally someone did just written the word pig, you know, and it's like, that's

589
00:31:24,760 --> 00:31:25,760
not a pig.

590
00:31:25,760 --> 00:31:28,280
I mean, it says pig, but these models didn't learn to read.

591
00:31:28,280 --> 00:31:31,800
And so, and so you get this weird kind of fun outlier detection.

592
00:31:31,800 --> 00:31:33,880
And you know, you can cluster the space too.

593
00:31:33,880 --> 00:31:39,920
That's often done with word-to-vec models where you say, okay, like for cats, there are,

594
00:31:39,920 --> 00:31:44,320
it's not that there are just, you know, several million, completely unique cats.

595
00:31:44,320 --> 00:31:48,560
People kind of either draw them in profile, you know, or they draw them just the cat's

596
00:31:48,560 --> 00:31:49,560
head.

597
00:31:49,560 --> 00:31:50,880
And sometimes the profile flips us where this way.

598
00:31:50,880 --> 00:31:54,480
And if you, in this embedding space, if you do clustering in the embedding space, so

599
00:31:54,480 --> 00:31:59,040
you look for, you know, it's not like given the number of possible, the possible values

600
00:31:59,040 --> 00:32:01,200
in this embedding, everything is equally spread out.

601
00:32:01,200 --> 00:32:03,160
It's like, there are mountains, right?

602
00:32:03,160 --> 00:32:06,240
And here's the profile mountain, and here's the profile, the other way mountain, and here's

603
00:32:06,240 --> 00:32:07,240
the face mountain.

604
00:32:07,240 --> 00:32:09,880
And you can visualize that and kind of get an idea, you know, what

605
00:32:09,880 --> 00:32:12,560
people are really doing with these drawings.

606
00:32:12,560 --> 00:32:19,760
Can you then identify a distance from your average pig vector beyond which you won't be

607
00:32:19,760 --> 00:32:21,800
able to recognize something as a pig?

608
00:32:21,800 --> 00:32:24,400
Yeah, and that's the classifiers job.

609
00:32:24,400 --> 00:32:27,840
In this case, it was an autoencoder, so it wasn't trying to decide whether something was

610
00:32:27,840 --> 00:32:28,840
a pig or not.

611
00:32:28,840 --> 00:32:30,520
It was trying to draw a pig.

612
00:32:30,520 --> 00:32:34,000
So there's no, there's no magic value, but you could always calculate one, right?

613
00:32:34,000 --> 00:32:36,920
You can kind of figure out where, where you're going.

614
00:32:36,920 --> 00:32:38,720
It's important to keep track of the goals, right?

615
00:32:38,720 --> 00:32:39,720
Right.

616
00:32:39,720 --> 00:32:43,560
You know, this is, this is a kind of small, it's a large data set in terms of the number

617
00:32:43,560 --> 00:32:47,160
of samples we have, but you know, it's a pretty simple drawing task.

618
00:32:47,160 --> 00:32:51,240
I think it's, it's fun to think about where we can go if we have, if we have better data

619
00:32:51,240 --> 00:32:52,920
and, and better models.

620
00:32:52,920 --> 00:32:54,240
And where is that?

621
00:32:54,240 --> 00:32:56,600
So I think there are a couple of possibilities.

622
00:32:56,600 --> 00:33:04,560
One is that we just nail it, and we, we basically do something that happens one in a million

623
00:33:04,560 --> 00:33:08,680
times, which is we invent a new art form, where we enable a new art form.

624
00:33:08,680 --> 00:33:13,080
Now, it's been done before, technology has done this before, the film camera, enabled

625
00:33:13,080 --> 00:33:14,800
a new art form, right?

626
00:33:14,800 --> 00:33:17,520
The drum machine enabled a new art form.

627
00:33:17,520 --> 00:33:20,920
And we could probably, together we could riff, and if we had a whiteboard, we'd come up

628
00:33:20,920 --> 00:33:21,920
with 30 or 40 of them.

629
00:33:21,920 --> 00:33:25,760
But we wouldn't want to forget that there were another, you know, thousands and thousands

630
00:33:25,760 --> 00:33:30,440
of, did the theorem in invent a new art form, maybe, right?

631
00:33:30,440 --> 00:33:32,560
The synthesizer, yes, right?

632
00:33:32,560 --> 00:33:36,800
So maybe we'll invent a new art form, and it's really interesting to think like what

633
00:33:36,800 --> 00:33:41,320
would, the core of that art form would be that the artist would have something smart

634
00:33:41,320 --> 00:33:43,240
against which to, to project ideas.

635
00:33:43,240 --> 00:33:45,040
I think that would be the core.

636
00:33:45,040 --> 00:33:48,840
That, you know, you can try things out with, with your sequencer, you know, or you can

637
00:33:48,840 --> 00:33:53,720
try things out with Photoshop, or you can try things out with, with a pen and paper.

638
00:33:53,720 --> 00:34:00,360
But the idea that we would have machine intelligence models trained that are, by analogy,

639
00:34:00,360 --> 00:34:03,120
as smart as translate, right?

640
00:34:03,120 --> 00:34:04,120
Because they're trained right.

641
00:34:04,120 --> 00:34:06,520
And you, like, that's really, really interesting, I mean.

642
00:34:06,520 --> 00:34:10,960
There's an inherent limitation in that, in that it, we're training on, for the most

643
00:34:10,960 --> 00:34:13,280
part, things that we've seen before, right?

644
00:34:13,280 --> 00:34:19,600
And so it kind of cuts off, at least intuitively, seems to cut off this creative avenue for

645
00:34:19,600 --> 00:34:24,520
innovating, for, you know, creating the holy new thing.

646
00:34:24,520 --> 00:34:25,520
I agree.

647
00:34:25,520 --> 00:34:31,120
And I think that, I think that if we create a new art form, it will be through gradual,

648
00:34:31,120 --> 00:34:34,840
through many, many interaction loops of artists working with this technology.

649
00:34:34,840 --> 00:34:39,160
So for, like, that's not that crazy to think about, if you watch what happened with the

650
00:34:39,160 --> 00:34:44,240
808 drum machine and drum machines moving forward, people first use drum machines on, on

651
00:34:44,240 --> 00:34:46,320
sort of, you know, Roland's terms.

652
00:34:46,320 --> 00:34:49,360
This is what a drum machine is, you know, but gradually, they just are like, I'm going

653
00:34:49,360 --> 00:34:51,040
to use it however I want, right?

654
00:34:51,040 --> 00:34:54,760
And then the drum machines were manufactured to adapt to that, et cetera.

655
00:34:54,760 --> 00:34:58,280
And I think one way to think about this is, if this were to actually generate a new

656
00:34:58,280 --> 00:35:03,040
art form of interest, you train a model, it generates some things that are somewhat surprising

657
00:35:03,040 --> 00:35:04,800
to you, but you like, you keep them.

658
00:35:04,800 --> 00:35:08,440
You change your writing style somewhat or you change your performance style somewhat.

659
00:35:08,440 --> 00:35:12,080
Maybe, you know, like, I can imagine living in a world where the data is so important,

660
00:35:12,080 --> 00:35:15,560
like part of what you do as an artist is create data.

661
00:35:15,560 --> 00:35:20,160
And so in some sense, you're like, you are helping create this model because you're the

662
00:35:20,160 --> 00:35:23,840
one providing it with its lifeblood with data, or ultimately, you're hacking the model,

663
00:35:23,840 --> 00:35:25,360
either because you're a coder, right?

664
00:35:25,360 --> 00:35:27,880
So then you say, okay, this model changed slightly how I do music.

665
00:35:27,880 --> 00:35:29,840
Maybe it just added some rhythms to my repertoire.

666
00:35:29,840 --> 00:35:30,840
I don't usually use it.

667
00:35:30,840 --> 00:35:34,480
It does some crazy harmonization that I wouldn't normally do.

668
00:35:34,480 --> 00:35:37,920
And then you make some more music and then you retrain your model, but now there's new

669
00:35:37,920 --> 00:35:38,920
stuff there, right?

670
00:35:38,920 --> 00:35:40,640
And you've moved a little bit, right?

671
00:35:40,640 --> 00:35:44,520
I don't think the movement has to be earthquake large.

672
00:35:44,520 --> 00:35:48,320
You know, it doesn't have to be groundbreakingly large to be new, right?

673
00:35:48,320 --> 00:35:53,120
I mean, imagine just like if we kind of work out a new way to harmonize music.

674
00:35:53,120 --> 00:35:56,000
So all, you know, you have a melody and you have all these extra voices.

675
00:35:56,000 --> 00:36:00,240
You know, just imagine via a model that's trained right, we suddenly get different kinds

676
00:36:00,240 --> 00:36:02,480
of harmonizations that we didn't get before.

677
00:36:02,480 --> 00:36:07,120
That would already be interesting or another example I like, which is crazy hard,

678
00:36:07,120 --> 00:36:09,280
but I think it's very evocative.

679
00:36:09,280 --> 00:36:14,800
I think like a long form, something long form like a novel, right?

680
00:36:14,800 --> 00:36:16,720
And think about plot, right?

681
00:36:16,720 --> 00:36:18,640
Like plot, plot is hard, right?

682
00:36:18,640 --> 00:36:21,680
You can, like I, my hat's off to someone who can write like a long,

683
00:36:21,680 --> 00:36:26,400
my hat's off to Game of Thrones and keeping all these characters straight, right?

684
00:36:26,400 --> 00:36:30,080
George R. Martin is my hero, but you could imagine like,

685
00:36:30,080 --> 00:36:33,680
imagine that some writers care less about plot and they really care about character,

686
00:36:33,680 --> 00:36:38,160
they care about texture, imagine the right kind of machine learning model

687
00:36:38,160 --> 00:36:43,520
that can generate intricate plots with really interesting relationships between characters

688
00:36:43,520 --> 00:36:45,680
and movement of action, right?

689
00:36:45,680 --> 00:36:51,200
Such that like maybe in a way that's even, you know, at some level of complexity

690
00:36:51,200 --> 00:36:54,560
too hard for a human to do because it can search out more possibilities,

691
00:36:54,560 --> 00:36:58,640
but yet maybe everything kind of clicks and lands and it feels good as the reader to be like,

692
00:36:58,640 --> 00:37:02,480
oh wow, that character just came and did that, that's crazy, right?

693
00:37:02,480 --> 00:37:05,120
Almost like writing jokes that have sort of three punchlines that all

694
00:37:05,120 --> 00:37:06,960
in, you know, land at the same time.

695
00:37:06,960 --> 00:37:10,880
That like because, you know, machine learning algorithms are really good at

696
00:37:10,880 --> 00:37:13,760
high dimensional spaces with lots of possibilities.

697
00:37:13,760 --> 00:37:16,720
You know, maybe we'd land at some new form of storytelling.

698
00:37:16,720 --> 00:37:19,600
I don't think it would be one where we would care about reading the computer's story,

699
00:37:19,600 --> 00:37:24,960
but that the computer might add something to the writer's world where like,

700
00:37:24,960 --> 00:37:26,800
in some sense the writer might offload plot.

701
00:37:26,800 --> 00:37:28,560
You're like, that's not, I don't do plot.

702
00:37:28,560 --> 00:37:30,640
What I do with the plot is something even more interesting.

703
00:37:30,640 --> 00:37:34,000
Like I shape it, I craft it, I make it beautiful to read, right?

704
00:37:34,640 --> 00:37:37,360
So that's one way to think about it that like, you know,

705
00:37:37,360 --> 00:37:39,840
I don't want this to be too long-winded, but it's really important to me like,

706
00:37:39,840 --> 00:37:43,120
what the drum machine did was in one really important way,

707
00:37:43,120 --> 00:37:45,680
it offloaded some of the percussion work.

708
00:37:46,640 --> 00:37:50,240
But, you know, if you want to be cynical about it, then you're like,

709
00:37:50,240 --> 00:37:52,720
oh yeah, and it just killed, it just killed percussion.

710
00:37:52,720 --> 00:37:53,760
It didn't.

711
00:37:53,760 --> 00:37:58,080
It offloaded one thing and that opened up a bunch of other opportunities, right?

712
00:37:58,080 --> 00:38:03,600
And so then the idea is, well, what sort of things can you offload onto a super smart

713
00:38:03,600 --> 00:38:06,800
machine learning model? And to be honest, I don't know, I don't know the answer,

714
00:38:06,800 --> 00:38:11,200
but I mean, if you move through different kinds of media from painting to music,

715
00:38:11,200 --> 00:38:14,800
to literature, you certainly can get some ideas.

716
00:38:14,800 --> 00:38:16,960
Yeah, awesome, awesome.

717
00:38:16,960 --> 00:38:22,880
And so magenta as a whole is kind of an umbrella for a bunch of these different

718
00:38:22,880 --> 00:38:28,480
directions, right? Maybe you take a second to talk about magenta and how Google

719
00:38:28,480 --> 00:38:31,280
thinks about that, why Google's even, you know,

720
00:38:31,280 --> 00:38:33,680
bothering. Tell us about magenta.

721
00:38:34,480 --> 00:38:37,440
Yeah, this isn't funny. This question usually comes early, right?

722
00:38:37,440 --> 00:38:40,160
It's good to have this come after people have heard some specifics.

723
00:38:40,160 --> 00:38:43,680
So the basic idea is of magenta is posing the question, you know,

724
00:38:43,680 --> 00:38:47,520
what can we do with deep learning and reinforcement learning in this basic creativity?

725
00:38:47,520 --> 00:38:54,880
And we've already fully realized that the meat of the problem lies in these algorithms

726
00:38:54,880 --> 00:38:59,680
interacting with musicians and artists. And that's not because we're afraid of trying to get

727
00:38:59,680 --> 00:39:02,800
these models to be interesting on their own, but that I think that's how art works.

728
00:39:02,800 --> 00:39:05,680
It's collaborative, you know, other artists are working together and we're thinking,

729
00:39:05,680 --> 00:39:07,680
you know, these things are going to work with artists.

730
00:39:07,680 --> 00:39:12,720
And so we, in the last year, magenta has been around publicly for about a year and before

731
00:39:12,720 --> 00:39:15,360
we launched, we were working on it for another six months.

732
00:39:15,360 --> 00:39:20,320
We've done work in music sequence generation, including musical scores and also performances

733
00:39:20,320 --> 00:39:26,080
like we did today. We've done work in generating new kinds of sounds and synth project, which is

734
00:39:26,080 --> 00:39:30,000
basically building a synthesizer where all of the sounds are dreamt up by a neural network.

735
00:39:30,960 --> 00:39:33,760
And we've done some unpublished work in joke telling.

736
00:39:33,760 --> 00:39:38,720
And it's, it's unpublished because, well, it wasn't very funny.

737
00:39:38,720 --> 00:39:44,160
Now we did a summer internship like looking at joke telling as exercise in generating

738
00:39:44,160 --> 00:39:47,680
interesting surprises. I think especially punchline driven humor is like, oh, that was

739
00:39:47,680 --> 00:39:52,240
surprising in a nice way. You know, yes, my kids like getting to the level of dad joke.

740
00:39:52,240 --> 00:39:57,200
It should be hard. I know. I know. I know we're both dads here. We should be experts at this.

741
00:39:57,200 --> 00:40:02,720
Like if we just team up. Yeah. And what my motivation actually is speaking of kids might,

742
00:40:03,440 --> 00:40:08,240
okay, so let me finish the, and then thinking a little bit more about language, but most of what we've

743
00:40:08,240 --> 00:40:12,400
had to do, oh, and then learning to sketch the sketch to sketch our NN stuff. And then some

744
00:40:12,400 --> 00:40:18,000
implementations of style transfer for images. So, so we've got kind of a wide array of things

745
00:40:18,000 --> 00:40:24,240
that we're trying. I think the glue of the project, what holds it together is a, we're limiting

746
00:40:24,240 --> 00:40:29,040
ourselves to deep learning and reinforcement learning. We're looking at creative applications

747
00:40:29,040 --> 00:40:32,960
of machine learning, but we're not going to try everything. Like there are lots of ways to solve

748
00:40:32,960 --> 00:40:36,320
these problems. Lots of great ways. Right. We're not saying that machine learning is the only way

749
00:40:36,320 --> 00:40:38,960
of the sort we're doing. It's just like, we got to limit ourselves to something. So we're part,

750
00:40:38,960 --> 00:40:44,640
we know, we're really linked to TensorFlow. Also that really what we care about are like behind

751
00:40:44,640 --> 00:40:50,960
these, this one year of trying some new things, I think the really core issues lift above any

752
00:40:50,960 --> 00:40:56,720
specific medium. I think it's about, it's about storytelling and narrative, whether it's music or

753
00:40:56,720 --> 00:41:03,040
paintings or literature. And it's about structure and narrative arc and about these ideas about

754
00:41:03,040 --> 00:41:07,520
surprise and what, what makes something simple. So I think there's a, there's a whole area of like,

755
00:41:07,520 --> 00:41:12,960
you know, generative models for media that, that, you know, can we generate interesting

756
00:41:12,960 --> 00:41:17,360
bits of media for us to share that help us tell the story of our lives and, you know,

757
00:41:17,360 --> 00:41:21,840
different modes of communication, possibly coming from this. Actually, almost certainly coming

758
00:41:21,840 --> 00:41:26,640
from this. You know, those are the goals. Why is Google doing this? Well, like, one thing is,

759
00:41:26,640 --> 00:41:30,800
we're publishing a lot of papers. So we're part of Google Brain and the framework of generative

760
00:41:30,800 --> 00:41:35,840
models is important. It's important. It's an important research topic. And the idea that you,

761
00:41:35,840 --> 00:41:41,440
you know, maybe you'd want to generate, you know, new candidate molecules, maybe you'd want to

762
00:41:41,440 --> 00:41:46,480
look at healthcare, maybe you'd want to look at something for robotics and generating trajectories.

763
00:41:46,480 --> 00:41:52,560
But I think, you know, when I look at what my kids are doing with their mobile devices,

764
00:41:52,560 --> 00:41:56,800
I've got a 13 year old and an 18 year old, you know, they're, they're information seeking,

765
00:41:56,800 --> 00:42:01,200
they're entertainment and they're communicating with their friends, right? So there's a huge chunk

766
00:42:01,200 --> 00:42:06,080
of what we're doing with computation that has to do with entertainment. And I think, you know, it's

767
00:42:06,080 --> 00:42:11,200
a really important area of research. I mean, just, you know, just point blank. I think music and

768
00:42:11,200 --> 00:42:15,440
art are important. They are important for our lives and it's absolutely worth investing some time

769
00:42:15,440 --> 00:42:19,680
into it. Awesome. That's what we're doing. Awesome. Anything else that you'd like to

770
00:42:20,400 --> 00:42:27,200
mention or any other places that folks should look or are there, you know, three canonical

771
00:42:27,200 --> 00:42:31,920
resources to, you know, for folks that really want to dig into this? Yeah. So there's two things

772
00:42:31,920 --> 00:42:38,560
I would say. First, please visit g.co slash magenta. That's the shortest link I have. And we have

773
00:42:38,560 --> 00:42:42,320
a blog, we have a blog there that's getting, if you want to geek out, would you call it nerd time?

774
00:42:42,320 --> 00:42:48,320
What did you say? Uh, nerd alert. Nerd. Yeah. We nerds. Listen or nerds. If you like that,

775
00:42:48,320 --> 00:42:54,160
come look at our blog. And the other thing is we're very, very actively trying to engage with

776
00:42:54,160 --> 00:43:00,160
some particular types of folks in the community. There are three types. One type is pretty obvious.

777
00:43:00,160 --> 00:43:05,440
Artists and musicians, fun. And then there's always the machine learning folks, nerd alert. But

778
00:43:05,440 --> 00:43:10,240
there's this middle, middle ground where I think there's probably more there than any place else. And

779
00:43:10,240 --> 00:43:15,440
it's, it's in this kind of world of creative coding. Do you know what I mean by that? So like,

780
00:43:15,440 --> 00:43:20,320
I mean, what do you mean by that? Well, I mean, I'm sort of stealing that, you know, creative coding

781
00:43:20,320 --> 00:43:25,600
is, is just that it's coding. I mean, the way I define it is it's the creative aspects of coding.

782
00:43:25,600 --> 00:43:30,240
Right. So I don't want to add to it. So coding or coding applied to creative. It's that. Yeah.

783
00:43:30,240 --> 00:43:34,480
Sorry. I said wrong. Yeah. And I think, you know, I was talking about how a musician might

784
00:43:35,120 --> 00:43:39,040
play a few hours of music and then use that to drive a machine learning model. And in some sense,

785
00:43:39,040 --> 00:43:43,280
they're hacking the model because they're providing the data. But also building these models

786
00:43:43,280 --> 00:43:48,000
as a creative thing in and of itself. And we're trying to build some frameworks where like,

787
00:43:48,000 --> 00:43:53,200
if there were an artist who knew how to code some, you don't have to be like the world's best

788
00:43:53,200 --> 00:43:58,160
machine learning coder. But we have some models where, you know, you could change a few things.

789
00:43:58,160 --> 00:44:01,920
You could, you could say, I want, if you had some way that you could, for example, let's say you

790
00:44:01,920 --> 00:44:07,200
wanted the music that was generated by the model to be more shimmery. Whatever that means. Fine.

791
00:44:07,200 --> 00:44:10,960
If you think you know what it means and you can define that in a way that if we get a piece of

792
00:44:10,960 --> 00:44:14,640
music and you say, oh, that sounds more shimmery, then we can actually use that with the, we can train

793
00:44:14,640 --> 00:44:18,720
the model to do a better job of generating that kind of music. So I don't know. I think there's

794
00:44:18,720 --> 00:44:24,960
a whole, a whole direction of like having coding be part of artistic generation. And that machine

795
00:44:24,960 --> 00:44:29,680
learning and a project like magenta is really a core place to try that. And so we're trying to get

796
00:44:29,680 --> 00:44:33,280
more people through open source to work with us, to collaborate with us, to make art and make music

797
00:44:33,280 --> 00:44:37,280
and hack stuff. Awesome. And so we'd love to see more people from, you know, from your listeners

798
00:44:37,280 --> 00:44:42,320
join us. That's it. That's what I want to say. Awesome. Well, thanks so much. I really enjoyed

799
00:44:42,320 --> 00:44:45,840
this conversation and I'm sure folks will enjoy listening to it. Thanks for all of your great

800
00:44:45,840 --> 00:44:55,280
questions, Sam. Thank you. All right, everyone. That is our show. Thanks so much for listening and for

801
00:44:55,280 --> 00:45:01,440
your continued support, comments and feedback. A special thanks goes out to our series sponsor,

802
00:45:01,440 --> 00:45:06,880
Intel Nirvana. If you didn't catch the first show in this series where I talked to Naveen Rao,

803
00:45:06,880 --> 00:45:11,920
the head up Intel's AI product group about how they plan to leverage their leading position and

804
00:45:11,920 --> 00:45:17,520
proven history and Silicon innovation to transform the world of AI, you're going to want to check

805
00:45:17,520 --> 00:45:24,400
that out next. For more information about Intel Nirvana's AI platform, visit intelnervana.com.

806
00:45:25,120 --> 00:45:30,880
Remember that with this series, we've kicked off our giveaway for tickets to the AI conference.

807
00:45:31,680 --> 00:45:37,200
To enter, just let us know what you think about any of the podcasts in the series or post your

808
00:45:37,200 --> 00:45:42,960
favorite quote from any of them on the show notes page on Twitter or via any of our social media

809
00:45:42,960 --> 00:45:51,840
channels. Make sure to mention at Twomo AI, at Intel AI and at the AI Conf so that we know

810
00:45:51,840 --> 00:45:58,480
you want to enter the contest. Full details can be found on the series page and of course,

811
00:45:58,480 --> 00:46:03,840
all entrants get one of our slick Twomo laptop stickers. Speaking of the series page,

812
00:46:03,840 --> 00:46:10,480
you can find links to all of the individual show notes pages by visiting Twomo AI.com slash

813
00:46:10,480 --> 00:46:40,320
O'Reilly AINY. Thanks so much for listening and catch you next time.

