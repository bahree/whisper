I think we also hitting maybe a little bit of a wall in an LP, where maybe just using text alone is not enough, you know, even when you scale, even when you scale to this really really huge sizes, you still have this little thing that's still missing and it's probably something that you need to find in other modalities, maybe you cannot model the full world that you would like to model with on the text, at least that's what I believe.
All right, everyone, I am here with Thomas Wolf. Thomas is co-founder and chief science officer at Hugging Face. Thomas, welcome to the Twoma AI podcast.
Hi, thanks for having me.
Absolutely, I am looking forward to digging into the conversation with you. We'll be talking about a wide range of topics, including transformers, Hugging Face, of course, some of your initiatives and research and the big science project and more.
But before I'd love to get to know you a little bit better and one thing I found super interesting was that you did a PhD in quantum physics and then went on to get a law degree and practice as a lawyer.
How did all of that come about and how did you then end up in AI?
Yeah, how do you make a, how do you make a coherent story out of that?
No, I guess when I was studying, actually, also ML was was was very strong and I was very small. I mean, I didn't didn't even know about this field. So in France, physics was a physical physical was was very interesting to me.
And so I did I did this PhD. I was actually in the US at some time here. I hesitated between MIT and then Paris and then in the end, Paris was was was more interesting for me for the city, I would say.
And one of the problem with saying physics is experiments are like super long, but we don't have any idea when we train a model for four months, we like this is super long, but a real experiments physics like four years, you need to buy a Christ, you need to install it and it you have all these problems.
We don't realize how fortunate we are in the NML that we can reproduce our experiments very, very easily, everything is quite easy to do. And so at the end of my PhD, I was contemplating going to academia, but that meant basically staying 10 years on the same, same topic if you want to do something back full.
And at the same time, just writing my PhD was was actually a very pleasant experience. And so I thought I was looking for a job where I could write more.
You don't hear that often, I don't think. Yeah, people find it painful, like education and talking about talking about research, I found this super interesting.
And so a friend was a lawyer and told me that, yeah, if you want to write, you should be a lawyer. So I was like, okay, let's try. And I went up to be an API tourney for five years, a little bit more than five years.
And that was very enjoyable. I think I like this a lot. I had my pool of startups mostly started, but also some big groups that was helping like with all IP question writing, writing patterns and everything. And at the ends around 2014, 14 years, something that was the beginning activity of deep learning, I think, well, at least it was starting to work and like some startups were trying to to use the computer vision models.
And so I was writing the patterns. I was like, yeah, this is actually a physics equation, you know, they're just rebranded.
This is they don't call that free energy, but in the end, it's a little bit the same. They're just doing the same thing.
And so I decided to dig a little bit in this. So I remember like printing the deep learning paper from from Jan, the deep learning book, you know, just was not published yet, but there was like an alpha version of the web and reading that.
And at the same time, I was supposed to move to the US and so basically all my low degree were just not worth much anymore because I should have like take a low school in the US. So I was like, okay, this is a bit annoying.
I don't want to do a low school anymore.
And so Julian was a founding hugging face in Brooklyn at the time with Climo. And it was a it was a game company. It was a chatbot company and they were like, well, this LSTM stuff are kind of starting to work.
Maybe we can do a bit of science on the side. I was like, okay, I would be happy to do to do the science part that I can face.
And we had this very long dream that maybe at some point we could be, I don't know, like a deep mind, like a big research lab that seems that seems so far away at the time.
And so yeah, that was the beginning of hugging face.
In the end, I didn't move to the US. I stayed in the Netherlands. And Julia was was in France and Climo in the US. So we kind of started this this remote company from from the beginning, actually.
And this is how we started five years ago.
So for quite some time, we were mostly doing chatbots and and game. And at some point we started to open source these few tools that we had build.
That was the first one was actually some transfer learning called deep moji, which was before GPT, but that was I think the first time transferring kind of really worked well.
What happened is just that our open source activity got some like a lot of speed quite quickly and after some after some time we were just all on the open source.
And so a few a few years after in 2019 we decided to pivot and to stop with the with the game and to concentrate on the open source.
Which we have notes regretted at all. I think it's I'm curious. You know, I feel like we get to have a whole conversation just about kind of the hugging faces of business and, you know, the strategy there.
And we won't do that. But I'm curious, you know, when you think about where that all goes, like what's the direction that you all have in mind.
Yeah, I think it's, I mean, the nice thing about hugging faces, it's very, I would say transparent company. You can see a lot of it from the outside. So, and then exactly the same inside. So we have this very clear direction around like more open source, more open science, more like what we call responsible or good AI, which is making an AI, which is not.
Yeah, not only focus, but take into account all the social impact that you that you have and try to have people around the table that knows about this impacts note about the ethical question around that.
And so, yeah, that's the direction. I think today there is a couple of kind of wearing trans that that basically we think are things that are not so healthy like this kind of focus on private models on private company, the big distance between academia and like industrial labs.
So we're kind of trying to fight basically this thing. And we think that's in the end, you know, maybe much more important, which would you build if we reach idea or not, I don't know, but I think the journey that you follow if it's if we build a healthy community that's actually much more important in the end.
If like the research community is something where people can sit around the table can participate together in research project, people share knowledge, people like don't hesitate to share tools to share their results. I think that's much more important in the end, maybe don't even what we build.
Yeah, I think of hugging face as or when I think of hugging face, I think of NLP in particular, language models, that kind of thing.
There's also, you know, there's tons of different types of models in the hub. How much of the company's focus and energy is specific to NLP language models, all of that stuff versus other types of things.
Yeah, that's a good question. I think until maybe one year, one year, half ago, there was the whole company was fully fully on NLP. And since last year, we've kind of be expanding first on the open source side.
So we've been doing a lot of speech since about one year ago when Patrick Patrick from platinum decided to to push for this direction.
So we have now quite a strong presence there. And more recently, we've been, we've been experiencing more with computer vision. Again, first on the open source side, and now on the research side, we also, we also expanding what we do.
Also because, well, I think we also hitting maybe a little bit of a wall in NLP, where maybe just using text alone is not enough, you know, even when you scale, even when you scale to this really, really huge sizes, you still have this little thing that's still missing.
And it's probably something that you need to find in other modalities. Maybe you cannot model the full world that you would like to model with only text, at least that's what I believe.
And so generally, we're expanding so so easier because the tools are getting similar and similar everybody's using transformers.
And there is this kind of conversions of touring that that make it quite natural beyond the open source your team on the science side is focused on research.
So the science team has been growing quite a lot. So mostly last year around this big science project, and now that big science is reaching its end, it's starting a couple of other projects and the idea is basically to together people around impactful projects.
So maybe to have a research team, which is quite ambitious, which can tackle kind of fundamental problems. So it's quite exciting. I think we're reaching now closer to this idea that we had in beginning that I've always admired a lot deep mind and the way they can gather people around a project.
And at the same time, I was always a bit frustrated that it was quite close tools and not really shared with the rest of the community. And so here the idea is to try to do the same type of research lab, which which kind of select a handful of projects.
But do we do it in a kind of open source open science way.
The big science project is that turned out to be kind of an umbrella for the kind of full breadth of your research program or is it a specific project and you have other projects exploring other areas.
Yeah, so big science project is a specific one. So it started a pretty much one year ago, a little bit more. And the discussion was around a good relationship we had with with with a compute cluster.
It's always it's always good to have a nice relationship with the with the neighborhood compute cluster. So there is this big public compute cluster in France called Jansée, which is quite big. Actually, it has more than 3000 GPU.
And they were like asking us, they were a bit like under under used and they were asking us, okay, we can we do something together. You want to use the cluster for something. And so at the same time, like people were kind of releasing this series of large language model, well, I mean, releasing the paper, at least.
And I was like, okay, that would be nice to to make maybe a large language model that would be public. And so we could do that there. And the more we were thinking about it, the more we thought it's it's a bit.
It's a bit of knowing to do that just taking face because we'll just reproduce something that we that we think is missing today, which is just.
Like we just do the same that people are doing, which is like having five people in a small like a team training, but the way we think this should be done is more like what I've seen when I was doing like my PhD, which was this very large like physics collaboration.
So the example I take off on is the LHC, which is this this huge Hadron Collider, which is in Switzerland that cost like 10 billion dollars. And so obviously 10 billion dollars that's much. And that's that's really outside of the reach of any any university lab. So how people do that in physics, they just get together a bunch of research university, a bunch of research lab and then they basically ask a big grants of 10 billion.
So thankfully we don't need 10 million to train a language model yet, maybe in the future, but you you kind of need 10 million, which is still quite significant.
And so I thought maybe we could do the same, maybe we could just gather a lot of research labs together from industry from academia. And then we ask we ask a public.
A public cluster to give some hours. And it turns out you you can do that and that work basically we ask them 5 million GPU hours, let's say yes, there you have them.
We actually asked them everything that they that they were supposed to grant for for six months. So the conclusion was that they extended the they made the cluster itself bigger, which is also a nice outcome because then the cluster will be bigger for all future research.
And so the big sense project started in one year ago around this big grant of compute time from from Jean Zé from the Jean Zé super computer.
And then we had everything to do to to build a large data sets to to gather the idea was to gather around the table people from social science people from like data people from modeling people from also maybe.
And also feels they would be interesting using language model like historical documents biomedical people who want to maybe use them everybody who could be interested in using a large language model should be able to to join the table.
And so today there is I think a little bit more than 1000 researcher who have parts who are like signed the.
So not everybody is active obviously that's like a voluntary basis, but I would guess maybe 200 people are daily active on this and we are now reaching the last stages so we finished a big data set just just last week, which is 800 gigabytes.
And we are starting the training probably next week, which will go for four month so that's quite long to train this business.
I mean, I have 200 researchers working on creating this data set.
Yeah, that's I mean, they don't all work on the data set the data set.
There is also the model to build, but yeah, I think it's it's very surprisingly great because you know.
More and more we see that these models are only as good as their data set are basically it's there just kind of a reflection of basically the data set that you feed in.
Most of the time part of this data set are like crawl data for instance, but here just because there are so many people there we could have like a kind of crowdsource of the data set so we ask all the native speaker.
So this is a multilingual model that that that we are training and we ask all this like native speaker to select good quality source in their own languages.
And because there are so many people when you guys are all that's that give you like 800 gigabytes, which is enough to train this huge mobile actually.
So you don't even need to crawl the web basically to get that just because of the power of community, I would say somehow, at least a large collaboration.
What gives you the confidence to say that now before you've actually trained.
That's right. You guys me there. We still need to change it.
But I think the data set in itself is actually also a super interesting artifact and and a lot of work was also spent on the legal aspect and the governance aspect of being able to share this data set.
Because that's a crucial resource that's missing today, you know, if you even if you have access, I don't know to GPT three or some of this model, you never really have access to the data set.
And that would be super interesting for a lot of research. If you want to know if you're actually doing future. If you want to know if your model is good there because it was like 10 times or 100 times or 1000 times in a training data set.
If you want to do I don't know PII research.
And for a lot of research would be super useful to be able to dig in the training data set and know what the model has really seen.
And so being able to have like a very large multi multilingual data set is a super valuable resource already. I think sometimes we focus a lot on models, but data set they live longer actually than the model.
But yeah, now we still need to train the model. That's right.
Recognizing that you haven't trained the model yet. I'm curious what.
How you think about this approach to curating these data sets and like how you think it's going to play out in terms of the model.
And I think the analogy that I'm constructing is, you know, with deep learning, we've kind of said, hey, instead of like curating this data set and, you know, doing feature engineering and all this kind of more manual hand crafty kind of things.
That's just throw a bunch of data at a sufficiently large model and, you know, it'll figure out the patterns that it needs to provide value to us.
And I think I'm drawing an analogy to, hey, let's just slurp up the entire way and train a model and it's going to, you know, that's going to result in good things that we know some of the downfalls of that.
Like the web lots of corners of the web can be a bad place and I'm just.
I'm curious like how you think it's all going to come together, you know, relative to the way we're creating these models today.
Yeah, I mean, there's a lot of aspects with you asking that's very good question.
I would say first, I think we can come back a little bit from just scrapping the web and just feeling the models with that.
And there's actually a good lesson in big sense because to test the scaling in the beginning, we trained a 13 billion model just unlike a bunch of common crawl data.
13 billion years, which is already quite big, I would say that's that's probably bigger than a lot of.
Is there a certain billion of it? Well, there is the 20 billion now from a later on since it's two weeks ago, but that's that's quite big.
And the performances were pretty bad actually. And when we try to dive in it, the reason was the data if you just take common call like this, it's not very high quality.
And so we compared with other like manually selected data set like the pile or like another set of common call and all of the all of the pointer were just that it was all about the data that was not high.
And so I think we can come back a little bit from that. There was also a bunch of research around like how when you just did duplicate your data set, you actually improved the performances of your model like drastically.
And if you look at even you know like GPT 2 GPT 3, the data was still quite carefully selected from links and read it or something like that.
So that kind of sounds like a bad place on the internet, but that's still quite a good indication of good quality.
So I think this is now people more more people consider that very important.
A similar thing around the training data set of like clip or thing like that people have been really struggling to reproduce the performances that's mostly from the quality of the data set.
I think open AI is really regretting in carefully curating high quality data set that's something that they have very well perfected.
And the thing that we think of as like we're going to slope everything and create a model is not actually what's happening.
It's a lot more carefully curated than that.
Yeah, that's what I think. Yeah, definitely.
But then on the other hand, there is also the question of how you evaluate the model.
And if you want obviously if you want like for instance to evaluate your model mostly on, I don't know, benchmark that are made from Wikipedia data, you probably want to tailor your model around this distribution.
And so in big sense, we've made the choice of trying to have a data set that is representative of what people are actually reading on the internet.
So there's diversity of source, but some of these source there know where in our evaluation benchmark of ML people, right.
So these are like modes of the data set that the model is learning for nothing in terms of evaluation.
But we think this is actually more important than the data set somehow is representative of like what the human is accessing maybe more than the benchmark of today.
But that might mean that it might underperform on some benchmark because it's not really tailored around these benchmarks.
Can you talk a little bit more about the evaluation part of the process of the training process and kind of how you do that at scale when we're talking about language models.
Yeah, so on big science actually that's one of the parts that did not follow much.
There's so many parts, so I follow mostly data set and model and the evaluation was also a huge area.
So a lot of this, so they are like a little bit more than 40 working group in big sense.
Most of them are about like at least 40 60% so that there's a lot of activity.
A lot of them are actually submitting papers and there is a lot of side side research on evaluation.
They've been also doing a lot of research on evaluation itself.
So yeah, I think on this the best would be to read the paper that they have been just submitting.
I think last month there was a couple of papers submitted and they're also writing some today on bias.
I was seeing that, but yeah, I won't be able to give very good feedback. Maybe I can just say that we've been using the eluter AIRness mostly for our tests, which is I think a great, a great framework.
So just a quick shoot out to them. Good work on this.
Nice. So speaking of Luther there, another kind of distributed group that's trying to create open source language models.
How do you differentiate what you're doing with big science? Besides from the community dynamic that you're trying to create and the process is the expected result.
Going to be something that you think of as comparable or are there more fundamental differences in the way that you're thinking about the problem.
So I mean, there is a lot of interaction. I think many, many people from eluter are very active.
If we look at the papers that are already been out from big sense, you will see there are eluter, eluter papers that are very active, but there is also a lot of them Leo.
So there is really like a big overlap and we try to share like knowledge.
I think the main difference. So one will be that we were from the beginning targeting a multilingual model.
I think that was important for us because one aspect that I found a bit frustrating is that there is this super strong dominance of English in this field right now in the large language model field in particular, maybe Chinese as well, if you want.
Another aspect is really that we wanted, we tried to make a project, a workshop where we could have a lot of academic people.
So from the beginning, we were thinking, okay, how can you build a collaboration where academics can participate along with the industry?
You know, a line of leads in incentives. So you have together a team from deep speed people. They are very involved. They are doing really great work in video people. So Google people. So, for instance, the evaluation part is a lot driven by some Google people, Facebook people as well.
That's probably actually the first project where Google Facebook and video Microsoft are all training a single model together.
I don't think this has existed in the past.
So we were thinking a lot, okay, how can you align the incentive of all these people so that it can participate together?
So yeah, I mean, that was the difference in kind of the meta direction, meta approach or goal that we wanted.
Or that at least I found important. But overall, I think it's nice if we have a couple of open source model.
Which I would find is a bit annoying if we have just one and then people we start overfitting on this artifact and thinking that I don't know some weird thing they have seen is a general rule while it may just be around the coincidence.
And so big science has been a big focus for you, but in terms of other activities of the science team and research group, you mentioned multi modality as an area that you're interested in.
Can you talk a little bit about what you're doing there?
Yeah, I think that's kind of a natural extension somehow right when you see that's even when you scale this takes model there's still a little bit of things missing.
You want to extend to also modality so one of the project we would like to tackle now is like a bit the same multi modality and here the same I've seen that there's a lot of models coming out in the recent month from a lot of companies.
And most of in most of them we see the same problem like the data set our private the model then to be private so I think here again there is kind of a need to to come and to say hey, maybe we can share things more maybe can build on the shoulders of each other.
But more generally I think this this kind of scaling that we've seen in LPE should be applied and interesting in also domain as well I think this kind of your short performance we would like to see them also in vision we'd like to see them also in speech.
And so that's the area we want to explore in the coming month and cannot relate it to this is the extension to when you have multi modality at some point you would like even to interact and so on body environments is the natural extension.
And he also I think this is very exciting area and maybe today is a good is a good day to start tackling this thing today because we're all talking about metaphors and things like that are.
Yeah, no, not really I mean I like the idea of the metaphors I'm a bit afraid it might be kind of Facebook only then they don't make this a bit less collaborative community endeavor but I think the general idea of of using visual environment or things like that are interesting also because I'm always very frustrated how this this model language so differently than us human right we learn language in action we learn language in situation.
We flag all this kind of multi model and even even interaction and I think at some point if we could bring the way we train this model a bit closer to the way human language we might also be able to solve this kind of inconsistency that we get when we don't really understand why the model is failing here and so that that's probably a direction but more generally I think also just simulation and sensitivity data are interesting on many many.
So, for instance, when you think about bias and like PII and all things if you could build your data set yourself on control it and at the same time having a diversity which is wide enough to learn the concept that you want reliably it would make a lot of like ethical and social aspect easier in my opinion.
Have you done have you do you have projects like active projects around that I'm curious how you know practice how you go about doing that.
I wanted to dig a little bit deeper into kind of constructing these data sets such that they are sufficiently diverse after what you're trying to do but also you know unbiased and I'm thinking about yeah I'm just want to learn from your experiences there if it's something that you that you're actively working on are there you know techniques.
Tools you know things that help you know can help someone do that so other than I'm and maybe a more like a naturally a more like a builder of tools you know I mean even big size it's just kind of a very big tool for the community in my opinion that's just it.
And the same for visual environment I think today it's it's very difficult if you're like an NLP researcher and you're like hey I would like to do some grounding it's really scary you need to you need to like use totally different tools than the one you're used to so for instance you need to switch to I don't know unity learn C sharp or even C++ if you want to ground unreal so there is like this huge.
A tool in gap I would say and tools are important when you saw you know like I don't know like part by torsho 10 so forth and they're really enabled research in themselves we if we didn't have them we would be like still really light years in the back so I think right now there's this main problem here like if you want to you to create synthetic data or virtual environment it's very difficult to do is and to do it in a flexible enough way so that's the first step.
That's the first step that we that we're doing right now trying to make a tool around this and these do that as as primarily easier interfaces for researchers and ML people to interact with or simpler environments you know kind of ground up meaning you know something some kind of way to make unity.
Something with unity is level of richness you know easier to use so that they don't have to do CC++ or is it you know coming up with a simpler model that you know still exercises the idea of grounding and embodiment.
Yeah I don't have the answer yet because the problem is still we have this first first version but they're very early but I guess in a few months in a few months you will see the answer hopefully if everything work well I think it's a mix of both ideally you would like to to be able at the same time to or in kind of a similar setup to try your model on simple grounding you would like to try and simple I don't know to the world or even text world and then if you want to scale to like unity.
You would like to be able to do that quite simply and at some point which maybe even photorealism and transfer but along the way there's a lot of also research problem that are still open so the question also that you were hitting out which is a do we have today like a synthetic environment where for instance you can train a computer vision that can match.
Image net performance on of a resident train on image net I don't think today it really exists so this is more in my opinion some things we still don't know exactly about how to measure data set diversity on how to know exactly what is the day the diversity we need to have.
Maybe more fundamentally when you think about kind of embodied learning are you thinking about it from a language and NLP perspective primarily and what does embodiment mean in that context like when we're talking about you know reinforcement learning and control problems and robotics like you know that.
Grounding connection is clear when we're talking about like grounding text and you know images and things like that that's clear what changes about.
You know an NLP or language model or something like that in an agent type of environment that.
You know fundamentally creates grounding yeah that's that's a good question well I mean there's a lot of you to see that and I don't think anybody has a real answer but in my opinion a lot of things we're missing is kind of this interactive thing around around in the world so it's both being in the world but also being interactive with with another agent or human like you can have a human.
So it's like learning it's like this usage based kind of way of learning language which is really at least how we human language which I think is one of the reasons you see kind of this weird effect with this language model that they have problem a little bit of problem theory of mind this like discovering their own.
The universe with the universe of somebody else because they never how can this interactive learning that you could have so some people are training more in text right you could learn also in dialogue just pure text but in the end you know that when you're talking about an apple there is like also an object which is the apple so if you can ground that also in this object that would be even better.
So part of the sounds like a big part of the idea around embodiment is just it's about like multi party interactions and creating data sets that are kind of natively dialogue based as opposed to embodiment somehow necessarily grounds like single party embodiment like grounds language or something like that.
Yeah you know I think I think as well yeah it's a lot about grounding in in interaction and then the big question is how can you make that efficient right you know that error is like really not that efficient we know that well you so so there is all this interesting question that are connected around me which is around this which is the development of now.
Some form of offline learning in RL let's start to work maybe a little bit all this idea around maybe being able to do transfer learning also think we we often associate like virtual or simulation of our month with RL but they can also very much be used with supervised learning or unsupervised learning like we do here we you don't have to have like just a scalar we will you can have a lot more information.
So I think we start to break that that's I think what is super interesting maybe the end of last year beginning of this year we see a lot of bridges that are breaking between NLP and vision between maybe RL and NLP and vision and that's super interesting I've not seen that.
Fuck yeah I've never really seen that in the AML field that all these people are switching so easily you see NLP people starting to the computer vision you see how well people doing on body learning with like interaction and LP.
And you mentioned the convergence of two lanes in particular transformers is being a big driver of that.
And the simplicity of the transformers products actually make us more interested in data and environments and complexity and diversity which is really in the end which what's matter so we also.
Stopping a little bit with all the architectural complexity we were adding on top of our LSTM going back to a very simple architecture and so we can focus now to maybe things that are more important which is how what is our training procedure and what is actually our inputs and.
Yeah all these questions are there things that you've come across that you find to be like the you know the harder parts of really understanding transformers or things that like folks don't sufficiently understand you know in your view or things that surprise people when they learn.
I think transformers are actually much more simpler to explain than all the LSTM or that we had before I think the well I mean just like everybody I guess the first time I was reading transform paper I was like ooh this is super super complex but now if you start playing if your code when yourself you're like oh this is actually a very simple architecture in the end and it's getting simple and simple yeah but I think what most people are still kind of missing but we also see the shift today is this shift.
To what I was saying data right but on doing and people are that are also pushing and saying hey data some trick where I really think it's something we need today to to pay more attention to this part of the pipeline and to this part and here we still lacking tool also according to me we we lacking data measurements tool we liking often when people.
People put out data say they mostly say it's like X billion tokens and here is a rough idea how I how I gathered it and that's it but if anybody was like publishing a model and just saying it's like X billion parameters and he is a rough idea of how it was trained but I never I didn't evaluate anything I didn't do any measurements that would seems really strange but that's kind of what we do today with data said we don't measure them we don't really analyze them.
So that I think here there is a lot of room to build interesting tools and interesting insights.
Everyone's methods in that direction like the data cards for data sets and things like that sounds like you're talking about more concrete tools that you download and point at your data set and they.
Yeah so make me tell yeah for instance that there was this effort last year with make me tell such a such a journey you seen Angie at attacking face on building a data data measurement tool that kind of tried to give you insight but that that's really the beginning I think this can be could be developed a lot more.
Yeah I agree very much here with you.
Yeah is that tool can you talk a little bit more about that tool and kind of what it what it did and where it's edges work.
Yeah so it's a tool where you where you can input your data sets and it give you a lot of different metrics on like evaluation of your of your data and you can also use that to compare to data set together.
So you have some aggregates metrics like the zip the zip load you know this is like the most frequent words and this kind of help you to know is your data set kind of following I don't know stand out English or is your data set kind of strangely biased.
And you have tools to see some correlation between words and you can see some for instance bias information is male always correlated with doctor or this kind of thing or like more complex you have also very simple thing that actually people don't do much which is just looking at the duplicates.
Example and when you take many common data set I don't know like squad or thing like that you see that there are many duplicates actually and just this simple thing should be a requisite to investigate why do we have duplicates or strange entries.
You have tools around perplexity that this is a way to tell a little bit maybe where what domain is your is your data set which kind of domain is it's more on but then it's just it's just the beginning and the idea is to add more metrics and to also contextualize this because it's a bit new so when you see I don't know that on this this kind of data set metrics have this score what you would like to understand is is this good or not is my data set actually.
Wrongly selected on does it have some some problems that I don't understand or not so I think there's a lot of work around us to contextualize this and basically generally education understanding how you can understand your data.
If you look at the data measurements tools that's a measurement tool that's that's the first one that that came out got it got it so I'll get those links from you and we'll make sure to include them in the show notes page.
I also wanted to ask about what you're working on in retrieval space from a research perspective how are you thinking about those problems and.
So this is also super interesting that's also major frustration with today's model right so so when you take I don't know the one so so for instance we've been talking about GPT 3 it still doesn't know really about covid and you're like well it should be very nice to be able to update this model because that's a huge part of our life and the same with birth which still think that Trump is pretty don't and so.
This model are like this static thing and we would like to make them able to evolve and retrieval is one way I think today it's maybe one of the most interesting but.
The other way so yeah this is one way to to both being able to to make your model learn some some things and also to have some maybe a.
Better understanding on how how your model produce its output at least you you can have a little bit more information by by knowing what it was a querying in the database.
Also thing today we we do a lot of retrieval in NLP but maybe could also be just like when we just like what we've been saying it could also be maybe interesting in vision or like other other fields as a general way to to retrieve some kind of.
Static memory at least something that the model can use as an endpoint so yeah so so we have a couple of people newsrimers is mostly leading here.
And then there is a couple of interns mostly at the moment working on on different topic yeah and is your model from a research perspective kind of along the same lines or along the traditional you know publish.
Or your you know how do your goals you know differ based on where you're at sounds like a big part of it is.
You know producing open source data sets I imagine producing open source you know models and other things as well as a big effort is that in addition to publishing papers is it you know does a swing a little bit more towards open source than papers.
Do you think that's the right way to do it do you know how do you think about the whole space yeah no that's right that's that's something I spend a lot of time thinking.
I think the focus on papers is is good on some aspect with like there is some serious thing when you write a paper you like go back to the literature your sites really want work.
But there is also a bit of overfitting right when you just write a paper to just write a paper for this deadline and you need to you need to I know to have a paper so.
At hugging face because we think that tools are as important as research basically and that they can go together we focus more on like.
General general notion of an artifact so maybe at the end of your research what you have produced is a model maybe to data set maybe it's a paper or maybe to a code tool.
Or maybe it's even just yeah it could be some reflection or like something around I don't know the ethics or the social impact or something that could be in a block post and I could have a huge impact but I don't think.
You need to have a paper to have huge impact I think these are more like side product of what you what you're doing actually.
You're doing something and at some point you're like okay let's write a report and oh we have this deadline we can submit it there in this conference.
We focus more on what you what you progress in I kind of a back full artifact out of what you what you what you do as a research more than producing papers it let you also I think think a bit longer if you want to to make a one year project.
It's a bit strange to to think in terms of a one year paper that's a bit depressing maybe even.
When you think about you know how far the companies come from chat bots you know this is kind of at the same time that.
And LP broadly is evolved tremendously it is now you know greatly impacting other areas of machine learning like where do you see it all going you think about you know natural language understanding generation you know all of that stuff.
I don't know I really feel like we're at the beginning again of of like something we think about this year yeah I think there's so many open door I think yeah.
So obviously we we had to make I remember we had to make a choice on when so Victor Victor San is one of our lead scientists.
When he joined he was like really strong in computer vision and I told him yeah let's try NLP this is this kind of under understood it field but that's also nice.
And now I'm trying to do the opposite I'm like now you you're really into NLP but let's let's extend let's come back to let's come back to computer vision it's also nice but I think computer vision there's a lot of things you can do.
And the most interesting I think just like NLP are just about reinventing the field and thinking hey we're training on this fixed to the images but is it actually what we would like to do as a vision object detection or like kind of thing.
Do we don't want to more like try to understand the 3d world for themselves either even 40 with like things that are moving and so what is very interesting I think today is.
Stop being a and say hey maybe we need to reinvent this field and stop I should just stop maximizing my metrics on this benchmark but I should maybe reinvent the benchmark itself and it's really.
So the same is happening in Lp I think with like this blurriness between this task that would historically very separated you had like test classification or question answering of this thing that were very separated.
And with this large language model you have one model like an attacker only thing that can kind of question your.
The boundaries between even this task is does it really has a sense outside of the data set that you're using is your task activity something that's really well defined.
It's less and less clear but that's even more interesting because then you ask maybe more fundamental question around why you why you do an Lp in the beginning right.
Yeah maybe to close out I want to give you an opportunity to talk about your book the book you co author natural language processing with transformers I think it just.
Hit virtual bookshelves couple days ago.
Yeah there was a process long in the making so it's really a lot of the work to is is a Lewis and Leandro and Leandro's work they've been really the one like starting the ID for a long time I was people asking me like I mean publisher was saying you should write a book and I was like no too much time investments it's really.
And Lewis and Leandro came and say hey we've already wrote the chapter can you read it what do you think you want to join us as an author and I read the chapter of like yeah that's actually like really really really really interesting like the way you're right.
And so and so we started that now one year and a half ago like spreading the chapters between each other that was very long we wanted to make this in the way that we that we've love to read to read recent book which is book with like also code notebooks where you feel like you're actually learning at the same time kind of a fundamental question like for instance what is good evaluation how do you work with like.
A few data say a few labels or even zero shots like this fundamental question but also at the same time that you feel like you you've implemented something so every time we like take a data set and kind of work together like tackling all the problems and even at this other at the last part is like training a very large language model.
We also like build data set together from from GitHub and make a code kind of a copilot or like a code parrots we call it together on how you scale this so they it it range from like very small language models like distilled bird very efficient how you quantize them to like very huge one billion parameter model that that's trained on code.
So yeah very I'm very happy about the results and I'm super excited I'm the only one who didn't receive the physical copy yet so I'm still kind of waiting to to get it in my hand and feel that it's real.
I tried to order one and unlike I don't think I've ever seen this before it was like a prime book and they said you know the delivery time was long like you know you'll get it between February 24th and March 5th.
Oh yeah yeah that's because like it's in hot demand and they can't get enough copies from all right they hope as well yeah we don't know yet to obviously because we have no we don't have the first the first numbers but I mean anyway I'm I'm very proud of of this book I think this was a very a very great collaboration and actually after the during the collaboration Luis and Léon don't join her in face as a consequence is and Lewis has been now working on that.
Of course because we also have a course that's hugging face which is kind of why it's it was not written as same time as the book but the the idea is it's a bit the same but really more code oriented so it's really how to use all the other system all the data set transformer.
I've heard really good things about the course in the way it's organized.
Yeah yeah I think it's great and then the last part is also being finished now which is more a bit more extended but all the the age chapters that are that are out now give you a very good grounding if you want to start like using all these tools yeah.
Did I read correctly that the book was written using the the kind of fast AI tool chain that germans of values for.
Yeah definitely yeah so I mean Sylvan is also working at the face so this is all these people know each other and we are very great admirers of what Jeremy has done.
So yeah we use this this this same thing the NVDA and the notebook environments and it's nice because now we can have all these notebooks all this collapse that are directly directly outside to play with.
And one more question on the book the you know O'Reilly is known for these animal covers on their books and this one is a parrot.
Is that a nod to the stochastic parrots paper and Meg you know the Mexico author.
Yeah no that was the best surprise of the book right because you can't choose the animal you have no choice.
Yeah and when they said yeah here is your animal I was like oh that's that's really great and that's the reason we decided to name the so this big code model that I was talking about the one billion that we trained together in the book in the last chapter.
We decided to name it code parrots because that's that's what these models are in the end you know that's this today.
Yeah awesome awesome well Thomas thanks so much for joining us and sharing a bit about what you're up to and thinking about working on thanks.
