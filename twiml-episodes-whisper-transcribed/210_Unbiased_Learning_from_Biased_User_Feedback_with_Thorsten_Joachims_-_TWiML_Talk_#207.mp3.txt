Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
For this the final episode of our reinvent series were joined by Thorsten Joachim's professor
in the Department of Computer Science at Cornell University.
Thorsten participated in the conference's AI Summit, presenting his research on unbiased
learning from biased user feedback.
In our conversation we take a look at some of the various ways that inherent biases
are introduced in recommender systems and how to avoid them.
We discuss how inference techniques can be used to make learning algorithms robust to these
types of biases and how they can be enabled with the right types of logging policies.
Don't forget, next week I'm in Seattle at CubeCon and I'd love to connect with any
listeners in the area or in attendance.
Take me up on Twitter at Sam Charrington or via email or the Twimble AI website.
And now on to the show.
Alright everyone, I am on the line with Thorsten Joachim's.
Thorsten is professor in the Department of Computer Science at Cornell University.
Thorsten, welcome to this week in machine learning and AI.
Thank you for having me.
It's a pleasure.
Absolutely.
So, before we dive in, I'd love for you to share a little bit about your background
and how you got interested and involved in machine learning and artificial intelligence.
I was actually quite accidental how I got to machine learning and how all of the started.
I got my degrees at the University of Dortmund in Germany and there was exactly one professor
who did artificial intelligence and that was always something that fascinated me.
And that was Katarina Morik and she did machine learning and so that's how I got into machine
learning.
But I very quickly realized that within AI, this is the, I think one of the most exciting
areas to be in because it really enables AI in a way that we can't really foresee and
that creates interesting new results that we just, I mean, that we haven't programmed
in our systems.
So, I think this idea of learning from data is just fascinating.
Absolutely.
Absolutely.
I think you have many in the listening audience that will agree with you.
And so, you were here in Las Vegas.
I'm still in Las Vegas.
You are back at Cornell.
You were here in Las Vegas presenting at the AI Summit at the Reinvent Conference.
What were you presenting on there?
Yeah, that's right.
So much of my work, actually both as a professor here at Cornell, as well as an Amazon
Scholar working in Amazon Music, has to do with learning from lock data, from interaction
locks.
And what I mean by this is the kind of data that we observe in our systems, kind of as
a side product of people using the system.
So it's like the locks of a search engine where you can see what people query and where
they click or recommender systems where, you know, we make recommendations and we see
whether people follow this recommendation, e-commerce systems, but also now reaching
more and more out into the physical world, like smart homes and self-driving cars.
Any time where people use the system and we receive feedback, I think this is one of
the most plentiful data that we have and it's certainly, I mean, it reveals the choices
that people make and that reveals a lot of knowledge about the world.
But at the same time, it's data that's actually pretty difficult to analyze because it's
biased in multiple ways.
And dealing with these biases, I think, I mean, that's what my talk was about and that's
what much of my research is also about.
How do we learn from this data and how do we de-bias it?
Okay.
I can give you a simple example of what I mean by bias here and bias has many different
meanings and actually many of these different meanings apply, but let me be very specific
about one of the meanings.
So think about a movie recommendation system.
Netflix released this big data set, now many years ago, actually, at this point.
And if you look at that data set and look at the average star rating that people give,
you're kind of in the four star region.
And if you think about it, well, I mean, that can be reflective of the kind of average
rating that people would give an Amazon, a Netflix movie, right?
So if I actually drew a random movie from the Netflix catalog and had a random user, a
uniformed random, rate that movie, that wouldn't be a four star movie, probably, right?
It would be much lower.
So if I look at the ratings that people provide, they're actually self-selected, right?
They're biased.
People rate the movies that they watch.
And of course, they watch the movies that they think they will like.
So if I look at the revealed ratings that people give, then that's a biased sample of
all the ratings that they would probably want to give.
Now that's a problem because if I don't account for this, I can be really far off in evaluating
how good my movie recommendation will do, a system will do.
And in particular, if I now think about what a recommendation system should actually do,
is it should actually change the distribution of ratings that the user gives, right?
What the user wants from a recommendation system is interesting new choices.
So if I now take all data that I have and then train a new recommendation system from
it, deploy that, the data that I'm gathering now actually will have a different distribution,
right?
It will be biased by the recommendations that the new system makes.
And so what I really want is, I have to account for the fact that if I train a system and
then field it, I'm actually intervening in the world and I'm changing the distribution
of my data.
I always have these kind of counterfactual problems that I need to solve of what will happen
if I change my system.
And that's what makes this learning from log data hard because what I observe is the behavior
on my old system, but what I really want to optimize is the behavior or the benefit that
people draw from my new system.
But for that, I don't have data yet.
So what I talked about in the talk is basically how to deal with these kind of bias and shift
problems.
This is a super interesting topic.
I remember a conversation that I had with someone on the podcast.
I don't remember how it came up or who it was, but we were talking about this issue of
bias and user rankings and user feedback.
And one of the comments that I remember making was that, you know, you kind of describing
this almost bias at the level of the data set, but there's also, you know, individual bias.
Like some, you know, I may be a, you know, my kind of average movies are three, but for
some people it's a two and for other people it's a four.
And I remember asking the person in this context that they've heard of kind of research
into how to deal with all of that and we didn't come up with anything.
So I mean, I imagine these are issues that you're very familiar with.
Yeah.
I mean, right.
There's, there's, as I said, bias has many different meanings in this context.
What I've been talking about is selection bias, really.
There's also this kind of, you could call this, you know, that people use a rating scale
in a different way.
And we've actually done research on that as well.
So in particular, that comes up when you do things like peer grading or peer reviewing.
And so that's right.
So one reviewer may use, you know, a scale from one to 10, like, you know, mostly give
like high ratings or low ratings.
Another user may, you know, kind of anchor the scale at some other place.
Yeah.
We've actually done work on that problem as well.
Although that's that's somewhat different.
And we've actually deployed this at the KDD conference in 2015 when I was one of the program
co-chairs and trying to actually de-buy as reviewer ratings to come at kind of fairer decisions
about which papers to accept.
But that's, that's slightly different this problem.
And when I'm talking about here is really more that it's about where we get data.
And you know, that there's a selection of, you know, what ratings we observe and which
ones we don't.
So the extreme case, I mean, this comes up basically in all online systems.
So and some of the extreme cases are, for example, an ad placement system.
And there it's really very clear.
So if I think about an ad placement system, let's say display advertising, then I have
my current system that's displaying ads.
And so a new user comes in to a particular page.
My current system now selects an ad to display.
And then for that particular ad, I get to see whether the user clicks on it or not.
But I don't get to see what would have happened if the system had presented a different ad.
So here I very much have a selection bias that the current system in production influences
where I observe feedback or think about a search engine where your current ranking function,
you know, in response to a particular query, a potential ranking.
And that really puts a strong selection bias where I get clicks, I'm going to get clicks
on the, you know, top few results.
You know, it's very unlikely that anybody is going to go to position 100 and reveal
any of the kind of, you know, whether something was relevant there.
So again, here we have that the current system that's in production really biases where
I get clicks.
And now if I use or where I get feedback and now if I use this data kind of naively to
do learning from what I'm basically just rediscovering is, you know, whatever my old system
did.
And so what we've been doing is to rethink how to use this data from actually from a perspective
of causal inference.
So let's let's take the ad placement system as the canonical example here, but it actually
applies to all of these systems.
Then the way that you can think about this is really not as a prediction problem, but
much more as a problem of, you know, just like what you have in like a medical setting
of applying a treatment.
So think, you know, if you are, if you want to come up with a personalized treatment
policy for a particular person, then you may have, let's say some lab measurements for
that person, then the doctor decides, you know, to give that person, you know, drug
A or drug B or the surgery, that's the treatment.
And then you get to observe for that particular treatment that was chosen, whether the person
gets better or not.
And it's really the same setup that we have in our online systems as well, like think
about ad placement, right?
The user comes in, we have some idea, you know, about the user profile.
Then we take an action, we apply a treatment, that's a particular ad that we place.
And then we see the outcome for that particular ad that was chosen.
But we don't get to see what would have happened if I had applied any of the other treatments,
any of the other ads.
And I can make the same story for almost any online system as well.
So it's really, you know, what we've been doing is we've rethought what it means to do
learning and online systems kind of from the perspective of learning a policy that makes
interventions and we want these interventions to have the desired effects in the world,
right?
That's what I meant when I said about the recommendation system, what I really want is
I want to make recommendations to the user and I want to change behavior in a way that
the user appreciates it.
So it's really learning, it's learning to intervene, it's not learning to predict.
And that has interesting implications for, for machine learning and it kind of puts
it into relation to problems like covariate shift and really causal inference, right?
We want to have a policy that makes interventions that cause some desired behavior in the world.
And so is the approach related in some way to the notion of like A, B testing or multivariate
testing where you're displaying multiple options to the user to give you in this case
to give you kind of more insights into what you may have missed out on?
Yeah, that's right on.
I mean, the kind of gold standard for doing causal inference is a controlled randomized trial,
right?
That's what the user meant.
And if you want to figure out whether drug A is rather than drug B, basically what we
do is we give some fraction of the population or of our patient's drug A, randomized, another
fraction gets, gets the other treatment and if we do the randomization correctly, then
that's a strong evidence of the causal effect of the treatment, right?
And randomization is the key here.
It's easier than it sounds.
It is actually a part of that it sounds easy.
It sounds easy.
It sounds easy.
In practice, it can be quite hard.
And basically what we're saying, I mean, what you're doing in an AB test and an online
system is exactly a controlled randomized trial, right?
Some fraction of your users gets, you know, a ranking function A or a recommended function
A.
The other one gets a recommended function B.
And then you can compare which one works better.
That's really the gold standard for causal inference, but it's also incredibly expensive.
What you need to do is you need to code up that, you know, new ranking function or policy
more generally.
Then you need to productionize it, you need to test it, and then you need to field it
on your system, you know, for at least a week because otherwise, you know, you have
circular effects from the week and probably even longer to get reliable results.
If you have a lot of different systems that you want to evaluate this way, it's going
to take you forever, and your machine learning development cycle is going to be incredibly
slow.
Or if you don't have a lot of traffic.
If you don't have a lot of traffic, right, I mean, basically the limiting factor is also
traffic, right?
It's developer's time for productionizing all of these policies, and then it's traffic.
We already have, you know, probably, you know, terabytes of old log data, existing log
data lying around.
And if you think about it, online AB testing is actually really wasteful.
We, you know, we put these policies into production, we collect the data, and then we never
reuse that data.
We use it exactly once, and then, you know, we don't actually know what to do with it.
So the kind of the intriguing idea that we've pursued in over the last few years is whether
you can actually avoid online AB tests, but instead do something like counterfactual or
offline AB tests.
And that's basically addressing the question of, here's a lot of old log data that we already
have.
And I'll, we'll have to qualify that this has to be somewhat special log data.
I'll come back to that in a bit.
But that we have a lot of log data lying around.
And that we can now do, answer the following counterfactual question.
If we have a new policy that we want to evaluate, and do causal inference on, that we asked
the question, how well would that policy have done if we had used it instead of the policy
that was actually running at the time.
So it's this counterfactual question.
But essentially it gets at the same, at the heart of causal inference again is, you know,
how good is this policy?
It can be used the existing log data to evaluate a new policy without ever having to field that
new policy in a new AB test.
And it turns out in certain conditions, it's actually possible to do this.
And if you're able to reuse old log data, you can evaluate a new policy, or let's say
a new ranking function when you recommend a policy in seconds.
And you can do that for many, many new policies.
And now, you know, you're basically now speeding up your development cycle or your evaluation
cycle from weeks that it takes to do an online AB test to seconds that it takes to do one
of these counterfactual or offline AB tests.
So the question of course is, when is this possible?
And it's possible to do this offline AB testing if the policy that you use to record your
original data, you know, there's terabytes of log data that you already had, if that
policy was sufficiently stochastic.
So that gets us back to controlled randomized trials, right?
There we also exploited that the assignment of people to or of subjects to conditions
was random, but it turns out that it doesn't have to be uniformly at random.
It doesn't have to be, you know, you flip a coin and assign people this way.
Any form of randomness, even if it's like, you know, 95% to 5% is sufficient to even
in hindsight, in retrospect, compute unbiased estimates of the performance of a new policy.
So that leads us into questions or into techniques from causal infants like inverse propensity
waiting.
And what this basically means is that you take your existing log data, that was collected
according to a stochastic policy, and you basically just re-weight distributions to your new
policy.
And if you do that in a particular way, you can actually prove that you can get unbiased
estimates of the performance of a new policy, basically the same number that you would get
in an online AB test, but just by reusing existing data under the right conditions.
And if you can do that, then you can also do learning.
Okay.
You've mentioned a couple times the, you know, the right conditions and the right ways
of doing these things.
How restrictive are these conditions?
If you want to evaluate a new policy, you can, you have to restrict that policy to actions
that had non-zero probability of being selected in the past.
And that's pretty intuitive.
If you have a new policy that picks actions that you could not have possibly ever taken
in the past, there's just no way to know whether that action is any good.
So the basic condition is that you can only evaluate policies on the actions that, you
know, you're logging policy that collected the data had basically non-zero probability
of choosing.
It doesn't mean that it needs to have chosen all of these possible actions, typically only
a very small subset of it, but it had to have non-zero probability to do it.
Because this condition derived from kind of like an information theoretic type of approach,
like you need to have this information in sufficient quantities in your log store, you
know, to practically do anything, or is it more, you know, from a probabilistic inference
perspective and, you know, base theorem and dividing by zero?
It's more actually, I mean, it's more something totally different.
There is actually, you never really get a dividing by zero problem.
You get it.
So there's two problems.
If you're trying to evaluate the policy that picks actions that weren't even available
in the past, there's just, do you have no information about them?
And so at that point, your estimate, your offline A-B test becomes biased.
It's no longer unbiased.
Just because, you know, you don't know what to do for these actions, so you're going
to impute basically a zero there, or, you know, whatever you want to impute.
And but that's kind of drawn out of the hat, and you don't know whether that's true.
So if you have zero probability, then you get a biased estimate, but you can avoid it
by restricting your new policy to only those actions that didn't have zero probability.
The second issue is that, well, maybe that action had a tiny probability under when you
locked the data.
And then you're dividing.
So if you do this inverse propensity waiting, you're basically dividing by that probability.
You're dividing your estimate by this very small number, and that drives up the variance
of your estimate.
You're unbiased, but you get an estimate that has large variance, which is also not good.
So there's kind of the fundamental issue of zero probabilities, and then you become biased.
And then there's the practical issue of if you have very small probabilities of having
chosen a particular action in your locks, then you become, you're still unbiased, but
you have large variance, and which is also not, you would have need a lot of data to overcome
that.
Right.
Right.
And so from the perspective of someone who wants to try to use this, how complex is
it?
The mathematics behind it are actually really simple.
In terms of designing of computation during this offline AB tests is easy.
It's more, as you said before, you know, things like making sure that the assignment is
random and computing or logging these propensities, conceptually, that can be quite tricky.
And then there's the question of learning, right.
Once you can do this offline evaluation, these offline AB tests, then basically you can
take existing learning algorithms like, you know, things like a conditional random field
or a deep network.
And instead of training them with kind of hand labeled data, you can train them with
log data because basically what all of these methods are doing is empirical risk minimization.
So they would, they're minimizing training error, right.
And training error is nothing like an unbiased estimate of your generalization error.
And typically we use, you know, the fraction of errors as our unbiased estimate of the generalization
error.
And that's what we use as training error.
But these inverse propensity score estimators are also unbiased estimators of my generalization
error. So I can basically just substitute my normal training error for which I need hand
labeled data with these IPS weighted types of training errors for which I can use log data
to train them.
So I can basically this way, I can repurpose many existing, as I said, like deep learning
or other learning algorithms, just by training them according to a different objective.
And that's something that we've been developing over the past few years as well.
So how can we train now based on log data, all of these methods, instead of having to
use hand labeled full information data?
And so is it the case that I'm only able to use this technique going forward in the
sense of, you know, once I start to generate my experiment in a way that's consistent with
the conditions we've discussed, you know, then I have access to all of this log data,
but, you know, likely if I've not been thinking about this previously, and I try to apply
it, everything that I have is, you know, either kind of a mess, or I don't have enough
applicable rigor, or my approach hasn't been systemized in a way that I can do this.
Or can you always kind of select, you know, a subset of cases for which your old log
data is useful, and then just work with that?
Right.
Yeah.
So it's certainly easiest if you're logging your data already in mind that you want it
to be stochastic.
So for example, a simple way of doing this is if you're currently using some deterministic
rule that maybe, you know, scores your candidates and then picks the candidate with the maximum
score.
That's, you know, how many ad placement systems, for example, work.
That would be a deterministic blogging policy, and that would be difficult to use.
But you can easily turn this into a stochastic logging policy by changing this arc max into
a softmax.
And then you would be logging new data where you know exactly, you know, the probability
of choosing each of these individual actions.
That's certainly the easiest.
And you refer to this as a logging policy, it's not the logging itself that you're changing.
It's kind of the underlying thing that you're doing, you know, displaying ads or recommendations
or what have you, and you're just logging what happens.
Am I interpreting that correctly?
That's right.
What I mean by logging policy is just whatever your system was, that was in operations when
you created the logs.
So it's the ranking function that you've used or, you know, the ad placement function or
the recommender that was actually in production when you recorded the logs.
Okay.
So yeah, it's easiest if that is explicitly stochastic and you made it stochastic in a way
that you can easily record all of the propensities and the probabilities of choosing the respective
actions.
That's great.
But in many cases, I would argue that the data that you have, the log data that you have
is already stochastic.
So for example, if you've been running AB tests in the past, then the assignment of users
to different conditions of your AB test, that was a random assignment already.
So it's already stochastic in a sense that any particular user could have gotten multiple
different versions of the system.
And that's stochasticity that you already have and that you can exploit.
Actually we've shown that in ranking settings where you have, let's say, a search engine,
there you can, even if you have a deterministic ranking function, you can exploit that your
users are stochastic.
So in particular, some users will go down to position three, some will go down position
five, some will go down to position 20.
And this stochasticity is something that you can actually exploit, even though your system
is deterministic, the logs are still stochastic because your users are stochastic.
To do that, you would then need to estimate something like propensity curve based on position.
But that's something we can do and we actually have an upcoming wisdom paper for how to do
that very efficiently.
It strikes me that there might be some implications on the effectiveness of this relative to whatever
your distribution is.
You kind of alluded to this earlier with the comments about the variants.
If you've got a distribution that has a wide variance or many choices, for example, do
all of these things come into play and your ability to learn and converge on a solution?
Yeah.
I mean, you're right on in a sense that if the system that logged the data is very different
from the system that you're now evaluating effectively, we often talk about effective
sample size in these settings when you do these counterfactual estimates.
If these two systems are very far apart, then it's quite intuitive that whatever is in
your log data doesn't tell you that much about the evaluation of this new system.
If they're very different, the overlap between the two is very small.
This will typically show itself in kind of having large variants in your offline AB tests.
So really there is practical limitations about what you can evaluate and how reliably
can evaluate.
This works best if the new system that you want to evaluate is not that far away from
the production system or from the logging policy that logged the data.
And there are diagnostics to kind of see how reliable you are here.
But you're right.
If your new system does something completely different from the old system, there's
just not enough information in the logs.
But it would argue in many cases, you're making incremental improvements, you're changing
your ranking function, you're changing a little bit, you're changing your ad placement
function a little bit.
So I'd say many of the kind of online AB tests that you're doing today could probably
be replaced by these types of offline kind of actual AB tests.
But then once in a while, you have to go out, collect new data probably.
And I would also say that before making a big launch decision, I would just double check
in an online AB test that really everything's fine and that there's nothing unforeseen
is happening.
But I think these offline AB tests are really interesting for speeding up your development
cycle that you don't have to go out, get new data for every little decision that you're
making.
But you just need to kind of get a reality check once in a while.
Does this impose a requirement on folks that are using this technique that they need to
keep track of their distributions in some way, like in their logs in a way that they're
probably not doing today?
Yeah.
So the biggest thing is probably that whenever you pick an action and you're logging this,
that you also log what's called the propensity, which is the probability of taking that action
under the current policy.
So the logging policy that, as I said, ideally is stochastic.
So it will pick actions from a distribution.
And when it makes its choice, you just record that one number of what the probability of
that choice was.
And that's the most important thing to log.
And with that number, you can, you know, that's the most important number for doing this
in first propensity waiting.
And it's really just one additional number that you need to log.
It's not that you need to log the whole distribution or anything complicated.
It's just this one number.
It sounds like a really interesting technique, does it fit into a broader array of tools kind
of based around a similar idea that folks should be looking into?
Yeah.
I mean, this idea of doing this counterfactual evaluation has been something that, you know,
especially also people at Microsoft, but now much more broadly in it has been accepted
in industry, you know, there were very interesting papers at the last Rex's conference on both
from YouTube, implementing this kind of counterfactual learning techniques from Spotify.
So I mean, this is an area that is maturing and where there's actually now a lot of interest
both in academia and also in industry on how to design better estimators, how to design
learning algorithms that are robust in this setting.
Because arguably, really, that's where the data is, right?
That's where we have lots and lots of training data.
And it's much cheaper to work with this data than to get, you know, hand labeled data.
Very interesting, Seth.
Were there any other things that you covered in your talk at the conference?
One thing that I would love to have covered more, but I didn't have the time is actually
the, you know, the other meaning of, of bias.
And that's the meaning in terms of fairness.
Because I think actually all of these questions are quite related.
So once you think about, you know, what your system does not as prediction, but as a policy
that has effects in the real world and that has desirable effects and undesirable effects
that you may want to optimize or minimize, then really thinking about, I mean, that is
really the right vocabulary to think about fairness of your policy as well, right?
And we've been thinking, I mean, that's actually been very interesting for me personally
in terms of my research.
And it's really enabled me to talk about fairness, let's say, of a search engine in a much more
concise way.
So for example, you know, if you think about what a search engine does, it's really a
system that, you know, allocates attention among the items that it ranks.
So from that perspective, you know, it's a policy and it's a policy that has an effect
on both the users of the system.
Those would be the people typing in the query.
But it's also a policy that has an effect on the items that are being ranked, right?
And in particular, in the way that this policy allocates exposure, you know, things that
get ranked to position one, get more exposure than things that are ranked to position 10.
So now, if you think about it that way, what we want is we want policies that allocate
exposure in a way that is fair.
And what's fair, it's both, it has to be fair both to the users as well as to the items
being ranked.
Because, you know, if you think about the settings where we use rankings today, the items,
you know, could be people that are candidates for a job.
And so we want to make sure that, you know, that we don't, for example, if they're protected
groups among our job candidates, that we're allocating exposure in a fair way and we're
not biasing how the system allocates exposure.
What was actually, what's interesting is an information retrieval going back to the 70s
coming out of library science, there was this very strong focus on maximizing the utility
of the system to the people who type in the query.
And that was fine when we were ranking books in the library because, you know, there it's
really more mostly about the, you know, the people coming to the library wanting to find
books.
And the books really didn't have many rights or needed much protection.
It was really a tool for finding what people wanted.
But now when we're ranking, let's say job candidates or, you know, ranking romantic,
potential romantic partners or, you know, we're ranking anything, it's really that we
have to rethink what it means to design a ranking system.
And this old principle of ranking by probability of relevance is actually not necessarily fair.
I mean, we want to rank based on marriage and married equals relevance, but we can allocate
exposure in many different ways.
So current retrieval system basically are a winner takes all types of system wherever
claims the first spot is, you know, getting that by far the most attention.
But let's say for ranking job candidates, if we have two candidates that are, let's
say we have 10 candidates that have almost the same qualifications for the job, one person
is absolutely better than let's say the other people.
Is it really fair that that epsilon better person gets far more than epsilon more attention
than all the other 10?
I'm not sure, right?
So we may argue for any particular situation that we actually, we still want to allocate
exposure based on relevance, but this kind of winner takes all is not maybe not the right
way.
Maybe we want to actually make exposure proportional to relevance, which would mean that
all of these 10 candidates would get almost the same exposure, just this one person gets
like a little bit of epsilon more, but it's not a winner takes all system anymore.
So I think many of the techniques of debiasing data that I've talked about here for selection
biases and propensity waiting are actually extremely useful for also dealing with fairness
in this settings.
And so I think there's an interesting connection there.
Okay.
Is that later interpretation something that you've published on today?
Yeah, that we had a KDD paper at the last KDD conference.
It sounds like really interesting work and I'm really thankful for you for taking the
time to share it with us.
Yeah, it was fun.
Thank you.
Absolutely.
Thanks, Thorson.
All right, everyone, that's our show for today.
For more information on Thorson or any of the topics covered in this show, visit twimmalei.com
slash talk slash 207.
As always, thanks so much for listening and catch you next time.
