1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,320
I'm your host Sam Charrington. For those challenged with promoting the use of machine learning

4
00:00:34,320 --> 00:00:40,760
in an organization and making it more accessible, a key to success is to support data scientists

5
00:00:40,760 --> 00:00:46,320
and machine learning engineers with modern processes, tools and platforms.

6
00:00:46,320 --> 00:00:51,480
This is a topic we're excited to address here on the podcast with the AI Platforms podcast

7
00:00:51,480 --> 00:00:56,000
series that you're currently listening to, as well as a series of e-books that we'll

8
00:00:56,000 --> 00:00:58,080
be publishing on the topic.

9
00:00:58,080 --> 00:01:02,760
The first of these e-books takes a bottoms up look at AI platforms and is focused on the

10
00:01:02,760 --> 00:01:08,160
open source Kubernetes project, which is used to deliver scalable machine learning infrastructure

11
00:01:08,160 --> 00:01:12,840
at places like Airbnb, booking.com and open AI.

12
00:01:12,840 --> 00:01:17,640
The second book in the series looks at scaling data science and ML engineering from the top

13
00:01:17,640 --> 00:01:23,720
down, exploring the internal platforms, companies like Airbnb, Facebook and Uber have built,

14
00:01:23,720 --> 00:01:26,600
and what enterprises can learn from them.

15
00:01:26,600 --> 00:01:31,280
If these are topics that you're interested in, and especially if part of your job involves

16
00:01:31,280 --> 00:01:36,880
making machine learning more accessible, I'd encourage you to visit twimbleai.com slash

17
00:01:36,880 --> 00:01:44,640
AI platforms and sign up to be notified as soon as these books are published.

18
00:01:44,640 --> 00:01:49,800
In this episode of our AI Platforms series, we're joined by Lima Nasseri, senior engineering

19
00:01:49,800 --> 00:01:54,000
manager and head of the recommendations team at Comcast.

20
00:01:54,000 --> 00:01:58,520
Lima spoke at the strange loop conference a few months ago on resurrecting a recommendations

21
00:01:58,520 --> 00:01:59,920
platform.

22
00:01:59,920 --> 00:02:05,040
In our conversation, Lima and I discuss how she and her team resurrected Comcasts Xfinity

23
00:02:05,040 --> 00:02:11,760
X1 recommendations platform, including rebuilding the data pipeline, the machine learning process,

24
00:02:11,760 --> 00:02:15,320
and the deployment and training of their updated models.

25
00:02:15,320 --> 00:02:20,400
We also touch on the importance of A-B testing and maintaining their rebuilt infrastructure.

26
00:02:20,400 --> 00:02:24,120
This is a very fun interview, which I know you'll enjoy.

27
00:02:24,120 --> 00:02:26,280
And now on to the show.

28
00:02:26,280 --> 00:02:32,120
Alright everyone, I am on the line with Lima Nasseri.

29
00:02:32,120 --> 00:02:35,640
Lima is a senior engineering manager at Comcast.

30
00:02:35,640 --> 00:02:38,640
Lima, welcome to this week in machine learning and AI.

31
00:02:38,640 --> 00:02:40,800
Hi Sam, thanks for having me.

32
00:02:40,800 --> 00:02:44,800
Also, why don't we get started by talking a little bit about your background.

33
00:02:44,800 --> 00:02:52,000
You've been at Comcast for nine years, and as of about 14 months ago, you have been leading

34
00:02:52,000 --> 00:02:54,640
the recommendations group there.

35
00:02:54,640 --> 00:02:57,600
What are you doing in and around recommendations at Comcast?

36
00:02:57,600 --> 00:02:58,600
Yeah, sure, definitely.

37
00:02:58,600 --> 00:03:04,360
Yeah, so I've been here for a while, most recently, so the past 14 months, like you've said,

38
00:03:04,360 --> 00:03:06,840
I switched over to leading the recommendations team.

39
00:03:06,840 --> 00:03:09,800
So when I did that switch, it was a really small team.

40
00:03:09,800 --> 00:03:13,040
Other companies, their recommendations team, or really their personalization team, is

41
00:03:13,040 --> 00:03:17,800
one of the most important and biggest part of their content discovery platforms or content

42
00:03:17,800 --> 00:03:18,800
discovery teams.

43
00:03:18,800 --> 00:03:21,920
At Comcast, that wasn't quite the case.

44
00:03:21,920 --> 00:03:24,080
About 14 months ago, it was about three people.

45
00:03:24,080 --> 00:03:29,920
The main focus, unfortunately, at the time was just training a model and then deploying

46
00:03:29,920 --> 00:03:35,120
that model in production to surface predictions or to surface what people should watch on

47
00:03:35,120 --> 00:03:37,480
cable in one way or another.

48
00:03:37,480 --> 00:03:42,320
It was a one size fits all model, so essentially, we had one model that did everything.

49
00:03:42,320 --> 00:03:47,520
So regardless of the context in which the quote unquote predictions were being served,

50
00:03:47,520 --> 00:03:52,680
we still use the same model, and we didn't tweak it as much as we should, based on where

51
00:03:52,680 --> 00:03:58,400
it was being surfaced on X1, X1 being our content discovery platform for cable.

52
00:03:58,400 --> 00:03:59,560
So anyway, so it was a small team.

53
00:03:59,560 --> 00:04:02,960
So it was pretty much like an engineer on the team, less management, more engineering,

54
00:04:02,960 --> 00:04:07,480
which was fun for me, and it's tripled in size since then, which is great.

55
00:04:07,480 --> 00:04:12,920
But the goal of when I switched over to that team was to essentially resurrect the recommendations

56
00:04:12,920 --> 00:04:19,200
platform, which sounds kind of morbid, but I like the eliteration, so that's the name

57
00:04:19,200 --> 00:04:22,240
of my talk, strangely, resurrecting a recommendations platform.

58
00:04:22,240 --> 00:04:23,240
Nice.

59
00:04:23,240 --> 00:04:30,080
And so the focus of your talk is, what, how you kind of went in, a commando style and

60
00:04:30,080 --> 00:04:36,480
like, essentially, essentially, so the focus of the talk is really building a end-to-end

61
00:04:36,480 --> 00:04:38,440
machine learning platform.

62
00:04:38,440 --> 00:04:42,240
And coming from my background, I majored in computer science, and then I didn't obviously

63
00:04:42,240 --> 00:04:43,960
start in AI and machine learning.

64
00:04:43,960 --> 00:04:48,200
I instead started in the web service tier, so I built web services that got millions

65
00:04:48,200 --> 00:04:54,000
of requests per day, as you can imagine, like we get the scale at Comcast is pretty high.

66
00:04:54,000 --> 00:04:58,440
So having that background of building web services and platforms that scale really helped

67
00:04:58,440 --> 00:05:04,160
this team to go from simply having this really old school platform where it was a bunch

68
00:05:04,160 --> 00:05:09,480
of machine learning jobs that ran on physical infrastructure that took hours and hours to

69
00:05:09,480 --> 00:05:15,200
complete, to building that to be more event driven and real time updates, and also like

70
00:05:15,200 --> 00:05:20,760
owning the end-to-end data collection, you know, requests from the client, everything.

71
00:05:20,760 --> 00:05:22,080
That's essentially what we did.

72
00:05:22,080 --> 00:05:27,160
We went from owning just the model to owning, surfacing the recommendations, training the

73
00:05:27,160 --> 00:05:30,520
model, evaluating the model, and then having multiple models in production.

74
00:05:30,520 --> 00:05:35,840
I thought it was interesting when you describe the world that you walked into when you

75
00:05:35,840 --> 00:05:40,760
transitioned into this role as, unfortunately, we just had a model and we're making some

76
00:05:40,760 --> 00:05:45,560
recommendations with machine learning, like a lot of companies are, you know, trying

77
00:05:45,560 --> 00:05:46,560
to get to that.

78
00:05:46,560 --> 00:05:47,560
Right.

79
00:05:47,560 --> 00:05:48,560
That's a really good point.

80
00:05:48,560 --> 00:05:54,080
I think the unfortunate aspect of that was, as an engineer, the code and the infrastructure

81
00:05:54,080 --> 00:05:56,960
was not where I wanted it to be.

82
00:05:56,960 --> 00:06:01,560
For example, and you'll hear this in so many machine learning talks where they talk about

83
00:06:01,560 --> 00:06:04,840
the end-to-end, the infrastructure is probably the most important part.

84
00:06:04,840 --> 00:06:13,160
And at the time, we had like a 13-node cluster running community addition of our Hadoop distribution.

85
00:06:13,160 --> 00:06:16,440
So if there were any problems, we couldn't call anyone.

86
00:06:16,440 --> 00:06:21,320
And again, it was MapReduce, which, you know, one significant benefit of MapReduce is

87
00:06:21,320 --> 00:06:22,320
all I owe.

88
00:06:22,320 --> 00:06:24,400
So as long as you have space, you really fail.

89
00:06:24,400 --> 00:06:26,960
But again, that meant that the jobs ran forever.

90
00:06:26,960 --> 00:06:32,520
I mean, we had a job that we recently deprecated, which was exciting, that literally took 18

91
00:06:32,520 --> 00:06:33,520
hours.

92
00:06:33,520 --> 00:06:38,840
So it's still responsibility was to sessionize data so that it could feed our model.

93
00:06:38,840 --> 00:06:39,840
Wow.

94
00:06:39,840 --> 00:06:40,840
Yeah, 18 hours.

95
00:06:40,840 --> 00:06:44,240
I mean, imagine what you can get done in 18 hours, and this job still wasn't done.

96
00:06:44,240 --> 00:06:45,240
Wow.

97
00:06:45,240 --> 00:06:46,240
Right.

98
00:06:46,240 --> 00:06:50,280
So walk us through this process of resurrecting this recommendation platform.

99
00:06:50,280 --> 00:06:51,280
Yeah.

100
00:06:51,280 --> 00:06:52,440
That sounds good.

101
00:06:52,440 --> 00:06:57,280
So kind of in my mind, when I first took this on, I broke it down to four main parts,

102
00:06:57,280 --> 00:06:58,520
the data platform.

103
00:06:58,520 --> 00:07:02,120
So we wanted to build a proper data pipeline to consume the data into our ecosystem so

104
00:07:02,120 --> 00:07:06,280
that we could easily aggregate those usage events of what customers are doing on our

105
00:07:06,280 --> 00:07:09,040
platform so that it could feed a model.

106
00:07:09,040 --> 00:07:13,520
And you'll ask anyone that does anything with machine learning, the data is sometimes

107
00:07:13,520 --> 00:07:15,520
the hardest part.

108
00:07:15,520 --> 00:07:20,160
Sometimes it actually hurts me to hear, I'll hear researchers at Comcast say that they're

109
00:07:20,160 --> 00:07:21,160
working on a model.

110
00:07:21,160 --> 00:07:25,360
And then I hear them talk about the pain that they take into creating the training data

111
00:07:25,360 --> 00:07:26,360
set.

112
00:07:26,360 --> 00:07:27,800
So that's what we were trying to abstract.

113
00:07:27,800 --> 00:07:29,960
So the data platform was the first part.

114
00:07:29,960 --> 00:07:36,320
And this is without going too deeply into this part before you walk through them.

115
00:07:36,320 --> 00:07:46,040
I'm imagining that much of what you're dealing with is kind of the traditional telco, cable

116
00:07:46,040 --> 00:07:51,600
service provider type of data sources, provisioning systems, and that kind of thing.

117
00:07:51,600 --> 00:07:53,600
Or are you dealing with any of that stuff or is this?

118
00:07:53,600 --> 00:07:54,600
Definitely not.

119
00:07:54,600 --> 00:07:59,640
No, it's actually, I think it was a lot of, it's what people are doing on the platform.

120
00:07:59,640 --> 00:08:03,320
So it's just the interactive side, you don't have to talk about that.

121
00:08:03,320 --> 00:08:07,560
Yeah, like what they're clicking on, what they're watching, what they're not watching,

122
00:08:07,560 --> 00:08:10,120
what they get surfaced, but then are ignoring.

123
00:08:10,120 --> 00:08:11,880
So it's our content discovery.

124
00:08:11,880 --> 00:08:14,040
So which do they watch on TV?

125
00:08:14,040 --> 00:08:15,040
Some more of that side.

126
00:08:15,040 --> 00:08:16,440
Definitely not the old school cable side.

127
00:08:16,440 --> 00:08:17,440
Okay.

128
00:08:17,440 --> 00:08:20,240
I try to stay away from that part.

129
00:08:20,240 --> 00:08:24,480
So yeah, so breaking it up into four pieces, there's the data, the data tier.

130
00:08:24,480 --> 00:08:27,080
And then there was just orchestrating the machine learning aspect.

131
00:08:27,080 --> 00:08:31,800
So easily orchestrating the key steps that are obviously required to build a model and

132
00:08:31,800 --> 00:08:33,960
then foresee that model into production.

133
00:08:33,960 --> 00:08:38,160
That was actually one of my goals which we achieved was to have more than one model.

134
00:08:38,160 --> 00:08:43,560
And that's not for a big company like this, that's not a crazy thing to have, to have

135
00:08:43,560 --> 00:08:49,560
a model that's fine tuned to suggest content that is available for rent versus having a

136
00:08:49,560 --> 00:08:54,880
model that's fine tuned for suggesting music recommendations for customers that have

137
00:08:54,880 --> 00:08:58,600
a higher propensity to watch music videos on on Comcast.

138
00:08:58,600 --> 00:09:04,160
So orchestrating the process and training, evaluating, and then of course deploying.

139
00:09:04,160 --> 00:09:07,680
The model was the second part that we had to do because previously we lived in a world

140
00:09:07,680 --> 00:09:11,280
where we just had one and we just had to make sure we had to keep the lights on that

141
00:09:11,280 --> 00:09:15,720
one machine learning, like quote unquote, that one.

142
00:09:15,720 --> 00:09:19,640
So yeah, so the ability to experiment and deploy was the second thing that we wanted

143
00:09:19,640 --> 00:09:20,640
to tackle.

144
00:09:20,640 --> 00:09:23,240
And the third part was AB testing.

145
00:09:23,240 --> 00:09:29,000
So before I switched to the recommendations team, I actually worked on a big data platform

146
00:09:29,000 --> 00:09:31,240
which I could use that experience for this team.

147
00:09:31,240 --> 00:09:32,960
So it worked out fairly well.

148
00:09:32,960 --> 00:09:36,600
But while also on that team we built an AB testing platform which meant we essentially

149
00:09:36,600 --> 00:09:40,160
created a system that tagged accounts.

150
00:09:40,160 --> 00:09:44,440
And then if you tag like or if you tag users then you should be able to give them different

151
00:09:44,440 --> 00:09:49,000
types of experiences based on that tag, you know, super high level, AB testing logic.

152
00:09:49,000 --> 00:09:54,000
But what was ironic is we built that and then we had nothing to AB test.

153
00:09:54,000 --> 00:09:59,120
So the luxury of moving to the recommendations platform is there's so much to AB test.

154
00:09:59,120 --> 00:10:01,800
So I pulled that over with me when I made this switch.

155
00:10:01,800 --> 00:10:05,960
And then last but not least was the infrastructure problem that I mentioned earlier.

156
00:10:05,960 --> 00:10:09,440
As we grew as a company, the infrastructure did not grow.

157
00:10:09,440 --> 00:10:14,640
So the more customers that we were getting onto the X1 platform, our infrastructure stayed

158
00:10:14,640 --> 00:10:15,640
the same.

159
00:10:15,640 --> 00:10:19,640
So that's somewhat mind boggling if you think about that.

160
00:10:19,640 --> 00:10:24,280
Like the more data you get, the more you need to scale out your platform.

161
00:10:24,280 --> 00:10:29,280
And one of the changes we made to support that was of course moving to the cloud.

162
00:10:29,280 --> 00:10:34,120
So yeah, at high levels I think those were the four key pieces needed to resurrect a

163
00:10:34,120 --> 00:10:35,440
recommendations platform.

164
00:10:35,440 --> 00:10:36,440
Okay.

165
00:10:36,440 --> 00:10:37,440
Nice.

166
00:10:37,440 --> 00:10:38,440
Well, let's dig in.

167
00:10:38,440 --> 00:10:39,440
Yeah.

168
00:10:39,440 --> 00:10:40,440
We can start at the beginning.

169
00:10:40,440 --> 00:10:43,320
The data platform building out this data pipeline.

170
00:10:43,320 --> 00:10:49,120
We're starting with interaction data, click stream data, that kind of thing.

171
00:10:49,120 --> 00:10:54,640
What do you need from a pipeline perspective to support building these kinds of models

172
00:10:54,640 --> 00:10:55,640
of scale?

173
00:10:55,640 --> 00:10:56,640
Right.

174
00:10:56,640 --> 00:11:00,400
So the biggest issue that we saw with what I call the legacy platform, which is the one

175
00:11:00,400 --> 00:11:04,920
that ran on the physical infrastructure, the MapReduce specific platform.

176
00:11:04,920 --> 00:11:11,360
What I noticed was that we had so much code to get the data in the right format.

177
00:11:11,360 --> 00:11:14,800
And that's I'm sure you'll hear that all the time with machine learning.

178
00:11:14,800 --> 00:11:17,880
It's always getting the data in the right format at the right time.

179
00:11:17,880 --> 00:11:20,040
So what we did is we decided to go the serverless route.

180
00:11:20,040 --> 00:11:25,280
So we have a battalion of Lambda functions that consume the events in real time.

181
00:11:25,280 --> 00:11:30,640
So as the event happens, we eventually consume that into our into our data platform.

182
00:11:30,640 --> 00:11:35,760
And then right at that moment, we transform it to meet the specific requirements of the

183
00:11:35,760 --> 00:11:36,760
platform.

184
00:11:36,760 --> 00:11:41,240
So, fortunately, at Comcast, and I know this is a problem outside of Comcast, allow

185
00:11:41,240 --> 00:11:43,720
the data is originally in plain text.

186
00:11:43,720 --> 00:11:47,720
And plain text obviously doesn't play well with any data processing framework.

187
00:11:47,720 --> 00:11:52,360
So the first step typically is transforming those into a proper schema, which is usually

188
00:11:52,360 --> 00:11:55,000
Avro or some binary format.

189
00:11:55,000 --> 00:11:58,720
And then once we have that, then we start enriching the data so that it has more context,

190
00:11:58,720 --> 00:12:03,640
like simple things, like if they've watched this TV show in rich, it was some metadata.

191
00:12:03,640 --> 00:12:10,200
This TV show is the genre or the year it was made or what language it was in.

192
00:12:10,200 --> 00:12:15,280
And then once we've enriched it with that metadata, the next Lambda function was aggregating

193
00:12:15,280 --> 00:12:17,280
it for the user.

194
00:12:17,280 --> 00:12:21,720
So it was kind of, the goal was to build this seamless event-driven platform that we could

195
00:12:21,720 --> 00:12:25,000
kind of close our eyes and it would just do what we expected.

196
00:12:25,000 --> 00:12:26,400
It wasn't like scheduled.

197
00:12:26,400 --> 00:12:32,240
It wasn't based on scheduling jobs and some ask a ban or oozee or some job scheduler

198
00:12:32,240 --> 00:12:36,920
was, when the event happens, something is triggered and it goes through the process.

199
00:12:36,920 --> 00:12:40,600
I really was not expecting to hear serverless come up in this conversation.

200
00:12:40,600 --> 00:12:42,440
That's really, sorry about that.

201
00:12:42,440 --> 00:12:48,080
I mean, typically pipeline, you know, we're talking about like ETL or workflow engines,

202
00:12:48,080 --> 00:12:50,640
like Airflow or something like that.

203
00:12:50,640 --> 00:12:54,120
This is super interesting that you want the serverless route.

204
00:12:54,120 --> 00:12:58,840
When you are ready in AWS for this application.

205
00:12:58,840 --> 00:13:02,840
No, we weren't at all, which was, it took far longer than I thought, because again,

206
00:13:02,840 --> 00:13:07,240
it wasn't like a lift and shift to AWS, this is our first introduction, specifically

207
00:13:07,240 --> 00:13:09,800
my, at least my first introduction to AWS.

208
00:13:09,800 --> 00:13:14,160
So the learning curve was somewhat steep, but it really paid off.

209
00:13:14,160 --> 00:13:15,360
And I mean, you mentioned Airflow.

210
00:13:15,360 --> 00:13:20,480
So we do utilize Airflow for the second part of the four key parts to building a machine

211
00:13:20,480 --> 00:13:22,320
learning platform.

212
00:13:22,320 --> 00:13:25,200
And that was for the orchestration of when we do what.

213
00:13:25,200 --> 00:13:29,200
But yeah, so I kind of, when I hear the word ETL, it kind of, I get like, it kind of

214
00:13:29,200 --> 00:13:33,560
irks me because it's so much more than ETL, you know, if you build a data platform that,

215
00:13:33,560 --> 00:13:39,240
you know, is avoiding the construct of a pipeline jungle, it could be like somewhat kind

216
00:13:39,240 --> 00:13:40,840
of beautiful, you know.

217
00:13:40,840 --> 00:13:47,040
How specifically does serverless help you avoid the pipeline jungle?

218
00:13:47,040 --> 00:13:48,680
Oh, yeah, that's a good question.

219
00:13:48,680 --> 00:13:54,040
Well, specifically in the Lambda route, what was nice about Lambda's is that very, very

220
00:13:54,040 --> 00:13:59,160
well-managed in AWS, meaning like when, so obviously, data flux should work.

221
00:13:59,160 --> 00:14:04,680
So it's depending on the day of the week, when it comes to content consumption on TV, you'll

222
00:14:04,680 --> 00:14:08,080
see Sundays and Thursdays, the usage is far higher.

223
00:14:08,080 --> 00:14:11,560
So Lambda's scaled in and out for us, which was awesome.

224
00:14:11,560 --> 00:14:17,240
And just the ability to have them be triggered based on some other, like, some other action

225
00:14:17,240 --> 00:14:21,400
was also critical, which so that, yeah, so that's the route.

226
00:14:21,400 --> 00:14:27,680
That's the reason why I think serverless in the Lambda context worked for this data pipeline.

227
00:14:27,680 --> 00:14:33,440
Now, I haven't done a ton with serverless hands-on, but the attempts that I've made

228
00:14:33,440 --> 00:14:38,240
to play with it, it just seemed like a configuration mess.

229
00:14:38,240 --> 00:14:41,800
It was a little bit actually, you know, hard to kind of wrap my head around, like how

230
00:14:41,800 --> 00:14:48,480
you manage these kind of function artifacts, you know, the way that you would manage, you

231
00:14:48,480 --> 00:14:53,240
know, version control and, like, how you tie everything together, like, was there a big

232
00:14:53,240 --> 00:14:54,560
learning curve either?

233
00:14:54,560 --> 00:14:55,560
There was.

234
00:14:55,560 --> 00:15:00,880
So we utilized Terraform, and that helps the ton Terraform and Jenkins to handle the whole

235
00:15:00,880 --> 00:15:06,360
version control and putting the artifact into a place where AWS could reference it.

236
00:15:06,360 --> 00:15:11,960
But I think the incorporation of Terraform made everything that you said a lot easier.

237
00:15:11,960 --> 00:15:12,960
But it did take its time.

238
00:15:12,960 --> 00:15:14,400
We didn't start with Terraform.

239
00:15:14,400 --> 00:15:18,920
We started with, you know, just AWS CLI and it was a big mess.

240
00:15:18,920 --> 00:15:20,560
So going to Terraform route helps a ton.

241
00:15:20,560 --> 00:15:22,080
Now, that's kind of blowing my mind.

242
00:15:22,080 --> 00:15:28,440
And I think of Terraform as like laying down images and containers and stuff like that.

243
00:15:28,440 --> 00:15:30,200
What does that have to do with serverless?

244
00:15:30,200 --> 00:15:33,480
I mean, so you could deploy your land of functions using Terraform, right?

245
00:15:33,480 --> 00:15:36,600
Or you could configure them using Terraform.

246
00:15:36,600 --> 00:15:37,600
Okay.

247
00:15:37,600 --> 00:15:42,400
So yes, I mean, a lot of what we do, it utilizes, like, the goal is never to put anything

248
00:15:42,400 --> 00:15:46,160
in AWS without using Terraform in one way or another.

249
00:15:46,160 --> 00:15:47,680
Got it.

250
00:15:47,680 --> 00:15:53,640
So your functions are essentially configuration, that's the point and you're using Terraform

251
00:15:53,640 --> 00:15:58,800
as configuration management and it's managing the like battalion of functions that need

252
00:15:58,800 --> 00:15:59,800
to be triggered.

253
00:15:59,800 --> 00:16:00,800
Yeah.

254
00:16:00,800 --> 00:16:01,800
Right.

255
00:16:01,800 --> 00:16:06,000
I'm kind of trying to figure out how far to geek out on serverless, like, how far you use.

256
00:16:06,000 --> 00:16:11,880
How did you get your developers comfortable from a tooling perspective with Lambda functions?

257
00:16:11,880 --> 00:16:17,240
Like, is it to the point now where there's kind of IDE support or it's?

258
00:16:17,240 --> 00:16:21,880
So I think the biggest struggle we've had is testing changes locally.

259
00:16:21,880 --> 00:16:24,160
I think that and we're still struggling with that.

260
00:16:24,160 --> 00:16:26,000
I know there are frameworks out there that we could use.

261
00:16:26,000 --> 00:16:30,600
We just haven't had the, you know, time or the resources to experiment with them.

262
00:16:30,600 --> 00:16:35,520
But at some point, we were essentially deploying and making changes in production in our,

263
00:16:35,520 --> 00:16:37,000
you know, quote unquote production platform.

264
00:16:37,000 --> 00:16:40,800
We did live in a space where we had both the legacy and the production at the same time.

265
00:16:40,800 --> 00:16:46,200
So that was a kind of a luxury in that we can make changes to our data pipeline.

266
00:16:46,200 --> 00:16:50,120
There are all these Lambda functions in our AWS production platform.

267
00:16:50,120 --> 00:16:53,800
And then if any, if we mess anything up, we could always rely on our physical infrastructure

268
00:16:53,800 --> 00:16:57,600
to copy the data that we may have, you know, dropped because we made a mistake.

269
00:16:57,600 --> 00:17:01,680
But yeah, I think the biggest struggle was being careful for what we did.

270
00:17:01,680 --> 00:17:03,840
And also the other biggest role was the cost.

271
00:17:03,840 --> 00:17:09,920
I'm there was this one time where we enabled CloudWatch to test one of our Lambda functions.

272
00:17:09,920 --> 00:17:12,000
And we had it on debug mode.

273
00:17:12,000 --> 00:17:16,800
And unfortunately, like after just like three or so hours, it resulted in 20K.

274
00:17:16,800 --> 00:17:17,800
Wow.

275
00:17:17,800 --> 00:17:18,800
Expenses.

276
00:17:18,800 --> 00:17:19,800
Yeah.

277
00:17:19,800 --> 00:17:20,800
It's a lot.

278
00:17:20,800 --> 00:17:21,800
It's a ton.

279
00:17:21,800 --> 00:17:22,800
And it was just four hours.

280
00:17:22,800 --> 00:17:24,400
I remember frantically figuring out, like, how do we turn this off?

281
00:17:24,400 --> 00:17:27,040
Because it wasn't one of my Lambda functions.

282
00:17:27,040 --> 00:17:28,040
What caused that?

283
00:17:28,040 --> 00:17:30,600
You shifted over too much traffic to it or something like that?

284
00:17:30,600 --> 00:17:32,400
Well, we just turned on debug logging.

285
00:17:32,400 --> 00:17:33,400
Ah, okay.

286
00:17:33,400 --> 00:17:34,400
Okay.

287
00:17:34,400 --> 00:17:40,800
And so we're sending a ton of logs to invoke this, you know, we're invoking these Lambda functions.

288
00:17:40,800 --> 00:17:44,120
It was just like the logs that we specifically wrote.

289
00:17:44,120 --> 00:17:47,200
It was all the logs that were created by all the dependencies that are pulled in.

290
00:17:47,200 --> 00:17:51,560
So we were seeing, you know, logs from Apache, logs, everything and its mother was being

291
00:17:51,560 --> 00:17:52,560
logged.

292
00:17:52,560 --> 00:17:53,560
So 20K.

293
00:17:53,560 --> 00:17:54,560
Yeah.

294
00:17:54,560 --> 00:17:55,560
That's a good story.

295
00:17:55,560 --> 00:17:56,560
Wow.

296
00:17:56,560 --> 00:17:57,560
Yeah.

297
00:17:57,560 --> 00:17:58,560
Wow.

298
00:17:58,560 --> 00:17:59,560
Yeah.

299
00:17:59,560 --> 00:18:00,560
Now we're really careful.

300
00:18:00,560 --> 00:18:01,560
But it could have been way worse.

301
00:18:01,560 --> 00:18:02,560
I was, I made a joke with the team that if that were me, it probably would have been like 60K.

302
00:18:02,560 --> 00:18:03,560
Wow.

303
00:18:03,560 --> 00:18:04,560
Because I lost a ton.

304
00:18:04,560 --> 00:18:12,040
And so how about the stringing together of the Lambda functions?

305
00:18:12,040 --> 00:18:17,920
It sounds like you were, you're essentially creating these pipeline in Lambda functions is

306
00:18:17,920 --> 00:18:21,760
presumably AWS makes that pretty easy if you stay within the Lambda environment.

307
00:18:21,760 --> 00:18:22,760
Is that right?

308
00:18:22,760 --> 00:18:23,760
Yeah.

309
00:18:23,760 --> 00:18:24,760
Definitely.

310
00:18:24,760 --> 00:18:26,840
So it's, it's very like action oriented.

311
00:18:26,840 --> 00:18:32,200
So these Lambda functions are triggered based on these defined actions that we specify.

312
00:18:32,200 --> 00:18:37,680
So for example, it all starts with consuming data from Kinesis or Kafka or some event stream.

313
00:18:37,680 --> 00:18:42,760
The Lambda function gets triggered to, you know, pull or automatically Kinesis triggers

314
00:18:42,760 --> 00:18:43,760
itself.

315
00:18:43,760 --> 00:18:47,920
And it reads data from the event stream and then we say, OK, this is the raw because, you

316
00:18:47,920 --> 00:18:52,760
know, data 101 never get rid of the raw data because you may, you know, mess up how

317
00:18:52,760 --> 00:18:53,960
you transform it.

318
00:18:53,960 --> 00:18:57,560
So we stick the raw data into our bucket, our s3 bucket.

319
00:18:57,560 --> 00:19:01,640
And then there's another Lambda function that's, you know, waiting for these events to

320
00:19:01,640 --> 00:19:03,360
be written to that path.

321
00:19:03,360 --> 00:19:04,600
So it gets triggered on that.

322
00:19:04,600 --> 00:19:08,920
So every time there's like a new file that's created, another subsequent Lambda function

323
00:19:08,920 --> 00:19:11,560
gets triggered and it does the enrichment aspect.

324
00:19:11,560 --> 00:19:16,680
And then once, once we've enriched that file, the usage events in that file, we write

325
00:19:16,680 --> 00:19:20,160
that back out to a different part of that s3 bucket.

326
00:19:20,160 --> 00:19:24,080
And then there's a subsequent, different Lambda function that, you know, starts aggregating

327
00:19:24,080 --> 00:19:29,720
the data to start building the feature data set that the models eventually consume, if

328
00:19:29,720 --> 00:19:30,720
that makes sense.

329
00:19:30,720 --> 00:19:32,920
Like events mean, it does make sense.

330
00:19:32,920 --> 00:19:37,600
So I'm going to ask again, a question that I asked earlier, you referred to, I forget

331
00:19:37,600 --> 00:19:42,760
actually the specific wording, but like avoiding a pipeline hell or pipeline spaghetti or

332
00:19:42,760 --> 00:19:44,240
something like that.

333
00:19:44,240 --> 00:19:48,000
You've essentially created some number of pipelines here.

334
00:19:48,000 --> 00:19:49,000
Yeah.

335
00:19:49,000 --> 00:19:52,680
What specifically do you avoid, you know, that you had to deal with before?

336
00:19:52,680 --> 00:19:57,880
Before the data was discopied from one cluster to another cluster.

337
00:19:57,880 --> 00:20:04,440
Not often that discopy would fail or there would be, you know, the data would lack in its

338
00:20:04,440 --> 00:20:05,440
full sense.

339
00:20:05,440 --> 00:20:10,120
Like they had dropped events and we wouldn't know until we realized like, until there was

340
00:20:10,120 --> 00:20:15,440
until our QA team was like, this box clearly watched the sopranos, but we're not, we're

341
00:20:15,440 --> 00:20:19,880
not seeing that in the usage data that's being fed into the training model.

342
00:20:19,880 --> 00:20:24,800
So it was just a lack of visibility is what we gained visibility of the most important

343
00:20:24,800 --> 00:20:28,480
part of a machine learning platform, our data pipeline.

344
00:20:28,480 --> 00:20:32,840
And just being able to, you know, have alerts when we see like all of a sudden, a drop

345
00:20:32,840 --> 00:20:38,040
in events, is it because of our infrastructure or is it because of upstream clients?

346
00:20:38,040 --> 00:20:42,920
We never had that before because again, we didn't, we relied on another quote unquote data

347
00:20:42,920 --> 00:20:45,040
like to provide us that data.

348
00:20:45,040 --> 00:20:52,960
So the kind of, at, at a missity for lack of a better term of the, like ownership, the

349
00:20:52,960 --> 00:20:56,760
functional environment gave you a lot more transparency and ownership of the different

350
00:20:56,760 --> 00:20:57,760
pieces.

351
00:20:57,760 --> 00:21:04,120
And kind of forced you to think about transforming an individual piece of data at a time like

352
00:21:04,120 --> 00:21:08,760
a log entry as opposed to doing big batch runs of transformations or something.

353
00:21:08,760 --> 00:21:09,760
Right.

354
00:21:09,760 --> 00:21:10,760
Right.

355
00:21:10,760 --> 00:21:11,760
Right.

356
00:21:11,760 --> 00:21:12,760
Right.

357
00:21:12,760 --> 00:21:13,760
Yeah.

358
00:21:13,760 --> 00:21:16,400
So avoiding the 18 hour MapReduce job for just doing it in somewhat real time.

359
00:21:16,400 --> 00:21:18,400
And so that's the data platform.

360
00:21:18,400 --> 00:21:24,120
Yeah, so then there's the second part, the like orchestrating of the machine learning machine

361
00:21:24,120 --> 00:21:25,720
learning side.

362
00:21:25,720 --> 00:21:29,160
And that's where we did you, we just, we have started to use airflow, which is great

363
00:21:29,160 --> 00:21:30,400
that you mentioned that.

364
00:21:30,400 --> 00:21:35,640
But yeah, so the key part to that was before it was, there was no way for us to introduce

365
00:21:35,640 --> 00:21:37,360
a new model into our platform.

366
00:21:37,360 --> 00:21:40,840
There was just the one, the one machine learning model and that's it.

367
00:21:40,840 --> 00:21:41,840
And it did everything for us.

368
00:21:41,840 --> 00:21:42,840
It was a super high level.

369
00:21:42,840 --> 00:21:45,000
It was ALS matrix factorization.

370
00:21:45,000 --> 00:21:50,360
We were find it in different ways and we, we somewhat tested those changes, but once

371
00:21:50,360 --> 00:21:55,560
we made a change to like our similarity matrix or anything specific to the model itself,

372
00:21:55,560 --> 00:22:00,520
there was no way of partitioning subscribers to only get this new algorithm.

373
00:22:00,520 --> 00:22:02,040
It's everyone gets it.

374
00:22:02,040 --> 00:22:08,520
So just being able to train a new or new model or change and then evaluate that in an offline

375
00:22:08,520 --> 00:22:11,160
fashion and then push that into production.

376
00:22:11,160 --> 00:22:16,120
And only for like a subset of subscribers was the next big effort that we focused on.

377
00:22:16,120 --> 00:22:17,280
And how did you get there?

378
00:22:17,280 --> 00:22:18,280
Yeah, definitely.

379
00:22:18,280 --> 00:22:23,840
So one part we started utilizing the AB testing platform that I brought from my other team.

380
00:22:23,840 --> 00:22:29,400
And then two, making the, the way that what we had to introduce, which also led to like

381
00:22:29,400 --> 00:22:34,600
a reason why my previous like work history was perfect for this team, we introduced our

382
00:22:34,600 --> 00:22:36,240
own service layer.

383
00:22:36,240 --> 00:22:41,800
So before it was our logic was super really tightly coupled with the client.

384
00:22:41,800 --> 00:22:45,560
So if we think about it in a very high level fashion, this is obviously not how it's

385
00:22:45,560 --> 00:22:46,560
implemented.

386
00:22:46,560 --> 00:22:51,800
When you watch TV, you see a bunch of options on your TV saying, you know, these are

387
00:22:51,800 --> 00:22:55,280
the TV shows you should watch here, the top movies for you.

388
00:22:55,280 --> 00:22:57,560
We were super tightly coupled to that infrastructure.

389
00:22:57,560 --> 00:23:04,600
So if we wanted to introduce a new model, there was no way to tell the, the cable box or

390
00:23:04,600 --> 00:23:08,440
the set up box, hey, go, go get this model for just this account.

391
00:23:08,440 --> 00:23:13,400
So adding a service layer in between us allowed us to be more independent in serving what

392
00:23:13,400 --> 00:23:16,240
we wanted to serve when, if that makes sense.

393
00:23:16,240 --> 00:23:23,600
So you're able to encapsulate the, the business logic of what model to use to respond to a specific

394
00:23:23,600 --> 00:23:29,240
prediction requests at the service layer, as opposed to just kind of serving up just

395
00:23:29,240 --> 00:23:30,560
raw model results.

396
00:23:30,560 --> 00:23:31,560
Yep.

397
00:23:31,560 --> 00:23:33,040
Just in serving raw predictions, regardless.

398
00:23:33,040 --> 00:23:34,040
Yep.

399
00:23:34,040 --> 00:23:37,280
So this is the next layer, which is we started to get context.

400
00:23:37,280 --> 00:23:42,640
We started asking the client to pass this context into what, where this is being served so

401
00:23:42,640 --> 00:23:48,760
that we can later, specifically if we're looking at, you know, a view on X1 or on, on your

402
00:23:48,760 --> 00:23:51,640
cable box, it's like, here, here's all the music videos.

403
00:23:51,640 --> 00:23:54,920
Let's go to the model that's finally tuned for music.

404
00:23:54,920 --> 00:23:56,640
So yeah, so it was a combination of a few things.

405
00:23:56,640 --> 00:24:00,560
It was one being able to get the data in the right format so that we could easily train

406
00:24:00,560 --> 00:24:02,480
it on different data.

407
00:24:02,480 --> 00:24:08,760
So different feature sets to introducing that service layer in between the producing the

408
00:24:08,760 --> 00:24:12,560
predictions and serving the predictions to the client.

409
00:24:12,560 --> 00:24:18,960
And three, we beefed up our evaluation metrics before when we had this model, we really

410
00:24:18,960 --> 00:24:21,720
kind of was flying blind, I think is the phrase.

411
00:24:21,720 --> 00:24:22,720
I'm not sure.

412
00:24:22,720 --> 00:24:25,960
But we had really no idea how our models were performing.

413
00:24:25,960 --> 00:24:30,960
We would run, and this is before I joined the team, I used to say they, some getting

414
00:24:30,960 --> 00:24:36,200
used to saying we, we would train our model and then our evaluation metrics were essentially

415
00:24:36,200 --> 00:24:37,200
just precision.

416
00:24:37,200 --> 00:24:41,600
And it would run once a month or so, which is not ideal, especially for a customer-facing

417
00:24:41,600 --> 00:24:42,600
platform.

418
00:24:42,600 --> 00:24:45,240
So we did spend a lot of time, this is kind of like a weekend project for me.

419
00:24:45,240 --> 00:24:52,280
I spent a lot of time on building apps that surface what are like our recall or precision,

420
00:24:52,280 --> 00:24:54,080
what that was at an hourly basis.

421
00:24:54,080 --> 00:24:58,240
So we could see this historical view of how our predictions are performing.

422
00:24:58,240 --> 00:25:01,440
And then we could use that as a baseline to make changes.

423
00:25:01,440 --> 00:25:06,080
And was that easy to do with your pre-existing models, meaning were they sufficiently

424
00:25:06,080 --> 00:25:13,240
instrumented for you to be able to even report on their performance, or did you have to do

425
00:25:13,240 --> 00:25:15,680
a lot of work to instrument them?

426
00:25:15,680 --> 00:25:17,160
That's a good question.

427
00:25:17,160 --> 00:25:22,240
So it did take a good amount of work to be able to easily grab what we would predict

428
00:25:22,240 --> 00:25:25,920
right now, because what I wanted to do is I wanted to build, or what I did do.

429
00:25:25,920 --> 00:25:34,440
What I built was an online offline recall platform, which essentially all it did was right

430
00:25:34,440 --> 00:25:40,400
now, I'm going to check all the events that people are like, all the TV shows that a subset

431
00:25:40,400 --> 00:25:42,120
of subscribers are watching.

432
00:25:42,120 --> 00:25:46,280
And then right in this moment, I want to check what our recommendations platform is suggesting

433
00:25:46,280 --> 00:25:49,400
and do they match up, you know, basic recall evaluation.

434
00:25:49,400 --> 00:25:55,280
The hardest part was being able to easily be able to check right now what we are predicting

435
00:25:55,280 --> 00:26:00,640
because it was encapsulated like deep in this, you know, legacy platform, which utilized

436
00:26:00,640 --> 00:26:01,640
couch-based.

437
00:26:01,640 --> 00:26:04,000
So introducing that service layer made it easy.

438
00:26:04,000 --> 00:26:08,320
So essentially the evaluation platform that we built, all it does is it hits our service

439
00:26:08,320 --> 00:26:09,320
layer.

440
00:26:09,320 --> 00:26:11,720
We pass it, you know, context for who the user is.

441
00:26:11,720 --> 00:26:16,280
And then it returns for this specific model what we would recommend.

442
00:26:16,280 --> 00:26:18,800
And then we just check that with what they're actually watching.

443
00:26:18,800 --> 00:26:25,240
You mentioned couch-based, presumably you, you know, ran your 18-hour map, or

444
00:26:25,240 --> 00:26:29,960
reduced jobs, created a model, and it started in a database.

445
00:26:29,960 --> 00:26:35,880
Are you hitting a model based in code now, or are you also kind of caching it in some

446
00:26:35,880 --> 00:26:37,440
database data structure?

447
00:26:37,440 --> 00:26:38,440
Yeah.

448
00:26:38,440 --> 00:26:43,400
So right now we're at, we're going to routes, they're, we're going one route where there's

449
00:26:43,400 --> 00:26:48,280
a model that's in base in a, you know, a recommender backend that we incorporate into

450
00:26:48,280 --> 00:26:52,560
our web service, that's work in progress, all the work that we've done since moving

451
00:26:52,560 --> 00:26:57,440
from our legacy platform has been pre-computing recommendations and writing it into Redis.

452
00:26:57,440 --> 00:26:59,280
We've moved away from couch-based.

453
00:26:59,280 --> 00:27:04,800
And only because, again, Redis is more of a AWS managed service, so it's easy to stand

454
00:27:04,800 --> 00:27:07,320
up and scale out.

455
00:27:07,320 --> 00:27:08,320
So yes, that's the goal.

456
00:27:08,320 --> 00:27:13,280
The goal is to also go the route of, of having the model be encapsulated in code and then

457
00:27:13,280 --> 00:27:18,120
also put into our web service, but yeah, we'll see how that goes.

458
00:27:18,120 --> 00:27:25,560
And would you envision in doing that, putting the actual predictions behind a Lambda function

459
00:27:25,560 --> 00:27:28,240
or does inference take too long for?

460
00:27:28,240 --> 00:27:33,520
Yeah, I, I think I would probably keep our service serverless side or a Lambda route in

461
00:27:33,520 --> 00:27:34,520
the back.

462
00:27:34,520 --> 00:27:40,120
I think for now, keeping, I'm a bit nervous on the performance of Lambda functions with

463
00:27:40,120 --> 00:27:42,480
anything that's like customer impacting.

464
00:27:42,480 --> 00:27:53,080
So you built out the capability to use multiple models and benchmark those, the models relative

465
00:27:53,080 --> 00:27:55,440
to the previous system.

466
00:27:55,440 --> 00:28:01,640
You mentioned earlier that part of what you wanted to do on the machine learning side

467
00:28:01,640 --> 00:28:08,920
is just make it easier to experiment with models and with the data and to deploy.

468
00:28:08,920 --> 00:28:10,920
What have you done in that regard?

469
00:28:10,920 --> 00:28:12,760
Yeah, definitely.

470
00:28:12,760 --> 00:28:19,160
So in that regard, to be able to easily experiment deploy, we started Ulyse Redshift.

471
00:28:19,160 --> 00:28:24,840
So one of the big parts of machine learning and any model is your model is as good as

472
00:28:24,840 --> 00:28:26,640
the data that you train it on.

473
00:28:26,640 --> 00:28:30,360
So we started to experiment with what if we, and this is basic constructs that I'm sure

474
00:28:30,360 --> 00:28:33,920
other content discovery platforms already went down.

475
00:28:33,920 --> 00:28:37,200
What if we started to train our model based on different criteria?

476
00:28:37,200 --> 00:28:40,520
So, and this is before we went down the deep learning route where we're playing with

477
00:28:40,520 --> 00:28:41,520
hyper parameters.

478
00:28:41,520 --> 00:28:46,560
This is just the basic our similarity matrix and everything that's like just super simple

479
00:28:46,560 --> 00:28:48,760
machine learning, let's start playing with that.

480
00:28:48,760 --> 00:28:52,720
So what we started to do is first, let's put our data in a place that we can easily access

481
00:28:52,720 --> 00:28:58,000
so that we could train models quickly and not have to spend days babysitting them.

482
00:28:58,000 --> 00:29:05,360
So we put all our training data in Redshift, which is like a big database, I think Oracle,

483
00:29:05,360 --> 00:29:07,080
but for big data.

484
00:29:07,080 --> 00:29:13,760
And what that allowed us to do was start segmenting subscribers based on attributes.

485
00:29:13,760 --> 00:29:17,920
So I could explain one of the models that we recently deployed to production at a high

486
00:29:17,920 --> 00:29:18,920
level.

487
00:29:18,920 --> 00:29:22,280
What we first did is we group subscribers based on their location.

488
00:29:22,280 --> 00:29:26,640
So people in Philly, versus people in DC, versus people in New York.

489
00:29:26,640 --> 00:29:30,840
And then just on those small clusters, we started grouping them based on what they're

490
00:29:30,840 --> 00:29:32,960
similar to watch to each other.

491
00:29:32,960 --> 00:29:35,840
First we started clustering based, segmenting based on locality.

492
00:29:35,840 --> 00:29:38,880
Then we started clustering within that based on what they watched.

493
00:29:38,880 --> 00:29:44,240
And then we fed those accounts into our training, our models to produce recommendations.

494
00:29:44,240 --> 00:29:48,760
We found what that was, the model trained a lot quicker.

495
00:29:48,760 --> 00:29:53,400
We could do them in parallel, so instead of waiting hours and hours for one model to train

496
00:29:53,400 --> 00:29:58,840
based on all of our subscriber base, we had multiple models training at the same time.

497
00:29:58,840 --> 00:30:04,000
And then where we leveraged Redis and our web services that we could easily, based on

498
00:30:04,000 --> 00:30:08,720
the key, write those predictions to Redis for this web service to pick up.

499
00:30:08,720 --> 00:30:13,840
I'm hearing the folks talking about doing training directly against the data warehouse,

500
00:30:13,840 --> 00:30:18,280
whether it's Redshift or BigQuery or something else.

501
00:30:18,280 --> 00:30:24,240
A lot more than I used to, and I don't know if that's because it's just more business

502
00:30:24,240 --> 00:30:33,480
as opposed to academic or more production as opposed to toy problems or what.

503
00:30:33,480 --> 00:30:39,640
I think it's this evolution of engineers, because at least what I've observed is before

504
00:30:39,640 --> 00:30:43,080
there was, you know, research, traditional researchers, and then there were engineers

505
00:30:43,080 --> 00:30:47,560
who kind of had it played a blind eye to everything that went on with machine learning.

506
00:30:47,560 --> 00:30:51,480
I think we're like starting to build machine learning engineers.

507
00:30:51,480 --> 00:30:56,080
And that's like the sweet spot where they can understand the intricacies and the detail

508
00:30:56,080 --> 00:31:00,080
oriented that's required for training models and evaluating them.

509
00:31:00,080 --> 00:31:06,000
But then they also know how to build platforms that can, you know, manage terabytes and petabytes

510
00:31:06,000 --> 00:31:09,600
worth of data to be able to access, if that makes sense.

511
00:31:09,600 --> 00:31:16,640
And so the data warehouse enabled you to kind of slice and thus easily access this data

512
00:31:16,640 --> 00:31:20,280
query language, yep.

513
00:31:20,280 --> 00:31:27,520
Create kind of new features on the fly, presumably, did you do anything in terms of creating

514
00:31:27,520 --> 00:31:31,840
a feature store or trying to achieve feature reusability across models?

515
00:31:31,840 --> 00:31:32,840
Yep.

516
00:31:32,840 --> 00:31:33,840
So we did a few things.

517
00:31:33,840 --> 00:31:37,760
One, we tried to go down the route of, when we created a feature store, we would create

518
00:31:37,760 --> 00:31:38,760
a view.

519
00:31:38,760 --> 00:31:42,120
For some reason, we found difficult creating a view in Redshift at the time.

520
00:31:42,120 --> 00:31:48,240
So what we ended up doing was we would output that data to a different part of S3.

521
00:31:48,240 --> 00:31:54,000
So we have our, you know, our partition, our path in S3 where our bigger training data

522
00:31:54,000 --> 00:31:59,040
set exists and, you know, that data's offloaded or unloaded into Redshift.

523
00:31:59,040 --> 00:32:02,320
Once we came to a point where we've evaluated a model and then we're like, okay, we wanted

524
00:32:02,320 --> 00:32:05,720
to play that production, but we want to save the training, the feature set.

525
00:32:05,720 --> 00:32:11,960
We would output that into a specific, you know, training data set part of our S3 bucket.

526
00:32:11,960 --> 00:32:16,840
And if you're developing new features for a model and you need to backfill, is that

527
00:32:16,840 --> 00:32:21,000
something you do manually or do you have some kind of automation in there?

528
00:32:21,000 --> 00:32:27,400
So the only automation that we've gotten to at this point is utilizing Airflow to continuously

529
00:32:27,400 --> 00:32:33,240
update the tables to the tables in our, what we call our training tables in Redshift.

530
00:32:33,240 --> 00:32:38,000
We haven't gotten to the point where we are automated to offloading the feature data

531
00:32:38,000 --> 00:32:41,000
set, but that's definitely something we'd like to look into.

532
00:32:41,000 --> 00:32:45,520
And then kind of back to this idea of experimentation.

533
00:32:45,520 --> 00:32:53,360
How are you managing experiments in terms of, you know, recording that iterative process?

534
00:32:53,360 --> 00:32:54,360
Right, right.

535
00:32:54,360 --> 00:32:57,880
So there's a lot of metadata that goes along with experimenting, right?

536
00:32:57,880 --> 00:33:02,480
It's the version of the model, the, you know, the time frame in which you extracted the

537
00:33:02,480 --> 00:33:05,680
training data set, you know, was it the last three months or the last nine months or

538
00:33:05,680 --> 00:33:10,680
the last 13 months, the instance type that you use, the GPUs.

539
00:33:10,680 --> 00:33:16,440
So we offload that metadata into dynamo DB at the time, or currently.

540
00:33:16,440 --> 00:33:20,400
And we often reference that when we have to deploy the model's production.

541
00:33:20,400 --> 00:33:26,320
But right now are what we call our, our model metadata store, it's reutilizing dynamo.

542
00:33:26,320 --> 00:33:29,720
Is that a manual process to keep that up to date as you're experimenting?

543
00:33:29,720 --> 00:33:36,240
Yeah, it's one of the steps in our Airflow workflow is once we've went before we start

544
00:33:36,240 --> 00:33:41,360
training a model, we first insert a record that describes like what the model is, here's

545
00:33:41,360 --> 00:33:45,960
the training data sets going after it's typically part of every, it's the first step before

546
00:33:45,960 --> 00:33:46,960
we start training it.

547
00:33:46,960 --> 00:33:51,680
And then the last step is it outputs the time, like how long it took.

548
00:33:51,680 --> 00:33:55,560
And then, you know, the recall about the high level evaluation, the recall evaluation

549
00:33:55,560 --> 00:33:59,720
or the precision that was determined after the predictions were produced.

550
00:33:59,720 --> 00:34:02,440
It's part of the end and flow.

551
00:34:02,440 --> 00:34:06,800
But I didn't, I didn't think I made that connection earlier part of the way you're using

552
00:34:06,800 --> 00:34:10,800
Airflow is to manage the training workflow.

553
00:34:10,800 --> 00:34:11,800
Yep, yep, yep.

554
00:34:11,800 --> 00:34:16,040
And it took us a while to get there before we were super manual.

555
00:34:16,040 --> 00:34:20,480
You know, we would execute a job, wait, and then I'll put this data into a Google Excel

556
00:34:20,480 --> 00:34:21,480
sheet.

557
00:34:21,480 --> 00:34:26,280
You know, like it was, and then we realized, wait, we should do something about this.

558
00:34:26,280 --> 00:34:31,400
And so where there are multiple steps to get to using Airflow, or did you realize the

559
00:34:31,400 --> 00:34:36,840
problem and then, you know, big bang, you've put the Airflow solution in place.

560
00:34:36,840 --> 00:34:39,760
It was fairly straightforward.

561
00:34:39,760 --> 00:34:44,720
In the past, we've, I mean, we've dealt with things like oozy workflows that were traditional

562
00:34:44,720 --> 00:34:46,120
with the Hadoop infrastructure.

563
00:34:46,120 --> 00:34:50,000
Airflow was really straightforward from my perspective, at least.

564
00:34:50,000 --> 00:34:52,680
And it seems to be the best practice what everyone else is using.

565
00:34:52,680 --> 00:34:56,880
I remember we were going down the route of looking at something else and then we had a few

566
00:34:56,880 --> 00:35:00,520
blog posts that were suggesting stick with Airflow, so it's what we did.

567
00:35:00,520 --> 00:35:07,520
For training, do you have multiple workflows for different types of training jobs, or have

568
00:35:07,520 --> 00:35:13,000
you kind of abstracted training as a workflow and you've got different ways to parameterize

569
00:35:13,000 --> 00:35:14,000
that?

570
00:35:14,000 --> 00:35:16,480
We have different, so at the moment, that's a good question of parameterizing.

571
00:35:16,480 --> 00:35:18,680
That's actually the dream would be to parameterize that.

572
00:35:18,680 --> 00:35:23,120
Right now, we have different workflows, but we also have like a lot of different types

573
00:35:23,120 --> 00:35:28,960
of algorithms, because you know, like there's the typical recommendations isn't just producing

574
00:35:28,960 --> 00:35:33,800
recommendations, there's a relevancy aspect to it too, so our relevancy models require

575
00:35:33,800 --> 00:35:38,760
a different workflow than our core recommendations models.

576
00:35:38,760 --> 00:35:40,520
So right now, there are different workflows.

577
00:35:40,520 --> 00:35:45,000
It'd be great to get to the point where they're parameterized, as we'll see, hopefully

578
00:35:45,000 --> 00:35:46,000
one day.

579
00:35:46,000 --> 00:35:53,640
Last bit on this experimentation piece, have you incorporated any aspect of automated hyperparameter

580
00:35:53,640 --> 00:35:55,040
tuning?

581
00:35:55,040 --> 00:35:56,040
That would be great.

582
00:35:56,040 --> 00:35:57,640
Actually, that's one of, that's on our to-do list.

583
00:35:57,640 --> 00:36:00,000
One, our research team would love that.

584
00:36:00,000 --> 00:36:06,000
So we are, as I mentioned earlier, in the previous world, or 14 months ago, we had this

585
00:36:06,000 --> 00:36:12,800
one machine learning algorithm that was one size fits all, it was at super high levels

586
00:36:12,800 --> 00:36:18,440
and ALS, matrix factorization, and what we wanted to do more, especially with everyone's

587
00:36:18,440 --> 00:36:22,520
talking about AI and ML, we've started to go down three routes.

588
00:36:22,520 --> 00:36:26,560
So we mentioned the utilizing, clustering, and recommendations together, so that's where

589
00:36:26,560 --> 00:36:28,440
out that's already in production.

590
00:36:28,440 --> 00:36:32,400
Our research team is going down the route of utilizing deep learning models that are somewhat

591
00:36:32,400 --> 00:36:37,080
based off of word-to-vec, and that's where we want to do exactly what you say with the

592
00:36:37,080 --> 00:36:38,080
hyperparameters.

593
00:36:38,080 --> 00:36:39,080
We're not quite there yet.

594
00:36:39,080 --> 00:36:44,040
We're still very much in the experimentation phase with this deep learning model that's

595
00:36:44,040 --> 00:36:46,200
similar to word-to-vec.

596
00:36:46,200 --> 00:36:47,200
So we'll see.

597
00:36:47,200 --> 00:36:52,640
If it gains momentum, maybe we'll start abstracting that, but as of now, we've seen some limitations,

598
00:36:52,640 --> 00:36:54,040
so we'll see how far this goes.

599
00:36:54,040 --> 00:36:59,440
So if you hit experimentation, deployment, you've talked about server-side and redis,

600
00:36:59,440 --> 00:37:06,520
and then you've alluded to this next step, which is A-B testing previously, but I get the

601
00:37:06,520 --> 00:37:09,360
impression that there's more to it than what we've talked about so far.

602
00:37:09,360 --> 00:37:10,360
Yeah, definitely.

603
00:37:10,360 --> 00:37:14,760
So A-B testing is my sweet spot, just because I built this platform prior to joining

604
00:37:14,760 --> 00:37:16,920
the recommendations team.

605
00:37:16,920 --> 00:37:22,840
And the A-B testing platform that we built, what it does is it allows for in partnership

606
00:37:22,840 --> 00:37:28,000
with our web service, it allows for a certain set of customers to get one model, and then

607
00:37:28,000 --> 00:37:30,360
another subset of customers get a different model.

608
00:37:30,360 --> 00:37:33,720
And then, of course, it always takes a new account breaking out the control.

609
00:37:33,720 --> 00:37:36,560
One thing that I always say when we talk about A-B testing, it's quite expensive.

610
00:37:36,560 --> 00:37:42,200
It does take time, so when in doubt, try to do as much offline evaluation as you can.

611
00:37:42,200 --> 00:37:46,000
And it's also, at least with TV viewing behavior, there are sometimes where you just

612
00:37:46,000 --> 00:37:49,760
you can't run an A-B test, like on Comcast, like working at Comcast.

613
00:37:49,760 --> 00:37:53,600
You can't run an A-B test during the Olympics, the data fluctuates so much.

614
00:37:53,600 --> 00:37:59,240
It's so skewed to Olympics that for that entire month, you can't experiment, right?

615
00:37:59,240 --> 00:38:02,080
So we get that a lot in the TV world.

616
00:38:02,080 --> 00:38:07,600
So when I mean A-B testing is expensive, when we find a good time where data is inflectioning,

617
00:38:07,600 --> 00:38:12,320
the usage patterns are, you can predict, then we execute an A-B test.

618
00:38:12,320 --> 00:38:15,120
And what that platform entails is a few parts.

619
00:38:15,120 --> 00:38:20,800
It's the tagging of putting essentially traits onto accounts, and then associating those

620
00:38:20,800 --> 00:38:25,240
similar traits or tags that are on counts to the models so that the models are only surfaced

621
00:38:25,240 --> 00:38:29,800
for those accounts, or those customers, sorry, I'm talking such cable lingo.

622
00:38:29,800 --> 00:38:34,880
So yeah, so it facilitates that, it orchestrates allowing different models to be available for

623
00:38:34,880 --> 00:38:36,520
different customers.

624
00:38:36,520 --> 00:38:42,520
And then the second part is the metrics aspect, which is deriving analytics on how the

625
00:38:42,520 --> 00:38:47,160
customers are actually interacting with those new rows or those new content, the new content

626
00:38:47,160 --> 00:38:48,160
that's being surfaced.

627
00:38:48,160 --> 00:38:54,320
So your A-B testing is primarily kind of at scale across the client base.

628
00:38:54,320 --> 00:38:55,320
Yeah.

629
00:38:55,320 --> 00:39:00,480
The luxuries that we have millions of subscribers, so the sample size, we could do something

630
00:39:00,480 --> 00:39:03,160
like we're A-B testing something where it's like a million.

631
00:39:03,160 --> 00:39:08,400
So there's never a doubt whether this meets statistical significance because our sample

632
00:39:08,400 --> 00:39:09,800
sizes are so huge.

633
00:39:09,800 --> 00:39:14,080
We talked a little bit about kind of evaluation metrics before in the context of when you

634
00:39:14,080 --> 00:39:19,440
kind of instrumented your previous model, what kind of instrumentation do you have on the

635
00:39:19,440 --> 00:39:22,960
current model and how does that play into your A-B testing system?

636
00:39:22,960 --> 00:39:23,960
Right.

637
00:39:23,960 --> 00:39:27,240
So they're actually, they're very tangential to each other.

638
00:39:27,240 --> 00:39:34,560
The A-B testing framework is more so on the, I guess the front office side of our platform

639
00:39:34,560 --> 00:39:40,760
whereas the orchestration of our models is more of our back office side because if at some

640
00:39:40,760 --> 00:39:45,480
point one of the models isn't performing as we expect, we need to be able to easily turn

641
00:39:45,480 --> 00:39:48,760
that off for customers.

642
00:39:48,760 --> 00:39:55,160
So utilizing what the web service to be able to gate us from doing that, to enable us

643
00:39:55,160 --> 00:40:00,960
to do that to gate different features from different customers, I guess is the key difference.

644
00:40:00,960 --> 00:40:03,480
That makes sense.

645
00:40:03,480 --> 00:40:09,560
So meaning you're collecting metrics on model performance and primarily consuming, consuming

646
00:40:09,560 --> 00:40:16,040
those metrics within the web service tier so that you can take action if predictions

647
00:40:16,040 --> 00:40:17,040
start to degrade.

648
00:40:17,040 --> 00:40:18,040
Yeah, quicker action.

649
00:40:18,040 --> 00:40:19,040
Right.

650
00:40:19,040 --> 00:40:24,760
And it's not like a historic, like for the evaluation of models and typically what we call

651
00:40:24,760 --> 00:40:29,320
the back office side, we have like historical usage, right?

652
00:40:29,320 --> 00:40:33,000
For A-B testing, we really just care of what just happened the past day or the past two

653
00:40:33,000 --> 00:40:34,520
days or the past week.

654
00:40:34,520 --> 00:40:40,680
So keeping that small subset of usage from the recent time frame is a lot easier on the

655
00:40:40,680 --> 00:40:44,600
web service side than it would be in the, if we were keeping historical data.

656
00:40:44,600 --> 00:40:45,600
Got it, got it.

657
00:40:45,600 --> 00:40:49,840
So you're the A-B testing is pure kind of online.

658
00:40:49,840 --> 00:40:55,000
You've got two models in flight, you've labeled traffic and you're serving up different models

659
00:40:55,000 --> 00:40:59,920
and you're just comparing very short-term results between those models.

660
00:40:59,920 --> 00:41:00,920
Exactly.

661
00:41:00,920 --> 00:41:06,600
What do you tend to see in terms of model, shelf life, model degradation, that kind of thing

662
00:41:06,600 --> 00:41:10,880
for these kind of recommendation models that you're building?

663
00:41:10,880 --> 00:41:13,160
That's a good question.

664
00:41:13,160 --> 00:41:20,600
So the biggest, I guess, issue that I've seen is the frequency in which we need to train

665
00:41:20,600 --> 00:41:22,000
these models.

666
00:41:22,000 --> 00:41:26,240
If the customer keeps seeing the same exact pre-computer recommendations over and over

667
00:41:26,240 --> 00:41:28,440
again, it gets kind of old, right?

668
00:41:28,440 --> 00:41:35,640
So being able to frequently update the pre-computed recommendations that are produced by our models

669
00:41:35,640 --> 00:41:39,000
is something that we're trying to experiment on how we could do more often.

670
00:41:39,000 --> 00:41:44,000
For example, waiting higher, what you just just watched for something that you've watched

671
00:41:44,000 --> 00:41:45,920
six months ago.

672
00:41:45,920 --> 00:41:50,120
That's something that we're trying to evaluate how we could not overemphasize something

673
00:41:50,120 --> 00:41:55,040
that you just watched, but some sweet spot where it's just enough where you see the impact

674
00:41:55,040 --> 00:41:56,640
and what you're being served.

675
00:41:56,640 --> 00:41:58,880
The last piece of the puzzle was infrastructure.

676
00:41:58,880 --> 00:41:59,880
Right.

677
00:41:59,880 --> 00:42:04,360
We kind of discussed that a bit with everything that we mentioned with the AWS or public

678
00:42:04,360 --> 00:42:05,360
cloud.

679
00:42:05,360 --> 00:42:09,520
But it was fairly straightforward as the legacy platform that we'd built was primarily

680
00:42:09,520 --> 00:42:14,440
running on this physical infrastructure, and we weren't at the point where we could scale

681
00:42:14,440 --> 00:42:18,240
that out where we could add more nodes to the Hadoop cluster.

682
00:42:18,240 --> 00:42:22,640
So the easiest decision was moving up to the public cloud, AWS.

683
00:42:22,640 --> 00:42:27,440
Of course, with every new decision where you make or you're dramatically changing something,

684
00:42:27,440 --> 00:42:31,520
some aspect of your platform, you find new problems, your old problems go away, but you

685
00:42:31,520 --> 00:42:32,520
get new problems.

686
00:42:32,520 --> 00:42:36,360
And one of those is, it's very easy to spend money in AWS.

687
00:42:36,360 --> 00:42:41,480
It's very easy to lose money in AWS, so that was one of the struggles that we've been

688
00:42:41,480 --> 00:42:42,480
working through.

689
00:42:42,480 --> 00:42:46,960
And another aspect is it's very easy just to start picking new technologies.

690
00:42:46,960 --> 00:42:52,400
There's something new in AWS every month that feels like maintaining the, yeah, at least

691
00:42:52,400 --> 00:42:57,240
maintaining the key tech stack, and then not trying to move too far from it.

692
00:42:57,240 --> 00:43:01,720
Otherwise, you're just going to build this, this ginormous technical debt where you're

693
00:43:01,720 --> 00:43:05,240
continually slaying the learn and then update and then you just realize, like, wait, like

694
00:43:05,240 --> 00:43:10,440
there's this example, we went down the path of utilizing, before we went down lambdas,

695
00:43:10,440 --> 00:43:14,320
we decided to use Kafka Connect for consuming data from Kafka.

696
00:43:14,320 --> 00:43:19,360
And because that wasn't specifically managed by AWS, there was a lot of inherent difficulties

697
00:43:19,360 --> 00:43:23,600
with monitoring and getting the logs and just seeing how it's performing that we realized

698
00:43:23,600 --> 00:43:25,320
we can't just pick any technology.

699
00:43:25,320 --> 00:43:29,280
We have to really think of the technology that we picked since we're in this public cloud

700
00:43:29,280 --> 00:43:30,280
space.

701
00:43:30,280 --> 00:43:34,400
So yeah, so I was, there's actually whenever I talk about infrastructure and I ever make

702
00:43:34,400 --> 00:43:40,320
the case for more budget, I often referenced this, there was this paper that was, that was

703
00:43:40,320 --> 00:43:44,600
published at the NIPPS conference by these Google developers, it was something on the lines.

704
00:43:44,600 --> 00:43:49,320
I've read the exact titles, like hidden technical debt in machine learning systems or machine

705
00:43:49,320 --> 00:43:50,320
learning platforms.

706
00:43:50,320 --> 00:43:53,560
Machine learning is the high interest credit card of technical debt.

707
00:43:53,560 --> 00:43:54,560
Yeah, exactly.

708
00:43:54,560 --> 00:43:55,560
Yeah, exactly.

709
00:43:55,560 --> 00:43:57,040
It's my favorite paper.

710
00:43:57,040 --> 00:43:58,040
It's a great paper.

711
00:43:58,040 --> 00:44:02,720
So I guess that I often reference it when I have to make the case for our cloud infrastructure

712
00:44:02,720 --> 00:44:08,520
needs more budget or you need to do this because it essentially explains that it's quite,

713
00:44:08,520 --> 00:44:13,800
it's really easy to start building technical debt when implementing a platform like this,

714
00:44:13,800 --> 00:44:15,880
regardless of any platform.

715
00:44:15,880 --> 00:44:20,280
And in that paper, they go into detail, the risk factors, you know, data dependency and

716
00:44:20,280 --> 00:44:23,320
configuration issues, things that we've talked about today.

717
00:44:23,320 --> 00:44:28,080
What I also really like about that paper is that it like specifically states that like machine

718
00:44:28,080 --> 00:44:32,560
learning systems have a special capacity for like acquiring this technical debt.

719
00:44:32,560 --> 00:44:37,360
I mean, a lot of platforms that, you know, that was written with code do this, but then

720
00:44:37,360 --> 00:44:42,920
there's an additional set of debt that can be gained from machine learning issues.

721
00:44:42,920 --> 00:44:48,320
And I think moving our infrastructure to AWS, we realize that really quickly.

722
00:44:48,320 --> 00:44:56,120
And so how do you think that the machine learning in the cloud kind of accentuates that?

723
00:44:56,120 --> 00:45:02,000
Well, of course, there's the budget aspect and then even there's just there are easy ways

724
00:45:02,000 --> 00:45:07,520
where we, it's really easy to deploy to the cloud.

725
00:45:07,520 --> 00:45:12,280
And whereas before there were always constraints where, oh, we don't have enough space, like

726
00:45:12,280 --> 00:45:15,320
we're just, we're just going to keep collecting data and then we're going to just delete

727
00:45:15,320 --> 00:45:20,080
the last, you know, the most historical data we have that's not a problem in AWS.

728
00:45:20,080 --> 00:45:24,840
Like, of course, there's glacier and there are ways to archive data, but you can kind

729
00:45:24,840 --> 00:45:29,200
of keep, you know, adding and picking up the latest and greatest, you know, version of

730
00:45:29,200 --> 00:45:33,720
Spark and then just releasing with that and not realize that you're, you're kind of building

731
00:45:33,720 --> 00:45:38,680
this platform that has so many different pieces because it's easy to just hop on and,

732
00:45:38,680 --> 00:45:45,640
you know, spin up an EC2 instance do that and or, you know, spin up an EMR instance to run

733
00:45:45,640 --> 00:45:49,200
this clustering algorithm and then you kind of just forget about it, whereas in a physical

734
00:45:49,200 --> 00:45:53,520
infrastructure, you can only do as much as you're allowed to, right?

735
00:45:53,520 --> 00:45:54,520
Otherwise, right?

736
00:45:54,520 --> 00:45:55,520
Right, right.

737
00:45:55,520 --> 00:45:56,520
You have backlog of jobs waiting.

738
00:45:56,520 --> 00:45:57,840
Yeah, that's really interesting.

739
00:45:57,840 --> 00:46:02,720
There's a story in there somewhere like the double edged sort of agility in the cloud

740
00:46:02,720 --> 00:46:03,720
like you.

741
00:46:03,720 --> 00:46:04,720
Right.

742
00:46:04,720 --> 00:46:09,960
There's friction in the real world, not the real world because cloud is real for sure

743
00:46:09,960 --> 00:46:15,760
at this point, but the traditional, there's friction and inefficiencies, I guess, that

744
00:46:15,760 --> 00:46:21,800
makes you think about the way you do things and cloud in taking away some of that makes

745
00:46:21,800 --> 00:46:26,120
it easy to not think about some of the things you're doing, which leads to debt.

746
00:46:26,120 --> 00:46:27,120
That's really interesting.

747
00:46:27,120 --> 00:46:28,120
Very well said.

748
00:46:28,120 --> 00:46:29,120
Thank you.

749
00:46:29,120 --> 00:46:32,880
Those are essentially the four parts of resurrecting a recommendation platform.

750
00:46:32,880 --> 00:46:33,880
Nice.

751
00:46:33,880 --> 00:46:37,160
How do you characterize the business impact of all this?

752
00:46:37,160 --> 00:46:38,320
That's a great question.

753
00:46:38,320 --> 00:46:43,040
So that was actually the biggest question I had in all honesty when I first switched over

754
00:46:43,040 --> 00:46:47,760
this team about 14 months ago is I just couldn't see the value that these recommendations were

755
00:46:47,760 --> 00:46:48,760
producing.

756
00:46:48,760 --> 00:46:54,600
I mean, I saw that we are recommending content, but we weren't really measuring it.

757
00:46:54,600 --> 00:46:59,120
Were we taking to account what the business wants us to recommend?

758
00:46:59,120 --> 00:47:00,480
So that's something that we've been playing with a lot.

759
00:47:00,480 --> 00:47:04,960
We've been specifically building models that are taking into account what the business

760
00:47:04,960 --> 00:47:05,960
wants.

761
00:47:05,960 --> 00:47:11,560
For example, promoting content that is specifically given to us by a provider.

762
00:47:11,560 --> 00:47:15,800
Let's see if we can figure out the customers who are likely to watch that and include

763
00:47:15,800 --> 00:47:19,920
that in their recommendations, whereas previously it would have just been at the bottom of the

764
00:47:19,920 --> 00:47:20,920
barrel.

765
00:47:20,920 --> 00:47:21,920
We would have never looked at it.

766
00:47:21,920 --> 00:47:26,280
So definitely taking into account what the business goals are we've been trying to consider

767
00:47:26,280 --> 00:47:30,560
on the recommendations team, and also from a business perspective, how they're being

768
00:47:30,560 --> 00:47:33,760
surfaced, there's something that we've been abtesting.

769
00:47:33,760 --> 00:47:37,200
It's just the names of the rows we're serving content.

770
00:47:37,200 --> 00:47:38,360
We're going down the path.

771
00:47:38,360 --> 00:47:43,600
We are on the path in surfacing because you watched Wonder Woman, how does that play against

772
00:47:43,600 --> 00:47:48,280
a row that's surfacing the same content, but it's called top movie picks for you.

773
00:47:48,280 --> 00:47:53,520
So leveraging how the business works and what works best and taking that into account

774
00:47:53,520 --> 00:47:58,720
in our machine learning platform and utilizing abtesting is kind of the big picture to see

775
00:47:58,720 --> 00:48:04,160
if we can increase hours watched or increase engagement by making small tweaks this platform.

776
00:48:04,160 --> 00:48:05,680
Well, Lima, thank you so much.

777
00:48:05,680 --> 00:48:08,000
This is a really, really fun conversation.

778
00:48:08,000 --> 00:48:09,000
Thank you.

779
00:48:09,000 --> 00:48:10,160
Thanks for having me.

780
00:48:10,160 --> 00:48:14,200
All right, everyone.

781
00:48:14,200 --> 00:48:15,800
That's our show for today.

782
00:48:15,800 --> 00:48:20,880
For more information on Lima or any of the topics covered in this episode, head over

783
00:48:20,880 --> 00:48:25,760
to twimmelai.com slash talk slash 201.

784
00:48:25,760 --> 00:48:32,200
To learn more about our AI platform series or to download our eBooks, visit twimmelai.com

785
00:48:32,200 --> 00:48:34,560
slash AI platforms.

786
00:48:34,560 --> 00:49:00,440
As always, thanks so much for listening and catch you next time.

