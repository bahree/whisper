1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,720
I'm your host, Sam Charrington.

4
00:00:31,720 --> 00:00:36,680
In this, the final episode of our Strata Data Conference series, we're joined by Zachary

5
00:00:36,680 --> 00:00:41,000
Hanif, director of machine learning at Capital One.

6
00:00:41,000 --> 00:00:45,440
Zach led a session at Strata called Network Effects, working with modern graph analytical

7
00:00:45,440 --> 00:00:50,160
systems, which we had a great chat about back in New York.

8
00:00:50,160 --> 00:00:53,680
We start our discussion with a look at the role of graph analytics in the machine learning

9
00:00:53,680 --> 00:00:58,600
toolkit, including some important application areas for graph based systems.

10
00:00:58,600 --> 00:01:02,640
We continue with an overview of the different ways to implement graph analytics with a

11
00:01:02,640 --> 00:01:07,960
particular emphasis on the emerging role of what he calls graphical processing engines,

12
00:01:07,960 --> 00:01:11,240
which are systems that excel at handling large data sets.

13
00:01:11,240 --> 00:01:15,560
We also discussed the relationship between graph analytics and probabilistic graphical

14
00:01:15,560 --> 00:01:22,960
models, graphical embedding models, and graph convolutional neural networks in deep learning.

15
00:01:22,960 --> 00:01:26,920
Capital One is a long time supporter of my work in this podcast, and I'd like to spend

16
00:01:26,920 --> 00:01:30,720
a big shout out to them for sponsoring this series.

17
00:01:30,720 --> 00:01:35,600
At the NIPPS conference in Montreal this December, Capital One will be co-hosting a workshop

18
00:01:35,600 --> 00:01:40,720
focused on challenges and opportunities for AI and financial services and the impact

19
00:01:40,720 --> 00:01:45,040
of fairness, explainability, accuracy, and privacy.

20
00:01:45,040 --> 00:01:48,840
A call for papers is open now through October 25th.

21
00:01:48,840 --> 00:01:54,400
For more information on submissions, visit twimmolai.com slash C1 NIPPS.

22
00:01:54,400 --> 00:01:57,680
The letter C, the number one NIPPS.

23
00:01:57,680 --> 00:02:02,480
Also a huge thanks to our friends at Cloudera, who also sponsor this series.

24
00:02:02,480 --> 00:02:06,840
If you didn't catch my interview with Cloudera's Justin Norman earlier in this series, you'll

25
00:02:06,840 --> 00:02:09,520
definitely want to check it out.

26
00:02:09,520 --> 00:02:14,080
Cloudera's modern platform for machine learning and analytics, optimized for the cloud,

27
00:02:14,080 --> 00:02:21,120
that you build and deploy AI solutions at scale, efficiently and securely, anywhere you want.

28
00:02:21,120 --> 00:02:26,600
In addition, Cloudera Fast Forward Labs' expert guidance helps you realize your AI future

29
00:02:26,600 --> 00:02:28,000
faster.

30
00:02:28,000 --> 00:02:33,360
To learn more, visit Cloudera's machine learning resource center at Cloudera.com slash

31
00:02:33,360 --> 00:02:34,880
ML.

32
00:02:34,880 --> 00:02:39,040
If you're a fan of this show, please show some love to our sponsors and make sure to

33
00:02:39,040 --> 00:02:46,600
tell them that Twimmol sent you and now on to the show.

34
00:02:46,600 --> 00:02:47,600
All right, everyone.

35
00:02:47,600 --> 00:02:52,400
I am at Strata in New York City and I am here with Zachary Hanif.

36
00:02:52,400 --> 00:02:57,280
Zach is the director of machine learning at C4ML.

37
00:02:57,280 --> 00:03:01,400
That's the center for machine learning at Capital One.

38
00:03:01,400 --> 00:03:03,680
Zach, welcome to this week in machine learning and AI.

39
00:03:03,680 --> 00:03:04,680
Thank you for having me.

40
00:03:04,680 --> 00:03:05,680
It's pleasure to be here.

41
00:03:05,680 --> 00:03:06,680
Absolutely.

42
00:03:06,680 --> 00:03:07,680
Absolutely.

43
00:03:07,680 --> 00:03:09,320
I'm going to go back a little bit.

44
00:03:09,320 --> 00:03:17,760
He was a guest's presenter at the AI summit that I did in actually not even the AI summit.

45
00:03:17,760 --> 00:03:19,760
It was the future of data summit.

46
00:03:19,760 --> 00:03:25,240
It was a year and a half ago or something at this point.

47
00:03:25,240 --> 00:03:30,640
Why don't we start out by having you introduce yourself to the audience?

48
00:03:30,640 --> 00:03:31,640
Sure.

49
00:03:31,640 --> 00:03:32,640
No problem.

50
00:03:32,640 --> 00:03:39,400
As I've said, my name is Zach, I've spent probably about the last decade working inside

51
00:03:39,400 --> 00:03:43,440
of machine learning and large scale data analytics.

52
00:03:43,440 --> 00:03:49,040
The specific domain that I traditionally worked in has been in the area of security and

53
00:03:49,040 --> 00:03:56,400
kind of adversarial domains, so paying attention to cyber security, fraud, money laundering,

54
00:03:56,400 --> 00:03:57,400
all sorts of things like that.

55
00:03:57,400 --> 00:04:02,800
You kind of play a cat and mouse game with the entity behind your data to try and keep

56
00:04:02,800 --> 00:04:05,000
something safe and to keep it protected.

57
00:04:05,000 --> 00:04:09,240
I've been doing that for about 10 years in a variety of different contexts and for the

58
00:04:09,240 --> 00:04:16,600
last two, two and a half years now at Capital One, I've been a leader in Capital One Center

59
00:04:16,600 --> 00:04:22,080
for Machine Learning, helping Capital One kind of bring machine learning into every single

60
00:04:22,080 --> 00:04:28,120
corner, every little area of the business as a whole to kind of spread out the future,

61
00:04:28,120 --> 00:04:29,120
so to speak.

62
00:04:29,120 --> 00:04:30,120
Awesome.

63
00:04:30,120 --> 00:04:31,120
Awesome.

64
00:04:31,120 --> 00:04:37,080
And I interviewed Adam Wenzhel, who heads up that center not too long ago on the podcast,

65
00:04:37,080 --> 00:04:42,360
so for folks that want to learn more about the center and what the center is up to.

66
00:04:42,360 --> 00:04:47,920
We had a really great conversation about how an organization like Capital One particularly

67
00:04:47,920 --> 00:04:53,080
with all of the constraints that a financial services organization has, how an organization

68
00:04:53,080 --> 00:04:58,360
like that builds out a capability for data science and machine learning, and we'll link

69
00:04:58,360 --> 00:05:04,960
to that show in the show notes, but that you've got a presentation later today actually.

70
00:05:04,960 --> 00:05:06,720
What's the title of your presentation?

71
00:05:06,720 --> 00:05:11,720
So my presentation is about doing graph analytics over large data sets, and it's a survey at

72
00:05:11,720 --> 00:05:18,480
the end of the day of modern open source technologies and techniques for doing graph analytics

73
00:05:18,480 --> 00:05:20,840
and machine learning over graphs.

74
00:05:20,840 --> 00:05:27,120
I'll be perfectly honest, the title suddenly escapes me, Sam knows, I just got off the

75
00:05:27,120 --> 00:05:32,120
train coming up from the DC region, which is where I'm based out of, so my head's a little

76
00:05:32,120 --> 00:05:33,760
all over the place.

77
00:05:33,760 --> 00:05:38,800
But the talk's a whole is kind of an examination of how we use graph analytics and how I've

78
00:05:38,800 --> 00:05:43,880
used graph analytics in the past to solve a lot of very hairy problems, especially inside

79
00:05:43,880 --> 00:05:48,320
of security and cyber defense domains.

80
00:05:48,320 --> 00:05:54,800
Why don't we start by having you give us some examples to contextualize what motivated

81
00:05:54,800 --> 00:05:56,680
this look into graph analytics.

82
00:05:56,680 --> 00:06:00,280
What are some of the use cases where it's been helpful for you?

83
00:06:00,280 --> 00:06:01,280
Absolutely.

84
00:06:01,280 --> 00:06:06,480
So graphs are helpful really anywhere where the connectivity of your nodes, the connectivity

85
00:06:06,480 --> 00:06:10,760
of your data is relevant, the topology of the data is relevant.

86
00:06:10,760 --> 00:06:17,560
Many fields have data sets where individual records really aren't related to each other.

87
00:06:17,560 --> 00:06:21,520
There's a very well known trial data set, which pretty much any machine learning engineer

88
00:06:21,520 --> 00:06:25,400
probably deals with at least once when they're getting an understanding of how to do machine

89
00:06:25,400 --> 00:06:27,000
learning in a practical sense.

90
00:06:27,000 --> 00:06:33,040
And it's this data set of flowers, some researchers somewhere went out and collected the pedal lengths

91
00:06:33,040 --> 00:06:38,360
and widths and other as physical dimensions of flowers in a study in an attempt to determine

92
00:06:38,360 --> 00:06:43,520
whether or not machine learning could differentiate different types of flowers from each other.

93
00:06:43,520 --> 00:06:47,720
In data sets like that, those data sets don't really have, each element of data doesn't

94
00:06:47,720 --> 00:06:52,920
have a relationship with any other element outside of the class that that particular row

95
00:06:52,920 --> 00:06:54,760
represents.

96
00:06:54,760 --> 00:07:00,480
Graphs are useful in kind of the opposite area where every single, every single element that

97
00:07:00,480 --> 00:07:06,000
you have has a relationship or some kind of interconnectivity with other other elements

98
00:07:06,000 --> 00:07:07,200
in your data set.

99
00:07:07,200 --> 00:07:11,800
So where measurements of flowers aren't really interrelated in and of themselves, the

100
00:07:11,800 --> 00:07:16,960
relationships between people or the relationships between entities definitely have a great deal

101
00:07:16,960 --> 00:07:23,160
of contextual information embedded inside of their, the relationship itself, right?

102
00:07:23,160 --> 00:07:29,000
So to kind of put a fine point on it, you and I have a relationship, obviously as a result

103
00:07:29,000 --> 00:07:33,640
of our previous collaborations and as a result of this particular discussion here, now

104
00:07:33,640 --> 00:07:38,200
you and I both have individual elements of data associated just with ourselves, right?

105
00:07:38,200 --> 00:07:43,800
We're different people, obviously, but we are interconnected and that interconnection

106
00:07:43,800 --> 00:07:48,560
in and of itself contains valuable information that we can learn from, which we wouldn't

107
00:07:48,560 --> 00:07:53,160
be able to learn if we just simply looked at yourself or myself kind of in isolation.

108
00:07:53,160 --> 00:07:54,160
Does that make sense?

109
00:07:54,160 --> 00:07:55,160
It does.

110
00:07:55,160 --> 00:07:59,240
Do you make it more concrete by talking about specific business applications that you've

111
00:07:59,240 --> 00:08:00,240
looked at?

112
00:08:00,240 --> 00:08:01,240
Absolutely.

113
00:08:01,240 --> 00:08:08,560
Right now, my group is using graph analytics to gain a better understanding of money laundering

114
00:08:08,560 --> 00:08:14,120
and other financial crimes that people attempt to commit using the modern financial system.

115
00:08:14,120 --> 00:08:18,040
And as Capital One is very much a part of the modern financial system, we will occasionally

116
00:08:18,040 --> 00:08:23,200
have individuals attempt to perform criminal activity, your laundrom money or something like

117
00:08:23,200 --> 00:08:29,040
that, you know, inappropriately using Capital One's capabilities, right?

118
00:08:29,040 --> 00:08:34,640
And so our job is to utilize graph analytics to detect this kind of activity occurring and

119
00:08:34,640 --> 00:08:39,400
make sure that it is dealt with in an appropriate fashion so that it doesn't affect either

120
00:08:39,400 --> 00:08:44,200
any of Capital One's customers, anyone else in the United States financial system or doesn't

121
00:08:44,200 --> 00:08:50,560
pose the kind of moral and legalistic threat that the activities that generate this kind,

122
00:08:50,560 --> 00:08:52,640
these kind of funds can represent.

123
00:08:52,640 --> 00:08:59,440
Yeah, I imagine folks that are outside of the financial services industry might not have

124
00:08:59,440 --> 00:09:06,080
an appreciation for the amount of regulatory pressure that banks are under with regards

125
00:09:06,080 --> 00:09:13,520
to anti-money laundering AML, but also the amount of investment that banks make in fighting

126
00:09:13,520 --> 00:09:14,520
money laundering.

127
00:09:14,520 --> 00:09:16,840
It comes up in a lot of different areas.

128
00:09:16,840 --> 00:09:19,640
Yeah, it's definitely very important.

129
00:09:19,640 --> 00:09:24,560
Capital One's regulators have a very vested and very good interest in making sure that

130
00:09:24,560 --> 00:09:28,840
the financial security of the United States remains strong.

131
00:09:28,840 --> 00:09:33,400
And so they've definitely taken an appropriate stance in making sure that we are vigilant

132
00:09:33,400 --> 00:09:38,160
about making sure the activity that goes on through our bank is appropriate.

133
00:09:38,160 --> 00:09:44,480
And we take that responsibility very, very seriously, investing both internally and working

134
00:09:44,480 --> 00:09:48,080
with, you know, working collaboratively with academics and other researchers and other

135
00:09:48,080 --> 00:09:53,400
organizations who specialize in this to make sure that we're as correct as we can possibly

136
00:09:53,400 --> 00:09:54,400
be.

137
00:09:54,400 --> 00:10:00,400
Because I mean, there's a lot of, it's a very significant responsibility on our part.

138
00:10:00,400 --> 00:10:04,760
And so you found graph analytics to be useful here.

139
00:10:04,760 --> 00:10:11,240
When I hear graph come up, I tend to immediately think of a handful of different things on the

140
00:10:11,240 --> 00:10:18,280
one end of the spectrum, I think of graphical models like from a machine learning perspective,

141
00:10:18,280 --> 00:10:21,560
but I also think of graphical databases.

142
00:10:21,560 --> 00:10:24,680
It sounds like you're incorporating graphical databases for sure.

143
00:10:24,680 --> 00:10:26,840
Are you incorporating graphical models as well?

144
00:10:26,840 --> 00:10:30,680
We're actually incorporating a lot of things across that overall spectrum, right?

145
00:10:30,680 --> 00:10:34,440
Everything from graphical models, which would be things like obviously graphical,

146
00:10:34,440 --> 00:10:35,440
basic networks.

147
00:10:35,440 --> 00:10:39,960
There are some more modern techniques being used inside of the deep learning space for

148
00:10:39,960 --> 00:10:42,400
generating graphical embeddings.

149
00:10:42,400 --> 00:10:47,560
And of course, the traditional algorithms in the area of performing label and belief propagation,

150
00:10:47,560 --> 00:10:48,560
right?

151
00:10:48,560 --> 00:10:52,040
These are all different areas that you can apply inside of the probabilistic graph modeling

152
00:10:52,040 --> 00:10:53,040
space.

153
00:10:53,040 --> 00:10:54,200
We absolutely do do that.

154
00:10:54,200 --> 00:10:58,480
In addition to that, we're working not just with graph databases in the vein of say something

155
00:10:58,480 --> 00:11:03,600
like Neo4j or something like Titan or Janus graph, for example, we're also working with

156
00:11:03,600 --> 00:11:10,560
graph processing engines, such as the material inside of Giraffe, Apache Giraffe, I should

157
00:11:10,560 --> 00:11:14,560
say, Apache Spark, a GraphX, GraphFrames.

158
00:11:14,560 --> 00:11:20,800
I believe Flink even has a graph API behind it, and even some more esoteric systems as

159
00:11:20,800 --> 00:11:21,800
well.

160
00:11:21,800 --> 00:11:26,320
So we're really trying to cover the entirety of that spectrum because having a diversity

161
00:11:26,320 --> 00:11:31,480
in our modeling approaches really allows us to kind of explore the entire space, and

162
00:11:31,480 --> 00:11:35,200
it gives us lots of options to try and find the best way to solve the various problems

163
00:11:35,200 --> 00:11:37,200
that we're attempting to address.

164
00:11:37,200 --> 00:11:39,040
So what's the best way to walk through that?

165
00:11:39,040 --> 00:11:40,360
How do you cover any of your talk?

166
00:11:40,360 --> 00:11:44,000
Do you kind of go bottom up or top down or use case out?

167
00:11:44,000 --> 00:11:49,720
I'm actually going in my talk, starting with kind of going in the direction of starting

168
00:11:49,720 --> 00:11:55,680
with graph processing engines, comparing and contrasting them with graph databases, and

169
00:11:55,680 --> 00:12:00,760
then finally talking about what is kind of a pet project that we've got going on right

170
00:12:00,760 --> 00:12:05,360
now, which is exploring neural network applications inside of graphs.

171
00:12:05,360 --> 00:12:10,360
I chose that flow because of a conversation that I actually had about four or five years

172
00:12:10,360 --> 00:12:11,520
ago.

173
00:12:11,520 --> 00:12:18,120
Four or five years ago, I was working with a couple of my colleagues at another organization

174
00:12:18,120 --> 00:12:24,320
and preparing a talk on using graphs inside of malware networks.

175
00:12:24,320 --> 00:12:27,160
So getting an understanding of how malware is interrelated.

176
00:12:27,160 --> 00:12:30,440
It was a different role that I was in, a different company at the time, and I was talking

177
00:12:30,440 --> 00:12:33,960
with some of my co-workers at this organization.

178
00:12:33,960 --> 00:12:39,600
And one of them mentioned to me that they were curious to know how we were going to make

179
00:12:39,600 --> 00:12:45,280
this kind of graphical model scale because he said that in his experience, there were

180
00:12:45,280 --> 00:12:49,200
a number of graph databases and they didn't scale very well at a certain level of nodes

181
00:12:49,200 --> 00:12:52,480
and edges, and he said, well, how are we going to get around this, right?

182
00:12:52,480 --> 00:12:56,120
And over the course of this conversation, it kind of became, I think it became clear

183
00:12:56,120 --> 00:13:01,520
that both of us, there's a much wider variety of different things inside of the graph

184
00:13:01,520 --> 00:13:05,520
analysis space that not everyone is completely aware of.

185
00:13:05,520 --> 00:13:10,960
Certainly because of events like strata and the larger big data movement that came through

186
00:13:10,960 --> 00:13:15,240
several years ago and is still very much ongoing, we've got a really great understanding

187
00:13:15,240 --> 00:13:18,360
of batch processing and stream processing systems.

188
00:13:18,360 --> 00:13:22,480
But graphs are still kind of a niche application inside of that space.

189
00:13:22,480 --> 00:13:26,760
And so kind of as a direct result, when people think graphs, they think graph databases,

190
00:13:26,760 --> 00:13:32,720
and they naturally think of some of the scalability concerns of graph databases in that space.

191
00:13:32,720 --> 00:13:40,160
And they're not always aware of kind of the related material inside of graph processing

192
00:13:40,160 --> 00:13:44,640
engines and some of the probabilistic graphical models and other ways to kind of solve and

193
00:13:44,640 --> 00:13:50,640
tackle this problem while dealing with a model in a graph space.

194
00:13:50,640 --> 00:13:55,560
So let's walk through this, the way that you do in your presentation, graphical processing

195
00:13:55,560 --> 00:13:56,880
engine.

196
00:13:56,880 --> 00:13:58,880
What are they doing for us?

197
00:13:58,880 --> 00:14:00,600
How do they help you solve these problems?

198
00:14:00,600 --> 00:14:05,360
So any kind of graph computation engine, graph processing engine is going to be a system

199
00:14:05,360 --> 00:14:12,680
that's similar to the way we think about the relationship between Spark and Hadoop or

200
00:14:12,680 --> 00:14:14,400
HDFS, right?

201
00:14:14,400 --> 00:14:20,600
You have some system that works on some series of data either in memory or with disk flushes,

202
00:14:20,600 --> 00:14:27,000
cashing, and it allows you to model your data as a graph and operate it, operate on it

203
00:14:27,000 --> 00:14:32,960
with a graph DSL or using graph formalisms, right?

204
00:14:32,960 --> 00:14:39,200
And so a couple of clear examples in the software space are graph X, which is attached to the

205
00:14:39,200 --> 00:14:46,440
Spark project, graph frames, which is a library that's being built into and around graph X,

206
00:14:46,440 --> 00:14:53,520
a Pachi giraffe, which is a BSP style graph computation engine that's modeled after Google's

207
00:14:53,520 --> 00:14:54,920
prequel.

208
00:14:54,920 --> 00:15:00,000
There's a now defunct project, I believe, called a Pachi Hama, or if it's still ongoing,

209
00:15:00,000 --> 00:15:02,720
it's definitely died down in its activity.

210
00:15:02,720 --> 00:15:06,280
You said BSP, BSP bulk synchronous parallel.

211
00:15:06,280 --> 00:15:12,480
It's a model of computation, which is very extensible and applies well to working with

212
00:15:12,480 --> 00:15:15,080
graphs in a distributed environment.

213
00:15:15,080 --> 00:15:19,240
The works primarily off of the concept of performing message passing between individual

214
00:15:19,240 --> 00:15:24,680
nodes or vertices in the graph and passing messages along the individual connective

215
00:15:24,680 --> 00:15:25,680
edges.

216
00:15:25,680 --> 00:15:31,240
It allows you to take a vertex centric view of your graph, which allows you to calculate

217
00:15:31,240 --> 00:15:38,440
aspects of each individual vertex based on the connectivity of that vertex to its neighbors.

218
00:15:38,440 --> 00:15:43,480
A good example of this would be something like belief propagation or reputation propagation,

219
00:15:43,480 --> 00:15:48,960
where individual entities may or may not have some known level of trust inside of the

220
00:15:48,960 --> 00:15:49,960
graph.

221
00:15:49,960 --> 00:15:55,240
They propagate that trust to everyone else around them for multiple humps with some kind

222
00:15:55,240 --> 00:15:57,080
of decay value attached.

223
00:15:57,080 --> 00:16:02,600
What this functionally allows people to do is allows them to label a set of nodes, propagate

224
00:16:02,600 --> 00:16:06,560
those labels and say, based on the things I know about and based on what they're connected

225
00:16:06,560 --> 00:16:11,680
to, how do we start adding labels to those other nodes?

226
00:16:11,680 --> 00:16:16,840
It allows us to, specifically inside of the work that we're doing right now, it allows

227
00:16:16,840 --> 00:16:22,560
us to say, hey, we suspect that this particular entity of performing some kind of money laundering

228
00:16:22,560 --> 00:16:27,280
or fraudulent activity, and I've seen that this person has transacted with 10 people

229
00:16:27,280 --> 00:16:28,280
around them.

230
00:16:28,280 --> 00:16:32,280
I don't have any view of the 10 people's reputation around this individual.

231
00:16:32,280 --> 00:16:34,080
What should my view be?

232
00:16:34,080 --> 00:16:37,680
So it's kind of a suspicion by association model.

233
00:16:37,680 --> 00:16:50,440
The way you describe the BSP, it makes me think of, as opposed to a top-down analytical

234
00:16:50,440 --> 00:16:56,840
operation on this graph, you're almost treating the graph as a distributed system.

235
00:16:56,840 --> 00:17:04,160
And each of the vertices in the graph is transacting with its neighbors and accumulating,

236
00:17:04,160 --> 00:17:08,680
for example, certain properties, and then you allow this to happen over time, and then

237
00:17:08,680 --> 00:17:15,040
you can take a look at the graph top-down, I guess, and learn about these relationships.

238
00:17:15,040 --> 00:17:16,680
That's absolutely the case.

239
00:17:16,680 --> 00:17:23,440
I think that's a really great way of expressing how BSP works in practice.

240
00:17:23,440 --> 00:17:30,400
It's a very interesting distributed systems architecture because it allows the graph itself

241
00:17:30,400 --> 00:17:36,040
to compute in its own manner, and then there's a concept called a superstep where every single

242
00:17:36,040 --> 00:17:39,400
vertex turns around and says, OK, I've done all the things I need to do.

243
00:17:39,400 --> 00:17:43,720
I'm confident that there's no more work for me to do here when your graph says, hey,

244
00:17:43,720 --> 00:17:47,600
each individual node is complete, then at that point you say, OK, we take a look at the

245
00:17:47,600 --> 00:17:51,880
entirety of the graph, we see if it's reached convergence or some other stopping function,

246
00:17:51,880 --> 00:17:56,240
and then do any kind of work that we have to do at that level, and then we allow the next

247
00:17:56,240 --> 00:17:58,160
stage of computation to continue again.

248
00:17:58,160 --> 00:18:03,120
Before you comparing these different graphical processing engines in your talk, or really

249
00:18:03,120 --> 00:18:07,200
just sharing that they exist and what some of the major capabilities are, so it's really

250
00:18:07,200 --> 00:18:08,200
more of an overview.

251
00:18:08,200 --> 00:18:11,520
So sharing their major capabilities and kind of what makes them special when you should

252
00:18:11,520 --> 00:18:13,480
be using them, right?

253
00:18:13,480 --> 00:18:17,720
I think one of the big takeaways that I'd like to communicate to the audience is that we

254
00:18:17,720 --> 00:18:22,520
should be using these graph processing engines when we want to work with very large graphs

255
00:18:22,520 --> 00:18:26,760
that don't fit well into memory, and we're attempting to perform some kind of computation

256
00:18:26,760 --> 00:18:33,760
on those graphs to calculate a value for each vertex or a large subset of the vertices

257
00:18:33,760 --> 00:18:40,000
in the graph that isn't easily expressed in a traversal-based system, right?

258
00:18:40,000 --> 00:18:44,840
Or maybe complicated enough, so that way each of the individual nodes kind of affects

259
00:18:44,840 --> 00:18:47,240
all the other ones around it, right?

260
00:18:47,240 --> 00:18:51,760
By doing this, it kind of allows us to do the thing that Spark and other distributed processing

261
00:18:51,760 --> 00:18:53,320
engines are really great at.

262
00:18:53,320 --> 00:18:57,400
It allows us to take a large amount of data, iterate it on it very quickly in memory,

263
00:18:57,400 --> 00:19:02,880
right, and then present a result or more likely a long series of results, right, for the

264
00:19:02,880 --> 00:19:06,840
user then to interact with in a more interactive method elsewhere.

265
00:19:06,840 --> 00:19:10,640
And I think that one of the things that I communicate in the talk, or I try to communicate

266
00:19:10,640 --> 00:19:14,600
in the talk, is that we want to use this in the same way that you would use Spark.

267
00:19:14,600 --> 00:19:18,960
We don't use Spark generally speaking for truly interactive analytics.

268
00:19:18,960 --> 00:19:22,480
You could build probably a text search engine and Spark or something like that, but it's

269
00:19:22,480 --> 00:19:25,880
probably better to do those computations in Spark and then load that into something

270
00:19:25,880 --> 00:19:27,960
like a elastic search.

271
00:19:27,960 --> 00:19:31,160
And that kind of model, as applied to graphs, is one of the things we're talking about.

272
00:19:31,160 --> 00:19:35,840
That's my transition point from talking about graph processing engines and graph computation

273
00:19:35,840 --> 00:19:40,480
engines to talking about graph databases themselves.

274
00:19:40,480 --> 00:19:46,120
Before we jump into graph databases, the systems that you mentioned, you'll certainly Spark

275
00:19:46,120 --> 00:19:55,280
exists in the Java ecosystem. A lot of machine learning is happening in the Python ecosystem.

276
00:19:55,280 --> 00:20:00,800
And I actually asked the previous interview guest this question earlier.

277
00:20:00,800 --> 00:20:07,320
You seeing any activity in this graphical processing realm in that Python ecosystem?

278
00:20:07,320 --> 00:20:10,760
There's a lot of graphical interest going on inside of Python.

279
00:20:10,760 --> 00:20:16,080
It's mostly expressed in the area of probabilistic graphical models because there's so much

280
00:20:16,080 --> 00:20:19,760
such a strong ecosystem around scientific computing inside of Python.

281
00:20:19,760 --> 00:20:24,960
I think the JVM ecosystem is seen a little bit more in the distributed systems space for

282
00:20:24,960 --> 00:20:29,800
similar reasons. There's a lot of material that already exists for building distributed

283
00:20:29,800 --> 00:20:35,200
systems, especially when we're talking about graph processing engines that are built on

284
00:20:35,200 --> 00:20:40,280
top or with a lot of the same fundamental concepts as some of the more traditional processing

285
00:20:40,280 --> 00:20:45,520
systems. Spark, for example, it makes plenty of sense to allow

286
00:20:45,520 --> 00:20:53,040
these developers to kind of have that historical basis and use those historical tools.

287
00:20:53,040 --> 00:20:56,400
One of the things we're seeing a lot of, however, is that in the same way that Spark

288
00:20:56,400 --> 00:21:02,920
has PySpark, we're also starting to see Python implementations of GraphX and Python implementations

289
00:21:02,920 --> 00:21:09,880
of graph frames that call out to JVM-based systems on the back end or performing similar

290
00:21:09,880 --> 00:21:13,200
computations themselves in a Python manner.

291
00:21:13,200 --> 00:21:17,760
I think one of the areas that we're seeing a lot of excitement inside of Python space

292
00:21:17,760 --> 00:21:23,320
for graph processing is not in the distributed system space, but in the single node space,

293
00:21:23,320 --> 00:21:28,200
which is actually really exciting because there's a lot of stuff that you can do that is

294
00:21:28,200 --> 00:21:36,400
arguably even more efficient in terms of certain metrics on a single node as opposed to multiple

295
00:21:36,400 --> 00:21:40,760
nodes. This has been discussed in academic literature over the last couple of years from

296
00:21:40,760 --> 00:21:46,240
about 2015 on, there's been an ongoing discussion in distributed systems, the academic literature

297
00:21:46,240 --> 00:21:50,680
for distributed systems, talking about kind of what is the configuration that outperforms

298
00:21:50,680 --> 00:21:55,520
a single thread and things like that. It's kind of interesting to look at the two different

299
00:21:55,520 --> 00:21:58,800
areas where we're seeing a lot of this development. There's a great deal of sophisticated

300
00:21:58,800 --> 00:22:04,320
graph processing libraries meant for single node work in the Python ecosystem, running

301
00:22:04,320 --> 00:22:11,240
from network X to other more sophisticated tools that are very relevant.

302
00:22:11,240 --> 00:22:16,720
Each of these camps is building on their respective strengths job on the distributed

303
00:22:16,720 --> 00:22:22,400
computing side, Python on the scientific computing side and data science side and building extensions

304
00:22:22,400 --> 00:22:28,360
and bridges toward the other side, but those historical strengths still remain.

305
00:22:28,360 --> 00:22:32,200
Still remain, right? I think that's something that we want to encourage inside of computer

306
00:22:32,200 --> 00:22:36,440
science as a whole, right? Identifying those foundational elements that have strengths

307
00:22:36,440 --> 00:22:41,720
and allowing them to be good at them while opening up APIs and bridges, as you said, to

308
00:22:41,720 --> 00:22:47,840
allow people who don't have necessarily that specific underlying knowledge of that foundation

309
00:22:47,840 --> 00:22:52,520
or that language element to be able to use those systems effectively. This is a little

310
00:22:52,520 --> 00:23:00,200
bit of a fringe area, but there's a fair amount of graph processing work going on right

311
00:23:00,200 --> 00:23:07,280
now, even in the compiled language space. Right now, there's a researcher by the name

312
00:23:07,280 --> 00:23:15,440
of Frank McSherry, who is doing a spiritual successor to the Niant system that was hosted

313
00:23:15,440 --> 00:23:21,440
and published and worked on at Microsoft Research. It's called Timely Data Flow, and there's

314
00:23:21,440 --> 00:23:26,280
a lot of work going on there. He is an incredible researcher.

315
00:23:26,280 --> 00:23:31,120
That's heard his name before. Yes. He's involved in a lot of things. He was one of the authors

316
00:23:31,120 --> 00:23:35,920
of the cost paper I referenced earlier. One of his major research areas is understanding

317
00:23:35,920 --> 00:23:40,440
distributed systems. When do we need a distributed system? When do we need a single node system

318
00:23:40,440 --> 00:23:45,360
and doing high-performance computing? He's been using Rust recently to do a lot of very

319
00:23:45,360 --> 00:23:51,640
interesting things, exploring what the data flow model of computation is, and exploring

320
00:23:51,640 --> 00:23:56,960
how that can be applied to more traditional data processing problems, as well as graph

321
00:23:56,960 --> 00:24:01,600
processing problems. I mentioned this simply because I think that the area that he's working

322
00:24:01,600 --> 00:24:08,320
is just fascinating. While that work, I don't think is entirely production ready yet.

323
00:24:08,320 --> 00:24:12,160
By his own admission, the work that he's doing is he's exploring the space and doing

324
00:24:12,160 --> 00:24:15,800
a lot of research. I think that there's a lot of stuff that's going on that's bearing

325
00:24:15,800 --> 00:24:20,200
some very meaningful fruit, and I think people inside of the distributed system space

326
00:24:20,200 --> 00:24:27,760
should definitely be aware of those discussions that are going on. You also mentioned a project

327
00:24:27,760 --> 00:24:33,320
out of Google. What was that one? Pregel. P-R-E-G-E-L. Pregel? Yes. Much like the

328
00:24:33,320 --> 00:24:38,880
MapReduce paper, which was launched some time ago, and then kicked off a whole new

329
00:24:38,880 --> 00:24:43,480
revolution in open source computing. Google published another paper slightly there a couple

330
00:24:43,480 --> 00:24:51,320
years after which described a system called Pregel, in which they describe building a system

331
00:24:51,320 --> 00:24:57,520
internally that they built out. In some cases, if I remember the paper correctly, they

332
00:24:57,520 --> 00:25:04,920
said that it had turned into one of their primary processing engines, competing with or

333
00:25:04,920 --> 00:25:09,600
supplementing or assisting MapReduce, their MapReduce implementation internally at

334
00:25:09,600 --> 00:25:17,880
the time, at least, and they called it Pregel, which relied on the BSP model of computation

335
00:25:17,880 --> 00:25:23,200
to perform these large distributed graph computations. If you take a look, you can imagine

336
00:25:23,200 --> 00:25:30,640
that they have tons of to do. Absolutely. To be completely clear, I'm fairly

337
00:25:30,640 --> 00:25:34,920
sure that they published this paper and I have no idea how it's used internally or if

338
00:25:34,920 --> 00:25:38,600
it's still being used today. I don't know anything about that, but it was a fascinating

339
00:25:38,600 --> 00:25:42,920
paper when they published it at the time because it kind of exposed the concept of doing

340
00:25:42,920 --> 00:25:48,040
distributed graph computations, how they solved particular, hairy problems in the space,

341
00:25:48,040 --> 00:25:52,800
and they also exposed kind of an API, the internal API that they're utilizing, and saying,

342
00:25:52,800 --> 00:25:56,800
if you define a system that has the following properties, it would be similar to the Pregel

343
00:25:56,800 --> 00:26:00,840
system we have internally, and this is how we've modeled that style of computation. Again,

344
00:26:00,840 --> 00:26:06,880
very similar to their original MapReduce and HDFS papers, right? Or GFS for them, I suppose.

345
00:26:06,880 --> 00:26:12,560
That's an interesting system. I'd love to know what's happened to it since the publication

346
00:26:12,560 --> 00:26:15,840
of that paper, but unfortunately, I don't have that inside our knowledge.

347
00:26:15,840 --> 00:26:19,360
What's the open source project that's implemented that?

348
00:26:19,360 --> 00:26:26,600
Giraffe. G-I-R-A-P-H. Apache Giraffe. Yeah, I may have misspelled that. Apache Giraffe

349
00:26:26,600 --> 00:26:33,960
is the project, I believe, that's the closest BSP style implementation since then. I

350
00:26:33,960 --> 00:26:41,000
think that while both Jelly, which is the graph processing system attached to Flink and

351
00:26:41,000 --> 00:26:46,640
GraphX, which is the graph processing system, again, attached to Spark, both have Pregel

352
00:26:46,640 --> 00:26:52,640
operators, so you can use the Pregel API. My understanding is that the backend is informed

353
00:26:52,640 --> 00:26:57,320
by the BSP style model of computation, but has been continually developed and represents

354
00:26:57,320 --> 00:27:01,960
kind of a bit of a step in a slightly different direction.

355
00:27:01,960 --> 00:27:06,800
Meaning the backend of the Giraffe, as opposed to the Flink and the...

356
00:27:06,800 --> 00:27:10,480
Yeah, the backend of Giraffe, I believe, is much more similar to the original publication

357
00:27:10,480 --> 00:27:22,320
of the BSP model of computation, whereas my understanding is that both Spark, the GraphX

358
00:27:22,320 --> 00:27:30,520
system and Flink system were informed by it, but are not as tightly tied to the BSP style.

359
00:27:30,520 --> 00:27:38,040
And so, you transitioned from talking about graphical processing engines to graph databases.

360
00:27:38,040 --> 00:27:45,080
Absolutely. How do you see kind of the state of the art there, or maybe let's go back

361
00:27:45,080 --> 00:27:50,560
to the original flow, like, what's the role of the graphical database in helping you

362
00:27:50,560 --> 00:27:53,920
solve the kinds of problems you're solving with graphical networks?

363
00:27:53,920 --> 00:28:00,040
So in the architecture that I propose, right, I propose that users are probably best suited

364
00:28:00,040 --> 00:28:06,760
to use a graphical processing engine to compute most of the values that they need, right,

365
00:28:06,760 --> 00:28:12,680
to modify all of the properties in their graph, right, and then to output those results

366
00:28:12,680 --> 00:28:18,080
into a graph database, which is much easier and much more native for humans to actually

367
00:28:18,080 --> 00:28:19,080
interact with.

368
00:28:19,080 --> 00:28:23,600
I think in my talk, I actually go out and say, you know, someone does a whole bunch of

369
00:28:23,600 --> 00:28:28,080
math on things, and that's great, but at some point humans need to actually use the

370
00:28:28,080 --> 00:28:31,240
resultant data.

371
00:28:31,240 --> 00:28:37,080
And so one of the things I state is that because graphical processing engines are capable

372
00:28:37,080 --> 00:28:43,120
of doing large-scale computations, but are not designed for interactive analytics, for

373
00:28:43,120 --> 00:28:48,040
example, saying, show me all the friends of this particular individual filter that list

374
00:28:48,040 --> 00:28:52,240
by the following criteria, so on and so forth.

375
00:28:52,240 --> 00:28:56,320
One of the things that I suggest is performing as many of these computations as possible

376
00:28:56,320 --> 00:29:00,800
beforehand, so if you want to do a label propagation or something like that, you do that in

377
00:29:00,800 --> 00:29:06,120
your graph computation engine, and then you load the graph with all of those computed results

378
00:29:06,120 --> 00:29:12,320
into a graph database, which is designed for people to interact with and allows you

379
00:29:12,320 --> 00:29:21,160
often comes with dedicated domain-specific languages for doing traversals very efficiently

380
00:29:21,160 --> 00:29:24,440
and very easily and very natively, right?

381
00:29:24,440 --> 00:29:33,160
So in the same way that we probably wouldn't do dynamic astyle analytics using Spark, usually,

382
00:29:33,160 --> 00:29:37,200
we would want to provide an analyst with a SQL front-end to be able to do that sort

383
00:29:37,200 --> 00:29:38,200
of thing.

384
00:29:38,200 --> 00:29:40,760
That's kind of the same model that we're applying here.

385
00:29:40,760 --> 00:29:44,800
You compute as much as you can inside of your processing engine, and then you pass that

386
00:29:44,800 --> 00:29:49,720
material over to a graph database, which actually allows someone to interact with it.

387
00:29:49,720 --> 00:29:56,040
Historically, graph databases have had some degree or another of this graphical processing

388
00:29:56,040 --> 00:30:04,440
capability built in, and if only because the graphical processing engines weren't standalone,

389
00:30:04,440 --> 00:30:13,440
they weren't popular as standalone components, so for example, Neo4j has long had a fairly

390
00:30:13,440 --> 00:30:21,920
well-developed graphical programming model that you can use for data that's in the database.

391
00:30:21,920 --> 00:30:29,840
Is the idea behind separating that outstrictly scalability notion, or are there other reasons

392
00:30:29,840 --> 00:30:31,080
why you'd want to do that?

393
00:30:31,080 --> 00:30:33,600
Well, I think there's two main ones.

394
00:30:33,600 --> 00:30:36,320
Scalability is absolutely one of the main cases.

395
00:30:36,320 --> 00:30:41,560
I think the second one ultimately comes down to how you're comfortable expressing certain

396
00:30:41,560 --> 00:30:49,440
concepts inside of that graph to kind of relate it back to a topic that I think probably

397
00:30:49,440 --> 00:30:52,960
all of your listeners have worked within the past sequel, right?

398
00:30:52,960 --> 00:30:56,680
Sequel is a touring complete language, but there's plenty of programs I wouldn't want

399
00:30:56,680 --> 00:30:59,800
to write in sequel, right?

400
00:30:59,800 --> 00:31:03,000
Just because it's touring complete doesn't mean that it's necessarily easy, or that it's

401
00:31:03,000 --> 00:31:05,760
really the tool that's designed for this particular purpose.

402
00:31:05,760 --> 00:31:11,840
And so while Neo4j, as you referenced, historically had the Gremlin interface, the Gremlin query

403
00:31:11,840 --> 00:31:18,680
language attached to it, and I believe now supports the cipher query language, I wouldn't

404
00:31:18,680 --> 00:31:23,800
suggest that either of those query languages are always the best choice for all of the

405
00:31:23,800 --> 00:31:27,000
graph computations you may want to compute, right?

406
00:31:27,000 --> 00:31:32,640
So even ignoring possibilities of benefiting from a distributed systems environment, right,

407
00:31:32,640 --> 00:31:34,280
for scalability.

408
00:31:34,280 --> 00:31:40,880
The way you are able to express your computations may be benefited by working in one environment

409
00:31:40,880 --> 00:31:43,080
or the other.

410
00:31:43,080 --> 00:31:47,360
Possibly due to my background, possibly due to my own biases, I generally have an easier

411
00:31:47,360 --> 00:31:52,720
way of thinking about graphs from a vertex or a graph-based centric model of computation,

412
00:31:52,720 --> 00:31:55,000
and so I tend to favor that.

413
00:31:55,000 --> 00:31:58,880
That combined with the scalability metric is kind of where my recommendation comes from.

414
00:31:58,880 --> 00:32:05,880
And is there an emerging, you know, gql kind of analogy to SQL for the interface between

415
00:32:05,880 --> 00:32:09,520
the graphical processing engines and the graphical databases?

416
00:32:09,520 --> 00:32:12,080
That is a super hot topic.

417
00:32:12,080 --> 00:32:13,080
Okay.

418
00:32:13,080 --> 00:32:19,040
So, I think that's probably the easiest way of handling that is doing your computation

419
00:32:19,040 --> 00:32:23,880
first, emitting that in, let's just say, for the purposes of discussion, CSV or JSON

420
00:32:23,880 --> 00:32:27,960
or something like that, and then loading that into your graphical database, right?

421
00:32:27,960 --> 00:32:33,520
That may be a little ineligant at some point, but it definitely works.

422
00:32:33,520 --> 00:32:39,520
And actually, there's a couple of new services coming out that make doing that particular path

423
00:32:39,520 --> 00:32:40,520
a little bit easier.

424
00:32:40,520 --> 00:32:44,560
Let's put that to the side for just a second, right?

425
00:32:44,560 --> 00:32:49,680
One of the things, one of the projects that's definitely being worked on right now is an

426
00:32:49,680 --> 00:32:55,440
interface to GraphX utilizing the Grumlin query language.

427
00:32:55,440 --> 00:33:00,080
And I think that that's one very interesting way that you would be able to go about doing

428
00:33:00,080 --> 00:33:01,080
this.

429
00:33:01,080 --> 00:33:06,840
And I would be reasonably certain that that would allow you to kind of dynamically load

430
00:33:06,840 --> 00:33:11,480
some data into various Graph databases that that may exist out there.

431
00:33:11,480 --> 00:33:15,520
Or at the very least, it's an area that I'd like to see some development kind of go into.

432
00:33:15,520 --> 00:33:21,080
There are new Graph Processing systems getting, not just Graph Processes, but Graph Data

433
00:33:21,080 --> 00:33:27,120
Bases getting published on a regular basis. One of them is AWS's Neptune, which I should

434
00:33:27,120 --> 00:33:32,320
at this point, insert my common disclaimer, I'm a gentleman at Capital One, but nothing

435
00:33:32,320 --> 00:33:36,880
in here when I talk about any technologies is Capital One telling you to use any of these

436
00:33:36,880 --> 00:33:37,880
tools.

437
00:33:37,880 --> 00:33:42,440
I'm merely expressing the fact that I've used them before and I've had various results,

438
00:33:42,440 --> 00:33:45,560
but I'm not a mouthpiece for my company when I'm talking to you.

439
00:33:45,560 --> 00:33:47,880
Sorry, got to do that.

440
00:33:47,880 --> 00:33:56,520
Neptune is a new graphical offering from AWS, which has a lot of interesting properties,

441
00:33:56,520 --> 00:34:03,480
including saving snapshots of your data into S3, reloading snapshots, reloading data dynamically,

442
00:34:03,480 --> 00:34:09,360
which might actually take the somewhat in-allegate method that I described kind of at the beginning

443
00:34:09,360 --> 00:34:16,160
of this area of the discussion and make it a little bit more workable depending on the

444
00:34:16,160 --> 00:34:17,160
situation.

445
00:34:17,160 --> 00:34:24,160
Things to think about, taking that from kind of, I think I talk about mostly just dynamically

446
00:34:24,160 --> 00:34:29,960
just loading your data and taking a snapshot, loading it into a graph database in my talk.

447
00:34:29,960 --> 00:34:34,160
I talk about that specifically because that's something that's relatively easy to understand

448
00:34:34,160 --> 00:34:40,200
and discuss and doing something in a more dynamic fashion or a less elegant fashion is a

449
00:34:40,200 --> 00:34:43,760
matter of engineering and not always a matter of capability.

450
00:34:43,760 --> 00:34:48,280
As a result, there's a lot of area that you can play in there and that's really judged

451
00:34:48,280 --> 00:34:54,720
by what's your use case, what kind of reliability and speed guarantees you need to fulfill,

452
00:34:54,720 --> 00:34:57,000
and that differs on every single area.

453
00:34:57,000 --> 00:35:02,560
So I'm trying to describe more of a model of architecture, more so than these are the

454
00:35:02,560 --> 00:35:07,440
specific links that you definitely should be using because all of these graph systems

455
00:35:07,440 --> 00:35:10,720
at the end of the day, they're really specialist tools, even though some of them are becoming

456
00:35:10,720 --> 00:35:17,360
more, I don't want to say generic, but more generalized frameworks or being built on

457
00:35:17,360 --> 00:35:18,760
generalized frameworks.

458
00:35:18,760 --> 00:35:25,680
I think graph computation is something that is very, very tied to the kind of work that

459
00:35:25,680 --> 00:35:30,520
you're attempting to do and the kind of space you're operating in and as a result, you

460
00:35:30,520 --> 00:35:36,760
want to be very sensitive to your use case at every single stage of the operation.

461
00:35:36,760 --> 00:35:44,600
So maybe taking a further philosophical detour, do you think that in the space we need

462
00:35:44,600 --> 00:35:52,880
something analogous to a standard query language or conversely do we not need it because

463
00:35:52,880 --> 00:35:57,440
of what you just said that the architectures and the way we build these applications is

464
00:35:57,440 --> 00:36:04,840
very use case dependent or is it the case that kind of times of change and if the relational

465
00:36:04,840 --> 00:36:11,000
database was being invented now as opposed to you have for many years ago, maybe we wouldn't

466
00:36:11,000 --> 00:36:15,120
have a sequel because we're so comfortable kind of throwing around Jason and manipulating

467
00:36:15,120 --> 00:36:17,000
it on the fly, that kind of thing.

468
00:36:17,000 --> 00:36:19,880
So it's funny, it's funny you bring that up.

469
00:36:19,880 --> 00:36:24,840
I happen to be a proponent in the general case, certainly for graph databases and other

470
00:36:24,840 --> 00:36:32,320
traversal based systems to say that a common query language is likely to increase adoption.

471
00:36:32,320 --> 00:36:38,000
I think having the sequel standard is what allowed SQL databases to become as large as

472
00:36:38,000 --> 00:36:43,360
they effectively have, just simply because of the fact that you knew that you could train

473
00:36:43,360 --> 00:36:49,080
a series of people in a particular, in a particular style of SQL and it would probably

474
00:36:49,080 --> 00:36:55,540
transfer in 90% of its volume to any other implementation of the SQL standard assets

475
00:36:55,540 --> 00:37:01,920
pressed by my sequel or by Postgres or by Oracle or so on and so forth, there's obviously

476
00:37:01,920 --> 00:37:06,920
some caveats there, but in general I think that led to many, many good things.

477
00:37:06,920 --> 00:37:11,880
I would suggest that standardization around a query language or a standard for query languages

478
00:37:11,880 --> 00:37:17,280
is probably something that will have similar beneficial effects as well.

479
00:37:17,280 --> 00:37:22,080
I've got my own thoughts on which ones I think would be interesting to have, but I don't

480
00:37:22,080 --> 00:37:26,000
know. I gave you an opportunity.

481
00:37:26,000 --> 00:37:30,800
I actually favor Gremlin quite a lot, but that may be because that was one of the first

482
00:37:30,800 --> 00:37:35,920
graph DSLs that I really worked with or spent any kind of time with and so that's probably

483
00:37:35,920 --> 00:37:41,360
a bias that I have there, but I like the standard as a whole and I think that that organization

484
00:37:41,360 --> 00:37:47,800
has done a good job of stewardship, the TinkerPop organization used to be a standalone company

485
00:37:47,800 --> 00:37:53,000
which before that it was a research project from one of the key developers and they've

486
00:37:53,000 --> 00:37:57,080
transitioned it into kind of a public Apache project since then and so I'd like seeing

487
00:37:57,080 --> 00:38:01,640
that chain of stewardship moving forward and that's part of that key part of creating

488
00:38:01,640 --> 00:38:03,640
an eventual standard.

489
00:38:03,640 --> 00:38:08,280
I think there's a lot of good things about it, so that's one of the areas that I would

490
00:38:08,280 --> 00:38:12,280
go in, but one way or the other I generally believe that standardization for query languages

491
00:38:12,280 --> 00:38:17,600
definitely can lead to good things and at a bare minimum definitely leads to kind of

492
00:38:17,600 --> 00:38:23,000
a greater ease of acceptance, certainly less shock when you deal with a new system for

493
00:38:23,000 --> 00:38:24,000
the first time.

494
00:38:24,000 --> 00:38:28,160
It's funny you bring up the statement of chucking around JSON and some of the more modern

495
00:38:28,160 --> 00:38:33,920
database families moving away from SQL because I think we've actually seen, I'm remembering

496
00:38:33,920 --> 00:38:38,760
back to when kind of the big wave of distributed key value stores started pouring out into

497
00:38:38,760 --> 00:38:45,880
the Apache projects and the JVM ecosystem again systems like Cassandra for example which

498
00:38:45,880 --> 00:38:51,560
originally was a pure key value store around because I think it was Cassandra 2.0 when

499
00:38:51,560 --> 00:38:57,680
they started integrating the literal CQL query language into it which looks very similar

500
00:38:57,680 --> 00:39:06,520
to SQL and I think we've seen that with a couple of other databases which kind of just

501
00:39:06,520 --> 00:39:13,960
talks about SQL's overall kind of cultural dominance, the hegemony that it represents

502
00:39:13,960 --> 00:39:15,880
is kind of very interesting.

503
00:39:15,880 --> 00:39:22,840
The databases role is almost like if you're using a database to serve up a model like

504
00:39:22,840 --> 00:39:28,200
you're building the essentially building the model, the graphical model using the processing

505
00:39:28,200 --> 00:39:33,080
engine and then you're taking that and sticking into the database for more interactive use.

506
00:39:33,080 --> 00:39:36,680
I think you're right, I think at the end of the day you're definitely serving the results.

507
00:39:36,680 --> 00:39:39,080
I don't know if I want to say you're serving the model because it's almost a little bit

508
00:39:39,080 --> 00:39:43,120
more restrictive right because your model in a standard machine learning environment if

509
00:39:43,120 --> 00:39:47,280
you're just serving a model, you're definitely just serving one particular thing that takes

510
00:39:47,280 --> 00:39:52,400
a defined input and exports some kind of defined output whereas for a graph database you've

511
00:39:52,400 --> 00:39:56,040
got a lot more flexibility you can actually write your own queries and at some level you

512
00:39:56,040 --> 00:40:01,160
can write secondary stage analytics on top of that if you if that suits your use case.

513
00:40:01,160 --> 00:40:06,760
But you're definitely right it would be serving kind of a pre-digested at some level some

514
00:40:06,760 --> 00:40:11,560
kind of digested result in an interface that's maybe a little bit more native to your

515
00:40:11,560 --> 00:40:12,560
use and utilization.

516
00:40:12,560 --> 00:40:15,160
Yeah, yeah, absolutely, okay.

517
00:40:15,160 --> 00:40:19,880
And then so the next part of your talk is then starting to look at graphical models

518
00:40:19,880 --> 00:40:26,960
from machine learning perspective and potentially even you mentioned graphical embeddings which

519
00:40:26,960 --> 00:40:31,800
is I'm very intrigued as to the direction that that goes.

520
00:40:31,800 --> 00:40:39,360
So graph convolutional neural networks are a very active research field currently and

521
00:40:39,360 --> 00:40:44,120
I caveat any discussion about them by saying that like from an academic perspective the academic

522
00:40:44,120 --> 00:40:49,840
community still isn't completely decided on whether or not they are a good approach,

523
00:40:49,840 --> 00:40:53,280
how powerful they actually are and what's going on.

524
00:40:53,280 --> 00:40:55,920
There's a lot of there's a lot of interest and there's a lot of papers that have been

525
00:40:55,920 --> 00:40:59,760
published that speak to kind of different techniques and different approaches and different

526
00:40:59,760 --> 00:41:04,360
ways of thinking about it working with this which have published positive results and

527
00:41:04,360 --> 00:41:05,360
material like that.

528
00:41:05,360 --> 00:41:10,200
But as anything inside of academia, two years, three years is not really enough time to

529
00:41:10,200 --> 00:41:13,360
kind of come to a global consensus on things, right?

530
00:41:13,360 --> 00:41:17,040
Even in the larger neural networks space that we're seeing right now, there's still discussion

531
00:41:17,040 --> 00:41:21,280
on like what are we actually learning and how are we actually doing it, right?

532
00:41:21,280 --> 00:41:29,960
And so in a more specialized area of that larger discussion, something that's even at some

533
00:41:29,960 --> 00:41:34,040
level newer and has fewer researches kind of staring at it, of course that's a new

534
00:41:34,040 --> 00:41:35,280
discussion as I'm going.

535
00:41:35,280 --> 00:41:42,040
One immediate question I have is, has a standard data set, a training data set emerged for

536
00:41:42,040 --> 00:41:45,720
graphical operations, like is there an image net for graph stuff?

537
00:41:45,720 --> 00:41:49,600
You know that's interesting, there's actually a lot of different data sets that are frequently

538
00:41:49,600 --> 00:41:53,440
cited in literature in general, right?

539
00:41:53,440 --> 00:41:56,840
And they range from the very small to the very large, right?

540
00:41:56,840 --> 00:42:03,240
Stanford, the SNAP data sets, Stanford publishes, I believe it's Stanford, but it's the SNAP

541
00:42:03,240 --> 00:42:09,240
data sets, publishes this large list of publicly available graph data sets, and they include

542
00:42:09,240 --> 00:42:16,200
things like the trust relationship of the website slash dot.

543
00:42:16,200 --> 00:42:23,000
I don't know if I'm not sure what the demographic age range of your audience usually tends towards,

544
00:42:23,000 --> 00:42:28,960
but if you're an individual of a certain age, you probably remember slash dot.

545
00:42:28,960 --> 00:42:33,040
It's old, old kind of things before bread and dig and all that and stuff.

546
00:42:33,040 --> 00:42:38,480
So the trust graph of slash dot is one of those data sets.

547
00:42:38,480 --> 00:42:45,160
Live journal had a friendship listing, there's Twitter follower graphs, going all the way

548
00:42:45,160 --> 00:42:49,880
down to the very small, which are things like kind of a famous graph testing data set called

549
00:42:49,880 --> 00:42:58,720
Zachary's Karate Club, which is, I kid you not, a collection of I think just a couple

550
00:42:58,720 --> 00:43:05,520
handfuls of individuals inside of a youth Karate Club associating who identified as friends

551
00:43:05,520 --> 00:43:07,520
of whom, right?

552
00:43:07,520 --> 00:43:09,360
So there's less than you publish this data set.

553
00:43:09,360 --> 00:43:12,440
This isn't me.

554
00:43:12,440 --> 00:43:13,840
That data set predates me.

555
00:43:13,840 --> 00:43:20,360
It just happens actually at the different exactly entirely, but so there's a number of these

556
00:43:20,360 --> 00:43:24,720
data sets that have been published, which are very well studied.

557
00:43:24,720 --> 00:43:30,720
So there's a couple of them that get get changed about and because of the fact that image

558
00:43:30,720 --> 00:43:34,720
that image that has a lot of benefits for it in that it's a very large, a very general

559
00:43:34,720 --> 00:43:40,080
law, all sorts of different images are contained there in graphs, which model kind of the interconnectivity

560
00:43:40,080 --> 00:43:46,880
of individual nodes mean that attempts at getting one standard kind of data set are going

561
00:43:46,880 --> 00:43:47,880
to look very different.

562
00:43:47,880 --> 00:43:53,000
The PGP Web of Trust looks very different than your Facebook association, then your LinkedIn

563
00:43:53,000 --> 00:43:56,280
association patterns, everything looks a little different.

564
00:43:56,280 --> 00:43:58,840
Looks very different in what sense?

565
00:43:58,840 --> 00:44:05,040
The number of things that you connect to, how willing you are to connect to certain things,

566
00:44:05,040 --> 00:44:10,840
and kind of the general level of interconnectivity of individual nodes, right?

567
00:44:10,840 --> 00:44:14,880
I would say that probably on social media sites, Facebook and Twitter and things like

568
00:44:14,880 --> 00:44:19,920
that, you're highly incentivized to connect with a large number of people.

569
00:44:19,920 --> 00:44:23,920
People that are your close, close friends and people who are acquaintances that you've

570
00:44:23,920 --> 00:44:28,960
interacted with a few times before, or even in the case of, I guess, both of those websites,

571
00:44:28,960 --> 00:44:34,280
people who you don't know personally, but they produce content or have opinions that

572
00:44:34,280 --> 00:44:35,280
you find interesting.

573
00:44:35,280 --> 00:44:39,400
And so you want to subscribe to them to get their material sent to you, right?

574
00:44:39,400 --> 00:44:47,160
As opposed to say something that's much more intentional, the PGP Web of Trust, for example,

575
00:44:47,160 --> 00:44:52,520
for people who use encrypted communications for signing emails and things like that, those

576
00:44:52,520 --> 00:44:57,480
are intended to be very intentional communications, right?

577
00:44:57,480 --> 00:45:02,400
You wouldn't state in that kind of Web of Trust that you trust an individual you've never

578
00:45:02,400 --> 00:45:03,560
met before.

579
00:45:03,560 --> 00:45:07,320
You probably only trust individuals that you've deliberately sent mail to who you know

580
00:45:07,320 --> 00:45:10,120
are the people you want to talk to, things like that, right?

581
00:45:10,120 --> 00:45:19,680
So is the idea then that the popping it up a level, the density or sparsity of the networks

582
00:45:19,680 --> 00:45:26,240
is dramatically different in, and maybe just not the level of density, but some quality

583
00:45:26,240 --> 00:45:33,840
of the density is different in that because the methods that we use to process these graphs

584
00:45:33,840 --> 00:45:40,920
are very sensitive to that, it doesn't make sense to have some general data set.

585
00:45:40,920 --> 00:45:45,040
Yeah, I would say that's a good way of describing it, right?

586
00:45:45,040 --> 00:45:49,880
It's the way the graph was formed, the way the graph evolves over time, you know, kind

587
00:45:49,880 --> 00:45:54,920
of all these things are embedded in the actual structure of the network itself.

588
00:45:54,920 --> 00:46:01,800
And so as kind of a result, I would suggest that when looking at, you know, papers discussing

589
00:46:01,800 --> 00:46:05,880
graph computation, you'll usually see, especially looking at systems papers discussing graph

590
00:46:05,880 --> 00:46:12,320
computation, you will see individuals publishing these papers study the way their algorithm

591
00:46:12,320 --> 00:46:17,480
or their system performs over multiple different graphs, simply to be aware of that.

592
00:46:17,480 --> 00:46:22,580
But there are definitely a series of these well-published, well-understood, well-studied

593
00:46:22,580 --> 00:46:24,080
graphs out there.

594
00:46:24,080 --> 00:46:26,840
That's not quite like ImageNet, but not that different.

595
00:46:26,840 --> 00:46:31,720
So then these graphical neural networks, what's, can I give us a snap?

596
00:46:31,720 --> 00:46:33,960
Parts out of the state of the state there.

597
00:46:33,960 --> 00:46:35,040
Sure, sure, absolutely.

598
00:46:35,040 --> 00:46:40,600
So the intuition behind it is, can we use a neural network to gain an understanding of

599
00:46:40,600 --> 00:46:44,040
individual vertices in a graph, right?

600
00:46:44,040 --> 00:46:48,320
And over the course of a series of papers, the concept of using neural networks effectively

601
00:46:48,320 --> 00:46:54,480
modified convolutional neural networks to get an understanding of an embedding of individual

602
00:46:54,480 --> 00:46:57,120
nodes inside of a graph has come about.

603
00:46:57,120 --> 00:47:02,680
So in the same way that we kind of look at data that is traditionally, maybe a little

604
00:47:02,680 --> 00:47:07,920
difficult to vectorize or very, very sparse, and we want to build a denser representation

605
00:47:07,920 --> 00:47:13,080
of that data, word to vac being kind of the most famous example, but there's all sorts

606
00:47:13,080 --> 00:47:17,800
of, you know, star to vac and all sorts of things running around right now.

607
00:47:17,800 --> 00:47:21,160
We would want to do that for graphs as well, because for graphs, right, if you look at

608
00:47:21,160 --> 00:47:29,680
it in terms of like adjacency matrix, adjacency matrices in large graphs are almost necessarily

609
00:47:29,680 --> 00:47:31,240
very sparse.

610
00:47:31,240 --> 00:47:34,000
There's only so many people that you can possibly know.

611
00:47:34,000 --> 00:47:35,960
There are so many people in the world.

612
00:47:35,960 --> 00:47:41,600
And so no matter how well traveled or socially, maybe you'll never really make a dent, a

613
00:47:41,600 --> 00:47:45,720
meaningful dent in the overall population of the planet, right?

614
00:47:45,720 --> 00:47:51,080
So for a theoretically maximal social graph, right, it's going to be a very sparse matrix.

615
00:47:51,080 --> 00:47:54,040
Working with sparse matrices is very difficult.

616
00:47:54,040 --> 00:47:57,760
There's all sorts of mathematical properties you run into to say nothing of like space

617
00:47:57,760 --> 00:48:00,080
requirements and things like that.

618
00:48:00,080 --> 00:48:04,400
And so one of the things that people would want it to do is, you know, to come up with

619
00:48:04,400 --> 00:48:08,560
an embedding matrix to take kind of each of those individual things and bring it to an

620
00:48:08,560 --> 00:48:14,880
a easily or more easily described point in space that you can perform distance measurements

621
00:48:14,880 --> 00:48:17,080
on, right?

622
00:48:17,080 --> 00:48:22,520
And graphical convolutional networks are a means to do such a thing.

623
00:48:22,520 --> 00:48:25,480
And it's an active area of research right now and there's all sorts of different levels

624
00:48:25,480 --> 00:48:30,400
of debate going on about it, which I personally find fascinating.

625
00:48:30,400 --> 00:48:35,920
And one of the things I talk about in my talk is kind of what are they and a couple of

626
00:48:35,920 --> 00:48:38,880
different ways that you can, you can work with them to some degree.

627
00:48:38,880 --> 00:48:43,920
One of the things that I'm currently experimenting with now is getting an idea of whether or

628
00:48:43,920 --> 00:48:48,120
not we can use graphical, the results of these embeddings, I should say, if we can use

629
00:48:48,120 --> 00:48:51,600
these embeddings to perform more like this queries.

630
00:48:51,600 --> 00:48:56,280
So let's say that we identify that there is a fraudster somewhere.

631
00:48:56,280 --> 00:49:00,000
We know that this person has committed fraud and we know that the pattern of which this

632
00:49:00,000 --> 00:49:03,840
person was committing fraud is somehow distinct or unique.

633
00:49:03,840 --> 00:49:10,640
One of an investigator asks themself, how do I find more people like this?

634
00:49:10,640 --> 00:49:13,120
Can you show me some system out there?

635
00:49:13,120 --> 00:49:19,720
Can I be shown a listing of individuals who have similar patterns of their transactions

636
00:49:19,720 --> 00:49:25,400
or of the way they behave inside of the financial network that may lead me to believe that

637
00:49:25,400 --> 00:49:28,920
there is more people like this, this is a newer emerging trend, this is something we can

638
00:49:28,920 --> 00:49:30,920
look at.

639
00:49:30,920 --> 00:49:35,400
And getting an idea of what that looks like, we're currently exploring the use of embeddings

640
00:49:35,400 --> 00:49:39,560
to be able to kind of see what that gives us and how we work with that.

641
00:49:39,560 --> 00:49:43,760
So in the talk, I kind of expressed that idea, you know, introduce the concept of embeddings

642
00:49:43,760 --> 00:49:46,760
and talk a little bit about how you would serve embeddings in a manner that would allow

643
00:49:46,760 --> 00:49:52,600
you to do more like this queries in something significantly more efficient than, you know,

644
00:49:52,600 --> 00:49:55,760
O to the N squared time, which is always fun.

645
00:49:55,760 --> 00:50:00,160
You mentioned that it's very early in this space.

646
00:50:00,160 --> 00:50:04,920
Are there specific areas where it's been or what are the characteristics of areas that

647
00:50:04,920 --> 00:50:09,640
has been demonstrated to be a useful technique?

648
00:50:09,640 --> 00:50:15,040
So there's a couple of areas, most of them in the areas of kind of clustering, graph

649
00:50:15,040 --> 00:50:21,320
coloring, and association into groups, defining community detection.

650
00:50:21,320 --> 00:50:24,920
That's probably the one that I've spent most of my time looking at.

651
00:50:24,920 --> 00:50:29,280
There are a handful of others, of course, in the spaces of whole, but I would suggest

652
00:50:29,280 --> 00:50:33,480
that that's probably the most fruitful and initial area, identifying communities inside

653
00:50:33,480 --> 00:50:36,040
of a graph, which is a very difficult problem.

654
00:50:36,040 --> 00:50:43,080
What is the pre-processing or training that's required to train up one of these graphical

655
00:50:43,080 --> 00:50:44,080
neural networks?

656
00:50:44,080 --> 00:50:50,560
Is it, are you basically kind of vectorizing your graph in some kind of way as some huge

657
00:50:50,560 --> 00:50:55,040
sparse vector and just feeding that into a standard CNN or have we changed the...

658
00:50:55,040 --> 00:50:56,040
So it's a more...

659
00:50:56,040 --> 00:50:57,040
I mean, architecture, like...

660
00:50:57,040 --> 00:51:00,680
As everything, it's a little bit more complicated than that, but that's a good kind of

661
00:51:00,680 --> 00:51:04,440
imagined stuff, that's a good high level representation.

662
00:51:04,440 --> 00:51:08,280
At the end of the day, you're still working with that adjacency graph, right?

663
00:51:08,280 --> 00:51:09,600
The adjacency matrix.

664
00:51:09,600 --> 00:51:14,240
And you're still passing kind of the window that you're working on of the neural network

665
00:51:14,240 --> 00:51:15,240
over that adjacency matrix.

666
00:51:15,240 --> 00:51:16,840
Oh, well that's a thing right there.

667
00:51:16,840 --> 00:51:26,080
It's like, hey, we've got this huge connectivity, or the connectivity of our graph can be expressed

668
00:51:26,080 --> 00:51:30,000
as this huge sparse matrix, but there's no way we're turning that into a vector and

669
00:51:30,000 --> 00:51:34,600
feeding it to a network where we're windowing around within this environment.

670
00:51:34,600 --> 00:51:35,600
Kind of like...

671
00:51:35,600 --> 00:51:36,760
I guess you can think of it as a convolution.

672
00:51:36,760 --> 00:51:38,440
I guess that's the whole point of this.

673
00:51:38,440 --> 00:51:39,440
That's the whole thing.

674
00:51:39,440 --> 00:51:45,160
And feeding the data into that convolution for the particular techniques that I'm working

675
00:51:45,160 --> 00:51:49,680
with right now are very similar to performing kind of a random walk through that graph, right?

676
00:51:49,680 --> 00:51:53,640
So you're following this random walk, you run a series of those random walks, and then

677
00:51:53,640 --> 00:51:56,880
that gets you kind of what you're looking for based on a particular starting point.

678
00:51:56,880 --> 00:52:02,800
And you feed that, which is effectively a subgraph into your neural network.

679
00:52:02,800 --> 00:52:03,800
There's a couple of papers.

680
00:52:03,800 --> 00:52:07,880
I'll send you a few after after we talk a little bit today, because right now the majority

681
00:52:07,880 --> 00:52:12,200
of the work that I'm doing in that particular space is working with techniques that have

682
00:52:12,200 --> 00:52:16,720
been kind of published and discussed and trying to see whether or not they give reasonable

683
00:52:16,720 --> 00:52:20,720
results so that I'm trying to effectively, I'm trying to find kind of where, what the

684
00:52:20,720 --> 00:52:25,680
best area to dive in is based on some experimental trials in the data sets that we're working

685
00:52:25,680 --> 00:52:27,080
with now.

686
00:52:27,080 --> 00:52:31,320
So for a kind of a deeper mathematical based understanding, I'll send you a paper or two,

687
00:52:31,320 --> 00:52:35,600
and I think you'll be able to put it up for your listeners or something like that.

688
00:52:35,600 --> 00:52:39,320
Do you know off the top of your head, some of the key researchers who are working in this

689
00:52:39,320 --> 00:52:40,320
area?

690
00:52:40,320 --> 00:52:42,840
I'm terrible with names, so I wouldn't be able to tell you off the top of head.

691
00:52:42,840 --> 00:52:44,840
Well, we'll include it in the shout outs for sure.

692
00:52:44,840 --> 00:52:45,840
Absolutely.

693
00:52:45,840 --> 00:52:49,920
Oh, and fortunately, many of those researchers are really good about doing kind of reproducible

694
00:52:49,920 --> 00:52:50,920
research.

695
00:52:50,920 --> 00:52:57,440
So many of them have published the implementations they had for their individual papers on GitHub,

696
00:52:57,440 --> 00:53:02,120
and you know, it's always really great to see it when researchers do that kind of work.

697
00:53:02,120 --> 00:53:03,120
Absolutely.

698
00:53:03,120 --> 00:53:04,120
That's always amazing.

699
00:53:04,120 --> 00:53:05,120
Absolutely.

700
00:53:05,120 --> 00:53:08,280
So we've walked through these kind of three elements of your presentation.

701
00:53:08,280 --> 00:53:09,960
How did you some things up for folks?

702
00:53:09,960 --> 00:53:14,120
So effectively, I tried some things up by drawing an architectural diagram saying like,

703
00:53:14,120 --> 00:53:18,200
you know, if you're working with large scale graphs, if you have problems similar to kind

704
00:53:18,200 --> 00:53:20,760
of this set of things, you may want to consider an

705
00:53:20,760 --> 00:53:23,800
architecture that looks a little bit similar to this, right?

706
00:53:23,800 --> 00:53:30,000
Where your underlying layer is some kind of large block data store, you have a graphical

707
00:53:30,000 --> 00:53:35,000
processing engine directly over that, reading from that and performing its computations,

708
00:53:35,000 --> 00:53:39,240
feeding those results through some means and mechanism into a graphical database, and

709
00:53:39,240 --> 00:53:43,160
then serving the results of that graph database, either directly through interfacing directly

710
00:53:43,160 --> 00:53:48,920
with the graph DB itself or through a series of APIs, a restful API or even a graph

711
00:53:48,920 --> 00:53:51,080
of results or something.

712
00:53:51,080 --> 00:53:52,080
Exactly.

713
00:53:52,080 --> 00:53:53,080
Something along those lines, right?

714
00:53:53,080 --> 00:53:56,760
Because at the end of the day, the goal of all of this material, right, is to enable

715
00:53:56,760 --> 00:54:03,440
some kind of analysis, to enable some individual to perform useful work over that data, right?

716
00:54:03,440 --> 00:54:08,640
And so providing that in different avenues and different ways is definitely relevant because

717
00:54:08,640 --> 00:54:15,480
you'll have extremely technical and sophisticated investigators or analysts doing direct line

718
00:54:15,480 --> 00:54:17,320
queries into the database.

719
00:54:17,320 --> 00:54:22,040
You'll have others that are consuming that material through reporting or through kind

720
00:54:22,040 --> 00:54:26,880
of a guided interface in some form or fashion, depending on what your use case is.

721
00:54:26,880 --> 00:54:29,520
You will probably be in some spectrum there.

722
00:54:29,520 --> 00:54:33,200
And so as a direct result, we want to make sure that you have the ability to communicate

723
00:54:33,200 --> 00:54:36,400
those results to kind of all those different use cases.

724
00:54:36,400 --> 00:54:37,400
Awesome.

725
00:54:37,400 --> 00:54:38,400
Awesome.

726
00:54:38,400 --> 00:54:39,400
Well, sounds like an awesome talk.

727
00:54:39,400 --> 00:54:42,120
I'm glad to say much for taking some time to share with us.

728
00:54:42,120 --> 00:54:43,800
Been a pleasure being here.

729
00:54:43,800 --> 00:54:49,440
All right, everyone, that's our show for today.

730
00:54:49,440 --> 00:54:55,040
For more information on Zach or any of the topics we covered in this show, visit twomolai.com

731
00:54:55,040 --> 00:54:58,000
slash talk slash 188.

732
00:54:58,000 --> 00:55:04,120
For more information on the entire Strata Data Conference podcast series, visit twomolai.com

733
00:55:04,120 --> 00:55:08,080
slash strata and Y 2018.

734
00:55:08,080 --> 00:55:12,200
Thanks again to our sponsors, Capital One and Clara for their support of this show in

735
00:55:12,200 --> 00:55:13,760
this series.

736
00:55:13,760 --> 00:55:17,440
As always, thanks so much for listening and catch you next time.

