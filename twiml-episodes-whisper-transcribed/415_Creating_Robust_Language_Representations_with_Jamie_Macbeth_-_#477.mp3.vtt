WEBVTT

00:00.000 --> 00:16.320
All right, everyone. I am here with Jamie McBeth. Jamie is an assistant professor in the Department

00:16.320 --> 00:21.120
of Computer Science at Smith College. Jamie, welcome to the Twoma AI podcast.

00:22.160 --> 00:27.600
Thank you. Thanks for having me. Hey, I'm super excited to dig into our conversation and learn

00:27.600 --> 00:33.200
a bit about your research and what you're up to. Let's get started by having you share a bit

00:33.200 --> 00:39.040
about your background with our audience. How did you come to work in AI and cognitive systems in

00:39.040 --> 00:48.640
particular? Sure. So originally I was, I would say a physicist actually a mathematician and a

00:48.640 --> 00:56.800
physicist as an undergraduate and also sometime as a grad student. I then fell in love with

00:56.800 --> 01:02.560
computer science. I studied computer science in graduate school. And towards the end of my

01:02.560 --> 01:07.840
career in graduate school, I also fell in love with the specific topic that I work on now,

01:08.560 --> 01:14.720
which is artificial intelligence systems and cognitive systems for performing natural

01:14.720 --> 01:19.840
language understanding and the the issues associated with that. That's great. That's great.

01:19.840 --> 01:28.720
When we were chatting earlier, you spoke a little bit more about your, the way you think about

01:28.720 --> 01:36.320
cognitive systems and kind of how that's different from a lot of the contemporary application

01:36.320 --> 01:40.560
of machine learning and AI. I'd love to hear you elaborate on that a bit for our audience.

01:41.280 --> 01:45.520
Sure. Sure. Yeah. So those of us in the cognitive systems community,

01:45.520 --> 01:52.960
where we're a part of the artificial intelligence community, but people are focused, people in the

01:52.960 --> 02:00.240
cognitive systems community are focused quite a bit more on using artificial intelligence as a

02:00.240 --> 02:08.240
vehicle for a better understanding of human intelligence and not particularly of using AI to

02:08.240 --> 02:15.920
just score well at particular tasks and do well on the leaderboard. I think some of the negative

02:15.920 --> 02:22.000
things that have been associated with artificial intelligence these days, such as biases and things

02:22.000 --> 02:30.320
like that, have to do with there being a little bit too much hype around the systems that people

02:30.320 --> 02:39.680
are building and the way you're able to show good numbers at these test problems and focusing

02:39.680 --> 02:46.000
less on the actual science. Okay. What really can these systems do? So yeah, in the cognitive systems

02:46.000 --> 02:52.800
community, or I care much more about building systems that have a human-like intelligence.

02:52.800 --> 03:02.320
Yeah. It's an interesting contrast. I think we see all the time in popular media nowadays,

03:03.520 --> 03:08.000
you know, research or publishes a paper about AI scoring well against some benchmark,

03:08.000 --> 03:14.400
and then journalist writes article, AI learns to understand x, y, z, and you must be like,

03:14.400 --> 03:27.200
no way, not understand. Yeah, yeah. Yeah, I agree. So I think that the, I think things that have

03:27.200 --> 03:32.720
happened in the machine learning community, I think we've, we've, machine learning our

03:32.720 --> 03:38.400
artificial intelligence have come a long way in the past couple of decades, but I think another

03:38.400 --> 03:47.040
thing that's happened is that that we've begun or not begun, but there has evolved a particular

03:47.040 --> 03:55.760
kind of machine learning research that's just focused on doing doing well, scoring higher than

03:55.760 --> 04:01.840
a little bit higher than the last person did on that one particular data set. And then what I

04:01.840 --> 04:06.960
found in my work is that if you scratch the service a little bit, you find that there are important

04:06.960 --> 04:15.920
issues with things like the metrics that people are using, and the possibility that you, you,

04:15.920 --> 04:23.360
you have systems that are fairly overfit to particular data sets. You know, you throw adversarial

04:23.360 --> 04:29.040
examples at a system and you see it kind of crumble. It's unable to really do the things that,

04:29.040 --> 04:32.480
that you thought it was doing because it scored well in that data set.

04:32.480 --> 04:42.720
So when you, when you talk about AI as a vehicle for better understanding human intelligence,

04:42.720 --> 04:51.120
what are some examples of, of that, you know, ways that we understand human intelligence

04:51.120 --> 04:57.840
now better because of AI research. Oh, yeah, that's a, that's a great question.

04:57.840 --> 05:06.720
You know, I, my examples for this come from the kind of decades ago,

05:09.520 --> 05:18.160
traditions of AI. For example, Roger Shank and Robert Abelson's original work on

05:18.160 --> 05:25.280
scripts, goals, plans, and understanding. And that's a, that's a, that's a text or those are ideas

05:25.280 --> 05:32.640
that have kind of permeated a large, a large path throughout, throughout, throughout,

05:32.640 --> 05:38.640
through many different, throughout many different communities, including the cognitive psychology

05:38.640 --> 05:44.240
community, the cognitive linguistics community, even people performing social science,

05:45.280 --> 05:50.720
quote, this idea of scripts, which basically that means that there are knowledge structures that

05:50.720 --> 05:58.160
people use for navigating a commonly encountered social situation. So that's, that's a, I think

05:58.160 --> 06:04.880
that's a really good example that book from 1977 is still cited. It, you know, it's got tens of

06:04.880 --> 06:10.400
thousands of citations and people still cited over and over again. And when I talk to social scientists

06:10.400 --> 06:15.680
who know about this concept of scripts, they don't even, they don't even realize that it comes from

06:15.680 --> 06:21.840
artificial intelligence research, but it's important concept that people use across many disciplines.

06:21.840 --> 06:23.840
So that, that, that's a good example, I think.

06:24.880 --> 06:30.320
Got it. Yeah. And great to hear you mentioned that book. It doesn't get mentioned on the podcast

06:30.320 --> 06:36.320
very often. I think it's come up once or twice before. I went to grad school at Northwestern,

06:36.320 --> 06:43.680
and I think that's where Roger Shank was for quite a bit of time. And that was the first book

06:43.680 --> 06:49.680
that I ever came across about AI. In fact, I picked it up to like a use, you know, either like the,

06:50.480 --> 06:55.040
you know, use bookshelf at Barnes and Nobles or like a flea market or something like that.

06:57.840 --> 07:05.040
The, you also talk a little bit about, well, you mentioned that a big focus

07:05.680 --> 07:09.920
from an application perspective for you is on natural language understanding.

07:09.920 --> 07:17.200
How does that tie into the broader goal of your cognitive systems work?

07:18.240 --> 07:26.160
Yeah. What you, what you end up realizing, when I started studying this, I knew what,

07:26.160 --> 07:32.560
what lots of people can see as natural language processing, what I understood was, oh, you know,

07:32.560 --> 07:36.720
you just have these systems that are calculating statistics having to do with words or

07:36.720 --> 07:42.560
parsing to figure out what the grammatical structure is of a sentence and maybe doing some things

07:42.560 --> 07:48.720
like that. What you eventually realize is that if you, if you want to build systems and build

07:48.720 --> 07:54.800
systems that are performing something like the way humans understand language and produce language,

07:55.760 --> 08:03.760
because language expresses all, expresses ideas, you realize that your systems need to be able to

08:03.760 --> 08:08.960
represent ideas and manipulate ideas and basically have thought representations. So,

08:08.960 --> 08:14.560
your systems that, that at first you think, natural language processing is just about messing

08:14.560 --> 08:19.840
with words and grammar, you eventually come to the conclusion that natural language understanding

08:19.840 --> 08:24.960
is about building systems machines that can really think like people if you're really going to

08:24.960 --> 08:30.400
get it to understand that text and say, answer a question about that text.

08:30.400 --> 08:37.440
And so, are the tasks that you're focused on, the same tasks that we see,

08:38.800 --> 08:42.240
you know, traditional tasks in the NLU community like question answering?

08:43.520 --> 08:49.040
Yes, yeah, that's one important task, not the only thing. I mean, you can think of others

08:49.040 --> 08:56.400
such as translation or summarization and things along those lines, but more recently,

08:56.400 --> 09:02.240
folks in the machine learning, natural language processing community have started to generate

09:02.240 --> 09:09.680
large data sets that have those tasks in them that have things like, you know, a data set that

09:09.680 --> 09:15.840
has paragraphs and lots of questions and answers. But when you scratch the surface of the machine

09:15.840 --> 09:23.680
learning deep learning systems that are working on those tasks, what you, what you find out basically

09:23.680 --> 09:29.840
is that they're taking advantage of patterns in the texts and in the questions in order to come

09:29.840 --> 09:38.320
up with correct answers to the questions according to what's in the test answers for the data set.

09:38.880 --> 09:42.560
And if you change things around just a little bit, all of a sudden the system doesn't really

09:42.560 --> 09:50.480
understand. But yeah, those are things like, you know, reading a story and answering questions

09:50.480 --> 09:56.240
about the story, summarizing a story, paraphrasing a story, those, those, those are the kinds of

09:56.240 --> 10:05.840
things that I work on. Okay, and so in the, I don't think traditional is the right word here,

10:05.840 --> 10:15.040
but in the typical NLU setup, you've got this task of let's say question answering and you're just

10:15.040 --> 10:21.440
trying to perform well on the question answering data set. You're not necessarily trying to perform

10:21.440 --> 10:28.000
while you're trying to gain deeper insights into something. What are the things that you're trying

10:28.000 --> 10:34.560
to gain deeper insights to and like, what are, how do you approach that problem or what's your

10:34.560 --> 10:40.480
problem set up and where does your learning, you know, come out of thinking about these kinds of

10:40.480 --> 10:49.280
problems? Yeah, one of the deeper, deeper insights I want to get to by looking at, say,

10:50.880 --> 10:57.840
story understanding or question answering is what are the, how, how do people form a mental

10:57.840 --> 11:04.640
picture of what the text describes? How do people create that mental picture and then how do the

11:04.640 --> 11:12.320
people or how to systems manipulate that mental picture in order to be able to reason about the text

11:12.320 --> 11:19.600
and also the questions that were asked about the text? And the main thing I found in my work,

11:19.600 --> 11:26.640
the most important thing to me is that it turns out ironically that to represent

11:28.160 --> 11:33.120
knowledge thought meaning well, you have to have a significant part of your representation that

11:33.120 --> 11:38.720
really has nothing to do with the language at all. It really has to do with these imagery,

11:38.720 --> 11:44.720
mental model pictures that you create that represent the meaning and then those get mapped

11:44.720 --> 11:50.720
onto language. They get mapped from language in understanding to that representation and then when

11:50.720 --> 11:57.280
people say things or produce language, the idea which is this non-linguistic conceptual structure

11:57.280 --> 12:05.280
then gets mapped back onto language and that's one of the important things I've found in my research

12:05.280 --> 12:11.920
and an important thing I'm trying to get to by looking at tasks like question answering or paraphrasing

12:11.920 --> 12:17.920
because in many ways that's what paraphrasing is, you realize that to say, well, there are many

12:17.920 --> 12:25.040
different ways of saying the same thing. That same thing stuff must be this other thing that's

12:25.040 --> 12:34.400
different from the language, the ways of saying it. Yeah, I'm wondering what that means in practice.

12:34.400 --> 12:45.200
Like, I think of a deep learning model is building some representation that is some vector space,

12:45.200 --> 12:51.200
is what you're saying that are you adding additional constraints to that representation that says

12:51.200 --> 13:01.360
that it has more image-like properties that should I be taking what you're saying about building

13:01.360 --> 13:10.320
a picture literally or is that more figurative? It's in some senses, literal in some senses,

13:10.320 --> 13:20.480
figurative. Yeah, when it comes to say, for example, artificial neural networks or deep learning

13:20.480 --> 13:27.920
systems that are professed to be able to build their own representations, the problem that I see

13:27.920 --> 13:34.480
is that I haven't seen people demonstrate very well, particularly when it comes to natural

13:34.480 --> 13:40.960
language processing problems, demonstrate well that the representations these systems are building

13:40.960 --> 13:49.920
are like the ones that I think we should be building to build natural language understanding

13:49.920 --> 13:56.160
systems. So, you're supposed to be building your own, the system's supposed to be building its own

13:56.160 --> 14:01.840
representations. It's not obvious based on the inputs that deep learning systems are being given,

14:01.840 --> 14:08.480
say, for example, examples of paragraphs and examples of questions and answers to those

14:10.480 --> 14:16.000
relevant to those paragraphs. It's not clear that the deep learning systems are building

14:16.000 --> 14:25.840
that they're building representations that are like the ones that I think probably would be good

14:25.840 --> 14:34.000
for representing things. In my opinion, the representation systems that I work on, for the time

14:34.000 --> 14:40.880
being, I'm building them by hand, and these representation systems try to decompose meaning into

14:40.880 --> 14:47.040
complex combinations of conceptual primitive structures. For example, if I said something like

14:48.880 --> 14:55.760
Mary kicked the ball on one hand, and I said something on the other hand, like Mary moved

14:55.760 --> 15:02.480
her foot towards the ball and struck it. I didn't use the word kick in that other expression,

15:02.480 --> 15:07.440
but you're still in that other expression able to compose a picture of what happened,

15:07.440 --> 15:18.000
and see that it's equivalent to the first sentence that I gave. So, I've been building systems by

15:18.000 --> 15:25.280
hand that can start with a conceptual structure that looks like this. It's got these decomposed

15:25.280 --> 15:31.440
conceptual primitives, like, for example, for kick. One conceptual primitive meaning that

15:31.440 --> 15:37.680
Mary moved her foot, another primitive act, meaning that the foot struck the ball, and another

15:37.680 --> 15:44.160
primitive act, meaning that the ball took off and went somewhere. Then I have these other systems

15:44.160 --> 15:50.240
that actually can generate lots of paraphrases of this conceptual structure. It can generate

15:50.240 --> 15:56.720
language from it. Then what I've been doing in some ways is feeding these paraphrases into deep

15:56.720 --> 16:03.600
learning systems that supposedly can understand language and finding that they don't really.

16:06.480 --> 16:12.080
So, when you talk about this representation that you're hand crafting and you're

16:13.680 --> 16:20.720
creating equivalences between kicking and moving feet is that within the

16:20.720 --> 16:30.800
natural language domain, or is that in some vector space, or are you mapping to some kind of

16:32.640 --> 16:37.360
image-based representation of these things, or something totally different?

16:38.320 --> 16:45.520
Yeah. So, these kinds of representations have been studied by AI people before we were talking

16:45.520 --> 16:51.600
about Roger Shank, and we were talking about script plans, goals, and understanding. If you

16:51.600 --> 16:57.520
read that book, you might have remembered the original restaurant script had all these conceptual

16:57.520 --> 17:10.000
primitive acts in it, such as the waiter, P. Francis, his or herself, themselves to the customer,

17:10.000 --> 17:17.520
and then leads the customer to the table, and then the customer, M. Francis, their order to the

17:18.480 --> 17:26.960
server, and things like that. So, Shank had the system called conceptual dependency,

17:26.960 --> 17:33.440
and the idea was that try to reduce the number of primitive acts that you have to represent

17:33.440 --> 17:42.560
things, and then to represent more complex things, just add more primitive acts to make it more specific.

17:44.320 --> 17:49.920
So, for the time being, those are the kinds of representations that I work on.

17:50.880 --> 17:58.000
They have primitive, such as some object moving through space, some object moving to the inside

17:58.000 --> 18:03.280
of another object that was called ingest, some object moving from the inside to the outside of

18:03.280 --> 18:09.520
another object, expel, and then other primitive acts like that, trying to break things down in

18:09.520 --> 18:16.240
kind of molecular structure kind of way. There are other systems that people have developed in the

18:17.920 --> 18:23.440
cognitive linguistics community, specifically these systems called image schemas that were popularized

18:23.440 --> 18:30.960
by Lake Hoff and Johnson. And in research, we're only just now over the past few years figuring out

18:30.960 --> 18:37.440
some of the correspondences between a Shank's old system of conceptual dependency,

18:37.440 --> 18:41.840
and these other systems that were developed by people in the cognitive linguistics community,

18:41.840 --> 18:48.480
and trying to learn from them. Sorry, if that's kind of too long. That's great.

18:48.480 --> 19:01.200
I'm wondering about you kind of, as part of the setup you talked about how deep learning systems,

19:01.200 --> 19:11.680
as an example, are often overfitted on a given benchmark, and there's a lot of competition

19:11.680 --> 19:17.600
in the research community to one up the next and achieving state-of-the-art performance on a

19:17.600 --> 19:31.920
given benchmark. Is your research, are you like saying, hey, I'm not going to play that game,

19:31.920 --> 19:37.680
I'm trying to get understanding, and if you aren't, if you're opting out of that game,

19:37.680 --> 19:41.120
how do you evaluate the performance of your representations?

19:41.120 --> 19:50.400
Yeah, that's a great question. In some ways, I am not playing that game, and in some ways,

19:51.280 --> 19:58.320
I am playing that game. In recent work that was published at the Advanced

19:58.320 --> 20:04.480
Executive Systems Conference last summer, what I did was basically, and I think I described

20:04.480 --> 20:11.840
this a little bit earlier, there are deep learning systems that can do paraphrase recognition,

20:11.840 --> 20:17.440
and there are data sets that are devoted to training and evaluating systems that can

20:18.880 --> 20:25.840
read two sentences and determine to some degree whether those sentences are paraphrases or not.

20:26.720 --> 20:32.320
What I did is I took one of those systems, and then I took a system that Shank and his students

20:32.320 --> 20:39.600
worked on at Stanford in the early 70s and I enhanced it, and what the system does is the system

20:39.600 --> 20:44.240
was called Margie, and what it could do was you could input some natural language, it would

20:44.240 --> 20:50.080
translate it over to this non-linguistic language-free conceptual representation, conceptual dependency,

20:50.800 --> 20:58.080
and then it could, based on its language-free thought representation of the original sentence,

20:58.080 --> 21:03.680
it could generate paraphrases of the original sentence. I enhanced the system so that it

21:03.680 --> 21:10.560
could generate a lot more paraphrases than it could in the 70s because in the 70s, the computers

21:10.560 --> 21:17.040
that people had and the systems that people had were very limited, and so I generated a set of

21:18.000 --> 21:27.600
tens of thousands of paraphrased pairs that meant the same thing, and I also tested that these

21:27.600 --> 21:33.200
pairs meant the same thing because I sent them to crowd workers and asked, okay, do these sentences,

21:33.200 --> 21:39.040
you think these sentences mean the same thing, and overwhelmingly they did. Then what I did is I

21:39.040 --> 21:44.000
took these pairs of sentences that all meant the same thing, and then I sent them to one of these,

21:44.960 --> 21:51.760
I didn't send them, but I used one of these deep learning paraphrase detection paraphrase

21:51.760 --> 21:57.920
recognition systems, and I asked this paraphrase recognition system, okay, do you think these sentences

21:57.920 --> 22:03.840
mean the same thing, and what was interesting about the sentences that my system generated was that

22:04.480 --> 22:14.400
they encompassed a wide range of linguistic variation, they were able to express the same idea

22:14.400 --> 22:21.120
using lots of different words and different syntactic structures. So you had sentences that

22:21.120 --> 22:27.280
meant the same thing that maybe only had one word in common, and syntactically they're very different,

22:27.840 --> 22:33.040
and humans looked at these pairs of sentences and said, yeah, those mean the same thing,

22:33.040 --> 22:38.000
the deep learning systems that were trained to do paraphrase recognition and detection,

22:38.720 --> 22:46.480
they fell off a cliff, when the sentences stopped using similar words, and you got to the point

22:46.480 --> 22:51.280
where the sentences were not using the same words at all, meant the same thing, the deep learning system

22:51.280 --> 22:56.480
was basically like a coin flip, half of the time I would say they were paraphrases, half of the time

22:56.480 --> 23:00.480
they would not, it was basically just guessing because it wasn't really understanding

23:01.680 --> 23:07.600
what the sentences meant. So in that way, I play the game a little bit.

23:07.600 --> 23:20.160
I was just going to push a little further on that and try to have you explain in that context,

23:21.040 --> 23:26.000
what are you benchmarking your systems results against, like is it?

23:29.600 --> 23:34.800
It's not necessarily trying to, well, maybe the goal of this research was to identify the

23:34.800 --> 23:44.560
deficiencies of the deep learning systems, and your metric was the number of paraphrases that they

23:44.560 --> 23:54.720
couldn't recognize or the performance, but I'm curious, do you ever compare the performance of

23:54.720 --> 24:00.240
your representations against the performance of a deep learning type of system,

24:00.240 --> 24:10.480
or are there other ways for you to understand whether the systems that you're able to create

24:10.480 --> 24:20.240
with your representations are more robust. It also does strike me that some ways maybe the metrics

24:20.240 --> 24:31.600
aren't rich enough, like envisioning a scenario where you're creating these paraphrases that

24:31.600 --> 24:38.800
are so much richer than what a deep learning model might create. And is the metric,

24:39.840 --> 24:45.440
have we created the metric for richness and what is that expressiveness? Maybe the traditional

24:45.440 --> 24:51.520
competitions aren't really judging the things that your representations are better at,

24:51.520 --> 24:58.080
but I'm also wondering broadly, if you opt out like how do you compare and how do you know

24:58.080 --> 25:03.600
when you're on the right track and when your research is getting you closer to understanding?

25:04.560 --> 25:13.040
Yeah, you raise a bunch of important questions. Let me try to address each of them in turn.

25:13.040 --> 25:18.720
Yeah, so in the study that I was describing, there are metrics that you can apply,

25:18.720 --> 25:26.240
but these are metrics that, well, not metrics, but we did statistical tests to determine

25:26.240 --> 25:38.960
that basically humans were better at recognizing that the sentences in this set of paraphrase

25:38.960 --> 25:44.240
pairs, that these paraphrase pairs, humans were better at seeing that these sentences meant the

25:44.240 --> 25:49.440
same thing, and we were able to perform statistical tests showing that humans saw that they meant

25:49.440 --> 25:56.080
the same thing, whereas the deep learning systems that we were testing, they basically declined

25:56.640 --> 26:04.080
in performance with greater degree of linguistic variation. You raise the important issue of,

26:04.080 --> 26:10.800
well, if linguistic variation is important, what are the measures for doing that?

26:11.840 --> 26:19.520
And I've developed some measures for linguistic variation such as you can do things like

26:20.320 --> 26:27.360
if you've got a corpus of text, if you've got two different corpore of text, you can do things like

26:27.360 --> 26:35.040
parse all the content, and then look at things like what were the part of speech tags that were given?

26:36.960 --> 26:42.320
What is the distribution of the part of speech tags that were given in this data set versus that

26:42.320 --> 26:48.960
data set? And I did that in a previous study where I was trying to better understand the differences

26:48.960 --> 27:00.080
between human caption generation and captions that were generated by the typical deep learning

27:00.080 --> 27:08.400
systems that people have used for caption generation problems. But I think it's something that needs

27:09.440 --> 27:12.880
studying these kinds of metrics is something that I want to work on in the future.

27:12.880 --> 27:20.240
There are projects that I'm working on now where I'm hoping I'll be able to actually build systems

27:20.240 --> 27:26.640
that, so I talked to you about this paraphrase generation system. I'm working on systems now that

27:26.640 --> 27:34.560
ideally can do the opposite. They can read and understand language and bringing into this

27:34.560 --> 27:42.080
bringing into this non-linguistic form. And we're using a data set from a data set called

27:42.080 --> 27:53.440
ProPara from the Allen AI Institute, I believe, and this data set has these texts that are about

27:54.720 --> 28:01.120
ProPara short for process paragraphs, and these texts are about things like geological processes

28:01.120 --> 28:05.920
and physical and chemical processes, and we're going to treat those like their stories. And that

28:05.920 --> 28:15.120
data set has tests that you can use to see if your system is really measuring up. So that is part

28:15.120 --> 28:24.480
of our plan. But I think the larger issue of having the right metrics is important.

28:25.840 --> 28:32.400
When it comes to judging these systems that perform paraphrase recognition, the metrics that

28:32.400 --> 28:38.000
are usually used are these bag of words metrics, like, for example, the blue metric. Basically,

28:38.000 --> 28:43.760
what they do is they take, they want to compare one text to another's text to see if they are

28:43.760 --> 28:50.640
kind of the same, and they just treat each text like a bag of words. The metric doesn't care

28:50.640 --> 28:56.560
about the order in which words come in and things like that, or you could have rearranged the words

28:56.560 --> 29:01.360
and totally, it could have meant something different. You're basically just trying to get your system

29:01.360 --> 29:12.400
to throw the right words in there that are relevant to the thing. You can say the same thing

29:12.400 --> 29:19.440
with different words, and then your metric doesn't really work anymore. But generally, this is an

29:19.440 --> 29:29.840
issue with the kind of research that people are doing where they just want to score better than

29:29.840 --> 29:34.960
the last person on that data set. And so they don't really care what the metric is very much,

29:34.960 --> 29:38.160
or whether the metric means anything. All they care about is their score.

29:42.720 --> 29:48.960
In some ways, your research makes me think of folks like Josh Tenenbaum at MIT.

29:50.400 --> 29:56.720
His is, I think, less natural language focused and more kind of visual. At least, I think about

29:56.720 --> 30:03.920
it like that. And a lot of what he'll talk about is trying to kind of capture and understand

30:03.920 --> 30:11.680
this idea of common sense, and, you know, external opportunity knowledge or things like that.

30:12.800 --> 30:17.680
Is that part of what you're looking to understand in your research and the language domain?

30:17.680 --> 30:27.840
Yeah, yeah. I'm flattered that you mentioned Josh Tenenbaum when talking about my work,

30:27.840 --> 30:32.160
and then you think of Josh Tenenbaum. I'm very honored that you would say that.

30:32.720 --> 30:38.240
And, yeah, I think that we are interested in many of the same issues.

30:39.760 --> 30:47.040
I'm trying to remember the term that Josh Tenenbaum uses. I can't remember it at the moment,

30:47.040 --> 30:52.160
but I do believe that from what I've heard him talk about, that he is interested in

30:52.800 --> 30:57.840
trying to figure out what the most abstract conceptual primitives or

31:00.400 --> 31:05.120
structures that people are using to kind of break things down and understand them.

31:05.920 --> 31:11.200
I think he's what I've seen. I've seen him do some interesting and important work involving

31:11.200 --> 31:19.360
children and their interactions with people around certain kinds of tasks that involve

31:19.360 --> 31:27.120
reasoning and social interaction and things like that. Yeah, I think it's a great important work.

31:28.640 --> 31:35.680
And so that's something I'm interested in, too, is if you believe like me that there are some

31:35.680 --> 31:43.280
structures that some cognitive structures that people evolve or, let's say, people develop

31:43.280 --> 31:47.600
at a young age before they even learn language. If you're going to say that they're these kind of

31:47.600 --> 31:54.160
pre-linguistic representations that people are using in their building language on top of those

31:54.160 --> 31:59.680
representations, you'd be interested to know what those are. And I think there are folks like

31:59.680 --> 32:07.280
Josh Tannenbaum who are trying to get it what that is or what those things are.

32:08.560 --> 32:14.800
But perhaps not so much doing it through language. You can see I'm trying to do it

32:15.360 --> 32:19.520
through these tasks that have to do largely to do with language and texts.

32:19.520 --> 32:32.320
Yeah. In the kind of traditional deep learning approach, like the knowledge of the system is

32:32.320 --> 32:37.760
kind of stored in like weights and embeddings and things like that. It's kind of inherent in the

32:37.760 --> 32:49.600
model. Sounds like your representations are more external. I wonder if there's anything you can

32:49.600 --> 33:00.320
elaborate on there. Does your work have this kind of traditional view of a model? How does that

33:00.320 --> 33:07.680
relate when you're building systems around your representations? What do they look

33:07.680 --> 33:12.240
like? Are they similar to what we might be used to with deep learning and machine learning?

33:12.240 --> 33:20.720
Or are they very different? In many ways they would look like structures,

33:21.600 --> 33:29.920
kinds of models and systems and structures that were way more popular perhaps in the 1970s

33:29.920 --> 33:37.840
in 1980s when people would call symbolic artificial intelligence, good old-fashioned artificial

33:37.840 --> 33:48.080
intelligence. Some people characterize them also as rule-based systems. I've been building

33:48.080 --> 33:56.320
a lot of systems by hand. Are you okay with all of those terminologies or are you

33:56.320 --> 34:03.760
air quoting it because you don't really like those terms? Well, I'm air quoting it because

34:03.760 --> 34:15.280
if you say symbolic AI, I have no problem with that. I guess the reason why air

34:15.280 --> 34:24.240
quoted is because people have negative associations with those kinds of technologies where you are

34:24.240 --> 34:31.840
building things by hand. Historically, people know that there have been ups and downs in artificial

34:31.840 --> 34:38.640
intelligence. AI winters, as they're often referred to, and there were AI winters that were

34:38.640 --> 34:44.400
associated with people building rule-based systems, symbolic systems where they were actually

34:44.400 --> 34:58.080
coding things by hand instead of using machine learning. My view is that there's a general

34:58.080 --> 35:05.120
negative association with doing any sort of building systems by hand. I think part of it is because

35:05.920 --> 35:11.920
the old AI tradition people thought about this idea that you'd have expert systems that were

35:11.920 --> 35:19.280
able to replace people completely and so what people came to realize is man, there's so much

35:19.280 --> 35:24.080
that people know and so much that I would need to encode in my own that there's no way I would

35:24.080 --> 35:30.000
ever be able to, if people were working around the clock, lots of them just encoding this knowledge

35:30.000 --> 35:36.160
by hand, it's impossible that we'd ever be able to build a real system. Notwithstanding

35:36.160 --> 35:45.600
double-enit and psych and projects like that, but my point of view is different. I'm not building

35:45.600 --> 35:52.640
systems by hand in the hopes that I would eventually be able to build an artificial general

35:52.640 --> 35:57.520
intelligence that I was coding by hand and I would eventually be able to encode all of the knowledge

35:57.520 --> 36:05.600
that people have, but I'm using symbolic systems, building systems by hand, or you can say,

36:05.600 --> 36:10.320
you can call them rule-based systems if you like to, if it's not too pejorative. I mean,

36:10.320 --> 36:17.920
the reason why I build these systems is to help try to understand what's going on, what the

36:17.920 --> 36:25.520
representation should be so that when we turn to say, okay, we're going to now use machine learning

36:25.520 --> 36:32.160
to try to build systems to do these tasks, we have better ideas. Instead of kind of what I see

36:32.160 --> 36:41.040
today is, there's deep learning, right? And it feels people, it seems like people think that

36:41.040 --> 36:46.720
deep learning can do anything as long as you supply enough data to it. But then the question is

36:46.720 --> 36:53.360
what kind of data should we supply to it? If we just supply text, is there stuff that we know

36:53.360 --> 36:59.520
that helps us understand text that is outside of the text itself? And do we also need to supply that?

36:59.520 --> 37:10.720
No. And so those, that's why I'm not opposed to building systems by hand. And in some of these

37:10.720 --> 37:16.880
studies, what I've done is I've built systems by hand that are able to say, for example, generate

37:16.880 --> 37:26.640
lots of paraphrases as an example and create a data set like that that demonstrates, okay,

37:26.640 --> 37:31.680
maybe these are the kinds of representations we should be aiming for in our deep learning,

37:31.680 --> 37:38.320
machine learning systems. I think that the new target, so right now the way things are is that people,

37:39.680 --> 37:43.840
people build these data sets say, for example, paragraphs, questions, and answers,

37:43.840 --> 37:47.920
and the target is just try to get the deep learning system to give the right answer.

37:47.920 --> 37:55.520
In my opinion, the new target should be, should be that you should be supplying data sets that

37:55.520 --> 38:01.440
help deep learning machine learning systems build the right representations. And those representations

38:01.440 --> 38:07.120
might be non-linguistic or language free. And in turn, those representations help you get the

38:07.120 --> 38:12.640
right answers. And then you can see, oh yeah, this thing is really kind of thinking the way I want

38:12.640 --> 38:18.080
it to think. Whereas right now it feels like deep learning is just giving us a black box

38:19.280 --> 38:24.800
where we can't quite see inside and see whether the representations it's building to solve the

38:24.800 --> 38:32.160
task, making sense. Yeah, that the way you describe that resonates really strongly with the

38:32.160 --> 38:38.560
conversation I had just the other day with Peter Abiel, a research out of Berkeley focused on

38:38.560 --> 38:47.600
robotics. And we were kind of comparing and contrasting his views as an academic thinking about

38:47.600 --> 38:53.280
kind of end-to-end deep learning. And you know, as you said, just throw enough data at a problem.

38:53.280 --> 39:02.080
And his more evolved views as an entrepreneur and a roboticist that's trying to solve problems

39:02.080 --> 39:13.600
for for companies. And the need to try to capture and incorporate knowledge that we have about

39:13.600 --> 39:19.040
these these problems. And he made this really interesting point that, you know, one, you know,

39:19.040 --> 39:23.920
if you say, okay, we want to incorporate, you know, this knowledge that we have about a problem,

39:25.040 --> 39:31.440
you know, one way to do it is to, you know, build rules into your system, you know, build your

39:31.440 --> 39:39.440
if-then statements or whatever that looks like. But what he's said that struck me as really interesting

39:39.440 --> 39:47.440
was another way to do that is to use your rules to generate more data for your deep learning systems.

39:47.440 --> 39:58.000
And so in that way, you're kind of training them on the cases that you know a lot about. But still

39:58.000 --> 40:06.160
not having to, you know, not having to take on the technical debt, if you will, of having a lot

40:06.160 --> 40:11.440
of rules, you know, the brittleness of rules, that kind of thing. And it strikes me that maybe your

40:11.440 --> 40:15.920
systems could be used in a simpler way. Like you've demonstrated the ability to create these really

40:15.920 --> 40:24.800
robust paraphrases that, you know, could be really interesting augmented data for a paraphrasing

40:24.800 --> 40:30.160
system that you might want to train. Maybe that is the kind of glue between your world and the deep

40:30.160 --> 40:37.760
learning world. Yeah, that is that is one way. I mean, so anytime you come up with an adversary,

40:37.760 --> 40:44.240
anytime someone comes up with a with anything that shows that the hey, the deep learning system

40:44.240 --> 40:50.960
is really isn't really doing what you want. Then the natural, you know, what the what the folks

40:50.960 --> 40:56.640
who are doing big data, the machine learning will just say, I think many of them will probably

40:56.640 --> 41:02.880
say is, well, okay, just give me the data that you just created that made my system. Give me those

41:02.880 --> 41:08.480
examples and I'll just feed them in. And then my system will, well, then be able to handle those

41:08.480 --> 41:16.880
kinds of will be able to handle those adversaries and will be better as a result. That's that's one

41:16.880 --> 41:24.320
way of looking at it. But then from from my standpoint, it's it's, you know, I can I can continue

41:24.320 --> 41:31.120
doing work on on these kinds, these these representational issues. And, you know, perhaps generating

41:31.120 --> 41:43.120
generating data. But I think I think generally, yeah, I think I think generally it's it's one one

41:43.120 --> 41:49.120
way to interface between those communities. I should say, though, also that I think part of the

41:49.120 --> 41:59.120
reason part of the reasons why the the in the export systems era that the rule based systems

41:59.120 --> 42:06.800
people had had a lot of difficulty was that they were using in many cases, in many cases, perhaps

42:06.800 --> 42:13.680
they're using certain kinds of logical inference engines. And I think the issue with logic is that

42:13.680 --> 42:19.280
people what people mostly tend to do is they create logical symbols that correspond to words. They

42:19.280 --> 42:25.760
don't necessarily create symbols in those systems that correspond to non linguistic conceptual

42:25.760 --> 42:31.200
representations that I think we have. And so I think if we're if we're creating knowledge structures

42:31.200 --> 42:37.680
and reasoning systems that have more of that non linguistic language free abstract primitive

42:37.680 --> 42:46.320
decomposition stuff that that we could we could do much better even even if we're even if we're

42:46.320 --> 42:50.640
not going to use machine learning at all. If we were just back to building export systems like we

42:50.640 --> 42:57.600
were in the early, well, I suppose in earlier decades, if we're back to building those

42:57.600 --> 43:03.040
export systems, if we're using better representations, those systems could have been better too. You know,

43:03.040 --> 43:08.720
maybe we would not have failed and had a had AI winters the way we way we did.

43:12.800 --> 43:17.760
What are, you know, kind of looking forward? What are you most excited about in terms of

43:17.760 --> 43:27.040
directions for your research? Yeah, so the systems that the these representational systems that

43:27.040 --> 43:38.880
they keep talking about for the time being they're we find that things are fairly straight much

43:38.880 --> 43:44.160
more straightforward when you're talking about things that are happening in the physical world

43:44.160 --> 43:50.800
like Mary kicking the ball being decomposed into someone's you know Mary's foot moving and striking

43:50.800 --> 43:59.120
the ball and then the ball moving and things like that. One thing that even even in earlier decades

43:59.120 --> 44:03.360
where they were working on this research of de you know trying to represent trying to come up with

44:03.360 --> 44:09.520
these non linguistic representations that they never I in my opinion they didn't really get a good

44:09.520 --> 44:17.120
handle on and and this is also true if you read scripts plans goals and understanding was well

44:17.120 --> 44:23.360
how do you how do you decompose the idea that someone should have a goal or how do you decompose

44:23.360 --> 44:33.760
the idea that someone should have a plan? How do you decompose a decision or or or other kinds

44:33.760 --> 44:41.840
of activities that involve thoughts? And they had a I feel like they got stuck back in those days

44:41.840 --> 44:47.360
and didn't make much progress in in figuring those things out. They in many cases created

44:49.440 --> 44:55.840
more and more diverse kinds of structures without doing the decomposition into primitive thing

44:55.840 --> 45:03.600
that I think made their work in the early 70s more cool versus their work in the mid mid 80s

45:03.600 --> 45:11.200
or so. And so that's something that I'm in future work that I'm then curious about interested in

45:12.720 --> 45:19.280
again about scripts plans goals and understanding one of the one of the important basic ideas from

45:19.280 --> 45:28.480
that book is that some of or some important reasoning involves from your episodic memories.

45:28.480 --> 45:35.680
So remember the restaurant script the idea that you tell this story about Mary going into the

45:35.680 --> 45:42.400
restaurant or you know she she eats the lobster and leaves and you're able to reason that

45:43.120 --> 45:48.640
the she ordered the lobster from the server and all these other things and those knowledge

45:48.640 --> 45:53.360
structures are built out of or at least theoretically the theoretical idea is that those

45:53.360 --> 45:58.240
knowledge structures are built out of your episodic memories of your experiences with restaurants.

45:59.280 --> 46:06.720
So you start wondering well you know what if what if other kinds of reasoning actually work that

46:06.720 --> 46:13.760
way where you can you can build lots of script structures representing people's common experiences

46:13.760 --> 46:19.680
and that's how people do a lot of their reasoning and a lot of their reasoning may involve scripts

46:19.680 --> 46:30.400
combining with each other to reason about unusual events such as you know what happens when you

46:30.400 --> 46:36.320
have a birthday party at the restaurant you combine the birthday party script with the restaurant

46:36.320 --> 46:40.480
script and then you start reasoning about things like well if so and so pays for my dinner is that

46:40.480 --> 46:48.000
considered a birthday gift at the restaurant something like that. And so that's that's something

46:48.000 --> 46:55.200
I'm really excited about is learning more about general general reasoning through these structures

46:55.200 --> 47:03.120
that are meant to represent people's episodic memories just people's experiences rather than saying

47:03.120 --> 47:10.560
well it must be first-order logic or or I guess on another you know if you take it the other way

47:10.560 --> 47:16.640
it must be deep learning is the only way or something like that so you know can you can you

47:16.640 --> 47:25.680
start building databases of scripts or or databases of episodic memories and start using those as

47:25.680 --> 47:31.120
the basis for structures for reasoning and things like that so those are those are just a couple

47:31.120 --> 47:35.280
a couple of examples of things that I'm really excited about in the future.

47:35.920 --> 47:43.040
Awesome awesome well as always we will link to your website and some of your recent work on the

47:43.040 --> 47:50.640
show notes page that is will be available when the episode is published. Thanks so much for

47:50.640 --> 47:56.640
taking the time to share a bit about what you're up to. Thank you Sam thank you so much everybody

47:56.640 --> 48:13.200
for having me.

