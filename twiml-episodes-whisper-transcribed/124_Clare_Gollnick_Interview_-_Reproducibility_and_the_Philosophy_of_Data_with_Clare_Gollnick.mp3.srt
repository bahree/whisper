1
00:00:00,000 --> 00:00:16,000
Hello and welcome to another episode of Twimble Talk, the podcast rye interview interesting

2
00:00:16,000 --> 00:00:20,840
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,840 --> 00:00:36,360
I'm your host Sam Charrington. In this episode I'm joined by Claire Galnick, CTO of Turbium

4
00:00:36,360 --> 00:00:43,720
Labs, to discuss our thoughts on the reproducibility crisis currently haunting the scientific landscape.

5
00:00:43,720 --> 00:00:49,800
For a little background, a nature survey in 2016 showed that more than 70% of researchers

6
00:00:49,800 --> 00:00:55,720
have tried and failed to reproduce another scientist experiments, and more than half have failed

7
00:00:55,720 --> 00:01:01,720
to reproduce their own experiments. Claire gives us her take on the situation and how it applies

8
00:01:01,720 --> 00:01:07,400
to data science, along with some great nuggets about the philosophy of data and a few interesting

9
00:01:07,400 --> 00:01:13,560
use cases as well. We also cover her thoughts on Bayesian versus frequentist techniques,

10
00:01:13,560 --> 00:01:19,400
and while we're at it, the VIM versus EMAX debate. No, actually I'm just kidding on that last one,

11
00:01:19,400 --> 00:01:23,400
but this was indeed a very fun conversation that I think you'll enjoy.

12
00:01:25,000 --> 00:01:30,920
Okay, you all know I traveled to a ton of events each year, and the event season is just getting

13
00:01:30,920 --> 00:01:37,320
started for me. On Friday, I'm on my way back to the Bay Area for the scaled ML conference and

14
00:01:37,320 --> 00:01:45,160
video GTC. But the event I'm most excited about is my very own AI summit, the successor to the

15
00:01:45,160 --> 00:01:51,080
awesome future of data summit I did last year. This year's event takes place on April 30th and

16
00:01:51,080 --> 00:01:57,160
May 1st, and is once again being held in Las Vegas in conjunction with the Interrupt ITX conference.

17
00:01:58,200 --> 00:02:02,600
This year's event is much more AI focused and is targeting enterprise line of business and

18
00:02:02,600 --> 00:02:09,640
IT managers and leaders who want to get smart on AI very quickly. Think of it as a two-day no-fluff

19
00:02:09,640 --> 00:02:15,160
technical MBA and machine learning and AI. I'll be presenting an ML and AI bootcamp,

20
00:02:15,160 --> 00:02:20,040
and I'll have experts coming in to present many workshops on topics like computer vision,

21
00:02:20,040 --> 00:02:25,560
natural language processing, conversational applications, machine learning and AI for IOT and

22
00:02:25,560 --> 00:02:33,400
industrial applications, data management for AI, building an AI first culture, and operationalizing

23
00:02:33,400 --> 00:02:38,200
machine learning and AI applications and systems. For more information on the program,

24
00:02:38,200 --> 00:02:43,960
visit twimmolai.com slash AI summit. And now on to the show.

25
00:02:49,160 --> 00:02:54,920
All right, everyone. I am on the line with Claire Galnick. Claire is CTO at Turbium Labs.

26
00:02:54,920 --> 00:02:59,480
Claire, welcome to this week in machine learning and AI. Thank you. I'm happy to be here.

27
00:03:00,280 --> 00:03:05,800
So you've got a background in neuroscience and biomedical engineering,

28
00:03:05,800 --> 00:03:10,040
but you've ended up spending a lot of your time working in data science and machine learning.

29
00:03:10,040 --> 00:03:12,040
Can you tell us a little bit about your path?

30
00:03:13,720 --> 00:03:19,640
Definitely. So my background, my graduate school research was actually in information processing

31
00:03:19,640 --> 00:03:25,320
and neural networks, but not artificial neural networks, not neural networks that data scientists

32
00:03:25,320 --> 00:03:31,080
are usually talking about. And so it was definitely an interesting transition, leaving

33
00:03:32,440 --> 00:03:37,160
leaving graduate school, spending all my time trying to figure out how the brain works,

34
00:03:37,160 --> 00:03:42,280
and then coming into data science and hearing people say, you know, like my algorithm works

35
00:03:42,280 --> 00:03:49,720
just like the brain, and I was like, wow, interesting. You should tell the neuroscientists how that works.

36
00:03:49,720 --> 00:03:54,360
But it was, yeah, it was an interesting transition. It's definitely a different type of problem,

37
00:03:54,360 --> 00:04:01,080
and a different way of using data to make the world better. So in science, you're trying to learn

38
00:04:01,080 --> 00:04:06,200
so that you in particular in biomedical engineering to help cure diseases. And now I work in

39
00:04:06,200 --> 00:04:10,920
cybersecurity. And so you're using data and fighting another hard problem. It's just a very different

40
00:04:10,920 --> 00:04:18,280
type of problem. Can you tell us a little bit more about your graduate work? What was your research

41
00:04:18,280 --> 00:04:27,880
there? Yes. So we studied how your brain processes sensory information. So after, you know,

42
00:04:27,880 --> 00:04:32,280
light touches your retina or something touches your skin or your somatosensory system,

43
00:04:33,240 --> 00:04:37,800
how is that information propagated through neural signals, through spike trains,

44
00:04:37,800 --> 00:04:43,400
and how is it that eventually that turns into perception would be the big question.

45
00:04:44,200 --> 00:04:49,720
And interestingly, with the engineering bent, the question was using this knowledge,

46
00:04:49,720 --> 00:04:56,040
could we replace this sensory signal with artificial signals so that we can mimic sensory input

47
00:04:56,040 --> 00:05:01,720
for people who have lost it? So for example, if you've lost a limb and you no longer have your

48
00:05:01,720 --> 00:05:06,440
sense of touch, could we replace that, for example, was a long-term goal?

49
00:05:06,440 --> 00:05:12,600
Interesting. And now when you're looking at this as a scientist, a neuroscientist,

50
00:05:12,600 --> 00:05:20,120
are you exploring it from at the level of chemicals, or are you looking at it, or were you looking at

51
00:05:20,120 --> 00:05:27,480
it more from a system level, electrical signals, or all of the above? Yeah. So we used a couple of

52
00:05:27,480 --> 00:05:31,960
different signals, but broadly we're operating on the like spike train or neuron level.

53
00:05:31,960 --> 00:05:37,240
And so I like to think of it as a, you know, a non-natural language. What is the language of the

54
00:05:37,240 --> 00:05:41,480
brain? And the best current model for understanding how information is transmitted

55
00:05:42,280 --> 00:05:48,200
in the brain is through neuron spiking. And these patterns of spiking and the different

56
00:05:48,200 --> 00:05:52,760
neurons spiking at different times to represent different patterns that could create different

57
00:05:52,760 --> 00:05:58,760
perceptions. Do you find it in working in data science and talking with data scientists that

58
00:05:58,760 --> 00:06:03,480
there are things that, you know, things that you learned about the brain and how those types

59
00:06:03,480 --> 00:06:09,560
of networks work that you feel would be more helpful, if that would be helpful, if they were more

60
00:06:09,560 --> 00:06:15,640
broadly appreciated within data science? That is an interesting question. The thing that I find

61
00:06:15,640 --> 00:06:23,320
most fascinating is how these two fields interact. One of the things that is true is that we don't

62
00:06:23,320 --> 00:06:29,880
know how the brain works. And often we're using mathematical frameworks. And we force that onto

63
00:06:29,880 --> 00:06:35,880
our model of the brain, as opposed, and we see if it's a good model for predicting, for understanding

64
00:06:35,880 --> 00:06:40,840
the brain activity. And so it's a weird interplay where you would, you would in many cases prefer it

65
00:06:40,840 --> 00:06:46,680
to be more from the brain to feeding the statistical modeling and the network modeling.

66
00:06:46,680 --> 00:06:53,960
But often it goes the other way around. A new statistical estimation technique comes, uh, is developed

67
00:06:53,960 --> 00:06:59,720
and then it is applied backwards on into neuroscience. It is an interesting dynamic because you would,

68
00:06:59,720 --> 00:07:05,000
you would imagine or hope in some cases that it went more the other way. Right, right. Especially

69
00:07:05,000 --> 00:07:10,680
going back to your comment, how people talk about artificial neural networks as being brain inspired

70
00:07:10,680 --> 00:07:16,600
when, uh, it often happens the other way around. Yeah, it can. I definitely see it going the other

71
00:07:16,600 --> 00:07:23,880
way around very often. Awesome. And so you've been recently spending some time thinking about,

72
00:07:25,240 --> 00:07:30,920
kind of going back to your roots in science, the reproducibility crisis that has been

73
00:07:31,960 --> 00:07:38,600
much talked about in that field and what data scientists can take away from that. Maybe start by

74
00:07:38,600 --> 00:07:43,320
talking about that reproducibility crisis for folks that aren't familiar with it. Definitely.

75
00:07:43,320 --> 00:07:48,840
So I would say that the reproducibility crisis is a little bit controversial. But the, the main,

76
00:07:48,840 --> 00:07:55,160
the main observation is that a lot of the literature that has been published, a lot of the

77
00:07:55,160 --> 00:07:59,400
experiments that have been summarized and the results that have been summarized in particular in

78
00:07:59,400 --> 00:08:03,960
the fields that I was in. So in biology and neuroscience and in, and in psychology,

79
00:08:05,000 --> 00:08:10,040
is not reproducible. Meaning that if you were to pick a paper out of the literature and try to

80
00:08:10,040 --> 00:08:15,560
perform the experiment as exactly a set out in the methods, you likely would not achieve the same

81
00:08:15,560 --> 00:08:21,720
results as the authors report. And you say it's, do you say it's controversial? Would you say it's

82
00:08:21,720 --> 00:08:31,000
controversial? That it exists or because it exists? I think it is controversial because it implies

83
00:08:31,000 --> 00:08:38,280
that the methods that scientists are using are not actually objective or perfect. And a lot of

84
00:08:38,280 --> 00:08:43,400
scientists work very hard and their work is very close to their identity. And so the implication

85
00:08:43,400 --> 00:08:47,400
that the body of work that a lot of very hard working and well-meeting scientists have put out

86
00:08:47,400 --> 00:08:55,480
into the world is not reliable as a, as a body of knowledge. It's a lot of people really

87
00:08:55,480 --> 00:08:59,800
core to their identity. And I think the, the way that it's controversial, it comes from, it comes

88
00:08:59,800 --> 00:09:06,920
from that sense of ownership over this body of literature. Is it, is it generally accepted though

89
00:09:06,920 --> 00:09:14,520
that it's a problem or is, is that still up for a debate in, in scientific corners?

90
00:09:15,480 --> 00:09:20,520
I would say that the fact that there is a problem is no longer debated. The scale of the problem

91
00:09:20,520 --> 00:09:27,400
is still up for debate. You know, when I've heard about, when the, when the topic of the reproducibility

92
00:09:27,400 --> 00:09:34,200
crisis comes up, a lot of times, you hear people throwing around like p-values and, and the

93
00:09:34,200 --> 00:09:40,040
implications of that, what's that all about? That's, that's very interesting. So I can talk

94
00:09:40,040 --> 00:09:45,720
from my own experience about how I was taught about hypothesis testing and p-values being a part

95
00:09:45,720 --> 00:09:51,080
of that. And I think it's interesting because I see a lot of parallels to the way that data

96
00:09:51,080 --> 00:09:56,600
scientists taught now. When I was in high school, I learned about hypothesis testing and I thought

97
00:09:56,600 --> 00:10:03,800
it was the coolest technology that anyone could possibly imagine because I so desperately wanted

98
00:10:03,800 --> 00:10:08,920
to be a scientist and I knew that scientists needed to be objective. So I needed this objective

99
00:10:08,920 --> 00:10:14,600
way to take an experiment that I ran and then know whether something was true, whether my experiment

100
00:10:14,600 --> 00:10:21,560
was meaningful. And the hypothesis testing was just so compelling as a tool because it'll

101
00:10:21,560 --> 00:10:25,560
basically allow me to just take this data and run it through an algorithm and then have an answer.

102
00:10:25,560 --> 00:10:33,720
And what is, what is interesting about that is that now these hypothesis tests are the center

103
00:10:33,720 --> 00:10:39,800
of the controversy over over the reproducibility crisis and a lot of people are attributing them.

104
00:10:39,800 --> 00:10:44,920
And I agree that it's a major part of the cause of the reproducibility crisis. And one of the most

105
00:10:44,920 --> 00:10:51,480
interesting things about it is that it's actually a problem of scale that p-values and hypothesis

106
00:10:51,480 --> 00:10:56,600
testing applied across many scientists simultaneously or over and over and over again on the same

107
00:10:56,600 --> 00:11:04,520
data set. That's a process we now call key hacking. It's actually re-analyzing the same data over

108
00:11:04,520 --> 00:11:07,720
and over and over again until you find something that appears to be statistically significant.

109
00:11:09,000 --> 00:11:14,360
And that's now considered one of the major problems and major causes of the reproducibility

110
00:11:14,360 --> 00:11:19,240
crisis. And what's interesting is that really machine learning is exactly that. It's re-analyzing

111
00:11:19,240 --> 00:11:24,440
the same data over and over and over again testing hypotheses in a very, very fast faster than

112
00:11:24,440 --> 00:11:28,920
even any human or any scientist normally would be able to and then trying to pick the one that best

113
00:11:28,920 --> 00:11:33,640
explains your data. And so there's a lot of parallels between these hypothesis testing and machine

114
00:11:33,640 --> 00:11:39,160
learning. So with that in mind, how should it impact the way we approach data science?

115
00:11:39,880 --> 00:11:45,240
That is an excellent question. So one of the things that is true about how machine learning is

116
00:11:45,240 --> 00:11:50,280
taught is you're always taught to cross validate, which means that you you hypothesis generate on

117
00:11:50,280 --> 00:11:57,160
a training set of data. You run all those models and then you test that data once. And that it's

118
00:11:57,160 --> 00:12:02,760
interesting is that testing testing this model once on a new set of data is a lot like making a

119
00:12:02,760 --> 00:12:08,760
very specific prediction and then running that experiment, which is what the scientific method is

120
00:12:08,760 --> 00:12:15,960
and it's best formulation, make a very specific prediction that is unlikely to occur in less

121
00:12:15,960 --> 00:12:20,600
your hypothesis is true and then run the experiment and see if that actually happened. And so in a

122
00:12:20,600 --> 00:12:27,640
lot of ways cross validation is taught and is is meant to solve the problem of machine learning

123
00:12:27,640 --> 00:12:33,800
being essentially pehacking. And what's interesting is when you think about it that way, you realize that

124
00:12:33,800 --> 00:12:40,200
a person, a data scientist who is well-meaning and trying to make a model really, really work

125
00:12:40,200 --> 00:12:43,960
could essentially start pehacking their data by repeating this training and testing,

126
00:12:43,960 --> 00:12:48,760
training and testing, training and testing over and over and over again. Sometimes people call this

127
00:12:48,760 --> 00:12:54,200
overfitting on your test set, but it's also just another formulation of pehacking.

128
00:12:54,920 --> 00:13:01,880
Right. Right. Interesting. I guess what's kind of thrown me for about that description is that

129
00:13:01,880 --> 00:13:06,600
it's also, I think, called data science, right? That's what people are fundamentally doing is

130
00:13:07,480 --> 00:13:14,920
trying to manipulate models and model parameters through an iterative loop to try to come up with

131
00:13:14,920 --> 00:13:21,640
something that works. Yeah. So what's really fascinating is so I became very frustrated with this

132
00:13:21,640 --> 00:13:27,000
when I started researching the reproducibility crisis was the thing that I got me interested,

133
00:13:27,000 --> 00:13:32,120
but it led me into this sort of academic deep dive into, wait, why? But why? But why? And I ended up

134
00:13:32,120 --> 00:13:38,360
eventually studying the philosophy of data and why is it that we believe that we can learn from data

135
00:13:38,360 --> 00:13:46,680
at all? And what I learned is that there's a couple important assumptions that you're making implicitly

136
00:13:46,680 --> 00:13:52,840
when you think that you can learn things from data. One of the most important ones is that if you

137
00:13:52,840 --> 00:13:59,800
think about it, 100% of the data that we have ever collected is about the past. Right. We don't have

138
00:13:59,800 --> 00:14:04,040
data about the future. Right. But most of the time, we're trying to learn something about the future.

139
00:14:04,040 --> 00:14:08,040
Right. We're trying to make predictions about the future. You're assuming some kind of predictive

140
00:14:08,040 --> 00:14:14,280
value or dependence. Exactly. You're making the assumption that something about the past and the

141
00:14:14,280 --> 00:14:20,280
future is shared or that there's a shared set of rules in the past and the future. That means that

142
00:14:20,280 --> 00:14:26,280
you can learn something about the past and use it to predict the future. And one of the things

143
00:14:26,280 --> 00:14:31,640
that's interesting is that if you think what is the counterfactual to that? What is the opposite

144
00:14:31,640 --> 00:14:37,480
of having rules? Potentially, things just happen due to chance. Right. Perhaps things that happened

145
00:14:37,480 --> 00:14:43,240
in the past just happened and there wasn't a rule. So when you choose to use data to try to make

146
00:14:43,240 --> 00:14:48,440
predictions about the future, you're implicitly saying, I believe this system that I am studying

147
00:14:48,440 --> 00:14:53,480
is determined by a set of rules. And I'm just trying to figure out what those rules are.

148
00:14:54,120 --> 00:14:58,440
And what's really cool about, or what's really interesting is if you think about the ways and

149
00:14:58,440 --> 00:15:03,640
machines, the ways and the problems in which machine learning has been the most successful,

150
00:15:04,280 --> 00:15:10,200
they're all in the types of problems that are extremely rule based. Or more often,

151
00:15:10,200 --> 00:15:17,480
we're humans determined the rules. For example, the first example is chess. Right. The very first

152
00:15:17,480 --> 00:15:24,040
example of automated intelligence. Yes, is a entirely rule based game that they're playing.

153
00:15:24,600 --> 00:15:29,000
And one of the great parts about it is that about making this attractable problem and making

154
00:15:29,000 --> 00:15:33,400
p-hacking not a problem is that you know the game you're playing, you know the rules. And if you

155
00:15:33,400 --> 00:15:36,440
someone tried to tell you, you know, oh, I'm going to move my pawn and I'm going to move it,

156
00:15:36,440 --> 00:15:41,240
you know, seven spaces diagonally to the right. You'd say, oh, no, that just breaks the rules.

157
00:15:41,240 --> 00:15:45,640
You can't do that. And because it's a rule based intractable like that is actually

158
00:15:45,640 --> 00:15:50,680
the machine learning ends up being an automated intelligence, ends up being a much more powerful

159
00:15:50,680 --> 00:15:55,320
tool in this type of problem than one in which the rules are not known to exist.

160
00:15:56,120 --> 00:16:01,560
So what's an example of a system where the rules aren't known or known to exist and

161
00:16:02,840 --> 00:16:08,200
you know, machine learning doesn't work as well? That's an excellent question. So I have a couple,

162
00:16:08,200 --> 00:16:13,800
but one of my favorites to talk about is actually talking about moving a problem along a spectrum

163
00:16:13,800 --> 00:16:19,880
between being very rule based and not rule based. It's not quite machine learning but it certainly

164
00:16:19,880 --> 00:16:28,680
is predictive and that is predicting elections. Okay. Elections in the US are rule based or

165
00:16:28,680 --> 00:16:34,200
they have been historically, which means, you know, one person, one vote. The definition of state

166
00:16:34,200 --> 00:16:40,680
boundaries are determined by a governing body and the amount of electoral votes or number of

167
00:16:40,680 --> 00:16:46,840
votes that these states get is determined in advance as a rule. And at the end of the day,

168
00:16:46,840 --> 00:16:53,080
what you end up predicting or measuring and predicting is just a few parameters. Basically,

169
00:16:53,080 --> 00:16:59,800
who will vote and who they'll vote for? And that makes it a very tractable problem for

170
00:16:59,800 --> 00:17:04,840
predictive type analytics, whether it's machine learning or, you know, your Bayesian approaches

171
00:17:04,840 --> 00:17:09,960
or however you want to go about it. An interesting thought experiment is to say, well, what happened?

172
00:17:09,960 --> 00:17:14,760
What would happen? How would we predict elections if those rules didn't apply? For example,

173
00:17:14,760 --> 00:17:19,000
if you didn't know how many votes each person got and you had to infer that as well,

174
00:17:19,640 --> 00:17:23,880
or if you didn't know what the state boundaries were and you had to use data to try to guess

175
00:17:23,880 --> 00:17:27,720
based on previous elections that you've seen as outcomes, what the state boundaries were,

176
00:17:28,440 --> 00:17:33,960
how much harder would this problem be? Or worse, if you're not in a functioning democracy where

177
00:17:33,960 --> 00:17:39,560
rules matter and the rules and votes can just be deleted and removed or added or stuffed arbitrarily

178
00:17:39,560 --> 00:17:45,400
randomly, then what use is your predictive technology? What use is data? I'm figuring out what

179
00:17:45,400 --> 00:17:52,440
the next outcome would be. And so what does it tell you to explore, you know, these alternate scenarios

180
00:17:52,440 --> 00:18:00,760
where the rules don't apply? So my favorite use case, like practical use case of this philosophy

181
00:18:00,760 --> 00:18:06,120
and this understanding is to think about what framing the problem means. So you'll often hear

182
00:18:06,120 --> 00:18:11,400
people say, oh, it's all about how you frame the problem. Often if you ask them, well, what does

183
00:18:11,400 --> 00:18:15,560
that mean? How do you frame the problem? It's hard to articulate what that is. For me,

184
00:18:16,280 --> 00:18:22,680
what understanding about this problem of like needing a rule-based system in order for learning

185
00:18:22,680 --> 00:18:29,480
from data to be a practical solution? You have to think about when you're framing the problem,

186
00:18:29,480 --> 00:18:35,960
you're trying to frame it so that the rules apply. So if you have a choice, if you want to choose

187
00:18:37,160 --> 00:18:41,480
potentially multiple different ways of framing your problem or setting up your model or your

188
00:18:41,480 --> 00:18:47,000
features, you're trying to pick features that are representative of true rules about how the domain

189
00:18:47,000 --> 00:18:54,280
actually works. It's basically an argument for domain expertise. It's not at its surface,

190
00:18:54,280 --> 00:18:58,600
it's basically like understand the system that you're using because that's what you need in order

191
00:18:58,600 --> 00:19:03,480
to make good models. I don't think that that's particularly controversial, but it does suggest

192
00:19:03,480 --> 00:19:09,320
that you can't just add a bunch of data into a model and then press go and get the meaning of

193
00:19:09,320 --> 00:19:14,280
the universe out on the other side. Is there an example from your experience at Turbium where

194
00:19:15,160 --> 00:19:22,600
a given use case, maybe there was an initial temptation to approach it without thinking about

195
00:19:22,600 --> 00:19:29,320
the underlying rules and kind of applying this philosophy and thinking about those rules,

196
00:19:29,320 --> 00:19:34,120
it changed your approach to modeling and your outcome? Yeah, that's an excellent question.

197
00:19:35,720 --> 00:19:40,520
So it's Turbium broadly works in the field of cybersecurity, so speaking a little more broadly

198
00:19:40,520 --> 00:19:48,120
than Turbium specifically, this is a problem that people in cybersecurity come up against all the time,

199
00:19:48,120 --> 00:19:54,600
which is that hackers are not rule-based entities. When they try to get into a system, they're trying

200
00:19:54,600 --> 00:20:01,400
to break the rules. So sometimes you have a choice. You have a choice of trying to learn the rules

201
00:20:03,000 --> 00:20:07,480
or you have a choice to try to set the rules in your system. And so for example,

202
00:20:08,200 --> 00:20:13,320
you could choose to try to learn how a network or a given company's network works and then

203
00:20:13,320 --> 00:20:21,880
try to detect anomalies in order to use machine learning to detect anomalies. And that approach

204
00:20:21,880 --> 00:20:26,600
will work, but it's hard. It's hard because the way the network interacts is changing all the time,

205
00:20:26,600 --> 00:20:31,560
the rules are changing, which computer is talking to, which computer are changing. But another way

206
00:20:31,560 --> 00:20:37,480
to approach the same problem is to set really good policies on your network, essentially permissions

207
00:20:37,480 --> 00:20:44,040
policies, and make those rules extremely formalized and designed by humans. Like this computer can

208
00:20:44,040 --> 00:20:49,240
only talk to this one. Like this is the rule. And then when you start observing anomalies, when you

209
00:20:49,240 --> 00:20:55,160
have a network that is really well designed, in which you have all the minimal access to any

210
00:20:55,160 --> 00:21:01,800
particular resource, then data suddenly starts working a lot better at detecting bad actors on your

211
00:21:01,800 --> 00:21:07,960
network for example. So you have a choice of I can just learn the rules as they exist or I can

212
00:21:07,960 --> 00:21:13,320
create the rules and then data suddenly becomes much more powerful as a tool if you create the rules.

213
00:21:13,960 --> 00:21:19,000
Right. It sounds to me like a combination between the previous point you're making about the

214
00:21:19,000 --> 00:21:27,800
importance of domain knowledge, but also just the utility of constraints and making a problem

215
00:21:27,800 --> 00:21:34,760
manageable, which I also don't think would be necessarily controversial as a point. But it's

216
00:21:34,760 --> 00:21:41,400
interesting to kind of layer it on to this way of thinking about data. You mentioned that you

217
00:21:41,400 --> 00:21:48,360
started researching the philosophy of data. Where did you find research about this or who's out

218
00:21:48,360 --> 00:21:55,480
there philosophizing about data? There's a lot. It's actually fairly interesting. I'll tell you

219
00:21:55,480 --> 00:22:04,680
because some of my favorites. So David Hume wrote about a long time ago about the law of induction

220
00:22:04,680 --> 00:22:09,800
and a lot of these a lot of these are philosophers. And so instead of talking in the concept of

221
00:22:09,800 --> 00:22:14,360
data, they're talking about thought experiments. That's their usual unit of work. And he posed a

222
00:22:14,360 --> 00:22:21,080
famous thought experiment that asked how many times do you have to observe the sun rise to be

223
00:22:21,080 --> 00:22:27,080
certain that it will rise again tomorrow? And this is a really compelling interesting question

224
00:22:27,080 --> 00:22:31,480
for me because the question is essentially how much data can I collect before I'm certain?

225
00:22:33,160 --> 00:22:37,640
And if you think about that question, right, it gives you this deep sense of like, wait, what?

226
00:22:37,640 --> 00:22:43,240
Like does this actually work? So that's one that's one writer I really like. Another one is Karl

227
00:22:43,240 --> 00:22:50,040
Popper. Karl Popper is famous for a lot of things, but one of the things that he did was come up with

228
00:22:50,040 --> 00:22:58,280
the concept of falsifiability. So in science and in data driven endeavors in general, you never

229
00:22:58,280 --> 00:23:03,800
prove anything true. You just demonstrate that things your best idea, your best model is not yet

230
00:23:03,800 --> 00:23:09,880
false. And I was taught this in high school. And I thought it had existed since like, you know,

231
00:23:09,880 --> 00:23:13,560
something like the beginning of time, you know, I thought that this was like, you know,

232
00:23:13,560 --> 00:23:17,560
a happen had come up, you have thousands of years ago as like the nature of knowledge, but it turns

233
00:23:17,560 --> 00:23:22,840
out that this concept of falsifiability that's so central is actually from like the 1960s.

234
00:23:22,840 --> 00:23:29,320
Oh wow. It's much newer than you would think. Then you would think, yeah. And that just gave me

235
00:23:29,320 --> 00:23:33,000
this feeling that like so many of the techniques that we've been using and the way we've been

236
00:23:33,000 --> 00:23:38,040
thinking about data and this entire endeavor to learn about our world using like the systematic

237
00:23:38,840 --> 00:23:44,840
collection of data is like very new in the world of like philosophy and how you how you learn from

238
00:23:44,840 --> 00:23:51,880
data. So it was very fascinating. So those are two, those are two of my favorites, but I'm always

239
00:23:51,880 --> 00:23:59,720
looking for more. Yeah, it's interesting. I mean, humor, obviously, I associate with philosophy,

240
00:23:59,720 --> 00:24:07,560
but never would have connected that question to one of data explicitly, although it's obvious

241
00:24:07,560 --> 00:24:13,080
now that you say it. If anyone else who's listening to this knows of some others, definitely send

242
00:24:13,080 --> 00:24:24,920
them my way. It's super interesting. So reproducibility crisis kind of led you to thinking more broadly

243
00:24:24,920 --> 00:24:33,800
about data and some of the, maybe some of the implicit and explicit assumptions we make about data.

244
00:24:35,560 --> 00:24:41,160
One of the thoughts that I had when you were describing this, you know, the inherent

245
00:24:41,160 --> 00:24:50,200
belief in, in, you know, that there are rules that govern our data is kind of brings me back

246
00:24:50,200 --> 00:24:55,000
to the whole Bayesian versus frequentist thing like the implication is almost that, you know,

247
00:24:55,000 --> 00:25:03,080
there's no such thing as a non Bayesian. Well, yes, the way I like to think of it is that Bayesian,

248
00:25:03,080 --> 00:25:07,640
I think Bayesian statistics as an approach is much stronger than frequentist statistics,

249
00:25:07,640 --> 00:25:14,040
but it doesn't actually solve the problem because in order to make a prior, in order to frame it,

250
00:25:14,040 --> 00:25:18,920
you're still making a lot of assumptions about what matters in the problem. So what's fun to do

251
00:25:18,920 --> 00:25:24,440
as a thought experiment? I think one again, Bayesian is just a much more powerful technique,

252
00:25:24,440 --> 00:25:30,440
but it still requires you to make this like an initial guess on how, on how the world works. And

253
00:25:32,120 --> 00:25:36,040
that's again, that rule-based thing like you have to assume that there are rules. So it's,

254
00:25:36,040 --> 00:25:44,040
it's interesting. Yeah. And so did you, you know, to kind of come back to the implication

255
00:25:44,040 --> 00:25:51,080
for data scientists about this reproducibility crisis? Like, do you have, and when you talk about

256
00:25:51,080 --> 00:25:59,320
this, is there, is there like a prescriptive list at the end that you, you know, do A, B, and C,

257
00:25:59,320 --> 00:26:05,960
and you'll be clear? You'll avoid the fate of these poor, you know, these poor, air-room-producible

258
00:26:05,960 --> 00:26:15,560
scientists? Or is it not that clean? Well, so what I, what I say is data is not magic. And you

259
00:26:15,560 --> 00:26:20,840
just have to be okay with that. At a certain extent, you have to think about data and inference as

260
00:26:20,840 --> 00:26:27,400
a sum of the tools in your toolkit that you can use to approach a problem, but not necessarily

261
00:26:27,400 --> 00:26:36,200
always the answer. The second thing that I say is, this is for, for people who are in the position

262
00:26:36,200 --> 00:26:42,440
of making strategic investment in data products. So for example, V, C, these are even managers,

263
00:26:42,440 --> 00:26:49,240
and you're trying to understand which of these solutions that someone has put in front of me

264
00:26:49,240 --> 00:26:55,400
is most likely to actually generalize and actually work when put out into, you know, either as a

265
00:26:55,400 --> 00:27:00,360
company or as a new model for your software or your platform or whatever it is. And at that

266
00:27:00,360 --> 00:27:06,200
moment, when you have to make a choice, a really important question to ask is how did you make this

267
00:27:06,200 --> 00:27:12,840
model? Like what was it that made it work? And a warning flag for me being in this position all

268
00:27:12,840 --> 00:27:19,640
the time, like having to make a decision about where to invest our company's resources is to hear

269
00:27:19,640 --> 00:27:26,280
a data scientist say, I just ran 17,000 different algorithms in different tasks. And I did a huge

270
00:27:26,280 --> 00:27:31,240
grid service across like all of the perimeter space. And I found the one that performed the best.

271
00:27:31,880 --> 00:27:37,720
Because to me, that sounds like p-hacking. And it has been proven out in my experience that

272
00:27:37,720 --> 00:27:44,520
that's a riskier model to choose to, to put into production. Then alternatively, if someone comes

273
00:27:44,520 --> 00:27:49,000
to you and they say, you know, how did you make this model happen? What was the thing that mattered?

274
00:27:49,000 --> 00:27:53,960
And they come and they say, hey, like I thought really hard about the problem. And when I realized,

275
00:27:53,960 --> 00:27:58,520
I realized that if you look at the problem differently, if you look at it this way as opposed to this

276
00:27:58,520 --> 00:28:05,400
way, then everything made sense. Then my model that didn't work suddenly started working. And to me,

277
00:28:05,400 --> 00:28:11,240
that sort of I reframed the problem. And then things I didn't have to work as hard to find the

278
00:28:11,240 --> 00:28:17,000
right answer. I didn't have to p-hack. I didn't have to really push, you know, beat my data in order

279
00:28:17,000 --> 00:28:22,440
to get an answer. That is a sign I think has of a model that's much more likely to work

280
00:28:23,000 --> 00:28:26,680
when pushed into production. Or when you invest in a model.

281
00:28:27,560 --> 00:28:35,800
Is there an implication there that you're not a fan of deep learning? A lot of the way I think

282
00:28:37,560 --> 00:28:43,800
deep learning, the deep learning camp thinks about the world is like throw a huge amount of data

283
00:28:43,800 --> 00:28:49,880
at this neural network and, you know, let the network figure it out. And we shouldn't have to

284
00:28:51,480 --> 00:28:56,840
you know, in kind of the pure sense, like we shouldn't have to bring a lot of our priori knowledge

285
00:28:56,840 --> 00:29:01,960
into the way that we, you know, build these networks or like, you know, process the data or what

286
00:29:01,960 --> 00:29:05,480
have you. That's the role of the data and the network to figure that stuff out.

287
00:29:06,920 --> 00:29:10,600
You have, you have me pegged at what I call a deep learning skeptic.

288
00:29:10,600 --> 00:29:18,600
I would say that much like any tool, there are specific situations in which I think that could

289
00:29:18,600 --> 00:29:25,000
be a really interesting approach, but that it is not a panacea to all problems. And I don't believe,

290
00:29:25,000 --> 00:29:30,440
based on my understanding of philosophy, based on my understanding of the assumptions that go

291
00:29:30,440 --> 00:29:38,200
into the process of learning from data, that there will ever be a modeling architecture or structure

292
00:29:38,200 --> 00:29:46,520
that avoids this process or makes, that makes this this problem go away. Until like what would

293
00:29:46,520 --> 00:29:51,160
it take, it would take a new system of logic, something different than our current definition of

294
00:29:51,160 --> 00:29:59,160
inference. A solution to Hume's law of induction, so to speak, before whatever new algorithm or

295
00:29:59,160 --> 00:30:04,600
new approach comes out, I would, I would think that it was a revolutionary change to how we

296
00:30:04,600 --> 00:30:11,720
solve, solve these problems. I know it strikes me that you're also saying something about

297
00:30:12,840 --> 00:30:18,280
you know, the role of optimization, like you can, you know, your example with like, you know,

298
00:30:18,280 --> 00:30:24,600
grid searching your, your model to death, so to speak, like is this kind of philosophy? Does

299
00:30:24,600 --> 00:30:30,200
it undergarative belief that, you know, that there's some inflection point where you're 90%

300
00:30:30,200 --> 00:30:37,160
model that's based on fundamental principles, you know, is, you know, way better than your 95%

301
00:30:37,160 --> 00:30:43,960
you know, performing model that is kind of brute forced when you, you know, try to actually put

302
00:30:43,960 --> 00:30:49,160
it in a production. Yeah, so I mean, this is sort of, this is where it gets hard, right? So

303
00:30:49,800 --> 00:30:54,760
what I would say is what you learn when you study philosophy of and learning from data is that

304
00:30:54,760 --> 00:31:03,720
the pursuit of pure objectivity is a fool's errand in some way. But so why, why do you do data

305
00:31:03,720 --> 00:31:08,280
science? Well, you do it. Why do you build these models? It's because they're useful. And what I do

306
00:31:08,280 --> 00:31:16,520
think is true is that there's potentially diminishing returns in the last 90 to 95%. But that's not

307
00:31:16,520 --> 00:31:22,360
true in all cases. In some cases, you're, you know, that's, I always say that you don't make

308
00:31:22,360 --> 00:31:26,600
decisions based on probabilities. You make decisions based on expected value. If you have a

309
00:31:26,600 --> 00:31:34,040
use case where failing is so catastrophically terrible that you can't risk that extra 2%,

310
00:31:34,040 --> 00:31:38,680
then it might be worth your time and effort to try for the hopes that it actually does work. And

311
00:31:38,680 --> 00:31:44,520
it is possible, you know, to stumble upon the answer, even if it's not a rigorous process,

312
00:31:45,480 --> 00:31:50,440
it is possible to find it, right? It just means that it's not a robust way of going about it or

313
00:31:50,440 --> 00:31:57,080
a reproducible way of going about it, which is the nature of the game. So it's really about

314
00:31:57,080 --> 00:32:04,200
understanding, you know, what are you doing? Are you trying to sell like 5% more products?

315
00:32:04,200 --> 00:32:11,000
Are you trying to prevent 5% more crime? Like how bad are your potential failures? And then,

316
00:32:11,000 --> 00:32:15,800
and then deciding whether it's worth it to try to risk it to like, to do this brute force approach.

317
00:32:15,800 --> 00:32:20,920
But if you have the option, like you only do that if you don't have the option to solve it by

318
00:32:20,920 --> 00:32:26,200
understanding something fundamental or rule based about the problems, they should all, that should

319
00:32:26,200 --> 00:32:33,480
always be, in my opinion, your first step in building a model. Now we've talked about the

320
00:32:34,600 --> 00:32:40,040
reproducibility crisis in science and applying that, you know, the lessons learned there,

321
00:32:40,040 --> 00:32:48,200
two data science. You also hear from time to time, you know, talk of reproducibility issues

322
00:32:48,200 --> 00:32:54,120
in data science, meaning in the machine learning research, the difficulty going from a research

323
00:32:54,120 --> 00:33:06,280
paper to a working implementation. Any thoughts on that? Yeah. That was a very heavy yeah.

324
00:33:06,280 --> 00:33:14,840
My feeling about that is that it's probably my guess. I've seen the reports of this. I have not

325
00:33:14,840 --> 00:33:21,640
myself tried to reproduce some of these these core central papers, but my, my guess if I were to

326
00:33:21,640 --> 00:33:27,320
make a guess on what the cause of this is is sort of rooted in the same idea where if it's applied

327
00:33:27,320 --> 00:33:33,480
to the right problem, a problem that has good, a good formulation of rules, it probably works the way

328
00:33:33,480 --> 00:33:37,880
the person presented in the original paper. And then if you take the exact same use and you

329
00:33:37,880 --> 00:33:41,880
tweet that problem even slightly, like sometimes that problem can just be like where you got your data

330
00:33:41,880 --> 00:33:47,240
from. Because the rules no longer apply in quite the same way that they did because of how you

331
00:33:47,240 --> 00:33:52,520
collected it or because of your bias or some amount of data that you're missing for whatever reason.

332
00:33:53,720 --> 00:33:59,080
And that might explain some of the lack of reproducibility. In the same way,

333
00:33:59,080 --> 00:34:06,120
you know, that that the reproducibility crisis in science might be explained in some part by people

334
00:34:06,120 --> 00:34:11,000
not reproducing in exactly the same way, not using exactly the same materials, not doing things with

335
00:34:11,000 --> 00:34:15,880
exactly the same technology. That that some of that variance might be explained. And what that's

336
00:34:15,880 --> 00:34:21,480
really saying is that you were relying on rules that weren't actually core and central to your problem,

337
00:34:21,480 --> 00:34:26,360
but they were artifacts. They were, you know, extra extra things like my technology works this way.

338
00:34:26,360 --> 00:34:30,760
And so because so this worked in this case, as opposed to this other case, it didn't generalize

339
00:34:30,760 --> 00:34:35,000
in the same way as I expected it to. Yeah. Yeah. Yeah. It strikes me that there are a number of

340
00:34:35,000 --> 00:34:42,280
things going on there. I think there's the case that you mentioned where I'm trying to apply,

341
00:34:42,840 --> 00:34:50,600
you know, this paper to my use case. And all of the things that you mentioned apply, it also ties

342
00:34:50,600 --> 00:34:59,800
back to, I should say, the kind of issue that the Ali Rahimi brought up at in his nips presentation,

343
00:34:59,800 --> 00:35:07,240
you may have heard of this where he kind of decried the lack of rigor in the field or at least

344
00:35:07,240 --> 00:35:15,800
certain parts of the field, specifically, I think, talking about deep learning. And you know,

345
00:35:15,800 --> 00:35:20,360
that kind of ties to reproducibility and that, you know, people will publish papers, you know, there's

346
00:35:20,360 --> 00:35:26,440
not a set of kind of fundamental rules for how you got your architecture or your hyper parameters.

347
00:35:26,440 --> 00:35:31,000
You just have them. If you don't share them with anyone, then they can't reproduce what you did

348
00:35:31,000 --> 00:35:35,640
because they're like magic numbers that you just found in your grid search. Yeah.

349
00:35:36,600 --> 00:35:42,440
I think if we could learn anything from how poorly the scientist are handling the reproducible

350
00:35:42,440 --> 00:35:47,640
body crisis on a human level, I would suggest that when we try to solve these problems, we do our

351
00:35:47,640 --> 00:35:55,080
best to not throw stones or name call or point out individuals and talk again about the problem

352
00:35:55,080 --> 00:36:00,920
as like, hey, this is how data works. And we have to know this about the tool that we are using.

353
00:36:02,360 --> 00:36:08,520
Going into it so that it doesn't feel like a direct attack on someone who's trying to make

354
00:36:08,520 --> 00:36:14,120
things work with the tools as they understand them. I think a better solution to this problem is

355
00:36:14,120 --> 00:36:19,640
to raise the awareness of the idea that, hey, like, this doesn't actually work. We all wanted to work,

356
00:36:19,640 --> 00:36:24,280
you know, have data, magic formula, and then we know things. But it doesn't, and it's actually

357
00:36:24,280 --> 00:36:27,720
harder than that. And we all have to be okay with that when we when we approach it.

358
00:36:28,680 --> 00:36:34,920
Awesome. Any closing thoughts to kind of tie things up for us?

359
00:36:34,920 --> 00:36:39,880
No. Which is a totally valid answer.

360
00:36:42,200 --> 00:36:46,200
Yeah, I guess I guess I do have one closing thought is that another really great thought

361
00:36:46,200 --> 00:36:51,720
experiment to look into is the infinite monkeys theorem or infinite monkeys thought experiment.

362
00:36:51,720 --> 00:36:56,680
It's a really good demonstration of of what happens when you scale up inference.

363
00:36:56,680 --> 00:37:00,840
And explain that. I love this thought experiment. So it goes it goes basically like this.

364
00:37:00,840 --> 00:37:05,720
You walk into a room and there's a monkey and the monkey's on a typewriter.

365
00:37:06,280 --> 00:37:10,200
And the monkey's begging around on the typewriter and you look at what the monkey is typing

366
00:37:10,200 --> 00:37:15,400
and you see, hey, look, this monkey has typed all of Shakespeare's hamlet.

367
00:37:16,600 --> 00:37:19,800
And you think, oh my gosh, like, this is so crazy.

368
00:37:20,680 --> 00:37:25,480
Something must be up with this monkey. There has to be a rule to explain why this monkey happened.

369
00:37:25,480 --> 00:37:31,400
Clearly, this monkey came from the alien overlords to like down to earth to rewrite Shakespeare's hamlet.

370
00:37:32,600 --> 00:37:36,200
So you're very surprised by this. And then but then someone tells you, hey,

371
00:37:36,200 --> 00:37:39,160
if you had gone over to that room over on your left or the one over on your right,

372
00:37:39,160 --> 00:37:42,360
you would have seen another monkey. And in fact, there's not only one monkey, but there's actually

373
00:37:42,360 --> 00:37:46,120
millions of monkeys or billions of monkeys. And they've been typing on these typewriters

374
00:37:47,400 --> 00:37:51,160
for as long as anyone can remember. And every day we've been looking and we've been looking and

375
00:37:51,160 --> 00:37:55,880
we've been looking, we've just trying to find that monkey that had typed something that we thought

376
00:37:55,880 --> 00:38:01,400
was compelling that looked like there was evidence of a rule. And we finally found it.

377
00:38:01,400 --> 00:38:06,520
And then you think, oh, well, it's not like it's not thinking about this monkey. It's not the fact

378
00:38:06,520 --> 00:38:10,200
that this data happened or this data was collected, but how we came to find it.

379
00:38:11,880 --> 00:38:15,480
And so one of the things I like to always say is when you're trying to evaluate how

380
00:38:15,480 --> 00:38:19,960
strong your evidence is, you should ask how many monkeys you want to know how many times you had

381
00:38:19,960 --> 00:38:24,680
to look to try to find it. Nice. You're going to have to find a way to work that into the title of

382
00:38:24,680 --> 00:38:33,240
this podcast. How many months? Awesome. Well, Claire, you've definitely given us a lot to think about.

383
00:38:33,240 --> 00:38:38,360
Thank you for taking the time to chat with me. Thank you. It's been great.

384
00:38:42,760 --> 00:38:48,360
All right, everyone. That's our show for today. For more information on Claire or any of the

385
00:38:48,360 --> 00:38:53,800
topics covered in this episode, you'll find the show notes online at twomla.com.

386
00:38:54,680 --> 00:38:59,160
If you're new to the pod and like what you hear or you're a veteran listener and haven't

387
00:38:59,160 --> 00:39:05,080
already done so, head on over to your podcast app of choice and leave us your most gracious

388
00:39:05,080 --> 00:39:09,880
rating and review. It helps new listeners find us, which certainly helps us grow.

389
00:39:09,880 --> 00:39:19,880
Thanks so much for listening and catch you next time.

