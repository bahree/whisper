WEBVTT

00:00.000 --> 00:15.840
All right, everyone. I am here with Kim Branson. Kim is senior vice president and global head

00:15.840 --> 00:22.480
of the artificial intelligence and machine learning at GSK. Kim, welcome to the Twomo AI podcast.

00:22.480 --> 00:29.040
Thanks, Sam. Good to be here. Looking forward to our conversation. Let's jump right in. You come

00:29.040 --> 00:37.280
to GSK by way of Genentech, which kind of this traditional or this historical Silicon Valley

00:37.280 --> 00:44.160
challenger. Before that, you spent a lot of time in Silicon Valley and now you're at a large

00:44.160 --> 00:50.800
pharma. Tell us about that journey and how it came to be. Sure. So, I guess my background,

00:50.800 --> 00:55.440
as you know, I did my PhD in machine learning, really looking at

00:55.440 --> 01:01.120
trying machine learning to actually drug discovery. So, small molecule drug discovery and design

01:01.120 --> 01:07.040
was what I started off a bit. And then says, I'm dating myself, but like, you know, in 2003,

01:07.040 --> 01:10.400
around then, and where we're, you know, the cutting edge techniques with things like support

01:10.400 --> 01:15.280
back to machines and random forests and things like that. But doing a lot of work on doing simulation,

01:15.280 --> 01:18.880
a physics simulation, things like that, or small molecules, spying to proteins and all the things

01:18.880 --> 01:24.880
around with that. From there, I ended up doing, you know, a very sort of academic thing of spending

01:24.880 --> 01:29.200
time at Stanford and others. There's a lot of sudden earlier work on machine learning applied

01:29.200 --> 01:33.680
there into graph convolution networks. That sort of stuff was happening. And a lot of my time

01:33.680 --> 01:37.760
was spent actually in, you know, I spent some time at Vertix Pharmaceuticals, but then a favorite

01:37.760 --> 01:43.120
time, so I've been start up and so things like, you know, was involved in early search startup,

01:43.120 --> 01:47.360
there was acquired by Twitter, another company doing large scale medical records aggregation

01:47.360 --> 01:51.440
and differential privacy machine learning on that sort of stuff. And although, like all things,

01:51.440 --> 01:55.840
90% of your time is building all the stuff to do ETLs, extract tables out of PDFs and all those

01:55.840 --> 02:00.800
wonderful things. And that was acquired by Apple. And then a lot of large scale machine learning

02:00.800 --> 02:04.480
on claims, other types of data, so all about like predicting like what's the probability of

02:04.480 --> 02:08.880
predicting disease X at time T given given someone's past medical history. So you could intervene

02:08.880 --> 02:13.680
early and things like that. And from there, you know, many friends at Genentech, I've always been

02:13.680 --> 02:20.640
involved in computational chemistry. So joined Genentech and really was, I guess, really recruited

02:20.640 --> 02:26.080
from Genentech to come to GSK, which, you know, it's one of the things that wasn't really on my radar

02:26.080 --> 02:30.160
as a place to be. And I had some friends there on, you know, I'd known of the company for a while,

02:30.160 --> 02:33.520
but they sort of committed me to come in and I realized that actually it's something very different

02:33.520 --> 02:40.080
happening here. So they put it in a new head of R&D, so how Baron had joined and, you know, how it

02:40.080 --> 02:44.000
was worked at Calico, with definitely Colin people like that. And I was sort of like, well,

02:44.000 --> 02:47.440
there's a guy who really, you know, actually understands what machine learning looks like and what

02:47.440 --> 02:52.480
it requires to be done properly almost. And sort of, you know, really explained to me that like,

02:52.480 --> 02:57.040
it's going to be such a core part of GSK, right, that like they're really serious about it. It's not

02:57.040 --> 03:01.600
just a company like I want to build a 10 person team or double and kind of half resource you,

03:01.600 --> 03:05.680
but it was going to be such a fundamental thing. So that's what sort of led me here and has

03:05.680 --> 03:10.640
led me to create the create the group and then scale as we do now. And give us some context for

03:10.640 --> 03:17.200
GSK. What's the core business and where does ML and AI fit in? Yeah, sure. So we're obviously a

03:17.200 --> 03:22.080
you know, pharmaceutical company, you know, we make, you know, medicines and vaccines across a

03:22.080 --> 03:27.600
bright range of things. And so GSK is an old company, right? It's been about 300 years, right? So

03:28.240 --> 03:31.440
all these companies, every now and then, they go through these sort of revolutions and they,

03:31.440 --> 03:37.040
you know, they sort of turn themselves inside out and reposition themselves. And, you know,

03:37.040 --> 03:43.040
what was really apparent is that we have this increasing body of sort of genetic data. So these

03:43.040 --> 03:48.560
are these large genetic databases. So where we have lots of people, we cost the sequencing has gone

03:48.560 --> 03:53.520
down. So, you know, members, I guess 20 years, we have the human genome, one, one, one, one genome,

03:53.520 --> 03:58.240
now we can do lots, lots of people, right? So you can see sequence lots of people you know about

03:58.240 --> 04:02.720
their medical histories. And you have, you can basically do it, answer a question like, you know,

04:02.720 --> 04:05.680
he's a hero bunch of people that got a disease, hero bunch of people like Brooklyn that don't get

04:05.680 --> 04:10.000
the disease. And ask a question, well, what's different in the genetics, right? The idea is that

04:10.000 --> 04:14.480
the genetics kind of points at a clue as to what you might want to do medicine for, how, what's

04:14.480 --> 04:19.920
involved in that disease. So a really increasing amount of just data being generated in the genetics

04:19.920 --> 04:26.160
side of things. And the other side of things is happening is really these technology for

04:26.160 --> 04:30.480
function genomics, right? So maybe the first wave you can think of as molecular biology, right?

04:30.480 --> 04:34.160
Restriction enzymes, we have passwords, we can like, you know, do genetic engineering and

04:34.160 --> 04:39.200
make a cell make a protein. The next, this is really the continuation of the evolution of those

04:39.200 --> 04:45.680
those tools is now with CRISPR and talons and those sorts of things where you can actually perturb

04:45.680 --> 04:50.480
a specific gene, right? We can turn it up, we'll turn it down in a particular cell type almost

04:50.480 --> 04:54.320
at an almost sort of single cell level. So you now you've got this other set of technologies where

04:54.320 --> 05:00.320
you can start to like generate huge amounts of data at scale. And what it turns out with is that

05:00.320 --> 05:04.960
we're just biology can measure so much more now. There's just this massive amount of measurement

05:04.960 --> 05:09.440
and it's multimodal. So it's, you know, looking at RNA-C kind of single cell levels that's looking

05:09.440 --> 05:14.880
at messenger RNA made as you're doing these edits. You can do cellular imaging, you can do proteomics,

05:14.880 --> 05:20.400
you've got all, you know, all the omics as we call them, you're always explosion of data. And so

05:20.400 --> 05:24.160
really you need machine learning in the middle of it to sort of make sense of the data,

05:24.160 --> 05:27.600
but also even to help you kind of make sense of all the literature you've gotten into plan

05:27.600 --> 05:32.000
the sort of next experiments. And that's in the discovery phase. So it's it's really core,

05:32.000 --> 05:36.800
we have this, this three-pronged strategy that's sort of this, the, you know, genetics, right?

05:36.800 --> 05:40.240
And function dynamics on the side and the AIML in the middle to sort of integrate together

05:40.960 --> 05:44.480
to help us really, you know, find the time that's in design better medicines.

05:44.480 --> 05:50.640
And so is it, is it accurate to think of genetics as the kind of the data source there,

05:50.640 --> 05:56.880
what's happening in the gene genomics as a control point, the way you influence what's happening,

05:56.880 --> 06:03.040
and then AI is telling you what influence to exert based on what you, you know, the patterns

06:03.040 --> 06:09.600
you see. Yeah, so I think the way to think about is, genetics gives us, so this genetic databases

06:09.600 --> 06:15.840
give us a clue of what to start looking at, right? And we know it's really important because we've

06:15.840 --> 06:20.000
done studies, and others have done this as well, we're showing that if you've got a medicine that

06:20.000 --> 06:25.040
works on a target, so remember genes, we call it messenger RNA, messenger RNA,

06:25.040 --> 06:31.280
from proteins, proteins is the thing that does the things in ourselves. And the proteins are,

06:31.280 --> 06:34.480
what we would call it, it's a type of the thing we want to modulate, typically, and that might

06:34.480 --> 06:38.880
would be with a small molecule, things like clinical, things like penicillin or aspirin,

06:38.880 --> 06:43.280
or it could be something like an antibody, right? Which usually extracellular, not usually inside

06:43.280 --> 06:48.480
cells, then it's sort of blocks, blocks something. And so the genetics is giving us the hint of

06:48.480 --> 06:53.120
what to look for, but it doesn't, it's not the whole story, you still need to actually kind of,

06:53.120 --> 06:56.640
it gives you a clue, but you need to sort of go and do further experiments and understand like,

06:56.640 --> 07:01.520
well, you know, is this the correct target, or is it something else that's in this pathway that's

07:01.520 --> 07:05.680
operating in? That's the function genomics coming in, where the function genomics allows us to

07:05.680 --> 07:11.520
basically, what happens when we turn this particular gene off, like lower the level of that protein,

07:11.520 --> 07:15.600
does that look like it has the right effect, right? And it's sort of mimicking the effect of

07:16.640 --> 07:20.640
making a medicines, we don't have to make a medicine to work out whether it works, we can use

07:20.640 --> 07:25.200
that kind of thing to inform it. So it sort of starts to put these things together and

07:26.000 --> 07:30.480
all these experiments now, because the cost of measurement has gone down, because we can take a

07:30.480 --> 07:36.480
single cell and make a change in a single cell, right? And then do RNA sequencing, like look at it,

07:36.480 --> 07:40.800
all the changes in messenger RNA on those single cell levels, it generates a whole bunch of data

07:40.800 --> 07:45.680
that's at a large scale. And that's where we sort of bring machine learning in,

07:45.680 --> 07:53.360
maybe I can illustrate the point, probably the one of the most, one of the key problem we work on

07:53.360 --> 07:59.360
is that in these large genetic databases, we really only want to know what to do with about

07:59.360 --> 08:07.920
probably 15 to 20% of what we call the variance, and a variance is where, you know, your DNA sequence

08:07.920 --> 08:11.760
will differ from mine, right? Maybe I get the disease and you don't, and I've got a particular

08:11.760 --> 08:17.760
mutation, right? If that mutation falls into what we call the open reading frame of a gene, or the

08:17.760 --> 08:21.840
bit that encodes for the protein, that gets translated, mRNA encodes the protein, we know what

08:21.840 --> 08:24.960
affects the protein, and we can go and look at that protein and work out what's happening, you know,

08:24.960 --> 08:29.520
is it not fold, is it fold, but it's missing, it's not as active, that kind of thing.

08:30.400 --> 08:35.200
A whole bunch of them fall outside of that, right? They forms of the regulatory regions of DNA,

08:35.200 --> 08:39.120
and you can imagine there's a whole bunch of control structures in DNA saying what's

08:39.120 --> 08:44.400
the turn on what proteins and what conditions. And so we spend a lot of time in building machine

08:44.400 --> 08:51.040
learning models, really to understand what genes those variants are regulating, right? The ones

08:51.040 --> 08:56.000
that are in the control area, right? It's not always like the closest gene, it can they can have

08:56.000 --> 09:00.640
quite long range effects, it might be different proteins, right? And so one thing that if we can

09:00.640 --> 09:05.920
sort of understand what things are regulating, it gives us a whole lot of sort of potential

09:05.920 --> 09:10.080
targets to go and look for. So that's the sort of thing we use machine learning on, right? So we

09:10.080 --> 09:14.800
use it on discovering kind of, you know, things to make medicines against, so better targets,

09:14.800 --> 09:19.120
and the genetically valid, so we know they're more like to work, and then actually maybe things

09:19.120 --> 09:25.600
on the closer to the clinical side of the world, right? So I've got a medicine, how best to use

09:25.600 --> 09:29.520
a who's going to respond, respond, how would we measure the response and things like that?

09:29.520 --> 09:34.000
And that's things like computational pathology or other types of things in the clinical domain.

09:34.000 --> 09:40.080
One of the things that I heard in your explanation, this is maybe a little bit of a tangent, was

09:41.200 --> 09:48.720
suggesting that, you know, I think of, you know, variants as, you know, one of the genes gets,

09:48.720 --> 09:52.720
you know, flipped from a A to a G or something like that, right? So you've got these four,

09:52.720 --> 09:58.560
and one of them gets flipped. But I heard in your description that, you know, that's a oversimplification,

09:58.560 --> 10:03.040
and maybe it's not just, you know, which gene is encoded. Am I hearing that correctly?

10:03.040 --> 10:08.800
Yeah, so, you know, our cells are sort of a, you know, they're a network. We have lots of

10:08.800 --> 10:12.000
different proteins to be made, and they, you know, they communicate to other proteins and

10:12.000 --> 10:19.200
form up various functions. And sometimes it's literally the amount of the protein, right,

10:19.200 --> 10:24.800
is important, right? And so, you know, there are classic diseases, and you can think of these,

10:24.800 --> 10:29.360
what we call a rare genetic disorder, where you've got a single mutation, right? And it makes

10:29.360 --> 10:33.280
the protein functionalist, you know, a canonical example might be, I mean, you think of hemoglobin,

10:33.280 --> 10:38.240
the sickle cell, or you can think of factor 10A deficiencies, you don't make it effective factor

10:38.240 --> 10:43.600
10A, you know, you need to put that protein back. Some things are about like the level of the

10:43.600 --> 10:49.040
proteins it is, it can influence the, the behavior. Some things are about, some of these mutations

10:49.040 --> 10:52.320
means that maybe the protein doesn't get made at all. Something that means the protein gets made,

10:52.320 --> 10:56.720
but it doesn't, it's not as stable, so it gets turned over rapidly, so you just don't have as much.

10:56.720 --> 11:01.520
And sometimes you might have a mutation that makes the protein always on, so it's always

11:01.520 --> 11:04.720
conforming its fate, so it's not regulated anymore, right? That's another thing, and because it's

11:04.720 --> 11:10.000
not regulated, it's driving the behavior other pathways, and it's leads to, you know, abhorrent

11:10.000 --> 11:14.880
function, which leads to pathology and disease, right? And so, it's not just as simple as like,

11:14.880 --> 11:19.600
I've got a mutant, right, you know, what does the mutant, this mutant tells me I need to make a

11:19.600 --> 11:22.960
drive against that, it's actually what's the effect of the mutant? And this is the kind of the

11:22.960 --> 11:27.280
thing before I was talking about that kind of the variant gene problem, as we phrase it.

11:27.280 --> 11:31.840
Okay. Another missing piece is that kind of the gene-to-function problem, right? And that's the

11:31.840 --> 11:38.720
other, that's another key thing. So you've, you've got these data sources that come from genetics

11:38.720 --> 11:47.120
and genomics, and you're applying AI, ultimately trying to develop new drugs, new interventions.

11:47.120 --> 11:53.360
Can you, you talk about some of the specific use cases or problems?

11:53.360 --> 11:58.160
Yeah, so I think that, you know, there's a lot of things, first of all, thinking about,

11:59.920 --> 12:03.440
obviously, you know, taking your clues from genomics and things to come up with those targets,

12:03.440 --> 12:07.600
right? And then you have to think about, well, what's the effect of what we're just talking about

12:07.600 --> 12:11.120
like that mutation on that target, right? So there might be, is it more stable or not?

12:11.120 --> 12:17.280
And then it might be, well, I think about, how do I, you know, what a thing is to take the,

12:19.120 --> 12:23.920
the target and that sort of silly context and know that you've kind of made something better or not,

12:23.920 --> 12:27.680
right? Like, how do I know that I've actually found a good target that's moving, you know,

12:27.680 --> 12:32.240
it's going to, it's going to become a good medicine in people. And that's where actually, so we do,

12:33.440 --> 12:37.040
all of the things we build, all the models we build actually have sort of a large sort of

12:37.040 --> 12:43.520
experimental feedback loop, right? And it's actually, we, for example, that variant to gene model

12:44.160 --> 12:47.120
that has sort of an experimental feedback loop where we're actually doing what we call

12:47.120 --> 12:50.720
experiments as code. So we're asking the model what it needs and it's really sort of adaptive

12:50.720 --> 12:55.120
sampling under uncertainty constraints. So, you know, rather than having data being generated by

12:55.120 --> 12:59.600
some other process at GSK and I'm trying to build a model of that where I might have, you know,

12:59.600 --> 13:03.680
900 examples a week or something I'm very good at, right? And I'd be like, I really like more

13:03.680 --> 13:07.840
examples of things I'm not good at. We actually sort of use a lot of sort of automated, like,

13:07.840 --> 13:12.240
like biology. So this is biology. I'm with robot robotic automation to generate data and things

13:12.240 --> 13:16.960
that sort of feed back into these models. And it's the model that becomes the tool that helps us

13:16.960 --> 13:24.080
solve the problem, right? So we can use the, we can use that, um, that model we've built to help

13:24.080 --> 13:28.560
us map more of those variants of those genes. But then we still need to understand that gene

13:28.560 --> 13:34.560
to function for. And again, there we use automated experimentation. But in this case, we're doing

13:35.120 --> 13:38.480
things with these various cellular models. So they can be what we call induced

13:38.480 --> 13:43.520
peripheral cells, right? These IPSC cells. So these like stem-like cells from patients that have

13:43.520 --> 13:48.160
a disease, patients that don't have a disease. And then we actually sort of want to work out,

13:48.800 --> 13:54.480
you know, how, what we want to basically take the disease tissue and make it look more like the

13:54.480 --> 13:58.480
normal tissue. And when we say look like it could be measured by a bunch of different ways,

13:58.480 --> 14:02.080
it could be measured by looking at imaging data, it could be measured looking at gene expression

14:02.080 --> 14:08.000
pattern or protein or some kind of functional consequence. And typically these assays are

14:08.000 --> 14:11.920
complicated. You can't do, you know, with CRISPR and things like that. Sometimes you can just do

14:11.920 --> 14:15.520
what we call a genome-wide screen. I'll just do all the things, right? All 20,000 genes.

14:16.240 --> 14:20.880
These ones are so complex that you can't do that. And you sort of need to do an adaptive

14:20.880 --> 14:25.440
experimentation thing. So you can take your clues, you start with from genetics and from the literature,

14:25.440 --> 14:30.800
for example. And you sort of see the model for that. And then the model sort of makes an experiment,

14:31.440 --> 14:34.080
right? And we do everything as a sequential learning product kind of problem. We make,

14:34.080 --> 14:38.960
we do an experiment. We perturb a few genes. We look at how it moves things. We feed that data back

14:38.960 --> 14:43.360
here and we say, okay, what have we learned? Based on that, my next best experiment is to do,

14:43.360 --> 14:47.520
is to do this other thing. So then I make another mutation. I do another sort of round of like

14:47.520 --> 14:51.760
interventions on that cellar system and then sort of iterate and anything about it's kind of like

14:51.760 --> 14:56.480
an optimization problem. I'm trying to find the best target to move it to that, right? And so

14:57.840 --> 15:01.760
and you want to get there with the fewest number of steps, right? And try and find the best things.

15:02.400 --> 15:06.160
And at the same time, why you're trying to make it look like, you know, it's

15:07.680 --> 15:11.760
affecting the disease. So you have to, like your ground truth is at cellular models. You also do

15:11.760 --> 15:17.120
other things like making sure that like, you know, there are certain proteins in the body that you

15:17.120 --> 15:22.480
can't touch, right? That like have toxicity associated with it, right? So a classic one is in,

15:22.480 --> 15:26.320
you know, cardio toxicity, right? Like it's no good making something into your rA drug if it's

15:26.320 --> 15:30.240
going to give you cardio toxic, right? So the certain things that we know we can't hit this,

15:30.240 --> 15:34.720
there's toxicity from what we call on target toxic. If I hit this protein, but something bad happens

15:34.720 --> 15:39.040
and then there are other sorts of toxicology, we can also at the same time have an AI

15:39.040 --> 15:43.280
system that's learning to learn, which targets a toxic, which targets not to touch based on

15:43.280 --> 15:47.760
prior data and things like that, which things are, which targets are easier to make a small molecule

15:47.760 --> 15:51.600
not. It is a multi objective optimization because I can come up with targets that are really great,

15:52.320 --> 15:56.800
right? And we sit around, well, we know no idea how to make a selective medicine against that,

15:56.800 --> 16:02.320
right? So an example of one of those things is a protein involved in cancer called K-RAS.

16:02.880 --> 16:06.400
You know, it took people years to come up with a selective K-RAS inhibitor. It was a

16:06.400 --> 16:11.120
always a great target, but it just was really what we call intractable. So it's this optimization

16:11.120 --> 16:16.320
of finding something that moves your, your model of the disease in the right area. It's kind of

16:16.320 --> 16:21.040
tractable. It's not toxic, right? And then we can put forward. So that's that sort of the thing

16:21.040 --> 16:24.640
there. So again, we use machine learning in that sense to help design those experiments that

16:24.640 --> 16:33.920
carry that out. So can we, can we maybe take a step back. Have you talked through a specific

16:33.920 --> 16:39.040
concrete example of a project, whether you're talking about the cancer one you just mentioned and

16:39.040 --> 16:45.280
you know, what the data sources are, what the, the valuation criteria are and then talk through

16:45.280 --> 16:50.560
how this sequential learning idea plays out in some concrete context. Sure, let's, let's,

16:50.560 --> 16:55.040
let's talk about the variant to gene one, because I think that's, that's something that everybody

16:55.040 --> 17:02.400
has more of a sense of that now, especially. So what we do in that model is we have some genetic

17:02.400 --> 17:06.560
variants from, from standard GWAS analysis, these genetic wide association studies. And these

17:06.560 --> 17:11.440
are saying these variants in this region of DNA, important in the role of the disease,

17:11.440 --> 17:15.200
but we don't know which gene it is. It could be gene A, it could be gene B, it could be gene C.

17:16.400 --> 17:21.920
And so what we actually do is the, the system that looks at that, it treats the whole thing sort

17:21.920 --> 17:27.600
of a bit as a ranking problem, right? So top level model is, is like a ranking model, right? And

17:27.600 --> 17:30.960
there are a whole bunch of different features that feed into that. So think of it as equivalent

17:30.960 --> 17:36.640
to web search, you're saying for this, for this disease and this variant, what is the gene, right?

17:36.640 --> 17:40.080
And you're coming up with your ranked list of genes, right? And you'd want your top ranked

17:40.080 --> 17:46.160
list gene to be first, you first on the page, right? It'd be like the causal gene. So that system

17:46.160 --> 17:50.960
actually has a bunch of different models, right? They're feed into that. So one of these models

17:50.960 --> 17:55.680
are things that are like sort of these stacked encoder models that feature as of raw DNA. So look

17:55.680 --> 18:00.640
at the raw DNA sequence and then they predict like which, where a transcription factor and which type

18:00.640 --> 18:05.760
may bind, whether that sequence of DNA is what we call open or close, which is when it's packed up

18:05.760 --> 18:12.400
and open or closed chromatin. There are other features that talk about whether those, these

18:12.400 --> 18:16.720
particular genes are expressed or not in that, in a particular tissue type, because not all genes

18:16.720 --> 18:20.720
are turned on in all different tissue types, right? You know, cardiac cells are different from

18:20.720 --> 18:25.920
neuronal cells or different from skin cells. There are other features that come out of, like, knowledge

18:25.920 --> 18:29.520
graphs are sort of like these node embeddings. And I can talk about how we have a pretty large

18:29.520 --> 18:33.760
knowledge graph we use behind things. All these different types of models and there are some of

18:33.760 --> 18:37.920
them are neural networks and are in right, some of them are different types of things. I will sort

18:37.920 --> 18:43.600
of features that go into this other model. And it's again a neural network type model. But again,

18:43.600 --> 18:47.680
it's supervised in the form that we say, okay, here's a variant and here what we think is the gold

18:47.680 --> 18:53.200
standard gene for that variant is, right? And then we go away and you learn how to weight those

18:53.200 --> 18:58.160
particular features, right? Just much like you would train everything else. The challenge we have,

18:58.160 --> 19:03.440
right, is there isn't a massive amount of gold, canonical, you know, variant to gene features,

19:03.440 --> 19:08.160
because I just told you that we only know what to do with 15% of them. So then we have to do the

19:08.160 --> 19:12.320
experiment part and experiment is where we bring in the function genomics. So what we actually do

19:12.320 --> 19:16.720
is take cell type, depending on what we're doing, it could be different types of cell types,

19:16.720 --> 19:23.280
but you can say it's a primary T cell from a human donor. We do the edit and then we actually

19:23.280 --> 19:27.360
sequence those cells. We look at the mRNA levels and we say, okay, we know what it was before

19:28.000 --> 19:32.560
and we know what was afterwards. It was the differential gene expression. And you say, I think

19:32.560 --> 19:36.240
this variant affects this gene. So then we go and look at that gene and look at the change in

19:36.240 --> 19:40.160
gene expression, right? And if we get it right, you know, that gene expression falls, right?

19:41.120 --> 19:45.680
And only that particular gene, right? And that becomes a training data. So then that kind of feeds

19:45.680 --> 19:52.560
back in, you know, we rebuild the model and off we run again. And so what it means is that

19:53.360 --> 19:59.520
the different teams that run those different sort of submodels, you know, they can, they also

19:59.520 --> 20:02.960
have different data sources that they will bring in and some of them are generated from external

20:02.960 --> 20:06.560
data, some of them internal data to build their kind of their feature factories that feed into this

20:06.560 --> 20:15.440
thing. But that's how we train the whole model. And it's, it's a really interesting scenario because

20:15.680 --> 20:22.000
probably I would say 45% of the time, right? A simple model, right? Which is like the variant

20:22.000 --> 20:26.720
affects the closest gene. We'll get you there and it'd be right there. The problem is is that the

20:26.720 --> 20:30.480
rest of the time that model doesn't work. And it's not the closest gene. It could be something

20:30.480 --> 20:34.640
quite far away, right? And one third of the time from doing this experiment, it's really, really

20:34.640 --> 20:38.560
far away. And so not what we expected. So we've been running this learning loop, which basically,

20:38.560 --> 20:43.360
like, build some model in general, it's a trained data at the same time. And as a result, the overall

20:43.360 --> 20:47.440
model that maps variants of genes gets better. And so we track that over time. So when we started

20:47.440 --> 20:51.920
off, you know, we were mapping like, they were like 15% of the unexplained things in the UK by

20:51.920 --> 20:57.920
when we could map them up to 24%. And they were at 40%. So we know what to do now with 40% of

20:57.920 --> 21:02.160
our genetic variants in this database, right? And that gives us a whole bunch of new potential

21:02.160 --> 21:05.280
targets to go and explore with some of the other systems I talked about.

21:05.280 --> 21:10.800
Got it. So is the, again, sequential learning loop? And in some ways, you describe it and it

21:10.800 --> 21:15.440
sounds like this, you know, automated thing that's kind of continuing to iterate over time,

21:15.440 --> 21:20.080
performing an optimization across, you know, all of the experiments you're doing. In other

21:20.080 --> 21:25.040
ways, it sounds like your applying machine learning, it's given you some, you know, some features,

21:25.040 --> 21:30.720
some signals. And then it, you know, goes into a scientist brain that determines, you know,

21:30.720 --> 21:37.760
what the next step is. How close does that loop? Yeah. So the tools I'm talking about, the models

21:37.760 --> 21:44.080
we build, you know, they're obviously scientists involved in running the, you know, the experiments

21:44.080 --> 21:48.000
as code, right? Sort of sort of like, you know, tending to the robots, depending on how complicated

21:48.000 --> 21:53.120
experiment and the throughput we're doing, involved in that. It's really where the, where the

21:53.120 --> 21:56.320
human science is getting involved is sort of the output of this sort of thing. You know,

21:56.320 --> 22:01.680
another experiment we have is involved in discovery of an air of cancer drugs, right? And it's,

22:01.680 --> 22:07.440
it's a concept, something called synthetic lethality. And all that means is basically all cells,

22:07.440 --> 22:12.080
most, both biology we have redundant pathways are really important things, right? Tumor cells

22:12.080 --> 22:17.760
grow really rapidly and they tend to sometimes like, because they divide so rapidly, they can tend

22:17.760 --> 22:22.800
to like break and only have one functional copy of it or something. And there's a, you know,

22:22.800 --> 22:28.720
and so what we know then is if we can identify which things are likely to break and I can make a

22:28.720 --> 22:33.280
drug against that sort of thing, because it doesn't have a backup anymore, it's selectively kills

22:33.280 --> 22:38.880
the tumour cell over your normal, your normal tissue, right? So a GSK drug like niraporib,

22:38.880 --> 22:44.240
right? It's a, it's a, it's a class of drug called a pop inhibitor, but pop inhibitors are involved

22:44.240 --> 22:50.880
in DNA repair, right? So basically if you stop the DNA repair, it then basically, you know,

22:50.880 --> 22:57.760
acts to sort of kill the cancer cell. So what we actually do is have another system and again,

22:57.760 --> 23:02.400
this one basically looks and tries to come up with what we call synthetic lethal pairs, right?

23:02.400 --> 23:07.360
If you see this mutation, then you can target this other particular gene, right? And so we have a

23:07.360 --> 23:12.640
commons of the things we know tumour cells like, you know, mutations they get and say well,

23:12.640 --> 23:17.440
what things pair with that? So again, we do experiments, we knock that thing out, we turn it

23:17.440 --> 23:21.280
out, we turn it down, we look at this effect on viability in a whole bunch of different tumour cell

23:21.280 --> 23:26.800
lines, right? But then the output of that doesn't automatically become like this is the target

23:26.800 --> 23:30.400
go away and make it anybody against it and make a small molecule against it. That's where we

23:30.400 --> 23:35.920
interface with our experimental colleagues, because there's a whole lot of, these are all narrow

23:35.920 --> 23:41.040
purpose ML systems we're building, right? So there's a whole lot of of data and things like that

23:41.040 --> 23:45.040
and things that they bring into bed to think about to work out like how all that happens and

23:45.040 --> 23:49.440
different experiments that they will then go and design to really, it's really a hypothesis

23:49.440 --> 23:53.840
that's suggested by this machine learning algorithm, for example. So, you know, all the things we do,

23:54.800 --> 23:58.480
you know, to surface the information to work with our colleagues and this sort of thing,

23:59.280 --> 24:02.960
and really sometimes the production of the ML model is a lot more automated, but then the use

24:02.960 --> 24:07.520
of the model is where the human scientists, right? These are tools for the scientists, right? We cannot

24:07.520 --> 24:12.400
encode all the background knowledge of biology and things in the way we want. And also,

24:12.400 --> 24:17.600
the scientific literature is really messy, right? Not everything in it is correct, right? So there's

24:17.600 --> 24:23.040
certainly a role for domain expertise in that. And did I hear you earlier reference work that you've

24:23.040 --> 24:29.120
done to apply machine learning to mind the scientific literature itself? That's right. So,

24:30.880 --> 24:34.000
one of the ways you think about if you're doing sequential learning and you're running all these

24:34.000 --> 24:38.800
big learning loops is like, you know, humans, we've been doing medicine for a long time, right? We

24:38.800 --> 24:43.280
know a lot about biology and medicine and things like that. It would be foolish to start from a,

24:43.280 --> 24:47.200
you know, a clean slate and have to learn all that and we'll take a lot more samples. So,

24:48.000 --> 24:51.280
there are ways to think about that. Like, how do I have structured priors if you're a Bayesian

24:51.280 --> 24:59.120
and things like that? So, we have another group who does, you know, one of the great advances you

24:59.120 --> 25:03.920
see obviously is in NLP, right? So, we have all these journal articles that are either in, you know,

25:03.920 --> 25:09.440
open source like on the bio archive or, you know, Elsevere or PubMed etc. Right? There's lots of

25:09.440 --> 25:15.600
those sources and there's also in, there's also data sets published as well. And so, we have a,

25:15.600 --> 25:22.320
a group that sort of builds a, an NLP type model and it's based on like birth type architectures

25:22.320 --> 25:26.400
again, we're seeing encoders sort of appear everywhere. And what that does is that, that sort of

25:26.400 --> 25:30.640
pulls out things, right? So, it entities, right? So, it's entity in relationship extraction. So,

25:30.640 --> 25:35.200
we put what we call a semantic triple, which is really a thing, a type of relation and another thing,

25:35.200 --> 25:41.280
or a subject, predicate object pairs, right? The predicates we care about are a limited set of

25:41.280 --> 25:45.760
things. And luckily, the scientific literature isn't, has free form as the rest of the thing,

25:45.760 --> 25:50.720
people writing things, right? So, you didn't say A has function in B, right? X does Y, X does not

25:50.720 --> 25:56.160
do Y, right? And where X is and Y could be genes, diseases, small molecules, you name it,

25:56.160 --> 26:01.360
when there's a set of things where we're interested. So, we mine all that out, we run all the sort of

26:01.360 --> 26:05.760
thing over the literature, we pull out all those semantic triples, and we stick that into a really

26:05.760 --> 26:10.960
big store. So, we have a graph that's like 500 billion nodes, right? It's huge.

26:10.960 --> 26:16.160
I mean, you were referring to earlier. Yeah, yeah. So, we don't use that whole thing, what we do is

26:16.160 --> 26:20.560
we pull out things. So, maybe I'm interested particularly in just genes, diseases, and you know,

26:20.560 --> 26:24.960
protein, protein products, which is genes and things like that. And I can pull out those subgraphs.

26:24.960 --> 26:30.640
And we might do some link prediction on that. So, actually, it's not known, but we're pretty sure

26:30.640 --> 26:34.880
that X does do Y, or maybe there's some weak evidence for it. You know, you can apply again,

26:34.880 --> 26:39.200
you're applying ML algorithms on top of that to build that knowledge base. And then we actually

26:39.200 --> 26:43.040
use, usually, node embedding. There's a different way to think about, how do I represent that data

26:43.040 --> 26:47.360
into my algorithm? I don't, it doesn't have to learn that, like, gene X does gene Y, we've really

26:47.360 --> 26:50.880
told it that this happens, right? So, how do you represent that structure knowledge that you're

26:50.880 --> 26:56.000
confident in? And this means that we can be kind of more sample efficient, right? You view all

26:56.000 --> 27:00.000
ML algorithms as sort of information engines. We can be more sample efficient with the data we're

27:00.000 --> 27:04.080
doing. So, we learn the things we don't know, right? Rather than relearn the things we do know,

27:04.800 --> 27:10.560
it is a key. So, that's another big area. And once you've built these knowledge graphs,

27:11.280 --> 27:14.560
you know, there's lots of other uses you can put them to, rather than just like machine learning

27:14.560 --> 27:19.280
group, right? And finally, I just can say, oh, you know, what's new about my protein? Okay,

27:19.280 --> 27:23.200
here's all the facts about my protein. Like, does X do what? I've got a hypothesis that X is involved

27:23.200 --> 27:28.000
in Y, right? No one holds the scientific literature in their head anymore, right? It's, you know,

27:28.000 --> 27:33.760
it's too complicated. So, you can go on query that. But the interesting thing is, like, you know,

27:33.760 --> 27:38.960
said before, a big 300-year-old company, like, there are things that have been in Gone and GSK

27:38.960 --> 27:42.720
that people have forgotten, right? Like, is that right? You know, so, like, you actually can

27:42.720 --> 27:46.720
mind your own data and go, oh, wow, we did experiment about that. Or we thought we thought, you know,

27:46.720 --> 27:50.720
is it interesting protein someone's got, by the way, you know, 20 years ago, we worked on this,

27:50.720 --> 27:54.000
someone worked in a related thing and they found a molecule that affects it. It wasn't what they

27:54.000 --> 27:58.160
were looking for at the time, but we've got it on a shelf somewhere. So that kind of thing becomes

27:58.160 --> 28:03.840
like, you know, the brain of GSK. And because, you know, it's not just scientific papers, it's also

28:03.840 --> 28:08.640
the data sets aside with papers, you can start putting whole data sets where people are doing these

28:08.640 --> 28:13.120
big experiments at scale and industrial scale into these knowledge graphs as well. So, there are

28:13.120 --> 28:17.440
lots of experiments where people are doing screens for a particular functional things like that,

28:17.440 --> 28:21.280
and they come up with lists of genes that are known to be involved in things. You can import

28:21.280 --> 28:25.680
that knowledge as well into the knowledge graph. So, it becomes a sort of, like, growing reference

28:25.680 --> 28:32.640
space to use. One thing I'm curious about is you think of kind of how your team operates against

28:32.640 --> 28:37.840
the quadrant of kind of innovating on the biology and innovating on the machine learning. I'm curious

28:37.840 --> 28:45.280
where you find them and where you want them to be. You know, this space is moving so quickly,

28:45.280 --> 28:50.320
oftentimes you may have to innovate on the machine learning to make it work for your application.

28:51.680 --> 29:00.720
Is that the case here? Yeah, so I think that we have, in general, we don't work on very many

29:00.720 --> 29:06.400
things as a group. So we're about a 120-person research group, and we're quite globally distributed,

29:06.400 --> 29:12.240
I've been in San Francisco, I have people in team members in Boston, Philly, London,

29:12.880 --> 29:16.480
Tel Aviv, Heidelberg, Switzerland, right? There's where we're kind of everywhere.

29:17.600 --> 29:22.240
But a lot of things we work on, like, there isn't a solution, right? There isn't a variant

29:22.240 --> 29:26.000
to gene off the shelf, piece of software algorithm, right? Because the data's not there,

29:26.000 --> 29:30.160
you've got to build the whole thing. But there are cases where we can borrow things,

29:30.160 --> 29:34.240
and there are cases where we have to do research. So we do a lot of research into causal

29:34.240 --> 29:39.600
machine learning, because obviously we want to come up with things that are causal for the disease.

29:39.600 --> 29:44.400
So like, if I, you know, and what I would say have a small level of clinical hysteresis,

29:44.400 --> 29:48.160
and that all that means is basically, I small change in this particular, like, your drug

29:48.160 --> 29:51.600
against this with thing like, I don't have to knock it down 100%, if I knock it down 10%,

29:51.600 --> 29:55.360
has a larger effect on the cause of disease. That's the easier medicine to make. There's something

29:55.360 --> 30:00.480
like, well, if you take this down to 99% of its level in the body, then you might see a clinical

30:00.480 --> 30:03.760
effect. That's probably not a good candidate for medicine. So we do a lot of work on like

30:03.760 --> 30:07.440
causal, reconstruction of causal data from network data and things like that.

30:08.160 --> 30:14.320
And there are some areas where, you know, you might start on off with just taking things that

30:14.320 --> 30:19.840
have worked really effectively. So computational pathologies is a great example. So pathology is

30:19.840 --> 30:23.760
when you have, you might have seen those, those ready purple slides of a tumor and things like

30:23.760 --> 30:28.320
that or a biopsy or things like that. And so typically they were like, you know, they were

30:28.320 --> 30:31.920
their human toxic ears and stained slides, and they looked at biohuman pathologists,

30:31.920 --> 30:36.640
who looked at things and those things like old stage one, two or three cancer, for example. We know

30:36.640 --> 30:43.040
some information in the image, higher than number, worse it is, right? Now, what happened was,

30:43.920 --> 30:48.160
you know, we got digitization happening. So we got people started to scan these slides,

30:48.160 --> 30:52.800
a high resolution, right? And these are big images, right? These like four terabyte images,

30:52.800 --> 30:58.240
okay? Gigapixel images. And then the other side, we've got confidence and units and things like

30:58.240 --> 31:02.240
that sort of sitting around. So the natural thing was like, well, I'll just take a unit. I'll take

31:02.240 --> 31:05.920
a confnet and I'll see what I can, what I can do. And I might want to segment things. I might count

31:05.920 --> 31:09.360
the number of types of cells in the slide, rather than having a pathologist go through and do that,

31:09.360 --> 31:14.480
right? I might want to say, well, what's the tumor stroma ratio? Like so, when you take a tissue

31:14.480 --> 31:17.920
block, you might have the tumors growing here and there's normal tissue around it, right? How we

31:17.920 --> 31:24.000
give that area, for example, what are the characteristics? And so it started off with people doing those

31:24.000 --> 31:29.520
types of things, right? And those technologies work. But then almost everywhere, when you go, we start

31:29.520 --> 31:34.480
ask more advanced questions, you innovate on my methodology, right? So some of the things we do now are,

31:35.120 --> 31:41.280
well, can I predict the genetic status of a tumor just from the image alone, right? And you ask

31:41.280 --> 31:44.720
pathologists, they're like, well, could you tell me whether this tumor has this particular mutation

31:44.720 --> 31:49.120
from looking at it? They're like, it's crazy. There's no way I can do that on the human, right? And

31:49.120 --> 31:53.680
you think, okay, well, but you can actually build models. And we've done this, they can actually predict

31:53.680 --> 31:57.760
like the genetic status of the tumor. So there are subtle microchanges in environment and stained

31:57.760 --> 32:02.480
density and things like that due to like the changes in the biological processes, right? A human

32:02.480 --> 32:07.040
eye can't be, can't obviously be sensitive to train that, but you can sometimes do be retrospective

32:07.040 --> 32:11.440
and see what features has the model learn. But suddenly you're taking the comb nets and these

32:11.440 --> 32:15.680
types of things and res nets, sort of frequently. And then you're starting to push, push them in

32:15.680 --> 32:19.280
into different areas and you start to do, start to tinker with them again. And then you're finding

32:19.280 --> 32:23.280
different architecture. So we're again, we're seeing that, we see that also in, you know,

32:23.280 --> 32:27.360
cellular imaging as well, where we use the same types of things. We're looking at cells and how

32:27.360 --> 32:31.520
they're phenotypes have changed, what they look like pictureally, as they change when we give them

32:31.520 --> 32:36.640
a drive or don't treat them the drive, for example. So it doesn't take much before you've taken some

32:36.640 --> 32:40.000
off the self-technology, but typically you're, you're starting with an architecture or something

32:40.000 --> 32:46.880
else like that that you will then adapt to a use case. So that big, you know, bearing to gene

32:46.880 --> 32:52.240
algorithm, right? You know, well, I cast as a ranking problem. There's been lots of machine learning

32:52.240 --> 32:56.080
research into ranking problems for a long time, right? There's lots of self-tooling and things

32:56.080 --> 33:00.960
like that and ways to think about things. So we, those are things we start with, right? We bring in.

33:00.960 --> 33:05.600
And, you know, but there are some things where it's, it's wholly new algorithms and architectures

33:05.600 --> 33:12.880
and the things that, you know, we were sort of having to invent as well. And did your team publish

33:12.880 --> 33:20.320
in those areas? Yeah. So, yeah, I mean, so we, we publish, you know, we publish all our code

33:20.320 --> 33:24.160
and our work and it's kind of really important you do that because you're talking before about,

33:24.160 --> 33:29.920
like, how do you find people, right? So I think what's happened is there's a lot of people that

33:29.920 --> 33:35.760
have realized that, you know, there's now lots of this data appearing in biology, right? I mean,

33:35.760 --> 33:40.240
since post-COVID-19, it's really interested in human health, right? And you want to find an

33:40.240 --> 33:46.400
environment where you have the computation resources and people look to do that. But nobody wants to

33:46.400 --> 33:50.640
join somewhere and then connect vanish into a black hole, right? And we use, the models you just

33:50.640 --> 33:55.200
talked about, like, you know, we're only essentially able to, I would apply ResNet, some

33:55.200 --> 33:58.480
to computation mythology and start with that because it's out there in the literature and the domain.

33:58.480 --> 34:04.160
And that's why AI is so fast, is that free exchange of ideas and test sets and study out benchmarks

34:04.160 --> 34:08.720
and we level those things as well? So, you know, we will, we publish that code, right? Because it's

34:08.720 --> 34:14.240
the data that's really important, right? So, you know, we will publish, if it's a model that we've

34:14.240 --> 34:18.240
built, we've got a code and there's a public data set and we can build a model, we would also

34:18.240 --> 34:24.000
publish that model build public data, right? Because it's the sort of the data GSK's

34:24.000 --> 34:28.400
generate rate and allows us to build a model at much higher quality, right? That's our kind of strategic

34:28.400 --> 34:33.200
advantage, right? So, it's, again, it's all about the data and then that's similar to other industries,

34:33.200 --> 34:36.800
right? You know, Facebook publishes lots of really cool graph algorithms and things like that

34:36.800 --> 34:41.520
that don't give you their social graph, right? Data. Similar for us. But it also means that we can contribute

34:41.520 --> 34:48.400
to the community. We're running a challenge, I think, at ICLR this year as well on gene discovery

34:48.400 --> 34:53.520
and causal discovery from networks and things like that. I think we've got two or three papers

34:53.520 --> 34:58.880
in Europe this year out of the group as well. So, yeah, we publish and, you know, in both in

34:58.880 --> 35:02.960
the conference proceedings and in other scientific literature as well. It's very important for us.

35:03.760 --> 35:12.240
Nice, nice. Do you think much about tooling and infrastructure platforms? I have read that you're,

35:12.240 --> 35:21.280
I've read about your AI hub a bit. Yeah, the answer is yes, but yeah, absolutely. So, you know,

35:21.280 --> 35:25.680
there's a few, so, the infrastructure, I mean, there's one thing I learned from doing start.

35:25.680 --> 35:28.960
It's like, you know, the, you start building infrastructure now on the next best way to start

35:28.960 --> 35:32.480
implementing infrastructures tomorrow because it allows you to scale and if you suddenly have

35:32.480 --> 35:37.360
infrastructure problems, it's really difficult to solve once you're in that phase. In the AI team,

35:37.360 --> 35:43.520
we have a whole group that's really an AML platform organization and they build all the kind of

35:43.520 --> 35:48.400
tooling and infrastructure for us to kind of, you know, deal with data containers and running

35:48.400 --> 35:53.120
things and scanning algorithms and other kind of those aspects. And it's about not only just,

35:54.320 --> 35:58.080
you know, GPUs, computing things, you know, we do a lot of pie talk extensively,

35:59.040 --> 36:04.080
is our, is our preferred platform of choice. But some of it even also comes down to the things

36:04.080 --> 36:08.880
we're looking at, we end up needing kind of like novel compute, right? So, we've had a strategic

36:08.880 --> 36:12.640
partnership with a company called Cerribris, which you may have, some of you that you may be familiar

36:12.640 --> 36:17.520
with, right? Cerribris have one of these companies they built like, you know, a really

36:17.520 --> 36:23.600
really amazing piece of hardware. And so we use Cerribris for a particular type of problem where

36:23.600 --> 36:29.040
we're building these encoder models on, on DNA. Now, what's interesting is these encoder models

36:29.040 --> 36:33.360
is we want to have a really, really large window size, right? And so you get to this, it's really

36:33.360 --> 36:39.280
challenging to build model parallel and data parallel kind of algorithms at the sort of scale. And

36:39.280 --> 36:43.360
the data sets we're passing over are really, really large as well, right? It's for these

36:43.360 --> 36:49.840
genomic data sets. So that was a really interesting problem that the Cerribris system is like,

36:49.840 --> 36:53.680
it's got massive throughput. It could be a really big model because of the scale of the chip

36:53.680 --> 36:59.520
products and the latency between that and the memory was really large. So, you know, we started

36:59.520 --> 37:04.720
working with them. We've got us, we have a CS1 system. We'll have our CS2 soon. And, you know,

37:04.720 --> 37:09.360
that becomes a strategic thing that we can start to build new algorithms and play new things on

37:09.360 --> 37:14.000
and actually build models for. And then, you know, so the, so the, the compute is really important.

37:14.000 --> 37:19.600
For us, it's a, it's key to be unconstrained by, by computer, right? To be like, you know, to think

37:19.600 --> 37:24.960
of like, you know, what's the best way to solve the problem. You know, we usually constrain by data,

37:24.960 --> 37:30.720
like the data I love to have, right? And so this is also why, you know, we also work with Nvidia, where

37:30.720 --> 37:36.000
we're pushing the bounds of CUDA. So we have an a strategic ground arrangement with Nvidia, where

37:36.000 --> 37:40.160
we have people on site in London, where, you know, we're making changes to low-level, you know,

37:40.160 --> 37:45.040
LibDNN and things like that or working with them on those types of things. So, you know, how do we,

37:45.040 --> 37:49.120
how do we, how do we make it easier for us to do it focus on like, you know, only one problem,

37:49.120 --> 37:52.640
which is the science problem rather than two problems, like the science problem and engineering

37:52.640 --> 37:57.520
problem, right? Because the, the challenge we face are hard enough, right? So, so that's, that's a key,

37:57.520 --> 38:02.720
is a key component for us. And, you know, the other thing is like, we want to think about how

38:02.720 --> 38:06.480
many iteration cycles a machine learning person can do per day, right? I don't want to be someone

38:06.480 --> 38:09.680
sitting there like, I've got an idea, I've kicked it off. Well, I guess that'll be done in two

38:09.680 --> 38:13.200
days time. Yeah, I'll sit here and read a thing. I want to be able to like look at something,

38:13.200 --> 38:16.960
have an idea or have a bunch of ideas, kick them off and then actually get the results back

38:16.960 --> 38:20.640
that afternoon and think about it and then, you know, run on to other sets. That's also really key

38:20.640 --> 38:26.640
for us is to be, you know, so as we grow, we've needed to add, you know, every every every new AML

38:26.640 --> 38:32.880
high requires, you know, a bunch of A 100s or whatever we need, like added to the stack, right? It's a cost.

38:32.880 --> 38:39.120
Yeah, you mentioned earlier, you reference a feature factory. So you're developing these features

38:39.120 --> 38:44.640
and you reference a feature factory, is that a concept or an idea or is that a physical thing,

38:44.640 --> 38:50.720
like a feature store? So you can imagine for, you know, if I've got, you know, that section of DNA

38:50.720 --> 38:56.560
and I've got my very antenna, right? Those, there are different models that can like tell you different

38:56.560 --> 39:01.120
things about it. So analogy with the web page, I would have the title of the web page, the links to

39:01.120 --> 39:05.600
it, the text and those links, you know, the content of the web page, the word count, author, the date,

39:05.600 --> 39:12.240
those are all features about the website, right? And, you know, that you have tools and code that

39:12.240 --> 39:16.400
could pull those things out and represent that and a featureisation to some kind of AML model.

39:16.400 --> 39:23.120
Similar in this case, there are, we look at that whole stretch of DNA and the disease that's

39:23.120 --> 39:27.520
in the similar context and there are algorithms. In this case, there are models themselves

39:28.000 --> 39:31.840
that work on how to featureize that, to represent that to that whole ranking algorithm.

39:31.840 --> 39:38.000
So we term those sort of things as a feature factory, right? So the ones that look at the raw DNA

39:38.000 --> 39:41.840
sequence and will say, well, this is open and closed chromatin, the other one will say, well,

39:41.840 --> 39:46.160
this gene isn't on in this cell type, right? And it might imagine the model when it's trying

39:46.160 --> 39:50.480
to, when it had ranked the importance of those things to give you, you know, it's candidates

39:50.480 --> 39:54.640
of genes might say, well, well, this gene isn't even on in this cell type that's involved in this

39:54.640 --> 39:59.920
disease. So this is probably a low unlikely thing, right? This one's in closed chromatin, that's

39:59.920 --> 40:03.520
unlikely. This one's in open chromatin, it's involved in disease and things like that. So it's

40:03.520 --> 40:08.320
becomes a good candidate. So it learns how to rank all these different things and how to combine

40:08.320 --> 40:14.880
them. And so it's, it's not a, it's not a sort of a physical thing, but it is a sort of like,

40:14.880 --> 40:19.040
it's a featureisation type aspect, right? So it's basically a featureisation by other

40:19.040 --> 40:27.280
sort of submodels themselves. Yeah, when you think about all of the, the various algorithms

40:27.280 --> 40:34.320
and tools and things that kind of factor into and enable what you're doing and look forward,

40:34.320 --> 40:42.960
what are the areas that, either you need the most innovation happening or you're excited

40:42.960 --> 40:48.080
because you see the innovation happening, you know, whether, you know, we're talking about algorithms

40:48.080 --> 40:55.280
or tooling infrastructure, that kind of thing. Yeah, I think one of the things that I'm deeply

40:55.280 --> 41:04.080
interested in is robustness and reliability constraints, right? And, and this plays into a,

41:04.080 --> 41:08.320
you know, a debate with, you know, in regulation and things like that, is that as we start to build

41:08.320 --> 41:12.800
sort of probabilistic reasoning systems and imagine, let's take the pathology example where I have images

41:12.800 --> 41:18.720
and things like that. And we've, maybe, you know, we've designed it well, so we've made sure we've

41:18.720 --> 41:21.840
got like, you know, bunch of people different backgrounds, you know, we've got a lot of,

41:21.840 --> 41:25.520
lots of training day, we'd have underrepresented groups, we've done the best case to do that,

41:25.520 --> 41:30.480
that's very important. And we built a model and the model has good performance characteristics,

41:30.480 --> 41:35.360
you know, maybe 80% of the time it's correct and it predicts someone's got to, as a, you know,

41:35.360 --> 41:39.440
it gets a genetic status right, so we know then who to sequence or not, for example, I'm just

41:39.440 --> 41:45.680
using a hypothetical scenario. How do we know how that model behaves when, you know, what's the

41:45.680 --> 41:50.000
episode image for like a, you know, for a pathology thing like it's like, maybe the area is out of

41:50.000 --> 41:55.440
focus, right, or it's got a pen mark, or it isn't enough tumistrum. How do we know that this model

41:55.440 --> 42:00.080
fails gracefully? How can we define as its balance of operation and things like that? And, you know,

42:00.080 --> 42:05.280
for some other methods and things, it's easier to define and construct that, you know, for a new

42:05.280 --> 42:11.040
network, it's harder to know how, you know, some of those changes result in, you know, the decision

42:11.040 --> 42:16.640
boundary actually happening. And so knowing that you've trained a model that's, um, for some

42:16.640 --> 42:21.840
of these scenarios, you would train off some, you know, some performance for robustness characteristics,

42:21.840 --> 42:26.960
right? But it is hard to know how those robustness and reliability characteristics happen. So,

42:26.960 --> 42:30.560
we're doing a lot of, we do a lot of research in that area and we have various groups we interact

42:30.560 --> 42:36.560
with and PhD students, we sponsor. But that's a really active area that's, you know, it's not just,

42:36.560 --> 42:41.600
you know, our organization, I have to particularly, it's, it's across the industry in various things,

42:41.600 --> 42:46.320
where people want to know, um, know those, those sorts of aspects and that's where you get into

42:46.320 --> 42:52.560
monitoring other sorts of things. But for us, it's, it's all about, um, knowing how I can measure,

42:54.080 --> 42:59.600
that I found a good robust solution and it's not brittle, right? The small changes that

42:59.600 --> 43:06.640
import don't lead to large changes in the activation. Um, that's, that's one key area. Uh, I think for

43:06.640 --> 43:14.400
us, the, you know, looking at, um, simpler transformer architectures that could lead to the same

43:14.400 --> 43:18.560
kind of performance is another really key thing is we can train them faster and things like that.

43:18.560 --> 43:24.480
So I understand a little bit of that sort of, you know, just the, you know, um, model parameters

43:24.480 --> 43:29.120
based performance trade-offs and sorts of sorts of things, you know, um, where you think, well,

43:29.120 --> 43:34.320
maybe there is a simpler architecture that can do just as well. Uh, that's, that's a, a common

43:34.320 --> 43:42.880
error research. And, you know, I've been a lot of it actually comes down to really, uh, biology is

43:42.880 --> 43:50.080
all about, um, low, low and high dimensionality, right? And, uh, sort of biases and time series,

43:50.080 --> 43:56.800
right? So a lot of our data, if you think about it, that idea where I've edited my variant in,

43:56.800 --> 44:04.640
I'm looking to see which genes change, right? Well, I can measure that six hours after I've made

44:04.640 --> 44:10.480
my edit 12 hours or 24 hours and the whole thing is changing over time, right? And so actually

44:10.480 --> 44:14.640
begins to start to integrate those, those, those temple dimensions. So this is where we have a lot

44:14.640 --> 44:19.120
of time series data and we can generate that in biology. And this is where I think that's another

44:19.120 --> 44:25.760
area that, of key research and probably the, the final one is really sort of multimodal, um,

44:25.760 --> 44:31.520
and end to end, and, and learning multimodal. So they're the classic examples. I have some

44:31.520 --> 44:36.160
cells in a dish. I'm taking images of them and I might pull them out and do RNA seek. And I

44:36.160 --> 44:39.680
wasn't at my time series, that's why they're in for good measure. And I want to start to look at

44:39.680 --> 44:45.360
that and I want to build a model that can classify when a perturbation has made a cell look like the

44:45.360 --> 44:49.920
wild type cell, the healthy tissue, and made its gene expression look like that, right? And I'd

44:49.920 --> 44:55.440
like to go to end to end learn that right now we typically learn the gene expression model,

44:55.440 --> 44:59.280
the cellular imaging component together. We'd maybe take the top two layers of those sorts of

44:59.280 --> 45:03.120
things and you throw that into another model learns to integrate them. We don't propagate the

45:03.120 --> 45:07.120
error back down through all of that, just because the complexity of the thing. But that's an area where

45:07.120 --> 45:15.840
I think that, you know, could be really useful. And, you know, typically, you know, you mean, the

45:15.840 --> 45:18.640
convolution size, right? That's that you could use attention on that. You can have these

45:18.640 --> 45:22.080
dilated and flexible convolutions where you could adapt that because it might not, you know,

45:22.080 --> 45:25.280
picking the one that looked good for that, it could be a very specific one that could work better

45:25.280 --> 45:29.200
for that problem, right? When you want to, you know, so that that's a that's a massive error

45:29.200 --> 45:35.600
research for us. And because we have that in medicine, right? We have, um, you know, your biopsy,

45:35.600 --> 45:40.080
your pathology biopsy might be done once, right? When you're diagnosed. But we can do clinical

45:40.080 --> 45:45.520
imaging every seven or eight weeks. It's like a CT scan or MRI, for example. But I might do your,

45:45.520 --> 45:50.000
your blood work, right? I can sample it. You take a file of blood and I might look for circulating

45:50.000 --> 45:54.880
tumor DNA, right? It's the things like grail or, or free gnome or those sorts of things, um,

45:54.880 --> 45:59.440
garden health, right? Those sorts of assays we're looking at literally DNA in the blood that's

45:59.440 --> 46:04.160
come from the tumor cells, right? And look sequencing that. And, and that could be done. They could,

46:04.160 --> 46:08.080
uh, they were done at different timescales, but they're all multimodal things about that

46:08.080 --> 46:12.560
particular patient and you're trying to integrate all those together to say, what's your particular

46:12.560 --> 46:15.680
outcome going to be? Are you responding to this therapy? Where are you going to go? When, what,

46:15.680 --> 46:21.120
what therapy should we give you next, for example? Mm hmm. And to some degree, that brings us back

46:21.120 --> 46:26.720
to compute because the scale required to integrate all this together is significant. Yeah, it's

46:26.720 --> 46:30.960
computing data, right? Because, um, you can, we can measure so many things, right? But we're

46:30.960 --> 46:37.600
measuring, pre-offent, often what you see in biology is we can measure more data on things that

46:37.600 --> 46:42.320
are wrestling, wrestling and people. So I can learn lots of data on a, on cells in a dish, right?

46:42.320 --> 46:46.240
But cells in a dish aren't going to give me the effect of like, you know, a whole person, right?

46:46.240 --> 46:51.440
I can't ever get information about whole organ failure from single cell culture of hepatocytes.

46:51.440 --> 46:56.080
But I mean, we're starting to see more complex innovations in biology. So things like organoids

46:56.080 --> 47:02.000
and things like that where they start to have more of the complexity and you see, and where we

47:02.000 --> 47:06.160
end up finding machine learning is actually building a bridging model from the, the thing that we can

47:06.160 --> 47:11.440
perturb a measure at scale and then how well does that correlate to, you know, to humans where we

47:11.440 --> 47:15.680
can only really like, we can't perturb humans. We can treat humans if we, if we've got a really

47:15.680 --> 47:22.080
good thing, we can measure things about us, right? So, um, that's sort of another area.

47:22.080 --> 47:26.320
Um, that's one of the things that we were actually doing with this sort of this King's College

47:26.320 --> 47:31.520
collaboration that was recently in the press where what we'll be doing is, and is, as we're taking,

47:31.520 --> 47:37.360
um, tumor samples from, from patients, right? And we can culture their tumor, and it's not,

47:37.360 --> 47:41.120
and it's not organoid. So it's the tumor, but it's passed their immune system and it's actually,

47:41.120 --> 47:46.320
quickly, it's that immune system combines from that particular patient. And then we can start to

47:46.320 --> 47:50.000
see how that responds, right? With various drugs, with influence, and we can measure very things

47:50.000 --> 47:54.480
about that and look at that over time. And the idea there is to sort of build a model of,

47:55.040 --> 47:58.320
you know, how best to treat that patient, one of the characteristics. So we even want to,

47:58.320 --> 48:02.240
what is the risk? Lung cancer, for example, you might resect it. You're hoping that you've

48:02.240 --> 48:06.080
got it all, and there's no secondary metastasis, but there are some people that will see a higher

48:06.080 --> 48:10.720
rate of secondary metastasis than others, right? So could you, how can you identify that, for example?

48:10.720 --> 48:15.920
So it's just a really interesting interplay between the development of experimental

48:15.920 --> 48:20.560
biological techniques, the ability to generate data at scale, right? And the ability to build

48:20.560 --> 48:25.760
models to kind of connect them back to humans. You mentioned robustness as being important,

48:25.760 --> 48:31.760
and before that, you talked a little bit about explainability. I'm curious how you think about that,

48:31.760 --> 48:38.240
and, and how you approach machine learning problems with those concerns in mind. Do you,

48:38.240 --> 48:44.160
you know, drive for performance and then back off to the explainability requirements,

48:44.160 --> 48:48.160
is it the other way around? Is there some kind of hybrid? It's, it's a really interesting

48:48.160 --> 48:55.280
debate, because a lot of the times, you know, I think people kind of use interpretability

48:55.280 --> 49:00.000
or these types of things as a proxy for I don't trust or understand sufficiently the engineering

49:00.000 --> 49:05.360
validation, right? And so, you know, I mean, I had the question, I was like, okay, I can give you a

49:05.360 --> 49:09.040
very, what was the simple model we like? I'm like, oh, I like a logistic regression with like six

49:09.040 --> 49:13.680
parameters. I'm like, okay, if I give you a logistic regression with six parameters, right?

49:13.680 --> 49:18.800
And maybe I only allow like, you know, positive non-zero coefficients, right? It's a huge number

49:18.800 --> 49:22.000
of functional forms can be known, but most people can't look at it in their head and really

49:22.000 --> 49:27.120
understand how that works, it makes the decision, right? Or have been said to the threshold. So,

49:27.120 --> 49:34.000
and we use technology and systems everywhere day to day without knowing how they work, right?

49:34.000 --> 49:39.840
Like everyone in the lab, where it comes down to is actually where you want reliability constraints

49:39.840 --> 49:45.440
and how it performs, right? And that's that's the sort of the trade-off, but there's also there's

49:45.440 --> 49:50.160
a trade-off between when do we really need to have secondary checks and things like that? We're

49:50.160 --> 49:56.720
making really big decisions, right? You know, you know, avionics for flying my plane or maybe doing

49:56.720 --> 49:59.760
I'm going about to diagnose someone with something else, you know, I really need to, how do I,

49:59.760 --> 50:03.600
how do I have a functional sources data? How to make sure it's robust? And there's sort of things

50:03.600 --> 50:09.200
in the discovery phrase where do we need to have some in explains to a human scientist? These are

50:09.200 --> 50:13.360
the key features and why we think this, this cell type is more like this cell type and others and

50:13.360 --> 50:18.000
this, this target's pushing it in the right direction versus and there's a trade-off between like,

50:18.000 --> 50:23.760
well, we're asking the machine to do tasks that a human can't. We're putting in so much data,

50:23.760 --> 50:29.200
and we're going to take those results and then check them in other ways, right? There, I would rather

50:29.200 --> 50:33.200
harness the full power of machine learning and not hamstring the system by saying, well, okay,

50:33.200 --> 50:37.280
here's a saliency map and having to us, I agree with that or not, or the functional form. So,

50:37.280 --> 50:41.520
depending what we're doing, it's a trade-off, but what we always care about is making sure we build

50:41.520 --> 50:46.240
a robust reliable model. So, like, understanding how you're assessing it, right? How you measure

50:46.240 --> 50:50.880
the performance? How do you understand them? Like, you know, you haven't somehow an information

50:50.880 --> 50:56.560
leakage and those sorts of things come through. So, it's attention, but where you do find things is where

50:57.520 --> 51:01.920
you have to make sure that you, you know, if you're doing, you're placing what someone would be

51:01.920 --> 51:06.160
doing manually, right? So, there's a whole thing of like, well, is this thing better than me? How do I

51:06.160 --> 51:10.320
know it's working? How is that quality aspect? And usually, once you want to say is like, look,

51:10.320 --> 51:14.400
I'm here to automate the boring, right? So, you can actually do and go to high-level science,

51:14.400 --> 51:19.200
then spend your time, you know, looking and analyzing this, right? And also giving them sort of an

51:19.200 --> 51:22.400
audit trail, they can go back and look at the data that went into the system and maybe look at it

51:22.400 --> 51:28.880
from a self or diagnosis tools, right, on the model's performance as well, you know? So, things like

51:28.880 --> 51:33.360
is the input vector within a vector space that's well bounded by the training set and the test set?

51:33.360 --> 51:38.480
What does the error manifold look over that thing? Is it uniform or is there a spiky regions where

51:38.480 --> 51:42.720
it, you know, because we, a lot of our performance measures are global measures or model performance,

51:42.720 --> 51:48.720
but you could easily take the input vectors, you can title them on a 2D plane, you can work out the

51:48.720 --> 51:53.120
error function over those things, right? And you can say, oh, wow, overall, it's a pretty good model,

51:53.120 --> 51:57.680
but like, you know, small molecules that look like this, this thing's lousy at, right? So, having

51:57.680 --> 52:01.840
uncertainty bounds, those sorts of things, they go a long way to actually putting these things in

52:01.840 --> 52:07.120
production. And, you know, another thing is that it's really important not just to have any model

52:07.120 --> 52:10.880
over spit out a number, anything we put in production has to give you both a number and a confidence

52:10.880 --> 52:16.480
bound and also has to, it also has to refuse to return a value. It's saying, I don't have that.

52:16.480 --> 52:21.520
This is so far out what I've seen, boss, I have no idea. We can collect those, we can log those,

52:21.520 --> 52:24.800
maybe we've got enough of them, we can build another model and over time almost every model becomes

52:24.800 --> 52:28.080
a cascade function. It's like, well, this is a global model for this one, this model for this,

52:28.080 --> 52:32.160
maybe eventually we can unify them again, but that's really important. Those are all the sort of

52:32.160 --> 52:37.440
the functional things because it's honestly, it's quick to make a model and once you give people

52:37.440 --> 52:41.360
a tool and it's very quick for them to use it, but it's the opportunity cost of the downstream

52:41.360 --> 52:44.800
decisions that they can make with it, right? They decide to do experiment A and not experiment B,

52:44.800 --> 52:51.440
for example. So, we think a lot about those sorts of things. And, you know, when you start off,

52:51.440 --> 52:54.720
it's always, I want to know how it works, what's the model thinking and things like that.

52:54.720 --> 53:01.680
But, you know, a lot of those theories of mind are not really truly how the model's thinking,

53:01.680 --> 53:05.920
even if we do distillation, right, and things like that. That's not really it is, it's sort of a

53:05.920 --> 53:12.960
hack on those things. Getting models to predict or describe their confidence, that's an active

53:12.960 --> 53:20.480
research area itself. Does that requirement that models, your model spit that out? Does that put

53:20.480 --> 53:26.800
a limit on the types of architectures you use, or is it itself kind of an area where you have to

53:26.800 --> 53:33.680
question reliability and trust of that confidence element? You know, we do a lot of work on that

53:33.680 --> 53:38.080
internally. I think, I guess, Francesco Ferrin has got some pretty good papers coming out on

53:38.080 --> 53:44.560
that. Well, he's published that we've done on that sort of aspect. We try and have some general

53:44.560 --> 53:48.080
genetic methods we can bolt onto any architecture. So, you can kind of separate the problem a little

53:48.080 --> 53:52.000
bit. Some architectures also give you, you can give you at the same time estimates of that,

53:52.000 --> 53:58.400
depending on what you're doing. But we also, depending where we are in development and where we're

53:58.400 --> 54:05.600
using it, you can, you might have great or less requirements for that kind of aspect of things,

54:05.600 --> 54:12.160
right? So, there's a bit of a, certainly once you're making big important decisions and you're

54:12.160 --> 54:16.640
putting things in production and it's going into other processes, you definitely need to have

54:16.640 --> 54:20.800
those aspects. But I wouldn't say every model of good at GSK by every group will always have

54:20.800 --> 54:25.280
those characteristics. For us, where we're making these big things that have, you know, big downstream

54:25.280 --> 54:31.040
consequences and decisions, you need to have that sort of place. And I think that, you know,

54:31.840 --> 54:37.680
I think just in the community globally, people are realizing that, right? You know, and that's also

54:37.680 --> 54:42.640
about sort of monitoring things in production, right? You know, of checking things. And, you know,

54:42.640 --> 54:48.640
there's plenty of examples of models going rogue and, you know, that kind of thing. And so,

54:48.640 --> 54:56.960
for us, it's a key thing. But we, depending on the stage, is how stringent we are on those

54:56.960 --> 55:02.320
requirements for it. Maybe kind of one more direction to briefly explore, kind of zooming out.

55:02.320 --> 55:12.480
The, can you speak a little bit about the, you know, kind of building an organization like yours

55:12.480 --> 55:20.080
in the context of a large, large company, large pharmaceutical company, you know, transformation

55:20.080 --> 55:27.840
implications, organizational receptivity to probabilistic models. That kind of thing is it,

55:27.840 --> 55:34.160
you know, it's a, it's a research organization or a scientific organization at its core. So,

55:34.160 --> 55:39.040
do you, you know, not have the resistance that, you know, places have or...

55:40.320 --> 55:44.800
Yeah, I mean, I wouldn't have come here if it wasn't for having, like, you know,

55:44.800 --> 55:48.560
how is the head of R&D? Because you've got someone who really gets it. And you know, you

55:48.560 --> 55:54.560
got to go, all right, by, by himself, like you said, engineer as well. It's really important to

55:54.560 --> 55:59.680
have an organization that, because it's core to the strategy, like people like, oh, we're going

55:59.680 --> 56:04.320
to have AIML. We're going to do it. And sort of when I came in, I'm like, look, we get, like,

56:04.320 --> 56:08.240
the normal you hire and do people who doesn't work anymore, right? I want any of you people,

56:08.240 --> 56:10.800
and I'm going to give an offer in a few days, like, like, three days. We're going to do it.

56:10.800 --> 56:13.360
We're going to use hacker rack. We have all these different things we bring people in, and we're

56:13.360 --> 56:17.200
not going to do the usual process, so you, they get offered in three months time or three,

56:17.200 --> 56:22.080
you know, a month, because they've got other places to be, right? We want to show them that we're

56:22.080 --> 56:27.280
not this giant, usified organization that we can move fast and do things, right? And, you know,

56:27.280 --> 56:30.320
even to the way we're working, you know, we're going to use Slack. We're going to be distributed

56:30.320 --> 56:34.560
everywhere, we're going to go with talent ears, you know, we use a laptop and a thing we can work

56:34.560 --> 56:39.040
that way. You know, post COVID, I think the rest of the organizations caught up to that. So we came

56:39.040 --> 56:46.000
in and like, you know, did things in a very, very different way, right? And like from the way we

56:46.000 --> 56:51.840
interviewed HR hiring, but also, you know, we always max like this HPC requirements are we're going

56:51.840 --> 56:56.480
to do things and like, and so we built like a whole new process to do things, right? That are

56:56.480 --> 57:00.000
the way we work, right? We work in two big sprints. We have these different types of things.

57:00.960 --> 57:07.920
What's interesting is actually seeing the wider organization kind of been given permission to

57:07.920 --> 57:11.040
think and innovate and like some of them picking up some of those tech techniques and things like

57:11.040 --> 57:14.720
that and the methods of managing science, right? Because I first looked at my biological colleagues,

57:14.720 --> 57:19.040
they're like, you don't stand, but it doesn't work that way. You can't plan things out for like this

57:19.040 --> 57:22.080
could take longer or shorter. I'm like, well, you know, computer science doesn't work that way either,

57:22.080 --> 57:26.080
right? Like we actually like, you know, it's, I love to think that like we know the thing and we

57:26.080 --> 57:29.680
write down all the steps and we just do it. That's not how it works, right? So it's always a garden

57:29.680 --> 57:34.240
of walking paths. So doing that, stopping every two weeks, going what work, what did work? Okay,

57:34.240 --> 57:38.560
now we'll do this. Let's have a really good way of planning and working across really complex

57:38.560 --> 57:44.560
teams as we do. So, you know, we broaden a lot of that culture. You know, there are always people

57:44.560 --> 57:48.320
that are true believers, right? They're like, oh my gosh, ML and AI could do everything. You're like,

57:48.320 --> 57:52.240
well, slow down, you know, let's just talk about what we're doing here. And then there are people

57:52.240 --> 57:56.560
that are super skeptical, right? That are like, well, you know, how is this anything different from

57:56.560 --> 58:01.120
before? You know, this stuff or to the, no, you'll never place a human science, this human

58:01.120 --> 58:05.600
ingenuity. You know, I've got a better way of picking tech. I can synthesize this on my head.

58:05.600 --> 58:10.240
And maybe they really do have an alpha value. Maybe they really do have that. But sometimes a lot of

58:10.240 --> 58:14.880
I'm like, yeah, I think you've got a bit of biorecal bias there as well. So it's attention to trade off.

58:16.080 --> 58:19.680
And like all these things, we'll work out where these, some of these tools, based on the

58:19.680 --> 58:26.480
technology maturity stack, where they're best used and where we're too early, right? Or where we don't

58:26.480 --> 58:31.840
have enough data or the right data, that sort of thing. But it is, it's been fun and it's been fun

58:31.840 --> 58:37.360
just sort of, you know, as the organizations grow and more people have come in. And I think what

58:37.360 --> 58:43.360
people have realized is that, you know, if you're interested in doing machine learning in bio and

58:43.360 --> 58:50.080
those sorts of things that come is like GSK actually have lots of compute. So if people that you,

58:50.080 --> 58:53.920
like, join me with the MIT is like, now I've got more compute and more data than I ever had, right?

58:53.920 --> 58:57.760
And I don't have to write grants and spend all my time doing these sorts of things. And if you

58:57.760 --> 59:02.560
get it right, you've got a whole machine that will translate to an impact, right? To patients,

59:03.280 --> 59:06.880
to really do those sorts of things. And that's, that's really important to a lot of people as well,

59:06.880 --> 59:11.280
is that is that connection to the part of the whole thing, rather than, okay, I'm an academic,

59:11.280 --> 59:15.040
I built some good idea. Now I have to make a start up into the whole thing, and this can take

59:15.040 --> 59:20.800
like so long for my work to get out there and actually influence the world. So those are all

59:20.800 --> 59:26.800
good things. You know, there are certain pluses and minuses to doing things in large corporations

59:26.800 --> 59:31.360
to smaller ones, right? A small start up, we can raise capital, you know, we're all in the same room

59:31.360 --> 59:34.400
or, you know, place that we know what we're doing, we're all in charge of scale, we don't have to

59:34.400 --> 59:39.120
have all this overhead big things. Large corporation takes some time to turn the ship, but once you

59:39.120 --> 59:44.560
can focus that whole thing on something, man, you can really drive, drive on it. So different skills,

59:45.280 --> 59:49.680
you know, certainly the largest corporation sort of thing I've sort of worked in. And but

59:50.800 --> 59:55.200
the organization has to want to do it. I think is, is the less than I've learned. Like, if they're not

59:55.200 --> 59:59.200
really into it, or the senior leadership aren't really into it, or a large fraction, it's not

59:59.200 --> 01:00:04.480
a core strategy. It's a very difficult thing, right? And different companies are in different stages,

01:00:04.480 --> 01:00:08.400
right? Some of them are externalizing it, but we decided to build a really large in-house team.

01:00:08.400 --> 01:00:13.120
What are the, what are the couple three top three things that keep you up at night? Like, what are

01:00:13.120 --> 01:00:22.400
you most worried about in your role? The first thing is being able to generate the data at the

01:00:22.400 --> 01:00:28.640
right kind of cadence, right? So one of the great, you know, if you're in different domains, right,

01:00:28.640 --> 01:00:33.600
you can, depending what you're doing, you can get, you know, high frequency lots of new data generated

01:00:33.600 --> 01:00:38.240
quickly. For what we have to do, you know, for example, if you think of like reinforcement learning,

01:00:38.240 --> 01:00:42.320
right? You know, I've got a simulator of the game or things like that. I get many, many samples

01:00:42.320 --> 01:00:47.120
that can run lots of experiments, right? I'm in BioLand, right? Like, I don't get, you know, 12

01:00:47.120 --> 01:00:53.200
million data points, I get like 300 data points every four to six weeks, and they cost. And by

01:00:53.200 --> 01:00:57.280
the way, it costs us a lot of money to generate the data points. So then, you know, you start to ask

01:00:57.280 --> 01:01:02.320
this question of like, you know, what's my information gain? What's my model performance gain per data

01:01:02.320 --> 01:01:06.160
point for time? Where am I? Am I linear? Am I a plateauing? Like, you know, how many cycles do I need

01:01:06.160 --> 01:01:11.920
to run? I can run 12 cycles a year. I can get so many data points. Is that enough? Right? So it's

01:01:11.920 --> 01:01:17.520
all about, for me, a lot of it is about a, can I'm, you know, am I ever going to be in a generator

01:01:17.520 --> 01:01:22.720
enough data to solve this problem? We're going to have the right data. And another thing is the

01:01:22.720 --> 01:01:26.960
cost of the experiment. But, you know, people will ask like, well, how much data do you need to build

01:01:26.960 --> 01:01:32.080
this model? I'm like, I don't really know yet. You know, I need more, but then we'll start to see

01:01:32.080 --> 01:01:38.000
a trend. But, you know, or am I collecting the right data? So it's really about those learning

01:01:38.000 --> 01:01:43.600
cycles and those sorts of aspects. I'm, I'm really, the other thing that sort of keeps me up

01:01:43.600 --> 01:01:54.240
enough is thinking about the best ways and the ways that we have other data sources and things

01:01:54.240 --> 01:02:01.040
that we generate data, there's lots of historical data and GSK that we pull it together and use it

01:02:01.040 --> 01:02:07.760
in the right fashion, right? And, you know, that we remove, it's really important to, you know, there's

01:02:07.760 --> 01:02:12.240
a patient data about individual people, right? But when we run a trial and we do things with people,

01:02:12.240 --> 01:02:15.520
they're contributing to medical research. And if you talk to a lot of people with the trials,

01:02:15.520 --> 01:02:19.120
they're like, you say, well, we ran your trial and we put it into a box and we, the medicine

01:02:19.120 --> 01:02:23.600
was successful or not, right? And like no one else can touch that data, right? I'm like, that's

01:02:23.600 --> 01:02:27.280
criminal, right? They contributed medicine. There's other things we can learn about it. So what I

01:02:27.280 --> 01:02:31.360
concern about is when we generate data and we're doing things as an organization, how do we make

01:02:31.360 --> 01:02:35.440
stackable data sources to build this like a long-weigh children compass, right? An individual medicine

01:02:35.440 --> 01:02:39.280
for a particular, for say, roomside arthritis, it may fail, right? Which is terrible for patients

01:02:39.280 --> 01:02:43.120
and terrible for us. I drug didn't work. We didn't work as well as we hoped. But the question is

01:02:43.120 --> 01:02:48.080
what do we learn from it? And how do we build data sets that have the same common longitudinal

01:02:48.080 --> 01:02:52.000
characteristics, you know, maybe a common course, I can join them up together and I can build this

01:02:52.000 --> 01:02:57.200
longitudinal corpus of data. And so a lot of time when people are doing an experimental lab,

01:02:58.000 --> 01:03:02.320
they do an experiment and then, you know, they'll analyze that for that particular use case

01:03:02.320 --> 01:03:06.480
and they lose the metadata or it's lost and things like that. So I have to tell people,

01:03:06.480 --> 01:03:10.560
like, you know, you know, build data for future use so you can use it again and also like,

01:03:11.360 --> 01:03:14.880
you know, collect those other data points at additional marginal cost, right? Like they're

01:03:14.880 --> 01:03:23.200
really useful. So those are the things that really keep me up. You know, you know, the final

01:03:23.200 --> 01:03:27.360
thing is really about like, from building models in pathology, we're starting to do things that are

01:03:27.360 --> 01:03:31.760
really like, you know, doing patient prognosis, doing prediction, doing sorts of things, saying

01:03:31.760 --> 01:03:35.520
these persons like their benefit from this medicine or not, as we started going to learn that,

01:03:35.520 --> 01:03:42.480
it's like, we have to be right. And the other thing is we also, we make medicines for everybody.

01:03:42.480 --> 01:03:49.920
The challenge we have is if we're using lots of prior information or I'm relying on a system

01:03:49.920 --> 01:03:54.720
where, you know, I can get digitized medical records or pathology can be uploaded, we can raise

01:03:54.720 --> 01:03:58.640
a risk of building really great AAR advances, but only work for people that are in, you know,

01:03:59.280 --> 01:04:03.040
countries that have the data infrastructure and things like that. So we think a lot about, you know,

01:04:03.040 --> 01:04:07.760
what are the data, what's the data culture? We have culture in the vaccines and medicine.

01:04:07.760 --> 01:04:10.640
What's the equivalent of that? You know, we don't want to build these great advances and say,

01:04:10.640 --> 01:04:14.880
oh, I'm sorry, you know, that's, you know, it's going to take five to 10 years for another country

01:04:14.880 --> 01:04:21.360
to be able to have access to it, right? Those are my top three things. Well, Kim, thanks so much

01:04:21.360 --> 01:04:28.160
for joining us and taking the time to share a bit about what you're working on and how you think

01:04:28.160 --> 01:04:33.120
about the problems in your space. Thanks, Sam. It's been a lot of fun. I'm a big fan of the

01:04:33.120 --> 01:04:43.600
cobs. Cheers. Thank you so much. Thanks so much.

