Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.
Alright everyone, I am here at CubeCon in San Diego and I'm with Bob Killan. Bob is
a research cloud administrator at the University of Michigan. Bob, welcome to the Tumel AI Podcast.
Thank you. Awesome. So we met yesterday. You were on the AI, ML, media and analyst panel
talking a little bit about your experiences using Kubernetes to support the researchers
there at the University of Michigan. And you're also one of the other roles you play as
your co-chair of the CNCF's research user group. So really interested in hearing a little bit about
your experiences with sounds like you work with Kubernetes kind of to support a broad portfolio
of applications, not just ML and AI, is that right? We run a wide variety of applications,
both, you know, AI and ML focused like Qflow, but we have a whole slew of other supporting
services that our researchers consume generally on our larger HPC cluster. Tell us a little bit
about your background. How did you get started working with Kubernetes? I moved over to ArcTS,
Microbe Advanced Research Computing and Technology Services about three years ago,
but before that I worked for the University of Michigan Hospital. And in 2015, we started,
you know, going down the container route for a lot of our clinical workflows. This is sort of
before Docker was a thing. So we were predominantly like Alexi containers. And we were first looking
at Kubernetes to orchestrate that, but Kubernetes wasn't mature enough at the time and we went
for mesos. From there, we then shipped it back and forth and also started supporting our research
workloads, sort of alongside our clinical workflows on there. They sort of saw, you know, some of the
benefits of containerization and just wanted to take advantage of the broader service that we were
offering. From there, I was hired by ArcTS to sort of build out the same thing, but be completely
research focused, as well as like managing a virtualization system. And Kubernetes has really
exploded when it comes to the broader management and adoption of containers. And you know, we have
things like the Cube Sponor in Jupyter Hub that just integrates directly with it. And from there,
it's just you said the Cube Sponor. Cube Sponor. I'm sorry. Cube Sponor in Jupyter Hub. So I can
spin up a notebook as a container itself. Can you maybe talk a little bit about the various
use cases that you're supporting? What are some of your researchers doing? Oh man, it's a pretty
broad spectrum. On top of Kubernetes, we support a bunch of social science stuff, various databases
that are consumed by other applications, and people running workloads in our larger HPC cluster.
And we do support, we have a significantly large Jupyter environment, a lot for classes,
and other researchers that are spinning stuff up. Bioinformatics, physics, I think those are
most of the big ones. Okay. And so is the primary user experience among your researchers, the
Jupyter notebook, and the ability to spawn off containers from the notebook environment? Yep.
Over time, we've seen a gradual shift of people, you know, sort of the moving away from the classic
HPC style system where you would log in to like, there'd be a sage and a login node and you'd run
your system or you'd queue something up and run it off. Now there's sort of an expectation
for to have the Jupyter notebook or to have some other sort of science gateway for the user to
consume. It's a lot, you know, friendlier than having to dig in and write like a, you know,
batch script to automate some of the stuff for you. One of the things that's different between kind
of the classical HPC scale-up systems and the more distributed environments that we see now and
that are common with Kubernetes is that in order to take advantage of these distributed environments,
the users in your case, the researchers have to know about them and kind of know how to use them
in a lot of cases right there, you know, code to take advantage of distributed computing.
Are your researchers, you know, doing that or are there, you know, things that you've put in place
that provide abstraction so that they don't have to think about that? We do have some abstractions
in play that make it much easier. Argo workflows and things of that nature have simplified things
greatly for people. Then, of course, there are some power users that, you know, we'll just want
to dive in and do everything themselves. But in general, we have a pretty good suite of tools and
libraries to make that easy for them. The social scientists isn't kind of the target user for a
cube flow. You know, yet they're, you know, using Kubernetes cube flow in your case, it sounds like
the Argo workflows, you know, are what you didn't say that necessarily the social scientists were using
the Argo workflows and cube flow. But, you know, I'm curious to hear a little bit about how cube flow
in particular is used in this environment. Cube flow itself, we don't have any social science people
using cube flow. The cube flow stuff is mostly used by a small subset of people. And right now,
it's a more in an experimentation stage. But the aspects of it really sort of make, especially
like the model lifecycle of things, significantly easier for people. And we sort of see much more
people shifting and adopting it going forward. Maybe talk a little bit about that user experience.
Like, what did they have to do to get their apps up and running in the cube flow environment?
They don't have to do too much. They can do most of it from the Jupyter notebooks that
cube flow spins up and manages for them. And then there's, I forget the Argo or the cube flow
CLI tool. But that allows them to create some workflows pretty easy too. And there's some other
things being built that allows for like better like designing building and building better pipelines
and more consumable workflows. Maybe talking generally what from your participation in this panel,
I got the impression that you're pretty excited about Kubernetes as a platform generally for
these types of research workloads. Maybe talk a little bit about why you're excited about it.
Sure. Kubernetes provides a lot of like proper abstractions for just managing a variety of
workloads. And the things that it cannot handle, it offers the extension points to easily extend
and augment. So like Kubernetes itself does not have any capability of running a like standard MPI
job or a, you know, gang schedule job. But because the extension points are there, it's easy enough
to sort of write your own scheduler to handle that. And it's just like a little, you know,
one variable changed, then use that scheduler to spin up and manage that workload. And by using
Kubernetes itself and this extension mechanism, you gain the accessibility of using everything else
that's being developed on top of Kubernetes. Whereas in the past, you know, again, going back to
the classic HPC system, there wasn't any of these other, this other tooling and APIs to really do
any of this stuff. So now we can get, you can do a lot more such as like a venting and triggering
different workflows off different things happening within the system. We're definitely seeing
them a much more broader adoption of things like data streaming and flink and, and you know, this
stuff you can really, you know, do well on a classic HPC system. But it works and fits quite
well on top of Kubernetes. And you can integrate it with a whole slew of other things that are being
built on top of it. Are you supporting users that are doing things like building out their own
schedulers or are these things that the broader community is doing and they can just kind of take
off the shelf and take advantage of? Right now, it's mostly from the broader community and our users
can then take that off the shelf. Cubeflow itself has a MPI operator built into it so that, you know,
makes it easier for the people consuming Cubeflow to spin up an MPI job. And then there's other things
like a volcano which are trying to offer much more of the classic batch computing things that we'd
find in HPC for Kubernetes directly. Joe down a little bit deeper into that. What is volcano
specifically? Volcano is a open-source project that is trying to be sort of the classic HPC
scheduler workload managers thinking like Slurm but built for Kubernetes. So it is offering
things like fair share, cues, backfill, as well as offering things like, you know, being able to
support game jobs and things of that nature. So the idea there being that as a research cloud
administrator, you've got some pool of resources, you've got some pool of people that want to
take advantage of these resources, how do you fairly or consistently give them access to the resources
and so, you know, fair share being, you know, one idea there, gang scheduling is more like,
I've got these five things that need to go at the same time. Exactly. How do I ensure that they're
happening in concert with one another? Yep. And then like, if a worker happens to die, you don't want
to necessarily kill the entire job, whereas, you know, if sort of the controller dies, you want to
then, you know, kill and re-cute the entire job for us and for sort of the larger sites that are,
you know, still primarily on-prem and focused on like bare metal, you know, we have a finite
amount of resources. We aren't necessarily bursting up to the cloud. So being able to backfill and
set priorities on things is very useful for us as well as, you know, cues. For us, it is like,
we want to backfill with, you know, jobs from students. They might be able to run those for free,
but if we have a, you know, researcher that's queuing something, we want to, you know, have those
take priority and sort of bump off those lower priority jobs. And we think about in the context of
machine learning, some of these jobs, like neural network training, deep learning training,
they can take, you know, many, you know, days or weeks. But that's not a new thing in the context
of HPC are the workloads that you're attending to support also, kind of these long running jobs,
but not necessarily deep learning training. Yeah, we have, we support, again, being a,
a, that handles the research needs for the entire university. We have a little bit of everything.
And most of our jobs would probably fit into that category where they, you know, might take,
you know, a week to complete or something or something of that nature. I think our max wall time
is 21 days. Oh, wow. And can you say what that is? I don't remember off the top of my head.
But we've had several that are just like cranking through, you know, terabytes and terabytes of
data. And if they have something that just, you know, it's the wall time, you know, it's unfortunate
for the researcher, but it will kill it off. Oh, so this max wall time isn't the biggest job
that you've seen. It's the limit that you've seen. Sorry. They have something that runs longer
than that. We're going to give them. Yeah, nice, nice, or not nice. Yeah.
Ask this question yesterday that you disagreed with, or that you disagreed with the premise,
and that that was that, you know, one of the things that I've, you know, noted here in
this conference, the conference has gotten huge. I was at, I don't think I was at the first one
in San Francisco. I may have been, but I was definitely at the second US one in Seattle. And
that was 1500 people. And now it's 12,000. It is massive. And, you know, the kind of thought that
I positive was, you know, that growth is not in machine learning and AI users. It's in like
cloud native, you know, I'm building web apps users, right? And those users have very different
concerns, potentially, you know, than the ML and AI users. And I kind of pose as a straw man.
Like, is it possible? Or is there, you know, any concern that, you know, that growing
kind of user base and momentum, but focused on kind of a different use case would have negative
impacts on the ML and AI users, like, hey, you know, to do X, Y, Z and Q flow, you need feature
or API, ABC, but that's a super low priority because none of the web app people need it.
Yeah. And you were like, yeah, no, that's, I'm not worried about that. So yeah, reactive that
a little bit. Some of that stuff, like, is there. So there's like still some issues with, like,
Numa and some of the more, yeah, it's the essentially mapping of RAM to CPU core. Okay. And when
you're scheduling a job, you really don't want to like cross Numa domains. So you really want to,
you know, and there'll be like GPUs and things like that that are also sort of tied to that. So
you don't want to have a pod scheduled on like this other core that then has to sort of jump and go
through a round to a different like Numa domain and path to talk to a GPU. And so some of that stuff,
those parameters aren't exactly there. But at this point, there's, you know, some pretty strong
drivers for that, a lot from Intel and Vidya and those groups to sort of, you know,
firm up those lower end components. And then some of the other things, there's like enough sort
of extension, extension points where the core of Kubernetes might not be able to, you know,
cater that well to a machine working AI and machine learning workloads. But an outside developer
could, you know, hook into it or replace that component fairly easily and then enable it for,
again, more research workloads. Yeah. Basically, the whole panel said essentially, yeah,
no, you have to, there's nothing to look at here and don't worry about that, which was a little
less unsatisfying. But I think the key takeaway was that the belief was that the Kubernetes core
is general enough and has enough extension points that the specific use case, you know,
the, the dominance of the web use case, you know, isn't necessarily going to derail its use in,
in other cases. And I think you or someone else mentioned it and even, you know, there's a lot
of attention paid to, it's not just the web use case isn't as homogenous as I'm making it sound.
There are a bunch of use cases there. And like, there's enough diversity in workloads that the,
the core is kind of staying general and extensible. You're shaking your head. Sorry. I probably should be
answering. No, no, go ahead. Go ahead. Yeah. The core, the people behind the core, you know,
Google board going all the way back, definitely built it in mind to be extendable. They knew from
the get go that, you know, lots of things might come and go in and if you sort of really tied it
to one, you know, specific design decision, you'd limit yourself in the future. Because you really
wouldn't know, you know, what's going to happen in a couple of years. You know, who knows, you know,
um, co scheduling or, you know, gang might actually make its way into Kubernetes core, but right now
that's not really a priority, but it's easy enough to extend and add through, you know, the various
extension points. What are the biggest gaps or things missing, you know, as you are trying to support
these AI and ML workloads? The biggest gap from a general adoption standpoint, I would say is
documentation of best practices. There's, you know, some blog posts and things like that out there,
but there is no concise guide when it comes to how do we design and build a cluster? What setting
should we set for this sort of thing? As part of the research user group, we've pulled, you know,
wholesale of users and, you know, we pulled them actually like live the other day during the session.
And documentation and just guided best practices was the number one request, both times,
and by a wide variety of groups. I've heard similar, by the way, and one particular point
that was raised was, you know, for the documentation that does exist and the tutorials that do exist,
it's all very GCP focused. And so, you know, there's a whole other set of mental exercises and
translations you need to do to get it up and running on AWS, for example. Yeah. So besides from
documentation, besides documentation from, I would say, broader standpoint, it's a lot of those,
the things that like volcano we're trying to add from a base functionality standpoint. So again,
things cues, backfill, better pod priority and preemption capabilities and things like that,
at least to support a better multi-tenant or I would say a multi workload environment.
And what's the state of GPU support for in Kubernetes? Is it as flexible as you need it to be
to some kind of a virus? Right now, you can't share GPUs like you can in some other environments.
So that's that's a little bit of a problem. I don't know the stats of it. I know there's
like an open issue on it, meaning you may support fractional CPUs or, you know, cores,
you'd like to be able to offer users, fractional GPUs. And then there's a like on some of the
NVGA GPUs, you can sort of like carve them up and give like, you know, half of it to a container.
But that flexibility still isn't like, isn't really firmed up yet.
It sounds like that's something that would have to come through Kubernetes core as opposed to an
extension. Or I think that would be probably through that might be through Nvidia and their
driver. I'm not 100% certain. Okay. I'd have to go back and look. There's there's a long issue
that's been open. I think since early 2018, where there's been a lot of discussion on in sort of,
you know, who should handle this and where should it be done? Okay. I can dig it up later too.
Sure. Yeah. I mean, you know, curious, be curious to kind of read through its evolution. So GPU
support, your role is kind of managing the Kubernetes cluster, kind of independent of these
use cases. How many kind of machines, containers, nodes, are you dealing with?
Right now, we have about 20 different research groups that are using our clusters.
We have three separate clusters, one that we sort of use that has a bunch of our persistent
services that we consume. And they provide services to the other clusters, so things like
identity management and central point for logging. And then we have another cluster that is for
our non-restricted data workloads. That's our relatively general use cluster. And I don't remember
the stats off the top of my head, but that's about like, honestly, it's actually pretty small,
about 14 nodes and probably 800 gigs of RAM. I don't remember the cores off the top of my head.
Okay. And then we have another, our restricted data cluster. We do have, it's a little bit smaller.
And we don't want the end users like talk to that one or consume that one directly,
but we will spin up and manage workloads for them. That's mostly for healthcare.
I kept a type of, okay, got it. When you think about the AI ML workloads that your users are asking
for, where do you see the, where do you see things going from a user perspective? What do they
want to do relative to where they are now? What are the things that they're asking about? What are
the things they're curious about, excited about? Honestly, it's mostly extending in better
integration like notebooks into the various workflows. Because we also have a classic HPC system,
it's very hard to integrate that with the Kubernetes native ecosystem of tools.
We've had and sort of been experimenting with a little bit of bridging the two worlds,
because our HPC system has like 30,000 cores and dramatically larger than our Kubernetes environment.
But it doesn't have like the, well, it sort of does, but it doesn't have like the ease of use
that Kubernetes and our front end does. So we've been doing a little bit of like
syncing POSIX identities, sort of the Linux UID and GID and mapping to our parallel shared
file system underneath the hood that's consumed by both services. And for our users that has been
sort of like the the next big evolution. And I know like some of the like national labs and
other larger sites are also having that same issue. And maybe kind of last question,
going back to this, you know, documentation being the the biggest gap, like how do you think
that gets solves in a community like this? Well, there are several groups that have some internal
documentation and they've sort of out some best practices and there's been a couple of good blog
posts. The open AI blog post, I think from about a year ago regarding like scaling Kubernetes and
running ML workloads on 2500 nodes was a really good one on some of the the issues that you'd run
into that sort of thing. Yep. And I did an interview with one of the folks on that team will drop a
link in the show notes, but continue. Cool. And like I've been talking to some of the folks that
like certain they have some they've teased out a lot of really good best practices. For now,
it's mostly been like time and sort of bringing it all concisely together. For now, it'll probably
be, you know, scrubbing it of any, you know, related, you know, internal related documentation and
then like dumping into the gate, get repo. There's a get repo for the CNCF research user group. And
that's sort of, you know, our collection point of all all this various stuff. And once it's there,
we'll sort of try and, you know, hit it again and make it a bit more concise.
It sounds like a lot of the need is broader than, you know, just research users. It's everything.
Yeah. Right. And there's a lot of people that still like don't necessarily how to run Kubernetes
well. And then definitely heard that a lot here at CubeGun. Yeah. Yeah. You mentioned Sarn. They're
kind of noted for using OpenStack for the infrastructure management. You use OpenStack.
We do not use OpenStack. We use it's a proprietary virtualization product that was donated to
the University of Michigan. It's by a local Michigan company. Cool. Well, Bob, thanks so much for
taking a few minutes to chat with us. Sure. Thank you. Thank you.
All right, everyone. That's our show for today. To learn more about today's guests or the
topics mentioned in the interview, visit twomelai.com slash shows. For more information on either
of our new study group offerings, call zone modeling and machine learning or the IBM enterprise AI
workflow, visit twomelai.com slash learn 2020. Of course, if you like what you hear on the podcast,
be sure to subscribe, rate, and review the show on your favorite pod catcher. Thanks so much for
listening and catch you next time.
