Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington. Let me start today's show with a huge thanks to everyone
who's taken the time out to vote for us in the People's Choice Podcast Awards. Voting ends
tomorrow, July 31st, and we are extremely appreciative of everyone who's voted, commented,
shared, or otherwise. We'll be sure to share the results of the voting as soon as they become
available. If you haven't already cast your vote, please, please, please visit twimbleai.com
slash nominate to do so. In this episode, I'm joined by Laura Lail Tai Sheh, 2017 recipient
of the prestigious Sophia Kovalev Skaya Award and professor at the Technical University of Munich,
where she leads the dynamic vision and learning group. In our conversation, Laura and I discuss
several of her recent projects, including work on image-based localization techniques that
fuse traditional model-based computer vision approaches with a data-driven approach based
on deep learning. We also discuss her paper on one-shot video object segmentation and the broader
vision for her research, which aims to create tools for allowing individuals to better navigate
cities using systems constructed from visual data. And now on to the show.
All right, everyone. I am on the line with Laura Lail Tai Sheh. Laura is a professor at the
Technical University of Munich, where she leads the dynamic vision and learning group.
Laura, welcome to this week in Machine Learning and AI.
Hello, Tim. Nice to be here. Why don't we get started by having you tell us a little bit about your
background and how you made your way into machine learning research? Yes, so I studied actually
not a role machine learning or computer vision. So I studied telecommunications engineering
back in Barcelona. And then I went to do my master thesis at Northeastern University in Boston.
And that's the first time that I took a course in computer vision. And so that's where,
let's say, my passion for this field started. And then I decided to pursue a PhD. So I went back
to Northern Germany to do a PhD in Hanover. And during the course of my PhD, machine learning
and especially deep learning started to become really, really popular within the field of computer
vision. And towards the end of my PhDs, when I started to focus a little bit more or try to
get into machine learning and understanding how could this be useful for the tasks that I was
dealing with here in my PhD. And after that, I went to ETH in Zurich for a couple of years,
for a post-op. And after that, I moved to Munich, also for another post-op, until well,
until recently, where I was giving the wonderful chance of becoming a tenure-track professor
also here in Munich. Great. What was the focus of your PhD research?
So I was working on multiple object tracking. I started with a project where we had to track
algis. And this was for to study essentially how algis get stuck or get attached into the
surfaces of ships. And so basically, they ruined the surfaces. And so the whole idea, this was
together with the department of chemistry and physics. So they were really studying the
possible materials that could repel these algis. And within this project, my goal was essentially
to analyze the way these algis moved. And of course, they had tons and tons of video data.
And they could not just manually follow each of these algis manually. So they wanted to have
an automatic tool for analyzing this large amount of data. And so this is how I started
with the topic of multiple object tracking. And then I moved a bit more towards
people tracking because the motion of people were more interesting. There was the whole
goal of analyzing the pedestrian motion, for example, for autonomous driving applications.
And so this is where basically I shifted more towards crowded scenes, analyzing people interaction
and using these interactions for better prediction of people motion.
And more recently, you've been working on visual localization. Can you tell us about that
problem and your approach to it? Yes. So visual localization is an entirely different problem.
So here, what we're interested in is estimating the position of a camera in 3D within a given scene.
And you can imagine that this given scene can be either a building for indoor localization
or an entire city for outdoor localization. And this scene, this model is essentially
reconstructed from images using standard computer vision techniques, such as structural
promotion. And from this, you can just crawl a bunch of images from flicker and reconstruct,
for example, entire cities in 3D. And so now the idea is that you're walking around that city
and you take a picture of some building and you want to know exactly where your camera was located
when you took that picture. And this has applications in, for example, augmented reality or robotic
navigation where you really need a very precise localization of your camera. And so this is not a
new field at all. So it has been tackled in computer vision for a really long time. But recently
people started tackling this problem, of course, with deep learning. And so we were looking into
this with a master thesis here at TUM. And we actually realized that people were kind of
forgetting all the classic research in these fields. So they were just blindly taking neural
networks and applying it to this problem where the input was an image and the output was directly
the camera poles. So what we're, what we're working on now is to kind of fusing the classic knowledge
of geometry, multiple view geometry, feature extraction and all the knowledge that was gathered
in something like 10 plus years of research in visual localization and trying to fuse this with
the data driven or deep learning methods. And within this project in particular, what we're doing
is not actually trying to extract an absolute pose of the camera, which is what methods like
PostNet are doing, but actually estimating a relative pose between a camera that you know
where dislocated because you used it to create your model and this new image that you're capturing.
And so this method is actually much more flexible because you can go anywhere, take a picture,
and as long as you have some other pictures around you that you know were used to create your model,
you can actually localize yourself, which is not true for PostNet where you actually need to
retrain a different neural network for every specific scene where you want to localize yourself.
And so we have, we have been working towards this goal of relative pose estimation with neural
networks and including some geometry, multiple view geometry information. And this was,
this is the work of one of my PhD students who also she recently started and it's also
together in collaboration with Thorsten Sadler from ETH Zurich. And the ultimate goal is,
of course, to handle more and more complex scenes that could not be handled before with classic
methods. And by more complex scenes, what I mean is, for example, dynamic scenes. And this is
where my expertise in multiple object tracking comes in. So if you are actually observing a city,
it's very easy to localize your camera if your city is empty. And so you only have the buildings
and the static parts in there. But as soon as you have crowds of people and all these dynamic objects,
they just, they are not good for your localization. So they're basically noise for your localization.
So we want eventually to build a pipeline that is robust enough to handle these dynamic scenes.
One of the things that you mentioned is a bit of a recurring theme here on the podcast. And that
is this idea of fusing deep learning based methods, CNNs, I imagine, in this case, and more
traditional methods to kind of inform those CNNs. And there's, there seems to be a tension in the
in the research about whether an end-to-end deep learning approach is, you know, better given
sufficient data and sufficient compute or whether we should try to incorporate the, you know,
the various things we've learned about these various problems we're trying to solve or the
physics of given situations. Maybe talk a little bit about the model that you're building to
fuse those two types of approaches and how you've gone after that specific part of it.
Yes, so that's actually a very, a very good question because it's really a central point that
is, that is coming up more and more in discussions within the community. So I think there was a
really big shift from these model-based methods to data-driven methods, fully data-driven.
And in theory, if one has enough computational resources enough data, this is the perfect setup
because you have a universal approximator that can just express any function that you want to find.
So this in theory is perfect. The problem is that data is very costly to obtain and so no people
are talking about constraining your deep architecture with some previous knowledge as you said,
like, like, physics or in our case, multiple view geometry. Now, the interesting question is,
where do you put this information? So do you put it in the loss function? Do you constrain your
activation maps to have certain shapes? Or you can also, for example, actually make your
architecture more modular so that each of the different parts is interpretable and has,
for example, physical meaning. And this is still a question that there is not entirely solved,
right? So this is something that people are actively researching and we don't have the ultimate
answer. So how we did it for the Visual Localization project is essentially,
we propose to mimic the pipeline of the feature matching that is also present in classical
Visual Localization methods where you have two images. You want to find the relative pose between
them. And essentially what you do is you compute the series of features, then you try to match
these features and find a pose that explains these matches in a coherent way so that the geometry
is okay for most of the matches. And this pipeline is actually very robust. And one of the problems
that it has is that this feature extraction step was completely handcrafted before. So people
were using sift features or surf features. And this worked really well for many scenes, but they
didn't work, for example, indoor. What are sift and surf features? Oh, these are so sifter scaling
variant features. They're essentially feature descriptors like a bit like corner detectors. And they
just, they tell you where an interesting or let's say a defining feature of your scene is.
And what is it's descriptor? So you attach a vector to this particular feature. So when you look
at the same scene, maybe from a slightly different angle, you get for the same point a very similar
descriptor. And then you can say, oh, these two points are actually the same 3D point. They are
just seen by two different from two different positions. But the point is that this descriptor
should be very similar for the same pointy 3D. So that the sift features are playing a role
similar to like an edge detector and classical object detection. Yeah, for example, yeah.
You mentioned a few different ways to incorporate this geometry into a pipeline.
How does the way that you've chosen to do it map up to some of the general ways that you
mentioned incorporating into the loss function, incorporating it into the shape of activations?
So it kind of approaches two sides. So one is definitely the loss function. The loss function is
acting directly on the essential matrix prediction, which is the matrix that gives you the relative
pose between the cameras. And the other thing is the the actual architecture. So we're making the
architecture modular in in the sense that since we're mimicking or trying to mimic this this
classic feature matching pipeline, we try also to place the architecture to design the architecture
so that each of the parts mimics this this visualization pipeline. So we know that in the first
CNN, you're going to extract some features that are going to be representative of your scene.
Then you're going to have a matching step and then you're going to have an essential matrix
or relative pose regression. And and this kind of dividing your neural network into different
interpretable parts is is also one way to first of all understand what's happening with your
neural network and second of all includes some information, some hard coded information in the middle
if you need to. You mentioned the essential matrix. What is what is that and what's the role that it
plays in this pipeline? So the essential matrix gives you essentially the relative pose between
two cameras. So it tells you how can you go from a point in one camera to the exact same point
but a scene from the other camera. So it basically transforms your coordinates from one camera
to the other. And this is this is all that we want to find. So once you have relative poses,
then you can localize your camera. You can create all these maps, these 3D maps from images that
I was talking about. And this is this is essentially what we want as a result from our algorithm.
Okay. And so how how do you characterize the results of this approach relative to the alternatives,
you know, both traditional as well as entirely data driven approaches?
Well, compared to other data driven approaches, one big difference is that most of the
approaches are still tackling the absolute pose estimation problem, which means that you have
a very well defined scene with an origin, which is the world coordinate of that origin,
of that scene, sorry. And then you you localize yourself within that world coordinate,
which means that suddenly now if you have another scene, this other scene will have a completely
different world coordinate. And so what you're going to do is you're going to have to train one
neural network per each of the scenes where you want to localize yourself. And this is not
something that we want to do. Of course, we want to build a system that is able with a single
network to localize yourself anywhere. And this is one of the key differences of our proposed method,
which actually tackles relative pose. So you can localize yourself everywhere with a single network.
This is one huge difference. And compared to classic visualization, we also showed in an
in an ICCV paper last year, 2017, we showed for the first time a comparison of classic methods versus
deep learning methods. And we showed essentially that that classic methods are much much more accurate
by by even an order of magnitude sometimes in some scenes, which means that well deep learning is
still not there yet. But an advantage that deep learning has is that it can handle, for example,
large indoor scenes, which have a few texture. So for example, large textureless surfaces like
white walls or repetitive structures. For example, in a building, you have all the stairs that look
the same, all the doors that look the same. And sometimes even as a person, if you go, for example,
to a hospital, it's hard to to know where you are in the hospital. It's very easy to get lost.
And this is the same for for classic methods where they focus on on these basic features,
and then they compare to staircases and they don't know which is which. And deep learning methods
are very, very good at capturing other subtle features that help them to localize better in this
repetitive indoor environments. So this is one real strength and where we see really the the
application for for deep learning methods. And this is why also we proposed a new data set called
TUM LSI large scale indoor localization data set. And this we also published last year in our
ICC paper. And this is a really, really challenging task to localize yourself in those indoor
environments. What's your intuition for why deep learning is so much better at that particular
type of environment relative to classical approaches and yet worse overall in the traditional
types of environments? Well, the key is that classic methods are based on these features that I
was talking about before. So sift or serve. And these features can only be present when you have
let's say characteristics parts in your image. So if you have a white wall with no corners,
no edges, then there's not going to be any feature on that white wall. Well, if you go for example
to marine plots and you have this this beautiful building, which is the city hall, it's full of
tiny details, tiny corners. And so you're going to have really many, many features firing on
that building. So when you have a localization pipeline that is based on those features,
it's really easy to localize yourself in marine plots, for example, because you have many, many
features to base your localization on. But if you just see a white wall and you have no features,
then you have nothing to compute your camera position, because there's no features, so there's
no matching happening. And this matching between 2D features and the features in your 3D model is
the key to the visual localization pipeline. And so deep learning looks more at the overall picture,
so looks at the white wall, but also maybe at the column on the right, maybe there's some chair
also in this scene, and try to look at the whole picture and give you a descriptor for the whole
scene. So even if you don't have many corners, or if you have like a white wall, you can also use
the fact that there's a white wall without features in there to localize yourself. And this is
something that classic methods don't do, or at least classic methods based on sift and serve
type of features. Now you've got in this architecture, you've got kind of this, I guess what I'm
envisioning as a feature identification or extraction stage or module or something along those lines.
And then once you've done that, are you reusing existing neural network architectures to perform
some of the rest of the pipeline, or is it all kind of new architectures?
No, so actually the feature extraction part is based on well-known architectures. So we tried
a Google net, we tried ResNet, currently ResNet is what works best. And so this essentially
is pre-trained on ImageNet, so it gives us a really, really good initialization for our ways.
And how we use it is essentially we're training to go from the image to a descriptor, a vector
of a certain number of elements. And for this, we do reuse these classic architectures. And then
the rest is trained from scratch. And how do you determine what the dimensionality of this
feature vector is? Well, this is a bit of a trial and error, right? So right now we're using,
we tried using something like 2000, I think it was 2048. We also try using 4000 or even smaller.
And in the end, you kind of do trial and error and see what works best for your problem.
And then taking a step back to the broader problem you've got, these kind of many images of these
worlds or environments from different angles. And you've talked about the relative performance
of these different types of approaches for kind of interior scenes versus exterior scenes.
Are you training models that are good at one of these environments or are you somehow
integrating these and starting to look at models that can handle different scales or different
transition from one type of environment to the next? That's actually an excellent question.
Ideally, so theoretically our network can handle any type of environment, indoor and outdoor.
But in practice, the best thing is to train an network for indoor and an network for outdoor.
So this is, in my opinion, still OK, because you only have two types of networks.
And currently, the bottleneck is actually the step before the one where you actually train
your network, which is to find pairs between your image, so the image that you are taking
a test time and the training images that you have around you. So of course, you can imagine that
you are in this city and you cannot evaluate all the possible training images that you have
from the city and that you use to reconstruct your model. And so you have to prune all these
images. And so a test time you arrive there, you take a picture and you want to find, let's say,
the 30 pictures that are most similar to your picture. And so for this, we use yet another CNN
architecture. And it's basically solved the problem that is called image retrieval. So it retweets
images that are similar to your own image. So for example, you want to localize yourself in
Munich. It doesn't make any sense to compute relative poses within your images and the images
from Paris or London. So this network is able to tell you, look, these are 30 images that are
most likely located in the same place where you are roughly speaking. And then you can compute
the relative pose between these 30 images and your own query or test image. And the problem is
that the CNN for image retrieval that works for outdoors doesn't work so well for indoors.
So now we're trying to figure out whether we need to retrain this network completely different
and just use two networks, one for indoor one for outdoor. Or if we can actually reuse some,
some of this network that actually works so well outdoors. So this is kind of what we're researching
right now. There's also a part of this that is related to scale and maybe it's kind of the same
answer and you've addressed it. But when I think of, you know, these outdoor images, you mentioned
at a square, you know, the camera tends to be a lot further from the buildings. Whereas if I'm
navigating a hospital, you know, corridor, the camera tends to be very, very close, does just
this, the idea of having two models address the scale issues or do you run into scale issues?
You know, for example, depending on the within, you know, either of these two types of environments.
So actually, we have to fix the scale of our essential matrices. So essentially, what we do is,
is we take the translation and we give it, we force it to have norm of one. And with this,
what we're essentially saying is we're fixing the scale of our essential matrix. Then we train
the network to predict only essential matrices with this particular scale. And then at a later
stage, we will triangulate from all the training images and the relative poses. We will triangulate
the real scale and the real pose of the image at test time. And so with this, actually, we can
handle varying scales. So it's not to say that the network is very robust to all the range of
scales. So this is also something that we need to dig into. But theoretically, you can actually
handle any type of scales. And how about generalization? How do you explore the generalizability of
this approach? That's an excellent question. Because what we have recently observed is that the, so the
more one of the advantages of our method is that since we're predicting relative poses and therefore
we can use any image from any scene. Now we're not bounded to one scene. We can use much, much more
training data. And we have observed that the more training that they use, of course, the better
your localization is. So first it gets worse, but then it gets better. But still, you do need to see
some examples from your test scene to be able to localize yourself properly there. So this is
actually something that we're trying to figure out. Why is that? Because technically, the generalization
part of the network is not perfect yet. Because ideally, you would want to train on a set of scenes
and then have a completely new scene that the network has never seen before and be able to
localize yourself there also. Well, this is not entirely true yet. So it does do a fairly good
job, but it does a much, much better job when you see at least some images from that test scene.
And so the generalization is still something that we're looking into.
Really interesting, really interesting. So you're also doing some work on object segmentation.
Is that related to this project or is it a totally separate effort?
Well, it's all part of the interest of my group of the dynamic vision and learning group,
which is to actually be able to analyze the dynamic scenes around you. And this, of course,
englobs a bunch of problems. So ranging from the multiple object tracking problem where I start
my PhD to the visual localization, which, of course, we're working also on video localization.
And to be able to fully analyze what's happening around you and also analyze it in time.
So not only image-based, but video-based, you also need to perform, for example,
video object segmentation. And this is one project that we are also doing in collaboration with
people from ETH Zurich. And in this particular project, what we wanted to do was we wanted to
tackle the problem of supervised video object segmentation. And by supervised, what I mean is that
you're given in the first frame of your video, a perfect mask that expresses which object you
want to follow, which object you want to segment in your video. And so your goal now is to segment
this object to find out which pixels in the following frames belong to this particular object.
And so you can imagine that this is kind of easy if the object doesn't move much, if the object
doesn't change the appearance much. But as soon as the object, for example, turns around and now
you're seeing a completely different side of the object or the illumination changes or the position
of the camera changes, these are all big challenges for this problem. And so what we proposed in a
work that we presented at CEPR 2017 already is to actually do this fully data driven.
So there is this wonderful data set created by Disney Research and ETH people that is called
the Davis data set. And this contains really accurate segmentation masks. And by accurate, I mean
sometimes crazy accurate where you see, I don't know, the hair of horses segmented out one by one.
Oh wow. Yeah, it's really crazy. And so with this now, you have like this wonderful source
to train your networks. And so what we propose to do is to actually, so the key idea in that paper
was actually the training scheme. So it's not a classical training where you just input your data
and you get your output. But first you take this network which is a classic convolutional neural
network. You kind of pre-training for the task of video object segmentation. So you give it a
bunch of of masks of object and you train your network. But what is happening now is that your
network is trained to do some sort of foreground background separation. But it doesn't know which object
you actually want to follow, right? You just have trained it with a bunch of objects. You can segment
a bus, a car, a chair. So there's all these types of objects in your data set. And so now you still
have to tell your network which object you actually want to segment. And so how we do it is in a
second training stage, we use this first mask of the object that we do have available. And we
kind of overfeed the network to that mask. So we say, really learn the appearance of that particular
object. So overfeed your network to that particular object. And then the only thing that you need
to do is you run your network for the rest of the frames. And it will be able to segment your
object for the rest of the video. And this was actually a really, really novel way of doing the
object segmentation. And there has been a bunch of works that have followed later this paradigm.
And this was presented at CBR 17 and it's called Osvos for one shot video object segmentation.
And the idea is that, so why we call it one shot is because you only see the object once in
this first frame. And then you're able to segment it and tracking over the whole video.
I'm trying to wrap my head around how you would go about the overfitting part of the training.
And maybe the question that I'm coming to is, is this process something that is
automatable or something that you can build into an automated process? Or is it a very manual
process that needs to be supervised by a researcher or practitioner in order to work?
No, essentially, this is completely automatic. So the only thing that you need is the
mask of your object for one of the frames. And then you pass this mask as a training example
to your network. You do all sorts of data segmentation, for example. And you train your network for
really quite some iterations. And during this process, essentially what your network learns is
what is the appearance of this object and what is the appearance of the background. And this is
completely automatic. So as long as you have the mask of your object, you're good to go.
Of course, the bad thing or the small drawback is that if your background or your object change
the appearance too much. For example, a new object appears in the scene or a second object that
looks exactly like your first object appears on the scene that is going to be a problem.
So we had this sequence, the famous two camel sequence, where you're segmenting this camel that
is moving along. And then a second camel appears on the scene. And of course, the second camel looks
exactly the same as the first camel. And so our network suddenly says, well, this is also my
object. I also want to follow this. And so it segments the two camels. But now the problem is that
you only want to segment one camel. So what you can do is is provide another training image to your
network where you have the two camels. And one is segmented as your true camel that you want to
follow. And the other is segmented as background. So you don't really care about the other camel.
So now you're giving your network the further information that there's a second camel in the
scene, but you don't really want to segment it. And with this, you can actually correct your
network and find you need a little bit more towards only the first camel. And so now you will be able
to segment only one camel in this scene. Does this apply to specific types of objects? How
broadly does it apply? For example, what is the types of objects that are in this Davis data set?
And does that, to what extent does that constrain the model or is the model focused on those types
of images? So actually, you can tackle any type of object. This is completely independent from
the object types. Davis has quite a wide range of objects, I would say. But the interesting thing
is that we're not basing ourselves on object proposals or on object classifiers. But it's really
this foreground background separation. So if you have a picture with whatever strange object
and this object is segmented out, the network is going to be able to learn the appearance of this
object. And it doesn't matter if it's an elephant or if it's a chair, you're going to be able to
follow it. Now, of course, if you are constrained by certain object types, you can do a better work.
And indeed, we also worked on that in the upcoming journal extension for this paper, which we
also published last year. And this was the idea that you can use all these object proposals,
for example, coming from Moscow CNN, to kind of help you and guide you through the segmentation.
So even the first mask, you find that you have a lot of overlap between your segmentation.
And for example, the proposal of a motorbike, then you're kind of sure that the object that
you're trying to follow is a motorbike. And then you can constrain your segmentation also
with the series of proposals that the maskar CNN gives you. But this is something that is,
it's kind of an extra to improve segmentation, but you don't need to do it. So if you don't know
what object you want to follow, you can still use the method as it is, and it will still be able to
follow it. There was another project that came across that you were working on called social maps.
What's that one about? Yeah, so social maps is actually a bit of my vision of what computer vision
and AI can do for society. So in this project, I actually proposed this whole research project
for an award. So it was this research grant proposal. And I was lucky enough that it was accepted
for the Sofia Kovalovskaya Award by the Humboldt Foundation last year. So very few really talented
researchers get this award. And I was lucky to be in this pool of researchers. And congratulations.
Thank you. Thank you very much. Well, the main advantage of course is that it comes with quite
some money, 1.65 million euros. And with that, of course, I can pay for PhD students for equipment.
So this is really the big advantage of such an award that I was able to start my research group
right away. And so I have right now three wonderful PhD students that are working really hard.
And so all these projects that I mentioned are actually with all of them. And essentially the
project social maps is very much related to the idea that I've said before that we want to
understand these dynamic scenes around us. But it has a very specific application. So if you think
about Google maps, for example, you see that they are really an excellent source of static information.
So they have very precise maps of where roads are, where shops and restaurants are. So wherever
you want to go, Google maps is going to give you a really optimized route that you can follow and
you will get to your place, wherever you are. And so the idea though is that this is still a
relatively limited information because it's only static information. And what I would like to
input into maps is the social information. And so what I mean by social information is essentially
how pedestrians, how people use public spaces. So I want to automatically analyze the motion of
pedestrians, what is happening in the streets in real time. And I want to input this information
into maps. So for example, let's assume that I have to go from my home, which is in the south of
Munich. And I have to go to my workplace, which is actually in Garching, so quite far away.
And I'm taking my car and I'm putting into Google maps, my destination. And then Google maps
just tells me a way to follow. But let's imagine that now I have some cameras in the middle of
the city of Munich. And I can detect automatically that there's some demonstration starting. So there's
a bunch of people gathering, they are cutting the streets. So I'm not going to be able to pass
through those streets. Now if you take Google maps as it is now, you will only be notified about
this once there's already a traffic jam. So traffic reports are included into Google maps,
but you don't want to be the first one that is stuck into traffic. So I want to be able to detect
how people use public spaces and whether there's a demonstration or whether kids are coming out
of school. I want to be able to detect it fast by using this dynamic scene understanding and
using computer vision and AI. And I want to input this information into maps so that now I don't get
the normal optimized route, but somehow a social route. And by social route, what I mean is
I want to decouple pedestrian traffic from vehicle traffic. So if cars are going always in certain
roads, and now these roads are being used by pedestrians, I want to tell cars to use
other routes or other roads. And this is kind of the, for example, one of the short-term
applications, but of course in the long term, having access to all these data and all these
analyzed data. So trajectories of pedestrians, how pedestrians use public spaces, where do they
tend to cross? How do which exits, for example, from a railway station do they use most?
If you provide this information to people that are actually planning the cities,
so the urban planners, then they can hopefully design better cities also for pedestrians.
So this is kind of the long-term goal of this project social maps.
Ah, interesting. It makes me think of a kind of a map system like Waze, but curated using,
or updated using video information as opposed to this telemetry from other drivers using the app.
Exactly, exactly. And the advantage is that you don't have to share your information,
so you are just there and things are just detected. But of course, you're not identified,
you're not followed. So it's just about following the trends of people, not the actual persons.
I think this is an important difference to make here.
So it sounds like the projects that we've talked about are you kind of think of as individual
pieces within this overarching vision of social maps. Are there specific projects that you are
working on that get you to the map piece, or does that come later? So that comes a bit later.
So I'm more interested in the dynamic understanding part. So I've also one PhD student that just
started and it's going to work more on what we talked about before, some called training networks
with physical models. And in particular, apply them to multiple object tracking. So this is still
a problem that is not solved. So we in 2014 opened a benchmark for multiple object tracking,
called Mod Challenge. And you can see there that multiple object tracking methods have been
really pushing the state of the art up. And so they've become better and better. But there's still
tons of problems there. And in crowded scenes where you can barely identify one pedestrian from
the other, this is still a big problem. So we want to still work on that particular problem.
We want to work also on semantic segmentation, so identifying where in the city you are and
which part of the city you're observing. Of course, for moving cameras, this is when the visual
localization project comes in. And then I have also one other PhD student working more
on general training or improving the training, making it easier to train big networks. So working
more on a bit theoretical machine learning. So it's all these little projects that are coming
together. But I'm still not working on the part of the actual map. So I think Google Maps already
has a good architecture and a good construction for these maps. So it would just be about
providing these information. So performing this this motion analysis for pedestrians. And that's
the main work of the that I want to tackle. Well, it sounds like really exciting work. Laura,
I really appreciate you taking the time to chat with me about it. Thanks so much. Yeah, thank you
very much for inviting me. It was great. All right, everyone. That's our show for today.
For more information on Laura or any of the topics covered in this episode, head over to Twomlai.com
slash talk slash 168. Don't forget July 31st is your last chance to nominate us for this year's
People's Choice Podcast Awards. Head over to Twomlai.com slash nominate and cast your vote right now.
As always, thanks so much for listening and catch you next time.
