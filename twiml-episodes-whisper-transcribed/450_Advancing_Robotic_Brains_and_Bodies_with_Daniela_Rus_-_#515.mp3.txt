All right, everyone. I am here with Daniella Rousse. Daniella is director of CSAIL and Deputy Dean of Research at MIT. Daniella, welcome to the Twomo AI podcast.
Thank you so much. It's great to be here.
I'm looking forward to our conversation. We're going to dig into your work and the area of robotics.
But before we do, I'd love to start by having you share a little bit about your background and how you, you know, what spurred your interest in that field.
I went into robotics because I wanted to work on something that brought together the world of mathematics with the world of physical things.
And I was really interested in the science and the engineering of autonomy.
And by this, I mean understanding the mathematical and the biological foundations of systems.
And I was interested in building bodies and building brains for these machines.
Now, this has been a long standing quest or a fantasy, if you like, that goes back to my childhood.
I grew up in Romania at the time of great austerity. And in my spare time, I read a lot of books and reflected and fantasized about superpowers.
I grew up in a family of scientists. And as a school student, I had really broad interest.
But the constraints of growing up in Romania really put me on a STEM track.
And so at that time, it was standard practice for all school students to spend a week each month working in a factory.
The Romanian government believed that this would help us get some skills and prepare us to become part of the proletariat.
And so for some time, one week each month, I worked in a factory that made spare parts for locomotives.
And so at that time, I was a teenager and that work just did not seem that useful.
But as I look back, I can now see how that experience has actually contributed to my growth in robotics.
Because I learned to use all sorts of machines like the late, I even made screws from scratch.
And so I began to understand the physical aspects of things.
And as the math we learned in school became more and more abstract, I realized that I wanted to do something STEM-y, yes.
But also something with a physical component.
And so now here I am, living my dream, fantasizing about machines with superpowers and building robots really that bring together the world of computation with a world of mechanisms and materials.
And this is really a great place to be, it's a great path towards having the background to develop the science and engineering of autonomy.
That's awesome.
I mentioned that you are the director of CSEL, you know, for those not familiar with it, it is a storied kind of research institution at MIT. But can you maybe give us a brief overview of the scope of that research center?
CSEL is such an amazing place and CSEL has an extraordinary history. CSEL stands for Computer Science and Artificial Intelligence Laboratory. And there are two parts to our name, the CS part and the AI part.
And the AI part goes back to 1956.
When Marvin Minsky was a young assistant professor and one summer he decided to gather his friends, they went to the woods of New Hampshire where they spent a month walking, talking, no doubt drinking red wine and thinking deeply about what the next big question in sciences.
And when they returned from that trip, they coined the term artificial intelligence and they announced the importance of developing a new science, the science and engineering of intelligence of making machines with human-like characteristics in how they move, how they perceive the world, how they play games, even how they learn.
So our community at MIT has been on this quest since the very beginnings in 1956. The other part of our name, CS, goes back to 1963 when the big dream was to build a machine that two people could use at the same time.
The machines were as big as the rooms we live in as our offices. And at the time, the project was called Project Mac, where Mac stood for machine-aided cognition, the idea that we're building machines as tools and these tools are meant to help people with cognitive tasks.
Ever since our researchers have developed the foundations of computing and along the way they invented so many things that we now take for granted.
So did you know, for instance, that besides the time sharing machine and the password and the optical mouse, our researchers also developed the first computer to display graphics and the first object-oriented programming language clue, which is the foundation of the entire software industry.
And cryptography, the technology that allows all of us to buy things online. And we take that for granted.
We have had extraordinary contributions to what makes computing the way it is now. And as we look into the future, our researchers really aim to invent the future of computing and figure out how to make the world better through computing.
I'm curious how you define robots. You know, it's a simple question, but I hear lots of different answers.
And I'm curious, you know, I suspect given what you said about your background and the early experiences you had with physicality that physicality might have play a role in that definition, but it doesn't for everyone. And I'm curious what robots are for you.
So the definition I share with my students is is the following a robot is a programmable mechanical device that can exert forces.
And so this captures the physicality of the machine and also the fact that the machine interact with the physical world.
And if I might go to a slightly more detailed definition, I would add that the program and the robot has sensors through which it perceives the world and actuators through which it impacts or affects the world.
And of course, you know, one of the things that we want to talk about is the interplay between, you know, the robots and intelligence AI. And I'm curious what you're, I'm curious like you're kind of broad take on, you know, the field of AI for robotics.
Now, let me say that we are living through extraordinary times for AI for machine learning and for robotics together these technologies and these fields of study combined to open up so many possibilities for the future.
And I actually differentiate between the three because I think of robotics as as the science that puts computation in motion. I see AI as the science that allows us to reason and make decisions and machine learning cuts across both and at the moment machine learning in computer science is about making predictions on data.
And finding patterns in data and giving suggestions to people on what is possible. And together these fields combined to open so many opportunities with these with these advancements, we can better engineer medicines.
We can better diagnose monitor and treat disease. We can ensure that there are no traffic fatalities. We can develop systems that provide customized education to everyone. Essentially, we can imagine a future where machines take on the routine tasks.
And leave people with time and space to focus on what is creative and what is interesting and important to them. We are really moving towards this phase where we can imagine a future with these extraordinary tools that are helping people with cognitive and physical tasks.
And there is so much good news in in this field and and really it's so fantastic to be in the middle of of this development.
Do you remember when Mickey Mouse summons the broomstick in the sorceress apprentice. This was a favorite cartoon of mine. Now just imagine that today you don't need magic to make that happen. You need robotics and AI. And that's extraordinary.
Well, you did a great job answering an admittedly very vague question. I realized after I asked it that I was kind of fishing around for one of the common themes that I hear when I speak to folks that are kind of working at this confluence of robotics and AI is this belief that
we never really will be able to get to intelligence without the kind of embodiment that robotics provides. And I guess I was curious if, you know, if you if you espouse a belief like that or if you think that that that's true or
you know what you think the role of embodiment and robotics is to advancing intelligence generally.
That's a great question. And I would like to emphasize the importance of the body in studying intelligence. In fact, every every machine, whether it's a natural system or an engineered system, every machine that has a body and the brain kind of combines the body and the brain to reach
all the capabilities and all the intelligence that the machine is capable of. And so from a from a scientific point of view, it's very important to study both the body and the brain and the connection between the two.
From an engineering point of view, that is also important working on the body and of the brain because for any kind of task we'd like to create a machine for we need a body that is capable of executing that task.
And then we need a brain that's capable of controlling the body to do the task. So we really need both.
And thinking about this relationship between the body and the brain and in particular, the human body, you know, it's composed of lots of distributed systems that are working, you know, independently into some degree or another.
You know, for example, kind of endocrine system and all these nervous system lots of different elements are coming together to create, as you put it, this, this intelligence.
And wondering, yeah, I believe some of your work is on like distributed robotics and more broadly, kind of the move toward in in machine learning of late has been kind of these giant end to end models that are kind of all encompassing as opposed to, you know, maybe what the, you know, the human body might suggest is lots of independent.
And lots of independent systems that are somehow collectively creating intelligence.
And I think you work on some area aspects of that. Can you comment on that?
Well, Sam, that's a lot of questions packaged in one. So let me.
Let me talk about this in stages. So first of all, you're asking about complex systems. And this is a really important question, because, because we have already in some sense mastered how to do simple tasks with machines.
So the question is how to do more complicated tasks. And whenever we ask a question like this, there is a scientific question, which is, how does the natural world do it? What do we know about all the natural processes that lead to those kinds of capabilities.
And then there is the engineering task, which is about how to build it, how to how to engineer how to create a system that has those desired capabilities.
And so the study of complex systems is super important in answering the engineering question.
And in working on complex systems where we have multiple machines that have to coordinate with one another interact.
We achieve new engineering capabilities, but we also get to potential hypothesis about how the natural world achieves certain things.
And because so many questions about the natural world remain mysterious. We are still studying how ants coordinate. We're still studying how colonies of bees work. We're still studying schools of fish.
And despite many years of studies, we still don't have the final theories if you want. And then if we want to move to studying the human mind, where that remains a huge big mysterious question, the study of human intelligence is one of the most important open problems that is facing us.
And studying intelligence means understanding ourselves. It means understanding life.
And it also means identifying new ideas for for building engineered systems. And this is extraordinarily important right now.
Because I'm moving into your question about machine learning. And what I want to emphasize is that it is important to know that today's greatest advances in machine learning are due to decades old ideas that are enhanced by vast amounts of data and computation.
Without new technical ideas and funding to back them, more and more people are going to be blowing the same field and the results will be increasingly incremental.
We really need major breakthroughs and insights if we are going to manage the major technical challenges that are facing this field.
And we also need computation infrastructure that ensures progress. In other words, we need an infrastructure that will deliver us data and computation like we get water and energy today.
And in studying intelligence in looking towards the natural world to identify new ideas for how to how to imagine machine learning systems. We can we can do better.
So in my work, we have been looking at small organisms in the natural world, they're called nematodes. We have been looking at how biologists mapped their neural system.
And we took that as inspiration to create a new model for machine learning. We call neural circuit policies. And so here is how neural circuit policies works in a regular deep neural network.
We have lots and lots of neurons. And each neuron does something very simple. Each neuron performs what is called the thresholding action. It's a step function.
And so interestingly, small organisms have neurons that do way more than that. If you look at how their computation is can be described, the chemistry can be described.
And then we conclude that in fact, the neurons compute differential equations, which are much more complex than these simple step functions. So at the basis of our neural circuit policies, we have a new kind of neuron that is able to to compute a differential equation whose time.
This time constant is actually varying. So we call them liquid time, because we are allowing time to vary. So liquid time differential equations.
And we have an architecture that is inspired by by the nematodes and architecture where we have specialized nodes, some nodes are motor nodes, some nodes are sensory nodes, some nodes are inter neurons.
So in the architecture, we can significantly reduce the size of the networks. In fact, in our self driving project, we have shown that it takes about 100,000 neurons and a half a million parameters to do end to end learning how to drive from human from humans.
That means you have a camera, you look at how a human steers and that's the size of the network that is needed in order to in order to learn to drive in order to follow the road.
Well, in our neural circuit policies network, we only need 19 nodes. And this is an extraordinary dimensionality reduction, because now with 19 neurons, we can get an understanding and the visualization of what the network is doing.
Whereas in the in the huge network model, we have no idea how the network reaches its its conclusions. And this is this is really important because because today's state of the art deep learning models are black boxes.
There is no way for users of the system to truly learn anything based on the systems working. And it is difficult to detect behavior that is abnormal from a safety point of view. And as a result, it is difficult to anticipate failure modes that are tied to rare inputs that could potentially lead to catastrophic consequences.
If you have more compact models, then you can understand what is happening inside them. If you have more compact models, you have a lot of other benefits. The models are more efficient, they are faster.
And importantly, they have a much much better environmental footprint. So we're talking a lot about climate change. We've had a really strange summer this year.
And this is an area where it is important to consider the role of AI and machine learning systems, because these systems consume enormous amounts of energy.
In fact, the researchers at the University of Massachusetts at Amherst estimated that training a large deep learning model produces 626,000 pounds of carbon dioxide. This is equal to the lifetime emission of five cars.
So just think about how many people are training deep learning models today and think about what each of those training sessions contributes.
And also, if you think about really big models, like the GPT-3, it costs $4.6 million in energy costs to train GPT-3.
The more pervasive these methods become, the more of these models will be needed. And this is something that we need to take seriously, because we can develop different models.
We can develop smaller models and simpler models that are more understandable, that are more efficient, and that can drastically reduce the carbon footprint of machine learning.
This isn't really a hypothetical. This is an area where at TSAIL, we have researchers that are making progress towards building pruning and compression of machine learning. We are looking at neurosurkit policies, another method to shrink the size.
And I think this is a very important line of work, because it will allow us to understand more about what is really going on in the models.
And it will allow us to do that with an eye towards sustainability.
You described the neuron that's at the heart of the neural circuit policies work as focused on differential equation over this liquid time concept.
Is it related to the work that some folks are doing around spiking neural nets?
Well, I mean, it's related at some level, and it's quite different at other levels. It's really an attempt to think about machine learning models in a different way.
The ways in which these models are different is threefold. First, the basic neuron model is different. Second, the neurons are not identical. They have different roles.
And third, we use inspiration from the natural world to create the actual architecture of the system. And so if you consider all these three dimensions, I will say that the work is starting a new kind of a new line of investigation, a new kind of model that we think can have really great potential.
At the moment, we are mostly demonstrating the utility of this model in the context of time series type application.
So your data is video or it's a series of data points like maybe in finance or weather. We have not yet shown the impact of this idea on discrete systems like like single images or text.
But this is coming. This is part of our ongoing work.
And did you mention that you have built self-driving systems out of out of these neurons out of these architectures?
Yes, we have a very exciting team that's working on autonomy for cars. We are pursuing a number of areas in autonomy. One is end to end learning for driving.
And so that means given some small amount of video data, how can you learn from a human how to steer in a variety of circumstances, whether you're alone on a road or whether there is your there are other cars that are moving in the same direction as you or whether you have incoming traffic.
And so we're looking at all of these different situations and we're showing that it is possible to do and to end learning in a state of the art models with deep learning.
But with neural circuit policies, we have simpler and more compact models and and our compact models also have interesting additional properties.
They are they seem to capture causality. So what that means is that the focus of attention of the model is in the in the subsection of the image space that that is important for the task.
For instance, if you look at driving, we want to really focus clean me on the horizon and on the side of the road. Now, if you compare the focus of attention of a neural circuit policy solution versus some other state of the art solutions, what we see is that in fact in in a more traditional approaches.
The focus of attention is very noisy. It's all over the image and in fact it's it's quite well established by now that many machine learning solutions use context in order to make a decision.
And with neural circuit policies, you can look at the at the focus at the action, not at the context to to get your decision.
And so in your work thus far, what what are the kind of primary tasks that you've benchmarked the neural circuit policies approach against and what kind of results have you seen.
Yeah, so so we've used neural circuit policies for a variety of navigation tasks on the ground and in the air. And we have shown that these NTPs are really compact.
They have causal correlations. They are more efficient than than the existing networks. And moreover, they can be represented with just a few hundred kilo bits.
They can run these models on a Raspberry Pi. And that is really extraordinary, especially for people who want to use machine learning for embedded applications that in addition to to this NTP approach to making models smaller.
We are also pursuing a number of pruning and compression approaches that allow us to take an already trained model and reduce it, reduce it significantly.
We show in our work that we can eliminate up to 90% of the parameters and still get approximately the same performance from the network. So that's again really intriguing and exciting.
If you can do that, why not just why not just use these approaches as a backend to any kind of machine learning model.
So you can train your model. And then you have a kind of a compilation process where you feed the result into into this engine that optimizes everything keeps what is important removes what is not not important and you end up with a much more efficient system with a lower environmental footprint.
We will ever get to a kind of cross architecture compilation, where as opposed to pruning and that type of thing, which is relatively incremental, you might compile into neural circuit policies from a traditional type of model.
That's the big dream. We don't have this compiler yet, but we would sure want to do that. In fact, I would sure want to have an engine where you give me where you give me your model.
And I can either shrink it or I can reshuffle it so that it becomes an NCP and this kind of, but this kind of automated generation of small models is an interesting, but remains a big challenge.
We talked about robotics being kind of this integration of brain and body NCP is some of your work on kind of the brain side of things you're also active and working on the body side of things in topics like soft robotics.
Can you talk a little bit about your work in that area.
This is a very exciting part of my field. And I want to share with you that the first industrial robot was a robotic arm that was invented in 1961.
So some number of years back, some decades back, but not a long time back. And so since then, we have really grown the field of industrial automation.
We've built these extraordinary machines that perform so much better than humans, but these machines remain isolated from humans on the factory floor because they're large and and heavy and dangerous to be around.
And so natural question is what would it take to to bring humans and machines closer together to make it safe for people to be around machines. And to me, the one answer to this question is making better bodies, making bodies that are more like our bodies that are safer to be around.
And so that's how I so that's how I became interested in the field of soft robotics, where soft really refers to the body of the robot and softness is measured by
a metric called Young's modulus. And if you compare Young's modulus of the materials, we use in traditional robotics. So these are metals and hard plastics.
So if you compare that against the tissue and the Young's modulus of elements from the natural world, there is a huge difference orders of magnitude different.
And so the question is, can we make machines that have the same Young's modulus as my body and your body, because then those machines would be safer to be around.
So the first thing observation is that the field of industrial robotics, or maybe if we look at the last 60 years of robotics, we see that most of the robots we invented are either inspired by the human form.
Or the arms or the human, humanoids or the boxes on wheels. And, and that kind of limits what these systems can do.
These systems are also made of metals and hard plastics. So can we reimagine what the robot is.
So that the robot can be the robot can be inspired by by the natural world with its form diversity or by the built environment with its form, form diversity.
And the robot is made out of any materials that are available to us. We can make robots out of silicone, out of paper. We can even make robots out of food and ice. So how can we reimagine what a robot is to consider a wide spectrum of shapes and a wide spectrum of materials.
And create a very carefully optimized machines that can truly help people with physical tasks.
What are some examples of the software bots projects that you forked on?
So one of our exciting projects is called Sophie, where Sophie stands for soft fish. So early on, maybe about five years ago or so.
We built a fish robot whose tail was soft and moved by on jolation. And that was accomplished by designing the tail in a certain way and then by pumping water in and out of the tail.
So we took this robot to coral reefs and we found that the robot moved quietly and naturally. So it could really kind of penetrate schools of fish without disturbing them.
It did not appear to be a foreign element in that environment, the way a thruster based robot would.
And so we got really excited about this idea of a soft robotic fish being kind of an underwater observatory for people to really give us to really extend our reach into a world that is typically hard for us to reach.
We've also then developed a number of robotic arms and robotic hands. I've been very inspired by the elephant trunk in this work.
So the elephant trunk is a really extraordinary thing.
The elephant trunk can be a super delicate manipulator. The elephant can pick a potato chip from the from the table without breaking it or the elephant can get a banana from your hands without hurting you.
And that trunk can also be used to fend off an intruder. So it's so interesting to think about a system that can be both delicate and strong.
We have been looking at this issue, namely how do we build soft robots that are both delicate and strong. How do we then create a kind of a sensory skin for these robots so that the robots can see the world through touch.
How can we how can we demonstrate the use of these robots in a variety of tasks that people care about we have shown that we saw fingers.
We can get very compliant hands that are capable of handling your groceries. They can pick up your milk, but then they can also pick up your grapes and broccoli.
And picking up grapes and the broccoli is very hard because we don't really have accurate models.
An industrial or a traditional robot or an industrial robot would really need to know exactly what the body looks like and where it's located.
We can calculate very precisely where to place the fingers so that it gets a solid grasp. If you want to imagine that, just try to use your fingernails to pick up something.
That's how that's how industrial robots work. Our hands are very compliant. So our hands are such that they can wrap around the object we try to grab.
Actually, I don't really need to know exactly what this what the shape of my phone is nor precisely where it is located.
So with soft materials and soft robots, we can begin to mimic this kind of compliance and robustness that we have in the natural world that we have with our hands, but that we have not achieved in industrial manipulators.
So that's another example. Oh, let me tell you one of my favorite examples. So we made.
We made an origami. We made a mini surgeon. That's what we call it. It's a mini surgeon.
It's a mini surgeon. It's essentially a little soft robot that we can package inside inside ice.
And the size of the ice is the same as as a regular pill as regular medicine.
And so you division is like this. You swallow this this robot. By the way, this robot is made out of sausage casing. So it's safe to swallow.
So you swallow this robot when the robot gets into your stomach, the ice melts that allows the robot to deploy.
And now you have the ability to do incision free procedures.
You can remove foreign objects like button batteries that are swallowed too often by people every day.
And batteries are very dangerous because within about an hour, they will puncture the tissue. And so that means that they become lodged inside the tissue and they require, they require surgery in order to be removed.
But you can also collect tissue samples and you can patch a wound. You can deliver medicine at a precise location inside the body. And so I'm very excited about the possibility of using new shapes for robots, new materials like sausage casing to open up a future for surgeries without incisions, without pain, without the risk of infections.
It's hard to imagine sausage casings doing all that.
We can watch some movies if you like. So far, these robots have been shown to be effective in lab experiments.
And the next phase is to go to in vivo experiments before we can proceed with the work. But our role is to show that the vision can be realized.
And what's the, can you talk a little bit about the technology behind it in the sense of is it, it's not sausage casings encapsulating something that's, you know, more traditional structure, imagining is it.
No, no, so yeah, yeah, I like to go property of the sausage casing that makes it, you know, an alley or.
It's kind of compressed like an accordion the robot also has a tiny magnet embedded in it. And the operation is imagined to take place inside a controllable magnetic field.
So what that means is that when you swallow the robot, you can locate it because of the magnet. And then you can manipulate the magnetic field to drive the robot as you as you wish.
And so in some sense, the control of the robot is external through this magnetic field.
And, but this is not an alien idea for medicine because we have MRI machines that are in use all the time at most hospitals around the world.
Awesome, awesome. Yeah, one more thing I'd love to have you share a bit about and that is some of your work on adaptive control and the autonomous vehicle domain.
We spoke a little bit about this before and your it kind of grows out of your interest in human and machine collaboration. Can you can you give us an overview of that work.
Let me let me say the following first. So I believe that advancing the science and engineering of autonomy requires that we spend some effort thinking about the body of the robot.
We spend efforts thinking about the brain of the robot, but also we spend efforts thinking about the interaction between robots with each other and with people.
And one interesting case study for this interaction. And in fact, for the connection between interaction body and brain is in the context of autonomous driving.
So by now, I am sure that your audience knows about many efforts to create the to create robot taxi. Well, my own belief is that we're really far from robot taxi. The technology is ready for slow moving safe speed vehicles, low speed vehicles in simpler environments.
We can deploy the technology into products. In these cases, but we still have big challenges. Some some of the challenges include.
Dealing with fast speeds. Some of the challenges include dealing with a great deal of interaction and dynamics in the environment. And some of the challenges include dealing with weather, sensors don't work well in rain and in snow.
And some of the challenges have to do with how you blend a robot car with human driven cars. There have been some deployments in cities where where the robots got really confused by the language that people naturally understand as they drive.
So if you're in an intersection, you can have a kind of a silent communication with an incoming car and you know if you can go or not. But robots don't understand that language.
And so one exciting project has been developing an adaptive algorithm for robot cars that is able to learn the personality, the personalities of the drivers around the ego robot car and adapt the controller to that.
So the car can the robot car can observe you a little bit and figure out whether you are an egoistic driver, a pro social driver or an altruistic driver.
If you are an egoistic driver and the robot wants to make a left hand turn, the robot better let you go because otherwise you'd have a dangerous situation.
But if you are a pro social or an altruistic driver and you want the robot to go, then then the robot has to understand that and hurry up and make that turn otherwise the robot would confuse traffic.
And just think about if you think about when you are learning to drive and you are at team intersections trying to figure out when is the right time to make an unprotected left turn. That is hard even for humans.
So we have some results in this space. And what we are doing is we are leveraging something called social value orientation. This is a metric developed by the social psychology community.
And it's a really cool metric because it maps human personalities onto a circle. In other words, it associates your person, your observed personality with an angle that measures the reward to self versus rewards to others.
So now when we have mathematics that we can associate with personalities, we can embed those mathematics in our cost functions in our learning functions and we can drive controllers to capture the right response in an adaptive way.
And I'm curious about how you collect ground truth for training models around the social value orientation. Do you have observers kind of looking at video and grading other drivers on the screen or is there something else happening.
Well, we are, we have done a lot of simulation and we have inserted so we have used publicly available data sets like NG sim and we have inserted our own control vehicles in those in the data.
And we have observed what happens.
Well, Daniela, thanks so much for taking the time to share a bit about what you're up to. Super interesting stuff and wonderful to chat with you.
Thank you so much for having me Sam and for your interest in my work.
