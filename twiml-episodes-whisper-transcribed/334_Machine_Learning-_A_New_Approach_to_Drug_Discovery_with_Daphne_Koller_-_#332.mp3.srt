1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimmel AI Podcast.

2
00:00:13,400 --> 00:00:21,320
I'm your host Sam Charrington.

3
00:00:21,320 --> 00:00:26,960
This week on the podcast, I'm happy to share just a few of the nearly 20 interviews I recorded

4
00:00:26,960 --> 00:00:31,800
earlier this month at the 33rd annual NURRIPS conference.

5
00:00:31,800 --> 00:00:35,920
If you've been waiting for the Twimmel pendulum to swing from workflow and deployment back

6
00:00:35,920 --> 00:00:39,920
over to AI and ML research, this is your time.

7
00:00:39,920 --> 00:00:45,200
We've got some great interviews and store for you over the upcoming weeks.

8
00:00:45,200 --> 00:00:49,280
Before we move on, I want to send a huge thanks to our friends at Shell for their support

9
00:00:49,280 --> 00:00:53,840
of the podcast and their sponsorship of this NURRIPS series.

10
00:00:53,840 --> 00:00:58,600
Shell has been an early adopter of a wide variety of AI technologies to support use cases

11
00:00:58,600 --> 00:01:04,920
across retail, trading, new energies, refineries, exploration, and many more, and is doing

12
00:01:04,920 --> 00:01:08,680
some really interesting things, but don't take it from me.

13
00:01:08,680 --> 00:01:14,720
Microsoft CEO Satya Nadella recently noted that what's happening at Shell is pretty amazing.

14
00:01:14,720 --> 00:01:19,600
They have a very deliberate strategy of using AI right across their operation from the drilling

15
00:01:19,600 --> 00:01:22,800
operations to safety.

16
00:01:22,800 --> 00:01:28,640
Last year, the company established the Shell.ai Residency Program, a two-year full-time program

17
00:01:28,640 --> 00:01:33,400
which allows data scientists and AI engineers to gain experience working on a variety of

18
00:01:33,400 --> 00:01:37,080
AI projects across all Shell businesses.

19
00:01:37,080 --> 00:01:40,960
If you're in a position to take advantage of an opportunity like this, I'd encourage

20
00:01:40,960 --> 00:01:49,400
you to hit pause now and head over to Shell.ai to learn more, once again that Shell.ai.

21
00:01:49,400 --> 00:01:55,760
And now on to the show.

22
00:01:55,760 --> 00:02:01,760
Alright everyone, I am here in not-so-sunny Vancouver, continuing my conversations from the

23
00:02:01,760 --> 00:02:07,120
33rd NURRIPS conference and I've got the pleasure of being seated with Daphne Kohler, founder

24
00:02:07,120 --> 00:02:09,240
and CEO of Ensitro.

25
00:02:09,240 --> 00:02:11,840
Daphne, welcome to the Twomo AI podcast.

26
00:02:11,840 --> 00:02:12,840
Glad to be here.

27
00:02:12,840 --> 00:02:13,840
Thank you.

28
00:02:13,840 --> 00:02:20,640
Well, I historically or typically start these off by having my guests share a little bit

29
00:02:20,640 --> 00:02:26,080
about their backgrounds and I will certainly allow you to do that, but I feel like we could

30
00:02:26,080 --> 00:02:31,320
spend the entire time on your background, you've done so much in this space.

31
00:02:31,320 --> 00:02:37,720
We were talking before we pressed record about your history even with this conference.

32
00:02:37,720 --> 00:02:41,320
But without further ado, please introduce yourself to our audience.

33
00:02:41,320 --> 00:02:42,320
Thank you.

34
00:02:42,320 --> 00:02:46,440
I do feel like an old timer at this point.

35
00:02:46,440 --> 00:02:51,120
I was doing machine learning long, long before it became popular.

36
00:02:51,120 --> 00:02:58,720
So I really got into this field around 93 or 94, maybe even a little earlier than that,

37
00:02:58,720 --> 00:03:05,160
have been a long time participant in this conference.

38
00:03:05,160 --> 00:03:14,320
In fact was the program chair in 2007 and the general chair in 2008 and I very much remember

39
00:03:14,320 --> 00:03:22,320
how in 2007 it was the first time the conference actually hit 1,000 attendees and 1,000 papers

40
00:03:22,320 --> 00:03:26,960
submitted and we all thought that was really, really big and now we have...

41
00:03:26,960 --> 00:03:28,760
12,000, 13 this year.

42
00:03:28,760 --> 00:03:34,360
This is 13,000 attendees plus five on the waiting list who couldn't get in.

43
00:03:34,360 --> 00:03:38,360
But I don't even know how many thousands of papers were submitted.

44
00:03:38,360 --> 00:03:43,800
Well, the graphs I see are exponential, they are ridiculous.

45
00:03:43,800 --> 00:03:48,440
And so I remember the conference and what we old timers feel is the good old days when

46
00:03:48,440 --> 00:03:53,840
you could actually go and run into people that you know and I can't even find them because

47
00:03:53,840 --> 00:03:56,360
there's just such a crowd that you can't even move.

48
00:03:56,360 --> 00:04:01,760
So it's an interesting transition that we've seen in this space in the last three to five

49
00:04:01,760 --> 00:04:09,040
years and in some ways it's been an amazing resurgence for this field and I'm super excited

50
00:04:09,040 --> 00:04:14,320
and proud of what we as a community have accomplished but at the same time you sort of somewhat

51
00:04:14,320 --> 00:04:18,240
miss the intimacy of this of the community as it used to be.

52
00:04:18,240 --> 00:04:25,520
But anyway, coming back to the other parts of my background so I started to work in as

53
00:04:25,520 --> 00:04:32,400
I said machine learning in the early 90s and at that point the data sets that we as machine

54
00:04:32,400 --> 00:04:37,400
learning people had to work with were honestly kind of boring and lame.

55
00:04:37,400 --> 00:04:44,680
So I remember the 20 news group data set which were a bunch of articles from 20 news

56
00:04:44,680 --> 00:04:49,440
groups and one of the quote unquote challenge problems was could you figure out which news

57
00:04:49,440 --> 00:04:58,800
group an article came from and it was rather not super interesting also from an aspirational

58
00:04:58,800 --> 00:05:05,600
perspective not only from a technical perspective because you can't get a lot of enthusiasm

59
00:05:05,600 --> 00:05:10,280
or please I couldn't get a lot of enthusiasm for classifying articles into news groups.

60
00:05:10,280 --> 00:05:17,640
And so in the late 90s I started to look around for data sets that I felt would have more

61
00:05:17,640 --> 00:05:24,880
of an aspirational nature to them and decided to work in the space of biology because at

62
00:05:24,880 --> 00:05:30,800
that point biologists were actually starting to accumulate data sets that seemed more interesting.

63
00:05:30,800 --> 00:05:35,880
Things like the first one I worked on was actually tuberculosis infections so there is

64
00:05:35,880 --> 00:05:41,720
an interesting social network graph of who might have infected who with tuberculosis as

65
00:05:41,720 --> 00:05:44,440
well as some clinical data for each of them.

66
00:05:44,440 --> 00:05:50,200
I worked on some of the earliest data sets where people measured gene expression gene

67
00:05:50,200 --> 00:05:56,440
activity profiles for cancer patients and what could you extract from that about the

68
00:05:56,440 --> 00:06:03,000
types of cancer that might exist worked on some of the earliest human genetics data sets

69
00:06:03,000 --> 00:06:06,400
as the human genome project came about.

70
00:06:06,400 --> 00:06:12,400
So really at that time machine learning on biological data is actually much more interesting

71
00:06:12,400 --> 00:06:16,840
on a lot of other kinds of machine learning both technically and certainly from the perspective

72
00:06:16,840 --> 00:06:20,160
you feel like you're doing something potentially make a difference.

73
00:06:20,160 --> 00:06:27,320
And so that was around the as I said the late 90s early 2000s and I worked in that area

74
00:06:27,320 --> 00:06:29,440
for a long time.

75
00:06:29,440 --> 00:06:30,440
Was it all at Stanford?

76
00:06:30,440 --> 00:06:36,800
Yeah, this was all at Stanford with the exception of a short sabbatical at UC San Francisco

77
00:06:36,800 --> 00:06:40,320
where I actually spent time in a real biology lab which was great.

78
00:06:40,320 --> 00:06:49,760
And I think it was a really exciting trajectory at that point to see how more and more technology

79
00:06:49,760 --> 00:06:54,480
developments were allowing biology to be measured in a quantitative way and not just biology

80
00:06:54,480 --> 00:06:55,480
but also medicine.

81
00:06:55,480 --> 00:07:02,960
So I worked on some really I think inspiring medical problems like one of them was on how

82
00:07:02,960 --> 00:07:11,520
can you take the measurements that are already taken from the premature babies in a NICU?

83
00:07:11,520 --> 00:07:18,360
These are teeny little babies that are sometimes 28 weeks gestational age 1500 grams or about

84
00:07:18,360 --> 00:07:20,840
as big as the palm of your hand.

85
00:07:20,840 --> 00:07:27,040
And by a combination of non-invasive measurements that machine learning could extract from the

86
00:07:27,040 --> 00:07:34,320
bedside monitors that measure their heart rate, respiratory rate and pulse ox.

87
00:07:34,320 --> 00:07:39,080
Can you predict much earlier which babies are going to need more tension and are going

88
00:07:39,080 --> 00:07:43,440
to have more significant medical difficulties than some of the others?

89
00:07:43,440 --> 00:07:50,680
It was actually really exciting to be able to discover new science if you will as a

90
00:07:50,680 --> 00:07:54,000
byproduct of what the machine learning was able to predict.

91
00:07:54,000 --> 00:07:57,440
So that was a lot of fun.

92
00:07:57,440 --> 00:08:02,480
And so that was sort of where I thought my career would continue to evolve.

93
00:08:02,480 --> 00:08:06,360
I fully expected to retire as an academic.

94
00:08:06,360 --> 00:08:14,440
And then sort of this unexpected transition happened that where work that I'd been doing

95
00:08:14,440 --> 00:08:18,840
at Stanford that had really nothing to do with my research.

96
00:08:18,840 --> 00:08:28,640
It was a side interest in technology assisted education by a long process of a couple years

97
00:08:28,640 --> 00:08:34,320
in collaboration with several others of my Stanford colleagues emerged in the launch

98
00:08:34,320 --> 00:08:40,920
of the first three Stanford massive open online courses back in the fall of 2011.

99
00:08:40,920 --> 00:08:47,280
And I don't think any of us had any expectation that this would turn out the way it did, but

100
00:08:47,280 --> 00:08:52,080
when we looked at those courses where each of the three had a hundred thousand people

101
00:08:52,080 --> 00:08:56,280
or more, it was kind of I was among that first batch in Andrew's course.

102
00:08:56,280 --> 00:08:57,280
Okay.

103
00:08:57,280 --> 00:09:00,880
Well, thank you for being our first or second batch.

104
00:09:00,880 --> 00:09:03,600
Well, thank you for being one of our earliest users.

105
00:09:03,600 --> 00:09:06,320
We are grateful.

106
00:09:06,320 --> 00:09:12,440
And it was one of those moments in time when you look at your life and there's a huge

107
00:09:12,440 --> 00:09:14,280
fork in the road.

108
00:09:14,280 --> 00:09:18,680
And you could say, well, I can continue on my current path, which is a great path.

109
00:09:18,680 --> 00:09:25,840
And likely if I do that, then this thing that I accidentally helped create would just

110
00:09:25,840 --> 00:09:31,400
likely die away because no one else was going to like take it up and run with it.

111
00:09:31,400 --> 00:09:37,160
And or I can, what I expected to was put my career and hold temporarily and go through

112
00:09:37,160 --> 00:09:40,280
this other thing for a couple years and really get it off the ground.

113
00:09:40,280 --> 00:09:46,600
And so at that point, Andrew and I both decided to take a leave of absence from Stanford

114
00:09:46,600 --> 00:09:48,760
and go and find a course here together.

115
00:09:48,760 --> 00:09:54,600
And so that was something that we did kind of got started in the fall of 2011 and then

116
00:09:54,600 --> 00:10:00,120
our leave of absence became official in the beginning of 2012.

117
00:10:00,120 --> 00:10:09,000
And then I ended up doing that for about five years from the, as I said, from about the

118
00:10:09,000 --> 00:10:14,320
fall of 2011 to the fall of 2016.

119
00:10:14,320 --> 00:10:18,880
And at some point in the middle of Stanford said, well, you know, your two years leave of

120
00:10:18,880 --> 00:10:22,240
absence are up and so are you coming back?

121
00:10:22,240 --> 00:10:24,680
And I said, well, not quite yet.

122
00:10:24,680 --> 00:10:29,280
And I said, well, if you're not coming back, then you have to resign and I said, okay,

123
00:10:29,280 --> 00:10:30,280
fine.

124
00:10:30,280 --> 00:10:31,280
So I did.

125
00:10:31,280 --> 00:10:33,680
And I don't regret that at all.

126
00:10:33,680 --> 00:10:39,280
I mean, I guess at that point, it wasn't that standard to leave a chaired professorship

127
00:10:39,280 --> 00:10:41,040
at a top university.

128
00:10:41,040 --> 00:10:45,240
I think a lot more people are doing that now.

129
00:10:45,240 --> 00:10:48,920
But I think it was the right thing to do because I don't believe the company would have survived

130
00:10:48,920 --> 00:10:52,840
if I'd gone back to Stanford in 2014.

131
00:10:52,840 --> 00:10:58,800
So I left Stanford in 2014, continue the course Sarah for another couple years.

132
00:10:58,800 --> 00:11:04,880
And then in 2016, the company was on a great trajectory still is, by the way.

133
00:11:04,880 --> 00:11:07,720
But it wasn't still as primarily a content company.

134
00:11:07,720 --> 00:11:13,240
I mean, one can try and sprinkle machine learning here and there and try and make it better.

135
00:11:13,240 --> 00:11:17,080
But it's not where the core of the business really lies.

136
00:11:17,080 --> 00:11:24,400
And I realized that the company would do fine if I left, but whereas around me, when

137
00:11:24,400 --> 00:11:27,800
I looked, machine learning was changing the world.

138
00:11:27,800 --> 00:11:34,040
All of a sudden, all that vague promise that had wrong man to the field in the first place,

139
00:11:34,040 --> 00:11:37,880
we were actually in a position to make that happen.

140
00:11:37,880 --> 00:11:41,560
But one place that it hadn't had much of an impact was on the life sciences.

141
00:11:41,560 --> 00:11:47,640
And that's really what brought me back because I felt like there was an incredible opportunity

142
00:11:47,640 --> 00:11:55,240
to take machine learning and apply it in an area where I was one of the very few people

143
00:11:55,240 --> 00:11:58,120
who could actually bridge those two worlds.

144
00:11:58,120 --> 00:12:03,120
Because I've been doing it for a really long time and I think on one of the few people,

145
00:12:03,120 --> 00:12:08,560
at least at the certain level of seniority, who was truly bilingual at this point.

146
00:12:08,560 --> 00:12:15,000
And I think that's what you critically need in this space is a few people who can really

147
00:12:15,000 --> 00:12:20,080
see both sides of it and bring them together in new ways rather than just kind of, here's

148
00:12:20,080 --> 00:12:25,200
a problem that someone has already kind of cut and dried and defined clearly now, go

149
00:12:25,200 --> 00:12:26,200
solve it.

150
00:12:26,200 --> 00:12:27,200
Yeah.

151
00:12:27,200 --> 00:12:28,200
Yeah.

152
00:12:28,200 --> 00:12:33,840
And so there are a ton of ways that you can apply machine learning in the broad domain

153
00:12:33,840 --> 00:12:37,880
of life sciences, healthcare, medicine.

154
00:12:37,880 --> 00:12:42,640
The particular one that you are involved in is in drug discovery.

155
00:12:42,640 --> 00:12:43,640
Right.

156
00:12:43,640 --> 00:12:48,880
I've seen you talk about, and others talk about some of the context there, some of the

157
00:12:48,880 --> 00:12:54,800
numbers there that kind of define, you know, how drug discovery is working for us.

158
00:12:54,800 --> 00:12:56,640
Maybe you can share some of those.

159
00:12:56,640 --> 00:12:57,640
Yeah.

160
00:12:57,640 --> 00:13:01,720
So I think it's known to pretty much everyone.

161
00:13:01,720 --> 00:13:06,840
You can't open a newspaper without seeing some discussion of drug pricing.

162
00:13:06,840 --> 00:13:07,840
Yeah.

163
00:13:07,840 --> 00:13:08,840
You know it's broken.

164
00:13:08,840 --> 00:13:15,800
We know it's badly broken, but I don't think people fully realize where some of those

165
00:13:15,800 --> 00:13:18,280
difficulties emerge from.

166
00:13:18,280 --> 00:13:24,000
So I think there is a narrative out there that all of it is you because pharma companies

167
00:13:24,000 --> 00:13:29,520
are kind of trying to gouge the consumer and the insurance companies in the government.

168
00:13:29,520 --> 00:13:34,680
And I think it's certainly true that there has been some bad acting out there and it

169
00:13:34,680 --> 00:13:36,600
in ways that are inappropriate.

170
00:13:36,600 --> 00:13:46,920
But I think people also don't fully appreciate just how prone to failure this field is and

171
00:13:46,920 --> 00:13:53,920
how much investment one needs to make in time and money to find even one successful

172
00:13:53,920 --> 00:13:54,920
drug.

173
00:13:54,920 --> 00:14:00,080
So if you look at the number similar to like startup investing, worse, I think.

174
00:14:00,080 --> 00:14:01,080
Okay.

175
00:14:01,080 --> 00:14:08,520
I mean, and because the costs are larger, I mean, the cost of making a single, getting

176
00:14:08,520 --> 00:14:12,400
a single drug approved irrespective of whether it's a blockbuster drug that's going to make

177
00:14:12,400 --> 00:14:19,480
a ton of money or just something for an orphan indication is $2.5 billion and rising.

178
00:14:19,480 --> 00:14:22,240
We don't pump that much money into most startups.

179
00:14:22,240 --> 00:14:24,240
No, we do not.

180
00:14:24,240 --> 00:14:29,440
And now admittedly, this is when you account for the cost of all of the failures for all

181
00:14:29,440 --> 00:14:32,160
of the things that didn't quite make it.

182
00:14:32,160 --> 00:14:37,280
But even so, if you look at, well, how much amortized across the entire industry does

183
00:14:37,280 --> 00:14:42,880
this cost to make one successful startup, it's not nearly $2.5 billion.

184
00:14:42,880 --> 00:14:49,840
So I think there's, my analogy for this is that drug discovery is like a really long road.

185
00:14:49,840 --> 00:14:57,560
It's, you know, 15 years is a not unreasonable estimate that has multiple forks in it.

186
00:14:57,560 --> 00:15:01,320
One of those paths is going to get you to success.

187
00:15:01,320 --> 00:15:03,960
Maybe if you're lucky, 99 will not.

188
00:15:03,960 --> 00:15:07,280
You have no idea which of them is going to be more successful.

189
00:15:07,280 --> 00:15:11,440
So oftentimes it's a bit of a gut instinct or a guess.

190
00:15:11,440 --> 00:15:16,200
And when you take the wrong path, it's not that you find out in a matter of six months.

191
00:15:16,200 --> 00:15:19,800
There's not such thing as a, okay, I'm going to do a product market fit like you do on

192
00:15:19,800 --> 00:15:20,800
a consumer.

193
00:15:20,800 --> 00:15:22,320
You don't fast fail.

194
00:15:22,320 --> 00:15:27,480
It's three years and hundreds of millions of dollars before you slow fail.

195
00:15:27,480 --> 00:15:32,680
And so it makes it very challenging to do this type of process.

196
00:15:32,680 --> 00:15:36,840
And one of the things that we really are hoping to do is to use machine learning to build

197
00:15:36,840 --> 00:15:38,920
where you might think it was a compass.

198
00:15:38,920 --> 00:15:43,400
Something that when you get to these forks in the road, you have a predictive model that

199
00:15:43,400 --> 00:15:48,520
is machine learning trained that says, you know, here's my probability distribution on

200
00:15:48,520 --> 00:15:50,800
the success of each of those paths.

201
00:15:50,800 --> 00:15:54,120
So here's the one that I recommend that you follow.

202
00:15:54,120 --> 00:16:01,720
And that's something that we hope will help avoid a lot of the wrong paths that are currently

203
00:16:01,720 --> 00:16:07,360
being taken and allow us to get to a successful drug much faster because you don't end up

204
00:16:07,360 --> 00:16:12,160
taking all that time to follow the wrong forks and with a much, much lower cost because

205
00:16:12,160 --> 00:16:17,680
if you're not spending all that effort on things that are not going to succeed, then hopefully

206
00:16:17,680 --> 00:16:22,800
that will really help bend that ridiculous cost curve that has come to be called e-rooms

207
00:16:22,800 --> 00:16:25,520
law, which is an interesting e-rooms law.

208
00:16:25,520 --> 00:16:26,520
e-rooms law.

209
00:16:26,520 --> 00:16:30,760
If you think about e-room, e-r-o-o-m, it's the inverse of Moore's law.

210
00:16:30,760 --> 00:16:31,760
Wow.

211
00:16:31,760 --> 00:16:32,760
Okay.

212
00:16:32,760 --> 00:16:37,760
And Moore's law is the exponential increase in productivity on the tech side, e-rooms

213
00:16:37,760 --> 00:16:44,240
laws, the exponential decrease in productivity on drug discovery in that the cost, the number

214
00:16:44,240 --> 00:16:50,520
of drugs approved per billion U.S. dollars has been decreasing exponentially consistently

215
00:16:50,520 --> 00:16:51,840
for the past 70 years.

216
00:16:51,840 --> 00:16:52,840
Wow.

217
00:16:52,840 --> 00:17:00,520
And in thinking about that law, is it, you know, how much of that is friction in the

218
00:17:00,520 --> 00:17:07,600
discovery process regulation, that kind of thing versus some kind of fundamental, you

219
00:17:07,600 --> 00:17:12,760
know, we just running out of things to try or, you know, we losing ground to disease

220
00:17:12,760 --> 00:17:15,760
and generally, like, what's the way to think about all the components of that?

221
00:17:15,760 --> 00:17:17,760
I think that's a really excellent question.

222
00:17:17,760 --> 00:17:23,160
And people have written entire papers trying to tease apart the different factors here.

223
00:17:23,160 --> 00:17:30,080
Partly, I think there's a legitimate case to be made that there's an increased regulatory

224
00:17:30,080 --> 00:17:36,400
burden, some justified trying to be more careful about the lives and health of patients in

225
00:17:36,400 --> 00:17:41,240
clinical trials, some less justified, just lots of bureaucracy and paperwork, doesn't

226
00:17:41,240 --> 00:17:43,320
actually add value, but it'll cost money.

227
00:17:43,320 --> 00:17:45,040
I think that's part of it.

228
00:17:45,040 --> 00:17:53,120
I think another part of it is what Jack's channel was one of the bigger experts in the

229
00:17:53,120 --> 00:17:59,880
space calls the better than the Beatles problem, which is that when you're in the

230
00:17:59,880 --> 00:18:08,120
business of movies or books and you're looking for the next blockbuster, next song, next

231
00:18:08,120 --> 00:18:15,560
movie, the movies from the past, the books from the past, people are looking for new stuff

232
00:18:15,560 --> 00:18:22,120
because they may have already consumed that old stuff and they want a new piece of content.

233
00:18:22,120 --> 00:18:23,120
That's not true for drugs.

234
00:18:23,120 --> 00:18:27,640
I mean, you actually have to be better than all the previously approved drugs for this

235
00:18:27,640 --> 00:18:30,640
to make sense.

236
00:18:30,640 --> 00:18:34,480
Doctors are not looking for the next thing.

237
00:18:34,480 --> 00:18:39,080
If you actually want to be better, if you want to actually have a market, you have to

238
00:18:39,080 --> 00:18:45,280
actually be better than everything that's been come before and has been approved.

239
00:18:45,280 --> 00:18:53,200
The floor keeps rising, if you will, the bar that you have to overcome keeps rising.

240
00:18:53,200 --> 00:18:58,320
I think when you look at it in that light, there's a couple of different, you can think

241
00:18:58,320 --> 00:19:01,240
of it as almost like a dichotomy, if you will.

242
00:19:01,240 --> 00:19:08,280
There's things for which we've already done a pretty decent job of making drugs, cardiovascular

243
00:19:08,280 --> 00:19:12,680
disease, diabetes, infectious disease.

244
00:19:12,680 --> 00:19:16,360
There are some pretty good drugs out there.

245
00:19:16,360 --> 00:19:22,360
The next drug really has to be better and to prove that it's better, you actually, in

246
00:19:22,360 --> 00:19:26,280
many cases, need a very large and very expensive clinical trial.

247
00:19:26,280 --> 00:19:32,560
Then there is the classes of diseases for which we really don't have any good drugs.

248
00:19:32,560 --> 00:19:39,240
I think of those CNS disease of the neural, of the central nervous system is probably

249
00:19:39,240 --> 00:19:42,000
the most obvious category.

250
00:19:42,000 --> 00:19:45,800
That's because we have, it's such a complicated system.

251
00:19:45,800 --> 00:19:50,360
We do not understand it, probably won't for a very long time in the level of mechanistic

252
00:19:50,360 --> 00:19:52,320
understanding.

253
00:19:52,320 --> 00:19:57,200
The model systems that we've been using developed drugs to disease in general, which are typically

254
00:19:57,200 --> 00:20:04,680
animal models, are a very far cry from the human central nervous system.

255
00:20:04,680 --> 00:20:09,400
It turns out that it just doesn't translate in the sense that we find drugs that make

256
00:20:09,400 --> 00:20:19,840
mice smarter, have less, more empathic, have less neuronal death, and it just doesn't

257
00:20:19,840 --> 00:20:24,160
translate into human disease, largely because the mice don't get the disease in the first

258
00:20:24,160 --> 00:20:25,560
place.

259
00:20:25,560 --> 00:20:30,760
Therefore, you are artificially creating a disease in the mouse, and when you cure the artificial

260
00:20:30,760 --> 00:20:34,680
disease, it turns out to have very little to do with curing the real disease.

261
00:20:34,680 --> 00:20:35,680
Interesting.

262
00:20:35,680 --> 00:20:36,680
Interesting.

263
00:20:36,680 --> 00:20:44,400
In Cetra, when you're thinking about this kind of compass analogy of what you do and whoever

264
00:20:44,400 --> 00:20:50,560
the actor is in this case is at the fork in the road, are you using machine learning to

265
00:20:50,560 --> 00:21:00,760
evaluate the efficacy potentially of some compound as a treatment, or are you applying machine

266
00:21:00,760 --> 00:21:06,720
learning to the broader system, like was the probability distribution that you're creating

267
00:21:06,720 --> 00:21:11,640
one that is incorporating market factors, and all these other things, are you really focused

268
00:21:11,640 --> 00:21:13,400
on the biology at this point?

269
00:21:13,400 --> 00:21:19,640
At this point, focus on the biology, and down the line, we might focus on the chemistry,

270
00:21:19,640 --> 00:21:24,120
and then maybe on things like the selection of patients for the clinical trial to identify

271
00:21:24,120 --> 00:21:30,960
the ones that are more likely to be responsive to the drug, and then downstream from that

272
00:21:30,960 --> 00:21:36,760
is, can we use machine learning to have better biomarkers of efficacy so that you can actually

273
00:21:36,760 --> 00:21:42,800
tell whether a patient is responding to your drug in ways other than literally a questionnaire

274
00:21:42,800 --> 00:21:48,080
that's filled out on sheets of paper that a nurse then transcribes into the computer,

275
00:21:48,080 --> 00:21:52,880
which is how this is often done, and even downstream in the manufacturing.

276
00:21:52,880 --> 00:21:57,520
I think there's a lot of opportunities before we get to market factors and things like

277
00:21:57,520 --> 00:21:58,520
that.

278
00:21:58,520 --> 00:22:07,680
Right now, our primary focus is really on the biology and making a prediction on if I make

279
00:22:07,680 --> 00:22:13,520
this intervention in a human, or even in this particular human, how likely is this human

280
00:22:13,520 --> 00:22:14,520
to respond?

281
00:22:14,520 --> 00:22:18,200
What is the clinical outcome going to be of that intervention?

282
00:22:18,200 --> 00:22:21,520
That's a very challenging problem to make a prediction on, but I think there's some

283
00:22:21,520 --> 00:22:26,040
of your tools out there, both on the machine learning and on the biology side, that if you

284
00:22:26,040 --> 00:22:29,000
put them together, give us a chance of making human predictions.

285
00:22:29,000 --> 00:22:35,960
That sounds to me like a personalized medicine type of application, which seems much further

286
00:22:35,960 --> 00:22:39,520
down than the, I'm at a compass trying to develop a drug.

287
00:22:39,520 --> 00:22:45,040
Yeah, no, the personalization is, I think, not really the key focus of what we're doing,

288
00:22:45,040 --> 00:22:51,360
but rather the recognition that a lot of the drugs that people have tried haven't succeeded

289
00:22:51,360 --> 00:22:55,360
because you're treating multiple diseases with one drug.

290
00:22:55,360 --> 00:23:02,680
So if you look at the big success in the field of precision oncology in the last decade

291
00:23:02,680 --> 00:23:08,080
or so, a lot of the successes have come from the realization that breast cancer, for

292
00:23:08,080 --> 00:23:11,160
instance, is not one disease.

293
00:23:11,160 --> 00:23:15,560
The patients who are hurt too positive are very different from the patients who have

294
00:23:15,560 --> 00:23:20,800
a block of one mutation, and you treat them with completely different therapies.

295
00:23:20,800 --> 00:23:25,840
Colleagues of mine who worked at Genenteic at the time that herceptin, which is the drug

296
00:23:25,840 --> 00:23:31,600
that targets her two positive breast cancers, was developed, say that if you had applied,

297
00:23:31,600 --> 00:23:36,840
if you tried out herceptin in an all-comers breast cancer population, you'd need a clinical

298
00:23:36,840 --> 00:23:41,240
trial of about 10,000 women in order to demonstrate even the tiny effect size because you're

299
00:23:41,240 --> 00:23:45,800
averaging out on a whole bunch of people who are not going to respond, but nevertheless

300
00:23:45,800 --> 00:23:47,120
have side effects.

301
00:23:47,120 --> 00:23:51,480
So in your case, it's less about personalization than targeting, really.

302
00:23:51,480 --> 00:23:52,480
Exactly.

303
00:23:52,480 --> 00:23:54,240
Targeting to the right patient population.

304
00:23:54,240 --> 00:24:00,400
You mentioned part of the opportunity here is kind of the application of new machine learning

305
00:24:00,400 --> 00:24:01,920
techniques to this round.

306
00:24:01,920 --> 00:24:06,680
What's kind of the landscape of techniques that you're able to apply?

307
00:24:06,680 --> 00:24:13,960
One of the things that we are doing is relying both on new developments in machine learning,

308
00:24:13,960 --> 00:24:21,280
but at the same time, on new developments in high throughput biology and bioengineering.

309
00:24:21,280 --> 00:24:27,560
That's a space where maybe less familiar to folks who are more expert in the technology

310
00:24:27,560 --> 00:24:31,720
side of things, but there's been as much progress on that side as there's been on machine

311
00:24:31,720 --> 00:24:32,720
learning.

312
00:24:32,720 --> 00:24:39,600
So, for instance, at this point, there is the capability for us to take a small sample

313
00:24:39,600 --> 00:24:47,000
of your skin or a small sample of your blood, then transform that some of those cells into

314
00:24:47,000 --> 00:24:51,680
what are called stem cells, and these are those cells that can then turn into any lineage

315
00:24:51,680 --> 00:24:52,680
in your body.

316
00:24:52,680 --> 00:25:00,000
I can basically create Daphne neurons, or Daphne cardiomyocytes, or Daphne hepatocytes,

317
00:25:00,000 --> 00:25:04,320
and all of them have the genetics that I have.

318
00:25:04,320 --> 00:25:09,840
So, if I have a certain propensity to disease that manifests in that particular cell type,

319
00:25:09,840 --> 00:25:14,400
you could potentially see it there, but it's the right cell lineage.

320
00:25:14,400 --> 00:25:19,080
And so, I have the ability at this point to basically do population diversity, but at

321
00:25:19,080 --> 00:25:21,000
the cellular level.

322
00:25:21,000 --> 00:25:26,240
And I have the further ability to use amazing technologies like CRISPR that allow us to

323
00:25:26,240 --> 00:25:30,920
modify the genome to even create mutations that we know are disease-causing.

324
00:25:30,920 --> 00:25:36,520
So, for instance, if I want to really see what a very high penetrant version of the disease

325
00:25:36,520 --> 00:25:42,680
looks like, I can introduce that mutation into a normal genome and see what the difference

326
00:25:42,680 --> 00:25:45,360
is between the width of mutation without the mutation.

327
00:25:45,360 --> 00:25:51,800
So, think of it as the ability to artificially create training sets on what disease looks

328
00:25:51,800 --> 00:25:53,600
like at the cellular level.

329
00:25:53,600 --> 00:25:54,600
Interesting.

330
00:25:54,600 --> 00:25:59,120
And so, now, you think about, well, that gives me a ton of data because you have potentially

331
00:25:59,120 --> 00:26:05,120
hundreds of genetic backgrounds, maybe even more, with tens of thousands of readouts from

332
00:26:05,120 --> 00:26:11,640
each of those cells, like the kind you might get from super-resolution microscopy, or

333
00:26:11,640 --> 00:26:15,640
transcriptional measurements of all of the genes in the cell.

334
00:26:15,640 --> 00:26:19,520
And now, you ask yourself, okay, with all of those measurements, if this is what healthy

335
00:26:19,520 --> 00:26:27,480
looks like, and this is all the population of unhealthy, what differentiates them?

336
00:26:27,480 --> 00:26:31,320
What does healthy cell look like relative to an unhealthy cell?

337
00:26:31,320 --> 00:26:36,280
And are the unhealthy cells, or are they all one big homogeneous cluster, or are there

338
00:26:36,280 --> 00:26:40,960
subclusters that are very distinctive of the molecular level?

339
00:26:40,960 --> 00:26:47,520
And then, with that, you have an understanding of what the disease looks like at the level

340
00:26:47,520 --> 00:26:50,760
of cellular phenotypes, as they're called.

341
00:26:50,760 --> 00:26:54,200
And because it's an intervenable system, it's not a human.

342
00:26:54,200 --> 00:26:57,200
We're doing an experiment and a human is really hard.

343
00:26:57,200 --> 00:27:04,000
You can ask yourself, if I put this compound into a bunch of cells that are from this cluster,

344
00:27:04,000 --> 00:27:07,760
does that revert the phenotype back to something that looks healthier?

345
00:27:07,760 --> 00:27:11,840
And if it does, maybe that's a drug that will also revert the phenotype at the clinical

346
00:27:11,840 --> 00:27:15,360
outcome level, which is what we ultimately care about, because you want to hear people

347
00:27:15,360 --> 00:27:16,680
not cells.

348
00:27:16,680 --> 00:27:22,360
So basically, you can think about the machine learning as helping us to distinguish between

349
00:27:22,360 --> 00:27:27,600
different cellular phenotypes in a way that aligns with human clinical outcome.

350
00:27:27,600 --> 00:27:28,600
Interesting.

351
00:27:28,600 --> 00:27:34,280
And so, from that perspective, is it primarily kind of unsupervised clustering types of

352
00:27:34,280 --> 00:27:41,200
approaches that you find most useful in your work, or is it a broad set of things?

353
00:27:41,200 --> 00:27:45,360
It's actually a broad set of things, because we do have some supervision signal, not as

354
00:27:45,360 --> 00:27:51,240
much as you would like, because there is, for instance, some understanding that if you

355
00:27:51,240 --> 00:27:54,600
have this mutation, then your chances of disease are really high.

356
00:27:54,600 --> 00:27:58,800
So you can think of it as kind of like a bit of a supervision signal, or you have supervision

357
00:27:58,800 --> 00:28:04,000
signal in the sense of, here is a hundred of these what's called induced pluripotent stem

358
00:28:04,000 --> 00:28:05,000
cells, IPS.

359
00:28:05,000 --> 00:28:08,600
Induced pluripotent stem cells.

360
00:28:08,600 --> 00:28:12,680
So pluripotent means they can go into multiple lineages, and induced means I created them

361
00:28:12,680 --> 00:28:16,480
from a skin sample rather than it's a fetal stem cell.

362
00:28:16,480 --> 00:28:22,040
So you have these IPS cells that you got from patients, that's the positives, and you

363
00:28:22,040 --> 00:28:25,880
have the IPS cells that you got from controls, that's the negatives.

364
00:28:25,880 --> 00:28:30,360
So there is a little bit of supervision signal, but certainly not as much as you want, and

365
00:28:30,360 --> 00:28:34,760
so I think if you had to put a label on it, it falls largely in the category of weekly

366
00:28:34,760 --> 00:28:39,600
supervised learning, not completely unsupervised, not completely supervised, but somewhere in

367
00:28:39,600 --> 00:28:40,600
between.

368
00:28:40,600 --> 00:28:41,600
Got it, got it.

369
00:28:41,600 --> 00:28:47,800
And as you describe this, you can almost envision a kind of closed loop process where you're

370
00:28:47,800 --> 00:28:54,480
able to, you know, I've seen some of the like the bio robots that can, you know, take

371
00:28:54,480 --> 00:28:58,640
an array and do all kinds of experiments at, you know, high throughput.

372
00:28:58,640 --> 00:28:59,640
Are you there yet?

373
00:28:59,640 --> 00:29:00,640
Absolutely.

374
00:29:00,640 --> 00:29:01,640
Okay.

375
00:29:01,640 --> 00:29:02,640
Well, we, sorry.

376
00:29:02,640 --> 00:29:03,640
Let me, let me.

377
00:29:03,640 --> 00:29:06,080
For those who can't see you, she's getting very excited.

378
00:29:06,080 --> 00:29:07,080
Yes.

379
00:29:07,080 --> 00:29:11,520
Now, that's definitely in our roadmap, and it's what we're working towards.

380
00:29:11,520 --> 00:29:13,200
The robots are already there.

381
00:29:13,200 --> 00:29:14,200
Yeah.

382
00:29:14,200 --> 00:29:22,080
They're doing a lot of the high throughput, more menial work automatically at this point.

383
00:29:22,080 --> 00:29:27,120
We don't have the full-blown closed loop system yet, because we only just got the lab

384
00:29:27,120 --> 00:29:28,440
up and running.

385
00:29:28,440 --> 00:29:34,800
But the goal is exactly that we would have the ability to take data off the instruments,

386
00:29:34,800 --> 00:29:42,560
process it automatically, and then use what we see to guide the next round of experiments.

387
00:29:42,560 --> 00:29:48,760
And I think that's going to be incredibly powerful, both at the high level of, you know,

388
00:29:48,760 --> 00:29:54,840
the high, just the experiments in terms of what makes for sick versus healthy cells.

389
00:29:54,840 --> 00:30:00,400
But we actually embed machine learning in every single part of what we do.

390
00:30:00,400 --> 00:30:09,560
So for instance, imaging a plate with imaging plate at 40X resolution across multiple

391
00:30:09,560 --> 00:30:12,920
fluorescent channels can take as long as 30 days.

392
00:30:12,920 --> 00:30:18,240
So very long experiment, because you just have to do tile by tile by tile.

393
00:30:18,240 --> 00:30:26,800
Do you really need to image every well and every tile in every well at 40X resolution,

394
00:30:26,800 --> 00:30:30,600
or can you imagine using machine learning to say, I'm going to do a very quick pass at

395
00:30:30,600 --> 00:30:31,600
10X.

396
00:30:31,600 --> 00:30:32,600
See where they're sitting.

397
00:30:32,600 --> 00:30:34,080
See where the interesting things are.

398
00:30:34,080 --> 00:30:38,800
And then dig down a higher resolution into the cells that are more likely to be interesting.

399
00:30:38,800 --> 00:30:42,760
You can think about machine learning in all sorts of different places, or here's another

400
00:30:42,760 --> 00:30:43,760
example.

401
00:30:43,760 --> 00:30:47,360
Do you really need to image every single one of those fluorescent channels, or can you

402
00:30:47,360 --> 00:30:53,440
infer from some channels what the other ones are going to be, and then image fewer channels?

403
00:30:53,440 --> 00:30:54,440
It sounds easy.

404
00:30:54,440 --> 00:30:58,080
But then I'm envisioning these, you know, you're not building the robots from scratch, you're

405
00:30:58,080 --> 00:31:03,320
getting them from roast or whoever or life sciences, and, you know, they're proprietary

406
00:31:03,320 --> 00:31:05,320
about the way that they control their robots.

407
00:31:05,320 --> 00:31:10,080
Like, can you easily insert your machine learning models into these off-the-shelf tools?

408
00:31:10,080 --> 00:31:11,720
So first of all, is that even an issue?

409
00:31:11,720 --> 00:31:12,720
Am I picking at the right thing?

410
00:31:12,720 --> 00:31:13,720
No, you're here.

411
00:31:13,720 --> 00:31:14,720
You're absolutely right.

412
00:31:14,720 --> 00:31:17,600
It is definitely an issue.

413
00:31:17,600 --> 00:31:24,640
Some manufacturers are more open than others in terms of making their APIs available so

414
00:31:24,640 --> 00:31:32,240
that you can control the robot, you can control the microscope, for instance, from the outside.

415
00:31:32,240 --> 00:31:36,880
And sometimes we have to actually kind of hack into this a little bit, but sometimes

416
00:31:36,880 --> 00:31:39,200
they're being more flexible.

417
00:31:39,200 --> 00:31:44,440
And in some cases, we actually build custom hardware because we have to do that, because

418
00:31:44,440 --> 00:31:48,640
it provides us the flexibility that we need both on the hardware side as well as the software

419
00:31:48,640 --> 00:31:49,640
side.

420
00:31:49,640 --> 00:31:50,640
Yeah.

421
00:31:50,640 --> 00:31:56,640
So we've talked through some examples of the kinds of problems that you're solving this

422
00:31:56,640 --> 00:31:57,640
way.

423
00:31:57,640 --> 00:32:02,560
I think there's still a hole for me in like the where do you start?

424
00:32:02,560 --> 00:32:03,560
What's the first step?

425
00:32:03,560 --> 00:32:06,080
What's the next step like?

426
00:32:06,080 --> 00:32:16,400
So I think the first step is really to build a team that is truly cross-functional and

427
00:32:16,400 --> 00:32:24,440
is able to communicate across what is usually a chasm between these two disciplines.

428
00:32:24,440 --> 00:32:31,760
Most machine learning people took biology, maybe back in high school sometime, and have

429
00:32:31,760 --> 00:32:35,080
a vague recollection of what they learned, but not much beyond that.

430
00:32:35,080 --> 00:32:41,800
I mean, there's so much there in what you're doing is biology, it's chemistry, it's genetics.

431
00:32:41,800 --> 00:32:47,040
And on the other side, most biologists don't really know much about computer science or

432
00:32:47,040 --> 00:32:48,880
machine learning.

433
00:32:48,880 --> 00:32:54,280
They may have done some whatever data analysis and an Excel spreadsheet, but it's really

434
00:32:54,280 --> 00:32:57,560
two communities that don't have a lot of common language.

435
00:32:57,560 --> 00:33:05,360
And one of the things that we're really building is a community of people who really either

436
00:33:05,360 --> 00:33:12,720
have a foot niche camp and we have a bunch of those people who are truly bilingual, or

437
00:33:12,720 --> 00:33:18,040
even if they don't, they have a real willingness to kind of reach out across the chasm and have

438
00:33:18,040 --> 00:33:20,000
a meaningful collaboration.

439
00:33:20,000 --> 00:33:26,440
And I think that's absolutely essential because so much of what we do really requires this

440
00:33:26,440 --> 00:33:28,840
interaction between both sides.

441
00:33:28,840 --> 00:33:35,080
And what we find today is that once you bridge that gap, you actually are in some ways

442
00:33:35,080 --> 00:33:38,960
programming into programming language simultaneously.

443
00:33:38,960 --> 00:33:44,400
There is the programming language of whatever TensorFlow or PyTorch, which we all are used

444
00:33:44,400 --> 00:33:47,120
to when we think about computational modules.

445
00:33:47,120 --> 00:33:53,600
But then there is equally valid experimental modules that you can kind of fit in.

446
00:33:53,600 --> 00:33:59,480
Like here's a set of CRISPR guides, introduce those CRISPR guides into the following set

447
00:33:59,480 --> 00:34:04,200
of cells, differentiate this set of cells into the following lineage.

448
00:34:04,200 --> 00:34:06,960
And I'm not saying each of those steps is easy.

449
00:34:06,960 --> 00:34:10,960
They're actually harder in some ways than the computational steps because it's a little,

450
00:34:10,960 --> 00:34:14,760
I mean, big subologies finicky and these are live cells and they don't always do what

451
00:34:14,760 --> 00:34:19,920
they're told, unlike the bits in the computer, which generally do do what they're told.

452
00:34:19,920 --> 00:34:26,080
But you can still create a level of abstraction on those biological steps that you can incorporate

453
00:34:26,080 --> 00:34:27,960
into your overall procedure.

454
00:34:27,960 --> 00:34:33,280
So your procedure now has blocks that are computational and blocks that are biological.

455
00:34:33,280 --> 00:34:39,040
And it's a single, almost integrated process where these different pieces fit together.

456
00:34:39,040 --> 00:34:44,640
And the whole is considerably larger than the sum of the parts where the typical approach

457
00:34:44,640 --> 00:34:49,120
is, okay, the biologists create some data, throw it over the fence, and then someone does

458
00:34:49,120 --> 00:34:50,120
the analysis.

459
00:34:50,120 --> 00:34:55,600
When you have this sort of real integration of those two steps, the space of what you

460
00:34:55,600 --> 00:34:59,880
can do is obviously exponentially larger.

461
00:34:59,880 --> 00:35:05,120
And therefore, it opens up capabilities to even, to think about problems that you would

462
00:35:05,120 --> 00:35:09,280
never even have thought about far less been able to solve without the integration of those

463
00:35:09,280 --> 00:35:10,280
tools.

464
00:35:10,280 --> 00:35:16,280
I've got to imagine as the CEO of a company that depends so heavily on finding people

465
00:35:16,280 --> 00:35:22,400
with these two independently unique skillsets, not to mention together, you maybe have an

466
00:35:22,400 --> 00:35:30,840
interesting perspective on auto-ML and lowering the barriers to getting to the, at least on

467
00:35:30,840 --> 00:35:34,840
the computational side, any reaction to that?

468
00:35:34,840 --> 00:35:43,000
I think that auto-ML is a great enabler in some ways in that when you get to the point

469
00:35:43,000 --> 00:35:51,440
that you have defined a problem that has well defined input output specifications, you

470
00:35:51,440 --> 00:35:56,760
know, you want to train your algorithm on the following, you want to train whatever predictor

471
00:35:56,760 --> 00:36:00,960
on the following dataset with these inputs and these outputs and the subjective function,

472
00:36:00,960 --> 00:36:06,840
then it allows you to avoid some of the annoying nitty gritty of hyperparameter architecture

473
00:36:06,840 --> 00:36:07,840
search.

474
00:36:07,840 --> 00:36:16,200
It doesn't do at all is tell you what problems to solve and what's the right data to create

475
00:36:16,200 --> 00:36:20,680
because we create our own data, so what data do you even create?

476
00:36:20,680 --> 00:36:23,880
What's the right objective to train the model too?

477
00:36:23,880 --> 00:36:28,520
Because that actually matters if you train your algorithm to regression and might not do

478
00:36:28,520 --> 00:36:33,560
as good a job at classification and vice versa and which is the right one for the problem

479
00:36:33,560 --> 00:36:39,400
that you're trying to solve is not clear and this is the simplest example that one can do.

480
00:36:39,400 --> 00:36:46,280
So I think it's going to help but I don't think it's going to solve the problem for us

481
00:36:46,280 --> 00:36:51,520
and I think also for a lot of other people because the heart of what a really good machine

482
00:36:51,520 --> 00:36:57,840
learning person can do is understand the domain enough that you can identify in your problems

483
00:36:57,840 --> 00:36:59,320
if no one has thought about.

484
00:36:59,320 --> 00:37:04,280
It sounds like that's the stage of the journey that your company is at very much so.

485
00:37:04,280 --> 00:37:09,840
And I think that's hopefully the stage of the journey that we will be at for a long

486
00:37:09,840 --> 00:37:13,840
time, not that we won't be solving some of the problems that we come up with today,

487
00:37:13,840 --> 00:37:20,160
but I think there's so many of those problems that one could imagine making a big impact

488
00:37:20,160 --> 00:37:25,400
on in the drug discovery and development process that even once we nail some of the earlier

489
00:37:25,400 --> 00:37:29,960
problems and maybe move more towards this iterative refinement mode, there's going to

490
00:37:29,960 --> 00:37:31,880
be new problems that we're going to have to tackle.

491
00:37:31,880 --> 00:37:38,080
When you think about the broad landscape of problems that gets us most quickly to a

492
00:37:38,080 --> 00:37:44,560
healthier population, how do you segment those or think about that landscape in terms

493
00:37:44,560 --> 00:37:49,960
of where are the opportunities for folks that are interested in applying machine learning

494
00:37:49,960 --> 00:37:51,480
to make folks healthier?

495
00:37:51,480 --> 00:37:52,480
Oh, I see.

496
00:37:52,480 --> 00:37:57,440
The drug discovery and in particular, you know, you're working on a specific niche within

497
00:37:57,440 --> 00:37:58,440
drug discovery.

498
00:37:58,440 --> 00:38:00,760
What are some other things that you think are interesting?

499
00:38:00,760 --> 00:38:08,360
Well, I think we're working today on a particular phase in the drug discovery process.

500
00:38:08,360 --> 00:38:13,640
It's not by any means the place that will end up because this for us is the beginning

501
00:38:13,640 --> 00:38:22,080
of the journey towards building what I hope will be the, you know, first fully data-enabled,

502
00:38:22,080 --> 00:38:27,240
data-driven drug discovery and development companies so that every step of the process

503
00:38:27,240 --> 00:38:34,080
is now based on data production and machine learning as core technologies.

504
00:38:34,080 --> 00:38:40,120
And I think that you've seen companies like that emerge in other parts of the tech space

505
00:38:40,120 --> 00:38:49,280
where, for instance, Amazon is a fully data-enabled retail company and it's not just in, you

506
00:38:49,280 --> 00:38:56,000
know, the early days of just being a little bit better at recommending items to you and

507
00:38:56,000 --> 00:39:01,360
having the orders be all managed automatically now to every step of the way they're enabled

508
00:39:01,360 --> 00:39:03,120
using data and machine learning and technology.

509
00:39:03,120 --> 00:39:06,560
We hope to be doing that for drug discovery and development.

510
00:39:06,560 --> 00:39:14,160
Before we go to other areas, are the traditional drug discovery companies that far behind?

511
00:39:14,160 --> 00:39:20,800
You know, I think there are islands in some of those companies where they're trying to

512
00:39:20,800 --> 00:39:28,080
bring in machine learning and technology to accelerate things, you know, and it varies.

513
00:39:28,080 --> 00:39:32,880
There was a recent announcement about one company that's using it to accelerate some manufacturing

514
00:39:32,880 --> 00:39:38,880
processes and another one that's using it to enable better design of small molecule binders

515
00:39:38,880 --> 00:39:44,880
to a particular protein. Most of those efforts fall into the category of, here's a problem

516
00:39:44,880 --> 00:39:50,960
I'm already solving anyway. I'm solving it, not maybe not in the perfect way, maybe it's

517
00:39:50,960 --> 00:39:55,280
too slow, maybe I can introduce some additional optimization, so I'm going to use a machine

518
00:39:55,280 --> 00:39:56,280
learning document.

519
00:39:56,280 --> 00:40:07,520
It's probably going to reduce the cost by whatever 20% or something like that. That's great,

520
00:40:07,520 --> 00:40:15,040
it's not transformative. I don't know of any companies that are in the process of saying,

521
00:40:15,040 --> 00:40:18,000
okay, no, we're just going to have to rethink the process from the ground up.

522
00:40:19,760 --> 00:40:26,160
One way of thinking about this, I don't know if it's true, is to ask whether of the big tech

523
00:40:26,160 --> 00:40:33,360
giants that are really fully data enabled from beginning to end, did any of them actually emerge

524
00:40:33,360 --> 00:40:40,640
from the existing incumbents? Amazon did not emerge from Walmart and Netflix didn't emerge

525
00:40:40,640 --> 00:40:47,040
from blockbusters or one of the Hollywood studios and Google didn't emerge from the yellow pages.

526
00:40:49,440 --> 00:40:56,480
These are all kind of the non-peck enabled precursors to those companies and in some cases,

527
00:40:56,480 --> 00:41:01,040
you need to ask yourself, okay, if I was building this from scratch, what would it look like?

528
00:41:01,040 --> 00:41:08,240
So, answer the second part of your earlier question. I think clearly there's opportunities

529
00:41:08,240 --> 00:41:15,440
beyond drug discovery and development. There's some interesting work happening in the device space

530
00:41:16,000 --> 00:41:23,280
in terms of using the mobile phone that we all carry in our pockets to give us both

531
00:41:23,280 --> 00:41:31,520
better tracking of our health state and better nudges to take a healthier lifestyle. Move us

532
00:41:31,520 --> 00:41:38,000
towards the tricorder? Yeah, move us forward to tricorder. I think the challenges here are that the

533
00:41:38,560 --> 00:41:45,360
folks that carry these devices and require those lifestyle changes are not nearly as obedient as

534
00:41:45,360 --> 00:41:52,320
the folks on Star Trek. So compliance is an issue. I think there's some interesting work that's

535
00:41:52,320 --> 00:41:59,200
happening in that space, but it's a road that has its own challenges. It's not maybe as much of

536
00:41:59,200 --> 00:42:05,440
a scientific challenge, but there's a lot of questions about human psychology and affecting

537
00:42:05,440 --> 00:42:12,560
behavior change. I think there's some interesting work that's happening in hospitals as well as

538
00:42:12,560 --> 00:42:22,640
potentially in insurance companies for early warning systems for getting people into care or

539
00:42:23,760 --> 00:42:29,360
warning the physicians and an emergency care ward that sounds a lot to crash or about to have

540
00:42:30,080 --> 00:42:34,800
a sepsis attack or something, and I think that's an interesting space. I think there's a lot of

541
00:42:34,800 --> 00:42:41,360
places where one can ask with the new tools that we have in hand the new ability to collect large

542
00:42:41,360 --> 00:42:46,880
amounts of data. What are problems that we can just solve that almost really ever try to solve

543
00:42:46,880 --> 00:42:52,160
before? Awesome. Awesome. Well, definitely thanks so much for taking some time to share with us

544
00:42:52,160 --> 00:42:56,720
what you're up to. Clearly very exciting stuff and pleasure meeting and chatting with you.

545
00:42:57,520 --> 00:43:00,000
It's been a pleasure for me too. Thank you for having me. Thank you.

546
00:43:02,400 --> 00:43:08,000
All right, everyone. That's our show for today. For more information on today's guest or our

547
00:43:08,000 --> 00:43:16,640
NURPS podcast series, head over to twimlai.com slash NURPS 2019. Thanks once again to Shell for

548
00:43:16,640 --> 00:43:23,680
sponsoring this week's series. Check out the shell.ai Residency program by typing shell.ai

549
00:43:23,680 --> 00:43:38,560
into your browsers address bar. Thanks so much for listening. Happy holidays and catch you next time.

